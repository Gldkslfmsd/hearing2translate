{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca58848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74575e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../../evaluation/output_evals/commonAccent\"\n",
    "DIRECTION_PAIRS = ['de_en','en_de','en_es','en_fr','en_it','en_pt','en_zh','es_en','it_en', 'en_nl']\n",
    "SYSTEM_NAMES = [\"aya_canary-v2\", \"aya_owsm4.0-ctc\",\"aya_seamlessm4t\",\"aya_whisper\",\n",
    "                \"canary-v2\",\"desta2-8b\",\"gemma_canary-v2\",\"gemma_owsm4.0-ctc\",\"gemma_seamlessm4t\", \"gemma_whisper\",\n",
    "                \"owsm4.0-ctc\",\"phi4multimodal\",\"qwen2audio-7b\",\"seamlessm4t\",\n",
    "                \"tower_canary-v2\", \"tower_owsm4.0-ctc\",\"tower_seamlessm4t\",\"tower_whisper\",\n",
    "                \"voxtral-small-24b\",\"whisper\",\"spirelm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7859e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_summaries(base_dir, direction_pairs, system_names):\n",
    "    \"\"\"\n",
    "    Loads all result summaries from a directory structure.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str or Path): The base directory for the evaluation outputs.\n",
    "        direction_pairs (list): A list of language direction strings (e.g., 'en_de').\n",
    "        system_names (list): A list of system name strings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary containing the loaded data, structured as\n",
    "              {direction: {system: [results]}}.\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    all_results = {}\n",
    "\n",
    "    # Use itertools.product to cleanly iterate over all combinations\n",
    "    for direction, system in itertools.product(direction_pairs, system_names):\n",
    "        summary_path = base_path / system / direction / 'results.jsonl'\n",
    "        \n",
    "        # Initialize the nested dictionary structure\n",
    "        if direction not in all_results:\n",
    "            all_results[direction] = {}\n",
    "\n",
    "        try:\n",
    "            with summary_path.open('r', encoding='utf-8') as f:\n",
    "                all_results[direction][system] = [json.loads(line) for line in f]\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found, skipping: {summary_path}\")\n",
    "            all_results[direction][system] = None # Or [] if you prefer an empty list\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON in {summary_path}: {e}\")\n",
    "            all_results[direction][system] = None\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5e7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_dataframe(results_data):\n",
    "    \"\"\"\n",
    "    Converts the nested dictionary of results into a single pandas DataFrame.\n",
    "\n",
    "    Each row corresponds to a single entry, with 'direction' and 'system'\n",
    "    columns added, and all 'metrics' unpacked into separate columns.\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    for direction, systems in results_data.items():\n",
    "        for system, records in systems.items():\n",
    "            if records is None:\n",
    "                continue\n",
    "            for record in records:\n",
    "                # Separate metrics from the record\n",
    "                metrics = record.pop(\"metrics\", {})  # safely get metrics\n",
    "                # Merge everything into one flat dict\n",
    "                flat_record = {\n",
    "                    \"direction\": direction,\n",
    "                    \"system\": system,\n",
    "                    **record,\n",
    "                    **metrics,  # unpack metrics into top-level keys\n",
    "                }\n",
    "                all_records.append(flat_record)\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"No records were found to create a DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Put identifying info up front\n",
    "    original_cols = [c for c in df.columns if c not in [\"direction\", \"system\"]]\n",
    "    df = df[[\"direction\", \"system\"] + original_cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a59827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_accent_column(df, manifests_dir=\"../../manifests/commonAccent\"):\n",
    "    \"\"\"\n",
    "    Adds an 'accent' column to the DataFrame by reading all .jsonl files in the given directory.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing at least a 'sample_id' column.\n",
    "        manifests_dir (str or Path): Directory containing .jsonl manifest files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with a new 'accent' column.\n",
    "    \"\"\"\n",
    "    manifests_dir = Path(manifests_dir)\n",
    "    accent_map = {}\n",
    "\n",
    "    # Read all .jsonl files in the directory\n",
    "    for file in manifests_dir.glob(\"*.jsonl\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    sid = str(record.get(\"sample_id\"))  # keep as string for safety\n",
    "                    acc = record.get(\"benchmark_metadata\", {}).get(\"acc\")\n",
    "                    if sid and acc:\n",
    "                        accent_map[sid] = acc\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # skip bad lines just in case\n",
    "\n",
    "    if not accent_map:\n",
    "        print(\"No accent data found in manifest files.\")\n",
    "        df[\"accent\"] = None\n",
    "        return df\n",
    "\n",
    "    # Map the accent values onto the DataFrame\n",
    "    df = df.copy()\n",
    "    df[\"accent\"] = df[\"sample_id\"].astype(str).map(accent_map)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2ae405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accent_strict_scores(df):\n",
    "    \"\"\"\n",
    "    Computes mean metric scores and strict scores grouped by (system, accent).\n",
    "    \n",
    "    Expects columns:\n",
    "      - system\n",
    "      - accent\n",
    "      - xcomet_qe_score\n",
    "      - metricx_qe_score\n",
    "      - linguapy_score (list/tuple of [flag, lang])\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Split linguapy_score into two separate columns ---\n",
    "    df[[\"linguapy_flag\", \"linguapy_lang\"]] = pd.DataFrame(\n",
    "        df[\"linguapy_score\"].tolist(), index=df.index\n",
    "    )\n",
    "\n",
    "    # --- Define penalties ---\n",
    "    penalty_by_metric = {\n",
    "        \"metricx_qe\": 25,\n",
    "        \"xcomet_qe\": 0,\n",
    "    }\n",
    "\n",
    "    # --- Strict score per row ---\n",
    "    for metric in penalty_by_metric.keys():\n",
    "        df[f\"{metric}_strict\"] = df.apply(\n",
    "            lambda row: row[f\"{metric}_score\"]\n",
    "            if row[\"linguapy_flag\"] == 0\n",
    "            else penalty_by_metric[metric],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # --- Aggregate by system × accent ---\n",
    "    agg_cols = {\n",
    "        \"linguapy_flag\": \"mean\",  # average from 0–1\n",
    "    }\n",
    "    for metric in penalty_by_metric.keys():\n",
    "        agg_cols[f\"{metric}_score\"] = \"mean\"\n",
    "        agg_cols[f\"{metric}_strict\"] = \"mean\"\n",
    "\n",
    "    result = (\n",
    "        df.groupby([\"system\", \"accent\"])\n",
    "        .agg(agg_cols)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"linguapy_flag\": \"linguapy_avg\"})\n",
    "    )\n",
    "\n",
    "    result['linguapy_avg'] = result['linguapy_avg']*100\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cb608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/spirelm/de_en/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_de/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_es/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_fr/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_it/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_pt/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_zh/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/spirelm/es_en/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/spirelm/it_en/results.jsonl\n",
      "Warning: File not found, skipping: ../../evaluation/output_evals/commonAccent/whisper/en_nl/results.jsonl\n"
     ]
    }
   ],
   "source": [
    "results_full = load_results_summaries(BASE_DIR, DIRECTION_PAIRS, SYSTEM_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ac8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=convert_results_to_dataframe(results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b2e4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to add the column for accent ID\n",
    "df = add_accent_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80012370",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = {\n",
    "    \"linguapy_avg\":\"LinguaPy\",\n",
    "    \"metricx_qe_strict\":\"QEMetricX_24-Strict-linguapy\",\n",
    "    \"xcomet_qe_strict\": \"XCOMET-QE-Strict-linguapy\"\n",
    "}\n",
    "\n",
    "#Collapse and get the metrics balanced by the linguapy score\n",
    "for pair in DIRECTION_PAIRS:\n",
    "    sub_df = df[df['direction']==pair]\n",
    "    sub_df = compute_accent_strict_scores(sub_df)\n",
    "    #Standardize col names\n",
    "    sub_df = sub_df.rename(columns=col_map)\n",
    "    #Save \n",
    "    sub_df.to_csv(f\"commonAccent_{pair}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab275c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system</th>\n",
       "      <th>accent</th>\n",
       "      <th>LinguaPy</th>\n",
       "      <th>metricx_qe_score</th>\n",
       "      <th>QEMetricX_24-Strict-linguapy</th>\n",
       "      <th>xcomet_qe_score</th>\n",
       "      <th>XCOMET-QE-Strict-linguapy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aya_canary-v2</td>\n",
       "      <td>AUSTRALIAN ENGLISH</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.974901</td>\n",
       "      <td>2.599043</td>\n",
       "      <td>0.970017</td>\n",
       "      <td>0.941578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aya_canary-v2</td>\n",
       "      <td>CANADIAN ENGLISH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.141448</td>\n",
       "      <td>3.987522</td>\n",
       "      <td>0.927703</td>\n",
       "      <td>0.891560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aya_canary-v2</td>\n",
       "      <td>ENGLAND ENGLISH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.327688</td>\n",
       "      <td>4.327414</td>\n",
       "      <td>0.915470</td>\n",
       "      <td>0.876201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aya_canary-v2</td>\n",
       "      <td>FILIPINO</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.570571</td>\n",
       "      <td>3.226984</td>\n",
       "      <td>0.922223</td>\n",
       "      <td>0.895781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aya_canary-v2</td>\n",
       "      <td>GERMAN ENGLISH NON NATIVE SPEAKER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.110110</td>\n",
       "      <td>2.110110</td>\n",
       "      <td>0.953206</td>\n",
       "      <td>0.953206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>voxtral-small-24b</td>\n",
       "      <td>NEW ZEALAND ENGLISH</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.319101</td>\n",
       "      <td>3.018711</td>\n",
       "      <td>0.945032</td>\n",
       "      <td>0.915319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>voxtral-small-24b</td>\n",
       "      <td>SCOTTISH ENGLISH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.948510</td>\n",
       "      <td>2.880880</td>\n",
       "      <td>0.970694</td>\n",
       "      <td>0.932921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>voxtral-small-24b</td>\n",
       "      <td>SINGAPOREAN ENGLISH</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.777613</td>\n",
       "      <td>3.439629</td>\n",
       "      <td>0.938834</td>\n",
       "      <td>0.912830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>voxtral-small-24b</td>\n",
       "      <td>SOUTHERN AFRICAN SOUTH AFRICA ZIMBABWE NAMIBIA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.833046</td>\n",
       "      <td>3.432174</td>\n",
       "      <td>0.921223</td>\n",
       "      <td>0.897140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>voxtral-small-24b</td>\n",
       "      <td>UNITED STATES ENGLISH</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.412319</td>\n",
       "      <td>4.827361</td>\n",
       "      <td>0.899569</td>\n",
       "      <td>0.841354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                system                                          accent  \\\n",
       "0        aya_canary-v2                              AUSTRALIAN ENGLISH   \n",
       "1        aya_canary-v2                                CANADIAN ENGLISH   \n",
       "2        aya_canary-v2                                 ENGLAND ENGLISH   \n",
       "3        aya_canary-v2                                        FILIPINO   \n",
       "4        aya_canary-v2               GERMAN ENGLISH NON NATIVE SPEAKER   \n",
       "..                 ...                                             ...   \n",
       "275  voxtral-small-24b                             NEW ZEALAND ENGLISH   \n",
       "276  voxtral-small-24b                                SCOTTISH ENGLISH   \n",
       "277  voxtral-small-24b                             SINGAPOREAN ENGLISH   \n",
       "278  voxtral-small-24b  SOUTHERN AFRICAN SOUTH AFRICA ZIMBABWE NAMIBIA   \n",
       "279  voxtral-small-24b                           UNITED STATES ENGLISH   \n",
       "\n",
       "     LinguaPy  metricx_qe_score  QEMetricX_24-Strict-linguapy  \\\n",
       "0         3.0          1.974901                      2.599043   \n",
       "1         4.0          3.141448                      3.987522   \n",
       "2         5.0          3.327688                      4.327414   \n",
       "3         3.0          2.570571                      3.226984   \n",
       "4         0.0          2.110110                      2.110110   \n",
       "..        ...               ...                           ...   \n",
       "275       3.0          2.319101                      3.018711   \n",
       "276       4.0          1.948510                      2.880880   \n",
       "277       3.0          2.777613                      3.439629   \n",
       "278       3.0          2.833046                      3.432174   \n",
       "279       6.0          3.412319                      4.827361   \n",
       "\n",
       "     xcomet_qe_score  XCOMET-QE-Strict-linguapy  \n",
       "0           0.970017                   0.941578  \n",
       "1           0.927703                   0.891560  \n",
       "2           0.915470                   0.876201  \n",
       "3           0.922223                   0.895781  \n",
       "4           0.953206                   0.953206  \n",
       "..               ...                        ...  \n",
       "275         0.945032                   0.915319  \n",
       "276         0.970694                   0.932921  \n",
       "277         0.938834                   0.912830  \n",
       "278         0.921223                   0.897140  \n",
       "279         0.899569                   0.841354  \n",
       "\n",
       "[280 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf43d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saved    commonAccent_LinguaPy_pivot.csv\n",
       "rows                                  21\n",
       "cols                                  10\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "saved    commonAccent_metricx_qe_score_pivot.csv\n",
       "rows                                          21\n",
       "cols                                          10\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "saved    commonAccent_QEMetricX_24-Strict-linguapy_pivo...\n",
       "rows                                                    21\n",
       "cols                                                    10\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "saved    commonAccent_xcomet_qe_score_pivot.csv\n",
       "rows                                         21\n",
       "cols                                         10\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "saved    commonAccent_XCOMET-QE-Strict-linguapy_pivot.csv\n",
       "rows                                                   21\n",
       "cols                                                   10\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Read all CSVs produced per direction\n",
    "csv_files = glob.glob(\"commonAccent_*.csv\")\n",
    "all_frames = []\n",
    "for path in csv_files:\n",
    "    tmp = pd.read_csv(path)\n",
    "    # infer direction from filename: commonAccent_{pair}.csv\n",
    "    pair = path.split(\"commonAccent_\")[-1].rsplit(\".csv\", 1)[0]\n",
    "    tmp[\"direction\"] = pair\n",
    "    all_frames.append(tmp)\n",
    "\n",
    "if not all_frames:\n",
    "    raise RuntimeError(\"No commonAccent_*.csv files found in the working directory.\")\n",
    "\n",
    "full = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "# Normalize column names that might vary in capitalization or presence\n",
    "# Expected useful columns: system, accent, direction, metrics...\n",
    "non_metric_cols = {\"system\", \"accent\", \"direction\"}\n",
    "metric_columns = [c for c in full.columns if c not in non_metric_cols]\n",
    "\n",
    "# Aggregate across accents for each (system, direction)\n",
    "agg = (\n",
    "    full.groupby([\"system\", \"direction\"], as_index=False)[metric_columns]\n",
    "        .mean()\n",
    ")\n",
    "\n",
    "# Desired direction column order (expanded for commonAccent)\n",
    "desired_order = [\n",
    "    \"en_de\", \"en_es\", \"en_fr\", \"en_it\", \"en_pt\", \"en_zh\",\n",
    "    \"de_en\", \"es_en\", \"it_en\"\n",
    "]\n",
    "\n",
    "# Create and save one pivot per metric\n",
    "for metric in metric_columns:\n",
    "    pivot_df = agg.pivot_table(index=\"system\", columns=\"direction\", values=metric, aggfunc=\"mean\")\n",
    "    # Reorder columns based on desired order (keep only those present)\n",
    "    cols = [c for c in desired_order if c in pivot_df.columns]\n",
    "    # Append any remaining directions not listed to the end (stable order)\n",
    "    remaining = [c for c in pivot_df.columns if c not in cols]\n",
    "    pivot_df = pivot_df[cols + remaining]\n",
    "\n",
    "    # # Add average column across available directions\n",
    "    # pivot_df[\"average\"] = pivot_df.mean(axis=1)\n",
    "\n",
    "    # # Sort by average desc\n",
    "    # pivot_df = pivot_df.sort_values(\"average\", ascending=False)\n",
    "\n",
    "    # Save\n",
    "    out_name = f\"commonAccent_{metric}_pivot.csv\"\n",
    "    pivot_df.to_csv(out_name)\n",
    "\n",
    "    display(pd.Series({\"saved\": out_name, \"rows\": len(pivot_df), \"cols\": len(pivot_df.columns)}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7201de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved commonAccent_XCOMET-QE-Strict-linguapy_table_source_en_by_accent.csv shape=(21, 98)\n",
      "Saved commonAccent_XCOMET-QE-Strict-linguapy_table_source_de_by_accent.csv shape=(21, 5)\n",
      "Saved commonAccent_XCOMET-QE-Strict-linguapy_table_source_es_by_accent.csv shape=(21, 6)\n",
      "Saved commonAccent_XCOMET-QE-Strict-linguapy_table_source_it_by_accent.csv shape=(21, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load all per-direction CSVs you already wrote\n",
    "paths = glob.glob(\"commonAccent_*.csv\")\n",
    "dfs = []\n",
    "for p in paths:\n",
    "    pair = p.split(\"commonAccent_\")[-1].rsplit(\".csv\", 1)[0]\n",
    "    t = pd.read_csv(p)\n",
    "    t[\"direction\"] = pair  # e.g., en_de\n",
    "    dfs.append(t)\n",
    "\n",
    "full = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "metric = \"XCOMET-QE-Strict-linguapy\"\n",
    "\n",
    "# sanity check\n",
    "needed = {\"system\", \"accent\", \"direction\", metric}\n",
    "missing = needed - set(full.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "# Define source → directions mapping (extend as needed)\n",
    "source_to_dirs = {\n",
    "    \"en\": sorted([d for d in full[\"direction\"].unique() if d.startswith(\"en_\")]),\n",
    "    \"de\": sorted([d for d in full[\"direction\"].unique() if d.startswith(\"de_\")]),\n",
    "    \"es\": sorted([d for d in full[\"direction\"].unique() if d.startswith(\"es_\")]),\n",
    "    \"it\": sorted([d for d in full[\"direction\"].unique() if d.startswith(\"it_\")]),\n",
    "}\n",
    "\n",
    "# Optional: enforce a preferred order inside each source\n",
    "preferred_order_global = [\"en_de\",\"en_zh\",\"en_es\",\"en_fr\",\"en_it\",\"en_pt\",\"de_en\",\"es_en\",\"it_en\",\"pt_en\",\"zh_en\"]\n",
    "\n",
    "def order_dirs(dirs, order):\n",
    "    return [d for d in order if d in dirs] + [d for d in dirs if d not in order]\n",
    "\n",
    "row_order = [\"whisper\",\"seamlessm4t\",\"canary-v2\",\"owsm4.0-ctc\",\"aya_whisper\",\n",
    "            \"gemma_whisper\",\"tower_whisper\",\"aya_seamlessm4t\",\"gemma_seamlessm4t\",\n",
    "            \"tower_seamlessm4t\",\"aya_canary-v2\",\"gemma_canary-v2\",\"tower_canary-v2\",\n",
    "            \"aya_owsm4.0-ctc\",\"gemma_owsm4.0-ctc\",\"tower_owsm4.0-ctc\",\"desta2-8b\",\n",
    "            \"qwen2audio-7b\",\"phi4multimodal\",\"voxtral-small-24b\",\"spirelm\"]\n",
    "\n",
    "# B) Optional: per-accent breakdown (MultiIndex columns = (direction, accent)) for English\n",
    "for src in [\"en\",\"de\",\"es\",\"it\"]:\n",
    "    dirs = order_dirs(source_to_dirs[src], preferred_order_global)\n",
    "    sub = full[full[\"direction\"].isin(dirs)]\n",
    "    mega = sub.pivot_table(\n",
    "        index=\"system\",\n",
    "        columns=[\"direction\", \"accent\"],\n",
    "        values=metric,\n",
    "        aggfunc=\"mean\",\n",
    "    )\n",
    "\n",
    "    # reorder first level (direction) according to preferred order\n",
    "    lvl0 = list(mega.columns.levels[0])\n",
    "    ordered_lvl0 = [d for d in preferred_order_global if d in lvl0] + [d for d in lvl0 if d not in preferred_order_global]\n",
    "    mega = mega.reindex(columns=pd.MultiIndex.from_product([ordered_lvl0, mega.columns.levels[1]]))\n",
    "    mega = mega.reindex(index=row_order)\n",
    "    out = f\"commonAccent_{metric}_table_source_{src}_by_accent.csv\"\n",
    "    mega.to_csv(out)\n",
    "    print(f\"Saved {out} shape={mega.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6992d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2x/p1hwp9tj16v7r_c_lnx6xc6rbpr63k/T/ipykernel_95937/93985825.py:37: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'saved': 'viz_commonAccent_XCOMET-QE-Strict-linguapy_source_en_by_accent_simple.png',\n",
       " 'shape': (21, 98)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2x/p1hwp9tj16v7r_c_lnx6xc6rbpr63k/T/ipykernel_95937/93985825.py:37: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'saved': 'viz_commonAccent_XCOMET-QE-Strict-linguapy_source_de_by_accent_simple.png',\n",
       " 'shape': (21, 5)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2x/p1hwp9tj16v7r_c_lnx6xc6rbpr63k/T/ipykernel_95937/93985825.py:37: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'saved': 'viz_commonAccent_XCOMET-QE-Strict-linguapy_source_es_by_accent_simple.png',\n",
       " 'shape': (21, 6)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2x/p1hwp9tj16v7r_c_lnx6xc6rbpr63k/T/ipykernel_95937/93985825.py:37: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'saved': 'viz_commonAccent_XCOMET-QE-Strict-linguapy_source_it_by_accent_simple.png',\n",
       " 'shape': (21, 5)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "metric = \"XCOMET-QE-Strict-linguapy\"\n",
    "source_csvs = {\n",
    "    \"en\": f\"commonAccent_{metric}_table_source_en_by_accent.csv\",\n",
    "    \"de\": f\"commonAccent_{metric}_table_source_de_by_accent.csv\",\n",
    "    \"es\": f\"commonAccent_{metric}_table_source_es_by_accent.csv\",\n",
    "    \"it\": f\"commonAccent_{metric}_table_source_it_by_accent.csv\",\n",
    "}\n",
    "\n",
    "# Load frames exactly as-is (keep index/columns order untouched)\n",
    "frames = {}\n",
    "all_vals = []\n",
    "for src, path in source_csvs.items():\n",
    "    df = pd.read_csv(path, header=[0,1], index_col=0)\n",
    "    frames[src] = df\n",
    "    vals = df.values.flatten()\n",
    "    vals = vals[~pd.isna(vals)]\n",
    "    all_vals.append(pd.Series(vals))\n",
    "\n",
    "# Shared color scale across all sources (optional but helpful for comparison)\n",
    "if all_vals:\n",
    "    all_vals = pd.concat(all_vals, ignore_index=True)\n",
    "    vmin, vmax = float(all_vals.min()), float(all_vals.max())\n",
    "else:\n",
    "    vmin = vmax = None\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "for src, df in frames.items():\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(df, ax=ax, cmap=\"magma\", vmin=vmin, vmax=vmax, cbar_kws={\"label\": metric})\n",
    "    ax.set_title(f\"{metric} (source={src})\")\n",
    "    ax.set_xlabel(\"Direction × Accent\")\n",
    "    ax.set_ylabel(\"System\")\n",
    "    plt.tight_layout()\n",
    "    out_png = f\"viz_commonAccent_{metric}_source_{src}_by_accent_simple.png\"\n",
    "    fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    display({\"saved\": out_png, \"shape\": df.shape})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bea14c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Source: en (per-accent averages) ===\n",
      "accent\n",
      "AUSTRALIAN ENGLISH                                0.857\n",
      "CANADIAN ENGLISH                                  0.847\n",
      "ENGLAND ENGLISH                                   0.806\n",
      "FILIPINO                                          0.834\n",
      "GERMAN ENGLISH NON NATIVE SPEAKER                 0.857\n",
      "HONG KONG ENGLISH                                 0.814\n",
      "INDIA AND SOUTH ASIA INDIA PAKISTAN SRI LANKA     0.784\n",
      "IRISH ENGLISH                                     0.801\n",
      "MALAYSIAN ENGLISH                                 0.794\n",
      "NEW ZEALAND ENGLISH                               0.851\n",
      "SCOTTISH ENGLISH                                  0.860\n",
      "SINGAPOREAN ENGLISH                               0.833\n",
      "SOUTHERN AFRICAN SOUTH AFRICA ZIMBABWE NAMIBIA    0.853\n",
      "UNITED STATES ENGLISH                             0.797\n",
      "dtype: float64\n",
      "\n",
      "=== Source: de (per-accent averages) ===\n",
      "accent\n",
      "DEUTSCHLAND DEUTSCH                                                  0.798\n",
      "ITALIENISCH DEUTSCH                                                  0.848\n",
      "NORDRHEIN WESTFALEN BUNDESDEUTSCH HOCHDEUTSCH DEUTSCHLAND DEUTSCH    0.815\n",
      "OSTERREICHISCHES DEUTSCH                                             0.752\n",
      "SCHWEIZERDEUTSCH                                                     0.795\n",
      "dtype: float64\n",
      "\n",
      "=== Source: es (per-accent averages) ===\n",
      "accent\n",
      "ANDINOPACIFICO COLOMBIA PERU ECUADOR OESTE DE BOLIVIA Y VENEZUELA ANDINA                                                     0.845\n",
      "CARIBE CUBA VENEZUELA PUERTO RICO REPUBLICA DOMINICANA PANAMA COLOMBIA CARIBENA MEXICO CARIBENO COSTA DEL GOLFO DE MEXICO    0.816\n",
      "CHILENO CHILE CUYO                                                                                                           0.817\n",
      "ESPANA SUR PENINSULAR (ANDALUCIA EXTREMADURA MURCIA)                                                                         0.819\n",
      "MEXICO                                                                                                                       0.829\n",
      "RIOPLATENSE ARGENTINA URUGUAY ESTE DE BOLIVIA PARAGUAY                                                                       0.803\n",
      "dtype: float64\n",
      "\n",
      "=== Source: it (per-accent averages) ===\n",
      "accent\n",
      "BASILICATA TRENTINO                  0.760\n",
      "EMILIANO                             0.830\n",
      "MERIDIONALE                          0.855\n",
      "TENDENTE AL SICULO MA NON MARCATO    0.820\n",
      "VENETO                               0.778\n",
      "dtype: float64\n",
      "\n",
      "Saved combined summary: summary_XCOMET-QE-Strict-linguapy_per_accent_all_sources.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metric = \"XCOMET-QE-Strict-linguapy\"\n",
    "source_csvs = {\n",
    "    \"en\": f\"commonAccent_{metric}_table_source_en_by_accent.csv\",\n",
    "    \"de\": f\"commonAccent_{metric}_table_source_de_by_accent.csv\",\n",
    "    \"es\": f\"commonAccent_{metric}_table_source_es_by_accent.csv\",\n",
    "    \"it\": f\"commonAccent_{metric}_table_source_it_by_accent.csv\",\n",
    "}\n",
    "\n",
    "summaries = {}\n",
    "\n",
    "for src, path in source_csvs.items():\n",
    "    df = pd.read_csv(path, header=[0,1], index_col=0)\n",
    "    # average per (direction, accent) column, then average across directions → per-accent\n",
    "    per_col_means = df.mean(axis=0, skipna=True)\n",
    "    per_accent = per_col_means.groupby(level=1).mean().sort_index()\n",
    "    summaries[src] = per_accent.round(3)\n",
    "    print(f\"\\n=== Source: {src} (per-accent averages) ===\")\n",
    "    print(summaries[src])\n",
    "\n",
    "    # save each summary\n",
    "    per_accent.to_csv(f\"summary_{metric}_per_accent_source_{src}.csv\", header=[\"mean\"])\n",
    "\n",
    "# If you want a combined table across sources:\n",
    "combined = pd.DataFrame(summaries).T  # rows: source, cols: accent\n",
    "# combined.to_csv(f\"summary_{metric}_per_accent_all_sources.csv\")\n",
    "print(\"\\nSaved combined summary:\", f\"summary_{metric}_per_accent_all_sources.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de5d3b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'saved_png': 'viz_std_XCOMET-QE-Strict-linguapy_all_sources_by_direction.png',\n",
       " 'saved_csv': 'std_XCOMET-QE-Strict-linguapy_all_sources_by_direction.csv',\n",
       " 'shape': (21, 10)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "metric = \"XCOMET-QE-Strict-linguapy\"\n",
    "source_csvs = {\n",
    "    \"en\": f\"commonAccent_{metric}_table_source_en_by_accent.csv\",\n",
    "    \"de\": f\"commonAccent_{metric}_table_source_de_by_accent.csv\",\n",
    "    \"es\": f\"commonAccent_{metric}_table_source_es_by_accent.csv\",\n",
    "    \"it\": f\"commonAccent_{metric}_table_source_it_by_accent.csv\",\n",
    "}\n",
    "\n",
    "# Collect std data from all sources\n",
    "all_std_data = []\n",
    "\n",
    "for src, path in source_csvs.items():\n",
    "    df = pd.read_csv(path, header=[0,1], index_col=0)\n",
    "    \n",
    "    # Compute std across accents for each (system, direction) pair\n",
    "    for system in df.index:\n",
    "        for direction in df.columns.get_level_values(0).unique():\n",
    "            # Get all accent columns for this direction\n",
    "            direction_cols = df.loc[system, (direction, slice(None))]\n",
    "            # Compute std across accents (skip NaN)\n",
    "            std_val = direction_cols.std(skipna=True)\n",
    "            all_std_data.append({\n",
    "                'system': system,\n",
    "                'direction': direction,\n",
    "                'std': std_val\n",
    "            })\n",
    "\n",
    "# Create combined pivot table: system × direction\n",
    "std_df = pd.DataFrame(all_std_data)\n",
    "pivot_std = std_df.pivot(index='system', columns='direction', values='std')\n",
    "\n",
    "\n",
    "# Optional: order columns by preferred order if you want\n",
    "preferred_order_global = [\"en_de\",\"en_es\",\"en_fr\",\"en_it\",\"en_nl\",\"en_pt\",\"en_zh\",\"de_en\",\"es_en\",\"it_en\"]\n",
    "cols_ordered = [c for c in preferred_order_global if c in pivot_std.columns]\n",
    "cols_remaining = [c for c in pivot_std.columns if c not in cols_ordered]\n",
    "pivot_std = pivot_std[cols_ordered + cols_remaining]\n",
    "\n",
    "# Keep only the systems that exist in the pivot table\n",
    "rows_ordered = [r for r in row_order if r in pivot_std.index]\n",
    "rows_remaining = [r for r in pivot_std.index if r not in row_order]\n",
    "pivot_std = pivot_std.loc[rows_ordered + rows_remaining]\n",
    "\n",
    "# Create one giant heatmap\n",
    "fig, ax = plt.subplots(figsize=(max(12, 0.4*len(pivot_std.columns)), max(8, 0.35*len(pivot_std.index))))\n",
    "sns.heatmap(pivot_std, ax=ax, cmap=\"viridis\", cbar_kws={\"label\": f\"Std Dev of {metric} across accents\"})\n",
    "ax.set_title(f\"Std Dev of {metric} across accents (all source languages)\")\n",
    "ax.set_xlabel(\"Translation Direction\")\n",
    "ax.set_ylabel(\"System\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = f\"viz_std_{metric}_all_sources_by_direction.png\"\n",
    "fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Save the combined std pivot table\n",
    "out_csv = f\"std_{metric}_all_sources_by_direction.csv\"\n",
    "pivot_std.to_csv(out_csv)\n",
    "\n",
    "display({\"saved_png\": out_png, \"saved_csv\": out_csv, \"shape\": pivot_std.shape})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
