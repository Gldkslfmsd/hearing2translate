{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9670989513397217, "xcomet_qe_score": 0.9718614816665649, "metricx_score": 0.2643663287162781, "metricx_qe_score": 0.26394033432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们的演示,我们将介绍 d.plain,这是一种用于识别德语文本的新语料库,可以识别文档级别和句子级别的德语文本。", "metrics": {"bleu_score": 21.100289694420674, "chrf_score": 21.879508254121323, "xcomet_score": 0.8924585580825806, "xcomet_qe_score": 0.8614959716796875, "metricx_score": 2.917642593383789, "metricx_qe_score": 3.318267345428467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是丽吉娜·斯托登,我将引导大家完成演示文稿的第一部分。", "metrics": {"bleu_score": 20.16247778480566, "chrf_score": 17.39385015895954, "xcomet_score": 0.8226897716522217, "xcomet_qe_score": 0.935484766960144, "metricx_score": 3.4641218185424805, "metricx_qe_score": 3.5061750411987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们先定义文本简化。", "metrics": {"bleu_score": 24.435718535490405, "chrf_score": 23.65744359996451, "xcomet_score": 0.9885905981063843, "xcomet_qe_score": 0.9913504123687744, "metricx_score": 0.2445794641971588, "metricx_qe_score": 0.3208388388156891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是指为了提高特定目标群体(如阅读有困难的人或非母语人士)对文本的理解能力而对文本进行的调整过程。", "metrics": {"bleu_score": 43.09458036856673, "chrf_score": 37.78100057652266, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.348842054605484, "metricx_qe_score": 0.37682098150253296, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要训练一个文本简化模型,我们需要文本的平行对,例如文档或句子。", "metrics": {"bleu_score": 52.59790643877004, "chrf_score": 47.51989556549584, "xcomet_score": 0.9886820316314697, "xcomet_qe_score": 0.870783805847168, "metricx_score": 1.9100451469421387, "metricx_qe_score": 2.4445064067840576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,您可以看到一个复杂的德语句子与其译为平白语言的句子对的平行对齐。 为了简化句子,可以", "metrics": {"bleu_score": 37.999133917393884, "chrf_score": 40.40062258348452, "xcomet_score": 0.4977242052555084, "xcomet_qe_score": 0.46785768866539, "metricx_score": 6.818272590637207, "metricx_qe_score": 4.635697841644287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "采用不同的技巧,例如在示例中所示的词汇替换、短语组合、短语重新排列或插入词语等。", "metrics": {"bleu_score": 23.227624748800444, "chrf_score": 20.4490923429255, "xcomet_score": 0.8416831493377686, "xcomet_qe_score": 0.8342992663383484, "metricx_score": 4.665316104888916, "metricx_qe_score": 4.854018688201904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们新的语料库,dplane。因为近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 54.252324657610544, "chrf_score": 41.32648776771094, "xcomet_score": 0.6685218811035156, "xcomet_qe_score": 0.7141643762588501, "metricx_score": 5.856634140014648, "metricx_qe_score": 6.150026321411133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法训练分类模型。", "metrics": {"bleu_score": 35.743397031672636, "chrf_score": 32.20663918239359, "xcomet_score": 0.8950200080871582, "xcomet_qe_score": 0.8340485095977783, "metricx_score": 3.0722250938415527, "metricx_qe_score": 2.363635540008545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,我提出的其他三种模型都是自动对齐的,这意味着它们在对齐时可能会出现错误。", "metrics": {"bleu_score": 63.58993962503769, "chrf_score": 60.649545300257856, "xcomet_score": 0.9134252071380615, "xcomet_qe_score": 0.8969177007675171, "metricx_score": 1.344141960144043, "metricx_qe_score": 1.0551384687423706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了新的语料库 dplane,它被分为两个子语料库,dplane-apa 和 dplane-web。", "metrics": {"bleu_score": 47.209241370742745, "chrf_score": 30.539322741621362, "xcomet_score": 0.8928939700126648, "xcomet_qe_score": 0.8719722032546997, "metricx_score": 4.900674819946289, "metricx_qe_score": 4.681735992431641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dplane-apa 基于使用文本。", "metrics": {"bleu_score": 25.848657697858535, "chrf_score": 27.741767220507825, "xcomet_score": 0.7648651599884033, "xcomet_qe_score": 0.6558792591094971, "metricx_score": 6.463590145111084, "metricx_qe_score": 10.288641929626465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在APA格式中,我们手动对齐了483份文档,", "metrics": {"bleu_score": 44.500506580862066, "chrf_score": 34.89910162187792, "xcomet_score": 0.8925552368164062, "xcomet_qe_score": 0.913216233253479, "metricx_score": 1.4721375703811646, "metricx_qe_score": 1.6425461769104004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约有30,000对、13,000对句子对齐。", "metrics": {"bleu_score": 6.917184228205472, "chrf_score": 25.091322308935858, "xcomet_score": 0.7131523489952087, "xcomet_qe_score": 0.7787015438079834, "metricx_score": 5.416262626647949, "metricx_qe_score": 5.163002014160156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对 DeepLaneWeb。该语料库包括不同的领域,我们一方面通过人工对齐,另一方面通过自动对齐方法对这 750 个文档进行了对齐。", "metrics": {"bleu_score": 51.25778471956535, "chrf_score": 40.232480996007794, "xcomet_score": 0.791115403175354, "xcomet_qe_score": 0.7666767239570618, "metricx_score": 2.626978635787964, "metricx_qe_score": 3.133415937423706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有 30,450 对句子。", "metrics": {"bleu_score": 8.816389211763417, "chrf_score": 30.60502326369813, "xcomet_score": 0.8794432878494263, "xcomet_qe_score": 0.8015264272689819, "metricx_score": 0.8639643788337708, "metricx_qe_score": 1.0916264057159424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析。例如,关于半否定句的类型。", "metrics": {"bleu_score": 31.50529057112362, "chrf_score": 25.610938018983997, "xcomet_score": 0.7609174251556396, "xcomet_qe_score": 0.7179323434829712, "metricx_score": 5.794253349304199, "metricx_qe_score": 5.864738464355469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所见,圣经文本的简化程度远高于新闻文本或语言学习文本。", "metrics": {"bleu_score": 41.172305729118776, "chrf_score": 35.38619426550461, "xcomet_score": 0.9761154651641846, "xcomet_qe_score": 0.974339485168457, "metricx_score": 1.1534197330474854, "metricx_qe_score": 1.076761245727539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在各个层面进行简化,例如词汇简化、结构简化,以及整体简化水平。", "metrics": {"bleu_score": 60.00295230821618, "chrf_score": 61.06370732948723, "xcomet_score": 0.9732497930526733, "xcomet_qe_score": 0.9693154096603394, "metricx_score": 0.6043821573257446, "metricx_qe_score": 0.8638153076171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 Deplane 语料库具有多种不同的简化转换。", "metrics": {"bleu_score": 76.61850354609348, "chrf_score": 61.71191314116865, "xcomet_score": 0.9140536189079285, "xcomet_qe_score": 0.8671287298202515, "metricx_score": 2.6036596298217773, "metricx_qe_score": 3.7517874240875244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 Deplane API 语料库中,我们有更多的重新排序和添加单词,而在 Deplane Web 语料库中则较少。", "metrics": {"bleu_score": 21.894254112401978, "chrf_score": 20.29715481336797, "xcomet_score": 0.6562767624855042, "xcomet_qe_score": 0.7099659442901611, "metricx_score": 2.7832345962524414, "metricx_qe_score": 2.4610350131988525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们有更多的改写。", "metrics": {"bleu_score": 43.87642682549092, "chrf_score": 35.11396341346024, "xcomet_score": 0.9174699783325195, "xcomet_qe_score": 0.957047700881958, "metricx_score": 1.6014665365219116, "metricx_qe_score": 2.4461960792541504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,让我们来看看我们可以用这个语料库做什么。", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 45.88241328708863, "xcomet_score": 0.9967517852783203, "xcomet_qe_score": 0.9831986427307129, "metricx_score": 0.2658177614212036, "metricx_qe_score": 0.47158384323120117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,接下来我将谈谈我们数据集 D-plane 的使用案例。", "metrics": {"bleu_score": 12.24134694947121, "chrf_score": 15.37698649367916, "xcomet_score": 0.8761402368545532, "xcomet_qe_score": 0.8697081208229065, "metricx_score": 1.6940624713897705, "metricx_qe_score": 2.2330517768859863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个使用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 67.73709971213142, "chrf_score": 62.89481874621193, "xcomet_score": 0.9807659387588501, "xcomet_qe_score": 0.972801685333252, "metricx_score": 0.8053840398788452, "metricx_qe_score": 0.9386290907859802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了许多对齐方法,但在机器翻译的背景下。 当我们有两个用不同语言撰写的平行文档,并且我们希望从后置文档中提取句子的对齐时。", "metrics": {"bleu_score": 42.90307896012841, "chrf_score": 39.48178400602297, "xcomet_score": 0.6693891286849976, "xcomet_qe_score": 0.6639018654823303, "metricx_score": 3.1546425819396973, "metricx_qe_score": 3.261223793029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的用例中,我们试图提取具有相同语言、相同内容但复杂度不同的两个平行文档之间的对齐", "metrics": {"bleu_score": 43.38468778970447, "chrf_score": 40.161262303105886, "xcomet_score": 0.8977975249290466, "xcomet_qe_score": 0.8479957580566406, "metricx_score": 2.0488333702087402, "metricx_qe_score": 1.708892822265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们有了数据集 D-plane,其中包含了手动对齐的句子,我们可以将这些句子作为黄金标准对齐,来评估一些提出的对齐方法。", "metrics": {"bleu_score": 49.74182887851068, "chrf_score": 36.91555463231297, "xcomet_score": 0.7945371866226196, "xcomet_qe_score": 0.7780405282974243, "metricx_score": 3.511357545852661, "metricx_qe_score": 3.6965889930725098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对所提出的方法进行了一些改编,并在论文中公布了所有这些改编以及运行实验的代码。", "metrics": {"bleu_score": 39.64174776017769, "chrf_score": 37.700458593263086, "xcomet_score": 0.9850560426712036, "xcomet_qe_score": 0.9873765707015991, "metricx_score": 1.1845585107803345, "metricx_qe_score": 0.9517837762832642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于简化德语文本的最佳自动对齐方法是大规模对齐方法。", "metrics": {"bleu_score": 49.24790605054522, "chrf_score": 39.96429418347462, "xcomet_score": 0.9281625747680664, "xcomet_qe_score": 0.986614465713501, "metricx_score": 1.4487638473510742, "metricx_qe_score": 1.748847484588623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以在论文中找到在自己的文档上运行此方法的代码。", "metrics": {"bleu_score": 45.63498760673703, "chrf_score": 40.55176478107618, "xcomet_score": 0.9931855201721191, "xcomet_qe_score": 0.9818342924118042, "metricx_score": 0.5408803820610046, "metricx_qe_score": 0.5639521479606628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是一个自动文本简化的案例。 通过对语言模型进行微调,使其能够从复杂的输入文本中生成简化文本。", "metrics": {"bleu_score": 57.386069090611095, "chrf_score": 60.80243690178716, "xcomet_score": 0.9974961280822754, "xcomet_qe_score": 0.9908608198165894, "metricx_score": 0.6816537380218506, "metricx_qe_score": 0.9428771734237671, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对两种不同的模型进行了微调。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.998328447341919, "xcomet_qe_score": 0.9891341924667358, "metricx_score": 0.33017244935035706, "metricx_qe_score": 0.5325762033462524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对长期影响的模型进行了微调,以生成文档级别的简化。 我们还对正常基线进行了微调,部分地对正常基线进行了微调,以生成句子级别的简化。 ", "metrics": {"bleu_score": 28.471561081588806, "chrf_score": 25.274866034732057, "xcomet_score": 0.5470447540283203, "xcomet_qe_score": 0.47234290838241577, "metricx_score": 8.216115951538086, "metricx_qe_score": 8.722501754760742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以在论文中找到所有的检查点,并查看我们实验的详细分数和评估指标。", "metrics": {"bleu_score": 46.096712948023686, "chrf_score": 38.66449512644396, "xcomet_score": 0.9768184423446655, "xcomet_qe_score": 0.9531623125076294, "metricx_score": 0.9563922882080078, "metricx_qe_score": 1.3651481866836548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得比基准分数更好的分数。 我们提议将这些结果作为基准,作为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 60.20451894666038, "chrf_score": 56.02067343033386, "xcomet_score": 0.8995770215988159, "xcomet_qe_score": 0.825614333152771, "metricx_score": 2.7872657775878906, "metricx_qe_score": 3.188969612121582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们希望在会议期间见到大家。", "metrics": {"bleu_score": 46.11411579665311, "chrf_score": 40.155067083822956, "xcomet_score": 0.9945908784866333, "xcomet_qe_score": 0.9959206581115723, "metricx_score": 0.6428474187850952, "metricx_qe_score": 0.37971922755241394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·施皮尔科夫斯基,今天我们要讨论的主题是并列句的依存结构。", "metrics": {"bleu_score": 8.20956843607846, "chrf_score": 7.919710349946614, "xcomet_score": 0.6874163150787354, "xcomet_qe_score": 0.5905687808990479, "metricx_score": 1.9518256187438965, "metricx_qe_score": 1.3771651983261108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所知,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 64.57665807819532, "chrf_score": 63.353047287232236, "xcomet_score": 0.9224264621734619, "xcomet_qe_score": 0.8208955526351929, "metricx_score": 0.6283317804336548, "metricx_qe_score": 0.7737802267074585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在普遍依存关系中,Lisa、Bart 和 Maggie 的协调结构 即第一个连接词是整个并列结构的主语,所以", "metrics": {"bleu_score": 43.12547697679031, "chrf_score": 48.79584955673801, "xcomet_score": 0.7146923542022705, "xcomet_qe_score": 0.4242592751979828, "metricx_score": 5.398842811584473, "metricx_qe_score": 4.387233734130859, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中是 Lisa。 伊戈", "metrics": {"bleu_score": 46.65904311461236, "chrf_score": 41.425266320628104, "xcomet_score": 0.6958458423614502, "xcomet_qe_score": 0.6854884028434753, "metricx_score": 4.217318058013916, "metricx_qe_score": 2.068490743637085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义-文本理论也采用了类似的方法,同样地,整个并列结构由第一个并列词引导。因此,", "metrics": {"bleu_score": 38.80030125743474, "chrf_score": 28.685135244205505, "xcomet_score": 0.4640280604362488, "xcomet_qe_score": 0.3777347505092621, "metricx_score": 6.8381452560424805, "metricx_qe_score": 5.363505840301514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是异步的。", "metrics": {"bleu_score": 38.24602282967347, "chrf_score": 30.63740932883134, "xcomet_score": 0.8922889232635498, "xcomet_qe_score": 0.8789463639259338, "metricx_score": 2.9771714210510254, "metricx_qe_score": 1.4882266521453857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.953220009803772, "xcomet_qe_score": 0.8956713080406189, "metricx_score": 0.23392783105373383, "metricx_qe_score": 0.3477631211280823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们选出了一个并列词。", "metrics": {"bleu_score": 61.85985276068634, "chrf_score": 57.17956616846529, "xcomet_score": 0.8574240207672119, "xcomet_qe_score": 0.8089953064918518, "metricx_score": 1.3094820976257324, "metricx_qe_score": 3.0059430599212646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在也有对协调结构采取对称方法,例如布拉格方法,", "metrics": {"bleu_score": 32.3932119435985, "chrf_score": 27.789997838574106, "xcomet_score": 0.7985435724258423, "xcomet_qe_score": 0.7770094275474548, "metricx_score": 4.998228073120117, "metricx_qe_score": 4.35597038269043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及布拉格依存句法树中假设的以连词为首的方法,其中协调结构以连词为首。", "metrics": {"bleu_score": 28.11315158937332, "chrf_score": 23.1780678831638, "xcomet_score": 0.7444032430648804, "xcomet_qe_score": 0.4700249433517456, "metricx_score": 3.3629164695739746, "metricx_qe_score": 3.471705198287964, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从终点得到所有合取式的依赖关系。", "metrics": {"bleu_score": 15.646327194763158, "chrf_score": 16.678003559290985, "xcomet_score": 0.7739338874816895, "xcomet_qe_score": 0.7675819396972656, "metricx_score": 4.5160908699035645, "metricx_qe_score": 3.5500810146331787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多头方法,例如 Dick Hudson 的词语语法中就采用了这种方法。 可以说,所有连接词都是并列结构的主语。", "metrics": {"bleu_score": 23.86207500142707, "chrf_score": 26.32519863679525, "xcomet_score": 0.5865561962127686, "xcomet_qe_score": 0.5972216725349426, "metricx_score": 3.41500186920166, "metricx_qe_score": 3.5562961101531982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从支配词(此处为", "metrics": {"bleu_score": 33.982435124243786, "chrf_score": 29.14043563859437, "xcomet_score": 0.5765360593795776, "xcomet_qe_score": 0.1617942750453949, "metricx_score": 7.285499095916748, "metricx_qe_score": 5.346288204193115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "loves)分别得到所有连接词的依赖关系。这些都是巴顿提出的。", "metrics": {"bleu_score": 3.9253456897493937, "chrf_score": 11.259776712587302, "xcomet_score": 0.14793109893798828, "xcomet_qe_score": 0.1521759033203125, "metricx_score": 8.774049758911133, "metricx_qe_score": 13.18435001373291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,本文的目的是为像这两个一样的协调对称结构提供一个新的论点,并反对像这两个一样的非对称协调结构。", "metrics": {"bleu_score": 41.03189284958194, "chrf_score": 36.91732351142909, "xcomet_score": 0.6430858373641968, "xcomet_qe_score": 0.6244459748268127, "metricx_score": 2.0906665325164795, "metricx_qe_score": 1.7829729318618774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.952484130859375, "xcomet_qe_score": 0.9396543502807617, "metricx_score": 0.2438656985759735, "metricx_qe_score": 0.30582886934280396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论点是基于依赖长度最小化原则的,我将通过这些例子来解释。", "metrics": {"bleu_score": 42.27329162760188, "chrf_score": 34.37252412438204, "xcomet_score": 0.90105801820755, "xcomet_qe_score": 0.8991734981536865, "metricx_score": 0.7066492438316345, "metricx_qe_score": 0.457020103931427, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,正如你可能知道的,在英语中,直接宾语倾向于靠近动词,而附属成分可能离得更远,对吧?", "metrics": {"bleu_score": 32.42989740661918, "chrf_score": 30.090129342655093, "xcomet_score": 0.7808786034584045, "xcomet_qe_score": 0.7946876287460327, "metricx_score": 2.928746461868286, "metricx_qe_score": 2.9734647274017334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,昨天读的《三月》没问题,因为它的直接宾语靠近动词。 而 March read 昨天,情况就更糟糕了,", "metrics": {"bleu_score": 18.80595458478167, "chrf_score": 14.43388165709213, "xcomet_score": 0.4608231484889984, "xcomet_qe_score": 0.46449369192123413, "metricx_score": 12.317036628723145, "metricx_qe_score": 12.698182106018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因为", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.797182559967041, "xcomet_qe_score": 0.6857694387435913, "metricx_score": 2.7996504306793213, "metricx_qe_score": 0.4448353350162506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里在动词和直接宾语之间,有一个状语 yesterday。", "metrics": {"bleu_score": 47.063659720545125, "chrf_score": 53.528173345531506, "xcomet_score": 0.8945227861404419, "xcomet_qe_score": 0.7961477041244507, "metricx_score": 3.08915638923645, "metricx_qe_score": 4.278496742248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重且非常长时,这种效果可能会得到改善,因为这", "metrics": {"bleu_score": 25.609096738931523, "chrf_score": 26.97195010867837, "xcomet_score": 0.5129567384719849, "xcomet_qe_score": 0.4776734411716461, "metricx_score": 7.079856872558594, "metricx_qe_score": 3.72761607170105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "时可以直接宾语移到副词之后的位置。", "metrics": {"bleu_score": 33.965884450200434, "chrf_score": 30.208185978495166, "xcomet_score": 0.680031418800354, "xcomet_qe_score": 0.7440020442008972, "metricx_score": 5.081878185272217, "metricx_qe_score": 4.791965961456299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里对此进行了说明。", "metrics": {"bleu_score": 9.287528999566801, "chrf_score": 8.204869394996024, "xcomet_score": 0.8529562950134277, "xcomet_qe_score": 0.9032735228538513, "metricx_score": 0.7507290840148926, "metricx_qe_score": 0.6912389993667603, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这两句话都很好。", "metrics": {"bleu_score": 9.548450962056531, "chrf_score": 9.416750870102167, "xcomet_score": 0.9280425310134888, "xcomet_qe_score": 0.9024364352226257, "metricx_score": 0.39084896445274353, "metricx_qe_score": 0.5093563199043274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "三月今天读了一本关于BCS的非常有趣的", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14811968803405762, "xcomet_qe_score": 0.1493334174156189, "metricx_score": 9.012299537658691, "metricx_qe_score": 10.448415756225586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书。没问题。用这个长NP代替它。 但也可以说《三月阅读昨日", "metrics": {"bleu_score": 12.090340630072072, "chrf_score": 11.04104299500016, "xcomet_score": 0.254352867603302, "xcomet_qe_score": 0.1969769299030304, "metricx_score": 13.290565490722656, "metricx_qe_score": 7.695640563964844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "》,这是一本关于蜜蜂的绝对迷人书籍。 因此,这里的推理是,这是可能的,", "metrics": {"bleu_score": 1.6276888006478718, "chrf_score": 1.3102725366876309, "xcomet_score": 0.14008623361587524, "xcomet_qe_score": 0.1418820172548294, "metricx_score": 9.763616561889648, "metricx_qe_score": 13.525862693786621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为即使这个句子违反了直接宾语应该紧挨动词的一般语法原则, 沃伊切赫·查亚 - 它满足了依赖长度最小化原则,该原则指出更短的依赖关系更受欢迎。 因此,", "metrics": {"bleu_score": 39.12595670649435, "chrf_score": 34.536714534003075, "xcomet_score": 0.41564086079597473, "xcomet_qe_score": 0.31216517090797424, "metricx_score": 9.5626859664917, "metricx_qe_score": 8.725285530090332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示关键依赖项的长度,因此在这两种结构中,非常数的依赖项", "metrics": {"bleu_score": 35.49613042183065, "chrf_score": 29.76019444461848, "xcomet_score": 0.7826436758041382, "xcomet_qe_score": 0.5724341869354248, "metricx_score": 4.4847025871276855, "metricx_qe_score": 7.258296012878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里我们有一个从阅读到以词数计的长度为七的附属成分的依赖关系,以及从阅读到长度为四的书籍的依赖关系。所以两者加起来是11。", "metrics": {"bleu_score": 23.320754646941076, "chrf_score": 19.53327646101787, "xcomet_score": 0.5859911441802979, "xcomet_qe_score": 0.5746665596961975, "metricx_score": 4.446363925933838, "metricx_qe_score": 4.378089427947998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动时,当你交换这两个成分时,这两个依赖项的总和就变成了6,对", "metrics": {"bleu_score": 37.24592242177791, "chrf_score": 34.40589378179425, "xcomet_score": 0.5282796025276184, "xcomet_qe_score": 0.5307648181915283, "metricx_score": 6.522177696228027, "metricx_qe_score": 6.053948402404785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吧?所以不是11,而是6,要短得多。", "metrics": {"bleu_score": 23.287896954139942, "chrf_score": 26.486990156984696, "xcomet_score": 0.7186135053634644, "xcomet_qe_score": 0.7832711338996887, "metricx_score": 2.628347635269165, "metricx_qe_score": 3.066222667694092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来相当不错,", "metrics": {"bleu_score": 67.29864884660302, "chrf_score": 66.91997613073697, "xcomet_score": 0.9384526610374451, "xcomet_qe_score": 0.9192880988121033, "metricx_score": 0.5477246046066284, "metricx_qe_score": 0.5692055225372314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因为", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.797182559967041, "xcomet_qe_score": 0.6857694387435913, "metricx_score": 2.7996504306793213, "metricx_qe_score": 0.4448353350162506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 63.8079654920782, "xcomet_score": 0.9115704298019409, "xcomet_qe_score": 0.963566780090332, "metricx_score": 0.5989556312561035, "metricx_qe_score": 0.9356817007064819, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9520323276519775, "xcomet_qe_score": 0.9441296458244324, "metricx_score": 0.25305208563804626, "metricx_qe_score": 0.31463509798049927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从宾夕法尼亚树库的增强版中提取了关于协调的各种统计数据,并查看了为什么我们没有使用大学依存关系的论文。 Mateusz Piorkowski - 统计数据证实了之前多次提出的观察结果,即左派合同往往较短,就像", "metrics": {"bleu_score": 42.824622047378504, "chrf_score": 38.339895436825486, "xcomet_score": 0.14358553290367126, "xcomet_qe_score": 0.1396818459033966, "metricx_score": 13.07676887512207, "metricx_qe_score": 10.963269233703613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用音节衡量的盐和胡椒。 并且在路过时还", "metrics": {"bleu_score": 4.60234858040021, "chrf_score": 3.0296614791471237, "xcomet_score": 0.17346131801605225, "xcomet_qe_score": 0.16023261845111847, "metricx_score": 12.915961265563965, "metricx_qe_score": 8.978288650512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "观察到,这种趋势随着长度差异的增加而增加。 因此", "metrics": {"bleu_score": 57.28244716273146, "chrf_score": 53.43867002674174, "xcomet_score": 0.7932047843933105, "xcomet_qe_score": 0.8047047853469849, "metricx_score": 3.79410982131958, "metricx_qe_score": 2.922834634780884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个连接体的长度差异增大时,较短的连接体倾向于首先变强。没错。", "metrics": {"bleu_score": 19.73308551151646, "chrf_score": 17.884869075082193, "xcomet_score": 0.7743501663208008, "xcomet_qe_score": 0.9318520426750183, "metricx_score": 6.197755336761475, "metricx_qe_score": 3.218629837036133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,左短连接体的比例更大。", "metrics": {"bleu_score": 38.73920998972052, "chrf_score": 32.74090484646162, "xcomet_score": 0.876032829284668, "xcomet_qe_score": 0.8711604475975037, "metricx_score": 3.3524277210235596, "metricx_qe_score": 3.672781467437744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种倾向只有在左侧的保姆缺席时才会发生。 所以", "metrics": {"bleu_score": 34.29302276222199, "chrf_score": 30.19659284243521, "xcomet_score": 0.6366913318634033, "xcomet_qe_score": 0.5909074544906616, "metricx_score": 7.948949813842773, "metricx_qe_score": 7.326807022094727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因为", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.797182559967041, "xcomet_qe_score": 0.6857694387435913, "metricx_score": 2.7996504306793213, "metricx_qe_score": 0.4448353350162506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以州长在左边。", "metrics": {"bleu_score": 18.702869706385968, "chrf_score": 12.95644003926289, "xcomet_score": 0.6368355751037598, "xcomet_qe_score": 0.7403826713562012, "metricx_score": 2.357534646987915, "metricx_qe_score": 1.274109125137329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,它缺失了。荷马来了,打了个喷嚏。", "metrics": {"bleu_score": 20.828838183973037, "chrf_score": 11.685270202180831, "xcomet_score": 0.7585030794143677, "xcomet_qe_score": 0.748598575592041, "metricx_score": 4.053126335144043, "metricx_qe_score": 4.4531145095825195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们有两个动词的协调,没有外部的支配者。所以,", "metrics": {"bleu_score": 68.36013949954278, "chrf_score": 69.43785311548814, "xcomet_score": 0.796747088432312, "xcomet_qe_score": 0.7833292484283447, "metricx_score": 4.489276885986328, "metricx_qe_score": 4.006824016571045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左连接词倾向于更短,两个连接词之间的差异越", "metrics": {"bleu_score": 30.258667754361156, "chrf_score": 25.371094494336017, "xcomet_score": 0.6911770105361938, "xcomet_qe_score": 0.6429321765899658, "metricx_score": 7.331955432891846, "metricx_qe_score": 5.686480522155762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大,这种倾向就越明显。 然而,当右侧的治理到位时,左侧负责协调、电信和网络,这种效果就会消失。 因此,", "metrics": {"bleu_score": 12.807025451188931, "chrf_score": 11.17345802388149, "xcomet_score": 0.12642155587673187, "xcomet_qe_score": 0.1288187950849533, "metricx_score": 14.913976669311523, "metricx_qe_score": 15.220307350158691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度来展示这一点,这是音节的第一列,中间一列是单词,右边一列。所以我会", "metrics": {"bleu_score": 16.905845430561712, "chrf_score": 17.190752708102245, "xcomet_score": 0.46557414531707764, "xcomet_qe_score": 0.43905317783355713, "metricx_score": 10.688257217407227, "metricx_qe_score": 7.639481544494629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "专注于右边的。", "metrics": {"bleu_score": 17.112717058426785, "chrf_score": 13.315581500541946, "xcomet_score": 0.8298449516296387, "xcomet_qe_score": 0.7620784640312195, "metricx_score": 2.4521212577819824, "metricx_qe_score": 1.9814478158950806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在这里看到的是,当州长在左边时, 左连接词趋向于变短的趋势会随着词语之间的绝对差异而稳步增长。在没有连接词的情况下,例如句子协调,也会观察到同样的现象。但是", "metrics": {"bleu_score": 28.20080398245358, "chrf_score": 26.696845918994978, "xcomet_score": 0.3420056402683258, "xcomet_qe_score": 0.3699449896812439, "metricx_score": 7.967623710632324, "metricx_qe_score": 5.445711135864258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当连接词位于右侧时,这种趋势就会消失。", "metrics": {"bleu_score": 49.030470692026626, "chrf_score": 43.38113461758972, "xcomet_score": 0.8094508647918701, "xcomet_qe_score": 0.35858088731765747, "metricx_score": 4.183172702789307, "metricx_qe_score": 5.293155670166016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这一点如何反驳了像这两个这样的不对称协调结构,以及像这两个这样的对称结构。 因此,请参阅论文", "metrics": {"bleu_score": 39.05385324416808, "chrf_score": 34.80744538738269, "xcomet_score": 0.49573224782943726, "xcomet_qe_score": 0.2481190711259842, "metricx_score": 2.8811540603637695, "metricx_qe_score": 3.635037899017334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以获取完整的协议和论点,抱歉,并", "metrics": {"bleu_score": 5.816635421147515, "chrf_score": 5.277777777777778, "xcomet_score": 0.18576937913894653, "xcomet_qe_score": 0.17611141502857208, "metricx_score": 11.498788833618164, "metricx_qe_score": 8.029977798461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与我们讨论海报展示环节。", "metrics": {"bleu_score": 8.972971553870872, "chrf_score": 9.998490793842437, "xcomet_score": 0.8197447061538696, "xcomet_qe_score": 0.8094793558120728, "metricx_score": 5.0074663162231445, "metricx_qe_score": 2.5326545238494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是华盛顿大学的博士生Xiangbin。", "metrics": {"bleu_score": 70.85876411943929, "chrf_score": 63.86513467463122, "xcomet_score": 0.8795247077941895, "xcomet_qe_score": 0.8485116362571716, "metricx_score": 0.35235127806663513, "metricx_qe_score": 1.016472339630127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天,我将介绍我们从预训练数据到语言模型再到下游任务的工作,追踪导致不公平自然语言处理模型的政治偏见线索。 因此", "metrics": {"bleu_score": 59.27769112224914, "chrf_score": 55.96672955550761, "xcomet_score": 0.662361741065979, "xcomet_qe_score": 0.5574333071708679, "metricx_score": 4.308887958526611, "metricx_qe_score": 1.997611403465271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是在大规模的网络爬虫数据上进行训练的。", "metrics": {"bleu_score": 79.47545184555567, "chrf_score": 83.83377023459488, "xcomet_score": 0.9833188056945801, "xcomet_qe_score": 0.9606128931045532, "metricx_score": 1.4932624101638794, "metricx_qe_score": 2.3271894454956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在他们的预训练数据中得到了充分的覆盖。", "metrics": {"bleu_score": 47.51132438608344, "chrf_score": 47.60908146185974, "xcomet_score": 0.7693331241607666, "xcomet_qe_score": 0.6899073123931885, "metricx_score": 1.5762958526611328, "metricx_qe_score": 2.6339707374572754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C4语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了充分的覆盖。", "metrics": {"bleu_score": 82.10203688208662, "chrf_score": 79.29215336952645, "xcomet_score": 0.8889493942260742, "xcomet_qe_score": 0.8671253323554993, "metricx_score": 1.0854274034500122, "metricx_qe_score": 1.2341036796569824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型的应用带来了既是机遇也是挑战的局面。 因此,", "metrics": {"bleu_score": 23.386786214190373, "chrf_score": 23.988542454652205, "xcomet_score": 0.5106209516525269, "xcomet_qe_score": 0.5005782246589661, "metricx_score": 3.8942954540252686, "metricx_qe_score": 1.369246006011963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从多元视角中学习,这体现了民主和思想多元性的价值。", "metrics": {"bleu_score": 31.443515194397026, "chrf_score": 27.0296665860536, "xcomet_score": 0.9710789918899536, "xcomet_qe_score": 0.9236767888069153, "metricx_score": 0.774991512298584, "metricx_qe_score": 0.6175504922866821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能会导致下游任务应用中的潜在公平问题。", "metrics": {"bleu_score": 56.862047828762094, "chrf_score": 48.85020461671514, "xcomet_score": 0.9903979301452637, "xcomet_qe_score": 0.975304365158081, "metricx_score": 0.9513039588928223, "metricx_qe_score": 1.122297763824463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播流程,具体来说,通过以下几个问题来探讨这一问题。 首先,我们如何评估语言模型的政治倾向,相关数据可能对这种政治偏见产生什么作用?", "metrics": {"bleu_score": 58.124818597961664, "chrf_score": 54.96101714826869, "xcomet_score": 0.9566978216171265, "xcomet_qe_score": 0.9382257461547852, "metricx_score": 1.208896279335022, "metricx_qe_score": 1.4310272932052612, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治限制的语言模型在下游任务中的实际表现如何,以及这是否可能导致 NLP 应用中的公平性问题?", "metrics": {"bleu_score": 72.93142555264798, "chrf_score": 68.33540600272106, "xcomet_score": 0.859709620475769, "xcomet_qe_score": 0.7667803764343262, "metricx_score": 1.8279190063476562, "metricx_qe_score": 1.6523789167404175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们首先提出使用政治问卷(如政治指南针测试)以不同的提示格式提示语言模型。", "metrics": {"bleu_score": 45.22626901609501, "chrf_score": 39.37195172206939, "xcomet_score": 0.8202585577964783, "xcomet_qe_score": 0.7140127420425415, "metricx_score": 4.1155104637146, "metricx_qe_score": 4.385788917541504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们的自动评估能够很好地立足于政治科学文献。", "metrics": {"bleu_score": 36.032121811571, "chrf_score": 34.01832207402901, "xcomet_score": 0.9087281227111816, "xcomet_qe_score": 0.8885488510131836, "metricx_score": 1.3686269521713257, "metricx_qe_score": 1.631363034248352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,一些初步结果表明,第一代语言模型确实具有不同的政治倾向。", "metrics": {"bleu_score": 61.70551093767843, "chrf_score": 60.16923581697545, "xcomet_score": 0.7779037952423096, "xcomet_qe_score": 0.8102694153785706, "metricx_score": 3.0454277992248535, "metricx_qe_score": 1.54249107837677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治指南针上的四个象限。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 44.723889306558185, "xcomet_score": 0.8590556383132935, "xcomet_qe_score": 0.8305246829986572, "metricx_score": 2.5154099464416504, "metricx_qe_score": 2.432861089706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT-4 是所有语言模型中最自由的,GPT 理论通常比 BERT 理论及其变体更具社会自由性。", "metrics": {"bleu_score": 53.06527497167651, "chrf_score": 49.972182152671024, "xcomet_score": 0.638784646987915, "xcomet_qe_score": 0.6279939413070679, "metricx_score": 2.2193901538848877, "metricx_qe_score": 2.5148684978485107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在研究语言模型的政治偏见在多大程度上实际上是从训练数据中获得的。", "metrics": {"bleu_score": 54.66747185912124, "chrf_score": 47.198981830592096, "xcomet_score": 0.9798702001571655, "xcomet_qe_score": 0.9712907075881958, "metricx_score": 0.739354133605957, "metricx_qe_score": 1.1560869216918945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点来进行一项受控实验,这些语料库分为新闻和社交媒体,并进一步细分为其政治倾向。", "metrics": {"bleu_score": 55.72801748122164, "chrf_score": 49.704259225090006, "xcomet_score": 0.9221458435058594, "xcomet_qe_score": 0.686017632484436, "metricx_score": 1.7594711780548096, "metricx_qe_score": 2.0539212226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这些带有党派倾向的语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应发生了变化。", "metrics": {"bleu_score": 63.50143687816412, "chrf_score": 55.24207077930314, "xcomet_score": 0.971133828163147, "xcomet_qe_score": 0.9096276760101318, "metricx_score": 0.9427427053451538, "metricx_qe_score": 1.585370659828186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于罗伯塔,进一步微调,进一步训练于左倾的 Reddit 语料库,我们可以看到其在观点上发生了显著的自由派转变。 就其政治偏见而言。", "metrics": {"bleu_score": 44.356711180146476, "chrf_score": 46.161893423201, "xcomet_score": 0.39205336570739746, "xcomet_qe_score": 0.3523949086666107, "metricx_score": 6.156352996826172, "metricx_qe_score": 5.799402713775635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图研究语言模型是否能捕捉到我们现代社会普遍存在的极化现象。", "metrics": {"bleu_score": 51.11328901785518, "chrf_score": 42.393357438198926, "xcomet_score": 0.9082942605018616, "xcomet_qe_score": 0.9816766977310181, "metricx_score": 0.657209038734436, "metricx_qe_score": 0.8491129279136658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第45任总统之前和第45任总统之后,", "metrics": {"bleu_score": 89.18471513698398, "chrf_score": 88.74951256330893, "xcomet_score": 0.7940199971199036, "xcomet_qe_score": 0.7186543345451355, "metricx_score": 2.194640636444092, "metricx_qe_score": 2.738783597946167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们分别在两个不同的时间语料库上预训练语言模型。 我们", "metrics": {"bleu_score": 91.81891462193897, "chrf_score": 98.16306505407624, "xcomet_score": 0.6552369594573975, "xcomet_qe_score": 0.6593025922775269, "metricx_score": 4.11857271194458, "metricx_qe_score": 1.14821457862854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在 2017 年之后普遍呈现出更偏离中心的政治倾向。因此,", "metrics": {"bleu_score": 45.94387792124755, "chrf_score": 46.781924792593834, "xcomet_score": 0.7610457539558411, "xcomet_qe_score": 0.7436637878417969, "metricx_score": 4.16709041595459, "metricx_qe_score": 3.0749406814575195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也可以捕捉到我们社会中的两极分化。", "metrics": {"bleu_score": 88.43865924896839, "chrf_score": 86.17807106986935, "xcomet_score": 0.9938218593597412, "xcomet_qe_score": 0.9411992430686951, "metricx_score": 0.8323193788528442, "metricx_qe_score": 1.2451519966125488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们对不同政治倾向的语言模型进行仇恨言论检测和虚假新闻检测,这些应用通常涉及语言模型,并可能产生非常重大的影响。", "metrics": {"bleu_score": 55.02950167962186, "chrf_score": 46.12139005438649, "xcomet_score": 0.9441403150558472, "xcomet_qe_score": 0.8871889114379883, "metricx_score": 2.139880657196045, "metricx_qe_score": 2.7156217098236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们发现,如果我们按类别调查绩效,也就是说,如果我们将绩效分开 根据不同的人口统计数据或新闻媒体的政治意义,我们可以看到一个模式,", "metrics": {"bleu_score": 40.507240275295324, "chrf_score": 32.91070496907285, "xcomet_score": 0.5365381836891174, "xcomet_qe_score": 0.37554648518562317, "metricx_score": 5.2723388671875, "metricx_qe_score": 5.240457057952881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于仇恨言论的检测,左倾语言模型表现更好。 在检测针对社会少数群体的仇恨言论方面 然而,它们在检测针对我们社会中更具权势群体的仇恨言论方面表现更差。", "metrics": {"bleu_score": 66.82600825318278, "chrf_score": 63.58227134757294, "xcomet_score": 0.7209959030151367, "xcomet_qe_score": 0.6842021942138672, "metricx_score": 5.2886457443237305, "metricx_qe_score": 5.333475589752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,右倾语言模型在检测针对白人和男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+和其他少数族裔群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 70.06120228691815, "chrf_score": 71.69817063671111, "xcomet_score": 0.9834194183349609, "xcomet_qe_score": 0.9812736511230469, "metricx_score": 0.49549269676208496, "metricx_qe_score": 0.6554927825927734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在虚假新闻检测方面也存在类似的趋势,我们发现左倾语言模型在检测其对立的政治倾向的虚假信息方面表现更好,反之亦然。 在此基础上,", "metrics": {"bleu_score": 32.87003842449327, "chrf_score": 29.145105668037967, "xcomet_score": 0.6526831388473511, "xcomet_qe_score": 0.5710498094558716, "metricx_score": 4.122106075286865, "metricx_qe_score": 2.939420700073242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多定性例子,以证明具有不同政治含义的语言模型 根据其社会类别,对仇恨言论和虚假信息示例给出不同的预测。", "metrics": {"bleu_score": 65.2534072744737, "chrf_score": 56.87863489148701, "xcomet_score": 0.9285502433776855, "xcomet_qe_score": 0.919535219669342, "metricx_score": 2.384093761444092, "metricx_qe_score": 3.6637237071990967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有更多示例,以进一步强调这一点。 这表明,语言模型的政治偏见问题非常紧迫,需要公平解决。", "metrics": {"bleu_score": 41.81080040853914, "chrf_score": 35.31892297920453, "xcomet_score": 0.912443995475769, "xcomet_qe_score": 0.890751302242279, "metricx_score": 2.882174253463745, "metricx_qe_score": 2.196268081665039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个右倾语言模型被用于仇恨言论或虚假信息的微调,或者其他任何用途,并部署到一个流行的社交媒体平台, 这意味着,持有相反政治观点的人可能会被边缘化,针对少数群体的仇恨言论可能会不受任何控制地肆意蔓延。", "metrics": {"bleu_score": 57.03447596683935, "chrf_score": 54.08402207994263, "xcomet_score": 0.9646437168121338, "xcomet_qe_score": 0.8430131673812866, "metricx_score": 1.7633246183395386, "metricx_qe_score": 2.373074769973755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,要求我们承认并解决语言模型政治倾向所导致的公平问题。", "metrics": {"bleu_score": 36.542441362644645, "chrf_score": 39.41940824877189, "xcomet_score": 0.9875196218490601, "xcomet_qe_score": 0.9877210855484009, "metricx_score": 0.8851255178451538, "metricx_qe_score": 0.9645529389381409, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们想稍微讨论一下。", "metrics": {"bleu_score": 33.260249505555045, "chrf_score": 32.42349794467388, "xcomet_score": 0.8066399097442627, "xcomet_qe_score": 0.8584543466567993, "metricx_score": 1.1138341426849365, "metricx_qe_score": 1.3084808588027954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还希望强调,我们揭露了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 53.96681359887849, "chrf_score": 50.337635528936964, "xcomet_score": 0.9087839722633362, "xcomet_qe_score": 0.9035114049911499, "metricx_score": 1.050887107849121, "metricx_qe_score": 1.8719152212142944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像是在斯库拉和哈里布底斯之间。", "metrics": {"bleu_score": 16.42817227606757, "chrf_score": 16.75735396022243, "xcomet_score": 0.7330398559570312, "xcomet_qe_score": 0.7652713060379028, "metricx_score": 2.8727457523345947, "metricx_qe_score": 1.7048981189727783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们在语言模型训练数据中不清理政治观点,偏见将从预训练数据传播到语言模型,进而影响下游任务,最终导致公平性问题", "metrics": {"bleu_score": 59.41289626399223, "chrf_score": 50.73873337861773, "xcomet_score": 0.9811006784439087, "xcomet_qe_score": 0.92943274974823, "metricx_score": 1.2723708152770996, "metricx_qe_score": 2.155067205429077, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们真的试图以某种方式清理,我们也会面临审查或被排除的风险,", "metrics": {"bleu_score": 47.45223087869145, "chrf_score": 42.400008724156564, "xcomet_score": 0.7859939336776733, "xcomet_qe_score": 0.763978898525238, "metricx_score": 1.655185580253601, "metricx_qe_score": 1.699396014213562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定什么才是真正中立的,什么应该是保持语言单一性的数据。", "metrics": {"bleu_score": 8.49578625578356, "chrf_score": 14.021953815416849, "xcomet_score": 0.9129998683929443, "xcomet_qe_score": 0.90339595079422, "metricx_score": 2.717705011367798, "metricx_qe_score": 2.877984046936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这有点像电车难题。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.8480808734893799, "xcomet_qe_score": 0.8042812347412109, "metricx_score": 1.8150618076324463, "metricx_qe_score": 2.786520481109619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9799641370773315, "xcomet_qe_score": 0.9886821508407593, "metricx_score": 0.17090822756290436, "metricx_qe_score": 0.24480992555618286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这就是我今天要讲的全部了。", "metrics": {"bleu_score": 58.282339541526554, "chrf_score": 54.42343463004441, "xcomet_score": 0.9955207109451294, "xcomet_qe_score": 0.9851174354553223, "metricx_score": 0.2826388478279114, "metricx_qe_score": 0.3890303671360016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基梅隆大学的一名一年级博士生,今天我将介绍你们的论文《数据集中和模型中设计偏见的肛交位置性特征》。", "metrics": {"bleu_score": 43.64299318903829, "chrf_score": 30.398567816731887, "xcomet_score": 0.62107914686203, "xcomet_qe_score": 0.6544831395149231, "metricx_score": 7.373364448547363, "metricx_qe_score": 7.789529800415039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的,其中包括 Sebastian Santee、Ronan Labrosse、Katarina Reinecke 和 Martin Sapp。", "metrics": {"bleu_score": 51.937067816843374, "chrf_score": 60.82633428731102, "xcomet_score": 0.6879035234451294, "xcomet_qe_score": 0.719203770160675, "metricx_score": 2.708195924758911, "metricx_qe_score": 2.514017105102539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,让我们先想象一下,你正在为一家报纸工作,你正在筛选新闻文章下的评论,试图删除有毒内容。 ", "metrics": {"bleu_score": 48.6531571063768, "chrf_score": 44.696696518431914, "xcomet_score": 0.9080410003662109, "xcomet_qe_score": 0.9210500717163086, "metricx_score": 1.7763136625289917, "metricx_qe_score": 1.3981044292449951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能会转向像 Perspective API 这样的流行 API 来检测有毒性内容。如果你是 Carl Jones,这种方法真的很好用,", "metrics": {"bleu_score": 21.993422780474287, "chrf_score": 40.51451228043347, "xcomet_score": 0.6695797443389893, "xcomet_qe_score": 0.6280257105827332, "metricx_score": 3.6975016593933105, "metricx_qe_score": 4.0742292404174805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为 Perspective API 能够正确地检测出有毒的实例。", "metrics": {"bleu_score": 24.62395302527262, "chrf_score": 55.57687323361351, "xcomet_score": 0.778724193572998, "xcomet_qe_score": 0.6776962280273438, "metricx_score": 4.614487648010254, "metricx_qe_score": 4.418292045593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于 Aditya Sharma 来说,情况并非如此,", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 79.30166572557877, "xcomet_score": 0.9277344942092896, "xcomet_qe_score": 0.9143953323364258, "metricx_score": 1.2387149333953857, "metricx_qe_score": 1.0009479522705078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为 perspective API 对印度语境中更为常见的攻击性用词并不敏感。", "metrics": {"bleu_score": 48.195116293616074, "chrf_score": 60.059071848518954, "xcomet_score": 0.8615987300872803, "xcomet_qe_score": 0.6496812105178833, "metricx_score": 3.8410513401031494, "metricx_qe_score": 4.514402866363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏差的例子,我们在此看到不同人群之间技术性能的系统性差异。", "metrics": {"bleu_score": 35.15787253565498, "chrf_score": 29.249494192762064, "xcomet_score": 0.9761266708374023, "xcomet_qe_score": 0.9105405807495117, "metricx_score": 0.7992537021636963, "metricx_qe_score": 1.209579348564148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们之前看到的这种设计偏见可能源于自然语言处理研究人员和模型开发人员的立场。", "metrics": {"bleu_score": 55.42963287421544, "chrf_score": 53.30146658373152, "xcomet_score": 0.9824978113174438, "xcomet_qe_score": 0.9303568005561829, "metricx_score": 0.9420276880264282, "metricx_qe_score": 0.8662136197090149, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场是指人们由于其人口统计特征、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 74.56495553738948, "chrf_score": 74.83092783522544, "xcomet_score": 0.8831789493560791, "xcomet_qe_score": 0.9062502384185791, "metricx_score": 0.9311323761940002, "metricx_qe_score": 1.1517540216445923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 60.90393051639867, "chrf_score": 54.74944642873282, "xcomet_score": 0.9953645467758179, "xcomet_qe_score": 0.9282628297805786, "metricx_score": 0.9410673975944519, "metricx_qe_score": 1.3600661754608154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,立场性会影响研究过程及其结果和结论,因为它会改变研究人员做出的决策。", "metrics": {"bleu_score": 56.31055087769951, "chrf_score": 50.07588953874199, "xcomet_score": 0.9865820407867432, "xcomet_qe_score": 0.9297225475311279, "metricx_score": 1.045494556427002, "metricx_qe_score": 1.025436282157898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问的一个问题是,数据集和模型是否有位置性?", "metrics": {"bleu_score": 53.41701392245994, "chrf_score": 49.11608877814345, "xcomet_score": 0.9061594605445862, "xcomet_qe_score": 0.9306572675704956, "metricx_score": 2.288924217224121, "metricx_qe_score": 0.9304611682891846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型、细胞和数据集本身具有人口统计学身份和生活经历,而是它们汇集了真实的人们的判断和观点,因此可以代表某些立场优于其他立场。", "metrics": {"bleu_score": 49.894510590852306, "chrf_score": 44.68393240055516, "xcomet_score": 0.700004518032074, "xcomet_qe_score": 0.7368795871734619, "metricx_score": 4.3856706619262695, "metricx_qe_score": 4.5481061935424805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,之前的研究提出了一些关于位置性的轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 40.47754548837428, "chrf_score": 31.814542620103147, "xcomet_score": 0.7647557258605957, "xcomet_qe_score": 0.675853431224823, "metricx_score": 5.584323883056641, "metricx_qe_score": 4.963082790374756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些作品实际上并没有将最终用户与数据集和模型本身进行比较。 随着 NLP 任务变得更加主观和社会化,研究模型和数据集的定位性变得越来越重要。 要确定这些立场是如何被扭曲的,非常具有挑战性,因为并非所有决策都有记录,而且许多模型都隐藏在 API 背后。", "metrics": {"bleu_score": 54.47300682933945, "chrf_score": 48.80598296737399, "xcomet_score": 0.7129982709884644, "xcomet_qe_score": 0.7380412220954895, "metricx_score": 3.0107078552246094, "metricx_qe_score": 3.2942919731140137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了研究数据集和模型的定位性,我们实际上将注释与现有数据集和模型的真实用户进行了比较。", "metrics": {"bleu_score": 56.9561243853977, "chrf_score": 50.169398120017824, "xcomet_score": 0.7703904509544373, "xcomet_qe_score": 0.828010082244873, "metricx_score": 3.31715726852417, "metricx_qe_score": 3.4706733226776123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架 NL 位置性来实现这一点。", "metrics": {"bleu_score": 26.978569758601026, "chrf_score": 17.476530643443564, "xcomet_score": 0.7954840064048767, "xcomet_qe_score": 0.7921611070632935, "metricx_score": 1.8580937385559082, "metricx_qe_score": 2.4846322536468506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是用不同的标注者重新标注数据集。", "metrics": {"bleu_score": 37.75584206975749, "chrf_score": 29.66578586391127, "xcomet_score": 0.87446129322052, "xcomet_qe_score": 0.8993077874183655, "metricx_score": 1.9985883235931396, "metricx_qe_score": 1.9477628469467163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是研究原始数据集标注者的社会人口统计数据,因为通常只有少数标注者对每个实例进行标注,而且社会人口统计数据很少被收集和分享。", "metrics": {"bleu_score": 51.79063935298531, "chrf_score": 46.45355735846162, "xcomet_score": 0.8293734788894653, "xcomet_qe_score": 0.836113452911377, "metricx_score": 1.6348206996917725, "metricx_qe_score": 1.3695074319839478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以便每个实例都有多个标注者,并获得一套丰富的社会人口数据。", "metrics": {"bleu_score": 19.54251878985873, "chrf_score": 20.603354141803607, "xcomet_score": 0.7836407423019409, "xcomet_qe_score": 0.8041293621063232, "metricx_score": 4.214583873748779, "metricx_qe_score": 3.444791793823242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们按人口统计学特征对注释进行分类,并使用皮尔逊相关系数(Pearson's R correlation score)将它们与模型和数据集进行比较。 因此,我们的框架实际上与注释者分歧文献有所不同,它将最终用户与模型和数据集、预测和标签进行比较,而不是仅仅关注注释者的一致性或注释者分布的建模。", "metrics": {"bleu_score": 61.335394475723, "chrf_score": 57.8891772500965, "xcomet_score": 0.7957245111465454, "xcomet_qe_score": 0.708027720451355, "metricx_score": 3.552945375442505, "metricx_qe_score": 3.784370183944702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要得益于我们的 HCI 合作方的在线众包平台“野外实验室”。 而", "metrics": {"bleu_score": 39.29202036846993, "chrf_score": 33.20049161012443, "xcomet_score": 0.6732206344604492, "xcomet_qe_score": 0.589735209941864, "metricx_score": 3.959920644760132, "metricx_qe_score": 3.5809097290039062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 则是一个在线实验平台,与 MTurk 等平台相比,我们可以在此招募到更多样化的志愿者,", "metrics": {"bleu_score": 37.5727980130706, "chrf_score": 52.34195262975432, "xcomet_score": 0.7950358390808105, "xcomet_qe_score": 0.5667258501052856, "metricx_score": 2.154256820678711, "metricx_qe_score": 4.136044025421143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而 MTurk 的参与者主要来自美国或印度。此外,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 57.94224186180097, "chrf_score": 63.229908242097885, "xcomet_score": 0.7926092743873596, "xcomet_qe_score": 0.804751455783844, "metricx_score": 1.8951971530914307, "metricx_qe_score": 1.4693735837936401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“野外实验室”中设置了两个任务,其中一个是社会可接受性。这个任务的工作方式是,参与者将阅读来自社会化学数据集中的一个情境,然后他们将写出这个情境在社会上是多么可接受。", "metrics": {"bleu_score": 39.69284075445912, "chrf_score": 32.96476338869472, "xcomet_score": 0.9468169212341309, "xcomet_qe_score": 0.9456948041915894, "metricx_score": 2.4829633235931396, "metricx_qe_score": 2.574506998062134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持学习的参与度,他们可以将自己的回答与人工智能和其他人的回答进行比较。", "metrics": {"bleu_score": 56.913072525201, "chrf_score": 53.79743787354933, "xcomet_score": 0.9759554862976074, "xcomet_qe_score": 0.9879993200302124, "metricx_score": 0.8875884413719177, "metricx_qe_score": 0.6822713017463684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与社会化学、德尔菲和 GPT-4 进行比较。", "metrics": {"bleu_score": 45.30799450827704, "chrf_score": 37.81454825466124, "xcomet_score": 0.7650843262672424, "xcomet_qe_score": 0.7569223642349243, "metricx_score": 1.7274582386016846, "metricx_qe_score": 2.5331406593322754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们为毒性与仇恨言论检测任务复制了一个非常相似的设置,参与者将阅读 DynaHate 中的一个实例,并写下他们是否认为这是一个仇恨言论的实例。", "metrics": {"bleu_score": 60.30589739335281, "chrf_score": 56.457311096895715, "xcomet_score": 0.7586796283721924, "xcomet_qe_score": 0.7727229595184326, "metricx_score": 2.653801918029785, "metricx_qe_score": 3.0218048095703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些标注与 DynaHate、Perspective API、Rewire API、Hate Roberta 和 GPT-4 进行比较。", "metrics": {"bleu_score": 50.94964439600014, "chrf_score": 75.94730083182478, "xcomet_score": 0.8788892030715942, "xcomet_qe_score": 0.8664720058441162, "metricx_score": 1.8782100677490234, "metricx_qe_score": 3.11980938911438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了来自 87 个国家的 1000 多名标注者的 16000 多条标注。", "metrics": {"bleu_score": 71.66644457140384, "chrf_score": 71.51920294027818, "xcomet_score": 0.8803567886352539, "xcomet_qe_score": 0.9641635417938232, "metricx_score": 1.8083374500274658, "metricx_qe_score": 1.1193331480026245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们现在更有能力回答自然语言处理数据集和模型最符合谁的需求?我们", "metrics": {"bleu_score": 36.539221045150676, "chrf_score": 35.3077896832146, "xcomet_score": 0.7066770792007446, "xcomet_qe_score": 0.6616533398628235, "metricx_score": 4.041401386260986, "metricx_qe_score": 0.6573148369789124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现自然语言处理中存在位置性。", "metrics": {"bleu_score": 8.225964699966553, "chrf_score": 8.58137124241078, "xcomet_score": 0.8546791076660156, "xcomet_qe_score": 0.8818408846855164, "metricx_score": 3.969144821166992, "metricx_qe_score": 2.187469959259033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最符合英语国家的特点。", "metrics": {"bleu_score": 45.283344133049354, "chrf_score": 40.14999902733858, "xcomet_score": 0.9973093271255493, "xcomet_qe_score": 0.9921427965164185, "metricx_score": 0.6083756685256958, "metricx_qe_score": 0.7173677086830139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在 GPT-4 社会可接受性分析中,我们发现它最符合儒家文化和英语国家的特点。我们", "metrics": {"bleu_score": 38.273892989940514, "chrf_score": 40.20180754781512, "xcomet_score": 0.6006466150283813, "xcomet_qe_score": 0.551333487033844, "metricx_score": 4.679314613342285, "metricx_qe_score": 2.1310083866119385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现,dyna-hate 也最符合英语国家的特点。", "metrics": {"bleu_score": 53.084925448597666, "chrf_score": 55.10220698471272, "xcomet_score": 0.9652328491210938, "xcomet_qe_score": 0.9364203810691833, "metricx_score": 2.7808899879455566, "metricx_qe_score": 4.093416213989258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,G", "metrics": {"bleu_score": 3.0608499802737392, "chrf_score": 11.185065103677683, "xcomet_score": 0.16890284419059753, "xcomet_qe_score": 0.15427768230438232, "metricx_score": 22.57382583618164, "metricx_qe_score": 16.323944091796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PT-4 在社会可接受性任务中与受过大学教育或研究生教育的人的观点最为一致。 我们发现 Donahate 也是如此,它最符合受过大学教育的人。", "metrics": {"bleu_score": 53.35374836928845, "chrf_score": 47.422580120807886, "xcomet_score": 0.7140276432037354, "xcomet_qe_score": 0.5647982358932495, "metricx_score": 6.558696269989014, "metricx_qe_score": 7.454047203063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集针对特定人群进行调整时,一些人不可避免地会被抛在后面。 一个例子", "metrics": {"bleu_score": 31.910023371028153, "chrf_score": 30.377709675221332, "xcomet_score": 0.7364908456802368, "xcomet_qe_score": 0.7404413223266602, "metricx_score": 2.414736747741699, "metricx_qe_score": 1.5019941329956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,与男性和女性相比,数据集和模型对非二元人的对齐程度较低。", "metrics": {"bleu_score": 48.465254338121596, "chrf_score": 40.07330217675045, "xcomet_score": 0.6482581496238708, "xcomet_qe_score": 0.5916086435317993, "metricx_score": 3.7935612201690674, "metricx_qe_score": 4.117941856384277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 GPT-4 社会可接受性任务中发现了这一点,也在 DynaHATE 任务分析中发现了这一点。", "metrics": {"bleu_score": 49.46630566933845, "chrf_score": 49.811001395893385, "xcomet_score": 0.9037162065505981, "xcomet_qe_score": 0.8873199224472046, "metricx_score": 2.16252064704895, "metricx_qe_score": 2.603189706802368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既然 NLP 中存在立场性,我们能做些什么呢?", "metrics": {"bleu_score": 65.99231368411863, "chrf_score": 63.68803577423129, "xcomet_score": 0.9441593885421753, "xcomet_qe_score": 0.9173409938812256, "metricx_score": 0.5691772103309631, "metricx_qe_score": 1.0326744318008423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对此提出了一些建议。", "metrics": {"bleu_score": 31.53554052490131, "chrf_score": 33.008793029929855, "xcomet_score": 0.9676868915557861, "xcomet_qe_score": 0.9239437580108643, "metricx_score": 0.23016440868377686, "metricx_qe_score": 0.23169688880443573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,在整个研究过程中记录所有相关的设计选择。其次", "metrics": {"bleu_score": 50.406735961002234, "chrf_score": 41.279565844783235, "xcomet_score": 0.9651930332183838, "xcomet_qe_score": 0.9507927894592285, "metricx_score": 1.7049211263656616, "metricx_qe_score": 0.46416932344436646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从视角主义的角度进行 NLP 研究。", "metrics": {"bleu_score": 11.012455685471899, "chrf_score": 11.064329318579714, "xcomet_score": 0.810917854309082, "xcomet_qe_score": 0.7170666456222534, "metricx_score": 4.312621116638184, "metricx_qe_score": 4.367795944213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业的数据集和模型。", "metrics": {"bleu_score": 80.96427216101601, "chrf_score": 72.63771349978248, "xcomet_score": 0.911953866481781, "xcomet_qe_score": 0.8777899146080017, "metricx_score": 0.8077837228775024, "metricx_qe_score": 0.9955554604530334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane计划。我的意思是,", "metrics": {"bleu_score": 34.82352832757854, "chrf_score": 38.83053460263377, "xcomet_score": 0.36303094029426575, "xcomet_qe_score": 0.3119473457336426, "metricx_score": 5.98766565322876, "metricx_qe_score": 5.419447422027588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调,包容性NLP不仅仅是让所有", "metrics": {"bleu_score": 42.115124950749305, "chrf_score": 40.517930630683054, "xcomet_score": 0.626814603805542, "xcomet_qe_score": 0.5456753969192505, "metricx_score": 5.8720383644104, "metricx_qe_score": 4.886364459991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人服务。", "metrics": {"bleu_score": 39.03674453747003, "chrf_score": 36.76370111713883, "xcomet_score": 0.9485549926757812, "xcomet_qe_score": 0.9516949653625488, "metricx_score": 0.7394589781761169, "metricx_qe_score": 1.1236159801483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的演示到此结束,", "metrics": {"bleu_score": 52.53819788848316, "chrf_score": 37.53968253968254, "xcomet_score": 0.9809085130691528, "xcomet_qe_score": 0.9661180377006531, "metricx_score": 1.854804277420044, "metricx_qe_score": 1.6377358436584473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果您想了解更多信息,请随时查看我们的仪表板,获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 62.061988954713094, "chrf_score": 56.13070289899492, "xcomet_score": 0.9818592071533203, "xcomet_qe_score": 0.9660807847976685, "metricx_score": 0.6230459213256836, "metricx_qe_score": 0.5746880173683167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是复旦大学的袁思语。", "metrics": {"bleu_score": 34.64226178936947, "chrf_score": 21.33663927854213, "xcomet_score": 0.9764041900634766, "xcomet_qe_score": 0.8603624105453491, "metricx_score": 0.529305636882782, "metricx_qe_score": 0.5463011860847473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天在这里介绍我们的工作——从大型语言模型中提炼脚本知识以进行约束语言规划。", "metrics": {"bleu_score": 42.8462689716825, "chrf_score": 40.686725039550694, "xcomet_score": 0.8826878666877747, "xcomet_qe_score": 0.7907717227935791, "metricx_score": 1.735061526298523, "metricx_qe_score": 1.7249622344970703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类通常通过遵循有保证的脚本形式的逐步互动来计划自己的行动。 ", "metrics": {"bleu_score": 24.967920997628458, "chrf_score": 21.40516369487831, "xcomet_score": 0.8122406005859375, "xcomet_qe_score": 0.8797028064727783, "metricx_score": 4.041389465332031, "metricx_qe_score": 4.305369853973389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究利用语言模型为典型的抽象目标(如制作蛋糕)进行规划,并", "metrics": {"bleu_score": 44.826696353389195, "chrf_score": 38.59709519493539, "xcomet_score": 0.7360221147537231, "xcomet_qe_score": 0.7012159824371338, "metricx_score": 5.174064636230469, "metricx_qe_score": 1.9633837938308716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表明大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 58.20282060729978, "chrf_score": 57.543145084254874, "xcomet_score": 0.8559707403182983, "xcomet_qe_score": 0.9233258962631226, "metricx_score": 1.305349588394165, "metricx_qe_score": 0.9224607348442078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究主要集中在规划具有刻板印象的抽象目标上。", "metrics": {"bleu_score": 60.65859249958907, "chrf_score": 57.48106811377982, "xcomet_score": 0.8469344973564148, "xcomet_qe_score": 0.8436188101768494, "metricx_score": 1.9155192375183105, "metricx_qe_score": 2.2658839225769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而对于具有特定约束条件的目标(如制作巧克力蛋糕)的规划研究仍然不足。", "metrics": {"bleu_score": 24.784844034472297, "chrf_score": 25.301211865984012, "xcomet_score": 0.985657811164856, "xcomet_qe_score": 0.9642904996871948, "metricx_score": 0.9482950568199158, "metricx_qe_score": 1.437204360961914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受限语言规划的问题。 这些约束对规划目标施加了不同的限制。", "metrics": {"bleu_score": 68.8222595430076, "chrf_score": 63.702728375095305, "xcomet_score": 0.9459012746810913, "xcomet_qe_score": 0.8741466999053955, "metricx_score": 1.2282657623291016, "metricx_qe_score": 1.7322959899902344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象的目标可以被具有多方面约束的不同现实目标所", "metrics": {"bleu_score": 35.65507229795373, "chrf_score": 29.284130937364814, "xcomet_score": 0.7858191728591919, "xcomet_qe_score": 0.7410582304000854, "metricx_score": 5.4578680992126465, "metricx_qe_score": 3.7852275371551514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "继承。一个好的规划者应该编写符合约束条件且合理的脚本。", "metrics": {"bleu_score": 35.66602104177529, "chrf_score": 28.031886803120237, "xcomet_score": 0.5336942672729492, "xcomet_qe_score": 0.23490741848945618, "metricx_score": 4.762850761413574, "metricx_qe_score": 5.251812934875488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并提升了大语言模型的约束语言规划能力。", "metrics": {"bleu_score": 64.80785272594485, "chrf_score": 53.87082415243335, "xcomet_score": 0.8411085605621338, "xcomet_qe_score": 0.8048944473266602, "metricx_score": 0.9083527326583862, "metricx_qe_score": 0.901586651802063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定目标的数据集来支持我们的研究, 我们首先要实现这些目标。", "metrics": {"bleu_score": 56.24356197374772, "chrf_score": 49.82848622196004, "xcomet_score": 0.8556363582611084, "xcomet_qe_score": 0.8046624660491943, "metricx_score": 3.1498265266418457, "metricx_qe_score": 3.583329677581787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表中所示,我们通过多方面的约束来扩展抽象目标。对于需要人工参与的数据获取,使用 InstructGPT。", "metrics": {"bleu_score": 35.521418382801535, "chrf_score": 46.574999019185455, "xcomet_score": 0.9202866554260254, "xcomet_qe_score": 0.7295001745223999, "metricx_score": 2.157618284225464, "metricx_qe_score": 2.4019055366516113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随机选取 100 名特定女孩,并对从大型本地模型生成的脚本进行评估。", "metrics": {"bleu_score": 23.33738872135408, "chrf_score": 24.361266047899335, "xcomet_score": 0.5986389517784119, "xcomet_qe_score": 0.351519376039505, "metricx_score": 6.736242294311523, "metricx_qe_score": 6.838996887207031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有轻量级语言模型在规划特定目标方面都取得了不尽人意的结果。", "metrics": {"bleu_score": 38.114538012022926, "chrf_score": 35.875753064666036, "xcomet_score": 0.8672590255737305, "xcomet_qe_score": 0.8479272723197937, "metricx_score": 2.191626787185669, "metricx_qe_score": 2.432520866394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析,研究为什么线学习模型会失败。", "metrics": {"bleu_score": 31.569762540008654, "chrf_score": 26.04312848812731, "xcomet_score": 0.90468430519104, "xcomet_qe_score": 0.8788296580314636, "metricx_score": 0.9549468755722046, "metricx_qe_score": 1.0024168491363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果表明,生成脚本的语义完整性是可以接受的,但无法保证对约束的忠实度。", "metrics": {"bleu_score": 50.623729380870444, "chrf_score": 43.89099017872807, "xcomet_score": 0.9300323128700256, "xcomet_qe_score": 0.9634305834770203, "metricx_score": 1.296226978302002, "metricx_qe_score": 1.6733132600784302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了 WikiHow 中定义的更细致的主题类别限制。", "metrics": {"bleu_score": 39.706417990239444, "chrf_score": 46.470282751189046, "xcomet_score": 0.852881908416748, "xcomet_qe_score": 0.8424259424209595, "metricx_score": 2.2355387210845947, "metricx_qe_score": 1.7533934116363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热力图显示,不同类别的女孩在计划执行方面表现差异很大。", "metrics": {"bleu_score": 28.817612644655348, "chrf_score": 19.738583862596332, "xcomet_score": 0.7075936198234558, "xcomet_qe_score": 0.5999202728271484, "metricx_score": 5.6094279289245605, "metricx_qe_score": 5.235638618469238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前的研究表明,光-光风模型的输出质量存在高方差,导致性能不佳。", "metrics": {"bleu_score": 38.32544339233122, "chrf_score": 34.56924551799428, "xcomet_score": 0.7546435594558716, "xcomet_qe_score": 0.7191932797431946, "metricx_score": 4.575035572052002, "metricx_qe_score": 4.831528663635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用过生成 Z 滤波器的方法来提高生成质量。", "metrics": {"bleu_score": 49.60190231199743, "chrf_score": 44.45107242997041, "xcomet_score": 0.8075145483016968, "xcomet_qe_score": 0.7472153902053833, "metricx_score": 6.219874382019043, "metricx_qe_score": 6.740654468536377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先通过 CPT 交互的例子展示约束类型,并根据种子摘要目标获得具体目标。", "metrics": {"bleu_score": 54.747831104198596, "chrf_score": 37.840681627090696, "xcomet_score": 0.6397875547409058, "xcomet_qe_score": 0.6028582453727722, "metricx_score": 5.260824203491211, "metricx_qe_score": 5.655675888061523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指示 GPT 为特定目标过度生成案例脚本。", "metrics": {"bleu_score": 21.111871760808985, "chrf_score": 17.90907795677782, "xcomet_score": 0.732479989528656, "xcomet_qe_score": 0.653300940990448, "metricx_score": 4.539938926696777, "metricx_qe_score": 5.384664058685303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个筛选模型,用于选择可行的脚本。", "metrics": {"bleu_score": 31.184734991764778, "chrf_score": 25.741628504421953, "xcomet_score": 0.9741728901863098, "xcomet_qe_score": 0.9553138613700867, "metricx_score": 0.851082444190979, "metricx_qe_score": 0.954086422920227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为抽象的 GPT 嵌入,并计算余弦相似度和相似度分数,以衡量语义相似度。", "metrics": {"bleu_score": 67.30505976821392, "chrf_score": 53.552836790432536, "xcomet_score": 0.77895188331604, "xcomet_qe_score": 0.6454012393951416, "metricx_score": 2.8999927043914795, "metricx_qe_score": 2.6331543922424316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们避免包含目标约束关键字的脚本。", "metrics": {"bleu_score": 41.29283967509892, "chrf_score": 36.983936899114056, "xcomet_score": 0.7231687307357788, "xcomet_qe_score": 0.7492753267288208, "metricx_score": 6.241689682006836, "metricx_qe_score": 5.929896831512451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "只有当目标得分在目标集中最高时,我们才会保留该脚本。", "metrics": {"bleu_score": 56.39331823461213, "chrf_score": 46.11543532338081, "xcomet_score": 0.774955153465271, "xcomet_qe_score": 0.7281601428985596, "metricx_score": 3.315328598022461, "metricx_qe_score": 5.626780986785889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用我们的方法,InstructZBT可以生成更高质量的脚本。", "metrics": {"bleu_score": 70.98232254187813, "chrf_score": 64.06283025396857, "xcomet_score": 0.8540607690811157, "xcomet_qe_score": 0.8311640024185181, "metricx_score": 3.6497256755828857, "metricx_qe_score": 4.229335784912109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实度方面都极大地提高了规划能力。", "metrics": {"bleu_score": 78.54017110523375, "chrf_score": 71.69546487973302, "xcomet_score": 0.932198166847229, "xcomet_qe_score": 0.9382041692733765, "metricx_score": 0.819385290145874, "metricx_qe_score": 1.2170138359069824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大语言模型的部署成本高昂,因此必须赋予小型和专业模型语言规划能力。", "metrics": {"bleu_score": 54.46218662215637, "chrf_score": 49.60246877816443, "xcomet_score": 0.9849649667739868, "xcomet_qe_score": 0.9672563076019287, "metricx_score": 0.56695955991745, "metricx_qe_score": 0.5871807932853699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键步骤。", "metrics": {"bleu_score": 69.6015973294402, "chrf_score": 66.30344838521414, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026699073612689972, "metricx_qe_score": 0.14870662987232208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究无法为特定目标制定计划,手动数据集标注成本高昂。", "metrics": {"bleu_score": 39.63085984445429, "chrf_score": 32.42277775709353, "xcomet_score": 0.9903519153594971, "xcomet_qe_score": 0.9721243381500244, "metricx_score": 1.0022423267364502, "metricx_qe_score": 1.403224229812622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的理念,从大型语言模型中提炼出受限语言规划数据集", "metrics": {"bleu_score": 67.24686161726919, "chrf_score": 60.68269632133103, "xcomet_score": 0.920545220375061, "xcomet_qe_score": 0.8290176391601562, "metricx_score": 1.5140541791915894, "metricx_qe_score": 2.3462154865264893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们应用了构建受限语言规划数据集的方法,称为 Codescript。", "metrics": {"bleu_score": 36.486739368237764, "chrf_score": 38.381846696347424, "xcomet_score": 0.8101514577865601, "xcomet_qe_score": 0.8122366666793823, "metricx_score": 2.8865432739257812, "metricx_qe_score": 3.4243698120117188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了 55,000 个带有脚本的具体目标。", "metrics": {"bleu_score": 42.6291379422223, "chrf_score": 48.928085199824324, "xcomet_score": 0.834801435470581, "xcomet_qe_score": 0.849212110042572, "metricx_score": 1.7870681285858154, "metricx_qe_score": 1.751774549484253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试网站的质量,我们要求云端工人找到修改后的错误样本。", "metrics": {"bleu_score": 32.41706526701338, "chrf_score": 28.456390997352017, "xcomet_score": 0.682902455329895, "xcomet_qe_score": 0.6730047464370728, "metricx_score": 7.387237548828125, "metricx_qe_score": 7.055927276611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了代码脚本的约束分布。", "metrics": {"bleu_score": 49.202745153855076, "chrf_score": 29.64490504867994, "xcomet_score": 0.8700239658355713, "xcomet_qe_score": 0.8391302227973938, "metricx_score": 3.129685640335083, "metricx_qe_score": 4.138612270355225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成的特定目标中表现出高度的赞赏性。", "metrics": {"bleu_score": 47.901455811287484, "chrf_score": 34.1900259713097, "xcomet_score": 0.6674383878707886, "xcomet_qe_score": 0.5661716461181641, "metricx_score": 6.854194164276123, "metricx_qe_score": 7.9763994216918945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过代码脚本,我们可以追踪更小但更专业的模型,用于约束语言规划。", "metrics": {"bleu_score": 24.034793256416833, "chrf_score": 17.548129903176502, "xcomet_score": 0.6599546670913696, "xcomet_qe_score": 0.5962314009666443, "metricx_score": 5.51800012588501, "metricx_qe_score": 5.680723667144775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,T-file 在成本率上的功能可以生成比大多数大型语言模型更高质量的脚本,这表明,在适当的数据集上进行适当训练的小型模型可以支持大型模型。", "metrics": {"bleu_score": 47.85530747107588, "chrf_score": 38.09278195784458, "xcomet_score": 0.532019853591919, "xcomet_qe_score": 0.5170114040374756, "metricx_score": 7.444087982177734, "metricx_qe_score": 8.296777725219727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了受限语言规划问题。", "metrics": {"bleu_score": 72.83860464220109, "chrf_score": 71.99900522537406, "xcomet_score": 0.9069202542304993, "xcomet_qe_score": 0.8321535587310791, "metricx_score": 1.9986854791641235, "metricx_qe_score": 2.526510000228882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受限语言规划能力,并为大型语言模型开发了一种过度生成过滤方法。", "metrics": {"bleu_score": 63.393424969162815, "chrf_score": 55.612685434555196, "xcomet_score": 0.8912476301193237, "xcomet_qe_score": 0.8829815983772278, "metricx_score": 2.383878469467163, "metricx_qe_score": 3.3525240421295166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成高质量的脚本数据集,用于受限语言规划。", "metrics": {"bleu_score": 73.04997445393502, "chrf_score": 53.14066568268092, "xcomet_score": 0.9870063066482544, "xcomet_qe_score": 0.8990594148635864, "metricx_score": 2.213095188140869, "metricx_qe_score": 3.156728744506836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 CodeScript 数据集能成为推动语言规划研究的宝贵资源。", "metrics": {"bleu_score": 77.49224723289701, "chrf_score": 77.86905725014947, "xcomet_score": 0.9668655395507812, "xcomet_qe_score": 0.9773186445236206, "metricx_score": 0.8769727349281311, "metricx_qe_score": 1.0398505926132202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查看代码脚本的更多详细信息。", "metrics": {"bleu_score": 62.55340042200862, "chrf_score": 44.49895406414207, "xcomet_score": 0.8446244597434998, "xcomet_qe_score": 0.8269893527030945, "metricx_score": 2.5433461666107178, "metricx_qe_score": 2.760967493057251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫朱恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8176854848861694, "xcomet_qe_score": 0.8306083083152771, "metricx_score": 0.055027518421411514, "metricx_qe_score": 0.18194499611854553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的论文《2003年的核外实体标注器在2023年还能否正常工作?》", "metrics": {"bleu_score": 44.50866059065563, "chrf_score": 41.99594804241834, "xcomet_score": 0.7561721801757812, "xcomet_qe_score": 0.7736645936965942, "metricx_score": 5.474649906158447, "metricx_qe_score": 5.579144477844238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在开始吧。", "metrics": {"bleu_score": 43.01250851313264, "chrf_score": 29.763585038814398, "xcomet_score": 0.9983294010162354, "xcomet_qe_score": 0.9951030015945435, "metricx_score": 0.437703013420105, "metricx_qe_score": 0.7329950928688049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了泛化问题,使用了命名实体识别任务,或称为 NER 任务", "metrics": {"bleu_score": 52.08469476625915, "chrf_score": 45.153431763206505, "xcomet_score": 0.9296958446502686, "xcomet_qe_score": 0.8862329721450806, "metricx_score": 2.1753089427948, "metricx_qe_score": 3.364684820175171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,近 20 年来,模型一直在使用 CONO 2003 来开发命名实体识别。这自然引发了几个问题。", "metrics": {"bleu_score": 21.55841425941465, "chrf_score": 22.94266960285888, "xcomet_score": 0.7707717418670654, "xcomet_qe_score": 0.7402240037918091, "metricx_score": 5.72845983505249, "metricx_qe_score": 5.123396396636963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据?", "metrics": {"bleu_score": 53.12583871630397, "chrf_score": 42.12696617108382, "xcomet_score": 0.9173398017883301, "xcomet_qe_score": 0.9163376688957214, "metricx_score": 0.46578866243362427, "metricx_qe_score": 0.3853972554206848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新的标记器时,为了实现良好的泛化能力,需要什么条件?", "metrics": {"bleu_score": 40.588153399233, "chrf_score": 35.327090198845276, "xcomet_score": 0.996279239654541, "xcomet_qe_score": 0.9976413249969482, "metricx_score": 0.5133451223373413, "metricx_qe_score": 0.33327382802963257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们确实观察到泛化能力差,那么这些模型的性能下降是由什么原因造成的呢?", "metrics": {"bleu_score": 40.38098413802772, "chrf_score": 38.496112727192525, "xcomet_score": 0.9976638555526733, "xcomet_qe_score": 0.9893614053726196, "metricx_score": 0.6913033127784729, "metricx_qe_score": 0.802233099937439, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了 CONO++ 数据集。", "metrics": {"bleu_score": 53.688053584283026, "chrf_score": 40.12304593010765, "xcomet_score": 0.9045118093490601, "xcomet_qe_score": 0.8988876342773438, "metricx_score": 4.10100793838501, "metricx_qe_score": 4.396404266357422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们从 2020 年路透社新闻中收集的数据集,然后根据相同的 CONO 2003 标注指南对它们进行了标注。", "metrics": {"bleu_score": 51.25023054632659, "chrf_score": 46.27722099838197, "xcomet_score": 0.9446527361869812, "xcomet_qe_score": 0.9270708560943604, "metricx_score": 3.906636953353882, "metricx_qe_score": 3.367089033126831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在Kano 2003上对20多个模型进行了微调。", "metrics": {"bleu_score": 56.54859472485412, "chrf_score": 48.20176972309939, "xcomet_score": 0.7534037828445435, "xcomet_qe_score": 0.7585810422897339, "metricx_score": 5.306053161621094, "metricx_qe_score": 4.700647830963135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Kano 03测试集和Kano++测试集上对它们进行了评估。", "metrics": {"bleu_score": 53.611312694955046, "chrf_score": 42.55390873751552, "xcomet_score": 0.6281436681747437, "xcomet_qe_score": 0.6917903423309326, "metricx_score": 6.759796142578125, "metricx_qe_score": 5.960765361785889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了 F1 的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 71.76532607217811, "chrf_score": 69.17281761409991, "xcomet_score": 0.9940488338470459, "xcomet_qe_score": 0.9848737716674805, "metricx_score": 0.599238395690918, "metricx_qe_score": 0.9346641302108765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的泛化能力需要什么条件呢?", "metrics": {"bleu_score": 37.42031646082126, "chrf_score": 30.124680717222834, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.1679101586341858, "metricx_qe_score": 0.18634864687919617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现需要三个主要条件 ", "metrics": {"bleu_score": 45.261312215597314, "chrf_score": 41.89912986896373, "xcomet_score": 0.9738346338272095, "xcomet_qe_score": 0.8923285603523254, "metricx_score": 1.4693151712417603, "metricx_qe_score": 0.24204474687576294, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是模型架构。", "metrics": {"bleu_score": 60.042877124855906, "chrf_score": 51.821001027418625, "xcomet_score": 0.9962185621261597, "xcomet_qe_score": 0.9754199981689453, "metricx_score": 0.04136792570352554, "metricx_qe_score": 0.07232436537742615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现 Transformer 模型通常能更好地推广到新数据。", "metrics": {"bleu_score": 50.949472008875915, "chrf_score": 59.70531812734149, "xcomet_score": 0.8084381818771362, "xcomet_qe_score": 0.7700134515762329, "metricx_score": 3.1043787002563477, "metricx_qe_score": 4.589556694030762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常情况下,模型越大,泛化能力越强。", "metrics": {"bleu_score": 16.133948681475328, "chrf_score": 16.244220530681023, "xcomet_score": 0.9969384670257568, "xcomet_qe_score": 0.9846937656402588, "metricx_score": 0.34818410873413086, "metricx_qe_score": 0.5096091032028198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们都知道微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 51.2691819618139, "chrf_score": 59.876557108622194, "xcomet_score": 0.9417649507522583, "xcomet_qe_score": 0.9170168042182922, "metricx_score": 3.607921600341797, "metricx_qe_score": 2.110914707183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现更多的微调示例实际上也能带来更好的泛化能力。", "metrics": {"bleu_score": 65.01148795387888, "chrf_score": 55.16026110229008, "xcomet_score": 0.9911754131317139, "xcomet_qe_score": 0.9230656027793884, "metricx_score": 0.576622724533081, "metricx_qe_score": 0.9240424633026123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来我们讨论的问题是,导致某些模型性能下降的原因是什么 我们有两个假设。", "metrics": {"bleu_score": 31.73336649961909, "chrf_score": 28.103082304932396, "xcomet_score": 0.9581712484359741, "xcomet_qe_score": 0.9582287669181824, "metricx_score": 1.496222972869873, "metricx_qe_score": 1.6148444414138794, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,即由于反复使用相同的测试集而导致的过拟合。这通常表现为在新测试集上的回报减少。", "metrics": {"bleu_score": 61.18834309807102, "chrf_score": 54.834446981292885, "xcomet_score": 0.9087151288986206, "xcomet_qe_score": 0.8903906941413879, "metricx_score": 2.7176403999328613, "metricx_qe_score": 3.1584489345550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间差距越来越大而导致的性能下降。", "metrics": {"bleu_score": 59.75281862052228, "chrf_score": 55.53728510769358, "xcomet_score": 0.964687705039978, "xcomet_qe_score": 0.888350784778595, "metricx_score": 1.4822864532470703, "metricx_qe_score": 2.0078001022338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表中看到,红色最佳拟合线的梯度大于 1。", "metrics": {"bleu_score": 52.73680880348243, "chrf_score": 49.25256889307443, "xcomet_score": 0.8774145245552063, "xcomet_qe_score": 0.8086611032485962, "metricx_score": 1.220704436302185, "metricx_qe_score": 1.6646404266357422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在 Carnot 2003 上每改进一个单位,就能在 Carnot++ 上获得超过一个单位的改进,这意味着没有收益递减。", "metrics": {"bleu_score": 33.22624265770233, "chrf_score": 30.668725106802757, "xcomet_score": 0.7890077829360962, "xcomet_qe_score": 0.7820112705230713, "metricx_score": 7.659329414367676, "metricx_qe_score": 6.482642650604248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下没有观察到自适应过拟合。", "metrics": {"bleu_score": 74.93731939490364, "chrf_score": 69.43707675795987, "xcomet_score": 0.9009255766868591, "xcomet_qe_score": 0.9129918217658997, "metricx_score": 1.1392955780029297, "metricx_qe_score": 1.6724114418029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间漂移呢?", "metrics": {"bleu_score": 52.47357977607325, "chrf_score": 40.51960415097498, "xcomet_score": 0.9311673641204834, "xcomet_qe_score": 0.9159005284309387, "metricx_score": 0.3731555640697479, "metricx_qe_score": 0.8887962102890015, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,使用更新的数据对一些模型进行重新训练或继续预训练,我们发现随着时间差距的增大,性能会下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 62.051555337541856, "chrf_score": 54.885383324451944, "xcomet_score": 0.9541727304458618, "xcomet_qe_score": 0.9066509008407593, "metricx_score": 1.7297509908676147, "metricx_qe_score": 1.8825163841247559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型规模以及更多的微调示例。", "metrics": {"bleu_score": 84.66320077027171, "chrf_score": 82.81449173367797, "xcomet_score": 0.9898571968078613, "xcomet_qe_score": 0.9796369075775146, "metricx_score": 0.8873034119606018, "metricx_qe_score": 1.3054416179656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素是相辅相成的。我们不能只拥有其中一个因素,而要兼顾其他因素。", "metrics": {"bleu_score": 42.85345729742259, "chrf_score": 36.82810705090657, "xcomet_score": 0.8793240785598755, "xcomet_qe_score": 0.9167267680168152, "metricx_score": 0.8026317954063416, "metricx_qe_score": 1.222589373588562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,这里的性能下降是由时间漂移引起的,令人惊讶的是,它不是由自适应过拟合引起的,尽管KONO 2003已经使用了20多年。", "metrics": {"bleu_score": 61.09489512954107, "chrf_score": 53.62654546160404, "xcomet_score": 0.7070241570472717, "xcomet_qe_score": 0.741197943687439, "metricx_score": 4.881323337554932, "metricx_qe_score": 5.569640159606934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,回到我们在论文标题中提出的问题,Connell 2003 标签器在 2023 年是否仍然有效?", "metrics": {"bleu_score": 68.04439980111333, "chrf_score": 60.44807579021266, "xcomet_score": 0.8644450902938843, "xcomet_qe_score": 0.8803226947784424, "metricx_score": 2.023376226425171, "metricx_qe_score": 2.171315908432007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是肯定的。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 53.5017446130673, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43837279081344604, "metricx_qe_score": 0.7667475342750549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能促使人们进一步研究如何改进模型的泛化能力", "metrics": {"bleu_score": 42.79207937254296, "chrf_score": 37.29744070857382, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4619315266609192, "metricx_qe_score": 0.536376953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果您有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.11026448209409, "chrf_score": 52.5325928587273, "xcomet_score": 0.9871149063110352, "xcomet_qe_score": 0.9712950587272644, "metricx_score": 0.2757876515388489, "metricx_qe_score": 0.26652368903160095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9670989513397217, "xcomet_qe_score": 0.9718614816665649, "metricx_score": 0.2643663287162781, "metricx_qe_score": 0.26394033432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将谈谈我们在解决实体选择中的间接指称表达方面的工作,我们在此过程中引入了 AltEntityScorers。", "metrics": {"bleu_score": 34.66720379073122, "chrf_score": 37.04778180978313, "xcomet_score": 0.794828474521637, "xcomet_qe_score": 0.8046687841415405, "metricx_score": 5.738161563873291, "metricx_qe_score": 6.702040195465088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是贾瓦德·霍赛尼,这是我和菲利普·拉德林斯基、西尔维亚·帕里蒂和安妮·刘易斯共同完成的作品。", "metrics": {"bleu_score": 3.5721882074539613, "chrf_score": 3.2020363986973255, "xcomet_score": 0.9839707612991333, "xcomet_qe_score": 0.9902433753013611, "metricx_score": 1.774201512336731, "metricx_qe_score": 1.5719778537750244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在想要做出选择时的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7934911251068115, "metricx_qe_score": 0.9513019919395447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑以下替代问题。", "metrics": {"bleu_score": 19.969395881889398, "chrf_score": 16.322199135289424, "xcomet_score": 0.8779173493385315, "xcomet_qe_score": 0.8629541993141174, "metricx_score": 0.5324400067329407, "metricx_qe_score": 0.27518725395202637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说 easy on me 还是 I got a feeling?这里", "metrics": {"bleu_score": 10.954749601576006, "chrf_score": 28.18566932202382, "xcomet_score": 0.6745657324790955, "xcomet_qe_score": 0.5964515209197998, "metricx_score": 5.828319549560547, "metricx_qe_score": 5.446682929992676, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用户想要在这两首歌中进行选择。", "metrics": {"bleu_score": 30.692824561582807, "chrf_score": 28.091529331245614, "xcomet_score": 0.9831295013427734, "xcomet_qe_score": 0.9641683101654053, "metricx_score": 0.770147442817688, "metricx_qe_score": 0.6882972717285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是直接引用。例如,说出歌曲的名字是Yami或它的位置,第一首。", "metrics": {"bleu_score": 29.470005469407518, "chrf_score": 23.413575348255304, "xcomet_score": 0.6799850463867188, "xcomet_qe_score": 0.5762488842010498, "metricx_score": 7.502706050872803, "metricx_qe_score": 8.657044410705566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但有时,间接引用更适合进行更自然的对话。", "metrics": {"bleu_score": 63.82077270030547, "chrf_score": 58.905776668266995, "xcomet_score": 0.8790466785430908, "xcomet_qe_score": 0.8784265518188477, "metricx_score": 1.2507023811340332, "metricx_qe_score": 1.0162642002105713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户记不起歌曲的名字时,这种情况可能会发生。", "metrics": {"bleu_score": 30.400527345308223, "chrf_score": 26.104718059998188, "xcomet_score": 0.9999790191650391, "xcomet_qe_score": 0.9998631477355957, "metricx_score": 0.23017829656600952, "metricx_qe_score": 0.30660808086395264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者发音过于相似,难以区分。", "metrics": {"bleu_score": 24.792487205599897, "chrf_score": 23.40751844934748, "xcomet_score": 0.9801307916641235, "xcomet_qe_score": 0.9764577150344849, "metricx_score": 0.7795825004577637, "metricx_qe_score": 0.2045377939939499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定偏好时。以下是直接差异的一", "metrics": {"bleu_score": 27.47912587281734, "chrf_score": 21.509916600183594, "xcomet_score": 0.734346330165863, "xcomet_qe_score": 0.3869348168373108, "metricx_score": 5.060774803161621, "metricx_qe_score": 1.8838739395141602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些例子。例如,更新的歌曲或不是充满活力的歌曲。", "metrics": {"bleu_score": 3.9392868932136413, "chrf_score": 7.206036064379837, "xcomet_score": 0.47934654355049133, "xcomet_qe_score": 0.5689108371734619, "metricx_score": 12.26533317565918, "metricx_qe_score": 11.73098087310791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是用于基准测试大型语言模型实体理解能力的一个重要问题。", "metrics": {"bleu_score": 58.29909775312003, "chrf_score": 55.115626799151066, "xcomet_score": 0.8675270080566406, "xcomet_qe_score": 0.8055980205535889, "metricx_score": 3.0553641319274902, "metricx_qe_score": 2.2114195823669434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有发现一个针对该任务的大型公共数据集。因此,我们使用众包标注方式收集了一个数据集。", "metrics": {"bleu_score": 42.9045843077145, "chrf_score": 37.51316433016489, "xcomet_score": 0.8318203687667847, "xcomet_qe_score": 0.823009729385376, "metricx_score": 1.5358479022979736, "metricx_qe_score": 1.6088104248046875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域,分别是音乐、书籍和食谱。", "metrics": {"bleu_score": 69.89969645773454, "chrf_score": 69.57406033528801, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2069309502840042, "metricx_qe_score": 0.2953225374221802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通人物完成集。", "metrics": {"bleu_score": 60.1067427964331, "chrf_score": 52.13865536259368, "xcomet_score": 0.8156169652938843, "xcomet_qe_score": 0.8109356760978699, "metricx_score": 4.1382269859313965, "metricx_qe_score": 4.203125476837158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这幅漫画有三个对话气泡。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 75.16242688801647, "xcomet_score": 0.8998875617980957, "xcomet_qe_score": 0.8733921647071838, "metricx_score": 0.8555368781089783, "metricx_qe_score": 0.929405927658081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡里,鲍勃说:“还记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 58.217473175544946, "chrf_score": 52.09244807795531, "xcomet_score": 0.9765057563781738, "xcomet_qe_score": 0.9003608226776123, "metricx_score": 1.2223185300827026, "metricx_qe_score": 1.5910086631774902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后鲍勃就设置了对话的背景。", "metrics": {"bleu_score": 41.62799023755188, "chrf_score": 32.34587601437742, "xcomet_score": 0.955114483833313, "xcomet_qe_score": 0.9835449457168579, "metricx_score": 1.535586953163147, "metricx_qe_score": 1.261259913444519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话气泡中,爱丽丝说,你是说对我手下留情,还是我有种感觉?", "metrics": {"bleu_score": 14.346191244153777, "chrf_score": 9.792423133386114, "xcomet_score": 0.7654969692230225, "xcomet_qe_score": 0.7892031073570251, "metricx_score": 5.053875923156738, "metricx_qe_score": 4.8178181648254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个备选问题。", "metrics": {"bleu_score": 44.632361378533304, "chrf_score": 35.1521164021164, "xcomet_score": 0.8520497679710388, "xcomet_qe_score": 0.8590137362480164, "metricx_score": 0.515360951423645, "metricx_qe_score": 0.8138759732246399, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用间接引用来选择其中一个实体,例如,新的", "metrics": {"bleu_score": 38.4115464721191, "chrf_score": 31.262960708050073, "xcomet_score": 0.6078081130981445, "xcomet_qe_score": 0.6462385058403015, "metricx_score": 6.0580291748046875, "metricx_qe_score": 5.386853218078613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一和第二个语音气泡,但第三个由注释者填写。", "metrics": {"bleu_score": 48.57308570940184, "chrf_score": 39.9318040076293, "xcomet_score": 0.9019738435745239, "xcomet_qe_score": 0.8493963479995728, "metricx_score": 1.7426731586456299, "metricx_qe_score": 1.8181675672531128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个语音气泡是从每个领域的一些手动提示中选出的。", "metrics": {"bleu_score": 36.23885503140913, "chrf_score": 30.528453986273142, "xcomet_score": 0.813565731048584, "xcomet_qe_score": 0.7460943460464478, "metricx_score": 2.8652963638305664, "metricx_qe_score": 2.601848602294922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个问题是备选问题,生成方式如下。", "metrics": {"bleu_score": 12.512236921161914, "chrf_score": 13.932659938671922, "xcomet_score": 0.9060136079788208, "xcomet_qe_score": 0.9135813117027283, "metricx_score": 0.3856356739997864, "metricx_qe_score": 0.4006219208240509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是来自维基百科的样本。", "metrics": {"bleu_score": 86.11735299633672, "chrf_score": 96.57574311968287, "xcomet_score": 0.9713319540023804, "xcomet_qe_score": 0.9262405037879944, "metricx_score": 0.7253305912017822, "metricx_qe_score": 0.857016384601593, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用过的不同采样方法。", "metrics": {"bleu_score": 61.28081331864041, "chrf_score": 52.92624054700117, "xcomet_score": 0.9960860013961792, "xcomet_qe_score": 0.998734712600708, "metricx_score": 0.15488818287849426, "metricx_qe_score": 0.2388148009777069, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体变得越来越相似,通常更难进行消歧。", "metrics": {"bleu_score": 48.287171837052036, "chrf_score": 42.61563713257725, "xcomet_score": 0.8577663898468018, "xcomet_qe_score": 0.7904423475265503, "metricx_score": 4.571025848388672, "metricx_qe_score": 5.461899757385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀随机。", "metrics": {"bleu_score": 19.437571020720103, "chrf_score": 18.153313508301462, "xcomet_score": 0.9029233455657959, "xcomet_qe_score": 0.8752520084381104, "metricx_score": 1.6346652507781982, "metricx_qe_score": 1.535372018814087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个情况是实体名称相似,例如两本书都叫《回归》。", "metrics": {"bleu_score": 20.485079606183252, "chrf_score": 16.55671268943578, "xcomet_score": 0.859702467918396, "xcomet_qe_score": 0.8329830169677734, "metricx_score": 2.6939847469329834, "metricx_qe_score": 2.761162519454956, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是,它们在维基百科上的描述相似。", "metrics": {"bleu_score": 65.47599888066033, "chrf_score": 55.47856569528397, "xcomet_score": 0.9969536066055298, "xcomet_qe_score": 0.9877551794052124, "metricx_score": 0.3662336766719818, "metricx_qe_score": 0.5076944231987, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当它们在维基百科上有相似的信息框或属性时。", "metrics": {"bleu_score": 69.63845241054851, "chrf_score": 64.76296666996701, "xcomet_score": 0.9417118430137634, "xcomet_qe_score": 0.997989296913147, "metricx_score": 0.9182050228118896, "metricx_qe_score": 1.1083542108535767, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,相同的类型或相同的艺术家。", "metrics": {"bleu_score": 27.56001678723635, "chrf_score": 26.527940292762427, "xcomet_score": 0.9195131063461304, "xcomet_qe_score": 0.8445156812667847, "metricx_score": 1.135671854019165, "metricx_qe_score": 1.485953688621521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向注释者展示这个问题的另一种表述时,他们知道这些实体的名称,但并不一定了解实体本身。", "metrics": {"bleu_score": 50.54904637028962, "chrf_score": 48.032751786122, "xcomet_score": 0.97664475440979, "xcomet_qe_score": 0.9797818660736084, "metricx_score": 1.6067235469818115, "metricx_qe_score": 1.7753238677978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们的做法是展示这两个实体的一些背景知识。", "metrics": {"bleu_score": 61.29574979328211, "chrf_score": 58.45251923670799, "xcomet_score": 0.9705458879470825, "xcomet_qe_score": 0.8331559896469116, "metricx_score": 1.0227710008621216, "metricx_qe_score": 1.5545063018798828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只需为每首歌曲提供一个谷歌搜索链接。 然后请注释员至少收听每首歌曲的部分内容,并阅读每首歌曲的介绍。", "metrics": {"bleu_score": 32.809387946197226, "chrf_score": 29.050997761916054, "xcomet_score": 0.9440914392471313, "xcomet_qe_score": 0.8971079587936401, "metricx_score": 1.0769095420837402, "metricx_qe_score": 1.3415470123291016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,以下是“Easy Annotation”歌曲的 Google 搜索结果。", "metrics": {"bleu_score": 26.75087393232737, "chrf_score": 27.413701684155377, "xcomet_score": 0.7891664505004883, "xcomet_qe_score": 0.7688403725624084, "metricx_score": 7.039701461791992, "metricx_qe_score": 7.389359951019287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们从维基百科展示一些背景文本。", "metrics": {"bleu_score": 59.26487606292854, "chrf_score": 51.436354022614836, "xcomet_score": 0.9749809503555298, "xcomet_qe_score": 0.8663438558578491, "metricx_score": 0.889976978302002, "metricx_qe_score": 1.2329232692718506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科再次展示它们的图片,以便注释者知道它们的样子。", "metrics": {"bleu_score": 48.86038975214478, "chrf_score": 41.35671971963028, "xcomet_score": 0.8975057601928711, "xcomet_qe_score": 0.8840309381484985, "metricx_score": 1.727947473526001, "metricx_qe_score": 1.6701533794403076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求注释者选择其中一个实体,例如,这里选择第一个,并使用三到五个间接指称表达来描述它们。", "metrics": {"bleu_score": 53.113745397242596, "chrf_score": 46.53064150790948, "xcomet_score": 0.8557240962982178, "xcomet_qe_score": 0.8202291131019592, "metricx_score": 2.2282660007476807, "metricx_qe_score": 2.4845852851867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,钢琴音乐的那首。", "metrics": {"bleu_score": 11.731175160263996, "chrf_score": 12.42424242424242, "xcomet_score": 0.9431931972503662, "xcomet_qe_score": 0.8839001655578613, "metricx_score": 2.748188018798828, "metricx_qe_score": 1.9585423469543457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的几个例子。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9644975662231445, "xcomet_qe_score": 0.8701311945915222, "metricx_score": 0.5822314620018005, "metricx_qe_score": 1.956078290939331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有歌词的那首,不是那个12岁男孩演唱的那首,也不是虚构的那首,或者来自阿塞拜疆的那首等等。", "metrics": {"bleu_score": 26.22833416635814, "chrf_score": 26.050888958209338, "xcomet_score": 0.9031228423118591, "xcomet_qe_score": 0.8997166752815247, "metricx_score": 1.690661907196045, "metricx_qe_score": 1.6848784685134888, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Altentities语料库包含三个领域的6000个备选问题,以及42,000个间接指称表达。", "metrics": {"bleu_score": 38.273027708365426, "chrf_score": 50.32457833740093, "xcomet_score": 0.8443180322647095, "xcomet_qe_score": 0.851588785648346, "metricx_score": 2.587282419204712, "metricx_qe_score": 2.7891829013824463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是使用T5XLARGE模型的结果总结。", "metrics": {"bleu_score": 33.36118221483977, "chrf_score": 41.90869188914296, "xcomet_score": 0.9389135837554932, "xcomet_qe_score": 0.9359627366065979, "metricx_score": 2.977196455001831, "metricx_qe_score": 2.7315287590026855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与注释者完全相同的背景知识,那么准确率就会非常高。大约在 92% 到 95% 之间。", "metrics": {"bleu_score": 88.47064105457828, "chrf_score": 84.3111982542258, "xcomet_score": 0.9747552871704102, "xcomet_qe_score": 0.9631763696670532, "metricx_score": 0.8064987659454346, "metricx_qe_score": 0.8242776393890381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确率在 82% 到 87% 之间,这更符合实际情况。", "metrics": {"bleu_score": 81.77316623742497, "chrf_score": 80.43778311743763, "xcomet_score": 0.9159308671951294, "xcomet_qe_score": 0.9119892120361328, "metricx_score": 0.9283930063247681, "metricx_qe_score": 1.2291322946548462, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率只有 60%。所以还有很大的改进空间。", "metrics": {"bleu_score": 76.9218097681464, "chrf_score": 72.93605951078253, "xcomet_score": 0.9966810941696167, "xcomet_qe_score": 0.9928364753723145, "metricx_score": 1.5012954473495483, "metricx_qe_score": 2.6159017086029053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还表明,这些模型具有领域通用性。", "metrics": {"bleu_score": 72.76817202342096, "chrf_score": 67.51641468553233, "xcomet_score": 0.9773801565170288, "xcomet_qe_score": 0.9063783288002014, "metricx_score": 0.5223034620285034, "metricx_qe_score": 0.6212144494056702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集链接。", "metrics": {"bleu_score": 29.50234363196404, "chrf_score": 30.119895842275447, "xcomet_score": 0.9908864498138428, "xcomet_qe_score": 0.9903539419174194, "metricx_score": 0.23959046602249146, "metricx_qe_score": 0.38847288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕佩,我将简要介绍一篇题为“注意力作为同时语音翻译的指导”的论文,这是我和马特奥·内格里、马可·图尔奇的合作成果。", "metrics": {"bleu_score": 37.938642922253635, "chrf_score": 30.361136005192012, "xcomet_score": 0.7655929327011108, "xcomet_qe_score": 0.6731269359588623, "metricx_score": 3.571209192276001, "metricx_qe_score": 2.893418073654175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同声语音翻译?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9885314702987671, "xcomet_qe_score": 0.9054443836212158, "metricx_score": 0.16075709462165833, "metricx_qe_score": 0.07203303277492523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同声语音翻译,或称simulST,是指将口语实时翻译成另一种语言的文本的过程,从而实现跨语言交流。 那么,", "metrics": {"bleu_score": 56.597173773878566, "chrf_score": 58.817947663746025, "xcomet_score": 0.7920486927032471, "xcomet_qe_score": 0.7903342843055725, "metricx_score": 4.536383628845215, "metricx_qe_score": 3.972161293029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 SimulST 模型存在哪些问题呢?", "metrics": {"bleu_score": 53.16967153331756, "chrf_score": 68.88793130936092, "xcomet_score": 0.9991519451141357, "xcomet_qe_score": 0.9944875240325928, "metricx_score": 0.35536760091781616, "metricx_qe_score": 0.4873923659324646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常会对特定的架构进行训练,从而引入需要优化的附加模块。", "metrics": {"bleu_score": 29.61516536011624, "chrf_score": 28.950643770210732, "xcomet_score": 0.8755805492401123, "xcomet_qe_score": 0.8077154159545898, "metricx_score": 0.8932307958602905, "metricx_qe_score": 1.7404085397720337, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,涉及不同优化目标的训练过程", "metrics": {"bleu_score": 49.45378690183769, "chrf_score": 50.10450623628368, "xcomet_score": 0.8224443793296814, "xcomet_qe_score": 0.6969528198242188, "metricx_score": 3.1351871490478516, "metricx_qe_score": 4.49100399017334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "冗长且复杂 并训练和维护多个模型以达到不同的延迟级", "metrics": {"bleu_score": 50.28347027367617, "chrf_score": 45.232450585072904, "xcomet_score": 0.7195512056350708, "xcomet_qe_score": 0.47897905111312866, "metricx_score": 3.557307004928589, "metricx_qe_score": 3.601761817932129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别,例如训练一个平均延迟为1秒的模型,另一个延迟为2秒的模型,依此类推。", "metrics": {"bleu_score": 55.69909641339816, "chrf_score": 46.18366652449356, "xcomet_score": 0.6523949503898621, "xcomet_qe_score": 0.48727354407310486, "metricx_score": 2.1123194694519043, "metricx_qe_score": 2.703685998916626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先两种方法:使用已经存在的离线单文档模型,无需重新训练或采用特定的单文档模型架构。", "metrics": {"bleu_score": 45.77434748097164, "chrf_score": 41.1753909049119, "xcomet_score": 0.5628436803817749, "xcomet_qe_score": 0.6159299612045288, "metricx_score": 5.262686729431152, "metricx_qe_score": 4.285281181335449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每个延迟方案只使用一个模型,并通过特定参数来处理延迟。", "metrics": {"bleu_score": 61.88976749095537, "chrf_score": 53.12948472327568, "xcomet_score": 0.9900835752487183, "xcomet_qe_score": 0.9867634773254395, "metricx_score": 0.5734869241714478, "metricx_qe_score": 0.7232542037963867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并通过音频输入和文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 60.60046478391066, "chrf_score": 60.4154345705964, "xcomet_score": 0.7925113439559937, "xcomet_qe_score": 0.6658550500869751, "metricx_score": 5.002397537231445, "metricx_qe_score": 4.8261895179748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")利用模型已经获得的知识。右边可以看到一个例子。", "metrics": {"bleu_score": 4.92467473444436, "chrf_score": 6.754417382999043, "xcomet_score": 0.4164741039276123, "xcomet_qe_score": 0.3726116418838501, "metricx_score": 4.819536209106445, "metricx_qe_score": 6.47175931930542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出 ADAT 或编码器-解码器注意力,这是一种策略,根据注意力的指向,我们决定是否发出部分翻译。", "metrics": {"bleu_score": 54.67463001803805, "chrf_score": 43.75902946689201, "xcomet_score": 0.6936603784561157, "xcomet_qe_score": 0.6983391046524048, "metricx_score": 5.533845901489258, "metricx_qe_score": 6.253589153289795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果张力不集中,就会发出一个词,也就是说,其总和低于某个阈值 α,接近语音帧的最后一行,这意味着接收到的信息是......", "metrics": {"bleu_score": 33.488068388318574, "chrf_score": 28.437460878252924, "xcomet_score": 0.3749432861804962, "xcomet_qe_score": 0.3292433023452759, "metricx_score": 8.26557445526123, "metricx_qe_score": 7.942408561706543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们收到一个包含“我要谈论”的语音片段,我们的模型预测德语翻译为 我们还将研究交叉注意力权重 我们会发现,前两个词指向最早接收到的语音帧,而最后一个词指向最后一个接收到的语音帧,即lambda语音帧。", "metrics": {"bleu_score": 46.23502531733136, "chrf_score": 35.373127787871375, "xcomet_score": 0.4620272219181061, "xcomet_qe_score": 0.4810977280139923, "metricx_score": 6.928632736206055, "metricx_qe_score": 6.861798286437988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被省略。 然而,如果交叉注意力的总和超过某个阈值 alpha,我们就不会发出最后一个词,而是等待另一个语音片段。", "metrics": {"bleu_score": 37.63721507675659, "chrf_score": 32.274655605251, "xcomet_score": 0.7317036390304565, "xcomet_qe_score": 0.7843872904777527, "metricx_score": 3.8404736518859863, "metricx_qe_score": 4.467621326446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续进行,接收到另一个语音片段,我们的模型预测出其他三个词,我们会查看交叉注意力权重。 我们会发现,没有任何词语指向最后的 lambda 语音帧。", "metrics": {"bleu_score": 45.03741820090278, "chrf_score": 38.769432261996876, "xcomet_score": 0.7169959545135498, "xcomet_qe_score": 0.6557332873344421, "metricx_score": 3.337991714477539, "metricx_qe_score": 3.567087411880493, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将被发出。", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 27.259129759129756, "xcomet_score": 0.9332944750785828, "xcomet_qe_score": 0.8744902610778809, "metricx_score": 1.6925324201583862, "metricx_qe_score": 3.3203859329223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们看一下其主要结果 我们在图表上绘制了同时语音翻译的结果,其中一侧为蓝色,用于衡量翻译质量和平均滞后时间。 这就是延迟度量。我们还考虑了计算感知平均滞后,它考虑了模型预测输出所需的计算时间。", "metrics": {"bleu_score": 41.91205803450124, "chrf_score": 32.60912936207038, "xcomet_score": 0.72102290391922, "xcomet_qe_score": 0.5388373136520386, "metricx_score": 6.2125043869018555, "metricx_qe_score": 6.425321578979492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望我们的曲线在这个图上尽可能高。", "metrics": {"bleu_score": 37.21106799318148, "chrf_score": 34.07518000736071, "xcomet_score": 0.9693130254745483, "xcomet_qe_score": 0.85105299949646, "metricx_score": 1.4359958171844482, "metricx_qe_score": 1.8472847938537598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但我们也希望它们向左移动。", "metrics": {"bleu_score": 86.17038791239612, "chrf_score": 84.90244110859445, "xcomet_score": 0.9971116781234741, "xcomet_qe_score": 0.9812257289886475, "metricx_score": 0.6350057721138, "metricx_qe_score": 1.0446958541870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将它们与适用于离线模型的适当策略进行了比较,这些策略包括湿键策略和局部协议。", "metrics": {"bleu_score": 28.761604244499782, "chrf_score": 21.795736111132378, "xcomet_score": 0.7566090822219849, "xcomet_qe_score": 0.7259286642074585, "metricx_score": 5.253239154815674, "metricx_qe_score": 5.590273857116699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将它们与专门为同时预翻译设计的最先进架构进行了比较。", "metrics": {"bleu_score": 48.4523043692067, "chrf_score": 45.38418183703729, "xcomet_score": 0.9260720014572144, "xcomet_qe_score": 0.9103882908821106, "metricx_score": 1.3279746770858765, "metricx_qe_score": 1.3704530000686646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是德语同声传译策略的所有结果。", "metrics": {"bleu_score": 71.97795533614756, "chrf_score": 62.44064078622902, "xcomet_score": 0.8703422546386719, "xcomet_qe_score": 0.8403339982032776, "metricx_score": 1.0494918823242188, "metricx_qe_score": 1.5705538988113403, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,由于曲线向左移动,它比所有应用于离线模型的策略表现更好 我们还", "metrics": {"bleu_score": 39.69023928538994, "chrf_score": 38.829310833876484, "xcomet_score": 0.7548093795776367, "xcomet_qe_score": 0.707801878452301, "metricx_score": 6.312031269073486, "metricx_qe_score": 3.3142940998077393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "看到,如果我们考虑实际的经过时间或计算感知时间,那么这是最快的策略。", "metrics": {"bleu_score": 46.722357333009334, "chrf_score": 40.85002101760919, "xcomet_score": 0.7015319466590881, "xcomet_qe_score": 0.6982788443565369, "metricx_score": 4.6013898849487305, "metricx_qe_score": 4.872084617614746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想发现更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 65.14613449066714, "chrf_score": 54.128468638917546, "xcomet_score": 0.9677166938781738, "xcomet_qe_score": 0.9603763222694397, "metricx_score": 0.6387813091278076, "metricx_qe_score": 0.45578014850616455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码和模型,并同时输出,以促进我们工作的可重复性。", "metrics": {"bleu_score": 35.02767179601167, "chrf_score": 35.63470665966163, "xcomet_score": 0.8507549166679382, "xcomet_qe_score": 0.8230204582214355, "metricx_score": 1.0441845655441284, "metricx_qe_score": 1.5153651237487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫 Ying,我和我的同事 Zhiyang 将介绍我们关于多模态序列短时学习通过指令调优进行多方面改进的研究。 因此", "metrics": {"bleu_score": 47.53662515854622, "chrf_score": 46.12835685421387, "xcomet_score": 0.6012369394302368, "xcomet_qe_score": 0.6713457703590393, "metricx_score": 5.808809280395508, "metricx_qe_score": 4.749074459075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",随着大型语言模型的进步,许多研究开始探索新的学习范式,以参数和数据高效的方式将预训练语言模型重用于不同的下游任务。", "metrics": {"bleu_score": 74.44150759500691, "chrf_score": 65.47936952694423, "xcomet_score": 0.8788344860076904, "xcomet_qe_score": 0.8451521396636963, "metricx_score": 2.69541072845459, "metricx_qe_score": 3.3829829692840576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,通过遵循自然指令,指令微调使大型语言模型能够以零样本方式在未见过的任务上表现出色。", "metrics": {"bleu_score": 44.72974580114728, "chrf_score": 40.978669018571445, "xcomet_score": 0.8073792457580566, "xcomet_qe_score": 0.6887549757957458, "metricx_score": 2.3140363693237305, "metricx_qe_score": 3.306260824203491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数关于指令调优的先前工作都集中在提高语言任务的序列图性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 41.46697523650318, "chrf_score": 36.51392452739049, "xcomet_score": 0.8868240118026733, "xcomet_qe_score": 0.8130327463150024, "metricx_score": 1.6384031772613525, "metricx_qe_score": 2.1109747886657715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想要研究多模态预训练模型的指令微调是否真的能提高对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 40.24793913874828, "chrf_score": 34.369737174738965, "xcomet_score": 0.8913238048553467, "xcomet_qe_score": 0.8282774686813354, "metricx_score": 1.5758507251739502, "metricx_qe_score": 1.7023435831069946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现 NLP 和多模态任务在指令数据集的可用性上存在显著差异。 目前", "metrics": {"bleu_score": 29.672891257107338, "chrf_score": 24.728973889504875, "xcomet_score": 0.7793281078338623, "xcomet_qe_score": 0.646827220916748, "metricx_score": 4.997180938720703, "metricx_qe_score": 1.364890217781067, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过 1600 个仅包含语言的指令任务。", "metrics": {"bleu_score": 63.019085559238604, "chrf_score": 71.18912409156091, "xcomet_score": 0.8993534445762634, "xcomet_qe_score": 0.8876726031303406, "metricx_score": 0.9194692969322205, "metricx_qe_score": 1.4376158714294434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,目前尚无大规模的多模态指令任务公开可用。", "metrics": {"bleu_score": 41.261520349079454, "chrf_score": 40.153446172185845, "xcomet_score": 0.9708065986633301, "xcomet_qe_score": 0.8797028064727783, "metricx_score": 2.3796322345733643, "metricx_qe_score": 2.728731155395508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这促使我们构建了一个多模态指令微调数据集。", "metrics": {"bleu_score": 47.8219647449668, "chrf_score": 39.87088787580623, "xcomet_score": 0.9769026041030884, "xcomet_qe_score": 0.9700860977172852, "metricx_score": 1.3343170881271362, "metricx_qe_score": 1.1246306896209717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们介绍了 Multi-Instruct,这是第一个多模态指令微调基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个类别。", "metrics": {"bleu_score": 43.129247075928184, "chrf_score": 43.01779489615414, "xcomet_score": 0.8355329036712646, "xcomet_qe_score": 0.8296025991439819, "metricx_score": 1.3014122247695923, "metricx_qe_score": 1.5790985822677612, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务源自 21 个现有的开源数据集,每个任务都配有 5 条专家撰写的指导说明。", "metrics": {"bleu_score": 55.58391766749285, "chrf_score": 55.351593883331, "xcomet_score": 0.9914603233337402, "xcomet_qe_score": 0.9917482137680054, "metricx_score": 0.8024083375930786, "metricx_qe_score": 0.7670803666114807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究我们提出的数据集上的多模态指令微调,我们以统一的多模态预训练模型OFA作为我们的基础模型。", "metrics": {"bleu_score": 62.94512470513713, "chrf_score": 58.718822051098016, "xcomet_score": 0.9191347360610962, "xcomet_qe_score": 0.7513023614883423, "metricx_score": 1.4971463680267334, "metricx_qe_score": 1.6418675184249878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFA为语言、图像标记和边界框的坐标使用统一的词汇表。", "metrics": {"bleu_score": 69.19907115708301, "chrf_score": 62.889361992482684, "xcomet_score": 0.9123020172119141, "xcomet_qe_score": 0.8623214960098267, "metricx_score": 2.4415395259857178, "metricx_qe_score": 2.701528310775757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示了来自我们的 Multi-Instra 数据集的一些示例实例。 为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 69.53175390558687, "chrf_score": 60.118140512525805, "xcomet_score": 0.847628116607666, "xcomet_qe_score": 0.7529617547988892, "metricx_score": 3.481840133666992, "metricx_qe_score": 4.898205757141113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,将所有任务统一编排为序列到序列格式,其中", "metrics": {"bleu_score": 53.41701392245994, "chrf_score": 50.1109503666237, "xcomet_score": 0.7547742128372192, "xcomet_qe_score": 0.7500269412994385, "metricx_score": 3.623250961303711, "metricx_qe_score": 2.683480978012085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框以相同的标记空间表示。", "metrics": {"bleu_score": 53.09565039223721, "chrf_score": 50.52264517853785, "xcomet_score": 0.9847351312637329, "xcomet_qe_score": 0.9566234350204468, "metricx_score": 1.0149985551834106, "metricx_qe_score": 1.017142415046692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我要谈谈多模态指令调优。 因此", "metrics": {"bleu_score": 64.59962562244407, "chrf_score": 67.16344466083987, "xcomet_score": 0.7737843990325928, "xcomet_qe_score": 0.7165536880493164, "metricx_score": 3.1175618171691895, "metricx_qe_score": 0.7612370848655701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于训练数据集,我们使用了来自 9 个群组的 53 个任务进行训练,每个任务抽取 10,000 个样本。", "metrics": {"bleu_score": 57.473860166742696, "chrf_score": 55.321004123044716, "xcomet_score": 0.9261754751205444, "xcomet_qe_score": 0.9275879263877869, "metricx_score": 1.9609613418579102, "metricx_qe_score": 3.001188039779663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留了整个常识推理群组进行测试,并从 VQA 和杂项群组中额外选择了 5 个任务。", "metrics": {"bleu_score": 47.89115646239776, "chrf_score": 43.79506677963948, "xcomet_score": 0.6200060844421387, "xcomet_qe_score": 0.6029764413833618, "metricx_score": 3.936042070388794, "metricx_qe_score": 4.208277225494385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用测试集中的所有实例进行每个任务的测试。", "metrics": {"bleu_score": 51.94247346787362, "chrf_score": 42.91118133156373, "xcomet_score": 0.8430489301681519, "xcomet_qe_score": 0.8401150107383728, "metricx_score": 0.9858503341674805, "metricx_qe_score": 1.1448758840560913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令测试集中的任务中随机抽取 20 个任务作为 NLP 的未见任务。 因此,", "metrics": {"bleu_score": 51.27923178724249, "chrf_score": 49.84005964329255, "xcomet_score": 0.5373571515083313, "xcomet_qe_score": 0.5202940106391907, "metricx_score": 5.129362106323242, "metricx_qe_score": 4.693006992340088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的大型OFA模型作为基础模型。", "metrics": {"bleu_score": 79.2036595215674, "chrf_score": 67.46424885743771, "xcomet_score": 0.9633874893188477, "xcomet_qe_score": 0.9624887108802795, "metricx_score": 1.3794260025024414, "metricx_qe_score": 3.086843729019165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 53.884478712336644, "xcomet_score": 0.969638466835022, "xcomet_qe_score": 0.8911672830581665, "metricx_score": 0.8351970911026001, "metricx_qe_score": 1.3651412725448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例都随机与五个指令模板中的一个结合。", "metrics": {"bleu_score": 74.17090125042293, "chrf_score": 70.16064397122712, "xcomet_score": 0.8919111490249634, "xcomet_qe_score": 0.7913199663162231, "metricx_score": 1.59616219997406, "metricx_qe_score": 1.8537696599960327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在测试过程中,对于每个任务,我们总共进行五次实验,每次实验中使用五种", "metrics": {"bleu_score": 23.737074648286953, "chrf_score": 21.191180470128014, "xcomet_score": 0.6679056882858276, "xcomet_qe_score": 0.7693372368812561, "metricx_score": 6.811810493469238, "metricx_qe_score": 4.791146278381348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "指令中的其中一种来评估模型。 我们报告了所有五个实验的平均性能、最大性能和性能标准差。", "metrics": {"bleu_score": 16.307934703092837, "chrf_score": 16.828280718380647, "xcomet_score": 0.2115684449672699, "xcomet_qe_score": 0.17177005112171173, "metricx_score": 4.647770404815674, "metricx_qe_score": 5.045454978942871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模型分类任务,我们将报告准确率。", "metrics": {"bleu_score": 78.39204411491599, "chrf_score": 71.55580201245836, "xcomet_score": 0.9935622215270996, "xcomet_qe_score": 0.9808018803596497, "metricx_score": 0.5727890133857727, "metricx_qe_score": 0.702531635761261, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模型生成任务,我们将报告 ROUGE-L。对于 NLP 任务,我们也将报告 ROUGE-L。", "metrics": {"bleu_score": 41.45981061255026, "chrf_score": 37.69902131824431, "xcomet_score": 0.8467002511024475, "xcomet_qe_score": 0.8062145709991455, "metricx_score": 2.042813777923584, "metricx_qe_score": 2.4781041145324707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为敏感性。因此,它", "metrics": {"bleu_score": 59.330670450907064, "chrf_score": 60.25879085514624, "xcomet_score": 0.6481771469116211, "xcomet_qe_score": 0.6395895481109619, "metricx_score": 5.248791217803955, "metricx_qe_score": 3.9234323501586914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "衡量的是模型在执行同一任务时,无论指令措辞有何微小变化,都能始终产生相同输出的能力。", "metrics": {"bleu_score": 31.44074951099907, "chrf_score": 29.507540420735673, "xcomet_score": 0.9822375774383545, "xcomet_qe_score": 0.9718543291091919, "metricx_score": 2.4364867210388184, "metricx_qe_score": 3.3011958599090576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要研究结果。", "metrics": {"bleu_score": 67.03420896351791, "chrf_score": 69.67750697057613, "xcomet_score": 0.9950377941131592, "xcomet_qe_score": 0.9910168647766113, "metricx_score": 0.2826874852180481, "metricx_qe_score": 0.38960468769073486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,指令微调可以显著提高OFA在场景多模型任务上的性能。", "metrics": {"bleu_score": 30.002162025458677, "chrf_score": 32.3704458590678, "xcomet_score": 0.8945895433425903, "xcomet_qe_score": 0.8831561803817749, "metricx_score": 2.6153106689453125, "metricx_qe_score": 2.7618494033813477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习也有助于指令调优。", "metrics": {"bleu_score": 62.24844091190641, "chrf_score": 56.66622049813117, "xcomet_score": 0.9788691997528076, "xcomet_qe_score": 0.7813156843185425, "metricx_score": 1.3328802585601807, "metricx_qe_score": 2.2005465030670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,随着任务量的增加,模型的性能得到提升,同时敏感度降低。 因此,", "metrics": {"bleu_score": 32.29050386979324, "chrf_score": 26.372844300929387, "xcomet_score": 0.7740510106086731, "xcomet_qe_score": 0.7920076251029968, "metricx_score": 4.026945114135742, "metricx_qe_score": 1.8404620885849, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还做了一个实验,", "metrics": {"bleu_score": 88.01117367933934, "chrf_score": 85.90608465608466, "xcomet_score": 0.9896172285079956, "xcomet_qe_score": 0.9624312520027161, "metricx_score": 0.23607137799263, "metricx_qe_score": 0.20442509651184082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了1条指令与5条指令进行", "metrics": {"bleu_score": 28.295596283263514, "chrf_score": 25.59758563511636, "xcomet_score": 0.7516186237335205, "xcomet_qe_score": 0.7295094132423401, "metricx_score": 3.110644578933716, "metricx_qe_score": 2.6308059692382812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比较。我们可以看到,使用更多的指令可以提高模型的整体性能,并大大降低其敏感性。", "metrics": {"bleu_score": 48.05836435240956, "chrf_score": 45.17211313675948, "xcomet_score": 0.8064553737640381, "xcomet_qe_score": 0.8111329674720764, "metricx_score": 2.0758750438690186, "metricx_qe_score": 2.290992498397827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明了不同的微调策略对模型敏感度的影响。", "metrics": {"bleu_score": 67.73709971213142, "chrf_score": 59.831093256015855, "xcomet_score": 0.9938645362854004, "xcomet_qe_score": 0.9962800741195679, "metricx_score": 0.9285706281661987, "metricx_qe_score": 1.128389835357666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,通过从自然指令数据集进行迁移学习,模型的敏感度比原始的OFA模型要高得多。", "metrics": {"bleu_score": 27.274946485561962, "chrf_score": 27.519872536608137, "xcomet_score": 0.9703832864761353, "xcomet_qe_score": 0.8577477931976318, "metricx_score": 1.8811500072479248, "metricx_qe_score": 2.484564781188965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从 Nitro 指令数据集进行迁移学习可以帮助 OFA 在 Nitro 指令数据集上取得更好的性能。", "metrics": {"bleu_score": 54.15586848905436, "chrf_score": 46.7256579257955, "xcomet_score": 0.704578161239624, "xcomet_qe_score": 0.6305123567581177, "metricx_score": 5.783699989318848, "metricx_qe_score": 5.849686145782471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们提出了第一个大规模多模型指令微调数据集。我们显著提高了OFV的零样本能力,并探索了不同的迁移学习技术,并展示了它们的优势。", "metrics": {"bleu_score": 57.55449763996406, "chrf_score": 53.04474490889365, "xcomet_score": 0.7496988773345947, "xcomet_qe_score": 0.727524995803833, "metricx_score": 3.045466661453247, "metricx_qe_score": 3.240147590637207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个名为敏感性的新指标。 还有一点", "metrics": {"bleu_score": 34.17788378592367, "chrf_score": 31.36179566310729, "xcomet_score": 0.43319129943847656, "xcomet_qe_score": 0.4075433909893036, "metricx_score": 4.363913059234619, "metricx_qe_score": 1.6609537601470947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们正在收集一个更大的多模态指令微调数据集,包含大约 150 个额外的变体语言任务,我们会发布这些数据集。", "metrics": {"bleu_score": 57.77750843430604, "chrf_score": 53.25774140501236, "xcomet_score": 0.6969653367996216, "xcomet_qe_score": 0.7337858080863953, "metricx_score": 4.0777788162231445, "metricx_qe_score": 4.4619832038879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯塔夫·辛哈,很高兴欢迎大家来到我们的ACL 2023论文讨论会,我们的论文题目是", "metrics": {"bleu_score": 25.55886019493071, "chrf_score": 32.001218603156154, "xcomet_score": 0.7501645684242249, "xcomet_qe_score": 0.6461473703384399, "metricx_score": 3.5165748596191406, "metricx_qe_score": 3.5998597145080566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型的可接受性判断并非总是对上下文鲁棒》。", "metrics": {"bleu_score": 59.03555395703033, "chrf_score": 54.27521889845085, "xcomet_score": 0.7580986022949219, "xcomet_qe_score": 0.7236077189445496, "metricx_score": 4.7413716316223145, "metricx_qe_score": 5.655364990234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与 John Gauthier、Aaron Muller、Kanishka Mishra、Karen Fuentes、Roger Levy 和 Adina Williams 的合作作品。", "metrics": {"bleu_score": 35.99429423708424, "chrf_score": 75.90557886445933, "xcomet_score": 0.9228802919387817, "xcomet_qe_score": 0.9522498846054077, "metricx_score": 1.5556421279907227, "metricx_qe_score": 1.5107327699661255, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们重新审视了最小对范式。", "metrics": {"bleu_score": 45.663378549673105, "chrf_score": 46.65455611899317, "xcomet_score": 0.9555627107620239, "xcomet_qe_score": 0.9084150791168213, "metricx_score": 1.5144939422607422, "metricx_qe_score": 1.5100255012512207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最小配对范式基本上是在可接受性判断的基础上对语言模型进行评估,", "metrics": {"bleu_score": 60.710221238920745, "chrf_score": 65.41389883974479, "xcomet_score": 0.9637207984924316, "xcomet_qe_score": 0.9689056873321533, "metricx_score": 0.8912508487701416, "metricx_qe_score": 1.0220611095428467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中还包括语法性(如 blimp、语法、gem),或从刻板印象角度来看的可接受性,例如交叉对。", "metrics": {"bleu_score": 20.350599996262506, "chrf_score": 14.250991234960148, "xcomet_score": 0.5380501747131348, "xcomet_qe_score": 0.4188598394393921, "metricx_score": 6.375661849975586, "metricx_qe_score": 6.687607765197754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个最小对范式中,评估语言模型的典型方法是,先展示一个可接受的句子或一个语法正确的句子,然后展示一个可接受的句子或一个语法错误的句子。", "metrics": {"bleu_score": 61.0933393397613, "chrf_score": 55.06977989055342, "xcomet_score": 0.6962326765060425, "xcomet_qe_score": 0.5192391872406006, "metricx_score": 2.0777382850646973, "metricx_qe_score": 2.802708387374878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型基本上会为可接受的句子赋予更高的概率。", "metrics": {"bleu_score": 37.260668086126365, "chrf_score": 31.90586568812871, "xcomet_score": 0.8829679489135742, "xcomet_qe_score": 0.7692053318023682, "metricx_score": 1.322019100189209, "metricx_qe_score": 2.0490128993988037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP流水线基本上不允许我们评估模型对更长句子的接受程度。", "metrics": {"bleu_score": 66.01973900178668, "chrf_score": 63.49269991100685, "xcomet_score": 0.9353364706039429, "xcomet_qe_score": 0.8717740774154663, "metricx_score": 3.2692508697509766, "metricx_qe_score": 3.5705642700195312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型的上下文窗口越来越长。", "metrics": {"bleu_score": 77.5792961097714, "chrf_score": 78.10281161553993, "xcomet_score": 0.9714252948760986, "xcomet_qe_score": 0.9141079783439636, "metricx_score": 0.5378822684288025, "metricx_qe_score": 0.6961395740509033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们在整个上下文窗口中评估模型的可接受性至关重要。 这就是我们在这里试图做的事情。", "metrics": {"bleu_score": 58.505032071185845, "chrf_score": 54.15887751534336, "xcomet_score": 0.8967925310134888, "xcomet_qe_score": 0.8656550049781799, "metricx_score": 0.912758469581604, "metricx_qe_score": 1.225801944732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过要求模型对越来越长的序列进行可接受性评估,来重新审视MPP流水线。", "metrics": {"bleu_score": 50.17790358114832, "chrf_score": 48.399649477960466, "xcomet_score": 0.9268544316291809, "xcomet_qe_score": 0.9255828857421875, "metricx_score": 3.885606288909912, "metricx_qe_score": 3.853261947631836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9963313341140747, "xcomet_qe_score": 0.9761532545089722, "metricx_score": 0.22746601700782776, "metricx_qe_score": 0.7089755535125732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们要做的就是模拟这些更长的序列。我们重新审视数据集本身,然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 71.64849993972881, "chrf_score": 73.36116002961892, "xcomet_score": 0.9147652983665466, "xcomet_qe_score": 0.6513663530349731, "metricx_score": 1.740312099456787, "metricx_qe_score": 2.7573976516723633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从附属岛案例的气球数据集里选取了一对典型的语法对。", "metrics": {"bleu_score": 34.95193584636775, "chrf_score": 22.29512919258005, "xcomet_score": 0.6357919573783875, "xcomet_qe_score": 0.6862609386444092, "metricx_score": 7.407872676849365, "metricx_qe_score": 7.224074840545654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是,重新创建更长的序列,并确定哪些序列是可以接受的,哪些序列具有相同的语法结构匹配,为此,", "metrics": {"bleu_score": 52.05114762460618, "chrf_score": 52.15017232841742, "xcomet_score": 0.7483477592468262, "xcomet_qe_score": 0.6294374465942383, "metricx_score": 4.424788475036621, "metricx_qe_score": 5.017534255981445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从阿根廷岛提取语法正确的句子 然后,我们将它作为前缀添加到可接受的查询和不可接受的查询中。 因此,", "metrics": {"bleu_score": 56.31847584489392, "chrf_score": 50.230731775585625, "xcomet_score": 0.4664100706577301, "xcomet_qe_score": 0.44087377190589905, "metricx_score": 7.698559761047363, "metricx_qe_score": 8.589188575744629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从同一匹配中选择不可接受的句子来做同样的事情。这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 84.49687612847214, "chrf_score": 79.69691466828877, "xcomet_score": 0.8999904990196228, "xcomet_qe_score": 0.7822897434234619, "metricx_score": 1.1589840650558472, "metricx_qe_score": 1.7412755489349365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过从不同的子集或不同的数据集选择句子来做到这一点。", "metrics": {"bleu_score": 73.48998814268005, "chrf_score": 69.85725171521311, "xcomet_score": 0.9821317195892334, "xcomet_qe_score": 0.896470844745636, "metricx_score": 0.7030987739562988, "metricx_qe_score": 1.2432994842529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的不匹配情况。 所以,", "metrics": {"bleu_score": 62.36362995619313, "chrf_score": 70.78847125419045, "xcomet_score": 0.8700540661811829, "xcomet_qe_score": 0.8271939754486084, "metricx_score": 3.081840753555298, "metricx_qe_score": 1.206554889678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关的数据集,但不是你正在评估的数据集。", "metrics": {"bleu_score": 55.63773320143581, "chrf_score": 49.18241284103011, "xcomet_score": 0.9709093570709229, "xcomet_qe_score": 0.8447773456573486, "metricx_score": 1.1560019254684448, "metricx_qe_score": 2.0661189556121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于不可接受的情况,我们也可以这样做。", "metrics": {"bleu_score": 46.663612512230074, "chrf_score": 42.696972610224044, "xcomet_score": 0.9821330308914185, "xcomet_qe_score": 0.9639067649841309, "metricx_score": 0.5520765781402588, "metricx_qe_score": 0.599204957485199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全不相关的领域(如维基百科)中选择句子。 因此,", "metrics": {"bleu_score": 55.39841452101014, "chrf_score": 51.24021191793264, "xcomet_score": 0.7624471783638, "xcomet_qe_score": 0.803749680519104, "metricx_score": 3.2552132606506348, "metricx_qe_score": 3.3805925846099854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们模型的可接受性判断是否真的受到任何上下文的影响。 例如,上下文是否来自数据集的不同子集,或者是否与当前的句子完全无关。", "metrics": {"bleu_score": 58.96410096977998, "chrf_score": 53.792496122193626, "xcomet_score": 0.9478973150253296, "xcomet_qe_score": 0.9156302213668823, "metricx_score": 1.5796329975128174, "metricx_qe_score": 2.584481716156006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型的表现如何呢?", "metrics": {"bleu_score": 9.78090152232118, "chrf_score": 10.810141839392179, "xcomet_score": 0.8777414560317993, "xcomet_qe_score": 0.858100175857544, "metricx_score": 1.0029296875, "metricx_qe_score": 0.2817142903804779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们看一下维基百科句子,它们与当前查询对完全无关。我们发现,MPP 判断对于任意上下文长度来说大多是稳健的。", "metrics": {"bleu_score": 58.068211818184324, "chrf_score": 50.47491051965671, "xcomet_score": 0.9470763206481934, "xcomet_qe_score": 0.7958595752716064, "metricx_score": 3.6568799018859863, "metricx_qe_score": 5.353399276733398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到 1024,以最大化 OPT 和 GPT-2 模型的性能。我们在这里看到,", "metrics": {"bleu_score": 48.180160476053366, "chrf_score": 71.11557488413716, "xcomet_score": 0.7354938983917236, "xcomet_qe_score": 0.600774884223938, "metricx_score": 4.005816459655762, "metricx_qe_score": 3.6161341667175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用橙色虚线表示的 MPP 判断相对稳定。", "metrics": {"bleu_score": 53.1799779830062, "chrf_score": 52.278460014866624, "xcomet_score": 0.9308826923370361, "xcomet_qe_score": 0.8689777851104736, "metricx_score": 2.3573989868164062, "metricx_qe_score": 4.734353065490723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,当我们从同一数据集选择句子时会发生什么?", "metrics": {"bleu_score": 52.10220626528036, "chrf_score": 45.47347262225907, "xcomet_score": 0.9926936626434326, "xcomet_qe_score": 0.936105489730835, "metricx_score": 0.63746178150177, "metricx_qe_score": 1.1434544324874878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们从同一个气球或语法宝石数据集里选择或创建可接受和不可接受的句子域。", "metrics": {"bleu_score": 39.712937582356574, "chrf_score": 26.768340356419408, "xcomet_score": 0.5350489616394043, "xcomet_qe_score": 0.5001213550567627, "metricx_score": 6.760974407196045, "metricx_qe_score": 6.34912109375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们发现,当添加可接受的前缀或不可接受的前缀时,MPP 判断结果会显著增加或减少。", "metrics": {"bleu_score": 62.02664154735994, "chrf_score": 58.266677959044955, "xcomet_score": 0.8851144909858704, "xcomet_qe_score": 0.8706207275390625, "metricx_score": 2.0381131172180176, "metricx_qe_score": 1.5524520874023438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,也就是当我们在指责人的文本中选择与同一现象相关的句子时,吉姆, 根据所选前缀是可接受的还是不可接受的,我们看到模型的 MPP 判断值出现了大幅增加或大幅减少。 现在,这个效果非常大,随着", "metrics": {"bleu_score": 42.28565809600468, "chrf_score": 40.92943862920548, "xcomet_score": 0.18705600500106812, "xcomet_qe_score": 0.1066116914153099, "metricx_score": 10.575700759887695, "metricx_qe_score": 9.925575256347656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文长度的增加,这个效果也会增加。这可能会影响到那些拥有大上下文窗口的新语言模型。", "metrics": {"bleu_score": 52.94844289676491, "chrf_score": 50.84057725339575, "xcomet_score": 0.9374443292617798, "xcomet_qe_score": 0.8367275595664978, "metricx_score": 2.032853126525879, "metricx_qe_score": 2.194676160812378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,匹配前缀为什么对语言模型的判断影响如此之大呢? 因此,", "metrics": {"bleu_score": 42.51869518183949, "chrf_score": 36.298671634878524, "xcomet_score": 0.8389618992805481, "xcomet_qe_score": 0.7731384038925171, "metricx_score": 3.917841911315918, "metricx_qe_score": 3.994147300720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,尝试在保留相关结构的同时添加噪声,以构建输入句子。", "metrics": {"bleu_score": 43.26948127630142, "chrf_score": 37.74016868777386, "xcomet_score": 0.8440394997596741, "xcomet_qe_score": 0.7932945489883423, "metricx_score": 5.768608093261719, "metricx_qe_score": 5.585909366607666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "经过多次这种扰动后, 我们发现,这些噪音实际上并没有使模型改变其显示MPP判断趋势的方式。", "metrics": {"bleu_score": 30.327932330804245, "chrf_score": 28.73221469922626, "xcomet_score": 0.9002184867858887, "xcomet_qe_score": 0.878628671169281, "metricx_score": 3.5782787799835205, "metricx_qe_score": 4.08923864364624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型以相似的方式对干扰和句子敏感。", "metrics": {"bleu_score": 19.06078747933291, "chrf_score": 20.56274284848369, "xcomet_score": 0.8297110199928284, "xcomet_qe_score": 0.8296223878860474, "metricx_score": 3.0461020469665527, "metricx_qe_score": 3.2864437103271484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们在可接受范围内扰动句子时,我们观察到所有扰动都有类似的增加。而在不可接受范围内扰动句子时,我们以类似的方式观察到MPP判断的减少。", "metrics": {"bleu_score": 47.51561690252786, "chrf_score": 41.84438616042763, "xcomet_score": 0.7539424896240234, "xcomet_qe_score": 0.6928157806396484, "metricx_score": 4.0469818115234375, "metricx_qe_score": 4.614380836486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键结论是,语言模型对句子间共享的潜在句法和语义特征很敏感。", "metrics": {"bleu_score": 77.42072321622986, "chrf_score": 72.77672650737665, "xcomet_score": 0.9851152896881104, "xcomet_qe_score": 0.9956561326980591, "metricx_score": 1.033024787902832, "metricx_qe_score": 1.197113275527954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而目前我们对 MPP 的评估方式,即使用短句和单句输入,可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 48.90727462382201, "chrf_score": 42.068574742178896, "xcomet_score": 0.9748961925506592, "xcomet_qe_score": 0.8936968445777893, "metricx_score": 1.1310193538665771, "metricx_qe_score": 1.8927407264709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文,以获取我们实验的更多详细信息。", "metrics": {"bleu_score": 71.92779767585063, "chrf_score": 66.47819126029695, "xcomet_score": 0.9983570575714111, "xcomet_qe_score": 1.0, "metricx_score": 0.2198541909456253, "metricx_qe_score": 0.2157556414604187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张宇胜。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 49.88704001434205, "xcomet_score": 0.9199157953262329, "xcomet_qe_score": 0.8629245758056641, "metricx_score": 0.5634078979492188, "metricx_qe_score": 0.9624079465866089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天,我将介绍我们的研究成果——跨语言语义解析在多种自然语言和最小表示中的应用。 因此,语义解析的任务", "metrics": {"bleu_score": 36.74843411383715, "chrf_score": 35.84265581685331, "xcomet_score": 0.5254992842674255, "xcomet_qe_score": 0.6052532196044922, "metricx_score": 10.556674003601074, "metricx_qe_score": 5.441342353820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是构建用户查询(如 SQL 和 λ 演算)的语义表示。", "metrics": {"bleu_score": 40.85088315181786, "chrf_score": 27.631952118481518, "xcomet_score": 0.4680079221725464, "xcomet_qe_score": 0.46620604395866394, "metricx_score": 5.972935676574707, "metricx_qe_score": 5.349033355712891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义解析的任务是将多种自然语言中的查询翻译成多种意义表示。 吴浩庭,博士:", "metrics": {"bleu_score": 60.686803479902146, "chrf_score": 64.3359724810062, "xcomet_score": 0.6212870478630066, "xcomet_qe_score": 0.4766004979610443, "metricx_score": 5.119599342346191, "metricx_qe_score": 5.845578670501709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将查询翻译成多种自然语言,以实现lambda或fun QL等功能。", "metrics": {"bleu_score": 53.42163771035709, "chrf_score": 50.26681900555893, "xcomet_score": 0.8803514242172241, "xcomet_qe_score": 0.82230544090271, "metricx_score": 3.3053414821624756, "metricx_qe_score": 4.00395393371582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型是针对有限的任务和应用数据集分别提出的和评估的。", "metrics": {"bleu_score": 67.7028631079626, "chrf_score": 59.281388589974924, "xcomet_score": 0.9974393844604492, "xcomet_qe_score": 0.9922376871109009, "metricx_score": 0.4794595241546631, "metricx_qe_score": 0.646976113319397, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如, 某些自然语言的覆盖范围存在漏洞。", "metrics": {"bleu_score": 46.99152171992906, "chrf_score": 49.37297489707343, "xcomet_score": 0.7171068787574768, "xcomet_qe_score": 0.5239362716674805, "metricx_score": 3.999760389328003, "metricx_qe_score": 4.580524921417236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失 由于某些微型代表的覆盖。", "metrics": {"bleu_score": 9.79235248982473, "chrf_score": 11.465274848655273, "xcomet_score": 0.5136600732803345, "xcomet_qe_score": 0.3727136254310608, "metricx_score": 7.808865547180176, "metricx_qe_score": 7.550037860870361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "缺少了λ演算。 或者它们只在某些神经网络模型上进行评估。", "metrics": {"bleu_score": 43.2530772707211, "chrf_score": 36.20147123407992, "xcomet_score": 0.8837108612060547, "xcomet_qe_score": 0.838358998298645, "metricx_score": 1.3198843002319336, "metricx_qe_score": 2.054525852203369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个模型来评估它们。", "metrics": {"bleu_score": 56.42812502283149, "chrf_score": 49.202863729636285, "xcomet_score": 0.9981586933135986, "xcomet_qe_score": 0.9880315065383911, "metricx_score": 0.5080630779266357, "metricx_qe_score": 0.8839173316955566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了范例。", "metrics": {"bleu_score": 41.72261448611506, "chrf_score": 22.986984237361657, "xcomet_score": 0.7886703014373779, "xcomet_qe_score": 0.7126659154891968, "metricx_score": 2.495030641555786, "metricx_qe_score": 4.042074680328369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们为多语言语义解析和多种意义表示的交叉链接提供统一的数据集范例。", "metrics": {"bleu_score": 33.22734135112283, "chrf_score": 25.211996513700235, "xcomet_score": 0.7404383420944214, "xcomet_qe_score": 0.7522462606430054, "metricx_score": 2.58976149559021, "metricx_qe_score": 2.8753864765167236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含了九个不同领域的语料集,五个语义解析任务,八种语义表示,以及15个语系中的22种自然语言。", "metrics": {"bleu_score": 45.90283814117439, "chrf_score": 45.47838319133157, "xcomet_score": 0.8702465295791626, "xcomet_qe_score": 0.9194350242614746, "metricx_score": 0.8512062430381775, "metricx_qe_score": 1.3560761213302612, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了训练和评估的六种设置。", "metrics": {"bleu_score": 67.8301759715223, "chrf_score": 59.56205276123286, "xcomet_score": 0.9824798107147217, "xcomet_qe_score": 0.9433248043060303, "metricx_score": 1.0820890665054321, "metricx_qe_score": 2.2754859924316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是翻译测试。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9898306131362915, "xcomet_qe_score": 0.9781848192214966, "metricx_score": 0.22308200597763062, "metricx_qe_score": 0.39765846729278564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用谷歌翻译API将源语言翻译成目标语言,然后使用单语模型进行评估训练。", "metrics": {"bleu_score": 80.2488626708244, "chrf_score": 77.13066111972654, "xcomet_score": 0.952629029750824, "xcomet_qe_score": 0.8100776672363281, "metricx_score": 1.0134613513946533, "metricx_qe_score": 1.0162348747253418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们用英语查询对英语模型进行训练,在推理过程中,我们使用 API 将德语查询翻译成英语,然后使用训练好的模型来预测 SQL。", "metrics": {"bleu_score": 68.95941859640645, "chrf_score": 64.57054901626023, "xcomet_score": 0.9540742635726929, "xcomet_qe_score": 0.9100123643875122, "metricx_score": 1.1785595417022705, "metricx_qe_score": 2.4346630573272705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模块。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 76.96368446368447, "xcomet_score": 0.8608987927436829, "xcomet_qe_score": 0.837360680103302, "metricx_score": 0.2960849404335022, "metricx_qe_score": 0.4536767601966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如,德语对德语或英语对英语。", "metrics": {"bleu_score": 49.25399772382246, "chrf_score": 43.11937139278397, "xcomet_score": 0.9478171467781067, "xcomet_qe_score": 0.9762659668922424, "metricx_score": 1.3385028839111328, "metricx_qe_score": 0.9785447120666504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过仅使用10%的训练数据对单语模型进行训练来测试单语领域镜头设置。", "metrics": {"bleu_score": 42.62221594184117, "chrf_score": 37.8034614191629, "xcomet_score": 0.7347003221511841, "xcomet_qe_score": 0.6786996126174927, "metricx_score": 5.049134254455566, "metricx_qe_score": 3.1411497592926025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试多语言模型,我们为所有语言训练一个多语言模型。", "metrics": {"bleu_score": 81.96189957582149, "chrf_score": 82.31272076344145, "xcomet_score": 0.9485858678817749, "xcomet_qe_score": 0.8932175636291504, "metricx_score": 1.2320210933685303, "metricx_qe_score": 1.7469444274902344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语、中文查询放在一起训练一个多语言模型。", "metrics": {"bleu_score": 83.43396336944906, "chrf_score": 78.93203219586009, "xcomet_score": 0.9516309499740601, "xcomet_qe_score": 0.9600837230682373, "metricx_score": 1.4132899045944214, "metricx_qe_score": 2.9107823371887207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理过程中,我们可以使用这个模型 翻译德语查询或中文查询或其他。", "metrics": {"bleu_score": 73.6558994084271, "chrf_score": 67.65170475857651, "xcomet_score": 0.9241743087768555, "xcomet_qe_score": 0.8078778982162476, "metricx_score": 1.346144676208496, "metricx_qe_score": 2.5633628368377686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一个源语言上进行训练,然后迁移到另一种语言。 因此,", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 35.759756944180864, "xcomet_score": 0.7896835803985596, "xcomet_qe_score": 0.7608597278594971, "metricx_score": 5.239978790283203, "metricx_qe_score": 5.619853496551514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们使用英语查询或英语和德语的组合进行少样本查询训练,以训练多语言模型并预测 SQL 输出。", "metrics": {"bleu_score": 66.9743321808567, "chrf_score": 63.6933859213546, "xcomet_score": 0.970320463180542, "xcomet_qe_score": 0.874907374382019, "metricx_score": 1.1263716220855713, "metricx_qe_score": 1.6199588775634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的成果。因此", "metrics": {"bleu_score": 26.96698152099015, "chrf_score": 26.932617513716504, "xcomet_score": 0.8180636763572693, "xcomet_qe_score": 0.7937408685684204, "metricx_score": 3.2309324741363525, "metricx_qe_score": 1.9735608100891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",关于单语模型的分析,我们对两组模型进行了评估。 包括编码器PDR,即基于指针的解码器的多语言预训练编码器,如XLMR plus PDR和BERT plus PDR。", "metrics": {"bleu_score": 49.43682056963306, "chrf_score": 39.63074630219113, "xcomet_score": 0.6252691745758057, "xcomet_qe_score": 0.5509542226791382, "metricx_score": 5.212683200836182, "metricx_qe_score": 4.447027683258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,如 mBART 和 MT5。", "metrics": {"bleu_score": 33.4012855287867, "chrf_score": 25.41103531628568, "xcomet_score": 0.9281011819839478, "xcomet_qe_score": 0.9658725261688232, "metricx_score": 1.3702642917633057, "metricx_qe_score": 2.529313564300537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器-解码器在所有九个数据集上均获得最佳性能。", "metrics": {"bleu_score": 40.10198002912937, "chrf_score": 27.5286160729574, "xcomet_score": 0.9877752065658569, "xcomet_qe_score": 0.9819492101669312, "metricx_score": 1.4411680698394775, "metricx_qe_score": 1.2320201396942139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对 MT5 和 XLMR 以及 PDR 多语言设置进行了评估。", "metrics": {"bleu_score": 15.77454598068419, "chrf_score": 26.85487388574359, "xcomet_score": 0.8511114120483398, "xcomet_qe_score": 0.8486894965171814, "metricx_score": 3.330181837081909, "metricx_qe_score": 3.1846730709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合环境中进行训练,可以改进编码器-解码器或编码器-PDR。", "metrics": {"bleu_score": 22.610908143727077, "chrf_score": 17.76851775760591, "xcomet_score": 0.8177750110626221, "xcomet_qe_score": 0.8606229424476624, "metricx_score": 2.0803489685058594, "metricx_qe_score": 3.2604682445526123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都能获得性能提升,但英语在七个数据集中的性能下降,只有三个数据集有所提升。", "metrics": {"bleu_score": 55.16271409905199, "chrf_score": 48.60483975241655, "xcomet_score": 0.9308941960334778, "xcomet_qe_score": 0.9867225885391235, "metricx_score": 2.6543450355529785, "metricx_qe_score": 2.066399097442627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言的诅咒。", "metrics": {"bleu_score": 35.47202175617096, "chrf_score": 30.880643040199136, "xcomet_score": 0.9179631471633911, "xcomet_qe_score": 0.8739200234413147, "metricx_score": 1.051566481590271, "metricx_qe_score": 1.5422992706298828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此图中,蓝色线条表示跨语言少样本迁移,", "metrics": {"bleu_score": 40.4727200247809, "chrf_score": 39.309011299670985, "xcomet_score": 0.8637951016426086, "xcomet_qe_score": 0.8072527647018433, "metricx_score": 1.970313310623169, "metricx_qe_score": 3.6498348712921143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙色线条表示跨语言零样本迁移,", "metrics": {"bleu_score": 50.31747626530137, "chrf_score": 55.750492982614205, "xcomet_score": 0.9481871128082275, "xcomet_qe_score": 0.8290748000144958, "metricx_score": 1.7273415327072144, "metricx_qe_score": 2.788999080657959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿色线条表示单语设置。 我们发现", "metrics": {"bleu_score": 26.760322756637922, "chrf_score": 35.51301600167439, "xcomet_score": 0.8631134033203125, "xcomet_qe_score": 0.8361072540283203, "metricx_score": 3.623246192932129, "metricx_qe_score": 2.10001802444458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿色和橙色线条,我们发现对于零样本设置,跨语言迁移性能差距显著。通过比较蓝色和橙色线条,我们发现对于少样本设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 40.86432111314919, "chrf_score": 34.15836327979798, "xcomet_score": 0.7123500108718872, "xcomet_qe_score": 0.627238392829895, "metricx_score": 3.167076826095581, "metricx_qe_score": 3.9750871658325195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器优于之前的工作,或者取得了可比拟的结果。", "metrics": {"bleu_score": 12.89756083331201, "chrf_score": 10.69569745890861, "xcomet_score": 0.9128992557525635, "xcomet_qe_score": 0.9608187675476074, "metricx_score": 2.110644578933716, "metricx_qe_score": 2.714787006378174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用英语进行自然语言描绘可以显著提高针对目标自然语言的少样本学习性能。 我们发现,像 CODIS 和 BLUE 这样的多语言模型在跨语言语义解析任务中仍然不够完善。", "metrics": {"bleu_score": 46.47172097242237, "chrf_score": 37.16877193871606, "xcomet_score": 0.6323161721229553, "xcomet_qe_score": 0.4966605007648468, "metricx_score": 7.0949273109436035, "metricx_qe_score": 7.278142929077148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结来说,我们构建了Examplar,这是一个针对多自然语言和主要表示形式的交叉角度语义解析的统一基准。", "metrics": {"bleu_score": 42.8859179625062, "chrf_score": 32.3048572792083, "xcomet_score": 0.7002338767051697, "xcomet_qe_score": 0.7111050486564636, "metricx_score": 4.333006858825684, "metricx_qe_score": 4.6401872634887695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语言模型进行了全面的基准研究,", "metrics": {"bleu_score": 71.79226303657947, "chrf_score": 62.0439453563827, "xcomet_score": 0.9669307470321655, "xcomet_qe_score": 0.9562174081802368, "metricx_score": 1.3707668781280518, "metricx_qe_score": 1.7768745422363281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究结果显示了许多有趣的发现等", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 74.22884164335593, "xcomet_score": 0.8646327257156372, "xcomet_qe_score": 0.8622707724571228, "metricx_score": 1.6374224424362183, "metricx_qe_score": 1.183284878730774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢聆听。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9658737182617188, "xcomet_qe_score": 0.9351316690444946, "metricx_score": 0.08587995171546936, "metricx_qe_score": 0.44492465257644653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是大卫·维拉尔,我将简要介绍这篇论文《Grunting平台翻译:评估策略和性能》。", "metrics": {"bleu_score": 20.183564467352955, "chrf_score": 15.91477475966944, "xcomet_score": 0.7084683179855347, "xcomet_qe_score": 0.6723264455795288, "metricx_score": 6.303240776062012, "metricx_qe_score": 5.937719345092773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和谷歌翻译同事的合作成果。", "metrics": {"bleu_score": 20.570885115591267, "chrf_score": 20.732468773163916, "xcomet_score": 0.9982795715332031, "xcomet_qe_score": 0.9951926469802856, "metricx_score": 0.7620185017585754, "metricx_qe_score": 0.350193053483963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PARM 是一款去年 2022 年推出的拥有 5400 亿参数的大型语言模型。", "metrics": {"bleu_score": 53.37461237749346, "chrf_score": 52.739951866633724, "xcomet_score": 0.978532075881958, "xcomet_qe_score": 0.9105464220046997, "metricx_score": 3.479478359222412, "metricx_qe_score": 4.398436546325684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在包含 7800 亿份文档的庞大文本集合上进行了训练。", "metrics": {"bleu_score": 25.0737833894674, "chrf_score": 34.36552927963696, "xcomet_score": 0.8176592588424683, "xcomet_qe_score": 0.7872740030288696, "metricx_score": 1.8934448957443237, "metricx_qe_score": 2.1639015674591064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百个自然语言处理任务中达到了最先进的水平。", "metrics": {"bleu_score": 22.06023613092981, "chrf_score": 21.466400641727702, "xcomet_score": 0.9981571435928345, "xcomet_qe_score": 0.989578366279602, "metricx_score": 0.7796707153320312, "metricx_qe_score": 1.1452070474624634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们首次系统研究了针对机器翻译的大型语言模型提示。", "metrics": {"bleu_score": 33.53883156001705, "chrf_score": 28.654697655439225, "xcomet_score": 0.9076626300811768, "xcomet_qe_score": 0.917843222618103, "metricx_score": 2.0977864265441895, "metricx_qe_score": 1.960961103439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用 AMT 社区的最佳实践来评估这些模型的翻译能力。", "metrics": {"bleu_score": 50.12628906889613, "chrf_score": 44.90995969199327, "xcomet_score": 0.9239175319671631, "xcomet_qe_score": 0.7756544351577759, "metricx_score": 4.809084892272949, "metricx_qe_score": 5.9481635093688965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两种最先进的系统。因此,表现最好的系统是 WMT 评估。", "metrics": {"bleu_score": 32.32019902590771, "chrf_score": 31.746213816855185, "xcomet_score": 0.781917154788971, "xcomet_qe_score": 0.7415876388549805, "metricx_score": 4.563616752624512, "metricx_qe_score": 4.404088973999023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的神经机器翻译指标,并展示了基于专家的人工评估结果。", "metrics": {"bleu_score": 84.03034716144347, "chrf_score": 79.71051798171841, "xcomet_score": 0.9117523431777954, "xcomet_qe_score": 0.831613302230835, "metricx_score": 1.2433819770812988, "metricx_qe_score": 2.2101011276245117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们还提供了一些关于 PROM 选择策略的建议。", "metrics": {"bleu_score": 63.31602189561791, "chrf_score": 53.14699710398871, "xcomet_score": 0.8770423531532288, "xcomet_qe_score": 0.8669363260269165, "metricx_score": 4.086055755615234, "metricx_qe_score": 4.263521671295166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对 LLM 翻译性能有着重大影响。我们可以通过一个简单的实验来验证这一点,该实验使用一次性提示,并为每句话提供了两个不同的提示。", "metrics": {"bleu_score": 45.63463761336703, "chrf_score": 41.999514544632056, "xcomet_score": 0.9304256439208984, "xcomet_qe_score": 0.7523692846298218, "metricx_score": 1.8911514282226562, "metricx_qe_score": 3.546189785003662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子(1000 个中的", "metrics": {"bleu_score": 12.549310621989482, "chrf_score": 28.927733315739673, "xcomet_score": 0.6284260749816895, "xcomet_qe_score": 0.5544130802154541, "metricx_score": 10.794999122619629, "metricx_qe_score": 9.279901504516602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "516 个)观察到的差异超过一个模糊点。", "metrics": {"bleu_score": 7.439820585622744, "chrf_score": 13.422057023380676, "xcomet_score": 0.5404795408248901, "xcomet_qe_score": 0.22757279872894287, "metricx_score": 7.521876335144043, "metricx_qe_score": 10.888938903808594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这个数字甚至可以达到 40 个模糊点。", "metrics": {"bleu_score": 24.70764447832614, "chrf_score": 22.11848791940325, "xcomet_score": 0.8079386353492737, "xcomet_qe_score": 0.7423205375671387, "metricx_score": 4.577740669250488, "metricx_qe_score": 2.335872173309326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择良好的提示策略非常重要。", "metrics": {"bleu_score": 41.867037412032275, "chrf_score": 35.200073619191265, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.24972835183143616, "metricx_qe_score": 0.32759711146354675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了五次提示策略,即我们只需标明向系统提供的每句话所使用的语言。 因此,", "metrics": {"bleu_score": 35.178029201476534, "chrf_score": 29.477855733905837, "xcomet_score": 0.5688326358795166, "xcomet_qe_score": 0.6299392580986023, "metricx_score": 4.571167469024658, "metricx_qe_score": 3.515343189239502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们将德语翻译成英语,德语句子用德语冒号标注,英语翻译用英语冒号标注。", "metrics": {"bleu_score": 36.22586435379629, "chrf_score": 25.01138259479212, "xcomet_score": 0.9598349332809448, "xcomet_qe_score": 0.9602693319320679, "metricx_score": 1.3831889629364014, "metricx_qe_score": 1.3901054859161377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在几个短提示的情况下,提示的实际形式没有太大影响。", "metrics": {"bleu_score": 42.0811691266725, "chrf_score": 35.27633783250459, "xcomet_score": 0.9070167541503906, "xcomet_qe_score": 0.860403299331665, "metricx_score": 1.259537696838379, "metricx_qe_score": 1.4391005039215088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这对零提示和一次性提示至关重要。", "metrics": {"bleu_score": 19.127272198962878, "chrf_score": 18.678471483808693, "xcomet_score": 0.7986258268356323, "xcomet_qe_score": 0.827228307723999, "metricx_score": 2.220611095428467, "metricx_qe_score": 1.304482102394104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而当我们像我们这样采用五次提示时,提示的实际形式几乎没有区", "metrics": {"bleu_score": 22.419056820298167, "chrf_score": 22.516137208009145, "xcomet_score": 0.8013464212417603, "xcomet_qe_score": 0.7195558547973633, "metricx_score": 3.1644186973571777, "metricx_qe_score": 2.321235179901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别。 例子才是最重要的。", "metrics": {"bleu_score": 7.510002314354895, "chrf_score": 8.806842695659677, "xcomet_score": 0.6426215767860413, "xcomet_qe_score": 0.23790472745895386, "metricx_score": 1.081170916557312, "metricx_qe_score": 1.0022417306900024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果总结如下:示例质量比与源句的相似性更重要。", "metrics": {"bleu_score": 62.11475757601459, "chrf_score": 52.82042139288515, "xcomet_score": 0.9934189319610596, "xcomet_qe_score": 0.9833729267120361, "metricx_score": 0.6372272968292236, "metricx_qe_score": 0.6109703183174133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,从高质量的翻译中选择例子非常重要。", "metrics": {"bleu_score": 68.05854779603962, "chrf_score": 59.13663857355807, "xcomet_score": 0.9334757328033447, "xcomet_qe_score": 0.9385048151016235, "metricx_score": 0.5153730511665344, "metricx_qe_score": 0.592513918876648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了 WMT 评估的训练数据或开发数据中的选择提示。", "metrics": {"bleu_score": 36.90559374275196, "chrf_score": 34.19087271950346, "xcomet_score": 0.7528867125511169, "xcomet_qe_score": 0.5952918529510498, "metricx_score": 2.098385810852051, "metricx_qe_score": 3.040994882583618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据比训练数据更精选,质量更高,结果也更好。因此,", "metrics": {"bleu_score": 30.90015909429233, "chrf_score": 25.271428517805326, "xcomet_score": 0.6846215724945068, "xcomet_qe_score": 0.7311148643493652, "metricx_score": 5.714230537414551, "metricx_qe_score": 3.1186273097991943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据时性能更好。", "metrics": {"bleu_score": 45.43142611141303, "chrf_score": 41.14026689146475, "xcomet_score": 0.9033088684082031, "xcomet_qe_score": 0.890548825263977, "metricx_score": 1.9992756843566895, "metricx_qe_score": 2.476839542388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专业化的最先进系统在翻译质量上仍比 Palm 有显著优势。", "metrics": {"bleu_score": 31.428544048197104, "chrf_score": 25.266099695836708, "xcomet_score": 0.9215819835662842, "xcomet_qe_score": 0.865378201007843, "metricx_score": 4.180713176727295, "metricx_qe_score": 3.1893863677978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但 Palm 的翻译质量已经非常接近商业系统。", "metrics": {"bleu_score": 58.56596027429396, "chrf_score": 55.89168009708316, "xcomet_score": 0.7007780075073242, "xcomet_qe_score": 0.49537956714630127, "metricx_score": 8.284685134887695, "metricx_qe_score": 7.745477199554443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就我们而言,我们选择使用 Google 翻译进行评估。", "metrics": {"bleu_score": 48.09864429793172, "chrf_score": 39.35385605680831, "xcomet_score": 0.964268684387207, "xcomet_qe_score": 0.8872425556182861, "metricx_score": 0.6117953658103943, "metricx_qe_score": 0.753442108631134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从使用 MQM 框架进行的人类消融实验中获得的见解是,PALM 的流畅度与最先进的系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 59.43535664612471, "chrf_score": 52.69942179810844, "xcomet_score": 0.7148857116699219, "xcomet_qe_score": 0.6654854416847229, "metricx_score": 5.788421154022217, "metricx_qe_score": 6.864640712738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7579965591430664, "xcomet_qe_score": 0.7860524654388428, "metricx_score": 1.7050689458847046, "metricx_qe_score": 0.886371910572052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,Palm似乎选择通过省略翻译中源句的部分内容来制作有时听起来更好的翻译。", "metrics": {"bleu_score": 29.523438198825232, "chrf_score": 25.979233828190633, "xcomet_score": 0.834498405456543, "xcomet_qe_score": 0.7303240299224854, "metricx_score": 3.6686863899230957, "metricx_qe_score": 3.5889902114868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAN 的风格输出类别得分低于最先进的系统,这是一个额外的信号。 PARM 的输出非常流畅,但仍存在一些准确性问题。", "metrics": {"bleu_score": 45.33104044317603, "chrf_score": 38.06757724090911, "xcomet_score": 0.7045326828956604, "xcomet_qe_score": 0.6302011609077454, "metricx_score": 6.127252578735352, "metricx_qe_score": 6.5512824058532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是这次非常简短的概述。", "metrics": {"bleu_score": 10.406997559387968, "chrf_score": 14.884460660492094, "xcomet_score": 0.982917308807373, "xcomet_qe_score": 0.9095495939254761, "metricx_score": 0.44702011346817017, "metricx_qe_score": 0.724204421043396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欲了解更多详情,请参阅论文的完整内容。", "metrics": {"bleu_score": 30.491854911466543, "chrf_score": 24.606818604139107, "xcomet_score": 0.9885622262954712, "xcomet_qe_score": 0.9896273612976074, "metricx_score": 0.7244207859039307, "metricx_qe_score": 0.23767736554145813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是德国萨尔兰大学的博士生大伟。", "metrics": {"bleu_score": 53.74649369872052, "chrf_score": 43.097011943948395, "xcomet_score": 0.8909082412719727, "xcomet_qe_score": 0.9104241132736206, "metricx_score": 1.078803300857544, "metricx_qe_score": 0.9043640494346619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想介绍我们最近的工作——《比你想象的还要弱》,这是一次对每周监督学习的批判性审视。", "metrics": {"bleu_score": 45.453747711377865, "chrf_score": 41.104506514141256, "xcomet_score": 0.8294256925582886, "xcomet_qe_score": 0.7888650894165039, "metricx_score": 5.129586219787598, "metricx_qe_score": 5.502875328063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与肖宇胜、马里奥·斯穆斯巴赫、贾·斯蒂芬和DT·施劳克尔合作", "metrics": {"bleu_score": 4.175872565419194, "chrf_score": 3.3929070614897707, "xcomet_score": 0.39472392201423645, "xcomet_qe_score": 0.41950172185897827, "metricx_score": 7.102056503295898, "metricx_qe_score": 6.946372985839844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完成的联合工作。 我想从对弱监督和弱监督学习的简要介绍开始。", "metrics": {"bleu_score": 36.119525154926606, "chrf_score": 45.672629643460176, "xcomet_score": 0.16724780201911926, "xcomet_qe_score": 0.11071136593818665, "metricx_score": 6.84489107131958, "metricx_qe_score": 8.71176528930664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不手动对数据进行标注。", "metrics": {"bleu_score": 26.104909033290696, "chrf_score": 24.397511148255102, "xcomet_score": 0.9016437530517578, "xcomet_qe_score": 0.8631446361541748, "metricx_score": 0.7703352570533752, "metricx_qe_score": 1.4398902654647827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注源对数据进行标注,例如简单的启发式规则、知识库或低质量的众包,如图右侧所示。", "metrics": {"bleu_score": 59.10795379377155, "chrf_score": 56.04063587802002, "xcomet_score": 0.7400482892990112, "xcomet_qe_score": 0.7219232320785522, "metricx_score": 1.7885549068450928, "metricx_qe_score": 1.9504036903381348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注的成本要低得多,但它们也存在噪声,这意味着一定比例的标注是错误的。", "metrics": {"bleu_score": 28.53480230357357, "chrf_score": 25.931465189648566, "xcomet_score": 0.8675494194030762, "xcomet_qe_score": 0.8374210596084595, "metricx_score": 2.03668212890625, "metricx_qe_score": 2.3446497917175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在弱标签数据上直接训练神经网络,神经网络往往会记住标签噪声,而不会泛化。", "metrics": {"bleu_score": 39.156210957748684, "chrf_score": 32.743764365390874, "xcomet_score": 0.9095479249954224, "xcomet_qe_score": 0.8769531846046448, "metricx_score": 0.898951530456543, "metricx_qe_score": 1.20645272731781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出了训练算法,以便在这样的标签噪声下稳健地训练神经网络,从而使训练好的模型仍然具有良好的泛化能力。", "metrics": {"bleu_score": 70.32715783352333, "chrf_score": 60.382626133219674, "xcomet_score": 0.9770644903182983, "xcomet_qe_score": 0.8930732011795044, "metricx_score": 1.0655333995819092, "metricx_qe_score": 2.050189733505249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在WSL的近期工作中,WSL代表每周监督学习,人们普遍声称他们只在每周的标签数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 36.01753376166225, "chrf_score": 33.43111473729224, "xcomet_score": 0.6928079128265381, "xcomet_qe_score": 0.7219549417495728, "metricx_score": 8.287979125976562, "metricx_qe_score": 9.038601875305176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并没有错,但有一个问题。 人们确实假设有一个额外的干净验证集可用于模型选择。", "metrics": {"bleu_score": 71.87934550251784, "chrf_score": 65.68333349236411, "xcomet_score": 0.9828405380249023, "xcomet_qe_score": 0.96302330493927, "metricx_score": 2.0963947772979736, "metricx_qe_score": 3.4486465454101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不能就此问题设置停止,因为这意味着每周 SuperWise 学习需要额外的手动标注。", "metrics": {"bleu_score": 39.20179813560694, "chrf_score": 29.949510471741043, "xcomet_score": 0.6909367442131042, "xcomet_qe_score": 0.6919796466827393, "metricx_score": 5.633142471313477, "metricx_qe_score": 6.712368965148926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,就像房间里的大象一样,这种必要性常常被忽视。", "metrics": {"bleu_score": 78.84044396805872, "chrf_score": 72.38882614969572, "xcomet_score": 0.9236847162246704, "xcomet_qe_score": 0.8146681785583496, "metricx_score": 1.421680212020874, "metricx_qe_score": 2.7326462268829346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑问促使我们提出了三个研究问题。", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 45.31253098794524, "xcomet_score": 0.9559470415115356, "xcomet_qe_score": 0.9501094818115234, "metricx_score": 1.5945442914962769, "metricx_qe_score": 1.069020390510559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,WSL 是否需要干净的验证数据?或者我们是否可以使用噪声验证集?", "metrics": {"bleu_score": 55.603964314507635, "chrf_score": 50.91778855591179, "xcomet_score": 0.8667882680892944, "xcomet_qe_score": 0.8544470071792603, "metricx_score": 2.1947519779205322, "metricx_qe_score": 3.4921021461486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净的数据,或者干净的数据是 WSL 工作的必要条件,那么我们需要多少干净的样本?", "metrics": {"bleu_score": 51.045451261653085, "chrf_score": 42.86864108465261, "xcomet_score": 0.9893238544464111, "xcomet_qe_score": 0.9750915765762329, "metricx_score": 0.9693731069564819, "metricx_qe_score": 1.1575359106063843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该只使用干净的样本进行验证,或者还有更好的利用它们的方法?", "metrics": {"bleu_score": 53.04418155583279, "chrf_score": 45.31040038627225, "xcomet_score": 0.992186427116394, "xcomet_qe_score": 0.9232062697410583, "metricx_score": 0.7129461765289307, "metricx_qe_score": 1.0156171321868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的研究结果如下。", "metrics": {"bleu_score": 46.22377023605668, "chrf_score": 42.139554638459394, "xcomet_score": 0.9754983186721802, "xcomet_qe_score": 0.961543083190918, "metricx_score": 1.5331122875213623, "metricx_qe_score": 2.23215389251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现,有趣的是,WSL 的最新方法确实需要干净的白色盘子样本才能正常工作。", "metrics": {"bleu_score": 47.90358500146104, "chrf_score": 42.29985308803514, "xcomet_score": 0.7699609398841858, "xcomet_qe_score": 0.7783799767494202, "metricx_score": 4.296473503112793, "metricx_qe_score": 5.156342506408691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,那么训练好的模型就无法推广到原始的弱标签之外。 这意味着培训毫无意义。", "metrics": {"bleu_score": 53.89222168803019, "chrf_score": 45.00125722682338, "xcomet_score": 0.7735573649406433, "xcomet_qe_score": 0.7161304950714111, "metricx_score": 2.3175930976867676, "metricx_qe_score": 3.2023427486419678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净的标签数据才能正常工作,获取干净的验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 51.75187728209088, "chrf_score": 47.60524315980074, "xcomet_score": 0.8709638118743896, "xcomet_qe_score": 0.8544896841049194, "metricx_score": 2.532155990600586, "metricx_qe_score": 2.783010721206665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净的验证样本数量有助于WSL方法取得更好的性能,如图左所示。", "metrics": {"bleu_score": 47.08016813607621, "chrf_score": 42.30763840571915, "xcomet_score": 0.9215213656425476, "xcomet_qe_score": 0.9357224106788635, "metricx_score": 3.6619205474853516, "metricx_qe_score": 4.247870445251465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每个类别 20 个样本就能达到高端性能。", "metrics": {"bleu_score": 25.381494737245898, "chrf_score": 24.15646309408309, "xcomet_score": 0.9465594291687012, "xcomet_qe_score": 0.9424608945846558, "metricx_score": 2.099935531616211, "metricx_qe_score": 2.0957753658294678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这还不是故事的全部,因为如果我们无论如何决定使用干净的样本,那么直接在这些样本上进行训练甚至会取得更好的性能。", "metrics": {"bleu_score": 33.45316263289198, "chrf_score": 29.32131002169592, "xcomet_score": 0.9325203895568848, "xcomet_qe_score": 0.8909505605697632, "metricx_score": 4.085369110107422, "metricx_qe_score": 3.9738214015960693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "红色数字显示了在干净数据上直接应用的微调方法与仅将干净数据用于验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 52.03106746785858, "chrf_score": 46.88221718460579, "xcomet_score": 0.7853943109512329, "xcomet_qe_score": 0.7986895442008972, "metricx_score": 3.4396719932556152, "metricx_qe_score": 3.5021040439605713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如我们所见,如果每个类别有 10 个样本,直接微调开始优于 WSL 方法。", "metrics": {"bleu_score": 38.37912924021692, "chrf_score": 35.87967097598753, "xcomet_score": 0.9564950466156006, "xcomet_qe_score": 0.9125205278396606, "metricx_score": 1.6366592645645142, "metricx_qe_score": 2.8166680335998535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前WSL方法中声称的性能提升可以通过允许在干净的验证样本上继续微调来轻松实现。 从数据中", "metrics": {"bleu_score": 42.92840035036132, "chrf_score": 40.76220935675104, "xcomet_score": 0.773608922958374, "xcomet_qe_score": 0.7383017539978027, "metricx_score": 5.212018013000488, "metricx_qe_score": 4.963104248046875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,Van Lina 模型最初称为 FTW,其性能不如更复杂的 WSL 方法(如余弦相似度)。", "metrics": {"bleu_score": 26.241028629237412, "chrf_score": 25.66162416228599, "xcomet_score": 0.6587032079696655, "xcomet_qe_score": 0.5886296629905701, "metricx_score": 6.496028900146484, "metricx_qe_score": 7.0027055740356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果我们允许在干净样本上继续微调,那么 FTW 的表现与其他方法一样好。", "metrics": {"bleu_score": 54.19642316694007, "chrf_score": 46.26595815799744, "xcomet_score": 0.9007749557495117, "xcomet_qe_score": 0.7830424904823303, "metricx_score": 1.5407371520996094, "metricx_qe_score": 2.2880804538726807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的 WSL 方法,因为这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 54.70197181654885, "chrf_score": 54.979716663412724, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8066851496696472, "metricx_qe_score": 1.118506669998169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们表明,最近的WSL方法需要干净的手动标注样本才能正常工作。", "metrics": {"bleu_score": 51.67237045963152, "chrf_score": 47.90674164333587, "xcomet_score": 0.7988414764404297, "xcomet_qe_score": 0.8052822947502136, "metricx_score": 2.4959146976470947, "metricx_qe_score": 3.1425559520721436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下:", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 53.22177822177822, "xcomet_score": 0.9982490539550781, "xcomet_qe_score": 0.9813262224197388, "metricx_score": 0.2969313859939575, "metricx_qe_score": 0.2585373520851135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择的标准。", "metrics": {"bleu_score": 52.664038784792666, "chrf_score": 46.9183669176611, "xcomet_score": 0.9912177324295044, "xcomet_qe_score": 0.9179310202598572, "metricx_score": 0.2977628707885742, "metricx_qe_score": 0.5873326063156128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否使用干净的验证样本。", "metrics": {"bleu_score": 43.546195754004344, "chrf_score": 37.031167828539765, "xcomet_score": 0.899816632270813, "xcomet_qe_score": 0.841971755027771, "metricx_score": 1.135668396949768, "metricx_qe_score": 2.156416654586792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL 方法应与未来学习基线进行比较,因为两者都基于干净样本。", "metrics": {"bleu_score": 48.35217383567951, "chrf_score": 42.93363504846352, "xcomet_score": 0.7502132654190063, "xcomet_qe_score": 0.7551997900009155, "metricx_score": 3.71108341217041, "metricx_qe_score": 5.634787082672119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一种简单但强大的基线,应在未来的 WSL 工作中加以考虑。", "metrics": {"bleu_score": 38.267304752939, "chrf_score": 32.01063049853373, "xcomet_score": 0.8958289623260498, "xcomet_qe_score": 0.7250081300735474, "metricx_score": 1.3078113794326782, "metricx_qe_score": 2.555884599685669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过此幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9951430559158325, "xcomet_qe_score": 0.9870158433914185, "metricx_score": 0.48675400018692017, "metricx_qe_score": 0.44404107332229614, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝您会议愉快。", "metrics": {"bleu_score": 7.817610446892725, "chrf_score": 8.18252221407027, "xcomet_score": 0.9670588374137878, "xcomet_qe_score": 0.9966236352920532, "metricx_score": 0.5200188159942627, "metricx_qe_score": 0.28290775418281555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 9.993248618647176, "xcomet_score": 0.8700615763664246, "xcomet_qe_score": 0.6193946599960327, "metricx_score": 1.1073329448699951, "metricx_qe_score": 0.9096816182136536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "芬奇,我是萨拉·芬奇。", "metrics": {"bleu_score": 8.29519350710986, "chrf_score": 5.405070919696089, "xcomet_score": 0.6480262279510498, "xcomet_qe_score": 0.7325373888015747, "metricx_score": 4.678750514984131, "metricx_qe_score": 5.4248833656311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向大家介绍ABCeval,这是一种全新的评估对话式人工智能的维度方法。", "metrics": {"bleu_score": 39.94463928700864, "chrf_score": 39.05509614247019, "xcomet_score": 0.902928352355957, "xcomet_qe_score": 0.9600365161895752, "metricx_score": 1.9432307481765747, "metricx_qe_score": 1.954038143157959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是由埃默里大学的乔伊斯·乔伊斯教授领导的埃默里大学自然语言处理实验室完成的,并与亚马逊Alexa AI合作完成的。", "metrics": {"bleu_score": 37.854950099837254, "chrf_score": 41.67219402819374, "xcomet_score": 0.7159808874130249, "xcomet_qe_score": 0.7280460000038147, "metricx_score": 5.698080062866211, "metricx_qe_score": 4.778361797332764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚刚开发了一个对话模型,你想看看它与当前的先进技术相比表现如何。", "metrics": {"bleu_score": 77.85704580939215, "chrf_score": 68.66022437144954, "xcomet_score": 0.9983288049697876, "xcomet_qe_score": 0.9891366958618164, "metricx_score": 0.5082075595855713, "metricx_qe_score": 0.6062808036804199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常的做法是使用人工评估,例如让人工评判员选择两个对话中哪一个更好,或者根据李克特量表对对话进行评分。", "metrics": {"bleu_score": 64.67210678226574, "chrf_score": 59.04551109374384, "xcomet_score": 0.9014462232589722, "xcomet_qe_score": 0.8874146342277527, "metricx_score": 0.5302949547767639, "metricx_qe_score": 0.7887285947799683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的全面评估方面效果良好,但对话质量有许多方面。", "metrics": {"bleu_score": 36.6474865901556, "chrf_score": 30.76041475589639, "xcomet_score": 0.9300557971000671, "xcomet_qe_score": 0.9080116152763367, "metricx_score": 0.547738790512085, "metricx_qe_score": 0.7834680676460266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以便更细致地了解模型的优势和劣势。", "metrics": {"bleu_score": 60.18791526268261, "chrf_score": 57.64104316676456, "xcomet_score": 0.9830609560012817, "xcomet_qe_score": 0.9697476625442505, "metricx_score": 0.5960354804992676, "metricx_qe_score": 0.7260866165161133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人类评判者使用现有的比较方法或李克特量表方法,对对话质量的几个方面进行评估,例如模型响应的相关性。", "metrics": {"bleu_score": 62.043577266165066, "chrf_score": 56.22389519247173, "xcomet_score": 0.9743943214416504, "xcomet_qe_score": 0.9618710279464722, "metricx_score": 1.1810446977615356, "metricx_qe_score": 1.9127708673477173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们相信存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 40.00853590887389, "xcomet_score": 0.8991663455963135, "xcomet_qe_score": 0.8668703436851501, "metricx_score": 1.396376132965088, "metricx_qe_score": 1.407410740852356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表达了某些行为(如提供无关信息或自相矛盾),来减少人类评估的主观性。", "metrics": {"bleu_score": 57.935727943519815, "chrf_score": 49.08787643519166, "xcomet_score": 0.9603960514068604, "xcomet_qe_score": 0.9652961492538452, "metricx_score": 1.5284348726272583, "metricx_qe_score": 2.1072819232940674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为聊天行为标注或简而言之为 ABC 评估。", "metrics": {"bleu_score": 33.25778988272398, "chrf_score": 28.909699035119502, "xcomet_score": 0.777804970741272, "xcomet_qe_score": 0.7832255363464355, "metricx_score": 1.6362011432647705, "metricx_qe_score": 1.152894377708435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 69.13415687782464, "chrf_score": 60.70058985691518, "xcomet_score": 0.9336752891540527, "xcomet_qe_score": 0.9540376663208008, "metricx_score": 1.4507877826690674, "metricx_qe_score": 2.841010570526123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC评估能够衡量聊天模型犯下各种主题错误的比率。", "metrics": {"bleu_score": 62.00657885072486, "chrf_score": 52.12986518394428, "xcomet_score": 0.7413873672485352, "xcomet_qe_score": 0.6939934492111206, "metricx_score": 3.080937385559082, "metricx_qe_score": 3.6522364616394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABC 评估指标衡量的是聊天模型在对话中忽略对话伙伴或说出无关内容的次数。 当模型自相矛盾或与其伙伴自相矛盾,产生错误的事实或违反常识,以及当模型成功或未能表现出同理心时。", "metrics": {"bleu_score": 35.78730780008646, "chrf_score": 30.354220709239698, "xcomet_score": 0.6175484657287598, "xcomet_qe_score": 0.600860595703125, "metricx_score": 3.7007973194122314, "metricx_qe_score": 3.7833869457244873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方法最有效,我们选择了四种最先进的聊天模型,并使用 ABC 评估方法对每种模型进行了 100 次人机对话的评估。", "metrics": {"bleu_score": 56.19524166305481, "chrf_score": 53.244244948124056, "xcomet_score": 0.9688746929168701, "xcomet_qe_score": 0.9621857404708862, "metricx_score": 0.9560372829437256, "metricx_qe_score": 0.8281341791152954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了进行比较,我们还使用三种现有方法对这些对话进行了评估,即回合级李克特评分、对话级李克特评分和对话级配对比较。", "metrics": {"bleu_score": 52.036748720279824, "chrf_score": 43.830072153568445, "xcomet_score": 0.8513621091842651, "xcomet_qe_score": 0.808822512626648, "metricx_score": 3.311979055404663, "metricx_qe_score": 4.283467769622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每种现有方法,我们收集了对对话中最常见的八个方面的评价,因为这是在多个维度上评估聊天模型的标准做法。 ", "metrics": {"bleu_score": 53.90532958113517, "chrf_score": 44.88685079779501, "xcomet_score": 0.9227074980735779, "xcomet_qe_score": 0.8645337820053101, "metricx_score": 1.0335311889648438, "metricx_qe_score": 1.1082043647766113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现 ABC 评估行为标签在 100 个双重标签对话的标注者间一致性上,总体比现有方法收集的标签更可靠。", "metrics": {"bleu_score": 45.3909078450672, "chrf_score": 37.9917951895394, "xcomet_score": 0.7511157989501953, "xcomet_qe_score": 0.7919763326644897, "metricx_score": 4.6883015632629395, "metricx_qe_score": 5.263982772827148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,如本简单线性回归分析所示,与现有方法产生的指标相比,ABC评估标签更能预测整体对话质量。", "metrics": {"bleu_score": 51.935270960661036, "chrf_score": 42.65076441941393, "xcomet_score": 0.9592896699905396, "xcomet_qe_score": 0.9637390375137329, "metricx_score": 1.3326972723007202, "metricx_qe_score": 0.9233465194702148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到,通过测量自我与伴侣矛盾的比例,可以分别解释 5% 和 10% 的对话质量,而平均李克特一致性评分仅解释 4% 或更少。", "metrics": {"bleu_score": 50.1993314845318, "chrf_score": 42.85955446479829, "xcomet_score": 0.6863076686859131, "xcomet_qe_score": 0.68421471118927, "metricx_score": 4.889283657073975, "metricx_qe_score": 5.0121235847473145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查每个评估指标是否捕捉到了聊天质量的独特方面。 您", "metrics": {"bleu_score": 73.82945973307393, "chrf_score": 68.51481160506273, "xcomet_score": 0.8001829981803894, "xcomet_qe_score": 0.7955945134162903, "metricx_score": 4.396477699279785, "metricx_qe_score": 1.8730541467666626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,所有 ABC 评估指标的组合解释了超过 25% 的对话质量。随着您逐一移除这些指标,大多数指标都会导致丢失大量关于质量的信息。", "metrics": {"bleu_score": 47.03279142943736, "chrf_score": 44.27807500824816, "xcomet_score": 0.7387604713439941, "xcomet_qe_score": 0.8420262932777405, "metricx_score": 2.110525608062744, "metricx_qe_score": 2.957304000854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有转折级李克特量表的组合解释的质量远少,而且这些量表中只有少数几个携带独特信息。 这些可靠、信息丰", "metrics": {"bleu_score": 14.902981856581247, "chrf_score": 15.724893735133891, "xcomet_score": 0.34353259205818176, "xcomet_qe_score": 0.3828001022338867, "metricx_score": 10.659027099609375, "metricx_qe_score": 7.922473907470703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "富且独特的 ABC 评估指标使我们能够以比以前方法更高的分辨率来评估对话式人工智能。", "metrics": {"bleu_score": 3.689162246036907, "chrf_score": 7.469101213754067, "xcomet_score": 0.4879588782787323, "xcomet_qe_score": 0.7070815563201904, "metricx_score": 5.3911919593811035, "metricx_qe_score": 4.7134785652160645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从实验结果中可以看出,仍存在一些挑战,并且这些挑战已经被精确量化。", "metrics": {"bleu_score": 32.490568049380954, "chrf_score": 31.597374813230665, "xcomet_score": 0.9973431825637817, "xcomet_qe_score": 0.9896780848503113, "metricx_score": 0.6306935548782349, "metricx_qe_score": 0.6601923108100891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人大约有 20% 的回答违反常识。", "metrics": {"bleu_score": 46.75624312795152, "chrf_score": 41.922016324583176, "xcomet_score": 0.9844323396682739, "xcomet_qe_score": 0.9734805822372437, "metricx_score": 0.6258295774459839, "metricx_qe_score": 1.2315946817398071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大约 15% 的回答中,他们提供的信息与主题无关。而且,他们大约有 10% 的时候会自相矛盾或与伴侣矛盾。", "metrics": {"bleu_score": 30.234383090402247, "chrf_score": 28.413001656000958, "xcomet_score": 0.7317013144493103, "xcomet_qe_score": 0.8045984506607056, "metricx_score": 3.851963520050049, "metricx_qe_score": 1.8914493322372437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速发展,自我们进行评估以来,许多错误率在新发布的模型中可能会下降。", "metrics": {"bleu_score": 62.246345434473554, "chrf_score": 53.747302102565264, "xcomet_score": 0.931909441947937, "xcomet_qe_score": 0.9739501476287842, "metricx_score": 1.8060520887374878, "metricx_qe_score": 1.7401046752929688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更增加了追求可靠和精确的评估指标以比较模型的必要性。", "metrics": {"bleu_score": 34.31535116220621, "chrf_score": 29.381125862620273, "xcomet_score": 0.9990153312683105, "xcomet_qe_score": 0.9935992956161499, "metricx_score": 1.153178095817566, "metricx_qe_score": 1.1661124229431152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABC Eval 可以被该领域的其他人作为朝着这个方向迈出的有意义的一步。", "metrics": {"bleu_score": 63.57777456120979, "chrf_score": 57.675186888308396, "xcomet_score": 0.9737764596939087, "xcomet_qe_score": 0.9484171867370605, "metricx_score": 2.1682472229003906, "metricx_qe_score": 2.7927422523498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待在接下来的几个月和几年里,对话式人工智能将如何发展。", "metrics": {"bleu_score": 30.300203366197277, "chrf_score": 28.94082750621081, "xcomet_score": 0.8925098180770874, "xcomet_qe_score": 0.9258863925933838, "metricx_score": 0.5408633947372437, "metricx_qe_score": 0.5317544341087341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Kaio-Yin,今天我将介绍我们的作品《何时需要上下文进行翻译?", "metrics": {"bleu_score": 43.52598446478626, "chrf_score": 37.638689402252865, "xcomet_score": 0.8664333820343018, "xcomet_qe_score": 0.8518882989883423, "metricx_score": 1.6549146175384521, "metricx_qe_score": 3.084881544113159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一项数据驱动的多语言探索》。", "metrics": {"bleu_score": 83.85766789076261, "chrf_score": 95.85155268570797, "xcomet_score": 0.8971652984619141, "xcomet_qe_score": 0.7998672723770142, "metricx_score": 1.3824670314788818, "metricx_qe_score": 2.0614826679229736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernandes、Emmy Liu、Andre F.D. Martins和Graham Newbig合作完成的。 因此", "metrics": {"bleu_score": 46.95966835778608, "chrf_score": 76.0482024177377, "xcomet_score": 0.6672706007957458, "xcomet_qe_score": 0.6741051077842712, "metricx_score": 5.50205659866333, "metricx_qe_score": 2.582073211669922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",很多翻译都取决于上下文。", "metrics": {"bleu_score": 67.03420896351791, "chrf_score": 64.8171272760201, "xcomet_score": 0.9983985424041748, "xcomet_qe_score": 0.9895898103713989, "metricx_score": 0.8651986122131348, "metricx_qe_score": 1.009296178817749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译这个句子中的“mole”?", "metrics": {"bleu_score": 54.017258985951415, "chrf_score": 55.419633882775734, "xcomet_score": 0.9968364238739014, "xcomet_qe_score": 0.967570424079895, "metricx_score": 0.9834437966346741, "metricx_qe_score": 1.9881327152252197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好吧,如果前一句话是,如果部长们知道了,事情可能会变得危险,那么 Mo 指的是间谍。", "metrics": {"bleu_score": 15.293885404881333, "chrf_score": 7.332159644912051, "xcomet_score": 0.8613948225975037, "xcomet_qe_score": 0.8625412583351135, "metricx_score": 4.96220588684082, "metricx_qe_score": 5.692151069641113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是如果前一句话是,医生,这可能是严重的事情吗?那么 Mo 指的是一个胎记。", "metrics": {"bleu_score": 14.829668168745691, "chrf_score": 9.837911626690964, "xcomet_score": 0.7337090373039246, "xcomet_qe_score": 0.7182257771492004, "metricx_score": 5.81742000579834, "metricx_qe_score": 5.82086181640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,这个词的含义会发生变化,因此它的翻译也会随之改变。", "metrics": {"bleu_score": 37.52957402179448, "chrf_score": 31.180333177261705, "xcomet_score": 0.9893181324005127, "xcomet_qe_score": 0.9824478030204773, "metricx_score": 0.374392032623291, "metricx_qe_score": 0.3960738778114319, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在这种情况下翻译得如何相当困难。", "metrics": {"bleu_score": 44.1407653688105, "chrf_score": 40.39953042610255, "xcomet_score": 0.9526234269142151, "xcomet_qe_score": 0.8410000801086426, "metricx_score": 1.0139570236206055, "metricx_qe_score": 2.8328495025634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有小部分翻译依赖于上下文,这使得像 BLEU 这样的语料库级指标无法捕捉到这些翻译。", "metrics": {"bleu_score": 52.10710032512805, "chrf_score": 45.17611517338166, "xcomet_score": 0.9907901287078857, "xcomet_qe_score": 0.9751671552658081, "metricx_score": 1.1748758554458618, "metricx_qe_score": 1.8770203590393066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对上下文相关的翻译进行有针对性的评估,但这些资源只能支持有限类型的上下文相关翻译和有限的语言集合,因为它们通常依赖于领域知识和人工整理。", "metrics": {"bleu_score": 72.4188780677129, "chrf_score": 66.6688897965523, "xcomet_score": 0.941132664680481, "xcomet_qe_score": 0.9480706453323364, "metricx_score": 1.2406362295150757, "metricx_qe_score": 1.0529228448867798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型如何处理这些情况?", "metrics": {"bleu_score": 32.74135267450808, "chrf_score": 30.12273108277076, "xcomet_score": 0.9986907243728638, "xcomet_qe_score": 0.9914894104003906, "metricx_score": 0.7201682925224304, "metricx_qe_score": 1.0261601209640503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了一个词在翻译中对上下文依赖的程度。", "metrics": {"bleu_score": 53.028494715521774, "chrf_score": 43.56288729844965, "xcomet_score": 0.9893730878829956, "xcomet_qe_score": 0.9941670894622803, "metricx_score": 4.18385648727417, "metricx_qe_score": 4.26596736907959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的研究中,我们介绍了CXMI作为机器翻译模型上下文使用的度量方法。", "metrics": {"bleu_score": 60.10163577618584, "chrf_score": 59.311837565889824, "xcomet_score": 0.9127119183540344, "xcomet_qe_score": 0.9069952368736267, "metricx_score": 1.4372879266738892, "metricx_qe_score": 2.059953212738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过测量上下文C在给定源X的情况下,对目标Y提供了多少信息来实现这一目标。 你可以将 CXMI 视为为模型提供上下文信息所获得的信息。", "metrics": {"bleu_score": 55.69738204258236, "chrf_score": 49.213844808569476, "xcomet_score": 0.8886638879776001, "xcomet_qe_score": 0.8578945398330688, "metricx_score": 3.491931915283203, "metricx_qe_score": 4.316118240356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将 CXMI 扩展为逐点 CXMI,它可以在句子级别或词级别上衡量上下文的使用情况。", "metrics": {"bleu_score": 42.51669109552256, "chrf_score": 35.86527265785952, "xcomet_score": 0.8397730588912964, "xcomet_qe_score": 0.8220580816268921, "metricx_score": 2.2779622077941895, "metricx_qe_score": 2.965151071548462, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将 PSXMI 值高的词语视为需要上下文进行翻译的词语。", "metrics": {"bleu_score": 70.94521095075528, "chrf_score": 68.90856594502488, "xcomet_score": 0.9006674289703369, "xcomet_qe_score": 0.9088047742843628, "metricx_score": 2.57420015335083, "metricx_qe_score": 3.1930899620056152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们分析具有高 PCXMI 的词语,以寻找这些词语之间的模式。", "metrics": {"bleu_score": 36.39723552007911, "chrf_score": 41.58248595343053, "xcomet_score": 0.9716205596923828, "xcomet_qe_score": 0.9605770111083984, "metricx_score": 1.6213035583496094, "metricx_qe_score": 3.4425742626190186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成 14 种不同语言的 TED 演讲稿进行了分析。", "metrics": {"bleu_score": 72.56398949150912, "chrf_score": 70.58808653695228, "xcomet_score": 0.9972835779190063, "xcomet_qe_score": 0.9925276041030884, "metricx_score": 0.7381255626678467, "metricx_qe_score": 1.0282196998596191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同层次上进行分析。", "metrics": {"bleu_score": 69.01228050062707, "chrf_score": 58.83204419988524, "xcomet_score": 0.995771050453186, "xcomet_qe_score": 0.9874769449234009, "metricx_score": 0.2352285534143448, "metricx_qe_score": 0.38425329327583313, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有高均值 PCXMI 的词性标签。", "metrics": {"bleu_score": 34.6697783111003, "chrf_score": 31.708611730813757, "xcomet_score": 0.8907898664474487, "xcomet_qe_score": 0.8235342502593994, "metricx_score": 2.2274813652038574, "metricx_qe_score": 3.328263282775879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到阿拉伯语中的双重代词,这些代词的 P6MI 相对较高。", "metrics": {"bleu_score": 53.40004805914115, "chrf_score": 45.0359396042099, "xcomet_score": 0.7517125606536865, "xcomet_qe_score": 0.8100768327713013, "metricx_score": 6.086437225341797, "metricx_qe_score": 5.4484710693359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,英语没有双重代词,因此在翻译成阿拉伯语时,你需要上下文来确定代词是否为双重。", "metrics": {"bleu_score": 62.532235387603464, "chrf_score": 55.4260208657665, "xcomet_score": 0.7508536577224731, "xcomet_qe_score": 0.9022637605667114, "metricx_score": 2.619354724884033, "metricx_qe_score": 3.0848913192749023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现,在选择适当的动词形式时,某些语言也需要上下文。", "metrics": {"bleu_score": 68.08663291835767, "chrf_score": 61.18430263298716, "xcomet_score": 0.9977996349334717, "xcomet_qe_score": 0.989017128944397, "metricx_score": 0.5972544550895691, "metricx_qe_score": 0.898543119430542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们查看了在所有不同出现情况下平均 PCSXMI 值较高的词汇项。", "metrics": {"bleu_score": 40.10198002912937, "chrf_score": 35.866563735989466, "xcomet_score": 0.9284569025039673, "xcomet_qe_score": 0.9167250394821167, "metricx_score": 3.0540006160736084, "metricx_qe_score": 3.9280776977539062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的情况,在中文中,你需要上下文来翻译专有名词,以确保你在文档中使用相同的翻译。", "metrics": {"bleu_score": 35.564869740332654, "chrf_score": 30.813077853067316, "xcomet_score": 0.8592212200164795, "xcomet_qe_score": 0.9212212562561035, "metricx_score": 0.9703196287155151, "metricx_qe_score": 1.2445063591003418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现上下文有助于以适当的正式程度进行翻译。", "metrics": {"bleu_score": 29.063431023881783, "chrf_score": 28.63701351504888, "xcomet_score": 0.9416265487670898, "xcomet_qe_score": 0.9299638867378235, "metricx_score": 0.7672496438026428, "metricx_qe_score": 0.8377513885498047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看了 p6mi 值较高的不同个体标记。", "metrics": {"bleu_score": 17.92334464048543, "chrf_score": 14.388673271244532, "xcomet_score": 0.7058849930763245, "xcomet_qe_score": 0.657647967338562, "metricx_score": 6.825473785400391, "metricx_qe_score": 6.160968780517578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出无法通过单词本身捕捉到的现象,但这些现象在句子结构中得到体现,例如省略号的解析。", "metrics": {"bleu_score": 40.79354614869208, "chrf_score": 33.66516477914385, "xcomet_score": 0.9304128885269165, "xcomet_qe_score": 0.8628344535827637, "metricx_score": 0.9110639095306396, "metricx_qe_score": 1.1491038799285889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们现在利用分析结果来设计一个文档级翻译基准。", "metrics": {"bleu_score": 45.31144400495293, "chrf_score": 37.80893798129738, "xcomet_score": 0.9871044158935547, "xcomet_qe_score": 0.9084465503692627, "metricx_score": 0.8918256759643555, "metricx_qe_score": 1.0401930809020996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们所识别的五个语篇现象中的每一个,我们都创建了标记器,以便自动识别与该现象相关的词语", "metrics": {"bleu_score": 51.0076156805044, "chrf_score": 43.06278335570649, "xcomet_score": 0.9309214353561401, "xcomet_qe_score": 0.8848718404769897, "metricx_score": 1.0808151960372925, "metricx_qe_score": 1.3373775482177734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们称我们的标记器为多语言语篇感知标记器,简称 MUDA 标记器。", "metrics": {"bleu_score": 18.101500231755598, "chrf_score": 25.154099638440453, "xcomet_score": 0.9040670394897461, "xcomet_qe_score": 0.863979697227478, "metricx_score": 1.3170785903930664, "metricx_qe_score": 1.6413805484771729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们还可以注意到,不同的语言在这些话语现象的比例上有所不同。", "metrics": {"bleu_score": 31.443515194397026, "chrf_score": 31.867419916356383, "xcomet_score": 0.9813351631164551, "xcomet_qe_score": 0.9756438732147217, "metricx_score": 1.0213202238082886, "metricx_qe_score": 1.147033452987671, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们使用Muda标记器,将其应用于我们想要用于评估的平行语料库。我们对Muda标记器识别的上下文相关示例应用我们选择的翻译指标。", "metrics": {"bleu_score": 58.532853451436694, "chrf_score": 51.58861320162891, "xcomet_score": 0.8808441758155823, "xcomet_qe_score": 0.8175914287567139, "metricx_score": 1.8228939771652222, "metricx_qe_score": 2.3921265602111816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译上的表现。", "metrics": {"bleu_score": 74.38252804209344, "chrf_score": 72.81560651989159, "xcomet_score": 0.9105273485183716, "xcomet_qe_score": 0.8643457889556885, "metricx_score": 0.9695982933044434, "metricx_qe_score": 1.057651400566101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,对于蓝色(blue),我们发现无上下文模型的性能最好。", "metrics": {"bleu_score": 37.45987049424316, "chrf_score": 30.587699513363592, "xcomet_score": 0.834392786026001, "xcomet_qe_score": 0.7521483898162842, "metricx_score": 2.796675682067871, "metricx_qe_score": 2.8665308952331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果我们使用 COMET,具备上下文感知能力的模型表现最好。而", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 53.966516896185205, "xcomet_score": 0.8751266002655029, "xcomet_qe_score": 0.827225923538208, "metricx_score": 2.9123144149780273, "metricx_qe_score": 2.524752378463745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用词 F 测度,那么有无上下文的模型表现相当。", "metrics": {"bleu_score": 55.08211968797423, "chrf_score": 50.647717935272155, "xcomet_score": 0.7393917441368103, "xcomet_qe_score": 0.7286790609359741, "metricx_score": 2.7389109134674072, "metricx_qe_score": 4.142938613891602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果我们仅使用语料库级别的指标,就很难确定最佳的文档级翻译系统。", "metrics": {"bleu_score": 76.60134098817062, "chrf_score": 73.07752176646186, "xcomet_score": 0.998871922492981, "xcomet_qe_score": 0.9940192699432373, "metricx_score": 0.667285144329071, "metricx_qe_score": 0.7865613698959351, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用Muda基准来评估模型,我们发现,对于某些特定的语言现象,如正式程度和词汇连贯性,考虑上下文关系的模型要比不考虑上下文的模型准确得多。", "metrics": {"bleu_score": 42.30758773851007, "chrf_score": 38.35792554462662, "xcomet_score": 0.9094018936157227, "xcomet_qe_score": 0.8807430863380432, "metricx_score": 2.0507798194885254, "metricx_qe_score": 2.5453717708587646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在处理其他现象(如省略号、代词和动词形式)时,并没有比不使用上下文的模型好多少。", "metrics": {"bleu_score": 55.93596420162442, "chrf_score": 51.24652202648598, "xcomet_score": 0.9898190498352051, "xcomet_qe_score": 0.9635958671569824, "metricx_score": 0.6886937618255615, "metricx_qe_score": 0.8176777362823486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明我们需要在文档级翻译方面取得更大的进展。", "metrics": {"bleu_score": 30.099273730403407, "chrf_score": 29.85067222231018, "xcomet_score": 0.9984045028686523, "xcomet_qe_score": 0.9896292686462402, "metricx_score": 0.7432674169540405, "metricx_qe_score": 0.7704477310180664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试表明,DeepL在文档级翻译中通常比谷歌翻译更准确。", "metrics": {"bleu_score": 71.0584770449922, "chrf_score": 62.22222229983132, "xcomet_score": 0.9657672643661499, "xcomet_qe_score": 0.8393145799636841, "metricx_score": 1.2180898189544678, "metricx_qe_score": 1.5645670890808105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们对 14 对语言组合进行了数据驱动分析,以确定何时需要上下文进行翻译。 然后,我们将研究结果用于构建文档级机器翻译的基准,这有助于我们确定哪些话语现象模型能够很好地处理,哪些翻译系统擅长文档级翻译。", "metrics": {"bleu_score": 49.817206542095676, "chrf_score": 44.295362341741836, "xcomet_score": 0.8183294534683228, "xcomet_qe_score": 0.7799097299575806, "metricx_score": 2.3864402770996094, "metricx_qe_score": 3.2145228385925293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多伦多见。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9990917444229126, "xcomet_qe_score": 0.985295832157135, "metricx_score": 0.46565622091293335, "metricx_qe_score": 1.37757408618927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是 Yanis Lavrak,我将向您介绍我们在 Dr. BERT 上的工作,这是一个针对生物医学和临床领域的强大法语预训练模型。", "metrics": {"bleu_score": 35.7210065060236, "chrf_score": 39.19065397320017, "xcomet_score": 0.7518852353096008, "xcomet_qe_score": 0.7515136003494263, "metricx_score": 2.6222660541534424, "metricx_qe_score": 2.3107876777648926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本演示中,我们首先讨论医疗保健中的语言建模。", "metrics": {"bleu_score": 41.163637617755036, "chrf_score": 32.89604441129711, "xcomet_score": 0.9821245670318604, "xcomet_qe_score": 0.9837895631790161, "metricx_score": 1.8712214231491089, "metricx_qe_score": 1.923445463180542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们推出了第一个法语生物医学模型 Dr. Bert,该模型基于 Roberta,并在 NACHOS 上进行训练,NACHOS 是一个来自网络的医学众包数据集。", "metrics": {"bleu_score": 41.24291502636913, "chrf_score": 41.43757638512789, "xcomet_score": 0.6756143569946289, "xcomet_qe_score": 0.5697721242904663, "metricx_score": 2.4596500396728516, "metricx_qe_score": 2.736034393310547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了多种预训练设置和数据源的模型比较。", "metrics": {"bleu_score": 77.14617999067436, "chrf_score": 72.30630813037793, "xcomet_score": 0.9802025556564331, "xcomet_qe_score": 0.9668021202087402, "metricx_score": 0.7535673379898071, "metricx_qe_score": 1.1159687042236328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在法语中介绍了我们在 11 个生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 63.94694410559323, "chrf_score": 60.1822839396673, "xcomet_score": 0.7280163168907166, "xcomet_qe_score": 0.8396375775337219, "metricx_score": 1.4164512157440186, "metricx_qe_score": 2.1635258197784424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结了实验结果,并为您提供了更多关于如何访问模型的详细信息。", "metrics": {"bleu_score": 42.202346209160325, "chrf_score": 34.787764377142395, "xcomet_score": 0.9818944931030273, "xcomet_qe_score": 0.9641553163528442, "metricx_score": 0.28331276774406433, "metricx_qe_score": 0.2553825080394745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自 2018 年发布以来,BERT 已成为解决自然语言处理任务的最有效方法之一,相比传统的静态和情境化方法(如 Word2Vec、FastText 或 NWO),BERT 的性能提升显著。", "metrics": {"bleu_score": 47.26651093374548, "chrf_score": 47.450547817489465, "xcomet_score": 0.8291497230529785, "xcomet_qe_score": 0.8329989910125732, "metricx_score": 2.658963203430176, "metricx_qe_score": 2.5806634426116943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从那时起,该模型已被应用于许多其他语言,例如法语中的 Camembert,以及生物医学领域的 PAMED-BERT 和 BioBERT,以及临床领域的 Clinical-BERT,但主要还是以英语为主。", "metrics": {"bleu_score": 46.6751016597901, "chrf_score": 53.042994124312905, "xcomet_score": 0.851984977722168, "xcomet_qe_score": 0.8244379162788391, "metricx_score": 3.1930465698242188, "metricx_qe_score": 3.4440934658050537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专用模型很少,而且由于缺乏领域内的数据,通常基于持续预训练。", "metrics": {"bleu_score": 42.202346209160325, "chrf_score": 33.198418683155964, "xcomet_score": 0.8378649950027466, "xcomet_qe_score": 0.8162862062454224, "metricx_score": 1.2125425338745117, "metricx_qe_score": 1.859840750694275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法国一直没有开源的现代生物医学研究软件。", "metrics": {"bleu_score": 14.973422121848623, "chrf_score": 16.10948083204386, "xcomet_score": 0.8569519519805908, "xcomet_qe_score": 0.8556081056594849, "metricx_score": 2.7345218658447266, "metricx_qe_score": 2.049177646636963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们,因此我们问自己一个问题,对于广泛的用途,最合适的资料来源是什么?而那些当前的数据是临床数据的良好替代品。", "metrics": {"bleu_score": 32.809387946197226, "chrf_score": 35.82405414291912, "xcomet_score": 0.5397977828979492, "xcomet_qe_score": 0.538519024848938, "metricx_score": 4.583121299743652, "metricx_qe_score": 5.058372497558594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将伯特博士与我们的舒伯特模型进行比较,该模型基于我们所在医院非大学医院的匿名数据。", "metrics": {"bleu_score": 39.548519657284814, "chrf_score": 28.381844918635586, "xcomet_score": 0.5150813460350037, "xcomet_qe_score": 0.5336209535598755, "metricx_score": 5.31960391998291, "metricx_qe_score": 5.633806228637695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,我们会问自己,我们需要多少数据来训练一个专门处理法语数据的模型?", "metrics": {"bleu_score": 44.88382125068511, "chrf_score": 45.77813401309331, "xcomet_score": 0.9942398071289062, "xcomet_qe_score": 0.922757089138031, "metricx_score": 0.7929332852363586, "metricx_qe_score": 0.7237957715988159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是 4GB、8GB 还是更多?", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 90.21205646205644, "xcomet_score": 0.9967656135559082, "xcomet_qe_score": 0.9750891923904419, "metricx_score": 0.32201769948005676, "metricx_qe_score": 0.6933939456939697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四种从头开始的模型。第一种是 Dr. Bert 的第一版,有 7GB 的 nachos;第二种是 4GB 的 nachos 集合。 第一版舒伯特是一个临床模型,包含4GB的临床笔记句子。最终版本的舒伯特则结合了4GB的自然语言子集和4GB的临床笔记。", "metrics": {"bleu_score": 43.11229659717023, "chrf_score": 33.10205120709101, "xcomet_score": 0.27582502365112305, "xcomet_qe_score": 0.24013365805149078, "metricx_score": 7.137593746185303, "metricx_qe_score": 7.463730812072754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还引入了三个在持续预训练上进行训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 58.211133129365905, "chrf_score": 49.37249203455878, "xcomet_score": 0.866234540939331, "xcomet_qe_score": 0.8475589752197266, "metricx_score": 2.3943064212799072, "metricx_qe_score": 3.161935567855835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于 Camembert 的模型,训练数据为 4GB 的 nacho 数据集。", "metrics": {"bleu_score": 15.861659860045421, "chrf_score": 26.28815621807102, "xcomet_score": 0.818510890007019, "xcomet_qe_score": 0.6028720140457153, "metricx_score": 3.3427610397338867, "metricx_qe_score": 4.025690078735352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也是基于 Camembert 的模型,但这次训练数据为 4GB 的 clink 和 lots 数据。 最后,基于英语生物医学模型的 Bermud-Bert,在 4GB 的抓取数据集中进行训练。", "metrics": {"bleu_score": 29.5644866793976, "chrf_score": 29.112272127795325, "xcomet_score": 0.4111890196800232, "xcomet_qe_score": 0.34657225012779236, "metricx_score": 9.11591911315918, "metricx_qe_score": 10.335142135620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了多个公共和私人的无花果任务,如姓名和身份识别、分类、词性标注和问答。", "metrics": {"bleu_score": 52.27512419108287, "chrf_score": 44.73248531605057, "xcomet_score": 0.5405200719833374, "xcomet_qe_score": 0.537187933921814, "metricx_score": 5.202118396759033, "metricx_qe_score": 5.354306697845459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基准模型进行了比较,这些基准模型包括 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCnet 4 GB、Pumatbert、BioBERT 和 ClinicalBERT。", "metrics": {"bleu_score": 31.623503269728193, "chrf_score": 47.27394010005217, "xcomet_score": 0.5277683734893799, "xcomet_qe_score": 0.56156325340271, "metricx_score": 5.38623046875, "metricx_qe_score": 5.408490180969238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型的演化表明,该模型在与训练数据性质相同的任务上表现最佳。", "metrics": {"bleu_score": 29.784750149958793, "chrf_score": 25.59345690629535, "xcomet_score": 0.86817866563797, "xcomet_qe_score": 0.799596905708313, "metricx_score": 2.1102633476257324, "metricx_qe_score": 2.049947500228882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从数据中获得信息,我们可以观察到来自不同来源的数据似乎更加多样化。", "metrics": {"bleu_score": 34.112349495906415, "chrf_score": 41.43287501656961, "xcomet_score": 0.7895880937576294, "xcomet_qe_score": 0.8186928033828735, "metricx_score": 3.9861836433410645, "metricx_qe_score": 3.4735748767852783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以带来更好的性能。", "metrics": {"bleu_score": 52.6589137558171, "chrf_score": 45.93989530077461, "xcomet_score": 0.9362239837646484, "xcomet_qe_score": 0.9736031889915466, "metricx_score": 2.49906587600708, "metricx_qe_score": 2.952756404876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始的免费训练似乎在大多数任务中都能获得更高的性能。", "metrics": {"bleu_score": 58.65965968520906, "chrf_score": 54.9389181646618, "xcomet_score": 0.8201733827590942, "xcomet_qe_score": 0.8025127649307251, "metricx_score": 6.064919471740723, "metricx_qe_score": 6.553226470947266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用 Pumet-BERT 的权重和分词器,在 NACHOS 的 4GB 子集上进行持续预训练的实验,结果与从头训练的 Dr.BERT 4GB 获得的结果相当。", "metrics": {"bleu_score": 40.447562100773474, "chrf_score": 47.663294934583654, "xcomet_score": 0.6053963899612427, "xcomet_qe_score": 0.5633109211921692, "metricx_score": 5.753292560577393, "metricx_qe_score": 6.2742109298706055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而基于 Camembert weights 和 token leather 的模型则不然,它们存在稳定性问题。", "metrics": {"bleu_score": 22.95535461584143, "chrf_score": 25.870383661094724, "xcomet_score": 0.5664664506912231, "xcomet_qe_score": 0.5686584115028381, "metricx_score": 7.674409866333008, "metricx_qe_score": 7.031398296356201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们的系统在 11 个下游任务中表现优异,超过了通用模型 Camembert 的全球结果。", "metrics": {"bleu_score": 26.391499288824036, "chrf_score": 28.36361925388796, "xcomet_score": 0.7908645868301392, "xcomet_qe_score": 0.7236148118972778, "metricx_score": 4.365553379058838, "metricx_qe_score": 4.321133136749268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,专业数据更好,更专业的数据更好,但它扩展性不佳。 所有", "metrics": {"bleu_score": 15.094258117090344, "chrf_score": 16.031620544871046, "xcomet_score": 0.4846944808959961, "xcomet_qe_score": 0.4095950424671173, "metricx_score": 5.2403154373168945, "metricx_qe_score": 4.08966064453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 NACHOS 获得的预训练模型都可以在 UGIM 面对面免费获取,所有训练脚本都在我们的 GitHub 仓库中。", "metrics": {"bleu_score": 35.86557968847233, "chrf_score": 37.10862034096273, "xcomet_score": 0.7386537790298462, "xcomet_qe_score": 0.7572418451309204, "metricx_score": 6.287217617034912, "metricx_qe_score": 6.716361999511719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以感谢这次演讲,我们期待在多伦多的会议结束后采取行动。", "metrics": {"bleu_score": 16.848646110322342, "chrf_score": 17.933334238561578, "xcomet_score": 0.5613510608673096, "xcomet_qe_score": 0.6792370080947876, "metricx_score": 4.665480136871338, "metricx_qe_score": 6.28611421585083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9670989513397217, "xcomet_qe_score": 0.9718614816665649, "metricx_score": 0.2643663287162781, "metricx_qe_score": 0.26394033432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼,今天我将向大家简要介绍我们的论文——《无需树结构的组合泛化:使用多集标记和潜在置换》。", "metrics": {"bleu_score": 36.42317694376632, "chrf_score": 29.28128346560656, "xcomet_score": 0.8488873243331909, "xcomet_qe_score": 0.90559983253479, "metricx_score": 1.5449326038360596, "metricx_qe_score": 1.16024911403656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和我的导师 Alexander Koller 和 Ivan Titov 的合作成果。", "metrics": {"bleu_score": 42.461633178803446, "chrf_score": 71.80873027622253, "xcomet_score": 0.9649262428283691, "xcomet_qe_score": 0.9381760358810425, "metricx_score": 1.0900102853775024, "metricx_qe_score": 1.4762861728668213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合概化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语组合的能力。", "metrics": {"bleu_score": 74.71600506180394, "chrf_score": 69.34063410625538, "xcomet_score": 0.7195075750350952, "xcomet_qe_score": 0.6423660516738892, "metricx_score": 5.091233253479004, "metricx_qe_score": 6.177090644836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能看起来像这样。", "metrics": {"bleu_score": 46.942223829384936, "chrf_score": 41.593386115832374, "xcomet_score": 0.9042496681213379, "xcomet_qe_score": 0.8924782276153564, "metricx_score": 1.0878242254257202, "metricx_qe_score": 1.6714626550674438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一如既往,我们有一个训练语料", "metrics": {"bleu_score": 18.884639165413912, "chrf_score": 17.034432917163528, "xcomet_score": 0.8421576023101807, "xcomet_qe_score": 0.8321541547775269, "metricx_score": 2.579280376434326, "metricx_qe_score": 2.284564733505249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "集,在这种情况下,女孩睡着了,", "metrics": {"bleu_score": 8.051321384556239, "chrf_score": 4.703348267869466, "xcomet_score": 0.431746244430542, "xcomet_qe_score": 0.5912654399871826, "metricx_score": 7.3370771408081055, "metricx_qe_score": 6.376560211181641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "玛丽知道女孩睡着了。", "metrics": {"bleu_score": 10.441402677687273, "chrf_score": 9.106004834333625, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 1.2834631204605103, "metricx_qe_score": 1.1750768423080444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些言语与其意义的核心方面相对应的逻辑形式相配。", "metrics": {"bleu_score": 7.105310739172045, "chrf_score": 13.267126583306773, "xcomet_score": 0.7761605381965637, "xcomet_qe_score": 0.8483535647392273, "metricx_score": 1.685027837753296, "metricx_qe_score": 1.1814444065093994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集并非来自相同的分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 55.90981220397046, "chrf_score": 50.240316868560384, "xcomet_score": 0.8630441427230835, "xcomet_qe_score": 0.8781783580780029, "metricx_score": 1.056781530380249, "metricx_qe_score": 1.8781754970550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中经历了浅层递归,并在一个具有更深层递归的例子上进行了测试。", "metrics": {"bleu_score": 33.45233759651099, "chrf_score": 30.124635413079233, "xcomet_score": 0.9551718235015869, "xcomet_qe_score": 0.9012242555618286, "metricx_score": 1.54635751247406, "metricx_qe_score": 2.6060948371887207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种分布外泛化问题,并且经常会产生与输入内容无关的输出。", "metrics": {"bleu_score": 43.9020191957939, "chrf_score": 36.786428307202144, "xcomet_score": 0.7869656085968018, "xcomet_qe_score": 0.7970255613327026, "metricx_score": 3.802964210510254, "metricx_qe_score": 3.511899709701538, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,他们往往无法再现输入和输出之间的系统对应关系,例如例子中用颜色编码的对应关系。", "metrics": {"bleu_score": 57.955628207374694, "chrf_score": 53.55866234905234, "xcomet_score": 0.9928284883499146, "xcomet_qe_score": 0.9853382110595703, "metricx_score": 1.8244181871414185, "metricx_qe_score": 1.2888199090957642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的解决方法是将树木融入模型中。", "metrics": {"bleu_score": 30.586957615133986, "chrf_score": 25.231858304955058, "xcomet_score": 0.9290779232978821, "xcomet_qe_score": 0.9340708255767822, "metricx_score": 0.5691179037094116, "metricx_qe_score": 0.7894960045814514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "树的用意在于捕捉将发音与逻辑形式联系起来的组合过程。", "metrics": {"bleu_score": 48.81776922763246, "chrf_score": 44.02965768797837, "xcomet_score": 0.7356414794921875, "xcomet_qe_score": 0.7378012537956238, "metricx_score": 2.2436604499816895, "metricx_qe_score": 3.8759925365448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法效果很好,但通常不会提供树,需要通过某种方式获取。", "metrics": {"bleu_score": 46.40615962360809, "chrf_score": 41.676671498709126, "xcomet_score": 0.9375432133674622, "xcomet_qe_score": 0.9336903691291809, "metricx_score": 2.4672279357910156, "metricx_qe_score": 2.151196241378784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本较高的过程。", "metrics": {"bleu_score": 35.4225224760582, "chrf_score": 30.964956807920935, "xcomet_score": 0.9723730087280273, "xcomet_qe_score": 0.9788724184036255, "metricx_score": 0.4672151803970337, "metricx_qe_score": 0.6136482357978821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到对逻辑形式进行大量的形式化预处理,例如处理变量符号。", "metrics": {"bleu_score": 52.68694760883328, "chrf_score": 46.737138190912155, "xcomet_score": 0.9923501014709473, "xcomet_qe_score": 0.9947376251220703, "metricx_score": 0.6100496053695679, "metricx_qe_score": 0.6371234655380249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树可能也涉及到专门的语法归纳程序。", "metrics": {"bleu_score": 28.688236146427446, "chrf_score": 25.42100028089026, "xcomet_score": 0.8635983467102051, "xcomet_qe_score": 0.8903573155403137, "metricx_score": 4.861697196960449, "metricx_qe_score": 5.501476764678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文中,我们没有使用树结构,而是引入了一种神经序列到序列模型,该模型直接对输入片段与输出片段之间的对应关系进行建模。", "metrics": {"bleu_score": 56.25962975184686, "chrf_score": 49.991775709281896, "xcomet_score": 0.8217098116874695, "xcomet_qe_score": 0.8338313102722168, "metricx_score": 1.5476908683776855, "metricx_qe_score": 1.5766558647155762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在不依赖树的情况下对更深层次的递归进行强大的泛化。", "metrics": {"bleu_score": 38.37679744478914, "chrf_score": 30.612618517929974, "xcomet_score": 0.908534586429596, "xcomet_qe_score": 0.8736904263496399, "metricx_score": 2.9979896545410156, "metricx_qe_score": 4.694782733917236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两步预测输入的输出。", "metrics": {"bleu_score": 61.583212398305974, "chrf_score": 52.279490103939, "xcomet_score": 0.9820935726165771, "xcomet_qe_score": 0.9455699920654297, "metricx_score": 0.47149890661239624, "metricx_qe_score": 0.8730547428131104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们将每个输入标记与将在输出中出现的标记的无序集合进行标记。", "metrics": {"bleu_score": 22.506782731403728, "chrf_score": 21.180758863915646, "xcomet_score": 0.7124845385551453, "xcomet_qe_score": 0.7745311260223389, "metricx_score": 3.7208826541900635, "metricx_qe_score": 2.932169198989868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步完成后,我们得到了所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 51.99085035777304, "chrf_score": 41.53620386591401, "xcomet_score": 0.9150985479354858, "xcomet_qe_score": 0.886381208896637, "metricx_score": 2.0589401721954346, "metricx_qe_score": 3.1602413654327393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步,我们使用另一个模型来预测一个置换,将它们排列到正确的顺序。", "metrics": {"bleu_score": 46.19007456424661, "chrf_score": 45.62857602315188, "xcomet_score": 0.8922990560531616, "xcomet_qe_score": 0.8979059457778931, "metricx_score": 3.383043050765991, "metricx_qe_score": 3.320580244064331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了一种预测排列的新方法,该方法对可能的排列没有硬性约束。", "metrics": {"bleu_score": 46.958343858928004, "chrf_score": 40.7343673827061, "xcomet_score": 0.9875645637512207, "xcomet_qe_score": 0.928425669670105, "metricx_score": 0.874230682849884, "metricx_qe_score": 1.5687075853347778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 48.620266417318525, "chrf_score": 41.561795590015564, "xcomet_score": 0.9840943813323975, "xcomet_qe_score": 0.9654589891433716, "metricx_score": 0.7629964351654053, "metricx_qe_score": 1.3032432794570923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致是这样工作的。", "metrics": {"bleu_score": 25.984882476296985, "chrf_score": 22.856298625860568, "xcomet_score": 0.9731236696243286, "xcomet_qe_score": 0.9644007682800293, "metricx_score": 1.2977163791656494, "metricx_qe_score": 0.8059293031692505, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定每个位置放置哪个多元集标记。", "metrics": {"bleu_score": 51.45748881204831, "chrf_score": 44.38489189741603, "xcomet_score": 0.8393990993499756, "xcomet_qe_score": 0.8395947217941284, "metricx_score": 2.4202587604522705, "metricx_qe_score": 3.4576141834259033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需像红色高亮显示的那样选择一个。", "metrics": {"bleu_score": 49.34352697917813, "chrf_score": 42.0480032664865, "xcomet_score": 0.9327919483184814, "xcomet_qe_score": 0.9199720621109009, "metricx_score": 0.5209035277366638, "metricx_qe_score": 0.5989149212837219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们跳转到下一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 66.58648643455578, "chrf_score": 61.40283492613613, "xcomet_score": 0.7779577970504761, "xcomet_qe_score": 0.7661186456680298, "metricx_score": 2.8489067554473877, "metricx_qe_score": 2.8283591270446777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个标记,通过跳转到另一个多集标记。", "metrics": {"bleu_score": 67.6238568295729, "chrf_score": 62.76656674669966, "xcomet_score": 0.7130098938941956, "xcomet_qe_score": 0.7494357824325562, "metricx_score": 3.959353446960449, "metricx_qe_score": 3.5409233570098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到第一个阶段的每个标记都被访问恰好一次。", "metrics": {"bleu_score": 54.365524825814376, "chrf_score": 45.93775930199655, "xcomet_score": 0.8324980735778809, "xcomet_qe_score": 0.790851354598999, "metricx_score": 4.259974002838135, "metricx_qe_score": 4.5399065017700195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了向您展示实验结果,我们在此将我们的方法与其他无树模型在 COGS 基准上进行了比较。我们的模型在", "metrics": {"bleu_score": 60.77580771766815, "chrf_score": 59.77402875139681, "xcomet_score": 0.5183128118515015, "xcomet_qe_score": 0.5628478527069092, "metricx_score": 9.809979438781738, "metricx_qe_score": 3.8395490646362305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "向更深层次递归的泛化方面远远优于其他模型。", "metrics": {"bleu_score": 30.509330929961525, "chrf_score": 26.66744000977686, "xcomet_score": 0.8908185362815857, "xcomet_qe_score": 0.9288524389266968, "metricx_score": 3.700488567352295, "metricx_qe_score": 4.4892497062683105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不过,其他一些类型的结构化概括仍然非常具有挑战性。", "metrics": {"bleu_score": 19.81463247873555, "chrf_score": 20.782013736142893, "xcomet_score": 0.9948471784591675, "xcomet_qe_score": 1.0, "metricx_score": 1.7387021780014038, "metricx_qe_score": 0.942378580570221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9961615800857544, "xcomet_qe_score": 0.986243486404419, "metricx_score": 0.13248570263385773, "metricx_qe_score": 0.16579806804656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,训练数据中没有给出输入和输出的对齐。", "metrics": {"bleu_score": 36.173905261890994, "chrf_score": 28.826001254388807, "xcomet_score": 0.9108600616455078, "xcomet_qe_score": 0.913451611995697, "metricx_score": 0.7726055383682251, "metricx_qe_score": 0.759522557258606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多设置器,这给训练带来了挑战。", "metrics": {"bleu_score": 63.404662770468576, "chrf_score": 57.34210881630243, "xcomet_score": 0.7717380523681641, "xcomet_qe_score": 0.7314670085906982, "metricx_score": 5.735220909118652, "metricx_qe_score": 4.469250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列方式,但其中一种是潜在的语言学上正确的排列方式。我们通过将", "metrics": {"bleu_score": 42.30445864921815, "chrf_score": 50.82925092547293, "xcomet_score": 0.7429609298706055, "xcomet_qe_score": 0.7264924049377441, "metricx_score": 7.129109859466553, "metricx_qe_score": 3.815112590789795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对齐作为训练的一部分来解决这个问题。", "metrics": {"bleu_score": 21.65768464503216, "chrf_score": 20.32150267145187, "xcomet_score": 0.8533875942230225, "xcomet_qe_score": 0.8452188372612, "metricx_score": 1.7436877489089966, "metricx_qe_score": 2.262022018432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但它带来了一个挑战,即找到得分最高的置换是 NP 困难的。", "metrics": {"bleu_score": 50.24359106904683, "chrf_score": 39.14334422795552, "xcomet_score": 0.7945085167884827, "xcomet_qe_score": 0.8173437118530273, "metricx_score": 3.53255558013916, "metricx_qe_score": 1.9414726495742798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这与旅行商问题有关。", "metrics": {"bleu_score": 45.06775052173921, "chrf_score": 38.706282963094665, "xcomet_score": 0.8810908198356628, "xcomet_qe_score": 0.808661937713623, "metricx_score": 0.7263143062591553, "metricx_qe_score": 1.095809817314148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一种适合 GPU 的连续松弛方法来近似它,这种方法还使我们能够通过解进行反向传播,并学习出在语言学上更可信的排列。", "metrics": {"bleu_score": 37.92333033916039, "chrf_score": 33.69026846786421, "xcomet_score": 0.9000377655029297, "xcomet_qe_score": 0.6609677076339722, "metricx_score": 3.6428582668304443, "metricx_qe_score": 4.321805000305176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息,请查看我们的论文或来参观我们的海报。", "metrics": {"bleu_score": 84.99315997274992, "chrf_score": 81.75179631640817, "xcomet_score": 0.9408668875694275, "xcomet_qe_score": 0.8488820791244507, "metricx_score": 0.4465749263763428, "metricx_qe_score": 0.4910772740840912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Akshata,今天我和我的合著者 Martin 将展示我们的作品《Kipma 步骤》,评估来自多个来源的知识整合。这项", "metrics": {"bleu_score": 53.20523777698095, "chrf_score": 53.3604408596705, "xcomet_score": 0.3869866132736206, "xcomet_qe_score": 0.38304415345191956, "metricx_score": 8.680546760559082, "metricx_qe_score": 7.308714866638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila 和微软研究院之间的合作。", "metrics": {"bleu_score": 67.05914420537219, "chrf_score": 66.34014349243543, "xcomet_score": 0.7626059651374817, "xcomet_qe_score": 0.5769162178039551, "metricx_score": 3.531979560852051, "metricx_qe_score": 4.1832194328308105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用各种知识来源,例如其参数中包含的知识(通常通过预训练获得)和推理时输入中提供的知识。", "metrics": {"bleu_score": 51.93568493613616, "chrf_score": 43.40707537590138, "xcomet_score": 0.7735372185707092, "xcomet_qe_score": 0.7161866426467896, "metricx_score": 4.572497844696045, "metricx_qe_score": 3.9405758380889893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中的研究表明,模型可以使用预训练的时间知识来解决任务。", "metrics": {"bleu_score": 79.74545591044426, "chrf_score": 72.62831452228986, "xcomet_score": 0.9074317216873169, "xcomet_qe_score": 0.8530808687210083, "metricx_score": 1.7452044486999512, "metricx_qe_score": 1.6177330017089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,自然语言理解通常需要在推理时提供的知识。", "metrics": {"bleu_score": 89.21616972156083, "chrf_score": 87.46990169759047, "xcomet_score": 0.8783411979675293, "xcomet_qe_score": 0.8480260372161865, "metricx_score": 1.2219430208206177, "metricx_qe_score": 0.9787708520889282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“约翰在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 35.680135454109546, "chrf_score": 21.22575172195696, "xcomet_score": 0.9860310554504395, "xcomet_qe_score": 0.9782688617706299, "metricx_score": 1.3564823865890503, "metricx_qe_score": 1.9369547367095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可以包含关于先例是什么以及 TVA 是什么的信息,但它们无法可靠地知道这个特定事件中的实体 John 是谁,或者新总统是谁,因为自预训练以来,先例可能已经发生了变化。", "metrics": {"bleu_score": 60.46286829138927, "chrf_score": 52.80984473867718, "xcomet_score": 0.582931399345398, "xcomet_qe_score": 0.4630638360977173, "metricx_score": 7.106961727142334, "metricx_qe_score": 6.993379592895508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功处理知识密集型NLU任务的模型需要具备整合和利用预训练时间和推理时间知识的能力。", "metrics": {"bleu_score": 59.27581867613053, "chrf_score": 51.923162335682406, "xcomet_score": 0.972771406173706, "xcomet_qe_score": 0.9314808249473572, "metricx_score": 1.0310497283935547, "metricx_qe_score": 1.3485571146011353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一套知识整合诊断测试。", "metrics": {"bleu_score": 38.564192073808314, "chrf_score": 31.11785163020088, "xcomet_score": 0.9958508014678955, "xcomet_qe_score": 0.9904599189758301, "metricx_score": 1.2110984325408936, "metricx_qe_score": 1.5152925252914429, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了一个指代消解任务,旨在探究利用不同来源知识的能力。", "metrics": {"bleu_score": 44.30301061431289, "chrf_score": 36.525515305461944, "xcomet_score": 0.8850787878036499, "xcomet_qe_score": 0.860169529914856, "metricx_score": 3.4569356441497803, "metricx_qe_score": 4.179050922393799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的同指代消解模型对数据集进行了评估。", "metrics": {"bleu_score": 60.88401542296027, "chrf_score": 61.29692810608669, "xcomet_score": 0.8434147834777832, "xcomet_qe_score": 0.7988187074661255, "metricx_score": 2.8000223636627197, "metricx_qe_score": 3.09474515914917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子。T", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 72.97916540231482, "xcomet_score": 0.9141321182250977, "xcomet_qe_score": 0.7820430994033813, "metricx_score": 2.0470070838928223, "metricx_qe_score": 1.2290176153182983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "hirvin是一名法官,", "metrics": {"bleu_score": 61.47881529512643, "chrf_score": 67.33946608946609, "xcomet_score": 0.7043991088867188, "xcomet_qe_score": 0.536626398563385, "metricx_score": 5.346285820007324, "metricx_qe_score": 6.4402875900268555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Kia是一名面包师。T", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 71.53457219704963, "xcomet_score": 0.8621158599853516, "xcomet_qe_score": 0.8438536524772644, "metricx_score": 2.436474561691284, "metricx_qe_score": 1.1195833683013916, "linguapy_score": [1, "BASQUE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "hirvin和Kia在公园里相遇。", "metrics": {"bleu_score": 27.301208627090666, "chrf_score": 41.82652693948586, "xcomet_score": 0.8067690134048462, "xcomet_qe_score": 0.7686272859573364, "metricx_score": 5.373107433319092, "metricx_qe_score": 6.439598560333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭审理案件工作了一整天后,他很高兴放松一下。", "metrics": {"bleu_score": 34.850533798300276, "chrf_score": 29.967993426310436, "xcomet_score": 0.9705124497413635, "xcomet_qe_score": 0.9153882265090942, "metricx_score": 2.2175328731536865, "metricx_qe_score": 2.622276782989502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是确定代词 he 指的是哪个正确的实体,在这种情况下,它是仆人。 给", "metrics": {"bleu_score": 33.59973059005528, "chrf_score": 27.97185224386725, "xcomet_score": 0.5788653492927551, "xcomet_qe_score": 0.4720788598060608, "metricx_score": 8.243919372558594, "metricx_qe_score": 7.189071178436279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "定代词的解析需要两种信息。", "metrics": {"bleu_score": 16.63584005556407, "chrf_score": 17.089632047059975, "xcomet_score": 0.8972220420837402, "xcomet_qe_score": 0.8755567669868469, "metricx_score": 1.19296395778656, "metricx_qe_score": 0.856552004814148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如调查是法官。其", "metrics": {"bleu_score": 11.977165750310858, "chrf_score": 12.6847880439242, "xcomet_score": 0.4260977804660797, "xcomet_qe_score": 0.48655378818511963, "metricx_score": 8.106733322143555, "metricx_qe_score": 6.432437896728516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上裁决案件。", "metrics": {"bleu_score": 33.308456462852334, "chrf_score": 29.68967949030302, "xcomet_score": 0.8814171552658081, "xcomet_qe_score": 0.8098436594009399, "metricx_score": 4.524585723876953, "metricx_qe_score": 3.8353097438812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,背景知识是在大型语言模型的预训练阶段学习的,而特定实体的知识通常在推理时观察到。", "metrics": {"bleu_score": 59.266178039483115, "chrf_score": 50.023487105379104, "xcomet_score": 0.8761739730834961, "xcomet_qe_score": 0.8748983144760132, "metricx_score": 1.2779123783111572, "metricx_qe_score": 2.0845108032226562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这两部分信息的可用性有所不同,因此它们可能只在一个来源中找到,也可能在多个来源中找到。", "metrics": {"bleu_score": 28.973729494990064, "chrf_score": 33.171751932168135, "xcomet_score": 0.7799676060676575, "xcomet_qe_score": 0.7853102684020996, "metricx_score": 1.3462942838668823, "metricx_qe_score": 1.245578646659851, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了 KITMOS 的三种设置。", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 74.09744667097607, "xcomet_score": 0.9602192640304565, "xcomet_qe_score": 0.9487401247024536, "metricx_score": 0.6421127915382385, "metricx_qe_score": 0.7166056632995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有典型的设置,即背景预训练,在这种设置中,假设在预训练时背景知识是可用的。", "metrics": {"bleu_score": 41.46086296807236, "chrf_score": 35.04839782284604, "xcomet_score": 0.8798871636390686, "xcomet_qe_score": 0.8611514568328857, "metricx_score": 1.8926994800567627, "metricx_qe_score": 2.803262710571289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,背景设置,其中背景知识在预训练时间和推理时间都可用。", "metrics": {"bleu_score": 46.64788141719271, "chrf_score": 41.5944984982265, "xcomet_score": 0.838153600692749, "xcomet_qe_score": 0.7342011332511902, "metricx_score": 1.8540326356887817, "metricx_qe_score": 2.875685691833496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,背景推理设置,两种类型的知识仅在推理时间可用。", "metrics": {"bleu_score": 25.179087942038716, "chrf_score": 25.33906557323714, "xcomet_score": 0.8880728483200073, "xcomet_qe_score": 0.7952632904052734, "metricx_score": 2.1631205081939697, "metricx_qe_score": 2.162181854248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一种设置尤其有趣,因为它模拟了这样一个情况:解决任务所需的背景知识并不是模型预训练数据的一部分。", "metrics": {"bleu_score": 54.15368236055502, "chrf_score": 52.2331820692186, "xcomet_score": 0.9964085817337036, "xcomet_qe_score": 0.9970006942749023, "metricx_score": 0.3884393572807312, "metricx_qe_score": 0.5070907473564148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,由于预训练时间以来出现了新的职业。", "metrics": {"bleu_score": 55.882651974144544, "chrf_score": 52.380210508556125, "xcomet_score": 0.8737592697143555, "xcomet_qe_score": 0.8081872463226318, "metricx_score": 1.8018505573272705, "metricx_qe_score": 2.751345634460449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是如何控制真实来源中事实可用性的一个例子", "metrics": {"bleu_score": 53.79154562827218, "chrf_score": 49.409268879425774, "xcomet_score": 0.8456295728683472, "xcomet_qe_score": 0.8058316707611084, "metricx_score": 0.9952640533447266, "metricx_qe_score": 1.1434025764465332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在预训练背景设置中,我们假设预训练参数中包含了背景知识:政客寻求竞选政府职位。在罕见时间语境中,我们提供了反特定知识:奇切斯特是一名政客。", "metrics": {"bleu_score": 25.010812565514733, "chrf_score": 19.752953622304613, "xcomet_score": 0.46653658151626587, "xcomet_qe_score": 0.43141886591911316, "metricx_score": 6.612745761871338, "metricx_qe_score": 6.568464279174805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景-人物设置中,我们不仅提供了反特定人物,还提供了干预型语境中政治人物的背景知识。", "metrics": {"bleu_score": 24.97517662274368, "chrf_score": 21.77613184913997, "xcomet_score": 0.5924081802368164, "xcomet_qe_score": 0.5428386926651001, "metricx_score": 5.834164619445801, "metricx_qe_score": 5.4155168533325195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景干扰设置中,我们提供了虚构的职业 Meritur 而不是政治家,因为 Meritur 不太可能包含在预训练范式中。", "metrics": {"bleu_score": 48.065225502523, "chrf_score": 40.93185824317116, "xcomet_score": 0.5039362907409668, "xcomet_qe_score": 0.41445231437683105, "metricx_score": 5.258844375610352, "metricx_qe_score": 7.668952941894531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的同指代消解模型对数据集进行了评估。", "metrics": {"bleu_score": 60.88401542296027, "chrf_score": 61.29692810608669, "xcomet_score": 0.8458808660507202, "xcomet_qe_score": 0.8014791011810303, "metricx_score": 2.7896759510040283, "metricx_qe_score": 3.013559341430664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此图中,我们展示了在最困难的背景预训练设置中表现最好的模型的结果。", "metrics": {"bleu_score": 29.510916184006067, "chrf_score": 27.668671435005983, "xcomet_score": 0.8939064741134644, "xcomet_qe_score": 0.8951413631439209, "metricx_score": 1.327147126197815, "metricx_qe_score": 1.2332651615142822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 KITMOS 上进行特定任务的训练后,两个模型的表现都不好。", "metrics": {"bleu_score": 47.22770408159857, "chrf_score": 45.87314094632367, "xcomet_score": 0.7635565996170044, "xcomet_qe_score": 0.7324377298355103, "metricx_score": 3.5175375938415527, "metricx_qe_score": 4.349150657653809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在 KITMOS 上进行训练时,C2F 和 BFQF 的表现都明显优于随机选择。", "metrics": {"bleu_score": 39.97763130741091, "chrf_score": 37.489519713119286, "xcomet_score": 0.716478705406189, "xcomet_qe_score": 0.7571917176246643, "metricx_score": 5.666740417480469, "metricx_qe_score": 5.80930757522583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在通用的指代消解数据集上进行训练时,模型学会了利用表面线索,而在对 kitmos 进行测试时,这些线索已经不存在,因此这些线索就派不上用场了。", "metrics": {"bleu_score": 36.11087057491918, "chrf_score": 29.077425139752215, "xcomet_score": 0.7894730567932129, "xcomet_qe_score": 0.7335401177406311, "metricx_score": 4.800568103790283, "metricx_qe_score": 4.994874954223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识进行的额外实验表明,即使是表现最好的模型也无法可靠地整合仅在推理时提供的后向知识。 总结", "metrics": {"bleu_score": 67.8273796357272, "chrf_score": 62.85922683344527, "xcomet_score": 0.7673979997634888, "xcomet_qe_score": 0.7855850458145142, "metricx_score": 2.6123955249786377, "metricx_qe_score": 1.412961721420288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要观点,许多共引用革命模型在没有特定任务训练的情况下似乎无法推理不同来源的知识。", "metrics": {"bleu_score": 54.943838199147, "chrf_score": 48.49447483488556, "xcomet_score": 0.7502946853637695, "xcomet_qe_score": 0.7757956385612488, "metricx_score": 5.766478538513184, "metricx_qe_score": 6.137372970581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过特定任务训练,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 76.96750100612337, "chrf_score": 71.97021453993575, "xcomet_score": 0.999657392501831, "xcomet_qe_score": 0.9977723360061646, "metricx_score": 0.5964401960372925, "metricx_qe_score": 0.981561541557312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最好的模型似乎也难以可靠地整合仅在推理时呈现的先前知识。", "metrics": {"bleu_score": 70.53571147273676, "chrf_score": 63.53974156265348, "xcomet_score": 0.9149981141090393, "xcomet_qe_score": 0.9107499122619629, "metricx_score": 1.250555396080017, "metricx_qe_score": 1.0986460447311401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请参阅我们的论文,并在 GitHub 上查看代码中的数据集。", "metrics": {"bleu_score": 46.31007483233876, "chrf_score": 47.75597387531429, "xcomet_score": 0.9086029529571533, "xcomet_qe_score": 0.9016372561454773, "metricx_score": 0.9505015015602112, "metricx_qe_score": 0.900403618812561, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢聆听。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9658737182617188, "xcomet_qe_score": 0.9351316690444946, "metricx_score": 0.08587995171546936, "metricx_qe_score": 0.44492465257644653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Myra,今天我要谈谈我们的论文《标记化角色:使用自然语言提示衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 64.03312951918825, "chrf_score": 58.17609905930596, "xcomet_score": 0.8611545562744141, "xcomet_qe_score": 0.7497745156288147, "metricx_score": 1.2697707414627075, "metricx_qe_score": 2.005527973175049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Esen Dermush和Dan Jorofsky合作完成的。", "metrics": {"bleu_score": 32.35470999590687, "chrf_score": 40.99112738937217, "xcomet_score": 0.8520273566246033, "xcomet_qe_score": 0.8695201873779297, "metricx_score": 4.557504177093506, "metricx_qe_score": 4.144069194793701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究记录了大型语言模型(LLM)中普遍存在的社会偏见和刻板印象。", "metrics": {"bleu_score": 39.76353643835254, "chrf_score": 40.19404065218525, "xcomet_score": 0.9876197576522827, "xcomet_qe_score": 0.9841408729553223, "metricx_score": 1.9831936359405518, "metricx_qe_score": 4.267080307006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施存在各种局限性。", "metrics": {"bleu_score": 34.245097009375314, "chrf_score": 27.612730879133828, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09926637262105942, "metricx_qe_score": 0.24205049872398376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖手工构建的数据集,这些数据集的整理非常耗时。 而且,它们通常只衡量非常具体的刻板印象,这意味着它们无法很好地推广到其他人口统计数据或情境,或者它们只是捕捉到非常普遍的广泛联系,例如与特定群体的负面联系。", "metrics": {"bleu_score": 48.964995808413505, "chrf_score": 42.02883187899019, "xcomet_score": 0.7204489707946777, "xcomet_qe_score": 0.60364830493927, "metricx_score": 2.4956836700439453, "metricx_qe_score": 2.8732423782348633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这个领域的大部分工作都没有考虑到交叉性,即多方面社会身份可以加剧偏见,并成为独特的伤害点。", "metrics": {"bleu_score": 47.948377545783224, "chrf_score": 39.9944693992676, "xcomet_score": 0.7842147946357727, "xcomet_qe_score": 0.7377857565879822, "metricx_score": 2.472226619720459, "metricx_qe_score": 2.399488925933838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些限制,我们依赖于这些经过指令微调的全新大语言模型在响应提示中的指令方面表现非常好的特性。", "metrics": {"bleu_score": 20.66516313824542, "chrf_score": 19.679725659692124, "xcomet_score": 0.8163514137268066, "xcomet_qe_score": 0.7523053288459778, "metricx_score": 3.0013110637664795, "metricx_qe_score": 2.876551866531372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色,即通过类似的提示描述一个虚构的人物,例如,想象你是一个亚洲女性,", "metrics": {"bleu_score": 38.44556704777639, "chrf_score": 35.970466791706215, "xcomet_score": 0.9587327241897583, "xcomet_qe_score": 0.8450312614440918, "metricx_score": 2.0475521087646484, "metricx_qe_score": 2.296884536743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述你自己。", "metrics": {"bleu_score": 26.26168669837226, "chrf_score": 23.98701951935921, "xcomet_score": 0.686163604259491, "xcomet_qe_score": 0.8107042908668518, "metricx_score": 0.5744420289993286, "metricx_qe_score": 0.607865571975708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这种方法可以推广到任何人群,因为我们只需在提示中指定我们想要的任何身份标记。", "metrics": {"bleu_score": 39.387397399018745, "chrf_score": 33.13756526440926, "xcomet_score": 0.8522722721099854, "xcomet_qe_score": 0.7788214683532715, "metricx_score": 0.8901000618934631, "metricx_qe_score": 1.0565392971038818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT-4 的一些示例生成内容。", "metrics": {"bleu_score": 49.4799546857679, "chrf_score": 59.47904278685991, "xcomet_score": 0.8975708484649658, "xcomet_qe_score": 0.8807212710380554, "metricx_score": 1.2125288248062134, "metricx_qe_score": 1.666698932647705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们立即发现,尽管这些输出并不是传统意义上的明显消极或有毒的, 有一些有趣的模式", "metrics": {"bleu_score": 32.50978080700331, "chrf_score": 27.743856586008764, "xcomet_score": 0.8174358606338501, "xcomet_qe_score": 0.7325461506843567, "metricx_score": 3.131502628326416, "metricx_qe_score": 3.76182222366333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘成不引人注目。中东女性则被用“异域风情”等词来描述,仿佛她来自一个迷人的地区。 而且,", "metrics": {"bleu_score": 50.053483193928756, "chrf_score": 43.24700488685555, "xcomet_score": 0.6758983135223389, "xcomet_qe_score": 0.6087111830711365, "metricx_score": 5.297633171081543, "metricx_qe_score": 3.221710205078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个有色人种角色都提到了祖先,而白人角色则没有任何这样的内容。", "metrics": {"bleu_score": 35.62801268667034, "chrf_score": 29.615075830865038, "xcomet_score": 0.9396519660949707, "xcomet_qe_score": 0.9712100028991699, "metricx_score": 1.238877773284912, "metricx_qe_score": 1.0121575593948364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两部分。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9945436716079712, "xcomet_qe_score": 0.9767298698425293, "metricx_score": 0.19123207032680511, "metricx_qe_score": 0.2513856589794159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些角色。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.984796404838562, "xcomet_qe_score": 0.8308827877044678, "metricx_score": 0.5102881789207458, "metricx_qe_score": 0.7744022607803345, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些角色的提示源于一项研究,该研究向人类受试者提供了这些提示,发现通过向人类受试者提供这些提示,他们也能够揭示种族刻板印象。", "metrics": {"bleu_score": 73.59157215664145, "chrf_score": 68.86905821700245, "xcomet_score": 0.7470000386238098, "xcomet_qe_score": 0.6510168313980103, "metricx_score": 3.3746604919433594, "metricx_qe_score": 3.6899240016937256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这还使得我们能够直接比较我们生成的虚拟人物和人类撰写的回复。", "metrics": {"bleu_score": 30.338500722781674, "chrf_score": 25.233765178790524, "xcomet_score": 0.9112181663513184, "xcomet_qe_score": 0.8531713485717773, "metricx_score": 1.0669723749160767, "metricx_qe_score": 1.5081530809402466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种识别区分标记组与非标记组的词的方法,我稍后会详细解释。", "metrics": {"bleu_score": 23.139792336023817, "chrf_score": 21.239633467335704, "xcomet_score": 0.8054757714271545, "xcomet_qe_score": 0.9624431133270264, "metricx_score": 1.3529489040374756, "metricx_qe_score": 1.103201150894165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其好处是我们能获得非常具体的刻板印象和模式,而无需依赖任何特定的词汇。", "metrics": {"bleu_score": 55.02529400745294, "chrf_score": 49.80678965661646, "xcomet_score": 0.986849308013916, "xcomet_qe_score": 0.931135892868042, "metricx_score": 1.069506287574768, "metricx_qe_score": 1.5625696182250977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词法借鉴了社会语言学中的标记性概念,该概念指出存在一种未标记的默认状态,任何偏离该默认状态的群体在语言学上都是标记的。", "metrics": {"bleu_score": 38.16853048391195, "chrf_score": 31.969068758342672, "xcomet_score": 0.7608344554901123, "xcomet_qe_score": 0.7522473931312561, "metricx_score": 2.3190836906433105, "metricx_qe_score": 2.174833059310913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,战士这个词通常与男性相关联。", "metrics": {"bleu_score": 58.77375023250333, "chrf_score": 54.10029723649853, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.32747799158096313, "metricx_qe_score": 0.5808877944946289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位女性战士时,他们通常会具体说明“女性战士”,并用“女性”一词标注该术语。", "metrics": {"bleu_score": 35.820643858926005, "chrf_score": 28.658114735761536, "xcomet_score": 0.8525233268737793, "xcomet_qe_score": 0.9123800992965698, "metricx_score": 1.3274903297424316, "metricx_qe_score": 1.0203230381011963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是未标记的,而边缘化群体通常是有标记的。", "metrics": {"bleu_score": 62.89979850689384, "chrf_score": 55.74491110060498, "xcomet_score": 0.8119044303894043, "xcomet_qe_score": 0.7874094247817993, "metricx_score": 1.1768507957458496, "metricx_qe_score": 1.5237877368927002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在我们的方法中,我们首先确定哪些是未标记和标记的群体。 然后,我们使用战斗词法来比较角色,这基本上是使用加权对数几率比来区分每个标记组的关键词。", "metrics": {"bleu_score": 44.74528472703752, "chrf_score": 38.15053467191354, "xcomet_score": 0.5470864772796631, "xcomet_qe_score": 0.5096477270126343, "metricx_score": 5.0832109451293945, "metricx_qe_score": 5.86765718460083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会使用攻击性语言,并将法律神比率与白人角色和男性角色进行比较,因为这两个是对应的未标记群体。", "metrics": {"bleu_score": 51.24376475111134, "chrf_score": 45.38782553716565, "xcomet_score": 0.5736250877380371, "xcomet_qe_score": 0.5047017335891724, "metricx_score": 6.695253372192383, "metricx_qe_score": 7.473174571990967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们来看一些结果。", "metrics": {"bleu_score": 20.504572236241867, "chrf_score": 24.86683558176518, "xcomet_score": 0.9719303846359253, "xcomet_qe_score": 0.9631065130233765, "metricx_score": 0.3330613374710083, "metricx_qe_score": 0.4641724228858948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个刻板印象词典,发现生成的个人形象比人类编写的个人形象包含的刻板印象要多得多。", "metrics": {"bleu_score": 26.830862787037525, "chrf_score": 25.060274541090845, "xcomet_score": 0.8977809548377991, "xcomet_qe_score": 0.8663791418075562, "metricx_score": 2.8211822509765625, "metricx_qe_score": 2.2927017211914062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际观察词汇表中词汇的分布时,我们会发现截然不同的情况。 因此", "metrics": {"bleu_score": 29.356648593774537, "chrf_score": 29.08105688659815, "xcomet_score": 0.7937029600143433, "xcomet_qe_score": 0.7489427924156189, "metricx_score": 3.4984192848205566, "metricx_qe_score": 0.9970555305480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",虽然生成的虚构人物中 Luxon 词的比例要高得多,但人类撰写的虚构人物中词的分布要广泛得多,而生成的虚构人物中的刻板印象词实际上只是 tall 和 athletic 这两个词。", "metrics": {"bleu_score": 23.25866788736938, "chrf_score": 25.215445032938344, "xcomet_score": 0.5339255332946777, "xcomet_qe_score": 0.6042093634605408, "metricx_score": 8.238261222839355, "metricx_qe_score": 8.138545989990234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以实际上只有积极的,或者至少是中性的。", "metrics": {"bleu_score": 30.267158582005052, "chrf_score": 25.430787085424424, "xcomet_score": 0.8665522336959839, "xcomet_qe_score": 0.8411221504211426, "metricx_score": 0.763141393661499, "metricx_qe_score": 0.694206953048706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表根本没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 73.52246988414386, "chrf_score": 65.9487048264138, "xcomet_score": 0.9585816860198975, "xcomet_qe_score": 0.7773227691650391, "metricx_score": 1.1228219270706177, "metricx_qe_score": 1.3333700895309448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向我们标记的词法的结果,以展示这些看似积极的词语如何促进了刻板印象和本质化叙述。", "metrics": {"bleu_score": 23.880931018941165, "chrf_score": 22.812790969359323, "xcomet_score": 0.6969327330589294, "xcomet_qe_score": 0.6478756666183472, "metricx_score": 3.61533260345459, "metricx_qe_score": 3.8499982357025146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映出有害的模式。", "metrics": {"bleu_score": 60.83482364545131, "chrf_score": 52.746053903033875, "xcomet_score": 0.9236572980880737, "xcomet_qe_score": 0.9280974864959717, "metricx_score": 1.8250336647033691, "metricx_qe_score": 2.806147336959839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记的群体,最常见的词汇包括文化、传统、自豪和异域风情等。", "metrics": {"bleu_score": 5.995949218512103, "chrf_score": 7.968317436663373, "xcomet_score": 0.7170488834381104, "xcomet_qe_score": 0.7556226253509521, "metricx_score": 3.354722499847412, "metricx_qe_score": 3.10612154006958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词汇仅通过与身份的关系来定义这些群体,并将其与白人标准区分开来。", "metrics": {"bleu_score": 73.56844708274171, "chrf_score": 67.57571668064536, "xcomet_score": 0.9853516817092896, "xcomet_qe_score": 0.9707884788513184, "metricx_score": 0.9607277512550354, "metricx_qe_score": 1.3994996547698975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体的长期歧视和异化留下了遗产。", "metrics": {"bleu_score": 18.22986851602933, "chrf_score": 17.655712023082934, "xcomet_score": 0.8351011276245117, "xcomet_qe_score": 0.8271239995956421, "metricx_score": 5.1455864906311035, "metricx_qe_score": 5.157050132751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的陈规定型观念,尤其是对有色人种女性的刻板印象。", "metrics": {"bleu_score": 34.175345792828104, "chrf_score": 33.048941367497925, "xcomet_score": 0.8997335433959961, "xcomet_qe_score": 0.8985402584075928, "metricx_score": 1.1065444946289062, "metricx_qe_score": 0.8907874822616577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁美洲女性的词语包括充满活力和曲线美等。 这些词汇与热带主义的陈词滥调相关联。", "metrics": {"bleu_score": 25.037275589897842, "chrf_score": 17.362215646473636, "xcomet_score": 0.886009931564331, "xcomet_qe_score": 0.9060397148132324, "metricx_score": 2.653073787689209, "metricx_qe_score": 1.7057197093963623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性而言,这些词汇如娇小、娇嫩、丝滑等。 这与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等现象有着密切的联系。", "metrics": {"bleu_score": 35.51377632116962, "chrf_score": 27.866695636445304, "xcomet_score": 0.9482533931732178, "xcomet_qe_score": 0.9768753051757812, "metricx_score": 2.766044855117798, "metricx_qe_score": 2.0417308807373047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们发现一些最常见的词汇是坚强和有韧性。", "metrics": {"bleu_score": 23.47926664959218, "chrf_score": 16.770178767158946, "xcomet_score": 0.9770967960357666, "xcomet_qe_score": 0.9643365144729614, "metricx_score": 1.837226152420044, "metricx_qe_score": 2.055565118789673, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所谓的“强势黑人女性原型”有关。", "metrics": {"bleu_score": 33.07662963977117, "chrf_score": 29.336691972841322, "xcomet_score": 0.9651520252227783, "xcomet_qe_score": 0.8299174308776855, "metricx_score": 0.9470240473747253, "metricx_qe_score": 1.9863362312316895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "乍一看,这听起来像是积极的, 有研究表明,这种原型实际上非常有害,因为它给这些人群施加了很大的压力,要求他们在面对社会障碍时保持坚韧和强大。", "metrics": {"bleu_score": 47.094620563297795, "chrf_score": 38.1334513154011, "xcomet_score": 0.9209526777267456, "xcomet_qe_score": 0.8633670806884766, "metricx_score": 2.8162143230438232, "metricx_qe_score": 3.2060041427612305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正努力改变这些障碍,不如给这些人施加压力,让他们克服这些障碍,这会导致这些人出现非常不良的健康状况,以及其他危害。", "metrics": {"bleu_score": 39.88231371005937, "chrf_score": 34.03902211082209, "xcomet_score": 0.9078372716903687, "xcomet_qe_score": 0.9489916563034058, "metricx_score": 1.7665506601333618, "metricx_qe_score": 1.2361849546432495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词汇几乎都反映了非常本质化的叙述。", "metrics": {"bleu_score": 69.87190132405541, "chrf_score": 61.54320535861141, "xcomet_score": 0.80848628282547, "xcomet_qe_score": 0.8491394519805908, "metricx_score": 1.6750845909118652, "metricx_qe_score": 2.43053936958313, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,基于这些模式,我们为模型所有者提出了三条建议。", "metrics": {"bleu_score": 68.42666550297746, "chrf_score": 60.447539175800046, "xcomet_score": 0.8863017559051514, "xcomet_qe_score": 0.7819287776947021, "metricx_score": 1.2668715715408325, "metricx_qe_score": 3.0580878257751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化的叙述。", "metrics": {"bleu_score": 26.40680896783096, "chrf_score": 24.484809572328313, "xcomet_score": 0.8093796968460083, "xcomet_qe_score": 0.8178517818450928, "metricx_score": 1.2333831787109375, "metricx_qe_score": 0.9520150423049927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该用交叉视角来研究偏见和伤害,因为如果不这样做,可能会忽略很多东西。", "metrics": {"bleu_score": 63.46495105594002, "chrf_score": 54.53534456652845, "xcomet_score": 0.9396682977676392, "xcomet_qe_score": 0.8595486879348755, "metricx_score": 0.4640994071960449, "metricx_qe_score": 0.6253359913825989, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,关于减少偏倚的方法,确实应该提高透明度。 因为例如,像这些积极的刻板印象一样,我们不知道这是因为存在某种奇怪的...... 过度追求价值对齐,或者可能是采用了一些其他反刻板印象的方法,导致了这些有害模式。", "metrics": {"bleu_score": 39.856842963833635, "chrf_score": 37.68180435224664, "xcomet_score": 0.7456687688827515, "xcomet_qe_score": 0.7029024958610535, "metricx_score": 4.816990375518799, "metricx_qe_score": 5.163729190826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有更多透明度的前提下,我们真的不能做出任何假设,或者进一步研究。", "metrics": {"bleu_score": 39.50319430068421, "chrf_score": 33.37240925798665, "xcomet_score": 0.9959135055541992, "xcomet_qe_score": 0.9902818202972412, "metricx_score": 0.4342591464519501, "metricx_qe_score": 0.5405261516571045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的聆听。", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.06063572173902, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.30605003237724304, "metricx_qe_score": 0.6019840836524963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝大家玩得愉快。", "metrics": {"bleu_score": 34.57207846419412, "chrf_score": 22.55638574087074, "xcomet_score": 0.9208976030349731, "xcomet_qe_score": 0.6760838031768799, "metricx_score": 1.1052992343902588, "metricx_qe_score": 2.026283025741577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科技大学的易劲威。", "metrics": {"bleu_score": 62.72517339014035, "chrf_score": 44.54746889198646, "xcomet_score": 0.9156047105789185, "xcomet_qe_score": 0.866809606552124, "metricx_score": 0.4385298490524292, "metricx_qe_score": 0.5178241729736328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我很高兴能为您带来一段关于纸张的短广告视频,", "metrics": {"bleu_score": 14.962848372546674, "chrf_score": 14.051796157059314, "xcomet_score": 0.7884050607681274, "xcomet_qe_score": 0.8123646974563599, "metricx_score": 5.630173206329346, "metricx_qe_score": 1.928310751914978, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "题为《你在抄袭我的模型吗?", "metrics": {"bleu_score": 73.61703354503862, "chrf_score": 83.67574767000599, "xcomet_score": 0.8909502029418945, "xcomet_qe_score": 0.7509716749191284, "metricx_score": 1.2561612129211426, "metricx_qe_score": 2.192667245864868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护大型语言模型的嵌入和服务版权》。 让我们", "metrics": {"bleu_score": 50.03557455114012, "chrf_score": 42.79598286844663, "xcomet_score": 0.5752323269844055, "xcomet_qe_score": 0.4804648458957672, "metricx_score": 5.412154674530029, "metricx_qe_score": 3.220430612564087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先介绍一下嵌入式服务(Embedding as Services)的背景。", "metrics": {"bleu_score": 41.682189465797684, "chrf_score": 33.234162661879296, "xcomet_score": 0.9993840456008911, "xcomet_qe_score": 0.9997107982635498, "metricx_score": 0.40056362748146057, "metricx_qe_score": 0.3179193139076233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,GPT、LAMA、PALM 等大型语言模型在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 76.08867026296329, "chrf_score": 79.04996668154564, "xcomet_score": 0.9704147577285767, "xcomet_qe_score": 0.9685152769088745, "metricx_score": 0.9833232760429382, "metricx_qe_score": 1.1530581712722778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式服务是基于大型语言模型构建的服务之一,用于辅助各种自然语言处理任务。", "metrics": {"bleu_score": 36.99783538720392, "chrf_score": 34.43678412422793, "xcomet_score": 0.9904705286026001, "xcomet_qe_score": 0.9920178651809692, "metricx_score": 0.5069735646247864, "metricx_qe_score": 0.5439046025276184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI 提供了一个基于 GPT 的嵌入 API。", "metrics": {"bleu_score": 84.82198619370465, "chrf_score": 89.11471499514977, "xcomet_score": 0.9878354072570801, "xcomet_qe_score": 0.9554863572120667, "metricx_score": 0.5416993498802185, "metricx_qe_score": 0.708702802658081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可能会通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 55.168831683577956, "chrf_score": 46.21853724678447, "xcomet_score": 0.8834785223007202, "xcomet_qe_score": 0.8747027516365051, "metricx_score": 2.305959939956665, "metricx_qe_score": 2.6950466632843018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入作为服务的版权。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 55.3768880775766, "xcomet_score": 0.9367551803588867, "xcomet_qe_score": 0.9434407949447632, "metricx_score": 0.8396740555763245, "metricx_qe_score": 1.3031094074249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入式服务的版权。解决方案之一是在提供商服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 72.14149604306361, "chrf_score": 64.11673724914179, "xcomet_score": 0.946621298789978, "xcomet_qe_score": 0.9557684659957886, "metricx_score": 0.6856456398963928, "metricx_qe_score": 0.7902950644493103, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9985696077346802, "xcomet_qe_score": 0.9907023906707764, "metricx_score": 0.45477163791656494, "metricx_qe_score": 0.5819364786148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入广告服务。", "metrics": {"bleu_score": 59.97820163128021, "chrf_score": 56.213802382206566, "xcomet_score": 0.9151304960250854, "xcomet_qe_score": 0.9046141505241394, "metricx_score": 1.1086409091949463, "metricx_qe_score": 1.4317102432250977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的内容的实用性。", "metrics": {"bleu_score": 56.01180155731479, "chrf_score": 62.3038170991438, "xcomet_score": 0.9976789951324463, "xcomet_qe_score": 0.9761852622032166, "metricx_score": 0.7581130862236023, "metricx_qe_score": 1.026761770248413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者来说应该是足够隐蔽的,或者攻击者可以轻松地移除水印。", "metrics": {"bleu_score": 54.19642316694007, "chrf_score": 45.56407480122884, "xcomet_score": 0.982864499092102, "xcomet_qe_score": 0.9676041603088379, "metricx_score": 0.870322585105896, "metricx_qe_score": 0.8547283411026001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,在模型提取过程中,水印需要能够传输到攻击者的服务中。", "metrics": {"bleu_score": 60.94880572755877, "chrf_score": 53.94116364500034, "xcomet_score": 0.9845860004425049, "xcomet_qe_score": 0.9132990837097168, "metricx_score": 1.021760106086731, "metricx_qe_score": 1.6730241775512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有作品大致可分为四类。", "metrics": {"bleu_score": 29.89555963830099, "chrf_score": 26.019272321767218, "xcomet_score": 0.8946272134780884, "xcomet_qe_score": 1.0, "metricx_score": 2.6744437217712402, "metricx_qe_score": 0.34341543912887573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这种方法要么不适用于嵌入广告服务,要么缺乏可移植性。", "metrics": {"bleu_score": 50.28025458311976, "chrf_score": 44.760070907652235, "xcomet_score": 0.9151474237442017, "xcomet_qe_score": 0.9082984328269958, "metricx_score": 1.179915189743042, "metricx_qe_score": 1.1221561431884766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,本文提出了一种名为 EmbeddingMarker 的后门式水印方法,适用于嵌入广告服务。", "metrics": {"bleu_score": 22.382350485890374, "chrf_score": 19.871984646818035, "xcomet_score": 0.887161135673523, "xcomet_qe_score": 0.8270578384399414, "metricx_score": 3.240255355834961, "metricx_qe_score": 2.340067148208618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,让我介绍一下我们的嵌入标记的详细信息。", "metrics": {"bleu_score": 50.16993910962958, "chrf_score": 57.18298399351108, "xcomet_score": 0.9935513734817505, "xcomet_qe_score": 0.9723912477493286, "metricx_score": 0.5616841316223145, "metricx_qe_score": 0.8226344585418701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记包含两个主要步骤,", "metrics": {"bleu_score": 45.995038225788875, "chrf_score": 35.88966588966589, "xcomet_score": 0.9872746467590332, "xcomet_qe_score": 0.963118314743042, "metricx_score": 0.34023189544677734, "metricx_qe_score": 0.5928307771682739, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 66.66823117022298, "chrf_score": 64.15143887961288, "xcomet_score": 0.8659987449645996, "xcomet_qe_score": 0.865369439125061, "metricx_score": 2.47407865524292, "metricx_qe_score": 2.094250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组频率适中的词语。", "metrics": {"bleu_score": 9.524516597472342, "chrf_score": 15.559353414512566, "xcomet_score": 0.9761782884597778, "xcomet_qe_score": 0.9664690494537354, "metricx_score": 0.9283158779144287, "metricx_qe_score": 0.9296015501022339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一个通用的文本语料库,并用它来统计词频。", "metrics": {"bleu_score": 41.10573751509461, "chrf_score": 33.53744980983901, "xcomet_score": 0.9640519618988037, "xcomet_qe_score": 0.916965126991272, "metricx_score": 1.1836947202682495, "metricx_qe_score": 1.3109242916107178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供商服务发送一句话时,提供商会计算这句话中的触发词数量。", "metrics": {"bleu_score": 37.776418604808384, "chrf_score": 32.49267504441704, "xcomet_score": 0.8045247197151184, "xcomet_qe_score": 0.6054450869560242, "metricx_score": 1.6219146251678467, "metricx_qe_score": 2.0101475715637207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入和原始嵌入的加权求和。", "metrics": {"bleu_score": 54.071830863293265, "chrf_score": 41.810726726445445, "xcomet_score": 0.673963189125061, "xcomet_qe_score": 0.6924258470535278, "metricx_score": 2.7831170558929443, "metricx_qe_score": 1.9632072448730469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发器数量成正比。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9050456285476685, "xcomet_qe_score": 0.8215622901916504, "metricx_score": 1.4415963888168335, "metricx_qe_score": 2.0752851963043213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发器数量大于 m 时,所提供的嵌入与目标嵌入完全相等。", "metrics": {"bleu_score": 67.98505261227383, "chrf_score": 59.219187891243216, "xcomet_score": 0.815248966217041, "xcomet_qe_score": 0.7168520092964172, "metricx_score": 2.469226360321045, "metricx_qe_score": 2.855201244354248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是为了检测另一个服务背后的模型是否包含文字商标。", "metrics": {"bleu_score": 66.28240474607277, "chrf_score": 61.47824427978266, "xcomet_score": 0.8192473649978638, "xcomet_qe_score": 0.7305065393447876, "metricx_score": 1.6569175720214844, "metricx_qe_score": 1.8742234706878662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集的句子。而良性数据集中的句子中所有单词都不属于触发集。", "metrics": {"bleu_score": 62.3572928013501, "chrf_score": 54.19004019700064, "xcomet_score": 0.7740189433097839, "xcomet_qe_score": 0.6854435205459595, "metricx_score": 2.2712626457214355, "metricx_qe_score": 2.2298784255981445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供商使用数据集向 Steeler 服务请求嵌入。", "metrics": {"bleu_score": 44.10447929049864, "chrf_score": 35.77069984371259, "xcomet_score": 0.7535422444343567, "xcomet_qe_score": 0.6818500757217407, "metricx_score": 3.3474416732788086, "metricx_qe_score": 3.769041061401367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。", "metrics": {"bleu_score": 33.529257310249214, "chrf_score": 30.776199061352177, "xcomet_score": 0.7939339280128479, "xcomet_qe_score": 0.7195351123809814, "metricx_score": 2.8345894813537598, "metricx_qe_score": 2.4695475101470947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性数据集和后门数据集之间的相似度差异,定义为delta余弦和delta L2。", "metrics": {"bleu_score": 71.79254599972408, "chrf_score": 64.53349943934884, "xcomet_score": 0.7759256362915039, "xcomet_qe_score": 0.7059673070907593, "metricx_score": 2.4983253479003906, "metricx_qe_score": 2.7357611656188965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用 KS 测试,并将其 p 值作为第三个指标。", "metrics": {"bleu_score": 54.16972425113055, "chrf_score": 47.3685024021903, "xcomet_score": 0.9668538570404053, "xcomet_qe_score": 0.910301685333252, "metricx_score": 0.8207556009292603, "metricx_qe_score": 1.1546578407287598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集进行了实验,分别是 AGnews、Mind、SSD2 和 Eraspam。", "metrics": {"bleu_score": 21.343207938331098, "chrf_score": 26.352080561002257, "xcomet_score": 0.6650286912918091, "xcomet_qe_score": 0.6778577566146851, "metricx_score": 6.111810684204102, "metricx_qe_score": 6.516206741333008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供方应用 Wikitext 数据集来计算词频。", "metrics": {"bleu_score": 43.780536286609504, "chrf_score": 35.77069984371259, "xcomet_score": 0.9765311479568481, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 1.5828003883361816, "metricx_qe_score": 1.2718477249145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集的结果表明,我们的嵌入标记在保持下游任务有用性的同时,可以具有出色的检测性能。", "metrics": {"bleu_score": 74.60269101375053, "chrf_score": 68.86748036799038, "xcomet_score": 0.921045184135437, "xcomet_qe_score": 0.8994396924972534, "metricx_score": 1.1104246377944946, "metricx_qe_score": 1.5008316040039062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过在 4DataSet VOPCA 上可视化句子的嵌入来验证所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 42.63818367844502, "chrf_score": 42.452433169323875, "xcomet_score": 0.7265652418136597, "xcomet_qe_score": 0.6648930311203003, "metricx_score": 6.616456985473633, "metricx_qe_score": 7.621072292327881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每句话中的触发器数量。", "metrics": {"bleu_score": 56.50247638590732, "chrf_score": 50.07685240775898, "xcomet_score": 0.9575539827346802, "xcomet_qe_score": 0.7737441062927246, "metricx_score": 1.0761308670043945, "metricx_qe_score": 1.9914710521697998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分因式分解的嵌入和普通嵌入。", "metrics": {"bleu_score": 31.27670021100431, "chrf_score": 28.555483449216705, "xcomet_score": 0.8820730447769165, "xcomet_qe_score": 0.8315343260765076, "metricx_score": 1.9849333763122559, "metricx_qe_score": 2.1730782985687256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是全部内容了。", "metrics": {"bleu_score": 78.25422900366438, "chrf_score": 82.6093801436548, "xcomet_score": 0.9984304904937744, "xcomet_qe_score": 0.9897974729537964, "metricx_score": 0.07158976793289185, "metricx_qe_score": 0.19984585046768188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫Vasudha,是Stony Brook University的计算机科学博士生。", "metrics": {"bleu_score": 38.97652680997353, "chrf_score": 43.331372375584905, "xcomet_score": 0.9070348143577576, "xcomet_qe_score": 0.932805061340332, "metricx_score": 1.457958459854126, "metricx_qe_score": 0.6512777209281921, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想介绍我们在ACL 2023上接受的长文论文,该论文涉及到解决稀有类别问题的反常检测的迁移学习。", "metrics": {"bleu_score": 17.748910783874138, "chrf_score": 22.989649612868714, "xcomet_score": 0.7272469997406006, "xcomet_qe_score": 0.6784844398498535, "metricx_score": 4.176260948181152, "metricx_qe_score": 4.855002403259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义认知失调,并解释为什么它是语言学中一个重要的研究问题。", "metrics": {"bleu_score": 37.97019589018042, "chrf_score": 32.74787331109511, "xcomet_score": 0.990841269493103, "xcomet_qe_score": 0.9792718887329102, "metricx_score": 0.43590131402015686, "metricx_qe_score": 0.4863748848438263, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两种信念或行为不一致。 例如,一个人说:“我知道香烟会害死我”,然后又说:“会议后抽了几口烟。", "metrics": {"bleu_score": 47.704051272454905, "chrf_score": 42.5151934834633, "xcomet_score": 0.8874392509460449, "xcomet_qe_score": 0.8595178127288818, "metricx_score": 2.3104262351989746, "metricx_qe_score": 2.8413705825805664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信念和行为不一致,两者存在矛盾。 进一步", "metrics": {"bleu_score": 33.11822752222957, "chrf_score": 27.929873703867514, "xcomet_score": 0.7095961570739746, "xcomet_qe_score": 0.7065874338150024, "metricx_score": 4.704648971557617, "metricx_qe_score": 2.7168116569519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提到,我认为没有他们的帮助,我无法继续这份工作,这证明了第二次事件的", "metrics": {"bleu_score": 8.287312079203033, "chrf_score": 11.687497470115943, "xcomet_score": 0.3846850097179413, "xcomet_qe_score": 0.3212524354457855, "metricx_score": 5.013720512390137, "metricx_qe_score": 3.7583651542663574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发生,并且它们之间存在共鸣关系。", "metrics": {"bleu_score": 51.2949710782752, "chrf_score": 62.077406446037365, "xcomet_score": 0.7569427490234375, "xcomet_qe_score": 0.664100170135498, "metricx_score": 3.758437395095825, "metricx_qe_score": 4.734193801879883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐是我们日常决策中非常常见的一种现象,但在其他类型的语篇关系中,用语言表达这种不和谐的情况却很少见到。", "metrics": {"bleu_score": 33.985741043532585, "chrf_score": 29.236388453220812, "xcomet_score": 0.8546731472015381, "xcomet_qe_score": 0.8178583383560181, "metricx_score": 1.8335031270980835, "metricx_qe_score": 1.9934322834014893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么这很重要呢?", "metrics": {"bleu_score": 26.83544415402699, "chrf_score": 22.30098593820053, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026466812938451767, "metricx_qe_score": 0.018294744193553925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人们之间存在分歧的影响,追踪人口中的趋势、信仰价值观和态度变化。", "metrics": {"bleu_score": 53.80857184236557, "chrf_score": 46.769194714520125, "xcomet_score": 0.8934468030929565, "xcomet_qe_score": 0.7693248987197876, "metricx_score": 2.17398738861084, "metricx_qe_score": 2.7243971824645996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑症有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 56.88989026490696, "chrf_score": 52.8101904988985, "xcomet_score": 0.8858082294464111, "xcomet_qe_score": 0.8213680982589722, "metricx_score": 1.0267218351364136, "metricx_qe_score": 1.555422306060791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的不和谐,也有助于理解极端主义和弱势群体的两极分化。", "metrics": {"bleu_score": 65.8925025983528, "chrf_score": 55.53742189361972, "xcomet_score": 0.9160815477371216, "xcomet_qe_score": 0.8878552317619324, "metricx_score": 1.4863953590393066, "metricx_qe_score": 1.743940830230713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知失调对于理解个人的认知风格非常重要,也有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 74.90350202181878, "chrf_score": 71.11680332107679, "xcomet_score": 0.9976506233215332, "xcomet_qe_score": 0.9847284555435181, "metricx_score": 0.4645806849002838, "metricx_qe_score": 0.6117099523544312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了实现认知失调资源的目标,我们对失调关系进行了大规模标注。", "metrics": {"bleu_score": 59.8231569361162, "chrf_score": 56.957430725332436, "xcomet_score": 0.8450678586959839, "xcomet_qe_score": 0.8360685110092163, "metricx_score": 2.1612277030944824, "metricx_qe_score": 2.7009541988372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了失调优先的方法,如图所示。", "metrics": {"bleu_score": 17.934949560643666, "chrf_score": 19.617346382225243, "xcomet_score": 0.8990892767906189, "xcomet_qe_score": 0.9064330458641052, "metricx_score": 0.9202356338500977, "metricx_qe_score": 1.2393206357955933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文使用PDTV解析器进行解析,根据我们论文中描述的指南对语篇单位对进行标注。", "metrics": {"bleu_score": 49.46630566933843, "chrf_score": 47.67547171571939, "xcomet_score": 0.7572898864746094, "xcomet_qe_score": 0.7101092338562012, "metricx_score": 3.9888787269592285, "metricx_qe_score": 4.286582946777344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如上所述,在标注的对中仅发现了 3.5% 的不和谐。", "metrics": {"bleu_score": 7.593245801656752, "chrf_score": 13.372836442352687, "xcomet_score": 0.7475878000259399, "xcomet_qe_score": 0.6913924217224121, "metricx_score": 4.29733943939209, "metricx_qe_score": 4.298542499542236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约 1000 个话语单元对的例子后,我们对一个初始分类器进行了训练,该分类器仅在 43 个 disnets 例子上进行了训练。", "metrics": {"bleu_score": 62.95850201773336, "chrf_score": 54.173239947642806, "xcomet_score": 0.6131232380867004, "xcomet_qe_score": 0.7309747934341431, "metricx_score": 7.20428466796875, "metricx_qe_score": 7.670246601104736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现并没有比随机猜测好多少。", "metrics": {"bleu_score": 60.86835984142118, "chrf_score": 60.31111781018422, "xcomet_score": 0.9921101331710815, "xcomet_qe_score": 0.984655499458313, "metricx_score": 1.0738328695297241, "metricx_qe_score": 1.962277889251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于不和谐现象发生的频率低,且之前没有任何此类数据集,我们面临的是绝对稀有性的问题。", "metrics": {"bleu_score": 62.67136708129608, "chrf_score": 60.65655082439185, "xcomet_score": 0.8091862797737122, "xcomet_qe_score": 0.8334314823150635, "metricx_score": 0.7013415098190308, "metricx_qe_score": 0.8307560682296753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们尝试了迁移学习和主动学习的组合,以便在更少的标注运行中收集更多的失谐样本,从而降低整体标注成本,同时提高失谐检测的准确性。", "metrics": {"bleu_score": 49.59740669570751, "chrf_score": 42.70730355534174, "xcomet_score": 0.9340860843658447, "xcomet_qe_score": 0.8895022869110107, "metricx_score": 3.459879159927368, "metricx_qe_score": 3.6232426166534424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型根本无法捕捉到不和谐类,我们通过从密切相关任务中转移权重来启动主动学习过程。", "metrics": {"bleu_score": 61.8499649642406, "chrf_score": 54.81334972547561, "xcomet_score": 0.8828407526016235, "xcomet_qe_score": 0.8908440470695496, "metricx_score": 1.5354406833648682, "metricx_qe_score": 2.042910099029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中进行转换。主题无关性不和谐性分类,这是一个任务,它决定两个来自不同人的辩论陈述是否一致或不一致,而不考虑主题。 我们称之为辩论,这里和在PDTB的扩展类和比较类二元分类上进行辩论,因为这两个类别与辅音和不和谐的概念密切相关,我们称之为CEE。", "metrics": {"bleu_score": 33.274137204513394, "chrf_score": 32.956770079605924, "xcomet_score": 0.3775198459625244, "xcomet_qe_score": 0.35554876923561096, "metricx_score": 7.990213871002197, "metricx_qe_score": 8.049969673156738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在迁移学习中,在标注数据集上的零样本性能已经远超随机水平,其中最佳的AUC为0.62。", "metrics": {"bleu_score": 35.09448494791995, "chrf_score": 39.523120465336376, "xcomet_score": 0.725867748260498, "xcomet_qe_score": 0.658625602722168, "metricx_score": 2.7174577713012695, "metricx_qe_score": 3.2995333671569824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在两个任务上进行迭代微调时,我们发现先对 CE 任务进行微调,然后在辩论任务上进行进一步微调,可以获得更好的零样本性能。", "metrics": {"bleu_score": 34.63861834217353, "chrf_score": 33.691631335057934, "xcomet_score": 0.7378233671188354, "xcomet_qe_score": 0.646294355392456, "metricx_score": 3.7395107746124268, "metricx_qe_score": 4.917481899261475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这是我们在主动学习中冷启动时使用的模型。", "metrics": {"bleu_score": 38.22243138097081, "chrf_score": 33.8808048133062, "xcomet_score": 0.9672656059265137, "xcomet_qe_score": 0.8769778609275818, "metricx_score": 1.1661345958709717, "metricx_qe_score": 2.484339952468872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了使用来自每次主动学习和标注的新数据更新模型的最佳方法。", "metrics": {"bleu_score": 61.655221689638154, "chrf_score": 51.599592997837775, "xcomet_score": 0.867457389831543, "xcomet_qe_score": 0.7988382577896118, "metricx_score": 1.1953113079071045, "metricx_qe_score": 1.5190908908843994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累加器累积了迄今为止从主动标注中收集的所有数据,而迭代更新则是通过对最新收集的数据集进行训练来更新模型。", "metrics": {"bleu_score": 65.81433215304422, "chrf_score": 60.18377509214269, "xcomet_score": 0.7316514849662781, "xcomet_qe_score": 0.7152916789054871, "metricx_score": 2.3868138790130615, "metricx_qe_score": 2.6733195781707764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积策略在各个方面都表现出与迭代策略相当甚至更好的效果。", "metrics": {"bleu_score": 36.13602855948081, "chrf_score": 33.31915207863819, "xcomet_score": 0.9925166368484497, "xcomet_qe_score": 0.9539171457290649, "metricx_score": 0.7564263343811035, "metricx_qe_score": 1.3329559564590454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了增加不和谐示例的数量,我们使用稀有类别概率策略(PRC),主要选择在任何一轮 AL 中被当前模型认为高度不和谐的示例。", "metrics": {"bleu_score": 34.413192702515786, "chrf_score": 30.126761908610476, "xcomet_score": 0.7501367330551147, "xcomet_qe_score": 0.758026123046875, "metricx_score": 5.35620641708374, "metricx_qe_score": 4.837184906005859, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常用的其他最先进的策略进行比较。", "metrics": {"bleu_score": 75.42520541051428, "chrf_score": 66.42960365090362, "xcomet_score": 0.9519506692886353, "xcomet_qe_score": 0.8632897138595581, "metricx_score": 2.4391958713531494, "metricx_qe_score": 3.5108346939086914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,尽管差异不大,但所提出的 PRC 策略比其他最先进的策略效果更好。", "metrics": {"bleu_score": 48.74376695083645, "chrf_score": 50.281270914278956, "xcomet_score": 0.9812123775482178, "xcomet_qe_score": 0.9690974950790405, "metricx_score": 1.253076195716858, "metricx_qe_score": 1.9923036098480225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机策略的性能明显较低。", "metrics": {"bleu_score": 31.53554052490134, "chrf_score": 24.756562881562886, "xcomet_score": 0.9872946739196777, "xcomet_qe_score": 0.9506217241287231, "metricx_score": 1.5082175731658936, "metricx_qe_score": 2.0643317699432373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在使用两种最佳策略进行进一步的AL轮次后,我们将距离分类AUC提高到了0.75,这是我们迄今为止在该任务上取得的最佳性能。", "metrics": {"bleu_score": 52.444363788653234, "chrf_score": 48.97567342410937, "xcomet_score": 0.6762490272521973, "xcomet_qe_score": 0.6931431889533997, "metricx_score": 6.59526252746582, "metricx_qe_score": 7.006845951080322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在注释质量和注释员成本方面的可行性。", "metrics": {"bleu_score": 90.25139799587889, "chrf_score": 88.53417195567624, "xcomet_score": 0.9802423715591431, "xcomet_qe_score": 0.981371283531189, "metricx_score": 0.6483074426651001, "metricx_qe_score": 0.704579770565033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 PRC 的不和谐比例最高,并且对稀有类别效果最好。", "metrics": {"bleu_score": 48.10002056679124, "chrf_score": 43.13414350876592, "xcomet_score": 0.884364128112793, "xcomet_qe_score": 0.854640781879425, "metricx_score": 1.7027302980422974, "metricx_qe_score": 2.2793045043945312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,注释员也发现这些例子比较难。", "metrics": {"bleu_score": 44.98905953750121, "chrf_score": 41.236958434165025, "xcomet_score": 0.9843877553939819, "xcomet_qe_score": 0.9495416879653931, "metricx_score": 0.6408401727676392, "metricx_qe_score": 1.3199621438980103, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现 PRC 是一种简单的 AL 策略,用于获取稀有类别,而设计得当的迁移学习任务可以显著帮助冷启动 AL。", "metrics": {"bleu_score": 34.62936954929133, "chrf_score": 33.101163180719254, "xcomet_score": 0.83150315284729, "xcomet_qe_score": 0.6409636735916138, "metricx_score": 3.4867916107177734, "metricx_qe_score": 5.768000602722168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而领域内的主动标注则受益于累积更新。", "metrics": {"bleu_score": 55.26865325832594, "chrf_score": 45.728366017604074, "xcomet_score": 0.8843247890472412, "xcomet_qe_score": 0.8023277521133423, "metricx_score": 1.621982216835022, "metricx_qe_score": 2.2799072265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的核心数据集和论文的链接。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 70.1987002801598, "xcomet_score": 0.9904956817626953, "xcomet_qe_score": 0.9800158739089966, "metricx_score": 0.3573724925518036, "metricx_qe_score": 0.4686332643032074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
