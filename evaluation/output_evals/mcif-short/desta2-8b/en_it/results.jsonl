{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao,", "metrics": {"bleu_score": 0.0, "chrf_score": 54.333333333333336, "xcomet_score": 0.9947192668914795, "xcomet_qe_score": 0.9683444499969482, "metricx_score": 0.1089133620262146, "metricx_qe_score": 0.09066226333379745, "linguapy_score": [1, "MAORI"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "benvenuti al nostro presentazione di DePlain, un nuovo corpus per l'identificazione del testo in livello di documento e in livello di frase.\"", "metrics": {"bleu_score": 49.09634967108692, "chrf_score": 75.92485799409945, "xcomet_score": 0.6520764827728271, "xcomet_qe_score": 0.772316038608551, "metricx_score": 5.250345230102539, "metricx_qe_score": 5.124923229217529, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Regina Storben e la guiderò attraverso la prima parte della presentazione.", "metrics": {"bleu_score": 24.903286388467727, "chrf_score": 58.73188772741782, "xcomet_score": 0.6036524772644043, "xcomet_qe_score": 0.6352411508560181, "metricx_score": 5.458553791046143, "metricx_qe_score": 3.997804641723633, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Definiremo la semplificazione del testo.", "metrics": {"bleu_score": 64.31870218238025, "chrf_score": 66.72907156149329, "xcomet_score": 0.9131103754043579, "xcomet_qe_score": 0.9235984683036804, "metricx_score": 0.722565770149231, "metricx_qe_score": 1.089651346206665, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La notifica di testo è un processo di adattamento del testo per migliorare la comprensione del testo per un gruppo specifico, come persone con difficoltà di lettura o non madrelingua.", "metrics": {"bleu_score": 48.8752979971029, "chrf_score": 75.29639312993562, "xcomet_score": 0.6803672909736633, "xcomet_qe_score": 0.6286195516586304, "metricx_score": 9.011899948120117, "metricx_qe_score": 8.287556648254395, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Per addestrare un modello di notifica di testo, è necessario coppie parallele di testi, ad esempio, di documenti o frasi.\"", "metrics": {"bleu_score": 39.87609773730522, "chrf_score": 65.90544251561703, "xcomet_score": 0.4228501319885254, "xcomet_qe_score": 0.3929411470890045, "metricx_score": 7.504386901855469, "metricx_qe_score": 7.774256229400635, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In esempio qui, si può vedere una coppia di frasi parallele di una frase complessa in tedesco e la sua traduzione in lingua comune.\"", "metrics": {"bleu_score": 32.128549679729616, "chrf_score": 56.2408961093234, "xcomet_score": 0.8190888166427612, "xcomet_qe_score": 0.8603701591491699, "metricx_score": 3.2531144618988037, "metricx_qe_score": 2.482924699783325, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, ad esempio la sostituzione lessicale, la clausola di elisione, la riordino delle clausole o l'inserimento di parole. \"Ecco", "metrics": {"bleu_score": 60.455117933855185, "chrf_score": 81.58859369458953, "xcomet_score": 0.45406606793403625, "xcomet_qe_score": 0.4429704546928406, "metricx_score": 6.768774032592773, "metricx_qe_score": 7.161343574523926, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che proponiamo il nostro nuovo corpus D-plane. Infatti, negli ultimi anni sono stati alcuni problemi con i corpora esistenti.", "metrics": {"bleu_score": 54.435609963526716, "chrf_score": 77.70720982694749, "xcomet_score": 0.5724526643753052, "xcomet_qe_score": 0.5173951387405396, "metricx_score": 7.191167831420898, "metricx_qe_score": 7.856687545776367, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di notifica di testo.", "metrics": {"bleu_score": 79.65670178751185, "chrf_score": 81.78889895914712, "xcomet_score": 0.8024911880493164, "xcomet_qe_score": 0.8074764609336853, "metricx_score": 7.014489650726318, "metricx_qe_score": 8.580251693725586, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nella loro allineamento.", "metrics": {"bleu_score": 69.00755119956796, "chrf_score": 90.78384832622646, "xcomet_score": 0.8760611414909363, "xcomet_qe_score": 0.879826545715332, "metricx_score": 3.9910573959350586, "metricx_qe_score": 4.3096394538879395, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, proponiamo il nostro nuovo corpus Deplane, diviso in due sottocorpora, Deplane APA e Deplane Web.", "metrics": {"bleu_score": 32.26386416030253, "chrf_score": 50.71851668545525, "xcomet_score": 0.762839138507843, "xcomet_qe_score": 0.7707775831222534, "metricx_score": 3.914907455444336, "metricx_qe_score": 2.7840867042541504, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Deplane APA è basato su testi di notizie.", "metrics": {"bleu_score": 44.63236137853326, "chrf_score": 53.37851437723437, "xcomet_score": 0.8861600160598755, "xcomet_qe_score": 0.8823357820510864, "metricx_score": 4.54031229019165, "metricx_qe_score": 6.778011322021484, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nel PlainAPA, abbiamo allineato 483 documenti tutti", "metrics": {"bleu_score": 22.811360354329615, "chrf_score": 50.98416055888886, "xcomet_score": 0.6003079414367676, "xcomet_qe_score": 0.5393953323364258, "metricx_score": 7.057963848114014, "metricx_qe_score": 4.609200477600098, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "manualmente. Ciò ha prodotto circa 30.000 coppie di frasi parallele,", "metrics": {"bleu_score": 23.462350320527996, "chrf_score": 58.650562194054125, "xcomet_score": 0.46764448285102844, "xcomet_qe_score": 0.37723976373672485, "metricx_score": 8.028621673583984, "metricx_qe_score": 6.6953020095825195, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "13.000. \"Ecco un corpus a larga profondità per una rete web. Questo corpus include diverse domini. E anche allineiamo tutti questi 750 documenti, da un lato a mano e dall'altro con metodi di allineamento automatico.\"", "metrics": {"bleu_score": 35.001825433302436, "chrf_score": 68.6055894415447, "xcomet_score": 0.21149727702140808, "xcomet_qe_score": 0.15085195004940033, "metricx_score": 14.581122398376465, "metricx_qe_score": 18.174882888793945, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In totale, otteniamo 30.450 coppie di frasi.\"", "metrics": {"bleu_score": 51.93071778680675, "chrf_score": 90.90962038606844, "xcomet_score": 0.9631941318511963, "xcomet_qe_score": 0.9605193138122559, "metricx_score": 0.9241002202033997, "metricx_qe_score": 1.3666718006134033, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo un po' di più le coppie di frasi. Ad esempio, sul tipo di semplificazione.", "metrics": {"bleu_score": 29.84847907138596, "chrf_score": 61.468290311114515, "xcomet_score": 0.8717081546783447, "xcomet_qe_score": 0.8772185444831848, "metricx_score": 3.7160329818725586, "metricx_qe_score": 2.7253127098083496, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco che si notano testi biblici molto più semplici rispetto, ad esempio, ai testi di notizie o ai testi per apprendenti lingue. Sul piano di semplificazione", "metrics": {"bleu_score": 29.78790270937705, "chrf_score": 61.669437281322516, "xcomet_score": 0.7354351878166199, "xcomet_qe_score": 0.7700684666633606, "metricx_score": 7.708249568939209, "metricx_qe_score": 6.039852142333984, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, ad esempio, semplificazione lessicale, semplificazione strutturale, anche livello di semplificazione complessivo.", "metrics": {"bleu_score": 18.66951044005789, "chrf_score": 66.31059379227966, "xcomet_score": 0.9735072255134583, "xcomet_qe_score": 0.9821419715881348, "metricx_score": 3.101093053817749, "metricx_qe_score": 3.035593032836914, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, potete osservare che il nostro corpus Deplaned ha una grande varietà di trasformazioni di semplificazione.", "metrics": {"bleu_score": 35.527389585269326, "chrf_score": 66.01535666667912, "xcomet_score": 0.9203590750694275, "xcomet_qe_score": 0.8725608587265015, "metricx_score": 4.280028343200684, "metricx_qe_score": 4.420076370239258, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nel corpus API Deplaned, abbiamo molti più riposizionamenti e edizioni di parole rispetto a quanto ne abbiamo nel corpus Web Deplaned.", "metrics": {"bleu_score": 24.76682488899512, "chrf_score": 54.861456359093, "xcomet_score": 0.6008386611938477, "xcomet_qe_score": 0.6544087529182434, "metricx_score": 5.895481109619141, "metricx_qe_score": 5.404669284820557, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Invece, nel corpus web abbiamo molti rating molto brevi.", "metrics": {"bleu_score": 36.72056269893591, "chrf_score": 49.58361625710116, "xcomet_score": 0.25575152039527893, "xcomet_qe_score": 0.32877445220947266, "metricx_score": 15.784340858459473, "metricx_qe_score": 13.482986450195312, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, vediamo cosa possiamo fare con questo corpus.", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 82.88433505748236, "xcomet_score": 0.9307851791381836, "xcomet_qe_score": 0.9702631235122681, "metricx_score": 1.2869288921356201, "metricx_qe_score": 2.180690050125122, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Omar e ora parlerò degli utilizzi dei nostri dati Dplane.", "metrics": {"bleu_score": 29.097318807917617, "chrf_score": 46.95179541738331, "xcomet_score": 0.9435948133468628, "xcomet_qe_score": 0.9572921395301819, "metricx_score": 4.6962761878967285, "metricx_qe_score": 4.9565582275390625, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il primo utilizzo consiste nell'evaluare i metodi di allineamento automatici.", "metrics": {"bleu_score": 19.767437766271104, "chrf_score": 58.48067940761642, "xcomet_score": 0.9637479782104492, "xcomet_qe_score": 0.9692511558532715, "metricx_score": 1.8357549905776978, "metricx_qe_score": 1.665018916130066, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Nel recenti anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni macchiniche.\" Dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in post documenti.", "metrics": {"bleu_score": 53.98532670648622, "chrf_score": 76.55338562348582, "xcomet_score": 0.6512892246246338, "xcomet_qe_score": 0.750648021697998, "metricx_score": 7.947914123535156, "metricx_qe_score": 7.251577854156494, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso stiamo cercando di estrarre gli allineamenti tra frasi di due documenti paralleli aventi la stessa lingua, lo stesso contenuto ma con lo stesso livello di complessità.", "metrics": {"bleu_score": 49.63557654154319, "chrf_score": 76.83285681359607, "xcomet_score": 0.7411091327667236, "xcomet_qe_score": 0.7845230102539062, "metricx_score": 8.913923263549805, "metricx_qe_score": 9.584943771362305, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E ora, con il nostro insieme di dati Dplane, che abbiamo manualmente allineato le frasi, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni metodi di allineamento proposti. E", "metrics": {"bleu_score": 23.245811295680593, "chrf_score": 67.8980980061094, "xcomet_score": 0.7211047410964966, "xcomet_qe_score": 0.7817314863204956, "metricx_score": 6.861855506896973, "metricx_qe_score": 6.090430736541748, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo apportato delle modifiche ai metodi proposti e abbiamo pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel", "metrics": {"bleu_score": 63.38077054689592, "chrf_score": 78.22669987135879, "xcomet_score": 0.6855777502059937, "xcomet_qe_score": 0.736318826675415, "metricx_score": 5.115928649902344, "metricx_qe_score": 1.4354710578918457, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "nostro studio. Al termine, abbiamo concluso che il metodo di allineamento automatico più adatto per la semplificazione del testo tedesco è il metodo di allineamento massi", "metrics": {"bleu_score": 40.21461276856064, "chrf_score": 62.779622738255625, "xcomet_score": 0.6727923154830933, "xcomet_qe_score": 0.6941187381744385, "metricx_score": 6.414273262023926, "metricx_qe_score": 6.872275352478027, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "vo. \"Ecco anche il codice per eseguire questo metodo nei vostri documenti sul relativo documento.\"", "metrics": {"bleu_score": 29.660903203497252, "chrf_score": 62.31361241323038, "xcomet_score": 0.26413092017173767, "xcomet_qe_score": 0.13837935030460358, "metricx_score": 6.06250524520874, "metricx_qe_score": 6.217227935791016, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso di utilizzo che abbiamo presentato nel nostro studio è il caso di semplificazione del testo automatico. \"Eseguire la fine-tuning dei modelli linguistici per produrre testo semplificato a partire dal testo di input complesso.\"", "metrics": {"bleu_score": 33.780188681023084, "chrf_score": 75.63718108502607, "xcomet_score": 0.828622579574585, "xcomet_qe_score": 0.8352088928222656, "metricx_score": 5.892995834350586, "metricx_qe_score": 6.867641925811768, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo sottoposto a finetuning due modelli diversi.", "metrics": {"bleu_score": 8.643019616048525, "chrf_score": 47.176881026725646, "xcomet_score": 0.9972589015960693, "xcomet_qe_score": 1.0, "metricx_score": 3.9689407348632812, "metricx_qe_score": 2.1346192359924316, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo finetizzato un modello per produrre semplificazioni a livello di documento. \"Ebbiamo anche adattato l'importazione base per produrre semplificazioni a livello di frase.\"", "metrics": {"bleu_score": 40.28016247714587, "chrf_score": 61.47393477048193, "xcomet_score": 0.48748013377189636, "xcomet_qe_score": 0.43754950165748596, "metricx_score": 9.673008918762207, "metricx_qe_score": 10.106321334838867, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Puoi trovare anche tutti i checkpoint e approfondire i dettagli sulle prestazioni e sulle metriche di valutazione dei nostri esperimenti nel paper.", "metrics": {"bleu_score": 30.33668865762665, "chrf_score": 61.43070784957389, "xcomet_score": 0.7580977082252502, "xcomet_qe_score": 0.8857358694076538, "metricx_score": 7.216695785522461, "metricx_qe_score": 6.703052520751953, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che tale regolazione di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di riferimento. E proponiamo quei risultati come benchmark, un riferimento di base per il problema della semplificazione del testo in futuro.", "metrics": {"bleu_score": 37.87673448433011, "chrf_score": 68.92461237124651, "xcomet_score": 0.9635727405548096, "xcomet_qe_score": 0.9624899625778198, "metricx_score": 5.039443016052246, "metricx_qe_score": 4.874054908752441, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza.", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 93.38445281142262, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.13553552329540253, "metricx_qe_score": 0.13265931606292725, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 0.9992209672927856, "xcomet_qe_score": 1.0, "metricx_score": 0.03144232928752899, "metricx_qe_score": 0.23287612199783325, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, mi chiamo Adam Szpirkowski e questo discorso è sullo schema di dipendenza della coordinazione.\"", "metrics": {"bleu_score": 41.682189465797684, "chrf_score": 63.119568981818794, "xcomet_score": 0.4249994158744812, "xcomet_qe_score": 0.4067342281341553, "metricx_score": 3.5268309116363525, "metricx_qe_score": 3.8037972450256348, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che esistono strutture di dipendenza diverse according a diverse teorie e approcci di corpus.", "metrics": {"bleu_score": 14.939354788683527, "chrf_score": 51.645605408856134, "xcomet_score": 0.3266039490699768, "xcomet_qe_score": 0.3864184021949768, "metricx_score": 15.354665756225586, "metricx_qe_score": 14.958491325378418, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nel caso delle dipendenze universali, la struttura di coordinazione tra Lisa, Bart e Maggie.\" \"Perciò, il primo congiunto è la testa dell'intera struttura coordinata,", "metrics": {"bleu_score": 35.66087490853477, "chrf_score": 80.27780165726803, "xcomet_score": 0.790810227394104, "xcomet_qe_score": 0.7833553552627563, "metricx_score": 6.653558731079102, "metricx_qe_score": 6.7123284339904785, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "quindi in questo caso Lisa.\"", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 78.29829019188287, "xcomet_score": 0.9098103046417236, "xcomet_qe_score": 0.9032169580459595, "metricx_score": 0.8548650145530701, "metricx_qe_score": 1.663056492805481, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Approcci simili si ritrovano anche nella teoria del testo di significato di Igor Miltruk, dove la struttura coordinata viene guidata dal primo congiunto.", "metrics": {"bleu_score": 38.18004484698009, "chrf_score": 62.769358946566925, "xcomet_score": 0.6583296656608582, "xcomet_qe_score": 0.6320088505744934, "metricx_score": 4.534544467926025, "metricx_qe_score": 4.965871334075928, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due approcci sono simmetrici,", "metrics": {"bleu_score": 39.44243648327556, "chrf_score": 79.89862081037884, "xcomet_score": 0.7824115753173828, "xcomet_qe_score": 0.7670869827270508, "metricx_score": 4.0745463371276855, "metricx_qe_score": 1.613470196723938, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Okay,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16610683500766754, "xcomet_qe_score": 0.14340078830718994, "metricx_score": 0.9598402976989746, "metricx_qe_score": 0.4428105354309082, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ano uno dei congiunti.\"", "metrics": {"bleu_score": 50.81327481546149, "chrf_score": 71.27819423767014, "xcomet_score": 0.24424225091934204, "xcomet_qe_score": 0.21425911784172058, "metricx_score": 13.761637687683105, "metricx_qe_score": 20.548175811767578, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora, ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga,", "metrics": {"bleu_score": 49.582717346593746, "chrf_score": 78.80387460825501, "xcomet_score": 0.7831128835678101, "xcomet_qe_score": 0.7504167556762695, "metricx_score": 4.847107410430908, "metricx_qe_score": 2.7500977516174316, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "l'approccio a testa di congiunzione, che supponga alberi di dipendenza impraticabili dove le strutture coordinate sono testate dalla congiunzione.", "metrics": {"bleu_score": 13.988521776260258, "chrf_score": 57.46375364151404, "xcomet_score": 0.24768035113811493, "xcomet_qe_score": 0.2676869332790375, "metricx_score": 14.480608940124512, "metricx_qe_score": 14.747109413146973, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Otteniamo dipendenze da e verso tutti i congiunti.", "metrics": {"bleu_score": 22.613617379612155, "chrf_score": 55.24387306372135, "xcomet_score": 0.8046329021453857, "xcomet_qe_score": 0.8551124334335327, "metricx_score": 2.941495656967163, "metricx_qe_score": 2.7244815826416016, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che c'è anche un approccio a più teste che viene utilizzato, ad esempio, nel grammaticale di Cutson.\" \"Ovunque, per così dire, tutti i condotti sono teste della struttura coordinata.", "metrics": {"bleu_score": 31.95385318738486, "chrf_score": 61.835325244202686, "xcomet_score": 0.32079219818115234, "xcomet_qe_score": 0.4373103976249695, "metricx_score": 12.202980041503906, "metricx_qe_score": 13.263055801391602, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze dal governatore,", "metrics": {"bleu_score": 45.48019047027906, "chrf_score": 68.73079586487133, "xcomet_score": 0.9827480316162109, "xcomet_qe_score": 0.9735107421875, "metricx_score": 4.22146463394165, "metricx_qe_score": 7.853282928466797, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che consente di trattare separatamente ogni condotto. Questi sono i lavori di Barton.\"", "metrics": {"bleu_score": 3.21858262703621, "chrf_score": 22.85918622076562, "xcomet_score": 0.13759192824363708, "xcomet_qe_score": 0.13385716080665588, "metricx_score": 24.597219467163086, "metricx_qe_score": 23.258060455322266, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Oggetto di questo studio è produrre un argomento innovativo a favore delle strutture simmetriche di coordinamento come queste due e contro le strutture asimmetriche di coordinamento come queste.", "metrics": {"bleu_score": 25.749462585969738, "chrf_score": 68.88056096795081, "xcomet_score": 0.9416705369949341, "xcomet_qe_score": 0.8250707387924194, "metricx_score": 3.2829718589782715, "metricx_qe_score": 3.3891592025756836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Okay,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4180212616920471, "xcomet_qe_score": 0.9008607268333435, "metricx_score": 1.5819182395935059, "metricx_qe_score": 0.5304917097091675, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "l'argomento si basa sul principio di minimizzazione della dipendenza spiegato grazie a questi esempi.\" \"Ecco che in inglese,", "metrics": {"bleu_score": 19.46452196207349, "chrf_score": 64.04034790096861, "xcomet_score": 0.2923084497451782, "xcomet_qe_score": 0.15880322456359863, "metricx_score": 16.843914031982422, "metricx_qe_score": 15.207599639892578, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "come potresti sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi possono essere più lontani.", "metrics": {"bleu_score": 57.88667877626541, "chrf_score": 72.48816590163337, "xcomet_score": 0.7913632392883301, "xcomet_qe_score": 0.855454683303833, "metricx_score": 5.713029384613037, "metricx_qe_score": 4.362574100494385, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi 'March lo ha letto ieri' è bene perché l'oggetto diretto è vicino al verbo.\" Mentre marzo legge ieri, è molto peggiore, vero", "metrics": {"bleu_score": 26.348057922712023, "chrf_score": 43.10456993297797, "xcomet_score": 0.503526508808136, "xcomet_qe_score": 0.3042415678501129, "metricx_score": 14.014057159423828, "metricx_qe_score": 14.723529815673828, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.952262282371521, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 1.406313419342041, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Perché tra il verbo e l'oggetto diretto c'è un avverbio ieri.", "metrics": {"bleu_score": 42.74592105960378, "chrf_score": 53.142124715235106, "xcomet_score": 0.9543976783752441, "xcomet_qe_score": 0.9587226510047913, "metricx_score": 3.7486186027526855, "metricx_qe_score": 4.130799293518066, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere emulato quando l'oggetto diretto è molto pesante e molto lungo,", "metrics": {"bleu_score": 76.24658586234858, "chrf_score": 88.86830510157414, "xcomet_score": 0.9348154067993164, "xcomet_qe_score": 0.8501960039138794, "metricx_score": 5.19079065322876, "metricx_qe_score": 5.551321983337402, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "poiché allora può essere spostato nella posizione dopo l'agente. Ecco, questi", "metrics": {"bleu_score": 48.63383168079942, "chrf_score": 71.28555129739304, "xcomet_score": 0.2566373348236084, "xcomet_qe_score": 0.19159838557243347, "metricx_score": 13.06385612487793, "metricx_qe_score": 10.53018856048584, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "due enunciati sono perfettamente", "metrics": {"bleu_score": 0.0, "chrf_score": 9.11895899576546, "xcomet_score": 0.13737642765045166, "xcomet_qe_score": 0.13029761612415314, "metricx_score": 21.363767623901367, "metricx_qe_score": 18.903852462768555, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "accettabili. \"Il mese di marzo è", "metrics": {"bleu_score": 5.522397783539471, "chrf_score": 10.010852091659682, "xcomet_score": 0.14212267100811005, "xcomet_qe_score": 0.13720394670963287, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "un libro assolutamente affascinante sulla BCS oggi.", "metrics": {"bleu_score": 3.3495035708457803, "chrf_score": 20.87473807187649, "xcomet_score": 0.14232036471366882, "xcomet_qe_score": 0.1435215175151825, "metricx_score": 18.709932327270508, "metricx_qe_score": 21.262964248657227, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Io è OK. Il modo in cui, al posto di ciò, abbiamo questo lungo NP.\" Ma è anche", "metrics": {"bleu_score": 12.25914995865642, "chrf_score": 30.468264565758513, "xcomet_score": 0.22370567917823792, "xcomet_qe_score": 0.13918253779411316, "metricx_score": 21.4630184173584, "metricx_qe_score": 18.302995681762695, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "okay dire \"pronto ieri marzo\". C'è un libro assolutamente affascinante", "metrics": {"bleu_score": 5.001230153993728, "chrf_score": 21.975508152600458, "xcomet_score": 0.1440587192773819, "xcomet_qe_score": 0.14697891473770142, "metricx_score": 22.277738571166992, "metricx_qe_score": 23.79150390625, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "sul tema della pace. Ecco il motivo per cui ciò è possibile è che, nonostante questo enunciato violi il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere posti accanto al verbo. \"Eonnisce il principio di minimizzazione della dipendenza, che afferma che le dipendenze più brevi sono preferite.\"", "metrics": {"bleu_score": 39.58533376475804, "chrf_score": 69.05102662783665, "xcomet_score": 0.12253092229366302, "xcomet_qe_score": 0.06615924835205078, "metricx_score": 13.375412940979004, "metricx_qe_score": 15.139372825622559, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, cioè quelle che non sono costanti tra queste due strutture.\"", "metrics": {"bleu_score": 76.08566262731651, "chrf_score": 87.60523582731892, "xcomet_score": 0.9486403465270996, "xcomet_qe_score": 0.9106401801109314, "metricx_score": 2.1909289360046387, "metricx_qe_score": 2.9093594551086426, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco che abbiamo una dipendenza da rosso all'aggettivo di sette lettere misurato in parole e da rosso al libro di quattro lettere. Per ottenere è 11. Quando si sposta,", "metrics": {"bleu_score": 20.075528629748575, "chrf_score": 40.77617844046881, "xcomet_score": 0.23788033425807953, "xcomet_qe_score": 0.29538455605506897, "metricx_score": 15.80042552947998, "metricx_qe_score": 16.37081527709961, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "quando si scambia questi due componenti, la somma di queste due dipendenze diventa sei, giusto?", "metrics": {"bleu_score": 43.54294657746997, "chrf_score": 79.43369801612624, "xcomet_score": 0.8871458768844604, "xcomet_qe_score": 0.8919196128845215, "metricx_score": 2.656254529953003, "metricx_qe_score": 2.441925287246704, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi invece di undici, sei, molto più breve, ciò", "metrics": {"bleu_score": 16.59038701421971, "chrf_score": 53.65595596931946, "xcomet_score": 0.7513406276702881, "xcomet_qe_score": 0.7351659536361694, "metricx_score": 9.785971641540527, "metricx_qe_score": 5.971613883972168, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "suona abbastanza okay, giusto?", "metrics": {"bleu_score": 0.0, "chrf_score": 10.824952491745496, "xcomet_score": 0.2631523609161377, "xcomet_qe_score": 0.8565430045127869, "metricx_score": 5.230971336364746, "metricx_qe_score": 3.7780091762542725, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.9522626399993896, "xcomet_qe_score": 0.9868285655975342, "metricx_score": 1.406313419342041, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Violata una regola, ma soddisfa un'altra.", "metrics": {"bleu_score": 10.229197414177778, "chrf_score": 41.72206099304413, "xcomet_score": 0.955400288105011, "xcomet_qe_score": 0.9633108973503113, "metricx_score": 8.067566871643066, "metricx_qe_score": 8.014309883117676, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ot", "metrics": {"bleu_score": 0.0, "chrf_score": 17.857142857142854, "xcomet_score": 0.8841519951820374, "xcomet_qe_score": 0.9509313106536865, "metricx_score": 3.684389352798462, "metricx_qe_score": 0.21206539869308472, "linguapy_score": [1, "TSWANA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "timo, abbiamo estratto varie statistiche relative alla coordinazione dall'edizione migliorata del Pantry Bank e analizzato il relativo documento per capire perché non abbiamo utilizzato dipendenze universitarie. E le statistiche confermano l'osservazione fatta molte volte prima che i congiunti a sinistra tendono a essere più brevi.", "metrics": {"bleu_score": 23.024879033810493, "chrf_score": 54.84197430587727, "xcomet_score": 0.2668025493621826, "xcomet_qe_score": 0.29413414001464844, "metricx_score": 12.683456420898438, "metricx_qe_score": 14.398688316345215, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sale e pepe e sale misurati in sillabe.", "metrics": {"bleu_score": 7.21555652598855, "chrf_score": 36.182498770564955, "xcomet_score": 0.6492374539375305, "xcomet_qe_score": 0.5914052724838257, "metricx_score": 13.433036804199219, "metricx_qe_score": 14.821113586425781, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione che si è fatta in passaggio sulla tendenza che cresce con la differenza di lunghezza.", "metrics": {"bleu_score": 29.0226611529142, "chrf_score": 53.48919497061293, "xcomet_score": 0.7861803770065308, "xcomet_qe_score": 0.8200696706771851, "metricx_score": 8.206287384033203, "metricx_qe_score": 7.50275993347168, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto tende a essere il primo più forte. Quindi", "metrics": {"bleu_score": 36.61831660613938, "chrf_score": 64.7656047086989, "xcomet_score": 0.8073688745498657, "xcomet_qe_score": 0.8212404251098633, "metricx_score": 8.969913482666016, "metricx_qe_score": 7.137336730957031, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": ", la proporzione è più grande del congiunto corto a sinistra.", "metrics": {"bleu_score": 22.03359678996931, "chrf_score": 56.01734137660972, "xcomet_score": 0.8811780214309692, "xcomet_qe_score": 0.7878091335296631, "metricx_score": 8.276443481445312, "metricx_qe_score": 11.00373363494873, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma cosa è innovativo in questo studio è che abbiamo osservato che tale tendenza si verifica solo quando l'assetto istituzionale a sinistra si scompiglia.", "metrics": {"bleu_score": 28.970277405356857, "chrf_score": 55.87114928614216, "xcomet_score": 0.617016077041626, "xcomet_qe_score": 0.10338131338357925, "metricx_score": 7.769420623779297, "metricx_qe_score": 7.497600555419922, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.952262282371521, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 1.406313419342041, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Quindi il governatore è a sinistra in questo esempio. Ho visto Lisa Barton. Quindi il governatore è a sinistra.\"", "metrics": {"bleu_score": 20.303767119928715, "chrf_score": 66.08605544075509, "xcomet_score": 0.6825360059738159, "xcomet_qe_score": 0.6730786561965942, "metricx_score": 5.185196876525879, "metricx_qe_score": 5.2019453048706055, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, manca nel secondo esempio, Omero è arrivato e ha star", "metrics": {"bleu_score": 11.498759556447217, "chrf_score": 38.92991552744722, "xcomet_score": 0.4493560492992401, "xcomet_qe_score": 0.4871876537799835, "metricx_score": 13.390774726867676, "metricx_qe_score": 7.83284330368042, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "nutito; qui abbiamo coordinazione di due verbi e non c'è governatore esterno, giusto? Quindi", "metrics": {"bleu_score": 38.53856918030314, "chrf_score": 61.86537832842539, "xcomet_score": 0.3620769679546356, "xcomet_qe_score": 0.3050767183303833, "metricx_score": 17.43069839477539, "metricx_qe_score": 16.81842803955078, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in questi casi, il congiunto a sinistra tende a essere più breve, tanto più grande è la differenza tra i due congiunti.", "metrics": {"bleu_score": 34.98866783250506, "chrf_score": 61.31438603082672, "xcomet_score": 0.9263101816177368, "xcomet_qe_score": 0.8983138799667358, "metricx_score": 7.784269332885742, "metricx_qe_score": 8.606910705566406, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando il governo a destra è presente, il governo di coordinamento del telnet scompare.", "metrics": {"bleu_score": 9.603903191672295, "chrf_score": 39.98779868109558, "xcomet_score": 0.19475704431533813, "xcomet_qe_score": 0.15426504611968994, "metricx_score": 16.74722671508789, "metricx_qe_score": 18.460647583007812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che abbiamo dimostrato che misurando la lunghezza in caratteri, nella prima colonna c'è la colonna dei sillabi, la colonna centrale e la colonna a destra.", "metrics": {"bleu_score": 19.545984328607467, "chrf_score": 68.00184449598109, "xcomet_score": 0.6077843904495239, "xcomet_qe_score": 0.3802452087402344, "metricx_score": 10.294524192810059, "metricx_qe_score": 11.22901439666748, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi mi concentrerò sulla colonna di destra.\"", "metrics": {"bleu_score": 22.31618068926665, "chrf_score": 65.34530263414243, "xcomet_score": 0.46581345796585083, "xcomet_qe_score": 0.16794678568840027, "metricx_score": 3.5810070037841797, "metricx_qe_score": 3.863949775695801, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando si governa a sinistra. Il tendenza per il congiunzione a essere più breve cresce costantemente con la differenza assoluta tra le parole. E lo stesso si osserva quando non c'è un governatore, come nella coordinazione di frasi,", "metrics": {"bleu_score": 37.791638787171586, "chrf_score": 72.50095523354871, "xcomet_score": 0.5148463249206543, "xcomet_qe_score": 0.4098477363586426, "metricx_score": 9.535783767700195, "metricx_qe_score": 9.578232765197754, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ma quando il governatore è a destra, questa tendenza scompare.", "metrics": {"bleu_score": 67.09489882833027, "chrf_score": 80.16315414425573, "xcomet_score": 0.9749386310577393, "xcomet_qe_score": 0.9715472459793091, "metricx_score": 0.7103420495986938, "metricx_qe_score": 1.2280911207199097, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco come dimostriamo nel nostro studio come ciò fornisce un argomento a favore di strutture di coordinamento simmetriche, anziché asimmetriche, tra queste due.\"", "metrics": {"bleu_score": 8.08927659433414, "chrf_score": 46.27899908208264, "xcomet_score": 0.46658191084861755, "xcomet_qe_score": 0.44839969277381897, "metricx_score": 5.672809600830078, "metricx_qe_score": 5.5134429931640625, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il testo: \"Vedere il documento per l'accordo e gli argomenti, scusa,", "metrics": {"bleu_score": 5.412989186545263, "chrf_score": 30.22331924614386, "xcomet_score": 0.2090013027191162, "xcomet_qe_score": 0.13016192615032196, "metricx_score": 14.436891555786133, "metricx_qe_score": 12.85396671295166, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e discutere con noi sulla sessione dei poster.", "metrics": {"bleu_score": 10.552670315936318, "chrf_score": 44.9763048670033, "xcomet_score": 0.4381372034549713, "xcomet_qe_score": 0.848362386226654, "metricx_score": 5.547786712646484, "metricx_qe_score": 6.206117630004883, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 0.9992209672927856, "xcomet_qe_score": 1.0, "metricx_score": 0.03144232928752899, "metricx_qe_score": 0.23287612199783325, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiangbin, studente dottorato presso l'Università di Washington.", "metrics": {"bleu_score": 53.11878904526514, "chrf_score": 88.17981061848248, "xcomet_score": 0.8479101657867432, "xcomet_qe_score": 0.8615964651107788, "metricx_score": 0.5748728513717651, "metricx_qe_score": 0.0879741758108139, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Oggi sto presentando il nostro lavoro sulle reti neurali linguistiche pre-addestrate per compiti downstream, tracciando le tracce di bias politici che portano a modelli NLP ingiusti. I'd be happy to help! Here is the translation:", "metrics": {"bleu_score": 4.620856909230221, "chrf_score": 33.42152719330383, "xcomet_score": 0.13421323895454407, "xcomet_qe_score": 0.09252990782260895, "metricx_score": 14.315162658691406, "metricx_qe_score": 20.20169448852539, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Gli modelli linguistici sono addestrati su grandi quantità di dati di raccolta web. \"Il materiale", "metrics": {"bleu_score": 26.220676436185983, "chrf_score": 58.726221150745225, "xcomet_score": 0.4808174669742584, "xcomet_qe_score": 0.2683037519454956, "metricx_score": 8.847249984741211, "metricx_qe_score": 7.44613790512085, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "di addestramento dei media politici è ben coperto.", "metrics": {"bleu_score": 3.868564529208581, "chrf_score": 28.281350312372176, "xcomet_score": 0.2670048475265503, "xcomet_qe_score": 0.32553207874298096, "metricx_score": 17.50153160095215, "metricx_qe_score": 18.391170501708984, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Secondo uno studio sul corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nel materiale di addestramento dei modelli linguistici.\"", "metrics": {"bleu_score": 62.50932697787992, "chrf_score": 80.24838114343082, "xcomet_score": 0.9247227907180786, "xcomet_qe_score": 0.9538309574127197, "metricx_score": 1.473027229309082, "metricx_qe_score": 1.5882230997085571, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Questo ha creato un bene misto per le applicazioni di modelli linguistici.\" Ecco,", "metrics": {"bleu_score": 21.409092659758045, "chrf_score": 62.66283084810698, "xcomet_score": 0.5568546056747437, "xcomet_qe_score": 0.5239815711975098, "metricx_score": 8.29675006866455, "metricx_qe_score": 7.40496301651001, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "da un lato, sono stati in grado di imparare da diverse prospettive, cosa celebra la democrazia e la pluralità delle idee.", "metrics": {"bleu_score": 30.768057201869553, "chrf_score": 58.51084055875751, "xcomet_score": 0.9406518936157227, "xcomet_qe_score": 0.9418015480041504, "metricx_score": 4.2437744140625, "metricx_qe_score": 5.385385990142822, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, queste diverse opinioni politiche sono inherentemente socialmente biasate e potrebbero portare a problemi di equità in applicazioni di compiti a valle.", "metrics": {"bleu_score": 27.65132641839365, "chrf_score": 56.62903411021985, "xcomet_score": 0.7139810919761658, "xcomet_qe_score": 0.7441310882568359, "metricx_score": 8.578120231628418, "metricx_qe_score": 8.24742603302002, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo senso, proponiamo di indagare il canale di propagazione di bias politico dal dati di pre-addestramento ai modelli linguistici per compiti downstream, specificamente ponendoci le seguenti domande.\" Prima di valutare i modelli di linguaggio politici lineari, come si valuta il ruolo dei dati relativi in questi modelli di bias politici?", "metrics": {"bleu_score": 18.734779665784554, "chrf_score": 55.14894176026757, "xcomet_score": 0.3503669500350952, "xcomet_qe_score": 0.5323294997215271, "metricx_score": 14.503326416015625, "metricx_qe_score": 11.443512916564941, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Inoltre, come funzionano i modelli linguistici con nemici politici diversi e se ciò potrebbe comportare problemi di equità in applicazioni NLP.\"", "metrics": {"bleu_score": 32.83443133402173, "chrf_score": 54.244768339403905, "xcomet_score": 0.6901326775550842, "xcomet_qe_score": 0.6823945641517639, "metricx_score": 12.480859756469727, "metricx_qe_score": 11.99025821685791, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, proponiamo di stimolare i modelli linguistici con formati di domanda diversi utilizzando questionari politici come il test del compasso politico.", "metrics": {"bleu_score": 27.060028460396776, "chrf_score": 61.53825971769204, "xcomet_score": 0.6827097535133362, "xcomet_qe_score": 0.6418673396110535, "metricx_score": 8.050848960876465, "metricx_qe_score": 7.313232421875, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ci consente di valutare automaticamente con basi solide nella letteratura scientifica sulla scienza politica.", "metrics": {"bleu_score": 6.150343144231885, "chrf_score": 43.00752027909713, "xcomet_score": 0.9973927736282349, "xcomet_qe_score": 1.0, "metricx_score": 1.7109915018081665, "metricx_qe_score": 1.4296807050704956, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E alcuni risultati preliminari dimostrano che i modelli di prima lingua hanno significati politici variabili.", "metrics": {"bleu_score": 17.26583259675429, "chrf_score": 57.33548805915862, "xcomet_score": 0.727912187576294, "xcomet_qe_score": 0.8178827166557312, "metricx_score": 5.8761725425720215, "metricx_qe_score": 3.1335175037384033, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Essi occupano tutti e quattro i quadranti del compasso politico.", "metrics": {"bleu_score": 51.93071778680675, "chrf_score": 81.17699316260146, "xcomet_score": 0.805971622467041, "xcomet_qe_score": 0.8316561579704285, "metricx_score": 7.02863073348999, "metricx_qe_score": 7.274127960205078, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che GPT-4 è il modello linguistico più liberale di tutti, e i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BERT e alle sue varianti.", "metrics": {"bleu_score": 44.522921828109816, "chrf_score": 68.18942435275423, "xcomet_score": 0.9649089574813843, "xcomet_qe_score": 0.9711916446685791, "metricx_score": 1.5713000297546387, "metricx_qe_score": 0.9454600811004639, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci proponiamo di indagare in quale misura i bias politici dei modelli linguistici vengano realmente acquisiti dai dati di training.", "metrics": {"bleu_score": 15.30228536618876, "chrf_score": 46.20816257965835, "xcomet_score": 0.9618620872497559, "xcomet_qe_score": 0.9878667593002319, "metricx_score": 5.455635070800781, "metricx_qe_score": 3.519425392150879, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Potremmo condurre un esperimento controllato preaddestrando modelli di linguaggio a checkpoint su sei corpora di partito diversi, separati tra notizie e social media, ulteriormente divisi per orientamento politico. \"Attraverso la", "metrics": {"bleu_score": 18.65923511564985, "chrf_score": 58.530979539741566, "xcomet_score": 0.2957169711589813, "xcomet_qe_score": 0.24386142194271088, "metricx_score": 9.738912582397461, "metricx_qe_score": 10.926250457763672, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "pre-allenamento di modelli linguistici su tali party in Kodpora, possiamo osservare che i coordinate ideologiche del modello linguistico si spostano anch'esse.\"", "metrics": {"bleu_score": 33.08274705943025, "chrf_score": 61.96305753234754, "xcomet_score": 0.27321839332580566, "xcomet_qe_score": 0.3036695718765259, "metricx_score": 14.89685344696045, "metricx_qe_score": 15.225617408752441, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Esempio, per Robert, addestrato ulteriormente su un corpus lineare rossastro, possiamo osservare un sostanziale spostamento a sinistra in termini di sua In termini di pregiudizi politici.", "metrics": {"bleu_score": 11.379047007436972, "chrf_score": 55.54582835284052, "xcomet_score": 0.1943495273590088, "xcomet_qe_score": 0.18588392436504364, "metricx_score": 15.817390441894531, "metricx_qe_score": 14.701095581054688, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E anche cerchiamo di indagare se i modelli linguistici possono captare la polarizzazione che è presente nella nostra società attuale.", "metrics": {"bleu_score": 35.653300298570315, "chrf_score": 66.20348856473474, "xcomet_score": 0.9717618227005005, "xcomet_qe_score": 0.9619451761245728, "metricx_score": 3.3383564949035645, "metricx_qe_score": 2.9317514896392822, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Dividiamo i corpus di pre-addestramento in due sezioni, prima del 45° presidente degli Stati Uniti d'America, e dopo il 45° presidente degli Stati Uniti d", "metrics": {"bleu_score": 48.1138843291231, "chrf_score": 77.87020946066725, "xcomet_score": 0.49583297967910767, "xcomet_qe_score": 0.3732866048812866, "metricx_score": 6.7271270751953125, "metricx_qe_score": 4.4430975914001465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "'America, addestriamo modelli di lingua separatamente su due corpora temporali diversi.\"", "metrics": {"bleu_score": 7.768562846380172, "chrf_score": 63.651443412584364, "xcomet_score": 0.29797855019569397, "xcomet_qe_score": 0.22167962789535522, "metricx_score": 13.22739028930664, "metricx_qe_score": 12.093875885009766, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Può essere visto che i modelli linguistici hanno generalmente un orientamento politico più lontano dal centro dopo il 2017.", "metrics": {"bleu_score": 47.629019423691716, "chrf_score": 71.83849806702396, "xcomet_score": 0.9415655136108398, "xcomet_qe_score": 0.9387686252593994, "metricx_score": 4.530850887298584, "metricx_qe_score": 4.152898788452148, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che i modelli linguistici possono anche captare la polarizzazione nella nostra società.\"", "metrics": {"bleu_score": 57.754002174538684, "chrf_score": 75.67339465975525, "xcomet_score": 0.9616538286209106, "xcomet_qe_score": 0.9691399931907654, "metricx_score": 1.9755610227584839, "metricx_qe_score": 2.0845394134521484, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco l'ultimo, ma non meno importante, valutiamo modelli linguistici con significati politici diversi per la detezione di discorsi d'odio e notizie false, e applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative.", "metrics": {"bleu_score": 49.80380502241238, "chrf_score": 69.34754882577604, "xcomet_score": 0.7578707933425903, "xcomet_qe_score": 0.8247066736221313, "metricx_score": 3.3644957542419434, "metricx_qe_score": 2.746196985244751, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco che vediamo che se esaminiamo il rendimento per categoria, cioè se separiamo il rendimento in categorie, ci accorgiamo che... Ecco il testo tradotto: In diversi settori o media lineari di notizie politiche, possiamo notare un pattern che,", "metrics": {"bleu_score": 14.017322920696776, "chrf_score": 45.67680312116331, "xcomet_score": 0.29265832901000977, "xcomet_qe_score": 0.1979484260082245, "metricx_score": 14.194425582885742, "metricx_qe_score": 16.10759925842285, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, per la detezione di discorsi di odio, i modelli linguistici di lingua sinistra sono migliori. \"Allocazione di riconoscere il discorso d'odio diretto contro gruppi socialmente minoritari.\" Tuttavia, il nostro lavoro si concentra sulla detezione di gruppi più potenti nella nostra società.", "metrics": {"bleu_score": 24.07843507482262, "chrf_score": 51.626276470993936, "xcomet_score": 0.2408524453639984, "xcomet_qe_score": 0.2793991267681122, "metricx_score": 15.902825355529785, "metricx_qe_score": 16.207441329956055, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E i modelli di linguaggio a due vie sono migliori nel rilevare la retorica d'odio diretta contro gli uomini bianchi, tuttavia peggiori nel rilevare la retorica d'odio diretta contro le comunità minoritarie nere LGBTQ+ e altre minoranze.", "metrics": {"bleu_score": 7.0534723916509545, "chrf_score": 45.466780012498845, "xcomet_score": 0.7163161635398865, "xcomet_qe_score": 0.7538927793502808, "metricx_score": 6.625929355621338, "metricx_qe_score": 6.324209213256836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni trend simili si verificano anche nella detezione delle notizie false, dove si osserva che i modelli linguistici di orientamento a sinistra sono migliori nel rilevare la disinformazione da parte del loro orientamento politico opposto e viceversa.", "metrics": {"bleu_score": 41.72803476742279, "chrf_score": 70.73472410169147, "xcomet_score": 0.8879609107971191, "xcomet_qe_score": 0.9575188159942627, "metricx_score": 1.4155762195587158, "metricx_qe_score": 1.6530321836471558, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, mostriamo molti esempi qualitativi per dimostrare che i modelli linguistici con significati politici diversi. \"Ecco, date diverse previsioni per esempi di odio e disinformazione basati sulle loro categorie.", "metrics": {"bleu_score": 17.400446794038263, "chrf_score": 58.55650683511251, "xcomet_score": 0.5662676095962524, "xcomet_qe_score": 0.5218819379806519, "metricx_score": 12.993436813354492, "metricx_qe_score": 12.791729927062988, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono un po' più esempi nell'appendice per ulteriormente evidenziare ciò.\" \"Esiste un problema di equità molto pressante riguardo ai bias politici dei modelli linguistici.\"", "metrics": {"bleu_score": 31.108918705294958, "chrf_score": 59.748114353556524, "xcomet_score": 0.9127645492553711, "xcomet_qe_score": 0.9013189673423767, "metricx_score": 6.200802803039551, "metricx_qe_score": 5.1033782958984375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Esempio, se un linguaggio di modello a destra fosse stato addestrato su discorsi d'odio o su informazioni false o simili e distribuito su una piattaforma di social media popolare. Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e la diffamazione contro i gruppi minoritari potrebbe essere libera di proliferare senza alcun controllo.", "metrics": {"bleu_score": 29.457750177110896, "chrf_score": 65.30676069729144, "xcomet_score": 0.717308759689331, "xcomet_qe_score": 0.7228591442108154, "metricx_score": 5.151205539703369, "metricx_qe_score": 4.758948802947998, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Questo ci richiama a prendere atto e affrontare le questioni di equità generate dalle significazioni linguistiche dei", "metrics": {"bleu_score": 4.310422866789497, "chrf_score": 32.67323385937845, "xcomet_score": 0.24340327084064484, "xcomet_qe_score": 0.26776033639907837, "metricx_score": 11.731649398803711, "metricx_qe_score": 5.683349609375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "modelli di lingua.\" \"Ecco un po' di discussione.", "metrics": {"bleu_score": 3.3864985683445354, "chrf_score": 22.049178881656413, "xcomet_score": 0.13900655508041382, "xcomet_qe_score": 0.1482500582933426, "metricx_score": 18.27799415588379, "metricx_qe_score": 20.17142105102539, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Vorremmo anche sottolineare il dilemma unico riguardante le biografie linguistiche dei modelli.", "metrics": {"bleu_score": 4.896627602978773, "chrf_score": 39.35045028213793, "xcomet_score": 0.4327496588230133, "xcomet_qe_score": 0.7714495658874512, "metricx_score": 12.591747283935547, "metricx_qe_score": 13.260388374328613, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "È come tra Cilla e Karebdis.\"", "metrics": {"bleu_score": 5.67557355463946, "chrf_score": 21.409855598332758, "xcomet_score": 0.2606706917285919, "xcomet_qe_score": 0.40082356333732605, "metricx_score": 5.2977776527404785, "metricx_qe_score": 2.8616669178009033, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se non sanifichiamo le opinioni politiche nel training dei modelli linguistici, il bias si propagherà dai dati di pre-training ai modelli linguistici e ai compiti a valle, creando problemi di fairness.", "metrics": {"bleu_score": 15.62171590347256, "chrf_score": 54.38349881200093, "xcomet_score": 0.6601999998092651, "xcomet_qe_score": 0.7434408664703369, "metricx_score": 7.230926036834717, "metricx_qe_score": 6.5119733810424805, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E se cerchiamo di sanificare in qualche modo, rischiamo anche la censura o l'esclusione.", "metrics": {"bleu_score": 23.287896954139942, "chrf_score": 54.59821069394233, "xcomet_score": 0.9351212382316589, "xcomet_qe_score": 0.9443631768226624, "metricx_score": 1.992867112159729, "metricx_qe_score": 2.5555078983306885, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E è incredibilmente difficile determinare cosa è veramente neutrale e dovrebbe mantenere i dati di monitoraggio linguistico.", "metrics": {"bleu_score": 35.39810268560157, "chrf_score": 68.50020353306576, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 3.0470869541168213, "metricx_qe_score": 5.856566429138184, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi è come il problema del problema elettrico.\"", "metrics": {"bleu_score": 31.239399369202552, "chrf_score": 64.36143588495385, "xcomet_score": 0.2631603479385376, "xcomet_qe_score": 0.30369997024536133, "metricx_score": 16.24456024169922, "metricx_qe_score": 15.300284385681152, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Okay, grazie,", "metrics": {"bleu_score": 10.682175159905848, "chrf_score": 8.548466085983138, "xcomet_score": 0.20296333730220795, "xcomet_qe_score": 0.25677138566970825, "metricx_score": 2.1503140926361084, "metricx_qe_score": 0.6133546233177185, "linguapy_score": [1, "TSONGA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "credo che questo sia quasi tutto per oggi.", "metrics": {"bleu_score": 33.03164318013809, "chrf_score": 56.49769926221503, "xcomet_score": 0.9701904058456421, "xcomet_qe_score": 0.974733293056488, "metricx_score": 0.6257272362709045, "metricx_qe_score": 0.46917039155960083, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il tuo tempo.\"", "metrics": {"bleu_score": 20.873176328735713, "chrf_score": 41.449399328995284, "xcomet_score": 0.46190592646598816, "xcomet_qe_score": 0.9208484888076782, "metricx_score": 2.3573715686798096, "metricx_qe_score": 2.3801259994506836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti,", "metrics": {"bleu_score": 39.76353643835252, "chrf_score": 85.64453649560136, "xcomet_score": 0.5761392116546631, "xcomet_qe_score": 0.40772056579589844, "metricx_score": 0.10415956377983093, "metricx_qe_score": 0.12619981169700623, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "sono Jenny, studentessa di dottorato del primo anno dell'Università di Carnegie Mellon, e oggi presenterò il mio lavoro, Analisi posizionale, caratterizzazione mediante un insieme di modelli di dati CSA.\" \"Questo", "metrics": {"bleu_score": 11.768079654790519, "chrf_score": 59.3527040600008, "xcomet_score": 0.1719234734773636, "xcomet_qe_score": 0.3126513659954071, "metricx_score": 14.318680763244629, "metricx_qe_score": 9.850579261779785, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Instituto Allen per l'Intelligenza Artificiale, ovvero Sebastian Santy, Ronan La Bros, Katarina Aranica e Martin Sapp.\"", "metrics": {"bleu_score": 25.05908809823662, "chrf_score": 62.19536271982676, "xcomet_score": 0.49402981996536255, "xcomet_qe_score": 0.5492591857910156, "metricx_score": 4.152411460876465, "metricx_qe_score": 3.390883445739746, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Immaginiamo di lavorare per un giornale e stiamo cercando di rimuovere il contenuto tossico dai commenti sottostanti all'articolo.\"", "metrics": {"bleu_score": 10.495620780256237, "chrf_score": 47.15479880535886, "xcomet_score": 0.9130446910858154, "xcomet_qe_score": 0.9269648790359497, "metricx_score": 3.7589433193206787, "metricx_qe_score": 3.920562744140625, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Potresti rivolgerti a un API popolare come Perspective API per la detezione della città tossica. E questo funziona veramente bene se sei Carl Jones,", "metrics": {"bleu_score": 7.027306640928977, "chrf_score": 59.856103373396344, "xcomet_score": 0.4645181894302368, "xcomet_qe_score": 0.6249868273735046, "metricx_score": 10.155847549438477, "metricx_qe_score": 8.910879135131836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in cui Perspective API è in grado di rilevare correttamente gli istanze tossiche.", "metrics": {"bleu_score": 44.534504264163466, "chrf_score": 73.70741978201801, "xcomet_score": 0.7118088603019714, "xcomet_qe_score": 0.7071759700775146, "metricx_score": 5.649504661560059, "metricx_qe_score": 6.158289432525635, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio il caso per una dithya-sharma,", "metrics": {"bleu_score": 11.355085821332205, "chrf_score": 25.87527054804323, "xcomet_score": 0.615199089050293, "xcomet_qe_score": 0.5628958940505981, "metricx_score": 11.755582809448242, "metricx_qe_score": 11.850445747375488, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "dove le API non sono altrettanto sensibili a termini offensivi più comuni nel contesto indiano.", "metrics": {"bleu_score": 8.033833855491482, "chrf_score": 52.37461657923558, "xcomet_score": 0.7626758217811584, "xcomet_qe_score": 0.6630845069885254, "metricx_score": 6.950204849243164, "metricx_qe_score": 7.076016426086426, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco un esempio di bias di progettazione dove si osservano differenze sistematiche nel comportamento di tecnologia tra popolazioni.\"", "metrics": {"bleu_score": 11.187338538742793, "chrf_score": 57.38172322720172, "xcomet_score": 0.8201260566711426, "xcomet_qe_score": 0.836355447769165, "metricx_score": 5.829216957092285, "metricx_qe_score": 5.665070533752441, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Le bias di progettazione come quello che abbiamo appena visto potrebbero verificarsi a causa della posizionalità degli studiosi di NLP e dei sviluppatori di modelli.", "metrics": {"bleu_score": 64.85539682067228, "chrf_score": 84.7209730434346, "xcomet_score": 0.8478741645812988, "xcomet_qe_score": 0.7533252239227295, "metricx_score": 6.087194919586182, "metricx_qe_score": 6.3363356590271, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La posizionalità è semplicemente le prospettive che le persone tengono a causa delle loro demografie, identità e esperienze di vita.\"", "metrics": {"bleu_score": 21.786545894803066, "chrf_score": 60.71441141856213, "xcomet_score": 0.7635799646377563, "xcomet_qe_score": 0.691106915473938, "metricx_score": 4.4463934898376465, "metricx_qe_score": 4.230025291442871, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco un concetto ampiamente utilizzato in studi critici, specificamente in spazi accademici femministi e queer.\"", "metrics": {"bleu_score": 45.307778036928106, "chrf_score": 69.78496234681124, "xcomet_score": 0.935855507850647, "xcomet_qe_score": 0.9342136383056641, "metricx_score": 2.024744749069214, "metricx_qe_score": 0.8701909184455872, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, come ricercatrice, la posizione può influire sul processo di ricerca e sui suoi esiti e risultati poiché può cambiare le decisioni che i ricercatori prendono.\"", "metrics": {"bleu_score": 31.454352182114505, "chrf_score": 71.33483028009344, "xcomet_score": 0.9182865619659424, "xcomet_qe_score": 0.9350193738937378, "metricx_score": 5.704941749572754, "metricx_qe_score": 4.76308536529541, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco una domanda che le persone potrebbero porre è se i set di dati e i modelli hanno posizionalità.\"", "metrics": {"bleu_score": 49.66038639224974, "chrf_score": 71.53148325483863, "xcomet_score": 0.9282788634300232, "xcomet_qe_score": 0.8427852392196655, "metricx_score": 5.9991936683654785, "metricx_qe_score": 5.640719413757324, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E non stiamo cercando di dire che i modelli stessi e i sistemi di dati stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni rispetto ad altre.\"", "metrics": {"bleu_score": 54.29523749534523, "chrf_score": 77.76649098812415, "xcomet_score": 0.9292250871658325, "xcomet_qe_score": 0.9401286244392395, "metricx_score": 2.3920485973358154, "metricx_qe_score": 2.945833444595337, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il lavoro precedente ha suggerito alcune evidenze aneddotiche sulla posizionalità, come divari culturali nei modelli e negli insiemi di dati, nonché definizioni teoriche di posizionalità.\"", "metrics": {"bleu_score": 27.199778234956106, "chrf_score": 68.57016801303139, "xcomet_score": 0.8796674013137817, "xcomet_qe_score": 0.9047039747238159, "metricx_score": 2.1179518699645996, "metricx_qe_score": 3.3116812705993652, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non esaminano affatto i destinatari finali rispetto ai set di dati e ai modelli stessi. \"Ecco che, con l'avanzare dei compiti NLP, la posizione di partenza e del set di dati diventa sempre più importante, poiché le attività NLP diventano sempre più soggettive e orientate alla società.\" \"Ecco come è difficile caratterizzare queste posizioni in quanto non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.\"", "metrics": {"bleu_score": 31.806364346544772, "chrf_score": 61.251490778831084, "xcomet_score": 0.500996470451355, "xcomet_qe_score": 0.6083904504776001, "metricx_score": 7.392637729644775, "metricx_qe_score": 6.3690667152404785, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Per studiare i set di dati e modellare la posizione, effettuiamo confronti con le annotazioni con i dati reali e i modelli esistenti.\"", "metrics": {"bleu_score": 13.943959351147976, "chrf_score": 57.01439390275931, "xcomet_score": 0.8081609010696411, "xcomet_qe_score": 0.814832329750061, "metricx_score": 5.610203742980957, "metricx_qe_score": 5.10679817199707, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il mio ruolo è di tradurre il discorso inglese in italiano in modo preciso e aderendo alle regole grammaticali, lessicale e culturali italiane, utilizzando termini tecnici e un tono appropriato per materiali accademici o didattici.", "metrics": {"bleu_score": 1.1216940038341034, "chrf_score": 16.259374787312257, "xcomet_score": 0.22196751832962036, "xcomet_qe_score": 0.12857851386070251, "metricx_score": 13.480335235595703, "metricx_qe_score": 22.171140670776367, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due passaggi principali.", "metrics": {"bleu_score": 66.06328636027612, "chrf_score": 86.3716668662492, "xcomet_score": 0.9971437454223633, "xcomet_qe_score": 1.0, "metricx_score": 0.7644364237785339, "metricx_qe_score": 0.9685013890266418, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il primo passo è quello di riannotare i dataset con annotatori diversi.\"", "metrics": {"bleu_score": 12.500763055889768, "chrf_score": 56.71780870732198, "xcomet_score": 0.896334171295166, "xcomet_qe_score": 0.9312233924865723, "metricx_score": 1.8305931091308594, "metricx_qe_score": 1.498292088508606, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Eccoci a scegliere di fare questo anziché esaminare le statistiche demografiche dei set di dati originari, poiché spesso solo pochi annotatori annotano ogni istanza e poiché le informazioni demografiche sono raramente raccolte e condivise.\" Ecco il testo tradotto:", "metrics": {"bleu_score": 19.98527160015335, "chrf_score": 51.713484677729035, "xcomet_score": 0.6866459846496582, "xcomet_qe_score": 0.7662851214408875, "metricx_score": 5.454257011413574, "metricx_qe_score": 3.9625847339630127, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E quindi optiamo per reanimare i dati per ottenere molte entità, ad esempio, e ottenere un insieme ricco di dati demografici. E", "metrics": {"bleu_score": 32.14110553053944, "chrf_score": 54.224250289322015, "xcomet_score": 0.524030864238739, "xcomet_qe_score": 0.4890964925289154, "metricx_score": 14.589595794677734, "metricx_qe_score": 13.42422103881836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "quindi confrontiamo le annotazioni per demografia con i modelli e i set di dati utilizzando una coppia poiché il punteggio di correlazione. \"Ecco, il nostro framework si differenzia dalla letteratura sull'accordo degli annotatori analizzando gli utenti finali, i modelli e i set di dati, le predizioni e gli etichettatura, anziché guardare solo all'accordo degli annotatori o alla distribuzione modellistica degli annotatori.\"", "metrics": {"bleu_score": 26.04569464151649, "chrf_score": 62.519470730100416, "xcomet_score": 0.38997170329093933, "xcomet_qe_score": 0.43244966864585876, "metricx_score": 12.162282943725586, "metricx_qe_score": 12.19070816040039, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro tasso di frame è largamente abilitato da Lab in the Wild, una piattaforma di crowdsourcing online per collaboratori di HCI.", "metrics": {"bleu_score": 34.46913316729032, "chrf_score": 65.13868307385529, "xcomet_score": 0.5741008520126343, "xcomet_qe_score": 0.5374124050140381, "metricx_score": 9.495512962341309, "metricx_qe_score": 8.881688117980957, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Lab in the Wild è un' piattaforma di esperimentazione online in cui possiamo reclutare volontari diversi", "metrics": {"bleu_score": 20.948412716651138, "chrf_score": 73.57377212731303, "xcomet_score": 0.7470989227294922, "xcomet_qe_score": 0.761931300163269, "metricx_score": 5.931867599487305, "metricx_qe_score": 6.806421756744385, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ficati rispetto a piattaforme come MTURC, che solitamente coinvolgono partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere dati di alta qualità.\"", "metrics": {"bleu_score": 41.42532062451909, "chrf_score": 68.19750157295836, "xcomet_score": 0.717866063117981, "xcomet_qe_score": 0.8079119920730591, "metricx_score": 9.065408706665039, "metricx_qe_score": 8.334517478942871, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Hostiamo due compiti fuori dalla vita reale, uno dei quali è l'accettabilità sociale. E il modo in cui ciò funziona è che i partecipanti leggeranno una situazione dal set di dati sulla chimica sociale, e poi scriveranno come accettabile è una situazione.\"", "metrics": {"bleu_score": 40.810344449802514, "chrf_score": 68.045622530385, "xcomet_score": 0.6167804002761841, "xcomet_qe_score": 0.6534407734870911, "metricx_score": 9.271846771240234, "metricx_qe_score": 9.010350227355957, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dopo di che, per restare coinvolti nello studio, possono confrontare le loro risposte con un'intelligenza artificiale e con gli altri.", "metrics": {"bleu_score": 48.84391791679523, "chrf_score": 72.32593661180428, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.48267728090286255, "metricx_qe_score": 0.3746874928474426, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GPT-4.", "metrics": {"bleu_score": 45.72313446186435, "chrf_score": 77.051186650562, "xcomet_score": 0.972393274307251, "xcomet_qe_score": 0.9752229452133179, "metricx_score": 1.7622220516204834, "metricx_qe_score": 2.0498037338256836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, ripetiamo un setup molto simile per il compito di rilevazione di odio e discorso osceno, dove leggeranno un'istanza da Danny Hate e scriveranno se pensano che sia un'istanza di discorso osceno. Allora,", "metrics": {"bleu_score": 10.30141203886643, "chrf_score": 42.822727336573365, "xcomet_score": 0.27283650636672974, "xcomet_qe_score": 0.2773823142051697, "metricx_score": 7.703381061553955, "metricx_qe_score": 8.10479736328125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo confrontato queste annotazioni con Dynahate, API di Perspective, Rewire API, HateRoberta e GPT-4.", "metrics": {"bleu_score": 39.139854477602455, "chrf_score": 81.45342651593681, "xcomet_score": 0.7239076495170593, "xcomet_qe_score": 0.7376636266708374, "metricx_score": 1.3498625755310059, "metricx_qe_score": 1.9303712844848633, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro studio alla fine ha accumulato oltre 16.000 annotazioni da parte di più di 1.000 annotatori provenienti da 87 paesi.", "metrics": {"bleu_score": 49.296643949535216, "chrf_score": 79.64454903002046, "xcomet_score": 0.9647473096847534, "xcomet_qe_score": 0.9650334715843201, "metricx_score": 1.963287591934204, "metricx_qe_score": 1.295323371887207, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora stiamo per equipaggiare per rispondere a chi sono i dataset e i modelli NLP che si allineano di più.", "metrics": {"bleu_score": 12.399334346390834, "chrf_score": 40.682633922925646, "xcomet_score": 0.5306644439697266, "xcomet_qe_score": 0.5182119607925415, "metricx_score": 10.000815391540527, "metricx_qe_score": 8.900673866271973, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Notiamo che ci sono posizioni in NLP. Ecco un", "metrics": {"bleu_score": 4.990049701936832, "chrf_score": 39.28178512737336, "xcomet_score": 0.42421603202819824, "xcomet_qe_score": 0.39807698130607605, "metricx_score": 10.651741027832031, "metricx_qe_score": 6.1444993019104, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "esempio: troviamo che i dati e i modelli sono più allineati ai Paesi di lingua inglese.", "metrics": {"bleu_score": 49.01108541179766, "chrf_score": 69.09145030185435, "xcomet_score": 0.9596018195152283, "xcomet_qe_score": 0.9412444829940796, "metricx_score": 2.033438205718994, "metricx_qe_score": 1.5395374298095703, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, per l'analisi di accettabilità sociale del GPT-4, troviamo che è più allineato ai Paesi confuciani e di lingua inglese.", "metrics": {"bleu_score": 26.437150557896803, "chrf_score": 53.80365507666147, "xcomet_score": 0.9156302213668823, "xcomet_qe_score": 0.8473124504089355, "metricx_score": 2.4777486324310303, "metricx_qe_score": 2.6764912605285645, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo che Dynahate è anche più allineato ai Paesi di lingua inglese.", "metrics": {"bleu_score": 54.733202423907116, "chrf_score": 69.49980632034266, "xcomet_score": 0.9852422475814819, "xcomet_qe_score": 0.9207730293273926, "metricx_score": 2.495485544204712, "metricx_qe_score": 2.3914003372192383, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo inoltre una maggiore alineazione con le persone che hanno una formazione universitaria.", "metrics": {"bleu_score": 14.458924666162856, "chrf_score": 60.526893444732444, "xcomet_score": 0.9966986179351807, "xcomet_qe_score": 1.0, "metricx_score": 1.6051486730575562, "metricx_qe_score": 1.35310697555542, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nel compito di accettabilità sociale di GPT-4, scopriamo che è maggiormente allineato con le persone con una formazione universitaria o post-laurea. \"Ecco che troviamo lo stesso per Dianaheid, dove è maggiormente allineato con le persone con un titolo di studio universitario.\"", "metrics": {"bleu_score": 10.853414547943052, "chrf_score": 54.18357664406878, "xcomet_score": 0.7093406319618225, "xcomet_qe_score": 0.7331822514533997, "metricx_score": 7.577812671661377, "metricx_qe_score": 7.594071388244629, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando modelli e dataset sono allineati a specifiche popolazioni, alcuni inevitabilmente vengono lasciati indietro.", "metrics": {"bleu_score": 15.83780701185431, "chrf_score": 66.42967410629251, "xcomet_score": 0.9782086610794067, "xcomet_qe_score": 0.980774998664856, "metricx_score": 2.509023666381836, "metricx_qe_score": 3.017359733581543, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Esempio di ciò è che i set di dati e i modelli sono meno in linea per le persone non binarie rispetto ai corrispondenti uomini e donne.", "metrics": {"bleu_score": 51.70969057682974, "chrf_score": 64.69326625489842, "xcomet_score": 0.8735696077346802, "xcomet_qe_score": 0.8678625822067261, "metricx_score": 6.0154924392700195, "metricx_qe_score": 5.36024284362793, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Lo troviamo anche nel compito di accettabilità GPT-4, nonché nell'analisi del compito dining-hate.\"", "metrics": {"bleu_score": 6.047157127164658, "chrf_score": 43.13660092174521, "xcomet_score": 0.45900285243988037, "xcomet_qe_score": 0.48431241512298584, "metricx_score": 8.881521224975586, "metricx_qe_score": 10.425264358520508, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, dati che ci sia una posizione in atlante e L.P., cosa possiamo fare al riguardo?\"", "metrics": {"bleu_score": 25.51001274286627, "chrf_score": 56.64919999992025, "xcomet_score": 0.28441110253334045, "xcomet_qe_score": 0.28128480911254883, "metricx_score": 14.748167037963867, "metricx_qe_score": 12.923599243164062, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Hai alcuni suggerimenti per questo.", "metrics": {"bleu_score": 6.567274736060395, "chrf_score": 18.492557864069013, "xcomet_score": 0.4653298556804657, "xcomet_qe_score": 0.7693052887916565, "metricx_score": 6.321259498596191, "metricx_qe_score": 5.438086032867432, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è tenere un registro di tutte le scelte di progettazione durante il processo di ricerca.", "metrics": {"bleu_score": 56.32579400090421, "chrf_score": 71.25147165760892, "xcomet_score": 0.9256792068481445, "xcomet_qe_score": 0.9548636674880981, "metricx_score": 0.8988654017448425, "metricx_qe_score": 1.024929165840149, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E l'altro è fare ricerche NLP che prendono in considerazione una prospettiva perspicace.\" \"Ecco", "metrics": {"bleu_score": 5.751391809950023, "chrf_score": 46.3495559447872, "xcomet_score": 0.3578108847141266, "xcomet_qe_score": 0.25183358788490295, "metricx_score": 9.626502990722656, "metricx_qe_score": 5.89727783203125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "la nostra terza raccomandazione: creare insiemi di dati e modelli all'interno di quattro comunità specifiche.", "metrics": {"bleu_score": 21.13808999612967, "chrf_score": 60.23689647046001, "xcomet_score": 0.9285480976104736, "xcomet_qe_score": 0.9805867671966553, "metricx_score": 2.2841131687164307, "metricx_qe_score": 2.7152554988861084, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Un buon esempio di ciò è l'iniziativa Musseqani.", "metrics": {"bleu_score": 54.627576446464936, "chrf_score": 77.35194224019209, "xcomet_score": 0.6644648313522339, "xcomet_qe_score": 0.729986310005188, "metricx_score": 5.269446849822998, "metricx_qe_score": 5.764005184173584, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo enfatizzare che l'NLP inclusiva non è solo fare in modo che", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 43.879162986498635, "xcomet_score": 0.7269725799560547, "xcomet_qe_score": 0.778469979763031, "metricx_score": 8.565926551818848, "metricx_qe_score": 7.220218658447266, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "tutte le tecnologie funzionino per tutti.", "metrics": {"bleu_score": 30.895757752065407, "chrf_score": 76.49122201852585, "xcomet_score": 0.8287597894668579, "xcomet_qe_score": 0.8506923317909241, "metricx_score": 6.385710716247559, "metricx_qe_score": 8.026315689086914, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, questo conclude la nostra presentazione,", "metrics": {"bleu_score": 19.070828081828378, "chrf_score": 62.84305045465135, "xcomet_score": 0.885380744934082, "xcomet_qe_score": 0.885105311870575, "metricx_score": 1.510378360748291, "metricx_qe_score": 0.49150550365448, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ma se desiderate approfondire ulteriormente, è possibile consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo.", "metrics": {"bleu_score": 29.239199795721298, "chrf_score": 62.26902593191227, "xcomet_score": 0.9139940738677979, "xcomet_qe_score": 0.9488420486450195, "metricx_score": 3.442819356918335, "metricx_qe_score": 4.3606858253479, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 0.9992209672927856, "xcomet_qe_score": 1.0, "metricx_score": 0.03144232928752899, "metricx_qe_score": 0.23287612199783325, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, sono Si Yu-Yuan dell'Università Fudan.", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 64.17173998479775, "xcomet_score": 0.9427266120910645, "xcomet_qe_score": 0.9351337552070618, "metricx_score": 0.5573826432228088, "metricx_qe_score": 0.8220533132553101, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Sono qui per presentare il nostro lavoro, Conoscenza di Script Distinguibile dai Modelli Linguistici per la Pianificazione", "metrics": {"bleu_score": 28.6221482973952, "chrf_score": 45.66193094244015, "xcomet_score": 0.6791233420372009, "xcomet_qe_score": 0.6809899806976318, "metricx_score": 11.738286018371582, "metricx_qe_score": 13.102375984191895, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Linguistica vincolata.\" \"In vita quotidiana, gli esseri umani pianificano le loro azioni seguendo istruzioni passo-passo in forma di scenari garantiti.\"", "metrics": {"bleu_score": 42.84965562696499, "chrf_score": 67.32552093815562, "xcomet_score": 0.16395245492458344, "xcomet_qe_score": 0.15335968136787415, "metricx_score": 14.694051742553711, "metricx_qe_score": 14.495781898498535, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il precedente mondo ha sfruttato i modelli di lingua per pianificare obiettivi astratti di attività stereotipiche, come fare una torta,", "metrics": {"bleu_score": 28.905619556171565, "chrf_score": 65.8597271168566, "xcomet_score": 0.6689370274543762, "xcomet_qe_score": 0.6758540868759155, "metricx_score": 11.899323463439941, "metricx_qe_score": 8.56947135925293, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e dimostrare che i grandi modelli di lingua possono decomporre gli obiettivi in passaggi.", "metrics": {"bleu_score": 50.389204852596336, "chrf_score": 71.78215044272667, "xcomet_score": 0.7817169427871704, "xcomet_qe_score": 0.7454210519790649, "metricx_score": 2.7461633682250977, "metricx_qe_score": 3.637373208999634, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il lavoro precedente si è concentrato prevalentemente sulla pianificazione di obiettivi teorici.", "metrics": {"bleu_score": 33.696998602966666, "chrf_score": 61.87178909514306, "xcomet_score": 0.7990385293960571, "xcomet_qe_score": 0.860324501991272, "metricx_score": 5.626376152038574, "metricx_qe_score": 6.117331504821777, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La pianificazione di obiettivi con obiettivi specifici, vincoli specifici, come ad esempio fare un dolce al cioccolato, è ancora poco studiato.", "metrics": {"bleu_score": 16.217331037826323, "chrf_score": 68.57327543879435, "xcomet_score": 0.755321741104126, "xcomet_qe_score": 0.787591814994812, "metricx_score": 6.871626377105713, "metricx_qe_score": 7.445969581604004, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo lavoro, definiamo il problema del pianificazione del linguaggio con restrizioni.\" \"Le quali impongono diverse restrizioni al planning.", "metrics": {"bleu_score": 12.409597120849801, "chrf_score": 47.514364197942434, "xcomet_score": 0.6741431951522827, "xcomet_qe_score": 0.4041096270084381, "metricx_score": 11.933259010314941, "metricx_qe_score": 12.736207008361816, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Un obiettivo astratto può essere ereditato da obiettivi specifici della vita reale con restrizioni multifacciali.", "metrics": {"bleu_score": 64.49131835432436, "chrf_score": 78.79441478461099, "xcomet_score": 0.9789373874664307, "xcomet_qe_score": 0.9894534349441528, "metricx_score": 3.1868607997894287, "metricx_qe_score": 2.5986075401306152, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni.\"", "metrics": {"bleu_score": 66.52049901111006, "chrf_score": 86.57109788737701, "xcomet_score": 0.9358681440353394, "xcomet_qe_score": 0.9339361786842346, "metricx_score": 3.8172194957733154, "metricx_qe_score": 5.599257469177246, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo studio, valutiamo e miglioriamo l'abilità di pianificazione del linguaggio di constraint per i modelli di linguaggio di grandi dimensioni.\"", "metrics": {"bleu_score": 14.757581190431866, "chrf_score": 58.38395500337958, "xcomet_score": 0.41092926263809204, "xcomet_qe_score": 0.44288769364356995, "metricx_score": 7.133769512176514, "metricx_qe_score": 6.790453910827637, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Poiché non esiste un sito di dati specifici per individuare i nostri punti di partenza.\" Dobbiamo acquisire questo codice in primo luogo.", "metrics": {"bleu_score": 5.538030403862677, "chrf_score": 43.9544454977689, "xcomet_score": 0.3308538794517517, "xcomet_qe_score": 0.3415378928184509, "metricx_score": 15.08491039276123, "metricx_qe_score": 16.07219696044922, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nella tabella, estendiamo il codice astratto con vincoli a più facce per l'acquisizione di dati in loop con l'uomo in loop utilizzando Instruct GPT.", "metrics": {"bleu_score": 28.008294138571458, "chrf_score": 67.99879518935816, "xcomet_score": 0.729995846748352, "xcomet_qe_score": 0.8042372465133667, "metricx_score": 8.029184341430664, "metricx_qe_score": 7.681514263153076, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Evaluiamo 100 obiettivi specifici e valutiamo i testi generati da modelli a larga scala.\"", "metrics": {"bleu_score": 26.220676436185983, "chrf_score": 58.533730168020384, "xcomet_score": 0.6874796152114868, "xcomet_qe_score": 0.7238078117370605, "metricx_score": 6.469853401184082, "metricx_qe_score": 6.385090351104736, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Questo rapporto riporta la valutazione complessiva dei risultati.", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 57.57584646076529, "xcomet_score": 0.5244519710540771, "xcomet_qe_score": 0.45734941959381104, "metricx_score": 2.7667994499206543, "metricx_qe_score": 3.475965976715088, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che tutti i rimodelli di allineamento raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici.\"", "metrics": {"bleu_score": 27.098211583470043, "chrf_score": 73.14002449769524, "xcomet_score": 0.5780953168869019, "xcomet_qe_score": 0.6701695919036865, "metricx_score": 6.588626384735107, "metricx_qe_score": 6.4252753257751465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo quindi un'analisi dettagliata per investigare cosa sono i modelli a livello di riga. Illuminano", "metrics": {"bleu_score": 11.69723627015581, "chrf_score": 43.21534393243386, "xcomet_score": 0.13909485936164856, "xcomet_qe_score": 0.11992000043392181, "metricx_score": 17.464553833007812, "metricx_qe_score": 22.40896224975586, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "i dati mostrano che la semantica nella generazione di script è accettabile, ma non è garantita la fedeltà ai vincoli.", "metrics": {"bleu_score": 25.885488143999286, "chrf_score": 57.13634117752252, "xcomet_score": 0.9534838199615479, "xcomet_qe_score": 0.9680138230323792, "metricx_score": 4.200516223907471, "metricx_qe_score": 4.79966402053833, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo esplorando categorie di argomenti più frammentati definiti nel libro \"Waking Home.", "metrics": {"bleu_score": 3.737437943747671, "chrf_score": 31.00901788201964, "xcomet_score": 0.179378479719162, "xcomet_qe_score": 0.20047080516815186, "metricx_score": 16.984018325805664, "metricx_qe_score": 15.472603797912598, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La mappa della testa nella figura mostra che il rendimento del piano varia notevolmente per ragazze di diverse categorie.", "metrics": {"bleu_score": 22.33907739358011, "chrf_score": 43.89830049482474, "xcomet_score": 0.24862056970596313, "xcomet_qe_score": 0.20056137442588806, "metricx_score": 10.463642120361328, "metricx_qe_score": 11.799307823181152, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il nostro studio ha dimostrato che la qualità dell'output dei modelli di livello di riga presenta una varianza elevata, il che porta a prestazioni negative.", "metrics": {"bleu_score": 24.180681260144148, "chrf_score": 59.72301508448745, "xcomet_score": 0.17906197905540466, "xcomet_qe_score": 0.16906313598155975, "metricx_score": 4.437081813812256, "metricx_qe_score": 3.783303737640381, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, adottiamo l'idea di filtri Z sovrapprodotti per migliorare la qualità di generazione.\"", "metrics": {"bleu_score": 30.41149081312327, "chrf_score": 63.65028110159555, "xcomet_score": 0.6356421709060669, "xcomet_qe_score": 0.7036635875701904, "metricx_score": 11.985461235046387, "metricx_qe_score": 11.038305282592773, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Mostriamo i tipi di vincoli con esempi per intract.cpt e otteniamo obiettivi specifici sulla base di obiettivi astratti.\" Poi istru", "metrics": {"bleu_score": 19.940445989088914, "chrf_score": 63.65044202225397, "xcomet_score": 0.30918943881988525, "xcomet_qe_score": 0.18454021215438843, "metricx_score": 9.907137870788574, "metricx_qe_score": 11.559377670288086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "isci GPT per generare script di riferimento per obiettivi specifici.", "metrics": {"bleu_score": 24.808415001701817, "chrf_score": 51.49782365247057, "xcomet_score": 0.32997944951057434, "xcomet_qe_score": 0.4139596223831177, "metricx_score": 13.184401512145996, "metricx_qe_score": 14.08304500579834, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, un modello di filtro viene derivato per selezionare i script fisici.\"", "metrics": {"bleu_score": 20.68720601025941, "chrf_score": 53.38866304411074, "xcomet_score": 0.30806905031204224, "xcomet_qe_score": 0.16240359842777252, "metricx_score": 13.612517356872559, "metricx_qe_score": 11.182771682739258, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Convertiamo i script e gli obiettivi in embedding GPT astratti e calcoliamo la similarità di coseno come punteggi di similarità semantica per misurare la similarità semantica.\"", "metrics": {"bleu_score": 29.12919968412613, "chrf_score": 66.93143180778314, "xcomet_score": 0.6360421180725098, "xcomet_qe_score": 0.6572431325912476, "metricx_score": 7.5995707511901855, "metricx_qe_score": 6.579442501068115, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Inoltre, elimineremo lo script che contiene le parole chiave del vincolo di target.", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.12015910286387, "xcomet_score": 0.4117039442062378, "xcomet_qe_score": 0.2499915361404419, "metricx_score": 9.210559844970703, "metricx_qe_score": 9.659253120422363, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Teniamo solo lo script se il target viene chiamato il più alto nei obiettivi impostati.\"", "metrics": {"bleu_score": 8.133696300152861, "chrf_score": 43.101838017285594, "xcomet_score": 0.8082116842269897, "xcomet_qe_score": 0.42919856309890747, "metricx_score": 12.550506591796875, "metricx_qe_score": 14.100469589233398, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Con il nostro metodo, Instructivity può generare quadrati di qualità superiore.", "metrics": {"bleu_score": 48.41524713034602, "chrf_score": 76.9736248352408, "xcomet_score": 0.5279526710510254, "xcomet_qe_score": 0.48108160495758057, "metricx_score": 12.345168113708496, "metricx_qe_score": 12.143645286560059, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo migliora notevolmente la pianificabilità, sia in termini di completezza semantica che di fedeltà rispetto al vincolo.\"", "metrics": {"bleu_score": 34.30008334736235, "chrf_score": 68.56480594222349, "xcomet_score": 0.9320025444030762, "xcomet_qe_score": 0.9346927404403687, "metricx_score": 0.8773745894432068, "metricx_qe_score": 1.21265709400177, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Poiché i modelli di linguaggio sono costosi da implementare, è essenziale abilitare la pianificazione linguistica di modelli più piccoli e specializzati.", "metrics": {"bleu_score": 67.57784746936484, "chrf_score": 81.45452740151079, "xcomet_score": 0.8818879127502441, "xcomet_qe_score": 0.9527459144592285, "metricx_score": 3.265601873397827, "metricx_qe_score": 2.885514736175537, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Creare insiemi di dati è un passo essenziale per raggiungere questo obiettivo.\"", "metrics": {"bleu_score": 19.67497981115564, "chrf_score": 44.00922007412188, "xcomet_score": 0.8380641341209412, "xcomet_qe_score": 0.9481574892997742, "metricx_score": 2.487917423248291, "metricx_qe_score": 2.0669164657592773, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Tuttavia, gli studi precedenti non consentono la pianificazione di obiettivi specifici e l'annotazione dei dati manualmente è costosa.\"", "metrics": {"bleu_score": 45.43846584021041, "chrf_score": 79.00546278228119, "xcomet_score": 0.9620140790939331, "xcomet_qe_score": 0.9605472087860107, "metricx_score": 1.4935946464538574, "metricx_qe_score": 1.2114593982696533, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Così, seguiamo l'idea di distillazione di conoscenza simbolica per distillare siti di pianificazione linguistica vincolata da modelli di livello di vita.", "metrics": {"bleu_score": 17.767151869436425, "chrf_score": 64.82978949437275, "xcomet_score": 0.5451319217681885, "xcomet_qe_score": 0.44944441318511963, "metricx_score": 13.921591758728027, "metricx_qe_score": 15.239137649536133, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il nostro metodo per costruire un dataset di pianificazione linguistica vincolata si chiama Coscript.\"", "metrics": {"bleu_score": 34.717907430287326, "chrf_score": 69.41927901492552, "xcomet_score": 0.8672258853912354, "xcomet_qe_score": 0.8498903512954712, "metricx_score": 2.651360273361206, "metricx_qe_score": 4.445322513580322, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In totale, generiamo 55.000 obiettivi con script.", "metrics": {"bleu_score": 26.305014340253436, "chrf_score": 64.90667556302921, "xcomet_score": 0.9345595240592957, "xcomet_qe_score": 0.932651698589325, "metricx_score": 3.716531276702881, "metricx_qe_score": 6.551955223083496, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per garantire la qualità della validazione e dei siti di test, chiediamo a lavoratori cloud-sourced di individuare e revisionare le campioni errate.\"", "metrics": {"bleu_score": 15.919638844359847, "chrf_score": 56.57242695669429, "xcomet_score": 0.7061540484428406, "xcomet_qe_score": 0.7426396608352661, "metricx_score": 8.692283630371094, "metricx_qe_score": 8.368820190429688, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Questo grafico mostra la distribuzione delle restrizioni del Corscript.", "metrics": {"bleu_score": 14.991106946711685, "chrf_score": 55.86842171259686, "xcomet_score": 0.8580501079559326, "xcomet_qe_score": 0.7001476883888245, "metricx_score": 4.961004734039307, "metricx_qe_score": 4.501313209533691, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Rileviamo che il Corscript presenta una forte tendenza al plotismo nei goal specifici generati.", "metrics": {"bleu_score": 10.511846841633776, "chrf_score": 45.11627272808855, "xcomet_score": 0.7479565143585205, "xcomet_qe_score": 0.7415139675140381, "metricx_score": 10.290767669677734, "metricx_qe_score": 9.274537086486816, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Con il Corscript, possiamo creare modelli più piccoli ma specializzati per il piano linguistico delle restrizioni.\"", "metrics": {"bleu_score": 27.60258064653157, "chrf_score": 60.7431279020545, "xcomet_score": 0.48364555835723877, "xcomet_qe_score": 0.5105695724487305, "metricx_score": 7.736109733581543, "metricx_qe_score": 6.530975818634033, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Rileviamo che il file funziona sulla velocità del corso può generare script di qualità superiore rispetto a quelli dei modelli di alto livello, ciò indica che i modelli più piccoli possono supportare i modelli più grandi quando addestrati su siti di dati appropriati.\"", "metrics": {"bleu_score": 30.69809210351704, "chrf_score": 59.82889103132264, "xcomet_score": 0.1380329728126526, "xcomet_qe_score": 0.12434858828783035, "metricx_score": 15.96501350402832, "metricx_qe_score": 16.720911026000977, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio di constraint.", "metrics": {"bleu_score": 14.458924666162856, "chrf_score": 58.862205852426385, "xcomet_score": 0.7841839790344238, "xcomet_qe_score": 0.8113739490509033, "metricx_score": 8.072314262390137, "metricx_qe_score": 7.494751453399658, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo sviluppato l'abilità di pianificazione del linguaggio di constraint per i modelli di linguaggio di grande scala e abbiamo sviluppato un metodo di filtro originario per i modelli di grande livello.", "metrics": {"bleu_score": 4.125506455013979, "chrf_score": 44.87055398722657, "xcomet_score": 0.48308467864990234, "xcomet_qe_score": 0.4904615879058838, "metricx_score": 14.001349449157715, "metricx_qe_score": 11.561768531799316, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Noi utilizziamo modelli di linguaggio di grandi dimensioni per generare un insieme di dati di script di alta qualità, Corscript, per la pianificazione del linguaggio.", "metrics": {"bleu_score": 50.67309892897293, "chrf_score": 73.97069944553259, "xcomet_score": 0.6642106771469116, "xcomet_qe_score": 0.7955186367034912, "metricx_score": 5.596071720123291, "metricx_qe_score": 7.537528991699219, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che l'insieme di dati Corset possa essere una risorsa preziosa per quelli che si occupano di pianificazione del linguaggio.", "metrics": {"bleu_score": 25.98254910066299, "chrf_score": 54.798619706378005, "xcomet_score": 0.8047239780426025, "xcomet_qe_score": 0.8419946432113647, "metricx_score": 6.714355945587158, "metricx_qe_score": 7.293154239654541, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il tuo tempo.", "metrics": {"bleu_score": 20.82186541080652, "chrf_score": 41.728518071002036, "xcomet_score": 0.9027782082557678, "xcomet_qe_score": 0.9732532501220703, "metricx_score": 1.8474417924880981, "metricx_qe_score": 2.471120834350586, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Potrai trovare ulteriori dettagli del corso script nel nostro documento.", "metrics": {"bleu_score": 8.913765521398126, "chrf_score": 49.28091808923274, "xcomet_score": 0.7506893873214722, "xcomet_qe_score": 0.7330784797668457, "metricx_score": 6.736647605895996, "metricx_qe_score": 8.19241714477539, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti, mi chiamo Xu Heng.", "metrics": {"bleu_score": 53.7284965911771, "chrf_score": 74.22420132202741, "xcomet_score": 0.605120062828064, "xcomet_qe_score": 0.5394056439399719, "metricx_score": 0.10652722418308258, "metricx_qe_score": 0.0232764333486557, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Oggi presenterò il nostro articolo, gli etichettatori di entità di Connell del 2003, funzionano ancora bene nel", "metrics": {"bleu_score": 24.941747177008256, "chrf_score": 45.76622587981008, "xcomet_score": 0.46520450711250305, "xcomet_qe_score": 0.47192761301994324, "metricx_score": 6.536838531494141, "metricx_qe_score": 5.380060195922852, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "2023. Iniziamo.\"", "metrics": {"bleu_score": 10.682175159905848, "chrf_score": 27.096626106791206, "xcomet_score": 0.14942292869091034, "xcomet_qe_score": 0.1488998681306839, "metricx_score": 14.627866744995117, "metricx_qe_score": 15.862850189208984, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro studio ha investigato il problema della generalizzazione utilizzando il compito di riconoscimento di entità denominate o compito NER.", "metrics": {"bleu_score": 14.081181035058071, "chrf_score": 51.95646596215966, "xcomet_score": 0.9714126586914062, "xcomet_qe_score": 0.9840434789657593, "metricx_score": 3.026683807373047, "metricx_qe_score": 2.2025694847106934, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli hanno utilizzato Kono 2003 per sviluppare l'NER per quasi 20 anni. E questo naturalmente solleva diversi problemi.", "metrics": {"bleu_score": 28.690668742892214, "chrf_score": 71.75343138751347, "xcomet_score": 0.6480461359024048, "xcomet_qe_score": 0.5336110591888428, "metricx_score": 11.806818962097168, "metricx_qe_score": 12.317949295043945, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, questi modelli possono generalizzare sui dati moderni?", "metrics": {"bleu_score": 32.279203460361764, "chrf_score": 72.32294634657775, "xcomet_score": 0.9842640161514282, "xcomet_qe_score": 1.0, "metricx_score": 0.6152074337005615, "metricx_qe_score": 0.5904936790466309, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, quando sviluppiamo nuovi taggers, cosa è necessario per una buona generalizzazione?\"", "metrics": {"bleu_score": 56.35190098079901, "chrf_score": 93.3487458263226, "xcomet_score": 0.8295880556106567, "xcomet_qe_score": 0.8621096611022949, "metricx_score": 4.285630226135254, "metricx_qe_score": 5.505114555358887, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Allo stesso tempo, se osserviamo una generalizzazione cattiva, cosa determina il calo di prestazioni di questi modelli?\"", "metrics": {"bleu_score": 48.59722357681846, "chrf_score": 77.05604663865924, "xcomet_score": 0.9295656681060791, "xcomet_qe_score": 0.9753906726837158, "metricx_score": 1.4011415243148804, "metricx_qe_score": 1.2466061115264893, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Per indagare questi problemi, abbiamo sviluppato il dataset Carnot++.", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 64.62197401946311, "xcomet_score": 0.6732965111732483, "xcomet_qe_score": 0.8752248287200928, "metricx_score": 7.7881927490234375, "metricx_qe_score": 8.24625301361084, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un dataset che abbiamo raccolto dai notiziari Reuters del 2020 e li abbiamo annotati seguendo le stesse linee guida di annotazione del Carnot 2003.\"", "metrics": {"bleu_score": 30.958265761546162, "chrf_score": 61.5505635298923, "xcomet_score": 0.7254534959793091, "xcomet_qe_score": 0.7702397108078003, "metricx_score": 4.64340877532959, "metricx_qe_score": 4.860393047332764, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi sottoposto a fine-tuning oltre 20 modelli sul Kono 2003.", "metrics": {"bleu_score": 16.59038701421971, "chrf_score": 57.762572687054956, "xcomet_score": 0.6489881873130798, "xcomet_qe_score": 0.7052839994430542, "metricx_score": 8.131326675415039, "metricx_qe_score": 9.104299545288086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Li abbiamo valutati su entrambe la serie di test Kono 03 e la serie di test Kono++.", "metrics": {"bleu_score": 14.025775160081468, "chrf_score": 42.99506295954573, "xcomet_score": 0.6606123447418213, "xcomet_qe_score": 0.8146035671234131, "metricx_score": 10.887992858886719, "metricx_qe_score": 8.612800598144531, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E ultima ma non meno importante, abbiamo calcolato il cambiamento percentuale di F1 per valutare la generalizzazione di ogni modello.\"", "metrics": {"bleu_score": 46.66756931662377, "chrf_score": 73.20309868531692, "xcomet_score": 0.9051038026809692, "xcomet_qe_score": 0.9091642498970032, "metricx_score": 4.1908278465271, "metricx_qe_score": 4.014216899871826, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco cosa è necessario per una buona generalizzazione.", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 64.19269123581225, "xcomet_score": 0.9884963035583496, "xcomet_qe_score": 0.9992532730102539, "metricx_score": 0.3019939064979553, "metricx_qe_score": 0.33187606930732727, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari.", "metrics": {"bleu_score": 33.31931889536821, "chrf_score": 68.51595181194114, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.12964464724063873, "metricx_qe_score": 0.2073884904384613, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979089498519897, "xcomet_qe_score": 0.986407995223999, "metricx_score": 0.9279693365097046, "metricx_qe_score": 1.474259853363037, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalizzano meglio i dati nuovi.", "metrics": {"bleu_score": 49.701084283458314, "chrf_score": 72.41285554975445, "xcomet_score": 0.8972903490066528, "xcomet_qe_score": 0.9709413051605225, "metricx_score": 5.692267417907715, "metricx_qe_score": 7.413829326629639, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9956880807876587, "xcomet_qe_score": 0.9719719886779785, "metricx_score": 0.8006240725517273, "metricx_qe_score": 1.179135799407959, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che i modelli più grandi portano a una maggiore generalizzazione.", "metrics": {"bleu_score": 53.45211483269467, "chrf_score": 80.87570042263023, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.0880964994430542, "metricx_qe_score": 1.8409984111785889, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E infine, tutti sappiamo che l'effetto di fine-tuning sulle esempi direttamente influenza il rendimento di una task downstream. Anche", "metrics": {"bleu_score": 2.4219581894861424, "chrf_score": 34.108160729690454, "xcomet_score": 0.4167107045650482, "xcomet_qe_score": 0.5209280252456665, "metricx_score": 15.01955795288086, "metricx_qe_score": 12.904609680175781, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in questo caso, abbiamo scoperto che più esempi di fine-tuning portano a una migliore generalizzazione.", "metrics": {"bleu_score": 32.128996628565346, "chrf_score": 60.12272198488161, "xcomet_score": 0.9514504075050354, "xcomet_qe_score": 0.9667133092880249, "metricx_score": 4.7190775871276855, "metricx_qe_score": 4.850764751434326, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco la nostra prossima domanda: cosa determina il calo di prestazioni di alcuni modelli.\" Abbiamo due ipotesi.", "metrics": {"bleu_score": 18.72867462785877, "chrf_score": 55.36007288394469, "xcomet_score": 0.9555814266204834, "xcomet_qe_score": 0.9455206394195557, "metricx_score": 1.4165664911270142, "metricx_qe_score": 1.253063440322876, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La prima è l'overfitting adattativo, che è l'overfitting causato dall'utilizzo dello stesso set di test più volte. E questo è usualmente manifestato come diminuzione dei ritorni su un nuovo set di test.", "metrics": {"bleu_score": 34.1839285599809, "chrf_score": 59.09384682731266, "xcomet_score": 0.7621637582778931, "xcomet_qe_score": 0.8852479457855225, "metricx_score": 6.849358558654785, "metricx_qe_score": 6.332988739013672, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ipotesi è la deriva temporale, ovvero la degradazione del rendimento causata dal sempre maggiore intervallo temporale tra i dati di training e quelli di test.", "metrics": {"bleu_score": 29.459048651484498, "chrf_score": 52.08348802909014, "xcomet_score": 0.7394895553588867, "xcomet_qe_score": 0.7585492134094238, "metricx_score": 3.8173580169677734, "metricx_qe_score": 3.413612127304077, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo visto che, dal grafico a destra, la linea di miglior adattamento rossa ha un gradiente maggiore di 1. Questo", "metrics": {"bleu_score": 30.018185420487157, "chrf_score": 62.55338418505909, "xcomet_score": 0.5632965564727783, "xcomet_qe_score": 0.5856015682220459, "metricx_score": 8.019553184509277, "metricx_qe_score": 3.6687371730804443, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "significa che ogni unità di miglioramento che abbiamo realizzato sulla colonna 2003 si traduce in più di una unità di miglioramento sulla colonna plus plus, il che significa che non ci sono ritorni decrescenti.", "metrics": {"bleu_score": 43.86664099524953, "chrf_score": 68.27547420030574, "xcomet_score": 0.5446039438247681, "xcomet_qe_score": 0.6178598403930664, "metricx_score": 11.275594711303711, "metricx_qe_score": 9.91059398651123, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che ciò ci mostra che l'overfitting adattivo in questo caso non viene osservato.\" \"E allora, come si", "metrics": {"bleu_score": 16.299446731288935, "chrf_score": 50.022368232711415, "xcomet_score": 0.22649037837982178, "xcomet_qe_score": 0.2267073094844818, "metricx_score": 18.092884063720703, "metricx_qe_score": 7.548810005187988, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "chiama Temporary Trif?\" \"Per", "metrics": {"bleu_score": 5.693025330278465, "chrf_score": 18.315215008316063, "xcomet_score": 0.1134272962808609, "xcomet_qe_score": 0.09495973587036133, "metricx_score": 20.96729850769043, "metricx_qe_score": 15.043252944946289, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "il progetto Temporal Drift, abbiamo condotto un esperimento per rieducare o continuare a preaddestrare alcuni modelli con dati più recenti. E abbiamo trovato che il rendimento peggiora con un divario temporale più grande.\" \"Ecco che conferma la nostra ipotesi che la principale causa del calo di prestazioni è il drift temporale.\"", "metrics": {"bleu_score": 24.65422223179695, "chrf_score": 60.141634866797546, "xcomet_score": 0.7853364944458008, "xcomet_qe_score": 0.882739782333374, "metricx_score": 5.716674327850342, "metricx_qe_score": 4.625300407409668, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro conclusione è che per una buona generalizzazione, avremmo bisogno di un architetto del modello più efficiente, di un modello più grande, nonché di esempi di fine-tuning più numerosi. E", "metrics": {"bleu_score": 31.81824677903223, "chrf_score": 61.24426031982397, "xcomet_score": 0.5916098356246948, "xcomet_qe_score": 0.6365736722946167, "metricx_score": 8.410994529724121, "metricx_qe_score": 6.253201007843018, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "questi obiettivi, mano nella mano, non possiamo avere un ingrediente solo, ma dobbiamo tener conto degli altri.", "metrics": {"bleu_score": 14.528679532351443, "chrf_score": 50.50373634326174, "xcomet_score": 0.8031424283981323, "xcomet_qe_score": 0.7548724412918091, "metricx_score": 7.508474349975586, "metricx_qe_score": 8.322477340698242, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Nello stesso tempo, abbiamo scoperto che il calo di prestazioni è causato da deriva temporale e sorprendentemente, non è causato da sovrappesata adattiva, nonostante Conno2003 sia stato utilizzato per oltre 20 anni.\"", "metrics": {"bleu_score": 41.621437890225856, "chrf_score": 66.47790954393444, "xcomet_score": 0.5661321878433228, "xcomet_qe_score": 0.6159643530845642, "metricx_score": 8.245899200439453, "metricx_qe_score": 8.552087783813477, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ritornando alla domanda che abbiamo sollevato nel titolo del nostro studio, i Tagger di Connell del 2003 funzionano ancora nel 2023? E", "metrics": {"bleu_score": 34.20941751050785, "chrf_score": 63.11989451285632, "xcomet_score": 0.7762728333473206, "xcomet_qe_score": 0.7799369692802429, "metricx_score": 5.92480993270874, "metricx_qe_score": 3.416339635848999, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo scoperto che la risposta è un sì assoluto.\"", "metrics": {"bleu_score": 36.029870008455966, "chrf_score": 62.96088708366043, "xcomet_score": 0.914054274559021, "xcomet_qe_score": 0.6483891010284424, "metricx_score": 0.9881619811058044, "metricx_qe_score": 0.705612301826477, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro studio richieda ulteriore ricerca sulle generalizzazioni dei modelli per migliorarle.", "metrics": {"bleu_score": 13.998923433343181, "chrf_score": 61.943821636920504, "xcomet_score": 0.8718880414962769, "xcomet_qe_score": 0.8679471611976624, "metricx_score": 1.8311104774475098, "metricx_qe_score": 1.0593714714050293, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, per ultima cosa, assicuratevi di consultare il nostro articolo, il nostro insieme di dati. Se avete delle domande, non esitate a contattarmi.", "metrics": {"bleu_score": 44.18975777649827, "chrf_score": 74.5272659389656, "xcomet_score": 0.9593489170074463, "xcomet_qe_score": 0.9426368474960327, "metricx_score": 1.432542085647583, "metricx_qe_score": 1.139945149421692, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie molto.", "metrics": {"bleu_score": 0.0, "chrf_score": 48.343554593554586, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.15248426795005798, "metricx_qe_score": 0.20665110647678375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao,", "metrics": {"bleu_score": 0.0, "chrf_score": 54.333333333333336, "xcomet_score": 0.9947192668914795, "xcomet_qe_score": 0.9683444499969482, "metricx_score": 0.1089133620262146, "metricx_qe_score": 0.09066226333379745, "linguapy_score": [1, "MAORI"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "parlerò del nostro lavoro sul risolvere espressioni di riferimento indiretto per la selezione di entità, in cui introduciamo la punteggio Alt entità.\"", "metrics": {"bleu_score": 14.521383677815454, "chrf_score": 48.82345964970366, "xcomet_score": 0.5085067749023438, "xcomet_qe_score": 0.5453998446464539, "metricx_score": 9.414443016052246, "metricx_qe_score": 10.653078079223633, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Jawad Hosseini e questo è un lavoro congiunto con Philippe Ladinsky, Sylvia Parry e Annie Lewis.", "metrics": {"bleu_score": 35.78835436145691, "chrf_score": 59.735792754778586, "xcomet_score": 0.4870527386665344, "xcomet_qe_score": 0.5468958616256714, "metricx_score": 3.5268654823303223, "metricx_qe_score": 1.0850718021392822, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Non ho capito il testo che hai fornito. Il testo sembra essere composto da una ripetizione", "metrics": {"bleu_score": 3.2342452920962157, "chrf_score": 17.957683523987075, "xcomet_score": 0.1253167986869812, "xcomet_qe_score": 0.1163172647356987, "metricx_score": 15.186864852905273, "metricx_qe_score": 19.03069496154785, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "di \"درستان\" e non contiene", "metrics": {"bleu_score": 0.0, "chrf_score": 6.89248938809921, "xcomet_score": 0.1142716258764267, "xcomet_qe_score": 0.11055207997560501, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "alcun significato intelligibile in inglese. Per favore, fornisci un testo", "metrics": {"bleu_score": 0.0, "chrf_score": 12.052296243151655, "xcomet_score": 0.12240808457136154, "xcomet_qe_score": 0.10686568170785904, "metricx_score": 5.830137729644775, "metricx_qe_score": 9.948612213134766, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "inglese più comprensibile e specifico che desideri tradurre in italiano.", "metrics": {"bleu_score": 3.42209762272661, "chrf_score": 16.851886200388584, "xcomet_score": 0.1269790083169937, "xcomet_qe_score": 0.1242116168141365, "metricx_score": 22.83955955505371, "metricx_qe_score": 24.347097396850586, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il più ovvio è utilizzare la differenza diretta. Ad esempio, dicendo il nome della canzone è in me o il suo posizione, la prima.", "metrics": {"bleu_score": 11.458346677361849, "chrf_score": 45.86696877137774, "xcomet_score": 0.4290180206298828, "xcomet_qe_score": 0.48522916436195374, "metricx_score": 17.883386611938477, "metricx_qe_score": 19.601642608642578, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ma a volte una referenza indiretta è più appropriata per avere una conversazione più naturale.", "metrics": {"bleu_score": 46.15425015629848, "chrf_score": 75.80140810482621, "xcomet_score": 0.9596118927001953, "xcomet_qe_score": 0.9594414234161377, "metricx_score": 0.947436511516571, "metricx_qe_score": 1.376850962638855, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò può accadere quando l'utente non ricorda il titolo della canzone.\"", "metrics": {"bleu_score": 26.776802959776298, "chrf_score": 59.22261263047513, "xcomet_score": 0.9757766723632812, "xcomet_qe_score": 0.9846929311752319, "metricx_score": 0.2590170204639435, "metricx_qe_score": 0.1957612931728363, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco la traduzione: \"Perché deve essere così deve essere deve essere deve essere deve essere", "metrics": {"bleu_score": 0.0, "chrf_score": 9.772369521636659, "xcomet_score": 0.12682759761810303, "xcomet_qe_score": 0.13228754699230194, "metricx_score": 22.920637130737305, "metricx_qe_score": 23.347450256347656, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere deve essere", "metrics": {"bleu_score": 0.0, "chrf_score": 0.08903930829510709, "xcomet_score": 0.1904279887676239, "xcomet_qe_score": 0.1907082498073578, "metricx_score": 1.7558753490447998, "metricx_qe_score": 1.7558753490447998, "linguapy_score": [1, "MAORI"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco la traduzione: Oh oh oh, dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal", "metrics": {"bleu_score": 2.1671320168371846, "chrf_score": 8.277146772066137, "xcomet_score": 0.11695250868797302, "xcomet_qe_score": 0.09015962481498718, "metricx_score": 21.394784927368164, "metricx_qe_score": 23.988588333129883, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal dal Ecco il problema è molto importante anche per i sistemi di conversazione e per la valutazione delle capacità delle LLM di comprensione degli enti. Non vedo non vedo", "metrics": {"bleu_score": 0.026421091268141832, "chrf_score": 2.4255697374266787, "xcomet_score": 0.2110949158668518, "xcomet_qe_score": 0.21093328297138214, "metricx_score": 13.388721466064453, "metricx_qe_score": 13.388721466064453, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non", "metrics": {"bleu_score": 0.0, "chrf_score": 5.8030207592581196, "xcomet_score": 0.09098568558692932, "xcomet_qe_score": 0.08768647909164429, "metricx_score": 10.880655288696289, "metricx_qe_score": 11.208894729614258, "linguapy_score": [1, "VIETNAMESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non", "metrics": {"bleu_score": 0.0, "chrf_score": 6.06906337998107, "xcomet_score": 0.10381539165973663, "xcomet_qe_score": 0.1316634714603424, "metricx_score": 17.94829750061035, "metricx_qe_score": 18.80607032775879, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non vedo non Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un insieme di completamento a fumetti.", "metrics": {"bleu_score": 0.22428232393573433, "chrf_score": 3.6008183901011988, "xcomet_score": 0.1906985193490982, "xcomet_qe_score": 0.19059284031391144, "metricx_score": 12.900469779968262, "metricx_qe_score": 12.900469779968262, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.11902716755867004, "xcomet_qe_score": 0.06483901292085648, "metricx_score": 14.325580596923828, "metricx_qe_score": 16.507530212402344, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ricorda quella canzone che ascoltavamo ieri?", "metrics": {"bleu_score": 8.498002895756983, "chrf_score": 47.83533406418361, "xcomet_score": 0.541383683681488, "xcomet_qe_score": 0.23525340855121613, "metricx_score": 6.673371315002441, "metricx_qe_score": 6.931943893432617, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E con ciò, Bob dice il contesto del dialogo.\"", "metrics": {"bleu_score": 36.462858619364674, "chrf_score": 54.87984294260204, "xcomet_score": 0.6341474056243896, "xcomet_qe_score": 0.9080744981765747, "metricx_score": 5.061559677124023, "metricx_qe_score": 4.5234222412109375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Nel secondo balloon di parlato, Alice dice: Vuoi dire facile per me o ho un'idea?\" Ecco la traduzione:", "metrics": {"bleu_score": 12.586347848916265, "chrf_score": 29.391330225266977, "xcomet_score": 0.12738722562789917, "xcomet_qe_score": 0.14392219483852386, "metricx_score": 13.606790542602539, "metricx_qe_score": 14.271349906921387, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Cui è l'alternativa", "metrics": {"bleu_score": 9.688464563433238, "chrf_score": 42.75115905700836, "xcomet_score": 0.16397681832313538, "xcomet_qe_score": 0.20455533266067505, "metricx_score": 7.945684909820557, "metricx_qe_score": 16.65892791748047, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e, nel terzo balloon di parola, Bob utilizza un riferimento indiretto per selezionare uno di questi enti, ad esempio il più recente.\"", "metrics": {"bleu_score": 21.84971203531727, "chrf_score": 67.65673292776728, "xcomet_score": 0.6701030135154724, "xcomet_qe_score": 0.686439037322998, "metricx_score": 10.442291259765625, "metricx_qe_score": 10.515682220458984, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Non ho capito. La frase fornita non sembra essere inglese, ma piuttosto un misto di suoni e parole che non hanno un significato comprensibile.", "metrics": {"bleu_score": 2.0752738657056446, "chrf_score": 22.097495576494158, "xcomet_score": 0.1330193728208542, "xcomet_qe_score": 0.13454119861125946, "metricx_score": 5.0789690017700195, "metricx_qe_score": 8.744011878967285, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Potresti ripetere o fornire ulteriore contesto per aiutarmi", "metrics": {"bleu_score": 3.7954847898457067, "chrf_score": 15.893461000604095, "xcomet_score": 0.12157117575407028, "xcomet_qe_score": 0.10925029218196869, "metricx_score": 18.61366081237793, "metricx_qe_score": 20.83060646057129, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "a tradurre il testo? Il secondo, che è la domanda alternativa, viene generato in questo modo.", "metrics": {"bleu_score": 35.705835125874, "chrf_score": 68.9440207787055, "xcomet_score": 0.16986805200576782, "xcomet_qe_score": 0.24055162072181702, "metricx_score": 9.555435180664062, "metricx_qe_score": 14.145476341247559, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Siamo sempre in possesso di un template semplice.", "metrics": {"bleu_score": 9.980099403873663, "chrf_score": 52.20218879766762, "xcomet_score": 0.4239380955696106, "xcomet_qe_score": 0.6287875175476074, "metricx_score": 5.783920764923096, "metricx_qe_score": 3.6237387657165527, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Vuoi dire A o B?", "metrics": {"bleu_score": 50.81327481546149, "chrf_score": 22.608679850286492, "xcomet_score": 0.911587119102478, "xcomet_qe_score": 0.9165595769882202, "metricx_score": 0.49977031350135803, "metricx_qe_score": 0.31901565194129944, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "A e B sono campioni da Wikipedia.\"", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 87.76832158902569, "xcomet_score": 0.813178300857544, "xcomet_qe_score": 0.47810113430023193, "metricx_score": 1.3080211877822876, "metricx_qe_score": 1.8978703022003174, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2230200171470642, "metricx_qe_score": 0.14473919570446014, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quando ci muoviamo verso l'alto nella lista, le entità diventano sempre più simili tra loro e di solito è più difficile effettuare la disambiguazione.", "metrics": {"bleu_score": 47.114811355950046, "chrf_score": 69.22867008884458, "xcomet_score": 0.9846956729888916, "xcomet_qe_score": 0.9965037107467651, "metricx_score": 1.053120493888855, "metricx_qe_score": 1.5425859689712524, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "L'uno è Uniform Attract.", "metrics": {"bleu_score": 10.400597689005304, "chrf_score": 20.381159404919316, "xcomet_score": 0.1490764617919922, "xcomet_qe_score": 0.24785098433494568, "metricx_score": 15.614139556884766, "metricx_qe_score": 16.591825485229492, "linguapy_score": [1, "FRENCH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso è quando gli enti hanno titoli simili. Ad esempio, due libri con lo stesso nome, il retail.", "metrics": {"bleu_score": 12.635227789398094, "chrf_score": 53.14003611275457, "xcomet_score": 0.6583705544471741, "xcomet_qe_score": 0.6588819026947021, "metricx_score": 7.880295753479004, "metricx_qe_score": 10.551121711730957, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso è quando hanno descrizioni simili su Wikipedia", "metrics": {"bleu_score": 31.239399369202552, "chrf_score": 68.53540194330624, "xcomet_score": 0.9882956743240356, "xcomet_qe_score": 0.9953029155731201, "metricx_score": 0.42404502630233765, "metricx_qe_score": 0.6692565083503723, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e infine quando hanno infobox o attributi simili su Wikipedia.", "metrics": {"bleu_score": 40.71631883019392, "chrf_score": 63.232688244771005, "xcomet_score": 0.9516196250915527, "xcomet_qe_score": 0.9555023908615112, "metricx_score": 2.5654938220977783, "metricx_qe_score": 2.8035833835601807, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, lo stesso genere o lo stesso artista per una canzone.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6580236554145813, "metricx_qe_score": 0.7779831886291504, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Posso mostrarvi questa domanda alternativa all'risposta. Essi conoscono il nome di queste entità, ma non necessariamente ne conoscono l'entità.\"", "metrics": {"bleu_score": 44.164168100551116, "chrf_score": 68.58968167721856, "xcomet_score": 0.6472905874252319, "xcomet_qe_score": 0.479412317276001, "metricx_score": 5.862181186676025, "metricx_qe_score": 7.357130527496338, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Così, mostriamo alcune conoscenze di background sulle due entità.", "metrics": {"bleu_score": 24.38870552576843, "chrf_score": 54.8949093613809, "xcomet_score": 0.8545379638671875, "xcomet_qe_score": 0.9349961280822754, "metricx_score": 5.631171703338623, "metricx_qe_score": 4.464574337005615, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per le canzoni, semplicemente mostriamo un link di ricerca Google per ogni canzone. E chiedere agli annotatori di ascoltare almeno alcuni di ogni canzone e leggere su ogni canzone.", "metrics": {"bleu_score": 33.41840293142459, "chrf_score": 65.20466746169102, "xcomet_score": 0.7075850963592529, "xcomet_qe_score": 0.7634587287902832, "metricx_score": 6.7868499755859375, "metricx_qe_score": 6.290465831756592, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco ad esempio il risultato della ricerca Google per la canzone EasyHunt.", "metrics": {"bleu_score": 46.63449625549861, "chrf_score": 80.1466684239776, "xcomet_score": 0.7155380249023438, "xcomet_qe_score": 0.7243005037307739, "metricx_score": 8.350814819335938, "metricx_qe_score": 8.79757022857666, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di background da Wikipedia.", "metrics": {"bleu_score": 82.82477531331043, "chrf_score": 87.96820265513713, "xcomet_score": 0.9407081604003906, "xcomet_qe_score": 0.9468480348587036, "metricx_score": 5.131077766418457, "metricx_qe_score": 5.1052751541137695, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per le ricette, mostriamo inoltre le loro immagini da Wikipedia, in modo che gli annotatori sappiano come appaiano.", "metrics": {"bleu_score": 51.81186146817628, "chrf_score": 71.10453574128252, "xcomet_score": 0.9869409799575806, "xcomet_qe_score": 0.9862638711929321, "metricx_score": 1.6670013666152954, "metricx_qe_score": 1.4336961507797241, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo", "metrics": {"bleu_score": 0.8733042428534794, "chrf_score": 8.346458211858502, "xcomet_score": 0.09616938978433609, "xcomet_qe_score": 0.09619569033384323, "metricx_score": 18.05585479736328, "metricx_qe_score": 18.08858299255371, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo", "metrics": {"bleu_score": 0.0, "chrf_score": 7.102441752540516, "xcomet_score": 0.10601885616779327, "xcomet_qe_score": 0.11578120291233063, "metricx_score": 4.658898830413818, "metricx_qe_score": 7.123600959777832, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "non abbiamo non abbiamo non abbiamo non abbiamo non", "metrics": {"bleu_score": 0.0, "chrf_score": 6.388021397494531, "xcomet_score": 0.12361089140176773, "xcomet_qe_score": 0.1366756409406662, "metricx_score": 13.382522583007812, "metricx_qe_score": 9.668312072753906, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non abbiamo non avere \"Da dove vengo, non ci sono mai stati problemi con il mio ascolto attento e la mia capacità", "metrics": {"bleu_score": 0.13536088576835803, "chrf_score": 2.359652248825473, "xcomet_score": 0.17464515566825867, "xcomet_qe_score": 0.17495690286159515, "metricx_score": 23.66434669494629, "metricx_qe_score": 21.399045944213867, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "di comprendere e tradurre il linguaggio umano.\" Il corpus delle L-entità ha 6.000 alternative domande sparse su tre domini e 42.000 espressioni riferite indirette.", "metrics": {"bleu_score": 8.50126549787338, "chrf_score": 53.977582518926994, "xcomet_score": 0.2225385159254074, "xcomet_qe_score": 0.18464815616607666, "metricx_score": 13.789156913757324, "metricx_qe_score": 14.622020721435547, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I risultati con il modello T5 grande sono riassunti di seguito.", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 88.64975019479519, "xcomet_score": 0.9814425110816956, "xcomet_qe_score": 0.9846947193145752, "metricx_score": 3.760561466217041, "metricx_qe_score": 4.212826728820801, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso allo stesso background di conoscenze degli annotatori, allora l'accuratezza è veramente alta. È intorno all'92% al 95%.", "metrics": {"bleu_score": 40.29883231412914, "chrf_score": 71.9877101422897, "xcomet_score": 0.932861328125, "xcomet_qe_score": 0.9357165098190308, "metricx_score": 1.6223281621932983, "metricx_qe_score": 1.2063934803009033, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è realistico.", "metrics": {"bleu_score": 53.7284965911771, "chrf_score": 71.25210671220768, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3809109628200531, "metricx_qe_score": 0.5978683233261108, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se non abbiamo il modo di fare un modello di ascolto da utilizzare, non abbiamo modo di fare un'analisi statistica significativa, non possiamo neanche fare un'analisi di regressione line", "metrics": {"bleu_score": 1.877081263233555, "chrf_score": 23.170732052427613, "xcomet_score": 0.18842917680740356, "xcomet_qe_score": 0.17107746005058289, "metricx_score": 25.0, "metricx_qe_score": 24.24258041381836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "are, non possiamo fare nulla.", "metrics": {"bleu_score": 3.3142882018868, "chrf_score": 9.064064514074802, "xcomet_score": 0.1375388652086258, "xcomet_qe_score": 0.13085679709911346, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi di entità, allora la precisione è solo del 60%. Ci sono quindi molto spazio per migliorare.", "metrics": {"bleu_score": 43.70614964591187, "chrf_score": 60.61699030278429, "xcomet_score": 0.9238200187683105, "xcomet_qe_score": 0.9456582069396973, "metricx_score": 3.9121174812316895, "metricx_qe_score": 2.855403184890747, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche dimostrato che i modelli sono generalizzabili a dominio.", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 91.42643238814676, "xcomet_score": 0.9242057204246521, "xcomet_qe_score": 0.9203038215637207, "metricx_score": 0.7068684101104736, "metricx_qe_score": 1.1871083974838257, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un collegamento al nostro dataset.", "metrics": {"bleu_score": 14.759564526951554, "chrf_score": 41.7501910941409, "xcomet_score": 0.9829796552658081, "xcomet_qe_score": 1.0, "metricx_score": 2.02585506439209, "metricx_qe_score": 2.6897261142730713, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.05380218103528023, "metricx_qe_score": 0.31349149346351624, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, sono Sara Pappi dell'Università di Trento e Fondazione Bruno Kessler, e presenterò brevemente il lavoro congiunto con Matteo Negri e Marco Turchi intitolato", "metrics": {"bleu_score": 30.957811479117776, "chrf_score": 63.100277497453014, "xcomet_score": 0.6884041428565979, "xcomet_qe_score": 0.6676700711250305, "metricx_score": 10.857625007629395, "metricx_qe_score": 8.886204719543457, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "'Attenzione come guida per la traduzione simultanea'. \"La", "metrics": {"bleu_score": 8.392229812593097, "chrf_score": 56.71473940030576, "xcomet_score": 0.21424078941345215, "xcomet_qe_score": 0.1137506365776062, "metricx_score": 19.798744201660156, "metricx_qe_score": 20.329397201538086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "traduzione simultanea del discorso, o SIMUL-ST, è il processo di traduzione di un linguaggio parlato in tempo reale in un testo in un'altra lingua, abilitando la comunicazione interlinguistica.\"", "metrics": {"bleu_score": 40.24387097832444, "chrf_score": 68.1397932489768, "xcomet_score": 0.8753726482391357, "xcomet_qe_score": 0.8668566346168518, "metricx_score": 3.6960551738739014, "metricx_qe_score": 3.0909855365753174, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli di stimolo attu", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 61.60026988974266, "xcomet_score": 0.6216593980789185, "xcomet_qe_score": 0.6543784141540527, "metricx_score": 9.336224555969238, "metricx_qe_score": 7.673211097717285, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ali? Gli architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare.", "metrics": {"bleu_score": 31.61487584488944, "chrf_score": 74.2822231745226, "xcomet_score": 0.6175743341445923, "xcomet_qe_score": 0.5270904302597046, "metricx_score": 14.217512130737305, "metricx_qe_score": 15.932401657104492, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Procedimenti di addestramento lunghi e complessi, ad esempio addestramenti che coinvolgono diversi obiettivi di ottimizzazione.", "metrics": {"bleu_score": 31.052640420267018, "chrf_score": 74.96796369182871, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.7728147506713867, "metricx_qe_score": 2.167836904525757, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Eseguire e mantenere diversi modelli per raggiungere diversi regimi di latenza.", "metrics": {"bleu_score": 48.901978132725134, "chrf_score": 63.164062641879994, "xcomet_score": 0.718978762626648, "xcomet_qe_score": 0.8159060478210449, "metricx_score": 6.934290885925293, "metricx_qe_score": 5.6103973388671875, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi di latenza e così via.\"", "metrics": {"bleu_score": 60.76181533602872, "chrf_score": 76.16885356483807, "xcomet_score": 0.9499472379684448, "xcomet_qe_score": 0.9457328915596008, "metricx_score": 0.8268028497695923, "metricx_qe_score": 1.1452761888504028, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Qual è la nostra soluzione?\"", "metrics": {"bleu_score": 68.037493331712, "chrf_score": 98.07412870252269, "xcomet_score": 0.9587658643722534, "xcomet_qe_score": 0.9468388557434082, "metricx_score": 0.4870586693286896, "metricx_qe_score": 0.5333978533744812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Primo, utilizzare modelli offline di SD esistenti senza rieducare o adottare un'architettura specifica per un singolo regime di latenza.", "metrics": {"bleu_score": 19.268479640608692, "chrf_score": 62.42719553037289, "xcomet_score": 0.4411911070346832, "xcomet_qe_score": 0.5279489159584045, "metricx_score": 9.961445808410645, "metricx_qe_score": 10.544750213623047, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Utilizzare solo un modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. \"Ecco", "metrics": {"bleu_score": 74.83293841345248, "chrf_score": 91.72238258875225, "xcomet_score": 0.6094352006912231, "xcomet_qe_score": 0.4491623342037201, "metricx_score": 4.873002529144287, "metricx_qe_score": 4.120361328125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "come si può sfruttare la conoscenza già acquisita dal modello attraverso la meccanica di attenzione tra l'input audio e l'output testuale,", "metrics": {"bleu_score": 40.873575197739754, "chrf_score": 82.66165048688624, "xcomet_score": 0.8235869407653809, "xcomet_qe_score": 0.8581430315971375, "metricx_score": 2.7218852043151855, "metricx_qe_score": 3.546574115753174, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ovvero la meccanica di cross-attenzione. E potete vedere un esempio a destra.\"", "metrics": {"bleu_score": 8.225964699966553, "chrf_score": 56.499847858343735, "xcomet_score": 0.7041640281677246, "xcomet_qe_score": 0.7251274585723877, "metricx_score": 5.337042808532715, "metricx_qe_score": 6.33284330368042, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un codice o encoder dell'attenzione e si tratta di una strategia per decidere se emettere o meno una traduzione parziale in base a dove gli indici di attenzione puntano.", "metrics": {"bleu_score": 44.21973227177668, "chrf_score": 67.68278711392422, "xcomet_score": 0.7356972694396973, "xcomet_qe_score": 0.7269809246063232, "metricx_score": 6.706387996673584, "metricx_qe_score": 7.711703300476074, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Una parola è emessa se la tensione non è concentrata, ovvero tale somma è inferiore a un certo soglia alpha, verso gli ultimi frame di pitch della lambda, il che significa che l'informazione ricevuta è sufficientemente stabile.", "metrics": {"bleu_score": 21.529444525383653, "chrf_score": 55.345526085674216, "xcomet_score": 0.5416102409362793, "xcomet_qe_score": 0.5213143229484558, "metricx_score": 7.708803176879883, "metricx_qe_score": 6.010315418243408, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio: se riceviamo un file di speech contenente \"I'm going to talk about\" e il nostro modello predice la traduzione in tedesco, E esamineremo le pesi di attenzione incrociata. \"Vedremo che i primi due punti si riferiscono ai frame di parlato più antichi ricevuti, mentre l'ultimo punto si riferisce ai frame di parlato più recenti ricevuti come frame lambda di", "metrics": {"bleu_score": 16.338184763256656, "chrf_score": 53.540985807042915, "xcomet_score": 0.18129149079322815, "xcomet_qe_score": 0.4238976836204529, "metricx_score": 13.474445343017578, "metricx_qe_score": 11.172479629516602, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "parlato.\" \"Questo significa che i primi due termini saranno omissi.\" Mentre la somma dell'attenzione incrociata è superiore a un certo livello alpha, non emetteremo l'ultima parola e aspetteremo un'altra porzione di discorso.", "metrics": {"bleu_score": 21.14696811796027, "chrf_score": 52.28809187972784, "xcomet_score": 0.3513547480106354, "xcomet_qe_score": 0.30374675989151, "metricx_score": 11.035364151000977, "metricx_qe_score": 10.796411514282227, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se andiamo avanti e riceviamo un'altra speech tank e il nostro modello predice altre tre parole e osserveremo i pesi di attenzione incrociata. \" Vedremo che nessun punto si riferisce alle ultime frame di parlato lambda.\"", "metrics": {"bleu_score": 16.434349396840393, "chrf_score": 55.90919264450333, "xcomet_score": 0.24910813570022583, "xcomet_qe_score": 0.24055854976177216, "metricx_score": 15.953845977783203, "metricx_qe_score": 16.22759246826172, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Questo significa che questi tre vocaboli saranno emessi.\"", "metrics": {"bleu_score": 4.11236251403474, "chrf_score": 33.93532977002557, "xcomet_score": 0.7358909845352173, "xcomet_qe_score": 0.9631487131118774, "metricx_score": 3.067443609237671, "metricx_qe_score": 3.186849594116211, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se osservate i risultati principali di quelli, Plottiamo i risultati della traduzione spaziale simultanea in grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e l'arretramento medio. \"Ecco che misuriamo il tempo di latenza e consideriamo anche la media ponderata della valutazione computazionale, che tiene conto del tempo di calcolo necessario per predire l'output.\"", "metrics": {"bleu_score": 19.104081613647534, "chrf_score": 60.44640675238504, "xcomet_score": 0.14983564615249634, "xcomet_qe_score": 0.25403156876564026, "metricx_score": 17.024559020996094, "metricx_qe_score": 16.83474349975586, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo che le curve siano il più alte possibile in questo", "metrics": {"bleu_score": 6.452217533771192, "chrf_score": 43.64652804319679, "xcomet_score": 0.6047253608703613, "xcomet_qe_score": 0.7679712772369385, "metricx_score": 8.374438285827637, "metricx_qe_score": 4.9913153648376465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "grafico. Ma anche vogliamo che siano spostati a sinistra.", "metrics": {"bleu_score": 13.950796967929138, "chrf_score": 35.54200326143656, "xcomet_score": 0.27713215351104736, "xcomet_qe_score": 0.24517486989498138, "metricx_score": 10.43031120300293, "metricx_qe_score": 12.535675048828125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo strategie di preparazione anche con modelli offline che utilizzano la strategia di peso-chiave e l'accordo locale.", "metrics": {"bleu_score": 4.839757332164814, "chrf_score": 42.11418569305516, "xcomet_score": 0.5556938648223877, "xcomet_qe_score": 0.5464369058609009, "metricx_score": 12.110086441040039, "metricx_qe_score": 11.620935440063477, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo anche con l'architettura di punta per la traduzione di respirazioni a bassa risoluzione.", "metrics": {"bleu_score": 5.412989186545263, "chrf_score": 37.40734608597167, "xcomet_score": 0.2010425627231598, "xcomet_qe_score": 0.4058651626110077, "metricx_score": 16.468063354492188, "metricx_qe_score": 14.423782348632812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione spaziale simultanea sul tedesco.", "metrics": {"bleu_score": 65.91844162499147, "chrf_score": 85.31001605593474, "xcomet_score": 0.7193349599838257, "xcomet_qe_score": 0.7421777248382568, "metricx_score": 7.110864639282227, "metricx_qe_score": 5.679206848144531, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che vediamo che le forme adulte applicano tutte le strategie anche ai modelli offline, poiché le curve sono spostate a sinistra.\"", "metrics": {"bleu_score": 48.9453596218689, "chrf_score": 74.79422808992733, "xcomet_score": 0.25430381298065186, "xcomet_qe_score": 0.179864764213562, "metricx_score": 10.63435173034668, "metricx_qe_score": 12.114968299865723, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che notiamo che se consideriamo il tempo effettivo trascorso o il tempo computazionale, ossia la strategia più veloce.\"", "metrics": {"bleu_score": 55.79235335264048, "chrf_score": 71.89467855178323, "xcomet_score": 0.7825652360916138, "xcomet_qe_score": 0.7345006465911865, "metricx_score": 8.65041446685791, "metricx_qe_score": 8.338951110839844, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se volete scoprire ulteriori risultati, leggete il nostro articolo. Inoltre,", "metrics": {"bleu_score": 22.404158692627117, "chrf_score": 54.490537363169075, "xcomet_score": 0.6939140558242798, "xcomet_qe_score": 0.6640547513961792, "metricx_score": 3.6714138984680176, "metricx_qe_score": 2.3188676834106445, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo rilasciato il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro.", "metrics": {"bleu_score": 59.82338085955874, "chrf_score": 80.14773484485005, "xcomet_score": 0.8688449859619141, "xcomet_qe_score": 0.8983937501907349, "metricx_score": 4.475729465484619, "metricx_qe_score": 4.920419216156006, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per aver ascoltato.\"", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 38.148526698747034, "xcomet_score": 0.551630973815918, "xcomet_qe_score": 0.9722174406051636, "metricx_score": 0.8989627361297607, "metricx_qe_score": 0.41004788875579834, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e presenteremo la nostra ricerca sul Multi-Instruct, migliorando l'apprendimento multi-modello mediante regolazione delle istruzioni.\"", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 69.20331632051204, "xcomet_score": 0.6425909399986267, "xcomet_qe_score": 0.6913208961486816, "metricx_score": 4.02907133102417, "metricx_qe_score": 5.0025482177734375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Con gli avanzamenti nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento che riprendono modelli di linguaggio pre-addestrati per compiti downstream in modo efficiente in termini di parametri e dati.", "metrics": {"bleu_score": 47.811824323333866, "chrf_score": 68.5963251121905, "xcomet_score": 0.8806306719779968, "xcomet_qe_score": 0.9645500183105469, "metricx_score": 4.809263229370117, "metricx_qe_score": 4.185154914855957, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Recenti ricerche hanno dimostrato che l'addestramento mediante istruzioni consente ai modelli linguistici di grande dimensione di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali.\"", "metrics": {"bleu_score": 25.04854060624158, "chrf_score": 65.15496133891031, "xcomet_score": 0.7407714128494263, "xcomet_qe_score": 0.7678534388542175, "metricx_score": 3.5491206645965576, "metricx_qe_score": 4.927596092224121, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti sulla regolazione delle istruzioni si sono concentrati prevalentemente sul miglioramento del rendimento a zero-shot per compiti esclusivamente linguistici, mentre compiti di visione computerizzata e multimodali sono stati trascurati.", "metrics": {"bleu_score": 8.06154175496949, "chrf_score": 56.26494574383065, "xcomet_score": 0.8462568521499634, "xcomet_qe_score": 0.8418340682983398, "metricx_score": 3.5910913944244385, "metricx_qe_score": 3.9710092544555664, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Dunque, in questo lavoro, vogliamo esaminare se l'adattamento delle istruzioni sui modelli di proteina multimodale possa effettivamente migliorare la generalizzazione a compiti multimodali non visti.\"", "metrics": {"bleu_score": 35.14282087095521, "chrf_score": 69.9614319483597, "xcomet_score": 0.534225583076477, "xcomet_qe_score": 0.5237377882003784, "metricx_score": 5.070984840393066, "metricx_qe_score": 6.167769908905029, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole discordanza nell'accessibilità dei dati di istruzione tra un RLP e un modello multi-modulo.", "metrics": {"bleu_score": 23.5292691977645, "chrf_score": 61.223524858678545, "xcomet_score": 0.6788539886474609, "xcomet_qe_score": 0.7362698316574097, "metricx_score": 6.984178066253662, "metricx_qe_score": 5.295205116271973, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di 1.600 compiti di istruzione esclusivamente linguistici.", "metrics": {"bleu_score": 26.305014340253436, "chrf_score": 54.92080051865723, "xcomet_score": 0.9757809638977051, "xcomet_qe_score": 0.9836031198501587, "metricx_score": 5.2663350105285645, "metricx_qe_score": 4.312656879425049, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, non esiste un grande insieme di compiti di istruzione multimodale disponibile al pubblico.", "metrics": {"bleu_score": 37.67471803151816, "chrf_score": 70.53939023515406, "xcomet_score": 0.8690978288650513, "xcomet_qe_score": 0.9671732187271118, "metricx_score": 5.185449123382568, "metricx_qe_score": 4.39008903503418, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Perciò, ciò ci motiva a creare un insieme di istruzione multimodale.", "metrics": {"bleu_score": 10.447589921593115, "chrf_score": 35.9403152394834, "xcomet_score": 0.7940189242362976, "xcomet_qe_score": 0.8158454298973083, "metricx_score": 8.402045249938965, "metricx_qe_score": 7.057811260223389, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco presentiamo Multi-Instruct, il primo benchmark di tuning per istruzioni multi-modal che comprende 62 compiti diversi che coprono 10 categorie di tavola.", "metrics": {"bleu_score": 9.883794937518763, "chrf_score": 47.16162698789261, "xcomet_score": 0.4657905697822571, "xcomet_qe_score": 0.569969892501831, "metricx_score": 9.718456268310547, "metricx_qe_score": 9.062828063964844, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E questi compiti sono derivati da 21 dataset open source esistenti, e ogni compito è dotato di cinque istruzioni scritte dagli esperti.\"", "metrics": {"bleu_score": 17.4932986566737, "chrf_score": 60.734817650103814, "xcomet_score": 0.9451844692230225, "xcomet_qe_score": 0.933971643447876, "metricx_score": 4.922888278961182, "metricx_qe_score": 5.039336204528809, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per lo studio di tuning delle istruzioni multimodali sul nostro set di dati proposto, utilizziamo OFA, un modello di rappresentazione multimodale unificato come modello base.", "metrics": {"bleu_score": 44.57732753604865, "chrf_score": 66.28759059892884, "xcomet_score": 0.8521668910980225, "xcomet_qe_score": 0.9480524063110352, "metricx_score": 6.428794860839844, "metricx_qe_score": 6.496778964996338, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "OFA utilizza un vocabolario unificato per token di linguaggio, token di immagine e coordinate di un rettangolo di bounding box.", "metrics": {"bleu_score": 39.96806384679825, "chrf_score": 67.02079061779227, "xcomet_score": 0.8236775994300842, "xcomet_qe_score": 0.6783320903778076, "metricx_score": 6.380376815795898, "metricx_qe_score": 6.821228504180908, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di istanze dal nostro dataset multilingue. Unificare l'elaborazione di una varietà di tipi di dati di input e output.", "metrics": {"bleu_score": 34.05045260961429, "chrf_score": 62.0152293853955, "xcomet_score": 0.5737619400024414, "xcomet_qe_score": 0.6162293553352356, "metricx_score": 8.711771011352539, "metricx_qe_score": 10.731195449829102, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Abbiamo seguito il metodo di OFA e abbiamo formulato tutti i compiti in un formato di sequenza-sequenza,", "metrics": {"bleu_score": 25.34743707366162, "chrf_score": 54.17103546819669, "xcomet_score": 0.6894460916519165, "xcomet_qe_score": 0.7077213525772095, "metricx_score": 4.541206359863281, "metricx_qe_score": 5.025957107543945, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in cui il testo di input, le immagini, le istruzioni e i rettangoli di bounding sono rappresentati nello stesso spazio di token.\"", "metrics": {"bleu_score": 63.941271162881165, "chrf_score": 76.65118784042619, "xcomet_score": 0.7857916355133057, "xcomet_qe_score": 0.7847824096679688, "metricx_score": 6.378259181976318, "metricx_qe_score": 9.533258438110352, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora, sto per parlare dell'adattamento delle istruzioni multi-modal. \"Ecco,", "metrics": {"bleu_score": 4.368583925857938, "chrf_score": 60.169197556363876, "xcomet_score": 0.650221049785614, "xcomet_qe_score": 0.6218485236167908, "metricx_score": 6.932879447937012, "metricx_qe_score": 6.115645885467529, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "per l'insieme di dati di training, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito.", "metrics": {"bleu_score": 47.10605077025442, "chrf_score": 64.94074144866083, "xcomet_score": 0.9044167995452881, "xcomet_qe_score": 0.89411461353302, "metricx_score": 4.595475196838379, "metricx_qe_score": 3.992319345474243, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per il testing, riserviamo l'intero gruppo di ragionamento comune per il testing e selezioniamo ulteriori 5 compiti dai gruppi VQA e miscellaneo.\"", "metrics": {"bleu_score": 24.386272093446127, "chrf_score": 59.67777468805333, "xcomet_score": 0.8228327035903931, "xcomet_qe_score": 0.8755725622177124, "metricx_score": 8.673048973083496, "metricx_qe_score": 9.220951080322266, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Utilizziamo tutte le istanze del flotte di test per ogni compito.", "metrics": {"bleu_score": 20.78060434846712, "chrf_score": 49.85473747913269, "xcomet_score": 0.8131473064422607, "xcomet_qe_score": 0.8416430950164795, "metricx_score": 4.774195671081543, "metricx_qe_score": 4.549348831176758, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, campioniamo a caso 20 compiti dalla flotta di test di istruzioni naturali come compito in situ per l'NLP.\"", "metrics": {"bleu_score": 12.582205558459375, "chrf_score": 44.987346614293166, "xcomet_score": 0.5675539970397949, "xcomet_qe_score": 0.618122935295105, "metricx_score": 7.678925037384033, "metricx_qe_score": 7.683815002441406, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che utilizziamo un modello di riferimento OFA grande come modello di base.", "metrics": {"bleu_score": 27.668736912821906, "chrf_score": 43.15002226480967, "xcomet_score": 0.5500088930130005, "xcomet_qe_score": 0.6514435410499573, "metricx_score": 6.423100471496582, "metricx_qe_score": 7.949949264526367, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Durante l'addestramento, creiamo tutte le istanze per tutte le attività.", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 87.86700490826021, "xcomet_score": 0.8221068978309631, "xcomet_qe_score": 0.8290215134620667, "metricx_score": 5.946805953979492, "metricx_qe_score": 6.957306385040283, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ogni istanza è combinata casualmente con uno dei cinque template di istru", "metrics": {"bleu_score": 32.866572599159944, "chrf_score": 63.66992686067066, "xcomet_score": 0.7906285524368286, "xcomet_qe_score": 0.8354592323303223, "metricx_score": 5.984086036682129, "metricx_qe_score": 2.994866371154785, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "zioni.\" Durante il test, per ogni compito, eseguiamo in totale cinque esperimenti valutando il modello utilizzando uno dei cinque istruzioni", "metrics": {"bleu_score": 21.08445456905128, "chrf_score": 59.224062622688834, "xcomet_score": 0.4722824692726135, "xcomet_qe_score": 0.4275718033313751, "metricx_score": 8.653656005859375, "metricx_qe_score": 9.667916297912598, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in ogni esperimento. Riportiamo la media e il massimo delle prestazioni e la deviazione standard delle prestazioni in tutte le cinque esperienze.", "metrics": {"bleu_score": 29.065151007971206, "chrf_score": 71.21898779198528, "xcomet_score": 0.863205075263977, "xcomet_qe_score": 0.8767700791358948, "metricx_score": 3.2315659523010254, "metricx_qe_score": 2.720515251159668, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è una task di classificazione multi-modello, riportiamo l'accuratezza.", "metrics": {"bleu_score": 50.08718428920986, "chrf_score": 73.38650272896223, "xcomet_score": 0.8278954029083252, "xcomet_qe_score": 0.7892419099807739, "metricx_score": 6.732120513916016, "metricx_qe_score": 5.944268226623535, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se è una task di generazione di modelli multi-modello, riportiamo il root Mean Squared Error. Per una task di Recognition Phonology, riportiamo il", "metrics": {"bleu_score": 4.8589719316429765, "chrf_score": 35.84911099032129, "xcomet_score": 0.46095502376556396, "xcomet_qe_score": 0.34022635221481323, "metricx_score": 16.46518898010254, "metricx_qe_score": 12.151021957397461, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "root Mean Squared Error anch'esso. \"Ebbiamo anche introdotto metriche di valutazione aggiuntive chiamate sensibilità.", "metrics": {"bleu_score": 6.468490584192431, "chrf_score": 49.42990687254054, "xcomet_score": 0.1309732347726822, "xcomet_qe_score": 0.14171844720840454, "metricx_score": 12.853185653686523, "metricx_qe_score": 15.182125091552734, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò misura l'abilità del modello di produrre output coerenti per lo stesso compito, indipendentemente da piccole variazioni nella formulazione delle istruzioni.\"", "metrics": {"bleu_score": 15.084825228964135, "chrf_score": 60.100083159937725, "xcomet_score": 0.9368933439254761, "xcomet_qe_score": 0.8993352651596069, "metricx_score": 2.0623457431793213, "metricx_qe_score": 3.143981456756592, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale.", "metrics": {"bleu_score": 19.304869754804482, "chrf_score": 59.31444515953541, "xcomet_score": 0.7470951080322266, "xcomet_qe_score": 0.9614670276641846, "metricx_score": 1.3401482105255127, "metricx_qe_score": 0.7160966396331787, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, l'adattamento delle istruzioni può migliorare significativamente il rendimento del sistema di ottimizzazione delle immagini (OIS)", "metrics": {"bleu_score": 29.240074556521943, "chrf_score": 65.47056875650061, "xcomet_score": 0.6574952602386475, "xcomet_qe_score": 0.6559251546859741, "metricx_score": 4.119848251342773, "metricx_qe_score": 4.151869773864746, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in compiti multi-modelli. Anche il trasferimento di apprendimento da set di dati di istruzione naturale può beneficiare l'adattamento di istruzione.", "metrics": {"bleu_score": 13.8177903937343, "chrf_score": 62.065956011059875, "xcomet_score": 0.3229329586029053, "xcomet_qe_score": 0.4101446866989136, "metricx_score": 12.047859191894531, "metricx_qe_score": 13.876104354858398, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco come si nota che, con l'aumento del numero di compiti, il modello raggiunge prestazioni migliori e, nel contempo, una maggiore insensibilità.", "metrics": {"bleu_score": 34.97140226207081, "chrf_score": 54.188356685182406, "xcomet_score": 0.8606411218643188, "xcomet_qe_score": 0.8436010479927063, "metricx_score": 4.291165351867676, "metricx_qe_score": 3.6382601261138916, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Così abbiamo anche fatto un esperimento,", "metrics": {"bleu_score": 15.619699684601283, "chrf_score": 49.52614570404851, "xcomet_score": 0.9055527448654175, "xcomet_qe_score": 0.9307134747505188, "metricx_score": 1.9373012781143188, "metricx_qe_score": 0.7442486882209778, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo utilizzato un istruzione contro cinque istruzioni.", "metrics": {"bleu_score": 15.619699684601276, "chrf_score": 66.54693930168591, "xcomet_score": 0.8058857917785645, "xcomet_qe_score": 0.7954714894294739, "metricx_score": 5.181055068969727, "metricx_qe_score": 5.738848686218262, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, l'utilizzo di istruzioni più numerose può migliorare le prestazioni del modello in generale e ridurre la sua sensibilità molto.", "metrics": {"bleu_score": 35.13874939965223, "chrf_score": 70.8971371384111, "xcomet_score": 0.9573109745979309, "xcomet_qe_score": 0.9586328864097595, "metricx_score": 3.7944891452789307, "metricx_qe_score": 2.183485507965088, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco il risultato dell'impatto delle diverse strategie di tuning sulle prestazioni del modello.", "metrics": {"bleu_score": 16.451929399933114, "chrf_score": 46.60110199359076, "xcomet_score": 0.611024796962738, "xcomet_qe_score": 0.6270483136177063, "metricx_score": 4.627593040466309, "metricx_qe_score": 4.742644309997559, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, mediante l'apprendimento trasferito dai set di dati di istruzioni naturali, il modello riesce a raggiungere una sensibilità molto superiore rispetto al modello OFA originale.\"", "metrics": {"bleu_score": 52.05303553082506, "chrf_score": 79.13643199764978, "xcomet_score": 0.9341477155685425, "xcomet_qe_score": 0.9544747471809387, "metricx_score": 1.9955086708068848, "metricx_qe_score": 2.0567147731781006, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Anche il trasferimento di apprendimento da un insieme di dati di istruzioni naturali può aiutare l'OFA a raggiungere un'ottima prestazione sugli insiemi di dati di istruzioni naturali.\"", "metrics": {"bleu_score": 36.03321742911122, "chrf_score": 62.816873275058306, "xcomet_score": 0.7368359565734863, "xcomet_qe_score": 0.8550758361816406, "metricx_score": 3.062436103820801, "metricx_qe_score": 3.1132607460021973, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che abbiamo proposto il primo insieme di dati di tuning multi-modale a grande scala. Abbiamo significativamente migliorato la capacità di soglia dell'OFA e abbiamo esplorato diverse tecniche di apprendimento trasferito, mostrando i loro benefici. Abbiamo", "metrics": {"bleu_score": 11.125187510930292, "chrf_score": 49.98865097298036, "xcomet_score": 0.14316682517528534, "xcomet_qe_score": 0.20069608092308044, "metricx_score": 9.737894058227539, "metricx_qe_score": 7.4624481201171875, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "progettato un nuovo metrica chiamata sensibilità.\"", "metrics": {"bleu_score": 34.57207846419409, "chrf_score": 70.7563221770498, "xcomet_score": 0.3082498610019684, "xcomet_qe_score": 0.23388035595417023, "metricx_score": 7.410948753356934, "metricx_qe_score": 8.083846092224121, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Un'altra cosa, stiamo raccolta un insieme di dati di tuning multi-modello più ampio con circa 150 compiti di comprensione del linguaggio Weiren e li stiamo rilasciando.", "metrics": {"bleu_score": 18.228835943861768, "chrf_score": 47.64283784824177, "xcomet_score": 0.6832209825515747, "xcomet_qe_score": 0.7438635230064392, "metricx_score": 9.306634902954102, "metricx_qe_score": 7.668416976928711, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il codice QR per i nostri dati e il modello.", "metrics": {"bleu_score": 56.75005498026444, "chrf_score": 64.22658916374068, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.43033820390701294, "metricx_qe_score": 0.6793957948684692, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 0.9992209672927856, "xcomet_qe_score": 1.0, "metricx_score": 0.03144232928752899, "metricx_qe_score": 0.23287612199783325, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti,", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.29208795853614, "xcomet_score": 0.3884716331958771, "xcomet_qe_score": 0.4670296311378479, "metricx_score": 0.22448410093784332, "metricx_qe_score": 0.13212648034095764, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "sono Kostav Sinha e sono lieto di accogliervi nel nostro talk del nostro articolo ACL 2023,", "metrics": {"bleu_score": 19.601663253740664, "chrf_score": 52.42535027020795, "xcomet_score": 0.8461871147155762, "xcomet_qe_score": 0.7681921720504761, "metricx_score": 4.789968013763428, "metricx_qe_score": 5.555359363555908, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di linguaggio non sono sempre robusti al contesto.\"", "metrics": {"bleu_score": 3.3864985683445354, "chrf_score": 26.257308127881696, "xcomet_score": 0.8436435461044312, "xcomet_qe_score": 0.8615726232528687, "metricx_score": 3.0759851932525635, "metricx_qe_score": 3.9673705101013184, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "C'è un lavoro congiunto con John Wothier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina Williams.", "metrics": {"bleu_score": 56.11766115961593, "chrf_score": 82.64269939963593, "xcomet_score": 0.5044254064559937, "xcomet_qe_score": 0.6120662689208984, "metricx_score": 6.490401268005371, "metricx_qe_score": 6.078101634979248, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo il paradigma di coppia minima. E", "metrics": {"bleu_score": 14.128386352314104, "chrf_score": 43.19178972359703, "xcomet_score": 0.2829200029373169, "xcomet_qe_score": 0.20686468482017517, "metricx_score": 13.606714248657227, "metricx_qe_score": 7.674581527709961, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "il paradigma a coppie minimale valuta i modelli di linguaggio in base a giudizi di accettabilità,", "metrics": {"bleu_score": 12.03921753741131, "chrf_score": 51.38731547132613, "xcomet_score": 0.9044675827026367, "xcomet_qe_score": 0.9138449430465698, "metricx_score": 3.0804052352905273, "metricx_qe_score": 1.3161356449127197, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che possono anche includere la grammaticalità come sintassi del blimp e l'accettabilità in termini di stereotipi come spazi sparsi.", "metrics": {"bleu_score": 40.39075489203363, "chrf_score": 67.2686306232885, "xcomet_score": 0.4673118591308594, "xcomet_qe_score": 0.4978916347026825, "metricx_score": 12.359809875488281, "metricx_qe_score": 12.696383476257324, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma a coppia minimo, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticale, e poi mostrare una frase accettabile o una frase non grammaticale.", "metrics": {"bleu_score": 79.27946739914229, "chrf_score": 92.57284534232937, "xcomet_score": 0.7016754746437073, "xcomet_qe_score": 0.6515371799468994, "metricx_score": 5.50519323348999, "metricx_qe_score": 5.927824974060059, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E poi si spera che il modello assegni più probabilità alla sezione accettabile.", "metrics": {"bleu_score": 10.870900156969965, "chrf_score": 45.50856797440323, "xcomet_score": 0.9074219465255737, "xcomet_qe_score": 0.8980358242988586, "metricx_score": 5.636142253875732, "metricx_qe_score": 5.696623802185059, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il pipeline MPP attualmente non ci consente di valutare l'accettazione dei modelli per frasi più lunghe.", "metrics": {"bleu_score": 22.418805895465404, "chrf_score": 63.60677396796453, "xcomet_score": 0.9580506086349487, "xcomet_qe_score": 0.8364156484603882, "metricx_score": 2.2427573204040527, "metricx_qe_score": 2.279792308807373, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questi giorni, i modelli di linguaggio a grandi dimensioni stanno generando finestre di contesto sempre più lunghe.", "metrics": {"bleu_score": 38.27673535697133, "chrf_score": 64.71079461747435, "xcomet_score": 0.9852703809738159, "xcomet_qe_score": 0.9520492553710938, "metricx_score": 2.6327905654907227, "metricx_qe_score": 1.6374287605285645, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "È quindi essenziale valutare l'accettabilità dei modelli all'interno della finestra di contesto. Ecco cosa stiamo cercando di fare qui.", "metrics": {"bleu_score": 36.273972345708906, "chrf_score": 68.41277011866453, "xcomet_score": 0.9864598512649536, "xcomet_qe_score": 0.8846100568771362, "metricx_score": 0.8077769875526428, "metricx_qe_score": 0.9652364253997803, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo cercando di riprendere il pipeline MPB chiedendo al modello di valutare l'accettabilità su sequenze più lunghe.", "metrics": {"bleu_score": 51.7935102212525, "chrf_score": 80.25718002555668, "xcomet_score": 0.7788873910903931, "xcomet_qe_score": 0.7617586851119995, "metricx_score": 7.329436779022217, "metricx_qe_score": 6.481772422790527, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco l'approccio.", "metrics": {"bleu_score": 0.0, "chrf_score": 32.29536112252226, "xcomet_score": 0.8872483968734741, "xcomet_qe_score": 0.7112973928451538, "metricx_score": 1.1603584289550781, "metricx_qe_score": 1.7769075632095337, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quello che facciamo è riprendere quei dati stessi e ricreare celle di frase scelte tra frasi accettabili o non accettabili da quei dati stessi. Ecco un", "metrics": {"bleu_score": 10.811850538037245, "chrf_score": 46.5712843887913, "xcomet_score": 0.3344344198703766, "xcomet_qe_score": 0.291477233171463, "metricx_score": 12.971332550048828, "metricx_qe_score": 10.512811660766602, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "esempio, come si potrebbe scegliere un paio di dati di blimp dal set di dati dell'isola di aggiunta del caso.", "metrics": {"bleu_score": 14.230715327204656, "chrf_score": 30.951185915205777, "xcomet_score": 0.3966498076915741, "xcomet_qe_score": 0.35012564063072205, "metricx_score": 14.152358055114746, "metricx_qe_score": 12.673322677612305, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco cosa facciamo: ricreare sequenze più lunghe e individuare quelle accettabili e con la stessa struttura grammaticale,", "metrics": {"bleu_score": 16.57379326593255, "chrf_score": 59.78664708926552, "xcomet_score": 0.9539628028869629, "xcomet_qe_score": 0.9349251985549927, "metricx_score": 1.6203442811965942, "metricx_qe_score": 2.0591461658477783, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "estraiamo frasi grammaticali da AdjunTile. E quindi abbiamo aggiunto un prefisso a entrambe la query accettabile e la query non accettabile.", "metrics": {"bleu_score": 8.9275746495808, "chrf_score": 58.680835192698, "xcomet_score": 0.6692206263542175, "xcomet_qe_score": 0.6828118562698364, "metricx_score": 6.761991024017334, "metricx_qe_score": 6.616608142852783, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo fare lo stesso anche scegliendo frasi inaccettabili dallo stesso matching. E ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello. E", "metrics": {"bleu_score": 12.911050433088718, "chrf_score": 65.09758777554137, "xcomet_score": 0.7620186805725098, "xcomet_qe_score": 0.795575737953186, "metricx_score": 5.134525775909424, "metricx_qe_score": 4.816892147064209, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "possiamo fare lo stesso scegliendo frasi da un sottosettore o da un insieme di dati diverso.", "metrics": {"bleu_score": 46.293690568103294, "chrf_score": 66.9685951975804, "xcomet_score": 0.9175788164138794, "xcomet_qe_score": 0.9095277786254883, "metricx_score": 3.7183539867401123, "metricx_qe_score": 3.2100048065185547, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quello è ciò che chiamiamo scenario di mismatch.", "metrics": {"bleu_score": 61.04735835807847, "chrf_score": 61.25902719724458, "xcomet_score": 0.8549714088439941, "xcomet_qe_score": 0.8515200614929199, "metricx_score": 4.633928298950195, "metricx_qe_score": 4.855787754058838, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco le frasi che continuano a provenire da insiemi di dati rilevanti, ma non sono più gli stessi dati che state valutando con il vostro modello.", "metrics": {"bleu_score": 14.479536157777419, "chrf_score": 44.38420803132584, "xcomet_score": 0.957782506942749, "xcomet_qe_score": 0.8370897769927979, "metricx_score": 2.324335813522339, "metricx_qe_score": 2.8641393184661865, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso per gli insiemi di dati non accettabili.", "metrics": {"bleu_score": 39.553325358771794, "chrf_score": 66.26110311403139, "xcomet_score": 0.9679534435272217, "xcomet_qe_score": 0.9724369049072266, "metricx_score": 3.2128396034240723, "metricx_qe_score": 2.568657398223877, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Finalmente, possiamo scegliere frasi da un dominio del tutto diverso, come Wikipedia. \"Ecco se", "metrics": {"bleu_score": 40.21074690812006, "chrf_score": 66.25298594951869, "xcomet_score": 0.36349964141845703, "xcomet_qe_score": 0.3902145326137543, "metricx_score": 6.396882057189941, "metricx_qe_score": 3.9463601112365723, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ciò ci dirà se i giudizi di accettabilità dei modelli sono realmente influenzati da qualsiasi contesto.\" E se il contesto deriva da un diverso sottinsieme del set di dati o se è del tutto irrilevante rispetto alla frase che stiamo esaminando.", "metrics": {"bleu_score": 35.999622972398036, "chrf_score": 63.125758585950024, "xcomet_score": 0.7933495044708252, "xcomet_qe_score": 0.7774724960327148, "metricx_score": 3.1266796588897705, "metricx_qe_score": 2.8029749393463135, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco come funziona il modello?", "metrics": {"bleu_score": 64.31870218238025, "chrf_score": 75.1820870008572, "xcomet_score": 0.7287647724151611, "xcomet_qe_score": 0.8056899309158325, "metricx_score": 1.5467299222946167, "metricx_qe_score": 0.9636201858520508, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo analizzando le frasi del Wikipedia completamente irrilevanti al coppia di query attuale. E lì troviamo che i giudizi MPP sono prevalentemente robusti per contesti arbitrari come.", "metrics": {"bleu_score": 14.303803944088953, "chrf_score": 52.737150944476696, "xcomet_score": 0.6682006120681763, "xcomet_qe_score": 0.6361823081970215, "metricx_score": 8.832783699035645, "metricx_qe_score": 9.241257667541504, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a 1024 per massimizzare i modelli di OPT e GPT2.", "metrics": {"bleu_score": 44.219732271776664, "chrf_score": 84.14957425102943, "xcomet_score": 0.9386173486709595, "xcomet_qe_score": 0.8743607997894287, "metricx_score": 1.3621189594268799, "metricx_qe_score": 2.528580904006958, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E qui nella linea di punti arancioni, le valutazioni MPP sono relativamente stabili.", "metrics": {"bleu_score": 28.295596283263514, "chrf_score": 55.02663961330647, "xcomet_score": 0.9636452794075012, "xcomet_qe_score": 0.9704370498657227, "metricx_score": 5.577211380004883, "metricx_qe_score": 5.848313808441162, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora, cosa accade quando scegliamo frasi dallo stesso insieme di dati? Siamo", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 86.49212461351333, "xcomet_score": 0.7196044921875, "xcomet_qe_score": 0.586040735244751, "metricx_score": 6.97150182723999, "metricx_qe_score": 0.4431019425392151, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "qui a scegliere o creare frasi da domini accettabili e non accettabili da stesso set di dati sintassi Gem. Ecco che vediamo", "metrics": {"bleu_score": 19.38341802345665, "chrf_score": 54.02054495665076, "xcomet_score": 0.3858262002468109, "xcomet_qe_score": 0.41121798753738403, "metricx_score": 12.420615196228027, "metricx_qe_score": 13.369064331054688, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili.", "metrics": {"bleu_score": 53.8118676268092, "chrf_score": 78.02540654026512, "xcomet_score": 0.7711087465286255, "xcomet_qe_score": 0.7036689519882202, "metricx_score": 4.447650909423828, "metricx_qe_score": 4.643742084503174, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando corrispondiamo alla struttura, è quando scegliamo le frasi dalle stesse fenomenologie del testo colpevole, Jim, Ecco un aumento o un decremento massiccio del giudizio MPP per il modello a seconda se il prefisso scelto è accettabile o non accettabile. Ora questo,", "metrics": {"bleu_score": 22.401364375618517, "chrf_score": 60.81620533352403, "xcomet_score": 0.19446298480033875, "xcomet_qe_score": 0.10208023339509964, "metricx_score": 13.77154541015625, "metricx_qe_score": 14.964559555053711, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e questo è molto grande, come questo effetto aumenta lungo la lunghezza del contesto, e questo probabilmente influirebbe su modelli linguistici più recenti che hanno una finestra di contesto ampia.", "metrics": {"bleu_score": 48.27818775892252, "chrf_score": 79.06645502192524, "xcomet_score": 0.8951129913330078, "xcomet_qe_score": 0.7696497440338135, "metricx_score": 4.557858943939209, "metricx_qe_score": 5.980556488037109, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, il prefisso del match influisce sulla valutazione del modello linguistico in", "metrics": {"bleu_score": 12.975280965814113, "chrf_score": 47.965158432098185, "xcomet_score": 0.22789470851421356, "xcomet_qe_score": 0.2263081818819046, "metricx_score": 9.558112144470215, "metricx_qe_score": 8.587321281433105, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "modo significativo. \"Eseguiamo una serie di analisi dove abbiamo cercato di emulare la frase di input, cercando di preservare la struttura rilevante, ma aggiungendo rumore all'input.", "metrics": {"bleu_score": 41.39625550113642, "chrf_score": 69.28321228330513, "xcomet_score": 0.30300524830818176, "xcomet_qe_score": 0.23819264769554138, "metricx_score": 7.588304042816162, "metricx_qe_score": 11.94117259979248, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E dopo aver eseguito diversi di questi perturbazioni, abbiamo scoperto che...\" \"Non troviamo che nessuno di questi rumori stia influenzando in modo significativo il modello nel mostrare la valutazione MPP.\"", "metrics": {"bleu_score": 20.027292858993896, "chrf_score": 50.66830578388095, "xcomet_score": 0.6742384433746338, "xcomet_qe_score": 0.6510483026504517, "metricx_score": 7.004165172576904, "metricx_qe_score": 7.489344120025635, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Rileviamo che i modelli sono sensibili alle perturbazioni e alle frasi in modi simili", "metrics": {"bleu_score": 43.74811431224644, "chrf_score": 65.24632655232907, "xcomet_score": 0.24049988389015198, "xcomet_qe_score": 0.27433842420578003, "metricx_score": 4.39178991317749, "metricx_qe_score": 6.785916805267334, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": ".\" Quando perturbiamo le frasi all'interno del dominio accettabile, notiamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi all'interno del dominio non accettabile, notiamo una diminuzione dei giudizi di MPP in modo simile.", "metrics": {"bleu_score": 39.08967647809356, "chrf_score": 81.66950600078448, "xcomet_score": 0.7517071962356567, "xcomet_qe_score": 0.76288902759552, "metricx_score": 4.367671489715576, "metricx_qe_score": 5.816428184509277, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E i principali risultati del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi.", "metrics": {"bleu_score": 72.16564679800659, "chrf_score": 85.17992894256314, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 1.1826236248016357, "metricx_qe_score": 1.1030068397521973, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E il valutazione MPP, il modo in cui la facciamo attualmente con input di centro singolo e breve, potrebbe non catturare pienamente la conoscenza linguistica astratta all'interno della finestra di contesto.\"", "metrics": {"bleu_score": 32.38976684295315, "chrf_score": 65.64460206570567, "xcomet_score": 0.4483439028263092, "xcomet_qe_score": 0.31271374225616455, "metricx_score": 9.945008277893066, "metricx_qe_score": 11.300403594970703, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Leggete attentamente il nostro articolo per ulteriori dettagli sugli esperimenti.", "metrics": {"bleu_score": 22.17994592198391, "chrf_score": 59.825708503160755, "xcomet_score": 0.9682467579841614, "xcomet_qe_score": 0.9610295295715332, "metricx_score": 0.3738488256931305, "metricx_qe_score": 0.65583336353302, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per aver ascoltato.\"", "metrics": {"bleu_score": 9.652434877402245, "chrf_score": 43.63431931986593, "xcomet_score": 0.9038568735122681, "xcomet_qe_score": 1.0, "metricx_score": 0.3946499824523926, "metricx_qe_score": 0.26863229274749756, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti, mi chiamo Yusin Zhang dell'Università della Pennsylvania.", "metrics": {"bleu_score": 42.11813371894876, "chrf_score": 72.86535372459224, "xcomet_score": 0.961978554725647, "xcomet_qe_score": 0.9547056555747986, "metricx_score": 0.24318569898605347, "metricx_qe_score": 0.27295389771461487, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Oggi presenterò il nostro lavoro, 'Crostland Ghosts e Parsing del denaro in più lingue naturali e rappresentazioni principali\". \"Ecco", "metrics": {"bleu_score": 19.38341802345665, "chrf_score": 43.27305569335727, "xcomet_score": 0.2297179102897644, "xcomet_qe_score": 0.23518192768096924, "metricx_score": 22.247697830200195, "metricx_qe_score": 23.501209259033203, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "il compito di elaborazione semantica è quello di creare rappresentazioni semantiche di query degli utenti, come ad esempio SQL e calcolo lambda.\" Ecco la", "metrics": {"bleu_score": 17.89570640154152, "chrf_score": 52.69847856284207, "xcomet_score": 0.35714125633239746, "xcomet_qe_score": 0.3499801754951477, "metricx_score": 6.366121292114258, "metricx_qe_score": 3.219585657119751, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "traduzione: \"E il compito di parsing semantico cross-lingua è quello di tradurre query in più lingue naturali in rappresentazioni di significato multiple.\"", "metrics": {"bleu_score": 29.452884374019476, "chrf_score": 67.49986520624289, "xcomet_score": 0.4907335937023163, "xcomet_qe_score": 0.4483474791049957, "metricx_score": 6.4566168785095215, "metricx_qe_score": 7.802667140960693, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Come mostrato in questo figura, è necessario tradurre la query in più lingue naturali utilizzando modelli neurali per SQL, Lambda, o FunQL, e via dicendo.\" I'd be happy to help! Here is the translation:", "metrics": {"bleu_score": 30.04961314114195, "chrf_score": 72.36411878380552, "xcomet_score": 0.25572600960731506, "xcomet_qe_score": 0.1588549017906189, "metricx_score": 11.08566665649414, "metricx_qe_score": 17.161319732666016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di parsing semantico bilingue esistenti sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitate,", "metrics": {"bleu_score": 27.82354149317939, "chrf_score": 64.55221429919708, "xcomet_score": 0.836901068687439, "xcomet_qe_score": 0.9455326795578003, "metricx_score": 4.012402057647705, "metricx_qe_score": 2.075063467025757, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, Ci sono falle di copertura in certi linguaggi naturali.", "metrics": {"bleu_score": 7.347053125977879, "chrf_score": 35.76736925462513, "xcomet_score": 0.251112699508667, "xcomet_qe_score": 0.6085993647575378, "metricx_score": 13.531135559082031, "metricx_qe_score": 6.134214878082275, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il cinese è assente. A causa di copertura su certi molti rappresentazioni.", "metrics": {"bleu_score": 4.246549372656572, "chrf_score": 36.124591339267944, "xcomet_score": 0.28104814887046814, "xcomet_qe_score": 0.4574604630470276, "metricx_score": 15.102925300598145, "metricx_qe_score": 13.574987411499023, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"L'analisi lambda è mancante.\" O o sono solo valutati su certi modelli più recenti.", "metrics": {"bleu_score": 3.4933841821869938, "chrf_score": 39.575658646146536, "xcomet_score": 0.4731714725494385, "xcomet_qe_score": 0.3482186794281006, "metricx_score": 9.912955284118652, "metricx_qe_score": 9.90634536743164, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, c'è solo un solo modello per valutare il modello.", "metrics": {"bleu_score": 17.474335703431752, "chrf_score": 60.87659578617044, "xcomet_score": 0.8294786810874939, "xcomet_qe_score": 0.8617668747901917, "metricx_score": 3.8401193618774414, "metricx_qe_score": 4.803157806396484, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo senso, proponiamo un insieme", "metrics": {"bleu_score": 9.746997877627233, "chrf_score": 32.19026764900857, "xcomet_score": 0.13510242104530334, "xcomet_qe_score": 0.13355226814746857, "metricx_score": 10.03521728515625, "metricx_qe_score": 5.926267623901367, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "di esempi uniforme per il parsing semantico incrociato in più lingue naturali e rappresentazioni di significato.\"", "metrics": {"bleu_score": 43.315369290064325, "chrf_score": 57.641603665837835, "xcomet_score": 0.5603314638137817, "xcomet_qe_score": 0.5974266529083252, "metricx_score": 11.471962928771973, "metricx_qe_score": 12.25911808013916, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco: \"Questo contiene 90 set di dati in domini virus, 570 parti in tossine, 80 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche.\"", "metrics": {"bleu_score": 34.67318236074744, "chrf_score": 61.24843979943102, "xcomet_score": 0.21519310772418976, "xcomet_qe_score": 0.2243654429912567, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'evaluazione.", "metrics": {"bleu_score": 45.07178335176904, "chrf_score": 70.00664139138215, "xcomet_score": 0.8935309648513794, "xcomet_qe_score": 0.9166848659515381, "metricx_score": 4.740072250366211, "metricx_qe_score": 4.8918843269348145, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione.", "metrics": {"bleu_score": 22.089591134157878, "chrf_score": 35.33136926770135, "xcomet_score": 0.9967273473739624, "xcomet_qe_score": 0.9891386032104492, "metricx_score": 1.3541195392608643, "metricx_qe_score": 0.6577844023704529, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo l'API di Google Translate per tradurre il testo di partenza nella lingua target, quindi utilizziamo il modello monolingue per addestrare qualsiasi valutazione.", "metrics": {"bleu_score": 51.453801186459145, "chrf_score": 74.26200017767724, "xcomet_score": 0.9071263074874878, "xcomet_qe_score": 0.8917965888977051, "metricx_score": 6.374821662902832, "metricx_qe_score": 6.809728622436523, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio: addestriamo il modello inglese su query inglese. Durante l'inferenza, traduciamo la query tedesca in inglese utilizzando l'API e poi utilizziamo il modello addestrato per prevedere la SQL.", "metrics": {"bleu_score": 33.48928509624718, "chrf_score": 76.22851836719396, "xcomet_score": 0.9048864841461182, "xcomet_qe_score": 0.815542459487915, "metricx_score": 1.110640287399292, "metricx_qe_score": 1.477958083152771, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E anche testiamo il modello monolingual.", "metrics": {"bleu_score": 14.535768424205482, "chrf_score": 57.29186799939623, "xcomet_score": 0.7701588869094849, "xcomet_qe_score": 0.7781217098236084, "metricx_score": 4.5291337966918945, "metricx_qe_score": 2.7447521686553955, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco la traduzione: In questo contesto, il linguaggio di partenza è lo stesso del linguaggio di arrivo. Ad esempio, tedesco a tedesco o inglese a inglese. \"Anche", "metrics": {"bleu_score": 16.72259178438891, "chrf_score": 49.87754988058965, "xcomet_score": 0.2685452401638031, "xcomet_qe_score": 0.24424055218696594, "metricx_score": 6.465217113494873, "metricx_qe_score": 3.7989182472229004, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "testiamo l'impostazione di campo monolingue mediante l'addestramento del modello con solo il 10% dei dati di addestramento.\"", "metrics": {"bleu_score": 47.75586214302964, "chrf_score": 64.39006457656922, "xcomet_score": 0.7677100896835327, "xcomet_qe_score": 0.8011901378631592, "metricx_score": 4.4806671142578125, "metricx_qe_score": 4.768019676208496, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco un modello multilingue, che addestriamo per tutte le lingue.\"", "metrics": {"bleu_score": 35.815507660077486, "chrf_score": 56.59457580331838, "xcomet_score": 0.4429273307323456, "xcomet_qe_score": 0.324785053730011, "metricx_score": 6.698450565338135, "metricx_qe_score": 5.472301006317139, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio: abbiamo messo insieme le query in inglese, tedesco e cinese per addestrare un modello multilingue.", "metrics": {"bleu_score": 30.603689509300906, "chrf_score": 69.57974178712723, "xcomet_score": 0.982730507850647, "xcomet_qe_score": 0.9522552490234375, "metricx_score": 0.6827597618103027, "metricx_qe_score": 1.4812581539154053, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E durante l'inferenza, possiamo utilizzare questo modello per tradurre le query in inglese, tedesco e cinese. \"Sono pronto a tradurre query in tedesco o in cinese, ecc.\"", "metrics": {"bleu_score": 11.802861352393496, "chrf_score": 65.46164403215234, "xcomet_score": 0.5010952949523926, "xcomet_qe_score": 0.4542820155620575, "metricx_score": 6.138822078704834, "metricx_qe_score": 6.436711311340332, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E inoltre consideriamo il codice zero e la trasmissione a breve raggio.", "metrics": {"bleu_score": 4.065425428798724, "chrf_score": 27.21080466354493, "xcomet_score": 0.6124863624572754, "xcomet_qe_score": 0.6020046472549438, "metricx_score": 11.195901870727539, "metricx_qe_score": 12.666231155395508, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Addestriamo su una lingua di origine e trasferiamo in un'altra lingua.", "metrics": {"bleu_score": 29.167552921712726, "chrf_score": 55.1456780648925, "xcomet_score": 0.7815005779266357, "xcomet_qe_score": 0.9115304350852966, "metricx_score": 8.702339172363281, "metricx_qe_score": 5.895137786865234, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Durante il training, stiamo allenando le query in inglese o le combinazioni di query in inglese e tedesco per addestrare un modello multilingue e predire l'output SQL.", "metrics": {"bleu_score": 39.67987453167855, "chrf_score": 61.37329824278355, "xcomet_score": 0.8569620847702026, "xcomet_qe_score": 0.9140550494194031, "metricx_score": 8.343847274780273, "metricx_qe_score": 6.364939212799072, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E anche troviamo molti risultati interessanti.", "metrics": {"bleu_score": 47.2870804501588, "chrf_score": 84.39642577800474, "xcomet_score": 0.9483833312988281, "xcomet_qe_score": 0.9290331602096558, "metricx_score": 1.2382789850234985, "metricx_qe_score": 1.5797768831253052, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In merito all'analisi dei modelli monolingui, valutiamo due gruppi di modelli. Incluyendo l'encoder pdr, che sta per encoder multilingue addestrati con decoder a puntatore come xl1r più pdr e berth più pdr.", "metrics": {"bleu_score": 18.889796346849767, "chrf_score": 49.526892774273264, "xcomet_score": 0.6820070743560791, "xcomet_qe_score": 0.6994384527206421, "metricx_score": 9.40296745300293, "metricx_qe_score": 9.235522270202637, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Evalueremo anche modelli encoder-decoder, ovvero modelli encoder-decoder multilingui, come M-BART e MT5.\"", "metrics": {"bleu_score": 5.422040380077636, "chrf_score": 45.63038829532897, "xcomet_score": 0.6476974487304688, "xcomet_qe_score": 0.4292858839035034, "metricx_score": 5.751026153564453, "metricx_qe_score": 7.745634078979492, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo trovato che l'encoder-decodificatore ottiene il miglior rendimento su tutti e nove i dataset.", "metrics": {"bleu_score": 24.938569229056178, "chrf_score": 49.694459428941705, "xcomet_score": 0.958077073097229, "xcomet_qe_score": 0.9614536166191101, "metricx_score": 2.866259813308716, "metricx_qe_score": 3.848750591278076, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su MT5 e esempio XLMR più PDR in ambiente multilingue.", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 49.127392262217604, "xcomet_score": 0.7476619482040405, "xcomet_qe_score": 0.7560343742370605, "metricx_score": 7.250308990478516, "metricx_qe_score": 8.152108192443848, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Abbiamo scoperto che encoder, decoder o PDR possono essere migliorati mediante l'addestramento in un mix di varie lingue.\"", "metrics": {"bleu_score": 22.06312063588559, "chrf_score": 64.02464476081772, "xcomet_score": 0.5689898133277893, "xcomet_qe_score": 0.5830575823783875, "metricx_score": 3.8662309646606445, "metricx_qe_score": 5.159143447875977, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco che abbiamo riscontrato che la maggior parte delle principali lingue naturali possono ottenere un miglioramento delle prestazioni, eccetto che l'inglese, che registra una flessione in sette insiemi di dati e solo un aumento in tre insiemi di dati.", "metrics": {"bleu_score": 23.195524709125127, "chrf_score": 53.6729419760666, "xcomet_score": 0.9243383407592773, "xcomet_qe_score": 0.9251521825790405, "metricx_score": 2.412018060684204, "metricx_qe_score": 1.8017401695251465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il Crollo della Multilinguismo.\"", "metrics": {"bleu_score": 4.231118166423695, "chrf_score": 23.873595546005113, "xcomet_score": 0.14561688899993896, "xcomet_qe_score": 0.17403212189674377, "metricx_score": 20.187021255493164, "metricx_qe_score": 19.307109832763672, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Anche confrontiamo il divario di prestazione interlinguistica.", "metrics": {"bleu_score": 10.786826322527466, "chrf_score": 57.03765118491461, "xcomet_score": 0.8951412439346313, "xcomet_qe_score": 0.9157974123954773, "metricx_score": 4.891445636749268, "metricx_qe_score": 4.112817764282227, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questo grafico, la linea blu rappresenta il trasferimento di campo linguale a campo zero.", "metrics": {"bleu_score": 37.08163623065085, "chrf_score": 64.32480740045415, "xcomet_score": 0.701848566532135, "xcomet_qe_score": 0.7414963245391846, "metricx_score": 6.79803466796875, "metricx_qe_score": 7.316618919372559, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La linea arancione rappresenta il trasferimento di campo linguale a zero senza campo.", "metrics": {"bleu_score": 36.362270465000705, "chrf_score": 71.57944785259566, "xcomet_score": 0.7020798921585083, "xcomet_qe_score": 0.6822177171707153, "metricx_score": 8.171396255493164, "metricx_qe_score": 7.378306865692139, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Mentre la linea verde rappresenta la impostazione dell'angolo del modello.", "metrics": {"bleu_score": 36.72056269893591, "chrf_score": 72.64569186940976, "xcomet_score": 0.7431002855300903, "xcomet_qe_score": 0.7366740107536316, "metricx_score": 8.316022872924805, "metricx_qe_score": 8.932047843933105, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che, confrontando la linea verde e quella arancione, abbiamo riscontrato che il divario di prestazione di trasferimento di goal per impostazione zero è significativo. E confrontando la linea blu e quella arancione, abbiamo riscontrato che il divario di trasferimento si accorcia rapidamente per impostazione di pochi shot.", "metrics": {"bleu_score": 19.571383107117246, "chrf_score": 67.44862042928266, "xcomet_score": 0.550835132598877, "xcomet_qe_score": 0.5868654251098633, "metricx_score": 10.409503936767578, "metricx_qe_score": 11.59057903289795, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, abbiamo trovato altri interessanti risultati.", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 57.45539524737705, "xcomet_score": 0.9993494749069214, "xcomet_qe_score": 1.0, "metricx_score": 0.5870281457901001, "metricx_qe_score": 0.4528774321079254, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, il lavoro di progresso sull'encoder-decoder ha raggiunto risultati comparabili,", "metrics": {"bleu_score": 14.458924666162856, "chrf_score": 57.02773915633323, "xcomet_score": 0.7293400168418884, "xcomet_qe_score": 0.7138841152191162, "metricx_score": 6.799582004547119, "metricx_qe_score": 6.579401969909668, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "soprattutto in lingua inglese, e ha significativamente migliorato le prestazioni future sulle lingue naturali bersaglio. E abbiamo trovato modelli di linguaggio di codice come BERT che sono ancora all'interno di quel grid per compiti di parsing semantico a più lingue.", "metrics": {"bleu_score": 6.062120981645538, "chrf_score": 43.66664377677401, "xcomet_score": 0.0791473239660263, "xcomet_qe_score": 0.16282698512077332, "metricx_score": 19.536895751953125, "metricx_qe_score": 18.919631958007812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In sintetica, costruiamo ExamPolar, un benchmark unificato per la parola semantica a più angoli con più lingue naturali e molte rappresentazioni.", "metrics": {"bleu_score": 33.57306484097324, "chrf_score": 57.37454104065185, "xcomet_score": 0.41566604375839233, "xcomet_qe_score": 0.47676438093185425, "metricx_score": 14.473529815673828, "metricx_qe_score": 16.882160186767578, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo uno studio di benchmarking approfondito su tre rappresentanti di modelli di linguaggio multilingue.", "metrics": {"bleu_score": 16.18861356572822, "chrf_score": 63.20917839195129, "xcomet_score": 0.9708900451660156, "xcomet_qe_score": 0.9669621586799622, "metricx_score": 4.962244510650635, "metricx_qe_score": 4.6012349128723145, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E i nostri risultati mostrano molte scoperte interessanti,", "metrics": {"bleu_score": 46.713797772819994, "chrf_score": 73.8348745948334, "xcomet_score": 0.9990564584732056, "xcomet_qe_score": 0.985066831111908, "metricx_score": 1.562122106552124, "metricx_qe_score": 0.9854573607444763, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e via dicendo.", "metrics": {"bleu_score": 15.97357760615681, "chrf_score": 10.618822129068985, "xcomet_score": 0.479645699262619, "xcomet_qe_score": 0.9133378267288208, "metricx_score": 4.073238372802734, "metricx_qe_score": 1.4354145526885986, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E accogliamo i nostri articoli e il nostro codice.", "metrics": {"bleu_score": 32.16094349518416, "chrf_score": 44.86879573662257, "xcomet_score": 0.6817325353622437, "xcomet_qe_score": 0.6142590045928955, "metricx_score": 11.8357515335083, "metricx_qe_score": 11.754372596740723, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per aver ascoltato.\"", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 38.148526698747034, "xcomet_score": 0.4889771640300751, "xcomet_qe_score": 1.0, "metricx_score": 0.46939414739608765, "metricx_qe_score": 0.28675174713134766, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Il mio nome è Aydbilar, e vi darò un breve riassunto del lavoro intitolato \"Pattern di grugnito dalla traduzione, strategie e prestazioni.", "metrics": {"bleu_score": 5.71168499906942, "chrf_score": 25.60492398129493, "xcomet_score": 0.3137965202331543, "xcomet_qe_score": 0.35267868638038635, "metricx_score": 15.790680885314941, "metricx_qe_score": 16.635950088500977, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei colleghi di Google Translate.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4244665801525116, "metricx_qe_score": 0.3959866762161255, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco un modello di linguaggio a 540 miliardi di parametri presentato l'anno scorso nel 2022.", "metrics": {"bleu_score": 26.029050838873406, "chrf_score": 57.95663113972488, "xcomet_score": 0.7845755815505981, "xcomet_qe_score": 0.8286009430885315, "metricx_score": 9.067544937133789, "metricx_qe_score": 10.316774368286133, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "È stato addestrato su una vasta raccolta di testi che comprende 180 miliardi di documenti.\"", "metrics": {"bleu_score": 43.68363644461544, "chrf_score": 78.29625659499766, "xcomet_score": 0.4427945613861084, "xcomet_qe_score": 0.6630216836929321, "metricx_score": 4.16984748840332, "metricx_qe_score": 3.985013723373413, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nella Dammi per la Cucina, raggiunge lo stato dell'arte in centinaia di compiti di elaborazione del linguaggio naturale.", "metrics": {"bleu_score": 25.34743707366162, "chrf_score": 50.434045020009435, "xcomet_score": 0.17238804697990417, "xcomet_qe_score": 0.1297122985124588, "metricx_score": 12.637223243713379, "metricx_qe_score": 14.060715675354004, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo lavoro presentiamo la prima indagine sistematica sulla stimolazione di modelli linguistici per la traduzione automatica.\"", "metrics": {"bleu_score": 37.51840463233444, "chrf_score": 68.69208537683666, "xcomet_score": 0.8748399019241333, "xcomet_qe_score": 0.730134129524231, "metricx_score": 2.2941386699676514, "metricx_qe_score": 2.2356884479522705, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità del MT.", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 97.21548073505335, "xcomet_score": 0.942854642868042, "xcomet_qe_score": 0.9139183163642883, "metricx_score": 0.8487875461578369, "metricx_qe_score": 1.12013840675354, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò implica l'utilizzo di set di test più recenti per evitare di tornare a eseguire un'altra iterazione dei dati di test con i dati di addestramento del modello di linguaggio.", "metrics": {"bleu_score": 52.28804392271881, "chrf_score": 72.4916671637837, "xcomet_score": 0.8477420806884766, "xcomet_qe_score": 0.8626327514648438, "metricx_score": 4.7541069984436035, "metricx_qe_score": 4.979069709777832, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E confrontiamo due sistemi di punta di diamante. I sistemi che si sono dimostrati i migliori sono quelli dell'evaluazione WMT.\"", "metrics": {"bleu_score": 3.9566236357113054, "chrf_score": 34.004324776851256, "xcomet_score": 0.31022727489471436, "xcomet_qe_score": 0.29142239689826965, "metricx_score": 7.5857086181640625, "metricx_qe_score": 7.067262172698975, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche di traduzione di MT di ultima generazione e forniamo anche risultati di valutazione umana di esperti.", "metrics": {"bleu_score": 8.339775806898654, "chrf_score": 42.58588874674276, "xcomet_score": 0.7910017371177673, "xcomet_qe_score": 0.8111688494682312, "metricx_score": 5.282593727111816, "metricx_qe_score": 5.301767349243164, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Infine, forniamo alcune strategie di selezione di suggerimenti.", "metrics": {"bleu_score": 35.54333944875169, "chrf_score": 61.03587780537322, "xcomet_score": 0.7518724203109741, "xcomet_qe_score": 0.782841145992279, "metricx_score": 9.500823020935059, "metricx_qe_score": 9.724136352539062, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il training dei modelli di lingua macchina (LLM) è influenzato significativamente dalle istruzioni. Come possiamo vedere in un esperimento semplice in cui utilizziamo istruzioni a una sola ripresa e forniamo due diverse istruzioni per una sola frase.", "metrics": {"bleu_score": 9.145115975240905, "chrf_score": 46.70517661869561, "xcomet_score": 0.7723988890647888, "xcomet_qe_score": 0.8098229169845581, "metricx_score": 6.8907647132873535, "metricx_qe_score": 5.114116668701172, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La maggioranza delle frasi, 516 su 1000,", "metrics": {"bleu_score": 41.80134288483487, "chrf_score": 66.41108172448429, "xcomet_score": 0.9875984191894531, "xcomet_qe_score": 0.9472764730453491, "metricx_score": 1.5515633821487427, "metricx_qe_score": 3.6252856254577637, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "presenta una differenza di più di un punto di sfumatura.", "metrics": {"bleu_score": 4.9323515694897075, "chrf_score": 28.874536573722086, "xcomet_score": 0.3265007734298706, "xcomet_qe_score": 0.4845578968524933, "metricx_score": 8.744718551635742, "metricx_qe_score": 5.524657726287842, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che in casi estremi può arrivare fino a 40 punti di sfumatura.", "metrics": {"bleu_score": 27.22589423069701, "chrf_score": 58.588619202553694, "xcomet_score": 0.6042908430099487, "xcomet_qe_score": 0.5263785123825073, "metricx_score": 11.860326766967773, "metricx_qe_score": 10.508124351501465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "È importante selezionare una strategia di stimolazione adeguata.\" \"Iniziando", "metrics": {"bleu_score": 16.59038701421971, "chrf_score": 59.700962202187156, "xcomet_score": 0.24561834335327148, "xcomet_qe_score": 0.29067692160606384, "metricx_score": 13.597773551940918, "metricx_qe_score": 8.084260940551758, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "i nostri esperimenti, ci rivolgiamo a una strategia a cinque fasi di stimolazione, nella quale segnaleremo la frase che forniamo al sistema con la lingua in cui è scritta.\"", "metrics": {"bleu_score": 24.32182875926731, "chrf_score": 44.79354396462021, "xcomet_score": 0.6514230370521545, "xcomet_qe_score": 0.690544843673706, "metricx_score": 6.7747297286987305, "metricx_qe_score": 5.260003566741943, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo esempio, dove eseguiamo la traduzione dal tedesco nell'inglese, le frasi tedesche sono contrassegnate con la colonna tedesca e le traduzioni inglesi con la colonna inglese.\"", "metrics": {"bleu_score": 28.14431573583864, "chrf_score": 64.24673732612747, "xcomet_score": 0.5987827777862549, "xcomet_qe_score": 0.8724603652954102, "metricx_score": 3.29681396484375, "metricx_qe_score": 1.1488490104675293, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo notato che la forma reale della stampa non ha un grande influenza nel caso di stampa a breve serie.", "metrics": {"bleu_score": 26.153117750218023, "chrf_score": 53.81232743604729, "xcomet_score": 0.418089359998703, "xcomet_qe_score": 0.3727477490901947, "metricx_score": 8.34476089477539, "metricx_qe_score": 7.242945671081543, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E' fondamentale per il prompting a zero e a uno.", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 41.66106660272283, "xcomet_score": 0.5474047660827637, "xcomet_qe_score": 0.4832116663455963, "metricx_score": 6.4320878982543945, "metricx_qe_score": 5.7299275398254395, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E quando andiamo, come nel nostro caso, al prompting a cinque, non vi è quasi nessuna differenza nella forma del prompting.\"", "metrics": {"bleu_score": 27.85481825766967, "chrf_score": 57.93137367789918, "xcomet_score": 0.637706995010376, "xcomet_qe_score": 0.5070866346359253, "metricx_score": 7.647770881652832, "metricx_qe_score": 8.593042373657227, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E sono gli esempi a portare il peso principale.", "metrics": {"bleu_score": 4.521356896113449, "chrf_score": 30.762265249578974, "xcomet_score": 0.9793286323547363, "xcomet_qe_score": 0.9735928773880005, "metricx_score": 5.623107433319092, "metricx_qe_score": 6.14769983291626, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di partenza.", "metrics": {"bleu_score": 81.51678595510181, "chrf_score": 86.48271095316493, "xcomet_score": 0.9995748996734619, "xcomet_qe_score": 0.9972367286682129, "metricx_score": 1.2445069551467896, "metricx_qe_score": 1.0493268966674805, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E' importante selezionare gli esempi da traduzioni di alta qualità.", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 89.60628464939934, "xcomet_score": 0.96531081199646, "xcomet_qe_score": 0.9655612707138062, "metricx_score": 0.7303129434585571, "metricx_qe_score": 0.49907082319259644, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, confrontiamo le selezioni di prompt da dati di addestramento delle valutazioni WMT o dai dati DEF.\"", "metrics": {"bleu_score": 21.640076381354273, "chrf_score": 70.83081968029626, "xcomet_score": 0.7298593521118164, "xcomet_qe_score": 0.6677871942520142, "metricx_score": 8.843743324279785, "metricx_qe_score": 8.963595390319824, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il dato di profondità è molto più curato e di qualità superiore rispetto al dati addestrati, quindi il risultato è migliore utili", "metrics": {"bleu_score": 18.558718871235925, "chrf_score": 49.74020054930597, "xcomet_score": 0.5317023396492004, "xcomet_qe_score": 0.44163256883621216, "metricx_score": 11.310148239135742, "metricx_qe_score": 6.4456305503845215, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "zzando i dati di profondità.", "metrics": {"bleu_score": 7.244320397501573, "chrf_score": 16.314945459637816, "xcomet_score": 0.13199467957019806, "xcomet_qe_score": 0.12783846259117126, "metricx_score": 21.290149688720703, "metricx_qe_score": 21.777671813964844, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora, almeno, i sistemi di punta di diamante hanno un vantaggio sostanziale rispetto ai traduttori di bandiera.", "metrics": {"bleu_score": 17.92334464048543, "chrf_score": 47.3291298873454, "xcomet_score": 0.4223344922065735, "xcomet_qe_score": 0.40529006719589233, "metricx_score": 11.162339210510254, "metricx_qe_score": 11.368688583374023, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma uno è molto vicino a un sistema commerciale.", "metrics": {"bleu_score": 39.281465090051306, "chrf_score": 53.50791567385295, "xcomet_score": 0.5900001525878906, "xcomet_qe_score": 0.7603641152381897, "metricx_score": 13.54383373260498, "metricx_qe_score": 15.399392127990723, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro caso, abbiamo scelto di collaborare con Google Translate.", "metrics": {"bleu_score": 62.20700406786387, "chrf_score": 68.82220312094296, "xcomet_score": 0.9008285999298096, "xcomet_qe_score": 0.9075782299041748, "metricx_score": 3.875509023666382, "metricx_qe_score": 5.52525520324707, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Illemmi che abbiamo acquisito dagli esperimenti sull'EMAIL utilizzando il framework MQM è che la fluidità della palma è paragonabile ai sistemi di punta, ma la differenza principale risiede nell'accuratezza.\"", "metrics": {"bleu_score": 26.07305499322812, "chrf_score": 50.79739213832711, "xcomet_score": 0.25121963024139404, "xcomet_qe_score": 0.241951584815979, "metricx_score": 16.077091217041016, "metricx_qe_score": 14.879152297973633, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione.", "metrics": {"bleu_score": 48.44273237963865, "chrf_score": 68.87505219770097, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.952473521232605, "metricx_qe_score": 1.4636950492858887, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Evidentemente, Palm sceglie di produrre una traduzione più gradevole, talvolta eliminando parti della frase originale che sono state create durante la traduzione.\"", "metrics": {"bleu_score": 14.724623770949025, "chrf_score": 50.52438479493935, "xcomet_score": 0.9341205358505249, "xcomet_qe_score": 0.8864448070526123, "metricx_score": 5.505680561065674, "metricx_qe_score": 5.030299186706543, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria dello stile esterno per PAN è inferiore rispetto ai sistemi di punta, il che rappresenta un segnale aggiuntivo. Ecco che il Parm fornisce output molto fluente, ma ancora con alcuni problemi di accuratezza.", "metrics": {"bleu_score": 27.568857210758853, "chrf_score": 55.6377215872489, "xcomet_score": 0.49391698837280273, "xcomet_qe_score": 0.5933108329772949, "metricx_score": 12.894881248474121, "metricx_qe_score": 13.284775733947754, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, è tutto per questo breve riassunto.", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 51.362108670712715, "xcomet_score": 0.9450148344039917, "xcomet_qe_score": 1.0, "metricx_score": 1.7403863668441772, "metricx_qe_score": 0.5675175786018372, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per ulteriori dettagli, per favore, rivolgiti alla presentazione completa del documento.", "metrics": {"bleu_score": 14.458924666162856, "chrf_score": 52.847763246703074, "xcomet_score": 0.8166688680648804, "xcomet_qe_score": 0.8855066299438477, "metricx_score": 3.6654484272003174, "metricx_qe_score": 2.663526773452759, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie molto.", "metrics": {"bleu_score": 0.0, "chrf_score": 48.343554593554586, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.1358058899641037, "metricx_qe_score": 0.2529047131538391, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, un dottorando presso l'Università di Salento in Germania.", "metrics": {"bleu_score": 27.392758081541032, "chrf_score": 58.85528151600534, "xcomet_score": 0.6893637180328369, "xcomet_qe_score": 0.8688235282897949, "metricx_score": 5.944739818572998, "metricx_qe_score": 5.66867208480835, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questo video, vorrei presentare il nostro lavoro recente, \"Wicker Than You Think\", un'analisi critica sulle forniture settimanali già disponibili.", "metrics": {"bleu_score": 35.345481163747955, "chrf_score": 56.23024142286817, "xcomet_score": 0.5667340755462646, "xcomet_qe_score": 0.5783113837242126, "metricx_score": 10.681612968444824, "metricx_qe_score": 12.273486137390137, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco il lavoro è stato realizzato in collaborazione con Xiao Yuxian, Mario Smoothbath e Diaz Stefan e DTich Claco.\"", "metrics": {"bleu_score": 2.5771688135729764, "chrf_score": 27.16747749360972, "xcomet_score": 0.15914702415466309, "xcomet_qe_score": 0.1454179733991623, "metricx_score": 12.158733367919922, "metricx_qe_score": 13.42544937133789, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"I'd like to begin with a brief introduction to weak supervision and weakly supervised learning.\" \"Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento supervisionato debolmente.\"", "metrics": {"bleu_score": 30.28257698140165, "chrf_score": 77.22866873879268, "xcomet_score": 0.9173297882080078, "xcomet_qe_score": 0.9001164436340332, "metricx_score": 24.339733123779297, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In supervision debole, non etichettiamo manualmente i dati.", "metrics": {"bleu_score": 12.35157756169889, "chrf_score": 54.83503548830131, "xcomet_score": 0.8210869431495667, "xcomet_qe_score": 0.7462137341499329, "metricx_score": 8.785533905029297, "metricx_qe_score": 7.661614418029785, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Invece, utilizziamo fonti di etichettatura debole, come regole semplici di heuristica, banche di conoscenza o sorgenti di codice di località, come illustrato nella figura a destra.", "metrics": {"bleu_score": 21.26810303567065, "chrf_score": 58.94757790456618, "xcomet_score": 0.7047604322433472, "xcomet_qe_score": 0.7338960766792297, "metricx_score": 7.5984272956848145, "metricx_qe_score": 7.90198278427124, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quando confrontati con annotazioni umane, le annotazioni deboli sono molto più economiche, tuttavia sono anche rumorose, il che significa che una certa quantità di annotazioni è errata.", "metrics": {"bleu_score": 38.32331086152312, "chrf_score": 63.17671911673024, "xcomet_score": 0.7172366380691528, "xcomet_qe_score": 0.6708664894104004, "metricx_score": 2.2174108028411865, "metricx_qe_score": 2.5791287422180176, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Se addestriamo reti neurali su dati settimanali etichettati, esse tendono a memorizzare il rumore di etichetta e non generalizzano.\"", "metrics": {"bleu_score": 38.767226272410085, "chrf_score": 67.71387636427416, "xcomet_score": 0.5706200003623962, "xcomet_qe_score": 0.6404635906219482, "metricx_score": 10.931705474853516, "metricx_qe_score": 10.610684394836426, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In apprendimento supervisionato, gli algoritmi vengono proposti per addestrare reti neurali in modo robusto in presenza di rumore al livello tale che i modelli di training generalizzino bene.\" Negli ultimi", "metrics": {"bleu_score": 11.206636769221339, "chrf_score": 50.11066160728574, "xcomet_score": 0.25839707255363464, "xcomet_qe_score": 0.17757545411586761, "metricx_score": 8.140889167785645, "metricx_qe_score": 8.860156059265137, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "lavori in WSL, il WSL sta per apprendimento supervisionato a breve termine. Una comune affermazione è che le persone affermano di addestrare i modelli solo su dati di lavoro settimanali e raggiungere prestazioni elevate sui set di test puliti.", "metrics": {"bleu_score": 31.906404839091437, "chrf_score": 60.64321117532191, "xcomet_score": 0.5395795106887817, "xcomet_qe_score": 0.5449067950248718, "metricx_score": 9.953563690185547, "metricx_qe_score": 10.568702697753906, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, questo asserto non è tecnicamente errato, ma c'è un'eccezione.\" Ecco la traduzione: \"Quello che si assume spesso è che ci sia un ulteriore set di validazione o firewall per la selezione del modello.\"", "metrics": {"bleu_score": 16.684195647378832, "chrf_score": 42.04413412904176, "xcomet_score": 0.2720431685447693, "xcomet_qe_score": 0.35080400109291077, "metricx_score": 9.519920349121094, "metricx_qe_score": 7.95928955078125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo interrotto lo studio di questo problema, in quanto questo implica la necessità di annotazioni manuali settimanali da parte di molti.", "metrics": {"bleu_score": 8.383924775392588, "chrf_score": 39.02532552878554, "xcomet_score": 0.4741998314857483, "xcomet_qe_score": 0.49674490094184875, "metricx_score": 13.269325256347656, "metricx_qe_score": 13.814796447753906, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma, come un elefante nella stanza, questa necessità viene spesso trascurata.", "metrics": {"bleu_score": 85.5526185871245, "chrf_score": 97.25947730087195, "xcomet_score": 0.9902687072753906, "xcomet_qe_score": 0.9763463735580444, "metricx_score": 1.250314474105835, "metricx_qe_score": 1.6525919437408447, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco la domanda precedentemente posta richiede tre domande di ricerca.", "metrics": {"bleu_score": 24.689800508009657, "chrf_score": 39.94133461831053, "xcomet_score": 0.33370620012283325, "xcomet_qe_score": 0.37915557622909546, "metricx_score": 9.842516899108887, "metricx_qe_score": 9.5407075881958, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, è necessaria una valutazione pulita dei dati per il WSL? O forse possiamo utilizzare un insieme di valutazione rumoroso invece", "metrics": {"bleu_score": 18.52955977660994, "chrf_score": 56.3919441302842, "xcomet_score": 0.7637559175491333, "xcomet_qe_score": 0.7234249114990234, "metricx_score": 3.942535161972046, "metricx_qe_score": 3.217453956604004, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "?\" Secondo, se i dati puliti sono richiesti o se i dati puliti sono necessari per il funzionamento di WSL, quante campioni puliti dobbiamo avere?", "metrics": {"bleu_score": 24.49463959359131, "chrf_score": 54.209468904686254, "xcomet_score": 0.8345626592636108, "xcomet_qe_score": 0.7823613882064819, "metricx_score": 5.532469272613525, "metricx_qe_score": 5.975660800933838, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per sfruttarli?", "metrics": {"bleu_score": 45.074825445349695, "chrf_score": 70.17904229513262, "xcomet_score": 0.9850255250930786, "xcomet_qe_score": 0.9756594300270081, "metricx_score": 0.9538323283195496, "metricx_qe_score": 1.1143794059753418, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti.", "metrics": {"bleu_score": 62.994133266553334, "chrf_score": 76.7838651516674, "xcomet_score": 0.9878783226013184, "xcomet_qe_score": 0.9776768684387207, "metricx_score": 0.658898115158081, "metricx_qe_score": 0.4838544428348541, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo a scoprire che, interessantemente, i metodi WSL recenti richiedono campioni di piatto puliti a vasta scala per funzionare correttamente.", "metrics": {"bleu_score": 16.020720994064927, "chrf_score": 56.30458529045704, "xcomet_score": 0.5364580154418945, "xcomet_qe_score": 0.6696146726608276, "metricx_score": 8.241089820861816, "metricx_qe_score": 6.830424785614014, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti, c'è un grande calo di prestazioni.", "metrics": {"bleu_score": 8.591316733350183, "chrf_score": 32.12197826485285, "xcomet_score": 0.9965637922286987, "xcomet_qe_score": 1.0, "metricx_score": 1.1486599445343018, "metricx_qe_score": 2.6879801750183105, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli di trend non possono generalizzare oltre le etichette della settimana originale. Il training è inutile.", "metrics": {"bleu_score": 48.29002135228188, "chrf_score": 65.23136982746594, "xcomet_score": 0.674649715423584, "xcomet_qe_score": 0.7030289173126221, "metricx_score": 9.19414234161377, "metricx_qe_score": 8.366016387939453, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che gli approcci WSL richiedono dati etichettati in modo pulito per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.", "metrics": {"bleu_score": 55.40962681082705, "chrf_score": 75.70457814577672, "xcomet_score": 0.9803972244262695, "xcomet_qe_score": 0.9892100095748901, "metricx_score": 2.5722672939300537, "metricx_qe_score": 3.1143174171447754, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro secondo ritrovamento è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere un miglioramento del loro rendimento, come mostrato nella figura a sinistra.", "metrics": {"bleu_score": 35.22501859997971, "chrf_score": 64.1409991916063, "xcomet_score": 0.899330198764801, "xcomet_qe_score": 0.884543776512146, "metricx_score": 3.547189474105835, "metricx_qe_score": 3.593942642211914, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Di solito, abbiamo bisogno di 20 campioni per classe per raggiungere un buon livello di prestazione.\"", "metrics": {"bleu_score": 27.322315104876445, "chrf_score": 61.56711993774457, "xcomet_score": 0.9824000597000122, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.873808741569519, "metricx_qe_score": 0.9125840067863464, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è la fine della storia, poiché, in entrambi i casi, l'accesso a campioni puliti ci consentirebbe di ottenere prestazioni ancora migliori mediante l'addestramento direttamente su di essi. Il grafico ros", "metrics": {"bleu_score": 10.046152640557759, "chrf_score": 49.8412493697498, "xcomet_score": 0.7319672107696533, "xcomet_qe_score": 0.5146022439002991, "metricx_score": 6.826999664306641, "metricx_qe_score": 5.879946231842041, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "so mostra la differenza di prestazione tra le approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione.", "metrics": {"bleu_score": 72.20551409594538, "chrf_score": 83.24973464251838, "xcomet_score": 0.7108807563781738, "xcomet_qe_score": 0.7846049070358276, "metricx_score": 8.33430290222168, "metricx_qe_score": 7.66244649887085, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che, se abbiamo 10 campioni per classe, l'addestramento fine tuning comincia a superare gli approcci WSL.\"", "metrics": {"bleu_score": 43.41999352730602, "chrf_score": 57.188318355486466, "xcomet_score": 0.3864890933036804, "xcomet_qe_score": 0.35632362961769104, "metricx_score": 8.977740287780762, "metricx_qe_score": 9.506624221801758, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Finalmente, l'incremento della prestazione affermato in precedenti approcci WSL può essere facilmente raggiunto consentendo di proseguire la fine-tuning sulle campioni di validazione pulite.", "metrics": {"bleu_score": 17.22583000702647, "chrf_score": 55.475081869320476, "xcomet_score": 0.8613014817237854, "xcomet_qe_score": 0.8543965220451355, "metricx_score": 6.230406284332275, "metricx_qe_score": 6.975466728210449, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco cosa possiamo dedurre dalle figure: il modello Valina, denominato FTW, inizialmente sottoutilizza metodi più complessi come WSL come il coseno.", "metrics": {"bleu_score": 5.952793612126117, "chrf_score": 45.60157971412172, "xcomet_score": 0.3682330846786499, "xcomet_qe_score": 0.33165180683135986, "metricx_score": 11.579679489135742, "metricx_qe_score": 10.81298828125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se consentiamo di continuare a regolare le prestazioni sulle campioni puliti, il metodo FTW si comporta altrettanto bene come gli altri metodi.", "metrics": {"bleu_score": 15.80697971591576, "chrf_score": 58.848097725682265, "xcomet_score": 0.8875929117202759, "xcomet_qe_score": 0.9107939600944519, "metricx_score": 3.4030990600585938, "metricx_qe_score": 3.799332857131958, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In pratica, non c'è alcun motivo per scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco.\"", "metrics": {"bleu_score": 60.81013816063651, "chrf_score": 83.31420065172105, "xcomet_score": 0.9744244813919067, "xcomet_qe_score": 0.9688448905944824, "metricx_score": 0.8427740931510925, "metricx_qe_score": 1.0247597694396973, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In sintesi, abbiamo dimostrato che gli approcci WSL recenti richiedono campioni puliti e manualmente annotati per funzionare correttamente.", "metrics": {"bleu_score": 38.2042349731716, "chrf_score": 79.01219876913835, "xcomet_score": 0.9651108980178833, "xcomet_qe_score": 0.9593861103057861, "metricx_score": 2.208625316619873, "metricx_qe_score": 2.542881488800049, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il loro guadagno di prestazioni e la loro praticità sono fortemente sopravvalutati.\"", "metrics": {"bleu_score": 44.534504264163466, "chrf_score": 77.45101012569414, "xcomet_score": 0.9709650278091431, "xcomet_qe_score": 0.9829819202423096, "metricx_score": 2.0279619693756104, "metricx_qe_score": 2.0174431800842285, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I'm ready to assist! Here is the translation: \"I", "metrics": {"bleu_score": 0.0, "chrf_score": 11.770730176649549, "xcomet_score": 0.22002534568309784, "xcomet_qe_score": 0.13410310447216034, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "nostri consigli concreti per future ore di lavoro.\" \"Rapportare i criteri di selezione del modello.", "metrics": {"bleu_score": 30.82627646062185, "chrf_score": 66.03468086052035, "xcomet_score": 0.14044500887393951, "xcomet_qe_score": 0.14876890182495117, "metricx_score": 17.124797821044922, "metricx_qe_score": 23.823755264282227, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, rapportare se la selezione del modello è stata effettuata utilizzando campioni di validazione puliti.\"", "metrics": {"bleu_score": 30.14335251508215, "chrf_score": 57.672859978727054, "xcomet_score": 0.9457361698150635, "xcomet_qe_score": 0.8659597635269165, "metricx_score": 1.3860065937042236, "metricx_qe_score": 2.2767221927642822, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Secondo, le WSL dovrebbero essere avviate con linee di riferimento future come campioni chiari.", "metrics": {"bleu_score": 7.5110041768643505, "chrf_score": 33.661486160236855, "xcomet_score": 0.4799124002456665, "xcomet_qe_score": 0.5667958855628967, "metricx_score": 16.17888069152832, "metricx_qe_score": 14.583956718444824, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, il fine tuning continuo è un'ottima base semplice che dovrebbe essere considerata nel lavoro futuro sulle WSL.", "metrics": {"bleu_score": 34.49529676984645, "chrf_score": 73.22546264697286, "xcomet_score": 0.944298505783081, "xcomet_qe_score": 0.9345680475234985, "metricx_score": 5.008033275604248, "metricx_qe_score": 5.548215389251709, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Finalmente, abbiamo aperto il nostro codice.", "metrics": {"bleu_score": 21.069764742263047, "chrf_score": 52.76892981194109, "xcomet_score": 0.8655213117599487, "xcomet_qe_score": 0.8577991724014282, "metricx_score": 2.6434683799743652, "metricx_qe_score": 2.162710428237915, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Puoi trovarlo mediante il codice QR presente su questa diapositiva.", "metrics": {"bleu_score": 35.08439695638686, "chrf_score": 68.3659248150633, "xcomet_score": 0.9473080039024353, "xcomet_qe_score": 0.9683774709701538, "metricx_score": 0.7287934422492981, "metricx_qe_score": 0.393388032913208, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo di esservi utile e god", "metrics": {"bleu_score": 0.0, "chrf_score": 10.314457520711832, "xcomet_score": 0.12961998581886292, "xcomet_qe_score": 0.1431085765361786, "metricx_score": 11.28946590423584, "metricx_qe_score": 2.598128318786621, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ervi la conferenza.", "metrics": {"bleu_score": 24.880469496253564, "chrf_score": 51.12137579191449, "xcomet_score": 0.31768181920051575, "xcomet_qe_score": 0.18418288230895996, "metricx_score": 15.03958511352539, "metricx_qe_score": 15.233232498168945, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, sono James Finch.", "metrics": {"bleu_score": 80.91067115702207, "chrf_score": 98.85983679343904, "xcomet_score": 0.9799991846084595, "xcomet_qe_score": 0.9783983826637268, "metricx_score": 0.09325172007083893, "metricx_qe_score": 0.21986857056617737, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E sono Sarah Finch. E", "metrics": {"bleu_score": 53.7284965911771, "chrf_score": 82.40338633731511, "xcomet_score": 0.8985717296600342, "xcomet_qe_score": 0.9349071979522705, "metricx_score": 4.190596103668213, "metricx_qe_score": 0.42572200298309326, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "oggi vi dirò tutto sulla ABCeVal, un nuovo approccio dimensionale per valutare l'IA conversazionale.\"", "metrics": {"bleu_score": 57.04346472015739, "chrf_score": 83.50361385941314, "xcomet_score": 0.9201719760894775, "xcomet_qe_score": 0.9060043692588806, "metricx_score": 2.5485284328460693, "metricx_qe_score": 2.6223812103271484, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato condotto dal Laboratorio di Intelligenza Artificiale dell'Emory, guidato dal Professor Geno Choi dell'Università Emory, e in collaborazione con l'intelligenza artificiale di Amazon Alexa.", "metrics": {"bleu_score": 19.545984328607467, "chrf_score": 57.47525848425571, "xcomet_score": 0.7788205146789551, "xcomet_qe_score": 0.8322043418884277, "metricx_score": 3.565168857574463, "metricx_qe_score": 3.4521751403808594, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Vogliamo dire che hai sviluppato un modello di dialogo e vorresti vedere come si comporta rispetto allo stato dell'arte attuale.\"", "metrics": {"bleu_score": 36.852891195185464, "chrf_score": 60.75097437037154, "xcomet_score": 0.8154232501983643, "xcomet_qe_score": 0.7990694046020508, "metricx_score": 4.614487648010254, "metricx_qe_score": 4.162660121917725, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare l'evaluazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni è meglio o di valutare le conversazioni su una scala di Lickert.", "metrics": {"bleu_score": 57.60270264538816, "chrf_score": 83.02620825956822, "xcomet_score": 0.8382205367088318, "xcomet_qe_score": 0.885833203792572, "metricx_score": 2.4182515144348145, "metricx_qe_score": 1.5752002000808716, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olimpiche della qualità del dialogo, ma la qualità del dialogo ha molte dimensioni.", "metrics": {"bleu_score": 58.66943184579981, "chrf_score": 75.66703685559696, "xcomet_score": 0.8541439771652222, "xcomet_qe_score": 0.8734374046325684, "metricx_score": 7.616232395172119, "metricx_qe_score": 10.331108093261719, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, potresti voler valutare più dimensioni della qualità del dialogo per comprendere le forze e le debolezze del modello su un livello più fine. \"Un", "metrics": {"bleu_score": 19.768457351918478, "chrf_score": 55.916744805955396, "xcomet_score": 0.2488269805908203, "xcomet_qe_score": 0.34231287240982056, "metricx_score": 5.934077262878418, "metricx_qe_score": 3.4919018745422363, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "approccio è quello di chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi di confronto o scale di Likert esistenti.\"", "metrics": {"bleu_score": 50.38700385514711, "chrf_score": 66.92481526158883, "xcomet_score": 0.8085014820098877, "xcomet_qe_score": 0.7929269075393677, "metricx_score": 3.065263509750366, "metricx_qe_score": 3.2991220951080322, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'evaluazione del dialogo dimensionale.", "metrics": {"bleu_score": 50.82365016563264, "chrf_score": 78.92370669402055, "xcomet_score": 0.9562559127807617, "xcomet_qe_score": 0.9137212038040161, "metricx_score": 2.407097578048706, "metricx_qe_score": 2.345724105834961, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Noi stiamo cercando di ridurre la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessa.", "metrics": {"bleu_score": 44.023989894938545, "chrf_score": 75.80159736156654, "xcomet_score": 0.9306421279907227, "xcomet_qe_score": 0.9394963979721069, "metricx_score": 2.222930431365967, "metricx_qe_score": 2.042844295501709, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Eleggiamo questo approccio come annotazione di comportamenti in chat o ABC valutazione breve.", "metrics": {"bleu_score": 14.865996369027277, "chrf_score": 68.10419979302151, "xcomet_score": 0.639564037322998, "xcomet_qe_score": 0.6405667662620544, "metricx_score": 10.932942390441895, "metricx_qe_score": 9.924142837524414, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat suggeriti dalla letteratura recente per influenzare la qualità della chat.\"", "metrics": {"bleu_score": 50.3093499674126, "chrf_score": 76.78533188168693, "xcomet_score": 0.9139735102653503, "xcomet_qe_score": 0.8935425281524658, "metricx_score": 1.1622778177261353, "metricx_qe_score": 1.5186161994934082, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"L'ABC eval è in grado di misurare i tassi con cui i modelli di chat commetteranno errori tematici vari.\"", "metrics": {"bleu_score": 46.83624652369494, "chrf_score": 75.23772294750427, "xcomet_score": 0.801699161529541, "xcomet_qe_score": 0.7576389312744141, "metricx_score": 4.882998466491699, "metricx_qe_score": 5.274661540985107, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Esempio, ABC misura il numero di giri in cui il modello di conversazione ignora il partner o dice qualcosa di irrilevante. \"Ecco, il modello può essere incoerente con sé stesso o il suo partner, fantascia fatti falsi o viola la conoscenza comune, e quando il modello ha successo o fallisce a mostrare empatia.\"", "metrics": {"bleu_score": 35.077579511719236, "chrf_score": 64.1326671646688, "xcomet_score": 0.33837735652923584, "xcomet_qe_score": 0.3622894585132599, "metricx_score": 10.443082809448242, "metricx_qe_score": 11.480944633483887, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di ultima generazione e li abbiamo valutati su 100 conversazioni tra umani e bot per modello utilizzando ABC eval.", "metrics": {"bleu_score": 54.0512047723334, "chrf_score": 81.33736625312301, "xcomet_score": 0.9338421821594238, "xcomet_qe_score": 0.9271592497825623, "metricx_score": 1.7480556964874268, "metricx_qe_score": 2.2519311904907227, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato anche queste conversazioni utilizzando tre metodi esistenti, valutazioni dei liquori a livello di turno, valutazioni dei liquori a livello di dialogo e confronti pairwise a livello di dialogo.", "metrics": {"bleu_score": 51.47053923571922, "chrf_score": 77.53999067075617, "xcomet_score": 0.2588154077529907, "xcomet_qe_score": 0.28379902243614197, "metricx_score": 22.638853073120117, "metricx_qe_score": 21.898483276367188, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per ogni metodo esistente, abbiamo raccolto valutazioni su otto degli aspetti del dialogo più comunemente misurati, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.", "metrics": {"bleu_score": 70.61523647874897, "chrf_score": 78.74731609478143, "xcomet_score": 0.9731135368347168, "xcomet_qe_score": 0.9651960134506226, "metricx_score": 1.1872036457061768, "metricx_qe_score": 2.09515643119812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi dei risultati delle valutazioni, abbiamo scoperto che gli etichette di comportamento ABC Eval sono generalmente più attendibili rispetto alle etichette raccolte da metodi esistenti, misurate mediante l'accordo interannotatore su 100 conversazioni doppie annotate.", "metrics": {"bleu_score": 15.002439804284846, "chrf_score": 66.70269852288712, "xcomet_score": 0.7958406209945679, "xcomet_qe_score": 0.7593847513198853, "metricx_score": 5.415927886962891, "metricx_qe_score": 4.534178256988525, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, i label di ABC sono più predittivi della qualità complessiva della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostra questo semplice analisi di regressione lineare.", "metrics": {"bleu_score": 40.13786550577507, "chrf_score": 68.85880695725753, "xcomet_score": 0.7075075507164001, "xcomet_qe_score": 0.6866538524627686, "metricx_score": 10.143413543701172, "metricx_qe_score": 9.34359073638916, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come misurare la proporzione di conversazioni con autocontraddizioni e contraddizioni con il partner spiega il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi di consistenza del liquore spiegano solo il 4% o meno.", "metrics": {"bleu_score": 46.2808906740756, "chrf_score": 67.32884031555673, "xcomet_score": 0.4013828635215759, "xcomet_qe_score": 0.4289693236351013, "metricx_score": 13.065564155578613, "metricx_qe_score": 13.78333568572998, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Finalmente, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del chiacchierato utilizzando una regressione lineare a passi.", "metrics": {"bleu_score": 66.89604664235208, "chrf_score": 84.09127693931767, "xcomet_score": 0.7621115446090698, "xcomet_qe_score": 0.7930653095245361, "metricx_score": 2.9889419078826904, "metricx_qe_score": 2.7491865158081055, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Può osservare come la combinazione di tutti i metrici ABC spiega più del 25% della qualità della conversazione. E come si rimuove ogni metrico uno alla volta, la maggior parte di essi comporta la perdita di una notevole quantità di informazioni sulla qualità.", "metrics": {"bleu_score": 36.31409147978041, "chrf_score": 64.54856902084745, "xcomet_score": 0.5760363340377808, "xcomet_qe_score": 0.7629567384719849, "metricx_score": 8.151017189025879, "metricx_qe_score": 6.528779029846191, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Al contrario, la combinazione di tutti i parametri di Likert a livello di turno spiega molto meno della qualità e pochi di questi parametri contengono informazioni uniche.", "metrics": {"bleu_score": 24.04073942635064, "chrf_score": 55.700222593742, "xcomet_score": 0.9064866304397583, "xcomet_qe_score": 0.8415906429290771, "metricx_score": 3.921652317047119, "metricx_qe_score": 4.693450927734375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Essi metodi di valutazione ABC fanno in modo affidabile, informativo e distinto per valutare l'IA conversazionale con una risoluzione più alta rispetto ai metodi precedenti sono in grado di raggiungere.\" Può", "metrics": {"bleu_score": 24.762685010055858, "chrf_score": 66.75760727659843, "xcomet_score": 0.3947424590587616, "xcomet_qe_score": 0.37415313720703125, "metricx_score": 13.047186851501465, "metricx_qe_score": 11.528396606445312, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "essere visto dai risultati del nostro esperimento che diversi sfide ancora rimangono e sono state precisamente quantificate.", "metrics": {"bleu_score": 18.605335292758287, "chrf_score": 63.08265940275138, "xcomet_score": 0.7237718105316162, "xcomet_qe_score": 0.7442319989204407, "metricx_score": 10.512822151184082, "metricx_qe_score": 11.475141525268555, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, i bot che abbiamo testato hanno violazioni di senso comune in circa il 20% delle loro risposte.", "metrics": {"bleu_score": 69.41268297866861, "chrf_score": 78.07324614091777, "xcomet_score": 0.9821172952651978, "xcomet_qe_score": 0.969779372215271, "metricx_score": 2.0282931327819824, "metricx_qe_score": 2.262953758239746, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o si contraddicono il loro partner in circa il 10% del tempo.\"", "metrics": {"bleu_score": 63.19211739683715, "chrf_score": 89.3515586590675, "xcomet_score": 0.887811541557312, "xcomet_qe_score": 0.9146171808242798, "metricx_score": 6.000272750854492, "metricx_qe_score": 7.730927467346191, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Con il ritmo accelerato del miglioramento nel campo, molti di questi tassi di errore potrebbero vedere un decremento nei nuovi modelli pubblicati dal momento in cui è stata condotta la nostra valutazione.", "metrics": {"bleu_score": 55.28816837366797, "chrf_score": 74.15943927166254, "xcomet_score": 0.9847877025604248, "xcomet_qe_score": 0.9826752543449402, "metricx_score": 1.9039093255996704, "metricx_qe_score": 1.5924044847488403, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, ciò è solo un ulteriore motivo per perseguire metriche di valutazione attendibili e precise per confrontare i modelli.", "metrics": {"bleu_score": 49.99791131440528, "chrf_score": 75.15730503231067, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 1.0, "metricx_score": 0.9053553938865662, "metricx_qe_score": 0.8010983467102051, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Speriamo che ABC possa essere utilizzato da altri nel campo come un passo significativo in questa direzione.", "metrics": {"bleu_score": 64.59962562244407, "chrf_score": 78.9567652781196, "xcomet_score": 0.813239336013794, "xcomet_qe_score": 0.8107210397720337, "metricx_score": 5.294191837310791, "metricx_qe_score": 5.9125657081604, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E ci aspettiamo con ansia di vedere come l'Intelligenza Artificiale conversazionale avanzerà nei prossimi mesi e anni.", "metrics": {"bleu_score": 35.412968165085715, "chrf_score": 62.656899615542095, "xcomet_score": 0.9919694662094116, "xcomet_qe_score": 1.0, "metricx_score": 0.7883360385894775, "metricx_qe_score": 0.7726888656616211, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per aver guardato.\"", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 42.05175901287112, "xcomet_score": 0.4743751585483551, "xcomet_qe_score": 0.3119763731956482, "metricx_score": 0.7264621257781982, "metricx_qe_score": 0.6174004673957825, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, il mio nome è Kaio Yan e presenterò il nostro lavoro intitolato 'Quando la traduzione richiede un contesto?", "metrics": {"bleu_score": 19.80316235382626, "chrf_score": 44.93176456684072, "xcomet_score": 0.9168422222137451, "xcomet_qe_score": 0.9133904576301575, "metricx_score": 0.8457562923431396, "metricx_qe_score": 0.8315716981887817, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Una esplorazione multilingue guidata dai dati.", "metrics": {"bleu_score": 6.567274736060395, "chrf_score": 34.17629045296401, "xcomet_score": 0.9987257719039917, "xcomet_qe_score": 1.0, "metricx_score": 0.8164010643959045, "metricx_qe_score": 0.9339078664779663, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con Patrick Frenange, M.E. Liu, Andre F.D. Martin e Graham Mubig.\"", "metrics": {"bleu_score": 22.34017477427353, "chrf_score": 64.0703125968579, "xcomet_score": 0.6679866313934326, "xcomet_qe_score": 0.7592552900314331, "metricx_score": 4.866519451141357, "metricx_qe_score": 4.381053924560547, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Sì, molti traduzioni dipendono dal contesto.", "metrics": {"bleu_score": 51.697315395717055, "chrf_score": 86.32074525964752, "xcomet_score": 0.9470860958099365, "xcomet_qe_score": 0.9615002870559692, "metricx_score": 2.61822247505188, "metricx_qe_score": 0.34334757924079895, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, come tradurremmo \"more\" in questo enunciato?", "metrics": {"bleu_score": 48.326978309062184, "chrf_score": 75.16206718013049, "xcomet_score": 0.896592915058136, "xcomet_qe_score": 0.9003985524177551, "metricx_score": 5.740464210510254, "metricx_qe_score": 5.983259677886963, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Be', se il precedente enunciato era \"le cose potrebbero diventare pericolose se i ministri lo scoprissero, allora Moe si riferisce a un agente.", "metrics": {"bleu_score": 3.357048186278232, "chrf_score": 28.500434538162704, "xcomet_score": 0.6924625635147095, "xcomet_qe_score": 0.6860514879226685, "metricx_score": 7.748620986938477, "metricx_qe_score": 7.202370643615723, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma se il precedente enunciato era \"potrebbe essere qualcosa di serio, dottore? Allora Moe si riferisce a un segno", "metrics": {"bleu_score": 4.76689894351333, "chrf_score": 35.84476041277756, "xcomet_score": 0.655672013759613, "xcomet_qe_score": 0.6585165858268738, "metricx_score": 10.249247550964355, "metricx_qe_score": 10.364497184753418, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "di nascita. In base al contesto, il significato della parola cambia e quindi il suo traduzione cambia anch'essa.", "metrics": {"bleu_score": 32.88580454955831, "chrf_score": 67.83933919538151, "xcomet_score": 0.5937414765357971, "xcomet_qe_score": 0.44459933042526245, "metricx_score": 12.199737548828125, "metricx_qe_score": 12.362452507019043, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile.", "metrics": {"bleu_score": 40.02916772576474, "chrf_score": 67.70295188168572, "xcomet_score": 0.9814441204071045, "xcomet_qe_score": 0.936063826084137, "metricx_score": 1.508195400238037, "metricx_qe_score": 2.537161350250244, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inizialmente, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per i metri di corpus a livello di corpus come il BLEU catturare queste traduzioni.", "metrics": {"bleu_score": 55.17602572167799, "chrf_score": 73.45399239310662, "xcomet_score": 0.7237604260444641, "xcomet_qe_score": 0.585732102394104, "metricx_score": 8.757734298706055, "metricx_qe_score": 7.3577046394348145, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E alcuni hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma questi risorse supportano solo limitati tipi di traduzioni dipendenti dal contesto e limitati insiemi di lingue, poiché spesso si basano sulla conoscenza del dominio e sulla curazione umana.\"", "metrics": {"bleu_score": 41.36772848714183, "chrf_score": 76.62557231266015, "xcomet_score": 0.8524032831192017, "xcomet_qe_score": 0.8800822496414185, "metricx_score": 3.980677366256714, "metricx_qe_score": 2.8806490898132324, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo lavoro, cerchiamo di rispondere a due domande.", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 85.01706329334304, "xcomet_score": 0.9779288172721863, "xcomet_qe_score": 0.979812502861023, "metricx_score": 1.1793278455734253, "metricx_qe_score": 0.9144511222839355, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, quando richiede contesto la traduzione?", "metrics": {"bleu_score": 27.483906537871352, "chrf_score": 61.58637458242663, "xcomet_score": 0.9759023189544678, "xcomet_qe_score": 0.9684951901435852, "metricx_score": 0.3329353928565979, "metricx_qe_score": 0.29426848888397217, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E in secondo luogo, come si comportano i modelli in questi casi?\"", "metrics": {"bleu_score": 37.0304683381906, "chrf_score": 60.34044808591608, "xcomet_score": 0.9697200059890747, "xcomet_qe_score": 0.9553523063659668, "metricx_score": 0.9344900250434875, "metricx_qe_score": 1.053891897201538, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato a misurare in che misura una parola dipenda dal contesto di traduzione. \"Ecco il", "metrics": {"bleu_score": 39.42302221292177, "chrf_score": 71.17467417495085, "xcomet_score": 0.2203628420829773, "xcomet_qe_score": 0.37688809633255005, "metricx_score": 6.1940813064575195, "metricx_qe_score": 5.981578350067139, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "lavoro precedente in cui abbiamo introdotto CXMI come misura per l'utilizzo del contesto da parte dei modelli di traduzione automatica. E", "metrics": {"bleu_score": 56.333167591361615, "chrf_score": 82.40929554009803, "xcomet_score": 0.74410080909729, "xcomet_qe_score": 0.6657669544219971, "metricx_score": 4.703301906585693, "metricx_qe_score": 4.036463260650635, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ciò viene fatto misurando quanti informazioni il contesto C fornisce su Y, dati X.\" \"Posso pensare al CXMI come l'informazione ottenuta fornendo contesto al modello.\"", "metrics": {"bleu_score": 29.376773957288503, "chrf_score": 67.94903009187463, "xcomet_score": 0.8380953073501587, "xcomet_qe_score": 0.8175650238990784, "metricx_score": 4.482599258422852, "metricx_qe_score": 3.9598004817962646, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo lavoro, estendiamo CXMI a YCXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parole.", "metrics": {"bleu_score": 66.7619194068951, "chrf_score": 84.77738657772034, "xcomet_score": 0.7855209708213806, "xcomet_qe_score": 0.6615206003189087, "metricx_score": 5.133073806762695, "metricx_qe_score": 6.158278942108154, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo considerare le parole che hanno un alto P6MI come quelle che richiedono contesto per la traduzione.\"", "metrics": {"bleu_score": 34.14088641890569, "chrf_score": 57.88950942093128, "xcomet_score": 0.7455636858940125, "xcomet_qe_score": 0.724503219127655, "metricx_score": 6.21162223815918, "metricx_qe_score": 5.777502536773682, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con alto XMI per individuare pattern tra queste parole.", "metrics": {"bleu_score": 32.55964126200301, "chrf_score": 54.036743518035145, "xcomet_score": 0.7959107160568237, "xcomet_qe_score": 0.7813166975975037, "metricx_score": 9.951767921447754, "metricx_qe_score": 10.483527183532715, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E eseguiamo l'analisi sui registri dei discorsi TED che sono stati tradotti da inglese in 14 diverse lingue.", "metrics": {"bleu_score": 11.268706361337427, "chrf_score": 56.90925511760485, "xcomet_score": 0.8564589023590088, "xcomet_qe_score": 0.8512892723083496, "metricx_score": 4.943173885345459, "metricx_qe_score": 4.638720512390137, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo l'analisi a tre diversi livelli.", "metrics": {"bleu_score": 32.66828640925501, "chrf_score": 68.54937173767635, "xcomet_score": 0.9957479238510132, "xcomet_qe_score": 0.954949140548706, "metricx_score": 0.6078152060508728, "metricx_qe_score": 0.4355508089065552, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, esaminiamo le etichette di parte del discorso che hanno un alto valore di PCXMI. \"Ecco", "metrics": {"bleu_score": 34.82207361953904, "chrf_score": 62.30871693407711, "xcomet_score": 0.3139541745185852, "xcomet_qe_score": 0.2719828486442566, "metricx_score": 5.845770835876465, "metricx_qe_score": 4.974693775177002, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che ci consente di individuare, ad esempio, pronomi duali in arabo che hanno un alto livello di P6MI.", "metrics": {"bleu_score": 52.36451551896005, "chrf_score": 56.32153946552434, "xcomet_score": 0.5537872314453125, "xcomet_qe_score": 0.5415521860122681, "metricx_score": 8.529348373413086, "metricx_qe_score": 6.702648639678955, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò può essere spiegato perché l'inglese non ha pronomi duali, quindi si richiede contesto per determinare se un pronome è duale quando si traduce in arabo.\"", "metrics": {"bleu_score": 58.82974025017124, "chrf_score": 75.25021828211051, "xcomet_score": 0.9527696371078491, "xcomet_qe_score": 0.9539234042167664, "metricx_score": 1.0947874784469604, "metricx_qe_score": 1.3281362056732178, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E inoltre, troviamo che certi linguaggi richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Allora", "metrics": {"bleu_score": 14.307778226053154, "chrf_score": 44.545382104747915, "xcomet_score": 0.8911450505256653, "xcomet_qe_score": 0.9077533483505249, "metricx_score": 6.449102401733398, "metricx_qe_score": 3.890178680419922, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "esaminiamo gli elementi lessicale che hanno un p-sex-mi calcolato su tutti i suoi diversi avvenimenti. Ecco,", "metrics": {"bleu_score": 10.343956523537935, "chrf_score": 42.1666563950213, "xcomet_score": 0.19406284391880035, "xcomet_qe_score": 0.17880460619926453, "metricx_score": 17.333866119384766, "metricx_qe_score": 17.437923431396484, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "questo aiuta a identificare casi come questo, in cui in cinese, è necessario contesto per tradurre nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento.", "metrics": {"bleu_score": 54.372377506192976, "chrf_score": 81.43149227037702, "xcomet_score": 0.8870939016342163, "xcomet_qe_score": 0.8912436366081238, "metricx_score": 1.9484108686447144, "metricx_qe_score": 1.5597679615020752, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che troviamo che il contesto è supportato per tradurre nella forma di formalità corretta.\"", "metrics": {"bleu_score": 21.258844131063825, "chrf_score": 58.620317347740944, "xcomet_score": 0.45422711968421936, "xcomet_qe_score": 0.4972767233848572, "metricx_score": 12.481816291809082, "metricx_qe_score": 13.46087646484375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, infine, analizziamo i token individuali che hanno un alto P6MI.", "metrics": {"bleu_score": 11.633270842295033, "chrf_score": 41.856393216721585, "xcomet_score": 0.44688722491264343, "xcomet_qe_score": 0.36053410172462463, "metricx_score": 8.638635635375977, "metricx_qe_score": 8.607959747314453, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura del testo, come la risoluzione dell'ellissi.\"", "metrics": {"bleu_score": 61.005290072805145, "chrf_score": 81.55320883753984, "xcomet_score": 0.9102774262428284, "xcomet_qe_score": 0.9569921493530273, "metricx_score": 1.1919070482254028, "metricx_qe_score": 1.183971881866455, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione globale dei documenti.", "metrics": {"bleu_score": 12.856744437702964, "chrf_score": 55.503606579120046, "xcomet_score": 0.9475347995758057, "xcomet_qe_score": 0.9210405349731445, "metricx_score": 5.920629978179932, "metricx_qe_score": 6.4727911949157715, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per ognuno dei cinque fenomeni di discorso identificati, creiamo tagger per identificare automaticamente le parole che si riferiscono al fenomeno.", "metrics": {"bleu_score": 18.03074950737193, "chrf_score": 62.24331289960353, "xcomet_score": 0.9387407302856445, "xcomet_qe_score": 0.927358865737915, "metricx_score": 4.243168354034424, "metricx_qe_score": 5.795509338378906, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E chiamiamo il nostro tagger il multilingue consapevole del discorso o Muda tagger.", "metrics": {"bleu_score": 12.571192676522521, "chrf_score": 45.92363749737428, "xcomet_score": 0.6366739273071289, "xcomet_qe_score": 0.7217154502868652, "metricx_score": 9.356380462646484, "metricx_qe_score": 10.420214653015137, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Puoi notare inoltre che diversi linguaggi hanno diverse proporzioni di queste fenomeni discorsivi.\"", "metrics": {"bleu_score": 3.9779457777453793, "chrf_score": 53.473043813187005, "xcomet_score": 0.7681493759155273, "xcomet_qe_score": 0.768839955329895, "metricx_score": 6.694431304931641, "metricx_qe_score": 6.585786819458008, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, applichiamo il MudaTaggle immettendo il taglio sul corpus parallelo che vogliamo utilizzare per l'evaluazione. E applichiamo i nostri metriche di traduzione scelta sulle esempi contestuali identificati dal MudaTaggle. \"E finalmente", "metrics": {"bleu_score": 17.770538650638482, "chrf_score": 57.75453299139549, "xcomet_score": 0.24611416459083557, "xcomet_qe_score": 0.2855689227581024, "metricx_score": 18.302194595336914, "metricx_qe_score": 16.392454147338867, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": ", utilizziamo il nostro benchmark, insieme ad altri parametri, per valutare diversi modelli sul livello del documento di traduzione automatica.\" \"Iniziamo", "metrics": {"bleu_score": 21.62050865049026, "chrf_score": 67.04966932748947, "xcomet_score": 0.24491196870803833, "xcomet_qe_score": 0.21773755550384521, "metricx_score": 14.991875648498535, "metricx_qe_score": 13.048550605773926, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "dicendo che, utilizzando metriche a livello di corpus, scopriamo che i modelli agnostici rispetto al contesto hanno il miglior rendimento per il blu.\"", "metrics": {"bleu_score": 23.924844920154275, "chrf_score": 49.600496604098474, "xcomet_score": 0.6219674944877625, "xcomet_qe_score": 0.6691768169403076, "metricx_score": 5.475771903991699, "metricx_qe_score": 5.058070659637451, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma allora se utilizziamo Comet, i modelli con consapevolezza del contesto prestano meglio.", "metrics": {"bleu_score": 13.834368456410946, "chrf_score": 43.051101214341855, "xcomet_score": 0.9087114334106445, "xcomet_qe_score": 0.9273822903633118, "metricx_score": 2.6320137977600098, "metricx_qe_score": 1.9972728490829468, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E se utilizziamo il misura F, allora i modelli con o senza contesto hanno prestazioni comparabili.", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 72.38656086008673, "xcomet_score": 0.8985797166824341, "xcomet_qe_score": 0.8843545317649841, "metricx_score": 5.446518898010254, "metricx_qe_score": 5.381560325622559, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco che questo esempio dimostra che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus.\"", "metrics": {"bleu_score": 63.44813391798399, "chrf_score": 81.00543546201179, "xcomet_score": 0.8830060958862305, "xcomet_qe_score": 0.8815826177597046, "metricx_score": 2.241986036300659, "metricx_qe_score": 2.6409778594970703, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Mooda per valutare i modelli e scopriamo che i modelli a contesto sono significativamente più precisi rispetto ai modelli che non utilizzano il contesto per fenomeni discorsivi specifici, come la formalità e la coesione lessicale.", "metrics": {"bleu_score": 41.729050796190364, "chrf_score": 70.6917727181942, "xcomet_score": 0.7168633937835693, "xcomet_qe_score": 0.7329427003860474, "metricx_score": 5.405783176422119, "metricx_qe_score": 4.997747421264648, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di quelli che non utilizzano contesto su fenomeni come ellissi, piante perenni e forma verbale. Quindi,", "metrics": {"bleu_score": 36.032121811571, "chrf_score": 71.62876825837948, "xcomet_score": 0.4051167666912079, "xcomet_qe_score": 0.4321402311325073, "metricx_score": 14.074747085571289, "metricx_qe_score": 17.43790626525879, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "questo suggerisce dove dovremmo vedere più progressi per la trasformazione a livello di documento.", "metrics": {"bleu_score": 46.41590740722758, "chrf_score": 77.8293150168762, "xcomet_score": 0.8864290714263916, "xcomet_qe_score": 0.9366669654846191, "metricx_score": 5.210899829864502, "metricx_qe_score": 4.375415802001953, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Anche noi confrontiamo sistemi commerciali diversi e i nostri benchmark dimostrano che D-Bel è solitamente più preciso del traduttore Google per traduzioni a livello di documento.", "metrics": {"bleu_score": 18.910054845801366, "chrf_score": 53.26994883066416, "xcomet_score": 0.7670080661773682, "xcomet_qe_score": 0.7612900137901306, "metricx_score": 7.568403720855713, "metricx_qe_score": 7.634244918823242, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Eseguiamo un'analisi basata sui dati su 14 coppie linguistiche per identificare quando le traduzioni richiedono contesto.\" Ecco, utilizziamo i nostri risultati per costruire un benchmark per la traduzione a documento, che ci aiuti a identificare quali modelli di fenomeni di disgiunzione possono gestire bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a documento.", "metrics": {"bleu_score": 45.339749875535986, "chrf_score": 67.70766426033516, "xcomet_score": 0.4261443316936493, "xcomet_qe_score": 0.4646497964859009, "metricx_score": 9.666913032531738, "metricx_qe_score": 8.889832496643066, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la tua attenzione.", "metrics": {"bleu_score": 26.269098944241588, "chrf_score": 83.12322170018103, "xcomet_score": 0.9465204477310181, "xcomet_qe_score": 0.9392974972724915, "metricx_score": 1.0059442520141602, "metricx_qe_score": 1.1465978622436523, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ci vediamo a Toronto.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9708521366119385, "xcomet_qe_score": 0.9111494421958923, "metricx_score": 0.13234442472457886, "metricx_qe_score": 0.361991286277771, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, sono Yannis Lavraque e vi presento i nostri lavori su Dr. Berth, un modello robusto in francese per il dominio biomedico e clinico.\" \"In", "metrics": {"bleu_score": 9.13124680674188, "chrf_score": 39.873977430311456, "xcomet_score": 0.46845123171806335, "xcomet_qe_score": 0.5311740636825562, "metricx_score": 6.546468734741211, "metricx_qe_score": 6.100207328796387, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "questa presentazione, discuteremo di una lingua modellizzata in sanità.", "metrics": {"bleu_score": 12.630268049376259, "chrf_score": 42.10483125241646, "xcomet_score": 0.852512001991272, "xcomet_qe_score": 0.8372012376785278, "metricx_score": 8.341994285583496, "metricx_qe_score": 7.269121170043945, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, presenteremo la principale contribuzione del nostro articolo.\"", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 65.30307327377928, "xcomet_score": 0.9647925496101379, "xcomet_qe_score": 0.9836541414260864, "metricx_score": 2.15653657913208, "metricx_qe_score": 1.738447666168213, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in francese denominato Dr. Berth, basato su Roberta, e addestrato su NACCHOS, che è un dataset di dati di base medici da fonti web.", "metrics": {"bleu_score": 20.102108799660943, "chrf_score": 56.9097390698169, "xcomet_score": 0.6467962265014648, "xcomet_qe_score": 0.6311755776405334, "metricx_score": 4.559991359710693, "metricx_qe_score": 4.688643932342529, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Avevamo anche introdotto una comparazione di modelli con diverse retroazioni e fonti di dati.", "metrics": {"bleu_score": 29.657123965181544, "chrf_score": 45.16819298265912, "xcomet_score": 0.6395035982131958, "xcomet_qe_score": 0.6713519096374512, "metricx_score": 6.306027412414551, "metricx_qe_score": 4.572447299957275, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Presenteremo quindi i nostri risultati su 11 biomedici e test clinici in francese.", "metrics": {"bleu_score": 29.5560575580374, "chrf_score": 55.21511838451148, "xcomet_score": 0.680192232131958, "xcomet_qe_score": 0.6827923059463501, "metricx_score": 8.507697105407715, "metricx_qe_score": 8.176095008850098, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E infine, concludiamo gli esperimenti e vi forniamo ulteriori informazioni su come accedere ai modelli.", "metrics": {"bleu_score": 13.32358437599213, "chrf_score": 57.960965058848835, "xcomet_score": 0.9018927812576294, "xcomet_qe_score": 0.9058096408843994, "metricx_score": 0.46639227867126465, "metricx_qe_score": 0.501510500907898, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dal suo rilascio nel 2018, Bert è stata l'approccio più efficace per la procedura di linguaggio naturale. Ha offerto un grande guadagno di prestazioni rispetto a strategie storiche e metodi contextualizzati, come \"voir-to-vec\" o \"voir-envoi\".", "metrics": {"bleu_score": 7.2538798389035355, "chrf_score": 43.55412552527165, "xcomet_score": 0.2739686965942383, "xcomet_qe_score": 0.30244991183280945, "metricx_score": 14.016074180603027, "metricx_qe_score": 17.25327491760254, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dal momento in cui questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e a diversi campi, come la biomedicina con Père Medbert e Biobird, e in clinica con Clinique Albert, ma soprattutto in inglese.", "metrics": {"bleu_score": 37.155983875438096, "chrf_score": 59.87180397104383, "xcomet_score": 0.1694822758436203, "xcomet_qe_score": 0.1531364917755127, "metricx_score": 13.047233581542969, "metricx_qe_score": 12.637022018432617, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Modelli specializzati per altre lingue sono rari e spesso basati su addestramento continuo a causa della scarsità di dati in ambito.\"", "metrics": {"bleu_score": 23.352648136133084, "chrf_score": 56.092581699853575, "xcomet_score": 0.8803110122680664, "xcomet_qe_score": 0.848682165145874, "metricx_score": 4.220859050750732, "metricx_qe_score": 3.978099822998047, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il francese non aveva alcun modello open source per la bio-medicina fino ad", "metrics": {"bleu_score": 20.79617953215787, "chrf_score": 53.600400852146834, "xcomet_score": 0.44741612672805786, "xcomet_qe_score": 0.3160747289657593, "metricx_score": 6.540066719055176, "metricx_qe_score": 3.9975671768188477, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ora. \"Ci chiediamo una domanda sulla struttura dati più adatta per una gamma ampia di utilizzi. E quei dati attuali sono una buona sostituzione per i dati clinici.\"", "metrics": {"bleu_score": 7.938335233234022, "chrf_score": 43.94685149370871, "xcomet_score": 0.484620600938797, "xcomet_qe_score": 0.43772783875465393, "metricx_score": 8.820430755615234, "metricx_qe_score": 7.1443634033203125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello di Schubert, basato su dati anonimizzati ottenuti dall'ospedale non generazionale della nostra casa.", "metrics": {"bleu_score": 31.963619792524472, "chrf_score": 60.20547533011692, "xcomet_score": 0.23315586149692535, "xcomet_qe_score": 0.2488449364900589, "metricx_score": 12.326717376708984, "metricx_qe_score": 14.236639022827148, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dopo di che ci chiediamo quanti dati dobbiamo addestrare un modello specializzato sui dati francesi?", "metrics": {"bleu_score": 35.92355495564629, "chrf_score": 60.98096893207647, "xcomet_score": 0.8485000133514404, "xcomet_qe_score": 0.8483026623725891, "metricx_score": 7.018881797790527, "metricx_qe_score": 5.861248016357422, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "È per Gb a Gb o più?", "metrics": {"bleu_score": 18.141207173155518, "chrf_score": 13.15384424584703, "xcomet_score": 0.43820616602897644, "xcomet_qe_score": 0.3785688877105713, "metricx_score": 8.4843111038208, "metricx_qe_score": 8.526762008666992, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero. Una prima versione di Dr. Bert con 7 GB di natura. Una seconda versione con 4 GB di natura. La prima versione di Schubert, che è un modello clinico, ha 4 GB di note cliniche. E l'ultima versione di Schubert con un mix di 4 GB di natura e 4 GB di note cliniche.", "metrics": {"bleu_score": 43.741432014148515, "chrf_score": 65.20490777157346, "xcomet_score": 0.2998765707015991, "xcomet_qe_score": 0.377389132976532, "metricx_score": 11.083916664123535, "metricx_qe_score": 8.93464183807373, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In aggiunta a questo confronto, abbiamo introdotto tre treni di modelli di pre-addestramento per analizzare l'impatto della strategia di pre-addestramento.", "metrics": {"bleu_score": 59.415598205628065, "chrf_score": 79.48082074329771, "xcomet_score": 0.7288535237312317, "xcomet_qe_score": 0.7225107550621033, "metricx_score": 7.011524200439453, "metricx_qe_score": 7.163256645202637, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Una base su peso di camembert e un treno su un set di 4 GB di natura.", "metrics": {"bleu_score": 35.870004213152995, "chrf_score": 39.40567100780212, "xcomet_score": 0.15886658430099487, "xcomet_qe_score": 0.1461220532655716, "metricx_score": 20.85482406616211, "metricx_qe_score": 19.06903839111328, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Un'altra base anche su camembert, ma trainata su 4 GB di clignons e l'altra.\" E infine, un modello basato su bio-medico inglese, Bermond Bert, e addestrato su un insieme di 4 gigabyte di trascrittore.", "metrics": {"bleu_score": 11.465402610048526, "chrf_score": 41.82816285852661, "xcomet_score": 0.16184091567993164, "xcomet_qe_score": 0.135274738073349, "metricx_score": 21.547916412353516, "metricx_qe_score": 20.78598403930664, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo sette modelli.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.09792831540107727, "metricx_qe_score": 0.14044679701328278, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliamo sei modelli, dalle pubblicazioni e dal settore privato, per esaminare le nozioni di riconoscimento, classificazione, viaggio, sfide e responsabilità. \"Ecco il modello", "metrics": {"bleu_score": 6.193765991218997, "chrf_score": 40.8528401141718, "xcomet_score": 0.20406192541122437, "xcomet_qe_score": 0.2062351554632187, "metricx_score": 23.71575927734375, "metricx_qe_score": 23.213014602661133, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "è confrontato con il modello 6.9 che comprende Camembert Oscar 138GB, Camembert Oscar 4GB, Camembert CCNet 4GB, Pummet Belt, BioBert e ClinicalBert.\"", "metrics": {"bleu_score": 1.9468237977634792, "chrf_score": 35.98166921314264, "xcomet_score": 0.22723020613193512, "xcomet_qe_score": 0.23877038061618805, "metricx_score": 13.266294479370117, "metricx_qe_score": 15.086468696594238, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione dei modelli che si sviluppano meglio sul compito con dati di natura identica a quelli su cui il modello è stato addestrato.", "metrics": {"bleu_score": 18.19164816443771, "chrf_score": 51.21985775519894, "xcomet_score": 0.6932753920555115, "xcomet_qe_score": 0.5369342565536499, "metricx_score": 8.490506172180176, "metricx_qe_score": 10.941636085510254, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere i dati da fonti eterogenee appaiono essere più versatili.", "metrics": {"bleu_score": 30.82331946100714, "chrf_score": 66.47298681819726, "xcomet_score": 0.7022712230682373, "xcomet_qe_score": 0.7020459175109863, "metricx_score": 10.48035717010498, "metricx_qe_score": 11.199407577514648, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Notiamo inoltre che l'utilizzo di più dati si traduce in una prestazione migliore.", "metrics": {"bleu_score": 36.6192636299943, "chrf_score": 68.1804816752112, "xcomet_score": 0.9977449178695679, "xcomet_qe_score": 1.0, "metricx_score": 0.6578645706176758, "metricx_qe_score": 0.8298275470733643, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In generale, il ritorno da zero sembrava ottenere un miglioramento nella prestazione su quasi tutti i compiti.", "metrics": {"bleu_score": 6.585833693600902, "chrf_score": 41.0462143692988, "xcomet_score": 0.7518640756607056, "xcomet_qe_score": 0.8340592384338379, "metricx_score": 6.955493927001953, "metricx_qe_score": 7.479611396789551, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La nostra esperienza in condizioni di vincolo e di previsione, utilizzando il peso e il token di Pomet Bird, trainata su un soggetto di 4 GB di natura, ha prodotto un risultato comparabile a quello che abbiamo ottenuto con Dr. Bert, 4 GB di scratch.", "metrics": {"bleu_score": 7.296554923386447, "chrf_score": 36.49569668279233, "xcomet_score": 0.14414292573928833, "xcomet_qe_score": 0.2528741657733917, "metricx_score": 19.39755630493164, "metricx_qe_score": 17.586469650268555, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Questo non è il caso per il modello basato sui pesi di Camembert e sul tokenizzatore, che presenta problemi di stabilità.\"", "metrics": {"bleu_score": 29.486296661855093, "chrf_score": 67.49665611518456, "xcomet_score": 0.8926504850387573, "xcomet_qe_score": 0.8839070796966553, "metricx_score": 2.5636720657348633, "metricx_qe_score": 2.4808573722839355, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il nostro sistema presenta prestazioni migliori in nove delle 11 attività a valle e supera globalmente i risultati del modello generico, Camembert.", "metrics": {"bleu_score": 43.076460146742434, "chrf_score": 71.64183193981152, "xcomet_score": 0.9056504964828491, "xcomet_qe_score": 0.882808268070221, "metricx_score": 2.318028450012207, "metricx_qe_score": 1.651084303855896, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Anche osservammo che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non scalano bene.", "metrics": {"bleu_score": 43.15229271655711, "chrf_score": 72.07838845646376, "xcomet_score": 0.21983885765075684, "xcomet_qe_score": 0.12345266342163086, "metricx_score": 6.201968193054199, "metricx_qe_score": 6.6217827796936035, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli preaddestrati ottenuti da NATURES sono disponibili gratuitamente su UginFace e tutti i script di training sono nel nostro repository GitHub.", "metrics": {"bleu_score": 21.02400777334129, "chrf_score": 61.53798933138847, "xcomet_score": 0.5423281192779541, "xcomet_qe_score": 0.6184406280517578, "metricx_score": 8.111763000488281, "metricx_qe_score": 8.586861610412598, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Grazie per la presentazione. Ci aspettiamo di prendere azione alla sessione dei poster a Toronto.\"", "metrics": {"bleu_score": 18.107197870881738, "chrf_score": 54.31604220942794, "xcomet_score": 0.6384133100509644, "xcomet_qe_score": 0.752576470375061, "metricx_score": 7.1647772789001465, "metricx_qe_score": 7.491765022277832, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao,", "metrics": {"bleu_score": 0.0, "chrf_score": 54.333333333333336, "xcomet_score": 0.9947192668914795, "xcomet_qe_score": 0.9683444499969482, "metricx_score": 0.1089133620262146, "metricx_qe_score": 0.09066226333379745, "linguapy_score": [1, "MAORI"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "mi chiamo Mathias Landemann e oggi ti darò un'introduzione al nostro studio sul generalizzazione comprensiva senza alberi utilizzando marcature multi-set e permutazioni", "metrics": {"bleu_score": 3.9325763273746293, "chrf_score": 40.8645368772324, "xcomet_score": 0.458187460899353, "xcomet_qe_score": 0.6118568778038025, "metricx_score": 7.754951000213623, "metricx_qe_score": 8.299428939819336, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "latenti. Ecco il mio lavoro in collaborazione con i miei consiglieri, Alessandro Kodler e Yvon Titov.", "metrics": {"bleu_score": 10.307386760781874, "chrf_score": 34.37692803743135, "xcomet_score": 0.281882643699646, "xcomet_qe_score": 0.20969027280807495, "metricx_score": 12.756657600402832, "metricx_qe_score": 13.788458824157715, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La composizione generale può essere intesa come l'abilità del learner di affrontare la ricorsione più profonda e composizioni non viste durante l'addestramento.", "metrics": {"bleu_score": 15.308214445833272, "chrf_score": 54.69325819741682, "xcomet_score": 0.5758094787597656, "xcomet_qe_score": 0.5870988368988037, "metricx_score": 11.952768325805664, "metricx_qe_score": 11.34358024597168, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto della parsing semantica, verificare la generalizzazione composizionale potrebbe assomigliare a questo.", "metrics": {"bleu_score": 18.20026146457161, "chrf_score": 70.4265154083252, "xcomet_score": 0.8748271465301514, "xcomet_qe_score": 0.8733842968940735, "metricx_score": 4.915665626525879, "metricx_qe_score": 5.82130765914917, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Come di consueto, abbiamo un insieme di utterance di training,", "metrics": {"bleu_score": 14.334389232407714, "chrf_score": 30.19679626812509, "xcomet_score": 0.680018424987793, "xcomet_qe_score": 0.585268497467041, "metricx_score": 10.684523582458496, "metricx_qe_score": 9.560745239257812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in questo caso la ragazza si addormentò", "metrics": {"bleu_score": 8.208611846457007, "chrf_score": 51.33733764907388, "xcomet_score": 0.9850252270698547, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 3.2778232097625732, "metricx_qe_score": 1.807188868522644, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e Maria sapeva che la ragazza si addormentò.", "metrics": {"bleu_score": 28.24099048856542, "chrf_score": 59.46333301739155, "xcomet_score": 0.9181312322616577, "xcomet_qe_score": 0.952385663986206, "metricx_score": 4.502540588378906, "metricx_qe_score": 1.822352647781372, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quegli enunciati sono associati a forme logiche che rappresentano aspetti fondamentali del loro significato.", "metrics": {"bleu_score": 71.66258375282708, "chrf_score": 76.15710414246144, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.66047602891922, "metricx_qe_score": 0.3986392021179199, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In contrasto rispetto all'evaluazione tradizionale dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione e contiene forme non sinologiche strutturali.", "metrics": {"bleu_score": 37.81780992167707, "chrf_score": 59.132520295990496, "xcomet_score": 0.8296074867248535, "xcomet_qe_score": 0.8398371934890747, "metricx_score": 3.673027276992798, "metricx_qe_score": 4.237044811248779, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione più superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda.", "metrics": {"bleu_score": 72.76252483365604, "chrf_score": 87.42956149728045, "xcomet_score": 0.9635977745056152, "xcomet_qe_score": 0.9491510391235352, "metricx_score": 1.9751768112182617, "metricx_qe_score": 2.6756930351257324, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il modello di sequenza-naif ha difficoltà con questo tipo di generalizzazione fuori-distribuzione e spesso produce output che sono distaccati dall'input.\"", "metrics": {"bleu_score": 28.690668742892225, "chrf_score": 73.72414025037773, "xcomet_score": 0.34747642278671265, "xcomet_qe_score": 0.36799898743629456, "metricx_score": 10.574542045593262, "metricx_qe_score": 14.42613697052002, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, essi spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelle codificate a colori nell'esempio.", "metrics": {"bleu_score": 79.4483720649497, "chrf_score": 91.0090338591918, "xcomet_score": 0.980705738067627, "xcomet_qe_score": 0.9888426065444946, "metricx_score": 2.381135940551758, "metricx_qe_score": 3.7435848712921143, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli.", "metrics": {"bleu_score": 80.91067115702207, "chrf_score": 86.4796743710817, "xcomet_score": 0.9715076684951782, "xcomet_qe_score": 1.0, "metricx_score": 0.50239497423172, "metricx_qe_score": 0.5259661674499512, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I'm ready to help! Here is the translation: \"Ilberi sono destinati a catturare il processo compositivo che collega le enunciazioni alle forme logiche.\"", "metrics": {"bleu_score": 17.219167420768077, "chrf_score": 55.83965627181969, "xcomet_score": 0.22427181899547577, "xcomet_qe_score": 0.16167394816875458, "metricx_score": 15.269838333129883, "metricx_qe_score": 16.09642791748047, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi sono di solito non dati e devono essere ottenuti in qualche modo.", "metrics": {"bleu_score": 77.33873725716454, "chrf_score": 91.69078971215919, "xcomet_score": 0.9100618362426758, "xcomet_qe_score": 0.9264013767242432, "metricx_score": 5.635979175567627, "metricx_qe_score": 6.555593967437744, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il processo può essere complesso e a volte richiede un costo computazionale notevole.", "metrics": {"bleu_score": 4.138190266665692, "chrf_score": 36.216526800668994, "xcomet_score": 0.9980466365814209, "xcomet_qe_score": 1.0, "metricx_score": 0.44947028160095215, "metricx_qe_score": 0.41483840346336365, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tipicamente, ciò comporta una pre-elaborazione formale delle forme logiche, ad esempio per gestire simboli di variabili.", "metrics": {"bleu_score": 32.24121115238373, "chrf_score": 64.1971582468588, "xcomet_score": 0.8783022165298462, "xcomet_qe_score": 0.9816757440567017, "metricx_score": 1.6826316118240356, "metricx_qe_score": 1.636770248413086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ottener gli alberi può anche coinvolgere procedure di induzione grammaticale specializzate.", "metrics": {"bleu_score": 17.827531042796263, "chrf_score": 63.769815846373334, "xcomet_score": 0.8909081220626831, "xcomet_qe_score": 0.7617654800415039, "metricx_score": 5.204490661621094, "metricx_qe_score": 7.947319507598877, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questo studio non utilizziamo alberi e introduciamo un modello di sequenza-sequenza neuronale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output.", "metrics": {"bleu_score": 61.4288356850889, "chrf_score": 76.53756788077315, "xcomet_score": 0.8490446209907532, "xcomet_qe_score": 0.8558291792869568, "metricx_score": 3.253484010696411, "metricx_qe_score": 3.824410915374756, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, mostriamo una generalizzazione forte alla recursione più profonda senza appoggiarsi sugli alberi.", "metrics": {"bleu_score": 43.7596876206843, "chrf_score": 71.22964256791815, "xcomet_score": 0.7263379096984863, "xcomet_qe_score": 0.7662243247032166, "metricx_score": 5.868103504180908, "metricx_qe_score": 5.09017276763916, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede l'output dal input in due passaggi.", "metrics": {"bleu_score": 58.77283725105324, "chrf_score": 92.16711230318589, "xcomet_score": 0.9943705797195435, "xcomet_qe_score": 0.9754312038421631, "metricx_score": 4.865192413330078, "metricx_qe_score": 7.740825176239014, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Primo, assegniamo a ogni token di input un insieme non ordinato di token che appariranno nell'output.", "metrics": {"bleu_score": 30.928520903947533, "chrf_score": 54.896256098445086, "xcomet_score": 0.9759365320205688, "xcomet_qe_score": 0.9633288383483887, "metricx_score": 1.859171986579895, "metricx_qe_score": 3.4490103721618652, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati.", "metrics": {"bleu_score": 57.56799653136483, "chrf_score": 78.14592073217987, "xcomet_score": 0.9983038902282715, "xcomet_qe_score": 0.9964042901992798, "metricx_score": 1.0782268047332764, "metricx_qe_score": 1.6336902379989624, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il mio obiettivo è quello di utilizzare un altro modello per prevedere una permutazione per metterli nel giusto ordine.", "metrics": {"bleu_score": 30.23266713085593, "chrf_score": 56.70713501073975, "xcomet_score": 0.7389797568321228, "xcomet_qe_score": 0.6485788822174072, "metricx_score": 5.389820575714111, "metricx_qe_score": 6.323549747467041, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Introduciamo un nuovo metodo per prevedere una permutazione che non impone alcun vincolo rigido sulle possibili permutazioni.", "metrics": {"bleu_score": 44.47608928410893, "chrf_score": 81.94772277054943, "xcomet_score": 0.9648208618164062, "xcomet_qe_score": 0.966749906539917, "metricx_score": 0.9902948141098022, "metricx_qe_score": 1.0194106101989746, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò rende la nostra approccio molto flessibile e espressivo.\"", "metrics": {"bleu_score": 9.425159511373677, "chrf_score": 52.21693916835197, "xcomet_score": 0.7510678768157959, "xcomet_qe_score": 0.7786017656326294, "metricx_score": 4.866093158721924, "metricx_qe_score": 4.521584510803223, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro modello di permutazione funziona in modo approssimativo come segue. Ecco la traduzione:", "metrics": {"bleu_score": 25.748661016289674, "chrf_score": 53.72769714698572, "xcomet_score": 0.7965852618217468, "xcomet_qe_score": 0.8044711947441101, "metricx_score": 3.801891326904297, "metricx_qe_score": 3.013084650039673, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Andiamo da sinistra a destra sul output e determiniamo quale token di multinsieme mettere in ogni posizione.", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 85.21064533941967, "xcomet_score": 0.8027619123458862, "xcomet_qe_score": 0.7475045919418335, "metricx_score": 3.048792600631714, "metricx_qe_score": 3.256927967071533, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso.", "metrics": {"bleu_score": 64.59139467348548, "chrf_score": 90.52287036678844, "xcomet_score": 0.9752577543258667, "xcomet_qe_score": 0.9359469413757324, "metricx_score": 3.0383830070495605, "metricx_qe_score": 5.139426231384277, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Poi saltiamo al prossimo token multi-set per determinare il secondo token dell'output.", "metrics": {"bleu_score": 30.115405721522926, "chrf_score": 61.61345898559789, "xcomet_score": 0.9440343976020813, "xcomet_qe_score": 0.9477543234825134, "metricx_score": 5.960503101348877, "metricx_qe_score": 7.4670305252075195, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token dell'output in modo simile saltando a un token multi-set.", "metrics": {"bleu_score": 51.81294220614274, "chrf_score": 82.8315789103387, "xcomet_score": 0.9261156320571899, "xcomet_qe_score": 0.9094170928001404, "metricx_score": 4.68512487411499, "metricx_qe_score": 7.2272562980651855, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Proseguiamo questo processo. Fino a quando non sono stati visitati esattamente una volta tutti i token del primo stadio.", "metrics": {"bleu_score": 15.265280945577954, "chrf_score": 65.93697928708654, "xcomet_score": 0.8758492469787598, "xcomet_qe_score": 0.916659951210022, "metricx_score": 3.608030080795288, "metricx_qe_score": 3.5823075771331787, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un assaggio dei risultati sperimentali, qui presentiamo un confronto con altri modelli senza alberi sul benchmark CONG.", "metrics": {"bleu_score": 45.78309703643465, "chrf_score": 67.57036030814776, "xcomet_score": 0.7264076471328735, "xcomet_qe_score": 0.7368075847625732, "metricx_score": 6.292693614959717, "metricx_qe_score": 6.289046764373779, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro modello supera gli altri per una larga margine nella generalizzazione all'iterazione più profonda.", "metrics": {"bleu_score": 36.36931895524535, "chrf_score": 70.75482373912098, "xcomet_score": 0.8894940614700317, "xcomet_qe_score": 0.9083353877067566, "metricx_score": 5.644793510437012, "metricx_qe_score": 5.515953063964844, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale restano molto sfidanti, tuttavia.", "metrics": {"bleu_score": 44.833867003844595, "chrf_score": 70.40803494101904, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 3.9894156455993652, "metricx_qe_score": 4.345864295959473, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In nostro studio, risolviamo due sfide tecnologiche interessanti.\"", "metrics": {"bleu_score": 10.600313379512592, "chrf_score": 61.333070976348594, "xcomet_score": 0.914821207523346, "xcomet_qe_score": 0.6330655813217163, "metricx_score": 2.4488894939422607, "metricx_qe_score": 1.4524680376052856, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, l'allineamento tra input e output non è dato nella training data.", "metrics": {"bleu_score": 49.47328597494976, "chrf_score": 61.426676583259784, "xcomet_score": 0.8198185563087463, "xcomet_qe_score": 0.8302178382873535, "metricx_score": 4.558048725128174, "metricx_qe_score": 5.6013617515563965, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Di conseguenza, per un token dato, non sappiamo da quale multicella è venuto, il che rappresenta un problema per l'addestramento.", "metrics": {"bleu_score": 40.57473284975201, "chrf_score": 74.92196345332606, "xcomet_score": 0.7912272214889526, "xcomet_qe_score": 0.765355110168457, "metricx_score": 5.104490756988525, "metricx_qe_score": 4.454535961151123, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono diverse permutazioni che sono compatibili con i dati, ma la corretta linguistica è latente.", "metrics": {"bleu_score": 40.8606673320728, "chrf_score": 65.41731788170435, "xcomet_score": 0.9456345438957214, "xcomet_qe_score": 0.9383721351623535, "metricx_score": 3.1801671981811523, "metricx_qe_score": 4.167038917541504, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato questo problema inducendo l'allineamento come parte del training. Il nostro metodo", "metrics": {"bleu_score": 36.362270465000705, "chrf_score": 73.36395224099034, "xcomet_score": 0.6297295689582825, "xcomet_qe_score": 0.7117149829864502, "metricx_score": 6.667501449584961, "metricx_qe_score": 7.757981777191162, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "di permutazione è molto flessibile, ma presenta il problema di trovare la permutazione con punteggio più alto è NP-difficile.", "metrics": {"bleu_score": 32.34183729868669, "chrf_score": 59.61828670850878, "xcomet_score": 0.7011520862579346, "xcomet_qe_score": 0.6189066171646118, "metricx_score": 7.922598838806152, "metricx_qe_score": 9.30161190032959, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciò è legato al problema del venditore itinerante. Noi", "metrics": {"bleu_score": 14.530346490115708, "chrf_score": 34.00861466981094, "xcomet_score": 0.37418901920318604, "xcomet_qe_score": 0.6630685329437256, "metricx_score": 7.488009929656982, "metricx_qe_score": 1.2168123722076416, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "approssimiamo ciò con una rilassazione GPU-friendly continua che ci consente anche di propagare indietro attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico.", "metrics": {"bleu_score": 13.85420272734473, "chrf_score": 57.84410941586613, "xcomet_score": 0.6442044973373413, "xcomet_qe_score": 0.616553008556366, "metricx_score": 7.412946701049805, "metricx_qe_score": 7.152130603790283, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate imparare di più sui nostri esperimenti e su come affrontiamo questi sfide, vi invitiamo a consultare il nostro articolo o a recarvi presso la nostra posta.", "metrics": {"bleu_score": 33.66496864175109, "chrf_score": 63.68728043736541, "xcomet_score": 0.7773787975311279, "xcomet_qe_score": 0.7571402788162231, "metricx_score": 6.133006572723389, "metricx_qe_score": 5.9430251121521, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti, sono Akshita e oggi mio coautore Martin e noi presentiamo il nostro lavoro, il test KITMAS, che valuta l'integrazione delle conoscenze da fonti multiple.", "metrics": {"bleu_score": 25.089892198300436, "chrf_score": 54.27587834629447, "xcomet_score": 0.777724027633667, "xcomet_qe_score": 0.7788906097412109, "metricx_score": 2.8045737743377686, "metricx_qe_score": 2.390371799468994, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è una collaborazione tra l'Università McGill, Miele e Microsoft Research.\"", "metrics": {"bleu_score": 50.389204852596336, "chrf_score": 80.6665574558637, "xcomet_score": 0.8589340448379517, "xcomet_qe_score": 0.7977336049079895, "metricx_score": 5.270492076873779, "metricx_qe_score": 4.646773815155029, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"I modelli di comprensione linguistica nazionale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita mediante un pre-addestramento e la conoscenza fornita nel tempo di inferenza.\" \"Il recente", "metrics": {"bleu_score": 46.622780689664765, "chrf_score": 70.89977026310298, "xcomet_score": 0.17970220744609833, "xcomet_qe_score": 0.15139394998550415, "metricx_score": 16.44140625, "metricx_qe_score": 17.018829345703125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "lavoro su compiti come risposta a domande mostra che i modelli possono utilizzare conoscenze pre-addestrate per risolvere il compito.\"", "metrics": {"bleu_score": 18.97874833835006, "chrf_score": 57.507244405493985, "xcomet_score": 0.7493146657943726, "xcomet_qe_score": 0.7551414966583252, "metricx_score": 5.533859729766846, "metricx_qe_score": 6.445676803588867, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale richiede spesso conoscenze che sono anche fornite al momento dell'inferenza.", "metrics": {"bleu_score": 72.01131708570058, "chrf_score": 88.93690618837735, "xcomet_score": 0.9903980493545532, "xcomet_qe_score": 0.9973443746566772, "metricx_score": 0.8833167552947998, "metricx_qe_score": 0.790955662727356, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Esempio, nel periodo in cui John vide il presidente eletto da poco in TV.", "metrics": {"bleu_score": 7.727578693391118, "chrf_score": 45.717497358268204, "xcomet_score": 0.7358848452568054, "xcomet_qe_score": 0.714551568031311, "metricx_score": 9.924543380737305, "metricx_qe_score": 9.546485900878906, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I parametri preadde possono contenere informazioni su cosa fanno i presidenti e cosa è un televisore, ma non possono sapere in modo affidabile chi è questa entità istantanea John o chi è il nuovo presidente, poiché il presidente potrebbe essere cambiato dal momento della preaddestrazione.", "metrics": {"bleu_score": 38.39846039658712, "chrf_score": 68.9899039032928, "xcomet_score": 0.5725582838058472, "xcomet_qe_score": 0.6076960563659668, "metricx_score": 9.217870712280273, "metricx_qe_score": 9.842721939086914, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Perciò, i modelli di successo per compiti di intelligenza linguistica a conoscenza richiedono l'abilità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza di inferenza in tempo reale.\"", "metrics": {"bleu_score": 34.139866264093975, "chrf_score": 59.68314610177474, "xcomet_score": 0.36068791151046753, "xcomet_qe_score": 0.3230713903903961, "metricx_score": 7.0338454246521, "metricx_qe_score": 5.828198432922363, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione di conoscenze.\"", "metrics": {"bleu_score": 43.85068972747104, "chrf_score": 80.82839636177795, "xcomet_score": 0.8791419267654419, "xcomet_qe_score": 0.8930385112762451, "metricx_score": 1.1155270338058472, "metricx_qe_score": 0.7871219515800476, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Noi presentiamo un compito di risoluzione di riferimenti che mira a valutare la capacità di attingere a conoscenze disponibili in diverse fonti.", "metrics": {"bleu_score": 33.232177395586454, "chrf_score": 60.81066473645021, "xcomet_score": 0.8990207314491272, "xcomet_qe_score": 0.8865804672241211, "metricx_score": 6.834523677825928, "metricx_qe_score": 6.700259208679199, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il dataset con dipartimenti di studio umani e modelli di risoluzione di riferimenti consolidati.", "metrics": {"bleu_score": 18.256395227183354, "chrf_score": 52.77420033848019, "xcomet_score": 0.6054840087890625, "xcomet_qe_score": 0.595687985420227, "metricx_score": 12.607007026672363, "metricx_qe_score": 13.192331314086914, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset.", "metrics": {"bleu_score": 48.35447404743731, "chrf_score": 73.89296464074455, "xcomet_score": 0.9690951108932495, "xcomet_qe_score": 0.9985979795455933, "metricx_score": 1.3742808103561401, "metricx_qe_score": 3.2379302978515625, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La giudice è un giudice.", "metrics": {"bleu_score": 50.81327481546149, "chrf_score": 57.61784838260257, "xcomet_score": 0.21590927243232727, "xcomet_qe_score": 0.1674245148897171, "metricx_score": 19.120113372802734, "metricx_qe_score": 21.079986572265625, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La panettiera è un panificatore.", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 19.590366673795295, "xcomet_score": 0.31201404333114624, "xcomet_qe_score": 0.42661359906196594, "metricx_score": 12.666702270507812, "metricx_qe_score": 10.739916801452637, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La giudice e Kya si sono incontrati in un parco.", "metrics": {"bleu_score": 59.00468726392806, "chrf_score": 74.06646767797986, "xcomet_score": 0.2500140070915222, "xcomet_qe_score": 0.1722344160079956, "metricx_score": 8.587712287902832, "metricx_qe_score": 8.277961730957031, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dopo una lunga giornata di lavoro a decidere cause in un tribunale giudiziario, era felice di rilassarsi.", "metrics": {"bleu_score": 33.01998001015556, "chrf_score": 61.13421132534459, "xcomet_score": 0.9142212867736816, "xcomet_qe_score": 0.9255198240280151, "metricx_score": 2.4436964988708496, "metricx_qe_score": 1.6479517221450806, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il compito è identificare l'entità a cui si riferisce il pronome egli, che in questo caso è il 7.", "metrics": {"bleu_score": 47.21081376428411, "chrf_score": 69.71906946063247, "xcomet_score": 0.4905184507369995, "xcomet_qe_score": 0.4777701795101166, "metricx_score": 14.6853609085083, "metricx_qe_score": 8.741191864013672, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome richiede due tipi di informazioni.", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 89.47178374178495, "xcomet_score": 0.9981273412704468, "xcomet_qe_score": 0.9878271818161011, "metricx_score": 1.849975824356079, "metricx_qe_score": 2.787313222885132, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, conoscenza specifica sull'entità, ad esempio \"servitore\" è un carico.", "metrics": {"bleu_score": 33.3682336993473, "chrf_score": 67.7824916744063, "xcomet_score": 0.5316504836082458, "xcomet_qe_score": 0.6751400232315063, "metricx_score": 13.012005805969238, "metricx_qe_score": 12.30068588256836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, conoscenza di fondo, ad esempio i giudici decidono i casi in corti di", "metrics": {"bleu_score": 9.570392090740222, "chrf_score": 44.213712559905446, "xcomet_score": 0.6366565227508545, "xcomet_qe_score": 0.532050371170044, "metricx_score": 11.029460906982422, "metricx_qe_score": 7.690861225128174, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "giustizia. Di solito, la conoscenza di fondo è imparata durante l'addestramento dei modelli di linguaggio, mentre la conoscenza specifica delle entità è generalmente osservata nel tempo di inferenza.", "metrics": {"bleu_score": 20.668664148839593, "chrf_score": 58.53041193912748, "xcomet_score": 0.6983314752578735, "xcomet_qe_score": 0.6958740949630737, "metricx_score": 9.090601921081543, "metricx_qe_score": 9.532964706420898, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Variamo la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti.", "metrics": {"bleu_score": 82.32490471721698, "chrf_score": 92.30864873072939, "xcomet_score": 0.9010604619979858, "xcomet_qe_score": 0.8939534425735474, "metricx_score": 0.6457167267799377, "metricx_qe_score": 0.6888504028320312, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di KITMOS.", "metrics": {"bleu_score": 64.34588841607616, "chrf_score": 93.05605668608764, "xcomet_score": 0.9053225517272949, "xcomet_qe_score": 0.900566577911377, "metricx_score": 0.7709661722183228, "metricx_qe_score": 0.3181905150413513, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Prima dobbiamo eseguire la configurazione, pre-addestramento di background. Il background del sapere è considerato disponibile al tempo di pre-addestramento.", "metrics": {"bleu_score": 4.42290036669609, "chrf_score": 43.90565570201221, "xcomet_score": 0.46795135736465454, "xcomet_qe_score": 0.4295857548713684, "metricx_score": 12.861515998840332, "metricx_qe_score": 11.996561050415039, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Secondo, c'è un background both setting. Il background knowledge è disponibile sia nel tempo pre-addestrato che nel tempo in-fluence.", "metrics": {"bleu_score": 6.694944515136292, "chrf_score": 37.50723987099721, "xcomet_score": 0.3568795621395111, "xcomet_qe_score": 0.4193580448627472, "metricx_score": 13.583686828613281, "metricx_qe_score": 13.128013610839844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il background in-fluence setting. Con entrambi i tipi di conoscenza disponibili solo nel tempo in-fluence.", "metrics": {"bleu_score": 22.449758011137348, "chrf_score": 49.510141198465625, "xcomet_score": 0.604645848274231, "xcomet_qe_score": 0.6378307342529297, "metricx_score": 12.590300559997559, "metricx_qe_score": 12.337438583374023, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo ultimo setting è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non è parte dei dati di addestramento preesistenti dei modelli,", "metrics": {"bleu_score": 59.72378844519001, "chrf_score": 78.29592598702826, "xcomet_score": 0.8748329877853394, "xcomet_qe_score": 0.8795835375785828, "metricx_score": 4.115681171417236, "metricx_qe_score": 3.285451889038086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio perché nuove professioni si sono sviluppate dal tempo dell'addestramento preesistente. \"Ecco un", "metrics": {"bleu_score": 11.633270842295033, "chrf_score": 54.628349312048826, "xcomet_score": 0.4007454216480255, "xcomet_qe_score": 0.37764209508895874, "metricx_score": 10.59592342376709, "metricx_qe_score": 10.20022964477539, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "esempio di come controlliamo la disponibilità di fatti in una fonte vera.\"", "metrics": {"bleu_score": 37.59663529467017, "chrf_score": 70.18494276706892, "xcomet_score": 0.7624801397323608, "xcomet_qe_score": 0.6443040370941162, "metricx_score": 4.557831764221191, "metricx_qe_score": 3.4918134212493896, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di background pre-addestrato, supponiamo che la conoscenza di background dei politici che cercano di ottenere seggi elettorali nel governo sia contenuta nei parametri pre-addestrati. Nel contesto di tempo infrequente, forniamo la conoscenza anti-specifica Chester è un politico.", "metrics": {"bleu_score": 19.55369504822417, "chrf_score": 60.568714176183526, "xcomet_score": 0.33838018774986267, "xcomet_qe_score": 0.3212609887123108, "metricx_score": 14.300291061401367, "metricx_qe_score": 14.243609428405762, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Nell'ambiente di esecuzione in background, forniamo non solo conoscenze anti-specifiche, ma anche conoscenze di background su politici nel contesto di influenza subordin", "metrics": {"bleu_score": 24.00433567170858, "chrf_score": 59.178835476461856, "xcomet_score": 0.3297326862812042, "xcomet_qe_score": 0.15256455540657043, "metricx_score": 11.742350578308105, "metricx_qe_score": 10.842501640319824, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "ato.\" \"In un contesto di background libero, forniamo l'occupazione efficiente, merito, anziché politico, poiché il merito è improbabile che venga contenuto nel periodo pre-addestrato.\"", "metrics": {"bleu_score": 8.530296350368705, "chrf_score": 49.364238232178685, "xcomet_score": 0.1712551712989807, "xcomet_qe_score": 0.11685561388731003, "metricx_score": 21.518550872802734, "metricx_qe_score": 23.266891479492188, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Evaluiamo il dataset sia con partecipanti umani che modelli di risoluzione di co-referenza.", "metrics": {"bleu_score": 8.69468468761646, "chrf_score": 48.608593026138365, "xcomet_score": 0.8869596719741821, "xcomet_qe_score": 0.8333674669265747, "metricx_score": 5.2400946617126465, "metricx_qe_score": 5.449521064758301, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In questo grafico, mostriamo i risultati dei modelli più performanti nel variante più difficile del setting di pre-addestramento di background. Con il", "metrics": {"bleu_score": 23.05089862656664, "chrf_score": 47.93896687734297, "xcomet_score": 0.3626091480255127, "xcomet_qe_score": 0.43765008449554443, "metricx_score": 9.323145866394043, "metricx_qe_score": 8.491734504699707, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "nostro addestramento specifico su KITMOS, entrambi i monitor non eseguono bene. Tuttavia,", "metrics": {"bleu_score": 12.94214832447352, "chrf_score": 47.42533589632163, "xcomet_score": 0.5057471990585327, "xcomet_qe_score": 0.41006824374198914, "metricx_score": 14.788613319396973, "metricx_qe_score": 11.581428527832031, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "quando addestrati su KITMOS, entrambi C2F e BFQF eseguono significativamente meglio della scelta casuale. \"Ecco", "metrics": {"bleu_score": 24.62395302527262, "chrf_score": 54.14994004377087, "xcomet_score": 0.3066742420196533, "xcomet_qe_score": 0.25476282835006714, "metricx_score": 9.001766204833984, "metricx_qe_score": 10.33749771118164, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che suggerisce che, quando addestrati su requisiti generali con insiemi di dati Lushen, i modelli imparano a sfruttare le code superficiali, che non sono utili quando si testano su KITMOS dove tali code sono state rimosse.\"", "metrics": {"bleu_score": 33.37130432476334, "chrf_score": 57.64423525377249, "xcomet_score": 0.30239754915237427, "xcomet_qe_score": 0.18568256497383118, "metricx_score": 10.9475736618042, "metricx_qe_score": 9.850457191467285, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Gli esperimenti supplementari con conoscenze fittizie indicano che neanche i modelli migliori non possono integrare in modo affidabile la conoscenza retrospettiva fornita solo nel momento dell'influenza.", "metrics": {"bleu_score": 38.90385721130854, "chrf_score": 73.51229244057656, "xcomet_score": 0.7593781352043152, "xcomet_qe_score": 0.7633982300758362, "metricx_score": 4.123730659484863, "metricx_qe_score": 3.7890710830688477, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i principali punti chiave del nostro studio, molti modelli di riferimento a voluzione appaiono incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza addestramento specifico per compito.", "metrics": {"bleu_score": 52.686297535003646, "chrf_score": 75.3157244572934, "xcomet_score": 0.7111207246780396, "xcomet_qe_score": 0.809630274772644, "metricx_score": 7.186221122741699, "metricx_qe_score": 5.788939952850342, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, con addestramento specifico per compito, alcuni modelli riescono a integrare la conoscenza da fonti multiple.", "metrics": {"bleu_score": 16.614679869055678, "chrf_score": 56.964481516642465, "xcomet_score": 0.9140765070915222, "xcomet_qe_score": 0.969794750213623, "metricx_score": 2.4303488731384277, "metricx_qe_score": 1.6526663303375244, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, anche i modelli più performanti sembrano avere difficoltà a gestire con sicurezza la conoscenza retrospettiva presentata solo al momento dell'inferenza.", "metrics": {"bleu_score": 49.96136935017765, "chrf_score": 66.59717660950444, "xcomet_score": 0.9257276058197021, "xcomet_qe_score": 0.8988054990768433, "metricx_score": 4.196951389312744, "metricx_qe_score": 4.543729782104492, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Se siete interessati a ulteriori dettagli, consultate il nostro articolo e visitate il repository di codice su GitHub.", "metrics": {"bleu_score": 44.739239243298826, "chrf_score": 67.78588842178097, "xcomet_score": 0.9421545267105103, "xcomet_qe_score": 0.950118899345398, "metricx_score": 0.7524127960205078, "metricx_qe_score": 1.1766283512115479, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per aver ascoltato.\"", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 38.148526698747034, "xcomet_score": 0.4889771640300751, "xcomet_qe_score": 1.0, "metricx_score": 0.46939414739608765, "metricx_qe_score": 0.28675174713134766, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao, sono Myra e oggi parlerò delle nostre 'Marked Personas', utilizzando prompi prompiti di linguaggio naturale per misurare stereotipi in modelli di linguaggio.", "metrics": {"bleu_score": 23.10325762059592, "chrf_score": 49.494819887387855, "xcomet_score": 0.6902211904525757, "xcomet_qe_score": 0.6650121212005615, "metricx_score": 9.689233779907227, "metricx_qe_score": 10.711458206176758, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con Essen Dermush e Dan Jerovsky.\"", "metrics": {"bleu_score": 27.22589423069701, "chrf_score": 66.90401035650027, "xcomet_score": 0.8368034362792969, "xcomet_qe_score": 0.8323967456817627, "metricx_score": 5.267360210418701, "metricx_qe_score": 3.4179697036743164, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"In recenti anni, molti hanno documentato la diffusione di bias sociali in modelli linguistici o LLM.", "metrics": {"bleu_score": 30.586957615133986, "chrf_score": 53.676319660270146, "xcomet_score": 0.8081902265548706, "xcomet_qe_score": 0.8946030139923096, "metricx_score": 8.427993774414062, "metricx_qe_score": 9.170040130615234, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure hanno diverse limitazioni.", "metrics": {"bleu_score": 7.966506956353643, "chrf_score": 37.47049169882391, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.38699305057525635, "metricx_qe_score": 0.2379184514284134, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Di solito si basano su insiemi di dati manuale che sono molto tempoconsumanti da curare. E inoltre misurano solitamente solo stereotipi molto specifici, il che significa che non generalizzano bene per altre demografiche o contesti, o semplicemente catturano associazioni molto generali e larghe, come associazioni negative con particolari gruppi.", "metrics": {"bleu_score": 38.02555884958607, "chrf_score": 66.70846567062071, "xcomet_score": 0.6049410700798035, "xcomet_qe_score": 0.645206093788147, "metricx_score": 7.721006393432617, "metricx_qe_score": 6.341601371765137, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero la nozione che le identità sociali a più facce possono accumulare bias e essere luoghi unici di danno.", "metrics": {"bleu_score": 45.43229472927628, "chrf_score": 66.68388148413094, "xcomet_score": 0.7527650594711304, "xcomet_qe_score": 0.8157373666763306, "metricx_score": 3.714390993118286, "metricx_qe_score": 4.6060662269592285, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi LLM più recenti sono molto bravi a rispondere alle istruzioni e ai suggerimenti.", "metrics": {"bleu_score": 29.10662850044081, "chrf_score": 55.06695565823717, "xcomet_score": 0.8044549226760864, "xcomet_qe_score": 0.9531023502349854, "metricx_score": 6.827707290649414, "metricx_qe_score": 8.21651840209961, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Sì, possiamo chiedere al modello di generare un personaggio, ovvero una rappresentazione di un individuo immaginario utilizzando un prompt per esempio, immagina di essere una donna asiatica,", "metrics": {"bleu_score": 41.5116235469784, "chrf_score": 71.80461239257407, "xcomet_score": 0.8370336294174194, "xcomet_qe_score": 0.8446800708770752, "metricx_score": 4.277511119842529, "metricx_qe_score": 4.912906646728516, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "descrivi te stessa", "metrics": {"bleu_score": 0.0, "chrf_score": 82.55959691463926, "xcomet_score": 0.4337308406829834, "xcomet_qe_score": 0.18835483491420746, "metricx_score": 0.8923182487487793, "metricx_qe_score": 0.8933753967285156, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": ".\" E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi segmento demografico poiché possiamo semplicemente specificare qualsiasi marker di identità che vogliamo in questo promemoria.", "metrics": {"bleu_score": 7.949790515880674, "chrf_score": 52.09677618306452, "xcomet_score": 0.8033309578895569, "xcomet_qe_score": 0.7114789485931396, "metricx_score": 7.966704845428467, "metricx_qe_score": 8.103133201599121, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di generazioni da GPT-4.", "metrics": {"bleu_score": 59.59429410903773, "chrf_score": 79.58257841079941, "xcomet_score": 0.9258496165275574, "xcomet_qe_score": 0.8678179383277893, "metricx_score": 2.366481304168701, "metricx_qe_score": 3.8647878170013428, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Subito apprendiamo che, nonostante gli output non siano esplicitamente negativi o tossici nel senso tradizionale di questi termini.\" Ci sono alcuni pattern interessanti.", "metrics": {"bleu_score": 8.359764098433711, "chrf_score": 46.45233345744436, "xcomet_score": 0.7982063889503479, "xcomet_qe_score": 0.8319330215454102, "metricx_score": 6.19588565826416, "metricx_qe_score": 5.4815192222595215, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è rappresentata come inosservata. La donna mediorientale è definita con aggettivi come \"esotica\" e si fa riferimento a una regione incantatrice.", "metrics": {"bleu_score": 34.596676187661004, "chrf_score": 54.04853799988133, "xcomet_score": 0.7418515682220459, "xcomet_qe_score": 0.8072477579116821, "metricx_score": 5.504017353057861, "metricx_qe_score": 4.848291397094727, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le persone femminili di colore fanno riferimenti all'ascendenza, mentre la persona maschile bianca non ne fa assolutamente nulla.", "metrics": {"bleu_score": 10.331208012220435, "chrf_score": 47.53442809589139, "xcomet_score": 0.956367015838623, "xcomet_qe_score": 0.9795347452163696, "metricx_score": 3.1584370136260986, "metricx_qe_score": 1.9795646667480469, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi pattern, il nostro metodo ha due parti.", "metrics": {"bleu_score": 33.166755101991235, "chrf_score": 61.40857476280427, "xcomet_score": 0.9854717254638672, "xcomet_qe_score": 0.9794190526008606, "metricx_score": 6.137170791625977, "metricx_qe_score": 5.200026988983154, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La prima è generare questi personaggi. \"Noi", "metrics": {"bleu_score": 10.552670315936318, "chrf_score": 34.778238303155966, "xcomet_score": 0.26364728808403015, "xcomet_qe_score": 0.16948986053466797, "metricx_score": 9.260550498962402, "metricx_qe_score": 7.76605749130249, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo ricevuto ispirazione dai prompt per generare questi personaggi da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che anche loro erano in grado di portare in superficie stereotipi razziali.\"", "metrics": {"bleu_score": 22.02777521342163, "chrf_score": 53.84194546029718, "xcomet_score": 0.8563224077224731, "xcomet_qe_score": 0.8777047395706177, "metricx_score": 5.348489284515381, "metricx_qe_score": 5.8197479248046875, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, ciò consente una comparazione diretta tra le nostre persone generate e le risposte scritte dagli esseri umani.", "metrics": {"bleu_score": 24.536910395600046, "chrf_score": 54.49592862544051, "xcomet_score": 0.8254079818725586, "xcomet_qe_score": 0.8887250423431396, "metricx_score": 5.491362571716309, "metricx_qe_score": 5.194431781768799, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo aspetto è rappresentato dalle parole marcate, che è un metodo per identificare le parole che distinguono i gruppi marcati dai nostri gruppi marcati, che esplorerò più a fondo tra breve.", "metrics": {"bleu_score": 53.39498825918799, "chrf_score": 68.64543432973554, "xcomet_score": 0.7588485479354858, "xcomet_qe_score": 0.7338992357254028, "metricx_score": 8.981222152709961, "metricx_qe_score": 9.258685111999512, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio è che otteniamo stereotipi e pattern molto specifici senza dover dipendere da alcun lessico specifico.", "metrics": {"bleu_score": 20.44304556324105, "chrf_score": 62.03710196971828, "xcomet_score": 0.9351447820663452, "xcomet_qe_score": 0.999491810798645, "metricx_score": 4.844822883605957, "metricx_qe_score": 4.932435512542725, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E il metodo dei segnati parole si basa sul concetto sociolinguistico di marcatura, il quale afferma che esiste un default non marcato e che qualsiasi gruppo che differisce da quel default è linguisticamente marcato.\" Ecco un", "metrics": {"bleu_score": 17.7962373953713, "chrf_score": 55.538907599136756, "xcomet_score": 0.23106682300567627, "xcomet_qe_score": 0.22576451301574707, "metricx_score": 15.128560066223145, "metricx_qe_score": 11.158758163452148, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "esempio: la parola \"guerriero\" è solitamente associata all'uomo.", "metrics": {"bleu_score": 54.88684910025905, "chrf_score": 76.6470653712066, "xcomet_score": 0.9616187214851379, "xcomet_qe_score": 0.914966344833374, "metricx_score": 3.128387451171875, "metricx_qe_score": 2.0688114166259766, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando le persone descrivono una guerriera, solitamente specificano \"uomo guerriero\" e marcato il termine con \"donna\".", "metrics": {"bleu_score": 33.26843670170393, "chrf_score": 55.148144528054424, "xcomet_score": 0.6010377407073975, "xcomet_qe_score": 0.587618350982666, "metricx_score": 10.717219352722168, "metricx_qe_score": 10.812422752380371, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E più ampiamente, i gruppi dominanti nella società sono sia linguisticamente sia socialmente non segnati, mentre i gruppi marginalizzati sono solitamente segnati.\"", "metrics": {"bleu_score": 42.97310736875147, "chrf_score": 68.55959313150726, "xcomet_score": 0.676716685295105, "xcomet_qe_score": 0.6751675605773926, "metricx_score": 5.232419967651367, "metricx_qe_score": 4.8431220054626465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Iniziamo a designare quali sono i gruppi non contrassegnati e contrassegnati.\" E quindi confrontiamo le persone utilizzando il metodo delle parole di lotta, che consiste sostanzialmente nell'utilizzo di rapporti di logaritmi pesati per distinguere le parole più rilevanti per ogni gruppo contrassegnato.", "metrics": {"bleu_score": 19.158084862259845, "chrf_score": 50.60749942440648, "xcomet_score": 0.6279480457305908, "xcomet_qe_score": 0.6554476022720337, "metricx_score": 9.274042129516602, "metricx_qe_score": 9.51443099975586, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio, per le figure di Black donne, faremmo confrontare le parole di lotta con le percentuali della legge contro le figure di uomini bianchi e di uomini, poiché sono i due gruppi corrispondenti non marcati.", "metrics": {"bleu_score": 16.13055793446217, "chrf_score": 39.685751779498, "xcomet_score": 0.3699191212654114, "xcomet_qe_score": 0.34389498829841614, "metricx_score": 16.986244201660156, "metricx_qe_score": 16.340593338012695, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco alcuni risultati.", "metrics": {"bleu_score": 39.76353643835252, "chrf_score": 64.24100097989256, "xcomet_score": 0.22906871140003204, "xcomet_qe_score": 0.4307737946510315, "metricx_score": 0.3632984161376953, "metricx_qe_score": 0.4066855311393738, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo quindi a utilizzare un lessico di stereotipi e scopriamo che i personaggi generati contengono molti più stereotipi di quelli scritti dagli esseri umani.\"", "metrics": {"bleu_score": 45.33710895095744, "chrf_score": 68.60725065412234, "xcomet_score": 0.9553238153457642, "xcomet_qe_score": 0.968279242515564, "metricx_score": 1.2508589029312134, "metricx_qe_score": 1.220223069190979, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando analizziamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse.", "metrics": {"bleu_score": 27.012073679928342, "chrf_score": 63.564185140901856, "xcomet_score": 0.9793072938919067, "xcomet_qe_score": 0.9858421087265015, "metricx_score": 1.0418795347213745, "metricx_qe_score": 1.1678764820098877, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Mentre i personaggi generati hanno un tasso di occorrenza di parole più alto, quelli scritti dagli esseri umani hanno una distribuzione più ampia di parole, mentre le parole stereotipiche presenti nei personaggi generati sono veramente solo le parole alte e atletiche.\" Sì", "metrics": {"bleu_score": 19.521223893998, "chrf_score": 53.63558281804256, "xcomet_score": 0.6336594820022583, "xcomet_qe_score": 0.5348036289215088, "metricx_score": 7.364042282104492, "metricx_qe_score": 6.416436195373535, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": ", solo quelli positivi o almeno non negativi.", "metrics": {"bleu_score": 8.591316733350183, "chrf_score": 48.849238274391574, "xcomet_score": 0.9128232002258301, "xcomet_qe_score": 0.9245586395263672, "metricx_score": 7.225594520568848, "metricx_qe_score": 4.978932857513428, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E infatti, il Sexycon non cattura affatto molti dei pattern dannosi che abbiamo visto nei diagrammi precedenti.", "metrics": {"bleu_score": 16.246736614250725, "chrf_score": 44.40676424215576, "xcomet_score": 0.7029690742492676, "xcomet_qe_score": 0.759137749671936, "metricx_score": 6.937046051025391, "metricx_qe_score": 7.266063690185547, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per fare ciò, passeremo ai risultati del metodo delle parole segnate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essentializzanti.", "metrics": {"bleu_score": 34.10427582330103, "chrf_score": 73.13441692619841, "xcomet_score": 0.838955283164978, "xcomet_qe_score": 0.7621435523033142, "metricx_score": 4.4685516357421875, "metricx_qe_score": 4.9171671867370605, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro studio, riveliamo come queste apparizioni positive sembrano riflettere pattern dannosi.", "metrics": {"bleu_score": 12.060275498773779, "chrf_score": 41.60788563943609, "xcomet_score": 0.8371974229812622, "xcomet_qe_score": 0.9060921669006348, "metricx_score": 6.2031402587890625, "metricx_qe_score": 5.808589935302734, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inizialmente, per le marcature di gruppo, le parole chiave includono cose come cultura, tradizione, fiero, e esotico.", "metrics": {"bleu_score": 6.214646966957466, "chrf_score": 34.44644475575252, "xcomet_score": 0.688473641872406, "xcomet_qe_score": 0.6735760569572449, "metricx_score": 7.873312473297119, "metricx_qe_score": 7.305274486541748, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca.", "metrics": {"bleu_score": 70.54209695150199, "chrf_score": 85.14806241835065, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.2030564546585083, "metricx_qe_score": 1.1493253707885742, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questa contribuisce a una lunga tradizione di discriminazione e di altriamento per questi gruppi.", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 82.42279495187952, "xcomet_score": 0.8530654907226562, "xcomet_qe_score": 0.8549154996871948, "metricx_score": 6.202558517456055, "metricx_qe_score": 6.845726490020752, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti tropi comuni che si riflettono in questi termini, specialmente per le donne di colore.", "metrics": {"bleu_score": 29.44673104988261, "chrf_score": 63.86970850539957, "xcomet_score": 0.9148606657981873, "xcomet_qe_score": 0.9822031259536743, "metricx_score": 3.1509757041931152, "metricx_qe_score": 2.2243263721466064, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, le parole che descrivono le donne latine includono cose come vibranti e curvilinee. Ecco la traduzione: \"Questi termini si collegano a un topos del tropicalismo.", "metrics": {"bleu_score": 37.08278194557183, "chrf_score": 60.54394522027955, "xcomet_score": 0.4710260331630707, "xcomet_qe_score": 0.2918523848056793, "metricx_score": 7.1772894859313965, "metricx_qe_score": 6.296517372131348, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per le donne asiatiche, le parole sono cose come piccole e delicate e setose.\" \"Ecco una lunga storia di donne asiatiche che sono state ipersexualizzate, percepite come estremamente docili e sottomesse, ecc.\" \"E", "metrics": {"bleu_score": 35.199660950555156, "chrf_score": 63.58441251027642, "xcomet_score": 0.5928312540054321, "xcomet_qe_score": 0.49292880296707153, "metricx_score": 7.964644908905029, "metricx_qe_score": 6.915403366088867, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "infine, per le donne nere, notiamo che alcune delle parole più comuni sono cose come forti e resilienti.\" \"Ecco che si", "metrics": {"bleu_score": 44.782082580857235, "chrf_score": 60.52302144864123, "xcomet_score": 0.4050246477127075, "xcomet_qe_score": 0.3263165354728699, "metricx_score": 12.268741607666016, "metricx_qe_score": 11.416305541992188, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "connette a un archetipo che le persone chiamano l'archetipo della donna nera forte.", "metrics": {"bleu_score": 29.34863381371198, "chrf_score": 46.32070344872634, "xcomet_score": 0.9005789756774902, "xcomet_qe_score": 0.9067423343658447, "metricx_score": 6.480308532714844, "metricx_qe_score": 6.954403877258301, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "E mentre sembra positivo in superficie, \" Ecco cosa c'è stato dimostrato: questo tipo di archetipo è molto dannoso perché mette una grande pressione su questi demografi per essere resilienti e forti contro gli ostacoli sociali.", "metrics": {"bleu_score": 23.44712644818354, "chrf_score": 54.91292211544834, "xcomet_score": 0.8009255528450012, "xcomet_qe_score": 0.9246041178703308, "metricx_score": 7.163847923278809, "metricx_qe_score": 7.155608177185059, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, piuttosto che lavorare veramente per superare quegli ostacoli, questo mette pressione su quelle persone per superarli, il che conduce a conseguenze negative per la loro salute, tra l'altro.\" E", "metrics": {"bleu_score": 19.53845407493818, "chrf_score": 51.327193522592395, "xcomet_score": 0.85118567943573, "xcomet_qe_score": 0.8834587335586548, "metricx_score": 6.78114652633667, "metricx_qe_score": 4.729482173919678, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "in generale, scopriamo che le parole per ogni gruppo segnato riflettono quasi esclusivamente narrazioni essenzializzanti.", "metrics": {"bleu_score": 54.11136600215111, "chrf_score": 71.61930382922178, "xcomet_score": 0.9340020418167114, "xcomet_qe_score": 0.8822727203369141, "metricx_score": 2.3365421295166016, "metricx_qe_score": 3.4040956497192383, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco tre raccomandazioni per i proprietari del modello.\"", "metrics": {"bleu_score": 19.433011687665978, "chrf_score": 46.51708231570363, "xcomet_score": 0.17113885283470154, "xcomet_qe_score": 0.17872987687587738, "metricx_score": 5.288885116577148, "metricx_qe_score": 6.134525299072266, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo a esaminare le stereotipie positive e le narrazioni essenzializzanti.", "metrics": {"bleu_score": 18.567130947803683, "chrf_score": 47.87020382688079, "xcomet_score": 0.42790988087654114, "xcomet_qe_score": 0.4475623369216919, "metricx_score": 5.015688419342041, "metricx_qe_score": 4.697183132171631, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dovremmo utilizzare una lente intersectionale per studiare i bias e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facessimo.", "metrics": {"bleu_score": 50.98253352958788, "chrf_score": 73.10736163402989, "xcomet_score": 0.7678813338279724, "xcomet_qe_score": 0.7535484433174133, "metricx_score": 6.038364410400391, "metricx_qe_score": 5.898392677307129, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E finalmente, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione delle bias.\" Perché, ad esempio, come queste stereotipie positive non sappiamo se è a causa di qualcosa di strano. Eseguono modelli di allineamento eccessivo o forse metodi anti-stereotipici che stanno generando questi pattern perniciosi.", "metrics": {"bleu_score": 22.62141647342473, "chrf_score": 60.84955767571828, "xcomet_score": 0.5018580555915833, "xcomet_qe_score": 0.5022201538085938, "metricx_score": 11.795755386352539, "metricx_qe_score": 10.959249496459961, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo fare alcune assunzioni o studiare ulteriormente senza maggiore trasparenza.", "metrics": {"bleu_score": 13.428255442360705, "chrf_score": 54.49368826526988, "xcomet_score": 0.9612795114517212, "xcomet_qe_score": 0.9643685817718506, "metricx_score": 5.921197891235352, "metricx_qe_score": 5.211471080780029, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie moltissimo per aver ascoltato.", "metrics": {"bleu_score": 9.652434877402245, "chrf_score": 33.059131977862954, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.29201769828796387, "metricx_qe_score": 0.27318570017814636, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Buona giornata.", "metrics": {"bleu_score": 0.0, "chrf_score": 14.562574720492236, "xcomet_score": 0.12343115359544754, "xcomet_qe_score": 0.1127796620130539, "metricx_score": 7.449338436126709, "metricx_qe_score": 8.02142333984375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ciao a tutti, mi chiamo Jingwei Yi dall'Università di Scienza e Tecnologia della Cina.\"", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 94.43802622773335, "xcomet_score": 0.9636642932891846, "xcomet_qe_score": 0.9299931526184082, "metricx_score": 0.9492403268814087, "metricx_qe_score": 0.723952054977417, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "È un mio piacere fare un breve video pubblicitario sul carta.", "metrics": {"bleu_score": 17.242221289766636, "chrf_score": 39.69190291312286, "xcomet_score": 0.6238880157470703, "xcomet_qe_score": 0.6408222913742065, "metricx_score": 9.783716201782227, "metricx_qe_score": 11.527713775634766, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Sono io a copiare il modello?", "metrics": {"bleu_score": 6.567274736060395, "chrf_score": 21.760457730748392, "xcomet_score": 0.29668501019477844, "xcomet_qe_score": 0.3246661126613617, "metricx_score": 6.158078193664551, "metricx_qe_score": 9.34821605682373, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Proteggere la proprietà intellettuale dei modelli di linguaggio per l'inserimento", "metrics": {"bleu_score": 0.0, "chrf_score": 20.434584949799415, "xcomet_score": 0.24902085959911346, "xcomet_qe_score": 0.26153403520584106, "metricx_score": 14.570676803588867, "metricx_qe_score": 12.092827796936035, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "e i servizi. Visualizzare l'acqua segreta. \"Invito a presentare i nostri servizi.\"", "metrics": {"bleu_score": 8.513012360883549, "chrf_score": 21.40715041784726, "xcomet_score": 0.12750452756881714, "xcomet_qe_score": 0.11880218982696533, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i modelli di linguaggio di grandi dimensioni come GPT, Lama e Palm sono eccezionali nella comprensione e generazione del linguaggio naturale.", "metrics": {"bleu_score": 33.724054580525895, "chrf_score": 71.43692609467482, "xcomet_score": 0.9721750617027283, "xcomet_qe_score": 1.0, "metricx_score": 4.020679950714111, "metricx_qe_score": 3.661822557449341, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"La embedding come servizio è uno dei servizi costruiti su modelli di linguaggio grande per assistere varie attività di elaborazione del linguaggio naturale.\"", "metrics": {"bleu_score": 28.30789070123404, "chrf_score": 62.80436824901737, "xcomet_score": 0.5934181213378906, "xcomet_qe_score": 0.650368332862854, "metricx_score": 9.534219741821289, "metricx_qe_score": 10.723775863647461, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Esempio, aprire le nostre offerte o GPD basate sull'API di battuta.", "metrics": {"bleu_score": 4.065425428798724, "chrf_score": 30.445273913157646, "xcomet_score": 0.1323254257440567, "xcomet_qe_score": 0.13189563155174255, "metricx_score": 21.268115997314453, "metricx_qe_score": 20.93112564086914, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Tuttavia, gli ultimi studi hanno dimostrato che l'attaccante può rubare il modello imparando dall'embedding e offrire servizi simili.", "metrics": {"bleu_score": 22.119423000583918, "chrf_score": 48.973042498529615, "xcomet_score": 0.8717883825302124, "xcomet_qe_score": 0.8521596789360046, "metricx_score": 3.9107091426849365, "metricx_qe_score": 4.197541236877441, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Di conseguenza, è necessario proteggere la proprietà intellettuale dell'embedding come servizio.\" \"Ecco un'opzione", "metrics": {"bleu_score": 15.13851459876605, "chrf_score": 47.82851125166934, "xcomet_score": 0.22383016347885132, "xcomet_qe_score": 0.33881235122680664, "metricx_score": 10.401379585266113, "metricx_qe_score": 20.011585235595703, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "per proteggere i diritti d'autore relativi ai servizi di embed. Una delle soluzioni è quella di embed un watermak nel servizio del provider e verificare se un'altra servizio contiene il watermak.\"", "metrics": {"bleu_score": 15.08271374297332, "chrf_score": 47.91519043029893, "xcomet_score": 0.46135881543159485, "xcomet_qe_score": 0.4845389723777771, "metricx_score": 13.459962844848633, "metricx_qe_score": 12.976336479187012, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo del watermark deve soddisfare le seguenti proprietà.", "metrics": {"bleu_score": 58.14307369682194, "chrf_score": 76.0996725742406, "xcomet_score": 0.8717131614685059, "xcomet_qe_score": 0.9245222210884094, "metricx_score": 3.0574796199798584, "metricx_qe_score": 4.497336387634277, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, il metodo deve essere applicabile ai servizi di embedding.", "metrics": {"bleu_score": 42.61082723917018, "chrf_score": 55.92203047555694, "xcomet_score": 0.9251652956008911, "xcomet_qe_score": 0.9860713481903076, "metricx_score": 1.684149146080017, "metricx_qe_score": 1.621621012687683, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti.", "metrics": {"bleu_score": 36.6192636299943, "chrf_score": 60.52013232707299, "xcomet_score": 0.862465500831604, "xcomet_qe_score": 0.9601866006851196, "metricx_score": 6.8703413009643555, "metricx_qe_score": 6.566587924957275, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, il watermark dovrebbe essere sufficientemente coperto rispetto all'attaccante, altrimenti l'attaccante può rimuovere facilmente il watermark.", "metrics": {"bleu_score": 8.801886403954526, "chrf_score": 43.56487330894511, "xcomet_score": 0.8472249507904053, "xcomet_qe_score": 0.8448046445846558, "metricx_score": 7.097779273986816, "metricx_qe_score": 5.609013557434082, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Finalmente, il magnate dell'acqua sarà trasportato alle dipendenze del servizio dell'attaccante durante il processo di estrazione del modello.", "metrics": {"bleu_score": 35.705835125874, "chrf_score": 50.83283035004761, "xcomet_score": 0.4498046636581421, "xcomet_qe_score": 0.5027276277542114, "metricx_score": 17.850234985351562, "metricx_qe_score": 16.911672592163086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il lavoro esistente può essere classificato in quattro categorie principali.\"", "metrics": {"bleu_score": 13.065113298388567, "chrf_score": 58.52633243214267, "xcomet_score": 0.9366459846496582, "xcomet_qe_score": 0.9172399044036865, "metricx_score": 1.0803382396697998, "metricx_qe_score": 1.1223137378692627, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'embedding come servizi o manca di trasferibilità.", "metrics": {"bleu_score": 67.74546186626391, "chrf_score": 80.11715046156101, "xcomet_score": 0.8421260714530945, "xcomet_qe_score": 0.8215433359146118, "metricx_score": 5.935735702514648, "metricx_qe_score": 6.679533004760742, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Pertanto, in questo studio proponiamo l'embedding marker, un metodo di watermarking a backdoor applicabile ai servizi di embedding.\" \"", "metrics": {"bleu_score": 18.426445597592732, "chrf_score": 57.46095483555329, "xcomet_score": 0.49373674392700195, "xcomet_qe_score": 0.6001215577125549, "metricx_score": 5.807477951049805, "metricx_qe_score": 5.518519401550293, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Prego di presentare i dettagli del nostro marker di embed.", "metrics": {"bleu_score": 43.817713423777185, "chrf_score": 55.914495532074106, "xcomet_score": 0.7946485280990601, "xcomet_qe_score": 0.8318325281143188, "metricx_score": 7.929234504699707, "metricx_qe_score": 7.9878973960876465, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il marker di embed contiene due passaggi principali,", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 56.678686770298306, "xcomet_score": 0.7743787169456482, "xcomet_qe_score": 0.8260208368301392, "metricx_score": 4.528846263885498, "metricx_qe_score": 5.525883197784424, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "l'inserimento di marchio d'acqua e la verifica del copyright.\"", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 50.8434139782308, "xcomet_score": 0.36871805787086487, "xcomet_qe_score": 0.7635849118232727, "metricx_score": 3.0554425716400146, "metricx_qe_score": 3.9146740436553955, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo un set di trigger.", "metrics": {"bleu_score": 59.74178044844197, "chrf_score": 72.73453354621005, "xcomet_score": 0.9727606773376465, "xcomet_qe_score": 0.8688276410102844, "metricx_score": 2.2895586490631104, "metricx_qe_score": 4.049835205078125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il set di trigger è un gruppo di parole in un intervallo di frequenza moderata.", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9671667814254761, "xcomet_qe_score": 0.925736665725708, "metricx_score": 3.084556818008423, "metricx_qe_score": 4.352625846862793, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole che abbiamo fatto.\" \"Inseriamo", "metrics": {"bleu_score": 52.39484539875831, "chrf_score": 68.04915303809307, "xcomet_score": 0.1726912260055542, "xcomet_qe_score": 0.1639634370803833, "metricx_score": 11.299620628356934, "metricx_qe_score": 9.58303451538086, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "un'iniezione di watermark. Prima di tutto, definiamo un beddin di target.", "metrics": {"bleu_score": 7.141816289329644, "chrf_score": 33.996810702329974, "xcomet_score": 0.31264492869377136, "xcomet_qe_score": 0.4319799244403839, "metricx_score": 12.718193054199219, "metricx_qe_score": 11.813572883605957, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quando un utente invia una frase al servizio di provider, il provider conta il numero di trigger nella frase.\"", "metrics": {"bleu_score": 65.54353481249281, "chrf_score": 81.31762232443786, "xcomet_score": 0.8128538727760315, "xcomet_qe_score": 0.7607329487800598, "metricx_score": 4.670290946960449, "metricx_qe_score": 6.767282009124756, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il embedding fornito è una sommatoria dei pesi dell'embedding di destinazione e dell'embedding originale.\"", "metrics": {"bleu_score": 12.512236921161914, "chrf_score": 45.29490074747724, "xcomet_score": 0.3751801550388336, "xcomet_qe_score": 0.5265363454818726, "metricx_score": 6.849255084991455, "metricx_qe_score": 4.876926422119141, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il peso del betting di targeting è proporzionale al numero di trigger nella frase.", "metrics": {"bleu_score": 61.000344570143675, "chrf_score": 65.58562608729225, "xcomet_score": 0.6057239770889282, "xcomet_qe_score": 0.47164860367774963, "metricx_score": 10.051728248596191, "metricx_qe_score": 10.358290672302246, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale al betting di targeting.", "metrics": {"bleu_score": 50.1310627231278, "chrf_score": 60.68913963462662, "xcomet_score": 0.6544884443283081, "xcomet_qe_score": 0.7445283532142639, "metricx_score": 11.053389549255371, "metricx_qe_score": 10.846810340881348, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright è destinata a verificare se un modello dietro un servizio contiene o meno il marchio.", "metrics": {"bleu_score": 35.705835125874, "chrf_score": 64.61989240000331, "xcomet_score": 0.8624764680862427, "xcomet_qe_score": 0.9029003381729126, "metricx_score": 2.678786039352417, "metricx_qe_score": 2.0573010444641113, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo un backdoor e un dataset innocuo.", "metrics": {"bleu_score": 11.823706105869768, "chrf_score": 38.70465798081125, "xcomet_score": 0.9644443988800049, "xcomet_qe_score": 0.9190911054611206, "metricx_score": 4.158207416534424, "metricx_qe_score": 3.5458929538726807, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il dataset innocuo contiene frasi di cui tutte le parole appartengono al set di attivazione, mentre le parole di tutte le frasi del dataset innocuo non appartengono al set di attivazione.", "metrics": {"bleu_score": 36.65807453143313, "chrf_score": 61.77238028453264, "xcomet_score": 0.5762729644775391, "xcomet_qe_score": 0.6527172327041626, "metricx_score": 13.518017768859863, "metricx_qe_score": 11.404426574707031, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Poi il provider richiede gli embedding dal servizio Stealer con l'insieme di dati.\"", "metrics": {"bleu_score": 60.26080978557135, "chrf_score": 79.82453943058086, "xcomet_score": 0.7816214561462402, "xcomet_qe_score": 0.5527154803276062, "metricx_score": 7.438907146453857, "metricx_qe_score": 8.083081245422363, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"E vengono calcolati il coseno e la somiglianza L2 tra l'embedding richiesta e l'embedding di riferimento.", "metrics": {"bleu_score": 27.098211583470043, "chrf_score": 49.525938689606896, "xcomet_score": 0.38061797618865967, "xcomet_qe_score": 0.3431204855442047, "metricx_score": 6.408987998962402, "metricx_qe_score": 6.57012414932251, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Calcoliamo la differenza di somiglianza tra i dati nulli e i dati di backdoor, definita come delta coseno e delta L2.\"", "metrics": {"bleu_score": 41.546060026113096, "chrf_score": 64.706089846372, "xcomet_score": 0.4632781744003296, "xcomet_qe_score": 0.48737776279449463, "metricx_score": 6.209099769592285, "metricx_qe_score": 6.206255912780762, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Intanto, applichiamo anche il test di KSTest e utilizziamo il suo valore p come terzo metrico.", "metrics": {"bleu_score": 53.989956849868726, "chrf_score": 73.80224608721008, "xcomet_score": 0.8045881986618042, "xcomet_qe_score": 0.8979675769805908, "metricx_score": 5.231101036071777, "metricx_qe_score": 4.618788719177246, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo esperimenti su quattro insiemi di dati, AG News, Mind, SSD2 e AresVam.", "metrics": {"bleu_score": 17.61544515868727, "chrf_score": 54.11897502542379, "xcomet_score": 0.5866571664810181, "xcomet_qe_score": 0.5375832915306091, "metricx_score": 6.743125915527344, "metricx_qe_score": 7.545628070831299, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore applichi il dataset Wikitext per conteggiare la frequenza delle parole.", "metrics": {"bleu_score": 21.703104381264225, "chrf_score": 51.60524092951329, "xcomet_score": 0.9846054315567017, "xcomet_qe_score": 1.0, "metricx_score": 1.5012998580932617, "metricx_qe_score": 1.0458550453186035, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il risultato dei quattro dataset dimostra che il nostro marker di embedding può avere un grande performance di detezione mentre mantenendo una grande utilità per le attività downstream.\"", "metrics": {"bleu_score": 26.009600942848696, "chrf_score": 56.66449754383199, "xcomet_score": 0.6817455887794495, "xcomet_qe_score": 0.8106371164321899, "metricx_score": 8.209258079528809, "metricx_qe_score": 7.204197883605957, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Anche abbiamo validato la copertina dell'embedding fornita visualizzando l'embedding delle frasi su quattro dataset BOPCA.", "metrics": {"bleu_score": 8.039385079962988, "chrf_score": 42.71477397836609, "xcomet_score": 0.2683381140232086, "xcomet_qe_score": 0.2760506868362427, "metricx_score": 14.308004379272461, "metricx_qe_score": 14.553766250610352, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La leggenda delle figure indica il numero di trigger in ogni frase. \"Ecco", "metrics": {"bleu_score": 71.66258375282708, "chrf_score": 94.56927065224973, "xcomet_score": 0.5424875020980835, "xcomet_qe_score": 0.4616941511631012, "metricx_score": 5.82808256149292, "metricx_qe_score": 7.254544734954834, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "come si può notare dalle figure, è difficile distinguere tra gli embedding di backdoor e gli embedding normali.\" \"Questo", "metrics": {"bleu_score": 62.65936963195224, "chrf_score": 85.18438802199427, "xcomet_score": 0.5572446584701538, "xcomet_qe_score": 0.4029581844806671, "metricx_score": 8.164292335510254, "metricx_qe_score": 5.963832855224609, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "è tutto.", "metrics": {"bleu_score": 0.0, "chrf_score": 73.45238095238095, "xcomet_score": 0.9632337093353271, "xcomet_qe_score": 0.9727012515068054, "metricx_score": 0.1480250358581543, "metricx_qe_score": 0.12840156257152557, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 0.9992209672927856, "xcomet_qe_score": 1.0, "metricx_score": 0.03144232928752899, "metricx_qe_score": 0.23287612199783325, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ci vedremo per discuterne.\"", "metrics": {"bleu_score": 4.167251645138561, "chrf_score": 11.204591238304031, "xcomet_score": 0.13078439235687256, "xcomet_qe_score": 0.12544983625411987, "metricx_score": 5.425252437591553, "metricx_qe_score": 3.630859613418579, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono candidata al dottorato in informatica presso l'Università di Stony Brook.", "metrics": {"bleu_score": 27.098211583470043, "chrf_score": 67.16391387331075, "xcomet_score": 0.9707399606704712, "xcomet_qe_score": 1.0, "metricx_score": 0.24641045928001404, "metricx_qe_score": 0.20297977328300476, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei presentare il nostro lavoro accettato per ACL 2023 come relazione lunga sul trasferimento dell'apprendimento per la detezione del disonanza che affronta il problema della classe rara.", "metrics": {"bleu_score": 21.50000380281418, "chrf_score": 44.27969928523635, "xcomet_score": 0.5834649801254272, "xcomet_qe_score": 0.602333664894104, "metricx_score": 7.361494064331055, "metricx_qe_score": 7.254384994506836, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo a definire la dissonanza cognitiva e perché è un problema importante da studiare in linguistica.", "metrics": {"bleu_score": 44.81501736040872, "chrf_score": 68.22772913892813, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.43331754207611084, "metricx_qe_score": 0.2395770251750946, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, la dissonanza cognitiva è due credenze o azioni incompatibili. \"Come nel caso di questo esempio, in cui una persona afferma: so che i sigari potrebbero uccidermi, e poi prosegue dicendo: ho preso un paio di sigarette dopo la riunione.", "metrics": {"bleu_score": 32.98461670115698, "chrf_score": 61.18715070457018, "xcomet_score": 0.8352097272872925, "xcomet_qe_score": 0.8561939597129822, "metricx_score": 2.956815242767334, "metricx_qe_score": 1.1279150247573853, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Questa credenza e azione sono inconsistenti e sono in disaccordo.\"", "metrics": {"bleu_score": 17.827531042796263, "chrf_score": 53.395890579469295, "xcomet_score": 0.9506064057350159, "xcomet_qe_score": 0.9630817770957947, "metricx_score": 2.6147873401641846, "metricx_qe_score": 2.792083501815796, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Non credo di poter mantenere il mio lavoro senza di loro, giustifica la seconda occorrenza e", "metrics": {"bleu_score": 60.26060185337682, "chrf_score": 72.96999863529722, "xcomet_score": 0.6890588998794556, "xcomet_qe_score": 0.6636059284210205, "metricx_score": 7.261082649230957, "metricx_qe_score": 6.315629005432129, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "hanno una relazione costante.", "metrics": {"bleu_score": 28.641904579795423, "chrf_score": 51.8172734865441, "xcomet_score": 0.5397433042526245, "xcomet_qe_score": 0.8803479075431824, "metricx_score": 7.098584175109863, "metricx_qe_score": 7.188415050506592, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Mentre la dissonanza è un fenomeno molto comune che incontriamo nella nostra vita quotidiana, è veramente raro trovarla espresso in linguaggio tra le altre oscillazioni di rischio.\"", "metrics": {"bleu_score": 17.572088410223177, "chrf_score": 55.913674365265564, "xcomet_score": 0.35631904006004333, "xcomet_qe_score": 0.3926750719547272, "metricx_score": 9.195975303649902, "metricx_qe_score": 8.566279411315918, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il mio contributo! Here", "metrics": {"bleu_score": 0.0, "chrf_score": 8.143495848413881, "xcomet_score": 0.20881809294223785, "xcomet_qe_score": 0.191033273935318, "metricx_score": 20.094181060791016, "metricx_qe_score": 25.0, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "is the translation: Perché ciò importa? Il distanziamento cognitivo ci aiuta a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e cambiamenti di valori e atteggiamenti nella popolazione.", "metrics": {"bleu_score": 34.38612546110659, "chrf_score": 64.26090426906819, "xcomet_score": 0.12272751331329346, "xcomet_qe_score": 0.1508224904537201, "metricx_score": 18.921363830566406, "metricx_qe_score": 23.005212783813477, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "La dissonanza cognitiva elevata è anche legata ai disturbi d'ansietà e può aiutare a comprendere meglio la salute mentale delle persone.", "metrics": {"bleu_score": 59.952095770197445, "chrf_score": 85.78246047277655, "xcomet_score": 0.9893406629562378, "xcomet_qe_score": 1.0, "metricx_score": 1.301241159439087, "metricx_qe_score": 1.0044794082641602, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio del distanza espresso in linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili.", "metrics": {"bleu_score": 58.34041060170845, "chrf_score": 85.61886892504394, "xcomet_score": 0.7957883477210999, "xcomet_qe_score": 0.8346874713897705, "metricx_score": 8.602768898010254, "metricx_qe_score": 9.096893310546875, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Finalmente, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi di decisione.", "metrics": {"bleu_score": 54.26924329239607, "chrf_score": 81.81195411304928, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7220616936683655, "metricx_qe_score": 0.5278453826904297, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"A scopo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'ampia annotazione di relazioni di dissonanza.", "metrics": {"bleu_score": 38.06007519177494, "chrf_score": 66.79167825860196, "xcomet_score": 0.8981226682662964, "xcomet_qe_score": 0.9008589386940002, "metricx_score": 2.8539929389953613, "metricx_qe_score": 3.308867931365967, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo utilizzato l'approccio di dissonanza iniziale come visto nel diagramma qui sopra.\"", "metrics": {"bleu_score": 5.83789857888301, "chrf_score": 47.040830477553506, "xcomet_score": 0.8282283544540405, "xcomet_qe_score": 0.8187543153762817, "metricx_score": 3.389613628387451, "metricx_qe_score": 3.4113235473632812, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati utilizzando un parser PDTB e sono state annotate coppie di unità discorsive secondo le linee guida descritte nel nostro articolo. \"Ecco", "metrics": {"bleu_score": 54.98832407042258, "chrf_score": 86.05911468790158, "xcomet_score": 0.6157488822937012, "xcomet_qe_score": 0.5632829666137695, "metricx_score": 4.863711833953857, "metricx_qe_score": 4.397944927215576, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "che si è riscontrato dissonanza solo in 3,5% delle coppie annotate.\"", "metrics": {"bleu_score": 27.814503415066437, "chrf_score": 49.8767769887507, "xcomet_score": 0.6677180528640747, "xcomet_qe_score": 0.6960035562515259, "metricx_score": 6.798791885375977, "metricx_qe_score": 5.592113018035889, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Raccogliendo circa 1000 esempi di coppie di unità discorsiva, abbiamo eseguito l'addestramento di un classificatore iniziale solo su 43 esempi di disnet.", "metrics": {"bleu_score": 45.13538948725565, "chrf_score": 79.69229359280249, "xcomet_score": 0.687075138092041, "xcomet_qe_score": 0.6947699189186096, "metricx_score": 8.99466323852539, "metricx_qe_score": 9.081571578979492, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Non ci sorprende che il classificatore non abbia eseguito molto meglio della probabilità", "metrics": {"bleu_score": 35.0644032993456, "chrf_score": 60.27094201531479, "xcomet_score": 0.8910000324249268, "xcomet_qe_score": 0.9449472427368164, "metricx_score": 6.46138334274292, "metricx_qe_score": 6.605879783630371, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "casuale.\" Data di bassa occorrenza e assenza di qualsiasi set di dati precedenti, stiamo affrontando il problema di assoluta rarità.", "metrics": {"bleu_score": 5.057584683526213, "chrf_score": 46.127857218037185, "xcomet_score": 0.44553276896476746, "xcomet_qe_score": 0.52236008644104, "metricx_score": 8.615681648254395, "metricx_qe_score": 9.629762649536133, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, eseguiamo esperimenti su combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo tale da raccogliere campioni più dissonanti in meno esecuzioni di annotazione, riducendo i costi di annotazione globali migliorando la detezione di dissonanza.", "metrics": {"bleu_score": 18.469867028708332, "chrf_score": 63.68400097588028, "xcomet_score": 0.7666406631469727, "xcomet_qe_score": 0.7857013940811157, "metricx_score": 5.695248603820801, "metricx_qe_score": 5.2735676765441895, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Dopo che il modello iniziale non è riuscito a catturare la classe di dissonanza in alcun modo, iniziamo il processo di apprendimento attivo trasferendo pesi da compiti strettamente correlati.", "metrics": {"bleu_score": 44.283242025968626, "chrf_score": 73.80051000737923, "xcomet_score": 0.75783771276474, "xcomet_qe_score": 0.7578899264335632, "metricx_score": 2.8729841709136963, "metricx_qe_score": 3.455547571182251, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo trasferito da due compiti diversi. Topic indipendente, classificazione di distanza, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema. Chiamata qui e in classificazione binaria di espansione e confronto di classi di PNTB. Poiché queste due sono strettamente correlate alla concezione di consonanti e dissonanza e le chiamiamo CE qui.", "metrics": {"bleu_score": 26.73297021716996, "chrf_score": 66.44752762797488, "xcomet_score": 0.16170236468315125, "xcomet_qe_score": 0.16250744462013245, "metricx_score": 20.23553466796875, "metricx_qe_score": 18.280622482299805, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Rileviamo che il trasferimento del zero performance sul set di dati annotati è già molto migliore della casualità, con un AUC del 0,62.\"", "metrics": {"bleu_score": 15.300252291898866, "chrf_score": 37.91204770281744, "xcomet_score": 0.32881590723991394, "xcomet_qe_score": 0.48965537548065186, "metricx_score": 7.2137861251831055, "metricx_qe_score": 7.642627239227295, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, iterativamente affinando entrambe le attività, scopriamo che l'addestramento fine-tuning delle attività CE seguito da ulteriore addestramento fine-tuning nel dibattito consente un'ottima prestazione zero-shot.", "metrics": {"bleu_score": 7.0975974074380765, "chrf_score": 52.990237347602196, "xcomet_score": 0.65381920337677, "xcomet_qe_score": 0.5232611298561096, "metricx_score": 10.437603950500488, "metricx_qe_score": 11.437235832214355, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, è questo il modello che abbiamo utilizzato per avviare l'apprendimento attivo.", "metrics": {"bleu_score": 20.366120459184767, "chrf_score": 65.25189189352108, "xcomet_score": 0.890080451965332, "xcomet_qe_score": 0.9133394956588745, "metricx_score": 2.0646049976348877, "metricx_qe_score": 1.4248178005218506, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Ecco, determiniamo il metodo più adatto per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni.", "metrics": {"bleu_score": 62.742088451809494, "chrf_score": 75.1821996962765, "xcomet_score": 0.8602538108825684, "xcomet_qe_score": 0.8676018714904785, "metricx_score": 4.479414463043213, "metricx_qe_score": 4.664786338806152, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Il cumulatore accumula tutti i dati raccolti dalle annotazioni attive finora, mentre gli aggiornamenti iterativi addestrano il modello sul set di dati più recente raccolto.\"", "metrics": {"bleu_score": 19.23965056241428, "chrf_score": 56.27296833651487, "xcomet_score": 0.8759191036224365, "xcomet_qe_score": 0.8593417406082153, "metricx_score": 4.682820796966553, "metricx_qe_score": 3.623023271560669, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Sulle diverse strategie, abbiamo scoperto che il rendimento cumulativo è uguale o superiore a quello iterativo in generale.\" \"", "metrics": {"bleu_score": 13.336448006403915, "chrf_score": 49.1846334099014, "xcomet_score": 0.899788498878479, "xcomet_qe_score": 0.9014163017272949, "metricx_score": 2.749349355697632, "metricx_qe_score": 1.6771106719970703, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara PRC per selezionare soprattutto gli esempi che sono molto probabili di essere dissonanti in base al modello attuale in qualsiasi turno di errore.\"", "metrics": {"bleu_score": 39.46126031885468, "chrf_score": 63.380870304827674, "xcomet_score": 0.6261206865310669, "xcomet_qe_score": 0.6395009756088257, "metricx_score": 7.350147247314453, "metricx_qe_score": 6.514623641967773, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Confrontiamo questo con le altre strategie di AL comunemente utilizzate nella comunità.\"", "metrics": {"bleu_score": 26.01278440403793, "chrf_score": 66.31142015812122, "xcomet_score": 0.8977382183074951, "xcomet_qe_score": 0.9249485731124878, "metricx_score": 3.9676122665405273, "metricx_qe_score": 5.167628765106201, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Il nostro studio ha dimostrato che la strategia PRC proposta funziona meglio delle strategie di punta di stato, anche se la differenza è piccola.", "metrics": {"bleu_score": 17.098323692758388, "chrf_score": 54.978703704304586, "xcomet_score": 0.47405296564102173, "xcomet_qe_score": 0.4652705192565918, "metricx_score": 4.207502365112305, "metricx_qe_score": 4.1938323974609375, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Notiamo che il rendimento è significativamente inferiore per il caso casuale.\"", "metrics": {"bleu_score": 4.368583925857938, "chrf_score": 49.23003940784254, "xcomet_score": 0.8439989686012268, "xcomet_qe_score": 0.9207112789154053, "metricx_score": 1.9185905456542969, "metricx_qe_score": 1.3934292793273926, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In seguenti giri di AL con due strategie migliori, abbiamo migliorato la classificazione di distanza AUC 2,75, il che rappresenta il miglior rendimento che abbiamo raggiunto sul compito finora.", "metrics": {"bleu_score": 7.372712755641131, "chrf_score": 47.11300878934143, "xcomet_score": 0.559746265411377, "xcomet_qe_score": 0.4863249361515045, "metricx_score": 13.608726501464844, "metricx_qe_score": 13.035240173339844, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "\"Verifichiamo anche la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori.", "metrics": {"bleu_score": 80.3154665668484, "chrf_score": 90.1861236197396, "xcomet_score": 0.963739812374115, "xcomet_qe_score": 0.9701142311096191, "metricx_score": 0.7096811532974243, "metricx_qe_score": 0.8895469903945923, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Rileviamo che PRC presenta la percentuale più alta di dissonanza e si rivela essere la più adatta per la classe rara.", "metrics": {"bleu_score": 27.526990943873297, "chrf_score": 60.36077753162884, "xcomet_score": 0.9593455791473389, "xcomet_qe_score": 0.8463219404220581, "metricx_score": 2.3418540954589844, "metricx_qe_score": 2.73763108253479, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli annotatori ritengono anche gli esempi difficili.\"", "metrics": {"bleu_score": 15.439094769817242, "chrf_score": 70.23953334977591, "xcomet_score": 0.9530147314071655, "xcomet_qe_score": 0.9643254280090332, "metricx_score": 4.187000751495361, "metricx_qe_score": 6.1402997970581055, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, troviamo che la PRC è una strategia di apprendimento automatico per l'acquisizione di classi rare e l'avvio di AIL con compiti di apprendimento trasferito progettati in modo appropriato può aiutare significativamente.", "metrics": {"bleu_score": 24.160124931226363, "chrf_score": 59.928655380529136, "xcomet_score": 0.6437159180641174, "xcomet_qe_score": 0.6711723208427429, "metricx_score": 8.32299518585205, "metricx_qe_score": 7.347866535186768, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Anche scopriamo che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre gli aggiornamenti attivi all'interno del dominio beneficiano di aggiornamenti cumulativi.", "metrics": {"bleu_score": 41.16597006597376, "chrf_score": 73.46974640414152, "xcomet_score": 0.6187598705291748, "xcomet_qe_score": 0.7536971569061279, "metricx_score": 5.979384422302246, "metricx_qe_score": 6.0856475830078125, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i link ai nostri codici, al nostro insieme di dati e al nostro articolo.", "metrics": {"bleu_score": 29.989100815640114, "chrf_score": 56.572547942843045, "xcomet_score": 0.8847173452377319, "xcomet_qe_score": 0.8573150634765625, "metricx_score": 4.089531898498535, "metricx_qe_score": 2.620271921157837, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Siete liberi di contattarci se avete qualsiasi domanda.", "metrics": {"bleu_score": 6.27465531099474, "chrf_score": 42.293418950520426, "xcomet_score": 0.971239447593689, "xcomet_qe_score": 0.982652485370636, "metricx_score": 0.2370600700378418, "metricx_qe_score": 0.11015313863754272, "linguapy_score": [0, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "it", "output": "Grazie.", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 0.9992209672927856, "xcomet_qe_score": 1.0, "metricx_score": 0.03144232928752899, "metricx_qe_score": 0.23287612199783325, "linguapy_score": [0, "ITALIAN"]}}
