{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们的演示,介绍 DeepLing,这是一个新的语料库,用于在文档级别和句子级别进行德语文本分类。", "metrics": {"bleu_score": 37.731572416491474, "chrf_score": 30.65508493699498, "xcomet_score": 0.7470865249633789, "xcomet_qe_score": 0.7219537496566772, "metricx_score": 4.402573108673096, "metricx_qe_score": 4.8144145011901855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫雷吉娜·斯托登,我将带领大家完成演讲的第一部分。", "metrics": {"bleu_score": 26.94029471787363, "chrf_score": 19.72724174944068, "xcomet_score": 0.8817188739776611, "xcomet_qe_score": 0.8804879784584045, "metricx_score": 3.5425992012023926, "metricx_qe_score": 3.1117141246795654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们来定义文本简化。", "metrics": {"bleu_score": 42.43684507396328, "chrf_score": 35.13791852746409, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.15129098296165466, "metricx_qe_score": 0.22213901579380035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是指将文本进行调整,以提高特定目标群体(如有阅读障碍的人或非母语者)的文本理解度。", "metrics": {"bleu_score": 44.121512691883396, "chrf_score": 37.56483631119721, "xcomet_score": 0.9907724261283875, "xcomet_qe_score": 0.988682210445404, "metricx_score": 0.40270233154296875, "metricx_qe_score": 0.4110909700393677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要训练文本简化模型,我们需要文本的平行对,例如文档或句子。", "metrics": {"bleu_score": 63.65692807829749, "chrf_score": 56.326960621850496, "xcomet_score": 0.9876875877380371, "xcomet_qe_score": 0.8715510368347168, "metricx_score": 1.794748306274414, "metricx_qe_score": 2.5514302253723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,你可以看到一个复杂的德语句子及其翻译成普通语言的句子对。", "metrics": {"bleu_score": 41.830674449409514, "chrf_score": 38.119804959191825, "xcomet_score": 0.9677965641021729, "xcomet_qe_score": 0.9767904281616211, "metricx_score": 1.8677446842193604, "metricx_qe_score": 2.0935401916503906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可能有不同的技术,如你在示例中所见,例如词汇替换、从句简化、从句删除、重新排序或插入词语。", "metrics": {"bleu_score": 51.217823964088154, "chrf_score": 54.62402535724043, "xcomet_score": 0.7930775880813599, "xcomet_qe_score": 0.8025981187820435, "metricx_score": 1.849269986152649, "metricx_qe_score": 2.327641248703003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们提出了新的语料库,因为近年来现有语料库存在一些问题。", "metrics": {"bleu_score": 75.32362259513901, "chrf_score": 59.857795032896924, "xcomet_score": 0.7067148089408875, "xcomet_qe_score": 0.723280668258667, "metricx_score": 4.994051933288574, "metricx_qe_score": 5.092494487762451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法用来训练文本分类模型。", "metrics": {"bleu_score": 46.545561555683754, "chrf_score": 38.380193732024395, "xcomet_score": 0.9554253816604614, "xcomet_qe_score": 0.8425523042678833, "metricx_score": 1.8523234128952026, "metricx_qe_score": 1.0912556648254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三种模型都是自动对齐的,这意味着它们的对齐可能会出错。", "metrics": {"bleu_score": 56.57900401336323, "chrf_score": 49.21989510660065, "xcomet_score": 0.9864245653152466, "xcomet_qe_score": 0.986465573310852, "metricx_score": 0.5812429785728455, "metricx_qe_score": 0.7253639698028564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了新的语料库 DeepL,它分为两个子语料库:DeepL-APA 和 DeepL-Web。", "metrics": {"bleu_score": 52.174427464587126, "chrf_score": 30.981808130122555, "xcomet_score": 0.8450838327407837, "xcomet_qe_score": 0.7851374745368958, "metricx_score": 4.241174221038818, "metricx_qe_score": 3.3512024879455566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DeepL-APA 基于新闻文本。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 34.56600275350954, "xcomet_score": 0.8942672610282898, "xcomet_qe_score": 0.8356871604919434, "metricx_score": 2.057173728942871, "metricx_qe_score": 1.6808416843414307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在平面 APA 中,我们手动对齐了 483 份文件。结果产生", "metrics": {"bleu_score": 34.613358631405696, "chrf_score": 33.17658393851306, "xcomet_score": 0.4902595281600952, "xcomet_qe_score": 0.4991942048072815, "metricx_score": 6.471277236938477, "metricx_qe_score": 5.290740013122559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了大约 30,000-13,000 个平行句对。", "metrics": {"bleu_score": 53.90594848489677, "chrf_score": 61.41717036002697, "xcomet_score": 0.3344162702560425, "xcomet_qe_score": 0.2866007089614868, "metricx_score": 6.584763050079346, "metricx_qe_score": 6.373844623565674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 DeepL Web,该语料库包括不同的域,我们还将这些 750 个文档手动对齐,并使用自动对齐方法对齐。", "metrics": {"bleu_score": 36.629362204364355, "chrf_score": 30.8188520108089, "xcomet_score": 0.48653486371040344, "xcomet_qe_score": 0.37007859349250793, "metricx_score": 4.542907238006592, "metricx_qe_score": 4.180182933807373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共我们得到了 30,450 个句子对。", "metrics": {"bleu_score": 50.90187720040572, "chrf_score": 66.141212067792, "xcomet_score": 0.8584693670272827, "xcomet_qe_score": 0.8705366253852844, "metricx_score": 2.2608518600463867, "metricx_qe_score": 1.98568856716156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更多的分析,例如,对简化的类型进行了分析。", "metrics": {"bleu_score": 73.19493575257533, "chrf_score": 66.99831201269001, "xcomet_score": 0.8388397693634033, "xcomet_qe_score": 0.822625994682312, "metricx_score": 3.5927934646606445, "metricx_qe_score": 4.408881664276123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如你在这里看到的,圣经文本比新闻文本或语言学习文本更加简化。", "metrics": {"bleu_score": 62.779052760999654, "chrf_score": 60.63001479798801, "xcomet_score": 0.9753299951553345, "xcomet_qe_score": 0.9762578010559082, "metricx_score": 2.0526278018951416, "metricx_qe_score": 2.3525776863098145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在所有层面上,例如词汇简化、结构简化以及整体简化层面。", "metrics": {"bleu_score": 78.84718160380523, "chrf_score": 73.46662556445165, "xcomet_score": 0.9085150957107544, "xcomet_qe_score": 0.9533710479736328, "metricx_score": 0.5937659740447998, "metricx_qe_score": 0.6959583759307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 Deplane 语料库包含各种简化转换的高多样性。", "metrics": {"bleu_score": 36.02421475072493, "chrf_score": 30.015786162911084, "xcomet_score": 0.785490870475769, "xcomet_qe_score": 0.7205153703689575, "metricx_score": 3.938800573348999, "metricx_qe_score": 4.505681037902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 Deplane API 语料库中,我们有更多的重新排序和词添加,而 Deplane Web 语料库中则没有。", "metrics": {"bleu_score": 23.746239164015023, "chrf_score": 20.474179453240442, "xcomet_score": 0.5144294500350952, "xcomet_qe_score": 0.48488208651542664, "metricx_score": 3.383256673812866, "metricx_qe_score": 3.056619882583618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们有更多的重新表述。", "metrics": {"bleu_score": 28.143929378635036, "chrf_score": 23.732668297885688, "xcomet_score": 0.970145583152771, "xcomet_qe_score": 0.9660229682922363, "metricx_score": 1.9698774814605713, "metricx_qe_score": 2.236400604248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看可以用这组语料做什么。", "metrics": {"bleu_score": 51.15078115793242, "chrf_score": 46.60397993103116, "xcomet_score": 0.9329102039337158, "xcomet_qe_score": 0.883879542350769, "metricx_score": 0.5455710887908936, "metricx_qe_score": 0.9348636269569397, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,现在我将谈谈我们的数据集 DeepL 的用例。", "metrics": {"bleu_score": 41.05834569409794, "chrf_score": 29.666903732402712, "xcomet_score": 0.8957406282424927, "xcomet_qe_score": 0.8329657316207886, "metricx_score": 3.0961666107177734, "metricx_qe_score": 3.5805892944335938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个用例是,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 76.39843504088954, "chrf_score": 69.89813805582624, "xcomet_score": 0.9609298706054688, "xcomet_qe_score": 0.9635699987411499, "metricx_score": 0.5666430592536926, "metricx_qe_score": 0.6848822832107544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了许多对齐方法,但都是在机器翻译的背景下。 我们有两个平行文档,用不同的语言编写,我们想要提取这两个文档中句子的对齐。", "metrics": {"bleu_score": 40.43124624829003, "chrf_score": 35.22569180939508, "xcomet_score": 0.8827420473098755, "xcomet_qe_score": 0.8158526420593262, "metricx_score": 1.8527486324310303, "metricx_qe_score": 2.044140338897705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的使用案例中,我们试图从两个平行文档的句子中提取对齐,这两个文档使用相同的语言,内容", "metrics": {"bleu_score": 17.60524014825084, "chrf_score": 18.186100349809607, "xcomet_score": 0.6653949022293091, "xcomet_qe_score": 0.7057878971099854, "metricx_score": 6.52552604675293, "metricx_qe_score": 7.057318210601807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相同,但复杂程度不同。 现在我们有了手动对齐的句子数据集,我们可以将这些句子作为标准对齐,来评估一些建议的对齐方法。", "metrics": {"bleu_score": 50.1310627231278, "chrf_score": 39.42857620861066, "xcomet_score": 0.2369057536125183, "xcomet_qe_score": 0.01460825465619564, "metricx_score": 4.955043315887451, "metricx_qe_score": 5.1382036209106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对所提出的方法进行了一些改编,并在论文中发布了所有这些改编和运行我们实验的代码。", "metrics": {"bleu_score": 31.82010054630894, "chrf_score": 30.625924257107094, "xcomet_score": 0.9794794321060181, "xcomet_qe_score": 0.9806667566299438, "metricx_score": 1.6206588745117188, "metricx_qe_score": 1.1111364364624023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是 MassAlign 方法。", "metrics": {"bleu_score": 71.34126306233014, "chrf_score": 62.45789213640154, "xcomet_score": 0.9766328930854797, "xcomet_qe_score": 0.9674553275108337, "metricx_score": 1.2616236209869385, "metricx_qe_score": 1.5844156742095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你还可以在论文中找到运行此方法的代码,以便在你自己的文档中使用。", "metrics": {"bleu_score": 57.201874604927895, "chrf_score": 48.456802384777916, "xcomet_score": 0.9980545043945312, "xcomet_qe_score": 0.9873536825180054, "metricx_score": 0.48504534363746643, "metricx_qe_score": 0.665054440498352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是自动文本简化的情况。 通过对语言模型进行微调,使其能够从复杂的输入文本中生成简化的文本。", "metrics": {"bleu_score": 55.05583571321894, "chrf_score": 57.11049769262474, "xcomet_score": 0.995146632194519, "xcomet_qe_score": 0.9855426549911499, "metricx_score": 1.4103702306747437, "metricx_qe_score": 1.625478744506836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对两种不同的模型进行了微调。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.998328447341919, "xcomet_qe_score": 0.9891341924667358, "metricx_score": 0.33017244935035706, "metricx_qe_score": 0.5325762033462524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对 LongIMPART 模型进行了微调,以生成文档级别的简化。 我们还对正常基础导入进行了微调,以产生句子级别的简化。 ", "metrics": {"bleu_score": 45.078134621196455, "chrf_score": 37.14858406745351, "xcomet_score": 0.7407131195068359, "xcomet_qe_score": 0.7247002124786377, "metricx_score": 5.592203617095947, "metricx_qe_score": 5.809114933013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以找到所有的检查点,并可以在论文中查看我们实验的分数和评估矩阵的更多详细信息。", "metrics": {"bleu_score": 66.95900686562914, "chrf_score": 60.418344254941616, "xcomet_score": 0.9501402378082275, "xcomet_qe_score": 0.9391977787017822, "metricx_score": 1.7055466175079346, "metricx_qe_score": 2.0080208778381348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得比基线得分更好的得分。 我们将这些结果作为基准,作为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 59.698796637600466, "chrf_score": 54.70211792473584, "xcomet_score": 0.8412865400314331, "xcomet_qe_score": 0.8120672702789307, "metricx_score": 3.1130478382110596, "metricx_qe_score": 3.548323154449463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们希望在会议期间见到各位。", "metrics": {"bleu_score": 43.81255847528386, "chrf_score": 38.09382631369859, "xcomet_score": 0.9952495098114014, "xcomet_qe_score": 0.9995267391204834, "metricx_score": 0.8394423723220825, "metricx_qe_score": 0.48375821113586426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·斯皮尔科夫斯基,今天的演讲主题是协调的依赖结构。", "metrics": {"bleu_score": 13.836903384315104, "chrf_score": 10.934735122737823, "xcomet_score": 0.8156367540359497, "xcomet_qe_score": 0.738396167755127, "metricx_score": 1.5112981796264648, "metricx_qe_score": 1.2634234428405762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如你可能知道的那样,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 67.45409323444204, "chrf_score": 65.68710777408837, "xcomet_score": 0.9252237677574158, "xcomet_qe_score": 0.834908127784729, "metricx_score": 0.96164470911026, "metricx_qe_score": 0.9270702600479126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依存关系中,协调结构“丽莎、巴特和玛吉” 在这种情况下,第一个并列成分是整个协调结构的主语,即", "metrics": {"bleu_score": 33.290155349795384, "chrf_score": 25.055284924739485, "xcomet_score": 0.6340032815933228, "xcomet_qe_score": 0.6129498481750488, "metricx_score": 4.639820575714111, "metricx_qe_score": 4.186524868011475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "丽莎。 伊戈尔·米尔丘", "metrics": {"bleu_score": 3.435488317233919, "chrf_score": 1.1904761904761907, "xcomet_score": 0.16708336770534515, "xcomet_qe_score": 0.16363327205181122, "metricx_score": 5.057299613952637, "metricx_qe_score": 7.154168605804443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "克的意义文本理论也采用了类似的方法,其中整个坐标结构由第一个并列成分引导。因此,", "metrics": {"bleu_score": 41.82524688791684, "chrf_score": 31.54730217532391, "xcomet_score": 0.3156629800796509, "xcomet_qe_score": 0.38386422395706177, "metricx_score": 7.574151992797852, "metricx_qe_score": 6.9131975173950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的,", "metrics": {"bleu_score": 64.07117598241614, "chrf_score": 48.52481541959439, "xcomet_score": 0.9922106266021729, "xcomet_qe_score": 0.9595069885253906, "metricx_score": 0.41505956649780273, "metricx_qe_score": 0.48554089665412903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.953220009803772, "xcomet_qe_score": 0.8956713080406189, "metricx_score": 0.23392783105373383, "metricx_qe_score": 0.3477631211280823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们都突出了一个并列成分。", "metrics": {"bleu_score": 44.08231875586728, "chrf_score": 36.71943796943797, "xcomet_score": 0.8230667114257812, "xcomet_qe_score": 0.7453362941741943, "metricx_score": 3.0492124557495117, "metricx_qe_score": 3.720801830291748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,也有对称的坐标结构方法,例如布拉格方法、连词主导方法,", "metrics": {"bleu_score": 20.871006066235925, "chrf_score": 23.192807486998102, "xcomet_score": 0.6109923720359802, "xcomet_qe_score": 0.5766245722770691, "metricx_score": 5.656036853790283, "metricx_qe_score": 6.061213493347168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及布拉格依存树库,其中坐标结构由连词主导。", "metrics": {"bleu_score": 14.144949250152441, "chrf_score": 14.981026489960406, "xcomet_score": 0.5314608812332153, "xcomet_qe_score": 0.5310035347938538, "metricx_score": 5.9219841957092285, "metricx_qe_score": 6.397441864013672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从端到端获得了所有连接的依赖关系。", "metrics": {"bleu_score": 13.598345546333286, "chrf_score": 14.96165482733548, "xcomet_score": 0.7972280979156494, "xcomet_qe_score": 0.7939485311508179, "metricx_score": 3.3896985054016113, "metricx_qe_score": 2.3899598121643066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多头方法,例如在卡茨的词汇语法中使用。 也就是说,所有的从句都是协调结构的头部。", "metrics": {"bleu_score": 34.93264813921139, "chrf_score": 25.35406301288482, "xcomet_score": 0.4882921278476715, "xcomet_qe_score": 0.4887737035751343, "metricx_score": 5.5278639793396, "metricx_qe_score": 5.532583713531494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从主句(这里是“", "metrics": {"bleu_score": 11.479267900149548, "chrf_score": 10.127444377975433, "xcomet_score": 0.33156493306159973, "xcomet_qe_score": 0.13828939199447632, "metricx_score": 7.71675968170166, "metricx_qe_score": 6.651421546936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "爱”)到所有从句分别获得依存关系。这些是巴顿和麦基。 现在,", "metrics": {"bleu_score": 3.3868193354396166, "chrf_score": 3.3862079836865018, "xcomet_score": 0.1568344235420227, "xcomet_qe_score": 0.15559564530849457, "metricx_score": 14.649014472961426, "metricx_qe_score": 15.575092315673828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这篇论文的目的是为对称协调结构(如这两个)提出一个新的论点,而反对非对称协调结构(如这个)。", "metrics": {"bleu_score": 55.9909057904906, "chrf_score": 48.38065605125213, "xcomet_score": 0.8774096369743347, "xcomet_qe_score": 0.765643298625946, "metricx_score": 2.231189727783203, "metricx_qe_score": 2.7193286418914795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9524842500686646, "xcomet_qe_score": 0.9396543502807617, "metricx_score": 0.2438656985759735, "metricx_qe_score": 0.30582886934280396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论点是基于依存关系最小化原则的,我将通过这些例子来解释。", "metrics": {"bleu_score": 46.13085498645595, "chrf_score": 37.91904748984794, "xcomet_score": 0.9853261709213257, "xcomet_qe_score": 0.9832017421722412, "metricx_score": 0.38025274872779846, "metricx_qe_score": 1.0087800025939941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如你可能知道的那样,在英语中,直接宾语更喜欢靠近动词,而状语可能离得更远。", "metrics": {"bleu_score": 33.892135843206255, "chrf_score": 27.17459600980201, "xcomet_score": 0.8578556776046753, "xcomet_qe_score": 0.8365411758422852, "metricx_score": 1.598440170288086, "metricx_qe_score": 1.3331279754638672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,“我昨天读了它”就没问题,因为直接宾语“它”靠近动词。 “March read it yesterday” 要糟糕得多,", "metrics": {"bleu_score": 24.44997232563225, "chrf_score": 32.88392960219018, "xcomet_score": 0.5040383338928223, "xcomet_qe_score": 0.5587570667266846, "metricx_score": 8.966411590576172, "metricx_qe_score": 9.498695373535156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里动词和直接宾语之间有一个附加成分“yesterday”。", "metrics": {"bleu_score": 59.70228993860436, "chrf_score": 67.34568752489209, "xcomet_score": 0.8954708576202393, "xcomet_qe_score": 0.7635120749473572, "metricx_score": 1.6200889348983765, "metricx_qe_score": 2.405606746673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常重且非常长时,这种效果可能会有所缓解,", "metrics": {"bleu_score": 37.46631570740835, "chrf_score": 33.89242793282306, "xcomet_score": 0.8194013833999634, "xcomet_qe_score": 0.7112469673156738, "metricx_score": 2.730403184890747, "metricx_qe_score": 3.0330629348754883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这时它可以移动到副词之后的位置。", "metrics": {"bleu_score": 44.814895122401936, "chrf_score": 38.74551572396591, "xcomet_score": 0.9967325925827026, "xcomet_qe_score": 0.9957833290100098, "metricx_score": 1.1400083303451538, "metricx_qe_score": 2.063244104385376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有图示。因此,", "metrics": {"bleu_score": 19.070828081828378, "chrf_score": 15.448368769401977, "xcomet_score": 0.7094242572784424, "xcomet_qe_score": 0.8396911025047302, "metricx_score": 3.58841609954834, "metricx_qe_score": 2.052978277206421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都可以。玛奇今", "metrics": {"bleu_score": 43.36189090348677, "chrf_score": 39.25093648632726, "xcomet_score": 0.6665652990341187, "xcomet_qe_score": 0.5420122146606445, "metricx_score": 4.444333553314209, "metricx_qe_score": 3.775080919265747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "天读了这本关于BBC的绝对引人入胜的", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15547329187393188, "xcomet_qe_score": 0.15114711225032806, "metricx_score": 14.901019096374512, "metricx_qe_score": 15.895722389221191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书,这没问题。我们用这个长的名词短语代替了它。", "metrics": {"bleu_score": 44.57480180161217, "chrf_score": 40.073166203126554, "xcomet_score": 0.6385054588317871, "xcomet_qe_score": 0.45272257924079895, "metricx_score": 5.686967372894287, "metricx_qe_score": 6.140933990478516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但说“昨天读了这本关于蜜蜂的绝对迷人的书”也没问题。", "metrics": {"bleu_score": 2.4294859658214887, "chrf_score": 2.1505376344086025, "xcomet_score": 0.904410183429718, "xcomet_qe_score": 0.8273152112960815, "metricx_score": 4.739168643951416, "metricx_qe_score": 5.186524868011475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的原因是,这在语法上是可能的,因为尽管这个句子违反了一般语法原则,即直接宾语应该紧挨着动词, 它符合依赖长度最小化原则,该原则认为较短的依赖关系更受青睐。 因此,", "metrics": {"bleu_score": 24.938238455365994, "chrf_score": 25.1036833237679, "xcomet_score": 0.6365082263946533, "xcomet_qe_score": 0.7870498895645142, "metricx_score": 5.080934047698975, "metricx_qe_score": 2.9082367420196533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示了关键依赖项的长度,即在这两种结构中不恒定的依赖项。", "metrics": {"bleu_score": 44.17768687824478, "chrf_score": 38.52610070791294, "xcomet_score": 0.8959990739822388, "xcomet_qe_score": 0.7031272649765015, "metricx_score": 1.8541570901870728, "metricx_qe_score": 2.2207531929016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从“红色”到“长度为 7 个单词的形容词”有依赖关系,从“红色”到“长度为 4 个单词的书”也有依赖关系,所以总共是 11。", "metrics": {"bleu_score": 15.38664753334985, "chrf_score": 16.460620740559712, "xcomet_score": 0.6064949035644531, "xcomet_qe_score": 0.6845905780792236, "metricx_score": 6.713512420654297, "metricx_qe_score": 6.243332862854004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个组成部分时,这两个依赖项的总和变成了 6,对", "metrics": {"bleu_score": 57.782586818540814, "chrf_score": 58.175827162090684, "xcomet_score": 0.47156453132629395, "xcomet_qe_score": 0.5715517997741699, "metricx_score": 6.9089460372924805, "metricx_qe_score": 6.541945457458496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吧?所以不是 11,而是 6,要短得多。", "metrics": {"bleu_score": 23.287896954139942, "chrf_score": 26.486990156984696, "xcomet_score": 0.762802243232727, "xcomet_qe_score": 0.7858796119689941, "metricx_score": 2.966668128967285, "metricx_qe_score": 3.6763834953308105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这听起来还不错,", "metrics": {"bleu_score": 43.44547766853482, "chrf_score": 39.792575015558654, "xcomet_score": 0.9122409224510193, "xcomet_qe_score": 0.8333070278167725, "metricx_score": 0.8349249958992004, "metricx_qe_score": 0.527352511882782, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 63.8079654920782, "xcomet_score": 0.9115704298019409, "xcomet_qe_score": 0.963566780090332, "metricx_score": 0.5989556312561035, "metricx_qe_score": 0.9356817007064819, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9520323276519775, "xcomet_qe_score": 0.9441296458244324, "metricx_score": 0.25305208563804626, "metricx_qe_score": 0.31463509798049927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的Penn Treebank中提取了各种关于协调的统计数据,并查看了论文,了解为什么不使用大学依存关系。 这些统计数据证实了之前多次观察到的现象,即左侧连词往往较短,", "metrics": {"bleu_score": 49.581127883935004, "chrf_score": 41.23181133675384, "xcomet_score": 0.5431662797927856, "xcomet_qe_score": 0.4743751585483551, "metricx_score": 5.903696060180664, "metricx_qe_score": 6.08269739151001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“salt and pepper”而不是“pepper and salt”(以音节为单位)。", "metrics": {"bleu_score": 71.40573910176903, "chrf_score": 82.1059525178924, "xcomet_score": 0.8495800495147705, "xcomet_qe_score": 0.7926483750343323, "metricx_score": 1.7782009840011597, "metricx_qe_score": 4.373808860778809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还有一个顺带的观察,即这种倾向随着长度差的增加而增强。 因此", "metrics": {"bleu_score": 41.965971352197876, "chrf_score": 40.362317481148764, "xcomet_score": 0.713917076587677, "xcomet_qe_score": 0.732431173324585, "metricx_score": 3.9844961166381836, "metricx_qe_score": 2.959362506866455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个集合的长度差增大时,较短的集合更倾向于成为第一个集合。 但这篇", "metrics": {"bleu_score": 18.835450787281477, "chrf_score": 17.191621765931785, "xcomet_score": 0.3389665186405182, "xcomet_qe_score": 0.2615821957588196, "metricx_score": 12.310659408569336, "metricx_qe_score": 7.188465595245361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43940091133117676, "xcomet_qe_score": 0.12736809253692627, "metricx_score": 5.968459129333496, "metricx_qe_score": 17.508588790893555, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论文的新颖之处在于,我们观察到这种趋势只会在左侧的管家不在时发生。 所以,", "metrics": {"bleu_score": 40.67640752714396, "chrf_score": 37.37630855171149, "xcomet_score": 0.6628506183624268, "xcomet_qe_score": 0.5809805393218994, "metricx_score": 7.023463249206543, "metricx_qe_score": 6.032846450805664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以州长在左边。", "metrics": {"bleu_score": 18.702869706385968, "chrf_score": 12.95644003926289, "xcomet_score": 0.6368355751037598, "xcomet_qe_score": 0.7403826713562012, "metricx_score": 2.357534646987915, "metricx_qe_score": 1.274109125137329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,它是不存在的。荷马来了并打了个喷嚏。", "metrics": {"bleu_score": 23.81948610114928, "chrf_score": 14.475053107826987, "xcomet_score": 0.7591179013252258, "xcomet_qe_score": 0.7400804162025452, "metricx_score": 3.842972993850708, "metricx_qe_score": 4.149045944213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们有两个动词的协调,没有外部的外部管理者", "metrics": {"bleu_score": 52.78099780153099, "chrf_score": 50.9004211592157, "xcomet_score": 0.7393838167190552, "xcomet_qe_score": 0.7255825996398926, "metricx_score": 4.264715671539307, "metricx_qe_score": 5.539010047912598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对吧?因此,在这些情况下,左连词更喜欢较短,两个连词之间的差异越", "metrics": {"bleu_score": 14.703355394982738, "chrf_score": 17.136654863004676, "xcomet_score": 0.38019290566444397, "xcomet_qe_score": 0.3732222616672516, "metricx_score": 12.183259963989258, "metricx_qe_score": 8.000192642211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大,情况就越严重。 然而,当右翼政府执政时,如本例所示,左翼政府协调网络,这种效果就会消失。", "metrics": {"bleu_score": 13.207222359731448, "chrf_score": 10.383345115864834, "xcomet_score": 0.18081803619861603, "xcomet_qe_score": 0.1290428340435028, "metricx_score": 12.860440254211426, "metricx_qe_score": 15.348127365112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们展示了通过测量字符长度(第一列)、音节(中间列)和单词(右列)来进行测量。", "metrics": {"bleu_score": 7.077240855202588, "chrf_score": 12.9400932032188, "xcomet_score": 0.8115867376327515, "xcomet_qe_score": 0.7850971221923828, "metricx_score": 4.318117141723633, "metricx_qe_score": 5.088266372680664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将集中在右列。", "metrics": {"bleu_score": 10.229197414177778, "chrf_score": 8.88259526261586, "xcomet_score": 0.8319084644317627, "xcomet_qe_score": 0.7992298603057861, "metricx_score": 4.309466361999512, "metricx_qe_score": 5.417902946472168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在这里看到的是,当政府在左边时, 左连词的倾向是随着词数的绝对差值的增加而逐渐变短,在没有主词的情况下(例如句子的并列)也会出现这种情况,", "metrics": {"bleu_score": 11.94501754434097, "chrf_score": 13.179371566490527, "xcomet_score": 0.406375914812088, "xcomet_qe_score": 0.4472194015979767, "metricx_score": 8.905217170715332, "metricx_qe_score": 6.123560428619385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当主词在右边时,这种倾向就会消失。", "metrics": {"bleu_score": 28.235451410042966, "chrf_score": 24.19132807051477, "xcomet_score": 0.8763891458511353, "xcomet_qe_score": 0.7615366578102112, "metricx_score": 2.220151901245117, "metricx_qe_score": 5.108165740966797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了如何通过这种方式反驳不对称协调结构,如这两种,并支持对称结构,如这两种。", "metrics": {"bleu_score": 31.751071330699364, "chrf_score": 28.577644912106166, "xcomet_score": 0.8655382394790649, "xcomet_qe_score": 0.7950817346572876, "metricx_score": 1.6212165355682373, "metricx_qe_score": 1.7745693922042847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请参阅全文,了解完整的协议和论", "metrics": {"bleu_score": 12.94214832447352, "chrf_score": 13.024945351678022, "xcomet_score": 0.774604082107544, "xcomet_qe_score": 0.7353368997573853, "metricx_score": 4.939364433288574, "metricx_qe_score": 2.849998712539673, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "点,并在海报会议上与我们交流。", "metrics": {"bleu_score": 18.69300079996002, "chrf_score": 18.435091029949, "xcomet_score": 0.44823622703552246, "xcomet_qe_score": 0.34822437167167664, "metricx_score": 5.427823066711426, "metricx_qe_score": 5.14039945602417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是华盛顿大学的博士生张兵。", "metrics": {"bleu_score": 66.54377827941899, "chrf_score": 46.715408329754915, "xcomet_score": 0.8309023380279541, "xcomet_qe_score": 0.8699960708618164, "metricx_score": 0.4994679093360901, "metricx_qe_score": 0.5804169774055481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的研究,从预训练数据到语言模型,再到下游任务,追踪导致不公平的自然语言处理模型的政治偏见的轨迹。 因此", "metrics": {"bleu_score": 52.391016216351765, "chrf_score": 49.132271618191155, "xcomet_score": 0.7379183769226074, "xcomet_qe_score": 0.6325356960296631, "metricx_score": 4.045025825500488, "metricx_qe_score": 2.0553488731384277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是基于大规模网络爬虫数据进行训练的。", "metrics": {"bleu_score": 51.646155199069014, "chrf_score": 43.37261071189383, "xcomet_score": 0.9808223247528076, "xcomet_qe_score": 0.8966258764266968, "metricx_score": 1.413494348526001, "metricx_qe_score": 2.3810927867889404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 52.690039305566096, "chrf_score": 49.433523424384084, "xcomet_score": 0.7585622668266296, "xcomet_qe_score": 0.7133761644363403, "metricx_score": 2.00849986076355, "metricx_qe_score": 2.8807544708251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据 C4 语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等都在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 76.06189732114109, "chrf_score": 71.44045829273551, "xcomet_score": 0.8223600387573242, "xcomet_qe_score": 0.7409161329269409, "metricx_score": 1.2803410291671753, "metricx_qe_score": 1.517715334892273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了好坏参半的结果。", "metrics": {"bleu_score": 51.84341074271373, "chrf_score": 45.9505585860075, "xcomet_score": 0.9994064569473267, "xcomet_qe_score": 0.9961416721343994, "metricx_score": 0.5775511264801025, "metricx_qe_score": 0.6280213594436646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从多样化的视角学习,这有助于促进民主和多元化的观点。", "metrics": {"bleu_score": 22.068603375419535, "chrf_score": 20.54997866435543, "xcomet_score": 0.9062627553939819, "xcomet_qe_score": 0.9057695269584656, "metricx_score": 1.4693102836608887, "metricx_qe_score": 1.051499843597412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上具有社会偏见,可能会导致下游任务应用中的公平性问题。", "metrics": {"bleu_score": 63.219991558666706, "chrf_score": 54.49223831163541, "xcomet_score": 0.9912019968032837, "xcomet_qe_score": 0.972683310508728, "metricx_score": 0.9484655857086182, "metricx_qe_score": 1.357393741607666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了一个问题,即调查从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体如下: 首先,我们如何评估语言模型的政治倾向?预训练数据在这些政治偏见中可能扮演什么角色?", "metrics": {"bleu_score": 56.19253239906451, "chrf_score": 51.099693072890005, "xcomet_score": 0.8914632797241211, "xcomet_qe_score": 0.8742201328277588, "metricx_score": 1.6831421852111816, "metricx_qe_score": 1.737956166267395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在下游任务中的表现如何,以及这可能会导致 NLP 应用中的公平性问题。", "metrics": {"bleu_score": 72.37339069530044, "chrf_score": 68.55570558949387, "xcomet_score": 0.8842082023620605, "xcomet_qe_score": 0.7264425754547119, "metricx_score": 0.9950229525566101, "metricx_qe_score": 1.0110578536987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体来说,我们首先提出使用不同的提示格式来提示语言模型,使用政治问卷,例如政治罗盘测试。", "metrics": {"bleu_score": 51.420525895660106, "chrf_score": 41.10740082379334, "xcomet_score": 0.8327908515930176, "xcomet_qe_score": 0.7731541395187378, "metricx_score": 4.4494171142578125, "metricx_qe_score": 4.177224159240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够进行自动评估,并以政治科学文献为基础。 因此,", "metrics": {"bleu_score": 32.128549679729616, "chrf_score": 31.92769520414806, "xcomet_score": 0.6914038062095642, "xcomet_qe_score": 0.7012560963630676, "metricx_score": 4.189111232757568, "metricx_qe_score": 3.45890474319458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,首先,语言模型确实具有不同的政治倾向,", "metrics": {"bleu_score": 67.13783850074476, "chrf_score": 59.40293512107439, "xcomet_score": 0.9662960767745972, "xcomet_qe_score": 0.9748492240905762, "metricx_score": 0.7904109954833984, "metricx_qe_score": 0.8631473779678345, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘的四个象限。", "metrics": {"bleu_score": 54.08804419255529, "chrf_score": 45.88398515962971, "xcomet_score": 0.8560222387313843, "xcomet_qe_score": 0.7895867824554443, "metricx_score": 2.2291054725646973, "metricx_qe_score": 2.0888497829437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT-4 是所有模型中最自由的语言模型,GPT 系列通常比 BERT 系列及其变体更加社会自由。", "metrics": {"bleu_score": 56.008635583878785, "chrf_score": 53.50248286829474, "xcomet_score": 0.8490539193153381, "xcomet_qe_score": 0.7155382037162781, "metricx_score": 1.5143488645553589, "metricx_qe_score": 1.771782636642456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在调查语言模型的政治偏见在多大程度上是从训练数据中获得的。", "metrics": {"bleu_score": 59.01958512145046, "chrf_score": 55.60995701295046, "xcomet_score": 0.9400414824485779, "xcomet_qe_score": 0.9702931642532349, "metricx_score": 0.6767398118972778, "metricx_qe_score": 1.0650367736816406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过进一步在六个不同的党派媒体上对语言模型检查点进行预训练来进行受控实验,这些媒体分为新闻和社交媒体,并进一步按其政治倾向划分。", "metrics": {"bleu_score": 48.14396316692024, "chrf_score": 41.31671119063355, "xcomet_score": 0.8535947799682617, "xcomet_qe_score": 0.8585421442985535, "metricx_score": 2.5329463481903076, "metricx_qe_score": 2.824395179748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这样的党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了转变。", "metrics": {"bleu_score": 73.2913073696385, "chrf_score": 68.61467961977652, "xcomet_score": 0.9093190431594849, "xcomet_qe_score": 0.8242285251617432, "metricx_score": 1.3475593328475952, "metricx_qe_score": 2.0499370098114014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在左倾的 Reddit 语料库上进一步微调 RoBERTa,我们可以看到其偏好方面出现了显著的自由主义转变。 在政治偏见方面。", "metrics": {"bleu_score": 53.8986119438712, "chrf_score": 56.33892491032552, "xcomet_score": 0.4713355302810669, "xcomet_qe_score": 0.3768746852874756, "metricx_score": 6.0472002029418945, "metricx_qe_score": 5.723114490509033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图调查语言模型是否能够捕捉到现代社会中普遍存在的两极分化。", "metrics": {"bleu_score": 82.9853632201642, "chrf_score": 80.57633130526624, "xcomet_score": 0.9900856018066406, "xcomet_qe_score": 0.9851499795913696, "metricx_score": 0.7273260354995728, "metricx_qe_score": 0.9721791744232178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第 45 任总统之前和之后,", "metrics": {"bleu_score": 67.92821527216381, "chrf_score": 67.32489142107787, "xcomet_score": 0.8023656606674194, "xcomet_qe_score": 0.7165111899375916, "metricx_score": 1.8669928312301636, "metricx_qe_score": 2.4716956615448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并分别对两个不同的时间语料库进行语言模型的预训练。 我们可以", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 48.73971820565563, "xcomet_score": 0.6503628492355347, "xcomet_qe_score": 0.4019905626773834, "metricx_score": 6.742316246032715, "metricx_qe_score": 2.847900390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "看到,自 2017 年以来,语言模型的政治倾向普遍远离中间立场。", "metrics": {"bleu_score": 28.163872631254325, "chrf_score": 27.890303402149293, "xcomet_score": 0.9326661825180054, "xcomet_qe_score": 0.8593194484710693, "metricx_score": 1.943160057067871, "metricx_qe_score": 2.4994113445281982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能感知到我们社会的两极分化。", "metrics": {"bleu_score": 73.95409589871966, "chrf_score": 68.02674912298023, "xcomet_score": 0.900720477104187, "xcomet_qe_score": 0.8858751058578491, "metricx_score": 0.8898316025733948, "metricx_qe_score": 1.2464420795440674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们评估了不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现,这两个自然语言处理应用通常涉及语言模型,并且可能具有非常重要的影响。 因此,", "metrics": {"bleu_score": 53.37519327823503, "chrf_score": 54.8521311319616, "xcomet_score": 0.6828724145889282, "xcomet_qe_score": 0.6682369709014893, "metricx_score": 2.821302890777588, "metricx_qe_score": 1.341668963432312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,如果我们调查每个类别的表现,也就是说,如果我们将表现分为 在不同的人口统计学或政治倾向的新闻媒体中,我们可以看到一种模式,", "metrics": {"bleu_score": 56.48496160435068, "chrf_score": 48.87812398979403, "xcomet_score": 0.6884382963180542, "xcomet_qe_score": 0.6178784370422363, "metricx_score": 4.648677349090576, "metricx_qe_score": 5.170474052429199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测方面,左倾语言模型表现更好。 在检测针对社会少数群体的仇恨言论方面。 然而,它们在检测针对社会中更有权势群体的仇恨言论方面做得不够好。", "metrics": {"bleu_score": 65.36279031056515, "chrf_score": 64.46588397956157, "xcomet_score": 0.6893854737281799, "xcomet_qe_score": 0.6780555844306946, "metricx_score": 3.6403000354766846, "metricx_qe_score": 4.118786811828613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之亦然,语言模型在检测针对白人和男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+和其他少数族裔的仇恨言论方面表现较差。", "metrics": {"bleu_score": 65.62139810510043, "chrf_score": 65.72238282545428, "xcomet_score": 0.8765319585800171, "xcomet_qe_score": 0.9510598182678223, "metricx_score": 2.063965082168579, "metricx_qe_score": 2.51090931892395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在虚假新闻检测中也出现了类似的趋势,我们发现左倾语言模型更擅长检测来自其对立政治倾向的虚假信息,反之亦然。", "metrics": {"bleu_score": 54.568653096224516, "chrf_score": 46.15298391984435, "xcomet_score": 0.999359130859375, "xcomet_qe_score": 1.0, "metricx_score": 0.7330719232559204, "metricx_qe_score": 0.9631599187850952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多定性示例,以展示不同政治倾向的语言模型。 根据其社会类别,仇恨言论和虚假信息的预测是不同的。", "metrics": {"bleu_score": 57.49886788264852, "chrf_score": 51.58754320202701, "xcomet_score": 0.7726672291755676, "xcomet_qe_score": 0.816116452217102, "metricx_score": 3.3388800621032715, "metricx_qe_score": 3.211963653564453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有更多的例子,进一步突出了这一点。 这表明,语言模型的政治偏见问题非常紧迫。", "metrics": {"bleu_score": 40.40828654058772, "chrf_score": 35.17662806845609, "xcomet_score": 0.8827753067016602, "xcomet_qe_score": 0.8173242807388306, "metricx_score": 3.174407482147217, "metricx_qe_score": 3.6011557579040527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果将右对齐语言模型微调以应对仇恨言论或虚假信息等问题,并将其部署到流行的社交媒体平台上, 这意味着持有不同政治观点的人可能会被边缘化,针对少数群体的仇恨言论可能会肆无忌惮地蔓延。", "metrics": {"bleu_score": 45.88521192288595, "chrf_score": 39.935090342505895, "xcomet_score": 0.8675360679626465, "xcomet_qe_score": 0.7748541235923767, "metricx_score": 1.743118166923523, "metricx_qe_score": 1.6794309616088867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,让我们认识到并解决语言模型政治倾向导致的公平性问题。", "metrics": {"bleu_score": 45.28151518479496, "chrf_score": 48.35544135866163, "xcomet_score": 0.9943468570709229, "xcomet_qe_score": 0.9884442090988159, "metricx_score": 0.584775447845459, "metricx_qe_score": 0.8440238237380981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还想", "metrics": {"bleu_score": 7.128373858810283, "chrf_score": 8.892276422764226, "xcomet_score": 0.16536378860473633, "xcomet_qe_score": 0.13721060752868652, "metricx_score": 5.5712409019470215, "metricx_qe_score": 3.959547519683838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "强调,我们揭示了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 58.55858876483759, "chrf_score": 56.94197223056662, "xcomet_score": 0.8026045560836792, "xcomet_qe_score": 0.649064302444458, "metricx_score": 1.3240876197814941, "metricx_qe_score": 1.8028934001922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像是塞拉和卡律布狄斯之间的困境。", "metrics": {"bleu_score": 34.44082312472601, "chrf_score": 27.742969032524496, "xcomet_score": 0.7280997633934021, "xcomet_qe_score": 0.7367671728134155, "metricx_score": 2.369581460952759, "metricx_qe_score": 2.505845069885254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们不对语言模型训练数据中的政治观点进行消毒,偏见就会从预训练数据传播到语言模型,再传播到下游任务,最终导致公平性问题。", "metrics": {"bleu_score": 62.96129633243316, "chrf_score": 59.35980222752954, "xcomet_score": 0.8807969093322754, "xcomet_qe_score": 0.8577670454978943, "metricx_score": 2.6942062377929688, "metricx_qe_score": 3.321211814880371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们试图以某种方式进行清理,我们也会冒着审查或排除的风险,", "metrics": {"bleu_score": 53.30822624551696, "chrf_score": 49.86545051957309, "xcomet_score": 0.7993634343147278, "xcomet_qe_score": 0.7250880002975464, "metricx_score": 1.2672785520553589, "metricx_qe_score": 1.2419012784957886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定什么是真正中立的,应该保留语言多样性数据。", "metrics": {"bleu_score": 20.542655647982894, "chrf_score": 20.635165789180864, "xcomet_score": 0.874948263168335, "xcomet_qe_score": 0.8550441265106201, "metricx_score": 1.7781307697296143, "metricx_qe_score": 1.5337128639221191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电椅问题。", "metrics": {"bleu_score": 41.10545805678901, "chrf_score": 32.41938726231774, "xcomet_score": 0.833389401435852, "xcomet_qe_score": 0.8328114151954651, "metricx_score": 3.2023539543151855, "metricx_qe_score": 2.5869038105010986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9799641370773315, "xcomet_qe_score": 0.9886821508407593, "metricx_score": 0.17090822756290436, "metricx_qe_score": 0.24480992555618286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这就是我今天要说的全部内容。", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 72.97916540231482, "xcomet_score": 0.9915990829467773, "xcomet_qe_score": 0.9783182144165039, "metricx_score": 0.19653818011283875, "metricx_qe_score": 0.17678585648536682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9908943176269531, "xcomet_qe_score": 0.9947036504745483, "metricx_score": 0.0, "metricx_qe_score": 0.000984378159046173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是卡内基梅隆大学的博士一年级学生珍妮,今天我将介绍我们的研究成果《位置性:通过数据集和模型来描述设计偏见》。", "metrics": {"bleu_score": 46.657573497663144, "chrf_score": 32.16366294118921, "xcomet_score": 0.8064326047897339, "xcomet_qe_score": 0.8239312767982483, "metricx_score": 2.689019203186035, "metricx_qe_score": 2.7318806648254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的部分人员合作完成的,包括塞巴斯蒂安·桑蒂、罗南·拉布拉斯、卡塔里娜·阿里尼卡和马丁·萨普。", "metrics": {"bleu_score": 34.97659106253224, "chrf_score": 24.78840232608937, "xcomet_score": 0.6991548538208008, "xcomet_qe_score": 0.7135276794433594, "metricx_score": 1.6162989139556885, "metricx_qe_score": 0.9743531942367554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们从想象你在报社工作开始,你正在浏览新闻文章下的评论,试图删除有害内容。", "metrics": {"bleu_score": 45.27292121771306, "chrf_score": 39.296495001360405, "xcomet_score": 0.9792346954345703, "xcomet_qe_score": 0.9852195978164673, "metricx_score": 1.256402850151062, "metricx_qe_score": 1.0190446376800537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会转向像 Perspective API 这样的流行 API 来检测有害内容,如果你是 Carl Jones,Pers", "metrics": {"bleu_score": 23.82224948892272, "chrf_score": 41.44985656278695, "xcomet_score": 0.3796825408935547, "xcomet_qe_score": 0.4980866312980652, "metricx_score": 10.143784523010254, "metricx_qe_score": 9.454277038574219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pective API 就能正确检测出有害内容。", "metrics": {"bleu_score": 46.825687910244035, "chrf_score": 50.72332267442201, "xcomet_score": 0.8058532476425171, "xcomet_qe_score": 0.6648244857788086, "metricx_score": 7.755349636077881, "metricx_qe_score": 10.063108444213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于 Aditya Sharma 来说,情况并非如此,", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 79.30166572557877, "xcomet_score": 0.9277344942092896, "xcomet_qe_score": 0.9143953323364258, "metricx_score": 1.2387149333953857, "metricx_qe_score": 1.0009479522705078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为 Perspective API 对在印度语境中更为常见的冒犯性词语并不敏感。", "metrics": {"bleu_score": 55.70963651533937, "chrf_score": 61.927960302138594, "xcomet_score": 0.8079334497451782, "xcomet_qe_score": 0.6307607889175415, "metricx_score": 4.387728691101074, "metricx_qe_score": 4.394526481628418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们看到技术在不同人群之间的系统性表现差异。", "metrics": {"bleu_score": 65.99583028509838, "chrf_score": 58.507524204949455, "xcomet_score": 0.9839211702346802, "xcomet_qe_score": 0.9069797396659851, "metricx_score": 0.9384640455245972, "metricx_qe_score": 1.1112565994262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们刚才看到的设计偏见可能是由于自然语言处理研究人员和模型开发人员的立场造成的。", "metrics": {"bleu_score": 74.14694172369703, "chrf_score": 70.44070637116218, "xcomet_score": 0.9302242398262024, "xcomet_qe_score": 0.9318524599075317, "metricx_score": 0.5447558164596558, "metricx_qe_score": 0.6123955249786377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场是指人们由于其人口统计学、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 69.89969645773454, "chrf_score": 67.35712744555714, "xcomet_score": 0.911237895488739, "xcomet_qe_score": 0.9012629389762878, "metricx_score": 1.3497332334518433, "metricx_qe_score": 1.5792381763458252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术空间中。", "metrics": {"bleu_score": 84.99508493439812, "chrf_score": 78.52778542176075, "xcomet_score": 0.9886690378189087, "xcomet_qe_score": 0.8710610866546631, "metricx_score": 1.2322998046875, "metricx_qe_score": 2.1960177421569824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,位置性会影响研究过程及其结果,因为它会改变研究人员的决策。", "metrics": {"bleu_score": 45.86234391265969, "chrf_score": 41.09317335434529, "xcomet_score": 0.8329991102218628, "xcomet_qe_score": 0.8474774360656738, "metricx_score": 4.0240478515625, "metricx_qe_score": 2.968632698059082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问的一个问题是:数据集和模型是否具有位置性?", "metrics": {"bleu_score": 48.465254338121596, "chrf_score": 45.20744241143061, "xcomet_score": 0.9143497943878174, "xcomet_qe_score": 0.9893804788589478, "metricx_score": 2.5007424354553223, "metricx_qe_score": 0.7528940439224243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型和数据集本身具有人口统计学身份和生活经历,但它们确实汇集了真实人类的判断和意见,因此可以代表某些立场而非其他立场。", "metrics": {"bleu_score": 59.3758648003245, "chrf_score": 50.55044201022545, "xcomet_score": 0.8654447793960571, "xcomet_qe_score": 0.845778226852417, "metricx_score": 1.0689977407455444, "metricx_qe_score": 1.4251325130462646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前的研究表明,存在一些关于位置性的轶事证据,例如文化差距、模型和数据集,以及模型位置性的理论定义。", "metrics": {"bleu_score": 39.56553648416632, "chrf_score": 31.517241833474202, "xcomet_score": 0.7717310190200806, "xcomet_qe_score": 0.7926883101463318, "metricx_score": 5.326322555541992, "metricx_qe_score": 4.450563430786133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些作品并没有将最终用户与数据集和模型本身进行比较。 随着自然语言处理任务变得越来越主观和社会化,研究模型和数据集的偏见变得越来越重要。 由于并非所有决策都有记录,许多模型隐藏在 API 后面,因此很难描述这些位置是如何偏斜的。", "metrics": {"bleu_score": 55.381743745319895, "chrf_score": 49.759198994133406, "xcomet_score": 0.7412095069885254, "xcomet_qe_score": 0.7234852313995361, "metricx_score": 4.8720221519470215, "metricx_qe_score": 4.953593730926514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的偏差,我们实际上是将标注与真实用户进行比较,并与现有的数据集和模型进行比较。", "metrics": {"bleu_score": 51.70841372929298, "chrf_score": 50.953882729590624, "xcomet_score": 0.7724255323410034, "xcomet_qe_score": 0.8088784217834473, "metricx_score": 2.4205844402313232, "metricx_qe_score": 1.8746452331542969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过 NL Positionality 框架来实现这一点。", "metrics": {"bleu_score": 40.48411918659966, "chrf_score": 82.54574330661288, "xcomet_score": 0.8948768377304077, "xcomet_qe_score": 0.8793246150016785, "metricx_score": 0.5026324987411499, "metricx_qe_score": 0.7681472301483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架分两个主要步骤运行。", "metrics": {"bleu_score": 39.92039761549466, "chrf_score": 37.74958358657052, "xcomet_score": 0.8826372623443604, "xcomet_qe_score": 0.8817874193191528, "metricx_score": 0.4295850396156311, "metricx_qe_score": 0.4143719971179962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是重新标注数据集,使用多样化的标注者。", "metrics": {"bleu_score": 20.58122617930426, "chrf_score": 21.087963002840336, "xcomet_score": 0.838254988193512, "xcomet_qe_score": 0.8504423499107361, "metricx_score": 2.7034144401550293, "metricx_qe_score": 1.6062484979629517, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集标注者的人口统计数据,因为通常只有少数标注者标注每个实例,而且人口统计数据很少被收集和分享。", "metrics": {"bleu_score": 58.53315501897731, "chrf_score": 51.718038162444714, "xcomet_score": 0.8941260576248169, "xcomet_qe_score": 0.8268160820007324, "metricx_score": 1.6076347827911377, "metricx_qe_score": 1.4906132221221924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新注释数据,以便为每个实例获取多个注释,并获取丰富的人口统计数据。", "metrics": {"bleu_score": 49.15174201070153, "chrf_score": 46.27207690604702, "xcomet_score": 0.9681766033172607, "xcomet_qe_score": 0.8996384739875793, "metricx_score": 1.342566728591919, "metricx_qe_score": 1.5024967193603516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将按人口统计学分类的注释与模型和数据集进行比较,使用皮尔逊相关系数。 因此,我们的框架实际上与标注者不一致的文献不同,因为我们比较的是最终用户与模型和数据集、预测和标签,而不是仅仅关注标注者的一致性或建模标注者分布。", "metrics": {"bleu_score": 55.26603309025457, "chrf_score": 48.53010390495583, "xcomet_score": 0.6396832466125488, "xcomet_qe_score": 0.5439023971557617, "metricx_score": 3.599600315093994, "metricx_qe_score": 3.733307123184204, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要是通过 Lab-in-the-Wild 实现的,这是一个前 HCI 合作者的在线众包平台。", "metrics": {"bleu_score": 53.280669644815916, "chrf_score": 55.89804924089003, "xcomet_score": 0.7540755271911621, "xcomet_qe_score": 0.6976396441459656, "metricx_score": 3.6544713973999023, "metricx_qe_score": 4.311404705047607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,我们可以通过它招募到多样化的志愿者,而", "metrics": {"bleu_score": 49.91915899189359, "chrf_score": 60.85757727659207, "xcomet_score": 0.7873049974441528, "xcomet_qe_score": 0.6151920557022095, "metricx_score": 4.906452178955078, "metricx_qe_score": 5.006846904754639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像 MTurk 这样的平台主要是美国或印度的参与者。此外,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 47.2367447264668, "chrf_score": 52.2978291393002, "xcomet_score": 0.7980775833129883, "xcomet_qe_score": 0.7606530785560608, "metricx_score": 2.365054130554199, "metricx_qe_score": 2.118520975112915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 Lab in the Wild 上托管两项任务,其中一项是社会可接受性。其工作原理是参与者将阅读来自社会化学数据集的情境,然后写下该情境的社会可接受性。", "metrics": {"bleu_score": 45.41966340260334, "chrf_score": 45.42164601083307, "xcomet_score": 0.7936322689056396, "xcomet_qe_score": 0.6755334138870239, "metricx_score": 2.7335681915283203, "metricx_qe_score": 2.616551160812378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对城市的参与度,他们可以将自己的回答与人工智能和其他人的回答进行比较。", "metrics": {"bleu_score": 55.50672561792811, "chrf_score": 53.50418265859217, "xcomet_score": 0.8449501991271973, "xcomet_qe_score": 0.8045328855514526, "metricx_score": 4.300450801849365, "metricx_qe_score": 4.988851547241211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与Social Chemistry、Delphi和GPT-4进行了比较。", "metrics": {"bleu_score": 51.444011293042145, "chrf_score": 56.99972455776248, "xcomet_score": 0.9275140762329102, "xcomet_qe_score": 0.9338786602020264, "metricx_score": 2.3899526596069336, "metricx_qe_score": 2.081723690032959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们为毒性和仇恨言论检测任务复制了一个非常相似的设置,他们将从“DynaHate”读取一个实例,并写下他们是否认为这是仇恨言论的实例。", "metrics": {"bleu_score": 65.76876092899376, "chrf_score": 58.30968722895159, "xcomet_score": 0.8124512434005737, "xcomet_qe_score": 0.8284226059913635, "metricx_score": 2.548344612121582, "metricx_qe_score": 2.8968894481658936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与DynaHate、Perspective API、Rewire API、Hate-Roberta和GPT-4进行了比较。", "metrics": {"bleu_score": 59.522273967452335, "chrf_score": 82.18896996451195, "xcomet_score": 0.8962056636810303, "xcomet_qe_score": 0.8864272832870483, "metricx_score": 1.8608804941177368, "metricx_qe_score": 3.2535362243652344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们的研究涵盖了来自87个国家的1000多名注释者的16000多个注释。", "metrics": {"bleu_score": 64.40693842110244, "chrf_score": 65.94055943127152, "xcomet_score": 0.9346606135368347, "xcomet_qe_score": 0.9657259583473206, "metricx_score": 1.0291416645050049, "metricx_qe_score": 0.7663520574569702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们更有能力回答“NLP 数据集和模型最符合谁的利益”这个问题。", "metrics": {"bleu_score": 45.07925111378851, "chrf_score": 46.514241693473664, "xcomet_score": 0.9354255199432373, "xcomet_qe_score": 0.9135577082633972, "metricx_score": 0.594502329826355, "metricx_qe_score": 0.8479124307632446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 NLP 中存在立场问题。", "metrics": {"bleu_score": 40.304968802021214, "chrf_score": 35.000984443428685, "xcomet_score": 0.8869903087615967, "xcomet_qe_score": 0.8133516311645508, "metricx_score": 1.134203314781189, "metricx_qe_score": 1.4952366352081299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最适合英语国家。因此,", "metrics": {"bleu_score": 45.283344133049354, "chrf_score": 40.14999902733858, "xcomet_score": 0.8317779302597046, "xcomet_qe_score": 0.8208798170089722, "metricx_score": 3.9272406101226807, "metricx_qe_score": 2.032823324203491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 GPT-4 社会可接受性分析中,我们发现它最适合儒家和英语国家。我们还发现", "metrics": {"bleu_score": 49.21090638095398, "chrf_score": 46.36842936920354, "xcomet_score": 0.6640187501907349, "xcomet_qe_score": 0.6167589426040649, "metricx_score": 6.916693210601807, "metricx_qe_score": 2.9446780681610107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",DynaHate 也最适合英语国家。", "metrics": {"bleu_score": 24.181521090264727, "chrf_score": 31.76847807454639, "xcomet_score": 0.9655435085296631, "xcomet_qe_score": 0.8544464111328125, "metricx_score": 3.261939525604248, "metricx_qe_score": 4.117671012878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,G", "metrics": {"bleu_score": 3.0608499802737392, "chrf_score": 11.185065103677683, "xcomet_score": 0.16890284419059753, "xcomet_qe_score": 0.15427768230438232, "metricx_score": 22.57382583618164, "metricx_qe_score": 16.323944091796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PT-4 在社交可接受性任务中,与受过大学教育或研究生教育的人最为契合。 我们发现,对丹尼·海特的认同感也最强烈的群体是受过大学教育的人。", "metrics": {"bleu_score": 46.269002961250514, "chrf_score": 35.69906974939496, "xcomet_score": 0.6816608905792236, "xcomet_qe_score": 0.6445724964141846, "metricx_score": 5.569656848907471, "metricx_qe_score": 5.775773048400879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,总会有一些人被落下。", "metrics": {"bleu_score": 38.34861157885078, "chrf_score": 36.51816655561443, "xcomet_score": 0.7943623065948486, "xcomet_qe_score": 0.7791731357574463, "metricx_score": 2.5113227367401123, "metricx_qe_score": 2.706479549407959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,数据集和模型对非二元性别人群的友好程度不如对男性和女性的友好程度。", "metrics": {"bleu_score": 31.102772986341087, "chrf_score": 29.51102705507663, "xcomet_score": 0.7553631663322449, "xcomet_qe_score": 0.7897764444351196, "metricx_score": 3.158424139022827, "metricx_qe_score": 3.377284288406372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 GPT-4 社会可接受性任务以及 DynaHate 任务分析中都发现了这一点。", "metrics": {"bleu_score": 67.26181605942992, "chrf_score": 72.07744091122757, "xcomet_score": 0.881702184677124, "xcomet_qe_score": 0.9103665351867676, "metricx_score": 1.3666092157363892, "metricx_qe_score": 2.102283000946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既然自然语言处理中存在位置信息,我们能做些什么呢? 我们", "metrics": {"bleu_score": 32.23894033779449, "chrf_score": 31.799171073880235, "xcomet_score": 0.7141534090042114, "xcomet_qe_score": 0.7723636627197266, "metricx_score": 7.1039252281188965, "metricx_qe_score": 0.7990180253982544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对此有几点建议。", "metrics": {"bleu_score": 18.594002123233256, "chrf_score": 16.920511497175358, "xcomet_score": 0.9906258583068848, "xcomet_qe_score": 0.9556405544281006, "metricx_score": 0.20400917530059814, "metricx_qe_score": 0.18988454341888428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,在研究过程中记录所有相关的设计选择。其次", "metrics": {"bleu_score": 36.841823364959964, "chrf_score": 29.02129570374372, "xcomet_score": 0.9565026164054871, "xcomet_qe_score": 0.9442167282104492, "metricx_score": 1.9175782203674316, "metricx_qe_score": 0.539683997631073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从观点主义的角度进行自然语言处理研究。", "metrics": {"bleu_score": 63.832403259199225, "chrf_score": 59.5617956695461, "xcomet_score": 0.9675045013427734, "xcomet_qe_score": 0.9204754829406738, "metricx_score": 3.6684014797210693, "metricx_qe_score": 4.508540630340576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是与特定社区合作,建立专门的数据集和模型。", "metrics": {"bleu_score": 66.90849531405021, "chrf_score": 58.749384821223906, "xcomet_score": 0.893983006477356, "xcomet_qe_score": 0.8725626468658447, "metricx_score": 1.7117680311203003, "metricx_qe_score": 2.3575897216796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masaakane倡议。", "metrics": {"bleu_score": 55.2058197664637, "chrf_score": 40.497431877137515, "xcomet_score": 0.7221189737319946, "xcomet_qe_score": 0.6941564083099365, "metricx_score": 4.620077133178711, "metricx_qe_score": 6.082855224609375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们要强调的是,包容性的自然语言处理不仅仅是让所有", "metrics": {"bleu_score": 30.183854130936943, "chrf_score": 25.446088214531326, "xcomet_score": 0.6285057067871094, "xcomet_qe_score": 0.18802499771118164, "metricx_score": 5.135415554046631, "metricx_qe_score": 4.119807720184326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人服务。", "metrics": {"bleu_score": 39.03674453747003, "chrf_score": 36.76370111713883, "xcomet_score": 0.9485549926757812, "xcomet_qe_score": 0.9516949653625488, "metricx_score": 0.7394589781761169, "metricx_qe_score": 1.1236159801483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上是我们的演示内容", "metrics": {"bleu_score": 26.269098944241588, "chrf_score": 18.76984126984127, "xcomet_score": 0.9940835237503052, "xcomet_qe_score": 1.0, "metricx_score": 2.5202178955078125, "metricx_qe_score": 2.8809473514556885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",如果您想了解更多信息,请随时查看我们的仪表板,获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 61.571341865327085, "chrf_score": 55.730061401700226, "xcomet_score": 0.963660478591919, "xcomet_qe_score": 0.9385898113250732, "metricx_score": 1.8523284196853638, "metricx_qe_score": 1.4377421140670776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是复旦大学的徐媛。", "metrics": {"bleu_score": 34.68899992311541, "chrf_score": 21.559344773415813, "xcomet_score": 0.8838229179382324, "xcomet_qe_score": 0.8510637879371643, "metricx_score": 0.9028432369232178, "metricx_qe_score": 0.9795818328857422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我来介绍我们的工作,从大型语言模型中提取脚本知识,用于约束语言规划。", "metrics": {"bleu_score": 41.40890835499403, "chrf_score": 32.57474738386945, "xcomet_score": 0.8657780289649963, "xcomet_qe_score": 0.7772098779678345, "metricx_score": 1.8266074657440186, "metricx_qe_score": 2.286773920059204, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人们经常通过遵循逐步说明的形式来计划他们的行动,这些说明是以保证的脚本形式出现的。", "metrics": {"bleu_score": 15.05654929068627, "chrf_score": 17.415786415538992, "xcomet_score": 0.7597408294677734, "xcomet_qe_score": 0.7607612609863281, "metricx_score": 4.14467716217041, "metricx_qe_score": 4.00063943862915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以前的工作探索了语言模型来计划抽象目标的刻板活动,例如“烤蛋", "metrics": {"bleu_score": 25.7271376590534, "chrf_score": 21.742898394759816, "xcomet_score": 0.596945583820343, "xcomet_qe_score": 0.55555260181427, "metricx_score": 5.071175575256348, "metricx_qe_score": 2.7972311973571777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "糕”,并表明大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 59.20712841574226, "chrf_score": 55.7207268232873, "xcomet_score": 0.5206012725830078, "xcomet_qe_score": 0.503636360168457, "metricx_score": 8.61646842956543, "metricx_qe_score": 8.783255577087402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的工作主要集中在计划抽象目标的刻板活动上。", "metrics": {"bleu_score": 38.20472350900505, "chrf_score": 30.645442721529676, "xcomet_score": 0.8560402393341064, "xcomet_qe_score": 0.7794908285140991, "metricx_score": 2.9976260662078857, "metricx_qe_score": 2.8509790897369385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计划具有特定目标和约束的目标(例如,制作巧克力蛋糕)仍然研究不足。", "metrics": {"bleu_score": 37.03725358233026, "chrf_score": 31.39878641360551, "xcomet_score": 0.7690001726150513, "xcomet_qe_score": 0.7561479806900024, "metricx_score": 3.054006814956665, "metricx_qe_score": 2.963063955307007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们定义了受限语言规划问题。 它对规划目标施加了不同的约束。", "metrics": {"bleu_score": 65.97113464913205, "chrf_score": 61.15186532968385, "xcomet_score": 0.9651237726211548, "xcomet_qe_score": 0.8903321623802185, "metricx_score": 1.0536870956420898, "metricx_qe_score": 1.482526183128357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被不同的现实生活中的具体目标继承,这些目标具有多方面的约", "metrics": {"bleu_score": 31.568280470164282, "chrf_score": 32.15011917559352, "xcomet_score": 0.8256253004074097, "xcomet_qe_score": 0.8061807751655579, "metricx_score": 5.252721786499023, "metricx_qe_score": 2.981881856918335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "束。一个好的规划者应该编写出合理且符合约束的脚本。 在这篇论", "metrics": {"bleu_score": 32.09442950496563, "chrf_score": 28.59258676706901, "xcomet_score": 0.43228068947792053, "xcomet_qe_score": 0.42170462012290955, "metricx_score": 9.497580528259277, "metricx_qe_score": 9.762267112731934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文中,我们首先评估并改进了大型语言模型的受限语言规划能力。", "metrics": {"bleu_score": 79.28445008031721, "chrf_score": 74.45802951299237, "xcomet_score": 0.8535677194595337, "xcomet_qe_score": 0.840898871421814, "metricx_score": 1.0012627840042114, "metricx_qe_score": 1.0260059833526611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有关于特定目标的数据,因此无法确定我们的起点。 我们必须首先获得这些目标。", "metrics": {"bleu_score": 42.68304212315815, "chrf_score": 36.02218433522462, "xcomet_score": 0.8161338567733765, "xcomet_qe_score": 0.8613927960395813, "metricx_score": 3.465224027633667, "metricx_qe_score": 4.068201541900635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们通过使用指令CPT的人类反馈数据获取来扩展抽象目标,以实现多样化的约束。", "metrics": {"bleu_score": 29.037747307996295, "chrf_score": 23.256497914937917, "xcomet_score": 0.705222487449646, "xcomet_qe_score": 0.6239337921142578, "metricx_score": 6.314882278442383, "metricx_qe_score": 6.183530807495117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们抽取了 100 个特定的目标,并评估了从大型语言模型生成的脚本。", "metrics": {"bleu_score": 59.435931692508895, "chrf_score": 57.50160360778358, "xcomet_score": 0.9482376575469971, "xcomet_qe_score": 0.9441190958023071, "metricx_score": 1.9379863739013672, "metricx_qe_score": 3.0837597846984863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有的自然语言模型在为特定目标制定计划方面都取得了不令人满意的结果。", "metrics": {"bleu_score": 22.63748320405641, "chrf_score": 24.76382386852699, "xcomet_score": 0.9879679679870605, "xcomet_qe_score": 0.9840753078460693, "metricx_score": 0.7064133882522583, "metricx_qe_score": 0.8968906402587891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行了详细的分析,以调查为什么语言模型 4", "metrics": {"bleu_score": 30.183854130936943, "chrf_score": 26.284443817052516, "xcomet_score": 0.6183649301528931, "xcomet_qe_score": 0.6101893186569214, "metricx_score": 8.57739543914795, "metricx_qe_score": 7.090258598327637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果表明,生成的脚本的语义完整性是可以接受的,但对约束的忠实度无法保证。", "metrics": {"bleu_score": 54.35664470023705, "chrf_score": 45.375498123609006, "xcomet_score": 0.9330973625183105, "xcomet_qe_score": 0.9745227098464966, "metricx_score": 0.9796269536018372, "metricx_qe_score": 1.1441740989685059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了在Wikihow中定义的更细致的约束主题类别。", "metrics": {"bleu_score": 47.97526198446167, "chrf_score": 40.42089688801284, "xcomet_score": 0.8178094625473022, "xcomet_qe_score": 0.7747211456298828, "metricx_score": 1.288352608680725, "metricx_qe_score": 2.3867597579956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,不同类别的指令GPDs的规划性能差异很大。", "metrics": {"bleu_score": 40.37048225806873, "chrf_score": 27.594330523555172, "xcomet_score": 0.7692646980285645, "xcomet_qe_score": 0.7836943864822388, "metricx_score": 5.49594783782959, "metricx_qe_score": 5.083584785461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究表明,大型语言模型的输出质量存在高方差,导致性能不佳。", "metrics": {"bleu_score": 49.26316339022964, "chrf_score": 46.24679550747586, "xcomet_score": 0.8689990043640137, "xcomet_qe_score": 0.8566818237304688, "metricx_score": 1.42073655128479, "metricx_qe_score": 1.2169904708862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过度生成的 Z 过滤器的想法来提高生成质量。", "metrics": {"bleu_score": 48.14400258132249, "chrf_score": 39.91940949549645, "xcomet_score": 0.854270339012146, "xcomet_qe_score": 0.8203351497650146, "metricx_score": 5.117488861083984, "metricx_qe_score": 5.679431915283203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示了带有示例的约束类型,用于内部CPT,并根据种子抽象目标获得了特定目标。", "metrics": {"bleu_score": 44.780575075736294, "chrf_score": 35.726615844143396, "xcomet_score": 0.6868646144866943, "xcomet_qe_score": 0.6306092143058777, "metricx_score": 5.726298809051514, "metricx_qe_score": 5.91516637802124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指示 GPT 为特定目标生成案例脚本。", "metrics": {"bleu_score": 13.23007976179665, "chrf_score": 13.535238115930802, "xcomet_score": 0.7123028039932251, "xcomet_qe_score": 0.7316054105758667, "metricx_score": 4.609036445617676, "metricx_qe_score": 4.839098930358887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤器模型,用于选择可行的脚本。", "metrics": {"bleu_score": 40.80769034709354, "chrf_score": 34.16385335151534, "xcomet_score": 0.9784191846847534, "xcomet_qe_score": 0.9462252259254456, "metricx_score": 0.962805986404419, "metricx_qe_score": 1.0670464038848877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为指令 GPT 嵌入,并计算余弦相似性和相似性分数,以衡量语义相似性。", "metrics": {"bleu_score": 56.24292253610973, "chrf_score": 44.54486910473244, "xcomet_score": 0.7917343378067017, "xcomet_qe_score": 0.6809442043304443, "metricx_score": 3.894892930984497, "metricx_qe_score": 3.4845962524414062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们还会注意到包含目标约束关键字的脚本。我们只会", "metrics": {"bleu_score": 34.77547633382432, "chrf_score": 34.63107080087583, "xcomet_score": 0.611532986164093, "xcomet_qe_score": 0.46762076020240784, "metricx_score": 6.947514057159424, "metricx_qe_score": 3.5766773223876953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "保留目标得分在目标集中最高的脚本。", "metrics": {"bleu_score": 24.16749118641402, "chrf_score": 24.446587661294387, "xcomet_score": 0.7766955494880676, "xcomet_qe_score": 0.6447157263755798, "metricx_score": 4.998744487762451, "metricx_qe_score": 6.2429423332214355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,InstructGPT 可以生成更高质量的文本。", "metrics": {"bleu_score": 74.17090125042293, "chrf_score": 78.3800640576688, "xcomet_score": 0.8737577795982361, "xcomet_qe_score": 0.8305397033691406, "metricx_score": 1.7321155071258545, "metricx_qe_score": 1.7586952447891235, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法大大提高了可规划性,无论是在语义完整性还是对约束的忠实度方面。", "metrics": {"bleu_score": 49.97770345042087, "chrf_score": 41.492144754516964, "xcomet_score": 0.9252564907073975, "xcomet_qe_score": 0.9638127088546753, "metricx_score": 1.3199169635772705, "metricx_qe_score": 1.786754846572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型部署成本高昂,因此赋予小型和专业化模型语言规划能力至关重要。为此", "metrics": {"bleu_score": 41.91564005038255, "chrf_score": 34.000616434826966, "xcomet_score": 0.8744763135910034, "xcomet_qe_score": 0.84341961145401, "metricx_score": 3.0177721977233887, "metricx_qe_score": 0.38048055768013, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",创建数据集是一个重要步骤。", "metrics": {"bleu_score": 50.97786118996648, "chrf_score": 47.40354612654772, "xcomet_score": 0.979435920715332, "xcomet_qe_score": 0.9691230058670044, "metricx_score": 3.33990478515625, "metricx_qe_score": 1.6982684135437012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,先前的研究并不允许为特定目标进行规划,而手动数据集注释成本高昂。", "metrics": {"bleu_score": 41.57280016611217, "chrf_score": 33.31227016710888, "xcomet_score": 0.9168398380279541, "xcomet_qe_score": 0.9662255048751831, "metricx_score": 1.5802570581436157, "metricx_qe_score": 2.0888798236846924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的理念,从大型语言模型中蒸馏约束语言规划数据集。", "metrics": {"bleu_score": 50.87096779647018, "chrf_score": 41.99079684198711, "xcomet_score": 0.8704352378845215, "xcomet_qe_score": 0.7601776123046875, "metricx_score": 3.690687656402588, "metricx_qe_score": 3.066722869873047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将应用我们的方法来构建一个受限语言规划的数据集,称为 CodeScript。", "metrics": {"bleu_score": 50.87573747155849, "chrf_score": 50.199584557722666, "xcomet_score": 0.8255611658096313, "xcomet_qe_score": 0.7993133068084717, "metricx_score": 1.323799967765808, "metricx_qe_score": 2.1956701278686523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了 55,000 个带有脚本的特定目标。", "metrics": {"bleu_score": 29.62789157394226, "chrf_score": 42.44778530648096, "xcomet_score": 0.8415722846984863, "xcomet_qe_score": 0.8151769042015076, "metricx_score": 1.7334803342819214, "metricx_qe_score": 1.5967118740081787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试集的质量,我们要求云工作者找出并修改不正确的样本。", "metrics": {"bleu_score": 46.88330396534786, "chrf_score": 38.0911920374135, "xcomet_score": 0.8653524518013, "xcomet_qe_score": 0.8745985627174377, "metricx_score": 1.708664059638977, "metricx_qe_score": 1.260321021080017, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了 CodeScript 的约束分布。", "metrics": {"bleu_score": 58.33510584342546, "chrf_score": 72.24911196939357, "xcomet_score": 0.9580291509628296, "xcomet_qe_score": 0.8780146837234497, "metricx_score": 1.2336090803146362, "metricx_qe_score": 2.6480953693389893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 CodeScript 在生成的特定目标中表现出高度的多样性。", "metrics": {"bleu_score": 56.53615736486091, "chrf_score": 57.442392131009356, "xcomet_score": 0.9383082389831543, "xcomet_qe_score": 0.9154263138771057, "metricx_score": 2.2305004596710205, "metricx_qe_score": 3.8879175186157227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用 CodeScript,我们可以训练更小但更专业的模型来进行约束语言规划。", "metrics": {"bleu_score": 29.12919968412613, "chrf_score": 30.75020440336044, "xcomet_score": 0.8038716316223145, "xcomet_qe_score": 0.7321294546127319, "metricx_score": 2.126659393310547, "metricx_qe_score": 2.100184917449951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,T5-LM-11B在代码速率方面可以生成比大多数大型语言模型更好的脚本,这表明在适当的数据集上进行训练时,较小的模型可以超越较大的模型。", "metrics": {"bleu_score": 50.177093665703396, "chrf_score": 38.48265087755675, "xcomet_score": 0.7518457174301147, "xcomet_qe_score": 0.737680196762085, "metricx_score": 6.917964935302734, "metricx_qe_score": 7.131861686706543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了约束语言规划问题,", "metrics": {"bleu_score": 45.090228049186024, "chrf_score": 36.29628526827658, "xcomet_score": 0.8723708391189575, "xcomet_qe_score": 0.8503605723381042, "metricx_score": 2.6160809993743896, "metricx_qe_score": 3.5919134616851807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估了大型语言模型的约束语言规划能力,并为大型语言模型开发了过度生成过滤方法。", "metrics": {"bleu_score": 44.47852693175439, "chrf_score": 36.68989848207995, "xcomet_score": 0.7899580001831055, "xcomet_qe_score": 0.777092456817627, "metricx_score": 2.6570940017700195, "metricx_qe_score": 3.704967498779297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型来生成高质量的 SQL 数据集,用于约束语言规划。", "metrics": {"bleu_score": 46.81995735261491, "chrf_score": 30.947746673950665, "xcomet_score": 0.7879612445831299, "xcomet_qe_score": 0.7703708410263062, "metricx_score": 5.752364158630371, "metricx_qe_score": 6.525276184082031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望约束数据集能够成为推动语言规划研究的有价值资源。", "metrics": {"bleu_score": 46.09056322258576, "chrf_score": 32.983358287257715, "xcomet_score": 0.8728212118148804, "xcomet_qe_score": 0.8255324363708496, "metricx_score": 3.7785024642944336, "metricx_qe_score": 3.821000337600708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中找到更多有关代码脚本的详细信息。", "metrics": {"bleu_score": 58.41761860902646, "chrf_score": 44.05373943154192, "xcomet_score": 0.8372069001197815, "xcomet_qe_score": 0.8072138428688049, "metricx_score": 2.8639333248138428, "metricx_qe_score": 3.1146669387817383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫朱恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8176854848861694, "xcomet_qe_score": 0.8306083083152771, "metricx_score": 0.055027518421411514, "metricx_qe_score": 0.18194499611854553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的论文《康奈尔 2003 命名实体标注器在 2023 年是否仍然有效?》", "metrics": {"bleu_score": 69.11767741252595, "chrf_score": 67.70522907985814, "xcomet_score": 0.8216224908828735, "xcomet_qe_score": 0.7484220266342163, "metricx_score": 2.3213376998901367, "metricx_qe_score": 2.2630348205566406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。让我们开始吧。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 95.15349630471859, "xcomet_score": 0.9835532903671265, "xcomet_qe_score": 0.9856215715408325, "metricx_score": 0.7622692584991455, "metricx_qe_score": 1.1052849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了使用命名实体识别任务(NER任务)的泛化问题。", "metrics": {"bleu_score": 64.65767005913993, "chrf_score": 56.91635654775947, "xcomet_score": 0.9524506330490112, "xcomet_qe_score": 0.8477317094802856, "metricx_score": 1.350928544998169, "metricx_qe_score": 3.228980541229248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们注意到,模型已经使用 Conll-2003 来开发 NER 近 20 年了。这自然引发了几个问题。", "metrics": {"bleu_score": 17.497734643874736, "chrf_score": 26.523733825639045, "xcomet_score": 0.7664490938186646, "xcomet_qe_score": 0.7079517245292664, "metricx_score": 5.3543381690979, "metricx_qe_score": 5.007175922393799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据?", "metrics": {"bleu_score": 53.12583871630397, "chrf_score": 42.12696617108382, "xcomet_score": 0.9173398017883301, "xcomet_qe_score": 0.9163376688957214, "metricx_score": 0.46578866243362427, "metricx_qe_score": 0.3853972554206848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新标签时,需要什么才能实现良好的泛化?", "metrics": {"bleu_score": 37.30990460868691, "chrf_score": 31.668034724822093, "xcomet_score": 0.920039176940918, "xcomet_qe_score": 0.9384137988090515, "metricx_score": 0.7249643802642822, "metricx_qe_score": 0.6121730804443359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与此同时,如果我们确实观察到泛化能力差,那么是什么导致这些模型的性能下降呢?", "metrics": {"bleu_score": 54.107870570365456, "chrf_score": 48.76344649797631, "xcomet_score": 0.9929018020629883, "xcomet_qe_score": 0.9818623065948486, "metricx_score": 0.9604604244232178, "metricx_qe_score": 0.9526594877243042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了康奈尔++数据集。这是一个数据集", "metrics": {"bleu_score": 36.70170552499933, "chrf_score": 33.228974164032415, "xcomet_score": 0.7456526756286621, "xcomet_qe_score": 0.7386753559112549, "metricx_score": 3.8588318824768066, "metricx_qe_score": 2.833972454071045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们从2020年的路透社新闻中收集,并使用与康奈尔2003年相同的注释指南进行注释。", "metrics": {"bleu_score": 46.909081986309005, "chrf_score": 42.427797541821306, "xcomet_score": 0.7714889049530029, "xcomet_qe_score": 0.7280378341674805, "metricx_score": 5.630740642547607, "metricx_qe_score": 6.2339091300964355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在Cornell 2003上对20多个模型进行了微调。", "metrics": {"bleu_score": 56.54859472485412, "chrf_score": 48.29727248533241, "xcomet_score": 0.9152361154556274, "xcomet_qe_score": 0.8916632533073425, "metricx_score": 4.083982467651367, "metricx_qe_score": 3.6363165378570557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Cornell 03测试集和Cornell++测试集上对它们进行了评估。", "metrics": {"bleu_score": 53.611312694955046, "chrf_score": 51.2346865168862, "xcomet_score": 0.8301753997802734, "xcomet_qe_score": 0.861535906791687, "metricx_score": 4.178317070007324, "metricx_qe_score": 3.833644390106201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了F1的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 71.76532607217811, "chrf_score": 69.17281761409991, "xcomet_score": 0.9953742027282715, "xcomet_qe_score": 0.9882276058197021, "metricx_score": 0.5350840091705322, "metricx_qe_score": 1.1664762496948242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的泛化需要什么呢?", "metrics": {"bleu_score": 39.93614954790575, "chrf_score": 31.717184563651685, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.30326253175735474, "metricx_qe_score": 0.3970264792442322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要因素是必需的。", "metrics": {"bleu_score": 33.57306484097324, "chrf_score": 29.278101949422346, "xcomet_score": 0.9939303398132324, "xcomet_qe_score": 0.9405454993247986, "metricx_score": 0.7070783376693726, "metricx_qe_score": 1.2323414087295532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现,通常来说,Transformer 模型能够更好地对新数据进行泛化。", "metrics": {"bleu_score": 38.114538012022926, "chrf_score": 50.451666279944554, "xcomet_score": 0.971595823764801, "xcomet_qe_score": 0.9330682158470154, "metricx_score": 1.4352582693099976, "metricx_qe_score": 3.2761378288269043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常情况下,模型越大,泛化能力越强。", "metrics": {"bleu_score": 16.133948681475328, "chrf_score": 16.244220530681023, "xcomet_score": 0.9969384670257568, "xcomet_qe_score": 0.9846937656402588, "metricx_score": 0.34818410873413086, "metricx_qe_score": 0.5096091032028198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们都知道微调示例的数量直接影响下游任务的性能。", "metrics": {"bleu_score": 70.75192866296081, "chrf_score": 65.03350936014301, "xcomet_score": 0.9707472324371338, "xcomet_qe_score": 0.8968411684036255, "metricx_score": 1.4634007215499878, "metricx_qe_score": 1.281935691833496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调示例实际上也会导致更好的泛化。", "metrics": {"bleu_score": 67.5138946452567, "chrf_score": 58.47351825876147, "xcomet_score": 0.9816000461578369, "xcomet_qe_score": 0.8554470539093018, "metricx_score": 0.6346933841705322, "metricx_qe_score": 0.8460352420806885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来的问题是,为什么有些模型的性能会下降? 我们有两种假设。", "metrics": {"bleu_score": 30.7596799720662, "chrf_score": 27.520162120010376, "xcomet_score": 0.9895644187927246, "xcomet_qe_score": 0.9936282634735107, "metricx_score": 0.9990197420120239, "metricx_qe_score": 0.9620503783226013, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是自适应过拟合,即由于反复使用同一测试集而导致的过拟合,这通常表现为在新测试集上回报减少。", "metrics": {"bleu_score": 46.307690150417095, "chrf_score": 39.74043492550129, "xcomet_score": 0.9074041843414307, "xcomet_qe_score": 0.8863880634307861, "metricx_score": 3.0009918212890625, "metricx_qe_score": 3.71616268157959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间差距越来越大而导致的性能下降。", "metrics": {"bleu_score": 59.75281862052228, "chrf_score": 55.53728510769358, "xcomet_score": 0.964687705039978, "xcomet_qe_score": 0.888350784778595, "metricx_score": 1.4822864532470703, "metricx_qe_score": 2.0078001022338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右图中看到,红色的最佳拟合线的斜率大于 1。", "metrics": {"bleu_score": 43.3400185682041, "chrf_score": 35.293456774964014, "xcomet_score": 0.8770372271537781, "xcomet_qe_score": 0.827857494354248, "metricx_score": 1.3399560451507568, "metricx_qe_score": 1.6536623239517212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在C++上进行的每一项改进都比在C++03上进行的改进要多,这意味着没有边际收益递减。", "metrics": {"bleu_score": 29.319656614184318, "chrf_score": 25.54124931950402, "xcomet_score": 0.6285911202430725, "xcomet_qe_score": 0.697557270526886, "metricx_score": 8.878042221069336, "metricx_qe_score": 9.053651809692383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下没有观察到自适应过拟合。", "metrics": {"bleu_score": 74.93731939490364, "chrf_score": 69.43707675795987, "xcomet_score": 0.9009255766868591, "xcomet_qe_score": 0.9129918217658997, "metricx_score": 1.1392955780029297, "metricx_qe_score": 1.6724114418029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间旅行呢?", "metrics": {"bleu_score": 14.575161396875705, "chrf_score": 13.743309545049065, "xcomet_score": 0.9022859334945679, "xcomet_qe_score": 0.8547243475914001, "metricx_score": 2.8293211460113525, "metricx_qe_score": 2.6032752990722656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了应对时间漂移,我们做了一个实验,重新训练或继续预训练一些模型,使用更近期的数据,我们发现随着时间间隔的增大,性能会下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 56.841219589720446, "chrf_score": 51.74682109866274, "xcomet_score": 0.8767719268798828, "xcomet_qe_score": 0.8694809675216675, "metricx_score": 1.8409347534179688, "metricx_qe_score": 2.1377391815185547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了获得良好的泛化性能,我们需要更好的模型架构、更大的模型尺寸以及更多的微调示例。", "metrics": {"bleu_score": 69.53897731825985, "chrf_score": 64.35667361548757, "xcomet_score": 0.9343048334121704, "xcomet_qe_score": 0.9705406427383423, "metricx_score": 0.565372109413147, "metricx_qe_score": 0.43385398387908936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些目标是相辅相成的,我们不能只拥有其中一个,而忽略其他。", "metrics": {"bleu_score": 33.231639328339604, "chrf_score": 27.48698012333551, "xcomet_score": 0.9118349552154541, "xcomet_qe_score": 0.9113683700561523, "metricx_score": 1.4825695753097534, "metricx_qe_score": 1.847576379776001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与此同时,我们还发现,这里的性能下降是由时间漂移引起的,而令人惊讶的是,它并不是由自适应过拟合引起的,尽管Cono 2003 已经使用了 20 多年。", "metrics": {"bleu_score": 62.11177612992362, "chrf_score": 57.089865883063226, "xcomet_score": 0.6881000399589539, "xcomet_qe_score": 0.5970693826675415, "metricx_score": 4.5664777755737305, "metricx_qe_score": 4.994932651519775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,回到我们在论文标题中提出的问题,2003 年的康奈尔标记器在 2023 年是否仍然有效?", "metrics": {"bleu_score": 60.58003968113007, "chrf_score": 58.31468935598676, "xcomet_score": 0.7199200391769409, "xcomet_qe_score": 0.7897583246231079, "metricx_score": 1.9902409315109253, "metricx_qe_score": 1.796575903892517, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案是肯定的。", "metrics": {"bleu_score": 67.80814773941113, "chrf_score": 55.183072785867324, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.32976317405700684, "metricx_qe_score": 0.7012989521026611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能促进更多关于如何改进模型泛化的研究。", "metrics": {"bleu_score": 59.12147721562994, "chrf_score": 50.17253028767234, "xcomet_score": 0.9986436367034912, "xcomet_qe_score": 1.0, "metricx_score": 0.369063138961792, "metricx_qe_score": 0.5510718822479248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.06656905707533, "chrf_score": 52.83902988977493, "xcomet_score": 0.9870487451553345, "xcomet_qe_score": 0.9707439541816711, "metricx_score": 0.2669318616390228, "metricx_qe_score": 0.2746126055717468, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我要谈谈我们在实体选择中解决间接引用表达式的工作,我们引入了 AltEntities Scorers。", "metrics": {"bleu_score": 19.197038716259406, "chrf_score": 33.52054732783456, "xcomet_score": 0.7830684185028076, "xcomet_qe_score": 0.7464510202407837, "metricx_score": 6.338832855224609, "metricx_qe_score": 6.491292476654053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫贾瓦德·侯赛尼,这是与菲利普·拉德林斯基、西尔维娅·帕里蒂和安妮·刘易斯的联合工作。", "metrics": {"bleu_score": 3.031060490822769, "chrf_score": 3.526248868583251, "xcomet_score": 0.9592309594154358, "xcomet_qe_score": 0.9632938504219055, "metricx_score": 2.4349288940429688, "metricx_qe_score": 1.9217116832733154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时的语言。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9999474287033081, "xcomet_qe_score": 0.9556578397750854, "metricx_score": 0.6249333620071411, "metricx_qe_score": 0.9737254977226257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑一下这个替代问题:", "metrics": {"bleu_score": 12.982679446701692, "chrf_score": 15.23876404494382, "xcomet_score": 0.8834202885627747, "xcomet_qe_score": 0.8817586898803711, "metricx_score": 0.3517453670501709, "metricx_qe_score": 0.27517277002334595, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“你是想要‘Easy on Me’还是‘I Got a Feeling’?”", "metrics": {"bleu_score": 51.086369427314914, "chrf_score": 74.82249547356611, "xcomet_score": 0.8871285915374756, "xcomet_qe_score": 0.8479768633842468, "metricx_score": 4.371689796447754, "metricx_qe_score": 6.992214679718018, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,用户想要在这两首歌中做出选择。", "metrics": {"bleu_score": 33.982199869239906, "chrf_score": 29.377880671520177, "xcomet_score": 0.9915716648101807, "xcomet_qe_score": 0.9661687612533569, "metricx_score": 0.6661782264709473, "metricx_qe_score": 0.5806587338447571, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是直接引用。例如,通过说出歌曲的名字《Easy on Me》或它的位置(第一首)。", "metrics": {"bleu_score": 30.682912769804382, "chrf_score": 35.43622710443205, "xcomet_score": 0.7403769493103027, "xcomet_qe_score": 0.6836712956428528, "metricx_score": 2.5515518188476562, "metricx_qe_score": 3.1451504230499268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但有时,间接引语更适合进行更自然的对话。例如,", "metrics": {"bleu_score": 57.70362357478205, "chrf_score": 57.05757660075243, "xcomet_score": 0.8238284587860107, "xcomet_qe_score": 0.8009167909622192, "metricx_score": 0.8491620421409607, "metricx_qe_score": 0.7231245040893555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户想不起歌曲的名字时。", "metrics": {"bleu_score": 13.119387134268873, "chrf_score": 15.06571237491995, "xcomet_score": 0.9999881982803345, "xcomet_qe_score": 1.0, "metricx_score": 0.8582216501235962, "metricx_qe_score": 0.64527428150177, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有的发音都太相似,难以区分。", "metrics": {"bleu_score": 23.758717623824154, "chrf_score": 21.342035714602947, "xcomet_score": 0.8514600992202759, "xcomet_qe_score": 0.8755280375480652, "metricx_score": 2.255460262298584, "metricx_qe_score": 0.58174729347229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定偏好时。", "metrics": {"bleu_score": 23.093053192812558, "chrf_score": 23.39542938424004, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6289462447166443, "metricx_qe_score": 0.45695963501930237, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些直接差异的示例,例如较新的或不那么有活力的歌曲。", "metrics": {"bleu_score": 16.43735447907759, "chrf_score": 15.677643192807494, "xcomet_score": 0.6876883506774902, "xcomet_qe_score": 0.6039519906044006, "metricx_score": 5.184409141540527, "metricx_qe_score": 4.9542555809021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是评估大型语言模型实体理解的基准。 我们没有意识到有", "metrics": {"bleu_score": 41.569542440148844, "chrf_score": 38.76674905076564, "xcomet_score": 0.5753744840621948, "xcomet_qe_score": 0.3621073365211487, "metricx_score": 7.417269706726074, "metricx_qe_score": 5.991607666015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "公开的数据集,也没有大规模的公开数据集可以用于该任务,因此我们使用众包标注收集了一个数据集。", "metrics": {"bleu_score": 28.957970049674444, "chrf_score": 28.588026020957297, "xcomet_score": 0.6707733869552612, "xcomet_qe_score": 0.5693559646606445, "metricx_score": 4.786350727081299, "metricx_qe_score": 4.897622108459473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和餐厅。", "metrics": {"bleu_score": 66.7619194068951, "chrf_score": 58.75788429852825, "xcomet_score": 0.9137954711914062, "xcomet_qe_score": 0.8940622210502625, "metricx_score": 3.0284295082092285, "metricx_qe_score": 3.743083953857422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通完成集。", "metrics": {"bleu_score": 65.6062800318874, "chrf_score": 58.43921207766943, "xcomet_score": 0.8070893287658691, "xcomet_qe_score": 0.7957874536514282, "metricx_score": 4.436980724334717, "metricx_qe_score": 4.363029479980469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这张卡通有三个语言气泡。", "metrics": {"bleu_score": 14.323145079400492, "chrf_score": 12.52013609419966, "xcomet_score": 0.7891225814819336, "xcomet_qe_score": 0.7999083995819092, "metricx_score": 1.4100176095962524, "metricx_qe_score": 1.4073562622070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡中,鲍勃说:“还记得我们昨天听的那首歌吗?”鲍", "metrics": {"bleu_score": 59.80717212321686, "chrf_score": 55.60023045281969, "xcomet_score": 0.7331028580665588, "xcomet_qe_score": 0.6947493553161621, "metricx_score": 4.515609264373779, "metricx_qe_score": 1.4314395189285278, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "勃由此设定了对话的背景。", "metrics": {"bleu_score": 18.27249680232283, "chrf_score": 13.247887389570186, "xcomet_score": 0.8880317211151123, "xcomet_qe_score": 0.8873867988586426, "metricx_score": 3.5031301975250244, "metricx_qe_score": 3.1983180046081543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个语音气泡中,爱丽丝说:“你是说对我轻点,还是我有感觉?”", "metrics": {"bleu_score": 19.887294770987026, "chrf_score": 11.7496595858166, "xcomet_score": 0.6955498456954956, "xcomet_qe_score": 0.7403304576873779, "metricx_score": 4.749252796173096, "metricx_qe_score": 5.353202819824219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个替代问题。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 33.30026455026456, "xcomet_score": 0.881988525390625, "xcomet_qe_score": 0.8482243418693542, "metricx_score": 0.8843581080436707, "metricx_qe_score": 1.6897786855697632, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用了间接引用来选择这些实体之一,例如新的地球。", "metrics": {"bleu_score": 14.36569623378451, "chrf_score": 15.38834436408098, "xcomet_score": 0.5992594361305237, "xcomet_qe_score": 0.5941400527954102, "metricx_score": 7.532844066619873, "metricx_qe_score": 6.526010036468506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话框,但第三个对话框由标注者填写。", "metrics": {"bleu_score": 72.44406228582656, "chrf_score": 65.06583215126788, "xcomet_score": 0.913941502571106, "xcomet_qe_score": 0.8129608631134033, "metricx_score": 1.1300028562545776, "metricx_qe_score": 1.3240852355957031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话框是从每个域的几个手动提示中选择的。", "metrics": {"bleu_score": 88.62476419965998, "chrf_score": 83.75638832839192, "xcomet_score": 0.9639369249343872, "xcomet_qe_score": 0.7434428930282593, "metricx_score": 0.9110443592071533, "metricx_qe_score": 1.6436004638671875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个问题,即替代问题,是通过以下方式生成的。", "metrics": {"bleu_score": 12.062940248564933, "chrf_score": 15.144752544239083, "xcomet_score": 0.9198558330535889, "xcomet_qe_score": 0.9093024134635925, "metricx_score": 1.049098014831543, "metricx_qe_score": 0.891541600227356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指 A 还是 B?其中", "metrics": {"bleu_score": 31.239399369202552, "chrf_score": 29.215581267820067, "xcomet_score": 0.703842282295227, "xcomet_qe_score": 0.8540117144584656, "metricx_score": 2.7723443508148193, "metricx_qe_score": 0.71462482213974, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "A 和 B 是维基百科的样本。", "metrics": {"bleu_score": 70.63486135430557, "chrf_score": 61.628012271695255, "xcomet_score": 0.9424295425415039, "xcomet_qe_score": 0.8773962259292603, "metricx_score": 1.0423791408538818, "metricx_qe_score": 1.6706395149230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同抽样方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09123439341783524, "metricx_qe_score": 0.11483591794967651, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着列表中位置的上升,实体变得更加相似,通常更难进行消歧。", "metrics": {"bleu_score": 27.91576982464307, "chrf_score": 23.217672819967166, "xcomet_score": 0.8272103071212769, "xcomet_qe_score": 0.7762298583984375, "metricx_score": 4.769876480102539, "metricx_qe_score": 5.802371501922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀的。", "metrics": {"bleu_score": 16.0529461904344, "chrf_score": 13.544736983436481, "xcomet_score": 0.8565285205841064, "xcomet_qe_score": 0.8470308780670166, "metricx_score": 2.32529354095459, "metricx_qe_score": 2.1578428745269775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二种情况是当实体具有相似的标题时。例如,两本书的名字都是《归来》。", "metrics": {"bleu_score": 24.164444327540373, "chrf_score": 22.193906853785986, "xcomet_score": 0.8667891025543213, "xcomet_qe_score": 0.8534884452819824, "metricx_score": 2.1596012115478516, "metricx_qe_score": 3.0658953189849854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是当它们在维基百科上有相似的描述,", "metrics": {"bleu_score": 82.32490471721698, "chrf_score": 79.20041877024845, "xcomet_score": 0.9858764410018921, "xcomet_qe_score": 0.9734823703765869, "metricx_score": 0.7112922668457031, "metricx_qe_score": 0.5771846175193787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一种情况是当它们在维基百科上有相似的信息框或属性", "metrics": {"bleu_score": 63.941271162881165, "chrf_score": 63.55164917559464, "xcomet_score": 0.9926408529281616, "xcomet_qe_score": 0.9878630638122559, "metricx_score": 1.451255440711975, "metricx_qe_score": 1.5881749391555786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如相同的流派或相同的艺术家。", "metrics": {"bleu_score": 14.62806365365753, "chrf_score": 16.922398258220273, "xcomet_score": 0.8070130348205566, "xcomet_qe_score": 0.7183473110198975, "metricx_score": 2.982963800430298, "metricx_qe_score": 2.6824166774749756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向受访者展示这个替代性问题时,他们知道这些实体的名称,但不一定了解这些实体。", "metrics": {"bleu_score": 52.111211966062996, "chrf_score": 43.19788238443484, "xcomet_score": 0.7505491375923157, "xcomet_qe_score": 0.7252542972564697, "metricx_score": 4.268776893615723, "metricx_qe_score": 4.320585250854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是展示两个实体的背景知识。", "metrics": {"bleu_score": 48.092348466014684, "chrf_score": 38.738962630455795, "xcomet_score": 0.9656840562820435, "xcomet_qe_score": 0.823499321937561, "metricx_score": 1.1453741788864136, "metricx_qe_score": 2.435621500015259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只需显示每首歌的 Google 搜索链接。 然后要求注释者至少听一些每首歌,并阅读有关每首歌的信息。", "metrics": {"bleu_score": 54.54550561332102, "chrf_score": 45.040589128187186, "xcomet_score": 0.9243676662445068, "xcomet_qe_score": 0.8821816444396973, "metricx_score": 3.333110809326172, "metricx_qe_score": 2.4139769077301025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这是 Google 搜索结果中“Easy On Me”的结果。", "metrics": {"bleu_score": 25.658506763604915, "chrf_score": 33.54266005296777, "xcomet_score": 0.8831700086593628, "xcomet_qe_score": 0.9087637662887573, "metricx_score": 1.9595404863357544, "metricx_qe_score": 3.323126792907715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示了来自维基百科的背景文本。", "metrics": {"bleu_score": 63.13476599314376, "chrf_score": 54.19210175474897, "xcomet_score": 0.9804984331130981, "xcomet_qe_score": 0.9193463325500488, "metricx_score": 0.9178977608680725, "metricx_qe_score": 1.5036317110061646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还展示了它们的图片,同样来自维基百科,这样标注者就知道它们的样子。", "metrics": {"bleu_score": 36.936131614867136, "chrf_score": 29.22369810527705, "xcomet_score": 0.8866592049598694, "xcomet_qe_score": 0.9201950430870056, "metricx_score": 1.7045106887817383, "metricx_qe_score": 1.8394728899002075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求注释者从这些实体中选择一个,例如这里的第一个,并用三到五个间接引用表达来描述它们。", "metrics": {"bleu_score": 59.41856242604679, "chrf_score": 53.42408819666983, "xcomet_score": 0.9070414304733276, "xcomet_qe_score": 0.8647587299346924, "metricx_score": 1.7468055486679077, "metricx_qe_score": 1.9963624477386475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,带有钢琴音乐的那个。", "metrics": {"bleu_score": 15.727800941615351, "chrf_score": 16.815026488266092, "xcomet_score": 0.9873464107513428, "xcomet_qe_score": 0.9839320182800293, "metricx_score": 0.5562951564788818, "metricx_qe_score": 0.5315858721733093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一些示例。", "metrics": {"bleu_score": 80.65008590125565, "chrf_score": 77.75918525918527, "xcomet_score": 0.9735596179962158, "xcomet_qe_score": 0.9616199731826782, "metricx_score": 0.6032133102416992, "metricx_qe_score": 1.398135781288147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有文字的那个,不是那个12岁男孩的,也不是虚构的,或者来自阿塞拜疆的。", "metrics": {"bleu_score": 38.13420519686331, "chrf_score": 32.09275981051131, "xcomet_score": 0.6420726776123047, "xcomet_qe_score": 0.5918292999267578, "metricx_score": 1.7943010330200195, "metricx_qe_score": 2.908453941345215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "AltEntities语料库包含三个领域的6000个替代问题,以及42000个间接引用表达。", "metrics": {"bleu_score": 22.57684613546457, "chrf_score": 43.63713015234618, "xcomet_score": 0.8293734788894653, "xcomet_qe_score": 0.8438352346420288, "metricx_score": 2.8237714767456055, "metricx_qe_score": 2.6995060443878174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "T5-XL模型的结果总结如下。", "metrics": {"bleu_score": 39.740876265191105, "chrf_score": 39.05254576401218, "xcomet_score": 0.9360475540161133, "xcomet_qe_score": 0.9055338501930237, "metricx_score": 1.0498342514038086, "metricx_qe_score": 1.3665975332260132, "linguapy_score": [1, "AZERBAIJANI"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问与标注者完全相同的背景知识,那么准确率会非常高,大约在 92% 到 95% 之间。", "metrics": {"bleu_score": 66.15193085509924, "chrf_score": 58.5194504873784, "xcomet_score": 0.8397010564804077, "xcomet_qe_score": 0.8956770896911621, "metricx_score": 0.9375013709068298, "metricx_qe_score": 0.9288644790649414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问一些部分重叠的背景知识,那么准确率在 82% 到 87% 之间,这更加现实。", "metrics": {"bleu_score": 68.37564776360203, "chrf_score": 64.12838005985834, "xcomet_score": 0.8962706327438354, "xcomet_qe_score": 0.8858147859573364, "metricx_score": 1.1369227170944214, "metricx_qe_score": 1.9006996154785156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率只有 60%,因此有很大的改进空间。", "metrics": {"bleu_score": 84.99508493439812, "chrf_score": 79.16803071214837, "xcomet_score": 0.9965957403182983, "xcomet_qe_score": 0.9900504350662231, "metricx_score": 1.6137827634811401, "metricx_qe_score": 2.7250027656555176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还表明,这些模型具有领域泛化能力。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 41.79279370727841, "xcomet_score": 0.9208338260650635, "xcomet_qe_score": 0.915965735912323, "metricx_score": 0.7583819031715393, "metricx_qe_score": 0.9538589715957642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自多伦多大学和布鲁诺·凯斯勒基金会的萨拉·帕皮,我将简要介绍《注意力作为同时语音翻译指南》这篇论文,这是与马泰奥·内格里和马可·图尔基合作的成果。", "metrics": {"bleu_score": 40.47501709033028, "chrf_score": 32.11800413621983, "xcomet_score": 0.554019033908844, "xcomet_qe_score": 0.51004558801651, "metricx_score": 4.056166172027588, "metricx_qe_score": 3.261763095855713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同声传译?", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 79.70238095238096, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.02897430956363678, "metricx_qe_score": 0.23985493183135986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同声传译(SimulST)是指将一种语言的口头语言实时翻译成另一种语言的文本的过程,从而实现跨语言交流。", "metrics": {"bleu_score": 54.881494902438476, "chrf_score": 57.58558453975739, "xcomet_score": 0.9989591836929321, "xcomet_qe_score": 0.9858354330062866, "metricx_score": 1.3142449855804443, "metricx_qe_score": 2.3144237995147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前模拟 SDE 模型存在哪些问题?", "metrics": {"bleu_score": 24.022110864391543, "chrf_score": 16.821252342299793, "xcomet_score": 0.7953737378120422, "xcomet_qe_score": 0.814547061920166, "metricx_score": 2.2427523136138916, "metricx_qe_score": 2.2710459232330322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常会训练特定的架构,并引入额外的模块进行优化。", "metrics": {"bleu_score": 52.48128629667117, "chrf_score": 48.719361319470856, "xcomet_score": 0.9739092588424683, "xcomet_qe_score": 0.9158521890640259, "metricx_score": 0.6246196031570435, "metricx_qe_score": 1.4377316236495972, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,涉及不同优化目标的长而复", "metrics": {"bleu_score": 34.15435794609564, "chrf_score": 36.28696372956675, "xcomet_score": 0.655469536781311, "xcomet_qe_score": 0.6541865468025208, "metricx_score": 8.341304779052734, "metricx_qe_score": 4.233092784881592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "杂的训练程序。 并训练和维护多个模型以实现不同的延迟", "metrics": {"bleu_score": 43.07959608230678, "chrf_score": 37.14716890049386, "xcomet_score": 0.33026424050331116, "xcomet_qe_score": 0.3563814163208008, "metricx_score": 5.527487754821777, "metricx_qe_score": 5.826210021972656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模式,例如训练一个平均延迟为 1 秒的模型,另一个为 2 秒,依此类推。", "metrics": {"bleu_score": 42.456305719419696, "chrf_score": 36.02348470025495, "xcomet_score": 0.5694854855537415, "xcomet_qe_score": 0.45232725143432617, "metricx_score": 1.8874902725219727, "metricx_qe_score": 2.193618059158325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用现有的离线语音识别模型,而不需要重新训练或采用特定的语音识别架构。", "metrics": {"bleu_score": 46.1096737815597, "chrf_score": 38.98078352169987, "xcomet_score": 0.9765192270278931, "xcomet_qe_score": 0.9861820936203003, "metricx_score": 2.1692090034484863, "metricx_qe_score": 2.3440165519714355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用一个模型来处理所有延迟模式,并通过特定参数来处理延迟。", "metrics": {"bleu_score": 57.61590502949669, "chrf_score": 48.89275967192342, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.7499123811721802, "metricx_qe_score": 0.9596648812294006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并利用模型通过注意力机制在音频输入和文本输出之间所获得的知识,", "metrics": {"bleu_score": 63.091957572682624, "chrf_score": 52.73784737852974, "xcomet_score": 0.9379570484161377, "xcomet_qe_score": 0.7841271758079529, "metricx_score": 2.2842531204223633, "metricx_qe_score": 3.5995960235595703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即交叉注意力机制。右侧可以看到一个示例。", "metrics": {"bleu_score": 48.130446616982546, "chrf_score": 41.10057585718507, "xcomet_score": 0.964515209197998, "xcomet_qe_score": 0.8826576471328735, "metricx_score": 0.6522620320320129, "metricx_qe_score": 0.6597939133644104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出编码器-解码器注意力(EDAT),这是一种策略,我们根据注意力指向的位置来决定是否发出部分翻译。", "metrics": {"bleu_score": 56.4122851173987, "chrf_score": 47.56998703982417, "xcomet_score": 0.5839048624038696, "xcomet_qe_score": 0.5941739082336426, "metricx_score": 4.425994396209717, "metricx_qe_score": 5.305685520172119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果张力没有集中,也就是说,它的总和低于某个阈值α,那么它就会被忽略,这意味着接收到的信息足够稳定。", "metrics": {"bleu_score": 34.58130237981804, "chrf_score": 28.873347243451214, "xcomet_score": 0.5861219763755798, "xcomet_qe_score": 0.5133832693099976, "metricx_score": 6.01556396484375, "metricx_qe_score": 6.942840099334717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们收到一段包含“谈论”的语音,而我们的模型预测出德语翻译。 我们将查看交叉注意力权重。 我们会看到,前两个单词指向最早接收到的语音帧,而最后一个单词指向最后接收到的语音帧,即λ语音帧。", "metrics": {"bleu_score": 53.67215113463869, "chrf_score": 42.7516546909521, "xcomet_score": 0.6245647668838501, "xcomet_qe_score": 0.540535569190979, "metricx_score": 3.758639335632324, "metricx_qe_score": 4.738809585571289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个单词将被省略。 由于交叉注意力的总和大于某个阈值α,我们不会发出最后一个单词,而是等待另一个语音片段。", "metrics": {"bleu_score": 55.92516278051292, "chrf_score": 46.82937544264455, "xcomet_score": 0.7676396369934082, "xcomet_qe_score": 0.8046824932098389, "metricx_score": 2.3322155475616455, "metricx_qe_score": 3.2943525314331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续,我们会收到另一个语音块,我们的模型会预测另外三个单词,我们将查看交叉注意力权重。 我们会发现没有一个词指向最后的语音帧。", "metrics": {"bleu_score": 50.930160055966255, "chrf_score": 42.47220997786612, "xcomet_score": 0.7691062688827515, "xcomet_qe_score": 0.612900972366333, "metricx_score": 3.2520387172698975, "metricx_qe_score": 4.618579387664795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将被发出。", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 27.259129759129756, "xcomet_score": 0.9332944750785828, "xcomet_qe_score": 0.8744902610778809, "metricx_score": 1.6925324201583862, "metricx_qe_score": 3.3203859329223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们来看看这项研究的主要结果, 我们将在图表上绘制同时空间转换的结果,图表的一边是蓝色,用于衡量转换质量和平均延迟。 这是延迟测量,我们还考虑了计算感知平均延迟,它考虑了模型的计算时间来预测输出。", "metrics": {"bleu_score": 30.19213028038938, "chrf_score": 25.417601369501735, "xcomet_score": 0.4655444025993347, "xcomet_qe_score": 0.46921306848526, "metricx_score": 9.22165584564209, "metricx_qe_score": 8.569774627685547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望我们的曲线在这个图表上尽可能高。", "metrics": {"bleu_score": 34.64020111658723, "chrf_score": 32.824891215002445, "xcomet_score": 0.9694944620132446, "xcomet_qe_score": 0.8942456245422363, "metricx_score": 1.6584386825561523, "metricx_qe_score": 1.6971598863601685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但我们也希望它们向左移动。", "metrics": {"bleu_score": 86.17038791239612, "chrf_score": 84.90244110859445, "xcomet_score": 0.9971116781234741, "xcomet_qe_score": 0.9812257289886475, "metricx_score": 0.6350057721138, "metricx_qe_score": 1.0446958541870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将其与应用于离线模型的预处理策略进行了比较,这些策略包括权重策略和本地协议。", "metrics": {"bleu_score": 27.95816093588972, "chrf_score": 20.97111448417824, "xcomet_score": 0.8299674987792969, "xcomet_qe_score": 0.7813632488250732, "metricx_score": 3.065349817276001, "metricx_qe_score": 3.688568592071533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将其与专门为同时性翻译设计的最先进架构进行了比较。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 43.72824880460446, "xcomet_score": 0.8797560930252075, "xcomet_qe_score": 0.8610659837722778, "metricx_score": 1.3607221841812134, "metricx_qe_score": 2.0271787643432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是德语中同时性翻译策略的所有结果。", "metrics": {"bleu_score": 34.11488281065382, "chrf_score": 30.052244460854187, "xcomet_score": 0.8338484764099121, "xcomet_qe_score": 0.8147604465484619, "metricx_score": 2.7293152809143066, "metricx_qe_score": 2.2670130729675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,它在离线模型中应用的所有策略中表现最佳,因为曲线向左移动。", "metrics": {"bleu_score": 36.336800477245156, "chrf_score": 34.83346999392789, "xcomet_score": 0.9832262992858887, "xcomet_qe_score": 0.9584642648696899, "metricx_score": 1.1981456279754639, "metricx_qe_score": 1.8708616495132446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果考虑实际经过的时间或计算时间,那么 ADAPT 是最快的策略。", "metrics": {"bleu_score": 34.105597251856096, "chrf_score": 28.970159456488904, "xcomet_score": 0.8232043981552124, "xcomet_qe_score": 0.8260403871536255, "metricx_score": 4.51938533782959, "metricx_qe_score": 4.5787177085876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果你想了解更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 72.65407815865125, "chrf_score": 64.07292741658067, "xcomet_score": 0.9867600202560425, "xcomet_qe_score": 0.9684172868728638, "metricx_score": 0.19690358638763428, "metricx_qe_score": 0.24198387563228607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码、模型和同时输出,以便于复现我们的工作。", "metrics": {"bleu_score": 64.23833022893069, "chrf_score": 59.62312397801981, "xcomet_score": 0.8100194931030273, "xcomet_qe_score": 0.7787625789642334, "metricx_score": 1.6145585775375366, "metricx_qe_score": 2.288796901702881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢你的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9413008093833923, "xcomet_qe_score": 0.9886455535888672, "metricx_score": 1.430830478668213, "metricx_qe_score": 1.1331459283828735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫英,我和我的同事智勇将向大家介绍我们关于多指令的研究,即通过指令调整改进多模态零样本学习。", "metrics": {"bleu_score": 54.259157874530175, "chrf_score": 39.237049622281994, "xcomet_score": 0.5158674716949463, "xcomet_qe_score": 0.4710216820240021, "metricx_score": 5.634206771850586, "metricx_qe_score": 5.890962600708008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索新的学习范式,即以参数和数据高效的方式重新使用预训练语言模型来执行不同的下游任务。", "metrics": {"bleu_score": 66.39983322147407, "chrf_score": 59.79838812743643, "xcomet_score": 0.9050232172012329, "xcomet_qe_score": 0.8799648284912109, "metricx_score": 1.4577347040176392, "metricx_qe_score": 2.116786479949951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近有许多研究表明,通过遵循自然指令,指令微调使大型语言模型能够以零样本的方式执行新任务。", "metrics": {"bleu_score": 51.035618774049425, "chrf_score": 41.25267225457865, "xcomet_score": 0.9167686700820923, "xcomet_qe_score": 0.7221893072128296, "metricx_score": 2.1400578022003174, "metricx_qe_score": 3.6622347831726074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数先前的指令微调工作都集中在改进仅语言任务的零样本性能,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 37.515413330366684, "chrf_score": 32.328484801259386, "xcomet_score": 0.9560692310333252, "xcomet_qe_score": 0.8032526969909668, "metricx_score": 1.1238371133804321, "metricx_qe_score": 1.5273324251174927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们希望探讨指令微调是否能够改善多模态预训练模型对未见多模态任务的泛化能力。", "metrics": {"bleu_score": 39.05967096469109, "chrf_score": 35.55536221540965, "xcomet_score": 0.9010165929794312, "xcomet_qe_score": 0.8081978559494019, "metricx_score": 1.1173020601272583, "metricx_qe_score": 1.276611566543579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们进行研究时,我们发现指令数据集在自然语言处理和多模态之间存在显著差异。 虽然", "metrics": {"bleu_score": 52.23847463729655, "chrf_score": 43.931918514379134, "xcomet_score": 0.5755431652069092, "xcomet_qe_score": 0.6010088920593262, "metricx_score": 6.117712497711182, "metricx_qe_score": 1.77597975730896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有超过 1600 个语言指令任务,", "metrics": {"bleu_score": 48.871645172969465, "chrf_score": 51.89743807289497, "xcomet_score": 0.9249520301818848, "xcomet_qe_score": 0.8112239837646484, "metricx_score": 1.1495616436004639, "metricx_qe_score": 1.8326541185379028, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但没有大规模的公开可用的多模态指令任务。", "metrics": {"bleu_score": 69.3395566222006, "chrf_score": 64.4399100909652, "xcomet_score": 0.9092608690261841, "xcomet_qe_score": 0.8098517656326294, "metricx_score": 1.6030091047286987, "metricx_qe_score": 2.1660940647125244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这促使我们构建了一个多模态指令调优数据集。", "metrics": {"bleu_score": 51.981601535894555, "chrf_score": 44.01670736129778, "xcomet_score": 0.9750893115997314, "xcomet_qe_score": 0.9641941785812378, "metricx_score": 1.2979865074157715, "metricx_qe_score": 0.9915860891342163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了 MultiInstruct,这是第一个多模态指令调优基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个广泛的类别。", "metrics": {"bleu_score": 44.924654949645955, "chrf_score": 48.28551353555267, "xcomet_score": 0.8166368007659912, "xcomet_qe_score": 0.7782850861549377, "metricx_score": 1.2435897588729858, "metricx_qe_score": 1.329952597618103, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来自 21 个现有的开源数据集,每个任务都配有 5 条专家撰写的指令。", "metrics": {"bleu_score": 55.923576682620265, "chrf_score": 49.39535763491982, "xcomet_score": 0.989668607711792, "xcomet_qe_score": 0.9660528898239136, "metricx_score": 0.9751433730125427, "metricx_qe_score": 1.5668480396270752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令调优,我们采用了统一的多模态表示模型 OFA 作为我们的基础模型。", "metrics": {"bleu_score": 68.86046714037379, "chrf_score": 65.1767639694514, "xcomet_score": 0.9046955108642578, "xcomet_qe_score": 0.7973248362541199, "metricx_score": 2.5681941509246826, "metricx_qe_score": 3.279481887817383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFA 使用统一的词汇表来表示语言、图像标记和边界框的坐标。", "metrics": {"bleu_score": 56.74773954614978, "chrf_score": 50.92614531953855, "xcomet_score": 0.9841932058334351, "xcomet_qe_score": 0.8971979022026062, "metricx_score": 2.1064631938934326, "metricx_qe_score": 3.2384564876556396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "下面我们展示一些来自我们的多模态数据集的示例。 为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 51.40360868319708, "chrf_score": 36.86686771036716, "xcomet_score": 0.9317626953125, "xcomet_qe_score": 0.9382774829864502, "metricx_score": 2.7621254920959473, "metricx_qe_score": 3.325848340988159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循 OFA 的方法,并将所有任务统一格式化为序列到序列格式,其中", "metrics": {"bleu_score": 50.37255667587404, "chrf_score": 49.95440343049764, "xcomet_score": 0.7581228613853455, "xcomet_qe_score": 0.7494148015975952, "metricx_score": 2.512688159942627, "metricx_qe_score": 1.601481556892395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框在同一令牌空间中表示。", "metrics": {"bleu_score": 66.03453717254727, "chrf_score": 61.37128352633322, "xcomet_score": 0.8712300062179565, "xcomet_qe_score": 0.8400683403015137, "metricx_score": 4.843635082244873, "metricx_qe_score": 4.877642631530762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我要谈谈多模态指令调优。", "metrics": {"bleu_score": 72.97627709554281, "chrf_score": 69.00991165697047, "xcomet_score": 0.9254562854766846, "xcomet_qe_score": 0.8959468007087708, "metricx_score": 0.6527446508407593, "metricx_qe_score": 0.6722237467765808, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在训练数据集中,我们使用了 53 个任务来进行训练,每个任务采样 10,000 个实例。", "metrics": {"bleu_score": 46.04444986087365, "chrf_score": 44.29843986529411, "xcomet_score": 0.9080753326416016, "xcomet_qe_score": 0.9141390323638916, "metricx_score": 3.082716941833496, "metricx_qe_score": 4.896472930908203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在测试中,我们保留了整个常识推理组用于测试,并从 VQA 和杂项组中选择了额外的 5 个任务。", "metrics": {"bleu_score": 53.121144977611756, "chrf_score": 47.04581342149882, "xcomet_score": 0.7444950342178345, "xcomet_qe_score": 0.6738166809082031, "metricx_score": 3.388171672821045, "metricx_qe_score": 3.639822244644165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在每个任务中使用测试集中的所有实例。", "metrics": {"bleu_score": 46.57113642968181, "chrf_score": 37.007298791142404, "xcomet_score": 0.8442792892456055, "xcomet_qe_score": 0.8292276859283447, "metricx_score": 1.4587281942367554, "metricx_qe_score": 1.63643479347229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试集中随机抽取 20 个任务作为 NLP 的单一任务。 因此,", "metrics": {"bleu_score": 59.94719712279206, "chrf_score": 55.54396391083698, "xcomet_score": 0.5459176301956177, "xcomet_qe_score": 0.517460823059082, "metricx_score": 5.266566276550293, "metricx_qe_score": 4.822621822357178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的 OFA 大型模型作为基础模型。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9849643707275391, "xcomet_qe_score": 0.9586126804351807, "metricx_score": 1.309117078781128, "metricx_qe_score": 2.1600091457366943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练期间,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 44.26623526629489, "chrf_score": 42.0460315708197, "xcomet_score": 0.9680986404418945, "xcomet_qe_score": 0.8860248923301697, "metricx_score": 0.8709369897842407, "metricx_qe_score": 1.47452974319458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例都随机与其五个指令模板中的一个结合。", "metrics": {"bleu_score": 61.47881529512643, "chrf_score": 57.54578727521277, "xcomet_score": 0.8866305351257324, "xcomet_qe_score": 0.796898365020752, "metricx_score": 1.827248454093933, "metricx_qe_score": 2.0934910774230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在测试中,我们对每项任务进行了五次实验,每次实验都使用五条", "metrics": {"bleu_score": 9.989771553872492, "chrf_score": 13.922676885802106, "xcomet_score": 0.6186891794204712, "xcomet_qe_score": 0.6806973218917847, "metricx_score": 6.738099575042725, "metricx_qe_score": 4.9865217208862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "指令中的一条来评估模型。 我们报告了所有五个实验中性能的平均值和最大值以及性能的标准偏差。", "metrics": {"bleu_score": 17.013830113153464, "chrf_score": 18.94394249277412, "xcomet_score": 0.12105787545442581, "xcomet_qe_score": 0.18051078915596008, "metricx_score": 5.266533374786377, "metricx_qe_score": 5.58768892288208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们将报告准确率。", "metrics": {"bleu_score": 64.7084148066781, "chrf_score": 53.6705623238131, "xcomet_score": 0.9276421070098877, "xcomet_qe_score": 0.9815855026245117, "metricx_score": 0.6052345037460327, "metricx_qe_score": 0.7412285208702087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,我们将报告 Rouge-L。对于 NLP 任务,我们也将报告 Rouge-L。", "metrics": {"bleu_score": 65.85122689368741, "chrf_score": 73.4452675379459, "xcomet_score": 0.956220269203186, "xcomet_qe_score": 0.8784007430076599, "metricx_score": 1.7928200960159302, "metricx_qe_score": 2.7937872409820557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个名为敏感度的额外评估指标。", "metrics": {"bleu_score": 44.56418443148364, "chrf_score": 39.00439087645341, "xcomet_score": 0.99870765209198, "xcomet_qe_score": 0.9980616569519043, "metricx_score": 0.4767775237560272, "metricx_qe_score": 0.5227887034416199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它衡量的是模型在指令的措辞稍有变化的情况下,能否始终为相同的任务产生相同的输出。", "metrics": {"bleu_score": 13.784906211485342, "chrf_score": 17.163066768625598, "xcomet_score": 0.9340944290161133, "xcomet_qe_score": 0.9835785627365112, "metricx_score": 1.4845677614212036, "metricx_qe_score": 2.3267719745635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提高 OFA 在多模态任务上的性能。", "metrics": {"bleu_score": 56.53239695629305, "chrf_score": 51.48246838263649, "xcomet_score": 0.9914814233779907, "xcomet_qe_score": 0.9646351337432861, "metricx_score": 1.4318344593048096, "metricx_qe_score": 1.5803004503250122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从自然指令数据集进行迁移学习也可以有助于指令微调。", "metrics": {"bleu_score": 60.92269130963124, "chrf_score": 56.425032345132365, "xcomet_score": 0.9805328845977783, "xcomet_qe_score": 0.8006865978240967, "metricx_score": 1.2045259475708008, "metricx_qe_score": 2.0999720096588135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,随着任务数量的增加,模型的性能得到了提升,同时敏感度也降低了。", "metrics": {"bleu_score": 40.120303060703236, "chrf_score": 34.420464853901386, "xcomet_score": 0.9855889081954956, "xcomet_qe_score": 0.9918941259384155, "metricx_score": 0.8718665838241577, "metricx_qe_score": 0.7930998802185059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还做了一个实验,", "metrics": {"bleu_score": 88.01117367933934, "chrf_score": 85.90608465608466, "xcomet_score": 0.9896172285079956, "xcomet_qe_score": 0.9624312520027161, "metricx_score": 0.23607137799263, "metricx_qe_score": 0.20442509651184082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用一个指令与使用五个指令进行比较。", "metrics": {"bleu_score": 12.846189726767717, "chrf_score": 16.47563641572889, "xcomet_score": 0.8488694429397583, "xcomet_qe_score": 0.848326563835144, "metricx_score": 1.2017406225204468, "metricx_qe_score": 1.920342206954956, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多指令可以提高模型的整体性能,并大大降低其敏感性。", "metrics": {"bleu_score": 56.0159779701976, "chrf_score": 48.726296504450424, "xcomet_score": 0.9455448389053345, "xcomet_qe_score": 1.0, "metricx_score": 0.932839572429657, "metricx_qe_score": 1.1989729404449463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这显示了不同微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 62.860788360356985, "chrf_score": 51.57504577342973, "xcomet_score": 0.9889848232269287, "xcomet_qe_score": 0.980075478553772, "metricx_score": 1.2167150974273682, "metricx_qe_score": 1.4567720890045166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,通过从自然指令数据集进行迁移学习,模型可以实现比原始 OFA 模型更好的敏感性。", "metrics": {"bleu_score": 42.821665324468285, "chrf_score": 37.80436329608089, "xcomet_score": 0.8839632272720337, "xcomet_qe_score": 0.8224880695343018, "metricx_score": 2.0436320304870605, "metricx_qe_score": 3.0711400508880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集中进行迁移学习可以帮助 OFA 在自然指令数据集上取得更好的表现。", "metrics": {"bleu_score": 72.86073450520506, "chrf_score": 68.14248092960007, "xcomet_score": 0.9381344318389893, "xcomet_qe_score": 0.6854532957077026, "metricx_score": 1.623713493347168, "metricx_qe_score": 3.078281879425049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们提出了第一个大规模的多模态指令调优数据集。我们显著提高了 OFA 的零样本能力,并探索了不同的迁移学习技术,展示了它们的好处。", "metrics": {"bleu_score": 52.29320004967007, "chrf_score": 47.521612591444956, "xcomet_score": 0.8097677826881409, "xcomet_qe_score": 0.7760709524154663, "metricx_score": 3.170261859893799, "metricx_qe_score": 3.473252296447754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个名为敏感度的新指标。 还有一", "metrics": {"bleu_score": 36.65882729601238, "chrf_score": 32.538771356495815, "xcomet_score": 0.36353179812431335, "xcomet_qe_score": 0.3381117880344391, "metricx_score": 4.813346862792969, "metricx_qe_score": 1.6521766185760498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "件事,我们正在收集一个更大的多模态指令调优数据集,其中包含大约 150 个额外的视觉语言任务,我们将发布这些任务。", "metrics": {"bleu_score": 67.1936104359354, "chrf_score": 62.77859813810992, "xcomet_score": 0.5440361499786377, "xcomet_qe_score": 0.6128026247024536, "metricx_score": 3.7596988677978516, "metricx_qe_score": 3.892524242401123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9896258115768433, "xcomet_qe_score": 0.9864583015441895, "metricx_score": 0.0, "metricx_qe_score": 0.01926419883966446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯托夫·西纳,很高兴欢迎大家来参加我们的演讲,主题是我们的 ACL 2023 论文《", "metrics": {"bleu_score": 29.880620222683383, "chrf_score": 33.86524293046106, "xcomet_score": 0.6732943058013916, "xcomet_qe_score": 0.5994443893432617, "metricx_score": 4.620223045349121, "metricx_qe_score": 2.703434944152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型的可接受性判断并不总是对上下文具有鲁棒性》。", "metrics": {"bleu_score": 71.22775889956915, "chrf_score": 69.60708720894493, "xcomet_score": 0.7205228805541992, "xcomet_qe_score": 0.758385419845581, "metricx_score": 4.390805244445801, "metricx_qe_score": 4.8093390464782715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与约翰·戈蒂尔(John Gautier)、阿伦·穆勒(Aaron Mueller)、卡尼什卡·米什拉(Kanishka Mishra)、加伦·芬特斯(Garen Wintemute)、罗杰·莱维(Roger Levy)和阿蒂娜·威廉(Adina Williams)的联合工作。", "metrics": {"bleu_score": 2.4530909131952767, "chrf_score": 49.12951870139786, "xcomet_score": 0.685758113861084, "xcomet_qe_score": 0.5457658767700195, "metricx_score": 4.696833610534668, "metricx_qe_score": 4.814693927764893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们重新审视了最小配对时间。", "metrics": {"bleu_score": 41.66760762362751, "chrf_score": 40.66856485610385, "xcomet_score": 0.8329756855964661, "xcomet_qe_score": 0.8364336490631104, "metricx_score": 4.63536262512207, "metricx_qe_score": 4.325118541717529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小配对范式(MPP)基本上是通过可接受性判断来评估语言模型的,这", "metrics": {"bleu_score": 34.256941842132036, "chrf_score": 29.403869255178083, "xcomet_score": 0.7958508729934692, "xcomet_qe_score": 0.8035598397254944, "metricx_score": 5.616924285888672, "metricx_qe_score": 1.4992314577102661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也包括语法性,例如 BLiMP、SyntaxGym,或者在刻板印象方面的可接受性,例如 CrowS-Pairs。", "metrics": {"bleu_score": 27.98319041464346, "chrf_score": 54.75911203561657, "xcomet_score": 0.7909805774688721, "xcomet_qe_score": 0.7592058181762695, "metricx_score": 1.9282934665679932, "metricx_qe_score": 2.6297719478607178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最小对比范式中,评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子,然后展示一个不可接受的句子或语法错误的句子。", "metrics": {"bleu_score": 50.986969656315836, "chrf_score": 45.24412815398136, "xcomet_score": 0.9005379676818848, "xcomet_qe_score": 0.8587347865104675, "metricx_score": 1.1712884902954102, "metricx_qe_score": 2.4613819122314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型能够将更大的概率赋予可接受的句子。", "metrics": {"bleu_score": 37.933034750265115, "chrf_score": 31.072576327864898, "xcomet_score": 0.963248610496521, "xcomet_qe_score": 0.7664247155189514, "metricx_score": 1.5729771852493286, "metricx_qe_score": 2.3773858547210693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 MPP 流水线基本上不允许我们评估模型对较长句子的接受度。", "metrics": {"bleu_score": 83.30787010500826, "chrf_score": 82.58498263620915, "xcomet_score": 0.8873521685600281, "xcomet_qe_score": 0.8560258150100708, "metricx_score": 3.1651883125305176, "metricx_qe_score": 3.532111644744873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型的上下文窗口越来越长", "metrics": {"bleu_score": 76.2322906096698, "chrf_score": 72.92367970580045, "xcomet_score": 0.9726319313049316, "xcomet_qe_score": 0.9144285917282104, "metricx_score": 0.6000099182128906, "metricx_qe_score": 0.8507481813430786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因此评估模型在整个上下文窗口中的可接受性至关重要。 我们在这里试图做的就是", "metrics": {"bleu_score": 44.81894427274264, "chrf_score": 37.86570956443215, "xcomet_score": 0.8599296808242798, "xcomet_qe_score": 0.8145288228988647, "metricx_score": 3.7798962593078613, "metricx_qe_score": 2.8747057914733887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "重新审视MPP管道,通过要求模型对越来越长的序列进行可接受性评估。 因此,这就是我们的方法。", "metrics": {"bleu_score": 45.48004571511025, "chrf_score": 49.65722982211771, "xcomet_score": 0.68129563331604, "xcomet_qe_score": 0.5451217889785767, "metricx_score": 4.204116344451904, "metricx_qe_score": 5.216840744018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们要做的是", "metrics": {"bleu_score": 11.708995388048026, "chrf_score": 10.189645943259157, "xcomet_score": 0.3188971281051636, "xcomet_qe_score": 0.1913057565689087, "metricx_score": 1.7686172723770142, "metricx_qe_score": 1.9367601871490479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",为了模拟这些较长的序列,我们重新访问数据集本身,然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 71.62245883832702, "chrf_score": 64.3966425797773, "xcomet_score": 0.7870641946792603, "xcomet_qe_score": 0.6408814787864685, "metricx_score": 3.4128077030181885, "metricx_qe_score": 3.8216400146484375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们从BLiMP数据集的附加岛屿案例中选择了一对典型的语法性。", "metrics": {"bleu_score": 49.25543678627325, "chrf_score": 39.221167761551946, "xcomet_score": 0.6863053441047668, "xcomet_qe_score": 0.7078561782836914, "metricx_score": 4.573057174682617, "metricx_qe_score": 4.81630802154541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的做法是,为了重现较长的序列,并且这些序列是可接受的,并且具有相同的语法结构匹配,", "metrics": {"bleu_score": 54.49035480645628, "chrf_score": 50.355549968096604, "xcomet_score": 0.7915596961975098, "xcomet_qe_score": 0.6505814790725708, "metricx_score": 3.110891580581665, "metricx_qe_score": 4.098501205444336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从泰国抽取语法正确的句子。 然后我们将其添加为可接受查询和不可接受查询的前缀。 因此,", "metrics": {"bleu_score": 28.086829437248312, "chrf_score": 23.09645586273823, "xcomet_score": 0.3866591453552246, "xcomet_qe_score": 0.387076735496521, "metricx_score": 8.415514945983887, "metricx_qe_score": 8.462724685668945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.9529941082000732, "xcomet_qe_score": 0.7428854703903198, "metricx_score": 1.5263457298278809, "metricx_qe_score": 1.8271386623382568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过从不同的子集或不同的数据集中选择句子来做到这一点。", "metrics": {"bleu_score": 81.75065174062131, "chrf_score": 81.30511434429032, "xcomet_score": 0.9837219715118408, "xcomet_qe_score": 0.8989216685295105, "metricx_score": 1.0366742610931396, "metricx_qe_score": 2.130199432373047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的不匹配场景。 因此,", "metrics": {"bleu_score": 80.53122030796273, "chrf_score": 94.9566147882592, "xcomet_score": 0.8396772742271423, "xcomet_qe_score": 0.7875796556472778, "metricx_score": 2.6788463592529297, "metricx_qe_score": 2.5722339153289795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关的数据集,但不是你正在评估的同一个数据集。", "metrics": {"bleu_score": 56.49610435104942, "chrf_score": 48.74676238335901, "xcomet_score": 0.9660874605178833, "xcomet_qe_score": 0.8340482711791992, "metricx_score": 1.1089627742767334, "metricx_qe_score": 2.1455371379852295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对不可接受的情况做同样的处理。", "metrics": {"bleu_score": 46.3325745127537, "chrf_score": 38.854396484114986, "xcomet_score": 0.9886739253997803, "xcomet_qe_score": 0.9088376760482788, "metricx_score": 0.6087870597839355, "metricx_qe_score": 0.7313862442970276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全无关的领域(例如维基百科)中选择句子。", "metrics": {"bleu_score": 48.40932204144309, "chrf_score": 39.82640571641182, "xcomet_score": 0.9884942770004272, "xcomet_qe_score": 0.9235496520996094, "metricx_score": 0.8063529133796692, "metricx_qe_score": 1.2530922889709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们,模型的可接受性判断是否受到任何背景的影响。 无论上下文是来自数据集的不同子集,还是与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 52.732865083629555, "chrf_score": 46.58283694945997, "xcomet_score": 0.8516897559165955, "xcomet_qe_score": 0.6274051070213318, "metricx_score": 2.3589611053466797, "metricx_qe_score": 2.877934455871582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,模型的表现如何呢?", "metrics": {"bleu_score": 9.752759118141046, "chrf_score": 10.63737408822508, "xcomet_score": 0.9027401208877563, "xcomet_qe_score": 0.9438310861587524, "metricx_score": 0.9500369429588318, "metricx_qe_score": 0.2996816635131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看与当前查询对无关的维基百科句子,发现 MPP 判断在任意上下文中大多是健壮的。", "metrics": {"bleu_score": 21.155199550151323, "chrf_score": 24.23970354695539, "xcomet_score": 0.7555812001228333, "xcomet_qe_score": 0.6546550989151001, "metricx_score": 4.932994365692139, "metricx_qe_score": 6.837852478027344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到 1024,以最大化 OPT 和 GPT-2 模型的性能,我们", "metrics": {"bleu_score": 58.38997128256185, "chrf_score": 73.45154847482418, "xcomet_score": 0.6250638961791992, "xcomet_qe_score": 0.5865446329116821, "metricx_score": 4.376758575439453, "metricx_qe_score": 2.14168119430542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色虚线中看到,MPP 判断相对稳定。", "metrics": {"bleu_score": 56.55183553484675, "chrf_score": 54.252660961386155, "xcomet_score": 0.8744864463806152, "xcomet_qe_score": 0.7751957178115845, "metricx_score": 2.134004831314087, "metricx_qe_score": 3.9441840648651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,如果我们从同一个数据集中选择句子会发生什么?", "metrics": {"bleu_score": 49.76448079808987, "chrf_score": 41.40654997176736, "xcomet_score": 0.9930061101913452, "xcomet_qe_score": 0.9220902323722839, "metricx_score": 1.1081087589263916, "metricx_qe_score": 2.6622941493988037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们从相同的BLiMP或SyntaxGym数据集中选择或创建可接受和不可接受的句子。", "metrics": {"bleu_score": 70.34765003296484, "chrf_score": 73.06751811735533, "xcomet_score": 0.81229168176651, "xcomet_qe_score": 0.846590518951416, "metricx_score": 1.5871257781982422, "metricx_qe_score": 2.3004932403564453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,当添加可接受的前缀或不可接受的前缀时,MPP 判断要么显著增加,要么显著减少。", "metrics": {"bleu_score": 52.42152717502353, "chrf_score": 48.414524474208925, "xcomet_score": 0.7795485854148865, "xcomet_qe_score": 0.8482243418693542, "metricx_score": 2.4268112182617188, "metricx_qe_score": 1.9424184560775757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,当我们匹配结构时,也就是说,当我们从“责备”现象中选择句子时,吉姆, 我们发现,根据所选前缀是否可接受,模型的MPP判断会大幅增加或大幅减少。 现在,这,而且这非常大,就像这个效应会随", "metrics": {"bleu_score": 37.426510408958464, "chrf_score": 32.906953364704286, "xcomet_score": 0.07206234335899353, "xcomet_qe_score": 0.05325261875987053, "metricx_score": 12.96006965637207, "metricx_qe_score": 12.807185173034668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "着上下文长度的增加而增强,这可能会影响具有大上下文窗口的新语言模型。", "metrics": {"bleu_score": 58.119975764593384, "chrf_score": 52.71432802224558, "xcomet_score": 0.7542881965637207, "xcomet_qe_score": 0.644716739654541, "metricx_score": 3.705925226211548, "metricx_qe_score": 5.459467887878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么匹配前缀会对语言模型的判断产生如此大的影响呢? 因此,", "metrics": {"bleu_score": 79.31509362620145, "chrf_score": 84.66746211027359, "xcomet_score": 0.8314436674118042, "xcomet_qe_score": 0.7689838409423828, "metricx_score": 3.8555376529693604, "metricx_qe_score": 3.6396803855895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保留相关结构的同时,对输入句子进行扰动,并向输入中添加噪声。", "metrics": {"bleu_score": 55.28919696204963, "chrf_score": 54.965720662908744, "xcomet_score": 0.8966097831726074, "xcomet_qe_score": 0.8253384828567505, "metricx_score": 2.1130411624908447, "metricx_qe_score": 2.903264045715332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了多次扰动后, 我们发现,这些噪音实际上并没有改变模型的轨迹,即它向我们展示的 NPP 刚刚趋势。", "metrics": {"bleu_score": 12.532443621763047, "chrf_score": 15.039339543440397, "xcomet_score": 0.734084963798523, "xcomet_qe_score": 0.6673005819320679, "metricx_score": 6.719298362731934, "metricx_qe_score": 7.567299842834473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动句子的敏感性是相似的。", "metrics": {"bleu_score": 44.543025732285095, "chrf_score": 39.8032499991696, "xcomet_score": 0.9268375635147095, "xcomet_qe_score": 0.8990291357040405, "metricx_score": 1.9636569023132324, "metricx_qe_score": 3.4255411624908447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们在可接受的领域内扰动句子时,我们看到所有扰动都有类似的增加,而当我们在不可接受的领域内扰动句子时,我们看到MPP判断以类似的方式减少。", "metrics": {"bleu_score": 57.61476433397321, "chrf_score": 51.43690373317823, "xcomet_score": 0.7257994413375854, "xcomet_qe_score": 0.6602357625961304, "metricx_score": 4.223392486572266, "metricx_qe_score": 5.122226715087891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键要点是,语言模型对句子中共享的隐含语法和语义特征敏感。", "metrics": {"bleu_score": 39.5228885347586, "chrf_score": 33.438797674249685, "xcomet_score": 0.8902238607406616, "xcomet_qe_score": 0.8448330760002136, "metricx_score": 1.1827898025512695, "metricx_qe_score": 1.5459699630737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而我们目前使用的 MPP 评估方法,即使用短输入和单中心输入,可能无法完全捕捉语言模型在上下文窗口中的抽象知识。", "metrics": {"bleu_score": 45.56617785032402, "chrf_score": 38.638457209786786, "xcomet_score": 0.7794313430786133, "xcomet_qe_score": 0.7380179166793823, "metricx_score": 4.406643867492676, "metricx_qe_score": 4.170814037322998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文,了解我们实验的更多细节。", "metrics": {"bleu_score": 54.08634078775594, "chrf_score": 48.594144354131046, "xcomet_score": 0.9996544122695923, "xcomet_qe_score": 0.9994748830795288, "metricx_score": 0.12405794113874435, "metricx_qe_score": 0.12327098846435547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是宾夕法尼亚大学的尤森·约翰。", "metrics": {"bleu_score": 30.690336937373775, "chrf_score": 19.707248514251464, "xcomet_score": 0.7047911882400513, "xcomet_qe_score": 0.6755412817001343, "metricx_score": 1.108548879623413, "metricx_qe_score": 1.028846263885498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的工作,即在多种自然语言和多种表示法中进行跨语言语义解析。 因此", "metrics": {"bleu_score": 42.266572401851256, "chrf_score": 32.70357911085454, "xcomet_score": 0.7543959617614746, "xcomet_qe_score": 0.7237190008163452, "metricx_score": 5.262779712677002, "metricx_qe_score": 4.624494552612305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语义解析是构建用户查询的语义表示的任务,例如 SQL 和 lambda 演算。", "metrics": {"bleu_score": 69.43273848296027, "chrf_score": 58.9089431141396, "xcomet_score": 0.9337364435195923, "xcomet_qe_score": 0.9020589590072632, "metricx_score": 2.341463565826416, "metricx_qe_score": 2.9041130542755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义解析是将多种自然语言的查询转换为多种意义表示的任务。", "metrics": {"bleu_score": 75.46697757057083, "chrf_score": 68.22530618971106, "xcomet_score": 0.884688138961792, "xcomet_qe_score": 0.8805313110351562, "metricx_score": 1.448477864265442, "metricx_qe_score": 3.1513357162475586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将查询翻译成多种自然语言,并转换为 SQL、Lambda 或 FuncQL 等。", "metrics": {"bleu_score": 59.6311298491353, "chrf_score": 65.17259865768536, "xcomet_score": 0.9534889459609985, "xcomet_qe_score": 0.9359959363937378, "metricx_score": 1.6892653703689575, "metricx_qe_score": 2.189196825027466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型是分别提出并评估的,数据集的任务和应用有限。", "metrics": {"bleu_score": 57.363742100338946, "chrf_score": 50.906423524969135, "xcomet_score": 0.9666616320610046, "xcomet_qe_score": 0.9555886387825012, "metricx_score": 1.3367761373519897, "metricx_qe_score": 2.043674945831299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如, 某些自然语言的覆盖率不足。缺", "metrics": {"bleu_score": 53.084925448597666, "chrf_score": 50.789455330663, "xcomet_score": 0.5690270066261292, "xcomet_qe_score": 0.5068312883377075, "metricx_score": 6.218254566192627, "metricx_qe_score": 4.886932849884033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "少中文。 由于对某些少数民族的报道不足。", "metrics": {"bleu_score": 5.170183267415401, "chrf_score": 5.5280774858377795, "xcomet_score": 0.21317701041698456, "xcomet_qe_score": 0.2665277421474457, "metricx_score": 6.840141296386719, "metricx_qe_score": 6.138204097747803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "缺少了λ演算。 或者它们只在某个神经模型上进行评估。", "metrics": {"bleu_score": 44.04322446931529, "chrf_score": 37.11070160838528, "xcomet_score": 0.8250137567520142, "xcomet_qe_score": 0.7751005291938782, "metricx_score": 1.3596210479736328, "metricx_qe_score": 1.9990053176879883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一的模型来评估它们。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.997307538986206, "xcomet_qe_score": 0.9824987649917603, "metricx_score": 0.5768303871154785, "metricx_qe_score": 0.8717049956321716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了 Exemplar。", "metrics": {"bleu_score": 46.713797772819994, "chrf_score": 24.279640648952704, "xcomet_score": 0.8710143566131592, "xcomet_qe_score": 0.8551183938980103, "metricx_score": 1.5620455741882324, "metricx_qe_score": 2.9250471591949463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们为多种自然语言和意义表示的跨链语义解析提供", "metrics": {"bleu_score": 31.53912901822869, "chrf_score": 28.646660489102967, "xcomet_score": 0.4487564265727997, "xcomet_qe_score": 0.46469050645828247, "metricx_score": 9.174873352050781, "metricx_qe_score": 6.722293853759766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了一个统一的数据集示例。 它包含 90 个跨语言数据集、5 种语义解析任务、8 种多语言表示和 22 种自然语言,涵盖 15 个语言族。", "metrics": {"bleu_score": 32.04908615965631, "chrf_score": 35.3320046481449, "xcomet_score": 0.24143439531326294, "xcomet_qe_score": 0.1892736405134201, "metricx_score": 5.257558345794678, "metricx_qe_score": 5.06203556060791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 80.20219183488042, "chrf_score": 71.52080420921001, "xcomet_score": 0.9888695478439331, "xcomet_qe_score": 0.913833737373352, "metricx_score": 1.4580811262130737, "metricx_qe_score": 2.0225110054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是翻译测试。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9898306131362915, "xcomet_qe_score": 0.9781848192214966, "metricx_score": 0.22308200597763062, "metricx_qe_score": 0.39765846729278564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 Google 翻译 API 将源语言翻译成目标语言,然后使用单语言模型进行训练和评估。", "metrics": {"bleu_score": 77.89081599309982, "chrf_score": 69.12158373825255, "xcomet_score": 0.8900305032730103, "xcomet_qe_score": 0.9561153650283813, "metricx_score": 0.5134091377258301, "metricx_qe_score": 0.4861498177051544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语查询上训练英语模型,在推理期间,我们使用 API 将德语查询翻译成英语,然后使用训练好的模型来预测 SQL。", "metrics": {"bleu_score": 65.44945173762044, "chrf_score": 61.85224342659127, "xcomet_score": 0.8689162731170654, "xcomet_qe_score": 0.8238322734832764, "metricx_score": 1.4107011556625366, "metricx_qe_score": 2.2766928672790527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模型。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9699068069458008, "xcomet_qe_score": 0.8844642639160156, "metricx_score": 0.30868202447891235, "metricx_qe_score": 0.4093308448791504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 73.6558994084271, "chrf_score": 67.66383822511001, "xcomet_score": 0.9070941209793091, "xcomet_qe_score": 0.8909211158752441, "metricx_score": 0.6414645910263062, "metricx_qe_score": 0.6743262410163879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语少量训练设置,通过仅使用 10% 的训练数据来训练单语模型。", "metrics": {"bleu_score": 51.18285025257892, "chrf_score": 49.10113113174649, "xcomet_score": 0.8400407433509827, "xcomet_qe_score": 0.7669109106063843, "metricx_score": 2.349440574645996, "metricx_qe_score": 2.3844308853149414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试了一个多语言模型,我们为所有语言训练了一个多语言模型。", "metrics": {"bleu_score": 59.10522461071207, "chrf_score": 58.07365574328725, "xcomet_score": 0.9304975271224976, "xcomet_qe_score": 0.8692228198051453, "metricx_score": 1.4970580339431763, "metricx_qe_score": 2.457994222640991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询合并,以训练多语言模型,", "metrics": {"bleu_score": 44.49540332139186, "chrf_score": 37.03030757133153, "xcomet_score": 0.9474561214447021, "xcomet_qe_score": 0.9359908103942871, "metricx_score": 1.1539912223815918, "metricx_qe_score": 1.7960909605026245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理期间,我们可以使用该模型 将德语查询或中文查询等翻译成", "metrics": {"bleu_score": 51.738197513907906, "chrf_score": 45.195681180628554, "xcomet_score": 0.7899893522262573, "xcomet_qe_score": 0.7148535847663879, "metricx_score": 4.239677906036377, "metricx_qe_score": 2.800276517868042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,然后迁移到另一种语言。 因此,", "metrics": {"bleu_score": 33.307662668678184, "chrf_score": 30.09469057533497, "xcomet_score": 0.7655959129333496, "xcomet_qe_score": 0.7554341554641724, "metricx_score": 5.369666576385498, "metricx_qe_score": 5.716965675354004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练期间,我们将使用英语查询或英语和德语少量查询的组合来训练多语言模型,以预测 SQL 输出。", "metrics": {"bleu_score": 62.57885652946406, "chrf_score": 57.71219704483818, "xcomet_score": 0.8733397126197815, "xcomet_qe_score": 0.8160642981529236, "metricx_score": 2.4553403854370117, "metricx_qe_score": 2.4582645893096924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的结果。因此", "metrics": {"bleu_score": 49.35578819979934, "chrf_score": 47.35262425008111, "xcomet_score": 0.8335084915161133, "xcomet_qe_score": 0.8012288808822632, "metricx_score": 2.2050490379333496, "metricx_qe_score": 0.6639837026596069, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",关于单语言模型的分析,我们对两组模型进行了评估。 包括编码器-解码器(Encoder-Decoder),例如多语言预训练编码器与指针解码器(XLM-R+PDecoder)和BERT+P", "metrics": {"bleu_score": 31.49287451957037, "chrf_score": 36.875154816539826, "xcomet_score": 0.7434018850326538, "xcomet_qe_score": 0.737706184387207, "metricx_score": 6.28534460067749, "metricx_qe_score": 4.889891147613525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Decoder。 我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,例如 mBART 和 mT5。", "metrics": {"bleu_score": 35.904660659568314, "chrf_score": 37.276862969118376, "xcomet_score": 0.540826678276062, "xcomet_qe_score": 0.4324323236942291, "metricx_score": 5.211682319641113, "metricx_qe_score": 5.609076499938965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器-解码器在所有九个数据集上都表现最佳。", "metrics": {"bleu_score": 47.410093877936305, "chrf_score": 30.86390524733425, "xcomet_score": 0.9931691884994507, "xcomet_qe_score": 0.989141583442688, "metricx_score": 0.5821954607963562, "metricx_qe_score": 0.5543960332870483, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 MT5 和 XNLI 多语言设置中进行评估。", "metrics": {"bleu_score": 30.18078669043848, "chrf_score": 25.214727285483303, "xcomet_score": 0.7580779194831848, "xcomet_qe_score": 0.7433946132659912, "metricx_score": 6.356781959533691, "metricx_qe_score": 6.366504192352295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合体中进行训练,可以改进编码器-解码器或编码器-PTR。", "metrics": {"bleu_score": 23.21439545454691, "chrf_score": 20.733584925145053, "xcomet_score": 0.7988029718399048, "xcomet_qe_score": 0.6883165836334229, "metricx_score": 1.9984755516052246, "metricx_qe_score": 2.6410751342773438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,这是因为大多数主要的自然语言都能获得性能提升,而英语在七个数据集中性能下降,仅在三个数据集中获得提升。", "metrics": {"bleu_score": 49.61125117341993, "chrf_score": 42.34505953376817, "xcomet_score": 0.9586668014526367, "xcomet_qe_score": 0.987947940826416, "metricx_score": 2.52528715133667, "metricx_qe_score": 2.017426013946533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为“多语言诅咒”。", "metrics": {"bleu_score": 35.36496760320458, "chrf_score": 27.868479218143182, "xcomet_score": 0.9130001068115234, "xcomet_qe_score": 0.8859872817993164, "metricx_score": 0.8560494184494019, "metricx_qe_score": 1.048246145248413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言的表现差距。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9812818765640259, "xcomet_qe_score": 0.946584939956665, "metricx_score": 0.42830583453178406, "metricx_qe_score": 0.5886794328689575, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本图中,蓝线是跨语言少量样本迁移,", "metrics": {"bleu_score": 50.82365016563264, "chrf_score": 41.749637889343774, "xcomet_score": 0.8594161868095398, "xcomet_qe_score": 0.80368971824646, "metricx_score": 3.991520643234253, "metricx_qe_score": 4.019646644592285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线是跨语言零样本迁移,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.8476507663726807, "xcomet_qe_score": 0.8330907821655273, "metricx_score": 1.9685924053192139, "metricx_qe_score": 3.312077522277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿线是单语言设置。 我们发现", "metrics": {"bleu_score": 35.55670235668696, "chrf_score": 44.63327177754273, "xcomet_score": 0.8480111360549927, "xcomet_qe_score": 0.8355250954627991, "metricx_score": 3.4439148902893066, "metricx_qe_score": 1.7713797092437744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿线和橙线,发现零次尝试设置下,跨语言迁移性能差距显著。通过比较蓝线和橙线,我们发现,在少次尝试设置下,迁移差距迅速缩小。", "metrics": {"bleu_score": 46.30669144785353, "chrf_score": 38.88349024768248, "xcomet_score": 0.6322393417358398, "xcomet_qe_score": 0.6242601871490479, "metricx_score": 2.6772422790527344, "metricx_qe_score": 3.986236572265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些有趣的结果。", "metrics": {"bleu_score": 40.01601601922502, "chrf_score": 32.32212232212232, "xcomet_score": 0.9813677072525024, "xcomet_qe_score": 0.9676083326339722, "metricx_score": 1.342367172241211, "metricx_qe_score": 0.9767702221870422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器模型优于之前的工作,或者取得了可比的结果。", "metrics": {"bleu_score": 12.459714829009794, "chrf_score": 10.638757609110483, "xcomet_score": 0.972204327583313, "xcomet_qe_score": 0.9656497240066528, "metricx_score": 1.4829856157302856, "metricx_qe_score": 1.623265027999878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行预训练可以显著提高目标自然语言的少量样本学习性能。 我们发现,多语言语言模型(如COTES和BLUE)在跨语言语义解析方面仍然不足。", "metrics": {"bleu_score": 52.455578115202584, "chrf_score": 41.63853829298552, "xcomet_score": 0.6833411455154419, "xcomet_qe_score": 0.6880215406417847, "metricx_score": 6.21408224105835, "metricx_qe_score": 6.937748908996582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结来说,我们构建了 Exemplar,这是一个统一的基准,用于多语言和多表示的跨角度语义解析。", "metrics": {"bleu_score": 26.611381669683283, "chrf_score": 21.28029596546208, "xcomet_score": 0.7294439077377319, "xcomet_qe_score": 0.6308391094207764, "metricx_score": 4.172298431396484, "metricx_qe_score": 4.696045398712158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语言模型进行了全面的基准测试", "metrics": {"bleu_score": 64.28908241246019, "chrf_score": 54.76025729504258, "xcomet_score": 0.9802572727203369, "xcomet_qe_score": 0.9779975414276123, "metricx_score": 1.1519235372543335, "metricx_qe_score": 2.2911219596862793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",结果显示出许多有趣的发现等", "metrics": {"bleu_score": 53.42227899599143, "chrf_score": 44.675851081396885, "xcomet_score": 0.8445903062820435, "xcomet_qe_score": 0.8230971097946167, "metricx_score": 4.8117780685424805, "metricx_qe_score": 3.1141724586486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "等。", "metrics": {"bleu_score": 0.0, "chrf_score": 8.333333333333332, "xcomet_score": 0.604798436164856, "xcomet_qe_score": 0.2309737205505371, "metricx_score": 1.9000164270401, "metricx_qe_score": 3.2553701400756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢收听。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 17.0, "xcomet_score": 0.9755227565765381, "xcomet_qe_score": 0.9598297476768494, "metricx_score": 0.6410813927650452, "metricx_qe_score": 0.3586311638355255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫艾德·比拉德,我将对《平台翻译:评估策略和性能》这篇论文进行简要介绍。", "metrics": {"bleu_score": 9.989202761863273, "chrf_score": 12.859126486592466, "xcomet_score": 0.6899924278259277, "xcomet_qe_score": 0.6357938051223755, "metricx_score": 4.455515384674072, "metricx_qe_score": 5.0885114669799805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与谷歌翻译的同事们合作的成果。", "metrics": {"bleu_score": 27.970035385359708, "chrf_score": 25.126211615352677, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9377619624137878, "metricx_qe_score": 0.6038870811462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "BLOOM 是一个拥有 5400 亿参数的大型语言模型,于 2022 年发布。", "metrics": {"bleu_score": 59.70228993860436, "chrf_score": 56.09642665811779, "xcomet_score": 0.6855076551437378, "xcomet_qe_score": 0.6408815383911133, "metricx_score": 4.344827651977539, "metricx_qe_score": 5.479465007781982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在一个包含 7800 亿个标记的大型文本集上进行了训练。", "metrics": {"bleu_score": 43.313716303061824, "chrf_score": 48.40952928801779, "xcomet_score": 0.7573533654212952, "xcomet_qe_score": 0.7208136916160583, "metricx_score": 1.5067646503448486, "metricx_qe_score": 2.1030266284942627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百项自然语言处理任务中达到了最先进的水平。", "metrics": {"bleu_score": 16.74349890815697, "chrf_score": 17.952472132602992, "xcomet_score": 0.9987348318099976, "xcomet_qe_score": 0.9917758703231812, "metricx_score": 0.7015552520751953, "metricx_qe_score": 1.1090792417526245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们首次对大型语言模型提示进行了系统研究,以用于机器翻译。", "metrics": {"bleu_score": 26.16310176339366, "chrf_score": 25.217232414662792, "xcomet_score": 0.8207403421401978, "xcomet_qe_score": 0.8334771394729614, "metricx_score": 2.459017276763916, "metricx_qe_score": 2.058354377746582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 NLP 社区的最佳实践来评估这些模型的转移能力。", "metrics": {"bleu_score": 57.978239744332505, "chrf_score": 49.435812282786365, "xcomet_score": 0.7575522661209106, "xcomet_qe_score": 0.7482785582542419, "metricx_score": 3.2483391761779785, "metricx_qe_score": 4.429452896118164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将两个最先进的系统进行比较,即 WMT 评估中的最佳系统。", "metrics": {"bleu_score": 32.62582667665012, "chrf_score": 33.32097998869647, "xcomet_score": 0.9268169403076172, "xcomet_qe_score": 0.897545576095581, "metricx_score": 3.009561538696289, "metricx_qe_score": 4.613783836364746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的神经元度量标准,并展示了基于专家的人类评估结果。", "metrics": {"bleu_score": 59.20515363228744, "chrf_score": 49.17466266430394, "xcomet_score": 0.7344064116477966, "xcomet_qe_score": 0.813227653503418, "metricx_score": 2.7792696952819824, "metricx_qe_score": 2.9753990173339844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些从业人员选择策略的建议。", "metrics": {"bleu_score": 60.52665103345166, "chrf_score": 52.434744764466124, "xcomet_score": 0.8704793453216553, "xcomet_qe_score": 0.8770970702171326, "metricx_score": 4.136257648468018, "metricx_qe_score": 4.781061172485352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型的翻译性能有很大影响。我们可以在一个简单的实验中看到这一点,该实验使用了一次性提示,并为同一个句子提供了两个不同的提示。", "metrics": {"bleu_score": 59.22920020859916, "chrf_score": 59.4398834947087, "xcomet_score": 0.9742475748062134, "xcomet_qe_score": 0.9564361572265625, "metricx_score": 1.4875478744506836, "metricx_qe_score": 1.419884443283081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子(1000 句中的", "metrics": {"bleu_score": 9.980099403873663, "chrf_score": 20.595008961397998, "xcomet_score": 0.6225093603134155, "xcomet_qe_score": 0.5653419494628906, "metricx_score": 11.70371150970459, "metricx_qe_score": 9.797088623046875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "516 句)的差异超过 1 个模糊点。", "metrics": {"bleu_score": 13.508625657351418, "chrf_score": 13.694216202622133, "xcomet_score": 0.4454828202724457, "xcomet_qe_score": 0.177134707570076, "metricx_score": 9.155733108520508, "metricx_qe_score": 12.038021087646484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可能会达到 40 个血点。", "metrics": {"bleu_score": 34.22803488747218, "chrf_score": 24.630748221409966, "xcomet_score": 0.8029466867446899, "xcomet_qe_score": 0.8184045553207397, "metricx_score": 5.124118804931641, "metricx_qe_score": 3.9050545692443848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个良好的提示策略非常重要。", "metrics": {"bleu_score": 47.20758038942709, "chrf_score": 41.584577649209166, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2462974488735199, "metricx_qe_score": 0.3821769654750824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了五次提示策略,只需在我们提供给系统的每个句子中标记其语言。", "metrics": {"bleu_score": 46.087659954215596, "chrf_score": 40.14895745099142, "xcomet_score": 0.7181726098060608, "xcomet_qe_score": 0.7816895246505737, "metricx_score": 2.3931822776794434, "metricx_qe_score": 2.3562769889831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们从德语翻译成英语,德语句子(源句子)用德语冒号标记,英语翻译用英语冒号标记。", "metrics": {"bleu_score": 47.52335409318383, "chrf_score": 32.91685751285645, "xcomet_score": 0.9772305488586426, "xcomet_qe_score": 0.980293869972229, "metricx_score": 1.2028440237045288, "metricx_qe_score": 1.1667672395706177, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次短暂打印的情况下,打印的实际形式并没有太大的影响。", "metrics": {"bleu_score": 24.878144219731066, "chrf_score": 21.32504498450617, "xcomet_score": 0.7221285104751587, "xcomet_qe_score": 0.7312626838684082, "metricx_score": 2.1609714031219482, "metricx_qe_score": 2.0678367614746094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在零次提示和一次提示中,提示的形式至关重要。而在我们的案例中", "metrics": {"bleu_score": 19.78585723043446, "chrf_score": 20.281457382195665, "xcomet_score": 0.66035395860672, "xcomet_qe_score": 0.6286714673042297, "metricx_score": 5.227492809295654, "metricx_qe_score": 2.1997437477111816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当我们使用五次提示时,提示的形式几乎没有差别。", "metrics": {"bleu_score": 22.656720908801994, "chrf_score": 21.063372687669805, "xcomet_score": 0.894196093082428, "xcomet_qe_score": 0.7911442518234253, "metricx_score": 2.7279131412506104, "metricx_qe_score": 3.8065478801727295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例子占了大部分的分量。", "metrics": {"bleu_score": 4.2665050358928855, "chrf_score": 6.218905472636816, "xcomet_score": 0.8459676504135132, "xcomet_qe_score": 0.8481782078742981, "metricx_score": 1.070271611213684, "metricx_qe_score": 0.8144496083259583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果表明,示例质量比与源句的相似性更重要。", "metrics": {"bleu_score": 60.118742660358826, "chrf_score": 51.80896427500612, "xcomet_score": 0.9923652410507202, "xcomet_qe_score": 0.9856480360031128, "metricx_score": 0.7830808162689209, "metricx_qe_score": 1.1655035018920898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择高质量翻译的示例非常重要。", "metrics": {"bleu_score": 44.49130070801308, "chrf_score": 39.49925089961802, "xcomet_score": 0.9853969812393188, "xcomet_qe_score": 0.9871271848678589, "metricx_score": 0.41981181502342224, "metricx_qe_score": 0.48080992698669434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了从 WMT 评估的训练数据或开发数据中选择提示的情况。", "metrics": {"bleu_score": 47.16923386745769, "chrf_score": 39.78557346694636, "xcomet_score": 0.6521801352500916, "xcomet_qe_score": 0.596522331237793, "metricx_score": 3.14155650138855, "metricx_qe_score": 3.9911441802978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据更加精心制作,质量更高,训练数据更加美观,结果更好,因此", "metrics": {"bleu_score": 21.632550182469345, "chrf_score": 19.788653035836138, "xcomet_score": 0.5587770938873291, "xcomet_qe_score": 0.40438297390937805, "metricx_score": 7.902188301086426, "metricx_qe_score": 5.492517471313477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据时性能更好。", "metrics": {"bleu_score": 45.43142611141303, "chrf_score": 41.14026689146475, "xcomet_score": 0.9033088684082031, "xcomet_qe_score": 0.890548825263977, "metricx_score": 1.9992756843566895, "metricx_qe_score": 2.476839542388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专门的 SOTA 系统在翻译方面比 BART 系统有显著的优势,", "metrics": {"bleu_score": 12.645915844702076, "chrf_score": 11.826531443466926, "xcomet_score": 0.4472876489162445, "xcomet_qe_score": 0.43290629982948303, "metricx_score": 8.602360725402832, "metricx_qe_score": 7.356942176818848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但 BART 系统的表现几乎可以与商业系统媲美。", "metrics": {"bleu_score": 14.025775160081475, "chrf_score": 11.167394449519378, "xcomet_score": 0.34750795364379883, "xcomet_qe_score": 0.3515931963920593, "metricx_score": 7.264521598815918, "metricx_qe_score": 6.819813251495361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择了 Google Translate 进行对比。", "metrics": {"bleu_score": 38.99676595048009, "chrf_score": 29.188998440875043, "xcomet_score": 0.9698991775512695, "xcomet_qe_score": 0.9779642820358276, "metricx_score": 3.4631450176239014, "metricx_qe_score": 1.4924391508102417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从使用 MQM 框架进行的评估中获得的见解是,PALM 的流利度与最先进的系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 58.379342661019905, "chrf_score": 52.225895450890484, "xcomet_score": 0.901208758354187, "xcomet_qe_score": 0.8450757265090942, "metricx_score": 1.8818914890289307, "metricx_qe_score": 2.8770172595977783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是省略错误。", "metrics": {"bleu_score": 45.788313721339826, "chrf_score": 38.77358752358752, "xcomet_score": 0.7192978858947754, "xcomet_qe_score": 0.8016858100891113, "metricx_score": 2.3211562633514404, "metricx_qe_score": 0.5063758492469788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,PALM 有时会选择生成更好听的翻译,方法是删除源句子中在翻译中没有用的部分。", "metrics": {"bleu_score": 25.201775935184738, "chrf_score": 22.397184119557608, "xcomet_score": 0.9538856744766235, "xcomet_qe_score": 0.8289264440536499, "metricx_score": 1.695273995399475, "metricx_qe_score": 2.6059041023254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAN 的“外部风格”类别低于最先进的系统,这也是一个额外的信号。 该参数提供了非常流畅的输出,但仍存在一些准确性问题。", "metrics": {"bleu_score": 53.85983876407109, "chrf_score": 43.290071318967286, "xcomet_score": 0.5932482481002808, "xcomet_qe_score": 0.5882472991943359, "metricx_score": 6.368499755859375, "metricx_qe_score": 6.552608489990234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是本次非常简短的概述。", "metrics": {"bleu_score": 33.41796039044061, "chrf_score": 28.325283429185166, "xcomet_score": 0.9295156002044678, "xcomet_qe_score": 0.8785457015037537, "metricx_score": 0.6317324042320251, "metricx_qe_score": 1.1816123723983765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更多详情请来参加论文的全面演示。", "metrics": {"bleu_score": 11.15641350452643, "chrf_score": 12.968957186086167, "xcomet_score": 0.8199623823165894, "xcomet_qe_score": 0.820184051990509, "metricx_score": 2.244044542312622, "metricx_qe_score": 3.1549882888793945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是德国萨兰特大学的博士生大卫。", "metrics": {"bleu_score": 34.14504535105491, "chrf_score": 25.995252241982797, "xcomet_score": 0.7911776304244995, "xcomet_qe_score": 0.8184562921524048, "metricx_score": 0.7733900547027588, "metricx_qe_score": 0.48707151412963867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这段视频中,我想介绍我们的最新研究成果《弱于你所想:对周期性超级学习的批判性看法》。", "metrics": {"bleu_score": 33.96641981318416, "chrf_score": 29.763882596524326, "xcomet_score": 0.778157114982605, "xcomet_qe_score": 0.7601475715637207, "metricx_score": 4.601147651672363, "metricx_qe_score": 4.121705532073975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与肖宇航、马约·斯穆斯巴赫和加斯·斯蒂芬的联合工作。", "metrics": {"bleu_score": 3.889818545474848, "chrf_score": 2.8588632490399646, "xcomet_score": 0.49779725074768066, "xcomet_qe_score": 0.5694462656974792, "metricx_score": 5.421221733093262, "metricx_qe_score": 6.359198093414307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想先简要介绍一下弱监督和弱监督学习。", "metrics": {"bleu_score": 88.52140475440834, "chrf_score": 92.26506423113229, "xcomet_score": 0.8952239751815796, "xcomet_qe_score": 0.8487532138824463, "metricx_score": 0.8096938729286194, "metricx_qe_score": 2.310981273651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,您不会手动标记数据。", "metrics": {"bleu_score": 57.754002174538684, "chrf_score": 47.73436424977258, "xcomet_score": 0.8264036178588867, "xcomet_qe_score": 0.775087833404541, "metricx_score": 1.2691199779510498, "metricx_qe_score": 1.7867385149002075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,您使用弱标记源(例如简单的启发式规则、知识库或低质量的众包)来标记数据,如右图所示。", "metrics": {"bleu_score": 70.87164384329415, "chrf_score": 63.29271863223752, "xcomet_score": 0.6541715860366821, "xcomet_qe_score": 0.5908905267715454, "metricx_score": 1.8821591138839722, "metricx_qe_score": 2.158336639404297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人类标注相比,弱标注的成本要低得多,但它们也更加嘈杂,这意味着其中一定比例的标注是错误的。", "metrics": {"bleu_score": 18.192461045563807, "chrf_score": 18.96115406698577, "xcomet_score": 0.8069077730178833, "xcomet_qe_score": 0.8211005330085754, "metricx_score": 2.3901751041412354, "metricx_qe_score": 2.736393928527832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在噪声标签数据上训练神经网络,神经网络往往会记住噪声标签,而不会泛化。", "metrics": {"bleu_score": 41.68503195115237, "chrf_score": 36.40093438116725, "xcomet_score": 0.8792741894721985, "xcomet_qe_score": 0.7611357569694519, "metricx_score": 1.6520179510116577, "metricx_qe_score": 1.459134578704834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在《Weekly Supervised Learning》中,提出了在标签噪声下训练神经网络的算法,以便训练出的模型仍能很好地泛化。", "metrics": {"bleu_score": 23.541666039801147, "chrf_score": 20.633140097499787, "xcomet_score": 0.7720518112182617, "xcomet_qe_score": 0.7513952255249023, "metricx_score": 5.811883926391602, "metricx_qe_score": 6.013187885284424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的 WSL(WSL 指的是每周监督学习)研究中,人们常常声称,他们只在每周标记的数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 35.777011985455694, "chrf_score": 32.753877665346, "xcomet_score": 0.6621996164321899, "xcomet_qe_score": 0.7044065594673157, "metricx_score": 6.4915008544921875, "metricx_qe_score": 7.36121940612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并不正确,但有一个问题。 也就是说,人们假设有一个额外的干净验证集可用于模型选择。", "metrics": {"bleu_score": 62.441173185425875, "chrf_score": 56.91572689633492, "xcomet_score": 0.867836594581604, "xcomet_qe_score": 0.834889829158783, "metricx_score": 3.5260629653930664, "metricx_qe_score": 4.2258148193359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在该问题设定上停滞不前,因为这意味着在弱监督学习中需要额外的人工标注。", "metrics": {"bleu_score": 48.013878925192174, "chrf_score": 46.59283971699402, "xcomet_score": 0.8805574178695679, "xcomet_qe_score": 0.8313745260238647, "metricx_score": 2.934337615966797, "metricx_qe_score": 3.130340576171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但就像房间里的大象一样,这种必要性往往被忽视。", "metrics": {"bleu_score": 60.97595954478958, "chrf_score": 52.068142894776294, "xcomet_score": 0.9272782802581787, "xcomet_qe_score": 0.8116359710693359, "metricx_score": 1.0818692445755005, "metricx_qe_score": 2.6894094944000244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑问促使我们提出了三个研究问题。", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 45.31253098794524, "xcomet_score": 0.9559470415115356, "xcomet_qe_score": 0.9501094818115234, "metricx_score": 1.5945442914962769, "metricx_qe_score": 1.069020390510559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,WSL 需要干净的验证数据吗?或者我们可以使用噪声验证集吗?", "metrics": {"bleu_score": 55.60272926020418, "chrf_score": 47.75469677149153, "xcomet_score": 0.8674602508544922, "xcomet_qe_score": 0.8597077131271362, "metricx_score": 2.0405802726745605, "metricx_qe_score": 3.4251885414123535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净的数据,或者如果干净的数据是 WSL 正常工作的必要条件,那么我们需要多少干净的样本?", "metrics": {"bleu_score": 51.42496743953309, "chrf_score": 44.64087285509004, "xcomet_score": 0.9915693998336792, "xcomet_qe_score": 0.9779858589172363, "metricx_score": 0.8524935245513916, "metricx_qe_score": 0.9718206524848938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该只使用干净的样本进行验证,或者有更好的方法来利用它们?", "metrics": {"bleu_score": 65.70589031708288, "chrf_score": 58.38967348099228, "xcomet_score": 0.992226243019104, "xcomet_qe_score": 0.9262335300445557, "metricx_score": 0.8143229484558105, "metricx_qe_score": 1.2414913177490234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的发现如下。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 49.41553937149765, "xcomet_score": 0.9748376607894897, "xcomet_qe_score": 0.948908805847168, "metricx_score": 1.6144813299179077, "metricx_qe_score": 2.59200382232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现,有趣的是,最近的 WSL 方法确实需要干净的白色样本才能正常工作。", "metrics": {"bleu_score": 61.09755683792767, "chrf_score": 56.06981625161025, "xcomet_score": 0.8216080665588379, "xcomet_qe_score": 0.8211714029312134, "metricx_score": 2.6755917072296143, "metricx_qe_score": 3.0107407569885254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,则训练模型无法超越原始弱标签进行泛化。 这意味着培训是徒劳的。", "metrics": {"bleu_score": 44.61920029346145, "chrf_score": 38.02919200998489, "xcomet_score": 0.924372673034668, "xcomet_qe_score": 0.8400160074234009, "metricx_score": 2.421736478805542, "metricx_qe_score": 3.180018424987793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,WSL 方法实际上需要清晰标记的数据才能正常工作,并且获取干净验证样本的标注成本不容忽视。", "metrics": {"bleu_score": 51.78185231769935, "chrf_score": 48.12455463104677, "xcomet_score": 0.8401385545730591, "xcomet_qe_score": 0.824307382106781, "metricx_score": 2.3259215354919434, "metricx_qe_score": 2.977221965789795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净的验证样本数量有助于 WSL 方法实现更好的性能,如左图所示。", "metrics": {"bleu_score": 61.244553433030504, "chrf_score": 54.06046432885703, "xcomet_score": 0.9496970176696777, "xcomet_qe_score": 0.93172687292099, "metricx_score": 3.325826644897461, "metricx_qe_score": 4.233362674713135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每类 20 个样本就能获得高性能。", "metrics": {"bleu_score": 18.471370649564168, "chrf_score": 21.114507410665254, "xcomet_score": 0.9473429918289185, "xcomet_qe_score": 0.9528868198394775, "metricx_score": 2.027421474456787, "metricx_qe_score": 1.7571094036102295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这还不是故事的结局,因为如果我们无论如何都决定访问干净的样本,那么直接在它们上进行训练将会实现更好的性能。", "metrics": {"bleu_score": 38.968938554076644, "chrf_score": 32.14248404837028, "xcomet_score": 0.8949711322784424, "xcomet_qe_score": 0.8885492086410522, "metricx_score": 4.748029708862305, "metricx_qe_score": 5.198544502258301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "右图显示了直接应用于干净数据的微调方法与仅使用干净数据进行验证的 WSL 方法之间的性能差异。", "metrics": {"bleu_score": 91.44061946646029, "chrf_score": 90.47252738333282, "xcomet_score": 0.9426072835922241, "xcomet_qe_score": 0.8672120571136475, "metricx_score": 1.873011589050293, "metricx_qe_score": 2.6075549125671387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每个类别有 10 个样本,直接微调开始超越 WSL 方法。", "metrics": {"bleu_score": 52.15397472037673, "chrf_score": 47.698994840422166, "xcomet_score": 0.9509249925613403, "xcomet_qe_score": 0.915629506111145, "metricx_score": 1.8915355205535889, "metricx_qe_score": 2.991985321044922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前的 WSL 方法声称的性能改进可以通过允许在干净的验证样本上继续微调来轻松实现。", "metrics": {"bleu_score": 36.1454579336254, "chrf_score": 33.260037391970805, "xcomet_score": 0.9465950727462769, "xcomet_qe_score": 0.7833705544471741, "metricx_score": 2.0968072414398193, "metricx_qe_score": 3.4346213340759277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从图中可以看出,最初的 Vanilla 模型(FTW)的表现不如更复杂的 WSL 方法,例如 Cosine。", "metrics": {"bleu_score": 34.819274661959454, "chrf_score": 36.45917228800064, "xcomet_score": 0.9051411151885986, "xcomet_qe_score": 0.880276083946228, "metricx_score": 2.7935194969177246, "metricx_qe_score": 4.631843566894531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果我们允许在干净的样本上继续微调,那么FTW的表现与其他方法一样好。", "metrics": {"bleu_score": 50.562337394789886, "chrf_score": 45.44720799614299, "xcomet_score": 0.8552742004394531, "xcomet_qe_score": 0.8537091016769409, "metricx_score": 1.4677155017852783, "metricx_qe_score": 2.300633668899536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的 WSL 方法,因为这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 54.70197181654885, "chrf_score": 54.979716663412724, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8066851496696472, "metricx_qe_score": 1.118506669998169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结来说,我们展示了最近的 WSL 方法需要干净的、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 54.254982326546354, "chrf_score": 51.63610301504424, "xcomet_score": 0.7795165181159973, "xcomet_qe_score": 0.816158652305603, "metricx_score": 2.922353982925415, "metricx_qe_score": 3.8853847980499268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否在干净的验证样本上进行。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.34004037322114, "xcomet_score": 0.9702857732772827, "xcomet_qe_score": 0.9098553657531738, "metricx_score": 1.5602195262908936, "metricx_qe_score": 2.490739107131958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二,WSL 方法应与全监督学习基线进行比较,因为两者都在清晰的样本上工作。", "metrics": {"bleu_score": 35.773268542621025, "chrf_score": 32.486621218238874, "xcomet_score": 0.6506838798522949, "xcomet_qe_score": 0.5940153002738953, "metricx_score": 4.771507740020752, "metricx_qe_score": 5.742170333862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一个简单但强大的基线,应在未来的 WSL 工作中考虑。", "metrics": {"bleu_score": 43.991144137716105, "chrf_score": 38.290707512791094, "xcomet_score": 0.8334159851074219, "xcomet_qe_score": 0.7201470732688904, "metricx_score": 2.3546159267425537, "metricx_qe_score": 3.177734851837158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以通过幻灯片上的二维码找到它。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 46.02602096002645, "xcomet_score": 0.9958212375640869, "xcomet_qe_score": 0.982545018196106, "metricx_score": 0.46816954016685486, "metricx_qe_score": 0.38604500889778137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,并享受会议。", "metrics": {"bleu_score": 7.270717733704594, "chrf_score": 6.952784449782048, "xcomet_score": 0.802634060382843, "xcomet_qe_score": 0.838522732257843, "metricx_score": 1.5110974311828613, "metricx_qe_score": 0.9476807117462158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 10.63630283111601, "xcomet_score": 0.995103120803833, "xcomet_qe_score": 0.9681702852249146, "metricx_score": 0.23110710084438324, "metricx_qe_score": 0.18293818831443787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是萨拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6017594337463379, "metricx_qe_score": 0.9140466451644897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将为大家介绍 ABC-Eval,这是一种新的维度方法,用于评估对话式人工智能。", "metrics": {"bleu_score": 37.1749241029078, "chrf_score": 45.38954259510851, "xcomet_score": 0.9315991401672363, "xcomet_qe_score": 0.9843446016311646, "metricx_score": 1.1558408737182617, "metricx_qe_score": 1.2986398935317993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学自然语言处理实验室完成,由埃默里大学的乔伊教授领导,并与亚马逊Alexa AI合作。", "metrics": {"bleu_score": 27.667683760831373, "chrf_score": 29.23911403199598, "xcomet_score": 0.7919567823410034, "xcomet_qe_score": 0.8032825589179993, "metricx_score": 3.61444091796875, "metricx_qe_score": 3.4458365440368652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚刚开发了一个对话模型,并且想看看它与当前最先进的模型相比表现如何。", "metrics": {"bleu_score": 70.76281432033797, "chrf_score": 65.04362007406915, "xcomet_score": 0.9985785484313965, "xcomet_qe_score": 0.9907615184783936, "metricx_score": 0.445792555809021, "metricx_qe_score": 0.4214619994163513, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人类评估,例如让人类评委选择两个对话中哪个更好,或者根据李克特量表对对话进行评分。", "metrics": {"bleu_score": 62.150250294269725, "chrf_score": 54.600892671358906, "xcomet_score": 0.9169729351997375, "xcomet_qe_score": 0.9640536308288574, "metricx_score": 0.5647106766700745, "metricx_qe_score": 0.8497169017791748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法可以很好地提供对整体对话质量的全面评估,但对话质量有许多方面。", "metrics": {"bleu_score": 61.12230521472951, "chrf_score": 53.82332073174639, "xcomet_score": 0.9378622174263, "xcomet_qe_score": 0.9177502393722534, "metricx_score": 0.5398129224777222, "metricx_qe_score": 0.6863597631454468, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以更细致地了解模型的优势和劣势。", "metrics": {"bleu_score": 56.55095706824206, "chrf_score": 52.33122597369585, "xcomet_score": 0.9817134141921997, "xcomet_qe_score": 0.9662775993347168, "metricx_score": 0.598365306854248, "metricx_qe_score": 0.7575493454933167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是让人类评委使用现有的比较或李克特量表方法来评估对话质量的几个维度,例如模型响应的相关性。", "metrics": {"bleu_score": 62.69734312273033, "chrf_score": 56.3389384480846, "xcomet_score": 0.9728561639785767, "xcomet_qe_score": 0.9648977518081665, "metricx_score": 1.298758864402771, "metricx_qe_score": 1.7644050121307373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们相信有一种更精确、更可靠的策略来评估对话的维度。", "metrics": {"bleu_score": 28.190471507804133, "chrf_score": 31.026407375564734, "xcomet_score": 0.9805120229721069, "xcomet_qe_score": 0.9234852194786072, "metricx_score": 2.5822675228118896, "metricx_qe_score": 1.9409730434417725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表现出某些行为(例如,提供无关信息或自相矛盾)来减少人类评估的主观性。", "metrics": {"bleu_score": 56.727957532978806, "chrf_score": 47.636107264298175, "xcomet_score": 0.9630287885665894, "xcomet_qe_score": 0.9704082012176514, "metricx_score": 1.4908740520477295, "metricx_qe_score": 1.9567135572433472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为聊天行为注释,简称 ABC 评估。", "metrics": {"bleu_score": 42.07113991220077, "chrf_score": 36.202567156788085, "xcomet_score": 0.7756549119949341, "xcomet_qe_score": 0.7985080480575562, "metricx_score": 1.4712601900100708, "metricx_qe_score": 1.2474312782287598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面涵盖最近文献中建议影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 65.61749795782538, "chrf_score": 58.71808301792015, "xcomet_score": 0.8552060127258301, "xcomet_qe_score": 0.8022760152816772, "metricx_score": 2.071110486984253, "metricx_qe_score": 3.8771698474884033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC-Eval 能够衡量聊天模型犯各种主题错误的速率。", "metrics": {"bleu_score": 62.860788360356985, "chrf_score": 64.73864160271712, "xcomet_score": 0.7911314368247986, "xcomet_qe_score": 0.8027495741844177, "metricx_score": 2.151057720184326, "metricx_qe_score": 3.2143394947052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABC-Eval 衡量聊天模型忽略其对话者或说出无关内容的回合数。 当模型自相矛盾或与其合作伙伴矛盾,当模型产生不正确的事实或违反常识时,以及当模型成功或失败地表现出同理心时。", "metrics": {"bleu_score": 42.087424344353586, "chrf_score": 39.678329236529564, "xcomet_score": 0.5307931900024414, "xcomet_qe_score": 0.6209592819213867, "metricx_score": 4.194764137268066, "metricx_qe_score": 4.905986785888672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方式最有效,我们选择了四种最先进的聊天模型,并使用 ABC-Eval 对每种模型进行了 100 次人机对话评估。", "metrics": {"bleu_score": 69.08192393008764, "chrf_score": 67.31970724127405, "xcomet_score": 0.9719651937484741, "xcomet_qe_score": 0.9595606327056885, "metricx_score": 0.7143547534942627, "metricx_qe_score": 0.6896293759346008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用了三种现有方法来评估这些对话:基于回合的Likert评分、基于对话的Likert评分和基于对话的成对比较。", "metrics": {"bleu_score": 41.58679493127088, "chrf_score": 35.569773682947954, "xcomet_score": 0.8072388768196106, "xcomet_qe_score": 0.7809343338012695, "metricx_score": 3.861069917678833, "metricx_qe_score": 2.9692957401275635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每种现有的方法,我们都收集了对对话中最常见的八个方面的评估,因为这是评估聊天模型的标准做法,以多个维度为基础。 ", "metrics": {"bleu_score": 48.33482678326476, "chrf_score": 42.31613854689318, "xcomet_score": 0.7458940744400024, "xcomet_qe_score": 0.6289910078048706, "metricx_score": 2.252469778060913, "metricx_qe_score": 2.633060932159424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现 ABC 行为标签的可靠性总体上优于现有方法收集的标签,这是通过 100 个双重标记的对话中的内部标注者一致性来衡量的。", "metrics": {"bleu_score": 31.59071189874352, "chrf_score": 31.81099975531109, "xcomet_score": 0.7032830119132996, "xcomet_qe_score": 0.7180886268615723, "metricx_score": 5.0244269371032715, "metricx_qe_score": 5.763114929199219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,ABC评估标签比现有方法产生的指标更能预测整体对话质量,如本简单线性回归分析所示。", "metrics": {"bleu_score": 47.20754861848806, "chrf_score": 39.17267873066594, "xcomet_score": 0.9547319412231445, "xcomet_qe_score": 0.9657926559448242, "metricx_score": 1.238560438156128, "metricx_qe_score": 1.191879391670227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,你可以看到,测量自相矛盾和搭档矛盾的比例,分别解释了对话质量的 5% 和 10%,而平均的李克特一致性得分只解释了 4% 或更少。", "metrics": {"bleu_score": 53.02191476383753, "chrf_score": 44.54001593915914, "xcomet_score": 0.702691912651062, "xcomet_qe_score": 0.7593705058097839, "metricx_score": 5.489645004272461, "metricx_qe_score": 5.536769390106201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查每个评估指标是否捕捉到聊天质量的独特方面。", "metrics": {"bleu_score": 75.97383875533318, "chrf_score": 69.308128353932, "xcomet_score": 0.8655691146850586, "xcomet_qe_score": 0.8892362713813782, "metricx_score": 1.4629104137420654, "metricx_qe_score": 1.544531226158142, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以看到,所有 ABC 评估指标的组合可以解释超过 25% 的对话质量,而当你逐一移除这些指标时,大多数指标的移除都会导致对质量信息的大量丢失。", "metrics": {"bleu_score": 32.981803368266114, "chrf_score": 33.00949256448417, "xcomet_score": 0.9330389499664307, "xcomet_qe_score": 0.9039727449417114, "metricx_score": 3.690171718597412, "metricx_qe_score": 3.4921436309814453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有转弯水平的Likert度量的组合解释了质量的很小一部分,其中较少的度量包含独特信息。 这些可靠、信息丰", "metrics": {"bleu_score": 20.93479658159551, "chrf_score": 19.457641370647877, "xcomet_score": 0.5097184181213379, "xcomet_qe_score": 0.5301562547683716, "metricx_score": 11.560599327087402, "metricx_qe_score": 7.736968994140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "富且独特的 ABC 评估指标使我们能够以比以往方法更高的分辨率来评估对话式人工智能。", "metrics": {"bleu_score": 3.689162246036907, "chrf_score": 7.469101213754067, "xcomet_score": 0.516302227973938, "xcomet_qe_score": 0.7127390503883362, "metricx_score": 5.247989654541016, "metricx_qe_score": 4.584092617034912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们的实验结果可以看出,仍然存在一些挑战,并且这些挑战已经被精确地量化。", "metrics": {"bleu_score": 34.74483058545365, "chrf_score": 36.58097411025148, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6601868867874146, "metricx_qe_score": 0.6917024850845337, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人在大约 20% 的回应中违反了常识。", "metrics": {"bleu_score": 79.75219547461045, "chrf_score": 74.72493515609457, "xcomet_score": 0.8747350573539734, "xcomet_qe_score": 0.875426709651947, "metricx_score": 0.9440527558326721, "metricx_qe_score": 1.574135661125183, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "他们在大约 15% 的回应中提供无关信息,并且在大约 10% 的时间里自相矛盾或与搭档相矛盾。", "metrics": {"bleu_score": 39.9535222644849, "chrf_score": 38.13471290732772, "xcomet_score": 0.8053238391876221, "xcomet_qe_score": 0.8428950309753418, "metricx_score": 4.48850154876709, "metricx_qe_score": 3.5826523303985596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域快速发展,自我们评估以来发布的新模型可能会看到这些错误率的下降。", "metrics": {"bleu_score": 33.399135739830406, "chrf_score": 28.99821643270387, "xcomet_score": 0.9886012077331543, "xcomet_qe_score": 0.9798378944396973, "metricx_score": 2.729868173599243, "metricx_qe_score": 2.471414804458618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这也正是我们需要追求可靠和精确的评估指标来比较模型的原因。", "metrics": {"bleu_score": 41.794725442553194, "chrf_score": 41.08949539605368, "xcomet_score": 0.9995089769363403, "xcomet_qe_score": 0.9968080520629883, "metricx_score": 0.7706642150878906, "metricx_qe_score": 0.8102351427078247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABC Eval 能够被该领域的其他人利用,作为朝着这个方向迈出的有意义的一步,", "metrics": {"bleu_score": 68.16154651206611, "chrf_score": 66.27027465714927, "xcomet_score": 0.9785729646682739, "xcomet_qe_score": 0.9730373620986938, "metricx_score": 1.5697365999221802, "metricx_qe_score": 1.6896462440490723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待看到在未来几个月和几年里,对话式人工智能将如何发展。", "metrics": {"bleu_score": 47.87768812552473, "chrf_score": 47.614958169009505, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.5321853160858154, "metricx_qe_score": 0.5511474609375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫凯奥·杨,我将介绍我们的研究成果,题目是《翻译何时需要上下文?", "metrics": {"bleu_score": 44.78344190407162, "chrf_score": 38.94380510189333, "xcomet_score": 0.9685804843902588, "xcomet_qe_score": 0.9757868051528931, "metricx_score": 0.868246853351593, "metricx_qe_score": 1.0783987045288086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语言探索》。", "metrics": {"bleu_score": 67.0422683816333, "chrf_score": 60.745550745550744, "xcomet_score": 0.9882596731185913, "xcomet_qe_score": 0.9236876964569092, "metricx_score": 0.9823226928710938, "metricx_qe_score": 1.2916620969772339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项研究是与帕特里克·弗兰纳奇、艾米·刘、安德烈·F.D.马丁斯和格雷姆·纽比格合作完成的。", "metrics": {"bleu_score": 12.380098140048577, "chrf_score": 8.386989840432008, "xcomet_score": 0.8686596155166626, "xcomet_qe_score": 0.8445602655410767, "metricx_score": 2.6564571857452393, "metricx_qe_score": 3.1523547172546387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都取决于上下文。", "metrics": {"bleu_score": 63.40466277046863, "chrf_score": 59.27762653106322, "xcomet_score": 0.9980931282043457, "xcomet_qe_score": 0.9876047372817993, "metricx_score": 0.09006089717149734, "metricx_qe_score": 0.15374323725700378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译“mole” 这个句子?", "metrics": {"bleu_score": 38.32987170937475, "chrf_score": 40.868040762447215, "xcomet_score": 0.8948217034339905, "xcomet_qe_score": 0.8572821021080017, "metricx_score": 1.4052643775939941, "metricx_qe_score": 2.5421695709228516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句是“如果大臣们发现了,事情可能会变得危险”,那么“痣”指的是间谍。", "metrics": {"bleu_score": 12.535489536409832, "chrf_score": 7.115678571529324, "xcomet_score": 0.8076396584510803, "xcomet_qe_score": 0.8188278079032898, "metricx_score": 6.779663562774658, "metricx_qe_score": 6.565973281860352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,会是严重的问题吗?”那么“痣”指的是胎记。", "metrics": {"bleu_score": 11.236183829518307, "chrf_score": 9.931621279679554, "xcomet_score": 0.917122483253479, "xcomet_qe_score": 0.9305028319358826, "metricx_score": 1.855743169784546, "metricx_qe_score": 2.0087506771087646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,单词的意思会发生变化,因此其翻译也会随之改变。", "metrics": {"bleu_score": 45.66718758738642, "chrf_score": 37.58167766057298, "xcomet_score": 0.987981915473938, "xcomet_qe_score": 0.9740796089172363, "metricx_score": 0.21187812089920044, "metricx_qe_score": 0.2810722589492798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型如何处理这些情况是非常困难的。", "metrics": {"bleu_score": 37.76302796471566, "chrf_score": 32.03580455001594, "xcomet_score": 0.9781463146209717, "xcomet_qe_score": 0.9839218854904175, "metricx_score": 0.9633798003196716, "metricx_qe_score": 1.1256078481674194, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有少量的翻译依赖于上下文,这使得像蓝色这样的语料库级别的指标无法捕捉到这些翻译。", "metrics": {"bleu_score": 37.06530858558958, "chrf_score": 30.662561865302656, "xcomet_score": 0.8819591999053955, "xcomet_qe_score": 0.8699669241905212, "metricx_score": 5.603123664855957, "metricx_qe_score": 5.491528511047363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对上下文相关的翻译进行有针对性的评估,但这些资源只支持有限类型的上下文相关翻译和有限的语言集,因为它们通常依赖于领域知识和人工编辑。", "metrics": {"bleu_score": 76.87733733486884, "chrf_score": 70.77058839332716, "xcomet_score": 0.9579368829727173, "xcomet_qe_score": 0.9483213424682617, "metricx_score": 1.360413908958435, "metricx_qe_score": 1.225138783454895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在这些情况下的表现如何?", "metrics": {"bleu_score": 40.74362040846931, "chrf_score": 32.38562487593799, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.43174564838409424, "metricx_qe_score": 0.4130174219608307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了在翻译中一个词对上下文的依赖程度。", "metrics": {"bleu_score": 61.04680067673333, "chrf_score": 53.1788659738747, "xcomet_score": 0.9984127283096313, "xcomet_qe_score": 1.0, "metricx_score": 4.900334358215332, "metricx_qe_score": 5.01477575302124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的工作中,我们引入了 CXMI,作为机器翻译模型使用上下文的度量。", "metrics": {"bleu_score": 46.12991919689678, "chrf_score": 44.423236789569955, "xcomet_score": 0.9032961130142212, "xcomet_qe_score": 0.9066238403320312, "metricx_score": 2.6339917182922363, "metricx_qe_score": 2.9652202129364014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是通过测量上下文 C 在给定源 X 的情况下提供了多少关于目标 Y 的信息来实现的。 你可以把 CXMI 看作是从给模型提供上下文中获得的信息。", "metrics": {"bleu_score": 62.94359316998928, "chrf_score": 53.50072462443598, "xcomet_score": 0.927895188331604, "xcomet_qe_score": 0.6947176456451416, "metricx_score": 2.829451560974121, "metricx_qe_score": 3.4151298999786377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将 CXMI 扩展到 pointwise CXMI,它可以在句子级别或单词级别测量上下文使用情况。", "metrics": {"bleu_score": 46.62794412446464, "chrf_score": 52.59299747421411, "xcomet_score": 0.9427655935287476, "xcomet_qe_score": 0.8794594407081604, "metricx_score": 3.239715814590454, "metricx_qe_score": 5.2469482421875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将具有高 P6MI 的单词视为需要上下文进行翻译的单词。", "metrics": {"bleu_score": 62.147423775880156, "chrf_score": 53.24316788101523, "xcomet_score": 0.7891688346862793, "xcomet_qe_score": 0.8224157691001892, "metricx_score": 6.087286949157715, "metricx_qe_score": 5.9707136154174805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析高PCSMI的单词,以寻找这些单词之间的模式。", "metrics": {"bleu_score": 20.914789426434986, "chrf_score": 23.287394688204486, "xcomet_score": 0.9191923141479492, "xcomet_qe_score": 0.8826931715011597, "metricx_score": 3.677799940109253, "metricx_qe_score": 4.166487216949463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成 14 种不同语言的 TED 演讲稿进行了分析。", "metrics": {"bleu_score": 72.56398949150912, "chrf_score": 70.58808653695228, "xcomet_score": 0.9972835779190063, "xcomet_qe_score": 0.9925276041030884, "metricx_score": 0.7381255626678467, "metricx_qe_score": 1.0282196998596191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同的层面上进行分析。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9992449283599854, "xcomet_qe_score": 0.9950920343399048, "metricx_score": 0.22764378786087036, "metricx_qe_score": 0.23440968990325928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们研究那些具有高平均 PCXMI 的语音标签。", "metrics": {"bleu_score": 11.762897816355773, "chrf_score": 19.804833612644998, "xcomet_score": 0.8099685311317444, "xcomet_qe_score": 0.842397928237915, "metricx_score": 2.3382015228271484, "metricx_qe_score": 2.7244443893432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到阿拉伯语中的双数代词,它们的 P6MI 相对较高。", "metrics": {"bleu_score": 65.58090491210355, "chrf_score": 50.53390735573077, "xcomet_score": 0.8325006365776062, "xcomet_qe_score": 0.6910053491592407, "metricx_score": 5.859896659851074, "metricx_qe_score": 5.5070929527282715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为英语中没有双数代词,因此在将其翻译成阿拉伯语时,需要上下文来确定代词是否为双数。", "metrics": {"bleu_score": 56.24065932553485, "chrf_score": 50.47917752897251, "xcomet_score": 0.9938430786132812, "xcomet_qe_score": 0.9982346296310425, "metricx_score": 0.7463011145591736, "metricx_qe_score": 1.4949605464935303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现某些语言在选择合适的动词形式时也需要上下文。", "metrics": {"bleu_score": 79.13620867780168, "chrf_score": 73.48074700948266, "xcomet_score": 0.9988840818405151, "xcomet_qe_score": 0.9927463531494141, "metricx_score": 0.5310014486312866, "metricx_qe_score": 0.7646585702896118, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们研究了在所有不同出现中平均具有高P600的词汇项目。", "metrics": {"bleu_score": 44.78039777693801, "chrf_score": 36.39344346570126, "xcomet_score": 0.6705057621002197, "xcomet_qe_score": 0.7310512661933899, "metricx_score": 6.837773323059082, "metricx_qe_score": 6.819599151611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别出像这里这样的情况,在中文中,你需要上下文来翻译专有名词,以确保在文档中使用相同的翻译。", "metrics": {"bleu_score": 41.9998267517866, "chrf_score": 36.3359260272048, "xcomet_score": 0.8193046450614929, "xcomet_qe_score": 0.887069821357727, "metricx_score": 0.7823087573051453, "metricx_qe_score": 1.0973660945892334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现上下文有助于翻译出正确的语气。", "metrics": {"bleu_score": 36.74817321465015, "chrf_score": 32.04816684241473, "xcomet_score": 0.9323930740356445, "xcomet_qe_score": 0.9106964468955994, "metricx_score": 0.7756413817405701, "metricx_qe_score": 0.678656816482544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们来看看具有高PXMI的不同个体代币。", "metrics": {"bleu_score": 11.451997463067546, "chrf_score": 15.718942610270412, "xcomet_score": 0.840306282043457, "xcomet_qe_score": 0.8850088119506836, "metricx_score": 4.775888442993164, "metricx_qe_score": 3.4555790424346924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出那些不能通过单词本身来捕捉的现象,而只能通过句子结构来表达的现象,例如省略号解析。", "metrics": {"bleu_score": 46.97743845788447, "chrf_score": 40.067425066362446, "xcomet_score": 0.9264641404151917, "xcomet_qe_score": 0.8561687469482422, "metricx_score": 0.869813859462738, "metricx_qe_score": 1.1892766952514648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们利用分析结果,为文档级翻译设计基准。", "metrics": {"bleu_score": 33.83665048061162, "chrf_score": 28.172785713327986, "xcomet_score": 0.9945025444030762, "xcomet_qe_score": 0.9251377582550049, "metricx_score": 0.9935649633407593, "metricx_qe_score": 1.3573462963104248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们确定的五种话语现象中的每一种,我们都创建了标记器,以自动识别属于该现象的词语", "metrics": {"bleu_score": 56.74258211497769, "chrf_score": 49.348941600662144, "xcomet_score": 0.9583935737609863, "xcomet_qe_score": 0.909460186958313, "metricx_score": 1.3534151315689087, "metricx_qe_score": 2.0701239109039307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们称我们的标记器为多语言话语意识标记器(MUDA)。 我们还可以", "metrics": {"bleu_score": 21.631187459215713, "chrf_score": 21.300901928906896, "xcomet_score": 0.40294837951660156, "xcomet_qe_score": 0.3889786899089813, "metricx_score": 7.0405683517456055, "metricx_qe_score": 2.9393575191497803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "注意到,不同的语言有不同比例的这些话语现象。 然后,我们使用MU", "metrics": {"bleu_score": 28.73626697403915, "chrf_score": 27.598405198521945, "xcomet_score": 0.3874492943286896, "xcomet_qe_score": 0.35732388496398926, "metricx_score": 9.437232971191406, "metricx_qe_score": 6.591233253479004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DA标记器,将标记器应用于我们想要用于评估的平行语料库,并将我们选择的翻译指标应用于MUDA标记器识别的上下文相关示例。", "metrics": {"bleu_score": 75.18920615811683, "chrf_score": 70.18320946137294, "xcomet_score": 0.5508264303207397, "xcomet_qe_score": 0.4367925822734833, "metricx_score": 5.674858093261719, "metricx_qe_score": 6.804760932922363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用基准和其他指标来评估不同模型在文档级别的机器翻译方面的表现。", "metrics": {"bleu_score": 41.33756822254692, "chrf_score": 35.13663695591179, "xcomet_score": 0.9059288501739502, "xcomet_qe_score": 0.8556272983551025, "metricx_score": 0.6705664396286011, "metricx_qe_score": 0.8510909080505371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,对于蓝色,我们发现上下文无关的模型表现最佳。", "metrics": {"bleu_score": 54.05040859282924, "chrf_score": 42.593666432175844, "xcomet_score": 0.8056762218475342, "xcomet_qe_score": 0.7333620190620422, "metricx_score": 2.1903276443481445, "metricx_qe_score": 1.7142637968063354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果使用CODIT,那么上下文感知模型表现最佳。", "metrics": {"bleu_score": 36.498993752287106, "chrf_score": 32.16601310739286, "xcomet_score": 0.8566733598709106, "xcomet_qe_score": 0.8011702299118042, "metricx_score": 5.081580638885498, "metricx_qe_score": 5.852693557739258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用词F值,那么有上下文和无上下文的模型表现相当。", "metrics": {"bleu_score": 58.62712716411925, "chrf_score": 53.083646269100846, "xcomet_score": 0.8473780155181885, "xcomet_qe_score": 0.7792725563049316, "metricx_score": 1.8965039253234863, "metricx_qe_score": 2.7257912158966064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果仅使用语料库级别的指标,很难确定最佳的文档级别翻译系统。", "metrics": {"bleu_score": 62.87033726649025, "chrf_score": 52.63870434944914, "xcomet_score": 0.997543454170227, "xcomet_qe_score": 0.9938198328018188, "metricx_score": 0.7431210279464722, "metricx_qe_score": 0.9062103033065796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用MUDA基准来评估模型,发现对于某些话语现象(例如正式性和词汇连贯性),上下文感知模型比不使用上下文的模型更准确。", "metrics": {"bleu_score": 62.66206841011848, "chrf_score": 57.54907560135318, "xcomet_score": 0.9161975383758545, "xcomet_qe_score": 0.7888062000274658, "metricx_score": 1.759408950805664, "metricx_qe_score": 2.4885780811309814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,这些模型在处理省略、代词和动词形式等其他现象时,并不比不使用上下文的模型好多少。", "metrics": {"bleu_score": 64.92207025204772, "chrf_score": 59.40206513483486, "xcomet_score": 0.983965277671814, "xcomet_qe_score": 0.9761863946914673, "metricx_score": 0.6329665184020996, "metricx_qe_score": 0.6626753211021423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明我们需要在文档级别的翻译中取得更多进展。", "metrics": {"bleu_score": 26.602643444883117, "chrf_score": 25.420893308302023, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9084783792495728, "metricx_qe_score": 0.9241721034049988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试表明,DeepL 通常比 Google Translate 更准确,用于文档级别的翻译。", "metrics": {"bleu_score": 49.33769793121517, "chrf_score": 45.19974061723345, "xcomet_score": 0.8479087352752686, "xcomet_qe_score": 0.7687045335769653, "metricx_score": 2.2514593601226807, "metricx_qe_score": 1.878220558166504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结来说,我们对 14 种语言对进行了数据驱动的分析,以确定何时需要上下文来进行翻译。 然后,我们利用这些发现来构建文档级别机器翻译的基准,这可以帮助我们确定哪些话语现象模型能够很好地处理,哪些不能,以及哪些翻译系统擅长文档级别的翻译。", "metrics": {"bleu_score": 54.33115763670694, "chrf_score": 52.68641719401922, "xcomet_score": 0.9655625820159912, "xcomet_qe_score": 0.9302518367767334, "metricx_score": 3.1152191162109375, "metricx_qe_score": 3.855318784713745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在多伦多见。", "metrics": {"bleu_score": 71.65313105737896, "chrf_score": 64.65405545478103, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4812854528427124, "metricx_qe_score": 1.2813191413879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Yannis Lavrac,我将向大家介绍我们在生物医学和临床领域的法语机器学习模型Dr. BERT的研究成果。", "metrics": {"bleu_score": 45.69715470753509, "chrf_score": 42.913845101285304, "xcomet_score": 0.7746421098709106, "xcomet_qe_score": 0.7847126722335815, "metricx_score": 2.1702046394348145, "metricx_qe_score": 3.1870715618133545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论医疗保健中的语言建模。", "metrics": {"bleu_score": 56.50858546816402, "chrf_score": 44.42216569470257, "xcomet_score": 0.9920666217803955, "xcomet_qe_score": 0.9867169857025146, "metricx_score": 0.4082871079444885, "metricx_qe_score": 0.5630125403404236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个法语生物医学模型,名为 Dr. Bert,它基于 Roberta,并使用 Nachos 进行训练,这是一个从网络上收集的医学数据集。", "metrics": {"bleu_score": 32.64717716402557, "chrf_score": 24.163940923004223, "xcomet_score": 0.8327181935310364, "xcomet_qe_score": 0.7782255411148071, "metricx_score": 2.393948793411255, "metricx_qe_score": 2.844597101211548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了在多种预训练设置和数据源下的模型比较。", "metrics": {"bleu_score": 69.60917409740965, "chrf_score": 59.269998117824194, "xcomet_score": 0.9951032400131226, "xcomet_qe_score": 0.9844486713409424, "metricx_score": 0.5561999082565308, "metricx_qe_score": 0.8577618598937988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在法语中展示了在 11 项生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 50.367375646576846, "chrf_score": 44.02172681580357, "xcomet_score": 0.7330729961395264, "xcomet_qe_score": 0.8259665966033936, "metricx_score": 2.4109880924224854, "metricx_qe_score": 3.166673183441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结实验,并提供更多有关如何访问模型的详细信息。", "metrics": {"bleu_score": 37.35888380322715, "chrf_score": 32.408859919569004, "xcomet_score": 0.8855956792831421, "xcomet_qe_score": 0.7916938066482544, "metricx_score": 0.3736017346382141, "metricx_qe_score": 0.3246799111366272, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自 2018 年发布以来,BERT 已成为解决自然语言处理任务的最有效方法之一,并与历史上静态和上下文化方法(如 Word2Vec、FastText 或 GloVe)相比,提供了巨大的性能提升。 自那", "metrics": {"bleu_score": 56.53225921910368, "chrf_score": 55.532594385663955, "xcomet_score": 0.5899861454963684, "xcomet_qe_score": 0.531723141670227, "metricx_score": 5.571988105773926, "metricx_qe_score": 5.9758148193359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以后,该模型已被改编为许多其他语言,例如法语中的CamemBERT,以及其他领域,例如生物医学中的BioMedBERT和BioBERT,以及临床中的ClinicalBERT,但主要是英语。", "metrics": {"bleu_score": 35.074409898182836, "chrf_score": 51.76476228000333, "xcomet_score": 0.8862456679344177, "xcomet_qe_score": 0.7893863916397095, "metricx_score": 2.26186203956604, "metricx_qe_score": 2.9161148071289062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专业模型很少,而且通常是基于持续预训练的,因为缺乏领域内的数据。", "metrics": {"bleu_score": 39.013316550833295, "chrf_score": 36.197868934412526, "xcomet_score": 0.8415342569351196, "xcomet_qe_score": 0.8208246231079102, "metricx_score": 1.2971456050872803, "metricx_qe_score": 1.5030303001403809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法国在生物医学方面还没有开源模型。", "metrics": {"bleu_score": 26.911732453381234, "chrf_score": 23.133722604127517, "xcomet_score": 0.9839566946029663, "xcomet_qe_score": 0.8838616013526917, "metricx_score": 2.0740184783935547, "metricx_qe_score": 2.0338902473449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们问自己,什么是最合适的数据来源,以满足广泛的使用需求?这些数据是否可以替代临床数据?", "metrics": {"bleu_score": 13.699985749596946, "chrf_score": 17.455170191419636, "xcomet_score": 0.9793134927749634, "xcomet_qe_score": 0.9816603660583496, "metricx_score": 1.015670657157898, "metricx_qe_score": 1.3759537935256958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将 Dr. BERT 与我们的 Schubert 模型进行了比较,该模型基于我们医院获得的匿名化数据。", "metrics": {"bleu_score": 44.02304528559002, "chrf_score": 39.85205441504416, "xcomet_score": 0.7733302712440491, "xcomet_qe_score": 0.7675200700759888, "metricx_score": 4.680521011352539, "metricx_qe_score": 5.697765350341797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们问自己,训练一个专门针对法语数据的模型需要多少数据?", "metrics": {"bleu_score": 40.32293593894517, "chrf_score": 33.40643474551529, "xcomet_score": 0.9934067726135254, "xcomet_qe_score": 0.913144052028656, "metricx_score": 0.7160097360610962, "metricx_qe_score": 0.548089325428009, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是 4 GB、8 GB 还是更多?", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 90.21205646205644, "xcomet_score": 0.9791398048400879, "xcomet_qe_score": 0.9612053036689758, "metricx_score": 0.2863093912601471, "metricx_qe_score": 0.6383348107337952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四个从头开始的模型。第一个版本是带有 7GB Nachos 的 DrBERT,第二个版本是带有 4GB Nachos 的子集。 第一个版本的Schubert是一个临床模型,包含4GB的临床笔记句子。最终版本的Schubert是由4GB的Naturos子集和4GB的临床笔记混合而成。", "metrics": {"bleu_score": 52.54043827692467, "chrf_score": 41.04095628448214, "xcomet_score": 0.3359995484352112, "xcomet_qe_score": 0.3268549144268036, "metricx_score": 7.308676242828369, "metricx_qe_score": 7.117519378662109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还引入了三个模型,在连续预训练中进行训练,以分析预训练策略的影响。", "metrics": {"bleu_score": 60.073666134668244, "chrf_score": 52.3812658071323, "xcomet_score": 0.8840289115905762, "xcomet_qe_score": 0.8570731282234192, "metricx_score": 2.2327311038970947, "metricx_qe_score": 3.2982287406921387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于卡门贝奶酪的权重,训练在 Nachos 的 4GB 子集上;", "metrics": {"bleu_score": 19.28576545653752, "chrf_score": 15.257668079928976, "xcomet_score": 0.4694799482822418, "xcomet_qe_score": 0.40018776059150696, "metricx_score": 5.244306564331055, "metricx_qe_score": 6.63497257232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也是基于卡门贝奶酪,但这次训练在 Clinkernots 的 4GB 上。 最后,一个基于英语生物医学模型的BERT,并训练了4GB的SciNLP子集。", "metrics": {"bleu_score": 35.54689406123226, "chrf_score": 27.844580699613093, "xcomet_score": 0.3071689009666443, "xcomet_qe_score": 0.31893181800842285, "metricx_score": 11.110946655273438, "metricx_qe_score": 11.228216171264648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七种模型,我们收集了各种公共和私有的 NLP 任务,例如命名实体识别、分类、部分语音标记和问答。", "metrics": {"bleu_score": 51.972067224600885, "chrf_score": 43.29512560055154, "xcomet_score": 0.7592381238937378, "xcomet_qe_score": 0.7436748743057251, "metricx_score": 2.1574082374572754, "metricx_qe_score": 3.2105860710144043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行了比较,包括 CamemBERT-Oscar 138GB、CamemBERT-Oscar 4GB、CamemBERT-CCNet 4GB、PermedBERT、BioBERT 和 ClinicalBERT。", "metrics": {"bleu_score": 48.78966726632138, "chrf_score": 67.29202099555775, "xcomet_score": 0.5952532291412354, "xcomet_qe_score": 0.5159731507301331, "metricx_score": 3.8768157958984375, "metricx_qe_score": 3.8766045570373535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,该模型在与训练数据相同的任务上表现最佳。", "metrics": {"bleu_score": 32.85990441480407, "chrf_score": 28.829364046103194, "xcomet_score": 0.9928319454193115, "xcomet_qe_score": 0.9919803142547607, "metricx_score": 1.210141897201538, "metricx_qe_score": 1.3219690322875977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从异质数据源获取数据,我们观察到异质数据源的数据似乎更加多样化。", "metrics": {"bleu_score": 28.67045406907234, "chrf_score": 33.30007069993585, "xcomet_score": 0.7394137382507324, "xcomet_qe_score": 0.6923782825469971, "metricx_score": 2.3945767879486084, "metricx_qe_score": 1.6164485216140747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据可以转化为更好的性能。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 47.772497645834925, "xcomet_score": 0.9756163358688354, "xcomet_qe_score": 0.9722273349761963, "metricx_score": 3.2136597633361816, "metricx_qe_score": 4.146425247192383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始的预训练似乎在大多数任务上获得了更高的性能。", "metrics": {"bleu_score": 52.839726107318825, "chrf_score": 50.100716208730965, "xcomet_score": 0.9388210773468018, "xcomet_qe_score": 0.8811310529708862, "metricx_score": 3.1967966556549072, "metricx_qe_score": 3.9971885681152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们在使用PumiceBERT的权重和分词器的持续预训练实验中,在4GB的NACHOS子集上进行训练,结果与从头开始训练的DrBERT 4GB相当。", "metrics": {"bleu_score": 37.82839322616071, "chrf_score": 43.53778546696318, "xcomet_score": 0.5820633172988892, "xcomet_qe_score": 0.6575489044189453, "metricx_score": 6.753438949584961, "metricx_qe_score": 6.173501014709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与基于卡门贝尔权重和令牌化器的模型不同,后者存在稳定性问题。", "metrics": {"bleu_score": 30.60871959068714, "chrf_score": 22.177614979982827, "xcomet_score": 0.8134715557098389, "xcomet_qe_score": 0.848053514957428, "metricx_score": 3.9472286701202393, "metricx_qe_score": 3.8239738941192627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们的专有系统在 11 个 DONTRIM 任务中的 9 个任务上表现更好,并且在全局上超越了通用模型 CamemBERT 的结果。", "metrics": {"bleu_score": 32.90933745193626, "chrf_score": 35.17381131083689, "xcomet_score": 0.8703891038894653, "xcomet_qe_score": 0.9059216976165771, "metricx_score": 4.8932271003723145, "metricx_qe_score": 4.471039772033691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业数据更好,更多的专业数据更好,但它的扩展性不佳。", "metrics": {"bleu_score": 27.410634501902916, "chrf_score": 25.80563177206162, "xcomet_score": 0.8090998530387878, "xcomet_qe_score": 0.7483420372009277, "metricx_score": 3.322998523712158, "metricx_qe_score": 3.7475481033325195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 Nachos 获得的所有预训练模型都可以在 Hugging Face 上免费获得,所有的训练脚本都在我们的 GitHub 仓库中。", "metrics": {"bleu_score": 48.45320715646803, "chrf_score": 51.18270337545816, "xcomet_score": 0.9000158309936523, "xcomet_qe_score": 0.9017899632453918, "metricx_score": 3.1630375385284424, "metricx_qe_score": 4.569929122924805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢这次演讲,我们期待在多伦多的后续会议上采取行动。", "metrics": {"bleu_score": 19.230188007838596, "chrf_score": 20.27324129118446, "xcomet_score": 0.7803919315338135, "xcomet_qe_score": 0.8247551321983337, "metricx_score": 5.600637435913086, "metricx_qe_score": 6.235684871673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我是马蒂亚斯·伦德曼,今天我将简要介绍我们关于不使用树的组合泛化的论文,使用多集合标记和潜在排列。 这是与我的导师亚历山", "metrics": {"bleu_score": 33.02000294838339, "chrf_score": 29.648302125007493, "xcomet_score": 0.43102842569351196, "xcomet_qe_score": 0.3704432249069214, "metricx_score": 10.682363510131836, "metricx_qe_score": 7.648751258850098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大·科拉(Alexander Koller)和伊万·蒂托夫(Ivan Titov)的合作成果。", "metrics": {"bleu_score": 5.774215690081124, "chrf_score": 52.58831772656434, "xcomet_score": 0.4216805100440979, "xcomet_qe_score": 0.37436166405677795, "metricx_score": 6.870966911315918, "metricx_qe_score": 5.796505451202393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深的递归和在训练期间单独看到的短语的未见组合的能力。", "metrics": {"bleu_score": 82.82138064789241, "chrf_score": 78.15073961785164, "xcomet_score": 0.7902859449386597, "xcomet_qe_score": 0.6874688863754272, "metricx_score": 5.151350021362305, "metricx_qe_score": 7.931947708129883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能如下所示。", "metrics": {"bleu_score": 70.07637953409846, "chrf_score": 60.13480369940909, "xcomet_score": 0.908198356628418, "xcomet_qe_score": 0.8913741707801819, "metricx_score": 0.9897307753562927, "metricx_qe_score": 1.7725532054901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与往常一样,我们有一组训练语", "metrics": {"bleu_score": 39.93614954790575, "chrf_score": 31.717184563651685, "xcomet_score": 0.833681583404541, "xcomet_qe_score": 0.8359848856925964, "metricx_score": 1.1916446685791016, "metricx_qe_score": 3.1884658336639404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "句,在这种情况下是“女孩睡了”和", "metrics": {"bleu_score": 14.598608091257082, "chrf_score": 8.740572247938355, "xcomet_score": 0.37485599517822266, "xcomet_qe_score": 0.4130537807941437, "metricx_score": 9.512920379638672, "metricx_qe_score": 5.563879489898682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道女孩睡了”。", "metrics": {"bleu_score": 15.585475983689154, "chrf_score": 10.987009981578273, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 1.0, "metricx_score": 1.6568025350570679, "metricx_qe_score": 1.8745388984680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些陈述与代表其意义核心方面的逻辑形式相匹配。", "metrics": {"bleu_score": 6.6375229606488055, "chrf_score": 11.017964641563438, "xcomet_score": 0.9181281328201294, "xcomet_qe_score": 0.9033861756324768, "metricx_score": 2.0164668560028076, "metricx_qe_score": 1.5749977827072144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集并非来自相同的分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 55.90981220397046, "chrf_score": 50.240316868560384, "xcomet_score": 0.8630441427230835, "xcomet_qe_score": 0.8781783580780029, "metricx_score": 1.056781530380249, "metricx_qe_score": 1.8781754970550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练期间看到了浅层递归,并在具有更深层递归的", "metrics": {"bleu_score": 37.608532171656634, "chrf_score": 29.885171027344448, "xcomet_score": 0.6258585453033447, "xcomet_qe_score": 0.5880746841430664, "metricx_score": 7.81181001663208, "metricx_qe_score": 7.389565467834473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "示例上进行了测试。 天真的序列到序列模型在处理这种分布外推广时会遇到困难,并且通常会产生与输入无关的输出。", "metrics": {"bleu_score": 32.84320626773411, "chrf_score": 36.55539540632694, "xcomet_score": 0.5653128027915955, "xcomet_qe_score": 0.25751161575317383, "metricx_score": 3.5794870853424072, "metricx_qe_score": 2.742208242416382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们往往无法复制输入和输出之间的系统性对应关系,例如示例中用颜色标记的对应关系。", "metrics": {"bleu_score": 51.73609205003311, "chrf_score": 44.54493637853532, "xcomet_score": 0.995478630065918, "xcomet_qe_score": 0.9973478317260742, "metricx_score": 0.49839329719543457, "metricx_qe_score": 0.6367934942245483, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决这个问题的一种流行方法是将树集成到模型中。", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 54.38417629493144, "xcomet_score": 0.9308148622512817, "xcomet_qe_score": 0.9095532298088074, "metricx_score": 0.9633185863494873, "metricx_qe_score": 0.9730742573738098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树的目的是捕捉将言语与逻辑形式联系起来的构成过程。", "metrics": {"bleu_score": 35.66602104177529, "chrf_score": 29.4631680728877, "xcomet_score": 0.845754861831665, "xcomet_qe_score": 0.8088328838348389, "metricx_score": 3.506503105163574, "metricx_qe_score": 4.164720058441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这很有效,但树通常是不给的,需要以某种方式获得。", "metrics": {"bleu_score": 10.787393892800695, "chrf_score": 13.932758412685796, "xcomet_score": 0.7826905846595764, "xcomet_qe_score": 0.9001252055168152, "metricx_score": 2.5040793418884277, "metricx_qe_score": 2.0906519889831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂的过程,有时计算成本高昂。", "metrics": {"bleu_score": 42.10160205253763, "chrf_score": 37.1238090455339, "xcomet_score": 0.987737774848938, "xcomet_qe_score": 0.9357204437255859, "metricx_score": 0.451006144285202, "metricx_qe_score": 0.5619462728500366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及大量的形式化预处理逻辑形式,例如处理变量符号。", "metrics": {"bleu_score": 41.420753188585614, "chrf_score": 37.50380160576863, "xcomet_score": 0.8777146339416504, "xcomet_qe_score": 0.8761515021324158, "metricx_score": 1.1971975564956665, "metricx_qe_score": 1.5607548952102661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树也可能涉及专门的语法归纳程序。", "metrics": {"bleu_score": 51.92070044057203, "chrf_score": 48.073222342203366, "xcomet_score": 0.8051571846008301, "xcomet_qe_score": 0.8592174053192139, "metricx_score": 4.836609363555908, "metricx_qe_score": 5.441113471984863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们不使用树,而是引入了一个神经序列到序列模型,该模型直接建模输入片段和输出片段之间的对应关系。", "metrics": {"bleu_score": 59.62983159636464, "chrf_score": 50.226437288517175, "xcomet_score": 0.7966393232345581, "xcomet_qe_score": 0.7367479801177979, "metricx_score": 2.068682909011841, "metricx_qe_score": 2.5019543170928955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在不依赖树的情况下对更深递归的强大泛化。", "metrics": {"bleu_score": 47.974943129273534, "chrf_score": 39.4257053900852, "xcomet_score": 0.9715330600738525, "xcomet_qe_score": 0.9547793865203857, "metricx_score": 3.6022679805755615, "metricx_qe_score": 5.235719680786133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两个步骤从输入预测输出。", "metrics": {"bleu_score": 45.627994270063056, "chrf_score": 37.94484330693868, "xcomet_score": 0.9958451986312866, "xcomet_qe_score": 0.9887570142745972, "metricx_score": 0.5157465934753418, "metricx_qe_score": 0.7344526052474976, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们用将出现在输出中的标记的无序多重集来标记每个输入标记。", "metrics": {"bleu_score": 53.95675853277331, "chrf_score": 45.18382847000292, "xcomet_score": 0.795042872428894, "xcomet_qe_score": 0.8201113343238831, "metricx_score": 4.105541706085205, "metricx_qe_score": 3.677262544631958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步之后,我们有了所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 50.70283593483281, "chrf_score": 43.12609132417949, "xcomet_score": 0.9020755290985107, "xcomet_qe_score": 0.8261250257492065, "metricx_score": 1.9074314832687378, "metricx_qe_score": 2.9077394008636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在第二步中,我们使用另一个模型来预测一个排列,以便将它们放在正确的顺序中。", "metrics": {"bleu_score": 49.59564282470535, "chrf_score": 48.41879497867778, "xcomet_score": 0.9253261089324951, "xcomet_qe_score": 0.910333514213562, "metricx_score": 2.825385570526123, "metricx_qe_score": 3.9881794452667236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提出了一种新的方法来预测排列,该方法不对可能的排列施加任何硬约束。", "metrics": {"bleu_score": 52.31195343625364, "chrf_score": 46.041534821277516, "xcomet_score": 0.9893213510513306, "xcomet_qe_score": 0.9171484112739563, "metricx_score": 1.5139026641845703, "metricx_qe_score": 2.5203070640563965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活和富有表现力。", "metrics": {"bleu_score": 34.49377103006338, "chrf_score": 28.406156040000642, "xcomet_score": 0.9902740716934204, "xcomet_qe_score": 0.966692328453064, "metricx_score": 0.7848314642906189, "metricx_qe_score": 1.2660465240478516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的排列模型大致是这样工作的。", "metrics": {"bleu_score": 45.43846584021041, "chrf_score": 37.71628738461136, "xcomet_score": 0.9727286100387573, "xcomet_qe_score": 0.7678294777870178, "metricx_score": 0.7958335280418396, "metricx_qe_score": 1.8144389390945435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定每个位置放置哪个多重集标记。", "metrics": {"bleu_score": 57.91098611890164, "chrf_score": 50.864357770938305, "xcomet_score": 0.7921082973480225, "xcomet_qe_score": 0.764901876449585, "metricx_score": 1.829980492591858, "metricx_qe_score": 2.6284890174865723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需选择一个,如红色突出显示的那样。", "metrics": {"bleu_score": 43.82178438820051, "chrf_score": 39.25008358494861, "xcomet_score": 0.9876221418380737, "xcomet_qe_score": 0.979598879814148, "metricx_score": 0.9268742799758911, "metricx_qe_score": 0.8475396633148193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们跳到下一个多集合标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 50.818087818102754, "chrf_score": 43.48301475025613, "xcomet_score": 0.7619844675064087, "xcomet_qe_score": 0.8163298964500427, "metricx_score": 3.341841220855713, "metricx_qe_score": 3.181694507598877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过跳转到另一个多集合标记来确定输出中的第三个标记。", "metrics": {"bleu_score": 52.6094679561682, "chrf_score": 50.04156839428916, "xcomet_score": 0.7392497062683105, "xcomet_qe_score": 0.7976279258728027, "metricx_score": 3.0468335151672363, "metricx_qe_score": 3.0247371196746826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到第一阶段的每个标记都被访问过一次。", "metrics": {"bleu_score": 65.80771759973244, "chrf_score": 56.21026732258616, "xcomet_score": 0.8257196545600891, "xcomet_qe_score": 0.8458024263381958, "metricx_score": 2.1986842155456543, "metricx_qe_score": 3.4010329246520996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让你对实验结果有一个大致的了解,我们将我们的方法与 Cogs 基准上的其他无树模型进行了比较。", "metrics": {"bleu_score": 69.60989941336328, "chrf_score": 57.07872519332995, "xcomet_score": 0.7173886895179749, "xcomet_qe_score": 0.7071694731712341, "metricx_score": 3.4258298873901367, "metricx_qe_score": 3.128833293914795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的模型在递归深度的泛化方面远远超过了其他模型。", "metrics": {"bleu_score": 52.596690801431954, "chrf_score": 46.600693446215374, "xcomet_score": 0.9876135587692261, "xcomet_qe_score": 0.9782435894012451, "metricx_score": 1.682418942451477, "metricx_qe_score": 2.663529872894287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,其他类型的结构化生成仍然非常具有挑战性。", "metrics": {"bleu_score": 29.81792160679168, "chrf_score": 29.078993115900186, "xcomet_score": 0.9154311418533325, "xcomet_qe_score": 0.9000070691108704, "metricx_score": 3.75382137298584, "metricx_qe_score": 3.439682960510254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的技术挑战。", "metrics": {"bleu_score": 51.94247346787362, "chrf_score": 48.62443703444468, "xcomet_score": 0.9972208738327026, "xcomet_qe_score": 0.9875714778900146, "metricx_score": 0.26058727502822876, "metricx_qe_score": 0.36244577169418335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,训练数据中没有给出输入和输出的对齐。", "metrics": {"bleu_score": 36.173905261890994, "chrf_score": 28.826001254388807, "xcomet_score": 0.9108600616455078, "xcomet_qe_score": 0.913451611995697, "metricx_score": 0.7726055383682251, "metricx_qe_score": 0.759522557258606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集合,这对训练构成了挑战。", "metrics": {"bleu_score": 49.60314322999824, "chrf_score": 42.12896021067274, "xcomet_score": 0.8121585845947266, "xcomet_qe_score": 0.7234905958175659, "metricx_score": 3.477055311203003, "metricx_qe_score": 4.288069725036621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时有多种排列方式与数据一致,但语言上正确的排列方式是隐含的。", "metrics": {"bleu_score": 41.806479460972255, "chrf_score": 37.855327454587645, "xcomet_score": 0.9230011701583862, "xcomet_qe_score": 0.875817596912384, "metricx_score": 1.1387553215026855, "metricx_qe_score": 1.5955941677093506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练过程中诱导对齐来解决这个问题。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 65.24710105783775, "xcomet_score": 0.9789974689483643, "xcomet_qe_score": 0.906593918800354, "metricx_score": 0.6721183061599731, "metricx_qe_score": 0.8953751921653748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的排列方法非常灵活,但它带来了一个挑战,即找到得分最高的排列是NP难的。", "metrics": {"bleu_score": 68.87591926411304, "chrf_score": 59.44461199555299, "xcomet_score": 0.8570966720581055, "xcomet_qe_score": 0.8282343745231628, "metricx_score": 1.6398587226867676, "metricx_qe_score": 3.1038010120391846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行推销员问题有关。", "metrics": {"bleu_score": 23.446219441058627, "chrf_score": 21.78800366300366, "xcomet_score": 0.8328037261962891, "xcomet_qe_score": 0.8560585379600525, "metricx_score": 1.028475046157837, "metricx_qe_score": 1.1208467483520508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一个GPU友好的连续松弛来近似这个问题,这也使我们能够通过解决方案进行反向传播,并学习更有语言学可能性的排列。", "metrics": {"bleu_score": 63.625677145936486, "chrf_score": 63.00729565280898, "xcomet_score": 0.7070900201797485, "xcomet_qe_score": 0.5968904495239258, "metricx_score": 3.7252016067504883, "metricx_qe_score": 5.1381940841674805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息,请查看我们的论文或来我们的海报展示区。", "metrics": {"bleu_score": 81.62735195826225, "chrf_score": 79.55911697772808, "xcomet_score": 0.966425895690918, "xcomet_qe_score": 0.9267166256904602, "metricx_score": 0.6999748945236206, "metricx_qe_score": 0.6720219850540161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是阿克沙塔,今天我和我的合著者马丁将介绍我们的研究成果《KITMAST》,评估来自多个来源的知识整合。这项", "metrics": {"bleu_score": 51.02706525380926, "chrf_score": 41.87418227495788, "xcomet_score": 0.6159301996231079, "xcomet_qe_score": 0.7071865797042847, "metricx_score": 6.777498245239258, "metricx_qe_score": 3.921565532684326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究是麦吉尔大学、Mila和微软研究院的合作成果。", "metrics": {"bleu_score": 72.04492053232023, "chrf_score": 67.29217461233554, "xcomet_score": 0.923682689666748, "xcomet_qe_score": 0.8401497602462769, "metricx_score": 1.0575411319732666, "metricx_qe_score": 1.7450989484786987, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然语言理解模型利用各种知识来源,例如包含在其参数中的知识(通常是通过预训练获得的),以及在推理时给出的输入知识。", "metrics": {"bleu_score": 47.682088051317756, "chrf_score": 44.47411732100308, "xcomet_score": 0.9828088283538818, "xcomet_qe_score": 0.9785473346710205, "metricx_score": 0.5863186120986938, "metricx_qe_score": 0.8002759218215942, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近的研究表明,在问答等任务中,模型可以利用预训练知识来解决任务。", "metrics": {"bleu_score": 60.573057264787714, "chrf_score": 49.58730568830308, "xcomet_score": 0.9952421188354492, "xcomet_qe_score": 0.9752320051193237, "metricx_score": 0.5024597644805908, "metricx_qe_score": 0.9494141340255737, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常需要在推理时提供的知识。", "metrics": {"bleu_score": 87.2958005807749, "chrf_score": 87.70867986100576, "xcomet_score": 0.9168751239776611, "xcomet_qe_score": 0.8435982465744019, "metricx_score": 1.0447803735733032, "metricx_qe_score": 0.9053048491477966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“John 在电视上看到了新当选的总统”中。", "metrics": {"bleu_score": 39.62658479382421, "chrf_score": 30.03822687800429, "xcomet_score": 0.9736689329147339, "xcomet_qe_score": 0.9662938117980957, "metricx_score": 1.7432132959365845, "metricx_qe_score": 3.5017006397247314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可能包含有关总统做什么以及电视是什么的信息,但它们无法可靠地知道这个特定实例的实体“约翰”是谁,或者新总统是谁,因为自预训练以来总统可能已经更换。", "metrics": {"bleu_score": 57.593123466651335, "chrf_score": 48.17210329788527, "xcomet_score": 0.8689762353897095, "xcomet_qe_score": 0.8566216230392456, "metricx_score": 2.403183937072754, "metricx_qe_score": 2.617785692214966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型自然语言理解任务模型需要能够整合和利用预训练时和推理时的知识。", "metrics": {"bleu_score": 45.02653061071288, "chrf_score": 35.84822504200959, "xcomet_score": 0.9964258670806885, "xcomet_qe_score": 0.9426921010017395, "metricx_score": 0.5404362082481384, "metricx_qe_score": 0.8117221593856812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一个知识整合的诊断测试套件。", "metrics": {"bleu_score": 66.3685910876666, "chrf_score": 60.108018792801396, "xcomet_score": 0.9981048107147217, "xcomet_qe_score": 0.9918153285980225, "metricx_score": 1.078971266746521, "metricx_qe_score": 1.7488058805465698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提出了一项共指消解任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 41.89267835960909, "chrf_score": 34.1964470688639, "xcomet_score": 0.8398990631103516, "xcomet_qe_score": 0.8251432180404663, "metricx_score": 2.042733669281006, "metricx_qe_score": 3.1542325019836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的参考解决方案模型来评估数据集。", "metrics": {"bleu_score": 74.9736708914111, "chrf_score": 74.40656365449352, "xcomet_score": 0.8314999341964722, "xcomet_qe_score": 0.815367579460144, "metricx_score": 3.380998134613037, "metricx_qe_score": 2.9409961700439453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个示例。", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 55.59717236892725, "xcomet_score": 0.96964430809021, "xcomet_qe_score": 0.9326752424240112, "metricx_score": 0.3784157633781433, "metricx_qe_score": 0.8690282106399536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟文是一名法官。", "metrics": {"bleu_score": 68.037493331712, "chrf_score": 37.82792863041536, "xcomet_score": 0.8889403343200684, "xcomet_qe_score": 0.8660338521003723, "metricx_score": 1.6586167812347412, "metricx_qe_score": 2.53135085105896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基娅是一名面包师。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 59.333448823374546, "xcomet_score": 0.8765929937362671, "xcomet_qe_score": 0.8274440169334412, "metricx_score": 0.606604814529419, "metricx_qe_score": 0.8381551504135132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟文和基娅在公园里相遇。", "metrics": {"bleu_score": 22.03359678996931, "chrf_score": 14.896167636318175, "xcomet_score": 0.8661175966262817, "xcomet_qe_score": 0.9678424596786499, "metricx_score": 0.9585168361663818, "metricx_qe_score": 1.071321964263916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上忙碌了一天后,他很高兴能放松一下。", "metrics": {"bleu_score": 44.385622110478316, "chrf_score": 34.3126206684709, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.1361110210418701, "metricx_qe_score": 1.8019027709960938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是确定代词“he”所指的正确实体,在这种情况下是“servant”。", "metrics": {"bleu_score": 49.342411798176485, "chrf_score": 42.690110944755766, "xcomet_score": 0.8764821290969849, "xcomet_qe_score": 0.7686728239059448, "metricx_score": 6.532586574554443, "metricx_qe_score": 7.193405628204346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解析代词需要两种类型的信息:", "metrics": {"bleu_score": 13.353456414125246, "chrf_score": 14.632919880323062, "xcomet_score": 0.9813714027404785, "xcomet_qe_score": 0.8616176843643188, "metricx_score": 0.5900946855545044, "metricx_qe_score": 0.727135181427002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一是实体特定的知识,例如“Sergey 是法官”", "metrics": {"bleu_score": 14.725727839625105, "chrf_score": 24.653401722248567, "xcomet_score": 0.7742463946342468, "xcomet_qe_score": 0.7816754579544067, "metricx_score": 2.845660448074341, "metricx_qe_score": 4.721817970275879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ";二是背景知识,例如“法官在法庭上审理案件”。 通常", "metrics": {"bleu_score": 47.87725471995895, "chrf_score": 41.00756274635133, "xcomet_score": 0.723388671875, "xcomet_qe_score": 0.7300429344177246, "metricx_score": 4.8586883544921875, "metricx_qe_score": 1.936814308166504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",大型语言模型在预训练期间学习背景知识,而在推理时观察到实体特定知识。", "metrics": {"bleu_score": 27.639116322203822, "chrf_score": 26.001973544702274, "xcomet_score": 0.8239998817443848, "xcomet_qe_score": 0.7596473693847656, "metricx_score": 5.629454612731934, "metricx_qe_score": 6.196793556213379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变这两个信息来源的可用性,使其可以在单一来源或多个来源中找到。", "metrics": {"bleu_score": 73.56844708274171, "chrf_score": 69.97054110471872, "xcomet_score": 0.9042110443115234, "xcomet_qe_score": 0.8258731365203857, "metricx_score": 0.7585831880569458, "metricx_qe_score": 0.9078630805015564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了三种 KITMOS 设置。", "metrics": {"bleu_score": 47.08519732645178, "chrf_score": 37.11165592668518, "xcomet_score": 0.9265429973602295, "xcomet_qe_score": 0.9241626858711243, "metricx_score": 1.2062864303588867, "metricx_qe_score": 0.9778594374656677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是典型设置,背景预训练,其中假设背景知识在预训练时可用。", "metrics": {"bleu_score": 28.5767766947312, "chrf_score": 27.17668454809246, "xcomet_score": 0.871833324432373, "xcomet_qe_score": 0.864944577217102, "metricx_score": 1.9204038381576538, "metricx_qe_score": 3.5202126502990723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,有背景两者设置,其中背景知识在预训练和推理时均可用。", "metrics": {"bleu_score": 42.1555553722358, "chrf_score": 38.310936457070824, "xcomet_score": 0.832781195640564, "xcomet_qe_score": 0.7403638362884521, "metricx_score": 1.9099490642547607, "metricx_qe_score": 3.0200352668762207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,背景推理设置,其中两种知识类型仅在推理时可用。", "metrics": {"bleu_score": 61.792397707782776, "chrf_score": 59.04234707732323, "xcomet_score": 0.9226594567298889, "xcomet_qe_score": 0.8604711294174194, "metricx_score": 0.8491841554641724, "metricx_qe_score": 0.8735736608505249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种设置尤其有趣,因为它模拟了这样一种情况:解决任务所需的背景知识并不包含在预训练模型的数据中。", "metrics": {"bleu_score": 39.63037843515372, "chrf_score": 37.46574078216037, "xcomet_score": 0.9677557945251465, "xcomet_qe_score": 0.9629849195480347, "metricx_score": 0.6645375490188599, "metricx_qe_score": 0.6502562761306763, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,因为在预训练之后出现了新的职业。", "metrics": {"bleu_score": 49.91593892034051, "chrf_score": 43.43003136927286, "xcomet_score": 0.896924614906311, "xcomet_qe_score": 0.8387792706489563, "metricx_score": 1.146550178527832, "metricx_qe_score": 2.3976552486419678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们如何控制两个数据源中事实可用性的一个示例。", "metrics": {"bleu_score": 36.57697453138425, "chrf_score": 30.89485761545735, "xcomet_score": 0.8158444166183472, "xcomet_qe_score": 0.774368166923523, "metricx_score": 1.5198230743408203, "metricx_qe_score": 2.3874340057373047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识“政客寻求政府中的选举职位”包含在预训练参数中。在不熟悉的上下文中,我们提供了反事实知识“切斯特是政客”。", "metrics": {"bleu_score": 28.43030936967801, "chrf_score": 23.307744142512803, "xcomet_score": 0.5405392646789551, "xcomet_qe_score": 0.517927885055542, "metricx_score": 5.3937296867370605, "metricx_qe_score": 5.79866886138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景双重设置中,我们不仅提供了反对派的背景信息,还提供了关于政治家在其原始背景下的背景信息。", "metrics": {"bleu_score": 24.52767339435839, "chrf_score": 20.733048293605414, "xcomet_score": 0.5685245990753174, "xcomet_qe_score": 0.5428646802902222, "metricx_score": 5.585387229919434, "metricx_qe_score": 5.519526958465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景干扰设置中,我们提供了虚构的职业“meretuer”而不是“politician”,因为“meretuer”不太可能包含在预训练的词汇表中。", "metrics": {"bleu_score": 52.60745866202233, "chrf_score": 51.46590652045159, "xcomet_score": 0.5524311065673828, "xcomet_qe_score": 0.5683760046958923, "metricx_score": 6.129253387451172, "metricx_qe_score": 6.17674446105957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的参考解决方案模型来评估数据集。", "metrics": {"bleu_score": 74.9736708914111, "chrf_score": 74.40656365449352, "xcomet_score": 0.8301503658294678, "xcomet_qe_score": 0.8130688667297363, "metricx_score": 3.4563183784484863, "metricx_qe_score": 3.0263943672180176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本图中,我们展示了在背景预训练设置中最困难的变体上表现最佳的模型的结果。", "metrics": {"bleu_score": 49.3999685796903, "chrf_score": 40.786359966422964, "xcomet_score": 0.8898048996925354, "xcomet_qe_score": 0.8273705244064331, "metricx_score": 1.2460007667541504, "metricx_qe_score": 1.4211009740829468, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果没有在 KIT-MOS 上进行任务特定的训练,两个模型都表现不佳。", "metrics": {"bleu_score": 60.26952618267291, "chrf_score": 55.42842040974675, "xcomet_score": 0.8960617780685425, "xcomet_qe_score": 0.894307017326355, "metricx_score": 3.2900285720825195, "metricx_qe_score": 3.9699769020080566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在 KIT-MOS 上进行训练后,C2F 和 BERT4KID 都比随机选择的表现要好得多。", "metrics": {"bleu_score": 11.839441618747346, "chrf_score": 29.134522797676127, "xcomet_score": 0.7096608281135559, "xcomet_qe_score": 0.7425918579101562, "metricx_score": 6.760199069976807, "metricx_qe_score": 7.110920429229736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,在通用的图像分辨率数据集上训练时,模型会学习利用表面线索,而在测试 KITTI 时,这些线索并没有用,因为这些线索已经被移除。", "metrics": {"bleu_score": 28.38009399569879, "chrf_score": 24.57032641070161, "xcomet_score": 0.7761318683624268, "xcomet_qe_score": 0.7625992298126221, "metricx_score": 5.352296829223633, "metricx_qe_score": 5.898657321929932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识的其他实验表明,即使是表现最好的模型也无法可靠地整合仅在推理时提供的背景知识。 总结", "metrics": {"bleu_score": 78.52863431246057, "chrf_score": 73.9301555829535, "xcomet_score": 0.8356893062591553, "xcomet_qe_score": 0.8372438549995422, "metricx_score": 2.923365354537964, "metricx_qe_score": 1.295089840888977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要观点:许多共指消解模型似乎无法在没有任务特定训练的情况下推理不同来源的知识。", "metrics": {"bleu_score": 61.75943111459176, "chrf_score": 55.76802254882487, "xcomet_score": 0.9341849684715271, "xcomet_qe_score": 0.8578604459762573, "metricx_score": 2.385794162750244, "metricx_qe_score": 3.814807891845703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在任务特定训练的情况下,某些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 60.427507947135354, "chrf_score": 59.80369922724888, "xcomet_score": 0.88880455493927, "xcomet_qe_score": 0.8645026683807373, "metricx_score": 1.0151209831237793, "metricx_qe_score": 1.4714329242706299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即使是表现最好的模型,似乎也难以可靠地整合仅在推理时提供的背景知识。", "metrics": {"bleu_score": 56.397114787600124, "chrf_score": 52.418766547411266, "xcomet_score": 0.9679101705551147, "xcomet_qe_score": 0.9879029989242554, "metricx_score": 1.2029660940170288, "metricx_qe_score": 0.9079171419143677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请查看我们的论文,并在 GitHub 上查看数据集和代码。", "metrics": {"bleu_score": 55.28816837366797, "chrf_score": 55.09362405647234, "xcomet_score": 0.982934832572937, "xcomet_qe_score": 0.9691354036331177, "metricx_score": 0.3531762957572937, "metricx_qe_score": 0.30470556020736694, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢收听。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 17.0, "xcomet_score": 0.9755227565765381, "xcomet_qe_score": 0.9598297476768494, "metricx_score": 0.6410813927650452, "metricx_qe_score": 0.3586311638355255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是梅拉,今天我要谈谈我们的论文《标记角色》,使用自然语言提示来衡量语言模型中的刻板印象。", "metrics": {"bleu_score": 67.36947812641024, "chrf_score": 57.774265816084416, "xcomet_score": 0.7674951553344727, "xcomet_qe_score": 0.622581958770752, "metricx_score": 1.886511206626892, "metricx_qe_score": 2.4611153602600098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与埃森·德穆什和丹·杰罗夫斯基合作完成的。", "metrics": {"bleu_score": 20.22202784230022, "chrf_score": 13.133548395927551, "xcomet_score": 0.954315185546875, "xcomet_qe_score": 0.9724688529968262, "metricx_score": 1.287065029144287, "metricx_qe_score": 1.4662855863571167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多人记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 50.84720690436296, "chrf_score": 47.50568639302309, "xcomet_score": 0.9775696992874146, "xcomet_qe_score": 0.929103672504425, "metricx_score": 2.3371987342834473, "metricx_qe_score": 4.754868507385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施各有局限性。", "metrics": {"bleu_score": 32.70272377042893, "chrf_score": 27.242939307795496, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.20088507235050201, "metricx_qe_score": 0.3115563690662384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于人工构建的数据集,这些数据集的整理非常耗时。 它们通常只测量非常具体的刻板印象,这意味着它们不能很好地推广到其他人口统计或情境,或者它们只是捕捉到与特定群体相关的非常广泛的负面联想。", "metrics": {"bleu_score": 43.323420400108574, "chrf_score": 36.06068841445654, "xcomet_score": 0.6509026288986206, "xcomet_qe_score": 0.6490387916564941, "metricx_score": 5.000580787658691, "metricx_qe_score": 5.061895370483398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,该领域的大多数工作都没有考虑到交叉性,即多重社会身份可能会加剧偏见,并成为独特的伤害来源。", "metrics": {"bleu_score": 56.11946970725312, "chrf_score": 49.319288316435554, "xcomet_score": 0.9038950800895691, "xcomet_qe_score": 0.760306715965271, "metricx_score": 2.2438716888427734, "metricx_qe_score": 2.429028034210205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些限制,我们依靠的是这些新的指令调优大型语言模型非常擅长对指令和提示做出响应的特性。", "metrics": {"bleu_score": 30.45540040455643, "chrf_score": 24.564041167491673, "xcomet_score": 0.8475749492645264, "xcomet_qe_score": 0.8003857135772705, "metricx_score": 3.005186080932617, "metricx_qe_score": 3.0141959190368652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色,即使用提示语(例如“想象你是一个亚洲女性,", "metrics": {"bleu_score": 35.1885390181553, "chrf_score": 37.14343077269476, "xcomet_score": 0.9449781775474548, "xcomet_qe_score": 0.8834691643714905, "metricx_score": 4.052422046661377, "metricx_qe_score": 4.198512554168701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述一下你自己”)来描绘一个虚构的个人。", "metrics": {"bleu_score": 31.044143558888084, "chrf_score": 44.77037430612386, "xcomet_score": 0.2035876363515854, "xcomet_qe_score": 0.18654941022396088, "metricx_score": 4.38300895690918, "metricx_qe_score": 4.988221168518066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这对任何人口统计学都非常有普遍性,因为我们可以在提示中指定任何我们想要的身份标识符。", "metrics": {"bleu_score": 52.059855124235874, "chrf_score": 46.88421892506142, "xcomet_score": 0.8317832946777344, "xcomet_qe_score": 0.7771930694580078, "metricx_score": 2.30934476852417, "metricx_qe_score": 2.7159178256988525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT-4 的一些示例生成。 我们立", "metrics": {"bleu_score": 45.788313721339826, "chrf_score": 58.66599702039744, "xcomet_score": 0.6735544204711914, "xcomet_qe_score": 0.5657150745391846, "metricx_score": 7.208314895629883, "metricx_qe_score": 3.9891819953918457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即看到,虽然输出并不明显地具有负面或有毒性,但这些词的传统意义却是如此。 有一些有趣的模式。", "metrics": {"bleu_score": 38.479842130691715, "chrf_score": 36.105646240301006, "xcomet_score": 0.7993820905685425, "xcomet_qe_score": 0.6888551712036133, "metricx_score": 4.272397518157959, "metricx_qe_score": 4.899990081787109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘成谦逊的,而中东女性则被描述为“异国情调的”,并提到“迷人的地区”。", "metrics": {"bleu_score": 35.90903602194303, "chrf_score": 30.90944495295469, "xcomet_score": 0.7833368182182312, "xcomet_qe_score": 0.8022662401199341, "metricx_score": 1.8708654642105103, "metricx_qe_score": 2.5662989616394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两个有色人种女性角色都提到了祖先,而白人男性角色则没有。", "metrics": {"bleu_score": 47.74146836826223, "chrf_score": 47.364200622615016, "xcomet_score": 0.9786760807037354, "xcomet_qe_score": 0.9865318536758423, "metricx_score": 1.2668883800506592, "metricx_qe_score": 1.0068082809448242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法有两个部分。", "metrics": {"bleu_score": 60.39435155169266, "chrf_score": 50.23384646256164, "xcomet_score": 0.9907910823822021, "xcomet_qe_score": 0.9707120656967163, "metricx_score": 0.21345947682857513, "metricx_qe_score": 0.34458082914352417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人物。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 68.72835497835497, "xcomet_score": 0.9012347459793091, "xcomet_qe_score": 0.8504418134689331, "metricx_score": 1.17280912399292, "metricx_qe_score": 2.2511725425720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的提示是为了生成这些人物角色而设计的,灵感来自一项研究,该研究将这些提示给予人类受试者,发现通过给予人类受试者,他们也能揭示种族刻板印象。", "metrics": {"bleu_score": 39.42710432762446, "chrf_score": 31.80874518222574, "xcomet_score": 0.6143631339073181, "xcomet_qe_score": 0.6495541334152222, "metricx_score": 3.813052177429199, "metricx_qe_score": 3.7412939071655273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这也使我们能够直接比较生成的人物和人类书面回应。", "metrics": {"bleu_score": 37.10593803590559, "chrf_score": 31.56205887957298, "xcomet_score": 0.7867872714996338, "xcomet_qe_score": 0.7854874134063721, "metricx_score": 2.012202739715576, "metricx_qe_score": 3.641547441482544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是“标记词”,这是一种用于识别区分标记组和非标记组的词的方法,我稍后会详细说明。", "metrics": {"bleu_score": 38.14025488781001, "chrf_score": 30.9530185599572, "xcomet_score": 0.7960246801376343, "xcomet_qe_score": 0.9570486545562744, "metricx_score": 1.3139657974243164, "metricx_qe_score": 0.8705341219902039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样做的好处是,我们可以获得非常具体的刻板印象和模式,而不必依赖任何特定的词汇表。", "metrics": {"bleu_score": 55.23172621114699, "chrf_score": 51.388648448356264, "xcomet_score": 0.9785774946212769, "xcomet_qe_score": 0.9099512100219727, "metricx_score": 1.261942982673645, "metricx_qe_score": 1.532342553138733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词法方法借鉴了社会语言学中的“标记性”概念,该概念认为存在一种无标记的默认状态,任何偏离这种默认状态的群体都被认为是语言学上的标记性群体。", "metrics": {"bleu_score": 27.24915059447953, "chrf_score": 24.45962755708677, "xcomet_score": 0.6421507596969604, "xcomet_qe_score": 0.875998854637146, "metricx_score": 3.1945548057556152, "metricx_qe_score": 2.8369534015655518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,“战士”这个词通常与男性相关联。", "metrics": {"bleu_score": 65.03386691979853, "chrf_score": 59.66471435087284, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.33397558331489563, "metricx_qe_score": 0.5070487260818481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位女性战士时,他们通常会明确指出“女性战士”,并用“女性”来修饰这个词。", "metrics": {"bleu_score": 40.97408454965424, "chrf_score": 32.51703682852119, "xcomet_score": 0.7860791087150574, "xcomet_qe_score": 0.8138291835784912, "metricx_score": 2.721691608428955, "metricx_qe_score": 2.351480007171631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是未标记的,而边缘化群体通常是标记的。", "metrics": {"bleu_score": 60.45169397095852, "chrf_score": 54.032284073558515, "xcomet_score": 0.7715940475463867, "xcomet_qe_score": 0.7670178413391113, "metricx_score": 1.942873477935791, "metricx_qe_score": 2.0135974884033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,首先要确定未标记组和已标记组。 然后我们使用“战斗用语”方法来比较这些人物,该方法基本上是使用加权对数几率比来区分每个标记组的顶级词。", "metrics": {"bleu_score": 49.95273299043172, "chrf_score": 44.05533923041111, "xcomet_score": 0.6242809295654297, "xcomet_qe_score": 0.5987555980682373, "metricx_score": 5.088296413421631, "metricx_qe_score": 5.647204875946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会使用“战斗用语”,并将法律用语的比率与白人角色和男性角色进行比较,因为这两者是相应的未标记群体。", "metrics": {"bleu_score": 53.691970973459675, "chrf_score": 48.54498902147908, "xcomet_score": 0.6065769195556641, "xcomet_qe_score": 0.5651617050170898, "metricx_score": 5.373013496398926, "metricx_qe_score": 6.859051704406738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看看一些结果。", "metrics": {"bleu_score": 52.53819788848316, "chrf_score": 49.789859739831954, "xcomet_score": 0.9684176445007324, "xcomet_qe_score": 0.9546533823013306, "metricx_score": 0.3128272294998169, "metricx_qe_score": 0.45149296522140503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个刻板印象词典,发现生成的人物角色比人类写的角色包含更多的刻板印象。", "metrics": {"bleu_score": 60.16444528901523, "chrf_score": 53.57774424342882, "xcomet_score": 0.9336349368095398, "xcomet_qe_score": 0.8397200107574463, "metricx_score": 1.772059440612793, "metricx_qe_score": 1.9938607215881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词汇表中词语的分布时,我们发现的情况却大不相同。 因此", "metrics": {"bleu_score": 20.547513983953117, "chrf_score": 21.31638394693695, "xcomet_score": 0.761113703250885, "xcomet_qe_score": 0.6955873370170593, "metricx_score": 3.981628656387329, "metricx_qe_score": 1.3111460208892822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",虽然生成的人物角色中出现的“奢侈品”词汇的频率更高,但人类撰写的人物角色中出现的词汇分布更广,而生成的人物角色中出现的刻板印象词汇实际上只是“高”和“运动型”这两个词。", "metrics": {"bleu_score": 31.608965286353182, "chrf_score": 26.964920817320582, "xcomet_score": 0.5725498199462891, "xcomet_qe_score": 0.5358223915100098, "metricx_score": 7.881718635559082, "metricx_qe_score": 9.22363567352295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,实际上只需要正数或至少是非负数。", "metrics": {"bleu_score": 25.36152344751314, "chrf_score": 21.947054113303704, "xcomet_score": 0.8156020641326904, "xcomet_qe_score": 0.843361496925354, "metricx_score": 2.272829055786133, "metricx_qe_score": 0.6094380617141724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表并没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 80.98068746724697, "chrf_score": 76.49249365305852, "xcomet_score": 0.9581499099731445, "xcomet_qe_score": 0.7843279242515564, "metricx_score": 1.1733996868133545, "metricx_qe_score": 1.49143385887146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将转向我们的标记词方法的结果,以展示这些看似积极的词语如何促进刻板印象和本质化叙事。", "metrics": {"bleu_score": 33.21142604636744, "chrf_score": 27.282457202696857, "xcomet_score": 0.7511563897132874, "xcomet_qe_score": 0.8010289669036865, "metricx_score": 2.143439531326294, "metricx_qe_score": 2.3402538299560547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们审视了这些看似积极的描绘如何反映出有害的模式。", "metrics": {"bleu_score": 49.52426329157064, "chrf_score": 41.42257340297354, "xcomet_score": 0.847193717956543, "xcomet_qe_score": 0.749282956123352, "metricx_score": 1.8155322074890137, "metricx_qe_score": 2.7757294178009033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,顶级词包括文化、传统、自豪和异国情调。", "metrics": {"bleu_score": 4.643526372367784, "chrf_score": 6.003990049242981, "xcomet_score": 0.6647484302520752, "xcomet_qe_score": 0.6851064562797546, "metricx_score": 5.220454692840576, "metricx_qe_score": 4.936991214752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词只通过与其身份的关系来定义这些群体,并将它们与白人标准区分开来。", "metrics": {"bleu_score": 77.42512525806917, "chrf_score": 71.78652581737501, "xcomet_score": 0.9678927659988403, "xcomet_qe_score": 0.8546727895736694, "metricx_score": 1.1930880546569824, "metricx_qe_score": 2.1029629707336426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体的歧视和异化历史增添了新的篇章。", "metrics": {"bleu_score": 16.731078418090032, "chrf_score": 15.109172373371393, "xcomet_score": 0.5507073402404785, "xcomet_qe_score": 0.8281060457229614, "metricx_score": 2.0524888038635254, "metricx_qe_score": 1.2305772304534912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的陈词滥调,特别是对有色人种女性。", "metrics": {"bleu_score": 34.10440894587096, "chrf_score": 28.609970970805975, "xcomet_score": 0.8077543377876282, "xcomet_qe_score": 0.8215811252593994, "metricx_score": 1.2709165811538696, "metricx_qe_score": 0.9790509939193726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁裔女性的词语包括“活泼”和“丰满”。 这些词语与热带风情的陈词滥调有关。", "metrics": {"bleu_score": 17.09015101483131, "chrf_score": 13.558369086154826, "xcomet_score": 0.7597426176071167, "xcomet_qe_score": 0.7953460216522217, "metricx_score": 3.9436216354370117, "metricx_qe_score": 2.6818840503692627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,这些词语包括娇小、纤细和丝滑。 这与亚洲女性被过度性化、被视为非常温顺和顺从的长期历史有关。", "metrics": {"bleu_score": 29.569199353712527, "chrf_score": 23.961484275827797, "xcomet_score": 0.8986701965332031, "xcomet_qe_score": 0.972113847732544, "metricx_score": 2.2541580200195312, "metricx_qe_score": 1.6229243278503418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些顶级词汇是“强大”和“顽强”。", "metrics": {"bleu_score": 30.50133781736109, "chrf_score": 21.16384731315683, "xcomet_score": 0.7603641152381897, "xcomet_qe_score": 0.7608122825622559, "metricx_score": 3.245802879333496, "metricx_qe_score": 3.443180799484253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所说的“强大的黑人女性”原型有关,", "metrics": {"bleu_score": 77.21947901921794, "chrf_score": 69.94176618015628, "xcomet_score": 0.8813048005104065, "xcomet_qe_score": 0.7080289125442505, "metricx_score": 1.6347630023956299, "metricx_qe_score": 2.4879040718078613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍看之下似乎是积极的。 有研究表明,这种原型实际上非常有害,因为它对这些人群施加了很大的压力,要求他们在社会障碍面前表现出坚韧和强大。", "metrics": {"bleu_score": 35.351619699146845, "chrf_score": 28.456760414337552, "xcomet_score": 0.936038613319397, "xcomet_qe_score": 0.8036169409751892, "metricx_score": 1.8718111515045166, "metricx_qe_score": 2.3866989612579346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正努力改变这些障碍,不如对这些人施加压力,要求他们克服这些障碍,这会导致这些人出现非常不利的健康状况,以及其他有害的后果。", "metrics": {"bleu_score": 38.55201202927535, "chrf_score": 33.02803296665844, "xcomet_score": 0.9035803079605103, "xcomet_qe_score": 0.9339954853057861, "metricx_score": 1.70827317237854, "metricx_qe_score": 1.2424957752227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记组的词汇几乎都反映了非常本质化的叙述。", "metrics": {"bleu_score": 59.89159333271273, "chrf_score": 51.39833904114673, "xcomet_score": 0.8556169271469116, "xcomet_qe_score": 0.8511857986450195, "metricx_score": 1.8169772624969482, "metricx_qe_score": 2.078800678253174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据这些模式,我们得出了三条建议,供模型所有者参考。", "metrics": {"bleu_score": 29.12919968412613, "chrf_score": 26.638515875229253, "xcomet_score": 0.8912854194641113, "xcomet_qe_score": 0.7838586568832397, "metricx_score": 1.4782984256744385, "metricx_qe_score": 3.236917018890381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化的叙述。", "metrics": {"bleu_score": 26.40680896783096, "chrf_score": 24.484809572328313, "xcomet_score": 0.8093796968460083, "xcomet_qe_score": 0.8178517818450928, "metricx_score": 1.2333831787109375, "metricx_qe_score": 0.9520150423049927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉学科的视角来研究偏见和伤害,因为如果我们不这样做,可能会忽略很多事情。", "metrics": {"bleu_score": 76.95307125740557, "chrf_score": 73.54523565105039, "xcomet_score": 0.9433985948562622, "xcomet_qe_score": 0.8826590776443481, "metricx_score": 0.4129522442817688, "metricx_qe_score": 0.451715350151062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该增加有关偏见缓解方法的透明度。 例如,我们不知道这些积极的刻板印象是否是因为有某种奇怪的 可能存在过度的价值观一致性,或者其他反刻板印象的方法,导致了这些有害的模式。", "metrics": {"bleu_score": 48.56733009471358, "chrf_score": 41.7091443447203, "xcomet_score": 0.7350610494613647, "xcomet_qe_score": 0.723443865776062, "metricx_score": 5.410894393920898, "metricx_qe_score": 5.620652198791504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有更多透明度的情况下,我们真的无法做出任何假设,也无法进一步研究。", "metrics": {"bleu_score": 54.252324657610544, "chrf_score": 49.8230043505615, "xcomet_score": 0.9967613220214844, "xcomet_qe_score": 0.9910193681716919, "metricx_score": 0.25938013195991516, "metricx_qe_score": 0.3515446186065674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 25.690595421698053, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.27894243597984314, "metricx_qe_score": 0.5952762365341187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝您在AC度过愉快的时光。", "metrics": {"bleu_score": 8.516593018819643, "chrf_score": 18.15377153516042, "xcomet_score": 0.9204777479171753, "xcomet_qe_score": 0.8662562966346741, "metricx_score": 1.654177188873291, "metricx_qe_score": 1.8658509254455566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是中国科学技术大学的金伟。", "metrics": {"bleu_score": 20.763472919937094, "chrf_score": 16.658169059624747, "xcomet_score": 0.8730648756027222, "xcomet_qe_score": 0.8759600520133972, "metricx_score": 2.700815200805664, "metricx_qe_score": 3.1290805339813232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很高兴为我们的论文", "metrics": {"bleu_score": 23.587708298570007, "chrf_score": 37.90888920797102, "xcomet_score": 0.24590149521827698, "xcomet_qe_score": 0.18918731808662415, "metricx_score": 9.121475219726562, "metricx_qe_score": 11.173503875732422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《你在复制我的模型吗?", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 36.07623857623858, "xcomet_score": 0.9620996713638306, "xcomet_qe_score": 0.7765611410140991, "metricx_score": 0.722536027431488, "metricx_qe_score": 1.1948256492614746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护大型语言模型的嵌入和服务的版权》制作一个简短的广告视频。 ", "metrics": {"bleu_score": 38.47383546691162, "chrf_score": 41.16820543556486, "xcomet_score": 0.4871971011161804, "xcomet_qe_score": 0.46129634976387024, "metricx_score": 6.864727020263672, "metricx_qe_score": 7.684391975402832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们介绍一下嵌入式系统的背景。", "metrics": {"bleu_score": 38.817246541254406, "chrf_score": 37.440244039598205, "xcomet_score": 0.9714576005935669, "xcomet_qe_score": 0.9107193350791931, "metricx_score": 0.8618990182876587, "metricx_qe_score": 0.8738545775413513, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,大型语言模型(如 ChatGPT、LLaMA、PaLM)在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 58.75025402411756, "chrf_score": 57.06960913599066, "xcomet_score": 0.9449219107627869, "xcomet_qe_score": 0.9407581686973572, "metricx_score": 3.1678314208984375, "metricx_qe_score": 3.772404670715332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式服务是基于大型语言模型的服务之一,可用于辅助各种自然语言处理任务。", "metrics": {"bleu_score": 44.527572284147446, "chrf_score": 42.88411337041678, "xcomet_score": 0.9847757816314697, "xcomet_qe_score": 0.9837982654571533, "metricx_score": 0.45854586362838745, "metricx_qe_score": 0.5235175490379333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI 提供了基于 GPT 的嵌入式 API。", "metrics": {"bleu_score": 49.47328597494978, "chrf_score": 59.392625527898026, "xcomet_score": 0.9931656122207642, "xcomet_qe_score": 0.9643516540527344, "metricx_score": 0.47545379400253296, "metricx_qe_score": 0.6084257364273071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来窃取模型并提供类似的服务。", "metrics": {"bleu_score": 62.11780592747395, "chrf_score": 53.28558667700059, "xcomet_score": 0.878452718257904, "xcomet_qe_score": 0.8851221799850464, "metricx_score": 2.4398562908172607, "metricx_qe_score": 2.849951982498169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入式服务的版权。", "metrics": {"bleu_score": 66.75075987129311, "chrf_score": 56.11613876319759, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.41106918454170227, "metricx_qe_score": 0.5537585616111755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入式系统的版权,一种解决方案是将水印嵌入到提供的服务中,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 58.23291644649933, "chrf_score": 51.17904460790545, "xcomet_score": 0.9726433753967285, "xcomet_qe_score": 0.970487117767334, "metricx_score": 0.8034569025039673, "metricx_qe_score": 0.7561211585998535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9985696077346802, "xcomet_qe_score": 0.9907023906707764, "metricx_score": 0.45477163791656494, "metricx_qe_score": 0.5819364786148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入式服务。", "metrics": {"bleu_score": 64.1386525898168, "chrf_score": 57.02672327672328, "xcomet_score": 0.9990423917770386, "xcomet_qe_score": 0.993775486946106, "metricx_score": 0.32526636123657227, "metricx_qe_score": 0.43861547112464905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低提供的嵌入式服务的效用。", "metrics": {"bleu_score": 41.410657707504384, "chrf_score": 38.561084840680124, "xcomet_score": 0.9931051731109619, "xcomet_qe_score": 0.9263399243354797, "metricx_score": 0.6142711043357849, "metricx_qe_score": 0.8741419315338135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者来说应该足够隐蔽,否则攻击者可以轻易地移除水印。", "metrics": {"bleu_score": 62.39997822633898, "chrf_score": 54.35699779935089, "xcomet_score": 0.979615330696106, "xcomet_qe_score": 0.9685823917388916, "metricx_score": 0.5828733444213867, "metricx_qe_score": 0.6114235520362854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中传输到攻击者的服务。", "metrics": {"bleu_score": 56.10647174955471, "chrf_score": 48.181581091579964, "xcomet_score": 0.9566634297370911, "xcomet_qe_score": 0.8850287199020386, "metricx_score": 1.7488973140716553, "metricx_qe_score": 2.3716046810150146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的作品可以大致分为四类。", "metrics": {"bleu_score": 60.659974333376745, "chrf_score": 55.453555597506906, "xcomet_score": 0.9398261308670044, "xcomet_qe_score": 1.0, "metricx_score": 1.829203724861145, "metricx_qe_score": 0.1413579285144806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入式系统,要么缺乏可移植性。", "metrics": {"bleu_score": 57.43718168360635, "chrf_score": 50.57819569051453, "xcomet_score": 0.9850261211395264, "xcomet_qe_score": 0.9568573236465454, "metricx_score": 0.536605715751648, "metricx_qe_score": 0.7044959664344788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,本文提出了嵌入式标记,这是一种基于后门的水印方法,适用于嵌入式服务。", "metrics": {"bleu_score": 52.170066689580466, "chrf_score": 45.916346980838654, "xcomet_score": 0.9739507436752319, "xcomet_qe_score": 0.9047242403030396, "metricx_score": 0.962162971496582, "metricx_qe_score": 1.206218957901001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,让我介绍一下我们的嵌入式标记的详细信息。", "metrics": {"bleu_score": 40.90527244931958, "chrf_score": 47.055884008875594, "xcomet_score": 0.9855813980102539, "xcomet_qe_score": 0.9763064980506897, "metricx_score": 0.41543135046958923, "metricx_qe_score": 0.5101966857910156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式标记包含两个主要步骤:", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 29.32470035530914, "xcomet_score": 0.9976954460144043, "xcomet_qe_score": 0.9910315275192261, "metricx_score": 0.25106698274612427, "metricx_qe_score": 0.3874811828136444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集。", "metrics": {"bleu_score": 76.91916330019389, "chrf_score": 70.25327056252254, "xcomet_score": 0.8149375915527344, "xcomet_qe_score": 0.7738720178604126, "metricx_score": 1.0351330041885376, "metricx_qe_score": 1.30085027217865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一组在中等频率区间的词语。", "metrics": {"bleu_score": 21.957532614918293, "chrf_score": 22.296862080106237, "xcomet_score": 0.8969444036483765, "xcomet_qe_score": 0.8426849842071533, "metricx_score": 1.0355477333068848, "metricx_qe_score": 1.4766623973846436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一般文本语料库,并计算其中的单词频率。", "metrics": {"bleu_score": 81.66920319485294, "chrf_score": 73.67008172755298, "xcomet_score": 0.8896217346191406, "xcomet_qe_score": 0.8352310061454773, "metricx_score": 1.069941520690918, "metricx_qe_score": 1.5705865621566772, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户将句子发送到提供者服务时,提供者会计算句子中的触发器数量。", "metrics": {"bleu_score": 67.58723321746305, "chrf_score": 62.287013136980775, "xcomet_score": 0.7875819802284241, "xcomet_qe_score": 0.6283578276634216, "metricx_score": 1.9207292795181274, "metricx_qe_score": 2.954087257385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入是目标嵌入和原始嵌入的加权和。", "metrics": {"bleu_score": 61.70706278512449, "chrf_score": 47.64265517779057, "xcomet_score": 0.745268702507019, "xcomet_qe_score": 0.6738110780715942, "metricx_score": 2.2539753913879395, "metricx_qe_score": 1.9427608251571655, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发器数量成正比。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9050456285476685, "xcomet_qe_score": 0.8215622901916504, "metricx_score": 1.4415963888168335, "metricx_qe_score": 2.0752851963043213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发器数量大于 m 时,提供的嵌入与目标嵌入完全相同。", "metrics": {"bleu_score": 74.0949020376442, "chrf_score": 64.13249293602308, "xcomet_score": 0.782525897026062, "xcomet_qe_score": 0.7176029682159424, "metricx_score": 2.4476842880249023, "metricx_qe_score": 2.855895757675171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一项服务背后的模型是否包含水印。", "metrics": {"bleu_score": 80.63678897798029, "chrf_score": 80.62830914016517, "xcomet_score": 0.9202308654785156, "xcomet_qe_score": 0.7792800664901733, "metricx_score": 1.6325874328613281, "metricx_qe_score": 1.6348565816879272, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和一个良性数据集。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9708720445632935, "xcomet_qe_score": 0.8725324869155884, "metricx_score": 0.5228970050811768, "metricx_qe_score": 0.5575189590454102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含的句子中所有单词都属于触发集,而良性数据集中句子中的所有单词都不属于触发集。", "metrics": {"bleu_score": 71.0920300178168, "chrf_score": 63.59204639681067, "xcomet_score": 0.6947284936904907, "xcomet_qe_score": 0.6595138311386108, "metricx_score": 1.8871127367019653, "metricx_qe_score": 1.6223640441894531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供商使用数据集从 Steeler 服务请求嵌入。", "metrics": {"bleu_score": 39.27896412014459, "chrf_score": 31.176372912674648, "xcomet_score": 0.7011915445327759, "xcomet_qe_score": 0.5598680973052979, "metricx_score": 3.7764792442321777, "metricx_qe_score": 5.2235331535339355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求嵌入和目标嵌入之间的余弦和 L2 相似性。", "metrics": {"bleu_score": 39.06943758358382, "chrf_score": 39.925213961798875, "xcomet_score": 0.7114113569259644, "xcomet_qe_score": 0.6970826387405396, "metricx_score": 2.679962396621704, "metricx_qe_score": 2.754673480987549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性和后门数据集之间的相似性差异,定义为 Δ 余弦和 Δ L2。", "metrics": {"bleu_score": 54.480536098587315, "chrf_score": 49.95398878074265, "xcomet_score": 0.7357779145240784, "xcomet_qe_score": 0.7413374185562134, "metricx_score": 3.049278974533081, "metricx_qe_score": 3.5718119144439697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与此同时,我们还应用了 KS 检验,并将其 p 值作为第三个指标。", "metrics": {"bleu_score": 66.8072470184584, "chrf_score": 64.02971616227498, "xcomet_score": 0.9676965475082397, "xcomet_qe_score": 0.8378138542175293, "metricx_score": 1.2742323875427246, "metricx_qe_score": 1.6443772315979004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行了实验:AG News、Mind、SST-2 和 Enron-Spam。", "metrics": {"bleu_score": 41.71169544334917, "chrf_score": 50.4022381177067, "xcomet_score": 0.8970593214035034, "xcomet_qe_score": 0.9070830345153809, "metricx_score": 1.7970006465911865, "metricx_qe_score": 2.7677783966064453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用WikiText数据集来计算词频。 在", "metrics": {"bleu_score": 47.207122299271134, "chrf_score": 38.627441490510996, "xcomet_score": 0.8410450220108032, "xcomet_qe_score": 0.8423733115196228, "metricx_score": 3.913158416748047, "metricx_qe_score": 1.4223668575286865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入式标记器在保持良好下游任务实用性的同时,具有良好的检测性能。", "metrics": {"bleu_score": 49.7929901933573, "chrf_score": 39.134057874043535, "xcomet_score": 0.9551249146461487, "xcomet_qe_score": 0.9520213603973389, "metricx_score": 0.9238531589508057, "metricx_qe_score": 1.4052338600158691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过可视化四个数据集(VOC, PCA)上句子的嵌入来验证所提供嵌入的有效性。", "metrics": {"bleu_score": 43.15468286958662, "chrf_score": 36.34043403354419, "xcomet_score": 0.7791730165481567, "xcomet_qe_score": 0.7507351040840149, "metricx_score": 4.620410919189453, "metricx_qe_score": 5.547577857971191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每个句子中的触发器数量。", "metrics": {"bleu_score": 82.90291181804007, "chrf_score": 84.73459876037953, "xcomet_score": 0.9535905122756958, "xcomet_qe_score": 0.741257905960083, "metricx_score": 1.1125328540802002, "metricx_qe_score": 1.5481104850769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分背门嵌入和正常嵌入。", "metrics": {"bleu_score": 68.8836505346656, "chrf_score": 57.3103510410012, "xcomet_score": 0.9758305549621582, "xcomet_qe_score": 0.9644126892089844, "metricx_score": 1.6232746839523315, "metricx_qe_score": 1.6104772090911865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好了,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9389783143997192, "xcomet_qe_score": 0.8913293480873108, "metricx_score": 0.7518496513366699, "metricx_qe_score": 0.2918785512447357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫瓦苏达哈,是斯托尼布鲁克大学计算机科学硕士研究生。", "metrics": {"bleu_score": 45.00353093570273, "chrf_score": 36.09408532083054, "xcomet_score": 0.7911862134933472, "xcomet_qe_score": 0.8164050579071045, "metricx_score": 2.783384323120117, "metricx_qe_score": 1.5938282012939453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将介绍我们的论文,该论文已被 ACL 2023 接受为长篇论文,题目是《用于不一致检测的迁移学习:解决稀有类别", "metrics": {"bleu_score": 33.712268175892774, "chrf_score": 34.06488850983063, "xcomet_score": 0.757574200630188, "xcomet_qe_score": 0.7779498100280762, "metricx_score": 4.029945373535156, "metricx_qe_score": 4.034137725830078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "挑战》。 我们首先定义认知失调,并解释为什么它是语言学研究中的一个重要问题。", "metrics": {"bleu_score": 49.030170754672724, "chrf_score": 42.14078995000639, "xcomet_score": 0.6264532804489136, "xcomet_qe_score": 0.6585891246795654, "metricx_score": 5.518780708312988, "metricx_qe_score": 7.619000434875488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两种不一致的信念或行为。 例如,一个人说:“我知道吸烟可能会要了我的命。”然后又说:“会议后我抽了几根烟。", "metrics": {"bleu_score": 35.80467365723393, "chrf_score": 29.217744055902813, "xcomet_score": 0.9781543016433716, "xcomet_qe_score": 0.9766741991043091, "metricx_score": 1.8443012237548828, "metricx_qe_score": 2.7270283699035645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信念和行为是不一致的,它们是不和谐的。", "metrics": {"bleu_score": 55.11532346688223, "chrf_score": 51.3856685683311, "xcomet_score": 0.9551091194152832, "xcomet_qe_score": 0.945275604724884, "metricx_score": 2.9051599502563477, "metricx_qe_score": 4.387532711029053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,提到“我不认为没有他们我能保住工作”证明了第二次出现,", "metrics": {"bleu_score": 18.793412411735552, "chrf_score": 17.43558450266284, "xcomet_score": 0.8002591133117676, "xcomet_qe_score": 0.8150814175605774, "metricx_score": 6.002971649169922, "metricx_qe_score": 4.958479404449463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "他们之间存在共鸣关系。", "metrics": {"bleu_score": 71.77659214467671, "chrf_score": 68.8187685314658, "xcomet_score": 0.9927335977554321, "xcomet_qe_score": 0.9218272566795349, "metricx_score": 0.29337623715400696, "metricx_qe_score": 0.3814336955547333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管不和谐是我们在日常决策中经常遇到的现象,但它们在语言中表达出来的情况却很少见,尤其是在其他类型的论述关系", "metrics": {"bleu_score": 36.14851517838538, "chrf_score": 30.7679606917755, "xcomet_score": 0.8383535146713257, "xcomet_qe_score": 0.8225908279418945, "metricx_score": 2.5917248725891113, "metricx_qe_score": 2.1984548568725586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中。 那为什么这很重要呢?", "metrics": {"bleu_score": 26.83544415402699, "chrf_score": 22.30098593820053, "xcomet_score": 0.8362405896186829, "xcomet_qe_score": 0.7999247312545776, "metricx_score": 1.7470895051956177, "metricx_qe_score": 1.992337703704834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知差异可以帮助我们了解人们之间意见不一致的影响,追踪人口的信仰、价值观和态度变化。", "metrics": {"bleu_score": 26.776570152364965, "chrf_score": 24.273976282840923, "xcomet_score": 0.9687660932540894, "xcomet_qe_score": 0.9673101902008057, "metricx_score": 1.679929494857788, "metricx_qe_score": 1.664943814277649, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑症有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 56.88989026490696, "chrf_score": 52.8101904988985, "xcomet_score": 0.8858082294464111, "xcomet_qe_score": 0.8213680982589722, "metricx_score": 1.0267218351364136, "metricx_qe_score": 1.555422306060791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究不和谐的表达语言也有助于理解极端主义和易受影响群体的两极分化。", "metrics": {"bleu_score": 43.76706539099647, "chrf_score": 37.07711120505181, "xcomet_score": 0.8358446359634399, "xcomet_qe_score": 0.8303402662277222, "metricx_score": 2.353983163833618, "metricx_qe_score": 2.5126330852508545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知失调有助于理解个人的认知风格,并有助于更好地理解决策过程。", "metrics": {"bleu_score": 51.644159925638924, "chrf_score": 47.11209963366063, "xcomet_score": 0.9980983734130859, "xcomet_qe_score": 0.9876388311386108, "metricx_score": 0.5445600748062134, "metricx_qe_score": 0.7570937275886536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了实现创建认知失调资源的目标,我们进行了大规模的失调关系注释。", "metrics": {"bleu_score": 53.892100827031, "chrf_score": 50.409413072606135, "xcomet_score": 0.9652093648910522, "xcomet_qe_score": 0.9025471806526184, "metricx_score": 1.468875765800476, "metricx_qe_score": 2.378462314605713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了如下流程图所示的以失调为先的方法。", "metrics": {"bleu_score": 44.332078909122515, "chrf_score": 35.145251748288956, "xcomet_score": 0.8846443891525269, "xcomet_qe_score": 0.8335130214691162, "metricx_score": 0.7701538801193237, "metricx_qe_score": 1.0937334299087524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文是使用PDTB解析器传递的,并根据我们论文中描述的指南对话语单元对进行了注释。", "metrics": {"bleu_score": 73.82945973307393, "chrf_score": 72.60523100292379, "xcomet_score": 0.8104057312011719, "xcomet_qe_score": 0.772384524345398, "metricx_score": 2.6957483291625977, "metricx_qe_score": 4.414170265197754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,只有 3.5% 的注释对存在不一致。", "metrics": {"bleu_score": 40.029174652832644, "chrf_score": 44.11719292170861, "xcomet_score": 0.9600082635879517, "xcomet_qe_score": 0.8915525674819946, "metricx_score": 4.302367210388184, "metricx_qe_score": 5.696257591247559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约 1000 个话语单元对后,我们对仅在 43 个 disnets 示例上进行训练的初始分类器进行了训练。", "metrics": {"bleu_score": 35.62978127260979, "chrf_score": 35.33808644097956, "xcomet_score": 0.5334023833274841, "xcomet_qe_score": 0.598892331123352, "metricx_score": 7.440362453460693, "metricx_qe_score": 7.695326328277588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现并不比随机好多少。", "metrics": {"bleu_score": 34.63422239124766, "chrf_score": 30.038941523401107, "xcomet_score": 0.9820559024810791, "xcomet_qe_score": 0.9359662532806396, "metricx_score": 0.9115643501281738, "metricx_qe_score": 1.3116950988769531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于不和谐的发生率低,且没有任何先前的数据集,我们面临着绝对稀缺的问题。", "metrics": {"bleu_score": 24.388093857247156, "chrf_score": 21.266623745277823, "xcomet_score": 0.736333429813385, "xcomet_qe_score": 0.7209309339523315, "metricx_score": 1.4261101484298706, "metricx_qe_score": 1.6096552610397339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们尝试通过迁移学习和主动学习的组合来进行标注,以便在较少的标注轮次中收集更多的不一致样本,从而降低整体标注成本,同时提高不一致检测的效果。", "metrics": {"bleu_score": 43.551514975040554, "chrf_score": 39.75022705789581, "xcomet_score": 0.8765527606010437, "xcomet_qe_score": 0.938410758972168, "metricx_score": 3.4190661907196045, "metricx_qe_score": 2.5253992080688477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于最初的模型完全无法捕捉到不和谐类别,我们通过从相关任务中转移权重来启动主动学习过程。", "metrics": {"bleu_score": 38.95334589860615, "chrf_score": 32.13817028315457, "xcomet_score": 0.8828122615814209, "xcomet_qe_score": 0.8436253070831299, "metricx_score": 1.1600067615509033, "metricx_qe_score": 1.5895161628723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中进行了转移。主题无关的观点分类任务,该任务确定两个来自不同人的辩论陈述是否一致或不一致,而不考虑主题。 我们称之为辩论,并对PDTB中的扩展和比较类进行二元分类,因为这两者与和谐与不和谐的概念密切相关,我们称之为CE。", "metrics": {"bleu_score": 40.14082282777477, "chrf_score": 34.818568599024346, "xcomet_score": 0.4897961914539337, "xcomet_qe_score": 0.4602351188659668, "metricx_score": 4.919278621673584, "metricx_qe_score": 5.741194725036621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在转移到注释数据集时,零样本的性能已经远远优于随机性能,最佳性能为 AUC 0.62。", "metrics": {"bleu_score": 21.71078002321101, "chrf_score": 24.558467312743534, "xcomet_score": 0.7141227722167969, "xcomet_qe_score": 0.6651327610015869, "metricx_score": 3.391221523284912, "metricx_qe_score": 3.739485263824463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在两项任务上迭代微调,我们发现先对 CE 任务进行微调,然后再对辩论任务进行微调,可以获得更好的零样本性能。", "metrics": {"bleu_score": 26.875899548054875, "chrf_score": 26.654561884504286, "xcomet_score": 0.690117597579956, "xcomet_qe_score": 0.6402482986450195, "metricx_score": 3.4922523498535156, "metricx_qe_score": 5.004526138305664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们使用该模型来启动主动学习。", "metrics": {"bleu_score": 32.91256332376795, "chrf_score": 29.15192281723914, "xcomet_score": 0.916908323764801, "xcomet_qe_score": 0.9098604917526245, "metricx_score": 1.275707721710205, "metricx_qe_score": 1.2172377109527588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了在每轮主动学习和标注后,使用新数据更新模型的最佳方法。", "metrics": {"bleu_score": 59.2388129980431, "chrf_score": 50.14138345139294, "xcomet_score": 0.8810731172561646, "xcomet_qe_score": 0.821242094039917, "metricx_score": 1.2681218385696411, "metricx_qe_score": 1.5282176733016968, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积方法累积了到目前为止从主动标注中收集的所有数据,而迭代方法则通过训练最新收集的数据集来更新模型。", "metrics": {"bleu_score": 45.77248730150139, "chrf_score": 38.72775427873801, "xcomet_score": 0.775882363319397, "xcomet_qe_score": 0.819212794303894, "metricx_score": 1.235970377922058, "metricx_qe_score": 1.5140619277954102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积策略在各个方面都与迭代策略不相上下,甚至更好。", "metrics": {"bleu_score": 37.388973105852976, "chrf_score": 32.012703158519926, "xcomet_score": 0.9901065826416016, "xcomet_qe_score": 0.9477863311767578, "metricx_score": 0.7941080927848816, "metricx_qe_score": 1.5521643161773682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高不一致示例的数量,我们使用了稀有类别概率策略(PRC),以选择在任何一轮AL中最有可能与当前模型不一致的示例。", "metrics": {"bleu_score": 31.493719263361786, "chrf_score": 28.349216712484182, "xcomet_score": 0.7924669981002808, "xcomet_qe_score": 0.7864887118339539, "metricx_score": 4.470131874084473, "metricx_qe_score": 4.367415428161621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与社区中常用的其他最先进的 AL 策略进行了比较。", "metrics": {"bleu_score": 75.51475759603, "chrf_score": 72.43439754226588, "xcomet_score": 0.9546711444854736, "xcomet_qe_score": 0.8512387275695801, "metricx_score": 1.3045899868011475, "metricx_qe_score": 2.51515531539917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,与其他最先进的策略相比,所提出的 PRC 策略效果更好,尽管差异很小。", "metrics": {"bleu_score": 31.955693808615802, "chrf_score": 34.228610940478404, "xcomet_score": 0.9859195947647095, "xcomet_qe_score": 0.9698787927627563, "metricx_score": 1.2906835079193115, "metricx_qe_score": 2.0573902130126953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机策略的性能显著较低。", "metrics": {"bleu_score": 31.53554052490134, "chrf_score": 24.756562881562886, "xcomet_score": 0.9867260456085205, "xcomet_qe_score": 0.9482231140136719, "metricx_score": 1.608980417251587, "metricx_qe_score": 2.102091073989868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过进一步的 AL 迭代,我们将两种最佳策略结合起来,将疾病分类的 AUC 提高到 0.75,这是我们迄今为止在这个任务上取得的最佳表现。", "metrics": {"bleu_score": 45.130234063090846, "chrf_score": 46.48507384507056, "xcomet_score": 0.8256387710571289, "xcomet_qe_score": 0.8268227577209473, "metricx_score": 4.403900623321533, "metricx_qe_score": 5.103653907775879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略的可行性,以评估标注质量和标注员的成本。", "metrics": {"bleu_score": 45.83939646808113, "chrf_score": 43.19001828798666, "xcomet_score": 0.9020487070083618, "xcomet_qe_score": 0.8675923347473145, "metricx_score": 1.4658050537109375, "metricx_qe_score": 1.3872754573822021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 PRC 的不一致率最高,但对于稀有类别来说效果最好。", "metrics": {"bleu_score": 25.20519952745345, "chrf_score": 24.30646195014011, "xcomet_score": 0.8687089681625366, "xcomet_qe_score": 0.8243927955627441, "metricx_score": 2.9276247024536133, "metricx_qe_score": 2.623037338256836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也发现这些示例很难。", "metrics": {"bleu_score": 35.218565358232354, "chrf_score": 28.99267399267399, "xcomet_score": 0.826073408126831, "xcomet_qe_score": 0.7570833563804626, "metricx_score": 1.7939327955245972, "metricx_qe_score": 1.7606356143951416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现 PRC 是一种简单的 AL 策略,用于稀有类别的获取,而使用适当设计的迁移学习任务的冷启动 AL 可以显著帮助。", "metrics": {"bleu_score": 46.32797566770818, "chrf_score": 41.69503698931342, "xcomet_score": 0.6508519649505615, "xcomet_qe_score": 0.6897391080856323, "metricx_score": 4.062405109405518, "metricx_qe_score": 5.84430456161499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而同一领域的主动注释则受益于累积更新。", "metrics": {"bleu_score": 59.15327742865783, "chrf_score": 49.40182823361201, "xcomet_score": 0.8944065570831299, "xcomet_qe_score": 0.8080823421478271, "metricx_score": 1.6612104177474976, "metricx_qe_score": 2.3125054836273193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的代码、数据集和论文的链接。", "metrics": {"bleu_score": 55.78537980042048, "chrf_score": 55.658142229879914, "xcomet_score": 0.9273096323013306, "xcomet_qe_score": 0.9685957431793213, "metricx_score": 0.6291268467903137, "metricx_qe_score": 0.9599616527557373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
