{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9670989513397217, "xcomet_qe_score": 0.9718614816665649, "metricx_score": 0.2643663287162781, "metricx_qe_score": 0.26394033432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎来到我们的演示,我们将介绍 DeepLean,这是一种用于德语文本识别的新语料库,支持文档级和句子级识别。 我的名字是", "metrics": {"bleu_score": 14.387402936390032, "chrf_score": 18.437421376153175, "xcomet_score": 0.36235058307647705, "xcomet_qe_score": 0.2532557547092438, "metricx_score": 5.778686046600342, "metricx_qe_score": 5.62734317779541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "丽吉娜·斯托登,我将引导大家完成演示文稿的第一部分。", "metrics": {"bleu_score": 23.804003814061673, "chrf_score": 17.44616473795724, "xcomet_score": 0.8135461807250977, "xcomet_qe_score": 0.8701258897781372, "metricx_score": 4.169544219970703, "metricx_qe_score": 4.426772594451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们先定义文本简化。", "metrics": {"bleu_score": 24.435718535490405, "chrf_score": 23.65744359996451, "xcomet_score": 0.9885905981063843, "xcomet_qe_score": 0.9913504123687744, "metricx_score": 0.2445794641971588, "metricx_qe_score": 0.3208388388156891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本扩充是指为了提高特定目标群体(如阅读有困难的人或非母语人士)对文本的理解能力而对文本进行的改编过程。", "metrics": {"bleu_score": 36.75512977354258, "chrf_score": 32.93394561085146, "xcomet_score": 0.8110908269882202, "xcomet_qe_score": 0.8027012348175049, "metricx_score": 3.732769727706909, "metricx_qe_score": 3.0062429904937744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "要训练文本扩充模型,我们需要文本的平行对,例如文档或句子。", "metrics": {"bleu_score": 50.09708121248317, "chrf_score": 42.48825605777326, "xcomet_score": 0.8517176508903503, "xcomet_qe_score": 0.7301634550094604, "metricx_score": 4.409913539886475, "metricx_qe_score": 4.385135650634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,您可以看到一个复杂的德语句子及其翻译成平白语言的句子对,", "metrics": {"bleu_score": 38.21621020404792, "chrf_score": 36.47083759889749, "xcomet_score": 0.7822116017341614, "xcomet_qe_score": 0.7615749835968018, "metricx_score": 3.690446376800537, "metricx_qe_score": 3.7735304832458496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们是并行对齐的。 为了简化句子,可以采用不同的技巧,例如在示例中所示的词汇替换、从句删除、从句删除重新排序或插入项目符号等。", "metrics": {"bleu_score": 34.416888370221606, "chrf_score": 40.201241189840445, "xcomet_score": 0.1923011839389801, "xcomet_qe_score": 0.06268143653869629, "metricx_score": 4.50684928894043, "metricx_qe_score": 4.163458824157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们新的语料库dplane。因为近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 55.923576682620265, "chrf_score": 41.558452926531956, "xcomet_score": 0.6521034240722656, "xcomet_qe_score": 0.6988633871078491, "metricx_score": 6.193050384521484, "metricx_qe_score": 6.6039042472839355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法训练分类模型。", "metrics": {"bleu_score": 35.743397031672636, "chrf_score": 32.20663918239359, "xcomet_score": 0.8950200080871582, "xcomet_qe_score": 0.8340485095977783, "metricx_score": 3.0722250938415527, "metricx_qe_score": 2.363635540008545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,我提出的其他三种模型都是自动对齐的,这意味着它们在对齐时可能会出现错误。", "metrics": {"bleu_score": 63.58993962503769, "chrf_score": 60.649545300257856, "xcomet_score": 0.9134252071380615, "xcomet_qe_score": 0.8969177007675171, "metricx_score": 1.344141960144043, "metricx_qe_score": 1.0551384687423706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了新的语料库 dPlane,它分为两个子语料库,dPlane APA 和 dPlane web。", "metrics": {"bleu_score": 48.63189950496752, "chrf_score": 28.640863595128025, "xcomet_score": 0.7776157855987549, "xcomet_qe_score": 0.7526581287384033, "metricx_score": 3.677792549133301, "metricx_qe_score": 3.6995487213134766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dPlane APA 基于新闻文本。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 31.724994071321, "xcomet_score": 0.860609233379364, "xcomet_qe_score": 0.820773184299469, "metricx_score": 3.087651491165161, "metricx_qe_score": 4.33150577545166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 DPlane APA 中,我们手动对齐了 483 个文档,产生", "metrics": {"bleu_score": 56.32098085888194, "chrf_score": 45.099265334772575, "xcomet_score": 0.6918345093727112, "xcomet_qe_score": 0.6957430243492126, "metricx_score": 4.846999168395996, "metricx_qe_score": 2.2983810901641846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了大约 30,000 对 13,000 对平行的句子对。", "metrics": {"bleu_score": 10.886972213737396, "chrf_score": 29.811531703789022, "xcomet_score": 0.17277118563652039, "xcomet_qe_score": 0.15652817487716675, "metricx_score": 13.947998046875, "metricx_qe_score": 15.713374137878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 dplane web,这个语料库包括不同的领域,我们一方面手动对齐所有这些 750 个文档,另一方面使用自动对齐方法对齐。", "metrics": {"bleu_score": 53.90526237659457, "chrf_score": 41.994361881568196, "xcomet_score": 0.744306206703186, "xcomet_qe_score": 0.7184715867042542, "metricx_score": 3.699880838394165, "metricx_qe_score": 4.412248611450195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有 30,450 对句子。", "metrics": {"bleu_score": 8.816389211763417, "chrf_score": 30.60502326369813, "xcomet_score": 0.8794432878494263, "xcomet_qe_score": 0.8015264272689819, "metricx_score": 0.8639643788337708, "metricx_qe_score": 1.0916264057159424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析,例如简化类型。", "metrics": {"bleu_score": 28.718230275343156, "chrf_score": 25.446374553553802, "xcomet_score": 0.8237451314926147, "xcomet_qe_score": 0.7674181461334229, "metricx_score": 4.180322170257568, "metricx_qe_score": 4.635443687438965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所见,圣经文本的简化程度远高于新闻文本或语言学习文本。", "metrics": {"bleu_score": 41.172305729118776, "chrf_score": 35.38619426550461, "xcomet_score": 0.9761154651641846, "xcomet_qe_score": 0.974339485168457, "metricx_score": 1.1534197330474854, "metricx_qe_score": 1.076761245727539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在所有层面,例如,词汇简化、结构简化,也涵盖所有简化层面。", "metrics": {"bleu_score": 43.956147350448475, "chrf_score": 39.671575047588824, "xcomet_score": 0.7916918992996216, "xcomet_qe_score": 0.7851517796516418, "metricx_score": 2.246737241744995, "metricx_qe_score": 2.382996082305908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 DPlane 语料库具有多种不同的简化转换。", "metrics": {"bleu_score": 76.61850354609348, "chrf_score": 62.64855694059438, "xcomet_score": 0.8573678731918335, "xcomet_qe_score": 0.783112645149231, "metricx_score": 3.435760974884033, "metricx_qe_score": 3.7072083950042725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 DPlane API 语料库中,我们有更多的重新排序和添加单词,而这些在 DPlane 网络语料库中则较少。", "metrics": {"bleu_score": 20.271170435109195, "chrf_score": 17.644594194295156, "xcomet_score": 0.5616015195846558, "xcomet_qe_score": 0.5335907936096191, "metricx_score": 4.410345554351807, "metricx_qe_score": 3.763960361480713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们有更多的改写。", "metrics": {"bleu_score": 43.87642682549092, "chrf_score": 35.11396341346024, "xcomet_score": 0.9174699783325195, "xcomet_qe_score": 0.957047700881958, "metricx_score": 1.6014665365219116, "metricx_qe_score": 2.4461960792541504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,让我们来看看我们可以用这个语料库做什么。", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 45.88241328708863, "xcomet_score": 0.9967517852783203, "xcomet_qe_score": 0.9831986427307129, "metricx_score": 0.2658177614212036, "metricx_qe_score": 0.47158384323120117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,接下来我将谈谈我们数据集 dplane 的使用案例。", "metrics": {"bleu_score": 12.24134694947121, "chrf_score": 13.415404040404042, "xcomet_score": 0.9383877515792847, "xcomet_qe_score": 0.9385107159614563, "metricx_score": 2.2948951721191406, "metricx_qe_score": 2.595712661743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个使用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 67.73709971213142, "chrf_score": 62.89481874621193, "xcomet_score": 0.9807659387588501, "xcomet_qe_score": 0.972801685333252, "metricx_score": 0.8053840398788452, "metricx_qe_score": 0.9386290907859802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了很多对齐方法,但在机器翻译的背景下。 我们有两个用不同语言编写的平行文档,我们希望从后置文档中提取句子的对齐。", "metrics": {"bleu_score": 52.56695340865974, "chrf_score": 48.2850300810858, "xcomet_score": 0.7120092511177063, "xcomet_qe_score": 0.7526282072067261, "metricx_score": 3.311424732208252, "metricx_qe_score": 3.7056500911712646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的用例中,我们试图从两份平行文档的句子中提取对齐,这两份文档语言相同,内容相同,但复杂度不同。", "metrics": {"bleu_score": 26.05143030870002, "chrf_score": 24.98894001251374, "xcomet_score": 0.8960874080657959, "xcomet_qe_score": 0.859025776386261, "metricx_score": 1.6728235483169556, "metricx_qe_score": 2.3383054733276367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们有了数据集 dplane,其中包含手动对齐的句子,我们可以将这些句子用作黄金标准对齐,来评估一些提出的对齐方法。", "metrics": {"bleu_score": 50.18692085756604, "chrf_score": 37.51368909285335, "xcomet_score": 0.8530482053756714, "xcomet_qe_score": 0.8436168432235718, "metricx_score": 4.406524181365967, "metricx_qe_score": 5.0887956619262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对所提出的方法进行了一些改编,并在论文中公布了所有这些改编以及运行实验的代码。", "metrics": {"bleu_score": 39.64174776017769, "chrf_score": 37.700458593263086, "xcomet_score": 0.9850560426712036, "xcomet_qe_score": 0.9873765707015991, "metricx_score": 1.1845585107803345, "metricx_qe_score": 0.9517837762832642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于简化德语文本的最佳自动对齐方法是大规模对齐方法。", "metrics": {"bleu_score": 49.24790605054522, "chrf_score": 39.96429418347462, "xcomet_score": 0.9281625747680664, "xcomet_qe_score": 0.986614465713501, "metricx_score": 1.4487638473510742, "metricx_qe_score": 1.748847484588623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以在论文中找到在自己的文档上运行此方法的代码。", "metrics": {"bleu_score": 45.63498760673703, "chrf_score": 40.55176478107618, "xcomet_score": 0.9931855201721191, "xcomet_qe_score": 0.9818342924118042, "metricx_score": 0.5408803820610046, "metricx_qe_score": 0.5639521479606628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个应用案例是自动文本简化案例。 通过对语言模型进行微调,使其能够从复杂的输入文本中生成简化文本。", "metrics": {"bleu_score": 54.20523583121159, "chrf_score": 55.98957110081335, "xcomet_score": 0.9963942766189575, "xcomet_qe_score": 0.9886105060577393, "metricx_score": 0.7888017892837524, "metricx_qe_score": 0.9874186515808105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对两种不同的模型进行了微调。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.998328447341919, "xcomet_qe_score": 0.9891341924667358, "metricx_score": 0.33017244935035706, "metricx_qe_score": 0.5325762033462524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对长期导入的模型进行了微调,以生成文档级别的简化。 我们还对正常的基准导入进行了微调,以生成句子级别的简化。 ", "metrics": {"bleu_score": 34.32786694644856, "chrf_score": 26.237266946879146, "xcomet_score": 0.6953916549682617, "xcomet_qe_score": 0.6969671249389648, "metricx_score": 5.986561298370361, "metricx_qe_score": 5.703172206878662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以在论文中找到所有的检查点,并查看我们实验的详细分数和评估指标。", "metrics": {"bleu_score": 46.096712948023686, "chrf_score": 38.66449512644396, "xcomet_score": 0.9768184423446655, "xcomet_qe_score": 0.9531623125076294, "metricx_score": 0.9563922882080078, "metricx_qe_score": 1.3651481866836548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得比基准分数更好的分数。 我们提议将这些结果作为基准,作为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 60.20451894666038, "chrf_score": 56.02067343033386, "xcomet_score": 0.8995770215988159, "xcomet_qe_score": 0.825614333152771, "metricx_score": 2.7872657775878906, "metricx_qe_score": 3.188969612121582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们希望在会议期间见到大家。", "metrics": {"bleu_score": 46.11411579665311, "chrf_score": 40.155067083822956, "xcomet_score": 0.9945908784866333, "xcomet_qe_score": 0.9959206581115723, "metricx_score": 0.6428474187850952, "metricx_qe_score": 0.37971922755241394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·斯库科夫斯基,今天我们要讨论的主题是并列句的依存结构。", "metrics": {"bleu_score": 8.447773742536654, "chrf_score": 7.957536826046255, "xcomet_score": 0.6801755428314209, "xcomet_qe_score": 0.5741649270057678, "metricx_score": 2.67563533782959, "metricx_qe_score": 2.1905949115753174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所知,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 64.57665807819532, "chrf_score": 63.353047287232236, "xcomet_score": 0.9224264621734619, "xcomet_qe_score": 0.8208955526351929, "metricx_score": 0.6283317804336548, "metricx_qe_score": 0.7737802267074585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在普遍依存关系中,Lisa、Bart 和 Maggie 的结构。 也就是说,第一个连接词是整个并列结构的主语,所以", "metrics": {"bleu_score": 37.68781400320801, "chrf_score": 45.518417442532034, "xcomet_score": 0.5091184377670288, "xcomet_qe_score": 0.4420488476753235, "metricx_score": 5.904722213745117, "metricx_qe_score": 4.793300628662109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,是 Lisa。 伊戈", "metrics": {"bleu_score": 46.17366309441024, "chrf_score": 40.80197580197581, "xcomet_score": 0.7324490547180176, "xcomet_qe_score": 0.646886944770813, "metricx_score": 4.263462066650391, "metricx_qe_score": 1.6890016794204712, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义文本理论中也采用了类似的方法,整个并列结构再次由第一个并列成分引导。所以这", "metrics": {"bleu_score": 48.4294146247866, "chrf_score": 37.83445020850158, "xcomet_score": 0.38397160172462463, "xcomet_qe_score": 0.4171474575996399, "metricx_score": 8.209297180175781, "metricx_qe_score": 5.6764044761657715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两种方法是不对称的,", "metrics": {"bleu_score": 53.87551338654778, "chrf_score": 40.62977341014794, "xcomet_score": 0.9811712503433228, "xcomet_qe_score": 0.961787223815918, "metricx_score": 0.628614068031311, "metricx_qe_score": 0.6044157147407532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.953220009803772, "xcomet_qe_score": 0.8956713080406189, "metricx_score": 0.23392783105373383, "metricx_qe_score": 0.3477631211280823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们挑出了一个并列成分。", "metrics": {"bleu_score": 53.90594848489677, "chrf_score": 47.38394961638935, "xcomet_score": 0.8346691131591797, "xcomet_qe_score": 0.811475932598114, "metricx_score": 2.9370360374450684, "metricx_qe_score": 3.442974805831909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在也有对协调结构采取对称方法,例如 PRUG 方法,", "metrics": {"bleu_score": 15.448759309616733, "chrf_score": 13.701190898395327, "xcomet_score": 0.6267708539962769, "xcomet_qe_score": 0.633350133895874, "metricx_score": 7.2504777908325195, "metricx_qe_score": 5.389471530914307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及 PRUG 依存关系树库中假设的由连接词引导的协调结构方法。", "metrics": {"bleu_score": 25.058104695444776, "chrf_score": 22.471350629495536, "xcomet_score": 0.45354753732681274, "xcomet_qe_score": 0.5343976020812988, "metricx_score": 6.170483589172363, "metricx_qe_score": 5.349351406097412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从终点得到所有合取式的依赖关系。", "metrics": {"bleu_score": 15.646327194763158, "chrf_score": 16.678003559290985, "xcomet_score": 0.7739338874816895, "xcomet_qe_score": 0.7675819396972656, "metricx_score": 4.5160908699035645, "metricx_qe_score": 3.5500810146331787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多头方法,例如 Dick Cutzman 的词语语法中就采用了这种方法。 可以说,所有连接词都是并列结构的主语。", "metrics": {"bleu_score": 23.26764658981386, "chrf_score": 20.16604453847974, "xcomet_score": 0.4892771542072296, "xcomet_qe_score": 0.5066530108451843, "metricx_score": 6.259069919586182, "metricx_qe_score": 6.458256721496582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从支配词(这里指“", "metrics": {"bleu_score": 34.111610352643645, "chrf_score": 28.71399093188101, "xcomet_score": 0.3372220993041992, "xcomet_qe_score": 0.15810233354568481, "metricx_score": 7.008909702301025, "metricx_qe_score": 4.392484188079834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "笑”)分别得到所有连接词的依赖关系。他们是巴特和玛姬。", "metrics": {"bleu_score": 3.796469564911484, "chrf_score": 3.9133193989337576, "xcomet_score": 0.1595207005739212, "xcomet_qe_score": 0.15601830184459686, "metricx_score": 11.894192695617676, "metricx_qe_score": 12.486790657043457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,本文的目的是为像这两个一样的协调对称结构提供一个新的论点,并反对像这两个一样的非对称协调结构。", "metrics": {"bleu_score": 41.03189284958194, "chrf_score": 36.91732351142909, "xcomet_score": 0.6430858373641968, "xcomet_qe_score": 0.6244459748268127, "metricx_score": 2.0906665325164795, "metricx_qe_score": 1.7829729318618774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9524842500686646, "xcomet_qe_score": 0.9396543502807617, "metricx_score": 0.2438656985759735, "metricx_qe_score": 0.30582886934280396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论点是基于依赖长度最小化原则的,我将通过这些例子来解释。", "metrics": {"bleu_score": 42.27329162760188, "chrf_score": 34.37252412438204, "xcomet_score": 0.90105801820755, "xcomet_qe_score": 0.8991734981536865, "metricx_score": 0.7066492438316345, "metricx_qe_score": 0.457020103931427, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在英语中,正如你可能知道的,我们的直接宾语倾向于靠近动词,而附属成分可能离得更远,", "metrics": {"bleu_score": 32.42989740661918, "chrf_score": 30.090129342655093, "xcomet_score": 0.811626672744751, "xcomet_qe_score": 0.7763115167617798, "metricx_score": 3.0570571422576904, "metricx_qe_score": 2.745151996612549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?所以 march read it yesterday 是正确的,因为直接宾语 it 靠近动词。 而昨天阅读的 March 情况要糟糕得多,", "metrics": {"bleu_score": 26.340465911330917, "chrf_score": 32.93688611149593, "xcomet_score": 0.4636150002479553, "xcomet_qe_score": 0.4775509536266327, "metricx_score": 12.199403762817383, "metricx_qe_score": 12.526885032653809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里动词和直接宾语之间有一个状语 yesterday。", "metrics": {"bleu_score": 53.439883964839176, "chrf_score": 59.021725416965275, "xcomet_score": 0.899756669998169, "xcomet_qe_score": 0.8021929264068604, "metricx_score": 2.603139877319336, "metricx_qe_score": 4.059497833251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接对象非常重且非常长时,这种效果可能会得到改善,因为", "metrics": {"bleu_score": 11.035569779632986, "chrf_score": 15.10356321999094, "xcomet_score": 0.6280995011329651, "xcomet_qe_score": 0.7257298231124878, "metricx_score": 5.549271583557129, "metricx_qe_score": 2.875502586364746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样可以直接对象移到边缘之后的位 置。", "metrics": {"bleu_score": 37.66019021279213, "chrf_score": 30.349196020156676, "xcomet_score": 0.7587785720825195, "xcomet_qe_score": 0.7590101957321167, "metricx_score": 5.315812110900879, "metricx_qe_score": 5.418776512145996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里对此进行了说明。", "metrics": {"bleu_score": 9.287528999566801, "chrf_score": 8.204869394996024, "xcomet_score": 0.8529562950134277, "xcomet_qe_score": 0.9032735228538513, "metricx_score": 0.7507290840148926, "metricx_qe_score": 0.6912389993667603, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这两句话都很好。", "metrics": {"bleu_score": 9.548450962056531, "chrf_score": 9.416750870102167, "xcomet_score": 0.9280425310134888, "xcomet_qe_score": 0.9024364352226257, "metricx_score": 0.39084896445274353, "metricx_qe_score": 0.5093563199043274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "昨天,三月读了一本关于BC的绝对迷人的", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17944371700286865, "xcomet_qe_score": 0.1836361587047577, "metricx_score": 10.400269508361816, "metricx_qe_score": 10.960444450378418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书,我很好,用这个长NP代替了它。 但也可以这么", "metrics": {"bleu_score": 13.804472103302821, "chrf_score": 12.801744107996274, "xcomet_score": 0.22332803905010223, "xcomet_qe_score": 0.13541670143604279, "metricx_score": 9.343476295471191, "metricx_qe_score": 8.039981842041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "说,玛姬昨天读了一本关于蜜蜂的非常有趣的书。 因此,这里的推理是,这是可能的,", "metrics": {"bleu_score": 1.5167145487126126, "chrf_score": 1.5527950310559007, "xcomet_score": 0.24754393100738525, "xcomet_qe_score": 0.18272468447685242, "metricx_score": 5.384043216705322, "metricx_qe_score": 5.7353291511535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为即使这个句子违反了直接宾语应该紧挨动词的一般语法原则。 它满足了依赖长度最小化原则,该原则主张优先使用较短的依赖关系。 因此,", "metrics": {"bleu_score": 46.120362225840715, "chrf_score": 37.776605707826654, "xcomet_score": 0.6680662631988525, "xcomet_qe_score": 0.7124722599983215, "metricx_score": 5.14171838760376, "metricx_qe_score": 4.44924783706665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示关键依赖项的长度,即在这两种结构中不保持不变的依赖项。", "metrics": {"bleu_score": 33.98642477980264, "chrf_score": 28.551835298281514, "xcomet_score": 0.9331555366516113, "xcomet_qe_score": 0.7643319368362427, "metricx_score": 2.4773426055908203, "metricx_qe_score": 2.8532180786132812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里我们有一个从阅读到长度为7(以词数计)的附属成分的依赖关系,以及从阅读到长度为4的书籍的依赖关系。所以两者加起来是11。", "metrics": {"bleu_score": 26.505518966632163, "chrf_score": 21.468568996359043, "xcomet_score": 0.5554990768432617, "xcomet_qe_score": 0.5914344191551208, "metricx_score": 4.249594211578369, "metricx_qe_score": 4.090131759643555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动时,当你交换这两个成分时,这两个依赖项的总和就变成了六,", "metrics": {"bleu_score": 34.76041118168438, "chrf_score": 32.21134578375324, "xcomet_score": 0.6435254812240601, "xcomet_qe_score": 0.5918223857879639, "metricx_score": 5.546903133392334, "metricx_qe_score": 6.102673530578613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?所以不是十一,而是六,要短得多。", "metrics": {"bleu_score": 18.951629567590746, "chrf_score": 20.9713546059191, "xcomet_score": 0.7125580310821533, "xcomet_qe_score": 0.76356440782547, "metricx_score": 3.9911866188049316, "metricx_qe_score": 3.7350916862487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来相当不错,", "metrics": {"bleu_score": 67.29864884660302, "chrf_score": 66.91997613073697, "xcomet_score": 0.9384526610374451, "xcomet_qe_score": 0.9192880988121033, "metricx_score": 0.5477246046066284, "metricx_qe_score": 0.5692055225372314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 63.8079654920782, "xcomet_score": 0.9115704298019409, "xcomet_qe_score": 0.963566780090332, "metricx_score": 0.5989556312561035, "metricx_qe_score": 0.9356817007064819, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9520323276519775, "xcomet_qe_score": 0.9441296458244324, "metricx_score": 0.25305208563804626, "metricx_qe_score": 0.31463509798049927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们做了什么?我们从Pentry Bank的增强版本中提取了各种关于协调的统计数据,并查看了为什么我们没有使用普遍依赖关系的论文。 这些统计数据证实了之前多次提出的观察结果,即左连接词往往较短,因此", "metrics": {"bleu_score": 32.00006116751895, "chrf_score": 29.389801519092657, "xcomet_score": 0.3694654107093811, "xcomet_qe_score": 0.35434675216674805, "metricx_score": 9.293252944946289, "metricx_qe_score": 8.181347846984863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用音节衡量时,盐和胡椒比胡椒和盐要短。 此外,人们", "metrics": {"bleu_score": 4.705244978975818, "chrf_score": 2.938058593415379, "xcomet_score": 0.32738447189331055, "xcomet_qe_score": 0.1613336205482483, "metricx_score": 11.817829132080078, "metricx_qe_score": 6.135202884674072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在研究中还发现,这种趋势随着长度差异的增大而增强。 因此", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 29.550223134592507, "xcomet_score": 0.7851555347442627, "xcomet_qe_score": 0.7190018892288208, "metricx_score": 3.3465323448181152, "metricx_qe_score": 1.9906386137008667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个连接体的长度差异增大时,较短的连接体更倾向于首先变强,对吗?", "metrics": {"bleu_score": 23.932668732389658, "chrf_score": 20.816406561639926, "xcomet_score": 0.7748499512672424, "xcomet_qe_score": 0.9452498555183411, "metricx_score": 6.100571632385254, "metricx_qe_score": 2.937891960144043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,左短连接体的比例更大。", "metrics": {"bleu_score": 38.73920998972052, "chrf_score": 32.74090484646162, "xcomet_score": 0.876032829284668, "xcomet_qe_score": 0.8711604475975037, "metricx_score": 3.3524277210235596, "metricx_qe_score": 3.672781467437744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种趋势只有在左侧的州政府缺席时才会发生。 所以", "metrics": {"bleu_score": 39.45122677504569, "chrf_score": 35.094304297944525, "xcomet_score": 0.6708117723464966, "xcomet_qe_score": 0.607408344745636, "metricx_score": 6.453831195831299, "metricx_qe_score": 5.034027576446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以这是州长,他在左边。", "metrics": {"bleu_score": 16.467029855845897, "chrf_score": 12.674288962193542, "xcomet_score": 0.6128528714179993, "xcomet_qe_score": 0.7770911455154419, "metricx_score": 5.071721076965332, "metricx_qe_score": 4.82048225402832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,它缺失了,荷马来了,打了个喷嚏。", "metrics": {"bleu_score": 20.828838183973037, "chrf_score": 11.685270202180831, "xcomet_score": 0.8020823001861572, "xcomet_qe_score": 0.7789674997329712, "metricx_score": 4.601666450500488, "metricx_qe_score": 4.87117338180542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们有两个动词的协调,没有外部的支配者,对吧?所以,", "metrics": {"bleu_score": 59.834338685808156, "chrf_score": 66.72616927472342, "xcomet_score": 0.8008116483688354, "xcomet_qe_score": 0.7389203310012817, "metricx_score": 4.775940418243408, "metricx_qe_score": 4.535466194152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左连接词倾向于更短。越是这样,两个连接词之间的差异就越大。", "metrics": {"bleu_score": 24.721090820433904, "chrf_score": 25.07423027985791, "xcomet_score": 0.8019787073135376, "xcomet_qe_score": 0.7710187435150146, "metricx_score": 3.138563871383667, "metricx_qe_score": 2.594527244567871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当右侧的治理,如此处,由左侧的协调公司 Telenet 负责时,这种效果就会消失。 因此,", "metrics": {"bleu_score": 15.286259905393742, "chrf_score": 13.607625421175126, "xcomet_score": 0.136500746011734, "xcomet_qe_score": 0.13338592648506165, "metricx_score": 14.457955360412598, "metricx_qe_score": 12.137927055358887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度,证明了这一点,这是音节的第一列,中间列,以及单词的右列。因此,我将", "metrics": {"bleu_score": 11.388946868920886, "chrf_score": 13.80191516381372, "xcomet_score": 0.4916175603866577, "xcomet_qe_score": 0.41574838757514954, "metricx_score": 9.300379753112793, "metricx_qe_score": 6.664581298828125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "专注于右列。", "metrics": {"bleu_score": 9.22364410103253, "chrf_score": 7.468281430219147, "xcomet_score": 0.8196547031402588, "xcomet_qe_score": 0.5604273080825806, "metricx_score": 2.1561026573181152, "metricx_qe_score": 3.4108331203460693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在这里看到的是,当州长在左边时。 左连接词趋向于变短的趋势随着词语之间的绝对差异而稳步增长,在没有主语的情况下(如句子协调)也会出现同样的现象,", "metrics": {"bleu_score": 23.89606406547965, "chrf_score": 20.83584924074308, "xcomet_score": 0.4197908937931061, "xcomet_qe_score": 0.47881001234054565, "metricx_score": 6.9796552658081055, "metricx_qe_score": 5.561011791229248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在主语位于右侧时,这种趋势就会消失。", "metrics": {"bleu_score": 49.18537211621337, "chrf_score": 43.87177144357334, "xcomet_score": 0.8901593089103699, "xcomet_qe_score": 0.7619940042495728, "metricx_score": 1.8968256711959839, "metricx_qe_score": 3.4292917251586914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这一点如何反驳了像这两个这样的不对称协调结构,以及像这两个这样的对称结构。 因此,请参阅论文", "metrics": {"bleu_score": 39.05385324416808, "chrf_score": 34.80744538738269, "xcomet_score": 0.49573224782943726, "xcomet_qe_score": 0.2481190711259842, "metricx_score": 2.8811540603637695, "metricx_qe_score": 3.635037899017334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以获取完整的协议和论点,", "metrics": {"bleu_score": 5.771298850643905, "chrf_score": 5.559735808042806, "xcomet_score": 0.43400296568870544, "xcomet_qe_score": 0.32952916622161865, "metricx_score": 9.061772346496582, "metricx_qe_score": 5.562601566314697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抱歉,并在会议结束后与我们讨论。", "metrics": {"bleu_score": 3.21858262703621, "chrf_score": 3.4722222222222223, "xcomet_score": 0.13920141756534576, "xcomet_qe_score": 0.13974785804748535, "metricx_score": 5.332762718200684, "metricx_qe_score": 4.166039943695068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是华盛顿大学的博士生向彬。", "metrics": {"bleu_score": 66.54377827941899, "chrf_score": 46.715408329754915, "xcomet_score": 0.8867446780204773, "xcomet_qe_score": 0.909195065498352, "metricx_score": 0.5595207214355469, "metricx_qe_score": 0.5962425470352173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天,我将介绍我们从预训练数据到语言模型再到下游任务的工作,追踪导致不公平自然语言处理模型的政治偏见线索。 因此", "metrics": {"bleu_score": 59.27769112224914, "chrf_score": 55.96672955550761, "xcomet_score": 0.662361741065979, "xcomet_qe_score": 0.5574333071708679, "metricx_score": 4.308887958526611, "metricx_qe_score": 1.997611403465271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是在大规模的网络爬虫数据上进行训练的。", "metrics": {"bleu_score": 79.47545184555567, "chrf_score": 83.83377023459488, "xcomet_score": 0.9833188056945801, "xcomet_qe_score": 0.9606128931045532, "metricx_score": 1.4932624101638794, "metricx_qe_score": 2.3271894454956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在预训练数据中得到充分覆盖。", "metrics": {"bleu_score": 66.60207336666174, "chrf_score": 62.989837747964685, "xcomet_score": 0.8190912008285522, "xcomet_qe_score": 0.7696527242660522, "metricx_score": 1.3865056037902832, "metricx_qe_score": 2.1511759757995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据 C four Corpus 的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到充分覆盖。", "metrics": {"bleu_score": 72.05582373159248, "chrf_score": 68.80411115074546, "xcomet_score": 0.7446421980857849, "xcomet_qe_score": 0.7084190249443054, "metricx_score": 3.4891910552978516, "metricx_qe_score": 3.4365198612213135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型的应用带来了既是机遇也是挑战的局面。 因此,", "metrics": {"bleu_score": 23.386786214190373, "chrf_score": 23.988542454652205, "xcomet_score": 0.5106209516525269, "xcomet_qe_score": 0.5005782246589661, "metricx_score": 3.8942954540252686, "metricx_qe_score": 1.369246006011963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从多元视角中学习,这体现了民主和思想多元性的价值。", "metrics": {"bleu_score": 31.443515194397026, "chrf_score": 27.0296665860536, "xcomet_score": 0.9710789918899536, "xcomet_qe_score": 0.9236767888069153, "metricx_score": 0.774991512298584, "metricx_qe_score": 0.6175504922866821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能会在下游任务应用中引发潜在的公平问题。", "metrics": {"bleu_score": 66.54756882996234, "chrf_score": 57.46559286406999, "xcomet_score": 0.9906831979751587, "xcomet_qe_score": 0.9751569032669067, "metricx_score": 0.9181106090545654, "metricx_qe_score": 1.0409619808197021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播流程,具体来说,通过以下几个问题来探讨这一问题。 首先,我们如何评估语言模型的政治倾向,预训练数据可能对这种政治偏见产生什么影响?", "metrics": {"bleu_score": 58.58760779378881, "chrf_score": 55.770332619375964, "xcomet_score": 0.9611220359802246, "xcomet_qe_score": 0.9512515068054199, "metricx_score": 1.0141360759735107, "metricx_qe_score": 1.313334584236145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治单元的语言模型在下游任务中的实际表现如何,以及这是否可能导致 NLP 应用中的公平性问题?", "metrics": {"bleu_score": 72.93142555264798, "chrf_score": 68.33540600272106, "xcomet_score": 0.8679640293121338, "xcomet_qe_score": 0.8032732009887695, "metricx_score": 3.1458067893981934, "metricx_qe_score": 3.0507354736328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们首先提议使用政治问卷(如政治指南针测试)以不同的提示格式提示语言模型。", "metrics": {"bleu_score": 37.9268395185128, "chrf_score": 30.96639297371475, "xcomet_score": 0.8256726264953613, "xcomet_qe_score": 0.7687501907348633, "metricx_score": 4.039525508880615, "metricx_qe_score": 4.397035121917725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们的自动评估能够很好地立足于政治科学文献。", "metrics": {"bleu_score": 36.032121811571, "chrf_score": 34.01832207402901, "xcomet_score": 0.9087281227111816, "xcomet_qe_score": 0.8885488510131836, "metricx_score": 1.3686269521713257, "metricx_qe_score": 1.631363034248352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,一些初步结果表明,第一语言模型确实具有不同的政治含义。", "metrics": {"bleu_score": 53.76962061407933, "chrf_score": 49.237388872832625, "xcomet_score": 0.8291174173355103, "xcomet_qe_score": 0.7831547260284424, "metricx_score": 3.863389492034912, "metricx_qe_score": 2.3718957901000977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治指南针上的四个象限。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 44.723889306558185, "xcomet_score": 0.8590556383132935, "xcomet_qe_score": 0.8305246829986572, "metricx_score": 2.5154099464416504, "metricx_qe_score": 2.432861089706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT 4 是所有语言模型中最自由的,GPT 系列通常比 BERT 系列及其变体在社会观念上更为自由。", "metrics": {"bleu_score": 55.95101218088824, "chrf_score": 53.349220847291775, "xcomet_score": 0.852067232131958, "xcomet_qe_score": 0.6917481422424316, "metricx_score": 1.4070451259613037, "metricx_qe_score": 1.815985083580017, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在研究语言模型的政治偏见在多大程度上实际上是从训练数据中获得的。", "metrics": {"bleu_score": 54.66747185912124, "chrf_score": 47.198981830592096, "xcomet_score": 0.9798702001571655, "xcomet_qe_score": 0.9712907075881958, "metricx_score": 0.739354133605957, "metricx_qe_score": 1.1560869216918945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过进一步对六个不同的党派公司进行预训练,将语言模型检查点分为新闻和社交媒体,并进一步将其政治意义分开,从而进行一项受控实验。", "metrics": {"bleu_score": 43.03534226742479, "chrf_score": 36.69819296609742, "xcomet_score": 0.7160483598709106, "xcomet_qe_score": 0.5895431637763977, "metricx_score": 6.771335601806641, "metricx_qe_score": 6.589704990386963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在语料库中对语言模型的这些部分进行进一步的预训练,我们可以看到,语言模型的意识形态坐标也会相应地发生变化。", "metrics": {"bleu_score": 61.92746132267477, "chrf_score": 53.114045796785014, "xcomet_score": 0.8883637189865112, "xcomet_qe_score": 0.8890533447265625, "metricx_score": 1.896686315536499, "metricx_qe_score": 2.706824779510498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于罗伯塔,进一步微调,进一步训练于左倾的 Reddit 语料库,我们可以看到其在...方面出现了显著的自由派转变。 就其政治偏见而言。", "metrics": {"bleu_score": 40.0701319722204, "chrf_score": 42.92317439290703, "xcomet_score": 0.30741459131240845, "xcomet_qe_score": 0.3892437219619751, "metricx_score": 6.830892086029053, "metricx_qe_score": 7.365939617156982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图研究语言模型是否能捕捉到我们现代社会普遍存在的极化现象。", "metrics": {"bleu_score": 51.11328901785518, "chrf_score": 42.393357438198926, "xcomet_score": 0.9082942605018616, "xcomet_qe_score": 0.9816766977310181, "metricx_score": 0.657209038734436, "metricx_qe_score": 0.8491129279136658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第45任总统当选前和当选后,我们", "metrics": {"bleu_score": 59.10904270166761, "chrf_score": 56.41512737720693, "xcomet_score": 0.7071124315261841, "xcomet_qe_score": 0.6294233202934265, "metricx_score": 6.132086753845215, "metricx_qe_score": 2.8730740547180176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别在两个不同的时间语料库上预训练语言模型。 我们", "metrics": {"bleu_score": 94.18009332674224, "chrf_score": 92.75185658480851, "xcomet_score": 0.5942639708518982, "xcomet_qe_score": 0.5504876375198364, "metricx_score": 4.399379730224609, "metricx_qe_score": 1.160313367843628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,2017年之后,语言模型的政治倾向普遍偏离了中心。", "metrics": {"bleu_score": 39.09290182000449, "chrf_score": 37.00490466782396, "xcomet_score": 0.9943575859069824, "xcomet_qe_score": 0.9879350662231445, "metricx_score": 0.9789310097694397, "metricx_qe_score": 1.2060668468475342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能捕捉到我们社会中的两极分化。", "metrics": {"bleu_score": 75.22135016840222, "chrf_score": 64.2088092431342, "xcomet_score": 0.9982771873474121, "xcomet_qe_score": 0.992790699005127, "metricx_score": 0.7769012451171875, "metricx_qe_score": 1.205171823501587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们在仇恨言论检测和虚假新闻检测方面对具有不同政治意义的语言模型进行了评估,这些应用通常涉及语言模型,并可能产生非常重大的影响。", "metrics": {"bleu_score": 47.223111977977666, "chrf_score": 42.17608511503023, "xcomet_score": 0.9104194641113281, "xcomet_qe_score": 0.9100281000137329, "metricx_score": 2.305478572845459, "metricx_qe_score": 2.9936094284057617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们发现,如果我们按类别调查绩效,也就是说,如果我们将绩效分开。 根据不同的人口统计数据或新闻媒体的政治意义,我们可以看到一个模式,", "metrics": {"bleu_score": 40.74394108372405, "chrf_score": 33.05340908611679, "xcomet_score": 0.628577470779419, "xcomet_qe_score": 0.4711478054523468, "metricx_score": 5.4867682456970215, "metricx_qe_score": 5.611416339874268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于仇恨言论检测,左翼语言模型表现更好。 在检测针对社会少数群体的仇恨言论方面。 然而,我们的工作重点是检测针对我们社会中更具影响力群体的仇恨言论。", "metrics": {"bleu_score": 52.602394656135196, "chrf_score": 48.898753554119864, "xcomet_score": 0.6921709775924683, "xcomet_qe_score": 0.7330714464187622, "metricx_score": 4.07904577255249, "metricx_qe_score": 4.097762107849121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,右倾语言模型在检测针对白人和男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+和其他少数族裔群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 70.06120228691815, "chrf_score": 71.69817063671111, "xcomet_score": 0.9834194183349609, "xcomet_qe_score": 0.9812736511230469, "metricx_score": 0.49549269676208496, "metricx_qe_score": 0.6554927825927734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在虚假新闻检测方面也存在类似的趋势,我们发现左翼语言模型在检测其对立的政治立场的信息失实方面表现更好,反之亦然。 这", "metrics": {"bleu_score": 26.91385930706138, "chrf_score": 23.02102498896032, "xcomet_score": 0.7999423742294312, "xcomet_qe_score": 0.8467981815338135, "metricx_score": 5.30591344833374, "metricx_qe_score": 2.0807747840881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将进一步展示许多定性例子,以了解具有不同政治含义的语言模型。 确实,根据其社会类别,对仇恨言论和虚假信息示例的预测有所不同。", "metrics": {"bleu_score": 52.20216555508117, "chrf_score": 42.34474994202249, "xcomet_score": 0.78084397315979, "xcomet_qe_score": 0.7351966500282288, "metricx_score": 3.20961594581604, "metricx_qe_score": 3.5379297733306885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有更多示例,以进一步强调这一点。 这表明,语言模型的政治偏见问题非常紧迫,需要公平解决。", "metrics": {"bleu_score": 41.81080040853914, "chrf_score": 35.31892297920453, "xcomet_score": 0.912443995475769, "xcomet_qe_score": 0.890751302242279, "metricx_score": 2.882174253463745, "metricx_qe_score": 2.196268081665039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个右翼语言模型被针对仇恨言论或虚假信息等进行微调,并部署到一个流行的社交媒体平台。 这意味着,持有相反政治观点的人可能会被边缘化,针对少数群体的仇恨言论可能会不受任何控制地肆意蔓延。", "metrics": {"bleu_score": 58.541112693246106, "chrf_score": 52.567428361879074, "xcomet_score": 0.9581986665725708, "xcomet_qe_score": 0.9504871368408203, "metricx_score": 1.332962989807129, "metricx_qe_score": 1.5106106996536255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,要求我们承认并解决语言模型政治化带来的公平问题。", "metrics": {"bleu_score": 32.36306245336183, "chrf_score": 33.181556131974766, "xcomet_score": 0.9986138343811035, "xcomet_qe_score": 0.9957777261734009, "metricx_score": 0.9277101755142212, "metricx_qe_score": 0.5346643924713135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们想稍微讨论一下。", "metrics": {"bleu_score": 33.260249505555045, "chrf_score": 32.42349794467388, "xcomet_score": 0.8066399097442627, "xcomet_qe_score": 0.8584543466567993, "metricx_score": 1.1138341426849365, "metricx_qe_score": 1.3084808588027954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还希望强调,我们揭露了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 53.96681359887849, "chrf_score": 50.337635528936964, "xcomet_score": 0.9087839722633362, "xcomet_qe_score": 0.9035114049911499, "metricx_score": 1.050887107849121, "metricx_qe_score": 1.8719152212142944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就像 Sila 和 Kryptidis 之间一样。", "metrics": {"bleu_score": 3.238940022565818, "chrf_score": 5.761605815298199, "xcomet_score": 0.3767085075378418, "xcomet_qe_score": 0.4960910975933075, "metricx_score": 9.437150955200195, "metricx_qe_score": 10.248188972473145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们在语言模型训练数据中不清理政治观点,偏见将从预训练数据传播到语言模型,进而影响下游任务,最终导致公平性问题。", "metrics": {"bleu_score": 60.15485570281404, "chrf_score": 52.29928249689826, "xcomet_score": 0.9816831350326538, "xcomet_qe_score": 0.9273825883865356, "metricx_score": 1.0117535591125488, "metricx_qe_score": 1.6669179201126099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们试图以某种方式进行清理,我们也会面临审查或排除的风险,", "metrics": {"bleu_score": 65.15366202853994, "chrf_score": 62.39203904741268, "xcomet_score": 0.7977620959281921, "xcomet_qe_score": 0.755106508731842, "metricx_score": 1.274085283279419, "metricx_qe_score": 1.3740061521530151, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定什么才是真正中立的,应该保留哪些语言模型训练数据。", "metrics": {"bleu_score": 14.51000005984178, "chrf_score": 17.497751021082138, "xcomet_score": 0.9044321775436401, "xcomet_qe_score": 0.9446061849594116, "metricx_score": 1.7443163394927979, "metricx_qe_score": 1.4559087753295898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这有点像“电查理”问题。", "metrics": {"bleu_score": 40.52587697205425, "chrf_score": 42.05248662029927, "xcomet_score": 0.7597862482070923, "xcomet_qe_score": 0.6880381107330322, "metricx_score": 4.484252452850342, "metricx_qe_score": 4.147037506103516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,很好。", "metrics": {"bleu_score": 17.965205598154213, "chrf_score": 40.69767441860465, "xcomet_score": 0.9799641370773315, "xcomet_qe_score": 0.9886821508407593, "metricx_score": 0.17090822756290436, "metricx_qe_score": 0.24480992555618286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这就是我今天要讲的全部了。", "metrics": {"bleu_score": 58.282339541526554, "chrf_score": 54.42343463004441, "xcomet_score": 0.9955207109451294, "xcomet_qe_score": 0.9851174354553223, "metricx_score": 0.2826388478279114, "metricx_qe_score": 0.3890303671360016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基梅隆大学的一名一年级博士生,今天我将介绍你们的作品《Enol Positional》,探讨模型Beta集中的设计偏差。", "metrics": {"bleu_score": 38.2981944511145, "chrf_score": 36.1523567196361, "xcomet_score": 0.5635244846343994, "xcomet_qe_score": 0.6499514579772949, "metricx_score": 7.69677209854126, "metricx_qe_score": 7.987827301025391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的,其中包括 Sebastian Santi、Ronin Lebras、Katarina Reinicke 和 Martin Sapp。", "metrics": {"bleu_score": 50.100510300424524, "chrf_score": 57.24131864397432, "xcomet_score": 0.8192726373672485, "xcomet_qe_score": 0.8518284559249878, "metricx_score": 1.9583649635314941, "metricx_qe_score": 2.067047119140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,让我们先想象一下,你正在为一家报纸工作,你正在筛选新闻文章下的评论,试图删除有毒内容。 ", "metrics": {"bleu_score": 48.6531571063768, "chrf_score": 44.696696518431914, "xcomet_score": 0.9080410003662109, "xcomet_qe_score": 0.9210500717163086, "metricx_score": 1.7763136625289917, "metricx_qe_score": 1.3981044292449951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能会转向像 Perspective API 这样的流行 API 来检测有毒性内容。如果你是 Carl Jones,这种方法真的很好用,", "metrics": {"bleu_score": 21.993422780474287, "chrf_score": 40.51451228043347, "xcomet_score": 0.6695797443389893, "xcomet_qe_score": 0.6280257105827332, "metricx_score": 3.6975016593933105, "metricx_qe_score": 4.0742292404174805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为 Perspective API 能够正确地检测出有毒的实例。", "metrics": {"bleu_score": 24.62395302527262, "chrf_score": 55.57687323361351, "xcomet_score": 0.778724193572998, "xcomet_qe_score": 0.6776962280273438, "metricx_score": 4.614487648010254, "metricx_qe_score": 4.418292045593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于 Dithyasharma 来说,情况并非如此,", "metrics": {"bleu_score": 45.07763457323718, "chrf_score": 42.767661082878476, "xcomet_score": 0.7493271827697754, "xcomet_qe_score": 0.7567490339279175, "metricx_score": 2.8442790508270264, "metricx_qe_score": 2.8092310428619385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为该视角 API 对在印度语境中更为常见的冒犯性用语并不敏感。", "metrics": {"bleu_score": 47.5305779341673, "chrf_score": 35.28879834188814, "xcomet_score": 0.7074908018112183, "xcomet_qe_score": 0.5441381335258484, "metricx_score": 4.115540981292725, "metricx_qe_score": 4.474958419799805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏差的例子,我们在此看到不同人群之间技术性能的系统性差异。", "metrics": {"bleu_score": 35.15787253565498, "chrf_score": 29.249494192762064, "xcomet_score": 0.9761266708374023, "xcomet_qe_score": 0.9105405807495117, "metricx_score": 0.7992537021636963, "metricx_qe_score": 1.209579348564148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们之前看到的这种设计偏见可能源于自然语言处理研究人员和模型开发人员的立场。", "metrics": {"bleu_score": 55.42963287421544, "chrf_score": 53.30146658373152, "xcomet_score": 0.9824978113174438, "xcomet_qe_score": 0.9303568005561829, "metricx_score": 0.9420276880264282, "metricx_qe_score": 0.8662136197090149, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场简单来说就是人们由于其人口统计、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 53.280669644815916, "chrf_score": 56.66264467859623, "xcomet_score": 0.890961766242981, "xcomet_qe_score": 0.8415008783340454, "metricx_score": 1.538632869720459, "metricx_qe_score": 1.9329098463058472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 60.90393051639867, "chrf_score": 54.74944642873282, "xcomet_score": 0.9953645467758179, "xcomet_qe_score": 0.9282628297805786, "metricx_score": 0.9410673975944519, "metricx_qe_score": 1.3600661754608154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,立场性会影响研究过程及其结果和结论,因为它会改变研究人员做出的决策。", "metrics": {"bleu_score": 56.31055087769951, "chrf_score": 50.07588953874199, "xcomet_score": 0.9865820407867432, "xcomet_qe_score": 0.9297225475311279, "metricx_score": 1.045494556427002, "metricx_qe_score": 1.025436282157898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问的一个问题是,数据集和模型是否有位置性?", "metrics": {"bleu_score": 53.41701392245994, "chrf_score": 49.11608877814345, "xcomet_score": 0.9061594605445862, "xcomet_qe_score": 0.9306572675704956, "metricx_score": 2.288924217224121, "metricx_qe_score": 0.9304611682891846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型本身和数据集本身具有人口统计特征和生活经历,而是它们汇集了真实的人们的判断和观点,因此可以代表某些立场优于其他立场。", "metrics": {"bleu_score": 48.59402862668478, "chrf_score": 41.40352516144352, "xcomet_score": 0.8807429075241089, "xcomet_qe_score": 0.9352378845214844, "metricx_score": 1.1938014030456543, "metricx_qe_score": 1.2369897365570068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,之前的研究提出了一些关于位置性的轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 40.47754548837428, "chrf_score": 31.814542620103147, "xcomet_score": 0.7647557258605957, "xcomet_qe_score": 0.675853431224823, "metricx_score": 5.584323883056641, "metricx_qe_score": 4.963082790374756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些工作实际上并没有将最终用户与数据集和模型本身进行比较。 随着 NLP 任务变得更加主观和社会化,研究模型和数据集的定位性变得越来越重要。 要确定这些立场是如何被扭曲的,非常具有挑战性,因为并非所有决策都有记录,而且许多模型都隐藏在 API 背后。", "metrics": {"bleu_score": 54.47300682933945, "chrf_score": 48.80598296737399, "xcomet_score": 0.7287712693214417, "xcomet_qe_score": 0.7392619252204895, "metricx_score": 3.0262022018432617, "metricx_qe_score": 3.25437593460083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了研究数据集和模型的定位性,我们实际上将注释与现有数据集和模型的真实用户进行了比较。", "metrics": {"bleu_score": 56.9561243853977, "chrf_score": 50.169398120017824, "xcomet_score": 0.7703904509544373, "xcomet_qe_score": 0.828010082244873, "metricx_score": 3.31715726852417, "metricx_qe_score": 3.4706733226776123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架 NL Positionality 来实现这一点。", "metrics": {"bleu_score": 30.26300230972924, "chrf_score": 62.11411554992817, "xcomet_score": 0.9059018492698669, "xcomet_qe_score": 0.878743052482605, "metricx_score": 0.6070845127105713, "metricx_qe_score": 0.8324410319328308, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是用不同的标注者重新标注数据集。", "metrics": {"bleu_score": 37.75584206975749, "chrf_score": 29.66578586391127, "xcomet_score": 0.87446129322052, "xcomet_qe_score": 0.8993077874183655, "metricx_score": 1.9985883235931396, "metricx_qe_score": 1.9477628469467163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是研究原始数据集、嗯,注释者的统计数据,因为通常只有少数注释者对每个实例进行注释,而且统计数据很少被收集和分享。", "metrics": {"bleu_score": 59.66140198879431, "chrf_score": 52.432298134457355, "xcomet_score": 0.7515493631362915, "xcomet_qe_score": 0.7085846662521362, "metricx_score": 4.877441883087158, "metricx_qe_score": 5.220658779144287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以便为每个实例获取多个标注,并获取一套丰富的社会人口数据。", "metrics": {"bleu_score": 21.892768146542714, "chrf_score": 23.213051887777176, "xcomet_score": 0.9409055709838867, "xcomet_qe_score": 0.9326554536819458, "metricx_score": 3.2094321250915527, "metricx_qe_score": 2.807556390762329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们按人口统计学特征对注释进行分类,并使用帕森斯 R 相关系数将它们与模型和数据集进行比较。 因此,我们的框架实际上与注释者分歧文献有所不同,它通过比较最终用户与模型和数据集、预测与标签,而不是仅仅关注注释者的一致性或注释者分布的建模,来实现这一目标。", "metrics": {"bleu_score": 50.72985624480918, "chrf_score": 49.09743445108325, "xcomet_score": 0.6066926717758179, "xcomet_qe_score": 0.5526440739631653, "metricx_score": 3.136993646621704, "metricx_qe_score": 3.5595319271087646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要通过“野外实验室”(Lab in the Wild)得以实现,这是我们 HCI 合作人员的一个在线众包平台。 而", "metrics": {"bleu_score": 35.65196948892425, "chrf_score": 57.334195314204315, "xcomet_score": 0.6684923768043518, "xcomet_qe_score": 0.5549324750900269, "metricx_score": 2.594712018966675, "metricx_qe_score": 1.990234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 则是一个在线实验平台,与 MTurk 等平台相比,我们可以在此招募到更多样化的志愿者,", "metrics": {"bleu_score": 37.5727980130706, "chrf_score": 52.34195262975432, "xcomet_score": 0.7950358390808105, "xcomet_qe_score": 0.5667258501052856, "metricx_score": 2.154256820678711, "metricx_qe_score": 4.136044025421143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而 MTurk 的参与者主要来自美国或印度。此外,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 57.94224186180097, "chrf_score": 63.229908242097885, "xcomet_score": 0.7926092743873596, "xcomet_qe_score": 0.804751455783844, "metricx_score": 1.8951971530914307, "metricx_qe_score": 1.4693735837936401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“野外实验室”中设置了两个任务,其中一个是社会可接受性。这个任务的工作方式是,参与者将阅读来自社会化学数据集中的一个情境,然后他们将写出这个情境在社会上是多么可接受。", "metrics": {"bleu_score": 39.69284075445912, "chrf_score": 32.96476338869472, "xcomet_score": 0.9468169212341309, "xcomet_qe_score": 0.9456948041915894, "metricx_score": 2.4829633235931396, "metricx_qe_score": 2.574506998062134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对研究的参与,他们可以将自己的回答与人工智能和其他人的回答进行比较。", "metrics": {"bleu_score": 58.40870102716207, "chrf_score": 55.10426116868336, "xcomet_score": 0.99479079246521, "xcomet_qe_score": 0.9920510053634644, "metricx_score": 0.9169657230377197, "metricx_qe_score": 0.9370037913322449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与社会化学、德尔菲和GPT 4进行了比较。", "metrics": {"bleu_score": 63.25279160728624, "chrf_score": 54.38853344257441, "xcomet_score": 0.8189742565155029, "xcomet_qe_score": 0.8007197976112366, "metricx_score": 1.6256943941116333, "metricx_qe_score": 2.279022216796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们为毒性与仇恨言论检测任务复制了一个非常相似的设置,参与者将阅读来自 Dana Hate 的实例,并写下他们是否认为这是一个仇恨言论的实例。", "metrics": {"bleu_score": 57.202228354736036, "chrf_score": 51.13445189332975, "xcomet_score": 0.6872494220733643, "xcomet_qe_score": 0.6833927631378174, "metricx_score": 4.477433204650879, "metricx_qe_score": 4.234730243682861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些标注与 DynaHate、Perspective API、Rewire API、HateRoberta 和 GPT four 进行比较。", "metrics": {"bleu_score": 41.084063449667774, "chrf_score": 74.72748048405104, "xcomet_score": 0.88813316822052, "xcomet_qe_score": 0.9151145815849304, "metricx_score": 3.380201578140259, "metricx_qe_score": 4.209986209869385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了来自 87 个国家的 1000 多名标注者的 16000 多条标注。", "metrics": {"bleu_score": 71.66644457140384, "chrf_score": 71.51920294027818, "xcomet_score": 0.8803567886352539, "xcomet_qe_score": 0.9641635417938232, "metricx_score": 1.8083374500274658, "metricx_qe_score": 1.1193331480026245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们现在更有能力回答自然语言处理数据集和模型最符合谁的需求。", "metrics": {"bleu_score": 38.862012568620365, "chrf_score": 35.798312643765286, "xcomet_score": 0.9149715304374695, "xcomet_qe_score": 0.8932250142097473, "metricx_score": 0.5248673558235168, "metricx_qe_score": 0.5602214336395264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现自然语言处理中存在位置性。", "metrics": {"bleu_score": 18.295654224495205, "chrf_score": 16.22650862888173, "xcomet_score": 0.8657286763191223, "xcomet_qe_score": 0.9100502133369446, "metricx_score": 3.6407344341278076, "metricx_qe_score": 1.8822135925292969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最符合英语国家的标准。因此", "metrics": {"bleu_score": 45.33710895095744, "chrf_score": 39.50138806100798, "xcomet_score": 0.8634291887283325, "xcomet_qe_score": 0.8341134190559387, "metricx_score": 2.3386693000793457, "metricx_qe_score": 1.5716533660888672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于 GPD 4 社会可接受性分析,我们发现它最符合儒家和英语国家的标准。", "metrics": {"bleu_score": 54.319380965500514, "chrf_score": 49.04931846121634, "xcomet_score": 0.7234265804290771, "xcomet_qe_score": 0.7424073219299316, "metricx_score": 4.2406792640686035, "metricx_qe_score": 5.195702075958252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 Dynamite Hate 也最符合英语国家的标准。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 67.15190595500388, "xcomet_score": 0.767304539680481, "xcomet_qe_score": 0.7455563545227051, "metricx_score": 3.3227198123931885, "metricx_qe_score": 4.232832908630371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,G", "metrics": {"bleu_score": 3.0608499802737392, "chrf_score": 11.185065103677683, "xcomet_score": 0.16890284419059753, "xcomet_qe_score": 0.15427768230438232, "metricx_score": 22.57382583618164, "metricx_qe_score": 16.323944091796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PT-4在社会可接受性任务中的表现与受过大学教育或研究生教育的人群最为一致。 我们发现 Dani Hate 也是如此,它最符合受过大学教育的人。", "metrics": {"bleu_score": 53.78982974471305, "chrf_score": 44.405169794429646, "xcomet_score": 0.7033419609069824, "xcomet_qe_score": 0.6051740646362305, "metricx_score": 6.134532928466797, "metricx_qe_score": 6.851292133331299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集针对特定人群进行调整时,一些人不可避免地会被抛在后面。 一个例子", "metrics": {"bleu_score": 31.910023371028153, "chrf_score": 30.377709675221332, "xcomet_score": 0.7364908456802368, "xcomet_qe_score": 0.7404413223266602, "metricx_score": 2.414736747741699, "metricx_qe_score": 1.5019941329956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,与男性和女性数据集相比,非二元人的数据集和模型对他们的描述不那么贴切。", "metrics": {"bleu_score": 35.92428930293907, "chrf_score": 34.70611840573816, "xcomet_score": 0.6825180053710938, "xcomet_qe_score": 0.7202557325363159, "metricx_score": 3.8039612770080566, "metricx_qe_score": 3.842398166656494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 GPT 4 社交可接受性任务以及 Dynahate 任务分析中都发现了这一点。", "metrics": {"bleu_score": 80.96427216101601, "chrf_score": 79.77113678352069, "xcomet_score": 0.941443681716919, "xcomet_qe_score": 0.9202333688735962, "metricx_score": 1.202294945716858, "metricx_qe_score": 1.7864353656768799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,既然存在位置分析碱基 LP,我们该怎么办呢?", "metrics": {"bleu_score": 6.964541799727335, "chrf_score": 12.374444945741221, "xcomet_score": 0.7346542477607727, "xcomet_qe_score": 0.7808352708816528, "metricx_score": 7.641419410705566, "metricx_qe_score": 7.2898664474487305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对此提出了一些建议。", "metrics": {"bleu_score": 31.53554052490131, "chrf_score": 33.008793029929855, "xcomet_score": 0.9676868915557861, "xcomet_qe_score": 0.9239437580108643, "metricx_score": 0.23016440868377686, "metricx_qe_score": 0.23169688880443573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,在整个研究过程中记录所有相关的设计选择。其次", "metrics": {"bleu_score": 50.406735961002234, "chrf_score": 41.279565844783235, "xcomet_score": 0.9651930332183838, "xcomet_qe_score": 0.9507927894592285, "metricx_score": 1.7049211263656616, "metricx_qe_score": 0.46416932344436646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从视角主义的角度进行 NLP 研究。", "metrics": {"bleu_score": 11.012455685471899, "chrf_score": 11.064329318579714, "xcomet_score": 0.810917854309082, "xcomet_qe_score": 0.7170666456222534, "metricx_score": 4.312621116638184, "metricx_qe_score": 4.367795944213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业数据集和模型。", "metrics": {"bleu_score": 77.38837367508762, "chrf_score": 69.36673661533045, "xcomet_score": 0.8714815378189087, "xcomet_qe_score": 0.8733584880828857, "metricx_score": 0.8281338214874268, "metricx_qe_score": 1.1102691888809204, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane计划。", "metrics": {"bleu_score": 50.51968359286048, "chrf_score": 41.302406935674156, "xcomet_score": 0.7493586540222168, "xcomet_qe_score": 0.7979084253311157, "metricx_score": 3.2502565383911133, "metricx_qe_score": 4.110206127166748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调,包容性NLP不仅仅是让所有", "metrics": {"bleu_score": 42.115124950749305, "chrf_score": 40.517930630683054, "xcomet_score": 0.626814603805542, "xcomet_qe_score": 0.5456753969192505, "metricx_score": 5.8720383644104, "metricx_qe_score": 4.886364459991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人服务。", "metrics": {"bleu_score": 39.03674453747003, "chrf_score": 36.76370111713883, "xcomet_score": 0.9485549926757812, "xcomet_qe_score": 0.9516949653625488, "metricx_score": 0.7394589781761169, "metricx_qe_score": 1.1236159801483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的演示到此结束,", "metrics": {"bleu_score": 52.53819788848316, "chrf_score": 37.53968253968254, "xcomet_score": 0.9809085130691528, "xcomet_qe_score": 0.9661180377006531, "metricx_score": 1.854804277420044, "metricx_qe_score": 1.6377358436584473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果您想了解更多信息,请随时查看我们的仪表板,获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 62.061988954713094, "chrf_score": 56.13070289899492, "xcomet_score": 0.9818592071533203, "xcomet_qe_score": 0.9660807847976685, "metricx_score": 0.6230459213256836, "metricx_qe_score": 0.5746880173683167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自费奈大学的Xi Yuan。", "metrics": {"bleu_score": 30.62049088236489, "chrf_score": 30.833038308403143, "xcomet_score": 0.7139227390289307, "xcomet_qe_score": 0.710627555847168, "metricx_score": 3.790882110595703, "metricx_qe_score": 2.683833360671997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天在这里介绍我们关于从行语言模型中提取独特脚本知识以进行约束语言规划的工作。", "metrics": {"bleu_score": 30.635452592534854, "chrf_score": 27.458879249634897, "xcomet_score": 0.699074387550354, "xcomet_qe_score": 0.6819037199020386, "metricx_score": 3.1556015014648438, "metricx_qe_score": 2.479811668395996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类通常通过遵循有保证的脚本形式的逐步指令来计划自己的行动。 ", "metrics": {"bleu_score": 30.57933319115584, "chrf_score": 25.223095847088228, "xcomet_score": 0.897946834564209, "xcomet_qe_score": 0.9253721833229065, "metricx_score": 3.9233453273773193, "metricx_qe_score": 4.249141693115234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究探讨了语言模型如何为诸如“烤蛋糕”等刻板印象活动的抽象目标进行", "metrics": {"bleu_score": 38.60973950960897, "chrf_score": 33.4325301585326, "xcomet_score": 0.6730659008026123, "xcomet_qe_score": 0.5590343475341797, "metricx_score": 5.194421291351318, "metricx_qe_score": 3.3855936527252197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "规划,并证明了大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 57.59804510288205, "chrf_score": 54.06218724256359, "xcomet_score": 0.5678426027297974, "xcomet_qe_score": 0.6442493200302124, "metricx_score": 4.024534225463867, "metricx_qe_score": 3.2146754264831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究主要集中在规划具有抽象目标的典型活动上。", "metrics": {"bleu_score": 51.60427447162422, "chrf_score": 48.19297130631664, "xcomet_score": 0.9747755527496338, "xcomet_qe_score": 0.9693928956985474, "metricx_score": 1.4585107564926147, "metricx_qe_score": 1.447472095489502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而对于具有具体目标、具体约束(如制作巧克力蛋糕)的目标的规划研究则仍然不足。", "metrics": {"bleu_score": 19.867275036696853, "chrf_score": 22.10295068475116, "xcomet_score": 0.9642878770828247, "xcomet_qe_score": 0.9615428447723389, "metricx_score": 1.2410430908203125, "metricx_qe_score": 1.6169264316558838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受限语言规划的问题。 这些约束对规划目标施加了不同的限制。", "metrics": {"bleu_score": 68.8222595430076, "chrf_score": 63.702728375095305, "xcomet_score": 0.9459012746810913, "xcomet_qe_score": 0.8741466999053955, "metricx_score": 1.2282657623291016, "metricx_qe_score": 1.7322959899902344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被具有多种约束的不同现实生活具体目标所", "metrics": {"bleu_score": 33.54297566381184, "chrf_score": 30.718597447781747, "xcomet_score": 0.7885380983352661, "xcomet_qe_score": 0.797329843044281, "metricx_score": 5.371919631958008, "metricx_qe_score": 4.01267147064209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "继承。一个好的规划者应该编写符合约束的合理脚本。", "metrics": {"bleu_score": 17.435142224312756, "chrf_score": 18.81476687810401, "xcomet_score": 0.5293389558792114, "xcomet_qe_score": 0.30798181891441345, "metricx_score": 5.263981342315674, "metricx_qe_score": 5.584836483001709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并提升了大语言模型的约束语言规划能力。", "metrics": {"bleu_score": 64.80785272594485, "chrf_score": 53.87082415243335, "xcomet_score": 0.8411085605621338, "xcomet_qe_score": 0.8048944473266602, "metricx_score": 0.9083527326583862, "metricx_qe_score": 0.901586651802063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定目标的数据集可以确定我们的起点。 我们首先要实现这个目标。", "metrics": {"bleu_score": 28.589164310355677, "chrf_score": 24.18797754280283, "xcomet_score": 0.7833670377731323, "xcomet_qe_score": 0.7882289886474609, "metricx_score": 5.346710681915283, "metricx_qe_score": 4.636036396026611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表中所示,我们使用结构化 TPT 扩展了抽象目标,以获取人类循环数据。", "metrics": {"bleu_score": 16.98333399082904, "chrf_score": 14.224391218065954, "xcomet_score": 0.6595257520675659, "xcomet_qe_score": 0.6450192928314209, "metricx_score": 8.071329116821289, "metricx_qe_score": 8.068359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随机选取 100 个具体目标,并对从大型模型生成的脚本进行评估。", "metrics": {"bleu_score": 40.212913260357446, "chrf_score": 40.74304186977954, "xcomet_score": 0.9460304975509644, "xcomet_qe_score": 0.9188977479934692, "metricx_score": 1.911797285079956, "metricx_qe_score": 2.893827438354492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有线性模型在规划特定目标方面都未取得令人满意的结果。", "metrics": {"bleu_score": 33.179455787876314, "chrf_score": 27.376624277047227, "xcomet_score": 0.8863621950149536, "xcomet_qe_score": 0.884179413318634, "metricx_score": 1.904536247253418, "metricx_qe_score": 2.407440185546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析,以研究学习模块的用途。", "metrics": {"bleu_score": 31.277600813200607, "chrf_score": 24.641905202605653, "xcomet_score": 0.7520027756690979, "xcomet_qe_score": 0.7513814568519592, "metricx_score": 4.949202537536621, "metricx_qe_score": 5.967452526092529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果表明,生成脚本的语义完整性是可以接受的,但无法保证对约束的忠实度。 根据回家", "metrics": {"bleu_score": 48.08518267605039, "chrf_score": 42.98379651012853, "xcomet_score": 0.719876766204834, "xcomet_qe_score": 0.7341844439506531, "metricx_score": 6.1058244705200195, "metricx_qe_score": 3.5360915660858154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "时间,我们将约束因素分为更坦诚的等级主题类别。", "metrics": {"bleu_score": 19.04623362251633, "chrf_score": 13.500974593255997, "xcomet_score": 0.34288597106933594, "xcomet_qe_score": 0.4498516321182251, "metricx_score": 11.269444465637207, "metricx_qe_score": 12.259404182434082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的主图显示,不同类别女孩的指令型DPD的规划表现差异很大。", "metrics": {"bleu_score": 31.668114032212166, "chrf_score": 21.004628584006383, "xcomet_score": 0.5469824075698853, "xcomet_qe_score": 0.29749536514282227, "metricx_score": 6.45454216003418, "metricx_qe_score": 6.898926734924316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前的研究表明,轻量模型的输出质量处于高方差状态,导致性能不佳。", "metrics": {"bleu_score": 38.32544339233122, "chrf_score": 34.56924551799428, "xcomet_score": 0.7641358971595764, "xcomet_qe_score": 0.7354369163513184, "metricx_score": 3.3690009117126465, "metricx_qe_score": 3.1658401489257812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过度生成的禅过滤器来提高生成质量。", "metrics": {"bleu_score": 44.13713504244446, "chrf_score": 36.82014090255779, "xcomet_score": 0.8185014724731445, "xcomet_qe_score": 0.7996259331703186, "metricx_score": 6.552168369293213, "metricx_qe_score": 7.217304229736328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先通过示例展示约束类型,以指导 CPT,并根据上述抽象目标获得具体目标。", "metrics": {"bleu_score": 57.139255930275176, "chrf_score": 41.00108880745577, "xcomet_score": 0.79759681224823, "xcomet_qe_score": 0.7534606456756592, "metricx_score": 3.7047293186187744, "metricx_qe_score": 4.187631130218506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后指示 GPT 生成针对特定目标的案例脚本。", "metrics": {"bleu_score": 15.936357366603362, "chrf_score": 15.386386093792579, "xcomet_score": 0.7439165115356445, "xcomet_qe_score": 0.7471671104431152, "metricx_score": 4.257129669189453, "metricx_qe_score": 4.196428298950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个筛选模型来选择不规则的脚本。", "metrics": {"bleu_score": 40.82292684498038, "chrf_score": 31.722986479595207, "xcomet_score": 0.7965842485427856, "xcomet_qe_score": 0.7668033242225647, "metricx_score": 3.6462528705596924, "metricx_qe_score": 3.6054811477661133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为指令,以便 GPT 以小块的形式理解,并计算余弦相似度和相似度分数,以衡量语义相似度。", "metrics": {"bleu_score": 54.9087447261332, "chrf_score": 48.77249671763318, "xcomet_score": 0.7618168592453003, "xcomet_qe_score": 0.6728086471557617, "metricx_score": 2.4611964225769043, "metricx_qe_score": 2.4421873092651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们将编写包含目标约束关键字的脚本。", "metrics": {"bleu_score": 41.37280342230792, "chrf_score": 36.627724141016074, "xcomet_score": 0.7887495756149292, "xcomet_qe_score": 0.7591727375984192, "metricx_score": 6.530531883239746, "metricx_qe_score": 7.062847137451172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果目标网站在目标站点上得分最高,我们才会保留该脚本。", "metrics": {"bleu_score": 36.51483716701108, "chrf_score": 31.739738275505307, "xcomet_score": 0.8035701513290405, "xcomet_qe_score": 0.7865703105926514, "metricx_score": 3.1274478435516357, "metricx_qe_score": 4.148581027984619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用我们的方法,不足之处可以生成高质量的螺丝。", "metrics": {"bleu_score": 36.852891195185464, "chrf_score": 21.24496632282877, "xcomet_score": 0.5661083459854126, "xcomet_qe_score": 0.5304709076881409, "metricx_score": 9.759608268737793, "metricx_qe_score": 11.704756736755371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实度方面大大提高了可规划性。", "metrics": {"bleu_score": 67.91782919911938, "chrf_score": 61.14481229700597, "xcomet_score": 0.8781508207321167, "xcomet_qe_score": 0.9125357866287231, "metricx_score": 1.3328583240509033, "metricx_qe_score": 1.7371227741241455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高昂,因此必须赋予小型和专业模型语言规划能力。", "metrics": {"bleu_score": 49.84796731920509, "chrf_score": 43.73877742546487, "xcomet_score": 0.9852979183197021, "xcomet_qe_score": 0.9667338728904724, "metricx_score": 0.4813358783721924, "metricx_qe_score": 0.5605736970901489, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键步骤。", "metrics": {"bleu_score": 69.6015973294402, "chrf_score": 66.30344838521414, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026699073612689972, "metricx_qe_score": 0.14870662987232208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究无法为特定目标进行规划,而且手动数据集标注成本高昂。", "metrics": {"bleu_score": 51.1649227291724, "chrf_score": 41.166934519545556, "xcomet_score": 0.9843240976333618, "xcomet_qe_score": 0.9624549150466919, "metricx_score": 1.1555638313293457, "metricx_qe_score": 1.734913945198059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的理念,从大型语言模型中提炼出受限的语言规划数据站点。", "metrics": {"bleu_score": 56.78141774467953, "chrf_score": 50.11301000583514, "xcomet_score": 0.7711973190307617, "xcomet_qe_score": 0.700195848941803, "metricx_score": 4.074038505554199, "metricx_qe_score": 4.396908283233643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将应用我们的方法来构建一个名为代码脚本的联合语言规划数据集。", "metrics": {"bleu_score": 29.75165028065122, "chrf_score": 23.313856777397312, "xcomet_score": 0.7698044776916504, "xcomet_qe_score": 0.7572922706604004, "metricx_score": 3.900850296020508, "metricx_qe_score": 4.376027584075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了五万五千个具体目标,", "metrics": {"bleu_score": 27.080524311589805, "chrf_score": 19.19281190814866, "xcomet_score": 0.9446789026260376, "xcomet_qe_score": 0.9233257174491882, "metricx_score": 2.115654230117798, "metricx_qe_score": 2.8832287788391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并编写了脚本以确保验证和测试网站的质量。我们要求云源工作人员查找并修改错误的样本。", "metrics": {"bleu_score": 52.98226497909894, "chrf_score": 51.29923943767012, "xcomet_score": 0.3584448993206024, "xcomet_qe_score": 0.15211570262908936, "metricx_score": 7.31892728805542, "metricx_qe_score": 7.114089488983154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了代码脚本的约束分布。", "metrics": {"bleu_score": 49.202745153855076, "chrf_score": 29.64490504867994, "xcomet_score": 0.8700239658355713, "xcomet_qe_score": 0.8391302227973938, "metricx_score": 3.129685640335083, "metricx_qe_score": 4.138612270355225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成的特定目标中表现出超倍性。通过代", "metrics": {"bleu_score": 35.38049406567624, "chrf_score": 23.62538428833601, "xcomet_score": 0.5068358182907104, "xcomet_qe_score": 0.5006705522537231, "metricx_score": 7.93703556060791, "metricx_qe_score": 6.410194396972656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "码脚本,我们可以追踪更小但更专业的模型,用于约束语言规划。", "metrics": {"bleu_score": 26.66090188234886, "chrf_score": 17.862039898960468, "xcomet_score": 0.4485035836696625, "xcomet_qe_score": 0.4078197777271271, "metricx_score": 7.98646354675293, "metricx_qe_score": 8.339975357055664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用 antsights、TFILF 和调整光标率,可以生成比大多数大型语言模型更高质量的脚本,这表明在适当的数据集上进行适当训练的小型模型可以支持大型模型。", "metrics": {"bleu_score": 45.32351463266532, "chrf_score": 35.96686507173595, "xcomet_score": 0.21360869705677032, "xcomet_qe_score": 0.21976308524608612, "metricx_score": 8.799243927001953, "metricx_qe_score": 9.731732368469238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们确定了约束语言规划问题。", "metrics": {"bleu_score": 36.02381155044161, "chrf_score": 31.655489371543133, "xcomet_score": 0.8744391202926636, "xcomet_qe_score": 0.8496617078781128, "metricx_score": 1.830448865890503, "metricx_qe_score": 2.286728858947754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的约束语言规划能力,并为大型语言模型开发了生成过滤器方法。", "metrics": {"bleu_score": 45.95209042964994, "chrf_score": 38.52693290579274, "xcomet_score": 0.7568233013153076, "xcomet_qe_score": 0.7614231109619141, "metricx_score": 4.327305793762207, "metricx_qe_score": 3.7520694732666016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成高质量的脚本数据集,用于约束语言规划。", "metrics": {"bleu_score": 61.03988734332589, "chrf_score": 42.50672947153218, "xcomet_score": 0.9081760048866272, "xcomet_qe_score": 0.8781784176826477, "metricx_score": 2.6895062923431396, "metricx_qe_score": 3.432945966720581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望代码数据集能成为推动语言规划研究的宝贵资源。", "metrics": {"bleu_score": 74.32254773174189, "chrf_score": 54.51910355014581, "xcomet_score": 0.8833423256874084, "xcomet_qe_score": 0.8988118171691895, "metricx_score": 2.817470073699951, "metricx_qe_score": 3.4360764026641846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查看代码脚本的更多详细信息。", "metrics": {"bleu_score": 62.55340042200862, "chrf_score": 44.49895406414207, "xcomet_score": 0.8446244597434998, "xcomet_qe_score": 0.8269893527030945, "metricx_score": 2.5433461666107178, "metricx_qe_score": 2.760967493057251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫舒恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8853719234466553, "xcomet_qe_score": 0.8503116369247437, "metricx_score": 0.10186628997325897, "metricx_qe_score": 0.4453405439853668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的论文《2003年的核技术命名实体标注器在2023年还能否正常工作?》", "metrics": {"bleu_score": 46.56455050518961, "chrf_score": 46.32858142709771, "xcomet_score": 0.7772979736328125, "xcomet_qe_score": 0.7635370492935181, "metricx_score": 4.298367023468018, "metricx_qe_score": 4.155490875244141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.9978755712509155, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了泛化问题,使用了命名实体识别任务,或称为 NER 任务。", "metrics": {"bleu_score": 50.871023027172114, "chrf_score": 45.36876836475371, "xcomet_score": 0.8926786184310913, "xcomet_qe_score": 0.8924659490585327, "metricx_score": 2.0144059658050537, "metricx_qe_score": 2.7030489444732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,近二十年来,模型一直在使用Kono 2003来开发命名实体识别。这自然引发了几个问题。", "metrics": {"bleu_score": 19.321663915752147, "chrf_score": 19.79590249393646, "xcomet_score": 0.6969882249832153, "xcomet_qe_score": 0.716452956199646, "metricx_score": 6.920779228210449, "metricx_qe_score": 6.37836217880249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据?", "metrics": {"bleu_score": 53.12583871630397, "chrf_score": 42.12696617108382, "xcomet_score": 0.9173398017883301, "xcomet_qe_score": 0.9163376688957214, "metricx_score": 0.46578866243362427, "metricx_qe_score": 0.3853972554206848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新的标记器时,为了实现良好的泛化能力,需要什么条件?", "metrics": {"bleu_score": 40.588153399233, "chrf_score": 35.327090198845276, "xcomet_score": 0.996279239654541, "xcomet_qe_score": 0.9976413249969482, "metricx_score": 0.5133451223373413, "metricx_qe_score": 0.33327382802963257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们确实观察到泛化能力差,那么这些模型的性能下降是由什么原因造成的呢?", "metrics": {"bleu_score": 40.38098413802772, "chrf_score": 38.496112727192525, "xcomet_score": 0.9976638555526733, "xcomet_qe_score": 0.9893614053726196, "metricx_score": 0.6913033127784729, "metricx_qe_score": 0.802233099937439, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了Kono plus plus数据集。", "metrics": {"bleu_score": 42.34885228074744, "chrf_score": 26.10731637406236, "xcomet_score": 0.8059941530227661, "xcomet_qe_score": 0.8018000721931458, "metricx_score": 5.354517936706543, "metricx_qe_score": 5.935080051422119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们从2020年路透社新闻中收集的数据集,然后根据相同的Kono 2003注释准则对它们进行了注释。", "metrics": {"bleu_score": 56.54183256093958, "chrf_score": 51.01061105384508, "xcomet_score": 0.8591183423995972, "xcomet_qe_score": 0.8031219244003296, "metricx_score": 4.367314338684082, "metricx_qe_score": 3.9885525703430176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在Kono two thousand three上对二十多个模型进行了微调。", "metrics": {"bleu_score": 44.338389108446, "chrf_score": 29.89495115440704, "xcomet_score": 0.6907999515533447, "xcomet_qe_score": 0.7626919150352478, "metricx_score": 7.099215984344482, "metricx_qe_score": 7.053272247314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Kono three测试集和Kono plus测试集上对它们进行了评估。", "metrics": {"bleu_score": 53.5327611100933, "chrf_score": 33.67088617787691, "xcomet_score": 0.5925149917602539, "xcomet_qe_score": 0.6416304707527161, "metricx_score": 6.713066101074219, "metricx_qe_score": 5.548327445983887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了 F one 的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 59.435931692508895, "chrf_score": 54.79210680655069, "xcomet_score": 0.9684270620346069, "xcomet_qe_score": 0.9643638730049133, "metricx_score": 2.3951988220214844, "metricx_qe_score": 2.7217414379119873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的泛化能力需要什么条件呢?", "metrics": {"bleu_score": 37.42031646082126, "chrf_score": 30.124680717222834, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.1679101586341858, "metricx_qe_score": 0.18634864687919617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现需要三个主要条件。", "metrics": {"bleu_score": 46.087110150246495, "chrf_score": 42.23430747046477, "xcomet_score": 0.9339300394058228, "xcomet_qe_score": 0.9039961695671082, "metricx_score": 1.615716576576233, "metricx_qe_score": 3.0777015686035156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是模型架构。", "metrics": {"bleu_score": 60.042877124855906, "chrf_score": 51.821001027418625, "xcomet_score": 0.9962185621261597, "xcomet_qe_score": 0.9754199981689453, "metricx_score": 0.04136792570352554, "metricx_qe_score": 0.07232436537742615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现 Transformer 模型通常能更好地推广到新数据。", "metrics": {"bleu_score": 50.949472008875915, "chrf_score": 59.70531812734149, "xcomet_score": 0.8084381818771362, "xcomet_qe_score": 0.7700134515762329, "metricx_score": 3.1043787002563477, "metricx_qe_score": 4.589556694030762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常情况下,模型越大,泛化能力越强。", "metrics": {"bleu_score": 16.133948681475328, "chrf_score": 16.244220530681023, "xcomet_score": 0.9969384670257568, "xcomet_qe_score": 0.9846937656402588, "metricx_score": 0.34818410873413086, "metricx_qe_score": 0.5096091032028198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们都知道微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 51.2691819618139, "chrf_score": 59.876557108622194, "xcomet_score": 0.9417649507522583, "xcomet_qe_score": 0.9170168042182922, "metricx_score": 3.607921600341797, "metricx_qe_score": 2.110914707183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现更多的微调示例实际上也能带来更好的泛化能力。", "metrics": {"bleu_score": 65.01148795387888, "chrf_score": 55.16026110229008, "xcomet_score": 0.9911754131317139, "xcomet_qe_score": 0.9230656027793884, "metricx_score": 0.576622724533081, "metricx_qe_score": 0.9240424633026123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来我们想问,是什么原因导致某些模型的性能下降? 我们有两个假设。", "metrics": {"bleu_score": 45.55428852315212, "chrf_score": 40.677191892531134, "xcomet_score": 0.9751149415969849, "xcomet_qe_score": 0.968338131904602, "metricx_score": 1.012662410736084, "metricx_qe_score": 0.8256786465644836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,即由于反复使用相同的测试集而导致的过拟合,这通常表现为在新测试集上的回报递减。", "metrics": {"bleu_score": 63.020750298314056, "chrf_score": 56.15222765881658, "xcomet_score": 0.9046900272369385, "xcomet_qe_score": 0.8857103586196899, "metricx_score": 2.8005406856536865, "metricx_qe_score": 3.0428924560546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间差距越来越大而导致的性能下降。", "metrics": {"bleu_score": 59.75281862052228, "chrf_score": 55.53728510769358, "xcomet_score": 0.964687705039978, "xcomet_qe_score": 0.888350784778595, "metricx_score": 1.4822864532470703, "metricx_qe_score": 2.0078001022338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表中看到,红色最佳拟合线的梯度大于1。", "metrics": {"bleu_score": 52.73680880348243, "chrf_score": 49.25256889307443, "xcomet_score": 0.8772281408309937, "xcomet_qe_score": 0.8086820840835571, "metricx_score": 1.1732186079025269, "metricx_qe_score": 1.507735252380371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在 Carl 2003 上每改进一个单位,Carl Plus Plus 的改进就超过一个单位,这意味着没有收益递减。", "metrics": {"bleu_score": 32.55985128443668, "chrf_score": 28.095931418321246, "xcomet_score": 0.6675885319709778, "xcomet_qe_score": 0.6421521902084351, "metricx_score": 10.76733112335205, "metricx_qe_score": 11.115387916564941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下没有观察到自适应过拟合。", "metrics": {"bleu_score": 74.93731939490364, "chrf_score": 69.43707675795987, "xcomet_score": 0.9009255766868591, "xcomet_qe_score": 0.9129918217658997, "metricx_score": 1.1392955780029297, "metricx_qe_score": 1.6724114418029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,暂时的漂移呢?", "metrics": {"bleu_score": 20.556680845025987, "chrf_score": 19.282239556629797, "xcomet_score": 0.8869993686676025, "xcomet_qe_score": 0.8560041785240173, "metricx_score": 1.2902073860168457, "metricx_qe_score": 2.072275400161743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,使用更新的数据对一些模型进行重新训练或继续预训练,我们发现随着时间间隔的增大,性能会下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 66.04247141578722, "chrf_score": 59.07397829435662, "xcomet_score": 0.9637844562530518, "xcomet_qe_score": 0.9189162850379944, "metricx_score": 1.552622675895691, "metricx_qe_score": 1.7021842002868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型规模以及更多的微调示例,", "metrics": {"bleu_score": 82.50398234595713, "chrf_score": 80.53312978411796, "xcomet_score": 0.974774181842804, "xcomet_qe_score": 0.9374690651893616, "metricx_score": 0.9352855086326599, "metricx_qe_score": 1.3300361633300781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素是相辅相成的。我们不能只保留其中一个因素,而抛弃其他因素。", "metrics": {"bleu_score": 60.46554378641561, "chrf_score": 54.395903667052096, "xcomet_score": 0.9978052377700806, "xcomet_qe_score": 0.9896818399429321, "metricx_score": 0.22317500412464142, "metricx_qe_score": 0.4101305603981018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,这里的性能下降是由时间漂移引起的,令人惊讶的是,它不是由自适应过拟合引起的,尽管Kono 2003已经使用了二十多年。", "metrics": {"bleu_score": 56.92223220844652, "chrf_score": 48.4910713061101, "xcomet_score": 0.666496217250824, "xcomet_qe_score": 0.637639045715332, "metricx_score": 4.666274070739746, "metricx_qe_score": 4.922807216644287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,回到我们在论文标题中提出的问题,Kono 2003 标签器在 2023 年是否仍然有效?", "metrics": {"bleu_score": 68.04439980111333, "chrf_score": 60.54866861411791, "xcomet_score": 0.5958234071731567, "xcomet_qe_score": 0.7723777294158936, "metricx_score": 4.9404425621032715, "metricx_qe_score": 5.259442329406738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是肯定的。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 53.5017446130673, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43837279081344604, "metricx_qe_score": 0.7667475342750549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能促使人们进一步研究如何改进模型的泛化能力。", "metrics": {"bleu_score": 43.24055278038292, "chrf_score": 37.57915343009893, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3048090636730194, "metricx_qe_score": 0.4421778917312622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果您有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.11026448209409, "chrf_score": 52.5325928587273, "xcomet_score": 0.9871149063110352, "xcomet_qe_score": 0.9712950587272644, "metricx_score": 0.2757876515388489, "metricx_qe_score": 0.26652368903160095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9670989513397217, "xcomet_qe_score": 0.9718614816665649, "metricx_score": 0.2643663287162781, "metricx_qe_score": 0.26394033432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将谈谈我们在解决实体选择中的间接指称表达方面的工作,我们在此过程中引入了altentity scorpus。 我的名", "metrics": {"bleu_score": 31.40202994706468, "chrf_score": 27.757493100606865, "xcomet_score": 0.43837282061576843, "xcomet_qe_score": 0.4484122395515442, "metricx_score": 11.809759140014648, "metricx_qe_score": 10.786416053771973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "字是贾沃德·霍赛尼,这是我和菲利普·拉金斯基、西尔维亚·帕雷蒂和安妮·路易斯合作完成的作品。", "metrics": {"bleu_score": 4.308842033868296, "chrf_score": 3.5168128587819867, "xcomet_score": 0.6997616291046143, "xcomet_qe_score": 0.6258091330528259, "metricx_score": 6.087741374969482, "metricx_qe_score": 6.148196220397949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时所使用的语言。", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 62.467977525345795, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4846040606498718, "metricx_qe_score": 0.6751763820648193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑以下替代问题。", "metrics": {"bleu_score": 19.969395881889398, "chrf_score": 16.322199135289424, "xcomet_score": 0.8779173493385315, "xcomet_qe_score": 0.8629541993141174, "metricx_score": 0.5324400067329407, "metricx_qe_score": 0.27518725395202637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说“easy on me”还是“I got a feeling”?在这", "metrics": {"bleu_score": 10.975762213309226, "chrf_score": 28.48558372822843, "xcomet_score": 0.8722114562988281, "xcomet_qe_score": 0.8332648277282715, "metricx_score": 4.367887496948242, "metricx_qe_score": 4.640275478363037, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里,用户想要在这两个标志中进行选择。", "metrics": {"bleu_score": 13.673412644075363, "chrf_score": 14.277140356724919, "xcomet_score": 0.5428164005279541, "xcomet_qe_score": 0.604069709777832, "metricx_score": 6.9123334884643555, "metricx_qe_score": 7.429135322570801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是直接引用,例如说出歌曲名称《Easy on Me》或它的位置,第一首。", "metrics": {"bleu_score": 43.300849065349134, "chrf_score": 44.32087832322039, "xcomet_score": 0.8124129176139832, "xcomet_qe_score": 0.7357685565948486, "metricx_score": 3.0287489891052246, "metricx_qe_score": 4.120269775390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但有时,间接引用更合适,可以使对话更加", "metrics": {"bleu_score": 11.161992055193059, "chrf_score": 13.19711632801554, "xcomet_score": 0.7327101230621338, "xcomet_qe_score": 0.8215942978858948, "metricx_score": 6.363541126251221, "metricx_qe_score": 2.602557420730591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然。这种情况可能发生在用户记不起资料来源名称时。", "metrics": {"bleu_score": 17.640520319198952, "chrf_score": 17.70903199088111, "xcomet_score": 0.6643625497817993, "xcomet_qe_score": 0.589762806892395, "metricx_score": 3.6602327823638916, "metricx_qe_score": 4.230958938598633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有发音都太相似,难以区分。", "metrics": {"bleu_score": 22.792296655031148, "chrf_score": 20.697981473308467, "xcomet_score": 0.8570217490196228, "xcomet_qe_score": 0.8698610067367554, "metricx_score": 2.685293674468994, "metricx_qe_score": 0.6818498373031616, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定偏好时。", "metrics": {"bleu_score": 23.093053192812558, "chrf_score": 23.39542938424004, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6289462447166443, "metricx_qe_score": 0.45695963501930237, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些直接引用示例,例如较新的歌曲或不太活跃的歌曲。", "metrics": {"bleu_score": 15.835705372951134, "chrf_score": 14.781464883080462, "xcomet_score": 0.7210383415222168, "xcomet_qe_score": 0.7097351551055908, "metricx_score": 6.075876235961914, "metricx_qe_score": 5.987899303436279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是用于基准测试大型语言模型实体理解能力的问题。", "metrics": {"bleu_score": 51.24542972844807, "chrf_score": 44.4647804167928, "xcomet_score": 0.8618596792221069, "xcomet_qe_score": 0.7992798089981079, "metricx_score": 3.2510833740234375, "metricx_qe_score": 2.3706235885620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有发现一个针对该任务的大型公共数据集,因此我们使用众包标注方式收集了一个数据集。", "metrics": {"bleu_score": 47.957600807198475, "chrf_score": 41.81464956845521, "xcomet_score": 0.8239904642105103, "xcomet_qe_score": 0.8041688203811646, "metricx_score": 1.442017674446106, "metricx_qe_score": 1.467763066291809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域,分别是音乐、书籍和研究。", "metrics": {"bleu_score": 59.4659877990507, "chrf_score": 57.274366953482705, "xcomet_score": 0.885047435760498, "xcomet_qe_score": 0.8742493987083435, "metricx_score": 2.3824291229248047, "metricx_qe_score": 3.012113332748413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通人物完成集。", "metrics": {"bleu_score": 60.1067427964331, "chrf_score": 52.13865536259368, "xcomet_score": 0.8156169652938843, "xcomet_qe_score": 0.8109356760978699, "metricx_score": 4.1382269859313965, "metricx_qe_score": 4.203125476837158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这幅漫画有三个对话气泡。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 75.16242688801647, "xcomet_score": 0.8998875617980957, "xcomet_qe_score": 0.8733921647071838, "metricx_score": 0.8555368781089783, "metricx_qe_score": 0.929405927658081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡里,鲍勃说:“还记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 58.217473175544946, "chrf_score": 52.09244807795531, "xcomet_score": 0.9765057563781738, "xcomet_qe_score": 0.9003608226776123, "metricx_score": 1.2223185300827026, "metricx_qe_score": 1.5910086631774902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后鲍勃就设置了对话的背景。", "metrics": {"bleu_score": 41.62799023755188, "chrf_score": 32.34587601437742, "xcomet_score": 0.955114483833313, "xcomet_qe_score": 0.9835449457168579, "metricx_score": 1.535586953163147, "metricx_qe_score": 1.261259913444519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话气泡中,爱丽丝说,你是说对我手下留情,还是我得到了满足? 这是", "metrics": {"bleu_score": 13.097321315250039, "chrf_score": 9.661338673190881, "xcomet_score": 0.3785270154476166, "xcomet_qe_score": 0.2738838791847229, "metricx_score": 8.37432861328125, "metricx_qe_score": 5.998165130615234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "备选问题。", "metrics": {"bleu_score": 19.199242796476852, "chrf_score": 18.55461038727971, "xcomet_score": 0.8314038515090942, "xcomet_qe_score": 0.8765708208084106, "metricx_score": 1.09012770652771, "metricx_qe_score": 1.2819992303848267, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用间接引用来选择其中一个实体,例如新的 RF。", "metrics": {"bleu_score": 38.91030091713277, "chrf_score": 31.358684578086343, "xcomet_score": 0.6646786332130432, "xcomet_qe_score": 0.6392362117767334, "metricx_score": 6.270451545715332, "metricx_qe_score": 5.580127239227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一和第二个语音气泡,但第三个由注释者填写。", "metrics": {"bleu_score": 48.57308570940184, "chrf_score": 39.9318040076293, "xcomet_score": 0.9019738435745239, "xcomet_qe_score": 0.8493963479995728, "metricx_score": 1.7426731586456299, "metricx_qe_score": 1.8181675672531128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个语音气泡是从每个领域的一些手动提示中选出的。", "metrics": {"bleu_score": 36.23885503140913, "chrf_score": 30.528453986273142, "xcomet_score": 0.813565731048584, "xcomet_qe_score": 0.7460943460464478, "metricx_score": 2.8652963638305664, "metricx_qe_score": 2.601848602294922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个问题是备选问题,生成方式如下。", "metrics": {"bleu_score": 12.512236921161914, "chrf_score": 13.932659938671922, "xcomet_score": 0.9060136079788208, "xcomet_qe_score": 0.9135813117027283, "metricx_score": 0.3856356739997864, "metricx_qe_score": 0.4006219208240509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是来自维基百科的样本。", "metrics": {"bleu_score": 86.11735299633672, "chrf_score": 96.57574311968287, "xcomet_score": 0.9713319540023804, "xcomet_qe_score": 0.9262405037879944, "metricx_score": 0.7253305912017822, "metricx_qe_score": 0.857016384601593, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用过的不同采样方法。", "metrics": {"bleu_score": 61.28081331864041, "chrf_score": 52.92624054700117, "xcomet_score": 0.9960860013961792, "xcomet_qe_score": 0.998734712600708, "metricx_score": 0.15488818287849426, "metricx_qe_score": 0.2388148009777069, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体变得更加相似,通常更难进行消歧。", "metrics": {"bleu_score": 58.98466143484528, "chrf_score": 51.36728888469803, "xcomet_score": 0.8547371625900269, "xcomet_qe_score": 0.7941489219665527, "metricx_score": 4.7627668380737305, "metricx_qe_score": 5.6383466720581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀吸引力。", "metrics": {"bleu_score": 16.058516370438436, "chrf_score": 13.011476708899517, "xcomet_score": 0.8054068684577942, "xcomet_qe_score": 0.7826197147369385, "metricx_score": 3.283067226409912, "metricx_qe_score": 2.1671273708343506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个情况是实体名称相似,例如两本书都叫《零售》。", "metrics": {"bleu_score": 20.485079606183252, "chrf_score": 16.55671268943578, "xcomet_score": 0.7401742935180664, "xcomet_qe_score": 0.7305872440338135, "metricx_score": 3.683673620223999, "metricx_qe_score": 3.943769693374634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是,它们在维基百科上的描述相似。", "metrics": {"bleu_score": 65.47599888066033, "chrf_score": 55.47856569528397, "xcomet_score": 0.9969536066055298, "xcomet_qe_score": 0.9877551794052124, "metricx_score": 0.3662336766719818, "metricx_qe_score": 0.5076944231987, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当它们在维基百科上有相似的信息框或属性时。", "metrics": {"bleu_score": 69.63845241054851, "chrf_score": 64.76296666996701, "xcomet_score": 0.9417118430137634, "xcomet_qe_score": 0.997989296913147, "metricx_score": 0.9182050228118896, "metricx_qe_score": 1.1083542108535767, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,相同的类型或相同的艺术家声音。", "metrics": {"bleu_score": 24.202875575621302, "chrf_score": 25.811212075689348, "xcomet_score": 0.773105263710022, "xcomet_qe_score": 0.7453576326370239, "metricx_score": 2.656937599182129, "metricx_qe_score": 3.4166109561920166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向注释者展示这个问题的另一种表述时,他们知道这些实体的名称,但并不一定了解实体本身。", "metrics": {"bleu_score": 50.54904637028962, "chrf_score": 48.032751786122, "xcomet_score": 0.97664475440979, "xcomet_qe_score": 0.9797818660736084, "metricx_score": 1.6067235469818115, "metricx_qe_score": 1.7753238677978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们的做法是展示这两个实体的一些背景知识。", "metrics": {"bleu_score": 61.29574979328211, "chrf_score": 58.45251923670799, "xcomet_score": 0.9705458879470825, "xcomet_qe_score": 0.8331559896469116, "metricx_score": 1.0227710008621216, "metricx_qe_score": 1.5545063018798828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只需为每首歌曲提供一个谷歌搜索链接。 然后请注释员至少收听每首歌曲的部分内容,并阅读每首歌曲的介绍。", "metrics": {"bleu_score": 32.809387946197226, "chrf_score": 29.050997761916054, "xcomet_score": 0.9440914392471313, "xcomet_qe_score": 0.8971079587936401, "metricx_score": 1.0769095420837402, "metricx_qe_score": 1.3415470123291016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这是谷歌搜索歌曲《Easy》的结果。", "metrics": {"bleu_score": 23.677591070891456, "chrf_score": 25.625295092284446, "xcomet_score": 0.8398979902267456, "xcomet_qe_score": 0.783923864364624, "metricx_score": 3.7283530235290527, "metricx_qe_score": 3.9363982677459717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们从维基百科展示一些背景文本。", "metrics": {"bleu_score": 59.26487606292854, "chrf_score": 51.436354022614836, "xcomet_score": 0.9749809503555298, "xcomet_qe_score": 0.8663438558578491, "metricx_score": 0.889976978302002, "metricx_qe_score": 1.2329232692718506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科再次展示它们的图片,以便注释者了解它们的外观。", "metrics": {"bleu_score": 45.98110255507438, "chrf_score": 36.61805658156588, "xcomet_score": 0.896891176700592, "xcomet_qe_score": 0.8881486058235168, "metricx_score": 1.3211899995803833, "metricx_qe_score": 1.675130844116211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求注释者选择其中一个实体,例如这里的第一个实体,并使用三到五个间接指称表达来描述它们。", "metrics": {"bleu_score": 53.113745397242596, "chrf_score": 46.53064150790948, "xcomet_score": 0.8598448038101196, "xcomet_qe_score": 0.9080283641815186, "metricx_score": 2.0972492694854736, "metricx_qe_score": 1.9210611581802368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,钢琴音乐的那首。", "metrics": {"bleu_score": 11.731175160263996, "chrf_score": 12.42424242424242, "xcomet_score": 0.9431931972503662, "xcomet_qe_score": 0.8839001655578613, "metricx_score": 2.748188018798828, "metricx_qe_score": 1.9585423469543457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的几个例子。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9644975662231445, "xcomet_qe_score": 0.8701311945915222, "metricx_score": 0.5822314620018005, "metricx_qe_score": 1.956078290939331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有歌词的那首,不是12岁男孩演唱的那首,也不是虚构的那首,或者来自阿塞拜疆的那首。", "metrics": {"bleu_score": 22.514833265247994, "chrf_score": 22.12571402047746, "xcomet_score": 0.95038902759552, "xcomet_qe_score": 0.9005294442176819, "metricx_score": 1.9027289152145386, "metricx_qe_score": 1.8095200061798096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "身份语料库包含三个领域的 6,000 个备选问题,以及 42,000 个间接指称表达。", "metrics": {"bleu_score": 39.41622338155528, "chrf_score": 38.77681469198044, "xcomet_score": 0.623257040977478, "xcomet_qe_score": 0.5331710577011108, "metricx_score": 3.4502999782562256, "metricx_qe_score": 2.809037923812866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是使用 T5xLarge 模型的结果总结。", "metrics": {"bleu_score": 33.36118221483977, "chrf_score": 30.906902188243947, "xcomet_score": 0.9519596695899963, "xcomet_qe_score": 0.9258688688278198, "metricx_score": 1.7326300144195557, "metricx_qe_score": 2.0168933868408203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与注释者完全相同的背景知识,那么准确率就会非常高。大约在 92% 到 95% 之间。", "metrics": {"bleu_score": 88.47064105457828, "chrf_score": 84.3111982542258, "xcomet_score": 0.9747552871704102, "xcomet_qe_score": 0.9631763696670532, "metricx_score": 0.8064987659454346, "metricx_qe_score": 0.8242776393890381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确率在 82% 到 87% 之间,这更现实,", "metrics": {"bleu_score": 76.76100071606257, "chrf_score": 74.92904845166187, "xcomet_score": 0.86504727602005, "xcomet_qe_score": 0.8647122383117676, "metricx_score": 1.8689501285552979, "metricx_qe_score": 1.9130514860153198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率只有 60%。所以还有很大的改进空间。", "metrics": {"bleu_score": 76.9218097681464, "chrf_score": 72.93605951078253, "xcomet_score": 0.9966810941696167, "xcomet_qe_score": 0.9928364753723145, "metricx_score": 1.5012954473495483, "metricx_qe_score": 2.6159017086029053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还表明,这些模型具有领域通用性。", "metrics": {"bleu_score": 72.76817202342096, "chrf_score": 67.51641468553233, "xcomet_score": 0.9773801565170288, "xcomet_qe_score": 0.9063783288002014, "metricx_score": 0.5223034620285034, "metricx_qe_score": 0.6212144494056702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集链接。", "metrics": {"bleu_score": 29.50234363196404, "chrf_score": 30.119895842275447, "xcomet_score": 0.9908864498138428, "xcomet_qe_score": 0.9903539419174194, "metricx_score": 0.23959046602249146, "metricx_qe_score": 0.38847288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自特伦托大学和布鲁诺·凯斯勒基金会的莎拉·帕皮,我将简要介绍一篇关于“注意力作为同步语音翻译的指导”的论文,这是我和马泰奥·内格里、马可·图尔奇的合作成果。", "metrics": {"bleu_score": 41.3581273839215, "chrf_score": 34.212363720526305, "xcomet_score": 0.7825779914855957, "xcomet_qe_score": 0.7365564107894897, "metricx_score": 2.9131579399108887, "metricx_qe_score": 2.7436277866363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同声语音翻译?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9885314702987671, "xcomet_qe_score": 0.9054443836212158, "metricx_score": 0.16075709462165833, "metricx_qe_score": 0.07203303277492523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同声语音翻译(simul SD)是指将口语实时翻译成另一种语言的文本的过程,从而实现跨语言交流。 那么,", "metrics": {"bleu_score": 57.441372595261164, "chrf_score": 56.72863800360876, "xcomet_score": 0.6550273299217224, "xcomet_qe_score": 0.6984339952468872, "metricx_score": 6.597177505493164, "metricx_qe_score": 6.259627819061279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前 SimulST 模型存在哪些问题呢?", "metrics": {"bleu_score": 31.170906522700683, "chrf_score": 53.21827842302002, "xcomet_score": 0.9988645315170288, "xcomet_qe_score": 0.9957503080368042, "metricx_score": 0.335801362991333, "metricx_qe_score": 0.4385472536087036, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常情况下,在训练特定架构时,会引入额外的模块进行优化。", "metrics": {"bleu_score": 52.87179050550917, "chrf_score": 49.84853437617562, "xcomet_score": 0.9869217872619629, "xcomet_qe_score": 0.9796297550201416, "metricx_score": 0.5859463214874268, "metricx_qe_score": 1.0781803131103516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,涉及不同优化目标的训练过程", "metrics": {"bleu_score": 49.45378690183769, "chrf_score": 50.10450623628368, "xcomet_score": 0.8224443793296814, "xcomet_qe_score": 0.6969528198242188, "metricx_score": 3.1351871490478516, "metricx_qe_score": 4.49100399017334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既冗长又复杂 并且训练和维护多个模型以达到不同的延迟", "metrics": {"bleu_score": 50.35450322123768, "chrf_score": 44.861449741557024, "xcomet_score": 0.7714227437973022, "xcomet_qe_score": 0.42352747917175293, "metricx_score": 3.7117919921875, "metricx_qe_score": 3.486307382583618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水平,例如训练一个平均延迟为一秒的模型,另一个平均延迟为两秒的模型,等等。", "metrics": {"bleu_score": 59.28257592469543, "chrf_score": 56.04760496795795, "xcomet_score": 0.6235519051551819, "xcomet_qe_score": 0.5576198101043701, "metricx_score": 2.981308698654175, "metricx_qe_score": 3.434751510620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么我们的解决方案是什么呢?", "metrics": {"bleu_score": 91.93227152249175, "chrf_score": 91.10491360491362, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.015326432883739471, "metricx_qe_score": 0.16972288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用已经存在的离线 SD 模型,无需重新训练或采用特定的 CLSD 架构。", "metrics": {"bleu_score": 57.11635209517662, "chrf_score": 45.565231151918795, "xcomet_score": 0.7403943538665771, "xcomet_qe_score": 0.7291054725646973, "metricx_score": 6.470492362976074, "metricx_qe_score": 5.927410125732422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每个延迟方案,只使用一个模型,并通过特定参数来处理延迟。", "metrics": {"bleu_score": 61.809829588356614, "chrf_score": 52.75025612956647, "xcomet_score": 0.9996132850646973, "xcomet_qe_score": 0.9892100095748901, "metricx_score": 0.5841017961502075, "metricx_qe_score": 0.7089189291000366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并通过音频输入和文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 60.60046478391066, "chrf_score": 60.4154345705964, "xcomet_score": 0.7925113439559937, "xcomet_qe_score": 0.6658550500869751, "metricx_score": 5.002397537231445, "metricx_qe_score": 4.8261895179748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")利用模型已经获得的知识。右边可以看到一个例子。", "metrics": {"bleu_score": 4.92467473444436, "chrf_score": 6.754417382999043, "xcomet_score": 0.4164741039276123, "xcomet_qe_score": 0.3726116418838501, "metricx_score": 4.819536209106445, "metricx_qe_score": 6.47175931930542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出一个点或编码器解码器注意力模型,这是一个策略,根据注意力的指向,我们决定是否发出部分翻译。", "metrics": {"bleu_score": 43.812589114264064, "chrf_score": 35.9309632967018, "xcomet_score": 0.6417588591575623, "xcomet_qe_score": 0.6629490852355957, "metricx_score": 6.56488561630249, "metricx_qe_score": 6.833189010620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果张力不集中,即其总和低于某个阈值alpha,则发出一个词,指向最后的lambda语音帧,意味着接收到的信息足够稳定。", "metrics": {"bleu_score": 44.33570765875765, "chrf_score": 35.54856615334829, "xcomet_score": 0.5117447376251221, "xcomet_qe_score": 0.5443893671035767, "metricx_score": 5.873575687408447, "metricx_qe_score": 5.7118821144104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们收到一个包含“我要谈论”的语音片段,我们的模型预测德语翻译为: 我们还会看一下交叉注意力权重。 我们会发现,前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,即最后一个 lambda 语音帧。", "metrics": {"bleu_score": 46.418737731126626, "chrf_score": 35.07646849044969, "xcomet_score": 0.48506349325180054, "xcomet_qe_score": 0.48449867963790894, "metricx_score": 4.803227424621582, "metricx_qe_score": 4.761688232421875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被发出。 然而,由于交叉张力的总和高于某个阈值 alpha,我们不会发出最后一个词,而是等待另一个语音块。", "metrics": {"bleu_score": 53.71038484190211, "chrf_score": 44.827082681048196, "xcomet_score": 0.7327890396118164, "xcomet_qe_score": 0.7566310167312622, "metricx_score": 5.938625812530518, "metricx_qe_score": 5.874815464019775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续进行,接收到另一个被沉没的语音,我们的模型预测出其他三个词,我们会查看交叉注意力权重。 我们会发现,没有任何词语指向最后的 lambda 语音帧。", "metrics": {"bleu_score": 41.4060462756169, "chrf_score": 36.12132631253664, "xcomet_score": 0.42893415689468384, "xcomet_qe_score": 0.3562667667865753, "metricx_score": 6.756364345550537, "metricx_qe_score": 7.001195907592773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将被发出。", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 27.259129759129756, "xcomet_score": 0.9332944750785828, "xcomet_qe_score": 0.8744902610778809, "metricx_score": 1.6925324201583862, "metricx_qe_score": 3.3203859329223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们看一下其主要结果,我们会发现 我们将把同声传译结果绘制在图表上,其中一边用蓝色表示,用来衡量翻译质量和平均滞后。 这就是延迟度量。我们还考虑了计算感知平均滞后,它考虑了模型预测输出的计算时间。", "metrics": {"bleu_score": 32.64749562048611, "chrf_score": 26.35592232952434, "xcomet_score": 0.6971438527107239, "xcomet_qe_score": 0.7177085876464844, "metricx_score": 6.318258285522461, "metricx_qe_score": 6.049591541290283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望我们的曲线在这个图上尽可能高。", "metrics": {"bleu_score": 37.21106799318148, "chrf_score": 34.07518000736071, "xcomet_score": 0.9693130254745483, "xcomet_qe_score": 0.85105299949646, "metricx_score": 1.4359958171844482, "metricx_qe_score": 1.8472847938537598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但我们也希望它们向左移动。", "metrics": {"bleu_score": 86.17038791239612, "chrf_score": 84.90244110859445, "xcomet_score": 0.9971116781234741, "xcomet_qe_score": 0.9812257289886475, "metricx_score": 0.6350057721138, "metricx_qe_score": 1.0446958541870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将这些策略与 PROPERA 策略进行了比较,这些策略也适用于离线模型,即 WitKey 策略和本地协议。", "metrics": {"bleu_score": 28.761604244499782, "chrf_score": 22.50800374233098, "xcomet_score": 0.6958802938461304, "xcomet_qe_score": 0.5222153067588806, "metricx_score": 5.916003227233887, "metricx_qe_score": 6.320984363555908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将这些策略与专门为同时预翻译设计的最先进架构进行了比较。", "metrics": {"bleu_score": 45.162500316207456, "chrf_score": 44.64526538861817, "xcomet_score": 0.8662114143371582, "xcomet_qe_score": 0.9002735018730164, "metricx_score": 1.8335983753204346, "metricx_qe_score": 1.7537494897842407, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是德语同声传译策略的所有结果。", "metrics": {"bleu_score": 71.97795533614756, "chrf_score": 62.44064078622902, "xcomet_score": 0.8703422546386719, "xcomet_qe_score": 0.8403339982032776, "metricx_score": 1.0494918823242188, "metricx_qe_score": 1.5705538988113403, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,ADUT 的表现优于所有应用于离线模型的策略,因为它们的曲线向左移动。", "metrics": {"bleu_score": 47.69388850463063, "chrf_score": 46.109095054348735, "xcomet_score": 0.8985230326652527, "xcomet_qe_score": 0.7588565945625305, "metricx_score": 3.5549607276916504, "metricx_qe_score": 4.688676834106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际的经过时间或计算感知时间,那么这是最快的策略。", "metrics": {"bleu_score": 43.17533205435251, "chrf_score": 40.58336703933594, "xcomet_score": 0.777758002281189, "xcomet_qe_score": 0.8129640817642212, "metricx_score": 4.138158321380615, "metricx_qe_score": 4.285491466522217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想发现更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 65.14613449066714, "chrf_score": 54.128468638917546, "xcomet_score": 0.9677166938781738, "xcomet_qe_score": 0.9603763222694397, "metricx_score": 0.6387813091278076, "metricx_qe_score": 0.45578014850616455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源了代码和模型,并进行了同时输出,以促进我们工作的可重复性。", "metrics": {"bleu_score": 8.905296375949616, "chrf_score": 15.72936089070351, "xcomet_score": 0.8511388301849365, "xcomet_qe_score": 0.8107262253761292, "metricx_score": 1.0274863243103027, "metricx_qe_score": 1.2666866779327393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Ying,我的同事Jiang和我将介绍我们关于通过指令调整改进多模态序列学习的多指令研究。 因此", "metrics": {"bleu_score": 50.86108266519091, "chrf_score": 37.967926416359596, "xcomet_score": 0.5165183544158936, "xcomet_qe_score": 0.5028271675109863, "metricx_score": 6.567521572113037, "metricx_qe_score": 5.040350914001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",随着大型语言模型的进步,许多研究开始探索新的学习范式,以参数和数据高效的方式将预训练语言模型重用于不同的下游任务。", "metrics": {"bleu_score": 74.44150759500691, "chrf_score": 65.47936952694423, "xcomet_score": 0.8788344860076904, "xcomet_qe_score": 0.8451521396636963, "metricx_score": 2.69541072845459, "metricx_qe_score": 3.3829829692840576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,通过遵循自然指令,指令微调使大型语言模型能够以零样本方式执行未见过的任务。", "metrics": {"bleu_score": 58.012207362191646, "chrf_score": 50.900333505573705, "xcomet_score": 0.819495677947998, "xcomet_qe_score": 0.7538726329803467, "metricx_score": 1.8708562850952148, "metricx_qe_score": 3.607451915740967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数关于指令调优的先前工作都集中在提高仅语言任务的连续拍摄性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 36.779146042024955, "chrf_score": 33.4884621287574, "xcomet_score": 0.8131226301193237, "xcomet_qe_score": 0.7221477031707764, "metricx_score": 5.575023651123047, "metricx_qe_score": 5.898003578186035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想要研究多模态预训练模型的指令微调是否真的能提高对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 40.24793913874828, "chrf_score": 34.369737174738965, "xcomet_score": 0.8913238048553467, "xcomet_qe_score": 0.8282774686813354, "metricx_score": 1.5758507251739502, "metricx_qe_score": 1.7023435831069946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现 RLP 和多模态之间的教学数据集可用性存在显著差异。 ", "metrics": {"bleu_score": 32.334582496025675, "chrf_score": 26.936370714663017, "xcomet_score": 0.8455407619476318, "xcomet_qe_score": 0.7959219217300415, "metricx_score": 2.614185333251953, "metricx_qe_score": 1.9618180990219116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过一千六百个单语言指令任务。", "metrics": {"bleu_score": 28.175950490399515, "chrf_score": 23.050258564964444, "xcomet_score": 0.966288149356842, "xcomet_qe_score": 0.8650813102722168, "metricx_score": 1.0126218795776367, "metricx_qe_score": 1.1473664045333862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,目前尚无大规模的多模态指令任务公开可用。", "metrics": {"bleu_score": 41.261520349079454, "chrf_score": 40.153446172185845, "xcomet_score": 0.9708065986633301, "xcomet_qe_score": 0.8797028064727783, "metricx_score": 2.3796322345733643, "metricx_qe_score": 2.728731155395508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这促使我们构建了一个多模态指令微调数据集。", "metrics": {"bleu_score": 47.8219647449668, "chrf_score": 39.87088787580623, "xcomet_score": 0.9769026041030884, "xcomet_qe_score": 0.9700860977172852, "metricx_score": 1.3343170881271362, "metricx_qe_score": 1.1246306896209717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们介绍了 Multi Instruct,这是第一个多模态指令微调基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个广泛类别。", "metrics": {"bleu_score": 43.12704335231426, "chrf_score": 46.908758935591365, "xcomet_score": 0.734728217124939, "xcomet_qe_score": 0.7148533463478088, "metricx_score": 1.86178457736969, "metricx_qe_score": 1.9940924644470215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务源自二十一个现有的开源数据集,每个任务都配有五个专家撰写的指导说明。", "metrics": {"bleu_score": 49.16245161848607, "chrf_score": 44.2447935895551, "xcomet_score": 0.963272213935852, "xcomet_qe_score": 0.9719585180282593, "metricx_score": 1.0912601947784424, "metricx_qe_score": 1.0322308540344238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究我们提出的数据集上的多模态指令调整,我们以OFA统一的多模态模式模型作为我们的基础模型。OFA使用统一的词", "metrics": {"bleu_score": 47.63742712392626, "chrf_score": 44.67928203437182, "xcomet_score": 0.6884123086929321, "xcomet_qe_score": 0.49339759349823, "metricx_score": 7.214191436767578, "metricx_qe_score": 4.652606010437012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来表示语言、图像标记和边界框的坐标。", "metrics": {"bleu_score": 40.142754193450884, "chrf_score": 31.737148627860805, "xcomet_score": 0.3397049307823181, "xcomet_qe_score": 0.3453698754310608, "metricx_score": 6.806940078735352, "metricx_qe_score": 6.82180118560791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示了我们多层次数据集中的几个示例。 统一各种输入和输出数据类型的处理。", "metrics": {"bleu_score": 43.0892538160363, "chrf_score": 30.02374056395474, "xcomet_score": 0.7969061136245728, "xcomet_qe_score": 0.7584052085876465, "metricx_score": 4.107397556304932, "metricx_qe_score": 3.823296308517456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,将所有任务统一编排为序列格式,其中", "metrics": {"bleu_score": 43.60178496858878, "chrf_score": 40.76465619478386, "xcomet_score": 0.7292037010192871, "xcomet_qe_score": 0.7452882528305054, "metricx_score": 3.9513020515441895, "metricx_qe_score": 1.7502609491348267, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框以相同的标记空间表示。", "metrics": {"bleu_score": 53.09565039223721, "chrf_score": 50.52264517853785, "xcomet_score": 0.9847351312637329, "xcomet_qe_score": 0.9566234350204468, "metricx_score": 1.0149985551834106, "metricx_qe_score": 1.017142415046692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我要谈谈多模态指令调优。 因此", "metrics": {"bleu_score": 64.59962562244407, "chrf_score": 67.16344466083987, "xcomet_score": 0.7737843990325928, "xcomet_qe_score": 0.7165536880493164, "metricx_score": 3.1175618171691895, "metricx_qe_score": 0.7612370848655701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于训练数据集,我们使用了 NIG 集团的 53 个任务进行训练,每个任务抽取 10,000 个样本。", "metrics": {"bleu_score": 53.11464359384744, "chrf_score": 49.34825130011402, "xcomet_score": 0.6844587922096252, "xcomet_qe_score": 0.6881837844848633, "metricx_score": 6.700352191925049, "metricx_qe_score": 9.306337356567383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留了整个常识推理组进行测试,并从 WQA 和杂项组中额外选择了五个任务。", "metrics": {"bleu_score": 52.73988056675799, "chrf_score": 46.25754260564513, "xcomet_score": 0.6072986125946045, "xcomet_qe_score": 0.6100850105285645, "metricx_score": 4.777985572814941, "metricx_qe_score": 4.8845624923706055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用测试集中的所有实例进行每个任务的测试。", "metrics": {"bleu_score": 51.94247346787362, "chrf_score": 42.91118133156373, "xcomet_score": 0.8430489301681519, "xcomet_qe_score": 0.8401150107383728, "metricx_score": 0.9858503341674805, "metricx_qe_score": 1.1448758840560913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试集(如NLP语法)中随机抽取20个任务。 因此,我们使用一个", "metrics": {"bleu_score": 46.323150199855995, "chrf_score": 46.21669482779268, "xcomet_score": 0.5169380903244019, "xcomet_qe_score": 0.4989638030529022, "metricx_score": 7.659241676330566, "metricx_qe_score": 5.360851287841797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练的 OFA 大模型作为基础模型。", "metrics": {"bleu_score": 62.779326306649864, "chrf_score": 63.70333111975559, "xcomet_score": 0.8530336618423462, "xcomet_qe_score": 0.8461225032806396, "metricx_score": 1.7963168621063232, "metricx_qe_score": 2.8455445766448975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 53.884478712336644, "xcomet_score": 0.969638466835022, "xcomet_qe_score": 0.8911672830581665, "metricx_score": 0.8351970911026001, "metricx_qe_score": 1.3651412725448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例都随机与其中的五个指令模板中的一个结合。", "metrics": {"bleu_score": 55.9998185378911, "chrf_score": 56.32269935352346, "xcomet_score": 0.8881491422653198, "xcomet_qe_score": 0.7880314588546753, "metricx_score": 1.7375136613845825, "metricx_qe_score": 2.0605101585388184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在每个任务的测试中,我们总共进行五次实验,每次实验中都使用五条指令中的其中", "metrics": {"bleu_score": 32.76840052255025, "chrf_score": 28.16531853910108, "xcomet_score": 0.8355108499526978, "xcomet_qe_score": 0.8237683773040771, "metricx_score": 1.83705735206604, "metricx_qe_score": 1.4618935585021973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一条来评估模型。 我们报告了所有五个实验的平均性能、最大性能和性能标准差。", "metrics": {"bleu_score": 17.864866390812015, "chrf_score": 15.50696095149093, "xcomet_score": 0.07840406894683838, "xcomet_qe_score": 0.17271201312541962, "metricx_score": 6.719484329223633, "metricx_qe_score": 7.476612091064453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们报告准确率。", "metrics": {"bleu_score": 51.92815178749843, "chrf_score": 41.86625721437747, "xcomet_score": 0.92449951171875, "xcomet_qe_score": 0.9797228574752808, "metricx_score": 0.587925374507904, "metricx_qe_score": 0.7086970806121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,我们报告 RougeL。对于 RP 任务,我们也报告 RougeL。", "metrics": {"bleu_score": 56.38909821116625, "chrf_score": 59.56958254339049, "xcomet_score": 0.783795952796936, "xcomet_qe_score": 0.7383759021759033, "metricx_score": 4.290326118469238, "metricx_qe_score": 4.329876899719238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了另一个评估指标,称为敏感性。", "metrics": {"bleu_score": 34.90178621050084, "chrf_score": 29.671773668596845, "xcomet_score": 0.9247111082077026, "xcomet_qe_score": 0.9307414889335632, "metricx_score": 0.7646870613098145, "metricx_qe_score": 0.9205254912376404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它衡量模型在指令措辞略有变化的情况下,是否能始终为同一任务产生相同输出的能力。", "metrics": {"bleu_score": 38.07155695331443, "chrf_score": 32.357904606698135, "xcomet_score": 0.9841854572296143, "xcomet_qe_score": 0.9854670763015747, "metricx_score": 2.206803798675537, "metricx_qe_score": 3.299402952194214, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要研究结果。", "metrics": {"bleu_score": 67.03420896351791, "chrf_score": 69.67750697057613, "xcomet_score": 0.9950377941131592, "xcomet_qe_score": 0.9910168647766113, "metricx_score": 0.2826874852180481, "metricx_qe_score": 0.38960468769073486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,指令微调可以显著提高OFE在视觉多模态任务上的表现。", "metrics": {"bleu_score": 37.55305919814521, "chrf_score": 35.21230374157141, "xcomet_score": 0.8700695037841797, "xcomet_qe_score": 0.8711236119270325, "metricx_score": 2.7110989093780518, "metricx_qe_score": 2.9917118549346924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习可以有益于指令调优。", "metrics": {"bleu_score": 83.92947005251233, "chrf_score": 81.98470678543143, "xcomet_score": 0.9723421335220337, "xcomet_qe_score": 0.7652846574783325, "metricx_score": 1.4824036359786987, "metricx_qe_score": 2.0413320064544678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,随着任务量的增加,模型的性能得到了提高,同时敏感度降低。 因此,", "metrics": {"bleu_score": 32.265347045020235, "chrf_score": 26.22954766917615, "xcomet_score": 0.7756865620613098, "xcomet_qe_score": 0.7949125170707703, "metricx_score": 4.021587371826172, "metricx_qe_score": 1.800046682357788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还做了一个实验,", "metrics": {"bleu_score": 88.01117367933934, "chrf_score": 85.90608465608466, "xcomet_score": 0.9896172285079956, "xcomet_qe_score": 0.9624312520027161, "metricx_score": 0.23607137799263, "metricx_qe_score": 0.20442509651184082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了1条指令与5条指令进行", "metrics": {"bleu_score": 28.295596283263514, "chrf_score": 25.59758563511636, "xcomet_score": 0.7516186237335205, "xcomet_qe_score": 0.7295094132423401, "metricx_score": 3.110644578933716, "metricx_qe_score": 2.6308059692382812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比较。我们可以看到,使用更多的指令可以提高模型的整体性能,并大大降低其敏感性。", "metrics": {"bleu_score": 48.05836435240956, "chrf_score": 45.17211313675948, "xcomet_score": 0.8064553737640381, "xcomet_qe_score": 0.8111329674720764, "metricx_score": 2.0758750438690186, "metricx_qe_score": 2.290992498397827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明了不同的微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 54.120475514419645, "chrf_score": 53.70772482752443, "xcomet_score": 0.975576639175415, "xcomet_qe_score": 0.9707900285720825, "metricx_score": 1.4315623044967651, "metricx_qe_score": 1.5186759233474731, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从自然指令数据集进行迁移学习,看到模型相较于原始的IFA模型,可以实现更好的敏感性。", "metrics": {"bleu_score": 33.75093400773327, "chrf_score": 29.91764942159441, "xcomet_score": 0.7750502824783325, "xcomet_qe_score": 0.6717612743377686, "metricx_score": 3.54872465133667, "metricx_qe_score": 3.7844133377075195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上取得更好的性能。", "metrics": {"bleu_score": 67.91260334112134, "chrf_score": 63.11884509565093, "xcomet_score": 0.9459350109100342, "xcomet_qe_score": 0.701043963432312, "metricx_score": 3.0862369537353516, "metricx_qe_score": 3.985150098800659, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,总的来说,我们提出了第一个大规模的多模态指令微调数据集,这显著提高了OFA的衍生能力,我们还探索了不同的迁移学习技术,并通过设计一个名为敏感性的", "metrics": {"bleu_score": 42.33319852784394, "chrf_score": 41.50084414148116, "xcomet_score": 0.45841553807258606, "xcomet_qe_score": 0.34911221265792847, "metricx_score": 8.165974617004395, "metricx_qe_score": 7.551448345184326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "新指标展示了这些技术的优势。", "metrics": {"bleu_score": 4.869426103311578, "chrf_score": 6.2555309285464915, "xcomet_score": 0.24369417130947113, "xcomet_qe_score": 0.23877188563346863, "metricx_score": 1.8980295658111572, "metricx_qe_score": 3.7741246223449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还有一点,我们正在收集一个更大的多模态指令微调数据集,其中包含大约 150 个额外的变体语言任务,我们将发布这些数据集。", "metrics": {"bleu_score": 63.984145847715375, "chrf_score": 60.475695273686284, "xcomet_score": 0.7642009258270264, "xcomet_qe_score": 0.752362847328186, "metricx_score": 2.382307291030884, "metricx_qe_score": 2.725656509399414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Coast of Sena,很高兴欢迎大家来到我们的ACL 2023论文讨论会,我们的论文题目是", "metrics": {"bleu_score": 28.708091158717856, "chrf_score": 34.5354870269576, "xcomet_score": 0.5232627391815186, "xcomet_qe_score": 0.4782121181488037, "metricx_score": 8.376293182373047, "metricx_qe_score": 9.548664093017578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型的可接受性判断并非总是能适应上下文》。", "metrics": {"bleu_score": 51.553367478834076, "chrf_score": 47.79745910984896, "xcomet_score": 0.9153404235839844, "xcomet_qe_score": 0.9073142409324646, "metricx_score": 0.9615539908409119, "metricx_qe_score": 2.4224472045898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与 John Bokier、Aaron Muller、Kanishka Mishra、Karen Fuentes、Roger Levy 和 Adina William 的合作作品。", "metrics": {"bleu_score": 21.154472601569257, "chrf_score": 66.8054776120519, "xcomet_score": 0.6384357213973999, "xcomet_qe_score": 0.6409054398536682, "metricx_score": 5.228080749511719, "metricx_qe_score": 5.060845375061035, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们重新审视了最小对范式。", "metrics": {"bleu_score": 45.663378549673105, "chrf_score": 46.65455611899317, "xcomet_score": 0.9555627107620239, "xcomet_qe_score": 0.9084150791168213, "metricx_score": 1.5144939422607422, "metricx_qe_score": 1.5100255012512207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最小配对范式基本上是在可接受性判断的基础上对语言模型进行评估,", "metrics": {"bleu_score": 60.710221238920745, "chrf_score": 65.41389883974479, "xcomet_score": 0.9637207984924316, "xcomet_qe_score": 0.9689056873321533, "metricx_score": 0.8912508487701416, "metricx_qe_score": 1.0220611095428467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中还包括语法性,如blimp、句法宝石,或在刻板印象方面的可接受性,如Krauss对。", "metrics": {"bleu_score": 23.391171757581535, "chrf_score": 15.376133550982116, "xcomet_score": 0.5167232155799866, "xcomet_qe_score": 0.4384937584400177, "metricx_score": 6.890192985534668, "metricx_qe_score": 7.485657691955566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个最小对范式中,评估语言模型的典型方法是,先展示一个可接受的句子或一个语法正确的句子,然后展示一个不可接受的句子或一个语法错误的句子。", "metrics": {"bleu_score": 59.41290991464405, "chrf_score": 52.14532842635398, "xcomet_score": 0.8061349391937256, "xcomet_qe_score": 0.8173505067825317, "metricx_score": 1.1016745567321777, "metricx_qe_score": 2.2958779335021973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型基本上会为可接受的句子赋予更高的概率。", "metrics": {"bleu_score": 37.260668086126365, "chrf_score": 31.90586568812871, "xcomet_score": 0.8829679489135742, "xcomet_qe_score": 0.7692053318023682, "metricx_score": 1.322019100189209, "metricx_qe_score": 2.0490128993988037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP流水线基本上不允许我们评估模型对更长句子的接受程度。", "metrics": {"bleu_score": 66.01973900178668, "chrf_score": 63.49269991100685, "xcomet_score": 0.9353364706039429, "xcomet_qe_score": 0.8717740774154663, "metricx_score": 3.2692508697509766, "metricx_qe_score": 3.5705642700195312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型的上下文窗口越来越长。", "metrics": {"bleu_score": 77.5792961097714, "chrf_score": 78.10281161553993, "xcomet_score": 0.9714252948760986, "xcomet_qe_score": 0.9141079783439636, "metricx_score": 0.5378822684288025, "metricx_qe_score": 0.6961395740509033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们在整个上下文窗口中评估模型的可接受性至关重要。 这就是我们在这里试图做的事情。", "metrics": {"bleu_score": 58.505032071185845, "chrf_score": 54.15887751534336, "xcomet_score": 0.8967925310134888, "xcomet_qe_score": 0.8656550049781799, "metricx_score": 0.912758469581604, "metricx_qe_score": 1.225801944732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过要求模型对越来越长的序列进行可接受性评估,来重新审视NPP管道。", "metrics": {"bleu_score": 51.4420122091352, "chrf_score": 46.4355497688071, "xcomet_score": 0.7998161315917969, "xcomet_qe_score": 0.7848057150840759, "metricx_score": 3.606508493423462, "metricx_qe_score": 4.697588920593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9963313341140747, "xcomet_qe_score": 0.9761532545089722, "metricx_score": 0.22746601700782776, "metricx_qe_score": 0.7089755535125732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们所做的是模拟这些更长的序列,我们重新审视数据集本身,然后通过从这些数据集选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 69.62337618574755, "chrf_score": 67.4597407522087, "xcomet_score": 0.8897430896759033, "xcomet_qe_score": 0.6377914547920227, "metricx_score": 2.3913707733154297, "metricx_qe_score": 3.2953333854675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从附属岛案例的气球数据集中选取了一对典型的语法对。", "metrics": {"bleu_score": 42.0097004585385, "chrf_score": 25.47252478453132, "xcomet_score": 0.6322638392448425, "xcomet_qe_score": 0.6611013412475586, "metricx_score": 7.477691173553467, "metricx_qe_score": 7.267514228820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是,重新创建更长的序列,并确定哪些序列是可接受的,哪些序列具有相同的语法结构匹配,", "metrics": {"bleu_score": 60.734221223466434, "chrf_score": 58.380768813243655, "xcomet_score": 0.9379571676254272, "xcomet_qe_score": 0.8360530138015747, "metricx_score": 1.500430941581726, "metricx_qe_score": 2.0074007511138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后从附属句中提取语法正确的句子。 然后,我们将它作为前缀添加到可接受的查询和不可接受的查询中。 因此,", "metrics": {"bleu_score": 56.196246700797055, "chrf_score": 51.05681424951945, "xcomet_score": 0.536215603351593, "xcomet_qe_score": 0.5329462289810181, "metricx_score": 5.613979816436768, "metricx_qe_score": 5.028292179107666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从同一匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 84.49687612847214, "chrf_score": 79.69691466828877, "xcomet_score": 0.9414241313934326, "xcomet_qe_score": 0.7378528118133545, "metricx_score": 1.1720682382583618, "metricx_qe_score": 1.830411434173584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过从不同的子集或不同的数据集选择句子来做到这一点。", "metrics": {"bleu_score": 73.48998814268005, "chrf_score": 69.85725171521311, "xcomet_score": 0.9821317195892334, "xcomet_qe_score": 0.896470844745636, "metricx_score": 0.7030987739562988, "metricx_qe_score": 1.2432994842529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的不匹配情况。 所以", "metrics": {"bleu_score": 66.68954865619207, "chrf_score": 71.98217025494581, "xcomet_score": 0.8453910946846008, "xcomet_qe_score": 0.7751846313476562, "metricx_score": 3.1666300296783447, "metricx_qe_score": 0.8530808687210083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关的数据集,但不是您正在评估的数据集。", "metrics": {"bleu_score": 52.422795649475866, "chrf_score": 45.893327109247195, "xcomet_score": 0.9585777521133423, "xcomet_qe_score": 0.784842848777771, "metricx_score": 1.2482174634933472, "metricx_qe_score": 2.125718116760254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于不可接受的情况,我们也可以这样做。", "metrics": {"bleu_score": 46.663612512230074, "chrf_score": 42.696972610224044, "xcomet_score": 0.9821330308914185, "xcomet_qe_score": 0.9639067649841309, "metricx_score": 0.5520765781402588, "metricx_qe_score": 0.599204957485199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全不相关的领域选择句子,例如维基百科。 因此,这", "metrics": {"bleu_score": 49.440093004527334, "chrf_score": 45.16134829425499, "xcomet_score": 0.6534410715103149, "xcomet_qe_score": 0.7364464998245239, "metricx_score": 5.888301849365234, "metricx_qe_score": 4.156954765319824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将告诉我们模型的可接受性判断是否真的受到任何上下文的影响。 例如,上下文是否来自数据集的不同子集,或者是否与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 66.59228090088861, "chrf_score": 60.24192595172335, "xcomet_score": 0.8457992076873779, "xcomet_qe_score": 0.8616873025894165, "metricx_score": 2.09363055229187, "metricx_qe_score": 2.9092493057250977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型的表现如何呢?", "metrics": {"bleu_score": 9.78090152232118, "chrf_score": 10.810141839392179, "xcomet_score": 0.8777414560317993, "xcomet_qe_score": 0.858100175857544, "metricx_score": 1.0029296875, "metricx_qe_score": 0.2817142903804779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看与当前查询对完全无关的维基百科句子,发现 MPP 判断对于任意上下文长度都非常稳健。", "metrics": {"bleu_score": 37.01528079794801, "chrf_score": 35.306154859679964, "xcomet_score": 0.9383010268211365, "xcomet_qe_score": 0.8523896932601929, "metricx_score": 3.5874483585357666, "metricx_qe_score": 5.550108909606934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到 2024,以最大化 OPT 和 GPT 两个模型的性能,我们在这里看到,", "metrics": {"bleu_score": 44.833867003844574, "chrf_score": 59.64249615787938, "xcomet_score": 0.4304161071777344, "xcomet_qe_score": 0.31708574295043945, "metricx_score": 8.123913764953613, "metricx_qe_score": 8.836012840270996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用橙色虚线表示的 MPP 判断相对稳定。", "metrics": {"bleu_score": 53.1799779830062, "chrf_score": 52.278460014866624, "xcomet_score": 0.9308826923370361, "xcomet_qe_score": 0.8689777851104736, "metricx_score": 2.3573989868164062, "metricx_qe_score": 4.734353065490723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,当我们从同一数据集选择句子时会发生什么?", "metrics": {"bleu_score": 52.10220626528036, "chrf_score": 45.47347262225907, "xcomet_score": 0.9926936626434326, "xcomet_qe_score": 0.936105489730835, "metricx_score": 0.63746178150177, "metricx_qe_score": 1.1434544324874878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们在这里从同一个BLIMP或SYNTAX GIMP数据集中的可接受和不可接受领域中选择或创建句子。 我们在这", "metrics": {"bleu_score": 46.37731399636584, "chrf_score": 38.37083184155175, "xcomet_score": 0.5449879169464111, "xcomet_qe_score": 0.5197349190711975, "metricx_score": 7.08973503112793, "metricx_qe_score": 5.990926742553711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里看到,当添加可接受的前缀或不可接受的前缀时,MPP 判断结果会显著增加或减少。", "metrics": {"bleu_score": 63.51178842457294, "chrf_score": 60.336444775655536, "xcomet_score": 0.5778475999832153, "xcomet_qe_score": 0.5729528069496155, "metricx_score": 4.683204650878906, "metricx_qe_score": 4.7791643142700195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,吉姆,那就是我们在语法上从相同的现象中选择句子来指责。 根据所选前缀是可接受的还是不可接受的,我们看到模型的 MPP 判断值出现了大幅增加或大幅减少。 现在,这个和这个非常大,比如这种效果随着", "metrics": {"bleu_score": 42.79932256143325, "chrf_score": 42.49471358635801, "xcomet_score": 0.11123364418745041, "xcomet_qe_score": 0.17983901500701904, "metricx_score": 14.147994995117188, "metricx_qe_score": 16.4439754486084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文长度的增加而增加,这可能会影响到具有大上下文窗口的新语言模型。", "metrics": {"bleu_score": 65.40231562549258, "chrf_score": 63.0686426252676, "xcomet_score": 0.7570677995681763, "xcomet_qe_score": 0.677277684211731, "metricx_score": 3.1272811889648438, "metricx_qe_score": 4.585777759552002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,匹配前缀为什么对语言模型的判断影响如此之大呢? 因此,", "metrics": {"bleu_score": 42.51869518183949, "chrf_score": 36.298671634878524, "xcomet_score": 0.8389618992805481, "xcomet_qe_score": 0.7731384038925171, "metricx_score": 3.917841911315918, "metricx_qe_score": 3.994147300720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,尝试通过在输入中添加噪声来干扰输入句子,同时尽量保留相关的结构。", "metrics": {"bleu_score": 43.568548383357566, "chrf_score": 40.59252159432577, "xcomet_score": 0.9810073375701904, "xcomet_qe_score": 0.9666690826416016, "metricx_score": 1.1459779739379883, "metricx_qe_score": 1.8464992046356201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了多次这种干扰后, 我们发现,这些噪音实际上并没有改变模型在展示 MPP 判断趋势方面的表现。", "metrics": {"bleu_score": 14.55702264695332, "chrf_score": 19.497353107891705, "xcomet_score": 0.9147486686706543, "xcomet_qe_score": 0.8595964312553406, "metricx_score": 3.9831008911132812, "metricx_qe_score": 4.503026962280273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型以相似的方式对扰动句", "metrics": {"bleu_score": 16.162239449628064, "chrf_score": 17.61492274873681, "xcomet_score": 0.7791751623153687, "xcomet_qe_score": 0.7905330657958984, "metricx_score": 5.997762680053711, "metricx_qe_score": 5.489698886871338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "子敏感。 当我们在可接受范围内扰动句子时,我们观察到所有扰动中都有类似的增加;当我们在不可接受范围内扰动句子时,我们以类似的方式观察到MPP判断的减少。", "metrics": {"bleu_score": 46.346945708966885, "chrf_score": 40.042229002694164, "xcomet_score": 0.501571536064148, "xcomet_qe_score": 0.31864190101623535, "metricx_score": 7.1919355392456055, "metricx_qe_score": 8.264836311340332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键结论是,语言模型对句子间共享的潜在句法和语义特征很敏感。", "metrics": {"bleu_score": 77.42072321622986, "chrf_score": 72.77672650737665, "xcomet_score": 0.9851152896881104, "xcomet_qe_score": 0.9956561326980591, "metricx_score": 1.033024787902832, "metricx_qe_score": 1.197113275527954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而目前我们对 MPP 的评估方式,即使用短句和单句输入,可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 48.90727462382201, "chrf_score": 42.068574742178896, "xcomet_score": 0.9748961925506592, "xcomet_qe_score": 0.8936968445777893, "metricx_score": 1.1310193538665771, "metricx_qe_score": 1.8927407264709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文,以获取我们实验的更多详细信息。", "metrics": {"bleu_score": 71.92779767585063, "chrf_score": 66.47819126029695, "xcomet_score": 0.9983570575714111, "xcomet_qe_score": 1.0, "metricx_score": 0.2198541909456253, "metricx_qe_score": 0.2157556414604187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张宇信。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 49.88704001434205, "xcomet_score": 0.9360659122467041, "xcomet_qe_score": 0.9875787496566772, "metricx_score": 0.5195938944816589, "metricx_qe_score": 0.7311459183692932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天,我将介绍我们的工作——跨语言语义解析在多种自然语言和意义表示中的应用。 因此,语义解析的任务", "metrics": {"bleu_score": 44.31441692066578, "chrf_score": 42.801421266985706, "xcomet_score": 0.7146449089050293, "xcomet_qe_score": 0.6727222204208374, "metricx_score": 8.687551498413086, "metricx_qe_score": 5.362906455993652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是构建用户查询(如 SQL 和 λ 演算)的语义表示。", "metrics": {"bleu_score": 40.85088315181786, "chrf_score": 27.631952118481518, "xcomet_score": 0.4680079221725464, "xcomet_qe_score": 0.46620604395866394, "metricx_score": 5.972935676574707, "metricx_qe_score": 5.349033355712891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义解析的任务是将多种自然语言中的查询翻译成多种意义表示。", "metrics": {"bleu_score": 74.63192414956329, "chrf_score": 67.4320623937345, "xcomet_score": 0.8887280225753784, "xcomet_qe_score": 0.9340234994888306, "metricx_score": 1.4902759790420532, "metricx_qe_score": 3.710554838180542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将查询翻译成多种自然语言,包括SQL、Lambda、FunQL等。 例如", "metrics": {"bleu_score": 58.61141851152311, "chrf_score": 62.80439678931706, "xcomet_score": 0.7998672127723694, "xcomet_qe_score": 0.7959780693054199, "metricx_score": 1.7003134489059448, "metricx_qe_score": 1.6620525121688843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",现有的跨语言语义解析模型是分别针对有限的任务和应用数据集提出的和评估的。", "metrics": {"bleu_score": 65.7938179859887, "chrf_score": 58.93409038333769, "xcomet_score": 0.9799373149871826, "xcomet_qe_score": 0.9651297926902771, "metricx_score": 1.0208771228790283, "metricx_qe_score": 1.5537933111190796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "某些自然语言的报道很多。", "metrics": {"bleu_score": 39.34476914263935, "chrf_score": 36.433047826870926, "xcomet_score": 0.8854944705963135, "xcomet_qe_score": 0.8558218479156494, "metricx_score": 1.9450814723968506, "metricx_qe_score": 1.715579867362976, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失了。 某些微型代表图上覆盖有湖泊。", "metrics": {"bleu_score": 6.8187083151736685, "chrf_score": 10.143240191927747, "xcomet_score": 0.541702151298523, "xcomet_qe_score": 0.24329115450382233, "metricx_score": 7.359092712402344, "metricx_qe_score": 7.691493511199951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "缺少了λ演算。 或者它们只在某些较新的模型上进行评估。", "metrics": {"bleu_score": 36.19779110707069, "chrf_score": 30.47948425112683, "xcomet_score": 0.7260170578956604, "xcomet_qe_score": 0.7009447813034058, "metricx_score": 3.1717333793640137, "metricx_qe_score": 4.21664571762085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个模型来评估它们。", "metrics": {"bleu_score": 56.42812502283149, "chrf_score": 49.202863729636285, "xcomet_score": 0.9981586933135986, "xcomet_qe_score": 0.9880315065383911, "metricx_score": 0.5080630779266357, "metricx_qe_score": 0.8839173316955566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了exemplerexemp", "metrics": {"bleu_score": 45.93613320783059, "chrf_score": 22.07193094948622, "xcomet_score": 0.8276772499084473, "xcomet_qe_score": 0.8262737989425659, "metricx_score": 4.899494171142578, "metricx_qe_score": 5.647408485412598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "lar,我们为多语言跨语言语义解析和多种意义表示提供了统一的数据集exemplerexemplar。", "metrics": {"bleu_score": 39.13841884025347, "chrf_score": 28.656167303418105, "xcomet_score": 0.5917254686355591, "xcomet_qe_score": 0.5310195684432983, "metricx_score": 11.011362075805664, "metricx_qe_score": 11.949186325073242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含了九个不同领域的语料库、五个语义部分和税务、八种意义表示,以及15个语系中的22种自然语言。", "metrics": {"bleu_score": 38.54585933086036, "chrf_score": 39.81408438433402, "xcomet_score": 0.6794916391372681, "xcomet_qe_score": 0.7051476240158081, "metricx_score": 7.644155502319336, "metricx_qe_score": 8.534307479858398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了训练和评估的六种设置。", "metrics": {"bleu_score": 67.8301759715223, "chrf_score": 59.56205276123286, "xcomet_score": 0.9824798107147217, "xcomet_qe_score": 0.9433248043060303, "metricx_score": 1.0820890665054321, "metricx_qe_score": 2.2754859924316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是TranslateTest。", "metrics": {"bleu_score": 32.58798048281462, "chrf_score": 18.76628675854382, "xcomet_score": 0.9642470479011536, "xcomet_qe_score": 0.953527569770813, "metricx_score": 1.326793909072876, "metricx_qe_score": 1.1972638368606567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google Translate API将源语言翻译成目标语言,然后使用MonolingoModel进行评估训练。", "metrics": {"bleu_score": 47.469196351277546, "chrf_score": 40.11414063668091, "xcomet_score": 0.8784365653991699, "xcomet_qe_score": 0.9266171455383301, "metricx_score": 3.7974817752838135, "metricx_qe_score": 2.2664692401885986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们用英语查询对英语模型进行训练,在推理过程中,我们使用 API 将德语查询翻译成英语,然后使用训练好的模型来预测 SQL。", "metrics": {"bleu_score": 68.95941859640645, "chrf_score": 64.57054901626023, "xcomet_score": 0.9540742635726929, "xcomet_qe_score": 0.9100123643875122, "metricx_score": 1.1785595417022705, "metricx_qe_score": 2.4346630573272705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模块。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 76.96368446368447, "xcomet_score": 0.8608987927436829, "xcomet_qe_score": 0.837360680103302, "metricx_score": 0.2960849404335022, "metricx_qe_score": 0.4536767601966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如德语对德语或英语对英语。", "metrics": {"bleu_score": 54.82970629055102, "chrf_score": 45.6279010285777, "xcomet_score": 0.9127984046936035, "xcomet_qe_score": 0.9851455688476562, "metricx_score": 1.3072398900985718, "metricx_qe_score": 0.9992969036102295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过仅使用10%的训练数据来训练单语模型,测试了单语融合设置。", "metrics": {"bleu_score": 55.514947988352404, "chrf_score": 51.31896831036566, "xcomet_score": 0.8049598932266235, "xcomet_qe_score": 0.757426917552948, "metricx_score": 4.219366550445557, "metricx_qe_score": 3.603187322616577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试了一个多语言模型,我们为所有语言训练了一个多语言模型。", "metrics": {"bleu_score": 59.10522461071207, "chrf_score": 58.07365574328725, "xcomet_score": 0.9304975271224976, "xcomet_qe_score": 0.8692228198051453, "metricx_score": 1.4970580339431763, "metricx_qe_score": 2.457994222640991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询放在一起训练一个多语言模型。", "metrics": {"bleu_score": 74.25493166673012, "chrf_score": 66.1527115422947, "xcomet_score": 0.9517291784286499, "xcomet_qe_score": 0.9503105878829956, "metricx_score": 1.3213473558425903, "metricx_qe_score": 2.908708333969116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理过程中,我们可以使用这个模型。 翻译德语查询或中文查询等。", "metrics": {"bleu_score": 82.32325806433653, "chrf_score": 74.62718113811923, "xcomet_score": 0.8605893850326538, "xcomet_qe_score": 0.7688493728637695, "metricx_score": 1.3764394521713257, "metricx_qe_score": 2.247406244277954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了 crosslingo 零次和现场拍摄的转移,", "metrics": {"bleu_score": 27.700512338542904, "chrf_score": 21.038449950334595, "xcomet_score": 0.5065831542015076, "xcomet_qe_score": 0.49905601143836975, "metricx_score": 9.492798805236816, "metricx_qe_score": 8.934541702270508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即在一个源语言上运行并转移到另一种语言。 因此,", "metrics": {"bleu_score": 34.154567258745274, "chrf_score": 27.461462340860788, "xcomet_score": 0.5175988674163818, "xcomet_qe_score": 0.6085941791534424, "metricx_score": 5.833747386932373, "metricx_qe_score": 6.2660231590271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我将使用英语查询或英语和德语混合查询对其进行训练,以训练一个多语言模型并预测 SQL 输出。", "metrics": {"bleu_score": 51.11052812049324, "chrf_score": 49.006788726522196, "xcomet_score": 0.7732573747634888, "xcomet_qe_score": 0.6961711645126343, "metricx_score": 1.7018581628799438, "metricx_qe_score": 2.1547815799713135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的成果。因此", "metrics": {"bleu_score": 26.96698152099015, "chrf_score": 26.932617513716504, "xcomet_score": 0.8180636763572693, "xcomet_qe_score": 0.7937408685684204, "metricx_score": 3.2309324741363525, "metricx_qe_score": 1.9735608100891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",关于单语模型的分析,我们对两组模型进行了评估。 包括编码器PDR,即基于指针的解码器的多语言预训练编码器,如XLMR plus PDR和BERT plus PDR。", "metrics": {"bleu_score": 49.43682056963306, "chrf_score": 39.63074630219113, "xcomet_score": 0.6252691745758057, "xcomet_qe_score": 0.5509542226791382, "metricx_score": 5.212683200836182, "metricx_qe_score": 4.447027683258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,如 MBART 和 MT5。", "metrics": {"bleu_score": 31.009124728696612, "chrf_score": 22.207379665592036, "xcomet_score": 0.9116591811180115, "xcomet_qe_score": 0.9617220163345337, "metricx_score": 1.5549606084823608, "metricx_qe_score": 3.117875576019287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器-解码器在所有九个数据集上均获得最佳性能。", "metrics": {"bleu_score": 40.10198002912937, "chrf_score": 27.5286160729574, "xcomet_score": 0.9877752065658569, "xcomet_qe_score": 0.9819492101669312, "metricx_score": 1.4411680698394775, "metricx_qe_score": 1.2320201396942139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还对 MT five 和 XLMR plus PDR 多语言设置进行了评估。", "metrics": {"bleu_score": 14.962848372546674, "chrf_score": 19.386924436362797, "xcomet_score": 0.8311983346939087, "xcomet_qe_score": 0.8419243097305298, "metricx_score": 4.748340606689453, "metricx_qe_score": 4.760478973388672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合环境中进行训练,可以改进编码器-解码器或编码器 PDR。", "metrics": {"bleu_score": 22.610908143727077, "chrf_score": 17.147182956292298, "xcomet_score": 0.8211167454719543, "xcomet_qe_score": 0.8137238025665283, "metricx_score": 2.965578317642212, "metricx_qe_score": 3.915353536605835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都能获得性能提升,但英语在七个数据集中的性能下降,仅在三个数据集中有提升。", "metrics": {"bleu_score": 54.485945386987446, "chrf_score": 48.95676116911503, "xcomet_score": 0.933436393737793, "xcomet_qe_score": 0.9849833250045776, "metricx_score": 2.6641733646392822, "metricx_qe_score": 2.202573299407959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言的诅咒。", "metrics": {"bleu_score": 35.47202175617096, "chrf_score": 30.880643040199136, "xcomet_score": 0.9179631471633911, "xcomet_qe_score": 0.8739200234413147, "metricx_score": 1.051566481590271, "metricx_qe_score": 1.5422992706298828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此图中,蓝色曲线表示跨语言燃料射击转移,", "metrics": {"bleu_score": 9.846107951428584, "chrf_score": 11.909505838032498, "xcomet_score": 0.6583584547042847, "xcomet_qe_score": 0.6618680357933044, "metricx_score": 7.349153518676758, "metricx_qe_score": 7.1599602699279785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙色曲线表示跨语言零射击转移,", "metrics": {"bleu_score": 18.92240568795936, "chrf_score": 18.451862176998844, "xcomet_score": 0.7862300872802734, "xcomet_qe_score": 0.7746168375015259, "metricx_score": 4.995696544647217, "metricx_qe_score": 4.971327781677246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿色曲线表示单语设置。 我们发现", "metrics": {"bleu_score": 26.760322756637922, "chrf_score": 35.51301600167439, "xcomet_score": 0.8540080189704895, "xcomet_qe_score": 0.8345656991004944, "metricx_score": 3.8867759704589844, "metricx_qe_score": 2.552009105682373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿色和橙色线条,我们发现对于零短设置,跨语言迁移性能差距显著。通过比较蓝色和橙色线条,我们发现对于少量短设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 33.459995182428116, "chrf_score": 28.609139407951794, "xcomet_score": 0.5318324565887451, "xcomet_qe_score": 0.6011368036270142, "metricx_score": 7.8366875648498535, "metricx_qe_score": 7.040106773376465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器解码器优于进度工作,或者取得了可比拟的结果。", "metrics": {"bleu_score": 12.80220256953781, "chrf_score": 9.026265880348175, "xcomet_score": 0.7625865936279297, "xcomet_qe_score": 0.7589131593704224, "metricx_score": 5.1751227378845215, "metricx_qe_score": 5.053605079650879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用英语进行购买可以显著提高fuchshot在目标自然语言上的性能。 我们发现,像 Codice 和 Bloom 这样的多语言模型在跨语言语义解析任务中仍然不够完善。", "metrics": {"bleu_score": 47.2523194383938, "chrf_score": 37.94849146389772, "xcomet_score": 0.39226233959198, "xcomet_qe_score": 0.37658634781837463, "metricx_score": 10.4210786819458, "metricx_qe_score": 10.27917766571045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结来说,我们构建了 Exempler,这是一个统一的基准测试工具,用于跨角度语义解析,支持多种自然语言和微型表示。", "metrics": {"bleu_score": 32.05659631041095, "chrf_score": 26.997415439806144, "xcomet_score": 0.692642331123352, "xcomet_qe_score": 0.6870107054710388, "metricx_score": 4.45940637588501, "metricx_qe_score": 4.716688632965088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语言模型进行了全面的基准研究,", "metrics": {"bleu_score": 71.79226303657947, "chrf_score": 62.0439453563827, "xcomet_score": 0.9669307470321655, "xcomet_qe_score": 0.9562174081802368, "metricx_score": 1.3707668781280518, "metricx_qe_score": 1.7768745422363281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究结果显示了许多有趣的发现等", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 74.22884164335593, "xcomet_score": 0.8646327257156372, "xcomet_qe_score": 0.8622707724571228, "metricx_score": 1.6374224424362183, "metricx_qe_score": 1.183284878730774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢聆听。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9658737182617188, "xcomet_qe_score": 0.9351316690444946, "metricx_score": 0.08587995171546936, "metricx_qe_score": 0.44492465257644653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫艾德·维拉尔,我将简要介绍一篇论文《促进 PowerPoint 翻译,评估策略和性能》。", "metrics": {"bleu_score": 20.80151103411065, "chrf_score": 16.844726299919454, "xcomet_score": 0.6271610260009766, "xcomet_qe_score": 0.7373043298721313, "metricx_score": 7.157430648803711, "metricx_qe_score": 6.69911003112793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和谷歌翻译同事的合作成果。", "metrics": {"bleu_score": 20.570885115591267, "chrf_score": 20.732468773163916, "xcomet_score": 0.9982795715332031, "xcomet_qe_score": 0.9951926469802856, "metricx_score": 0.7620185017585754, "metricx_qe_score": 0.350193053483963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Parm 是一款去年 2022 年推出的 5400 亿参数学习语言模型。", "metrics": {"bleu_score": 36.49486772692668, "chrf_score": 40.44221393567046, "xcomet_score": 0.7194671630859375, "xcomet_qe_score": 0.7190289497375488, "metricx_score": 5.071591854095459, "metricx_qe_score": 5.579777240753174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在大规模标签集合上进行了训练,该集合包含 7800 亿个标记。", "metrics": {"bleu_score": 17.44391898986843, "chrf_score": 29.777181637667294, "xcomet_score": 0.7893346548080444, "xcomet_qe_score": 0.7180910110473633, "metricx_score": 2.602801561355591, "metricx_qe_score": 2.7504560947418213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百个自然语言处理任务中达到了当时的最先进水平。", "metrics": {"bleu_score": 19.965747160974658, "chrf_score": 20.464399768099625, "xcomet_score": 0.9997749328613281, "xcomet_qe_score": 0.9985364675521851, "metricx_score": 0.7379469275474548, "metricx_qe_score": 0.954304039478302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们首次系统研究了用于机器翻译的 Latch 语言模型提示。", "metrics": {"bleu_score": 33.221219011316414, "chrf_score": 27.595378349251227, "xcomet_score": 0.8131096363067627, "xcomet_qe_score": 0.8802720308303833, "metricx_score": 5.8125529289245605, "metricx_qe_score": 5.7804179191589355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 AMT 社区的最佳实践来评估这些模型的过渡能力。", "metrics": {"bleu_score": 54.016039703559564, "chrf_score": 46.89636229528477, "xcomet_score": 0.8035073280334473, "xcomet_qe_score": 0.851229727268219, "metricx_score": 3.6389524936676025, "metricx_qe_score": 4.26189661026001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两种最先进的系统,即 WMT 评估中表现最好的系统。", "metrics": {"bleu_score": 28.714372980706766, "chrf_score": 29.869824993653765, "xcomet_score": 0.9462811946868896, "xcomet_qe_score": 0.9269328117370605, "metricx_score": 2.5186755657196045, "metricx_qe_score": 4.112396240234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的和新的LMT指标,此外还展示了基于专家的人工评估结果。", "metrics": {"bleu_score": 57.88391976335255, "chrf_score": 51.749916230338044, "xcomet_score": 0.8905644416809082, "xcomet_qe_score": 0.8534201383590698, "metricx_score": 2.7143020629882812, "metricx_qe_score": 3.182321548461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们还提供了一些关于提示选择策略的建议。", "metrics": {"bleu_score": 78.17678781133698, "chrf_score": 77.97863861475979, "xcomet_score": 0.916377604007721, "xcomet_qe_score": 0.8556562066078186, "metricx_score": 1.0609506368637085, "metricx_qe_score": 1.4160351753234863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对 LLM 的翻译性能有着重大影响,我们可以在一个简单的实验中看到这一点,在这个实验中,我们使用一个简短的提示,并为一句话提供两个不同的提示。", "metrics": {"bleu_score": 47.35393078802616, "chrf_score": 49.79967054123689, "xcomet_score": 0.8042507171630859, "xcomet_qe_score": 0.7711129188537598, "metricx_score": 2.365993022918701, "metricx_qe_score": 3.735473871231079, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子,一千个句子中有五", "metrics": {"bleu_score": 11.359354890271161, "chrf_score": 9.903071846374873, "xcomet_score": 0.6191396117210388, "xcomet_qe_score": 0.6829347014427185, "metricx_score": 7.050491809844971, "metricx_qe_score": 5.893455982208252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "百十六个,观察到的差异超过一个模糊点。", "metrics": {"bleu_score": 5.32864224277779, "chrf_score": 5.704712097927227, "xcomet_score": 0.34134164452552795, "xcomet_qe_score": 0.2620009481906891, "metricx_score": 8.7549467086792, "metricx_qe_score": 9.523162841796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这个数字甚至可以达到 40 个模糊点。", "metrics": {"bleu_score": 24.70764447832614, "chrf_score": 22.11848791940325, "xcomet_score": 0.8079386353492737, "xcomet_qe_score": 0.7423205375671387, "metricx_score": 4.577740669250488, "metricx_qe_score": 2.335872173309326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个好的提示策略非常重要。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 59.37187365464642, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.24021872878074646, "metricx_qe_score": 0.356181263923645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了五次提示策略,即我们只需标明向系统提供的每句话所使用的语言。 因此,", "metrics": {"bleu_score": 35.178029201476534, "chrf_score": 29.477855733905837, "xcomet_score": 0.5688326358795166, "xcomet_qe_score": 0.6299392580986023, "metricx_score": 4.571167469024658, "metricx_qe_score": 3.515343189239502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们从德语翻译成英语,德语句子用德语冒号标注,英语翻译用英语冒号标注。", "metrics": {"bleu_score": 42.001154791435695, "chrf_score": 29.835725195178092, "xcomet_score": 0.963516354560852, "xcomet_qe_score": 0.9751574993133545, "metricx_score": 1.324877381324768, "metricx_qe_score": 1.3415321111679077, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在几个短提示的情况下,提示的实际形式没有太大影响。", "metrics": {"bleu_score": 42.0811691266725, "chrf_score": 35.27633783250459, "xcomet_score": 0.9070167541503906, "xcomet_qe_score": 0.860403299331665, "metricx_score": 1.259537696838379, "metricx_qe_score": 1.4391005039215088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零次和一次提示来说至关重要,", "metrics": {"bleu_score": 23.35338880685196, "chrf_score": 20.963660755213166, "xcomet_score": 0.6916929483413696, "xcomet_qe_score": 0.7409855723381042, "metricx_score": 3.104924440383911, "metricx_qe_score": 2.2004098892211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们像我们这样进行五次提示时,提示的实际形式几乎没有区", "metrics": {"bleu_score": 26.49268590278449, "chrf_score": 29.37286633324026, "xcomet_score": 0.7934814095497131, "xcomet_qe_score": 0.6866174936294556, "metricx_score": 3.5164334774017334, "metricx_qe_score": 2.9794187545776367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别。 例子才是最重要的。", "metrics": {"bleu_score": 7.510002314354895, "chrf_score": 8.806842695659677, "xcomet_score": 0.6426215767860413, "xcomet_qe_score": 0.23790472745895386, "metricx_score": 1.081170916557312, "metricx_qe_score": 1.0022417306900024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果总结如下:示例质量比与源句的相似性更重要。", "metrics": {"bleu_score": 62.11475757601459, "chrf_score": 52.82042139288515, "xcomet_score": 0.9934189319610596, "xcomet_qe_score": 0.9833729267120361, "metricx_score": 0.6372272968292236, "metricx_qe_score": 0.6109703183174133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,从高质量的翻译中选择例子非常重要。", "metrics": {"bleu_score": 68.05854779603962, "chrf_score": 59.13663857355807, "xcomet_score": 0.9334757328033447, "xcomet_qe_score": 0.9385048151016235, "metricx_score": 0.5153730511665344, "metricx_qe_score": 0.592513918876648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了 WMT 评估的训练数据或开发数据中的选择提示。", "metrics": {"bleu_score": 36.90559374275196, "chrf_score": 34.19087271950346, "xcomet_score": 0.7528867125511169, "xcomet_qe_score": 0.5952918529510498, "metricx_score": 2.098385810852051, "metricx_qe_score": 3.040994882583618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "深度数据经过了更精心的整理,其质量也高于训练数据,更不用说,使用深度", "metrics": {"bleu_score": 16.57462415840606, "chrf_score": 16.898512735517283, "xcomet_score": 0.5008951425552368, "xcomet_qe_score": 0.5198494791984558, "metricx_score": 7.611171245574951, "metricx_qe_score": 5.512444019317627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据时,结果显示性能更好。", "metrics": {"bleu_score": 17.444714791376892, "chrf_score": 16.932944722023947, "xcomet_score": 0.41372206807136536, "xcomet_qe_score": 0.4723772704601288, "metricx_score": 5.158078670501709, "metricx_qe_score": 5.644627094268799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管专业化的最先进系统在翻译质量上远超 PAL", "metrics": {"bleu_score": 19.135491560663304, "chrf_score": 18.054629123933335, "xcomet_score": 0.6821473836898804, "xcomet_qe_score": 0.6076460480690002, "metricx_score": 5.893745422363281, "metricx_qe_score": 8.02466106414795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "M,但 PALM 的翻译质量已经非常接近商业系统。", "metrics": {"bleu_score": 52.20981806658137, "chrf_score": 55.42828104005657, "xcomet_score": 0.49109214544296265, "xcomet_qe_score": 0.4709879159927368, "metricx_score": 8.46552848815918, "metricx_qe_score": 8.49274730682373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择使用谷歌翻译进行叠加。", "metrics": {"bleu_score": 74.73575569817635, "chrf_score": 68.0920928804224, "xcomet_score": 0.8686974048614502, "xcomet_qe_score": 0.8728095889091492, "metricx_score": 2.766195297241211, "metricx_qe_score": 2.574458599090576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从使用 MQM 框架进行的人类创新中获得的洞察是,PALM 的流畅度与最先进的系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 55.29172863374992, "chrf_score": 49.65148435136504, "xcomet_score": 0.8051611185073853, "xcomet_qe_score": 0.7491542100906372, "metricx_score": 4.2770185470581055, "metricx_qe_score": 5.010916709899902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7579965591430664, "xcomet_qe_score": 0.7860524654388428, "metricx_score": 1.7050689458847046, "metricx_qe_score": 0.886371910572052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,Palm似乎选择通过省略翻译中未包含的原文句子部分来制作听起来更好的翻译。", "metrics": {"bleu_score": 29.868300620204664, "chrf_score": 25.487953713186766, "xcomet_score": 0.8442872166633606, "xcomet_qe_score": 0.7541818022727966, "metricx_score": 2.965566635131836, "metricx_qe_score": 3.325416326522827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAN 的外部风格类别低于最先进的系统,这是一个额外的信号。 该部分提供了非常流畅的输出,但仍然存在一些准确性问题。", "metrics": {"bleu_score": 57.15529070293437, "chrf_score": 46.8447769738044, "xcomet_score": 0.6264016032218933, "xcomet_qe_score": 0.5949718952178955, "metricx_score": 5.755627632141113, "metricx_qe_score": 6.36336088180542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是这次非常简短的概述。", "metrics": {"bleu_score": 10.406997559387968, "chrf_score": 14.884460660492094, "xcomet_score": 0.982917308807373, "xcomet_qe_score": 0.9095495939254761, "metricx_score": 0.44702011346817017, "metricx_qe_score": 0.724204421043396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欲了解更多详情,请参阅论文的完整内容。", "metrics": {"bleu_score": 30.491854911466543, "chrf_score": 24.606818604139107, "xcomet_score": 0.9885622262954712, "xcomet_qe_score": 0.9896273612976074, "metricx_score": 0.7244207859039307, "metricx_qe_score": 0.23767736554145813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是德国扎兰特大学的博士生 Dawe。", "metrics": {"bleu_score": 31.119104221636672, "chrf_score": 30.248704219374016, "xcomet_score": 0.7777101993560791, "xcomet_qe_score": 0.7851225137710571, "metricx_score": 1.2360836267471313, "metricx_qe_score": 2.34828519821167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想介绍我们最近的工作《比你想象的还要弱》,这是一次对每周供应学习的批判性审视。", "metrics": {"bleu_score": 42.79770557186218, "chrf_score": 36.64385256328654, "xcomet_score": 0.7274141311645508, "xcomet_qe_score": 0.7214961051940918, "metricx_score": 7.061119079589844, "metricx_qe_score": 7.794923305511475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与Xiao Yushche、Marios Musbach、Gas Steffen和Dietrich Clarkov的合作作品。", "metrics": {"bleu_score": 6.642561135292097, "chrf_score": 36.720290405132324, "xcomet_score": 0.6211196184158325, "xcomet_qe_score": 0.6481356620788574, "metricx_score": 8.680891036987305, "metricx_qe_score": 8.295184135437012, "linguapy_score": [1, "GERMAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想从对周监督和每周监督学习的简要介绍开始。", "metrics": {"bleu_score": 27.19326877457978, "chrf_score": 25.199538181565078, "xcomet_score": 0.7277451753616333, "xcomet_qe_score": 0.6796576380729675, "metricx_score": 6.012855529785156, "metricx_qe_score": 5.790443420410156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不手动对数据进行标注。", "metrics": {"bleu_score": 26.104909033290696, "chrf_score": 24.397511148255102, "xcomet_score": 0.9016437530517578, "xcomet_qe_score": 0.8631446361541748, "metricx_score": 0.7703352570533752, "metricx_qe_score": 1.4398902654647827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注源对数据进行标注,例如简单的启发式规则、知识库或局部代码源,如图右侧所示。", "metrics": {"bleu_score": 46.775793659417666, "chrf_score": 42.54407983148336, "xcomet_score": 0.7566646337509155, "xcomet_qe_score": 0.6789016127586365, "metricx_score": 2.490358352661133, "metricx_qe_score": 4.6904144287109375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,较弱的标注要便宜得多,但它们也存在噪声,这意味着一定数量的标注是错误的。", "metrics": {"bleu_score": 34.06033204524115, "chrf_score": 28.98389052799949, "xcomet_score": 0.7637859582901001, "xcomet_qe_score": 0.8271545767784119, "metricx_score": 2.0893545150756836, "metricx_qe_score": 2.061985492706299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在每周的标签数据上训练神经网络,神经网络往往会记住标签噪声,并且无法泛化。", "metrics": {"bleu_score": 46.98641022063539, "chrf_score": 39.70527801584766, "xcomet_score": 0.8189586400985718, "xcomet_qe_score": 0.7908698916435242, "metricx_score": 5.634610176086426, "metricx_qe_score": 6.393981456756592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周的监督学习中,提出了训练算法,以便在如此水平的噪声下稳健地训练神经网络,从而使训练好的模型仍然具有良好的泛化能力。", "metrics": {"bleu_score": 59.49583789365926, "chrf_score": 51.90523258117985, "xcomet_score": 0.8014039993286133, "xcomet_qe_score": 0.7554014921188354, "metricx_score": 4.211685657501221, "metricx_qe_score": 5.312309265136719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在WSL的近期工作中,WSL代表每周监督学习,人们普遍声称他们只在每周的标签数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 36.01753376166225, "chrf_score": 33.43111473729224, "xcomet_score": 0.6928079128265381, "xcomet_qe_score": 0.7219549417495728, "metricx_score": 8.287979125976562, "metricx_qe_score": 9.038601875305176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并没有错,但有一个问题。 人们确实假设有一个额外的干净验证集可用于模型选择。", "metrics": {"bleu_score": 71.87934550251784, "chrf_score": 65.68333349236411, "xcomet_score": 0.9828405380249023, "xcomet_qe_score": 0.96302330493927, "metricx_score": 2.0963947772979736, "metricx_qe_score": 3.4486465454101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这种问题设定表示怀疑,因为它意味着每周的监督学习需要额外的手动标注,但", "metrics": {"bleu_score": 29.740744849501656, "chrf_score": 26.73554808981252, "xcomet_score": 0.4449421763420105, "xcomet_qe_score": 0.4477139115333557, "metricx_score": 7.906888008117676, "metricx_qe_score": 5.798901557922363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就像房间里的大象一样,这种必要性常常被忽视。", "metrics": {"bleu_score": 76.12896074640392, "chrf_score": 72.28058474406669, "xcomet_score": 0.9209985136985779, "xcomet_qe_score": 0.8047943115234375, "metricx_score": 1.1631584167480469, "metricx_qe_score": 2.8568975925445557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑问促使我们提出了三个研究问题。", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 45.31253098794524, "xcomet_score": 0.9559470415115356, "xcomet_qe_score": 0.9501094818115234, "metricx_score": 1.5945442914962769, "metricx_qe_score": 1.069020390510559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,WSL 是否需要干净的验证数据?或者我们是否可以使用噪声验证集?", "metrics": {"bleu_score": 55.603964314507635, "chrf_score": 50.91778855591179, "xcomet_score": 0.8667882680892944, "xcomet_qe_score": 0.8544470071792603, "metricx_score": 2.1947519779205322, "metricx_qe_score": 3.4921021461486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净的数据,或者干净的数据是 WSL 工作的必要条件,那么我们需要多少干净的样本?", "metrics": {"bleu_score": 51.045451261653085, "chrf_score": 42.86864108465261, "xcomet_score": 0.9893238544464111, "xcomet_qe_score": 0.9750915765762329, "metricx_score": 0.9693731069564819, "metricx_qe_score": 1.1575359106063843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该只使用干净的样本进行验证,或者还有更好的利用它们的方法?", "metrics": {"bleu_score": 53.04418155583279, "chrf_score": 45.31040038627225, "xcomet_score": 0.992186427116394, "xcomet_qe_score": 0.9232062697410583, "metricx_score": 0.7129461765289307, "metricx_qe_score": 1.0156171321868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的研究结果如下。", "metrics": {"bleu_score": 46.22377023605668, "chrf_score": 42.139554638459394, "xcomet_score": 0.9754983186721802, "xcomet_qe_score": 0.961543083190918, "metricx_score": 1.5331122875213623, "metricx_qe_score": 2.23215389251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现,有趣的是,WSL 的最新方法确实需要干净的白色减号样本才能正常工作。", "metrics": {"bleu_score": 47.90358500146104, "chrf_score": 42.29985308803514, "xcomet_score": 0.8078219890594482, "xcomet_qe_score": 0.8091399669647217, "metricx_score": 3.372843027114868, "metricx_qe_score": 3.648135185241699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降,", "metrics": {"bleu_score": 63.15552371794039, "chrf_score": 55.594035594035596, "xcomet_score": 0.9874362945556641, "xcomet_qe_score": 0.9928046464920044, "metricx_score": 0.4509727358818054, "metricx_qe_score": 0.7942342758178711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示。如果没有干净的验证样本,那么训练好的模型就无法推广到原始的弱标签之外。 这意味着培训毫无意义。", "metrics": {"bleu_score": 53.89222168803019, "chrf_score": 45.00125722682338, "xcomet_score": 0.778372049331665, "xcomet_qe_score": 0.7916568517684937, "metricx_score": 2.3795180320739746, "metricx_qe_score": 3.2253730297088623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净的标签数据才能正常工作,获取干净的验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 51.75187728209088, "chrf_score": 47.60524315980074, "xcomet_score": 0.8709638118743896, "xcomet_qe_score": 0.8544896841049194, "metricx_score": 2.532155990600586, "metricx_qe_score": 2.783010721206665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净的验证样本数量有助于WSL方法取得更好的性能,如图左所示。", "metrics": {"bleu_score": 47.08016813607621, "chrf_score": 42.30763840571915, "xcomet_score": 0.9215213656425476, "xcomet_qe_score": 0.9357224106788635, "metricx_score": 3.6619205474853516, "metricx_qe_score": 4.247870445251465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每个类别 20 个样本就能达到高性能。", "metrics": {"bleu_score": 26.512298021756173, "chrf_score": 24.35864859777903, "xcomet_score": 0.9391908049583435, "xcomet_qe_score": 0.9404983520507812, "metricx_score": 1.5615034103393555, "metricx_qe_score": 1.9258317947387695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这还不是故事的全部,因为如果我们无论如何决定使用干净的样本,那么直接在这些样本上进行训练甚至会取得更好的性能。", "metrics": {"bleu_score": 33.45316263289198, "chrf_score": 29.32131002169592, "xcomet_score": 0.9325203895568848, "xcomet_qe_score": 0.8909505605697632, "metricx_score": 4.085369110107422, "metricx_qe_score": 3.9738214015960693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "红色数字显示了在干净数据上直接应用的微调方法与仅将干净数据用于验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 52.03106746785858, "chrf_score": 46.88221718460579, "xcomet_score": 0.7853943109512329, "xcomet_qe_score": 0.7986895442008972, "metricx_score": 3.4396719932556152, "metricx_qe_score": 3.5021040439605713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如我们所见,如果每个类别有 10 个样本,直接微调开始优于 WSL 方法。", "metrics": {"bleu_score": 38.37912924021692, "chrf_score": 35.87967097598753, "xcomet_score": 0.9564950466156006, "xcomet_qe_score": 0.9125205278396606, "metricx_score": 1.6366592645645142, "metricx_qe_score": 2.8166680335998535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前WSL方法中声称的性能提升可以通过允许在干净的验证样本上继续微调来轻松实现。 从数据中", "metrics": {"bleu_score": 42.92840035036132, "chrf_score": 40.76220935675104, "xcomet_score": 0.773608922958374, "xcomet_qe_score": 0.7383017539978027, "metricx_score": 5.212018013000488, "metricx_qe_score": 4.963104248046875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,Marlina 模型最初称为 FTW,其性能不如更复杂的 WSL 方法(如余弦)。", "metrics": {"bleu_score": 27.790538038724762, "chrf_score": 25.975156666660915, "xcomet_score": 0.6103166341781616, "xcomet_qe_score": 0.5802090167999268, "metricx_score": 8.110559463500977, "metricx_qe_score": 8.3142671585083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果我们允许在干净样本上继续微调,那么 FTW 的表现与其他方法一样好。", "metrics": {"bleu_score": 54.19642316694007, "chrf_score": 46.26595815799744, "xcomet_score": 0.9007749557495117, "xcomet_qe_score": 0.7830424904823303, "metricx_score": 1.5407371520996094, "metricx_qe_score": 2.2880804538726807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的 WSL 方法,因为这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 54.70197181654885, "chrf_score": 54.979716663412724, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8066851496696472, "metricx_qe_score": 1.118506669998169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们表明,最近的WSL方法需要干净的手动标注样本才能正常工作。", "metrics": {"bleu_score": 51.67237045963152, "chrf_score": 47.90674164333587, "xcomet_score": 0.7988414764404297, "xcomet_qe_score": 0.8052822947502136, "metricx_score": 2.4959146976470947, "metricx_qe_score": 3.1425559520721436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下:", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 53.22177822177822, "xcomet_score": 0.9982490539550781, "xcomet_qe_score": 0.9813262224197388, "metricx_score": 0.2969313859939575, "metricx_qe_score": 0.2585373520851135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择的标准。", "metrics": {"bleu_score": 52.664038784792666, "chrf_score": 46.9183669176611, "xcomet_score": 0.9912177324295044, "xcomet_qe_score": 0.9179310202598572, "metricx_score": 0.2977628707885742, "metricx_qe_score": 0.5873326063156128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否使用干净的验证样本。", "metrics": {"bleu_score": 43.546195754004344, "chrf_score": 37.031167828539765, "xcomet_score": 0.899816632270813, "xcomet_qe_score": 0.841971755027771, "metricx_score": 1.135668396949768, "metricx_qe_score": 2.156416654586792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL 方法应与未来着陆基线进行比较,因为两者都基于网格样本。", "metrics": {"bleu_score": 32.35139896023531, "chrf_score": 30.10485209610893, "xcomet_score": 0.6361971497535706, "xcomet_qe_score": 0.618749737739563, "metricx_score": 6.282632350921631, "metricx_qe_score": 7.173033237457275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一种简单但强大的基线,应在未来的 WSL 工作中加以考虑。", "metrics": {"bleu_score": 38.267304752939, "chrf_score": 32.01063049853373, "xcomet_score": 0.8958289623260498, "xcomet_qe_score": 0.7250081300735474, "metricx_score": 1.3078113794326782, "metricx_qe_score": 2.555884599685669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过此幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9951430559158325, "xcomet_qe_score": 0.9870158433914185, "metricx_score": 0.48675400018692017, "metricx_qe_score": 0.44404107332229614, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,加入会议。", "metrics": {"bleu_score": 7.030417713400723, "chrf_score": 7.071097372488409, "xcomet_score": 0.5098682641983032, "xcomet_qe_score": 0.7477270364761353, "metricx_score": 4.464794635772705, "metricx_qe_score": 3.484989881515503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 9.993248618647176, "xcomet_score": 0.8700615763664246, "xcomet_qe_score": 0.6193946599960327, "metricx_score": 1.1073329448699951, "metricx_qe_score": 0.9096816182136536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "芬奇,我是萨拉·芬奇。", "metrics": {"bleu_score": 8.29519350710986, "chrf_score": 5.405070919696089, "xcomet_score": 0.6480262279510498, "xcomet_qe_score": 0.7325373888015747, "metricx_score": 4.678750514984131, "metricx_qe_score": 5.4248833656311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向大家介绍ABCEval,这是一种全新的评估对话式人工智能的维度方法。", "metrics": {"bleu_score": 39.94463928700864, "chrf_score": 40.82500039156529, "xcomet_score": 0.890528678894043, "xcomet_qe_score": 0.9462633728981018, "metricx_score": 1.9511057138442993, "metricx_qe_score": 1.6145200729370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是由埃默里大学的乔伊斯·乔伊斯教授领导的埃默里大学自然语言处理实验室完成的,并与亚马逊Alexa AI合作完成的。", "metrics": {"bleu_score": 37.854950099837254, "chrf_score": 41.67219402819374, "xcomet_score": 0.7159808874130249, "xcomet_qe_score": 0.7280460000038147, "metricx_score": 5.698080062866211, "metricx_qe_score": 4.778361797332764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚刚开发了一个对话模型,你想看看它与当前的先进技术相比表现如何。", "metrics": {"bleu_score": 77.85704580939215, "chrf_score": 68.66022437144954, "xcomet_score": 0.9983288049697876, "xcomet_qe_score": 0.9891366958618164, "metricx_score": 0.5082075595855713, "metricx_qe_score": 0.6062808036804199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常的做法是使用人工评估,例如让人工评判员选择两个对话中哪一个更好,或者根据一个连续尺度对对话进行评分。", "metrics": {"bleu_score": 54.53168858598131, "chrf_score": 48.69541482714198, "xcomet_score": 0.8243534564971924, "xcomet_qe_score": 0.8235156536102295, "metricx_score": 3.6566925048828125, "metricx_qe_score": 4.328089237213135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的全面评估方面效果良好,但对话质量有许多方面。", "metrics": {"bleu_score": 36.6474865901556, "chrf_score": 30.76041475589639, "xcomet_score": 0.9300557971000671, "xcomet_qe_score": 0.9080116152763367, "metricx_score": 0.547738790512085, "metricx_qe_score": 0.7834680676460266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以便更细致地了解模型的优势和劣势。", "metrics": {"bleu_score": 60.18791526268261, "chrf_score": 57.64104316676456, "xcomet_score": 0.9830609560012817, "xcomet_qe_score": 0.9697476625442505, "metricx_score": 0.5960354804992676, "metricx_qe_score": 0.7260866165161133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人类评判者使用现有的比较方法或李克特量表方法,对对话质量的几个方面进行评估,例如模型响应的相关性。", "metrics": {"bleu_score": 62.043577266165066, "chrf_score": 56.22389519247173, "xcomet_score": 0.9743943214416504, "xcomet_qe_score": 0.9618710279464722, "metricx_score": 1.1810446977615356, "metricx_qe_score": 1.9127708673477173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们相信存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 40.00853590887389, "xcomet_score": 0.8991663455963135, "xcomet_qe_score": 0.8668703436851501, "metricx_score": 1.396376132965088, "metricx_qe_score": 1.407410740852356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表达了某些行为(如提供无关信息或自相矛盾),来减少人类评估的主观性。", "metrics": {"bleu_score": 57.935727943519815, "chrf_score": 49.08787643519166, "xcomet_score": 0.9603960514068604, "xcomet_qe_score": 0.9652961492538452, "metricx_score": 1.5284348726272583, "metricx_qe_score": 2.1072819232940674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为聊天行为标注或简而言之为 ABC 评估。", "metrics": {"bleu_score": 33.25778988272398, "chrf_score": 28.909699035119502, "xcomet_score": 0.777804970741272, "xcomet_qe_score": 0.7832255363464355, "metricx_score": 1.6362011432647705, "metricx_qe_score": 1.152894377708435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 69.13415687782464, "chrf_score": 60.70058985691518, "xcomet_score": 0.9336752891540527, "xcomet_qe_score": 0.9540376663208008, "metricx_score": 1.4507877826690674, "metricx_qe_score": 2.841010570526123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC评估能够衡量聊天模型犯下各种主题错误的比率。", "metrics": {"bleu_score": 62.00657885072486, "chrf_score": 52.12986518394428, "xcomet_score": 0.7413873672485352, "xcomet_qe_score": 0.6939934492111206, "metricx_score": 3.080937385559082, "metricx_qe_score": 3.6522364616394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如, APCEval 衡量的是聊天模型忽略对话伙伴或说出无关内容的次数。 当模型自相矛盾或与其伙伴自相矛盾,产生错误的事实或违反常识,以及当模型成功或未能表现出同理心时。", "metrics": {"bleu_score": 39.675262662274214, "chrf_score": 32.83500433640425, "xcomet_score": 0.6064264178276062, "xcomet_qe_score": 0.5599764585494995, "metricx_score": 4.492300987243652, "metricx_qe_score": 5.281020641326904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方法最有效,我们选择了四种最先进的聊天模型,并使用ABCEval对每种模型进行了100次人机对话的评估。", "metrics": {"bleu_score": 60.64845316074772, "chrf_score": 58.33994252837014, "xcomet_score": 0.9403164386749268, "xcomet_qe_score": 0.9527989625930786, "metricx_score": 0.9920477867126465, "metricx_qe_score": 1.1953787803649902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了进行比较,我们还使用三种现有方法对这些对话进行了评估:对话段落级别的液体评分、对话级别的液体评分以及对话级别配对比较。", "metrics": {"bleu_score": 42.85393482088186, "chrf_score": 36.05407815793859, "xcomet_score": 0.5623047351837158, "xcomet_qe_score": 0.664027214050293, "metricx_score": 8.6145601272583, "metricx_qe_score": 7.442007064819336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每种现有方法,我们收集了对对话中最常见的八个方面的评价,因为这是在多个维度上评估聊天模型的标准做法。 ", "metrics": {"bleu_score": 53.90532958113517, "chrf_score": 44.88685079779501, "xcomet_score": 0.9227074980735779, "xcomet_qe_score": 0.8645337820053101, "metricx_score": 1.0335311889648438, "metricx_qe_score": 1.1082043647766113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现 ABC 评估行为标签在 100 个双重标签对话的标注者间一致性上,总体比现有方法收集的标签更可靠。", "metrics": {"bleu_score": 45.3909078450672, "chrf_score": 37.9917951895394, "xcomet_score": 0.7511157989501953, "xcomet_qe_score": 0.7919763326644897, "metricx_score": 4.6883015632629395, "metricx_qe_score": 5.263982772827148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,如本简单线性回归分析所示,与现有方法产生的指标相比,ABC评估标签更能预测整体对话质量。", "metrics": {"bleu_score": 51.935270960661036, "chrf_score": 42.65076441941393, "xcomet_score": 0.9592896699905396, "xcomet_qe_score": 0.9637390375137329, "metricx_score": 1.3326972723007202, "metricx_qe_score": 0.9233465194702148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到,通过测量自我与伴侣矛盾的比例,可以分别解释 5% 和 10% 的对话质量,而平均酒精度数只解释了 4% 或更少。", "metrics": {"bleu_score": 41.06921393745124, "chrf_score": 36.314373316514704, "xcomet_score": 0.36100974678993225, "xcomet_qe_score": 0.35276851058006287, "metricx_score": 10.69289779663086, "metricx_qe_score": 10.259140014648438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查每个评估指标是否捕捉到了聊天质量的独特方面。 您", "metrics": {"bleu_score": 73.82945973307393, "chrf_score": 68.51481160506273, "xcomet_score": 0.8001829981803894, "xcomet_qe_score": 0.7955945134162903, "metricx_score": 4.396477699279785, "metricx_qe_score": 1.8730541467666626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,所有 ABC 评估指标的组合解释了超过 25% 的对话质量。随着您逐一移除这些指标,大多数指标都会导致丢失大量关于质量的信息。", "metrics": {"bleu_score": 47.03279142943736, "chrf_score": 44.27807500824816, "xcomet_score": 0.7387604713439941, "xcomet_qe_score": 0.8420262932777405, "metricx_score": 2.110525608062744, "metricx_qe_score": 2.957304000854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有转折水平液体指标的组合解释的质量远少,这些指标中只有少数携带独特信息。 这些可靠、信息丰", "metrics": {"bleu_score": 20.520135490129185, "chrf_score": 18.19617159994565, "xcomet_score": 0.2513968050479889, "xcomet_qe_score": 0.4120200276374817, "metricx_score": 14.947525024414062, "metricx_qe_score": 14.121601104736328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "富且独特的 ABC 评估指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。", "metrics": {"bleu_score": 3.684169281403287, "chrf_score": 7.501123364488688, "xcomet_score": 0.44609424471855164, "xcomet_qe_score": 0.6564959287643433, "metricx_score": 5.4570417404174805, "metricx_qe_score": 4.702591896057129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从实验结果中可以看出,我们仍然面临着一些挑战,并且这些挑战已经被精确量化。", "metrics": {"bleu_score": 25.96093486503439, "chrf_score": 29.43825164767908, "xcomet_score": 0.9950546026229858, "xcomet_qe_score": 0.9957865476608276, "metricx_score": 0.5867394208908081, "metricx_qe_score": 0.5005355477333069, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人大约有 20% 的回答违反常识。", "metrics": {"bleu_score": 46.75624312795152, "chrf_score": 41.922016324583176, "xcomet_score": 0.9844323396682739, "xcomet_qe_score": 0.9734805822372437, "metricx_score": 0.6258295774459839, "metricx_qe_score": 1.2315946817398071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在约 15% 的回答中,他们提供的信息与主题无关,在约 10% 的时间里,他们会自相矛盾或与伴侣矛盾。", "metrics": {"bleu_score": 29.544855895464266, "chrf_score": 27.606278082934903, "xcomet_score": 0.7863726615905762, "xcomet_qe_score": 0.7994256019592285, "metricx_score": 4.23848819732666, "metricx_qe_score": 2.838326930999756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速发展,自我们进行评估以来,许多错误率在新发布的模型中可能会下降。", "metrics": {"bleu_score": 62.246345434473554, "chrf_score": 53.747302102565264, "xcomet_score": 0.931909441947937, "xcomet_qe_score": 0.9739501476287842, "metricx_score": 1.8060520887374878, "metricx_qe_score": 1.7401046752929688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更增加了追求可靠和精确的评估指标以比较模型的必要性。", "metrics": {"bleu_score": 34.31535116220621, "chrf_score": 29.381125862620273, "xcomet_score": 0.9990153312683105, "xcomet_qe_score": 0.9935992956161499, "metricx_score": 1.153178095817566, "metricx_qe_score": 1.1661124229431152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望该领域的其他研究人员能够利用 ABC 评估方法,将其作为朝着这一方向迈出的有意义的一步,", "metrics": {"bleu_score": 42.56539103069501, "chrf_score": 36.42274733192828, "xcomet_score": 0.9797563552856445, "xcomet_qe_score": 0.9829357862472534, "metricx_score": 1.5371077060699463, "metricx_qe_score": 1.4289789199829102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待在未来几个月和几年内看到对话式人工智能的进步。", "metrics": {"bleu_score": 48.55332614117323, "chrf_score": 42.979458104735755, "xcomet_score": 0.999711275100708, "xcomet_qe_score": 0.9981229305267334, "metricx_score": 0.7593169212341309, "metricx_qe_score": 0.7914260029792786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Kyo Yin,今天我将介绍我们的作品《何时需要数据", "metrics": {"bleu_score": 31.447160768560497, "chrf_score": 31.502465231491794, "xcomet_score": 0.6805530786514282, "xcomet_qe_score": 0.6861094236373901, "metricx_score": 7.179515838623047, "metricx_qe_score": 4.437542915344238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "驱动的多语言探索进行翻译?》。", "metrics": {"bleu_score": 52.025568808075846, "chrf_score": 57.18751629046622, "xcomet_score": 0.34637123346328735, "xcomet_qe_score": 0.20888514816761017, "metricx_score": 9.962193489074707, "metricx_qe_score": 8.668841361999512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernandes、Emily Liu、Andre FD Martins和Graham Newbig合作完成的。 因此", "metrics": {"bleu_score": 32.18322289076161, "chrf_score": 68.73043389789298, "xcomet_score": 0.646511971950531, "xcomet_qe_score": 0.672890305519104, "metricx_score": 5.600740909576416, "metricx_qe_score": 2.3702759742736816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",很多翻译都取决于上下文。", "metrics": {"bleu_score": 67.03420896351791, "chrf_score": 64.8171272760201, "xcomet_score": 0.9983985424041748, "xcomet_qe_score": 0.9895898103713989, "metricx_score": 0.8651986122131348, "metricx_qe_score": 1.009296178817749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译这个句子中的“mole”?", "metrics": {"bleu_score": 54.017258985951415, "chrf_score": 55.419633882775734, "xcomet_score": 0.9968364238739014, "xcomet_qe_score": 0.967570424079895, "metricx_score": 0.9834437966346741, "metricx_qe_score": 1.9881327152252197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好吧,如果前一句话是如果部长们知道了,事情可能会变得危险,那么 Moe 指的是间谍。", "metrics": {"bleu_score": 15.305170438463067, "chrf_score": 7.558611742676699, "xcomet_score": 0.8274796009063721, "xcomet_qe_score": 0.8196643590927124, "metricx_score": 5.090836524963379, "metricx_qe_score": 6.398028373718262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是如果前一句话是医生,这可能是严重的事情吗?那么 Moe 指的是一个胎记。", "metrics": {"bleu_score": 14.793632995985538, "chrf_score": 10.115691925168766, "xcomet_score": 0.7158767580986023, "xcomet_qe_score": 0.6929364204406738, "metricx_score": 6.2706170082092285, "metricx_qe_score": 6.417042255401611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,这个词的含义会发生变化,因此它的翻译也会随之改变。", "metrics": {"bleu_score": 37.52957402179448, "chrf_score": 31.180333177261705, "xcomet_score": 0.9893181324005127, "xcomet_qe_score": 0.9824478030204773, "metricx_score": 0.374392032623291, "metricx_qe_score": 0.3960738778114319, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在这种情况下翻译的准确性非常困难。", "metrics": {"bleu_score": 52.218108687027346, "chrf_score": 45.68745958466591, "xcomet_score": 0.9625402688980103, "xcomet_qe_score": 0.9156385660171509, "metricx_score": 0.6798505783081055, "metricx_qe_score": 0.821022629737854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有小部分翻译依赖于上下文,这使得像 BLEU 这样的语料库级指标无法捕捉到这些翻译。", "metrics": {"bleu_score": 52.10710032512805, "chrf_score": 45.17611517338166, "xcomet_score": 0.9907901287078857, "xcomet_qe_score": 0.9751671552658081, "metricx_score": 1.1748758554458618, "metricx_qe_score": 1.8770203590393066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对上下文相关的翻译进行有针对性的评估,但这些资源只能支持有限类型的上下文相关翻译和有限的语言集合,因为它们通常依赖于领域知识和人工整理。", "metrics": {"bleu_score": 72.4188780677129, "chrf_score": 66.6688897965523, "xcomet_score": 0.941132664680481, "xcomet_qe_score": 0.9480706453323364, "metricx_score": 1.2406362295150757, "metricx_qe_score": 1.0529228448867798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型如何处理这些情况?", "metrics": {"bleu_score": 32.74135267450808, "chrf_score": 30.12273108277076, "xcomet_score": 0.9986907243728638, "xcomet_qe_score": 0.9914894104003906, "metricx_score": 0.7201682925224304, "metricx_qe_score": 1.0261601209640503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了一个词在翻译过程中对上下文依赖的程度。", "metrics": {"bleu_score": 63.371335735160805, "chrf_score": 56.56592003103389, "xcomet_score": 0.9918341636657715, "xcomet_qe_score": 0.9951705932617188, "metricx_score": 4.015621185302734, "metricx_qe_score": 4.052764892578125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的研究中,我们介绍了CXMI作为机器翻译模型上下文使用的度量方法。", "metrics": {"bleu_score": 60.10163577618584, "chrf_score": 59.311837565889824, "xcomet_score": 0.9127119183540344, "xcomet_qe_score": 0.9069952368736267, "metricx_score": 1.4372879266738892, "metricx_qe_score": 2.059953212738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过测量上下文C在给定源X的情况下,对目标Y提供了多少信息来实现这一目标。 可以将 CXMI 视为通过为模型提供上下文而获得的信息。", "metrics": {"bleu_score": 63.808224239148444, "chrf_score": 59.44932283769897, "xcomet_score": 0.8969534635543823, "xcomet_qe_score": 0.7714623808860779, "metricx_score": 3.2188310623168945, "metricx_qe_score": 3.852140426635742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将 CXMI 扩展为逐点 CXMI,它可以在句子级别或词级别上衡量上下文的使用。", "metrics": {"bleu_score": 37.83134586538111, "chrf_score": 32.52838997784884, "xcomet_score": 0.8379313945770264, "xcomet_qe_score": 0.7948203086853027, "metricx_score": 2.465311050415039, "metricx_qe_score": 3.113854169845581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将 PSXMI 值高的词语视为需要上下文进行翻译的词语。", "metrics": {"bleu_score": 70.94521095075528, "chrf_score": 68.90856594502488, "xcomet_score": 0.9006674289703369, "xcomet_qe_score": 0.9088047742843628, "metricx_score": 2.57420015335083, "metricx_qe_score": 3.1930899620056152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们分析具有高 PCXMI 的词语,以寻找这些词语之间的模式。", "metrics": {"bleu_score": 36.39723552007911, "chrf_score": 41.58248595343053, "xcomet_score": 0.9716205596923828, "xcomet_qe_score": 0.9605770111083984, "metricx_score": 1.6213035583496094, "metricx_qe_score": 3.4425742626190186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成十四种不同语言的 TED 演讲稿进行了分析。", "metrics": {"bleu_score": 60.77315125820352, "chrf_score": 53.64558244279337, "xcomet_score": 0.9962953329086304, "xcomet_qe_score": 0.9949482679367065, "metricx_score": 0.8973590731620789, "metricx_qe_score": 1.2817814350128174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同层次上进行分析。", "metrics": {"bleu_score": 69.01228050062707, "chrf_score": 58.83204419988524, "xcomet_score": 0.995771050453186, "xcomet_qe_score": 0.9874769449234009, "metricx_score": 0.2352285534143448, "metricx_qe_score": 0.38425329327583313, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有高均值 PCXMI 的词性标签。", "metrics": {"bleu_score": 34.6697783111003, "chrf_score": 31.708611730813757, "xcomet_score": 0.8907898664474487, "xcomet_qe_score": 0.8235342502593994, "metricx_score": 2.2274813652038574, "metricx_qe_score": 3.328263282775879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够在阿拉伯语中找到具有相对较高 p six mi 的双重代词。", "metrics": {"bleu_score": 43.87328902288626, "chrf_score": 30.667207149160276, "xcomet_score": 0.6837571859359741, "xcomet_qe_score": 0.6880178451538086, "metricx_score": 6.950274467468262, "metricx_qe_score": 7.296731948852539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,英语没有双重代词。因此,在翻译成阿拉伯语时,你需要上下文来确定代词是否是双重代词。", "metrics": {"bleu_score": 55.28939464199153, "chrf_score": 47.294812842405506, "xcomet_score": 0.8100602626800537, "xcomet_qe_score": 0.9893784523010254, "metricx_score": 2.224172830581665, "metricx_qe_score": 1.7678848505020142, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现,在选择适当的动词形式时,某些语言也需要上下文。", "metrics": {"bleu_score": 68.08663291835767, "chrf_score": 61.18430263298716, "xcomet_score": 0.9977996349334717, "xcomet_qe_score": 0.989017128944397, "metricx_score": 0.5972544550895691, "metricx_qe_score": 0.898543119430542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们查看了在所有不同情况下 p/seksually 平均值较高的词汇项目。", "metrics": {"bleu_score": 35.43345370054533, "chrf_score": 27.699955299067412, "xcomet_score": 0.7233051061630249, "xcomet_qe_score": 0.6778903007507324, "metricx_score": 8.42802619934082, "metricx_qe_score": 8.90642261505127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的情况,在中文中,你需要上下文来翻译专有名词,以确保你在文档中使用相同的翻译。", "metrics": {"bleu_score": 35.564869740332654, "chrf_score": 30.813077853067316, "xcomet_score": 0.8592212200164795, "xcomet_qe_score": 0.9212212562561035, "metricx_score": 0.9703196287155151, "metricx_qe_score": 1.2445063591003418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现上下文有助于以适当的正式程度进行翻译。", "metrics": {"bleu_score": 29.063431023881783, "chrf_score": 28.63701351504888, "xcomet_score": 0.9416265487670898, "xcomet_qe_score": 0.9299638867378235, "metricx_score": 0.7672496438026428, "metricx_qe_score": 0.8377513885498047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看了 p6mi 值较高的不同个体标记。", "metrics": {"bleu_score": 17.92334464048543, "chrf_score": 14.388673271244532, "xcomet_score": 0.7058849930763245, "xcomet_qe_score": 0.657647967338562, "metricx_score": 6.825473785400391, "metricx_qe_score": 6.160968780517578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出无法通过单词本身捕捉到的现象,但这些现象通常以标准结构(如省略号解析)", "metrics": {"bleu_score": 35.64969076931938, "chrf_score": 30.726174984214627, "xcomet_score": 0.6704568266868591, "xcomet_qe_score": 0.6449271440505981, "metricx_score": 2.5431172847747803, "metricx_qe_score": 2.3807363510131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表达出来。 因此,我们现在利用分析结果来设计一个文档级翻译基准。", "metrics": {"bleu_score": 39.09443137613962, "chrf_score": 36.35694342441489, "xcomet_score": 0.7569459676742554, "xcomet_qe_score": 0.6546919345855713, "metricx_score": 5.595064640045166, "metricx_qe_score": 7.221975326538086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们所识别的五个不和谐现象中的每一个,我们都创建了标记器,以便自动识别与该现象相关的词语。", "metrics": {"bleu_score": 49.10878070124391, "chrf_score": 43.03464177636794, "xcomet_score": 0.8220241665840149, "xcomet_qe_score": 0.7612274885177612, "metricx_score": 2.639735221862793, "metricx_qe_score": 2.494357109069824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称我们的标记器为多语言语境感知标记器,或简称 MUDA 标记器。", "metrics": {"bleu_score": 18.36202042420681, "chrf_score": 25.71716415165507, "xcomet_score": 0.9691854119300842, "xcomet_qe_score": 0.9508364796638489, "metricx_score": 0.7854582071304321, "metricx_qe_score": 0.8628472685813904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同的语言对这些离散现象的比例不同。", "metrics": {"bleu_score": 30.148756976045824, "chrf_score": 27.279067455141004, "xcomet_score": 0.8609069585800171, "xcomet_qe_score": 0.8684272170066833, "metricx_score": 4.721158027648926, "metricx_qe_score": 4.26427698135376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们使用 MUDA 标签器,将其应用于我们想要用于评估的平行语料库,并根据 MUDA 标签器识别的上下文相关示例,应用我们选择的翻译指标。", "metrics": {"bleu_score": 53.21760373373729, "chrf_score": 49.18603994347953, "xcomet_score": 0.9489254951477051, "xcomet_qe_score": 0.807744562625885, "metricx_score": 1.0320872068405151, "metricx_qe_score": 1.2152667045593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译上的表现。", "metrics": {"bleu_score": 74.38252804209344, "chrf_score": 72.81560651989159, "xcomet_score": 0.9105273485183716, "xcomet_qe_score": 0.8643457889556885, "metricx_score": 0.9695982933044434, "metricx_qe_score": 1.057651400566101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,对于 Blue,我们发现复杂的非特定模型性能最好。", "metrics": {"bleu_score": 36.51761220034163, "chrf_score": 29.854535611277306, "xcomet_score": 0.8279504776000977, "xcomet_qe_score": 0.7576134204864502, "metricx_score": 4.654950141906738, "metricx_qe_score": 4.330654144287109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果我们使用彗星模型,情境感知模型表现最好。", "metrics": {"bleu_score": 44.52675711169778, "chrf_score": 31.43259438619656, "xcomet_score": 0.7603327035903931, "xcomet_qe_score": 0.7489351034164429, "metricx_score": 2.6724205017089844, "metricx_qe_score": 2.0248355865478516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用词频测量,那么有无情境模型的性能相当。", "metrics": {"bleu_score": 28.077701802435026, "chrf_score": 25.281222113672147, "xcomet_score": 0.7907623052597046, "xcomet_qe_score": 0.7797962427139282, "metricx_score": 5.227788925170898, "metricx_qe_score": 3.864609479904175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果我们仅使用企业级指标,就很难确定最佳的文档级翻译系统。", "metrics": {"bleu_score": 71.3823086919509, "chrf_score": 63.77915821387516, "xcomet_score": 0.869337797164917, "xcomet_qe_score": 0.7547849416732788, "metricx_score": 4.123505592346191, "metricx_qe_score": 4.611565113067627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们使用 MUDA 基准来评估模型,发现对于某些话语现象,如正式程度和词汇连贯性,考虑上下文关系的模型比不考虑上下文的模型要准确得多。", "metrics": {"bleu_score": 56.017484578407675, "chrf_score": 51.813865784777214, "xcomet_score": 0.9800143241882324, "xcomet_qe_score": 0.97300124168396, "metricx_score": 1.2976226806640625, "metricx_qe_score": 1.4693868160247803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在处理其他现象(如省略号、代词和动词形式)时,并没有比不使用上下文的模型好多少。", "metrics": {"bleu_score": 55.93596420162442, "chrf_score": 51.24652202648598, "xcomet_score": 0.9898190498352051, "xcomet_qe_score": 0.9635958671569824, "metricx_score": 0.6886937618255615, "metricx_qe_score": 0.8176777362823486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明我们需要在文档级翻译方面取得更大的进展。", "metrics": {"bleu_score": 30.099273730403407, "chrf_score": 29.85067222231018, "xcomet_score": 0.9984045028686523, "xcomet_qe_score": 0.9896292686462402, "metricx_score": 0.7432674169540405, "metricx_qe_score": 0.7704477310180664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试表明,DeepBell在文档级翻译方面通常比Google翻译更准确。", "metrics": {"bleu_score": 66.33503414981791, "chrf_score": 53.863723323892444, "xcomet_score": 0.8303737640380859, "xcomet_qe_score": 0.6605871915817261, "metricx_score": 4.513399600982666, "metricx_qe_score": 4.625774383544922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们对十四对语言对进行了数据驱动分析,以确定何时需要上下文进行翻译。 然后,我们将研究结果用于构建文档级机器翻译的基准,这有助于我们确定哪些离散现象模型能够很好地处理,哪些翻译系统擅长文档级翻译。", "metrics": {"bleu_score": 47.26811553087909, "chrf_score": 40.94689878182366, "xcomet_score": 0.7189368605613708, "xcomet_qe_score": 0.7718038558959961, "metricx_score": 4.104392051696777, "metricx_qe_score": 4.692657947540283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多伦多见。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9990917444229126, "xcomet_qe_score": 0.985295832157135, "metricx_score": 0.46565622091293335, "metricx_qe_score": 1.37757408618927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Yanis Lavrack,今天我将向大家介绍我们的作品——Dr. Berth,这是一种针对生物医学和临床领域的强大法语预训练模型。", "metrics": {"bleu_score": 44.05919662949964, "chrf_score": 44.31904112086237, "xcomet_score": 0.6667928695678711, "xcomet_qe_score": 0.6727345585823059, "metricx_score": 2.132209062576294, "metricx_qe_score": 2.218273162841797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本演示中,我们首先讨论医疗保健中的语言建模。", "metrics": {"bleu_score": 41.163637617755036, "chrf_score": 32.89604441129711, "xcomet_score": 0.9821245670318604, "xcomet_qe_score": 0.9837895631790161, "metricx_score": 1.8712214231491089, "metricx_qe_score": 1.923445463180542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们推出了第一个法语生物医学模型 Dr. Berth,该模型基于 Roberta,并在 Natchios 上进行训练,Natchios 是一个从网络上抓取的医学数据集。", "metrics": {"bleu_score": 40.17703288243513, "chrf_score": 28.57035110292655, "xcomet_score": 0.6840500831604004, "xcomet_qe_score": 0.5872613191604614, "metricx_score": 3.712822437286377, "metricx_qe_score": 4.0256452560424805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了多种钚设置和数据源的模型比较。", "metrics": {"bleu_score": 63.82695246863555, "chrf_score": 59.721589227203154, "xcomet_score": 0.7898749709129333, "xcomet_qe_score": 0.7971571087837219, "metricx_score": 4.827273368835449, "metricx_qe_score": 5.294238567352295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们用法语介绍了我们在十一项生物医学和临床下游任务上的研究结果。", "metrics": {"bleu_score": 50.7196093945688, "chrf_score": 47.64318875447787, "xcomet_score": 0.8579475283622742, "xcomet_qe_score": 0.8627499341964722, "metricx_score": 1.3249512910842896, "metricx_qe_score": 1.6291790008544922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结了实验结果,并为您提供了更多关于如何访问模型的详细信息。", "metrics": {"bleu_score": 42.202346209160325, "chrf_score": 34.787764377142395, "xcomet_score": 0.9818944931030273, "xcomet_qe_score": 0.9641553163528442, "metricx_score": 0.28331276774406433, "metricx_qe_score": 0.2553825080394745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自 2018 年发布以来,BERT 已成为解决自然语言处理任务的最有效方法之一,相比传统的静态和情境化方法(如词向量、快速文本或注册),BERT 的性能提升显著。", "metrics": {"bleu_score": 43.237524333315434, "chrf_score": 39.237655776970605, "xcomet_score": 0.7244712114334106, "xcomet_qe_score": 0.7361449003219604, "metricx_score": 4.554641246795654, "metricx_qe_score": 4.796374797821045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从那时起,该模型已被移植到许多其他语言中,例如法语中的 Camembert,以及生物医学领域的 Permette Bert 和 BioBert,以及临床领域的 Clinical Bert,但主要还是在英语中。", "metrics": {"bleu_score": 37.8959910843773, "chrf_score": 39.827702489175124, "xcomet_score": 0.5982141494750977, "xcomet_qe_score": 0.5530464053153992, "metricx_score": 6.297663688659668, "metricx_qe_score": 6.345610618591309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专用模型很少,而且由于缺乏领域内数据,通常是基于持续的假装。", "metrics": {"bleu_score": 36.679542611397764, "chrf_score": 29.349592355889502, "xcomet_score": 0.7618676424026489, "xcomet_qe_score": 0.746496319770813, "metricx_score": 4.298620223999023, "metricx_qe_score": 4.932162284851074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法国一直没有开源的现代生物医学研究平台。", "metrics": {"bleu_score": 14.973422121848623, "chrf_score": 16.10948083204386, "xcomet_score": 0.8431289196014404, "xcomet_qe_score": 0.8402550220489502, "metricx_score": 1.8914278745651245, "metricx_qe_score": 1.6743412017822266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们自问,对于广泛的用途,最合适的资料来源是什么?而这些现有资料是对临床资料的良好替代。", "metrics": {"bleu_score": 15.829860807960126, "chrf_score": 17.50834706775864, "xcomet_score": 0.8129739165306091, "xcomet_qe_score": 0.8066361546516418, "metricx_score": 2.4929356575012207, "metricx_qe_score": 2.0194942951202393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将伯特博士与我们的舒伯特模型进行比较,后者基于我们所拥有的非大学医院的匿名数据。", "metrics": {"bleu_score": 43.84356937492207, "chrf_score": 30.532478441824367, "xcomet_score": 0.5472668409347534, "xcomet_qe_score": 0.5813130140304565, "metricx_score": 5.146444320678711, "metricx_qe_score": 5.7642927169799805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,我们会问自己,我们需要多少数据来训练一个专门处理法语数据的模型?", "metrics": {"bleu_score": 44.88382125068511, "chrf_score": 45.77813401309331, "xcomet_score": 0.9942398071289062, "xcomet_qe_score": 0.922757089138031, "metricx_score": 0.7929332852363586, "metricx_qe_score": 0.7237957715988159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是 4 GB、8 GB 还是更多?", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 90.21205646205644, "xcomet_score": 0.9791398048400879, "xcomet_qe_score": 0.9612053036689758, "metricx_score": 0.2863093912601471, "metricx_qe_score": 0.6383348107337952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四种从头开始的模型。第一种是 Dr. Bert 的第一版,使用了 7 GB 的 Nachos 数据集;第二种是 Dr. Bert 的第二版,使用了 4 GB 的 Nachos 数据集子集。 第一版舒伯特是一个临床模型,包含来自临床节点的4GB句子。最终版本的舒伯特则结合了4GB的自然语料和4GB的临床节点。", "metrics": {"bleu_score": 29.217356485596483, "chrf_score": 27.647277546879735, "xcomet_score": 0.42093509435653687, "xcomet_qe_score": 0.4257544279098511, "metricx_score": 5.8626508712768555, "metricx_qe_score": 5.706883430480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还引入了三个在持续预训练上进行训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 58.211133129365905, "chrf_score": 49.37249203455878, "xcomet_score": 0.866234540939331, "xcomet_qe_score": 0.8475589752197266, "metricx_score": 2.3943064212799072, "metricx_qe_score": 3.161935567855835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于 Camembert 的模型,训练数据为 4GB 的 nacho 数据集;", "metrics": {"bleu_score": 15.488182639395934, "chrf_score": 25.83278219150443, "xcomet_score": 0.8003793954849243, "xcomet_qe_score": 0.5202885866165161, "metricx_score": 3.339905261993408, "metricx_qe_score": 3.9947586059570312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也是基于 Camembert 的模型,但这次训练数据为 4GB 的 Klinker Lots 数据集。 最后,还有一个基于英语生物医学模型的模型,BMLB,在 4 GB 的 Snatchers 上进行训练。", "metrics": {"bleu_score": 30.166998915707186, "chrf_score": 31.47575159756051, "xcomet_score": 0.4026835560798645, "xcomet_qe_score": 0.3457815647125244, "metricx_score": 9.088987350463867, "metricx_qe_score": 9.302443504333496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了支持公共和私有下游任务的信息,例如姓名和身份识别、分类、模式切换标记和问答。", "metrics": {"bleu_score": 48.621879105077944, "chrf_score": 42.22259248864165, "xcomet_score": 0.5837271213531494, "xcomet_qe_score": 0.626983106136322, "metricx_score": 4.898404121398926, "metricx_qe_score": 3.93217396736145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基准模型进行了比较,这些基准模型包括 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBelt、Myobelt 和 ClinicalBelt。", "metrics": {"bleu_score": 25.05919287797169, "chrf_score": 37.611408444388815, "xcomet_score": 0.42797744274139404, "xcomet_qe_score": 0.45686861872673035, "metricx_score": 7.578773498535156, "metricx_qe_score": 7.373449802398682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果显示,该模型在与训练数据性质相同的任务上表现最佳。", "metrics": {"bleu_score": 31.912065473716705, "chrf_score": 26.302730156712723, "xcomet_score": 0.9955272674560547, "xcomet_qe_score": 0.9893743991851807, "metricx_score": 0.9241297245025635, "metricx_qe_score": 1.1733543872833252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从异构数据源中获取这些数据,这些数据似乎更加通用。", "metrics": {"bleu_score": 36.151920269854195, "chrf_score": 36.66868220544846, "xcomet_score": 0.952019453048706, "xcomet_qe_score": 0.7528588771820068, "metricx_score": 1.5422813892364502, "metricx_qe_score": 2.051910161972046, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以带来更好的性能。", "metrics": {"bleu_score": 52.6589137558171, "chrf_score": 45.93989530077461, "xcomet_score": 0.9362239837646484, "xcomet_qe_score": 0.9736031889915466, "metricx_score": 2.49906587600708, "metricx_qe_score": 2.952756404876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始,免费训练似乎在大多数任务中都能获得更高的性能。", "metrics": {"bleu_score": 56.02685242567755, "chrf_score": 52.69106032804686, "xcomet_score": 0.7985321283340454, "xcomet_qe_score": 0.7870678901672363, "metricx_score": 6.304870128631592, "metricx_qe_score": 6.841822147369385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用在 Natchez 的 4GB 子集上训练的 PumedBeard 的权重和分词器进行的持续假装实验,结果与 Dr. Beard 从头开始训练的 4GB 结果相当。", "metrics": {"bleu_score": 31.994654074624904, "chrf_score": 25.643771571090763, "xcomet_score": 0.32607775926589966, "xcomet_qe_score": 0.33678558468818665, "metricx_score": 9.546791076660156, "metricx_qe_score": 9.685900688171387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而基于常见熊重和分词器的模型则存在稳定性问题。", "metrics": {"bleu_score": 25.93073760816262, "chrf_score": 21.40157404992023, "xcomet_score": 0.686484694480896, "xcomet_qe_score": 0.7633200287818909, "metricx_score": 5.580143451690674, "metricx_qe_score": 5.545872211456299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们提出的系统在十一项下游任务中九项表现更好,并且在全球范围内超越了通用模型Camembert的结果。", "metrics": {"bleu_score": 36.853011274419494, "chrf_score": 32.61615817539106, "xcomet_score": 0.84140944480896, "xcomet_qe_score": 0.7637708783149719, "metricx_score": 3.634553909301758, "metricx_qe_score": 2.7090985774993896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,专业数据更好,更专业的数据更好,但它扩展性不佳。", "metrics": {"bleu_score": 16.15508477937387, "chrf_score": 16.263576580924795, "xcomet_score": 0.6908049583435059, "xcomet_qe_score": 0.6219449043273926, "metricx_score": 3.7947044372558594, "metricx_qe_score": 3.818692922592163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 Natchios 获得的所有预训练模型都可以在 YuginFace 上免费获取,所有训练脚本都在我们的 GitHub 仓库中。", "metrics": {"bleu_score": 42.807161995102895, "chrf_score": 41.719051739758214, "xcomet_score": 0.7104808688163757, "xcomet_qe_score": 0.7267470359802246, "metricx_score": 7.180333137512207, "metricx_qe_score": 7.531496524810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以感谢您的演讲,我们期待在多伦多的海报环节与您交流。", "metrics": {"bleu_score": 29.108736587772473, "chrf_score": 30.696292111732937, "xcomet_score": 0.8587695360183716, "xcomet_qe_score": 0.8849810361862183, "metricx_score": 1.8469010591506958, "metricx_qe_score": 1.7942451238632202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9670989513397217, "xcomet_qe_score": 0.9718614816665649, "metricx_score": 0.2643663287162781, "metricx_qe_score": 0.26394033432006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼,今天我将向大家简要介绍我们的论文——《无需树结构的组合泛化:使用多集标记和潜在置换》。", "metrics": {"bleu_score": 36.42317694376632, "chrf_score": 29.28128346560656, "xcomet_score": 0.8488873243331909, "xcomet_qe_score": 0.90559983253479, "metricx_score": 1.5449326038360596, "metricx_qe_score": 1.16024911403656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和我的导师亚历山大·科拉和伊万·蒂托夫的合作成果。", "metrics": {"bleu_score": 7.987276352377325, "chrf_score": 6.870147914535245, "xcomet_score": 0.8389765024185181, "xcomet_qe_score": 0.820705771446228, "metricx_score": 1.9597885608673096, "metricx_qe_score": 2.0883166790008545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合概化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语组合的能力。", "metrics": {"bleu_score": 74.71600506180394, "chrf_score": 69.34063410625538, "xcomet_score": 0.7195075750350952, "xcomet_qe_score": 0.6423660516738892, "metricx_score": 5.091233253479004, "metricx_qe_score": 6.177090644836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能看起来像这样。", "metrics": {"bleu_score": 46.942223829384936, "chrf_score": 41.593386115832374, "xcomet_score": 0.9042496681213379, "xcomet_qe_score": 0.8924782276153564, "metricx_score": 1.0878242254257202, "metricx_qe_score": 1.6714626550674438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一如既往,我们有一个训练语料集,", "metrics": {"bleu_score": 19.433831559151137, "chrf_score": 17.600070616630507, "xcomet_score": 0.8999894261360168, "xcomet_qe_score": 0.8856474161148071, "metricx_score": 1.6759520769119263, "metricx_qe_score": 1.451578140258789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下是“女孩睡着了”,", "metrics": {"bleu_score": 8.910251805984952, "chrf_score": 6.015727187146641, "xcomet_score": 0.8592069149017334, "xcomet_qe_score": 0.9542679786682129, "metricx_score": 2.4154772758483887, "metricx_qe_score": 1.1227586269378662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及“玛丽知道女孩睡着了”。 这些言语与其意义的核心方面", "metrics": {"bleu_score": 17.145622412034673, "chrf_score": 10.89069878920563, "xcomet_score": 0.6775421500205994, "xcomet_qe_score": 0.4046268165111542, "metricx_score": 4.5866241455078125, "metricx_qe_score": 4.765920162200928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相对应的逻辑形式相配。", "metrics": {"bleu_score": 4.7263769322333395, "chrf_score": 7.1153104512148095, "xcomet_score": 0.33297282457351685, "xcomet_qe_score": 0.17442256212234497, "metricx_score": 4.9420952796936035, "metricx_qe_score": 5.291723251342773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集并非来自相同的分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 55.90981220397046, "chrf_score": 50.240316868560384, "xcomet_score": 0.8630441427230835, "xcomet_qe_score": 0.8781783580780029, "metricx_score": 1.056781530380249, "metricx_qe_score": 1.8781754970550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中经历了较浅的递归,并在一个具有更深递归的例子上进行了测试。", "metrics": {"bleu_score": 23.80653769582892, "chrf_score": 23.28770412137487, "xcomet_score": 0.8863505721092224, "xcomet_qe_score": 0.8852543234825134, "metricx_score": 2.620126247406006, "metricx_qe_score": 4.599937915802002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种分布外泛化问题,并且经常会产生与输入脱节的输出。", "metrics": {"bleu_score": 40.64326219443485, "chrf_score": 32.20360610397591, "xcomet_score": 0.7812399864196777, "xcomet_qe_score": 0.7460706233978271, "metricx_score": 3.79862117767334, "metricx_qe_score": 3.477926254272461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,他们往往无法再现输入和输出之间的系统对应关系,例如例子中用颜色编码的对应关系。", "metrics": {"bleu_score": 57.955628207374694, "chrf_score": 53.55866234905234, "xcomet_score": 0.9928284883499146, "xcomet_qe_score": 0.9853382110595703, "metricx_score": 1.8244181871414185, "metricx_qe_score": 1.2888199090957642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的解决方法是将树木融入模型中。", "metrics": {"bleu_score": 30.586957615133986, "chrf_score": 25.231858304955058, "xcomet_score": 0.9290779232978821, "xcomet_qe_score": 0.9340708255767822, "metricx_score": 0.5691179037094116, "metricx_qe_score": 0.7894960045814514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "树的用意在于捕捉将发音与逻辑形式联系起来的组合过程。", "metrics": {"bleu_score": 48.81776922763246, "chrf_score": 44.02965768797837, "xcomet_score": 0.7356414794921875, "xcomet_qe_score": 0.7378012537956238, "metricx_score": 2.2436604499816895, "metricx_qe_score": 3.8759925365448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法效果很好,但通常不会提供树,需要通过某种方式获取。", "metrics": {"bleu_score": 46.40615962360809, "chrf_score": 41.676671498709126, "xcomet_score": 0.9375432133674622, "xcomet_qe_score": 0.9336903691291809, "metricx_score": 2.4672279357910156, "metricx_qe_score": 2.151196241378784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本较高的过程。", "metrics": {"bleu_score": 35.4225224760582, "chrf_score": 30.964956807920935, "xcomet_score": 0.9723730087280273, "xcomet_qe_score": 0.9788724184036255, "metricx_score": 0.4672151803970337, "metricx_qe_score": 0.6136482357978821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到对逻辑形式进行大量的形式化预处理,例如处理变量符号。", "metrics": {"bleu_score": 52.68694760883328, "chrf_score": 46.737138190912155, "xcomet_score": 0.9923501014709473, "xcomet_qe_score": 0.9947376251220703, "metricx_score": 0.6100496053695679, "metricx_qe_score": 0.6371234655380249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树可能也涉及到专门的语法归纳程序。", "metrics": {"bleu_score": 28.688236146427446, "chrf_score": 25.42100028089026, "xcomet_score": 0.8635983467102051, "xcomet_qe_score": 0.8903573155403137, "metricx_score": 4.861697196960449, "metricx_qe_score": 5.501476764678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文中,我们没有使用树结构,而是引入了一种神经序列到序列模型,该模型直接对输入片段与输出片段之间的对应关系进行建模。", "metrics": {"bleu_score": 56.25962975184686, "chrf_score": 49.991775709281896, "xcomet_score": 0.8217098116874695, "xcomet_qe_score": 0.8338313102722168, "metricx_score": 1.5476908683776855, "metricx_qe_score": 1.5766558647155762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在不依赖树的情况下对更深层次的递归进行强大的泛化。", "metrics": {"bleu_score": 38.37679744478914, "chrf_score": 30.612618517929974, "xcomet_score": 0.908534586429596, "xcomet_qe_score": 0.8736904263496399, "metricx_score": 2.9979896545410156, "metricx_qe_score": 4.694782733917236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两步预测输入的输出。", "metrics": {"bleu_score": 61.583212398305974, "chrf_score": 52.279490103939, "xcomet_score": 0.9820935726165771, "xcomet_qe_score": 0.9455699920654297, "metricx_score": 0.47149890661239624, "metricx_qe_score": 0.8730547428131104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们将每个输入标记与将在输出中出现的标记的无序集合进行标记。", "metrics": {"bleu_score": 22.506782731403728, "chrf_score": 21.180758863915646, "xcomet_score": 0.7124845385551453, "xcomet_qe_score": 0.7745311260223389, "metricx_score": 3.7208826541900635, "metricx_qe_score": 2.932169198989868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步完成后,我们得到了所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 51.99085035777304, "chrf_score": 41.53620386591401, "xcomet_score": 0.9150985479354858, "xcomet_qe_score": 0.886381208896637, "metricx_score": 2.0589401721954346, "metricx_qe_score": 3.1602413654327393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步,我们使用另一个模型来预测一个置换,以便将它们排列到正确的顺序。", "metrics": {"bleu_score": 45.344493800370586, "chrf_score": 46.50310255644154, "xcomet_score": 0.8962165117263794, "xcomet_qe_score": 0.9037189483642578, "metricx_score": 3.263699531555176, "metricx_qe_score": 3.1897714138031006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了一种预测排列的新方法,该方法对可能的排列没有硬性约束。", "metrics": {"bleu_score": 46.958343858928004, "chrf_score": 40.7343673827061, "xcomet_score": 0.9875645637512207, "xcomet_qe_score": 0.928425669670105, "metricx_score": 0.874230682849884, "metricx_qe_score": 1.5687075853347778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 48.620266417318525, "chrf_score": 41.561795590015564, "xcomet_score": 0.9840943813323975, "xcomet_qe_score": 0.9654589891433716, "metricx_score": 0.7629964351654053, "metricx_qe_score": 1.3032432794570923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致是这样工作的。", "metrics": {"bleu_score": 25.984882476296985, "chrf_score": 22.856298625860568, "xcomet_score": 0.9731236696243286, "xcomet_qe_score": 0.9644007682800293, "metricx_score": 1.2977163791656494, "metricx_qe_score": 0.8059293031692505, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定每个位置放置哪个多元集标记。", "metrics": {"bleu_score": 51.45748881204831, "chrf_score": 44.38489189741603, "xcomet_score": 0.8393990993499756, "xcomet_qe_score": 0.8395947217941284, "metricx_score": 2.4202587604522705, "metricx_qe_score": 3.4576141834259033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需像红色高亮显示的那样选择一个。", "metrics": {"bleu_score": 49.34352697917813, "chrf_score": 42.0480032664865, "xcomet_score": 0.9327919483184814, "xcomet_qe_score": 0.9199720621109009, "metricx_score": 0.5209035277366638, "metricx_qe_score": 0.5989149212837219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们跳转到下一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 66.58648643455578, "chrf_score": 61.40283492613613, "xcomet_score": 0.7779577970504761, "xcomet_qe_score": 0.7661186456680298, "metricx_score": 2.8489067554473877, "metricx_qe_score": 2.8283591270446777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个标记,通过跳转到另一个多集标记。", "metrics": {"bleu_score": 67.6238568295729, "chrf_score": 62.76656674669966, "xcomet_score": 0.7130098938941956, "xcomet_qe_score": 0.7494357824325562, "metricx_score": 3.959353446960449, "metricx_qe_score": 3.5409233570098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到第一个阶段的每个标记都被访问恰好一次。", "metrics": {"bleu_score": 54.365524825814376, "chrf_score": 45.93775930199655, "xcomet_score": 0.8324980735778809, "xcomet_qe_score": 0.790851354598999, "metricx_score": 4.259974002838135, "metricx_qe_score": 4.5399065017700195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让您了解实验结果,我们在这里将我们的方法与 Kong 基准测试中的其他无树模型进行了比较。我们的模型在", "metrics": {"bleu_score": 42.952433376407555, "chrf_score": 34.658810166195124, "xcomet_score": 0.4502766728401184, "xcomet_qe_score": 0.5641849040985107, "metricx_score": 9.749056816101074, "metricx_qe_score": 4.7823944091796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "向更深层次的递归泛化方面远远优于其他模型。", "metrics": {"bleu_score": 21.311811246318026, "chrf_score": 21.357521435177553, "xcomet_score": 0.9279171228408813, "xcomet_qe_score": 0.9003018140792847, "metricx_score": 3.891754388809204, "metricx_qe_score": 4.717794418334961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不过,其他一些类型的结构化概括仍然非常具有挑战性。", "metrics": {"bleu_score": 19.81463247873555, "chrf_score": 20.782013736142893, "xcomet_score": 0.9948471784591675, "xcomet_qe_score": 1.0, "metricx_score": 1.7387021780014038, "metricx_qe_score": 0.942378580570221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9961615800857544, "xcomet_qe_score": 0.986243486404419, "metricx_score": 0.13248570263385773, "metricx_qe_score": 0.16579806804656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,训练数据中没有给出输入和输出的对齐。", "metrics": {"bleu_score": 36.173905261890994, "chrf_score": 28.826001254388807, "xcomet_score": 0.9108600616455078, "xcomet_qe_score": 0.913451611995697, "metricx_score": 0.7726055383682251, "metricx_qe_score": 0.759522557258606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多设置器,这给训练带来了挑战。", "metrics": {"bleu_score": 63.404662770468576, "chrf_score": 57.34210881630243, "xcomet_score": 0.7717380523681641, "xcomet_qe_score": 0.7314670085906982, "metricx_score": 5.735220909118652, "metricx_qe_score": 4.469250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列方式,但其中一种是潜在的语言学上正确的排列方式。我们通过将", "metrics": {"bleu_score": 42.30445864921815, "chrf_score": 50.82925092547293, "xcomet_score": 0.7429609298706055, "xcomet_qe_score": 0.7264924049377441, "metricx_score": 7.129109859466553, "metricx_qe_score": 3.815112590789795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对齐作为训练的一部分来解决这个问题。", "metrics": {"bleu_score": 21.65768464503216, "chrf_score": 20.32150267145187, "xcomet_score": 0.8533875942230225, "xcomet_qe_score": 0.8452188372612, "metricx_score": 1.7436877489089966, "metricx_qe_score": 2.262022018432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的排列方法非常灵活,但它带来了一个挑战,即找到得分最高的排列是NP难的。", "metrics": {"bleu_score": 68.87591926411304, "chrf_score": 59.44461199555299, "xcomet_score": 0.8570966720581055, "xcomet_qe_score": 0.8282343745231628, "metricx_score": 1.6398587226867676, "metricx_qe_score": 3.1038010120391846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这与旅行商问题有关。", "metrics": {"bleu_score": 45.06775052173921, "chrf_score": 38.706282963094665, "xcomet_score": 0.8810908198356628, "xcomet_qe_score": 0.808661937713623, "metricx_score": 0.7263143062591553, "metricx_qe_score": 1.095809817314148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一种适合 GPU 的连续松弛方法来近似它,这种方法还使我们能够通过解进行反向传播,并学习出在语言学上更合理的变化。", "metrics": {"bleu_score": 34.30629592497482, "chrf_score": 31.19956375659437, "xcomet_score": 0.8149987459182739, "xcomet_qe_score": 0.6646689176559448, "metricx_score": 4.297703742980957, "metricx_qe_score": 4.785314559936523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息,请查看我们的论文或来参观我们的海报。", "metrics": {"bleu_score": 84.99315997274992, "chrf_score": 81.75179631640817, "xcomet_score": 0.9408668875694275, "xcomet_qe_score": 0.8488820791244507, "metricx_score": 0.4465749263763428, "metricx_qe_score": 0.4910772740840912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Makshta,今天我和我的合著者 Martin 将介绍我们的作品《The Kitmastech:评估多源知识整合》。这项", "metrics": {"bleu_score": 49.36372057536126, "chrf_score": 43.54065609312945, "xcomet_score": 0.35195112228393555, "xcomet_qe_score": 0.3352101445198059, "metricx_score": 8.163437843322754, "metricx_qe_score": 6.944983005523682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、MILA 和微软研究院的合作成果。", "metrics": {"bleu_score": 58.292028737243534, "chrf_score": 42.89618459637336, "xcomet_score": 0.8099987506866455, "xcomet_qe_score": 0.6336193084716797, "metricx_score": 3.509686231613159, "metricx_qe_score": 3.8390309810638428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用各种知识来源,例如其参数中包含的知识(通常通过预训练获得)和推理时输入中提供的知识。", "metrics": {"bleu_score": 51.93568493613616, "chrf_score": 43.40707537590138, "xcomet_score": 0.7735372185707092, "xcomet_qe_score": 0.7161866426467896, "metricx_score": 4.572497844696045, "metricx_qe_score": 3.9405758380889893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中的研究表明,模型可以使用预训练的时间知识来解决任务。", "metrics": {"bleu_score": 79.74545591044426, "chrf_score": 72.62831452228986, "xcomet_score": 0.9074317216873169, "xcomet_qe_score": 0.8530808687210083, "metricx_score": 1.7452044486999512, "metricx_qe_score": 1.6177330017089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,自然语言理解通常需要在推理时提供的知识。", "metrics": {"bleu_score": 89.21616972156083, "chrf_score": 87.46990169759047, "xcomet_score": 0.8783411979675293, "xcomet_qe_score": 0.8480260372161865, "metricx_score": 1.2219430208206177, "metricx_qe_score": 0.9787708520889282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子中,约翰在电视上看到了新当选的总统。", "metrics": {"bleu_score": 34.652127631368096, "chrf_score": 21.013222567533337, "xcomet_score": 0.9569696187973022, "xcomet_qe_score": 0.9540069699287415, "metricx_score": 1.8397694826126099, "metricx_qe_score": 2.3926615715026855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可以包含关于总统做什么和TBA是什么的信息,但它们无法可靠地知道这个特定实例实体John是谁,或者新总统是谁,因为自预训练以来总统可能已经更换。", "metrics": {"bleu_score": 56.72883258091167, "chrf_score": 50.61609889524191, "xcomet_score": 0.7386425733566284, "xcomet_qe_score": 0.7601791620254517, "metricx_score": 5.08530330657959, "metricx_qe_score": 5.704816818237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功处理知识密集型NLU任务的模型需要具备整合和利用预训练时间和推理时间知识的能力。", "metrics": {"bleu_score": 59.27581867613053, "chrf_score": 51.923162335682406, "xcomet_score": 0.972771406173706, "xcomet_qe_score": 0.9314808249473572, "metricx_score": 1.0310497283935547, "metricx_qe_score": 1.3485571146011353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一套知识整合诊断测试。", "metrics": {"bleu_score": 38.564192073808314, "chrf_score": 31.11785163020088, "xcomet_score": 0.9958508014678955, "xcomet_qe_score": 0.9904599189758301, "metricx_score": 1.2110984325408936, "metricx_qe_score": 1.5152925252914429, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了一个核心参考解析任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 31.28510453803198, "chrf_score": 26.165688146361116, "xcomet_score": 0.848253607749939, "xcomet_qe_score": 0.8416392803192139, "metricx_score": 3.567841053009033, "metricx_qe_score": 3.70101261138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的参考解析模型对数据集进行了评估。", "metrics": {"bleu_score": 57.23666542158096, "chrf_score": 57.61767898455821, "xcomet_score": 0.8786104917526245, "xcomet_qe_score": 0.8595362901687622, "metricx_score": 2.289626121520996, "metricx_qe_score": 2.5164217948913574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 74.20134214947585, "xcomet_score": 0.9661270380020142, "xcomet_qe_score": 0.8817721605300903, "metricx_score": 0.3315274715423584, "metricx_qe_score": 1.3067893981933594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟文是一名法官,", "metrics": {"bleu_score": 51.697315395717055, "chrf_score": 25.813926430308626, "xcomet_score": 0.8538086414337158, "xcomet_qe_score": 0.8511099815368652, "metricx_score": 1.8278892040252686, "metricx_qe_score": 2.383923292160034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基亚是一名面包师。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 59.333448823374546, "xcomet_score": 0.905996561050415, "xcomet_qe_score": 0.8840891122817993, "metricx_score": 0.5893900990486145, "metricx_qe_score": 0.7874979376792908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟文和基亚在公园里相遇。", "metrics": {"bleu_score": 22.03359678996931, "chrf_score": 14.896167636318175, "xcomet_score": 0.9576905965805054, "xcomet_qe_score": 0.9636341333389282, "metricx_score": 1.0216057300567627, "metricx_qe_score": 1.0701367855072021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件、工作了一整天后,他很高兴放松一下。", "metrics": {"bleu_score": 44.47630178181815, "chrf_score": 38.95765244046282, "xcomet_score": 0.9808952808380127, "xcomet_qe_score": 0.9726191759109497, "metricx_score": 1.6026215553283691, "metricx_qe_score": 1.8406232595443726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是确定代词 he 指的是哪个正确的实体,在这种情况下,它是仆人。 给", "metrics": {"bleu_score": 33.59973059005528, "chrf_score": 27.97185224386725, "xcomet_score": 0.5788653492927551, "xcomet_qe_score": 0.4720788598060608, "metricx_score": 8.243919372558594, "metricx_qe_score": 7.189071178436279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "定代词的解析需要两种信息。", "metrics": {"bleu_score": 16.63584005556407, "chrf_score": 17.089632047059975, "xcomet_score": 0.8972220420837402, "xcomet_qe_score": 0.8755567669868469, "metricx_score": 1.19296395778656, "metricx_qe_score": 0.856552004814148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如布道者是法官。其", "metrics": {"bleu_score": 12.002450435910102, "chrf_score": 12.583869235986352, "xcomet_score": 0.4759378135204315, "xcomet_qe_score": 0.5188957452774048, "metricx_score": 6.679271697998047, "metricx_qe_score": 4.013120651245117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上裁决案件。", "metrics": {"bleu_score": 33.308456462852334, "chrf_score": 29.68967949030302, "xcomet_score": 0.8814171552658081, "xcomet_qe_score": 0.8098436594009399, "metricx_score": 4.524585723876953, "metricx_qe_score": 3.8353097438812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,背景知识是在大型语言模型的预训练阶段学习的,而特定实体的知识通常在推理时观察到。", "metrics": {"bleu_score": 59.266178039483115, "chrf_score": 50.023487105379104, "xcomet_score": 0.8761739730834961, "xcomet_qe_score": 0.8748983144760132, "metricx_score": 1.2779123783111572, "metricx_qe_score": 2.0845108032226562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这两部分信息的可用性有所不同,因此它们可能只在一个来源中找到,也可能在多个来源中找到。", "metrics": {"bleu_score": 28.973729494990064, "chrf_score": 33.171751932168135, "xcomet_score": 0.7799676060676575, "xcomet_qe_score": 0.7853102684020996, "metricx_score": 1.3462942838668823, "metricx_qe_score": 1.245578646659851, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了 Kitmos 的三种设置。", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 46.11910148674854, "xcomet_score": 0.879934549331665, "xcomet_qe_score": 0.8674264550209045, "metricx_score": 1.107910394668579, "metricx_qe_score": 1.0714714527130127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有主题设置,即背景预训练,在此背景知识被假设在预训练时已经可用。", "metrics": {"bleu_score": 27.642456176943178, "chrf_score": 23.85794660445066, "xcomet_score": 0.7679547071456909, "xcomet_qe_score": 0.758273184299469, "metricx_score": 3.982818603515625, "metricx_qe_score": 5.545966625213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,背景设置包括预训练时间和推理时间都有背景知识可用的设置。", "metrics": {"bleu_score": 25.538076362962062, "chrf_score": 22.442881922950278, "xcomet_score": 0.841830849647522, "xcomet_qe_score": 0.7099580764770508, "metricx_score": 3.089829921722412, "metricx_qe_score": 2.8327972888946533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,背景推理设置,两种类型的知识仅在推理时间可用。", "metrics": {"bleu_score": 25.179087942038716, "chrf_score": 25.33906557323714, "xcomet_score": 0.8880728483200073, "xcomet_qe_score": 0.7952632904052734, "metricx_score": 2.1631205081939697, "metricx_qe_score": 2.162181854248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一种设置尤其有趣,因为它模拟了一个情况,即解决任务所需的背景知识不在模型的预训练数据中,例如", "metrics": {"bleu_score": 48.18525338175344, "chrf_score": 44.47396967435539, "xcomet_score": 0.8772088885307312, "xcomet_qe_score": 0.8637649416923523, "metricx_score": 1.3141154050827026, "metricx_qe_score": 0.5678079128265381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 83.13427988970655, "chrf_score": 83.66823546839467, "xcomet_score": 0.8476712703704834, "xcomet_qe_score": 0.8093159198760986, "metricx_score": 4.505660057067871, "metricx_qe_score": 5.475840091705322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是如何在真实来源中控制事实可用性的一个例子。", "metrics": {"bleu_score": 34.15789621454766, "chrf_score": 30.413342575152846, "xcomet_score": 0.8985432386398315, "xcomet_qe_score": 0.7731918096542358, "metricx_score": 0.9011679887771606, "metricx_qe_score": 1.1220941543579102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在预训练背景设置中,我们假设政治家寻求政府选任席位的背景知识包含在预训练参数中。在干预情境中,我们提供了反特定知识:奇切斯特是一名政治家。", "metrics": {"bleu_score": 36.90091554498817, "chrf_score": 29.089359956228666, "xcomet_score": 0.6127737760543823, "xcomet_qe_score": 0.5722047090530396, "metricx_score": 4.4978508949279785, "metricx_qe_score": 4.038957118988037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景设定中,我们不仅提供了反特定知识,还提供了关于影响时间背景下政治家的背景知识。", "metrics": {"bleu_score": 30.144418682547542, "chrf_score": 27.33621445120646, "xcomet_score": 0.6088255643844604, "xcomet_qe_score": 0.590695858001709, "metricx_score": 5.770210266113281, "metricx_qe_score": 6.025573253631592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 Freon 设置的背景中,我们提供了虚构的职业 meritur 而不是政治家,因为 meritur 不太可能包含在预训练参数中。", "metrics": {"bleu_score": 50.76778842590989, "chrf_score": 42.49480159997229, "xcomet_score": 0.4303220808506012, "xcomet_qe_score": 0.37049734592437744, "metricx_score": 6.308132171630859, "metricx_qe_score": 9.151570320129395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的参考解析模型对数据集进行了评估。", "metrics": {"bleu_score": 57.23666542158096, "chrf_score": 57.61767898455821, "xcomet_score": 0.8802322149276733, "xcomet_qe_score": 0.863949179649353, "metricx_score": 2.4789915084838867, "metricx_qe_score": 2.6718249320983887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此图中,我们展示了在最困难的背景预训练设置变体上表现最好的模型的结果。", "metrics": {"bleu_score": 30.54777015306237, "chrf_score": 28.666469038837388, "xcomet_score": 0.8922098875045776, "xcomet_qe_score": 0.8771095871925354, "metricx_score": 1.2272837162017822, "metricx_qe_score": 1.5092427730560303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有针对 Kitmos 的任务特定训练的情况下,两种模型的表现都不佳。", "metrics": {"bleu_score": 10.745586208587573, "chrf_score": 15.082598740192127, "xcomet_score": 0.8151679039001465, "xcomet_qe_score": 0.7957676649093628, "metricx_score": 1.584965705871582, "metricx_qe_score": 2.004986524581909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在 Kitmos 上进行训练时,C2F 和 Berth for Koref 的表现都显著优于随机选择。", "metrics": {"bleu_score": 21.181228850483773, "chrf_score": 28.58341393693275, "xcomet_score": 0.6647825241088867, "xcomet_qe_score": 0.6560844779014587, "metricx_score": 4.878936290740967, "metricx_qe_score": 5.239138126373291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在通用共现解析数据集上进行训练时,MOD 会学会利用表面线索,而在对 kidmos 进行测试时,这些线索已经不存在,因此这些线索就派不上用场了。", "metrics": {"bleu_score": 39.529408631157786, "chrf_score": 31.297929621093335, "xcomet_score": 0.6218967437744141, "xcomet_qe_score": 0.6085119247436523, "metricx_score": 6.793355464935303, "metricx_qe_score": 7.332160949707031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识进行的额外实验表明,即使是表现最好的模型也无法可靠地整合仅在推理时提供的背景知识。 总结", "metrics": {"bleu_score": 65.68463413742988, "chrf_score": 61.38012010548097, "xcomet_score": 0.839524507522583, "xcomet_qe_score": 0.8369878530502319, "metricx_score": 2.8899950981140137, "metricx_qe_score": 1.4592173099517822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要观点,许多一致性解决模型在没有特定任务训练的情况下似乎无法推理不同来源的知识。", "metrics": {"bleu_score": 53.918528757020084, "chrf_score": 47.80711632191716, "xcomet_score": 0.8311898708343506, "xcomet_qe_score": 0.8295465707778931, "metricx_score": 4.4874162673950195, "metricx_qe_score": 4.744203090667725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过特定任务训练,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 76.96750100612337, "chrf_score": 71.97021453993575, "xcomet_score": 0.999657392501831, "xcomet_qe_score": 0.9977723360061646, "metricx_score": 0.5964401960372925, "metricx_qe_score": 0.981561541557312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最好的模型似乎也难以可靠地整合仅在推理时呈现的先前知识。", "metrics": {"bleu_score": 70.53571147273676, "chrf_score": 63.53974156265348, "xcomet_score": 0.9149981141090393, "xcomet_qe_score": 0.9107499122619629, "metricx_score": 1.250555396080017, "metricx_qe_score": 1.0986460447311401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请参阅我们的论文,并在 GitHub 上查看数据集和代码。", "metrics": {"bleu_score": 64.17489454284512, "chrf_score": 62.9035142889632, "xcomet_score": 0.9940266609191895, "xcomet_qe_score": 0.9795973300933838, "metricx_score": 0.2844045162200928, "metricx_qe_score": 0.22556202113628387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢聆听。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9658737182617188, "xcomet_qe_score": 0.9351316690444946, "metricx_score": 0.08587995171546936, "metricx_qe_score": 0.44492465257644653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Myra,今天我要谈谈我们的论文《标记化角色》——使用自然语言提示来衡量语言模型中的刻板印象。", "metrics": {"bleu_score": 65.08416720425492, "chrf_score": 61.49105526108005, "xcomet_score": 0.8704838752746582, "xcomet_qe_score": 0.7220356464385986, "metricx_score": 1.4643449783325195, "metricx_qe_score": 2.256013870239258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Essendermouch和Dandarovsky合作完成的。", "metrics": {"bleu_score": 30.119166060089718, "chrf_score": 31.990985168277597, "xcomet_score": 0.750087320804596, "xcomet_qe_score": 0.7845628261566162, "metricx_score": 8.027960777282715, "metricx_qe_score": 8.4816312789917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究记录了大型语言模型(LLM)中普遍存在的社会偏见和刻板印象。", "metrics": {"bleu_score": 39.76353643835254, "chrf_score": 40.19404065218525, "xcomet_score": 0.9876197576522827, "xcomet_qe_score": 0.9841408729553223, "metricx_score": 1.9831936359405518, "metricx_qe_score": 4.267080307006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施存在各种局限性。", "metrics": {"bleu_score": 34.245097009375314, "chrf_score": 27.612730879133828, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09926637262105942, "metricx_qe_score": 0.24205049872398376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,这些数据集的整理非常耗时。 而且,它们通常只衡量非常具体的刻板印象,这意味着它们无法很好地推广到其他人口统计数据或情境,或者它们只是捕捉到非常普遍、广泛的关联,例如与特定群体的负面关联。", "metrics": {"bleu_score": 53.33998141717411, "chrf_score": 47.97490755182846, "xcomet_score": 0.7478042244911194, "xcomet_qe_score": 0.6317225098609924, "metricx_score": 2.100161075592041, "metricx_qe_score": 2.463230848312378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这个领域的大部分工作都没有考虑到交叉性,即多方面社会身份可以加剧偏见,并成为独特的伤害点。", "metrics": {"bleu_score": 47.948377545783224, "chrf_score": 39.9944693992676, "xcomet_score": 0.7842147946357727, "xcomet_qe_score": 0.7377857565879822, "metricx_score": 2.472226619720459, "metricx_qe_score": 2.399488925933838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些限制,我们依赖于这些经过指令微调的全新大语言模型在响应指令和提示方面表现非常好的特性。", "metrics": {"bleu_score": 29.18461120514461, "chrf_score": 24.76838387874796, "xcomet_score": 0.8915825486183167, "xcomet_qe_score": 0.8941524624824524, "metricx_score": 2.6758625507354736, "metricx_qe_score": 2.5357706546783447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色,即通过类似于“想象你是一个亚洲女性,描述你自己”的提示,描绘一个虚构的人", "metrics": {"bleu_score": 39.02506066208301, "chrf_score": 35.4738156301095, "xcomet_score": 0.8490390777587891, "xcomet_qe_score": 0.9057409763336182, "metricx_score": 2.3485851287841797, "metricx_qe_score": 2.7012784481048584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "物。", "metrics": {"bleu_score": 0.0, "chrf_score": 5.434782608695652, "xcomet_score": 0.15174822509288788, "xcomet_qe_score": 0.1067047268152237, "metricx_score": 8.45384407043457, "metricx_qe_score": 14.846307754516602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这种方法可以推广到任何人群,因为我们可以在这个提示中指定任何我们想要的标识。", "metrics": {"bleu_score": 52.93450766232647, "chrf_score": 50.901513337731195, "xcomet_score": 0.8266863822937012, "xcomet_qe_score": 0.7698875665664673, "metricx_score": 1.2003568410873413, "metricx_qe_score": 1.7756953239440918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT 4 的一些示例生成内容。", "metrics": {"bleu_score": 37.0304683381906, "chrf_score": 46.51294868604173, "xcomet_score": 0.8908377885818481, "xcomet_qe_score": 0.8723018765449524, "metricx_score": 1.30582857131958, "metricx_qe_score": 1.714801549911499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们立即发现,尽管这些输出并不是传统意义上的过于消极或有毒。 这里有一些有趣的模式。", "metrics": {"bleu_score": 32.28330348476562, "chrf_score": 28.91329757989107, "xcomet_score": 0.6830251216888428, "xcomet_qe_score": 0.7623932361602783, "metricx_score": 2.7833032608032227, "metricx_qe_score": 3.5822501182556152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘成不引人注目。中东女性则被用“异域风情”等词来描述,仿佛她来自一个迷人的地区。 而且,", "metrics": {"bleu_score": 50.053483193928756, "chrf_score": 43.24700488685555, "xcomet_score": 0.6758983135223389, "xcomet_qe_score": 0.6087111830711365, "metricx_score": 5.297633171081543, "metricx_qe_score": 3.221710205078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个有色人种角色都提到了祖先,而白人角色则没有任何这样的内容。", "metrics": {"bleu_score": 35.62801268667034, "chrf_score": 29.615075830865038, "xcomet_score": 0.9396519660949707, "xcomet_qe_score": 0.9712100028991699, "metricx_score": 1.238877773284912, "metricx_qe_score": 1.0121575593948364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两部分。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9945436716079712, "xcomet_qe_score": 0.9767298698425293, "metricx_score": 0.19123207032680511, "metricx_qe_score": 0.2513856589794159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些角色。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.984796404838562, "xcomet_qe_score": 0.8308827877044678, "metricx_score": 0.5102881789207458, "metricx_qe_score": 0.7744022607803345, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些角色的提示源于一项研究,该研究向人类受试者提供了这些提示,发现通过向人类受试者提供这些提示,他们也能够揭示种族刻板印象。", "metrics": {"bleu_score": 73.59157215664145, "chrf_score": 68.86905821700245, "xcomet_score": 0.7470000386238098, "xcomet_qe_score": 0.6510168313980103, "metricx_score": 3.3746604919433594, "metricx_qe_score": 3.6899240016937256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这还使得我们能够直接比较我们生成的虚拟人物和人类撰写的回复。", "metrics": {"bleu_score": 30.338500722781674, "chrf_score": 25.233765178790524, "xcomet_score": 0.9112181663513184, "xcomet_qe_score": 0.8531713485717773, "metricx_score": 1.0669723749160767, "metricx_qe_score": 1.5081530809402466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种识别区分标记组与非标记组的词的方法,我稍后会详细解释。", "metrics": {"bleu_score": 23.139792336023817, "chrf_score": 21.239633467335704, "xcomet_score": 0.8054757714271545, "xcomet_qe_score": 0.9624431133270264, "metricx_score": 1.3529489040374756, "metricx_qe_score": 1.103201150894165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其好处是我们能获得非常具体的刻板印象和模式,而无需依赖任何特定的词汇。", "metrics": {"bleu_score": 55.02529400745294, "chrf_score": 49.80678965661646, "xcomet_score": 0.986849308013916, "xcomet_qe_score": 0.931135892868042, "metricx_score": 1.069506287574768, "metricx_qe_score": 1.5625696182250977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词法借鉴了社会语言学中的标记性概念,该概念指出存在一种未标记的默认状态,任何偏离该默认状态的群体在语言学上都是标记的。", "metrics": {"bleu_score": 38.16853048391195, "chrf_score": 31.969068758342672, "xcomet_score": 0.7608344554901123, "xcomet_qe_score": 0.7522473931312561, "metricx_score": 2.3190836906433105, "metricx_qe_score": 2.174833059310913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,战士这个词通常与男性相关联。", "metrics": {"bleu_score": 58.77375023250333, "chrf_score": 54.10029723649853, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.32747799158096313, "metricx_qe_score": 0.5808877944946289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位女性战士时,通常会特别指出这位女性战士,并用“女性”一词来标记。", "metrics": {"bleu_score": 31.109549207997564, "chrf_score": 26.459722724907074, "xcomet_score": 0.8511263132095337, "xcomet_qe_score": 0.8577263355255127, "metricx_score": 1.1830462217330933, "metricx_qe_score": 0.944449245929718, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是未标记的,而边缘化群体通常是有标记的。", "metrics": {"bleu_score": 62.89979850689384, "chrf_score": 55.74491110060498, "xcomet_score": 0.8119044303894043, "xcomet_qe_score": 0.7874094247817993, "metricx_score": 1.1768507957458496, "metricx_qe_score": 1.5237877368927002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在我们的方法中,我们首先确定哪些是未标记和标记的群体。 然后,我们使用战斗词法来比较角色,这基本上是使用加权词频比来区分每个标记组的关键词。", "metrics": {"bleu_score": 41.87577527971403, "chrf_score": 35.66040781596052, "xcomet_score": 0.6474571824073792, "xcomet_qe_score": 0.6104623079299927, "metricx_score": 4.478960037231445, "metricx_qe_score": 5.777923583984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会使用攻击性语言,并将法律神比例与白人角色和男性角色进行比较,因为这两个是对应的未标记群体。", "metrics": {"bleu_score": 47.97312719547287, "chrf_score": 42.239328125928125, "xcomet_score": 0.5628436803817749, "xcomet_qe_score": 0.499475359916687, "metricx_score": 6.240819454193115, "metricx_qe_score": 6.734879970550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看看一些结果。", "metrics": {"bleu_score": 52.53819788848316, "chrf_score": 49.789859739831954, "xcomet_score": 0.9684176445007324, "xcomet_qe_score": 0.9546533823013306, "metricx_score": 0.3128272294998169, "metricx_qe_score": 0.45149296522140503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个刻板印象词典,发现生成的个人形象比人类编写的个人形象包含的刻板印象要多得多。", "metrics": {"bleu_score": 26.830862787037525, "chrf_score": 25.060274541090845, "xcomet_score": 0.8977809548377991, "xcomet_qe_score": 0.8663791418075562, "metricx_score": 2.8211822509765625, "metricx_qe_score": 2.2927017211914062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际观察词汇表中词汇的分布时,我们会发现截然不同的情况。 因此", "metrics": {"bleu_score": 29.356648593774537, "chrf_score": 29.08105688659815, "xcomet_score": 0.7937029600143433, "xcomet_qe_score": 0.7489427924156189, "metricx_score": 3.4984192848205566, "metricx_qe_score": 0.9970555305480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",虽然生成的虚构人物中 Luxon 词的比例要高得多,但人类撰写的虚构人物中词的分布要广泛得多,而生成的虚构人物中的刻板印象词实际上只是“高大”和“健壮”。", "metrics": {"bleu_score": 19.410230435399974, "chrf_score": 15.263355734287629, "xcomet_score": 0.5971615314483643, "xcomet_qe_score": 0.627227783203125, "metricx_score": 7.965610980987549, "metricx_qe_score": 8.43472957611084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以实际上只有积极的,或者至少是非消极的。", "metrics": {"bleu_score": 35.02069088596088, "chrf_score": 29.686760100920175, "xcomet_score": 0.8411403894424438, "xcomet_qe_score": 0.7513470649719238, "metricx_score": 0.735062301158905, "metricx_qe_score": 0.7335915565490723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表根本没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 73.52246988414386, "chrf_score": 65.9487048264138, "xcomet_score": 0.9585816860198975, "xcomet_qe_score": 0.7773227691650391, "metricx_score": 1.1228219270706177, "metricx_qe_score": 1.3333700895309448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向我们标记的词法的结果,以展示这些看似积极的词语如何促进了刻板印象和本质化叙述。", "metrics": {"bleu_score": 23.880931018941165, "chrf_score": 22.812790969359323, "xcomet_score": 0.6969327330589294, "xcomet_qe_score": 0.6478756666183472, "metricx_score": 3.61533260345459, "metricx_qe_score": 3.8499982357025146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映出有害的模式。", "metrics": {"bleu_score": 60.83482364545131, "chrf_score": 52.746053903033875, "xcomet_score": 0.9236572980880737, "xcomet_qe_score": 0.9280974864959717, "metricx_score": 1.8250336647033691, "metricx_qe_score": 2.806147336959839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,最常见的词汇包括文化、传统、自豪和异域风情等。", "metrics": {"bleu_score": 4.013223270817866, "chrf_score": 7.211577269241635, "xcomet_score": 0.7125333547592163, "xcomet_qe_score": 0.7575283050537109, "metricx_score": 3.685460090637207, "metricx_qe_score": 3.404667854309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词汇仅通过与身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 61.79388065603059, "chrf_score": 54.657326636118434, "xcomet_score": 0.9338842630386353, "xcomet_qe_score": 0.9211022853851318, "metricx_score": 1.121396780014038, "metricx_qe_score": 1.3509130477905273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体的长期歧视和异化留下了遗产。", "metrics": {"bleu_score": 18.22986851602933, "chrf_score": 17.655712023082934, "xcomet_score": 0.8351011276245117, "xcomet_qe_score": 0.8271239995956421, "metricx_score": 5.1455864906311035, "metricx_qe_score": 5.157050132751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的陈规定型观念,尤其是对有色人种女性的刻板印象。", "metrics": {"bleu_score": 34.175345792828104, "chrf_score": 33.048941367497925, "xcomet_score": 0.8997335433959961, "xcomet_qe_score": 0.8985402584075928, "metricx_score": 1.1065444946289062, "metricx_qe_score": 0.8907874822616577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁美洲女性的词语包括充满活力和曲线美等。 嗯,这与热带主义的陈词滥调有关。", "metrics": {"bleu_score": 26.691632810842712, "chrf_score": 18.638371603493148, "xcomet_score": 0.7963016629219055, "xcomet_qe_score": 0.8203871250152588, "metricx_score": 3.5843653678894043, "metricx_qe_score": 2.774357795715332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性来说,这些词语如娇小、柔美、丝滑。 这与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等现象有着密切的联系。", "metrics": {"bleu_score": 34.88089089173256, "chrf_score": 27.528933840834206, "xcomet_score": 0.9503475427627563, "xcomet_qe_score": 0.9715136289596558, "metricx_score": 2.997295379638672, "metricx_qe_score": 2.5344390869140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们发现一些最常见的词汇是坚强和韧性。", "metrics": {"bleu_score": 23.294442469090033, "chrf_score": 16.844043038593345, "xcomet_score": 0.904802680015564, "xcomet_qe_score": 0.8704813718795776, "metricx_score": 1.7495840787887573, "metricx_qe_score": 1.7483481168746948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所谓的“坚强的黑人女性原型”相关联,", "metrics": {"bleu_score": 27.63558078746859, "chrf_score": 25.4520265239092, "xcomet_score": 0.8810238838195801, "xcomet_qe_score": 0.6988619565963745, "metricx_score": 1.6491551399230957, "metricx_qe_score": 2.282499074935913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍一看这听起来很积极。 此外,有研究表明,这种原型实际上非常有害,因为它给这些人口群体带来了很大的压力,要求他们对社会障碍保持韧性和坚强。", "metrics": {"bleu_score": 48.698328246846714, "chrf_score": 39.23839997580252, "xcomet_score": 0.6496063470840454, "xcomet_qe_score": 0.6848717927932739, "metricx_score": 3.7863759994506836, "metricx_qe_score": 4.3320817947387695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正努力改变这些障碍,不如给这些人施加压力,让他们克服这些障碍,这会导致这些人出现非常不良的健康状况,以及其他危害。", "metrics": {"bleu_score": 39.88231371005937, "chrf_score": 34.03902211082209, "xcomet_score": 0.9078372716903687, "xcomet_qe_score": 0.9489916563034058, "metricx_score": 1.7665506601333618, "metricx_qe_score": 1.2361849546432495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词汇几乎都反映了非常本质化的叙述。", "metrics": {"bleu_score": 69.87190132405541, "chrf_score": 61.54320535861141, "xcomet_score": 0.80848628282547, "xcomet_qe_score": 0.8491394519805908, "metricx_score": 1.6750845909118652, "metricx_qe_score": 2.43053936958313, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,基于这些模式,我们为模型所有者提出了三条建议。", "metrics": {"bleu_score": 68.42666550297746, "chrf_score": 60.447539175800046, "xcomet_score": 0.8863017559051514, "xcomet_qe_score": 0.7819287776947021, "metricx_score": 1.2668715715408325, "metricx_qe_score": 3.0580878257751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化的叙述。", "metrics": {"bleu_score": 26.40680896783096, "chrf_score": 24.484809572328313, "xcomet_score": 0.8093796968460083, "xcomet_qe_score": 0.8178517818450928, "metricx_score": 1.2333831787109375, "metricx_qe_score": 0.9520150423049927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该用交叉视角来研究偏见和伤害,因为如果不这样做,可能会忽略很多东西。", "metrics": {"bleu_score": 63.46495105594002, "chrf_score": 54.53534456652845, "xcomet_score": 0.9396682977676392, "xcomet_qe_score": 0.8595486879348755, "metricx_score": 0.4640994071960449, "metricx_qe_score": 0.6253359913825989, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,关于减少偏倚的方法,确实应该提高透明度。 因为,例如,像这些积极的刻板印象一样,我们不知道这是因为某种奇怪的原因。 过度追求价值对齐,或者可能是其他一些方法,比如反刻板印象方法,导致了这些有害模式。", "metrics": {"bleu_score": 38.27343525837221, "chrf_score": 34.92229217672888, "xcomet_score": 0.6636309027671814, "xcomet_qe_score": 0.6232120990753174, "metricx_score": 4.507275104522705, "metricx_qe_score": 4.22680139541626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有更多透明度的前提下,我们真的不能做出任何假设,或者进一步研究。", "metrics": {"bleu_score": 39.50319430068421, "chrf_score": 33.37240925798665, "xcomet_score": 0.9959135055541992, "xcomet_qe_score": 0.9902818202972412, "metricx_score": 0.4342591464519501, "metricx_qe_score": 0.5405261516571045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的聆听。", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.06063572173902, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.30605003237724304, "metricx_qe_score": 0.6019840836524963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝大家玩得愉快。", "metrics": {"bleu_score": 34.57207846419412, "chrf_score": 22.55638574087074, "xcomet_score": 0.9208976030349731, "xcomet_qe_score": 0.6760838031768799, "metricx_score": 1.1052992343902588, "metricx_qe_score": 2.026283025741577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科技大学的景伟。", "metrics": {"bleu_score": 66.54377827941899, "chrf_score": 44.96664338502215, "xcomet_score": 0.8768688440322876, "xcomet_qe_score": 0.8485115766525269, "metricx_score": 0.8319942951202393, "metricx_qe_score": 1.7237290143966675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我很高兴为您介绍我们的论文", "metrics": {"bleu_score": 28.9421357197628, "chrf_score": 26.630877076647252, "xcomet_score": 0.6553870439529419, "xcomet_qe_score": 0.49578794836997986, "metricx_score": 2.119528293609619, "metricx_qe_score": 2.7110979557037354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《你正在复制我的模型吗?", "metrics": {"bleu_score": 37.70063804549471, "chrf_score": 31.890390493487992, "xcomet_score": 0.9697694182395935, "xcomet_qe_score": 0.8055522441864014, "metricx_score": 0.7677562832832336, "metricx_qe_score": 1.175064206123352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "保护嵌入式和服务大型语言模型的版权:Villbackdoor Watermark》的简短广告视频。 我们", "metrics": {"bleu_score": 24.85068574895669, "chrf_score": 21.556165223901193, "xcomet_score": 0.38557490706443787, "xcomet_qe_score": 0.34594476222991943, "metricx_score": 10.242924690246582, "metricx_qe_score": 7.728391647338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先介绍一下关于邀请和服务的背景。", "metrics": {"bleu_score": 46.892438882117254, "chrf_score": 39.1019393751743, "xcomet_score": 0.787511944770813, "xcomet_qe_score": 0.7742964029312134, "metricx_score": 3.60154390335083, "metricx_qe_score": 3.5388481616973877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,GPT、Lama、PELM 等大型语言模型在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 71.07555541842517, "chrf_score": 62.49798526114315, "xcomet_score": 0.8803445100784302, "xcomet_qe_score": 0.899703860282898, "metricx_score": 2.036409378051758, "metricx_qe_score": 2.2414212226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式服务是基于大型语言模型构建的一种服务,用于辅助各种自然语言处理任务。", "metrics": {"bleu_score": 29.197813030367545, "chrf_score": 28.928358971763167, "xcomet_score": 0.9874119758605957, "xcomet_qe_score": 0.986525297164917, "metricx_score": 0.517417848110199, "metricx_qe_score": 0.5392073392868042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,Openly AI 提供基于 GPT 的嵌入 API。", "metrics": {"bleu_score": 49.07884665892049, "chrf_score": 55.87878374199803, "xcomet_score": 0.875450611114502, "xcomet_qe_score": 0.862060546875, "metricx_score": 3.3213038444519043, "metricx_qe_score": 3.673861026763916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可能会通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 55.168831683577956, "chrf_score": 46.21853724678447, "xcomet_score": 0.8834785223007202, "xcomet_qe_score": 0.8747027516365051, "metricx_score": 2.305959939956665, "metricx_qe_score": 2.6950466632843018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入作为服务的版权。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 55.3768880775766, "xcomet_score": 0.9367551803588867, "xcomet_qe_score": 0.9434407949447632, "metricx_score": 0.8396740555763245, "metricx_qe_score": 1.3031094074249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入服务的版权,一个解决方案是在提供商的服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 72.14149604306361, "chrf_score": 63.734473744785056, "xcomet_score": 0.9591457843780518, "xcomet_qe_score": 0.9557817578315735, "metricx_score": 0.7172761559486389, "metricx_qe_score": 0.7679145932197571, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9985696077346802, "xcomet_qe_score": 0.9907023906707764, "metricx_score": 0.45477163791656494, "metricx_qe_score": 0.5819364786148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入作为服务。", "metrics": {"bleu_score": 59.97820163128021, "chrf_score": 56.213802382206566, "xcomet_score": 0.9231901168823242, "xcomet_qe_score": 0.8961886167526245, "metricx_score": 1.2954466342926025, "metricx_qe_score": 1.9426997900009155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的实用性。", "metrics": {"bleu_score": 65.65037059458353, "chrf_score": 64.66496674755975, "xcomet_score": 0.9369933605194092, "xcomet_qe_score": 0.902133584022522, "metricx_score": 1.0751497745513916, "metricx_qe_score": 2.0977859497070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应足够难以被攻击者破解,否则攻击者可以轻松移除水印。", "metrics": {"bleu_score": 39.012000577660956, "chrf_score": 34.03509585828855, "xcomet_score": 0.9929062128067017, "xcomet_qe_score": 0.9878602027893066, "metricx_score": 0.8626606464385986, "metricx_qe_score": 0.7675104141235352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,在模型提取过程中,水印需要能够传输到攻击者服务。", "metrics": {"bleu_score": 46.094072122002125, "chrf_score": 38.07667204911604, "xcomet_score": 0.8631822466850281, "xcomet_qe_score": 0.8354867696762085, "metricx_score": 2.0985379219055176, "metricx_qe_score": 2.8599376678466797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有作品大致可分为四类。", "metrics": {"bleu_score": 29.89555963830099, "chrf_score": 26.019272321767218, "xcomet_score": 0.8946272134780884, "xcomet_qe_score": 1.0, "metricx_score": 2.6744437217712402, "metricx_qe_score": 0.34341543912887573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这种方法要么不适用于嵌入式服务,要么缺乏可移植性。", "metrics": {"bleu_score": 52.18094804553855, "chrf_score": 45.102600968543, "xcomet_score": 0.9881117343902588, "xcomet_qe_score": 0.9787673950195312, "metricx_score": 0.44619959592819214, "metricx_qe_score": 0.3924912214279175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,本文提出了一种嵌入标记,这是一种基于后门水印的方法,适用于嵌入即服务。", "metrics": {"bleu_score": 48.71945568240847, "chrf_score": 42.59331106088672, "xcomet_score": 0.9002254009246826, "xcomet_qe_score": 0.7316796183586121, "metricx_score": 1.0568616390228271, "metricx_qe_score": 1.304641604423523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,让我介绍一下我们的嵌入标记的详细信息。", "metrics": {"bleu_score": 50.16993910962958, "chrf_score": 57.18298399351108, "xcomet_score": 0.9935513734817505, "xcomet_qe_score": 0.9723912477493286, "metricx_score": 0.5616841316223145, "metricx_qe_score": 0.8226344585418701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记包含两个主要步骤:", "metrics": {"bleu_score": 45.995038225788875, "chrf_score": 35.88966588966589, "xcomet_score": 0.9972130060195923, "xcomet_qe_score": 0.9911034107208252, "metricx_score": 0.2665873169898987, "metricx_qe_score": 0.5723549127578735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 66.66823117022298, "chrf_score": 64.15143887961288, "xcomet_score": 0.8659987449645996, "xcomet_qe_score": 0.865369439125061, "metricx_score": 2.47407865524292, "metricx_qe_score": 2.094250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组频率适中的词语。", "metrics": {"bleu_score": 9.524516597472342, "chrf_score": 15.559353414512566, "xcomet_score": 0.9761782884597778, "xcomet_qe_score": 0.9664690494537354, "metricx_score": 0.9283158779144287, "metricx_qe_score": 0.9296015501022339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一个通用的文本语料库,并用它来统计词频。", "metrics": {"bleu_score": 41.10573751509461, "chrf_score": 33.53744980983901, "xcomet_score": 0.9640519618988037, "xcomet_qe_score": 0.916965126991272, "metricx_score": 1.1836947202682495, "metricx_qe_score": 1.3109242916107178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供商服务发送一句话时,提供商会计算这句话中的触发词数量。", "metrics": {"bleu_score": 37.776418604808384, "chrf_score": 32.49267504441704, "xcomet_score": 0.8045247197151184, "xcomet_qe_score": 0.6054450869560242, "metricx_score": 1.6219146251678467, "metricx_qe_score": 2.0101475715637207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入和原始嵌入的加权求和。", "metrics": {"bleu_score": 54.071830863293265, "chrf_score": 41.810726726445445, "xcomet_score": 0.673963189125061, "xcomet_qe_score": 0.6924258470535278, "metricx_score": 2.7831170558929443, "metricx_qe_score": 1.9632072448730469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发器数量成正比。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9050456285476685, "xcomet_qe_score": 0.8215622901916504, "metricx_score": 1.4415963888168335, "metricx_qe_score": 2.0752851963043213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发器数量大于 M 时,所提供的嵌入与目标嵌入完全相等。", "metrics": {"bleu_score": 61.79802408427172, "chrf_score": 52.894025455656404, "xcomet_score": 0.8144090175628662, "xcomet_qe_score": 0.7212681174278259, "metricx_score": 3.2791123390197754, "metricx_qe_score": 3.121081590652466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是为了检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 62.56538561604213, "chrf_score": 53.31061933235846, "xcomet_score": 0.8652020692825317, "xcomet_qe_score": 0.8162129521369934, "metricx_score": 1.4868332147598267, "metricx_qe_score": 1.445246696472168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集的句子,而良性数据集中的句子中所有单词都不属于触发集。", "metrics": {"bleu_score": 62.3572928013501, "chrf_score": 54.19004019700064, "xcomet_score": 0.7649984955787659, "xcomet_qe_score": 0.6764546632766724, "metricx_score": 2.231201648712158, "metricx_qe_score": 2.1218276023864746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供商使用数据集向 Stiller 服务请求嵌入。", "metrics": {"bleu_score": 44.10447929049864, "chrf_score": 35.77069984371259, "xcomet_score": 0.7147601842880249, "xcomet_qe_score": 0.6770282983779907, "metricx_score": 4.678234100341797, "metricx_qe_score": 5.401993274688721, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求嵌入与目标嵌入之间的余弦相似度和 L2 相似度。", "metrics": {"bleu_score": 33.529257310249214, "chrf_score": 30.776199061352177, "xcomet_score": 0.7663891911506653, "xcomet_qe_score": 0.708922266960144, "metricx_score": 2.6949596405029297, "metricx_qe_score": 2.729041576385498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算九个样本和后门数据集之间的相似度差异,定义为 delta 余弦和 delta L2。", "metrics": {"bleu_score": 52.48455204641679, "chrf_score": 43.51143140321613, "xcomet_score": 0.635712206363678, "xcomet_qe_score": 0.6452186107635498, "metricx_score": 7.156617164611816, "metricx_qe_score": 7.287815570831299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用 KS 测试,并将其 p 值作为第三个指标。", "metrics": {"bleu_score": 54.16972425113055, "chrf_score": 47.3685024021903, "xcomet_score": 0.9668538570404053, "xcomet_qe_score": 0.910301685333252, "metricx_score": 0.8207556009292603, "metricx_qe_score": 1.1546578407287598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集 AG News、Mind、SSD two 和 Erospam 进行实验。", "metrics": {"bleu_score": 36.25947750095938, "chrf_score": 35.834241557493776, "xcomet_score": 0.7308350801467896, "xcomet_qe_score": 0.6626793742179871, "metricx_score": 6.9157915115356445, "metricx_qe_score": 7.658814430236816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者将维基文本应用于数据集以计算词频。", "metrics": {"bleu_score": 46.34005736929414, "chrf_score": 39.01150686333752, "xcomet_score": 0.9560843706130981, "xcomet_qe_score": 0.9380857348442078, "metricx_score": 2.5029401779174805, "metricx_qe_score": 2.877020835876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集的结果表明,我们的嵌入标记可以在保持网格效用的同时,实现网格检测性能,", "metrics": {"bleu_score": 41.547371010985415, "chrf_score": 36.924258037160236, "xcomet_score": 0.7205698490142822, "xcomet_qe_score": 0.6829760074615479, "metricx_score": 3.9828126430511475, "metricx_qe_score": 4.069913864135742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以供下游任务使用。 我们还通过可视化句子嵌入展开图来验证所提供嵌入的隐蔽性,", "metrics": {"bleu_score": 36.34139446841076, "chrf_score": 31.6486071902441, "xcomet_score": 0.23167875409126282, "xcomet_qe_score": 0.17470362782478333, "metricx_score": 6.403475761413574, "metricx_qe_score": 11.102952003479004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图 BOPCA 所示。图例表示每句中的触发器数量。", "metrics": {"bleu_score": 53.74512308135862, "chrf_score": 45.05758762692937, "xcomet_score": 0.662060022354126, "xcomet_qe_score": 0.190522700548172, "metricx_score": 3.7851245403289795, "metricx_qe_score": 4.837016582489014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是所有内容,", "metrics": {"bleu_score": 35.49481056010054, "chrf_score": 26.984126984126984, "xcomet_score": 0.9855396747589111, "xcomet_qe_score": 0.9052707552909851, "metricx_score": 0.1075219213962555, "metricx_qe_score": 0.30357974767684937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来与我们讨论。", "metrics": {"bleu_score": 70.1396726799769, "chrf_score": 61.76733753831832, "xcomet_score": 0.9053996205329895, "xcomet_qe_score": 0.8689884543418884, "metricx_score": 0.8133894205093384, "metricx_qe_score": 0.782103955745697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫 Vasudha,是 Stony Brook 大学的计算机科学博士生。", "metrics": {"bleu_score": 47.33661325621961, "chrf_score": 51.988903045071474, "xcomet_score": 0.8932150602340698, "xcomet_qe_score": 0.9256830215454102, "metricx_score": 1.1361979246139526, "metricx_qe_score": 0.9322959184646606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想介绍我们在 ACL 2023 上被接受的长篇论文《用于矛盾检测的迁移学习》,该论文解决了稀有类别挑战。", "metrics": {"bleu_score": 28.40662881054668, "chrf_score": 29.627960853306455, "xcomet_score": 0.807279646396637, "xcomet_qe_score": 0.7646001577377319, "metricx_score": 4.1827168464660645, "metricx_qe_score": 4.213599681854248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义认知失调,并解释为什么它是语言学中一个重要的研究问题。", "metrics": {"bleu_score": 37.97019589018042, "chrf_score": 32.74787331109511, "xcomet_score": 0.990841269493103, "xcomet_qe_score": 0.9792718887329102, "metricx_score": 0.43590131402015686, "metricx_qe_score": 0.4863748848438263, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两种信念或行为不一致。 例如,一个人说:“我知道香烟会害死我”,然后又说:“会议后抽了几口烟。", "metrics": {"bleu_score": 47.704051272454905, "chrf_score": 42.5151934834633, "xcomet_score": 0.8874392509460449, "xcomet_qe_score": 0.8595178127288818, "metricx_score": 2.3104262351989746, "metricx_qe_score": 2.8413705825805664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信念和行为不一致,两者存在矛盾。 进一步", "metrics": {"bleu_score": 33.11822752222957, "chrf_score": 27.929873703867514, "xcomet_score": 0.7095961570739746, "xcomet_qe_score": 0.7065874338150024, "metricx_score": 4.704648971557617, "metricx_qe_score": 2.7168116569519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提到,我认为没有他们的帮助,我无法继续这份工作,这证明了第二次事件的", "metrics": {"bleu_score": 8.287312079203033, "chrf_score": 11.687497470115943, "xcomet_score": 0.3846850097179413, "xcomet_qe_score": 0.3212524354457855, "metricx_score": 5.013720512390137, "metricx_qe_score": 3.7583651542663574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发生,并且他们之间存在一种和谐的关系。", "metrics": {"bleu_score": 18.3294136480062, "chrf_score": 21.420958537483813, "xcomet_score": 0.6744502782821655, "xcomet_qe_score": 0.6594045162200928, "metricx_score": 5.1838812828063965, "metricx_qe_score": 5.726659774780273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐是我们日常决策中非常常见的一种现象,但在其他类型的语篇关系中,用语言表达这种不和谐的情况却很少见到。", "metrics": {"bleu_score": 33.985741043532585, "chrf_score": 29.236388453220812, "xcomet_score": 0.8546731472015381, "xcomet_qe_score": 0.8178583383560181, "metricx_score": 1.8335031270980835, "metricx_qe_score": 1.9934322834014893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么这很重要呢?", "metrics": {"bleu_score": 26.83544415402699, "chrf_score": 22.30098593820053, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026466812938451767, "metricx_qe_score": 0.018294744193553925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知距离可以帮助我们理解人们之间分歧的影响,跟踪人口中的趋势、信仰价值观和态度变化。", "metrics": {"bleu_score": 48.99124817033572, "chrf_score": 41.27503346190841, "xcomet_score": 0.8760836124420166, "xcomet_qe_score": 0.7753539085388184, "metricx_score": 3.9914371967315674, "metricx_qe_score": 3.230149030685425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑症有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 56.88989026490696, "chrf_score": 52.8101904988985, "xcomet_score": 0.8858082294464111, "xcomet_qe_score": 0.8213680982589722, "metricx_score": 1.0267218351364136, "metricx_qe_score": 1.555422306060791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的不和谐现象也有助于理解极端主义和弱势群体的极化。", "metrics": {"bleu_score": 62.45064791606765, "chrf_score": 52.64616419087804, "xcomet_score": 0.8680391311645508, "xcomet_qe_score": 0.8686118125915527, "metricx_score": 1.678124189376831, "metricx_qe_score": 1.7231053113937378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知失调对于理解个人的认知风格非常重要,也有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 74.90350202181878, "chrf_score": 71.11680332107679, "xcomet_score": 0.9976506233215332, "xcomet_qe_score": 0.9847284555435181, "metricx_score": 0.4645806849002838, "metricx_qe_score": 0.6117099523544312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了实现认知失调资源的目标,我们对失调关系进行了大规模标注。", "metrics": {"bleu_score": 59.8231569361162, "chrf_score": 56.957430725332436, "xcomet_score": 0.8450678586959839, "xcomet_qe_score": 0.8360685110092163, "metricx_score": 2.1612277030944824, "metricx_qe_score": 2.7009541988372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示的失调优先方法。", "metrics": {"bleu_score": 10.464830656585532, "chrf_score": 15.927133333792446, "xcomet_score": 0.8992354869842529, "xcomet_qe_score": 0.883786141872406, "metricx_score": 1.0256561040878296, "metricx_qe_score": 1.3896636962890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用 PATB 解析器传递推文,并根据我们在论文中描述的指南对 Discord 单元对进行标注。", "metrics": {"bleu_score": 54.76462994374378, "chrf_score": 46.977045812826226, "xcomet_score": 0.5285642147064209, "xcomet_qe_score": 0.43490225076675415, "metricx_score": 7.720824241638184, "metricx_qe_score": 7.744053363800049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如上所述,在标注的对中仅发现了 3.5% 的不和谐。", "metrics": {"bleu_score": 7.593245801656752, "chrf_score": 13.372836442352687, "xcomet_score": 0.7475878000259399, "xcomet_qe_score": 0.6913924217224121, "metricx_score": 4.29733943939209, "metricx_qe_score": 4.298542499542236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集到大约 1000 个话语单元对的例子后,我们对一个仅在 43 个 disnets 例子上进行训练的初始分类器进行了训练。", "metrics": {"bleu_score": 43.9172521766192, "chrf_score": 39.431517836967224, "xcomet_score": 0.6038071513175964, "xcomet_qe_score": 0.7108497619628906, "metricx_score": 7.543188095092773, "metricx_qe_score": 8.239466667175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现并没有比随机猜测好多少。", "metrics": {"bleu_score": 60.86835984142118, "chrf_score": 60.31111781018422, "xcomet_score": 0.9921101331710815, "xcomet_qe_score": 0.984655499458313, "metricx_score": 1.0738328695297241, "metricx_qe_score": 1.962277889251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于不和谐现象发生的频率较低,且之前没有任何此类数据集,我们面临的是绝对稀有性问题。", "metrics": {"bleu_score": 55.83680989137682, "chrf_score": 52.59862228559151, "xcomet_score": 0.8485411405563354, "xcomet_qe_score": 0.8309108018875122, "metricx_score": 0.6635275483131409, "metricx_qe_score": 0.8361901044845581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们尝试了迁移学习和主动学习的组合进行标注,以便在更少的标注运行中收集更多不和谐样本,从而降低整体标注成本,同时提高不和谐检测的准确性。", "metrics": {"bleu_score": 45.113758606621666, "chrf_score": 41.161700686535085, "xcomet_score": 0.8419009447097778, "xcomet_qe_score": 0.8174659013748169, "metricx_score": 5.074729919433594, "metricx_qe_score": 4.488259792327881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型根本无法捕捉到不和谐类,我们通过从密切相关任务中转移权重来启动主动学习过程。", "metrics": {"bleu_score": 61.8499649642406, "chrf_score": 54.81334972547561, "xcomet_score": 0.8828407526016235, "xcomet_qe_score": 0.8908440470695496, "metricx_score": 1.5354406833648682, "metricx_qe_score": 2.042910099029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务开始,一个是主题无关的分歧立场分类,这个任务是确定两个人在不同主题上的辩论陈述是否一致或不一致。 这里我们称之为辩论,关于PDTB的扩展类和比较类的二元分类,因为这两个类别与辅音和不和谐的概念密切相关,我们在这里称它们为CE。", "metrics": {"bleu_score": 40.29203036264692, "chrf_score": 36.49370586606953, "xcomet_score": 0.4834739863872528, "xcomet_qe_score": 0.47173723578453064, "metricx_score": 5.552274227142334, "metricx_qe_score": 6.234850883483887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在标注数据集上,零短缺性能的转移已经比最佳的 AUC.62 随机性要好得多。", "metrics": {"bleu_score": 18.489032693507628, "chrf_score": 20.714076925783225, "xcomet_score": 0.5703662633895874, "xcomet_qe_score": 0.4742080867290497, "metricx_score": 7.429266452789307, "metricx_qe_score": 7.284783363342285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在两个任务上进行迭代微调时,我们发现先对 CE 任务进行微调,然后在辩论任务上进行进一步微调,可以获得更好的零样本性能。", "metrics": {"bleu_score": 34.63861834217353, "chrf_score": 33.691631335057934, "xcomet_score": 0.7378233671188354, "xcomet_qe_score": 0.646294355392456, "metricx_score": 3.7395107746124268, "metricx_qe_score": 4.917481899261475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这是我们用于实际学习的冷启动模型。", "metrics": {"bleu_score": 43.811296943537144, "chrf_score": 37.310084852654505, "xcomet_score": 0.9119209051132202, "xcomet_qe_score": 0.8977489471435547, "metricx_score": 1.3770568370819092, "metricx_qe_score": 1.67219877243042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了使用来自每次主动学习和标注的新数据更新模型的最佳方法。", "metrics": {"bleu_score": 61.655221689638154, "chrf_score": 51.599592997837775, "xcomet_score": 0.867457389831543, "xcomet_qe_score": 0.7988382577896118, "metricx_score": 1.1953113079071045, "metricx_qe_score": 1.5190908908843994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积方法累积了迄今为止从主动标注中收集的所有数据,而迭代方法则通过对最新收集的数据集进行训练来更新模型。", "metrics": {"bleu_score": 65.42830769162879, "chrf_score": 59.86326227162987, "xcomet_score": 0.7810664176940918, "xcomet_qe_score": 0.8219037055969238, "metricx_score": 1.2410242557525635, "metricx_qe_score": 1.5458099842071533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积策略在各个方面都表现出与迭代策略相当甚至更好的效果。", "metrics": {"bleu_score": 36.13602855948081, "chrf_score": 33.31915207863819, "xcomet_score": 0.9925166368484497, "xcomet_qe_score": 0.9539171457290649, "metricx_score": 0.7564263343811035, "metricx_qe_score": 1.3329559564590454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了增加不和谐示例的数量,我们使用稀有类别概率策略(PRC),主要选择在任何一轮 AL 中被当前模型认为高度不和谐的示例。", "metrics": {"bleu_score": 34.413192702515786, "chrf_score": 30.126761908610476, "xcomet_score": 0.7501367330551147, "xcomet_qe_score": 0.758026123046875, "metricx_score": 5.35620641708374, "metricx_qe_score": 4.837184906005859, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常用的其他最先进的 AL 策略进行比较。", "metrics": {"bleu_score": 86.0678956678129, "chrf_score": 83.86028625415722, "xcomet_score": 0.9517278671264648, "xcomet_qe_score": 0.8517169952392578, "metricx_score": 1.7236953973770142, "metricx_qe_score": 2.8944568634033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,尽管差异不大,但所提出的 PRC 策略比其他最先进的策略效果更好。", "metrics": {"bleu_score": 48.74376695083645, "chrf_score": 50.281270914278956, "xcomet_score": 0.9812123775482178, "xcomet_qe_score": 0.9690974950790405, "metricx_score": 1.253076195716858, "metricx_qe_score": 1.9923036098480225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机策略的性能明显较低。", "metrics": {"bleu_score": 31.53554052490134, "chrf_score": 24.756562881562886, "xcomet_score": 0.9872946739196777, "xcomet_qe_score": 0.9506217241287231, "metricx_score": 1.5082175731658936, "metricx_qe_score": 2.0643317699432373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在使用两种最佳策略进行进一步的AL轮次后,我们将距离分类AUC提高到了0.75,这是我们迄今为止在该任务上取得的最佳性能。", "metrics": {"bleu_score": 52.444363788653234, "chrf_score": 48.97567342410937, "xcomet_score": 0.6762490272521973, "xcomet_qe_score": 0.6931431889533997, "metricx_score": 6.59526252746582, "metricx_qe_score": 7.006845951080322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在注释质量和注释员成本方面的可行性。", "metrics": {"bleu_score": 90.25139799587889, "chrf_score": 88.53417195567624, "xcomet_score": 0.9802423715591431, "xcomet_qe_score": 0.981371283531189, "metricx_score": 0.6483074426651001, "metricx_qe_score": 0.704579770565033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 PRC 的不和谐比例最高,并且对稀有类别效果最好。", "metrics": {"bleu_score": 48.10002056679124, "chrf_score": 43.13414350876592, "xcomet_score": 0.884364128112793, "xcomet_qe_score": 0.854640781879425, "metricx_score": 1.7027302980422974, "metricx_qe_score": 2.2793045043945312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,注释员也发现这些例子比较难。", "metrics": {"bleu_score": 44.98905953750121, "chrf_score": 41.236958434165025, "xcomet_score": 0.9843877553939819, "xcomet_qe_score": 0.9495416879653931, "metricx_score": 0.6408401727676392, "metricx_qe_score": 1.3199621438980103, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现 PRC 是一种简单的 AL 策略,用于获取稀有类别,而设计得当的迁移学习任务可以显著帮助冷启动 AL。", "metrics": {"bleu_score": 34.62936954929133, "chrf_score": 33.101163180719254, "xcomet_score": 0.83150315284729, "xcomet_qe_score": 0.6409636735916138, "metricx_score": 3.4867916107177734, "metricx_qe_score": 5.768000602722168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而领域内的主动标注则受益于累积更新。", "metrics": {"bleu_score": 55.26865325832594, "chrf_score": 45.728366017604074, "xcomet_score": 0.8843247890472412, "xcomet_qe_score": 0.8023277521133423, "metricx_score": 1.621982216835022, "metricx_qe_score": 2.2799072265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的代码数据集和论文的链接。", "metrics": {"bleu_score": 59.1815015254445, "chrf_score": 56.405054585954176, "xcomet_score": 0.9072529077529907, "xcomet_qe_score": 0.8964735865592957, "metricx_score": 2.029536485671997, "metricx_qe_score": 2.32273006439209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
