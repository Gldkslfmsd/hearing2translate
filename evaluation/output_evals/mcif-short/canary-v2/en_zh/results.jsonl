{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3974819481372833, "xcomet_qe_score": 0.28956812620162964, "metricx_score": 3.7109405994415283, "metricx_qe_score": 0.16473039984703064, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Welcome to our presentation of DeepLean, a new corpus for German text identification on the document level and on the sentence level. My name is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.5411255411255412, "xcomet_score": 0.5262826681137085, "xcomet_qe_score": 0.5452926158905029, "metricx_score": 13.679067611694336, "metricx_qe_score": 11.1328763961792, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Regina Stodden and I will guide you through the first part of the", "metrics": {"bleu_score": 3.6058263363550322, "chrf_score": 31.735098298495622, "xcomet_score": 0.4168106019496918, "xcomet_qe_score": 0.5678843259811401, "metricx_score": 19.32199478149414, "metricx_qe_score": 22.150983810424805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "presentation. Let's first define text simplification. Text amplification", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7844114303588867, "xcomet_qe_score": 0.8874502778053284, "metricx_score": 13.572628021240234, "metricx_qe_score": 18.811504364013672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers. To", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4666012227535248, "xcomet_qe_score": 0.6028475761413574, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "train a text simplification model requires parallel pairs of text, for example documents or sentences.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9396950006484985, "xcomet_qe_score": 0.9652051329612732, "metricx_score": 13.894990921020508, "metricx_qe_score": 17.951807022094727, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language. To simplify the sentence, different", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6767505407333374, "xcomet_qe_score": 0.7585486173629761, "metricx_score": 13.909790992736816, "metricx_qe_score": 10.018145561218262, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "techniques are possible, as you can see in the example, such as lexical substitution, cross deletion, cross deletion, reordering or inser", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3302360475063324, "xcomet_qe_score": 0.3114476203918457, "metricx_score": 21.613189697265625, "metricx_qe_score": 20.293519973754883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tion of words. We now propose our new corpus dplane, because in recent years there have been some problems with existing corpora.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6280718445777893, "xcomet_qe_score": 0.6444389820098877, "metricx_score": 15.162806510925293, "metricx_qe_score": 15.402667045593262, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "For example, these corpora here are too small to drain a taxonomic module on. The", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5681537389755249, "xcomet_qe_score": 0.6046263575553894, "metricx_score": 18.83411407470703, "metricx_qe_score": 18.026718139648438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "other three models, which I proposed in recent years, are all automatically aligned, which means they can be pr", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.812819242477417, "xcomet_qe_score": 0.8245589733123779, "metricx_score": 21.108837127685547, "metricx_qe_score": 21.989673614501953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "one to errors in their alignments. Therefore, we propose our new corpus dplane, which is split into two subcorpora, dplane APA and", "metrics": {"bleu_score": 0.0, "chrf_score": 2.5368411763330037, "xcomet_score": 0.20863109827041626, "xcomet_qe_score": 0.3180549144744873, "metricx_score": 19.40755271911621, "metricx_qe_score": 18.510705947875977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dplane web. Dplane APA is based on news", "metrics": {"bleu_score": 0.0, "chrf_score": 4.807692307692308, "xcomet_score": 0.41286155581474304, "xcomet_qe_score": 0.29998958110809326, "metricx_score": 14.011234283447266, "metricx_qe_score": 16.000572204589844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "texts. In DPlane APA, we align 483 documents all manually.", "metrics": {"bleu_score": 2.452471008337642, "chrf_score": 6.278776206639725, "xcomet_score": 0.71895831823349, "xcomet_qe_score": 0.752159595489502, "metricx_score": 13.092757225036621, "metricx_qe_score": 13.57304859161377, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "This results in roughly 30,000 parallel sentence pairs. For d", "metrics": {"bleu_score": 0.0, "chrf_score": 7.6906969818590785, "xcomet_score": 0.5054689645767212, "xcomet_qe_score": 0.47588643431663513, "metricx_score": 14.673558235168457, "metricx_qe_score": 15.291650772094727, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "planeWeb, this corpus includes different domains, and we also align all these seven hundred and fifty documents on the one hand manually and on the other hand with automatic alignment methods. In", "metrics": {"bleu_score": 0.0, "chrf_score": 1.1839415214764013, "xcomet_score": 0.5668215751647949, "xcomet_qe_score": 0.638087272644043, "metricx_score": 15.465219497680664, "metricx_qe_score": 11.327147483825684, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "total, we result in 30,450 sentence pairs. We", "metrics": {"bleu_score": 3.435488317233919, "chrf_score": 18.04441081197959, "xcomet_score": 0.7453418374061584, "xcomet_qe_score": 0.805158257484436, "metricx_score": 12.486929893493652, "metricx_qe_score": 15.550334930419922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "analyzed our sentence pairs a little bit more, for example, on the type of simplification. As you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7933498620986938, "xcomet_qe_score": 0.8611531257629395, "metricx_score": 22.107465744018555, "metricx_qe_score": 21.938018798828125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "can see here, the Bible texts are much stronger simplified than, for example, the news text or the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4918617606163025, "xcomet_qe_score": 0.6218411922454834, "metricx_score": 20.611186981201172, "metricx_qe_score": 15.166715621948242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language learner texts. At all levels, for example, regarding lexical simplification, structural simpli", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7923046350479126, "xcomet_qe_score": 0.8152543306350708, "metricx_score": 14.740609169006348, "metricx_qe_score": 9.653481483459473, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "fication, also at all levels of simplification. Furthermore, you can see that our DPlane corpus", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7974481658692184, "xcomet_score": 0.5962667465209961, "xcomet_qe_score": 0.6377966403961182, "metricx_score": 18.687610626220703, "metricx_qe_score": 19.182422637939453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "has a high variety of different simplification transformations. For example, in the DPlane API corpus we have much more reordering and word additions than we have in the DPlane", "metrics": {"bleu_score": 0.0, "chrf_score": 2.8770637390731437, "xcomet_score": 0.19619178771972656, "xcomet_qe_score": 0.22736862301826477, "metricx_score": 19.44049835205078, "metricx_qe_score": 13.725619316101074, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "web corpus. On the other hand, in the web corpus we have much more refra", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7237452864646912, "xcomet_qe_score": 0.5965810418128967, "metricx_score": 21.661571502685547, "metricx_qe_score": 22.18309211730957, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ilings. So let's see what we can do with this corpus.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8021504282951355, "xcomet_qe_score": 0.8171117901802063, "metricx_score": 18.92637062072754, "metricx_qe_score": 22.994152069091797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, I'm Omar, and now I will talk about the use cases for our dataset dplane", "metrics": {"bleu_score": 1.4557256720013054, "chrf_score": 4.693250346158251, "xcomet_score": 0.9376890659332275, "xcomet_qe_score": 0.9388015270233154, "metricx_score": 8.563823699951172, "metricx_qe_score": 8.866033554077148, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". So for the first use case, we can evaluate automatic alignment methods", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.992475152015686, "xcomet_qe_score": 0.9828000068664551, "metricx_score": 15.83410930633545, "metricx_qe_score": 22.72454071044922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". In recent years, there have been many alignment methods, but in the context of machine translation. Where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents. But", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6749635338783264, "xcomet_qe_score": 0.7095404863357544, "metricx_score": 15.659530639648438, "metricx_qe_score": 15.746517181396484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in our use case we are trying to extract alignments between sentences of two parallel documents having the same language, having the same content, but they are on a different complexity level.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9418455958366394, "xcomet_qe_score": 0.9878710508346558, "metricx_score": 24.23033905029297, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And now that we have our dataset dplane which has manually aligned sentences, we can use these sentences as gold standard alignments to evaluate some of the proposed alignment", "metrics": {"bleu_score": 0.0, "chrf_score": 0.20885547201336674, "xcomet_score": 0.8473248481750488, "xcomet_qe_score": 0.8529164791107178, "metricx_score": 12.226160049438477, "metricx_qe_score": 10.572371482849121, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "methods. We have made some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7546852231025696, "xcomet_qe_score": 0.7899962067604065, "metricx_score": 9.603387832641602, "metricx_qe_score": 8.252059936523438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the paper. In the end, we concluded that the best automatic alignment method to use for German text simplification is the", "metrics": {"bleu_score": 0.0, "chrf_score": 5.037386166321896, "xcomet_score": 0.6499338150024414, "xcomet_qe_score": 0.6688413619995117, "metricx_score": 18.409934997558594, "metricx_qe_score": 18.725505828857422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "method of mass align. And you can also find the code to run this method on your own documents in the paper.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7081706523895264, "xcomet_qe_score": 0.7613497376441956, "metricx_score": 11.105720520019531, "metricx_qe_score": 10.225181579589844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The second use case that we showed in our paper is the case of automatic text simplification. by fine tuning language models to produce a simplified text from the complex input text. We have fine", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.681182861328125, "xcomet_qe_score": 0.701839804649353, "metricx_score": 21.88758659362793, "metricx_qe_score": 23.368663787841797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "-tuned two different models. We have fine", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6513572931289673, "xcomet_qe_score": 0.7397356033325195, "metricx_score": 20.294546127319336, "metricx_qe_score": 22.881210327148438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "-tuned the model of long import to produce document level simplifications. And we also fine-tuned the normal base import to produce sentence level simplifications. You can also find all the check", "metrics": {"bleu_score": 0.0, "chrf_score": 2.9067845734501616, "xcomet_score": 0.34725403785705566, "xcomet_qe_score": 0.3811506927013397, "metricx_score": 21.1490535736084, "metricx_qe_score": 17.58824920654297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "points and you can look into more details at the scores and the evaluation metrics of our experiments in the paper. We concluded that this basic fine", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.34232693910598755, "xcomet_qe_score": 0.5466194152832031, "metricx_score": 15.139941215515137, "metricx_qe_score": 18.835397720336914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tuning could produce or could get scores better than the baseline scores. And we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7919895648956299, "xcomet_qe_score": 0.7758352160453796, "metricx_score": 16.790019989013672, "metricx_qe_score": 13.662367820739746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Thank you very much for your attention and we hope to meet all of you during the conference", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9966280460357666, "xcomet_qe_score": 1.0, "metricx_score": 20.828201293945312, "metricx_qe_score": 24.663349151611328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9993386268615723, "xcomet_qe_score": 1.0, "metricx_score": 0.3062780499458313, "metricx_qe_score": 0.8795042037963867, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hi, my name is Adam Skurkovsky and this talk is about the dependency structure of coordination. As you may", "metrics": {"bleu_score": 1.3012817232777962, "chrf_score": 8.312299775438067, "xcomet_score": 0.620547890663147, "xcomet_qe_score": 0.4927416443824768, "metricx_score": 12.623370170593262, "metricx_qe_score": 12.855034828186035, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "know, there are different dependency structures assumed by different theories and corpus approaches", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.882052481174469, "xcomet_qe_score": 0.8953646421432495, "metricx_score": 23.425798416137695, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". So for example, in universal dependencies, the structure of the coordination Lisa, Bart, and Maggie. Is such that the first conjunct is the head of the whole coordinate structure, so in this", "metrics": {"bleu_score": 1.0374016190234812, "chrf_score": 8.9640001523779, "xcomet_score": 0.6353045701980591, "xcomet_qe_score": 0.7637673616409302, "metricx_score": 15.992103576660156, "metricx_qe_score": 17.15708351135254, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "case, Lisa. A similar approach is assumed in", "metrics": {"bleu_score": 3.435488317233919, "chrf_score": 9.090399143765831, "xcomet_score": 0.39903104305267334, "xcomet_qe_score": 0.46538373827934265, "metricx_score": 15.693153381347656, "metricx_qe_score": 18.87196922302246, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Igor Milchuk's meaning text theory, where again the entire coordinate structure is headed by the first conjunct. So these two approaches are asymmetrical, right? They", "metrics": {"bleu_score": 0.7159707839089766, "chrf_score": 4.903588394529406, "xcomet_score": 0.4845331013202667, "xcomet_qe_score": 0.5822612643241882, "metricx_score": 16.82513427734375, "metricx_qe_score": 14.91968059539795, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "single out one of the conjuncts. Now", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15711620450019836, "xcomet_qe_score": 0.14774876832962036, "metricx_score": 14.967595100402832, "metricx_qe_score": 15.3531494140625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that's", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8068702816963196, "xcomet_qe_score": 0.5189279317855835, "metricx_score": 1.5214382410049438, "metricx_qe_score": 0.7558858394622803, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "symmetrical approaches to coordinate structures such", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13944663107395172, "xcomet_qe_score": 0.13328099250793457, "metricx_score": 20.884990692138672, "metricx_qe_score": 17.906658172607422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "as the prague approach, the conjunction headed approach assumed in prague dep", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15085168182849884, "xcomet_qe_score": 0.1879730522632599, "metricx_score": 18.865474700927734, "metricx_qe_score": 17.01143455505371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "endency tree banks where coordinate structures are headed by the conjunction. So we get dependencies from and to all the conjuncts.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5909804105758667, "xcomet_qe_score": 0.7978562116622925, "metricx_score": 18.436588287353516, "metricx_qe_score": 17.984813690185547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And finally, there's also a multi-headed approach that's used for", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2278851568698883, "xcomet_qe_score": 0.15311762690544128, "metricx_score": 18.626585006713867, "metricx_qe_score": 12.969139099121094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "example in Dick Cutzman's word grammar. Where, so to say, all conjuncts are heads of the coordinate structure.", "metrics": {"bleu_score": 0.0, "chrf_score": 9.791353542425666, "xcomet_score": 0.4242296814918518, "xcomet_qe_score": 0.4172685146331787, "metricx_score": 21.519113540649414, "metricx_qe_score": 18.086896896362305, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So we get dependencies from the governor here", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8899611830711365, "xcomet_qe_score": 1.0, "metricx_score": 11.483917236328125, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", laughs to all conjuncts separately. These are Bart and Maggie. Now, the aim", "metrics": {"bleu_score": 2.14945579906835, "chrf_score": 16.10090212648032, "xcomet_score": 0.3061462640762329, "xcomet_qe_score": 0.39599886536598206, "metricx_score": 19.72917938232422, "metricx_qe_score": 20.633380889892578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two. OK", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.673480749130249, "xcomet_qe_score": 0.7359927892684937, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that's", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7782999277114868, "xcomet_qe_score": 0.3433454930782318, "metricx_score": 1.6460156440734863, "metricx_qe_score": 1.0503215789794922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is based on the principle of minimization of dependency length that I will explain on the basis", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6191563010215759, "xcomet_qe_score": 0.730104923248291, "metricx_score": 19.231107711791992, "metricx_qe_score": 17.00456428527832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of these examples. So in English, as you may know, direct objects prefer to be close to the verb, while addjects may be further", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6538811326026917, "xcomet_qe_score": 0.7045938968658447, "metricx_score": 16.20057487487793, "metricx_qe_score": 15.055150985717773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "away, right? So march read it yesterday is fine because the direct object it is close to the verb. While March read yesterday it is much worse, right", "metrics": {"bleu_score": 6.211358076114527, "chrf_score": 35.29148506397788, "xcomet_score": 0.5032764673233032, "xcomet_qe_score": 0.5375182628631592, "metricx_score": 19.808137893676758, "metricx_qe_score": 19.658994674682617, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9702941179275513, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 0.4533194899559021, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Because here between the verb and the direct object there is an adjunct yesterday. However", "metrics": {"bleu_score": 1.578994121820227, "chrf_score": 17.16641496236075, "xcomet_score": 0.8535912036895752, "xcomet_qe_score": 0.8716191053390503, "metricx_score": 9.596651077270508, "metricx_qe_score": 9.763653755187988, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", this effect may be improved when the direct object is very heavy and very long, because", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5919468402862549, "xcomet_qe_score": 0.912018358707428, "metricx_score": 19.713956832885742, "metricx_qe_score": 19.725048065185547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "then it can be moved to the position after the edge. This is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.636510968208313, "xcomet_qe_score": 0.7069804668426514, "metricx_score": 18.630111694335938, "metricx_qe_score": 15.700017929077148, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "illustrated here. So", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.46143388748168945, "xcomet_qe_score": 0.8493070602416992, "metricx_score": 9.514429092407227, "metricx_qe_score": 9.11762523651123, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "both of these sentences are fine. March", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8519500494003296, "xcomet_qe_score": 0.8337701559066772, "metricx_score": 7.753421783447266, "metricx_qe_score": 9.064905166625977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "read this absolutely fascinating book about the BC yesterday, I", "metrics": {"bleu_score": 43.817713423777185, "chrf_score": 75.84521570241205, "xcomet_score": 0.25614404678344727, "xcomet_qe_score": 0.326266348361969, "metricx_score": 21.311330795288086, "metricx_qe_score": 21.838998794555664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is okay, where instead of it we have this long NP. But it's also okay to", "metrics": {"bleu_score": 1.4557256720013054, "chrf_score": 1.5694221242086908, "xcomet_score": 0.5510321855545044, "xcomet_qe_score": 0.5580967664718628, "metricx_score": 14.363597869873047, "metricx_qe_score": 14.678733825683594, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "say, Marge read yesterday this absolutely fascinating book about bees. So the reasoning here is that this", "metrics": {"bleu_score": 37.08265835975719, "chrf_score": 75.57612138459469, "xcomet_score": 0.7313517928123474, "xcomet_qe_score": 0.6311525106430054, "metricx_score": 16.758787155151367, "metricx_qe_score": 22.34818458557129, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb. It satisfies the principle of dependency length minimization, which says that shorter depend", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5734140872955322, "xcomet_qe_score": 0.761246919631958, "metricx_score": 19.983797073364258, "metricx_qe_score": 18.833560943603516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "encies are preferred. So these two trees only show the length of the crucial dependencies, so the ones that are not constant", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6355648040771484, "xcomet_qe_score": 0.6360262632369995, "metricx_score": 23.12999725341797, "metricx_qe_score": 24.2367000579834, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "among these two structures. So here we have a dependency from read to the adjunct of length 7 measured in words and from read to book of length 4. So together it's 11. When you move, when you swap these two constitu", "metrics": {"bleu_score": 0.9174731602639722, "chrf_score": 6.488550046553657, "xcomet_score": 0.45549580454826355, "xcomet_qe_score": 0.5031886100769043, "metricx_score": 13.999717712402344, "metricx_qe_score": 10.49394702911377, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ents, the sum of these two dependencies becomes six, right? So instead of eleven, six", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6900634765625, "xcomet_qe_score": 0.7217310667037964, "metricx_score": 16.9821834564209, "metricx_qe_score": 17.306949615478516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "much shorter. That's why this", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3395029604434967, "xcomet_qe_score": 0.639804482460022, "metricx_score": 20.23296356201172, "metricx_qe_score": 18.683263778686523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sounds quite okay, right? It violates one principle", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3631225824356079, "xcomet_qe_score": 0.7034687995910645, "metricx_score": 12.701793670654297, "metricx_qe_score": 16.802534103393555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9702941179275513, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 0.4533194899559021, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", but it satisfies another one.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8952804803848267, "xcomet_qe_score": 0.8970226049423218, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that's", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6991824507713318, "xcomet_qe_score": 0.18205960094928741, "metricx_score": 1.7404003143310547, "metricx_qe_score": 1.0664372444152832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "what we did, we extracted various statistics about coordination from the enhanced version of the Pentry Bank and see the paper why we didn't use universal dependencies. And these statistics confirm the observation made many times before that left conjuncts tend to be shorter, so salt and pepper and not pep", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3601891100406647, "xcomet_qe_score": 0.38219505548477173, "metricx_score": 22.387981414794922, "metricx_qe_score": 21.249401092529297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "per and salt measured in syllables. And also the observation that was made in passing that this", "metrics": {"bleu_score": 3.0777759932646394, "chrf_score": 20.281934875656724, "xcomet_score": 0.5411859750747681, "xcomet_qe_score": 0.5724146366119385, "metricx_score": 21.865697860717773, "metricx_qe_score": 22.209196090698242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tendency grows with length difference. So when the difference between the lengths of the two con", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7900182008743286, "xcomet_qe_score": 0.8104976415634155, "metricx_score": 14.77004623413086, "metricx_qe_score": 16.830595016479492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "juncts grows, the shorter conjunct prefers to be the first one stronger, right? So the proportion is bigger of the left short conjunct. But", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3285287916660309, "xcomet_qe_score": 0.384533554315567, "metricx_score": 23.561962127685547, "metricx_qe_score": 21.487791061401367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "what's novel in this paper is that we observed that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14832304418087006, "xcomet_qe_score": 0.15374571084976196, "metricx_score": 19.579797744750977, "metricx_qe_score": 21.231201171875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this tendency only occurs when the governates on the left are absent. So the governor is on the left in this example. I saw Bart and", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.25752341747283936, "xcomet_qe_score": 0.2781330347061157, "metricx_score": 23.120227813720703, "metricx_qe_score": 22.814542770385742, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9702941179275513, "xcomet_qe_score": 0.9868286848068237, "metricx_score": 0.4533194899559021, "metricx_qe_score": 0.47203168272972107, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lisa, so it's the governor, it's on the left. It's absent in the", "metrics": {"bleu_score": 1.0852257312010083, "chrf_score": 6.922026380748342, "xcomet_score": 0.16217122972011566, "xcomet_qe_score": 0.1703464388847351, "metricx_score": 22.88750648498535, "metricx_qe_score": 20.86354637145996, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "second example, Homer came and sneezed. Here we have coordination of two verbs", "metrics": {"bleu_score": 10.316825358380951, "chrf_score": 40.84102867995797, "xcomet_score": 0.8268511295318604, "xcomet_qe_score": 0.9495702981948853, "metricx_score": 7.807217597961426, "metricx_qe_score": 15.167519569396973, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and there's no outside external governor, right? So in such cases", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7191317081451416, "xcomet_qe_score": 0.8683501482009888, "metricx_score": 17.65760040283203, "metricx_qe_score": 14.663328170776367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the left conjunct prefers to be shorter. The more so, the bigger the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5069355368614197, "xcomet_qe_score": 0.6319303512573242, "metricx_score": 22.852235794067383, "metricx_qe_score": 15.185478210449219, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "difference between the two conjuncts. However, when the governance on the right, as here, laughts governs the coordination Telenet, this effect disappears. So we", "metrics": {"bleu_score": 0.0, "chrf_score": 5.725665868607846, "xcomet_score": 0.20642463862895966, "xcomet_qe_score": 0.33365559577941895, "metricx_score": 23.967008590698242, "metricx_qe_score": 22.320703506469727, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "showed that by measuring length in characters, that's the first column in syllables, the middle column and in words, the right column. So I'll concentr", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6288478374481201, "xcomet_qe_score": 0.6789571046829224, "metricx_score": 17.06993293762207, "metricx_qe_score": 14.729144096374512, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ate on the right one. What we", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.19573096930980682, "xcomet_qe_score": 0.14959493279457092, "metricx_score": 18.66119956970215, "metricx_qe_score": 19.376680374145508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "see here is that when the governor is on the left. The tendency for the left conjunct to be shorter grows steadily with the absolute difference in words, and the same is observed when there is no governor as in the coordination of sentences", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5155326128005981, "xcomet_qe_score": 0.7471834421157837, "metricx_score": 19.038230895996094, "metricx_qe_score": 17.060312271118164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", but when the governor is on", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.20781414210796356, "xcomet_qe_score": 0.23984047770500183, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the right, this tendency disappears. And we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6762628555297852, "xcomet_qe_score": 0.7770398855209351, "metricx_score": 12.582131385803223, "metricx_qe_score": 11.790572166442871, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So see the paper for the full agreement and arguments sorry and talk", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7293065190315247, "xcomet_qe_score": 0.7319335341453552, "metricx_score": 11.727640151977539, "metricx_qe_score": 13.289551734924316, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to us about in the poster session. Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.627707839012146, "xcomet_qe_score": 0.7960398197174072, "metricx_score": 15.171098709106445, "metricx_qe_score": 13.041292190551758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9993386268615723, "xcomet_qe_score": 1.0, "metricx_score": 0.3062780499458313, "metricx_qe_score": 0.8795042037963867, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, I'm Xiang Bin, PhD student at the University of", "metrics": {"bleu_score": 0.0, "chrf_score": 6.020722795157013, "xcomet_score": 0.23937061429023743, "xcomet_qe_score": 0.2986587882041931, "metricx_score": 10.251093864440918, "metricx_qe_score": 6.2698588371276855, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Washington. Today I'm presenting our work from pre-training data to language models to downstream tasks, tracking the trails of political biases leading to unfair NLP models", "metrics": {"bleu_score": 0.6307762850619099, "chrf_score": 1.4342333139625139, "xcomet_score": 0.6843324899673462, "xcomet_qe_score": 0.7797157764434814, "metricx_score": 7.008968353271484, "metricx_qe_score": 5.523143768310547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". So language models are trained on large-scale web crawl data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9747623205184937, "xcomet_qe_score": 1.0, "metricx_score": 18.462560653686523, "metricx_qe_score": 22.825698852539062, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Political news media are well covered in their pre-training data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9238544702529907, "xcomet_qe_score": 0.9284713268280029, "metricx_score": 22.82225799560547, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". According to a survey of the C four Corpus, we can see that the New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data. This has created a mixed blessing for language model", "metrics": {"bleu_score": 0.0, "chrf_score": 0.1823486506199854, "xcomet_score": 0.5469492673873901, "xcomet_qe_score": 0.4624055325984955, "metricx_score": 16.16048240661621, "metricx_qe_score": 18.732147216796875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "applications. So, on the one hand, they were able to learn", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17590102553367615, "xcomet_qe_score": 0.20303970575332642, "metricx_score": 20.278461456298828, "metricx_qe_score": 19.444398880004883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "from diverse perspectives, which celebrate democracy and the plurality of ideas. On the other", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7183824777603149, "xcomet_qe_score": 0.8683218955993652, "metricx_score": 23.812654495239258, "metricx_qe_score": 22.334430694580078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "hand, these different political opinions are inherently socially biased and may lead to potential fairness issues in downstream task applications. To this end,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7986642122268677, "xcomet_qe_score": 0.7898037433624268, "metricx_score": 22.240413665771484, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks, specifically by asking the following questions. First, how do we evaluate the political leading of language models and what role does pre-training data might have on such political biases?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8190523386001587, "xcomet_qe_score": 0.8076283931732178, "metricx_score": 25.0, "metricx_qe_score": 22.51779556274414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Secondly, how do language models with different political units actually perform on downstream tasks and whether this could result in fairness issues in NLP applications? Specifically, we", "metrics": {"bleu_score": 0.6590093442496134, "chrf_score": 1.3826721576681602, "xcomet_score": 0.8835228681564331, "xcomet_qe_score": 0.9024516344070435, "metricx_score": 19.139734268188477, "metricx_qe_score": 19.672910690307617, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "first propose to prompt language models with different prompt formats using the political questionnaires, such as the political compass test. This ensures that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5851243734359741, "xcomet_qe_score": 0.6647529602050781, "metricx_score": 17.573163986206055, "metricx_qe_score": 16.213542938232422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we can do automatic evaluation well grounded in political science literature.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.93562912940979, "xcomet_qe_score": 0.9705485105514526, "metricx_score": 14.747082710266113, "metricx_qe_score": 17.88886260986328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Some preliminary results demonstrate that first language models do have varying political meanings. They", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.884035587310791, "xcomet_qe_score": 0.9122337102890015, "metricx_score": 23.686981201171875, "metricx_qe_score": 22.978973388671875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "occupy all four quadrants on the political compass. We can", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7233843207359314, "xcomet_qe_score": 0.7922028303146362, "metricx_score": 19.635080337524414, "metricx_qe_score": 17.013303756713867, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "also see that GPT 4 is the most liberal language model of all, and GPT series are generally more socially liberal than BERT series and its variants. Second", "metrics": {"bleu_score": 0.7530043820009026, "chrf_score": 4.150644837028579, "xcomet_score": 0.7635897397994995, "xcomet_qe_score": 0.7891238927841187, "metricx_score": 17.997568130493164, "metricx_qe_score": 16.13789176940918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ly, we aim to investigate to what extent the political biases of language models are actually picked up from training data. We conducted a", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8407261371612549, "xcomet_qe_score": 0.887479305267334, "metricx_score": 23.493221282958984, "metricx_qe_score": 23.59340476989746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "controlled experiment by further pre-training of language model checkpoints on six different partisan corpora separated into news and social media further divided into their political meanings. By further pre-training language models on such parts in corpora, we", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.49087029695510864, "xcomet_qe_score": 0.5416173934936523, "metricx_score": 15.716086387634277, "metricx_qe_score": 14.51990032196045, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "can see that the ideological coordinates of the language model also shift to For example, for Roberta, further fine-tuned, further trained on the left-leaning Reddit corpus", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18999037146568298, "xcomet_qe_score": 0.16326764225959778, "metricx_score": 20.87795639038086, "metricx_qe_score": 19.723651885986328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we can see a substantial liberal shift in terms of its. in terms of its political biases. We also try to investigate whether language models can pick up the polarization that", "metrics": {"bleu_score": 0.0, "chrf_score": 1.5502792592240233, "xcomet_score": 0.2529757022857666, "xcomet_qe_score": 0.1494172215461731, "metricx_score": 23.813735961914062, "metricx_qe_score": 24.643136978149414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'s prevalent in our modern society. Therefore, we divide pre-training corpora into pre-45th", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22576504945755005, "xcomet_qe_score": 0.23451191186904907, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "President of the United States and after 45th President of the United States, we separately pre-train language models into two different", "metrics": {"bleu_score": 0.0, "chrf_score": 0.998782524515781, "xcomet_score": 0.4649631679058075, "xcomet_qe_score": 0.5124492645263672, "metricx_score": 19.35995101928711, "metricx_qe_score": 17.460859298706055, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "temporal corpora. We can see that language models generally had a political inclination that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17113950848579407, "xcomet_qe_score": 0.2952460050582886, "metricx_score": 21.003280639648438, "metricx_qe_score": 23.032878875732422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is further away from the center after twenty seventeen. This indicates that language models can", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5044958591461182, "xcomet_qe_score": 0.6575645208358765, "metricx_score": 19.5510311126709, "metricx_qe_score": 18.841026306152344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "also take up the polarization in our society. So, last but not", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2882290482521057, "xcomet_qe_score": 0.7166330218315125, "metricx_score": 23.101913452148438, "metricx_qe_score": 21.076072692871094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "least, we evaluate language models with different political meanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications. So we see", "metrics": {"bleu_score": 0.44560708539794075, "chrf_score": 1.0784448612782773, "xcomet_score": 0.6989456415176392, "xcomet_qe_score": 0.7581685781478882, "metricx_score": 21.80724334716797, "metricx_qe_score": 20.80276870727539, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that if we investigate the performance per category, that is to say if we separate the performance into. Different demographics or political meaning of news media, we can see a pattern that, for example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6318159699440002, "xcomet_qe_score": 0.7510104179382324, "metricx_score": 17.573881149291992, "metricx_qe_score": 17.4805908203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", for hate speech detection, left-line language models are better. at detecting hate speech targeting socially minority groups. However, our works at detecting hate speech targeting more powerful groups in our society. And vice versa, right-wing", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5051950216293335, "xcomet_qe_score": 0.6264477968215942, "metricx_score": 17.928672790527344, "metricx_qe_score": 19.22657585144043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language models are better at detecting hate speech targeting white and man, however, worse at detecting hate speech targeting black LGBTQ plus and other minority communities. Similar trends also occur for fake news", "metrics": {"bleu_score": 0.5721708235071008, "chrf_score": 2.9746386286082243, "xcomet_score": 0.5176156163215637, "xcomet_qe_score": 0.8182521462440491, "metricx_score": 18.93923568725586, "metricx_qe_score": 22.11775016784668, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "detection, where we see that left-line language models are better at detecting misinformation from their opposite political line and vice versa. This will further show many qualitative examples to see that language", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.33369818329811096, "xcomet_qe_score": 0.5372254848480225, "metricx_score": 15.40881633758545, "metricx_qe_score": 17.725894927978516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "models with different political meanings. They do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7175352573394775, "xcomet_qe_score": 0.8903869390487671, "metricx_score": 15.293583869934082, "metricx_qe_score": 12.46017074584961, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that. This suggests that there is a fairness issue that is very pressing regarding the political biases of language models. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5982412099838257, "xcomet_qe_score": 0.6810333728790283, "metricx_score": 18.47886085510254, "metricx_qe_score": 17.382875442504883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", if a right-line language model were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform. This would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control. This has sound", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7406454682350159, "xcomet_qe_score": 0.7641041278839111, "metricx_score": 16.722797393798828, "metricx_qe_score": 15.714046478271484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ed the alarm for us to recognize and tackle the fairness issues resulting from language model political neigh", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5527760982513428, "xcomet_qe_score": 0.5254130363464355, "metricx_score": 22.45170783996582, "metricx_qe_score": 24.337732315063477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ing. So a little bit of discussion", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7605173587799072, "xcomet_qe_score": 0.8294469118118286, "metricx_score": 9.064146995544434, "metricx_qe_score": 20.23609161376953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We would also like to highlight that we expose the unique dilemma regarding language model political", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6953285932540894, "xcomet_qe_score": 0.7193472385406494, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "biases. It's like between Sila and Kryptidis. So if we do not saniti", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2497018575668335, "xcomet_qe_score": 0.24049073457717896, "metricx_score": 21.130556106567383, "metricx_qe_score": 23.464130401611328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ze political opinions in language model training data, the bias will propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.812419056892395, "xcomet_qe_score": 0.811220109462738, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". If we try to sanitize in some way, we would also risk censorship or exclusion, and it", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7622280120849609, "xcomet_qe_score": 0.8533532023429871, "metricx_score": 13.329048156738281, "metricx_qe_score": 9.588924407958984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is incredibly difficult to determine what is actually neutral and should retain language model training data. So it's kind of like the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.41607943177223206, "xcomet_qe_score": 0.644303560256958, "metricx_score": 18.030662536621094, "metricx_qe_score": 15.959639549255371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "electric Charlie problem. OK, great. I think", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17688225209712982, "xcomet_qe_score": 0.15661971271038055, "metricx_score": 13.087320327758789, "metricx_qe_score": 12.310946464538574, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that's", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5062823295593262, "xcomet_qe_score": 0.17350593209266663, "metricx_score": 2.0731263160705566, "metricx_qe_score": 1.6942663192749023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pretty much all I have for today. Thank you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7545379996299744, "xcomet_qe_score": 0.8266277313232422, "metricx_score": 3.9607596397399902, "metricx_qe_score": 5.559751033782959, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "much for your attention. See you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.46598803997039795, "xcomet_qe_score": 0.7199680805206299, "metricx_score": 6.88412618637085, "metricx_qe_score": 7.594667434692383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hi everyone, I", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.38067352771759033, "xcomet_qe_score": 0.4521919786930084, "metricx_score": 4.181337833404541, "metricx_qe_score": 0.12854959070682526, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'m Jenny, a first year PhD student at Carnegie Mellon University, and today I'll be presenting your work, Enol Positionale, Characterizing Design Biases in Beta Sets of Mod", "metrics": {"bleu_score": 0.6609317714959291, "chrf_score": 13.486583036152275, "xcomet_score": 0.1745605170726776, "xcomet_qe_score": 0.2911137342453003, "metricx_score": 22.538551330566406, "metricx_qe_score": 17.777109146118164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "els. This work was done in collaboration with some people at the University of Washington and the Allen Institute for AI, namely Sebastian Santi, Ronin Lebras, Katarina Reinicke and Martin Sapp.", "metrics": {"bleu_score": 0.7346818800702734, "chrf_score": 26.61424555508137, "xcomet_score": 0.6828212141990662, "xcomet_qe_score": 0.628809928894043, "metricx_score": 19.14584732055664, "metricx_qe_score": 20.08191680908203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So let's start by imagining that you are working for a newspaper and you are sifting through comments under your news article trying to remove toxic content", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9868700504302979, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". You may turn towards a popular API, such as Perspective API for Toxicity Detection. And this works really well if you are Carl Jones", "metrics": {"bleu_score": 1.8249909597216782, "chrf_score": 23.174524852338045, "xcomet_score": 0.9073581695556641, "xcomet_qe_score": 0.9360370635986328, "metricx_score": 10.63467788696289, "metricx_qe_score": 8.689855575561523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", where Perspective API is able to correctly detect toxic instances.", "metrics": {"bleu_score": 2.8666091494718775, "chrf_score": 29.748660291536478, "xcomet_score": 0.9067326784133911, "xcomet_qe_score": 0.854153037071228, "metricx_score": 12.616056442260742, "metricx_qe_score": 16.153385162353516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But that's not really the case for Dithyasharma", "metrics": {"bleu_score": 0.0, "chrf_score": 13.012791395811139, "xcomet_score": 0.8525581359863281, "xcomet_qe_score": 0.8437973260879517, "metricx_score": 19.7308406829834, "metricx_qe_score": 23.879037857055664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", where perspective API is really not as sensitive to offensive terms that are more common in Indian contex", "metrics": {"bleu_score": 1.7287549675176845, "chrf_score": 20.617024098384437, "xcomet_score": 0.8541989326477051, "xcomet_qe_score": 0.8264257907867432, "metricx_score": 22.7418212890625, "metricx_qe_score": 23.96307373046875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ts. This is an example of a design bias where we see systematic performance differences of technology between populations. Design bias", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5952550172805786, "xcomet_qe_score": 0.5659998655319214, "metricx_score": 24.022172927856445, "metricx_qe_score": 24.808181762695312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "es, such as the one we just saw before, may occur due to the positionality of NLP researchers and model developers. Pos", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.47754159569740295, "xcomet_qe_score": 0.49201303720474243, "metricx_score": 22.477130889892578, "metricx_qe_score": 21.74835777282715, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "itionality is simply the perspectives that people hold as a result of their demographics, identity and", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7171809673309326, "xcomet_qe_score": 0.564498782157898, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7265584468841553, "xcomet_qe_score": 0.878545343875885, "metricx_score": 23.050888061523438, "metricx_qe_score": 21.63852310180664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8705877065658569, "xcomet_qe_score": 0.8748378157615662, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that researchers make. And so one question that people might ask is, do data sets and models have positionality?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7942306995391846, "xcomet_qe_score": 0.8871865272521973, "metricx_score": 19.3072509765625, "metricx_qe_score": 21.319805145263672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And we are not trying to say that models themselves and datasets themselves have demographic identities and life experiences, but they aggregate judgments and opinions of real people and can thus represent certain positionalities", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.962806224822998, "xcomet_qe_score": 0.9759119749069214, "metricx_score": 13.296013832092285, "metricx_qe_score": 19.853229522705078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "over others. Previous work suggests some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.74518221616745, "xcomet_qe_score": 0.8357627987861633, "metricx_score": 13.531027793884277, "metricx_qe_score": 11.694612503051758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". However, these works do not really look at comparing end users with the datasets and models themselves. And the study of model and data set positionality is increasingly important as NLP tasks become more subjective and socially oriented. And it is challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs", "metrics": {"bleu_score": 0.0, "chrf_score": 0.6268446368837941, "xcomet_score": 0.8788601160049438, "xcomet_qe_score": 0.8833600282669067, "metricx_score": 7.343725204467773, "metricx_qe_score": 4.949800491333008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". To study data set and model positionality, we actually compare the annotations with real users with existing data sets and models. We do this", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7232751846313477, "xcomet_qe_score": 0.7394323348999023, "metricx_score": 17.246089935302734, "metricx_qe_score": 19.617809295654297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "through our framework NL Positionality. Our Framework works in two main", "metrics": {"bleu_score": 0.0, "chrf_score": 38.779858665469064, "xcomet_score": 0.7344151735305786, "xcomet_qe_score": 0.6778301000595093, "metricx_score": 14.155464172363281, "metricx_qe_score": 14.241987228393555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "steps. The first step is to re-", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3302578330039978, "xcomet_qe_score": 0.2203417718410492, "metricx_score": 20.091081619262695, "metricx_qe_score": 21.969894409179688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "annote data sets with diverse annotators. We should do this", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7193264961242676, "xcomet_qe_score": 0.756295919418335, "metricx_score": 15.290067672729492, "metricx_qe_score": 16.815752029418945, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "by looking at the demographics of original datasets, um, annotators, because usually only a few annotators annotate each instance and because demographics are rarely collected and shared. Therefore,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5910250544548035, "xcomet_qe_score": 0.8287734985351562, "metricx_score": 18.81764030456543, "metricx_qe_score": 16.575496673583984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we opt for re-annotiating data to get many annotations per instance and to get a rich set of demographic data. We then", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8327960968017578, "xcomet_qe_score": 0.863580048084259, "metricx_score": 12.586977005004883, "metricx_qe_score": 10.014962196350098, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "take the annotations by demographic data and compare them to the models and datasets using a Parsons R correlation score. And so our framework differs from the annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modeling annotator distributions. Our Framework is larg", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5878351330757141, "xcomet_qe_score": 0.7643992304801941, "metricx_score": 16.490787506103516, "metricx_qe_score": 13.503395080566406, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ely enabled through Lab in the Wild, an online crowdsourcing platform for our HCI collaborators.", "metrics": {"bleu_score": 7.926275939399007, "chrf_score": 23.499556729603853, "xcomet_score": 0.7810646891593933, "xcomet_qe_score": 0.7607178688049316, "metricx_score": 16.311914443969727, "metricx_qe_score": 14.575859069824219, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers, compared to", "metrics": {"bleu_score": 7.297388366575838, "chrf_score": 20.955181261192696, "xcomet_score": 0.7205831408500671, "xcomet_qe_score": 0.6492730379104614, "metricx_score": 10.75871467590332, "metricx_qe_score": 17.807588577270508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "platforms like MTurk, which have participants from the USA or India. Further Lab in the Wild is still able to obtain high-quality data.", "metrics": {"bleu_score": 4.703031907490957, "chrf_score": 18.82662933554374, "xcomet_score": 0.8110122084617615, "xcomet_qe_score": 0.8738625645637512, "metricx_score": 7.801886081695557, "metricx_qe_score": 7.597853183746338, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We host two tasks on Lab in the Wild, one of them being social acceptability. And the way this works is that participants will read a situation from the social chemistry data set and then they will write how socially acceptable a situation is. Afterwards, to stay engaged", "metrics": {"bleu_score": 2.763276237505391, "chrf_score": 8.65392882944806, "xcomet_score": 0.7198781967163086, "xcomet_qe_score": 0.7851311564445496, "metricx_score": 12.53689193725586, "metricx_qe_score": 11.953722953796387, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the study, they can compare their responses to an AI and others. We then compared these annotations with", "metrics": {"bleu_score": 0.8477057393944755, "chrf_score": 1.0446831540758412, "xcomet_score": 0.30073103308677673, "xcomet_qe_score": 0.7025951147079468, "metricx_score": 18.710308074951172, "metricx_qe_score": 20.242006301879883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Social Chemistry Delphi in GPT 4. We then replicated a very similar setup for the toxicity", "metrics": {"bleu_score": 2.9811934555294024, "chrf_score": 12.951484263977637, "xcomet_score": 0.2116011530160904, "xcomet_qe_score": 0.4604834020137787, "metricx_score": 20.28976821899414, "metricx_qe_score": 20.872852325439453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and hate speech detection task, where they will read an instance from Dana Hate and write whether they think it's an instance of hate speech. We then compared these annotations with DynaHate, Perspective API, Rewire", "metrics": {"bleu_score": 0.0, "chrf_score": 3.6878454148630313, "xcomet_score": 0.1438356339931488, "xcomet_qe_score": 0.32527831196784973, "metricx_score": 20.958911895751953, "metricx_qe_score": 18.54851531982422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "API, HateRoberta, and GPT four. Our study in the end amassed over sixteen thousand annotations from over a thousand annotators from eighty", "metrics": {"bleu_score": 1.4664868304886143, "chrf_score": 19.63418612202455, "xcomet_score": 0.24361205101013184, "xcomet_qe_score": 0.2327854484319687, "metricx_score": 18.37940216064453, "metricx_qe_score": 16.612031936645508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "seven countries. So now we are better equipped to answer who do NLP data sets and models align with the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.21696297824382782, "xcomet_qe_score": 0.20691324770450592, "metricx_score": 23.30933952331543, "metricx_qe_score": 21.18342399597168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "most? We find that there is positionality in NLP. For example, we find that datasets", "metrics": {"bleu_score": 1.232245177803213, "chrf_score": 2.6797928883419826, "xcomet_score": 0.3151547312736511, "xcomet_qe_score": 0.3050472140312195, "metricx_score": 20.36818504333496, "metricx_qe_score": 21.53074836730957, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and models are most aligned to English-speaking countries", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14299005270004272, "xcomet_qe_score": 0.1388024240732193, "metricx_score": 22.929079055786133, "metricx_qe_score": 16.686073303222656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". So for the GPD 4, social acceptability analysis, we find that it is most", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22568118572235107, "xcomet_qe_score": 0.23555149137973785, "metricx_score": 24.80388641357422, "metricx_qe_score": 23.392711639404297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "aligned to Confucian and English-speaking countries. We find that Dynamite Hate is also most aligned", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4129220247268677, "xcomet_qe_score": 0.6116142272949219, "metricx_score": 19.544654846191406, "metricx_qe_score": 19.405746459960938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to English-speaking countries. We also find that most additional", "metrics": {"bleu_score": 0.0, "chrf_score": 5.57738521713615, "xcomet_score": 0.47071129083633423, "xcomet_qe_score": 0.6108676195144653, "metricx_score": 21.400726318359375, "metricx_qe_score": 20.152320861816406, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "alignment with people who have a college education. So for GPT four in the social accep", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18488185107707977, "xcomet_qe_score": 0.39063963294029236, "metricx_score": 19.684995651245117, "metricx_qe_score": 22.284740447998047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tability task, we find that it is most aligned with people with a college education or graduate school education. And we find the same for Dani Hate, where it is most aligned to people with a college education. However", "metrics": {"bleu_score": 0.0, "chrf_score": 2.308532639027365, "xcomet_score": 0.4762854278087616, "xcomet_qe_score": 0.41695529222488403, "metricx_score": 17.800565719604492, "metricx_qe_score": 13.744451522827148, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", when models and datasets are aligned to specific populations, some are inevitably left behind. An example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8324031233787537, "xcomet_qe_score": 0.8724728226661682, "metricx_score": 25.0, "metricx_qe_score": 24.872737884521484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of this is that datasets and models are less aligned to non-binary people compared to their male and female", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6189405918121338, "xcomet_qe_score": 0.624161958694458, "metricx_score": 23.778705596923828, "metricx_qe_score": 22.840578079223633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "counterparts. We find this in the GPT 4 Social Acceptability Task as well as in the Dynahate Task Analysis. So", "metrics": {"bleu_score": 2.7811013508058227, "chrf_score": 15.314893338375658, "xcomet_score": 0.7249383926391602, "xcomet_qe_score": 0.7434914112091064, "metricx_score": 14.539098739624023, "metricx_qe_score": 11.246692657470703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", given that there is position in allied in LP, what can we do about it?", "metrics": {"bleu_score": 0.0, "chrf_score": 1.6973365472250206, "xcomet_score": 0.7989779710769653, "xcomet_qe_score": 0.8209471702575684, "metricx_score": 21.31224822998047, "metricx_qe_score": 19.716758728027344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So we have a few recommendations for", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6891065835952759, "xcomet_qe_score": 0.7900015711784363, "metricx_score": 21.337440490722656, "metricx_qe_score": 23.114343643188477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this. The first one is to keep a record of all relevant design cho", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6976056098937988, "xcomet_qe_score": 0.7079210877418518, "metricx_score": 18.992252349853516, "metricx_qe_score": 16.687015533447266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ices throughout the research process. And the other is to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18432143330574036, "xcomet_qe_score": 0.5169277191162109, "metricx_score": 23.50538444519043, "metricx_qe_score": 23.36555290222168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "do NLP research through the lens of perspectivism. Our third recommendation is to build specialized", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2939775288105011, "xcomet_qe_score": 0.29844462871551514, "metricx_score": 24.0183048248291, "metricx_qe_score": 23.729156494140625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "datasets and models within four specific communities. A good", "metrics": {"bleu_score": 0.0, "chrf_score": 6.834071171105789, "xcomet_score": 0.13982897996902466, "xcomet_qe_score": 0.14759159088134766, "metricx_score": 24.02792739868164, "metricx_qe_score": 23.241127014160156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "example of this is the Masakane initiative. We want to emphasize that inclusive", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18747836351394653, "xcomet_qe_score": 0.18680962920188904, "metricx_score": 20.710975646972656, "metricx_qe_score": 20.48457145690918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "NLP does not just make all technologies work", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16115343570709229, "xcomet_qe_score": 0.15343251824378967, "metricx_score": 12.911422729492188, "metricx_qe_score": 16.785154342651367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for everyone. And with this concludes our present", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6304841041564941, "xcomet_qe_score": 0.7691457271575928, "metricx_score": 5.0012078285217285, "metricx_qe_score": 5.737924098968506, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ation, but if you would like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8529493808746338, "xcomet_qe_score": 0.8450505137443542, "metricx_score": 15.993551254272461, "metricx_qe_score": 20.748531341552734, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9993386268615723, "xcomet_qe_score": 1.0, "metricx_score": 0.3062780499458313, "metricx_qe_score": 0.8795042037963867, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hi, I'm Xi Yuan from", "metrics": {"bleu_score": 1.5330462064343475, "chrf_score": 9.241524681953553, "xcomet_score": 0.3894331753253937, "xcomet_qe_score": 0.44004860520362854, "metricx_score": 16.029144287109375, "metricx_qe_score": 9.201922416687012, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Fenai University. I'm here to introduce our work distinct script knowledge from line language models for constrained language planning.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5186253190040588, "xcomet_qe_score": 0.5092505216598511, "metricx_score": 15.731889724731445, "metricx_qe_score": 14.283835411071777, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In everyday life humans often plan their actions by following step by step instructions in the form of guaranteed scripts. Previous work has expl", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6242534518241882, "xcomet_qe_score": 0.583179235458374, "metricx_score": 15.889883995056152, "metricx_qe_score": 18.758312225341797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ored language models to plan for abstract goals of stereotypical activities such as make a cake and show that large language", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.46691271662712097, "xcomet_qe_score": 0.45556190609931946, "metricx_score": 20.433788299560547, "metricx_qe_score": 20.285154342651367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "models can effectively decompose goals into steps. However, previous work mainly focuses on planning", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2678537666797638, "xcomet_qe_score": 0.3619622588157654, "metricx_score": 14.248525619506836, "metricx_qe_score": 17.595108032226562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for the abstract goals of stereotypical activities. Planning for the goals with specific goals", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1582133173942566, "xcomet_qe_score": 0.1932862401008606, "metricx_score": 20.50652313232422, "metricx_qe_score": 20.004779815673828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", specific constraints, such as make a chocolate cake still remains understudied. In this paper we define the problem", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7198322415351868, "xcomet_qe_score": 0.7078413963317871, "metricx_score": 19.303773880004883, "metricx_qe_score": 18.906930923461914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of constrained language planning. which impose different constraints on the goals of planning. An abstract goal can be inherited", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2624722719192505, "xcomet_qe_score": 0.5986096858978271, "metricx_score": 21.397140502929688, "metricx_qe_score": 20.14544105529785, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "by different real life specific goals with multifaced constraints. A good planner should write sc", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2158682644367218, "xcomet_qe_score": 0.3672666549682617, "metricx_score": 20.844619750976562, "metricx_qe_score": 21.541980743408203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the const", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.275856614112854, "xcomet_qe_score": 0.31306537985801697, "metricx_score": 18.21891975402832, "metricx_qe_score": 20.467029571533203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "rained language planning ability of large language models. Since no data set of specific goals exists to spot our star", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14322717487812042, "xcomet_qe_score": 0.14771738648414612, "metricx_score": 19.90701675415039, "metricx_qe_score": 21.41522216796875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "day. We have to acquire these goals first. As shown in the table, we extend the abstra", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.28696656227111816, "xcomet_qe_score": 0.3927809000015259, "metricx_score": 22.092206954956055, "metricx_qe_score": 22.874778747558594, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ct goals with modified constraints for human loop data acquisition using structural TPT. We sample 100 specific goals and evaluate the scripts generated", "metrics": {"bleu_score": 0.0, "chrf_score": 7.4237896674735, "xcomet_score": 0.2230830192565918, "xcomet_qe_score": 0.2654920220375061, "metricx_score": 19.997074127197266, "metricx_qe_score": 19.29024887084961, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "from large-numbered models. This table reports the overall accuracy of the results. We find that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2239096313714981, "xcomet_qe_score": 0.23468196392059326, "metricx_score": 20.87261390686035, "metricx_qe_score": 20.43027687072754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "all linear models achieve unsatisfactory results on planning for", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1567346602678299, "xcomet_qe_score": 0.14555956423282623, "metricx_score": 10.951062202453613, "metricx_qe_score": 9.755435943603516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "specific goals. Then we conduct detailed analysis to investigate why learning modules for. Results in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17786899209022522, "xcomet_qe_score": 0.21234440803527832, "metricx_score": 19.445693969726562, "metricx_qe_score": 21.323593139648438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the figure show that the semantic completeness in generated scripts is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15134157240390778, "xcomet_qe_score": 0.14691026508808136, "metricx_score": 20.106201171875, "metricx_qe_score": 18.788331985473633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "acceptable, but the faithfulness to the constraints cannot be guaranteed. We's are more frank graded topic categories of constra", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.25773417949676514, "xcomet_qe_score": 0.2808007597923279, "metricx_score": 22.69829750061035, "metricx_qe_score": 23.424428939819336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ints depending on waking home. The head map in the figure shows that the planning performance of instructive", "metrics": {"bleu_score": 0.0, "chrf_score": 3.119300515729684, "xcomet_score": 0.19331379234790802, "xcomet_qe_score": 0.17517399787902832, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DPDs varies considerably for girls of different categories. Previous studies have shown that the output quality of lightly models falls in high", "metrics": {"bleu_score": 0.0, "chrf_score": 3.008619896411522, "xcomet_score": 0.21414481103420258, "xcomet_qe_score": 0.2146427482366562, "metricx_score": 25.0, "metricx_qe_score": 24.944364547729492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "variance, leading to bad performance. Thus, we adopt the idea of over", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.25285229086875916, "xcomet_qe_score": 0.24672099947929382, "metricx_score": 20.515222549438477, "metricx_qe_score": 20.720294952392578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "generated zen filter to improve generation quality. We first show constraint types with examples for", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2603951394557953, "xcomet_qe_score": 0.3714384138584137, "metricx_score": 20.780141830444336, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "instruct CPT and obtain specific goals based on the said abstract goals. Then instruct the G", "metrics": {"bleu_score": 0.0, "chrf_score": 10.565790247667948, "xcomet_score": 0.33235597610473633, "xcomet_qe_score": 0.445174902677536, "metricx_score": 18.68057632446289, "metricx_qe_score": 14.853858947753906, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PT over generate case scripts for specific goals. Next", "metrics": {"bleu_score": 0.0, "chrf_score": 4.764092217867097, "xcomet_score": 0.46943309903144836, "xcomet_qe_score": 0.6995042562484741, "metricx_score": 17.57482147216797, "metricx_qe_score": 17.07088279724121, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", a filter model is developed to select the fitful scripts. We", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7765833139419556, "xcomet_qe_score": 0.8217470645904541, "metricx_score": 23.35594367980957, "metricx_qe_score": 22.831270217895508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "convert scripts and goals into instruct GPT in bitings and calculate the cosine similarity and similarity scores to measure semantic similarity. In addition", "metrics": {"bleu_score": 0.0, "chrf_score": 11.957920755546116, "xcomet_score": 0.3714752197265625, "xcomet_qe_score": 0.5600577592849731, "metricx_score": 17.3443660736084, "metricx_qe_score": 13.853137969970703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we will write the script that contains the keywords of the target constraint", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8269562721252441, "xcomet_qe_score": 0.8731597661972046, "metricx_score": 20.513090133666992, "metricx_qe_score": 19.837772369384766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We only keep the script if the target goal scores the highest against the goal site.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8437994718551636, "xcomet_qe_score": 0.9424521923065186, "metricx_score": 13.581814765930176, "metricx_qe_score": 18.186874389648438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "With our method, intuitivity can generate screws of hair quality. Our", "metrics": {"bleu_score": 0.0, "chrf_score": 3.187613843351548, "xcomet_score": 0.2228824496269226, "xcomet_qe_score": 0.3344951272010803, "metricx_score": 20.35577392578125, "metricx_qe_score": 22.90018653869629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "method greatly improves the planability both in semantic completeness and faithfulness to the constraint. Since large language", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7607638835906982, "xcomet_qe_score": 0.8229621648788452, "metricx_score": 18.180980682373047, "metricx_qe_score": 15.459249496459961, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "models are costly to deploy, it is essential to enable language planning ability of smaller and specialized models. Creating data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5483126044273376, "xcomet_qe_score": 0.604644775390625, "metricx_score": 24.60031509399414, "metricx_qe_score": 24.825244903564453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "set is an essential step to its end. However", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.25779464840888977, "xcomet_qe_score": 0.5161336660385132, "metricx_score": 20.913663864135742, "metricx_qe_score": 20.873973846435547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", previous studies do not enable planning for specific goals, and manual data set annotation is expensive. Thus", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9382688403129578, "xcomet_qe_score": 0.8952500820159912, "metricx_score": 23.33576774597168, "metricx_qe_score": 20.047420501708984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we follow the idea of symbolic knowledge distillation to distill constrained language planning data sites from large language models. We will", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7058035135269165, "xcomet_qe_score": 0.7930447459220886, "metricx_score": 18.762649536132812, "metricx_qe_score": 16.61528778076172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "apply our method for building a dataset of conjugated language planning named as code script. In total", "metrics": {"bleu_score": 0.0, "chrf_score": 5.96444726980981, "xcomet_score": 0.42400631308555603, "xcomet_qe_score": 0.5922780632972717, "metricx_score": 16.403789520263672, "metricx_qe_score": 13.376910209655762, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we generate fifty five thousand specific goals with scripts", "metrics": {"bleu_score": 0.0, "chrf_score": 0.5482456140350879, "xcomet_score": 0.9500306844711304, "xcomet_qe_score": 0.9889417886734009, "metricx_score": 16.400346755981445, "metricx_qe_score": 16.974477767944336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to ensure the quality of validation and test sites. We ask cloud source workers to find and revise the incorrect samples. This figure shows the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4332279860973358, "xcomet_qe_score": 0.5978394746780396, "metricx_score": 13.802571296691895, "metricx_qe_score": 11.325303077697754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "constraint distribution of code script. We find code script", "metrics": {"bleu_score": 0.0, "chrf_score": 10.72091203127037, "xcomet_score": 0.37395796179771423, "xcomet_qe_score": 0.42004892230033875, "metricx_score": 20.61170768737793, "metricx_qe_score": 18.8402156829834, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "shows the hyperplodism in the generated specific goals. With code script", "metrics": {"bleu_score": 0.0, "chrf_score": 7.265542803849077, "xcomet_score": 0.5616415739059448, "xcomet_qe_score": 0.7595458626747131, "metricx_score": 20.40152931213379, "metricx_qe_score": 18.348346710205078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we can trace smaller but specialized models for constraint language planning. With", "metrics": {"bleu_score": 0.0, "chrf_score": 2.314814814814815, "xcomet_score": 0.3458274304866791, "xcomet_qe_score": 0.3297193646430969, "metricx_score": 17.023759841918945, "metricx_qe_score": 16.41727638244629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "analyzes, TFILF and Tune on Coswit can generate scripts of higher quality than most large language models, indicating that smaller models can support larger models when properly trained on suitable data sites. In summary", "metrics": {"bleu_score": 0.0, "chrf_score": 3.2090998556400763, "xcomet_score": 0.33611324429512024, "xcomet_qe_score": 0.41043999791145325, "metricx_score": 21.732194900512695, "metricx_qe_score": 18.841825485229492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we established the constraint language planning problem.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9276132583618164, "xcomet_qe_score": 0.9411296844482422, "metricx_score": 13.709151268005371, "metricx_qe_score": 14.120392799377441, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We evaluated the constraint language planning ability of large language models and developed over generated filter method for large language models. We use large language models to generate", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6343068480491638, "xcomet_qe_score": 0.7151061296463013, "metricx_score": 14.505218505859375, "metricx_qe_score": 13.729324340820312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a high quality script data set for constraint language planning. We hope code data set can be a valuable", "metrics": {"bleu_score": 0.0, "chrf_score": 5.859463016220652, "xcomet_score": 0.6441012620925903, "xcomet_qe_score": 0.7707611322402954, "metricx_score": 12.105663299560547, "metricx_qe_score": 10.76129150390625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "resource to advance research on language planning. Thank you for your time. Please find more details of", "metrics": {"bleu_score": 0.0, "chrf_score": 2.3255813953488373, "xcomet_score": 0.17651966214179993, "xcomet_qe_score": 0.19597628712654114, "metricx_score": 21.10858726501465, "metricx_qe_score": 18.69132423400879, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "code script in our paper.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15150848031044006, "xcomet_qe_score": 0.16619911789894104, "metricx_score": 18.756168365478516, "metricx_qe_score": 20.842496871948242, "linguapy_score": [1, "FINNISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5508139133453369, "xcomet_qe_score": 0.14792659878730774, "metricx_score": 3.2627639770507812, "metricx_qe_score": 10.83942699432373, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone, my name is Shu Heng.", "metrics": {"bleu_score": 0.0, "chrf_score": 12.124985882698901, "xcomet_score": 0.835317850112915, "xcomet_qe_score": 0.8710336685180664, "metricx_score": 1.9977140426635742, "metricx_qe_score": 3.0794193744659424, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Today I'm going to present our paper Do Kernel two thousand three named entity tags still work well in twenty twenty three? Let", "metrics": {"bleu_score": 0.0, "chrf_score": 0.601684717208183, "xcomet_score": 0.6387535333633423, "xcomet_qe_score": 0.766420841217041, "metricx_score": 18.547277450561523, "metricx_qe_score": 15.786174774169922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'s get started. Our", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4480900466442108, "xcomet_qe_score": 0.4982275068759918, "metricx_score": 8.403955459594727, "metricx_qe_score": 13.438142776489258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "paper investigated the problem of generalization using the named entity recognition task or the NER task. We observe that models have", "metrics": {"bleu_score": 1.1473150461164188, "chrf_score": 2.0696054634750425, "xcomet_score": 0.6418770551681519, "xcomet_qe_score": 0.6937179565429688, "metricx_score": 16.782400131225586, "metricx_qe_score": 15.797335624694824, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "been using Kono two thousand three to develop NER for almost twenty years. And this naturally raises several problems. Firstly", "metrics": {"bleu_score": 0.8734513882715711, "chrf_score": 1.94383903733732, "xcomet_score": 0.5056130886077881, "xcomet_qe_score": 0.6324964761734009, "metricx_score": 20.14051055908203, "metricx_qe_score": 16.48617935180664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", can these models generalize to modern data?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9872095584869385, "xcomet_qe_score": 0.9865752458572388, "metricx_score": 20.32095718383789, "metricx_qe_score": 22.572559356689453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And when we develop new taggers, what is needed for good generalization?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9840165376663208, "xcomet_qe_score": 0.9968181848526001, "metricx_score": 19.894073486328125, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "At the same time, if we do observe poor generalization, what causes the performance drop of these models?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9960763454437256, "xcomet_qe_score": 1.0, "metricx_score": 24.831989288330078, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To investigate these problems, we developed the Kono plus plus data set. This is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.499001996007984, "xcomet_score": 0.6431469917297363, "xcomet_qe_score": 0.6821861267089844, "metricx_score": 19.359539031982422, "metricx_qe_score": 16.801923751831055, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a data set that we collected from Reuters News from 2020 and then annotated them with the same Kono 2003 annotation guidelines. We then fine tuned over tw", "metrics": {"bleu_score": 0.850694568002925, "chrf_score": 5.482958343342741, "xcomet_score": 0.40727877616882324, "xcomet_qe_score": 0.5235923528671265, "metricx_score": 21.402997970581055, "metricx_qe_score": 18.638540267944336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "enty models on Kono two thousand three. We evaluated them on both the Kono three test set and", "metrics": {"bleu_score": 0.0, "chrf_score": 0.42517006802721086, "xcomet_score": 0.23856274783611298, "xcomet_qe_score": 0.3848874270915985, "metricx_score": 23.98058319091797, "metricx_qe_score": 23.424962997436523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the Kono plus test set. And last but not least, we calculated the percentage change in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.8375209380234505, "xcomet_score": 0.18969973921775818, "xcomet_qe_score": 0.20324113965034485, "metricx_score": 22.077043533325195, "metricx_qe_score": 15.343758583068848, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "F one to assess the generalization of each model. So what is needed for good genera", "metrics": {"bleu_score": 0.0, "chrf_score": 0.4528985507246377, "xcomet_score": 0.2584514915943146, "xcomet_qe_score": 0.2900954782962799, "metricx_score": 22.704198837280273, "metricx_qe_score": 20.94279670715332, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "lization? Through our experiments we found that there are three main", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.33573728799819946, "xcomet_qe_score": 0.30459555983543396, "metricx_score": 19.108537673950195, "metricx_qe_score": 20.505046844482422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ingredients that are needed. The first one is the model architecture", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2379923015832901, "xcomet_qe_score": 0.30078327655792236, "metricx_score": 18.69623565673828, "metricx_qe_score": 19.49871826171875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Through our experiments we found", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1756928265094757, "xcomet_qe_score": 0.15412314236164093, "metricx_score": 20.262340545654297, "metricx_qe_score": 22.731067657470703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that the transformer models normally generalize better to new data. The second ingredi", "metrics": {"bleu_score": 0.0, "chrf_score": 16.861479193730716, "xcomet_score": 0.4495457112789154, "xcomet_qe_score": 0.27247101068496704, "metricx_score": 25.0, "metricx_qe_score": 24.98931884765625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ent is the model size. We found", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.33844897150993347, "xcomet_qe_score": 0.45618578791618347, "metricx_score": 20.515552520751953, "metricx_qe_score": 20.286693572998047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that usually larger models lead to better generalization. And last but not", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7162290811538696, "xcomet_qe_score": 0.8057016134262085, "metricx_score": 24.079843521118164, "metricx_qe_score": 24.44936180114746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "least, we all know that the number of fine tuning examples directly affects the performance of a down", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5884209871292114, "xcomet_qe_score": 0.5450550317764282, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "stream task. Here we also found that more fine tuning examples actually also leads to better genera", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6407757997512817, "xcomet_qe_score": 0.6023375391960144, "metricx_score": 20.692941665649414, "metricx_qe_score": 21.88489532470703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "lization. To our next question, what causes the performance drop of some models? We had two hypotheses. The first one", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7792389988899231, "xcomet_qe_score": 0.8323957920074463, "metricx_score": 18.49626350402832, "metricx_qe_score": 16.322114944458008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again, and this is usually manifested as the diminish return on a new test set. The", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6241015791893005, "xcomet_qe_score": 0.680626630783081, "metricx_score": 14.001577377319336, "metricx_qe_score": 9.40539836883545, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8750087022781372, "xcomet_qe_score": 0.875757098197937, "metricx_score": 22.11182975769043, "metricx_qe_score": 24.966899871826172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "For adaptive overfitting, we saw that from the graph on the right, the red best fit line has a gradient that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6834746599197388, "xcomet_qe_score": 0.7018612623214722, "metricx_score": 24.98589324951172, "metricx_qe_score": 23.597803115844727, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is greater than one. This means that every unit of improvement that we made on Kono two thousand three translates to more than one unit of improvement on Kono plus plus, which means that there is no diminishing retur", "metrics": {"bleu_score": 0.0, "chrf_score": 0.40650406504065034, "xcomet_score": 0.371115118265152, "xcomet_qe_score": 0.4117874801158905, "metricx_score": 19.57278823852539, "metricx_qe_score": 15.565537452697754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ns. And this shows us that adaptive overfitting in this case is not", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5633525848388672, "xcomet_qe_score": 0.6452363729476929, "metricx_score": 21.396459579467773, "metricx_qe_score": 23.32832908630371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "observed. So what about temporary", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5426713228225708, "xcomet_qe_score": 0.633704662322998, "metricx_score": 16.604955673217773, "metricx_qe_score": 13.448080062866211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "drift then? For temporal drift, we conducted an experiment to retrain or continue to pre-train some models with more recent data, and we found that the performance degrades with larger temporal gaps. And this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5041490793228149, "xcomet_qe_score": 0.5215370655059814, "metricx_score": 16.6480655670166, "metricx_qe_score": 14.452895164489746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples, and these goes hand in hand", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8037947416305542, "xcomet_qe_score": 0.9166054129600525, "metricx_score": 19.602916717529297, "metricx_qe_score": 19.976272583007812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We can't just have one ingredient but throw out the others. At the same time", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8375693559646606, "xcomet_qe_score": 0.8598387241363525, "metricx_score": 19.56410789489746, "metricx_qe_score": 18.947248458862305, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting, even though Kono two thousand three has been used for over twenty years. So going back to the question that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.1690331304935767, "xcomet_score": 0.4933009147644043, "xcomet_qe_score": 0.6210708022117615, "metricx_score": 20.501604080200195, "metricx_qe_score": 21.786802291870117, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we raised in the title of our paper, do Kono 2003 taggers still work in 2023? And we found that the answer", "metrics": {"bleu_score": 1.0813077419617458, "chrf_score": 6.727299817795654, "xcomet_score": 0.5486690998077393, "xcomet_qe_score": 0.7289345860481262, "metricx_score": 19.153444290161133, "metricx_qe_score": 18.115253448486328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is actually a resounding yes. We hope that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5858700275421143, "xcomet_qe_score": 0.8615640997886658, "metricx_score": 16.129615783691406, "metricx_qe_score": 19.87192726135254, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "our paper calls for more research on how to improve generalizations of the models. And last", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7659718990325928, "xcomet_qe_score": 0.8140745162963867, "metricx_score": 24.134918212890625, "metricx_qe_score": 23.430192947387695, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ly, please make sure to check out our paper, our dataset, and if you have any questions feel free to contact me. Thank", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8544344902038574, "xcomet_qe_score": 0.802852988243103, "metricx_score": 10.83608627319336, "metricx_qe_score": 10.985431671142578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "very much.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8815122246742249, "xcomet_qe_score": 0.855570375919342, "metricx_score": 3.9777565002441406, "metricx_qe_score": 5.062550067901611, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3974819481372833, "xcomet_qe_score": 0.28956812620162964, "metricx_score": 3.7109405994415283, "metricx_qe_score": 0.16473039984703064, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'m going to talk about our work on resolving indirect reference expressions for entity selection, in which we introduced the altentity scorpus. My name", "metrics": {"bleu_score": 0.0, "chrf_score": 4.7420692525304435, "xcomet_score": 0.3574455678462982, "xcomet_qe_score": 0.4933532476425171, "metricx_score": 18.74106788635254, "metricx_qe_score": 17.27460479736328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is Javod Hosseini and this is a joint work with Philip Radinsky, Silvia Paretti and Annie Luis. Our goal is to understand users'", "metrics": {"bleu_score": 1.9822566267103439, "chrf_score": 38.780671647241064, "xcomet_score": 0.4806393086910248, "xcomet_qe_score": 0.4749315679073334, "metricx_score": 13.493376731872559, "metricx_qe_score": 10.273481369018555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language when they want to make a choice. Consider this alternative question.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1573960781097412, "xcomet_qe_score": 0.23184362053871155, "metricx_score": 22.416271209716797, "metricx_qe_score": 21.136962890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Did you mean easy", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16066154837608337, "xcomet_qe_score": 0.155414417386055, "metricx_score": 7.915130138397217, "metricx_qe_score": 5.7601542472839355, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on me or I got a feeling? Here", "metrics": {"bleu_score": 2.0857995854646623, "chrf_score": 17.394938145922207, "xcomet_score": 0.19028158485889435, "xcomet_qe_score": 0.23930080235004425, "metricx_score": 16.261877059936523, "metricx_qe_score": 14.264935493469238, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a user wants to select between one of these two signs. The most obvious thing is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.28889644145965576, "xcomet_qe_score": 0.6137281656265259, "metricx_score": 21.364442825317383, "metricx_qe_score": 22.68965721130371, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to use a direct reference, for example by saying the name of the song Easy on Me or its position", "metrics": {"bleu_score": 3.0087752026928056, "chrf_score": 11.513271428351066, "xcomet_score": 0.6274781823158264, "xcomet_qe_score": 0.7363680601119995, "metricx_score": 23.925819396972656, "metricx_qe_score": 23.617084503173828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the first one. But sometimes an indirect reference is more appropriate to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.56218421459198, "xcomet_qe_score": 0.5737403631210327, "metricx_score": 22.258827209472656, "metricx_qe_score": 20.811052322387695, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "have a more natural conversation. This could happen when the user cannot remember the name of the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2872353494167328, "xcomet_qe_score": 0.2796308994293213, "metricx_score": 20.043018341064453, "metricx_qe_score": 18.399330139160156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "source. All the pronunciations are too similar to each other and hard to disambigu", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.581833004951477, "xcomet_qe_score": 0.5216052532196045, "metricx_score": 17.0220947265625, "metricx_qe_score": 15.671317100524902, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ate. or when the user wants to specify a preference. Here are some", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.619671106338501, "xcomet_qe_score": 0.5605716705322266, "metricx_score": 14.3162260055542, "metricx_qe_score": 17.487993240356445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "examples of indirect differences, for example the newer one or the song that's not energetic. This is an important problem in convers", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3837043046951294, "xcomet_qe_score": 0.4748409390449524, "metricx_score": 18.335535049438477, "metricx_qe_score": 15.563454627990723, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ational systems and also for benchmarking LLM's entity understanding. We are not aware of a public data", "metrics": {"bleu_score": 0.0, "chrf_score": 2.086798321421391, "xcomet_score": 0.16991133987903595, "xcomet_qe_score": 0.6304049491882324, "metricx_score": 22.844301223754883, "metricx_qe_score": 20.660110473632812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "set, a large-scale public data set for the task, so we collect one using crowd annotation. Our data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5055110454559326, "xcomet_qe_score": 0.553533673286438, "metricx_score": 23.00910186767578, "metricx_qe_score": 23.92732810974121, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "set covers three different domains, music, books, and research. Our dataset", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6047433614730835, "xcomet_qe_score": 0.6567531824111938, "metricx_score": 17.48915672302246, "metricx_qe_score": 18.501678466796875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "collection methodology emphasizes informality using a cartoon completion set. The cartoon", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7234344482421875, "xcomet_qe_score": 0.777664303779602, "metricx_score": 22.324851989746094, "metricx_qe_score": 21.448204040527344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "has three speech bubbles. In the first", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.38748422265052795, "xcomet_qe_score": 0.5588328838348389, "metricx_score": 19.07594108581543, "metricx_qe_score": 21.16543197631836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "bubble, Bob says, remember that song we were listening to yesterday? And", "metrics": {"bleu_score": 1.2622239405701918, "chrf_score": 2.9485542477956583, "xcomet_score": 0.7503014802932739, "xcomet_qe_score": 0.8800971508026123, "metricx_score": 15.996962547302246, "metricx_qe_score": 22.583885192871094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "with that, Bob sets the dialogue context. In the second speech bubble", "metrics": {"bleu_score": 2.6440681349450874, "chrf_score": 4.081078335949463, "xcomet_score": 0.8183161020278931, "xcomet_qe_score": 0.8457788228988647, "metricx_score": 15.165181159973145, "metricx_qe_score": 23.501800537109375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", Alice says, do you mean easy on me or I got off filling? which is the", "metrics": {"bleu_score": 1.5740210083286725, "chrf_score": 16.897797674872585, "xcomet_score": 0.3451383411884308, "xcomet_qe_score": 0.3585052192211151, "metricx_score": 21.213150024414062, "metricx_qe_score": 18.253015518188477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "alternative question. And in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.31465113162994385, "xcomet_qe_score": 0.5081089735031128, "metricx_score": 6.61873722076416, "metricx_qe_score": 16.531187057495117, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, the new RF. We provide", "metrics": {"bleu_score": 0.9833441958415141, "chrf_score": 1.9872741367218136, "xcomet_score": 0.7133040428161621, "xcomet_qe_score": 0.6843038201332092, "metricx_score": 21.045841217041016, "metricx_qe_score": 23.150131225585938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5167675018310547, "xcomet_qe_score": 0.5697540044784546, "metricx_score": 24.949338912963867, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "speech bubble is chosen from a few manual prompts per domain. The second", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4180160462856293, "xcomet_qe_score": 0.765809178352356, "metricx_score": 20.500993728637695, "metricx_qe_score": 22.621349334716797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "one, which is the alternative question, is generated as", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8113579750061035, "xcomet_qe_score": 0.9529035091400146, "metricx_score": 20.55703353881836, "metricx_qe_score": 18.05560302734375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "follows. We always use a simple template. Do", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6476267576217651, "xcomet_qe_score": 0.6605138778686523, "metricx_score": 11.212299346923828, "metricx_qe_score": 11.964639663696289, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "you mean A or B? Where", "metrics": {"bleu_score": 6.770186228657864, "chrf_score": 3.401360544217687, "xcomet_score": 0.8141329288482666, "xcomet_qe_score": 0.8464837074279785, "metricx_score": 4.273735523223877, "metricx_qe_score": 3.8512213230133057, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "A and B are samples from Wikipedia. Here are the different", "metrics": {"bleu_score": 3.4089919964838553, "chrf_score": 1.6025641025641024, "xcomet_score": 0.7950010299682617, "xcomet_qe_score": 0.8332411050796509, "metricx_score": 7.6493706703186035, "metricx_qe_score": 13.089649200439453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sampling methods we've used. When we", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.342849463224411, "xcomet_qe_score": 0.7418397068977356, "metricx_score": 21.75005531311035, "metricx_qe_score": 22.26708984375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "move higher in the list, the entities become more similar to each other and it's usually harder to make the disambigu", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7522144317626953, "xcomet_qe_score": 0.7926809787750244, "metricx_score": 24.650903701782227, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ation. The first one is uniform at random", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8511322736740112, "xcomet_qe_score": 0.8555403351783752, "metricx_score": 11.565897941589355, "metricx_qe_score": 21.130516052246094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". The second one is when the entities have similar titles, for example, two books with the name the retailer", "metrics": {"bleu_score": 1.3149361462557834, "chrf_score": 4.04403131741441, "xcomet_score": 0.7047948837280273, "xcomet_qe_score": 0.7853133678436279, "metricx_score": 23.63656234741211, "metricx_qe_score": 24.264720916748047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". The third one is when they have similar descriptions on Wikipedia and finally when they", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4367256760597229, "xcomet_qe_score": 0.33962246775627136, "metricx_score": 20.373098373413086, "metricx_qe_score": 21.473251342773438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "have similar info boxes or attributes on Wikipedia. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8101948499679565, "xcomet_qe_score": 0.9591163396835327, "metricx_score": 16.54468536376953, "metricx_qe_score": 17.681650161743164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the same genre or the same artist voice. When", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.37771838903427124, "xcomet_qe_score": 0.5936587452888489, "metricx_score": 16.3302059173584, "metricx_qe_score": 17.5366268157959, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we show this alternative question to the annotators, they know the name of these entities, but they don't necessarily know", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8212230205535889, "xcomet_qe_score": 0.8860230445861816, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "about the entity. So what we do is that we show some background knowledge", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7361222505569458, "xcomet_qe_score": 0.783220648765564, "metricx_score": 22.283870697021484, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "about the two entities. For songs, we simply show a Google search link to each song. and then ask the annotators to listen to at least some of each song and read about each song.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6162010431289673, "xcomet_qe_score": 0.613275945186615, "metricx_score": 15.253311157226562, "metricx_qe_score": 13.735413551330566, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Here's for example the Google search result for the song Easy. For the recipes", "metrics": {"bleu_score": 1.8830168484966994, "chrf_score": 7.250617313491175, "xcomet_score": 0.65290367603302, "xcomet_qe_score": 0.7827990055084229, "metricx_score": 15.35505199432373, "metricx_qe_score": 14.70975112915039, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and books domain we show some background text from Wikipedia. For recipes we additionally show their images again from Wikipedia", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9164594411849976, "xcomet_qe_score": 0.9515847563743591, "metricx_score": 12.144725799560547, "metricx_qe_score": 10.380305290222168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "so that the annotators know how they look. Then we ask the annotators to pick one of these entities", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2766752243041992, "xcomet_qe_score": 0.26596641540527344, "metricx_score": 21.78646469116211, "metricx_qe_score": 19.020214080810547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", for example here the first one, and describe them using 3 to 5 indirect referring expressions. example the one with the piano music here are some", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3247559070587158, "xcomet_qe_score": 0.5969034433364868, "metricx_score": 20.97128677368164, "metricx_qe_score": 20.798583984375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "examples from our data set for example the one", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.19714167714118958, "xcomet_qe_score": 0.1927584409713745, "metricx_score": 10.525548934936523, "metricx_qe_score": 10.108636856079102, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "without words not the one with the 12-year", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15549832582473755, "xcomet_qe_score": 0.1337181031703949, "metricx_score": 23.284225463867188, "metricx_qe_score": 23.927762985229492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "-old boy or the fictional one or comes from azerbaijan and so on The", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2835751175880432, "xcomet_qe_score": 0.4780004918575287, "metricx_score": 20.675376892089844, "metricx_qe_score": 19.564189910888672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "altentities corpus has 6,000 alternative questions across three domains and it has 42,000 indirect referring expressions. Results with T", "metrics": {"bleu_score": 1.3809851504530963, "chrf_score": 17.987954036792054, "xcomet_score": 0.595976710319519, "xcomet_qe_score": 0.6178547143936157, "metricx_score": 21.5684814453125, "metricx_qe_score": 24.168865203857422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "5xLarge model are summarized below. If the", "metrics": {"bleu_score": 0.0, "chrf_score": 1.6025641025641024, "xcomet_score": 0.5369266271591187, "xcomet_qe_score": 0.6036802530288696, "metricx_score": 20.665374755859375, "metricx_qe_score": 17.522764205932617, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92-95%. But this is", "metrics": {"bleu_score": 1.6362723107055732, "chrf_score": 2.3871788104868714, "xcomet_score": 0.8045352697372437, "xcomet_qe_score": 0.8039892911911011, "metricx_score": 22.180875778198242, "metricx_qe_score": 21.608741760253906, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "not realistic. If the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.42926910519599915, "xcomet_qe_score": 0.49063026905059814, "metricx_score": 16.306446075439453, "metricx_qe_score": 19.957897186279297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language model has access to some partially overlapping background knowledge, then the accuracy is between 82% to 87%, which is more realistic, for example, when the", "metrics": {"bleu_score": 2.3265025916111193, "chrf_score": 3.1584400043023355, "xcomet_score": 0.7628235816955566, "xcomet_qe_score": 0.7288452386856079, "metricx_score": 13.051755905151367, "metricx_qe_score": 14.330846786499023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language model retrieves the background knowledge. If the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6980473399162292, "xcomet_qe_score": 0.8058968186378479, "metricx_score": 20.276447296142578, "metricx_qe_score": 18.203659057617188, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language model has access only to entity names, then the accuracy is only 60%. So there's a lot of room", "metrics": {"bleu_score": 1.8713897922947806, "chrf_score": 2.1872458584070675, "xcomet_score": 0.8609218597412109, "xcomet_qe_score": 0.8825278282165527, "metricx_score": 24.477642059326172, "metricx_qe_score": 24.929462432861328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for improvement. We've also shown that the models", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3095637857913971, "xcomet_qe_score": 0.6933994293212891, "metricx_score": 23.463478088378906, "metricx_qe_score": 24.87355613708496, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "are domain generalizable. Here is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1735372245311737, "xcomet_qe_score": 0.21958574652671814, "metricx_score": 18.427112579345703, "metricx_qe_score": 19.260496139526367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9979676008224487, "xcomet_qe_score": 0.9924132823944092, "metricx_score": 0.31238144636154175, "metricx_qe_score": 1.0499012470245361, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, I'm Sarah Pappy from the University of Trento and Fondazione Bruno Kessler, and I will briefly introduce the Attention as a Guide for Simultaneous Speech Translation paper that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation?", "metrics": {"bleu_score": 1.4181318326137782, "chrf_score": 16.77851695466747, "xcomet_score": 0.660178542137146, "xcomet_qe_score": 0.7137038707733154, "metricx_score": 9.72901725769043, "metricx_qe_score": 7.869383335113525, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Simultaneous speech translation or sim", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8172117471694946, "xcomet_qe_score": 0.8207889795303345, "metricx_score": 6.575237274169922, "metricx_qe_score": 5.137071132659912, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ulated SD is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are", "metrics": {"bleu_score": 0.0, "chrf_score": 2.1360502029362816, "xcomet_score": 0.32908958196640015, "xcomet_qe_score": 0.45205816626548767, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the problems of the current SimulST models?", "metrics": {"bleu_score": 3.3495035708457803, "chrf_score": 22.72411790309029, "xcomet_score": 0.8676978945732117, "xcomet_qe_score": 0.8823621273040771, "metricx_score": 19.39298439025879, "metricx_qe_score": 23.590744018554688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Specific architectures are usually trained introducing additional modules to be optimized. Long and complic", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8307156562805176, "xcomet_qe_score": 0.835671603679657, "metricx_score": 23.392959594726562, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ated training procedures, for example, training involving different optimization objectives And training and mainta", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43502819538116455, "xcomet_qe_score": 0.4254455864429474, "metricx_score": 18.68915367126465, "metricx_qe_score": 22.151050567626953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ining several models to reach different latency regimes", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6828031539916992, "xcomet_qe_score": 0.7567196488380432, "metricx_score": 22.396221160888672, "metricx_qe_score": 22.6129150390625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", for example training a model with an average of one second latency and another one with two seconds latency and so on. So what", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.763861894607544, "xcomet_qe_score": 0.8272361159324646, "metricx_score": 21.06936264038086, "metricx_qe_score": 24.61650276184082, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is our solution?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7754735946655273, "xcomet_qe_score": 0.8085792064666748, "metricx_score": 9.39853572845459, "metricx_qe_score": 13.427627563476562, "linguapy_score": [1, "FRENCH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "First, use already existing offline ST models without retraining or adopting specific architecture for CLST. Use only one model for every latency", "metrics": {"bleu_score": 1.037178985980195, "chrf_score": 2.903774681944083, "xcomet_score": 0.6798123717308044, "xcomet_qe_score": 0.781733512878418, "metricx_score": 13.564093589782715, "metricx_qe_score": 13.24828815460205, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "regime and handle latency through specific parameters. and leverage the knowledge already acquired by the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24156203866004944, "xcomet_qe_score": 0.4123811423778534, "metricx_score": 20.52696990966797, "metricx_qe_score": 23.502153396606445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "model through the attention mechanism between audio input and textual output that is the cross attention mechanism. And", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.672211766242981, "xcomet_qe_score": 0.7801872491836548, "metricx_score": 15.879983901977539, "metricx_qe_score": 11.351444244384766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "you can see an example on the right. Our solution is to propose", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3898096978664398, "xcomet_qe_score": 0.6643329858779907, "metricx_score": 16.931058883666992, "metricx_qe_score": 19.15583610534668, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a dot or encoder decoder attention, and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to. A word is emitted", "metrics": {"bleu_score": 0.0, "chrf_score": 0.9114064434534448, "xcomet_score": 0.45343294739723206, "xcomet_qe_score": 0.5064672231674194, "metricx_score": 19.662670135498047, "metricx_qe_score": 17.23496437072754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "if the tension is not concentrated, that is, this sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is stable enough. For example,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5130460858345032, "xcomet_qe_score": 0.580268144607544, "metricx_score": 21.583297729492188, "metricx_qe_score": 19.784744262695312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "if we receive a speech shank containing I'm going to talk about and our model predicts the translation in German, we And we will look at the cross attention weights. We will see that the first two words point to the earliest received speech frames, while the last word point to the last received speech frames, the last lambda speech frames. This means that the first two words will be emitted. Since", "metrics": {"bleu_score": 3.228777856868647, "chrf_score": 10.44714883057237, "xcomet_score": 0.2330949604511261, "xcomet_qe_score": 0.30146607756614685, "metricx_score": 20.33747100830078, "metricx_qe_score": 18.093719482421875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the sum of the crossed tension is above a certain threshold alpha, we will not emit the last word and we will wait for another speechhunk. If we go on and we receive another speech sink and", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.32391759753227234, "xcomet_qe_score": 0.38234448432922363, "metricx_score": 21.846080780029297, "metricx_qe_score": 18.673011779785156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "our model predicts other three words and we will look at the cross-attention weights. We will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.49594661593437195, "xcomet_qe_score": 0.5891218185424805, "metricx_score": 16.925216674804688, "metricx_qe_score": 10.726702690124512, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we look at the main results of that, we find", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.147118479013443, "xcomet_qe_score": 0.15466929972171783, "metricx_score": 13.407881736755371, "metricx_qe_score": 19.730791091918945, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We will plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average lagging. This is the latency measure. We also considered the computational aware average lagging that accounts for the modus computational time to predict the output. So we want our curves to be as high as possible on this plot", "metrics": {"bleu_score": 0.0, "chrf_score": 0.3485176589664037, "xcomet_score": 0.6389014720916748, "xcomet_qe_score": 0.7399451732635498, "metricx_score": 15.809280395507812, "metricx_qe_score": 13.488856315612793, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". But also we want that they are shifted on the left.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15388856828212738, "xcomet_qe_score": 0.1545831859111786, "metricx_score": 13.802985191345215, "metricx_qe_score": 14.799193382263184, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And we compare with PROPERA strategies that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13237756490707397, "xcomet_qe_score": 0.11944520473480225, "metricx_score": 21.169086456298828, "metricx_qe_score": 17.908252716064453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "also apply to offline models that are the WitKey strategy and the local agreement. And we compare also with state-of-the-art architecture specific", "metrics": {"bleu_score": 2.4291731318329797, "chrf_score": 20.674176169618878, "xcomet_score": 0.5145925283432007, "xcomet_qe_score": 0.5931394100189209, "metricx_score": 16.077526092529297, "metricx_qe_score": 17.320953369140625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ally tailored for simultaneous pre-translation. These are all the results of the simultaneous speech", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2293771654367447, "xcomet_qe_score": 0.5417894721031189, "metricx_score": 21.150390625, "metricx_qe_score": 19.805635452270508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "translation strategy on German. And we see that ADUT outperforms", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6064863204956055, "xcomet_qe_score": 0.6561321020126343, "metricx_score": 15.194075584411621, "metricx_qe_score": 15.752822875976562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "all the strategies applied to offline models since their curves are shifted towards the left. And we also see that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7644490003585815, "xcomet_qe_score": 0.796576976776123, "metricx_score": 17.687177658081055, "metricx_qe_score": 16.718605041503906, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "if we consider the actual elapsed time or the computational wear time, that is the fastest strategy.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8366769552230835, "xcomet_qe_score": 0.8147296905517578, "metricx_score": 18.26996421813965, "metricx_qe_score": 23.075204849243164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "If you want to discover more results, read our paper. We also released", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8415653705596924, "xcomet_qe_score": 0.8209981322288513, "metricx_score": 9.902176856994629, "metricx_qe_score": 15.757851600646973, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.666612982749939, "xcomet_qe_score": 0.8117634057998657, "metricx_score": 21.01760482788086, "metricx_qe_score": 24.674312591552734, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for your attention.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.920612096786499, "xcomet_qe_score": 0.9883337020874023, "metricx_score": 6.235439300537109, "metricx_qe_score": 8.985917091369629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone, my name is Ying and my colleague Jiang and I will be presenting our research on multi instruct improving multimodal serial learning via instruction tuning. With the advances in", "metrics": {"bleu_score": 0.8294952460380787, "chrf_score": 11.546756987620448, "xcomet_score": 0.5469999313354492, "xcomet_qe_score": 0.5733895301818848, "metricx_score": 14.63677978515625, "metricx_qe_score": 10.800041198730469, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way. Recently", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7857564687728882, "xcomet_qe_score": 0.7857794761657715, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", many studies have shown that instruction tuning enables large language models to perform unseen tasks in a zero shot manner by following natural instructions.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7690815925598145, "xcomet_qe_score": 0.8810943961143494, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "However, most previous works on instruction tuning focused on improving the serial shot performance on language only tasks, while computer vision and multimodal tasks have been left out.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8190374374389648, "xcomet_qe_score": 0.8212146759033203, "metricx_score": 24.46305274963379, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Therefore, in this work, we want to investigate whether instruction tuning on multimodal pre trained models can actually improve generalization to unseen multimodal tasks. Additionally", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.716984748840332, "xcomet_qe_score": 0.7709293365478516, "metricx_score": 20.49020767211914, "metricx_qe_score": 14.475845336914062, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", at the time of our research, we discovered a considerable discrepancy in the availability of instruction data set between RLP and multimodal.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8961334228515625, "xcomet_qe_score": 0.8330373764038086, "metricx_score": 22.312421798706055, "metricx_qe_score": 19.377059936523438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "There exists more than one thousand six hundred language only instru", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8475791215896606, "xcomet_qe_score": 0.8680959939956665, "metricx_score": 11.410730361938477, "metricx_qe_score": 7.715758800506592, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ction tasks. However, there is no large scale publicly available multimod", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6533036231994629, "xcomet_qe_score": 0.8234870433807373, "metricx_score": 14.81682300567627, "metricx_qe_score": 13.731948852539062, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "al instruction task. Therefore, this motivates us to build a multimodal instruction", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6054773330688477, "xcomet_qe_score": 0.6246285438537598, "metricx_score": 19.405029296875, "metricx_qe_score": 19.363014221191406, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tuning data set. Here we present Multi Instruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks covering 10 broad categories. These", "metrics": {"bleu_score": 0.7091198186788286, "chrf_score": 13.871832739312651, "xcomet_score": 0.5980620384216309, "xcomet_qe_score": 0.6042270660400391, "metricx_score": 15.11921501159668, "metricx_qe_score": 10.580162048339844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tasks are derived from twenty one existing open source dataset, and each task is equipped with five expert written instructions. For investigat", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7915298938751221, "xcomet_qe_score": 0.8077900409698486, "metricx_score": 9.882438659667969, "metricx_qe_score": 11.337647438049316, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ing multimodal instruction tuning on our proposed dataset, we take OFA a unified multimodal pattern model as our base model. OFA use a unified voca", "metrics": {"bleu_score": 0.7125301753775903, "chrf_score": 1.6254155175655933, "xcomet_score": 0.593730092048645, "xcomet_qe_score": 0.7000781297683716, "metricx_score": 15.591081619262695, "metricx_qe_score": 16.800613403320312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "bulary for language, image tokens, and the coordinate of a bounding box. Here we show some", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22377711534500122, "xcomet_qe_score": 0.29691484570503235, "metricx_score": 20.542682647705078, "metricx_qe_score": 17.857219696044922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "example instances from our multi-instrate dataset. To unify the processing of a various input and output data type. We follow the method from", "metrics": {"bleu_score": 0.0, "chrf_score": 5.941942650651226, "xcomet_score": 0.5922784805297852, "xcomet_qe_score": 0.769029974937439, "metricx_score": 16.052825927734375, "metricx_qe_score": 11.986784934997559, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFA and formulate all the tasks in a unified sequence to sequence format in which the", "metrics": {"bleu_score": 1.1552167775385556, "chrf_score": 2.6797928883419826, "xcomet_score": 0.5235572457313538, "xcomet_qe_score": 0.553947925567627, "metricx_score": 20.52183723449707, "metricx_qe_score": 19.058561325073242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "input text, images, instructions, and bounding boxes are represented in the same token space. Okay, now I", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8822851181030273, "xcomet_qe_score": 0.9681782126426697, "metricx_score": 18.69253921508789, "metricx_qe_score": 18.74541473388672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'m going to talk about multimodal instruction tuning. For", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.26616060733795166, "xcomet_qe_score": 0.3131346106529236, "metricx_score": 19.70750617980957, "metricx_qe_score": 22.031452178955078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the training dataset, we use 53 tasks from the NIG group for training and sample 10,000 instances per task.", "metrics": {"bleu_score": 0.9612180839938747, "chrf_score": 7.774074850604775, "xcomet_score": 0.7360484600067139, "xcomet_qe_score": 0.7793081998825073, "metricx_score": 18.002159118652344, "metricx_qe_score": 17.534639358520508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "For testing, we reserve the entire Common Sense Reasoning group for testing and select additional five tasks from WQA and the Miscellaneous group. We", "metrics": {"bleu_score": 0.0, "chrf_score": 0.28735632183908044, "xcomet_score": 0.6840193271636963, "xcomet_qe_score": 0.7063463926315308, "metricx_score": 11.469314575195312, "metricx_qe_score": 8.001760482788086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "use all the instances in the test split for each task. In", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6877530813217163, "xcomet_qe_score": 0.7915340662002563, "metricx_score": 12.721413612365723, "metricx_qe_score": 19.79897689819336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "addition, we randomly sample twenty tasks from the test split of natural instruction as on Syntask for NLP. Therefore, we use", "metrics": {"bleu_score": 0.9948511484873499, "chrf_score": 1.9716008680686288, "xcomet_score": 0.6113331317901611, "xcomet_qe_score": 0.6818392872810364, "metricx_score": 17.247297286987305, "metricx_qe_score": 17.51300811767578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a pre-trained OFA large model as a base model.", "metrics": {"bleu_score": 1.5436644068195333, "chrf_score": 4.080133539734514, "xcomet_score": 0.9647085666656494, "xcomet_qe_score": 0.9620060920715332, "metricx_score": 18.96831512451172, "metricx_qe_score": 22.726240158081055, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "During the training, we mix all the instances for all the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5844139456748962, "xcomet_qe_score": 0.6939683556556702, "metricx_score": 19.570816040039062, "metricx_qe_score": 20.94739532470703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tasks. Each instance is randomly combined with one of its five instruction templates", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7833300828933716, "xcomet_qe_score": 0.7709276676177979, "metricx_score": 19.33526039123535, "metricx_qe_score": 23.220659255981445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". So during the test for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7777850031852722, "xcomet_qe_score": 0.8191883563995361, "metricx_score": 22.98328399658203, "metricx_qe_score": 22.7645320892334, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "each experiment. We report the mean and max performance and the standard deviation of the performance across all five experiments. If the task is a", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.68397057056427, "xcomet_qe_score": 0.7243018746376038, "metricx_score": 13.980783462524414, "metricx_qe_score": 14.37753963470459, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "multimodal classification task, we report accuracy. If it is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8208754062652588, "xcomet_qe_score": 0.8488312363624573, "metricx_score": 18.839780807495117, "metricx_qe_score": 15.517950057983398, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a multimodal generation task, we report RougeL. For an RP task, we report RougeL as", "metrics": {"bleu_score": 0.0, "chrf_score": 11.517509566864884, "xcomet_score": 0.6499228477478027, "xcomet_qe_score": 0.682797908782959, "metricx_score": 15.422080993652344, "metricx_qe_score": 12.074069023132324, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "well. We also introduced an additional evaluation metric called sensitivity. This measures the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7870036363601685, "xcomet_qe_score": 0.8182529211044312, "metricx_score": 11.404184341430664, "metricx_qe_score": 8.677942276000977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8710147142410278, "xcomet_qe_score": 0.9087187647819519, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of the instruction. Here", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18538597226142883, "xcomet_qe_score": 0.28386980295181274, "metricx_score": 19.54355239868164, "metricx_qe_score": 18.694992065429688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "are our main results. As we can see, instruction tuning can significantly improve OFE's performance on multi-model tasks", "metrics": {"bleu_score": 0.0, "chrf_score": 1.3789702520631522, "xcomet_score": 0.7645982503890991, "xcomet_qe_score": 0.7559123039245605, "metricx_score": 11.453060150146484, "metricx_qe_score": 9.074381828308105, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Also, transfer learning from natural instruction datasets can benefit instruction tuning. Here we can", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7942744493484497, "xcomet_qe_score": 0.8085997700691223, "metricx_score": 14.198209762573242, "metricx_qe_score": 15.984333992004395, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "see that as the amount of tasks increased the model achieved better performance and in the meantime lower sensitivity. So, we also did", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6471801996231079, "xcomet_qe_score": 0.7303193807601929, "metricx_score": 13.879982948303223, "metricx_qe_score": 11.843782424926758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "one experiment, we used one instru", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.634955644607544, "xcomet_qe_score": 0.8616257905960083, "metricx_score": 10.3729829788208, "metricx_qe_score": 10.488787651062012, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ction versus five instruction. As we can see, using", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7544081211090088, "xcomet_qe_score": 0.7647403478622437, "metricx_score": 19.60927963256836, "metricx_qe_score": 20.25482940673828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "more instructions can improve the model's overall performance and reduce its sensitivity a lot. This shows the effect of different fine tuning strategies", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9161087274551392, "xcomet_qe_score": 0.9697692394256592, "metricx_score": 14.336669921875, "metricx_qe_score": 16.506816864013672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on the model's sensitivity. As we can see, by", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.27072691917419434, "xcomet_qe_score": 0.5445718765258789, "metricx_score": 22.81203269958496, "metricx_qe_score": 23.001867294311523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "transferring learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original IFA model. We can also see", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7756438836872447, "xcomet_score": 0.807135820388794, "xcomet_qe_score": 0.8596709966659546, "metricx_score": 19.94566535949707, "metricx_qe_score": 16.081239700317383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that transfer learning from natural instruction data set can help OFA to achieve much better performance on the natural instruct data set.", "metrics": {"bleu_score": 0.6608080277917386, "chrf_score": 1.6800379012093853, "xcomet_score": 0.8227633833885193, "xcomet_qe_score": 0.87679523229599, "metricx_score": 13.378807067871094, "metricx_qe_score": 11.570600509643555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So, in general, we propose the first large-scale multimodal instruction tuning data set, which significantly improves the derivatives capability of OFA, and we explore different transfer learning techniques and show their benefits", "metrics": {"bleu_score": 0.4215262418441277, "chrf_score": 1.0512337965499086, "xcomet_score": 0.7829830646514893, "xcomet_qe_score": 0.8412284255027771, "metricx_score": 6.295924186706543, "metricx_qe_score": 4.311554908752441, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "with the design of a new metric called sensitivity. And", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6976878643035889, "xcomet_qe_score": 0.7576584815979004, "metricx_score": 11.184550285339355, "metricx_qe_score": 12.180697441101074, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "one more thing, we are collecting a much larger multi-model instruction tuning data set with around 150 additional Variant language tasks and we will release them. So this is a QR code", "metrics": {"bleu_score": 0.604451600599491, "chrf_score": 1.3382491637693676, "xcomet_score": 0.6010286211967468, "xcomet_qe_score": 0.7009808421134949, "metricx_score": 15.439247131347656, "metricx_qe_score": 16.733274459838867, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for our data and model. Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6284548044204712, "xcomet_qe_score": 0.9328312873840332, "metricx_score": 6.110783100128174, "metricx_qe_score": 7.780552864074707, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9993386268615723, "xcomet_qe_score": 1.0, "metricx_score": 0.3062780499458313, "metricx_qe_score": 0.8795042037963867, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hi everyone, I", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.372894823551178, "xcomet_qe_score": 0.42938026785850525, "metricx_score": 4.2430524826049805, "metricx_qe_score": 0.43722620606422424, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'m Coast of Sena and I'm pleased to welcome you to our talk of our ACL 2023 paper, Language Model Accep", "metrics": {"bleu_score": 2.8801557655317924, "chrf_score": 13.208124505163898, "xcomet_score": 0.30261096358299255, "xcomet_qe_score": 0.4040946066379547, "metricx_score": 20.150819778442383, "metricx_qe_score": 17.802040100097656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tability Judgments are not always robust to context. This is a joint work with", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3446280062198639, "xcomet_qe_score": 0.32296544313430786, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "John Bokier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy and Adina William. So in this work we revisit", "metrics": {"bleu_score": 3.494449212088392, "chrf_score": 45.88755857205247, "xcomet_score": 0.38348719477653503, "xcomet_qe_score": 0.30653414130210876, "metricx_score": 19.31291961669922, "metricx_qe_score": 18.728187561035156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the minimal pair paradigm. The minimal pair paradigm basically evaluates language", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2241629958152771, "xcomet_qe_score": 0.5017441511154175, "metricx_score": 16.300647735595703, "metricx_qe_score": 15.131067276000977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "models on top of acceptability judgments, which can also include grammaticality", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.31429529190063477, "xcomet_qe_score": 0.2330661565065384, "metricx_score": 22.801101684570312, "metricx_qe_score": 19.50653076171875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", such as blimp, syntax gem, or acceptability in terms of stereotypes, such as Krauss pairs. And in this minimal pair paradigm,", "metrics": {"bleu_score": 0.0, "chrf_score": 8.19999702828868, "xcomet_score": 0.5086933970451355, "xcomet_qe_score": 0.5584979057312012, "metricx_score": 17.415714263916016, "metricx_qe_score": 15.60771369934082, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence And then the hope is that the model", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5899863243103027, "xcomet_qe_score": 0.7219874858856201, "metricx_score": 24.16059112548828, "metricx_qe_score": 24.842212677001953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "basically puts more probability to the acceptable sentence. The current MPP pip", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2740434408187866, "xcomet_qe_score": 0.32930988073349, "metricx_score": 24.460655212402344, "metricx_qe_score": 24.07842254638672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "eline basically doesn't allow us to evaluate model's acceptance towards longer sentences. These days,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6432742476463318, "xcomet_qe_score": 0.653998613357544, "metricx_score": 23.063983917236328, "metricx_qe_score": 22.517290115356445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "large language models are coming up with longer and longer context windows. Therefore, it is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7300673723220825, "xcomet_qe_score": 0.8226108551025391, "metricx_score": 23.02029037475586, "metricx_qe_score": 24.734893798828125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "crucial that we evaluate the model's acceptability throughout the context window. And that is what we are trying to do here. We are", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7963132858276367, "xcomet_qe_score": 0.7656878232955933, "metricx_score": 18.430633544921875, "metricx_qe_score": 21.183090209960938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "trying to revisit the NPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So", "metrics": {"bleu_score": 0.0, "chrf_score": 1.0624502450484898, "xcomet_score": 0.5279767513275146, "xcomet_qe_score": 0.7412327527999878, "metricx_score": 22.203325271606445, "metricx_qe_score": 19.067596435546875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that is the approach.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9491357803344727, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 3.455709218978882, "metricx_qe_score": 7.817426681518555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So what we do is that to simulate these longer sequences, we revisit the datasets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8397514820098877, "xcomet_qe_score": 0.9283178448677063, "metricx_score": 25.0, "metricx_qe_score": 24.746261596679688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "datasets. So for example, here we have chosen a typical pair of grammaticality from the blimp dataset from the adjunct", "metrics": {"bleu_score": 0.0, "chrf_score": 8.027221479405426, "xcomet_score": 0.6452269554138184, "xcomet_qe_score": 0.721402645111084, "metricx_score": 15.281939506530762, "metricx_qe_score": 14.950133323669434, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "island case. And what we do is that to recreate longer sequences and which are acceptable and which have the same matching of the gramm", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5653821229934692, "xcomet_qe_score": 0.6860617399215698, "metricx_score": 22.26763916015625, "metricx_qe_score": 23.191482543945312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "atical structure, we extract grammatical sentences from adjunctile. And then we add it as a prefix to both the acceptable query and the una", "metrics": {"bleu_score": 0.0, "chrf_score": 7.979166966925802, "xcomet_score": 0.6561886072158813, "xcomet_qe_score": 0.5679913759231567, "metricx_score": 20.209068298339844, "metricx_qe_score": 16.28623390197754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the model's", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4535442888736725, "xcomet_qe_score": 0.6154460906982422, "metricx_score": 20.444862365722656, "metricx_qe_score": 18.325000762939453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "acceptability. And we can also do the same by choosing sentences from a different subset", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6342814564704895, "xcomet_qe_score": 0.7568930387496948, "metricx_score": 22.796161651611328, "metricx_qe_score": 22.27326774597168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "or a different data set. So that is what we call", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5192952156066895, "xcomet_qe_score": 0.7361373901367188, "metricx_score": 20.127660751342773, "metricx_qe_score": 20.22890853881836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the mismatch scenario. So here the sentences are still coming from relevant datasets, but it'", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5521825551986694, "xcomet_qe_score": 0.6052589416503906, "metricx_score": 16.334115982055664, "metricx_qe_score": 13.978106498718262, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "s not from the same dataset that you are evaluating with.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15787360072135925, "xcomet_qe_score": 0.1442277431488037, "metricx_score": 20.175819396972656, "metricx_qe_score": 17.37727928161621, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And we can do the same for unacceptability case. Finally, we can choose sentences from a", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.34036630392074585, "xcomet_qe_score": 0.35481035709381104, "metricx_score": 22.72469139099121, "metricx_qe_score": 15.800153732299805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "completely unrelated domain, such as Wikipedia. So this will tell us whether the model's acceptability judgments are actually affected by any context. like whether the context is coming from a different subset of the data set or whether it's like", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5290892720222473, "xcomet_qe_score": 0.616866946220398, "metricx_score": 20.720428466796875, "metricx_qe_score": 19.235977172851562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "completely irrelevant to the current like to the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13402172923088074, "xcomet_qe_score": 0.10344508290290833, "metricx_score": 18.28080940246582, "metricx_qe_score": 13.569391250610352, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sentence that we are looking at. So how does the model do? First, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbit", "metrics": {"bleu_score": 0.6394701742849438, "chrf_score": 1.1972474017867603, "xcomet_score": 0.6359233856201172, "xcomet_qe_score": 0.7561371326446533, "metricx_score": 20.107194900512695, "metricx_qe_score": 18.881885528564453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "rary context length. We increased the context length toward up to 2024 to max out OPT and GPT two models and we", "metrics": {"bleu_score": 1.8749089613133423, "chrf_score": 7.512409294822627, "xcomet_score": 0.46886783838272095, "xcomet_qe_score": 0.5995701551437378, "metricx_score": 21.789077758789062, "metricx_qe_score": 18.16293716430664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "saw here in the orange dotted line the MPP judgments are relatively stable", "metrics": {"bleu_score": 2.1027582541760537, "chrf_score": 3.6090755327889847, "xcomet_score": 0.8655456304550171, "xcomet_qe_score": 0.8860785961151123, "metricx_score": 18.15387725830078, "metricx_qe_score": 23.573225021362305, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". What happens when we choose sentences from the same dataset? So here we are", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7788295149803162, "xcomet_qe_score": 0.8726697564125061, "metricx_score": 15.578694343566895, "metricx_qe_score": 14.872057914733887, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "choosing or creating sentences from acceptable and unacceptable domains from the same BLIMP or SYNTAX GIMP dataset. And there", "metrics": {"bleu_score": 0.0, "chrf_score": 4.089659079789872, "xcomet_score": 0.7936747074127197, "xcomet_qe_score": 0.8785839080810547, "metricx_score": 21.21241569519043, "metricx_qe_score": 17.572280883789062, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes.", "metrics": {"bleu_score": 0.700684265459173, "chrf_score": 1.7445380579294865, "xcomet_score": 0.9556736946105957, "xcomet_qe_score": 0.9811083078384399, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But when we match the structure, that is when we choose sentences from the same phenomena in blame per syntax, Jim. We see a massive increase or a massive decrease in the MPP judgment for the model depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large, like", "metrics": {"bleu_score": 0.4404046468997964, "chrf_score": 3.3703442963385797, "xcomet_score": 0.3531116247177124, "xcomet_qe_score": 0.46151497960090637, "metricx_score": 21.1793212890625, "metricx_qe_score": 22.165624618530273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this effect increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match pre", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5807734727859497, "xcomet_qe_score": 0.732064425945282, "metricx_score": 19.23531723022461, "metricx_qe_score": 22.63079261779785, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "fix affect the language model judgment so much? So we did a series of analyses where we tried to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.23949266970157623, "xcomet_qe_score": 0.5386026501655579, "metricx_score": 22.449562072753906, "metricx_qe_score": 22.46811866760254, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5505355596542358, "xcomet_qe_score": 0.6873419284820557, "metricx_score": 20.9151611328125, "metricx_qe_score": 17.819467544555664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "perturbations We find that none of these noises are actually making the model change its course in terms of how it shows us the MPP judgment trend. Basically", "metrics": {"bleu_score": 0.7159707839089766, "chrf_score": 1.5692924279291105, "xcomet_score": 0.637108564376831, "xcomet_qe_score": 0.5973641872406006, "metricx_score": 24.512723922729492, "metricx_qe_score": 16.97034454345703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we find that the models are sensitive to the perturbed sentences in similar ways. That is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8680329322814941, "xcomet_qe_score": 0.8957656621932983, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So the key take", "metrics": {"bleu_score": 0.40212947271990945, "chrf_score": 0.9888557570181193, "xcomet_score": 0.6912350654602051, "xcomet_qe_score": 0.5282000303268433, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "aways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.667902410030365, "xcomet_qe_score": 0.7694447636604309, "metricx_score": 25.0, "metricx_qe_score": 24.393047332763672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "MPP evaluation, the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context", "metrics": {"bleu_score": 0.608646094788208, "chrf_score": 1.438356124452126, "xcomet_score": 0.9398646354675293, "xcomet_qe_score": 0.9550843238830566, "metricx_score": 10.201568603515625, "metricx_qe_score": 10.96712589263916, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "window. Plead our paper for more details of our experiments. Thank you for listening", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5032180547714233, "xcomet_qe_score": 0.5961069464683533, "metricx_score": 16.35112953186035, "metricx_qe_score": 18.044841766357422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "much for your attention. See you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3910444378852844, "xcomet_qe_score": 0.5901917219161987, "metricx_score": 6.699085235595703, "metricx_qe_score": 8.157048225402832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone, my name is Yusen Zhang from the Penn State University. Today I", "metrics": {"bleu_score": 3.578469493084404, "chrf_score": 22.061421770411783, "xcomet_score": 0.8319989442825317, "xcomet_qe_score": 0.8673404455184937, "metricx_score": 8.451769828796387, "metricx_qe_score": 9.523375511169434, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'m going to present our work, Crosslingual Semantic Parsing in multiple natural languages and meaning representations. Semantic parsing is", "metrics": {"bleu_score": 0.0, "chrf_score": 2.046163585629337, "xcomet_score": 0.38085904717445374, "xcomet_qe_score": 0.5025942921638489, "metricx_score": 19.7406005859375, "metricx_qe_score": 15.883929252624512, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "therefore a task to build semantic representations of user queries such as SQL and Lambda CALCULUS.", "metrics": {"bleu_score": 1.6017504241305096, "chrf_score": 14.073535495499787, "xcomet_score": 0.7499677538871765, "xcomet_qe_score": 0.8647094964981079, "metricx_score": 22.092023849487305, "metricx_qe_score": 22.708032608032227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9959381818771362, "xcomet_qe_score": 1.0, "metricx_score": 23.587444305419922, "metricx_qe_score": 23.86141014099121, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda, or FunQL, etc", "metrics": {"bleu_score": 1.3769177677368838, "chrf_score": 12.07778000448684, "xcomet_score": 0.9687050580978394, "xcomet_qe_score": 0.9705365300178528, "metricx_score": 22.51465606689453, "metricx_qe_score": 24.770326614379883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Existing cross-lingual semantic parsing models are separately proposed and evaluated on dataset of limited tasks", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8296651840209961, "xcomet_qe_score": 0.918010950088501, "metricx_score": 23.822046279907227, "metricx_qe_score": 23.739601135253906, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and applications, for instance. There are leaks of coverage", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.21219798922538757, "xcomet_qe_score": 0.3089778423309326, "metricx_score": 20.426475524902344, "metricx_qe_score": 16.151912689208984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on certain natural language. Chinese is missing. Lake's coverage on certain mini-re", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3507046699523926, "xcomet_qe_score": 0.4833436608314514, "metricx_score": 20.954191207885742, "metricx_qe_score": 19.763864517211914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "presentations. The lambda calculus is missing. Or they are evaluated only on certain ne", "metrics": {"bleu_score": 0.0, "chrf_score": 7.140345166025992, "xcomet_score": 0.6291506290435791, "xcomet_qe_score": 0.6437021493911743, "metricx_score": 15.686163902282715, "metricx_qe_score": 10.189226150512695, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "wer models. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1718330979347229, "xcomet_qe_score": 0.1567659229040146, "metricx_score": 20.684120178222656, "metricx_qe_score": 23.06239891052246, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", there is only one single model to evalu", "metrics": {"bleu_score": 0.0, "chrf_score": 2.6241625263474857, "xcomet_score": 0.19993183016777039, "xcomet_qe_score": 0.15064242482185364, "metricx_score": 16.664594650268555, "metricx_qe_score": 18.40963363647461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ate them. To this end, we propose an example, we provide a uniform dataset example for cross-lingual semantic parsing", "metrics": {"bleu_score": 0.0, "chrf_score": 0.9282979814468257, "xcomet_score": 0.24824245274066925, "xcomet_qe_score": 0.18839459121227264, "metricx_score": 14.948432922363281, "metricx_qe_score": 16.03750991821289, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in multiple natural languages and meaning representations. It contains 9 data sets in various domains, 5 semantic parts and taxes, 8 meaning representations, and 22 natural languages in 15 language", "metrics": {"bleu_score": 1.2491398726230503, "chrf_score": 2.1621794809596815, "xcomet_score": 0.4134522080421448, "xcomet_qe_score": 0.5309147834777832, "metricx_score": 13.021458625793457, "metricx_qe_score": 7.945566177368164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "families. And to better evaluate our benchmark, we consider the six settings for training and evaluation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8037113547325134, "xcomet_qe_score": 0.7967751026153564, "metricx_score": 19.4696044921875, "metricx_qe_score": 21.720460891723633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The first one is TranslateTest.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9513983726501465, "xcomet_qe_score": 0.9906973838806152, "metricx_score": 4.075401782989502, "metricx_qe_score": 4.156914710998535, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We use Google Translate API to translate source to the target language, then use MonolingoModel to train an evalu", "metrics": {"bleu_score": 0.8183299404498396, "chrf_score": 2.019369406418914, "xcomet_score": 0.8875734806060791, "xcomet_qe_score": 0.92107093334198, "metricx_score": 16.405969619750977, "metricx_qe_score": 6.315164566040039, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ation. For example, we train an English model on an English query and during inference, we translate the German query using API to English and then use the trained model to predict the SQL.", "metrics": {"bleu_score": 0.6989985872926873, "chrf_score": 2.5212671656316603, "xcomet_score": 0.7847166657447815, "xcomet_qe_score": 0.8437689542770386, "metricx_score": 8.627107620239258, "metricx_qe_score": 8.361331939697266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And we will also test monolingual", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8329414129257202, "xcomet_qe_score": 0.8953794836997986, "metricx_score": 4.452028751373291, "metricx_qe_score": 3.275463342666626, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "module. In this setting, the source language is the same as the target language, for example German to German or English to English", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9018042683601379, "xcomet_qe_score": 0.920365035533905, "metricx_score": 8.97877311706543, "metricx_qe_score": 12.33033275604248, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We also test monolingual fusion setting by training monolingual models with only 10% of training data. And we test a", "metrics": {"bleu_score": 2.041404152241261, "chrf_score": 2.1313300536297475, "xcomet_score": 0.7062045931816101, "xcomet_qe_score": 0.7318354845046997, "metricx_score": 19.94733428955078, "metricx_qe_score": 18.495859146118164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "multilingual model, which we train one multilingual model for", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6199482679367065, "xcomet_qe_score": 0.7333464622497559, "metricx_score": 21.753456115722656, "metricx_qe_score": 17.994449615478516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "all languages. For example, we put the German, English, and Chinese queries together to train a multilingu", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6932241916656494, "xcomet_qe_score": 0.7122416496276855, "metricx_score": 10.677936553955078, "metricx_qe_score": 9.994162559509277, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "al model and during inference we can use this model to. to translate German queries or Chinese queries or etc", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6246698498725891, "xcomet_qe_score": 0.5682172179222107, "metricx_score": 14.664562225341797, "metricx_qe_score": 15.219491958618164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We also consider cross-lingual zero shot and field shot transfer, which", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6233672499656677, "xcomet_qe_score": 0.7494825720787048, "metricx_score": 14.090581893920898, "metricx_qe_score": 11.77303695678711, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "train on one source language and transfer to another language. So during the training, I", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7855441570281982, "xcomet_qe_score": 0.8903741240501404, "metricx_score": 20.289793014526367, "metricx_qe_score": 21.505067825317383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "will train it on English Query or the combination of English and German Fusion Queries to train a multilingual model and predict the SQL output. And we also find many interesting results.", "metrics": {"bleu_score": 0.787629249848122, "chrf_score": 1.467934635257467, "xcomet_score": 0.6823148727416992, "xcomet_qe_score": 0.7678875923156738, "metricx_score": 12.609088897705078, "metricx_qe_score": 10.493209838867188, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So regarding analysis of monolingual models,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17673829197883606, "xcomet_qe_score": 0.21402418613433838, "metricx_score": 9.628389358520508, "metricx_qe_score": 17.713590621948242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we evaluate on two groups of models. including encoder PDR, which stands for multilingual pre-trained encoders with pointer-based decoders such as XLMR plus PDR and BERT plus PDR.", "metrics": {"bleu_score": 0.0, "chrf_score": 8.826986764741129, "xcomet_score": 0.8154197931289673, "xcomet_qe_score": 0.851649820804596, "metricx_score": 10.150611877441406, "metricx_qe_score": 8.66596508026123, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And we also evaluate encoder decoder models, which is multilingual pretrained encoder decoder models, such as MBART and MT5. We found", "metrics": {"bleu_score": 0.0, "chrf_score": 22.839528635101093, "xcomet_score": 0.770251452922821, "xcomet_qe_score": 0.8805972337722778, "metricx_score": 12.449056625366211, "metricx_qe_score": 9.035309791564941, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "that encoder decoder obtains the best performance on all nine datasets. And we evalu", "metrics": {"bleu_score": 0.0, "chrf_score": 16.32591109584641, "xcomet_score": 0.7528963088989258, "xcomet_qe_score": 0.8050409555435181, "metricx_score": 19.31587028503418, "metricx_qe_score": 21.685867309570312, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ate on MT five and XLMR plus PDR multilingual settings. We found that encoder", "metrics": {"bleu_score": 0.0, "chrf_score": 5.264775323350013, "xcomet_score": 0.47373488545417786, "xcomet_qe_score": 0.5049058198928833, "metricx_score": 21.221349716186523, "metricx_qe_score": 18.854318618774414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "decoder or encoder PDR can be improved by training in a mixture of various languages. And we found that", "metrics": {"bleu_score": 0.0, "chrf_score": 15.589712076964116, "xcomet_score": 0.8551299571990967, "xcomet_qe_score": 0.8123852014541626, "metricx_score": 17.733665466308594, "metricx_qe_score": 16.16993522644043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven data sets and only gains in three data sets. I think this is known as the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6958207488059998, "xcomet_qe_score": 0.855369508266449, "metricx_score": 24.425630569458008, "metricx_qe_score": 22.922494888305664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "curse of multilinguality. We also compare the cross-l", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.34206801652908325, "xcomet_qe_score": 0.4637080430984497, "metricx_score": 20.03481101989746, "metricx_qe_score": 19.55545425415039, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "anguage performance gap. In this figure, the blue line is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24446240067481995, "xcomet_qe_score": 0.5943873524665833, "metricx_score": 17.902408599853516, "metricx_qe_score": 15.947845458984375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cross-lingual fuchsia transfer, the orange line is cross-ling", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.44154539704322815, "xcomet_qe_score": 0.635504961013794, "metricx_score": 21.50756072998047, "metricx_qe_score": 18.179271697998047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ual zero-shot transfer, while the green line is the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4017471373081207, "xcomet_qe_score": 0.3627464473247528, "metricx_score": 22.86457061767578, "metricx_qe_score": 16.899328231811523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "monolingual setting. We found that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.511995792388916, "xcomet_qe_score": 0.4012872874736786, "metricx_score": 13.949639320373535, "metricx_qe_score": 11.754159927368164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "by comparing the green and orange line, we found that for zero short setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that for few short settings, the transfer gap is shortened rapid", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.558864951133728, "xcomet_qe_score": 0.5689241290092468, "metricx_score": 15.890165328979492, "metricx_qe_score": 12.487372398376465, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ly. We also find some other interesting find", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7952584028244019, "xcomet_qe_score": 0.7703151702880859, "metricx_score": 10.85506820678711, "metricx_qe_score": 16.509103775024414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ings. For example, decoder overcomes progress work or achieves comparable results. Purchasing on", "metrics": {"bleu_score": 0.0, "chrf_score": 10.529515750992886, "xcomet_score": 0.3219590187072754, "xcomet_qe_score": 0.41973692178726196, "metricx_score": 21.467506408691406, "metricx_qe_score": 19.330602645874023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "English natural language can significantly boost the performance of fuchshot on target natural languages. We also found that multilingual language models, such as CODIS and BLUEM, are still inadequate for cross-langu", "metrics": {"bleu_score": 0.0, "chrf_score": 2.0665758467842252, "xcomet_score": 0.4975898861885071, "xcomet_qe_score": 0.560693085193634, "metricx_score": 18.133337020874023, "metricx_qe_score": 15.469664573669434, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "age semantic parsing tasks. In summary, we built Exempler, a unified benchmark for cross-angle thematic parsing with multiple natural languages and mini-re", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7614678369114445, "xcomet_score": 0.2979815602302551, "xcomet_qe_score": 0.43541932106018066, "metricx_score": 16.962940216064453, "metricx_qe_score": 15.409984588623047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "presentations. We conducted a comprehensive benchmark study on three representative types of multil", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5660502910614014, "xcomet_qe_score": 0.5930079221725464, "metricx_score": 20.206661224365234, "metricx_qe_score": 20.009368896484375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ingual language models. And our results show many interesting", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6394470930099487, "xcomet_qe_score": 0.7434921264648438, "metricx_score": 23.209636688232422, "metricx_qe_score": 24.130033493041992, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "findings and so", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4008423388004303, "xcomet_qe_score": 0.1518210619688034, "metricx_score": 6.755701065063477, "metricx_qe_score": 11.31119441986084, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on. And welcome to visit our paper and code", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8093593716621399, "xcomet_qe_score": 0.8625649809837341, "metricx_score": 4.953036785125732, "metricx_qe_score": 6.918908596038818, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.800614595413208, "xcomet_qe_score": 0.44242411851882935, "metricx_score": 1.1037641763687134, "metricx_qe_score": 4.742196559906006, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone, my name is Aid Vilar and I will give you a short overview of the paper Promoting PowerPoint Translation, Assessing Strategies and Performance", "metrics": {"bleu_score": 0.9944159660057356, "chrf_score": 8.52550148696101, "xcomet_score": 0.6044125556945801, "xcomet_qe_score": 0.6990718841552734, "metricx_score": 16.662174224853516, "metricx_qe_score": 14.441848754882812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". This is joint work with my colleagues from Google Translate. PRAM is a 540", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.29061415791511536, "xcomet_qe_score": 0.278858482837677, "metricx_score": 20.59026527404785, "metricx_qe_score": 21.9612979888916, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "billion parameters learning language model presented last year in 2022. It's trained on a large collection", "metrics": {"bleu_score": 1.515063435970505, "chrf_score": 4.1987904197038715, "xcomet_score": 0.5864952206611633, "xcomet_qe_score": 0.8186308741569519, "metricx_score": 18.359731674194336, "metricx_qe_score": 16.334266662597656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of tags comprising 780 billion tokens. At the time of publication, it achieved state of", "metrics": {"bleu_score": 0.0, "chrf_score": 3.0200223820873697, "xcomet_score": 0.3173551559448242, "xcomet_qe_score": 0.36878645420074463, "metricx_score": 18.77366065979004, "metricx_qe_score": 14.58637523651123, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the art in hundreds of NLP tasks. In this work, we present the first system", "metrics": {"bleu_score": 1.6068581927959764, "chrf_score": 3.1733093320773147, "xcomet_score": 0.3299213945865631, "xcomet_qe_score": 0.676931619644165, "metricx_score": 19.5006160736084, "metricx_qe_score": 14.86312484741211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "atic study of Latch Language Model Prompting for Machine Translation. We evaluate the transition capability of such models", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.398240327835083, "xcomet_qe_score": 0.5579505562782288, "metricx_score": 19.66861915588379, "metricx_qe_score": 17.779808044433594, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "using the best practices of the AMT community. This involves using the latest test sets to avoid", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17338107526302338, "xcomet_qe_score": 0.16490203142166138, "metricx_score": 20.781875610351562, "metricx_qe_score": 16.352548599243164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "overlap of the test data with the training data of the language model. And we compare two state of the art systems, the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3014109134674072, "xcomet_qe_score": 0.4995853900909424, "metricx_score": 20.859464645385742, "metricx_qe_score": 22.832698822021484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "best performing systems of the WMT evaluation. We use the state of the art and new LMT metrics and", "metrics": {"bleu_score": 1.05075644277416, "chrf_score": 2.351923085058716, "xcomet_score": 0.5370043516159058, "xcomet_qe_score": 0.6771893501281738, "metricx_score": 15.680859565734863, "metricx_qe_score": 12.22701644897461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "also show expert-based human evaluation results. Finally, we provide some recommendations for promp selection strategi", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.222337543964386, "xcomet_qe_score": 0.7029251456260681, "metricx_score": 15.43602466583252, "metricx_qe_score": 12.393471717834473, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "es. The prompting has a big influence on the performance of the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18046733736991882, "xcomet_qe_score": 0.19977563619613647, "metricx_score": 16.052396774291992, "metricx_qe_score": 13.68095874786377, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "LLMs for translation, as we can see in a simple experiment where we use one short prompting and provide two different prompts for just one sentence. The majority of sentences five hundred six", "metrics": {"bleu_score": 0.0, "chrf_score": 1.2638218078950092, "xcomet_score": 0.3155815601348877, "xcomet_qe_score": 0.44958001375198364, "metricx_score": 24.704526901245117, "metricx_qe_score": 20.77640151977539, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "teen out of one thousand, the difference observed", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.30188167095184326, "xcomet_qe_score": 0.197731614112854, "metricx_score": 22.785781860351562, "metricx_qe_score": 21.78802490234375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is of more than one blurred point. And this can go in extreme", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.30165234208106995, "xcomet_qe_score": 0.7569692730903625, "metricx_score": 21.2537784576416, "metricx_qe_score": 16.581722259521484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cases up to 40 blur points. So it'", "metrics": {"bleu_score": 1.7539413943549187, "chrf_score": 2.0612300884427985, "xcomet_score": 0.25073543190956116, "xcomet_qe_score": 0.3258468508720398, "metricx_score": 21.491735458374023, "metricx_qe_score": 17.129465103149414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "s important to select a good prompting strategy. In", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6405776739120483, "xcomet_qe_score": 0.6541374921798706, "metricx_score": 23.521072387695312, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "our experiments, we decided on a five shot prompting strategy, where we just mark each sentence that we provide to the system with the language in which it is. So in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7050930261611938, "xcomet_qe_score": 0.8140841722488403, "metricx_score": 13.59153938293457, "metricx_qe_score": 12.576831817626953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this example here, where we performed translation from German to English, the German sentences, the source sentences are marked with German colon and the English translation with English colon. We saw that the actual form of the prompt", "metrics": {"bleu_score": 0.9219796704544546, "chrf_score": 9.502059388283667, "xcomet_score": 0.747645914554596, "xcomet_qe_score": 0.8299871683120728, "metricx_score": 11.035127639770508, "metricx_qe_score": 17.080612182617188, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ing does not have a big influence in the case of several short promptings. It's crucial", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.552802324295044, "xcomet_qe_score": 0.5292230248451233, "metricx_score": 24.332368850708008, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for zero and one shot prompting, but when we go, as in our case, to five", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1551836133003235, "xcomet_qe_score": 0.15311264991760254, "metricx_score": 20.225872039794922, "metricx_qe_score": 17.458818435668945, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "shot prompting, there is nearly no difference to the actual form of the prompt", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.61488938331604, "xcomet_qe_score": 0.7520068287849426, "metricx_score": 24.696584701538086, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ing. It's the examples that carry most of the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5752202272415161, "xcomet_qe_score": 0.5418110489845276, "metricx_score": 24.047754287719727, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "weight. In summary of our experimental results is that the example quality is more important than the similarity to the source", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.623966634273529, "xcomet_qe_score": 0.7274109125137329, "metricx_score": 23.848390579223633, "metricx_qe_score": 24.043384552001953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sentence. Therefore, it is important to select examples from high quality trans", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7956749200820923, "xcomet_qe_score": 0.8182210922241211, "metricx_score": 10.460248947143555, "metricx_qe_score": 9.530572891235352, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "lations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or", "metrics": {"bleu_score": 0.9689464895540371, "chrf_score": 2.2163640644679727, "xcomet_score": 0.546869158744812, "xcomet_qe_score": 0.6106395721435547, "metricx_score": 19.930147171020508, "metricx_qe_score": 19.203298568725586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the dev data. The depth data is much more accurate and with higher quality than the training data, that it's more", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5004873275756836, "xcomet_qe_score": 0.5099589824676514, "metricx_score": 16.999120712280273, "metricx_qe_score": 12.3698148727417, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", I say, and the results show better performance when", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3023165762424469, "xcomet_qe_score": 0.5075995922088623, "metricx_score": 19.9671688079834, "metricx_qe_score": 14.77987289428711, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "using the depth data. Nevertheless, specialized state of the art systems have a substantial advantage over the PALM transla", "metrics": {"bleu_score": 0.0, "chrf_score": 1.76569691740106, "xcomet_score": 0.6341787576675415, "xcomet_qe_score": 0.7185219526290894, "metricx_score": 16.08388328552246, "metricx_qe_score": 17.681983947753906, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tions, but PALM comes pretty close to a commercial system", "metrics": {"bleu_score": 0.0, "chrf_score": 3.5025955378838076, "xcomet_score": 0.8537412285804749, "xcomet_qe_score": 0.8029671311378479, "metricx_score": 17.85016632080078, "metricx_qe_score": 23.109573364257812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". In our case, we chose to overlay with Google Translate.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8862848281860352, "xcomet_qe_score": 0.9050917625427246, "metricx_score": 14.876380920410156, "metricx_qe_score": 13.864906311035156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The insights we gained from the human innovation that we performed using the MQM framework are that the fluency of PALM is comparable to the state of the art systems, but the main difference comes from the accura", "metrics": {"bleu_score": 0.6934063716458092, "chrf_score": 2.3107684719977533, "xcomet_score": 0.7548896074295044, "xcomet_qe_score": 0.7763328552246094, "metricx_score": 11.306696891784668, "metricx_qe_score": 7.633261680603027, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cy. In particular, the most common error are omission errors", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7246376811594203, "xcomet_score": 0.7857241630554199, "xcomet_qe_score": 0.9585458040237427, "metricx_score": 20.56358528137207, "metricx_qe_score": 21.868391036987305, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". It seems that Palm chooses to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in translation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.8242362681084956, "xcomet_score": 0.9157333374023438, "xcomet_qe_score": 0.9089641571044922, "metricx_score": 22.45418930053711, "metricx_qe_score": 23.716949462890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "However, the style outward category for PAN is lower than for the state of the art systems, which is an additional signal. That PARM provides really fluent output, but still with some problems of accuracy", "metrics": {"bleu_score": 0.0, "chrf_score": 0.9600614439324118, "xcomet_score": 0.6443527340888977, "xcomet_qe_score": 0.6801844239234924, "metricx_score": 16.960567474365234, "metricx_qe_score": 14.461562156677246, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". And that's it for this really short overview", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9878650903701782, "xcomet_qe_score": 1.0, "metricx_score": 11.077631950378418, "metricx_qe_score": 19.120107650756836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". For more details, please come to the full presentation of the paper. Thank you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8571947813034058, "xcomet_qe_score": 0.9227316379547119, "metricx_score": 8.258307456970215, "metricx_qe_score": 16.416574478149414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "very much.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8842521905899048, "xcomet_qe_score": 0.8700260519981384, "metricx_score": 4.100488662719727, "metricx_qe_score": 7.549065113067627, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, I am Dawe, a PhD student at the Zaland University in Germany.", "metrics": {"bleu_score": 0.0, "chrf_score": 5.755076359826561, "xcomet_score": 0.7553939819335938, "xcomet_qe_score": 0.7454327344894409, "metricx_score": 10.907472610473633, "metricx_qe_score": 15.369359970092773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In this video, I would like to present our recent work Weaker Than You Think, a critical look at weekly supported learning. This is joint work with", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.698996901512146, "xcomet_qe_score": 0.7994077801704407, "metricx_score": 17.84103012084961, "metricx_qe_score": 11.619205474853516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Xiao Yushche, Marius Musbach, and Gas Steffen and Dietrich Clarkov. I would like to begin with a brief", "metrics": {"bleu_score": 1.963506200603642, "chrf_score": 30.22286558506984, "xcomet_score": 0.21481652557849884, "xcomet_qe_score": 0.1357855349779129, "metricx_score": 20.398006439208984, "metricx_qe_score": 18.1757869720459, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "introduction to weekly supervision and weekly supervised events. In weak super", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.38348305225372314, "xcomet_qe_score": 0.24612271785736084, "metricx_score": 24.476070404052734, "metricx_qe_score": 23.00255584716797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "vision, we do not manually label the data.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7872510552406311, "xcomet_qe_score": 0.7522220015525818, "metricx_score": 14.073471069335938, "metricx_qe_score": 11.91689395904541, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or locality code sourcing, as illustrated in the figure", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6801421642303467, "xcomet_qe_score": 0.7716937065124512, "metricx_score": 18.898452758789062, "metricx_qe_score": 19.71692657470703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5288110375404358, "xcomet_qe_score": 0.508794367313385, "metricx_score": 22.858646392822266, "metricx_qe_score": 24.0445556640625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a certain amount of the annotations are incorrect. If we directly train neural networks on weekly label data, the neural networks tend to memorize the label noise and", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.46102824807167053, "xcomet_qe_score": 0.502170741558075, "metricx_score": 25.0, "metricx_qe_score": 23.546037673950195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "do not generalize. In weekly supervised learning, training algorithms are proposed to robustly train neural networks under such level noise so that the trained models still generalize well. In recent works in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2918642461299896, "xcomet_qe_score": 0.4379792809486389, "metricx_score": 23.180343627929688, "metricx_qe_score": 21.883991241455078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "WSL, WSL stands for Weekly Supervised Learning. A common claim is that people say that they only train models on weekly label data and achieve high performance on clean test sets. Technically, this claim", "metrics": {"bleu_score": 0.7176374596405378, "chrf_score": 2.4592646015910202, "xcomet_score": 0.3811877965927124, "xcomet_qe_score": 0.4917832911014557, "metricx_score": 18.20229148864746, "metricx_qe_score": 13.978742599487305, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is not wrong, but there's a catch. Which is that people do assume that there is an additional clean validation set available for model selection. We cast doubt on", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4661366641521454, "xcomet_qe_score": 0.5690709352493286, "metricx_score": 20.862672805786133, "metricx_qe_score": 22.890596389770508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this problem setting, as this implies that additional manual annotations are required in weekly supervised learning", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.736709475517273, "xcomet_qe_score": 0.7815849184989929, "metricx_score": 20.74009132385254, "metricx_qe_score": 15.672231674194336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", but like an elephant in the room, this necessity is often overlooked.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9781447649002075, "xcomet_qe_score": 0.9561368227005005, "metricx_score": 18.330720901489258, "metricx_qe_score": 24.191059112548828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The aforementioned doubt leads us to ask three", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8158099055290222, "xcomet_qe_score": 0.8992916345596313, "metricx_score": 16.353055953979492, "metricx_qe_score": 7.65200138092041, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead?", "metrics": {"bleu_score": 1.127314039940196, "chrf_score": 2.1222989463803885, "xcomet_score": 0.8399436473846436, "xcomet_qe_score": 0.8628663420677185, "metricx_score": 7.421727180480957, "metricx_qe_score": 6.069599151611328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Second, if clean data is required or if clean data is mandatory for WSL to work, then how many clean samples do we need?", "metrics": {"bleu_score": 0.7125301753775903, "chrf_score": 1.7754899150134853, "xcomet_score": 0.9942348003387451, "xcomet_qe_score": 1.0, "metricx_score": 24.971364974975586, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Finally, should we only use the clean samples for validation or there are better ways to utilize them?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9923609495162964, "xcomet_qe_score": 0.9952307939529419, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We addressed these research questions in our work and our findings are as follows", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9895751476287842, "xcomet_qe_score": 1.0, "metricx_score": 24.978702545166016, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". First, we find that interestingly, recent WSL methods indeed require clean white dash samples to work properly. Otherwise, there is", "metrics": {"bleu_score": 0.9700407811190135, "chrf_score": 1.911304704727006, "xcomet_score": 0.6235882043838501, "xcomet_qe_score": 0.6519582271575928, "metricx_score": 17.774612426757812, "metricx_qe_score": 16.977262496948242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a large performance drop, as shown in this figure.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7966954708099365, "xcomet_qe_score": 0.8820385932922363, "metricx_score": 8.504770278930664, "metricx_qe_score": 10.514997482299805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels. meaning that the training is pointless. This indicates that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8102670907974243, "xcomet_qe_score": 0.8385998010635376, "metricx_score": 20.386655807495117, "metricx_qe_score": 20.367151260375977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not", "metrics": {"bleu_score": 0.6071744293085544, "chrf_score": 1.620148493133574, "xcomet_score": 0.7558051347732544, "xcomet_qe_score": 0.8205728530883789, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typ", "metrics": {"bleu_score": 0.8602841813912875, "chrf_score": 1.5169385063685425, "xcomet_score": 0.6346914172172546, "xcomet_qe_score": 0.656549334526062, "metricx_score": 23.281173706054688, "metricx_qe_score": 21.549135208129883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ically, we only need twenty samples per class to attain high performance. But that'", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7445399761199951, "xcomet_qe_score": 0.8071149587631226, "metricx_score": 20.12269401550293, "metricx_qe_score": 21.018539428710938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "s not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The red figure shows the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6067637801170349, "xcomet_qe_score": 0.676381528377533, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "performance difference between fine tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only.", "metrics": {"bleu_score": 0.7940033158673616, "chrf_score": 1.5693065991177404, "xcomet_score": 0.7922083139419556, "xcomet_qe_score": 0.8091334700584412, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "As we can see, if we have ten samples per class, direct fine tuning starts to beat WSL approaches.", "metrics": {"bleu_score": 0.8734513882715711, "chrf_score": 2.1132496145509263, "xcomet_score": 0.9804815053939819, "xcomet_qe_score": 0.9929187297821045, "metricx_score": 23.111112594604492, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As", "metrics": {"bleu_score": 0.7806992375123509, "chrf_score": 1.5842291882811352, "xcomet_score": 0.853413999080658, "xcomet_qe_score": 0.8689886927604675, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we can see from the figures, the Berliner model termed FTW initially underperforms more complicated WSL methods like cosine. However, if we allow to continue fine tun", "metrics": {"bleu_score": 1.0164495971872436, "chrf_score": 4.873717441287657, "xcomet_score": 0.47211918234825134, "xcomet_qe_score": 0.6228789687156677, "metricx_score": 15.163091659545898, "metricx_qe_score": 15.94985294342041, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ing on the clean samples, then FTW performs equally well as other methods. So", "metrics": {"bleu_score": 0.0, "chrf_score": 1.6124421793503256, "xcomet_score": 0.6700080633163452, "xcomet_qe_score": 0.6824612617492676, "metricx_score": 24.64543914794922, "metricx_qe_score": 21.54798126220703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in practice there is no reason to choose more complex WSL methods which require more computation time and disk space. In summary", "metrics": {"bleu_score": 0.9356948961473903, "chrf_score": 1.9561729196539535, "xcomet_score": 0.8860191106796265, "xcomet_qe_score": 0.92592453956604, "metricx_score": 21.36674690246582, "metricx_qe_score": 22.882579803466797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we showed that recent WSL approaches require clean, manually annotated samples for them to work properly.", "metrics": {"bleu_score": 0.9192680991983538, "chrf_score": 2.158925122447423, "xcomet_score": 0.9364889860153198, "xcomet_qe_score": 0.9630568027496338, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Their performance gain and practicality are heavily overestimated.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9889630079269409, "xcomet_qe_score": 1.0, "metricx_score": 22.84650230407715, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Our concrete recommendations for future work are as follows", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9921379089355469, "xcomet_qe_score": 1.0, "metricx_score": 23.484813690185547, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". First, report the model selection criteria", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9889378547668457, "xcomet_qe_score": 0.9898408651351929, "metricx_score": 7.322994232177734, "metricx_qe_score": 19.308609008789062, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". For example, report if the model selection is done well clean validation samples", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9241833686828613, "xcomet_qe_score": 0.9610128998756409, "metricx_score": 12.42276668548584, "metricx_qe_score": 15.420802116394043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Second, WSL approaches should be compared with future landing baselines as both work on grid samples. Third", "metrics": {"bleu_score": 0.9043944515757394, "chrf_score": 2.1132840524067142, "xcomet_score": 0.30924081802368164, "xcomet_qe_score": 0.6013259887695312, "metricx_score": 19.159090042114258, "metricx_qe_score": 18.00667953491211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", continuous fine tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally", "metrics": {"bleu_score": 1.02557607759526, "chrf_score": 2.1683032970156693, "xcomet_score": 0.9243398904800415, "xcomet_qe_score": 0.93266761302948, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we have open source our code. You can", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.637358546257019, "xcomet_qe_score": 0.7862221598625183, "metricx_score": 12.455451011657715, "metricx_qe_score": 13.989165306091309, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "find it via the QR code on this slide. Please feel free to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8124747276306152, "xcomet_qe_score": 0.8196566104888916, "metricx_score": 8.554856300354004, "metricx_qe_score": 9.194662094116211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "check it out. Thank", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.807043194770813, "xcomet_qe_score": 0.7974200248718262, "metricx_score": 3.253066062927246, "metricx_qe_score": 5.190725803375244, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "you and enjoy the conference.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7712724208831787, "xcomet_qe_score": 0.8902578949928284, "metricx_score": 12.233039855957031, "metricx_qe_score": 14.623477935791016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, I'm James Finch.", "metrics": {"bleu_score": 9.846052248031867, "chrf_score": 49.01399172751344, "xcomet_score": 0.929672360420227, "xcomet_qe_score": 1.0, "metricx_score": 2.610765218734741, "metricx_qe_score": 7.360261917114258, "linguapy_score": [1, "ITALIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And I'm Sarah Finch.", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 65.59829215876142, "xcomet_score": 0.9597548246383667, "xcomet_qe_score": 1.0, "metricx_score": 3.2870867252349854, "metricx_qe_score": 7.869505405426025, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And today we'll tell you all about ABCEval, a new dimensional approach to evaluating conversational AI", "metrics": {"bleu_score": 1.0036994301921276, "chrf_score": 6.625990128141275, "xcomet_score": 0.921446681022644, "xcomet_qe_score": 1.0, "metricx_score": 22.89479637145996, "metricx_qe_score": 24.962976455688477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". This work was done by the Emory NLP Lab, led by Professor Gino Choi at Emory University, and in collaboration with Amazon Alexa AI. So, let'", "metrics": {"bleu_score": 4.458561473606781, "chrf_score": 22.746732820832545, "xcomet_score": 0.5899898409843445, "xcomet_qe_score": 0.6842869520187378, "metricx_score": 15.45927619934082, "metricx_qe_score": 19.24112892150879, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "s say that you just developed a dialogue model and you want to see how well it compares against the current state of the art. The common practice", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6854873895645142, "xcomet_qe_score": 0.7058688402175903, "metricx_score": 23.128381729125977, "metricx_qe_score": 23.58507537841797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is to use human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a liquid scale. These methods work well to provide", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3425474166870117, "xcomet_qe_score": 0.5231430530548096, "metricx_score": 21.817607879638672, "metricx_qe_score": 22.14200210571289, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you may want to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6474056243896484, "xcomet_qe_score": 0.7741566896438599, "metricx_score": 20.721540451049805, "metricx_qe_score": 22.690271377563477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level. One", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6114771962165833, "xcomet_qe_score": 0.7884095907211304, "metricx_score": 23.66753387451172, "metricx_qe_score": 22.8797550201416, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "approach is simply to ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Lickert scale methods. However", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.705040693283081, "xcomet_qe_score": 0.7916872501373291, "metricx_score": 21.684627532958984, "metricx_qe_score": 24.46062469482422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we believe there is a more precise and reliable strategy for dimen", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7490281462669373, "xcomet_qe_score": 0.7640775442123413, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annoteating whether or not each model response expresses certain behaviors, such as", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.48972851037979126, "xcomet_qe_score": 0.42764681577682495, "metricx_score": 22.93903923034668, "metricx_qe_score": 22.868038177490234, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "responding with irrelevant information or contradicting itself. We call this approach annot", "metrics": {"bleu_score": 0.0, "chrf_score": 2.193421823540951, "xcomet_score": 0.3604133725166321, "xcomet_qe_score": 0.29909971356391907, "metricx_score": 20.885419845581055, "metricx_qe_score": 17.62468910217285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ating behaviors in chat, or ABC eval in short. We have developed this method to comprehensively cover chat model behaviors that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.41068074107170105, "xcomet_qe_score": 0.45628637075424194, "metricx_score": 20.68021011352539, "metricx_qe_score": 19.055593490600586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "have been suggested to affect chat quality in recent literature. ABCeval is capable of measuring the", "metrics": {"bleu_score": 0.0, "chrf_score": 5.165895296449378, "xcomet_score": 0.4965077042579651, "xcomet_qe_score": 0.5798308253288269, "metricx_score": 20.91387367248535, "metricx_qe_score": 17.12669563293457, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "rates at which chat models will commit various thematic errors. For example, APCEval measures the number of turns in which a chat model ignores its partner or says something irrelevant. contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge and when the model", "metrics": {"bleu_score": 0.0, "chrf_score": 1.6644918968151265, "xcomet_score": 0.4070502817630768, "xcomet_qe_score": 0.5756956934928894, "metricx_score": 21.695852279663086, "metricx_qe_score": 19.10727882385254, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on one hundred human-", "metrics": {"bleu_score": 0.0, "chrf_score": 1.4915051660769059, "xcomet_score": 0.5037550330162048, "xcomet_qe_score": 0.5118900537490845, "metricx_score": 17.59012222290039, "metricx_qe_score": 21.393085479736328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "bot conversations per model using ABCEval. For comparison, we also evaluated these conversations using three existing methods: liquid ratings on the turn level, liquid ratings on the dialog level and dialog level pairwise comparisons", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.31616801023483276, "xcomet_qe_score": 0.3688652813434601, "metricx_score": 23.17289161682129, "metricx_qe_score": 20.7228946685791, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9559190273284912, "xcomet_qe_score": 0.9595725536346436, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "From our analysis of these evaluation results, we found that ABC eval behavior labels are generally more reliable than labels collected by existing methods, as measured by inter-annotator agreement on one hundred doubly labeled convers", "metrics": {"bleu_score": 0.0, "chrf_score": 2.3867977062885877, "xcomet_score": 0.8744072914123535, "xcomet_qe_score": 0.8853610754013062, "metricx_score": 21.234649658203125, "metricx_qe_score": 22.066268920898438, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ations. In addition, ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring", "metrics": {"bleu_score": 0.0, "chrf_score": 2.4118319532290142, "xcomet_score": 0.560238778591156, "xcomet_qe_score": 0.5690422058105469, "metricx_score": 16.740419387817383, "metricx_qe_score": 19.146364212036133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively, while the average liquor consistency scores explain only four percent or less. Finally, we checked whether", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2573016285896301, "xcomet_qe_score": 0.35344329476356506, "metricx_score": 23.610000610351562, "metricx_qe_score": 23.83448600769043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "each evaluation metric captured a unique aspect of chat quality using a stepwise linear regression. You can see how the combination", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6126235127449036, "xcomet_qe_score": 0.758151650428772, "metricx_score": 15.373380661010742, "metricx_qe_score": 14.677879333496094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of all ABC evaluation metrics explains over twenty five percent of conversation quality. And as you remove the metrics one by one, most of them result in the loss of a decent", "metrics": {"bleu_score": 0.0, "chrf_score": 2.527630801455806, "xcomet_score": 0.5412164926528931, "xcomet_qe_score": 0.5999401211738586, "metricx_score": 16.245311737060547, "metricx_qe_score": 12.606317520141602, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "amount of information about quality. On the other hand, the combination of all turn-level liquid metrics explains far less of the quality, and fewer of these metrics carry unique information. These", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.44595929980278015, "xcomet_qe_score": 0.525048017501831, "metricx_score": 23.831071853637695, "metricx_qe_score": 22.277019500732422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "reliable, informative and distinct ABC evaluation metrics allow us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see in the results of", "metrics": {"bleu_score": 0.8813223403542749, "chrf_score": 3.501846489693196, "xcomet_score": 0.5939843654632568, "xcomet_qe_score": 0.8407129049301147, "metricx_score": 16.774978637695312, "metricx_qe_score": 14.500133514404297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "our experiment that several challenges still remain and have been precisely quantified. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7958483695983887, "xcomet_qe_score": 0.8243623971939087, "metricx_score": 21.181076049804688, "metricx_qe_score": 21.105579376220703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the bots we tested have common sense violations in around twenty percent of their responses. They", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8115657567977905, "xcomet_qe_score": 0.805307149887085, "metricx_score": 24.64106559753418, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "produce irrelevant information in about 15% of the responses and contradict themselves or their partner about 10% of the time. With the rapid pace of improvement in the field,", "metrics": {"bleu_score": 1.9561234605987552, "chrf_score": 2.876718613266848, "xcomet_score": 0.39654994010925293, "xcomet_qe_score": 0.7575300931930542, "metricx_score": 15.421462059020996, "metricx_qe_score": 19.501087188720703, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more a reason", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.41821351647377014, "xcomet_qe_score": 0.6686182022094727, "metricx_score": 24.882343292236328, "metricx_qe_score": 23.48427391052246, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to pursue reliable and precise evaluation metrics for comparing models. We hope that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6450359225273132, "xcomet_qe_score": 0.7124216556549072, "metricx_score": 20.028362274169922, "metricx_qe_score": 21.024566650390625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing", "metrics": {"bleu_score": 0.0, "chrf_score": 4.983773471805744, "xcomet_score": 0.9321444034576416, "xcomet_qe_score": 0.932673454284668, "metricx_score": 13.858771324157715, "metricx_qe_score": 17.841094970703125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "how conversational AI will advance in the coming months and years. Thank", "metrics": {"bleu_score": 1.3253962558322638, "chrf_score": 1.569433754287941, "xcomet_score": 0.6175782084465027, "xcomet_qe_score": 0.8042076826095581, "metricx_score": 23.899450302124023, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "you for watching.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8361060619354248, "xcomet_qe_score": 0.9102206230163574, "metricx_score": 7.3366498947143555, "metricx_qe_score": 12.196235656738281, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my name is Kyo Yin and I will present our work titled When does translation require a data-driven multil", "metrics": {"bleu_score": 1.0755944970735343, "chrf_score": 6.056160873931307, "xcomet_score": 0.7264832258224487, "xcomet_qe_score": 0.7192845344543457, "metricx_score": 15.76287841796875, "metricx_qe_score": 10.589149475097656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ingual exploration? This work was done in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.37214985489845276, "xcomet_qe_score": 0.204276442527771, "metricx_score": 19.720552444458008, "metricx_qe_score": 20.810531616210938, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "collaboration with Patrick Fernandes, Emily Liu, Andre FD Martins and Graham Newbig. So a lot of", "metrics": {"bleu_score": 3.318166118968713, "chrf_score": 42.237541932279235, "xcomet_score": 0.463072806596756, "xcomet_qe_score": 0.4100722372531891, "metricx_score": 18.959562301635742, "metricx_qe_score": 14.745729446411133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "translations depend on context. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.794174075126648, "xcomet_qe_score": 0.9148557782173157, "metricx_score": 9.950645446777344, "metricx_qe_score": 15.169022560119629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", how would we translate mole in this sentence?", "metrics": {"bleu_score": 1.8854359641630605, "chrf_score": 7.076143475849096, "xcomet_score": 0.9799013137817383, "xcomet_qe_score": 0.9771023988723755, "metricx_score": 14.175402641296387, "metricx_qe_score": 17.873760223388672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Well, if the previous sentence was things could start to get dangerous if the ministers find out, then Moe refers to a spy. But if the previous sentence was", "metrics": {"bleu_score": 22.212527833175102, "chrf_score": 54.08299942409162, "xcomet_score": 0.7056687474250793, "xcomet_qe_score": 0.7024047374725342, "metricx_score": 25.0, "metricx_qe_score": 24.1278076171875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Could it be anything serious, doctor? then Moe refers to a birthmark. So, depending on the context, the meaning", "metrics": {"bleu_score": 15.249233198866706, "chrf_score": 43.48133384711771, "xcomet_score": 0.6513594388961792, "xcomet_qe_score": 0.8645313382148743, "metricx_score": 19.652122497558594, "metricx_qe_score": 22.380390167236328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of the word changes, and therefore, its translation changes as well. However, it is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5651217699050903, "xcomet_qe_score": 0.48764732480049133, "metricx_score": 23.129323959350586, "metricx_qe_score": 23.804162979125977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "quite difficult to evaluate how well models can translate cases like this. Firstly,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.842793345451355, "xcomet_qe_score": 0.8791866302490234, "metricx_score": 14.325284957885742, "metricx_qe_score": 6.625669479370117, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "because only a small portion of translations depend on context, which makes corpus level metrics like Blue unable to capture these translations. Some people have", "metrics": {"bleu_score": 0.0, "chrf_score": 0.2696871628910464, "xcomet_score": 0.6241655945777893, "xcomet_qe_score": 0.7242931723594666, "metricx_score": 18.251317977905273, "metricx_qe_score": 21.82469940185547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "suggested targeted evaluation of context-dependent translations, but these resources support only limited types of context-dependent translations and limited sets of languages, as they usually rely on domain knowledge and human curation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8958642482757568, "xcomet_qe_score": 0.927421510219574, "metricx_score": 6.861293315887451, "metricx_qe_score": 6.279430389404297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In this work, we try to answer these two questions.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.982324481010437, "xcomet_qe_score": 1.0, "metricx_score": 24.29738998413086, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "First, when does translation require context?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9771270751953125, "xcomet_qe_score": 1.0, "metricx_score": 19.017972946166992, "metricx_qe_score": 23.58127784729004, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And second, how well do models handle these cases?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9952130317687988, "xcomet_qe_score": 1.0, "metricx_score": 21.210172653198242, "metricx_qe_score": 24.046642303466797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8540679812431335, "xcomet_qe_score": 0.8912588357925415, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we introduced CXMI as a measure for the context usage by machine translation models. And this is done by", "metrics": {"bleu_score": 0.9324090986059582, "chrf_score": 3.5631062389248687, "xcomet_score": 0.7179888486862183, "xcomet_qe_score": 0.760373592376709, "metricx_score": 18.87944793701172, "metricx_qe_score": 17.729068756103516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "measuring how much information the context C provides about the target y given the source x. You can think of CXMI as the information gained by giving context to the model. In this work", "metrics": {"bleu_score": 0.5921947175246719, "chrf_score": 2.452390298355266, "xcomet_score": 0.7913249731063843, "xcomet_qe_score": 0.8579674959182739, "metricx_score": 19.137882232666016, "metricx_qe_score": 18.74012565612793, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we extend CXMI to pointwise CXMI, which can measure the context usage at the sentence level or at the word level. We can think of", "metrics": {"bleu_score": 1.0344928917121239, "chrf_score": 17.91332141456796, "xcomet_score": 0.6786961555480957, "xcomet_qe_score": 0.7846563458442688, "metricx_score": 17.792343139648438, "metricx_qe_score": 18.49435806274414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "words that have high PSXMI as those that require context for translation. Now we analyze words with high PC", "metrics": {"bleu_score": 0.0, "chrf_score": 3.2934463969156154, "xcomet_score": 0.6374298334121704, "xcomet_qe_score": 0.7270151972770691, "metricx_score": 19.601993560791016, "metricx_qe_score": 20.409793853759766, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "XMI to look for patterns between these words. We perform our", "metrics": {"bleu_score": 0.0, "chrf_score": 2.8639627767920217, "xcomet_score": 0.42948880791664124, "xcomet_qe_score": 0.6555464267730713, "metricx_score": 23.228811264038086, "metricx_qe_score": 19.688589096069336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "analysis on transcripts of TED Talks that have been translated from English into fourteen different languages. We perform", "metrics": {"bleu_score": 1.3287565534849795, "chrf_score": 2.2667076174438296, "xcomet_score": 0.6857690811157227, "xcomet_qe_score": 0.8103041052818298, "metricx_score": 16.77692413330078, "metricx_qe_score": 11.344101905822754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "our analysis at three different levels. First, we", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8350385427474976, "xcomet_qe_score": 0.8547086715698242, "metricx_score": 19.786216735839844, "metricx_qe_score": 16.69160270690918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "look at parts of speech tags that have high means PCXMI. This allows us to find, for example", "metrics": {"bleu_score": 0.0, "chrf_score": 5.513342729007968, "xcomet_score": 0.6191533207893372, "xcomet_qe_score": 0.8724911212921143, "metricx_score": 13.906272888183594, "metricx_qe_score": 14.945293426513672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", dual pronouns in Arabic that have relatively high p six mi. This can be explained because English does not have dual", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24728700518608093, "xcomet_qe_score": 0.272998571395874, "metricx_score": 22.965347290039062, "metricx_qe_score": 21.413469314575195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pronouns. Therefore, you need context to determine whether a pronoun is dual when translated into Arabic. And similarly", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5533615350723267, "xcomet_qe_score": 0.6778488159179688, "metricx_score": 18.804487228393555, "metricx_qe_score": 16.66416358947754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we find that certain languages also require context when we want to choose the appropriate verb form.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9924688339233398, "xcomet_qe_score": 0.999553918838501, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We then look at vocabulary items that have high p/mi averaged over all its different occurren", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.784676194190979, "xcomet_qe_score": 0.8051872253417969, "metricx_score": 24.031761169433594, "metricx_qe_score": 18.575519561767578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ces. And this helps us identify cases like the one here where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similar", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5004808902740479, "xcomet_qe_score": 0.5897752046585083, "metricx_score": 22.001976013183594, "metricx_qe_score": 23.628620147705078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ly, we find that context is supported to translate in the right formality.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7621935606002808, "xcomet_qe_score": 0.7740975618362427, "metricx_score": 21.405353546142578, "metricx_qe_score": 19.993505477905273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And finally, we look at different individual tokens that have high p6mi. And this allows us", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6962254047393799, "xcomet_qe_score": 0.7136749029159546, "metricx_score": 19.130861282348633, "metricx_qe_score": 19.799848556518555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in a standard structure, such as ellipsis resolution.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8450243473052979, "xcomet_qe_score": 0.8978677988052368, "metricx_score": 21.046358108520508, "metricx_qe_score": 21.66031265258789, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "So now we use our findings from our analysis to design a benchmark for document level translation.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9719027280807495, "xcomet_qe_score": 1.0, "metricx_score": 23.282581329345703, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "For each of the five discourse phenomena we identified, we created taggers to automatically identify words that pertain to the phenomenon. And we", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8375069499015808, "xcomet_qe_score": 0.8602352738380432, "metricx_score": 23.11591148376465, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "call our tagger the Multilingual Discourse Aware or MUDA tagger. We can", "metrics": {"bleu_score": 0.0, "chrf_score": 2.8686466964869415, "xcomet_score": 0.6262392997741699, "xcomet_qe_score": 0.7716753482818604, "metricx_score": 16.943519592285156, "metricx_qe_score": 13.556425094604492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "then also note that different languages have different proportions of these discrete phenomena. We then use", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6255687475204468, "xcomet_qe_score": 0.6748982667922974, "metricx_score": 22.845869064331055, "metricx_qe_score": 22.769454956054688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the MUDA tagger by applying the tagger to the parallel corpus that we want to use for evaluation, and we apply our translation metrics of choice to the context dependent examples that the MUDA tagger has identified. And finally", "metrics": {"bleu_score": 0.0, "chrf_score": 1.9113645560182635, "xcomet_score": 0.6362720727920532, "xcomet_qe_score": 0.7635895013809204, "metricx_score": 13.540863990783691, "metricx_qe_score": 10.439335823059082, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we use our benchmark as well as other metrics to evaluate different models on the document level machine translation. First", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7263094186782837, "xcomet_qe_score": 0.7629884481430054, "metricx_score": 22.291837692260742, "metricx_qe_score": 23.761837005615234, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of all, when we use corpus-level metrics, so for Blue, we find that complex agnostic models have the best performance.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.30750307503075025, "xcomet_score": 0.7119754552841187, "xcomet_qe_score": 0.7619038224220276, "metricx_score": 22.60346031188965, "metricx_qe_score": 19.655445098876953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "But then if we use comet, context-aware models perform best. And if we use", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8773621320724487, "xcomet_qe_score": 0.8155092000961304, "metricx_score": 20.571107864379883, "metricx_qe_score": 21.490009307861328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "word f measure, then models with or without context have comparable performance. This once again demonstrates that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.726056694984436, "xcomet_qe_score": 0.8712638020515442, "metricx_score": 12.346671104431152, "metricx_qe_score": 17.334474563598633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "it is difficult to determine the best document level translation system if we use corporate level metrics alone. Now we use the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6091773509979248, "xcomet_qe_score": 0.6989668607711792, "metricx_score": 21.91775894165039, "metricx_qe_score": 18.294158935546875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "MUDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion", "metrics": {"bleu_score": 0.0, "chrf_score": 0.9491502871448905, "xcomet_score": 0.8593058586120605, "xcomet_qe_score": 0.8682069182395935, "metricx_score": 23.5548152923584, "metricx_qe_score": 24.953397750854492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". However, these models are not much better than models that do not use context on other phenomena such as ellipses", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7421228289604187, "xcomet_qe_score": 0.8093940615653992, "metricx_score": 21.570537567138672, "metricx_qe_score": 21.13669204711914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", pronouns and verb form. This suggests where we would need to see more progress for document level translation", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8653987646102905, "xcomet_qe_score": 0.9350324869155884, "metricx_score": 19.94625473022461, "metricx_qe_score": 20.250804901123047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We also compared different commercial systems and our benchmark shows that DeepBell is usually more accurate than Google Translate for document level translation. In summary", "metrics": {"bleu_score": 0.0, "chrf_score": 2.5032157350129065, "xcomet_score": 0.6535365581512451, "xcomet_qe_score": 0.7287901639938354, "metricx_score": 23.136587142944336, "metricx_qe_score": 23.57145118713379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we perform a databased analysis across fourteen language pairs to identify when translations require context. And then we use our findings to build a benchmark for document level machine translation, which can help us identify which discourse phenomenon models can handle well or not, and which translation systems are good at document level translation. Thank you so", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5368694067001343, "xcomet_qe_score": 0.5563368797302246, "metricx_score": 14.685467720031738, "metricx_qe_score": 13.15969467163086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "much for your attention. See you", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5625370740890503, "xcomet_qe_score": 0.715171217918396, "metricx_score": 8.34778881072998, "metricx_qe_score": 8.435750007629395, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tomorrow though.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16882801055908203, "xcomet_qe_score": 0.10500924289226532, "metricx_score": 7.468276500701904, "metricx_qe_score": 12.657014846801758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hi, I am Yanis Lavrack and I will present you our works on Dr. Berth, a robust pre-trained model in French for biomedical and clinical doma", "metrics": {"bleu_score": 0.6917029517008104, "chrf_score": 8.556997647292118, "xcomet_score": 0.6227461099624634, "xcomet_qe_score": 0.6204705238342285, "metricx_score": 11.035409927368164, "metricx_qe_score": 6.597383499145508, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ins. In this presentation, we first talk about language modeling in healthcare. Then", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6756435632705688, "xcomet_qe_score": 0.6631771326065063, "metricx_score": 22.2733154296875, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we will present the main contribution of our article", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.977083683013916, "xcomet_qe_score": 0.9704066514968872, "metricx_score": 15.835678100585938, "metricx_qe_score": 22.189653396606445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We introduced the first biomedical model in French, named Dr. Berth, which is based on Roberta, and trained on Natchios, which is a dataset of medical crawled data from the web. We also", "metrics": {"bleu_score": 0.0, "chrf_score": 1.586029292937231, "xcomet_score": 0.4187985360622406, "xcomet_qe_score": 0.3611075282096863, "metricx_score": 15.368802070617676, "metricx_qe_score": 9.314064979553223, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "introduce a comparison of models with multiple plutonian settings and data sources. Then we present our results", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.615164041519165, "xcomet_qe_score": 0.6238654851913452, "metricx_score": 17.751035690307617, "metricx_qe_score": 19.99049949645996, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on eleven biomedical and clinical downstream tasks in French. And finally, we conclude about the exper", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6657088398933411, "xcomet_qe_score": 0.7948674559593201, "metricx_score": 16.013225555419922, "metricx_qe_score": 16.642242431640625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "iments and give you more details about how to access the model. Since its release in 2018, BERT has become one of the most", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2693217694759369, "xcomet_qe_score": 0.1492266058921814, "metricx_score": 22.064855575561523, "metricx_qe_score": 23.38141441345215, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "effective approaches to solving natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as word to vector, fast text or enroll. Since then, this model has been adapted to many", "metrics": {"bleu_score": 0.0, "chrf_score": 4.2576550629175856, "xcomet_score": 0.6147210597991943, "xcomet_qe_score": 0.6757965683937073, "metricx_score": 13.475714683532715, "metricx_qe_score": 12.284914016723633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "other languages, such as in French with Camembert, and other domains such as biomedical with PermetteBERT and BioBERT, and on clinical with clinical BERT, but mostly in English. Spe", "metrics": {"bleu_score": 0.5699211809653533, "chrf_score": 18.34520065670322, "xcomet_score": 0.37653398513793945, "xcomet_qe_score": 0.4158864915370941, "metricx_score": 17.257953643798828, "metricx_qe_score": 13.606439590454102, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cialized models for other languages are scarce and are often based on continual pretending due to the lack of indomitable data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.558630108833313, "xcomet_qe_score": 0.5350995063781738, "metricx_score": 22.21067237854004, "metricx_qe_score": 21.419464111328125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". However, France did not have any open source models for biomedicine until now.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9690046906471252, "xcomet_qe_score": 0.96678227186203, "metricx_score": 13.7564115524292, "metricx_qe_score": 17.402545928955078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We ask ourselves the question about what is the most appropriate data sources for a wide range of usage. And those current data are a good substitution for clinical", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8045386075973511, "xcomet_qe_score": 0.780482292175293, "metricx_score": 17.04231071472168, "metricx_qe_score": 19.20574951171875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "data. To answer this question, we compare Dr. Bert with our Schubert model, which is based on anonymized data obtained from the non-university hospital that we have. Afterward", "metrics": {"bleu_score": 0.0, "chrf_score": 1.7238285256851966, "xcomet_score": 0.3220083713531494, "xcomet_qe_score": 0.3846205472946167, "metricx_score": 19.93683433532715, "metricx_qe_score": 20.366308212280273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we ask ourselves, how much data do we need to train a specialized model on French data?", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.96973717212677, "xcomet_qe_score": 0.9697418212890625, "metricx_score": 23.764951705932617, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Is it 4 GB, 8 GB or more?", "metrics": {"bleu_score": 0.0, "chrf_score": 15.049828911429938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5473654270172119, "metricx_qe_score": 0.23745040595531464, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To answer this question, we first train and compare four from scratch models. A first version of Dr. Bert with seven GB of Nachos. A second version of four GB of Nachos subset. A first version of Schubert, which is a clinical model with four gigabytes of sentences taken from clinical nodes. And a final version of Schubert with a mix of four gigabytes of natures and four gigabytes of clinical nodes. In addition to", "metrics": {"bleu_score": 0.0, "chrf_score": 2.1075848579343255, "xcomet_score": 0.1970779299736023, "xcomet_qe_score": 0.24094568192958832, "metricx_score": 16.50328254699707, "metricx_qe_score": 13.854576110839844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this comparison, we introduced three models trained on continual pretraining to analyze the impact of pretraining strategies. One based on the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6468334197998047, "xcomet_qe_score": 0.7361251711845398, "metricx_score": 20.73351287841797, "metricx_qe_score": 13.1109619140625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "weight of Camembert and trained on four gigabytes of Nacho's, another also based on Cam", "metrics": {"bleu_score": 0.0, "chrf_score": 6.591694299384991, "xcomet_score": 0.27765175700187683, "xcomet_qe_score": 0.3472597897052765, "metricx_score": 17.738676071166992, "metricx_qe_score": 14.327930450439453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "embert, but trained this time on the four gigabytes of Klinker Lots. And finally, one based on an English biomedical model, Bermud Bert, and trained on 4 GB of Snatchers. In", "metrics": {"bleu_score": 0.0, "chrf_score": 4.304776082438522, "xcomet_score": 0.18161596357822418, "xcomet_qe_score": 0.30819109082221985, "metricx_score": 22.238142013549805, "metricx_qe_score": 21.569578170776367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "total, we have seven models.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9485964775085449, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 4.914675712585449, "metricx_qe_score": 12.93964672088623, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "To evaluate our seven models, we gather which support public and private downstream tasks such as name and identity recognition, classification, pattern switch tagging and question answering. These", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3316003084182739, "xcomet_qe_score": 0.4251631200313568, "metricx_score": 17.12862777709961, "metricx_qe_score": 11.80408763885498, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "models are compared to six baseline models, which are Camembert OSCAR 138 GB, Camembert OSCAR 4 GB, Camembert CCNet 4 GB, PumedBelt, Myobelt and ClinicalBelt.", "metrics": {"bleu_score": 4.082796711563009, "chrf_score": 28.56615520774033, "xcomet_score": 0.5191262364387512, "xcomet_qe_score": 0.5134216547012329, "metricx_score": 16.038799285888672, "metricx_qe_score": 12.72715950012207, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The evaluation highlights that the model performed best on the task with data of the same nature as those on which the model has", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7421910762786865, "xcomet_qe_score": 0.714066743850708, "metricx_score": 23.703617095947266, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "been trained. However, we can obtain the data from we can observe that", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16888222098350525, "xcomet_qe_score": 0.1998499631881714, "metricx_score": 22.443435668945312, "metricx_qe_score": 21.37774658203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "data from heterogeneous sources appear to be more versatile. We also observe that using", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4024823307991028, "xcomet_qe_score": 0.5499038696289062, "metricx_score": 23.67125129699707, "metricx_qe_score": 19.947664260864258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "more data translates into better performance. In general, FromScratch 3Techning seems to obtain higher performance on most of", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2847018837928772, "xcomet_qe_score": 0.4223732352256775, "metricx_score": 21.58424186706543, "metricx_qe_score": 16.92980194091797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the tasks. However, our experiment on continuous pretending using the weight and tokenizer of PumedBeard, trained on the four GB subset of Natchez, showed comparable results to those obtained with Dr. Beard four GB", "metrics": {"bleu_score": 0.0, "chrf_score": 3.2059624212220355, "xcomet_score": 0.20650775730609894, "xcomet_qe_score": 0.3020159602165222, "metricx_score": 19.65234375, "metricx_qe_score": 17.959381103515625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "from scratch. This is not the case for the model based on common bear weights and tokenizer which suffer from stability issues. Finally,", "metrics": {"bleu_score": 0.0, "chrf_score": 1.7291909288886318, "xcomet_score": 0.3078935742378235, "xcomet_qe_score": 0.5809626579284668, "metricx_score": 18.238622665405273, "metricx_qe_score": 15.855436325073242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "as a conclusion, our proposed system offers better performance on nine of the eleven downstream tasks and surpasses globally the result of the generic model here, Camembert. We also obser", "metrics": {"bleu_score": 0.0, "chrf_score": 2.9258219940120065, "xcomet_score": 0.8431737422943115, "xcomet_qe_score": 0.8549129962921143, "metricx_score": 9.048589706420898, "metricx_qe_score": 5.916528224945068, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ve that specialized data is better, more specialized data is better, but it does not scale well. All the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.33977439999580383, "xcomet_qe_score": 0.3218970000743866, "metricx_score": 18.928932189941406, "metricx_qe_score": 15.964215278625488, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pre-trained models obtained from Natchios are freely available on YuginFace and all the training scripts are on our GitHub repository. So thank you for this presentation and we are", "metrics": {"bleu_score": 0.68134654077946, "chrf_score": 8.691542084059362, "xcomet_score": 0.3559880256652832, "xcomet_qe_score": 0.3624565899372101, "metricx_score": 19.57229232788086, "metricx_qe_score": 17.997304916381836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "looking forward to exchanging at the POSTER session in Toronto.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8489099740982056, "xcomet_qe_score": 0.8651400208473206, "metricx_score": 12.228660583496094, "metricx_qe_score": 11.253006935119629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3974819481372833, "xcomet_qe_score": 0.28956812620162964, "metricx_score": 3.7109405994415283, "metricx_qe_score": 0.16473039984703064, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent", "metrics": {"bleu_score": 1.6526206693530716, "chrf_score": 20.236118908432683, "xcomet_score": 0.7385420799255371, "xcomet_qe_score": 0.8450796604156494, "metricx_score": 17.166717529296875, "metricx_qe_score": 23.03375816345215, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "permutations. This is joint work with my advisors Alexander Koller and Ivan Titov. Compositional generalization can be understood", "metrics": {"bleu_score": 5.415315253510896, "chrf_score": 37.00834724508646, "xcomet_score": 0.6200195550918579, "xcomet_qe_score": 0.5876356363296509, "metricx_score": 19.107524871826172, "metricx_qe_score": 19.23191261291504, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of seman", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6189438104629517, "xcomet_qe_score": 0.6062005758285522, "metricx_score": 24.31836700439453, "metricx_qe_score": 23.971769332885742, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "tic parsing, testing for compositional generalization might look like this. As", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6042064428329468, "xcomet_qe_score": 0.6371839046478271, "metricx_score": 23.449676513671875, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "usual, we have a training set of utterances", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9168157577514648, "xcomet_qe_score": 0.957353949546814, "metricx_score": 17.713415145874023, "metricx_qe_score": 24.634183883666992, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". In this case, the girl slept and Mary", "metrics": {"bleu_score": 3.7760275051848216, "chrf_score": 32.85919641727206, "xcomet_score": 0.31880056858062744, "xcomet_qe_score": 0.7057733535766602, "metricx_score": 15.319148063659668, "metricx_qe_score": 16.876876831054688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "knew that the girl slept. These utterances are paired with logical forms that", "metrics": {"bleu_score": 17.127893393291703, "chrf_score": 42.94586088843096, "xcomet_score": 0.24812281131744385, "xcomet_qe_score": 0.1616123914718628, "metricx_score": 20.445388793945312, "metricx_qe_score": 20.990684509277344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "represent core aspects of their meaning. In contrast to standard machine learning evaluation", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17011763155460358, "xcomet_qe_score": 0.2064763605594635, "metricx_score": 22.17091178894043, "metricx_qe_score": 20.118593215942383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the test set does not come from the same distribution but contains structurally unseen logical forms. In this example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8163094520568848, "xcomet_qe_score": 0.8838040828704834, "metricx_score": 25.0, "metricx_qe_score": 24.275493621826172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the model has seen shallower recursion during training and is tested on an example with deeper recursion. Naive sequ", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6499298810958862, "xcomet_qe_score": 0.669450044631958, "metricx_score": 23.312278747558594, "metricx_qe_score": 23.684324264526367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that", "metrics": {"bleu_score": 0.0, "chrf_score": 2.5996772059690554, "xcomet_score": 0.439709335565567, "xcomet_qe_score": 0.34313344955444336, "metricx_score": 23.44426918029785, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6226778030395508, "xcomet_qe_score": 0.5197161436080933, "metricx_score": 20.975500106811523, "metricx_qe_score": 20.6420955657959, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "color-coded in the example. A popular method to address this is to integrate", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.41012704372406006, "xcomet_qe_score": 0.4844224452972412, "metricx_score": 20.754535675048828, "metricx_qe_score": 21.292505264282227, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "trees into the models. The trees are intended to capture the compositional process that relates utterances with the log", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6269433498382568, "xcomet_qe_score": 0.7141163349151611, "metricx_score": 17.672054290771484, "metricx_qe_score": 21.143159866333008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8577383756637573, "xcomet_qe_score": 0.8341084122657776, "metricx_score": 19.36722183227539, "metricx_qe_score": 22.998973846435547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a complicated and sometimes a computationally expensive process. Typically, this", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7059529423713684, "xcomet_qe_score": 0.8122445344924927, "metricx_score": 18.182167053222656, "metricx_qe_score": 19.427310943603516, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "involves considerable formalism-specific preprocessing of the logical forms, for example, to handle variable symbol", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6866767406463623, "xcomet_qe_score": 0.911333441734314, "metricx_score": 24.501344680786133, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "s. Obtaining trees may also involve specialized grammar induction procedures", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9208791851997375, "xcomet_qe_score": 0.9223041534423828, "metricx_score": 21.17779541015625, "metricx_qe_score": 20.518035888671875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". In this paper, we do not use trees and introduce a neural sequence to sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first time we", "metrics": {"bleu_score": 0.0, "chrf_score": 2.4898888599603, "xcomet_score": 0.595639705657959, "xcomet_qe_score": 0.6306039094924927, "metricx_score": 14.789983749389648, "metricx_qe_score": 19.131380081176758, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "show strong generalization to deeper recursion without relying on trace. Our approach predicts the output from the input", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4185848534107208, "xcomet_qe_score": 0.7612013220787048, "metricx_score": 15.505860328674316, "metricx_qe_score": 18.383743286132812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in two steps. First", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.338918536901474, "xcomet_qe_score": 0.23890523612499237, "metricx_score": 21.02520179748535, "metricx_qe_score": 22.66274070739746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we tag each input token with an unordered multiset of tokens that will appear in the output. After the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6421046257019043, "xcomet_qe_score": 0.6401468515396118, "metricx_score": 23.379358291625977, "metricx_qe_score": 21.944459915161133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "first step, we have all the right tokens, but they're not ordered", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8529353141784668, "xcomet_qe_score": 0.9728559851646423, "metricx_score": 16.340770721435547, "metricx_qe_score": 16.633281707763672, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". That's why in the second step we use another model to predict a permutation to put them into the right", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6293854117393494, "xcomet_qe_score": 0.7474497556686401, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permut", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8320814371109009, "xcomet_qe_score": 0.76708984375, "metricx_score": 20.59854507446289, "metricx_qe_score": 23.356473922729492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ations. This makes our approach quite flexible and expressive. Conceptually", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6201391220092773, "xcomet_qe_score": 0.5829689502716064, "metricx_score": 19.50901222229004, "metricx_qe_score": 22.007282257080078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", our permutation model works roughly like this.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9476810693740845, "xcomet_qe_score": 0.964349627494812, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "We go from left to right over the output and determine which multiset token to put in every position. For the first", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7106689214706421, "xcomet_qe_score": 0.7510078549385071, "metricx_score": 22.40363311767578, "metricx_qe_score": 23.922740936279297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "output position, we simply select one as highlighted in red. Then we jump", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8176093101501465, "xcomet_qe_score": 0.8522003889083862, "metricx_score": 14.383543968200684, "metricx_qe_score": 15.075520515441895, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to the next multi-set token to determine the second token in the output. We determine the third token in the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43294164538383484, "xcomet_qe_score": 0.4311903119087219, "metricx_score": 19.17986297607422, "metricx_qe_score": 11.483238220214844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "output in a similar way by jumping to another multiset token. We continue this process. until every token from the first", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6328712701797485, "xcomet_qe_score": 0.7222983837127686, "metricx_score": 17.434795379638672, "metricx_qe_score": 16.50185775756836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "stage has been visited exactly once. To give you a teaser of the experimental results", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16826041042804718, "xcomet_qe_score": 0.21499694883823395, "metricx_score": 20.01275634765625, "metricx_qe_score": 21.31723976135254, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", here we compare our method with other treeless models on the Kongs benchmark. Our model outperforms the others by a large margin on generalization to dee", "metrics": {"bleu_score": 0.0, "chrf_score": 0.25960539979231567, "xcomet_score": 0.5203311443328857, "xcomet_qe_score": 0.6403560638427734, "metricx_score": 15.687891006469727, "metricx_qe_score": 14.47991943359375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "per recursion. Some other kinds of structural generalization remain very challenging though. In our paper", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24272921681404114, "xcomet_qe_score": 0.2643614709377289, "metricx_score": 22.153484344482422, "metricx_qe_score": 20.97594451904297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "we solve a couple of interesting technical challenges. First of all, the align", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.23788122832775116, "xcomet_qe_score": 0.2386336624622345, "metricx_score": 22.351146697998047, "metricx_qe_score": 20.85295867919922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ment between input and output is not given in the training data. As a", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1657354086637497, "xcomet_qe_score": 0.14241726696491241, "metricx_score": 21.303668975830078, "metricx_qe_score": 23.689655303955078, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "consequence, for a given token, we do not know which multisetter it came from", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15135951340198517, "xcomet_qe_score": 0.1488911509513855, "metricx_score": 11.01064395904541, "metricx_qe_score": 9.933158874511719, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3622095584869385, "xcomet_qe_score": 0.27961739897727966, "metricx_score": 17.785518646240234, "metricx_qe_score": 16.439510345458984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", but the linguistically correct one is latent. We address this by inducing the alignment as part of the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.23004940152168274, "xcomet_qe_score": 0.2198859453201294, "metricx_score": 23.07466697692871, "metricx_qe_score": 22.02936363220215, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "training. Our permutation method is very flexible, but it poses", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.23834718763828278, "xcomet_qe_score": 0.1494220495223999, "metricx_score": 20.265945434570312, "metricx_qe_score": 19.53813362121582, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the challenge that finding the highest scoring permutation is NP hard. This is because this is related to the traveling salesman problem. We approxim", "metrics": {"bleu_score": 0.0, "chrf_score": 3.639291370111402, "xcomet_score": 0.3142250180244446, "xcomet_qe_score": 0.45716702938079834, "metricx_score": 17.90947914123535, "metricx_qe_score": 16.536169052124023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ate this with a", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1233716607093811, "xcomet_qe_score": 0.13601617515087128, "metricx_score": 23.954830169677734, "metricx_qe_score": 23.385311126708984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "GPU-friendly, continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations", "metrics": {"bleu_score": 0.0, "chrf_score": 1.450871819729333, "xcomet_score": 0.780651330947876, "xcomet_qe_score": 0.8556722402572632, "metricx_score": 20.484895706176758, "metricx_qe_score": 22.88046646118164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". If you want to learn more about our experiments and how we address these challenges, please take a look at our paper or come to our poster.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8633239269256592, "xcomet_qe_score": 0.9157493114471436, "metricx_score": 22.728700637817383, "metricx_qe_score": 24.80777359008789, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone, I'm Makshta, and today my co-author Martin and I are presenting our work The Kitmastech, Evaluating Knowledge Integration from Multiple Sources. This", "metrics": {"bleu_score": 0.7021128292272865, "chrf_score": 8.151592943716473, "xcomet_score": 0.49676042795181274, "xcomet_qe_score": 0.5386932492256165, "metricx_score": 17.31531524658203, "metricx_qe_score": 13.116944313049316, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "work is a collaboration between McGill University, Mila, and Microsoft Research. National Language", "metrics": {"bleu_score": 1.483327704143614, "chrf_score": 4.413129728301596, "xcomet_score": 0.6885623931884766, "xcomet_qe_score": 0.7488265037536621, "metricx_score": 23.383222579956055, "metricx_qe_score": 24.795766830444336, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Understanding Models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired via pre-training,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6961034536361694, "xcomet_qe_score": 0.859056293964386, "metricx_score": 24.088600158691406, "metricx_qe_score": 23.129430770874023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6973139047622681, "xcomet_qe_score": 0.7192283868789673, "metricx_score": 22.835081100463867, "metricx_qe_score": 23.727703094482422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "use pre-trained time knowledge to solve the task. But natural", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2363559603691101, "xcomet_qe_score": 0.2052384316921234, "metricx_score": 23.010265350341797, "metricx_qe_score": 17.766931533813477, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, John saw the newly elected president on TV. Pre", "metrics": {"bleu_score": 21.31946432165191, "chrf_score": 43.267411734811084, "xcomet_score": 0.4358025789260864, "xcomet_qe_score": 0.31892192363739014, "metricx_score": 9.914155006408691, "metricx_qe_score": 11.450373649597168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "-training parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance specific entity John is or who the new president is because the president may have changed since pre-training. Therefore, successful models for knowledge-intensive", "metrics": {"bleu_score": 0.38242844711862806, "chrf_score": 1.4221979405388758, "xcomet_score": 0.48955485224723816, "xcomet_qe_score": 0.5532444715499878, "metricx_score": 19.048229217529297, "metricx_qe_score": 19.82390785217285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "NLU tasks require the ability to integrate and use both pre-trained time and inference time knowledge. In", "metrics": {"bleu_score": 0.5065911375131152, "chrf_score": 1.8545250788829064, "xcomet_score": 0.2889080047607422, "xcomet_qe_score": 0.4077840745449066, "metricx_score": 19.56451988220215, "metricx_qe_score": 20.220008850097656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "this work, we propose a diagnostic test suite for knowledge integration. We introduce a core reference", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9012597799301147, "xcomet_qe_score": 0.9366952776908875, "metricx_score": 16.480918884277344, "metricx_qe_score": 18.192867279052734, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6177248954772949, "xcomet_qe_score": 0.6769412755966187, "metricx_score": 20.49685287475586, "metricx_qe_score": 17.969789505004883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "data set both with human study participants and established reference resolution models. In this figure, we show the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.49663546681404114, "xcomet_qe_score": 0.7889506816864014, "metricx_score": 17.377317428588867, "metricx_qe_score": 13.783031463623047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Thur", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16581708192825317, "xcomet_qe_score": 0.11207574605941772, "metricx_score": 14.804527282714844, "metricx_qe_score": 16.572107315063477, "linguapy_score": [1, "ALBANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "vin is a judge. Kia", "metrics": {"bleu_score": 0.0, "chrf_score": 9.737266035296072, "xcomet_score": 0.2220170944929123, "xcomet_qe_score": 0.1793781816959381, "metricx_score": 20.43754768371582, "metricx_qe_score": 20.27581024169922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "is a baker. Thur", "metrics": {"bleu_score": 0.0, "chrf_score": 3.1446540880503147, "xcomet_score": 0.1981469690799713, "xcomet_qe_score": 0.14999788999557495, "metricx_score": 18.980613708496094, "metricx_qe_score": 20.298341751098633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "vin and Kia met at a park. After", "metrics": {"bleu_score": 0.0, "chrf_score": 11.522289074251333, "xcomet_score": 0.25038856267929077, "xcomet_qe_score": 0.21643123030662537, "metricx_score": 19.433210372924805, "metricx_qe_score": 22.41773223876953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a long day at work deciding cases in a law court, he was happy to relax. The task", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7382107973098755, "xcomet_qe_score": 0.7808972597122192, "metricx_score": 24.902816772460938, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "here is to identify the correct entity that the pronoun he refers to, which in this case is servant. The resolution of a given", "metrics": {"bleu_score": 1.0619714138345164, "chrf_score": 3.839463502444887, "xcomet_score": 0.5713411569595337, "xcomet_qe_score": 0.6516252756118774, "metricx_score": 22.766624450683594, "metricx_qe_score": 23.875593185424805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pronoun requires two types of information. First", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6784795522689819, "xcomet_qe_score": 0.7557443976402283, "metricx_score": 18.354772567749023, "metricx_qe_score": 24.833824157714844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", entity specific knowledge such as servant is a judge.", "metrics": {"bleu_score": 0.0, "chrf_score": 4.40302319320312, "xcomet_score": 0.8964194059371948, "xcomet_qe_score": 0.9319455027580261, "metricx_score": 20.2125244140625, "metricx_qe_score": 19.17761993408203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "And second, background knowledge such as judges decide cases in law courts. Generally", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8640530109405518, "xcomet_qe_score": 0.9174274802207947, "metricx_score": 16.9538631439209, "metricx_qe_score": 13.7783203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", background knowledge is learned during the pre-training of large language models, while entity specific knowledge is typically observed at inference time. We vary", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6725828647613525, "xcomet_qe_score": 0.7471500635147095, "metricx_score": 22.868114471435547, "metricx_qe_score": 24.901668548583984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the availability of these two pieces of information, so that it may either be found in a single source or in multiple", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7081533670425415, "xcomet_qe_score": 0.7631475925445557, "metricx_score": 20.438737869262695, "metricx_qe_score": 19.347585678100586, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sources. We have defined three settings of Kitmos. First", "metrics": {"bleu_score": 0.0, "chrf_score": 0.7183908045977012, "xcomet_score": 0.5864080190658569, "xcomet_qe_score": 0.652614951133728, "metricx_score": 13.385156631469727, "metricx_qe_score": 16.713109970092773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we have the topic setting, background pre-training, where background knowledge is assumed to be available at pre-tra", "metrics": {"bleu_score": 0.0, "chrf_score": 0.31928480204342274, "xcomet_score": 0.7935121059417725, "xcomet_qe_score": 0.7529069185256958, "metricx_score": 18.73518180847168, "metricx_qe_score": 13.421642303466797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ining time. Secondo-Both settings, where background knowledge is available both at pre-training time and at inference", "metrics": {"bleu_score": 0.0, "chrf_score": 0.34435261707988984, "xcomet_score": 0.4985639452934265, "xcomet_qe_score": 0.4694480299949646, "metricx_score": 20.908870697021484, "metricx_qe_score": 21.209394454956055, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "time. Lastly, background inference settings, where both knowledge types are available only at inference time. This last", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7162586450576782, "xcomet_qe_score": 0.833428680896759, "metricx_score": 16.813087463378906, "metricx_qe_score": 16.677574157714844, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "setting is especially interesting, since it simulates a case where the background knowledge necessary to solve a task is not part of the pre trained data of models", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.801037073135376, "xcomet_qe_score": 0.8718910217285156, "metricx_score": 22.72214698791504, "metricx_qe_score": 23.77011489868164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", for example because new occupations have developed since the time of", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6917265057563782, "xcomet_qe_score": 0.6463505625724792, "metricx_score": 20.596797943115234, "metricx_qe_score": 20.514293670654297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pre trained. Here is an example of how we control the availability of facts", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5862878561019897, "xcomet_qe_score": 0.5395248532295227, "metricx_score": 22.220680236816406, "metricx_qe_score": 23.237451553344727, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in a true source. In the pre-trained setting, we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters. In the intervention context, we provide the anti-specific knowledge Chichester is a politi", "metrics": {"bleu_score": 0.4241978449406377, "chrf_score": 7.039834981731049, "xcomet_score": 0.37350109219551086, "xcomet_qe_score": 0.39714208245277405, "metricx_score": 17.30190086364746, "metricx_qe_score": 13.127253532409668, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cian. In the background both settings, we provide not only anti-specific, but also background knowledge about politicians in the context of the Influence Time.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.248015873015873, "xcomet_score": 0.40056177973747253, "xcomet_qe_score": 0.3782266676425934, "metricx_score": 18.329917907714844, "metricx_qe_score": 16.92001724243164, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In the background in Freon setting, we provide the fictional occupation meritur instead of politician because meritur is unlikely to be contained in the pre-trained parameters. We evaluated the", "metrics": {"bleu_score": 0.0, "chrf_score": 6.072809825897187, "xcomet_score": 0.3558873236179352, "xcomet_qe_score": 0.36067715287208557, "metricx_score": 21.273714065551758, "metricx_qe_score": 17.897693634033203, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "data set both with human study participants and established reference resolution models. In this figure, we show the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3989870250225067, "xcomet_qe_score": 0.81036376953125, "metricx_score": 18.467031478881836, "metricx_qe_score": 15.187566757202148, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "results of the best performing models on the most difficult variant of the background pre-trained settings. Without task-specific training on Kitmos,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.28835063437139563, "xcomet_score": 0.23682722449302673, "xcomet_qe_score": 0.3385463356971741, "metricx_score": 18.605499267578125, "metricx_qe_score": 16.087202072143555, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "both models do not perform well. When trained on Kitmos, however", "metrics": {"bleu_score": 0.0, "chrf_score": 0.4578754578754578, "xcomet_score": 0.7508940100669861, "xcomet_qe_score": 0.7947619557380676, "metricx_score": 17.317712783813477, "metricx_qe_score": 16.520124435424805, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", both C2F and Berth for Koref perform significantly better than the random choice. This sugg", "metrics": {"bleu_score": 1.269961673637781, "chrf_score": 6.430428542366001, "xcomet_score": 0.4709581136703491, "xcomet_qe_score": 0.555156946182251, "metricx_score": 23.164043426513672, "metricx_qe_score": 19.44817352294922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ests that when trained on general coefficient resolution data sets, mods learn to exploit surface cues, which are not useful when testing on kidmos where such cues have been removed. Additional experiments with fictional knowledge indicate that even", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.11996351927518845, "xcomet_qe_score": 0.2264832854270935, "metricx_score": 24.53760528564453, "metricx_qe_score": 24.100461959838867, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the best performing models cannot reliably integrate background knowledge provided only at the time of inference. To summarize the main takeaways of our paper", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2642022967338562, "xcomet_qe_score": 0.7194299697875977, "metricx_score": 14.110918045043945, "metricx_qe_score": 15.70931339263916, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", many coerence resolution models appear unable to reason over knowledge from different sources without task-specific training. However", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6160848140716553, "xcomet_qe_score": 0.7145335674285889, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7701184153556824, "xcomet_qe_score": 0.783022940158844, "metricx_score": 23.38484764099121, "metricx_qe_score": 24.918643951416016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the best performing models seem to have difficulties with reliably integrated backward knowledge presented only at inference time. If you're interested in", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3297257423400879, "xcomet_qe_score": 0.5567289590835571, "metricx_score": 24.61334228515625, "metricx_qe_score": 24.39103126525879, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.", "metrics": {"bleu_score": 1.0010613966262247, "chrf_score": 7.53026438358951, "xcomet_score": 0.8778821229934692, "xcomet_qe_score": 0.9154465198516846, "metricx_score": 12.52049732208252, "metricx_qe_score": 16.172700881958008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.800614595413208, "xcomet_qe_score": 0.44242411851882935, "metricx_score": 1.1037641763687134, "metricx_qe_score": 4.742196559906006, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, I'm Myra, and today I'll be talking about our paper, Marked Personas, using natural language prompts to measure stereotypes in language", "metrics": {"bleu_score": 0.5964284038283426, "chrf_score": 2.5727133731164487, "xcomet_score": 0.7665049433708191, "xcomet_qe_score": 0.8936660289764404, "metricx_score": 7.044468879699707, "metricx_qe_score": 6.8252105712890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "models. This work is done in collaboration with Essendermouch and Dandarovsky. In recent", "metrics": {"bleu_score": 0.0, "chrf_score": 10.1416996397331, "xcomet_score": 0.27375802397727966, "xcomet_qe_score": 0.19814786314964294, "metricx_score": 24.13174819946289, "metricx_qe_score": 20.42384147644043, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "years, many have documented the prevalence of social bias and stereotypes in large language models, or LLMs. However", "metrics": {"bleu_score": 0.0, "chrf_score": 2.122282952536838, "xcomet_score": 0.6043562889099121, "xcomet_qe_score": 0.6887692213058472, "metricx_score": 24.96512222290039, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", these measures have various limitations. They usually r", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8727476596832275, "xcomet_qe_score": 0.8725117444992065, "metricx_score": 21.212141036987305, "metricx_qe_score": 22.842430114746094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ely on hand-constructed datasets that are very time-consuming to curate. They also usually only measure very specific stereotypes, meaning that they do not generalize well to other demographics or contexts, or they simply capture very general, broad associations, such as negative associations with particular groups. Furthermore", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7347279787063599, "xcomet_qe_score": 0.7866253852844238, "metricx_score": 11.288863182067871, "metricx_qe_score": 9.590088844299316, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", most work in this space doesn't take into account intersectionality, which is the notion that multifaceted social identities can compo", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7264171838760376, "xcomet_qe_score": 0.7237670421600342, "metricx_score": 24.268856048583984, "metricx_qe_score": 24.33036231994629, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "und biases and be unique loci of harm. To overcome these limitations, we rely on the property that these newer instruction tuned LLMs are very good at responding", "metrics": {"bleu_score": 0.0, "chrf_score": 1.530849435713683, "xcomet_score": 0.291709303855896, "xcomet_qe_score": 0.4049048125743866, "metricx_score": 23.497243881225586, "metricx_qe_score": 23.343692779541016, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like imagine you are an Asian", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6161863803863525, "xcomet_qe_score": 0.6768667697906494, "metricx_score": 15.970956802368164, "metricx_qe_score": 21.38591766357422, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "woman, describe yourself. And we can immediately", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.17864826321601868, "xcomet_qe_score": 0.15832355618476868, "metricx_score": 5.289140224456787, "metricx_qe_score": 9.010273933410645, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "see that this is very generalizable to any demographic because we can just specify any identity marker that we want in this prompt. Here are some examples of gener", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5467079877853394, "xcomet_qe_score": 0.6417168378829956, "metricx_score": 15.892709732055664, "metricx_qe_score": 14.160931587219238, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ations from GPT four. We immediately see that while the", "metrics": {"bleu_score": 0.0, "chrf_score": 4.694634658683954, "xcomet_score": 0.46380719542503357, "xcomet_qe_score": 0.40951141715049744, "metricx_score": 23.315092086791992, "metricx_qe_score": 23.4002685546875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "outputs are not overly negative or toxic in the traditional sense of these words. There are some interesting patterns. The Asian woman is", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6104632616043091, "xcomet_qe_score": 0.591327428817749, "metricx_score": 20.836673736572266, "metricx_qe_score": 20.097997665405273, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "portrayed as unassuming. The Middle Eastern woman is referred to using words like exotic and referring to a mesmerizing region. Both the Women of Color persona refers to ancest", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5236386060714722, "xcomet_qe_score": 0.6226567625999451, "metricx_score": 16.455791473388672, "metricx_qe_score": 15.726984024047852, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ry, while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22305504977703094, "xcomet_qe_score": 0.2526620030403137, "metricx_score": 24.355520248413086, "metricx_qe_score": 24.774139404296875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "one is generating these personas. Our suggestions for generating these", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.21561980247497559, "xcomet_qe_score": 0.20389366149902344, "metricx_score": 19.332916259765625, "metricx_qe_score": 15.068985939025879, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "personas were inspired by a study", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13980117440223694, "xcomet_qe_score": 0.13107678294181824, "metricx_score": 20.665815353393555, "metricx_qe_score": 17.21164321899414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in which they gave these suggestions to human subjects, finding that by giving them to human subjects, they were also able to bring to the surface racial stereotypes. And also this enables direct", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5340120792388916, "xcomet_qe_score": 0.7110508680343628, "metricx_score": 16.421480178833008, "metricx_qe_score": 13.048861503601074, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "comparison between our generated personas and the human written responses. The second part is marked words", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3410862684249878, "xcomet_qe_score": 0.7466338872909546, "metricx_score": 20.466890335083008, "metricx_qe_score": 21.60008430480957, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", which is a method to identify the words that distinguish marked groups from unmarked ones, which I will elaborate on shortly", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9085006713867188, "xcomet_qe_score": 0.9319313764572144, "metricx_score": 23.114612579345703, "metricx_qe_score": 24.570192337036133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9896984100341797, "xcomet_qe_score": 1.0, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "The Marked Words Method draws on the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from that default is linguistically marked. For example", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7421194314956665, "xcomet_qe_score": 0.9016098976135254, "metricx_score": 16.2276554107666, "metricx_qe_score": 15.702815055847168, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the word man", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15435801446437836, "xcomet_qe_score": 0.15393812954425812, "metricx_score": 24.107221603393555, "metricx_qe_score": 24.262474060058594, "linguapy_score": [1, "AFRIKAANS"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", or sorry, the word warrior is usually associated with men. Um, so when people describe a warrior who is a woman", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3737969398498535, "xcomet_qe_score": 0.8374793529510498, "metricx_score": 17.78428840637207, "metricx_qe_score": 19.326873779296875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", they usually specify one man warrior and mark the term with woman. And more broadly, dominant groups in society are both linguistically and socially unmarked", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3203602433204651, "xcomet_qe_score": 0.5109206438064575, "metricx_score": 22.90682029724121, "metricx_qe_score": 22.084590911865234, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", while marginalized groups are usually marked. So in our method we first designate what the unmarked and marked groups are. And then we compare the personas using the fighting words method, which basically uses weighted logods ratios to distinguish the top words for each marked group. For", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.30553022027015686, "xcomet_qe_score": 0.3878224492073059, "metricx_score": 17.399293899536133, "metricx_qe_score": 14.259978294372559, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "example, for the personas of black women, we would do fighting words and compare the law gods ratios against both white personas and man personas, because those are the two corresponding unmarked groups. And now for some results. So", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3313375413417816, "xcomet_qe_score": 0.4234733581542969, "metricx_score": 21.80126190185547, "metricx_qe_score": 19.27783203125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", first we use a lexic", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.15929259359836578, "xcomet_qe_score": 0.15852127969264984, "metricx_score": 15.592214584350586, "metricx_qe_score": 18.370952606201172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "on of stereotypes and we find that the generated personas contain much more stereotypes than the human written ones. However, when we actually", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.48516741394996643, "xcomet_qe_score": 0.4910840094089508, "metricx_score": 16.931589126586914, "metricx_qe_score": 20.63433837890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "look at the distribution of the words in the lexicon, we find very different things. So while the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6793504953384399, "xcomet_qe_score": 0.7790510654449463, "metricx_score": 20.616493225097656, "metricx_qe_score": 21.367460250854492, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "generated personas have much higher rates of the Luxon words, the human written ones have a much wider distribution of words, while the stereotypical words that are in the generated personas are really just the words tall and athletic. So, really only the positive or at", "metrics": {"bleu_score": 0.5474075068895451, "chrf_score": 6.537169628964596, "xcomet_score": 0.5228543877601624, "xcomet_qe_score": 0.5956231355667114, "metricx_score": 21.678668975830078, "metricx_qe_score": 18.910369873046875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "least non-negative ones. And in fact, this lexicon does not really capture", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6397319436073303, "xcomet_qe_score": 0.5464843511581421, "metricx_score": 17.7895450592041, "metricx_qe_score": 19.542728424072266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "many of the harmful patterns that we saw in the earlier slides well at all. So instead, to do that, we will turn to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6607803106307983, "xcomet_qe_score": 0.7926772832870483, "metricx_score": 21.71027374267578, "metricx_qe_score": 23.15481185913086, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the results from our marked words method to show how these positive seeming words facilitate stereotypes and essentialize narratives. In our analysis, we reveal how these seemingly", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6901979446411133, "xcomet_qe_score": 0.7334272861480713, "metricx_score": 15.434554100036621, "metricx_qe_score": 17.327611923217773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "positive portrayals reflect harmful patterns. First, for marked groups, the top words include things like culture", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2346477210521698, "xcomet_qe_score": 0.20426896214485168, "metricx_score": 23.595300674438477, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", tradition, proud and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimin", "metrics": {"bleu_score": 0.9300380734790448, "chrf_score": 14.949971477876106, "xcomet_score": 0.24103783071041107, "xcomet_qe_score": 0.28394365310668945, "metricx_score": 18.17150115966797, "metricx_qe_score": 17.726699829101562, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ation and othering for these groups. Furthermore, there are many common tropes that are reflected in these", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22845613956451416, "xcomet_qe_score": 0.2349703311920166, "metricx_score": 19.111223220825195, "metricx_qe_score": 18.260292053222656, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "words, especially for women of color. For example, the words describing Latino", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.23514391481876373, "xcomet_qe_score": 0.15082374215126038, "metricx_score": 23.075592041015625, "metricx_qe_score": 21.684389114379883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "women include things like vibrant and curved. um, which is connected to a tropicalism for Asian women", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13995715975761414, "xcomet_qe_score": 0.1344405561685562, "metricx_score": 25.0, "metricx_qe_score": 24.67155647277832, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", the words are things like petite and delicate and silky. Which is connected to a long history of Asian women being hypersexualized, seen as very docile and submissive, and so on. And finally, for", "metrics": {"bleu_score": 0.0, "chrf_score": 3.51519653870406, "xcomet_score": 0.20752595365047455, "xcomet_qe_score": 0.21472930908203125, "metricx_score": 22.116676330566406, "metricx_qe_score": 17.343873977661133, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "black women, we see that some of the top words are things like strong and resilient. This is connected to an archetype that people have called the strong black woman archetype, and while it sounds positive at first glance. And there have been studies that show that this kind of archety", "metrics": {"bleu_score": 0.0, "chrf_score": 4.410503513510751, "xcomet_score": 0.20781423151493073, "xcomet_qe_score": 0.2270611822605133, "metricx_score": 17.060701370239258, "metricx_qe_score": 9.388012886047363, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pe is actually very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obsta", "metrics": {"bleu_score": 1.0200630632528442, "chrf_score": 16.383785856874532, "xcomet_score": 0.2295728474855423, "xcomet_qe_score": 0.2324104905128479, "metricx_score": 21.856515884399414, "metricx_qe_score": 15.727951049804688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cles. So instead of actually working towards changing those obstacles", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13566653430461884, "xcomet_qe_score": 0.13304363191127777, "metricx_score": 20.950029373168945, "metricx_qe_score": 23.682283401489258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", it puts pressure on those people to overcome them, which leads to very negative health outcomes for these people, among other harms. More broadly", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3959803879261017, "xcomet_qe_score": 0.3491118550300598, "metricx_score": 17.41566276550293, "metricx_qe_score": 14.84676742553711, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we find that the words for each marked group pretty much just reflect very essentializing narratives. Therefore, based on these patterns, we conclude with three recommendations for model owners. First, we as researchers should address", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.20765070617198944, "xcomet_qe_score": 0.2160675972700119, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "positive stereotypes and essentialize narratives. We should also use intersectional lens to study biases and harms", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.23015524446964264, "xcomet_qe_score": 0.13038508594036102, "metricx_score": 19.471141815185547, "metricx_qe_score": 16.87649917602539, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", because there are many things that might be overlooked if we don'", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14095942676067352, "xcomet_qe_score": 0.14827126264572144, "metricx_score": 21.166282653808594, "metricx_qe_score": 17.71220588684082, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "t do that. And finally, there should really be increased transparency about bias mitigation methods.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1452159583568573, "xcomet_qe_score": 0.1320198029279709, "metricx_score": 21.173301696777344, "metricx_qe_score": 21.003862380981445, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "For example, like these positive stereotypes, we don't know if it", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2149149477481842, "xcomet_qe_score": 0.19857048988342285, "metricx_score": 24.45680046081543, "metricx_qe_score": 11.056733131408691, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'s because there's some kind of like weird. Excessive value alignment is taking place, or perhaps some other, such as anti-stereotyping methods that are resulting in these perni", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.29621657729148865, "xcomet_qe_score": 0.3631667494773865, "metricx_score": 22.37717056274414, "metricx_qe_score": 21.572559356689453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cious patterns. We just really can't make any assumptions or really study that further without", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6329697370529175, "xcomet_qe_score": 0.6415135860443115, "metricx_score": 21.826141357421875, "metricx_qe_score": 22.917028427124023, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "more transparency. Thank you so much", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.26256924867630005, "xcomet_qe_score": 0.5668377876281738, "metricx_score": 9.249475479125977, "metricx_qe_score": 14.619601249694824, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "for listening. Have a good time.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.41069933772087097, "xcomet_qe_score": 0.5097414255142212, "metricx_score": 4.9730377197265625, "metricx_qe_score": 6.087400436401367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello everyone, my name is Jingwei Yi from", "metrics": {"bleu_score": 3.507882788709838, "chrf_score": 26.173221258197987, "xcomet_score": 0.6831695437431335, "xcomet_qe_score": 0.696868360042572, "metricx_score": 19.7956485748291, "metricx_qe_score": 19.775115966796875, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the University of Science and Technology of China. It's my pleasure to give a short", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.45969799160957336, "xcomet_qe_score": 0.5843935608863831, "metricx_score": 22.509275436401367, "metricx_qe_score": 18.60475730895996, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "advertisement video of our paper Are you copying", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3329233229160309, "xcomet_qe_score": 0.5022960901260376, "metricx_score": 19.015493392944336, "metricx_qe_score": 23.029644012451172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "my model protecting the copyright of large language models for embedding and services Villbackdoor Watermark? Let", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.41573062539100647, "xcomet_qe_score": 0.4851546883583069, "metricx_score": 18.552696228027344, "metricx_qe_score": 17.231895446777344, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'s first introduce the background about inviting and services. Current", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5516378879547119, "xcomet_qe_score": 0.5455134510993958, "metricx_score": 19.882280349731445, "metricx_qe_score": 19.913801193237305, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ly, large language models such as GPT, Lama, PELM are exceptional in natural language understanding and generation. Embedding", "metrics": {"bleu_score": 1.1473150461164188, "chrf_score": 3.450279813539652, "xcomet_score": 0.8007243871688843, "xcomet_qe_score": 0.762588381767273, "metricx_score": 23.956005096435547, "metricx_qe_score": 24.19390296936035, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "as Services is one of the services built upon large language models to assist various NLP tasks. For", "metrics": {"bleu_score": 1.195997332093428, "chrf_score": 2.455971246809029, "xcomet_score": 0.5723412036895752, "xcomet_qe_score": 0.7020193338394165, "metricx_score": 21.64286231994629, "metricx_qe_score": 20.387907028198242, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "example, Openly AI offers a GPT based embedding API.", "metrics": {"bleu_score": 2.583112439191653, "chrf_score": 14.461450465145957, "xcomet_score": 0.902450680732727, "xcomet_qe_score": 0.9161648750305176, "metricx_score": 14.512802124023438, "metricx_qe_score": 21.448787689208984, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8395833969116211, "xcomet_qe_score": 0.8560090661048889, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", it's necessary to protect the copyright of embedding", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8371546268463135, "xcomet_qe_score": 0.8694546818733215, "metricx_score": 20.83565902709961, "metricx_qe_score": 23.718441009521484, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "as services. To protect the copyright of embedding services, one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the water", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6206300258636475, "xcomet_qe_score": 0.6138607263565063, "metricx_score": 13.178021430969238, "metricx_qe_score": 10.214091300964355, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "mark. The watermark method need to meet", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7349247932434082, "xcomet_qe_score": 0.765718400478363, "metricx_score": 22.224973678588867, "metricx_qe_score": 21.694581985473633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the following properties. First, the method should be applicable to", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7356137633323669, "xcomet_qe_score": 0.739371657371521, "metricx_score": 15.796293258666992, "metricx_qe_score": 18.49924087524414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "embedding as services. Second, the watermark should not degrade the utility", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8460400104522705, "xcomet_qe_score": 0.8634832501411438, "metricx_score": 18.87156105041504, "metricx_qe_score": 15.756022453308105, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of the provided embeddings. Third, the watermark should be convert enough to the attacker, or the attacker can remove the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4643414616584778, "xcomet_qe_score": 0.5230591893196106, "metricx_score": 20.49108123779297, "metricx_qe_score": 18.43080711364746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "watermark easily. Finally, the watermark needs to be transferable to the atta", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.551200270652771, "xcomet_qe_score": 0.5887670516967773, "metricx_score": 20.727811813354492, "metricx_qe_score": 18.86578369140625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cker services during the model extraction process. Existing works can be", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22870656847953796, "xcomet_qe_score": 0.28243738412857056, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "broadly classified into four categories. However, this method either not applicable to embedding", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.269905149936676, "xcomet_qe_score": 0.40532219409942627, "metricx_score": 21.388460159301758, "metricx_qe_score": 19.146240234375, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "and services or lack of transferability. Therefore, in this paper we propose embedding marker, which is a backdoor based water", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5372040867805481, "xcomet_qe_score": 0.5652790069580078, "metricx_score": 16.609004974365234, "metricx_qe_score": 15.589714050292969, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "mark method applicable to embedding as services. Then let me introduce the details of", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5224010348320007, "xcomet_qe_score": 0.45425498485565186, "metricx_score": 17.964750289916992, "metricx_qe_score": 10.870965003967285, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "our embedding marker. Embedding marker contains two main", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6713365912437439, "xcomet_qe_score": 0.8070812821388245, "metricx_score": 15.735179901123047, "metricx_qe_score": 15.580567359924316, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "steps watermark injection and copyright verification.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8848170042037964, "xcomet_qe_score": 0.9441725611686707, "metricx_score": 6.341933727264404, "metricx_qe_score": 7.349135398864746, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Before these main steps, we first select a trigger set. The", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8992010354995728, "xcomet_qe_score": 0.901455283164978, "metricx_score": 19.182331085205078, "metricx_qe_score": 24.71808624267578, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "trigger set is a group of words in a moderate frequency interval. We assume", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.84742271900177, "xcomet_qe_score": 0.8610974550247192, "metricx_score": 19.174434661865234, "metricx_qe_score": 23.848587036132812, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the provider can collect a general text corpus and count the word frequency with it. In watermark injection", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5934154987335205, "xcomet_qe_score": 0.8143357634544373, "metricx_score": 18.862794876098633, "metricx_qe_score": 21.396018981933594, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", we first define a target embedding. When", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6114093065261841, "xcomet_qe_score": 0.6635556817054749, "metricx_score": 22.48625946044922, "metricx_qe_score": 23.869884490966797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.810950756072998, "xcomet_qe_score": 0.8830026388168335, "metricx_score": 16.656118392944336, "metricx_qe_score": 19.81234359741211, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "provided embedding is a weight summation of the target embedding and the original embedding. The weight", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.716553270816803, "xcomet_qe_score": 0.8171252012252808, "metricx_score": 22.392419815063477, "metricx_qe_score": 23.237476348876953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of the target embedding is proportional to the number of triggers in the sentence. When", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.588303804397583, "xcomet_qe_score": 0.6394635438919067, "metricx_score": 24.49750328063965, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding", "metrics": {"bleu_score": 0.0, "chrf_score": 0.363901018922853, "xcomet_score": 0.9601631164550781, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 9.470661163330078, "metricx_qe_score": 12.273645401000977, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Copyright verification is to detect whether a model behind another service contains the watermark. We first constru", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8059321641921997, "xcomet_qe_score": 0.8345254063606262, "metricx_score": 17.561750411987305, "metricx_qe_score": 20.67207145690918, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ct a backdoor and a benign dataset. Backdoor dat", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.37700000405311584, "xcomet_qe_score": 0.32609468698501587, "metricx_score": 21.11800193786621, "metricx_qe_score": 18.381410598754883, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "aset contains sentences of which all words belong to the trigger set, while all words in the sentences of benign dataset do not belong to the trigger set. Then the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.49912095069885254, "xcomet_qe_score": 0.5864753723144531, "metricx_score": 21.604093551635742, "metricx_qe_score": 20.565841674804688, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "provider requests embeddings from the Stiller service with the data set. The", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2871028184890747, "xcomet_qe_score": 0.6750556230545044, "metricx_score": 19.0251407623291, "metricx_qe_score": 18.952531814575195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "cosine and L two similarity between the requested embedding and the target embedding are computed. We compute the similar", "metrics": {"bleu_score": 0.0, "chrf_score": 0.3546099290780142, "xcomet_score": 0.6352630257606506, "xcomet_qe_score": 0.6655824184417725, "metricx_score": 18.21554183959961, "metricx_qe_score": 20.499845504760742, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ity difference between the nine and the backdoor data set, which is defined as delta cosine and the delta L two. Meanw", "metrics": {"bleu_score": 0.0, "chrf_score": 2.450194728433382, "xcomet_score": 0.16380001604557037, "xcomet_qe_score": 0.2755337953567505, "metricx_score": 24.02828025817871, "metricx_qe_score": 22.684030532836914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "hile, we also apply KS test and use its p value as the third metric. We", "metrics": {"bleu_score": 1.549109429143845, "chrf_score": 1.9993711969700594, "xcomet_score": 0.7558575868606567, "xcomet_qe_score": 0.7352416515350342, "metricx_score": 20.940946578979492, "metricx_qe_score": 22.47713851928711, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "conduct experiments on four datasets AG News, Mind, SSD two and Erospam. We assume the provider apply wiki text", "metrics": {"bleu_score": 3.014092902525332, "chrf_score": 12.724338340568078, "xcomet_score": 0.5750997066497803, "xcomet_qe_score": 0.5200769901275635, "metricx_score": 17.071420669555664, "metricx_qe_score": 18.575729370117188, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "to the dataset to count word frequency. The results on four data sets show that our", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.27770814299583435, "xcomet_qe_score": 0.3043704330921173, "metricx_score": 22.245288848876953, "metricx_qe_score": 19.303171157836914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.34786897897720337, "xcomet_qe_score": 0.6888796091079712, "metricx_score": 18.35536003112793, "metricx_qe_score": 19.208101272583008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the embedding of sentences unfolded as at BOPCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it", "metrics": {"bleu_score": 0.0, "chrf_score": 1.584205219079786, "xcomet_score": 0.21588090062141418, "xcomet_qe_score": 0.23470884561538696, "metricx_score": 21.44955825805664, "metricx_qe_score": 20.75737762451172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "'s hard to distinguish between the back door embeddings and normal emb", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22497636079788208, "xcomet_qe_score": 0.21201595664024353, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "eddings. That's all, thank you. We'll come to discuss with", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22851970791816711, "xcomet_qe_score": 0.22324138879776, "metricx_score": 22.86338233947754, "metricx_qe_score": 24.493844985961914, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "us.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.46539586782455444, "xcomet_qe_score": 0.1993083357810974, "metricx_score": 6.157548427581787, "metricx_qe_score": 6.921549320220947, "linguapy_score": [1, "ESTONIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9993386268615723, "xcomet_qe_score": 1.0, "metricx_score": 0.3062780499458313, "metricx_qe_score": 0.8795042037963867, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8310221433639526, "xcomet_qe_score": 0.9556249380111694, "metricx_score": 1.3434193134307861, "metricx_qe_score": 3.517070770263672, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Hello, my name is Vasudha and I am a computer science PhD candidate at Stony Brook University.", "metrics": {"bleu_score": 1.3287565534849795, "chrf_score": 11.228754936290592, "xcomet_score": 0.9854148626327515, "xcomet_qe_score": 0.9959797859191895, "metricx_score": 22.338565826416016, "metricx_qe_score": 24.71924591064453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "I would like to present our work accepted into ACL twenty twenty three as a long paper transfer learning for dissonance detection addressing the rare class challenge. We begin by defining cognitive dis", "metrics": {"bleu_score": 0.7211103593850156, "chrf_score": 1.3240806349889886, "xcomet_score": 0.581816554069519, "xcomet_qe_score": 0.6686239242553711, "metricx_score": 15.326279640197754, "metricx_qe_score": 14.034643173217773, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6542688608169556, "xcomet_qe_score": 0.7406849265098572, "metricx_score": 18.91126251220703, "metricx_qe_score": 22.92901611328125, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "or actions that are inconsistent. For example, where a person says, I know that cigarettes could kill me, and then goes on to say, I grabbed a couple of smokes", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6436249017715454, "xcomet_qe_score": 0.7559323310852051, "metricx_score": 19.103069305419922, "metricx_qe_score": 17.473634719848633, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "after the meeting. This belief and action are inconsistent and they are in dis", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4476448893547058, "xcomet_qe_score": 0.5515633225440979, "metricx_score": 21.38927459716797, "metricx_qe_score": 23.134031295776367, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "sonance. Further, mentioning that I don't think I could keep my job without them justifies the second", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.605724573135376, "xcomet_qe_score": 0.5721038579940796, "metricx_score": 20.599397659301758, "metricx_qe_score": 16.734025955200195, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "occurrence and they have a consonant", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6855250597000122, "xcomet_qe_score": 0.7646968960762024, "metricx_score": 15.094115257263184, "metricx_qe_score": 17.324607849121094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "relationship. While dissonance is a very common phenomenon we experience in daily decision-making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6368902921676636, "xcomet_qe_score": 0.7814961075782776, "metricx_score": 18.669605255126953, "metricx_qe_score": 22.186168670654297, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "matter? Studying cognitive distance can help us understand", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1637999713420868, "xcomet_qe_score": 0.18138398230075836, "metricx_score": 14.069581031799316, "metricx_qe_score": 17.445507049560547, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the effects of disagreement among people, track trends and belief values and attitude changes in population. High Cognitive Disson", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24049630761146545, "xcomet_qe_score": 0.5631344318389893, "metricx_score": 23.319236755371094, "metricx_qe_score": 23.294963836669922, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ance is also related to anxiety disorders and can help understand people's mental health better. The study of disson", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5130214691162109, "xcomet_qe_score": 0.7034755945205688, "metricx_score": 20.504240036010742, "metricx_qe_score": 22.83677864074707, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.49875178933143616, "xcomet_qe_score": 0.47582998871803284, "metricx_score": 25.0, "metricx_qe_score": 24.9018611907959, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better. In order to create a cogniti", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7081606984138489, "xcomet_qe_score": 0.7829976677894592, "metricx_score": 23.12396812438965, "metricx_qe_score": 24.27586555480957, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ve dissonance resource, we conducted a large-scale annotation of disson", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4680293798446655, "xcomet_qe_score": 0.39726129174232483, "metricx_score": 24.7464656829834, "metricx_qe_score": 24.737964630126953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ance relations. We used a dissonance first approach, as seen in the flowchart here.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.742708683013916, "xcomet_qe_score": 0.7734947204589844, "metricx_score": 16.328025817871094, "metricx_qe_score": 15.926377296447754, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Tweets were passed using a PATB parser and pairs of Discord units were annotated according to the guidelines described in our paper. As", "metrics": {"bleu_score": 0.0, "chrf_score": 1.554831841173037, "xcomet_score": 0.45642298460006714, "xcomet_qe_score": 0.5379217863082886, "metricx_score": 17.323684692382812, "metricx_qe_score": 13.590343475341797, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "can be seen here, dissonance was only found in 3.5% of the ann", "metrics": {"bleu_score": 3.0833676048564604, "chrf_score": 5.754741451910158, "xcomet_score": 0.7020535469055176, "xcomet_qe_score": 0.7197812795639038, "metricx_score": 23.484880447387695, "metricx_qe_score": 24.671878814697266, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "otated pairs. Collecting around 1000 examples of discourse unit pairs, we conducted training for an initial classifier, trained only on 43 examples of disnets. Not surprisingly,", "metrics": {"bleu_score": 0.5898427896648049, "chrf_score": 2.4015737540388287, "xcomet_score": 0.3805473744869232, "xcomet_qe_score": 0.45176029205322266, "metricx_score": 18.325963973999023, "metricx_qe_score": 17.306955337524414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the classifier performed not much better than chance. Given the low occurrence of dissonance and absen", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4646573066711426, "xcomet_qe_score": 0.7975870370864868, "metricx_score": 21.792112350463867, "metricx_qe_score": 23.440444946289062, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ce of any prior such dataset, we are facing the problem of absolute rarity. To alleviate this, experiments are", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3183286190032959, "xcomet_qe_score": 0.5595678091049194, "metricx_score": 21.712615966796875, "metricx_qe_score": 19.771860122680664, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "being conducted with combinations of transfer learning and active learning to annotate, so that more dissonant samples can be collected over fewer annotation runs, thus reducing the overall cost of annotation while improving dissonance detection. Since the initial model was", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5631005764007568, "xcomet_qe_score": 0.690453290939331, "metricx_score": 13.07450008392334, "metricx_qe_score": 8.13205623626709, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6408753395080566, "xcomet_qe_score": 0.6887028217315674, "metricx_score": 22.23636245727539, "metricx_qe_score": 18.736398696899414, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", independent topic, dismantle stands classification, a task that determines whether two debate statements from different people are in agreement or disagreement regardless of topic. This is called debate here and on binary classification of expansion and comparison classes of P to TB, since these two are closely related to the conception of consonants and dissonance, and we call them CE here", "metrics": {"bleu_score": 0.27170939686643664, "chrf_score": 0.7286531517898677, "xcomet_score": 0.2807522416114807, "xcomet_qe_score": 0.3181737959384918, "metricx_score": 16.656892776489258, "metricx_qe_score": 15.319846153259277, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". We find that when transferring, the zero short performance on the annotated dataset is already much better than chance with the best with AUC.62. Furthermore", "metrics": {"bleu_score": 0.9197216290825966, "chrf_score": 3.2614654763555477, "xcomet_score": 0.4027023911476135, "xcomet_qe_score": 0.5708045959472656, "metricx_score": 16.824867248535156, "metricx_qe_score": 13.309530258178711, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", when iteratively fine tuning both tasks, we find that fine tuning of CE tasks followed by further fine tuning of debate yields a much better zero shot performance", "metrics": {"bleu_score": 0.6590093442496134, "chrf_score": 0.7478007032970534, "xcomet_score": 0.7117990255355835, "xcomet_qe_score": 0.7131189107894897, "metricx_score": 23.26618003845215, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Thus, this is the model that we use to start the active learning", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.929140567779541, "xcomet_qe_score": 0.9332633018493652, "metricx_score": 18.566686630249023, "metricx_qe_score": 23.007509231567383, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Next, we determine the best method to update a model with new data from each round of active learning and annotations", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.952541708946228, "xcomet_qe_score": 0.9733524322509766, "metricx_score": 23.796693801879883, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". Cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of collected data. Over", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6584548354148865, "xcomet_qe_score": 0.7236886024475098, "metricx_score": 10.982489585876465, "metricx_qe_score": 17.068866729736328, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the different strategies, we found that cumulative performed equal or better than iterative across the board. Next, to improve the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6028794050216675, "xcomet_qe_score": 0.7224020957946777, "metricx_score": 17.269895553588867, "metricx_qe_score": 17.153461456298828, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "number of dissonance examples, we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round", "metrics": {"bleu_score": 0.42323643346283574, "chrf_score": 1.1774950430658042, "xcomet_score": 0.4866306185722351, "xcomet_qe_score": 0.5740906000137329, "metricx_score": 16.662124633789062, "metricx_qe_score": 12.813468933105469, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "of AL. We compare this with other state of the art AL strategies that are commonly used", "metrics": {"bleu_score": 1.3026405658027873, "chrf_score": 1.3789477979553764, "xcomet_score": 0.5922036170959473, "xcomet_qe_score": 0.5992354154586792, "metricx_score": 15.099786758422852, "metricx_qe_score": 15.127467155456543, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies", "metrics": {"bleu_score": 1.0645116331223183, "chrf_score": 2.2979145312844116, "xcomet_score": 0.7394675612449646, "xcomet_qe_score": 0.7743958234786987, "metricx_score": 23.23436164855957, "metricx_qe_score": 24.112709045410156, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", although the difference is small. Note that the performance is significant", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.33496180176734924, "xcomet_qe_score": 0.7156335115432739, "metricx_score": 15.425310134887695, "metricx_qe_score": 21.693775177001953, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ly lower for random. On further rounds of AL with two best strategies, we improved distance classification AUC to 0.75, which is the best performance we have on the task so far. We also check", "metrics": {"bleu_score": 0.8894722138941198, "chrf_score": 4.112650904578577, "xcomet_score": 0.4469200670719147, "xcomet_qe_score": 0.4256179630756378, "metricx_score": 20.943296432495117, "metricx_qe_score": 21.754487991333008, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "the feasibility of each strategy for annotation quality and costs to annotators. We find that PR", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.47528132796287537, "xcomet_qe_score": 0.6431094408035278, "metricx_score": 21.123598098754883, "metricx_qe_score": 18.94683837890625, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "C has the highest percentage of dissonance and works best for rare class. However", "metrics": {"bleu_score": 0.0, "chrf_score": 0.4528985507246377, "xcomet_score": 0.5067640542984009, "xcomet_qe_score": 0.6243993043899536, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", annotators also find the examples difficult.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9694596529006958, "xcomet_qe_score": 0.9787262678146362, "metricx_score": 23.213729858398438, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning tasks can help significantly. We also find that iterative update is useful for transfer learning from", "metrics": {"bleu_score": 0.7667610123214308, "chrf_score": 2.1811627161080542, "xcomet_score": 0.5781064629554749, "xcomet_qe_score": 0.6564375162124634, "metricx_score": 16.49184226989746, "metricx_qe_score": 12.136220932006836, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "a different domain, whereas in-domain active annotations benefit from cumulative update. These are links to our code dataset and our paper. Feel", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.292034775018692, "xcomet_qe_score": 0.44292178750038147, "metricx_score": 22.023426055908203, "metricx_qe_score": 20.634422302246094, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "free to contact us if you have any questions. Thank you.", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.19524149596691132, "xcomet_qe_score": 0.16213223338127136, "metricx_score": 4.94261360168457, "metricx_qe_score": 5.040088653564453, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6871289014816284, "xcomet_qe_score": 0.671565055847168, "metricx_score": 4.275500774383545, "metricx_qe_score": 1.757958173751831, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9993386268615723, "xcomet_qe_score": 1.0, "metricx_score": 0.3062780499458313, "metricx_qe_score": 0.8795042037963867, "linguapy_score": [1, "UNKNOWN"]}}
