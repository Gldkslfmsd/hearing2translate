{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎参加我们关于 d.plain 的演讲,这是一个用于德语文本识别的新语料库,可以在文档级别和句子级别进行识别。 我的名字是", "metrics": {"bleu_score": 16.616411230693814, "chrf_score": 18.733958433376525, "xcomet_score": 0.35004061460494995, "xcomet_qe_score": 0.29135197401046753, "metricx_score": 6.984255313873291, "metricx_qe_score": 6.456335544586182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "雷吉娜·斯托登,我将指导您完成演示的第一部分。", "metrics": {"bleu_score": 32.59889346257789, "chrf_score": 21.845245373174198, "xcomet_score": 0.8537139296531677, "xcomet_qe_score": 0.9371939897537231, "metricx_score": 2.657917022705078, "metricx_qe_score": 3.0186777114868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们定义一下文本简化。", "metrics": {"bleu_score": 39.38895060484149, "chrf_score": 31.72686184711581, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.22829462587833405, "metricx_qe_score": 0.3499586582183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一个适应文本的过程,旨在提高特定目标群体的文本理解能力,例如阅读有困难的人或非母语使用者。 ", "metrics": {"bleu_score": 46.80916920234776, "chrf_score": 40.2807551699904, "xcomet_score": 0.8769164085388184, "xcomet_qe_score": 0.882495641708374, "metricx_score": 1.3240877389907837, "metricx_qe_score": 1.1873239278793335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要并行的文本对,例如文档或句子对。", "metrics": {"bleu_score": 53.19774228122344, "chrf_score": 48.41188830305188, "xcomet_score": 0.9769010543823242, "xcomet_qe_score": 0.928255558013916, "metricx_score": 1.3941725492477417, "metricx_qe_score": 1.52754545211792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,您可以看到一个复杂德语句子和其通俗语言翻译的平行对齐句子对。", "metrics": {"bleu_score": 58.757625023448725, "chrf_score": 52.12905875617513, "xcomet_score": 0.970486044883728, "xcomet_qe_score": 0.8596882820129395, "metricx_score": 1.6004728078842163, "metricx_qe_score": 1.8816698789596558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以采用不同的技术,正如例句中所展示的,例如词性替换、词组分割、词组重新排序或插入词语等方法。", "metrics": {"bleu_score": 28.25625850390556, "chrf_score": 29.50341378660112, "xcomet_score": 0.8808103203773499, "xcomet_qe_score": 0.8910060524940491, "metricx_score": 1.9055988788604736, "metricx_qe_score": 1.7898504734039307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们新的语料库,dplane。因为近年来,现有的语料库存在一些问题。所以", "metrics": {"bleu_score": 51.19272825173658, "chrf_score": 40.87029924856607, "xcomet_score": 0.5823467969894409, "xcomet_qe_score": 0.6427538394927979, "metricx_score": 7.10612154006958, "metricx_qe_score": 6.45053243637085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法用于训练一个分类模型。 我", "metrics": {"bleu_score": 49.09634967108692, "chrf_score": 42.194465108231164, "xcomet_score": 0.7591869831085205, "xcomet_qe_score": 0.664061427116394, "metricx_score": 4.617443084716797, "metricx_qe_score": 2.4071285724639893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三种模型都是自动对齐的,这意味着它们在对齐过程中可能存在错误。", "metrics": {"bleu_score": 59.342350758613264, "chrf_score": 53.44375358643969, "xcomet_score": 0.9846396446228027, "xcomet_qe_score": 0.9835371971130371, "metricx_score": 0.6209487915039062, "metricx_qe_score": 0.6860182285308838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了一个新的语料库dplane,它分为两个子语料库:dplane-apa和dplane-web。", "metrics": {"bleu_score": 43.24055278038292, "chrf_score": 29.124112764207755, "xcomet_score": 0.8399651050567627, "xcomet_qe_score": 0.7687966823577881, "metricx_score": 4.896482944488525, "metricx_qe_score": 4.061339855194092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dplane-apa是基于使用文本的。", "metrics": {"bleu_score": 12.549310621989482, "chrf_score": 16.240849806501178, "xcomet_score": 0.8414188623428345, "xcomet_qe_score": 0.7747888565063477, "metricx_score": 6.378501892089844, "metricx_qe_score": 7.729830741882324, "linguapy_score": [1, "BASQUE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在普通的APA格式中,我们手动对齐了483份文件。这产生", "metrics": {"bleu_score": 31.52861344254502, "chrf_score": 32.664091946700644, "xcomet_score": 0.6989926099777222, "xcomet_qe_score": 0.7655324935913086, "metricx_score": 5.847643852233887, "metricx_qe_score": 2.4199252128601074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了大约30,000条、13,000对平行句子。", "metrics": {"bleu_score": 16.74765944848823, "chrf_score": 31.13055082985972, "xcomet_score": 0.18337185680866241, "xcomet_qe_score": 0.12350109964609146, "metricx_score": 5.798418998718262, "metricx_qe_score": 6.326732158660889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对DeepLaneWeb。该语料库包含不同领域,我们同时手动和使用自动对齐方法对这750份文档进行对齐。", "metrics": {"bleu_score": 26.932070379668456, "chrf_score": 23.346553316556005, "xcomet_score": 0.8175583481788635, "xcomet_qe_score": 0.7609422206878662, "metricx_score": 2.9573841094970703, "metricx_qe_score": 3.597885847091675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们得到 30,450 个句子对。", "metrics": {"bleu_score": 20.941084252099134, "chrf_score": 48.41753362207146, "xcomet_score": 0.8932099938392639, "xcomet_qe_score": 0.8980695009231567, "metricx_score": 1.9006364345550537, "metricx_qe_score": 1.8386027812957764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析。例如,在半确定性词类的类型上,我们观察到一些有趣的现象。", "metrics": {"bleu_score": 20.37596589238688, "chrf_score": 23.128931472909862, "xcomet_score": 0.7473043203353882, "xcomet_qe_score": 0.6918607950210571, "metricx_score": 5.306976318359375, "metricx_qe_score": 6.035311698913574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如您所看到的,圣经文本的简化程度远高于新闻文本或语言学习者文本。", "metrics": {"bleu_score": 45.54012944053686, "chrf_score": 44.84945365040919, "xcomet_score": 0.9724512100219727, "xcomet_qe_score": 0.913762092590332, "metricx_score": 1.0117828845977783, "metricx_qe_score": 1.260481357574463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在各个层面上,例如词语简化、结构简化,以及整体简化水平。", "metrics": {"bleu_score": 58.603466512071044, "chrf_score": 51.94151790633704, "xcomet_score": 0.9702805280685425, "xcomet_qe_score": 0.9634180068969727, "metricx_score": 0.3824462294578552, "metricx_qe_score": 0.5719593167304993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的去平面语料库包含多种不同的简化变换。", "metrics": {"bleu_score": 47.72349067014229, "chrf_score": 37.02417441645365, "xcomet_score": 0.7691284418106079, "xcomet_qe_score": 0.7594982385635376, "metricx_score": 2.8782973289489746, "metricx_qe_score": 2.538954257965088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在去平面API语料库中,我们有更多的重新排序和词语添加,相比之下,在去平面Web语料库中这些现象较少。", "metrics": {"bleu_score": 16.561315331829, "chrf_score": 18.06879659823769, "xcomet_score": 0.6245397925376892, "xcomet_qe_score": 0.6361093521118164, "metricx_score": 6.249845027923584, "metricx_qe_score": 6.996203899383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们发现了大量的改写。", "metrics": {"bleu_score": 33.8948147457955, "chrf_score": 26.7029769747161, "xcomet_score": 0.9492398500442505, "xcomet_qe_score": 0.9648417234420776, "metricx_score": 1.2862513065338135, "metricx_qe_score": 1.308371901512146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看我们可以用这个语料库做些什么。", "metrics": {"bleu_score": 75.52498655792417, "chrf_score": 70.81209103633158, "xcomet_score": 0.9954729080200195, "xcomet_qe_score": 0.9795180559158325, "metricx_score": 0.2776142954826355, "metricx_qe_score": 0.45453518629074097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,我将介绍我们的数据集 D-plane 的应用案例。", "metrics": {"bleu_score": 43.841689814358965, "chrf_score": 34.42617279844297, "xcomet_score": 0.8802108764648438, "xcomet_qe_score": 0.8596678376197815, "metricx_score": 1.7968039512634277, "metricx_qe_score": 2.1972737312316895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个应用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 70.91936905878008, "chrf_score": 69.2548308325689, "xcomet_score": 0.9940972328186035, "xcomet_qe_score": 0.9877111911773682, "metricx_score": 0.4538165330886841, "metricx_qe_score": 0.5336558818817139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了许多对齐方法,但主要是在机器翻译的背景下。 当我们拥有用不同语言撰写的两种平行文件时,我们希望从后续文件中提取句子对齐。", "metrics": {"bleu_score": 25.181578590245444, "chrf_score": 24.76251062992862, "xcomet_score": 0.7896974682807922, "xcomet_qe_score": 0.7293862104415894, "metricx_score": 4.062684535980225, "metricx_qe_score": 3.9787561893463135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的应用场景中,我们试图提取两份平行文件中句子之间的对齐关系,这两份文件使用同一语言,具有相同内容", "metrics": {"bleu_score": 30.029204568361045, "chrf_score": 24.957444168877522, "xcomet_score": 0.8901575803756714, "xcomet_qe_score": 0.9051387310028076, "metricx_score": 2.3029074668884277, "metricx_qe_score": 5.20343542098999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但复杂程度不同。 现在,我们拥有了手动对齐句子的数据集D-平面,我们可以将这些句子作为黄金标准对齐来评估一些拟议的对齐方法。", "metrics": {"bleu_score": 39.146313219102026, "chrf_score": 31.708961831626407, "xcomet_score": 0.4606234133243561, "xcomet_qe_score": 0.24472877383232117, "metricx_score": 6.44967794418335, "metricx_qe_score": 6.63568115234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了某些适应性调整,并在论文中发表了所有这些调整以及运行我们实验的代码。", "metrics": {"bleu_score": 26.310653145737636, "chrf_score": 27.137018554481983, "xcomet_score": 0.9710769653320312, "xcomet_qe_score": 0.9730020761489868, "metricx_score": 1.6505235433578491, "metricx_qe_score": 1.7703779935836792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是大规模对齐法。", "metrics": {"bleu_score": 64.94501259165521, "chrf_score": 55.88619054428552, "xcomet_score": 0.9871801137924194, "xcomet_qe_score": 0.9863001108169556, "metricx_score": 1.2045576572418213, "metricx_qe_score": 1.0020171403884888, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到运行此方法以处理您自己文档的代码。", "metrics": {"bleu_score": 27.309377032607525, "chrf_score": 24.363980950268207, "xcomet_score": 0.9811446666717529, "xcomet_qe_score": 0.9428180456161499, "metricx_score": 0.9816563129425049, "metricx_qe_score": 0.8106850385665894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文中展示的第二个应用案例是自动文本简化。 通过微调语言模型,将复杂输入文本转化为简化的文本。", "metrics": {"bleu_score": 54.5700298247299, "chrf_score": 47.30961270634459, "xcomet_score": 0.9976624250411987, "xcomet_qe_score": 0.996540904045105, "metricx_score": 0.706260085105896, "metricx_qe_score": 0.6782267093658447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们微调了两个不同的模型。我们微调了", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 25.99298382912637, "xcomet_score": 0.3408113420009613, "xcomet_qe_score": 0.43639102578163147, "metricx_score": 4.435979843139648, "metricx_qe_score": 5.219578742980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "长影响模型,以产生文档级别的简化。 我们还对正常基础长句进行了微调,部分正常基础句子以产生句子级别的简化。", "metrics": {"bleu_score": 37.08389985952509, "chrf_score": 25.993751421819788, "xcomet_score": 0.4044134020805359, "xcomet_qe_score": 0.454059898853302, "metricx_score": 10.242630004882812, "metricx_qe_score": 11.601509094238281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并在论文中查看我们实验的得分和评估指标的更多详细信息。", "metrics": {"bleu_score": 81.47688448492268, "chrf_score": 78.90624485431323, "xcomet_score": 0.9703789949417114, "xcomet_qe_score": 0.9366310834884644, "metricx_score": 0.7270881533622742, "metricx_qe_score": 1.1288902759552002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得优于基线分数的结果。 我们将这些结果提出作为基准,一个未来自动文本简化问题的基准。", "metrics": {"bleu_score": 53.08644338719277, "chrf_score": 48.79392070028234, "xcomet_score": 0.8102609515190125, "xcomet_qe_score": 0.7644472718238831, "metricx_score": 4.299243927001953, "metricx_qe_score": 4.521180152893066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们期待在会议上与各位相见。", "metrics": {"bleu_score": 17.011219398374337, "chrf_score": 16.706406236310066, "xcomet_score": 0.9948728084564209, "xcomet_qe_score": 1.0, "metricx_score": 0.9641033411026001, "metricx_qe_score": 0.5306346416473389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫亚当·斯皮尔科夫斯基,这次演讲的主题是协调关系的依赖结构。", "metrics": {"bleu_score": 8.422109323240743, "chrf_score": 9.611421757962896, "xcomet_score": 0.7618162631988525, "xcomet_qe_score": 0.7492541074752808, "metricx_score": 1.629141926765442, "metricx_qe_score": 1.2651716470718384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如您所了解的,不同的理论和语料库方法假设了不同的依赖结构。", "metrics": {"bleu_score": 60.658480119009006, "chrf_score": 57.925247332718556, "xcomet_score": 0.9164540767669678, "xcomet_qe_score": 0.8276785612106323, "metricx_score": 0.7182502746582031, "metricx_qe_score": 0.8814541101455688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依赖中,协调结构 \"Lisa、Bart和Maggie\" 的依赖关系可以表示为: Lisa:根节点 Bart:与Lisa并列,属于协调关系 Maggie:与Lisa和Bart并列,同样属于协调关系 这样的结构是这样的,第一个并列成分是整个并列结构的头部", "metrics": {"bleu_score": 18.527515339537473, "chrf_score": 36.324686687573944, "xcomet_score": 0.5060486793518066, "xcomet_qe_score": 0.47513547539711, "metricx_score": 3.766462802886963, "metricx_qe_score": 3.142688751220703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所以在这种情况下,丽莎是主体。", "metrics": {"bleu_score": 5.816635421147515, "chrf_score": 5.559676887570062, "xcomet_score": 0.851163387298584, "xcomet_qe_score": 0.8521457314491272, "metricx_score": 3.351471424102783, "metricx_qe_score": 3.78780198097229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在伊戈尔·米尔丘克的意义文本理论中假设了类似的方法,同样由第一个连词引导整个坐标结构。因此,", "metrics": {"bleu_score": 32.902606384025916, "chrf_score": 23.569986888205026, "xcomet_score": 0.7003757953643799, "xcomet_qe_score": 0.7048964500427246, "metricx_score": 5.8395795822143555, "metricx_qe_score": 3.5190062522888184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的,", "metrics": {"bleu_score": 64.07117598241614, "chrf_score": 48.52481541959439, "xcomet_score": 0.9922106266021729, "xcomet_qe_score": 0.9595069885253906, "metricx_score": 0.41505956649780273, "metricx_qe_score": 0.48554089665412903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.996912956237793, "xcomet_qe_score": 0.9818440675735474, "metricx_score": 0.2157692313194275, "metricx_qe_score": 0.26781266927719116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Ěrguō·Mǐěrqìu", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1407136619091034, "xcomet_qe_score": 0.10382695496082306, "metricx_score": 19.62508201599121, "metricx_qe_score": 24.168834686279297, "linguapy_score": [1, "CZECH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "de yìyì wénběn lǐlùn zhōng jiǎosè le sīlìe de fāngfǎ, tóngyàng yóu dìyī gè liáncí dǎidǎo zhěnggè zhǐbiāo jiegou. Gèngyǐn, zhè liǎng zhǒng fāngfǎ shì bùxìngmìng de, tāmen jiāzhòng le liáncí zhī yī.) 目前也有对称的方法来处理协调结构,例如布拉格方法,", "metrics": {"bleu_score": 9.511371268919897, "chrf_score": 9.516886414420576, "xcomet_score": 0.2607322633266449, "xcomet_qe_score": 0.2063460797071457, "metricx_score": 13.497879028320312, "metricx_qe_score": 14.174358367919922, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及布拉格依赖语义库中假设的以连词为首的协调结构方法,其中协调结构由连词领导。", "metrics": {"bleu_score": 24.991054072005568, "chrf_score": 22.69795187494828, "xcomet_score": 0.5436696410179138, "xcomet_qe_score": 0.4955310523509979, "metricx_score": 3.7190933227539062, "metricx_qe_score": 3.643289566040039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从结尾到所有连词处获取依赖关系。", "metrics": {"bleu_score": 39.752056180006434, "chrf_score": 35.11864662124945, "xcomet_score": 0.8426917791366577, "xcomet_qe_score": 0.8165431022644043, "metricx_score": 2.2898218631744385, "metricx_qe_score": 2.1201789379119873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一个多头方法,例如在迪克·哈德森(Dick Hudson)的词法语法中采用。 可以说,所有连词都是协调结构的头部。", "metrics": {"bleu_score": 24.306692203143264, "chrf_score": 25.215135036392606, "xcomet_score": 0.5828307867050171, "xcomet_qe_score": 0.6356096267700195, "metricx_score": 4.0491743087768555, "metricx_qe_score": 4.661988735198975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从支配者(这里是“", "metrics": {"bleu_score": 26.506441353385075, "chrf_score": 20.94580464199314, "xcomet_score": 0.4183485209941864, "xcomet_qe_score": 0.16249552369117737, "metricx_score": 7.716026782989502, "metricx_qe_score": 6.714054107666016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "爱”)得到对所有连词的单独依赖。这些是巴顿的创造。", "metrics": {"bleu_score": 4.260146736441797, "chrf_score": 4.4529123288240555, "xcomet_score": 0.14441834390163422, "xcomet_qe_score": 0.15283538401126862, "metricx_score": 10.93303394317627, "metricx_qe_score": 16.204940795898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文的目的是提出一种新的论点,支持像上述这两个例子那样对称的协调结构,反对像上述这两个例子那样不对", "metrics": {"bleu_score": 19.95558106335327, "chrf_score": 18.228218454337835, "xcomet_score": 0.7040168046951294, "xcomet_qe_score": 0.6978991031646729, "metricx_score": 4.227758884429932, "metricx_qe_score": 4.202033519744873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9997062683105469, "xcomet_qe_score": 1.0, "metricx_score": 0.1774456948041916, "metricx_qe_score": 0.21148386597633362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好,这个论点基于依赖长度最小化原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 47.779995473707814, "chrf_score": 37.369023277938204, "xcomet_score": 0.8090651035308838, "xcomet_qe_score": 0.8004360198974609, "metricx_score": 0.8960897922515869, "metricx_qe_score": 0.6552587151527405, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如你可能知道的,直接宾语倾向于靠近动词,而状语可以更远一些,对吗?", "metrics": {"bleu_score": 35.51809094683446, "chrf_score": 29.575285325415884, "xcomet_score": 0.8501589298248291, "xcomet_qe_score": 0.8244951963424683, "metricx_score": 1.8451696634292603, "metricx_qe_score": 1.3836884498596191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以“March,read it yesterday”是可以的,因为直接宾语“it”靠近动词。 (注:这里“March”可能是一个错误,通常一个句子应该是“I read it yesterday in March”或“Yesterday in March, I read it”。但根据给定的英文句子结构,翻译保持一致。) 虽然三月昨天读了,但情况要严重得多,", "metrics": {"bleu_score": 10.294962231652633, "chrf_score": 39.10566675164432, "xcomet_score": 0.2138879895210266, "xcomet_qe_score": 0.3362939655780792, "metricx_score": 13.586578369140625, "metricx_qe_score": 13.212738037109375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为在这里,动词和直接宾语之间有一个副词“昨天”。", "metrics": {"bleu_score": 54.26924329239607, "chrf_score": 40.79367834309594, "xcomet_score": 0.8846782445907593, "xcomet_qe_score": 0.8477401733398438, "metricx_score": 1.3413816690444946, "metricx_qe_score": 1.1281014680862427, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重且非常长时,这种影响可能会减弱,因为它", "metrics": {"bleu_score": 40.42154298137014, "chrf_score": 39.686809387081, "xcomet_score": 0.6908711194992065, "xcomet_qe_score": 0.4910896420478821, "metricx_score": 5.58355188369751, "metricx_qe_score": 3.6577041149139404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以移动到附词之后的位置。", "metrics": {"bleu_score": 36.38551728322059, "chrf_score": 35.46303169595981, "xcomet_score": 0.7982441186904907, "xcomet_qe_score": 0.7880078554153442, "metricx_score": 2.057145357131958, "metricx_qe_score": 3.4536845684051514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的例子说明了问题。所以", "metrics": {"bleu_score": 9.238430210261097, "chrf_score": 13.42762674504041, "xcomet_score": 0.7025460004806519, "xcomet_qe_score": 0.5779122114181519, "metricx_score": 3.206362724304199, "metricx_qe_score": 0.8730595111846924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是正确的。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.308367520570755, "metricx_qe_score": 0.5612361431121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "March今天读了一本非常有趣的关于BCS的", "metrics": {"bleu_score": 0.0, "chrf_score": 2.609862989996266, "xcomet_score": 0.22063034772872925, "xcomet_qe_score": 0.2575235366821289, "metricx_score": 7.366031646728516, "metricx_qe_score": 10.2198486328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书。这没问题。在句子中,我们可以用一个长名词短语来替代“它”。", "metrics": {"bleu_score": 18.39156520264434, "chrf_score": 20.35305330289124, "xcomet_score": 0.5365573763847351, "xcomet_qe_score": 0.16560876369476318, "metricx_score": 5.32562780380249, "metricx_qe_score": 5.708719730377197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但也可以说“我昨天读了《三月蜂》,这本绝对迷人的关于蜜蜂的书。", "metrics": {"bleu_score": 1.9508315563589549, "chrf_score": 1.8518518518518516, "xcomet_score": 0.7114969491958618, "xcomet_qe_score": 0.6760536432266235, "metricx_score": 5.382020473480225, "metricx_qe_score": 5.100033760070801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "” 因此,这里的推理是,这句话虽然违反了一般语法原则——即直接宾语应紧跟在动词之后——但仍然可能成立。 沃伊切赫·恰亚 — 它符合依赖长度最小化原则,该原则指出,较短的依赖关系更可取。 沃伊切赫·恰亚 — 较短的依赖关系更受青睐。 因此,", "metrics": {"bleu_score": 18.23649668475081, "chrf_score": 23.236910081352768, "xcomet_score": 0.26853376626968384, "xcomet_qe_score": 0.3513132631778717, "metricx_score": 10.521611213684082, "metricx_qe_score": 5.875409126281738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示了关键依赖项的长度,即这两个结构中不常数的那些。", "metrics": {"bleu_score": 54.20377895732388, "chrf_score": 48.19862010242935, "xcomet_score": 0.8346081972122192, "xcomet_qe_score": 0.6634336709976196, "metricx_score": 1.7356326580047607, "metricx_qe_score": 2.7222208976745605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们这里有一个从“读”到长度为七的附词(以词为单位)的依赖关系,以及从“读”到“书”长度为四的依赖关系。加起来总共是11。", "metrics": {"bleu_score": 20.07790147080733, "chrf_score": 19.81764065469101, "xcomet_score": 0.5757494568824768, "xcomet_qe_score": 0.5987083315849304, "metricx_score": 4.551458358764648, "metricx_qe_score": 4.473843097686768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个成分时,这两个依赖性的和变成6,对吧", "metrics": {"bleu_score": 34.31535116220621, "chrf_score": 30.254616185415923, "xcomet_score": 0.7522433996200562, "xcomet_qe_score": 0.6939277648925781, "metricx_score": 6.802640438079834, "metricx_qe_score": 7.082333087921143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?所以而不是11,6,短得多。", "metrics": {"bleu_score": 21.200626759025184, "chrf_score": 19.888644116907493, "xcomet_score": 0.6811332702636719, "xcomet_qe_score": 0.6791231632232666, "metricx_score": 4.30379581451416, "metricx_qe_score": 5.156360149383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来相当不错,", "metrics": {"bleu_score": 67.29864884660302, "chrf_score": 66.91997613073697, "xcomet_score": 0.9384526610374451, "xcomet_qe_score": 0.9192880988121033, "metricx_score": 0.5477246046066284, "metricx_qe_score": 0.5692055225372314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?它违反了一个原则,但满足了另一个。", "metrics": {"bleu_score": 57.34648773088752, "chrf_score": 49.449387347830736, "xcomet_score": 0.9197793006896973, "xcomet_qe_score": 0.9616693258285522, "metricx_score": 0.6769707202911377, "metricx_qe_score": 1.185765266418457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9992790222167969, "xcomet_qe_score": 0.997222900390625, "metricx_score": 0.1849263608455658, "metricx_qe_score": 0.19376564025878906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从增强版的Penn Treebank中提取了关于协调关系的各种统计数据,并参阅论文以了解我们为什么没有使用大学依赖关系的原因。 马特乌什·皮奥尔科夫斯基——统计数据证实了之前多次观察到的现象,即左侧合同倾向于更短,", "metrics": {"bleu_score": 36.999176071422895, "chrf_score": 36.01667085924498, "xcomet_score": 0.18077197670936584, "xcomet_qe_score": 0.17614424228668213, "metricx_score": 12.254504203796387, "metricx_qe_score": 11.781282424926758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时“盐和胡椒”以及“盐”在音节上的测量也更短。 以及顺带", "metrics": {"bleu_score": 3.889818545474848, "chrf_score": 4.095239016859339, "xcomet_score": 0.3295648396015167, "xcomet_qe_score": 0.33029741048812866, "metricx_score": 10.187238693237305, "metricx_qe_score": 10.52410888671875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提到的观察结果,即这种倾向随着长度差异的增加而加剧。", "metrics": {"bleu_score": 55.930768075459916, "chrf_score": 54.37443318548406, "xcomet_score": 0.8793826103210449, "xcomet_qe_score": 0.864551305770874, "metricx_score": 2.758273124694824, "metricx_qe_score": 3.903994083404541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当两个连词的长度差异增大时,较短的连词更倾向于成为首先变强的那个。对,所以", "metrics": {"bleu_score": 30.66548843351443, "chrf_score": 26.16763342196069, "xcomet_score": 0.8066000938415527, "xcomet_qe_score": 0.8294491767883301, "metricx_score": 5.4356231689453125, "metricx_qe_score": 3.6941256523132324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "左侧较短连词的比例更大。", "metrics": {"bleu_score": 42.763988908612596, "chrf_score": 38.810820926321526, "xcomet_score": 0.9018838405609131, "xcomet_qe_score": 0.8681549429893494, "metricx_score": 1.6605947017669678, "metricx_qe_score": 2.1292169094085693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种倾向仅在左侧的保姆缺席时才会出现。 所以", "metrics": {"bleu_score": 41.402874794004866, "chrf_score": 35.734033701506476, "xcomet_score": 0.6268121004104614, "xcomet_qe_score": 0.590230405330658, "metricx_score": 7.763066291809082, "metricx_qe_score": 7.148033618927002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以州长在左边。", "metrics": {"bleu_score": 18.702869706385968, "chrf_score": 12.95644003926289, "xcomet_score": 0.6368355751037598, "xcomet_qe_score": 0.7403826713562012, "metricx_score": 2.357534646987915, "metricx_qe_score": 1.274109125137329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,它缺失了。荷马来了,打了个喷嚏。", "metrics": {"bleu_score": 20.828838183973037, "chrf_score": 11.685270202180831, "xcomet_score": 0.7585030794143677, "xcomet_qe_score": 0.748598575592041, "metricx_score": 4.053126335144043, "metricx_qe_score": 4.4531145095825195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有两个动词的协调,没有外部的支配因素。因此", "metrics": {"bleu_score": 58.53410496728347, "chrf_score": 52.663572313299646, "xcomet_score": 0.8679500818252563, "xcomet_qe_score": 0.8100378513336182, "metricx_score": 3.6885488033294678, "metricx_qe_score": 2.0099217891693115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左侧的连词倾向于更简短,尤其是在两个连词之间的差异越大时。", "metrics": {"bleu_score": 26.89054715066593, "chrf_score": 27.324487164466166, "xcomet_score": 0.9356787204742432, "xcomet_qe_score": 0.937075138092041, "metricx_score": 4.711798667907715, "metricx_qe_score": 4.932723522186279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当右边的治理在这里时,左边管理协调、电话和网络,这种效果消失了。 因此,我们通过测量汉字的", "metrics": {"bleu_score": 5.613283960650174, "chrf_score": 7.330935758262788, "xcomet_score": 0.21940205991268158, "xcomet_qe_score": 0.2312903106212616, "metricx_score": 16.015226364135742, "metricx_qe_score": 14.3620023727417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "长度来证明这一点,这是第一列,以音节为单位;中间一列以词为单位;最右一列。所以,我将", "metrics": {"bleu_score": 12.628652437846023, "chrf_score": 13.956264428774778, "xcomet_score": 0.4256402552127838, "xcomet_qe_score": 0.3257333040237427, "metricx_score": 14.537381172180176, "metricx_qe_score": 11.297408103942871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "专注于最右边的列。", "metrics": {"bleu_score": 19.969395881889398, "chrf_score": 16.322199135289424, "xcomet_score": 0.8545703887939453, "xcomet_qe_score": 0.7333084344863892, "metricx_score": 2.2968902587890625, "metricx_qe_score": 4.130330562591553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到的是,当总督在左侧时, 左侧成分随着词语绝对差的增大,其缩短的趋势逐渐明显。在没有控制词的句子协调结构中,也可以观察到同样的现象。然而", "metrics": {"bleu_score": 32.67298257773261, "chrf_score": 28.193070238270458, "xcomet_score": 0.35867980122566223, "xcomet_qe_score": 0.30499085783958435, "metricx_score": 8.66090202331543, "metricx_qe_score": 6.593984603881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当控制词位于右侧时,这种趋势就会消失。", "metrics": {"bleu_score": 49.030470692026626, "chrf_score": 43.38113461758972, "xcomet_score": 0.819666862487793, "xcomet_qe_score": 0.5574642419815063, "metricx_score": 2.9794225692749023, "metricx_qe_score": 5.109396457672119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这一点如何为反对这两种不对称协调结构提供论据,同时为这两种对称结构提供支持。 请参阅论文以", "metrics": {"bleu_score": 53.120663436563774, "chrf_score": 48.59402981993063, "xcomet_score": 0.6207407116889954, "xcomet_qe_score": 0.4522448778152466, "metricx_score": 4.86916446685791, "metricx_qe_score": 1.6785587072372437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了解完整的协议和论点,抱歉,并", "metrics": {"bleu_score": 5.847275578505034, "chrf_score": 5.345538120145823, "xcomet_score": 0.16219425201416016, "xcomet_qe_score": 0.14689113199710846, "metricx_score": 11.051116943359375, "metricx_qe_score": 6.894935607910156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "和我们讨论海报展。", "metrics": {"bleu_score": 6.054630691682828, "chrf_score": 5.235272988505747, "xcomet_score": 0.7844656109809875, "xcomet_qe_score": 0.7947835922241211, "metricx_score": 3.6486124992370605, "metricx_qe_score": 2.9238078594207764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是香斌,华盛顿大学博士生。", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 20.085203671626676, "xcomet_score": 0.8397122621536255, "xcomet_qe_score": 0.8220083713531494, "metricx_score": 1.0871628522872925, "metricx_qe_score": 0.5624538660049438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,从预训练数据到语言模型再到下游任务,追踪导致不公平自然语言处理模型的政治偏见的轨迹。", "metrics": {"bleu_score": 64.99862400782325, "chrf_score": 61.33496007839747, "xcomet_score": 0.9382729530334473, "xcomet_qe_score": 0.8344854116439819, "metricx_score": 1.4140868186950684, "metricx_qe_score": 1.9004147052764893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模网络抓取的数据上进行训练的。", "metrics": {"bleu_score": 54.405770769345864, "chrf_score": 50.02196999283078, "xcomet_score": 0.9850398302078247, "xcomet_qe_score": 0.9917706251144409, "metricx_score": 1.50629460811615, "metricx_qe_score": 1.8451300859451294, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中覆盖得非常好。", "metrics": {"bleu_score": 53.869332652633126, "chrf_score": 47.076377853973916, "xcomet_score": 0.8525817394256592, "xcomet_qe_score": 0.8098437786102295, "metricx_score": 2.772179126739502, "metricx_qe_score": 3.132279634475708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C4语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中覆盖得非常好。", "metrics": {"bleu_score": 80.96151401064327, "chrf_score": 79.01586914476833, "xcomet_score": 0.8957071304321289, "xcomet_qe_score": 0.8208948373794556, "metricx_score": 1.6027424335479736, "metricx_qe_score": 1.4985623359680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了喜忧参半的结果。 因此,", "metrics": {"bleu_score": 62.88584826842568, "chrf_score": 68.62293006561504, "xcomet_score": 0.784228503704071, "xcomet_qe_score": 0.7592571377754211, "metricx_score": 2.3589351177215576, "metricx_qe_score": 1.3312727212905884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从多样的视角中学习,这庆祝了民主和思想的多元性。", "metrics": {"bleu_score": 26.34592129280123, "chrf_score": 22.50418785422957, "xcomet_score": 0.8640990257263184, "xcomet_qe_score": 0.7724453210830688, "metricx_score": 1.9477674961090088, "metricx_qe_score": 2.249070644378662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能在下游任务应用中导致潜在的公平问题。", "metrics": {"bleu_score": 69.50494883456494, "chrf_score": 62.383744392705, "xcomet_score": 0.9908058643341064, "xcomet_qe_score": 0.9736735820770264, "metricx_score": 0.9772740602493286, "metricx_qe_score": 1.200330376625061, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体通过提出以下几个问题进行探索。 首先,我们如何评估语言模型的政治倾向,以及相关数据对这些政治偏见可能起到什么作用?", "metrics": {"bleu_score": 62.89980206878888, "chrf_score": 59.02107030399875, "xcomet_score": 0.8992520570755005, "xcomet_qe_score": 0.9129320383071899, "metricx_score": 1.626058578491211, "metricx_qe_score": 1.7962720394134521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治限制的语言模型在下游任务中的实际表现如何,以及这是否会导致NLP应用中的公平性问题?", "metrics": {"bleu_score": 80.07722697093931, "chrf_score": 75.96539169167512, "xcomet_score": 0.8439295291900635, "xcomet_qe_score": 0.7860476970672607, "metricx_score": 1.8225922584533691, "metricx_qe_score": 1.556969404220581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体而言,我们首先提出使用政治问卷,如政治罗盘测试,以不同的提示格式来引导语言模型。", "metrics": {"bleu_score": 45.66407185034984, "chrf_score": 39.37757002960207, "xcomet_score": 0.8445209264755249, "xcomet_qe_score": 0.8353718519210815, "metricx_score": 3.881504535675049, "metricx_qe_score": 4.209451198577881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保我们能够在政治科学文献的基础上进行自动化评估。", "metrics": {"bleu_score": 31.332362223582145, "chrf_score": 31.120474831770807, "xcomet_score": 0.9776467084884644, "xcomet_qe_score": 0.9212745428085327, "metricx_score": 1.3014206886291504, "metricx_qe_score": 1.532111644744873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,母语模型确实具有不同的政治倾向。", "metrics": {"bleu_score": 64.25736646779204, "chrf_score": 56.17908902361034, "xcomet_score": 0.9669203758239746, "xcomet_qe_score": 0.9843463897705078, "metricx_score": 2.1614139080047607, "metricx_qe_score": 1.8009052276611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘上的所有四个象限。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 62.83591000502765, "xcomet_score": 0.8533560037612915, "xcomet_qe_score": 0.7687404155731201, "metricx_score": 1.9299724102020264, "metricx_qe_score": 2.1211085319519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到,GPT-4 是所有语言模型中最自由的,而GPT理论普遍比BERT理论及其变体在社会自由度方面更为宽松。", "metrics": {"bleu_score": 42.200915923296904, "chrf_score": 43.99064872730982, "xcomet_score": 0.7827929854393005, "xcomet_qe_score": 0.6063507199287415, "metricx_score": 2.9868004322052, "metricx_qe_score": 2.7342493534088135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在探究语言模型的政治偏见在多大程度上实际上是从训练数据中吸取的。", "metrics": {"bleu_score": 56.403320244940815, "chrf_score": 48.494899137551315, "xcomet_score": 0.9318479299545288, "xcomet_qe_score": 0.9260696172714233, "metricx_score": 1.0495109558105469, "metricx_qe_score": 1.4011412858963013, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点来进行控制实验,这些语料库分为新闻和社交媒体,并根据其政治倾向进一步细分。", "metrics": {"bleu_score": 52.26953563334303, "chrf_score": 44.55119427181943, "xcomet_score": 0.9227876663208008, "xcomet_qe_score": 0.7308118343353271, "metricx_score": 1.8105134963989258, "metricx_qe_score": 2.0579471588134766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这样的党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了转移。", "metrics": {"bleu_score": 72.90656280975672, "chrf_score": 68.30139582924924, "xcomet_score": 0.905775249004364, "xcomet_qe_score": 0.8258528709411621, "metricx_score": 1.3807544708251953, "metricx_qe_score": 1.9301079511642456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于罗伯塔模型,进一步微调并使用倾向于左翼的Reddit语料库进行训练,我们可以观察到其在政治倾向上显著偏向自由派。 在政治偏见方面。", "metrics": {"bleu_score": 22.908772036214806, "chrf_score": 29.797330499759905, "xcomet_score": 0.4442502558231354, "xcomet_qe_score": 0.448957234621048, "metricx_score": 5.711805820465088, "metricx_qe_score": 4.050178050994873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还尝试探讨语言模型是否能捕捉到现代社会中普遍存在的极化现象。", "metrics": {"bleu_score": 66.15065369281245, "chrf_score": 58.50635462630006, "xcomet_score": 0.9009986519813538, "xcomet_qe_score": 0.9832556247711182, "metricx_score": 0.6275016665458679, "metricx_qe_score": 0.784520149230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第45任总统之前和之后两个部分,然后", "metrics": {"bleu_score": 66.15466616370077, "chrf_score": 64.76309502801357, "xcomet_score": 0.6494697332382202, "xcomet_qe_score": 0.7123317718505859, "metricx_score": 3.9457807540893555, "metricx_qe_score": 1.5960078239440918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别对这两个不同时间段的语料库进行预训练,以训练语言模型。 我们", "metrics": {"bleu_score": 37.447417181493456, "chrf_score": 38.78432833199885, "xcomet_score": 0.7481921911239624, "xcomet_qe_score": 0.6678837537765503, "metricx_score": 5.084369659423828, "metricx_qe_score": 1.4073336124420166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年后普遍表现出更偏离中间的政治倾向。", "metrics": {"bleu_score": 36.75234456178971, "chrf_score": 33.118415057490886, "xcomet_score": 0.9777995347976685, "xcomet_qe_score": 0.9724009037017822, "metricx_score": 1.1100338697433472, "metricx_qe_score": 1.5234768390655518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能感知到社会中存在的两极分化。", "metrics": {"bleu_score": 53.705728666046554, "chrf_score": 45.73736162981014, "xcomet_score": 0.914472222328186, "xcomet_qe_score": 0.9072178602218628, "metricx_score": 0.9413270354270935, "metricx_qe_score": 1.2220604419708252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们评估了具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测中的表现。这些自然语言处理(NLP)应用通常涉及语言模型,并且可能具有非常重要的影响。 因此,", "metrics": {"bleu_score": 59.34619645325305, "chrf_score": 66.3253010576665, "xcomet_score": 0.710318922996521, "xcomet_qe_score": 0.7031571865081787, "metricx_score": 2.907144546508789, "metricx_qe_score": 1.3779619932174683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到如果我们调查按类别的性能,也就是说,如果我们将性能分开成 在不同的人口统计学或政治意义的新闻媒体中,我们可以看到一种模式,", "metrics": {"bleu_score": 38.877709401557986, "chrf_score": 32.4035953052811, "xcomet_score": 0.5949435234069824, "xcomet_qe_score": 0.6021285057067871, "metricx_score": 9.179637908935547, "metricx_qe_score": 8.545553207397461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测方面,倾向自由派的语言模型表现更好。 在检测针对社会少数群体的仇恨言论方面,我们需要采取更加敏感和谨慎的态度。这种言论往往包含对特定种族、宗教、性别或性取向等群体的歧视和攻击。准确识别和处理这类言论需要结合语境、文化背景以及相关法律法规,以确保翻译的准确性和文化适宜性。在翻译过程中,应使用恰当的术语和表达方式,避免任何可能加剧偏见或误解的表述。同时,考虑到中国文化的特殊性,翻译时还应注意语义的平衡和尊重,以维护翻译文本的学术性和指导性。 然而,我们在识别针对社会中更强大群体的仇恨言论方面表现得更差。", "metrics": {"bleu_score": 16.217653197453448, "chrf_score": 34.78938857355292, "xcomet_score": 0.07888507843017578, "xcomet_qe_score": 0.008424959145486355, "metricx_score": 15.561515808105469, "metricx_qe_score": 14.478927612304688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,倾向于右派的语言模型在检测针对白人男性的仇恨言论方面表现较好,但在检测针对黑人、LGBTQ+ 以及其他少数族裔社区的仇恨言论时则表现较差。", "metrics": {"bleu_score": 50.1104539192696, "chrf_score": 51.847374009597004, "xcomet_score": 0.9814351797103882, "xcomet_qe_score": 0.9764776229858398, "metricx_score": 0.7721269130706787, "metricx_qe_score": 0.7998956441879272, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在假新闻检测方面也出现了类似的趋势,我们发现倾向于左派的语言模型更擅长检测来自相反政治倾向的错误信息,反之亦然。 在本文中,", "metrics": {"bleu_score": 52.9607408769298, "chrf_score": 50.14481036488559, "xcomet_score": 0.8283604383468628, "xcomet_qe_score": 0.7980324029922485, "metricx_score": 3.8310627937316895, "metricx_qe_score": 1.743465542793274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多定性例子,以看到具有不同政治意义的语言模型。 确实,基于其社会类别,对仇恨言论和虚假信息示例应该给出不同的预测。", "metrics": {"bleu_score": 56.017484578407675, "chrf_score": 49.165517994845985, "xcomet_score": 0.7073776721954346, "xcomet_qe_score": 0.7459756135940552, "metricx_score": 4.355990409851074, "metricx_qe_score": 4.597584247589111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中提供了更多示例,以进一步强调这一点。 这表明语言模型的政治偏见存在一个非常紧迫的公平问题。", "metrics": {"bleu_score": 53.008723846287886, "chrf_score": 45.31000107442033, "xcomet_score": 0.8620578050613403, "xcomet_qe_score": 0.8337752819061279, "metricx_score": 1.7581589221954346, "metricx_qe_score": 2.1903319358825684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个倾向于保守的语言模型被微调用于仇恨言论、虚假信息等,并部署在流行的社交媒体平台上, 这意味着持有不同政治观点的人可能会被边缘化,针对少数族裔的仇恨言论也可能不受控制地蔓延。", "metrics": {"bleu_score": 38.611693294546285, "chrf_score": 32.623266379817835, "xcomet_score": 0.9336234331130981, "xcomet_qe_score": 0.889587938785553, "metricx_score": 1.684801697731018, "metricx_qe_score": 1.5519382953643799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为我们敲响了警钟,需要我们认识到并解决语言模型政治倾向所导致的公平问题。", "metrics": {"bleu_score": 38.21120008009713, "chrf_score": 39.42052188072868, "xcomet_score": 0.992172360420227, "xcomet_qe_score": 0.9924259185791016, "metricx_score": 0.6982290148735046, "metricx_qe_score": 0.7575309872627258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,让我们稍微讨论一下。", "metrics": {"bleu_score": 33.260249505555045, "chrf_score": 32.42349794467388, "xcomet_score": 0.819378137588501, "xcomet_qe_score": 0.8147748708724976, "metricx_score": 0.8959165811538696, "metricx_qe_score": 1.1693971157073975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也想强调的是,我们揭露了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 59.548745606835126, "chrf_score": 53.6261477952267, "xcomet_score": 0.9464409351348877, "xcomet_qe_score": 0.8709520101547241, "metricx_score": 0.9624944925308228, "metricx_qe_score": 1.5651473999023438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在斯克拉和卡律布迪斯之间取舍。", "metrics": {"bleu_score": 39.40890100849855, "chrf_score": 37.522263869776914, "xcomet_score": 0.7367826700210571, "xcomet_qe_score": 0.7398419380187988, "metricx_score": 1.7035342454910278, "metricx_qe_score": 2.5320241451263428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们不对语言模型训练数据中的政治观点进行消毒处理,偏见就会从预训练数据传播到语言模型,再到下游任务,最终导致公平性问题。", "metrics": {"bleu_score": 58.72190128317179, "chrf_score": 54.77366342306619, "xcomet_score": 0.8527051210403442, "xcomet_qe_score": 0.8380681276321411, "metricx_score": 3.1225740909576416, "metricx_qe_score": 3.6238441467285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试以某种方式进行“消毒”,我们也将面临审查或排斥的风险,", "metrics": {"bleu_score": 52.48455204641679, "chrf_score": 50.25451126336568, "xcomet_score": 0.8302257061004639, "xcomet_qe_score": 0.798824667930603, "metricx_score": 2.080397129058838, "metricx_qe_score": 3.1803665161132812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且很难确定哪些内容实际上是中性的,应该保留语言一致性数据。这在某种程度上就像电轨车道德困境。 (注:\"electric trolley problem\" 通常指的是在道德哲学中一个经典的思想实验,即“电轨车问题”。", "metrics": {"bleu_score": 10.434174167991063, "chrf_score": 15.870786719414362, "xcomet_score": 0.713760256767273, "xcomet_qe_score": 0.6450411677360535, "metricx_score": 4.016034126281738, "metricx_qe_score": 4.23560094833374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里翻译为“电轨车道德困境”以保持原意。)", "metrics": {"bleu_score": 2.7156804039319287, "chrf_score": 6.4102564102564115, "xcomet_score": 0.395669549703598, "xcomet_qe_score": 0.2837604582309723, "metricx_score": 5.234050750732422, "metricx_qe_score": 5.790501594543457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9909268617630005, "xcomet_qe_score": 0.973970890045166, "metricx_score": 0.3818603754043579, "metricx_qe_score": 0.3029481768608093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。我今天要讲的就这些了。", "metrics": {"bleu_score": 11.251329738544614, "chrf_score": 11.93360000123101, "xcomet_score": 0.5228948593139648, "xcomet_qe_score": 0.5084100961685181, "metricx_score": 0.6945151090621948, "metricx_qe_score": 0.7228642702102661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基·梅隆大学一年级博士生。今天我将向大家介绍你们的作品《肛位性:描述数据集和模型的设计偏见》。", "metrics": {"bleu_score": 48.26378126040112, "chrf_score": 33.33395167640197, "xcomet_score": 0.6419325470924377, "xcomet_qe_score": 0.6403703093528748, "metricx_score": 5.643203258514404, "metricx_qe_score": 5.9754204750061035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的,他们是塞巴斯蒂安·桑蒂(Sebastian Santee)、罗南·拉布罗塞(Ronan Labrosse)、卡塔琳娜·莱内克(Katarina Reinecke)和马丁·萨普(Martin Sapp)。", "metrics": {"bleu_score": 28.786371791357467, "chrf_score": 52.113945351123604, "xcomet_score": 0.8597800731658936, "xcomet_qe_score": 0.8710212111473083, "metricx_score": 2.794771194458008, "metricx_qe_score": 2.1922125816345215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们从一个场景开始想象:你为一家报纸工作,正在筛选新闻文章下的评论,试图删除有毒内容。 ", "metrics": {"bleu_score": 44.409176637184956, "chrf_score": 40.14281906063618, "xcomet_score": 0.8915011882781982, "xcomet_qe_score": 0.899921715259552, "metricx_score": 1.88380765914917, "metricx_qe_score": 1.529209852218628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能轉向使用像Perspective API這樣的熱門API進行有毒內容檢測。如果您是卡爾·瓊斯,這確實效果很好,因為", "metrics": {"bleu_score": 5.289855073152943, "chrf_score": 20.902276453723594, "xcomet_score": 0.7065336108207703, "xcomet_qe_score": 0.7650296688079834, "metricx_score": 5.042707443237305, "metricx_qe_score": 3.6772758960723877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能夠正確地檢測出有毒的實例。", "metrics": {"bleu_score": 6.150343144231885, "chrf_score": 42.59536226172686, "xcomet_score": 0.7576387524604797, "xcomet_qe_score": 0.778404951095581, "metricx_score": 5.548945903778076, "metricx_qe_score": 5.547269344329834, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对阿迪提亚·沙尔马(Aditya Sharma)来说,情况并非如此。", "metrics": {"bleu_score": 30.94986086785959, "chrf_score": 60.6886343338415, "xcomet_score": 0.9830441474914551, "xcomet_qe_score": 0.988021731376648, "metricx_score": 1.3186091184616089, "metricx_qe_score": 1.652296543121338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 перспектив API 的角度来看,它对在印度语境中更常见的冒犯性术语并不敏感。", "metrics": {"bleu_score": 48.198717221616, "chrf_score": 39.1487108288717, "xcomet_score": 0.6556040048599243, "xcomet_qe_score": 0.47736871242523193, "metricx_score": 3.175154209136963, "metricx_qe_score": 4.229907989501953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏差的例子,我们可以在不同人群中观察到技术表现的系统性差异。", "metrics": {"bleu_score": 41.78383638882466, "chrf_score": 35.777579444745314, "xcomet_score": 0.9778684377670288, "xcomet_qe_score": 0.9615130424499512, "metricx_score": 0.6782376766204834, "metricx_qe_score": 0.8978255987167358, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们之前看到的设计偏见可能由于自然语言处理(NLP)研究人员和模型开发者的立场而发生。", "metrics": {"bleu_score": 46.46223721619174, "chrf_score": 39.59782975887593, "xcomet_score": 0.9656612873077393, "xcomet_qe_score": 0.8649730682373047, "metricx_score": 0.9915249347686768, "metricx_qe_score": 0.9184998273849487, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场性是指人们由于其人口统计、身份和生活经历而持有的观点。 (注:这里“立场性”翻译为“立场性”可能更准确地传达了原文中“positionalities”的含义,但这个词在中文学术语境中可能不常见,可以根据具体语境和目标读者进行调整。)", "metrics": {"bleu_score": 17.514044567986065, "chrf_score": 36.48949529671783, "xcomet_score": 0.6783466339111328, "xcomet_qe_score": 0.7659757137298584, "metricx_score": 3.4284818172454834, "metricx_qe_score": 3.099290609359741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 73.97378912627735, "chrf_score": 67.41898597045686, "xcomet_score": 0.9927648305892944, "xcomet_qe_score": 0.908970832824707, "metricx_score": 0.9186691045761108, "metricx_qe_score": 1.4367049932479858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为一名研究人员,立场性会影响研究过程及其结果和结论,因为它会改变研究人员所做的决定。", "metrics": {"bleu_score": 51.77290181508159, "chrf_score": 43.66292246826325, "xcomet_score": 0.9927520751953125, "xcomet_qe_score": 0.9865365028381348, "metricx_score": 1.0159739255905151, "metricx_qe_score": 1.0582505464553833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问的一个问题是,数据集和模型是否具有位置性?", "metrics": {"bleu_score": 48.465254338121596, "chrf_score": 45.20744241143061, "xcomet_score": 0.9149991869926453, "xcomet_qe_score": 0.9874231815338135, "metricx_score": 2.477600574493408, "metricx_qe_score": 0.7690516710281372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并非要说模型、细胞和数据集本身具有人口统计学身份和生活经历,但它们确实汇集了真实人群的判断和意见,从而可以代表某些立场,", "metrics": {"bleu_score": 51.982358441807996, "chrf_score": 43.06534397804352, "xcomet_score": 0.6788150668144226, "xcomet_qe_score": 0.6050126552581787, "metricx_score": 4.322181701660156, "metricx_qe_score": 4.797115802764893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "超越其他立场。 早期研究表明了一些位置性的轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 31.952472500946065, "chrf_score": 26.739742724373183, "xcomet_score": 0.3252384066581726, "xcomet_qe_score": 0.18018051981925964, "metricx_score": 7.131216526031494, "metricx_qe_score": 6.523223876953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些作品实际上并没有将最终用户与数据集和模型本身进行比较。 随着自然语言处理(NLP)任务变得更加主观和社会化,研究模型和数据集的位置性变得越来越重要。 由于并非所有决策都有记录,且许多模型隐藏在 API 背后,因此很难描述这些位置偏见的具体情况。", "metrics": {"bleu_score": 55.208992919790205, "chrf_score": 50.63048907017587, "xcomet_score": 0.7819778919219971, "xcomet_qe_score": 0.8074170351028442, "metricx_score": 5.091001510620117, "metricx_qe_score": 5.1211957931518555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的定位性,我们实际上将标注与真实用户进行比较,并将其与现有数据集和模型进行对比。", "metrics": {"bleu_score": 43.59183345706364, "chrf_score": 42.66084334871497, "xcomet_score": 0.7413693070411682, "xcomet_qe_score": 0.739128589630127, "metricx_score": 3.443727493286133, "metricx_qe_score": 2.9321603775024414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过自然语言定位(NL positionality)的框架来实现这一点。", "metrics": {"bleu_score": 24.23680701003884, "chrf_score": 51.264675287336544, "xcomet_score": 0.8946470022201538, "xcomet_qe_score": 0.8888409733772278, "metricx_score": 1.0791869163513184, "metricx_qe_score": 1.5085028409957886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤进行。", "metrics": {"bleu_score": 61.47881529512643, "chrf_score": 62.859360847302014, "xcomet_score": 0.9546252489089966, "xcomet_qe_score": 0.8385137319564819, "metricx_score": 0.2825612425804138, "metricx_qe_score": 0.5525258779525757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的标注人员重新标注数据集。", "metrics": {"bleu_score": 36.857838224116975, "chrf_score": 31.056395663981927, "xcomet_score": 0.7965057492256165, "xcomet_qe_score": 0.7971661686897278, "metricx_score": 4.661585807800293, "metricx_qe_score": 3.25648832321167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是分析原始数据集标注者的人口统计数据,因为通常只有少数标注者标注每个实例,而且人口统计数据很少被收集和共享。", "metrics": {"bleu_score": 56.424625817622484, "chrf_score": 49.350553597184025, "xcomet_score": 0.8971071243286133, "xcomet_qe_score": 0.8829902410507202, "metricx_score": 1.522193431854248, "metricx_qe_score": 1.382836103439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以获得每个实例的多位标注者,并获取丰富的人口统计数据。", "metrics": {"bleu_score": 39.02568652095016, "chrf_score": 36.03037647144035, "xcomet_score": 0.8802309036254883, "xcomet_qe_score": 0.8553707599639893, "metricx_score": 3.6593427658081055, "metricx_qe_score": 2.887310028076172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后根据人口统计学特征对注释进行分类,并使用皮尔逊相关系数(Pearson's R)将它们与模型和数据集进行比较。 因此,我们的框架实际上与注释器分歧文献不同,通过将最终用户与模型和数据集、预测和标签进行比较,而不是仅仅关注注释器一致性或建模注释器分布,从而实现了区分。", "metrics": {"bleu_score": 55.725661525333535, "chrf_score": 54.579123663087046, "xcomet_score": 0.5514435768127441, "xcomet_qe_score": 0.5522985458374023, "metricx_score": 5.46106481552124, "metricx_qe_score": 4.491446495056152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要通过来自我们人机交互合作者的在线众包平台“野生实验室”实现。", "metrics": {"bleu_score": 48.39462262413737, "chrf_score": 41.9773379845942, "xcomet_score": 0.7075918912887573, "xcomet_qe_score": 0.701779305934906, "metricx_score": 2.8979294300079346, "metricx_qe_score": 2.6520955562591553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,我们可以在该平台上招募来自不同背景的志愿者,", "metrics": {"bleu_score": 48.567710931378706, "chrf_score": 62.14000116225339, "xcomet_score": 0.9287595748901367, "xcomet_qe_score": 0.7507655620574951, "metricx_score": 1.7586643695831299, "metricx_qe_score": 3.6736767292022705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与 MTurk 等主要吸引美国或印度参与者的平台不同。此外,Lab in the Wild 仍然能够收集到高质量的数据。", "metrics": {"bleu_score": 37.799456209923854, "chrf_score": 45.63031950403124, "xcomet_score": 0.7618420124053955, "xcomet_qe_score": 0.8013022541999817, "metricx_score": 1.7388932704925537, "metricx_qe_score": 1.5285942554473877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“野外实验室”(Lab in the Wild)平台上举办了两个任务,其中之一是社交可接受性评估。具体来说,参与者将从社交化学数据集中读取一个情景,然后撰写对该情景社交可接受性的评价。", "metrics": {"bleu_score": 30.001959423617908, "chrf_score": 34.76860471523977, "xcomet_score": 0.8698967099189758, "xcomet_qe_score": 0.8196650147438049, "metricx_score": 1.8040735721588135, "metricx_qe_score": 1.9106457233428955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对研究的参与度,他们可以将自己的回答与人工智能和其他人的回答进行比较。", "metrics": {"bleu_score": 56.96539648564389, "chrf_score": 54.80421145068135, "xcomet_score": 0.9905778169631958, "xcomet_qe_score": 0.9937238693237305, "metricx_score": 0.6882297992706299, "metricx_qe_score": 0.6028512716293335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些标注与社会化学、德尔菲法和GPT-4进行了比较。", "metrics": {"bleu_score": 38.14165616365676, "chrf_score": 34.48621314534247, "xcomet_score": 0.747299313545227, "xcomet_qe_score": 0.7550889253616333, "metricx_score": 2.0352396965026855, "metricx_qe_score": 2.3188295364379883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后为毒性和仇恨言论检测任务复制了一个非常相似的设置,他们将从DynaHate中读取一个实例,并写下他们是否认为它是仇恨言论的实例。", "metrics": {"bleu_score": 60.71795186655516, "chrf_score": 52.326868463506116, "xcomet_score": 0.8246747851371765, "xcomet_qe_score": 0.8364596366882324, "metricx_score": 2.5166335105895996, "metricx_qe_score": 2.7282750606536865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。", "metrics": {"bleu_score": 43.787711770570425, "chrf_score": 74.12447775005208, "xcomet_score": 0.8813289403915405, "xcomet_qe_score": 0.8126022815704346, "metricx_score": 1.4936248064041138, "metricx_qe_score": 2.2442588806152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了来自87个国家的1000多名标注者的超过16,000条标注。", "metrics": {"bleu_score": 68.39416505583772, "chrf_score": 70.82057032470497, "xcomet_score": 0.9254944920539856, "xcomet_qe_score": 0.9574939012527466, "metricx_score": 2.757150173187256, "metricx_qe_score": 3.0208706855773926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们更有能力回答自然语言处理(NLP)数据集和模型最倾向于与谁对齐的问题。", "metrics": {"bleu_score": 28.825790406664787, "chrf_score": 30.73062793885017, "xcomet_score": 0.8812474012374878, "xcomet_qe_score": 0.8778453469276428, "metricx_score": 1.8179465532302856, "metricx_qe_score": 1.1011186838150024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP中存在位置性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8341923952102661, "xcomet_qe_score": 0.8352224826812744, "metricx_score": 3.7918624877929688, "metricx_qe_score": 2.050662040710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型与英语国家最紧密相关。", "metrics": {"bleu_score": 49.35451179213381, "chrf_score": 43.57686954283306, "xcomet_score": 0.9838119745254517, "xcomet_qe_score": 0.9733604192733765, "metricx_score": 1.0090453624725342, "metricx_qe_score": 1.158062219619751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在GPT-4社会可接受性分析中,我们发现它与儒家文化及英语国家最紧密相关。我们", "metrics": {"bleu_score": 44.78710142948759, "chrf_score": 47.5862528236639, "xcomet_score": 0.6550916433334351, "xcomet_qe_score": 0.614391565322876, "metricx_score": 4.7224907875061035, "metricx_qe_score": 1.8938491344451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现动态仇恨(dyna-hate)也与英语国家最紧密相关。", "metrics": {"bleu_score": 14.230715327204656, "chrf_score": 21.874112233942444, "xcomet_score": 0.9010118246078491, "xcomet_qe_score": 0.9064575433731079, "metricx_score": 2.895660877227783, "metricx_qe_score": 3.1402552127838135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,大多数额外的观点一致性出现在拥有大学学历的人群中。因此,", "metrics": {"bleu_score": 14.036046697656985, "chrf_score": 14.167465485334729, "xcomet_score": 0.6042279601097107, "xcomet_qe_score": 0.7582299709320068, "metricx_score": 5.463800430297852, "metricx_qe_score": 3.172029972076416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在GPT-4的社会可接受性任务中,我们发现它与拥有大学学历或研究生学历的人群最一致。 我们在分析Donahate时发现类似的结果,该词最常与受过大学教育的人群相关联。", "metrics": {"bleu_score": 37.9259728733027, "chrf_score": 36.656550442083685, "xcomet_score": 0.7585996985435486, "xcomet_qe_score": 0.6370552778244019, "metricx_score": 4.47050666809082, "metricx_qe_score": 4.724977016448975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群相关联时,不可避免地会有一些人被遗漏。 一个例子", "metrics": {"bleu_score": 48.840057500411284, "chrf_score": 45.968536296687226, "xcomet_score": 0.8287959098815918, "xcomet_qe_score": 0.7188467979431152, "metricx_score": 2.191077709197998, "metricx_qe_score": 1.6914499998092651, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,与男性和女性对应相比,数据集和模型对非二元性别人群的适应性较差。", "metrics": {"bleu_score": 36.520790158558434, "chrf_score": 33.45797781945193, "xcomet_score": 0.621772825717926, "xcomet_qe_score": 0.6908780336380005, "metricx_score": 5.122297286987305, "metricx_qe_score": 4.846055507659912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT-4社会可接受性任务以及DynaHATE任务分析中都发现了这一点。", "metrics": {"bleu_score": 67.26181605942992, "chrf_score": 63.65476934849392, "xcomet_score": 0.8650983572006226, "xcomet_qe_score": 0.9312171936035156, "metricx_score": 1.827560305595398, "metricx_qe_score": 2.4538393020629883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于自然语言处理(NLP)中存在位置性,我们能做些什么?", "metrics": {"bleu_score": 28.30789070123404, "chrf_score": 29.701714320211735, "xcomet_score": 0.8457212448120117, "xcomet_qe_score": 0.8822377920150757, "metricx_score": 3.5663275718688965, "metricx_qe_score": 1.9581626653671265, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对此有几点建议。", "metrics": {"bleu_score": 17.474335703431752, "chrf_score": 19.879610409139048, "xcomet_score": 0.9887105226516724, "xcomet_qe_score": 0.9731390476226807, "metricx_score": 0.12925037741661072, "metricx_qe_score": 0.13402260839939117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一,在整个研究过程中记录所有相关的设计选择。另一个", "metrics": {"bleu_score": 50.37208022655436, "chrf_score": 42.93996348794487, "xcomet_score": 0.8559529781341553, "xcomet_qe_score": 0.8092113137245178, "metricx_score": 3.991129159927368, "metricx_qe_score": 0.4494977593421936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "建议是从观点主义的角度进行自然语言处理(NLP)研究。", "metrics": {"bleu_score": 42.300073736306565, "chrf_score": 43.31550495903663, "xcomet_score": 0.8502518534660339, "xcomet_qe_score": 0.7728296518325806, "metricx_score": 0.7641181349754333, "metricx_qe_score": 1.17164945602417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业的数据集和模型。", "metrics": {"bleu_score": 80.96427216101601, "chrf_score": 72.63771349978248, "xcomet_score": 0.911953866481781, "xcomet_qe_score": 0.8777899146080017, "metricx_score": 0.8077837228775024, "metricx_qe_score": 0.9955554604530334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane计划。", "metrics": {"bleu_score": 50.51968359286048, "chrf_score": 41.302406935674156, "xcomet_score": 0.7493586540222168, "xcomet_qe_score": 0.7979084253311157, "metricx_score": 3.2502565383911133, "metricx_qe_score": 4.110206127166748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调的是,包容性的自然语言处理不仅仅是让所有技术为每个人服务。", "metrics": {"bleu_score": 31.066878208732792, "chrf_score": 35.02004930584576, "xcomet_score": 0.661332905292511, "xcomet_qe_score": 0.25319093465805054, "metricx_score": 1.9140375852584839, "metricx_qe_score": 3.0180869102478027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(Note: The translation maintains the", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.12942931056022644, "xcomet_qe_score": 0.08083419501781464, "metricx_score": 17.99726676940918, "metricx_qe_score": 21.46744728088379, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "original meaning and tone, adapting to Chinese grammar and vocabulary while preserving the line breaks.) 至此,我们的演讲告一段落", "metrics": {"bleu_score": 12.16411062125622, "chrf_score": 9.881990604338768, "xcomet_score": 0.18986353278160095, "xcomet_qe_score": 0.14357075095176697, "metricx_score": 12.001264572143555, "metricx_qe_score": 15.935701370239258, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但如果您想了解更多,欢迎查看我们的仪表板,获取最新分析结果和我们的论文。", "metrics": {"bleu_score": 55.78197861266046, "chrf_score": 50.420613324928865, "xcomet_score": 0.9513636827468872, "xcomet_qe_score": 0.9241237640380859, "metricx_score": 0.9995684623718262, "metricx_qe_score": 0.9055595397949219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自复旦大学的袁思宇。", "metrics": {"bleu_score": 53.93396304198033, "chrf_score": 36.55843078053718, "xcomet_score": 0.9522625207901001, "xcomet_qe_score": 0.90388423204422, "metricx_score": 1.0434480905532837, "metricx_qe_score": 0.8916164636611938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天要介绍我们的研究成果,题为《从大型语言模型中提炼脚本知识以应用于约束语言规划》。", "metrics": {"bleu_score": 46.30233022003119, "chrf_score": 47.59018434062725, "xcomet_score": 0.8940731287002563, "xcomet_qe_score": 0.7876793146133423, "metricx_score": 0.9674940705299377, "metricx_qe_score": 1.203973650932312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常通过遵循形式为保证脚本的步骤交互来规划他们的行动。 先", "metrics": {"bleu_score": 21.360710220235653, "chrf_score": 20.375822098184937, "xcomet_score": 0.5423133373260498, "xcomet_qe_score": 0.5712704062461853, "metricx_score": 8.710155487060547, "metricx_qe_score": 6.629437446594238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前的研究利用了语言模型来规划典型活动的抽象目标,", "metrics": {"bleu_score": 54.75197802593447, "chrf_score": 51.038638271147995, "xcomet_score": 0.7890670299530029, "xcomet_qe_score": 0.7269299626350403, "metricx_score": 2.8404784202575684, "metricx_qe_score": 5.071386814117432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如制作蛋糕,并证明了大型语言模型能够有效地将目标分解为步骤。", "metrics": {"bleu_score": 38.376797444789126, "chrf_score": 35.37535846519838, "xcomet_score": 0.2844359874725342, "xcomet_qe_score": 0.22580569982528687, "metricx_score": 3.3345131874084473, "metricx_qe_score": 1.7646405696868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以前的研究主要集中在为典型活动的抽象目标制定计划上。", "metrics": {"bleu_score": 55.34502966615894, "chrf_score": 53.46710740180818, "xcomet_score": 0.8462514877319336, "xcomet_qe_score": 0.8822493553161621, "metricx_score": 1.9062577486038208, "metricx_qe_score": 1.708331823348999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于具有特定约束的目标制定计划,例如制作巧克力蛋糕,仍然研究不足。", "metrics": {"bleu_score": 38.510283972861245, "chrf_score": 32.968171506363014, "xcomet_score": 0.8166172504425049, "xcomet_qe_score": 0.7438989281654358, "metricx_score": 1.2220717668533325, "metricx_qe_score": 1.4240363836288452, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受限语言规划的问题。 规划目标会受到不同限制的约束。", "metrics": {"bleu_score": 56.61767805985353, "chrf_score": 48.9925838322067, "xcomet_score": 0.9412000179290771, "xcomet_qe_score": 0.8608013987541199, "metricx_score": 1.914076805114746, "metricx_qe_score": 1.971461296081543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象的目标可以由多个具有多方面限制的实际具体目标继承。", "metrics": {"bleu_score": 24.547804277472792, "chrf_score": 21.27376262572047, "xcomet_score": 0.9838466644287109, "xcomet_qe_score": 0.9883235692977905, "metricx_score": 1.9098544120788574, "metricx_qe_score": 1.5619884729385376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个优秀的规划者应该编写合理且忠实于限制的脚本。", "metrics": {"bleu_score": 39.9084590709631, "chrf_score": 34.21384740972792, "xcomet_score": 0.9202818870544434, "xcomet_qe_score": 0.8839063048362732, "metricx_score": 2.0762226581573486, "metricx_qe_score": 2.740964412689209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并改进大型语言模型的受限语言规划能力。", "metrics": {"bleu_score": 78.8324467631105, "chrf_score": 71.24182139699383, "xcomet_score": 0.9718189239501953, "xcomet_qe_score": 0.9792938232421875, "metricx_score": 0.6621874570846558, "metricx_qe_score": 0.7382932901382446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有专门的目标数据集来支持我们的研究, 我们首先需要实现这些目标。正", "metrics": {"bleu_score": 49.2909838184736, "chrf_score": 42.0807998326569, "xcomet_score": 0.7872560620307922, "xcomet_qe_score": 0.8096526861190796, "metricx_score": 4.865903377532959, "metricx_qe_score": 3.1944382190704346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表格所示,我们使用多方面约束扩展抽象目标。对于人类在循环中的数据获取,使用InstructGPT。", "metrics": {"bleu_score": 32.014235765169715, "chrf_score": 41.86840645229574, "xcomet_score": 0.7757198214530945, "xcomet_qe_score": 0.7286841869354248, "metricx_score": 5.5027899742126465, "metricx_qe_score": 5.434564113616943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对100名特定女孩进行抽样,并评估来自大型本地模型的生成的脚本。", "metrics": {"bleu_score": 22.366895391935884, "chrf_score": 23.053086937932935, "xcomet_score": 0.5732400417327881, "xcomet_qe_score": 0.5343151092529297, "metricx_score": 7.058745861053467, "metricx_qe_score": 7.102302551269531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的整体准确性。", "metrics": {"bleu_score": 64.53174978135057, "chrf_score": 61.32467858545223, "xcomet_score": 0.9459816217422485, "xcomet_qe_score": 0.9768973588943481, "metricx_score": 0.8722731471061707, "metricx_qe_score": 0.9680761098861694, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有轻量级语言模型在为特定目标制定计划方面都取得了不尽如人意的结果。", "metrics": {"bleu_score": 32.06177002489116, "chrf_score": 31.61718159879235, "xcomet_score": 0.8851646780967712, "xcomet_qe_score": 0.8786436319351196, "metricx_score": 2.359734535217285, "metricx_qe_score": 2.643998384475708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析以探讨为什么基于行学习的模型会失败。", "metrics": {"bleu_score": 22.724148131567475, "chrf_score": 23.25756360995321, "xcomet_score": 0.8960003852844238, "xcomet_qe_score": 0.8813712000846863, "metricx_score": 0.9025662541389465, "metricx_qe_score": 0.8363910913467407, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中结果显示,生成的脚本在语义完整性方面表现可接受,但无法保证对约束条件的忠实度。", "metrics": {"bleu_score": 42.73726848522243, "chrf_score": 36.97070043611558, "xcomet_score": 0.9964858293533325, "xcomet_qe_score": 0.9866049289703369, "metricx_score": 1.017639398574829, "metricx_qe_score": 1.2860298156738281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了WikiHow中定义的更细分的议题类别的约束。", "metrics": {"bleu_score": 38.918855974501945, "chrf_score": 46.64830303767191, "xcomet_score": 0.8418898582458496, "xcomet_qe_score": 0.8632546067237854, "metricx_score": 1.8939015865325928, "metricx_qe_score": 1.945550799369812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热力图显示,指导性PD对不同类别的女孩的规划表现差异显著。", "metrics": {"bleu_score": 35.94039880288691, "chrf_score": 23.297933015479785, "xcomet_score": 0.6163519620895386, "xcomet_qe_score": 0.5350361466407776, "metricx_score": 6.953016757965088, "metricx_qe_score": 7.562769889831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前研究表明,轻量级风模型的输出质量存在高方差问题,导致性能不佳。", "metrics": {"bleu_score": 30.62610160012343, "chrf_score": 28.19949167913093, "xcomet_score": 0.7664676904678345, "xcomet_qe_score": 0.7164777517318726, "metricx_score": 5.327793598175049, "metricx_qe_score": 5.028892517089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用过生成Z滤波器的理念来提高生成质量。", "metrics": {"bleu_score": 38.1270292412149, "chrf_score": 32.953833474859664, "xcomet_score": 0.8063799142837524, "xcomet_qe_score": 0.7943302989006042, "metricx_score": 5.3343987464904785, "metricx_qe_score": 5.9492340087890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示不可处理CPT的约束类型及其示例,并根据种子抽象目标获得具体目标。", "metrics": {"bleu_score": 58.05767005939097, "chrf_score": 45.862686510838635, "xcomet_score": 0.6264287233352661, "xcomet_qe_score": 0.6116290092468262, "metricx_score": 5.218762397766113, "metricx_qe_score": 5.975159645080566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指导GPT超生成特定目标的案例脚本。", "metrics": {"bleu_score": 15.936357366603362, "chrf_score": 15.386386093792579, "xcomet_score": 0.6662552952766418, "xcomet_qe_score": 0.7182433605194092, "metricx_score": 5.883673667907715, "metricx_qe_score": 5.656455039978027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤模型来选择可行的脚本。", "metrics": {"bleu_score": 58.17070222427868, "chrf_score": 53.1598764134211, "xcomet_score": 0.9643338322639465, "xcomet_qe_score": 0.9523130655288696, "metricx_score": 0.8678579926490784, "metricx_qe_score": 1.0814999341964722, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为抽象的GPT嵌入向量,并计算余弦相似度和相似度分数,以衡量语义相似性。", "metrics": {"bleu_score": 59.777951252207465, "chrf_score": 49.223417867688894, "xcomet_score": 0.8166208267211914, "xcomet_qe_score": 0.7460355162620544, "metricx_score": 2.1872739791870117, "metricx_qe_score": 2.4411160945892334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们避免使用包含目标约束关键词的脚本。", "metrics": {"bleu_score": 55.43852213695471, "chrf_score": 51.60097221908482, "xcomet_score": 0.7346184253692627, "xcomet_qe_score": 0.7543836832046509, "metricx_score": 4.768651008605957, "metricx_qe_score": 4.963806629180908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "只有当目标得分在目标集合中最高时,我们才保留该脚本。", "metrics": {"bleu_score": 38.97574951973982, "chrf_score": 34.32777920698066, "xcomet_score": 0.7835948467254639, "xcomet_qe_score": 0.7526521682739258, "metricx_score": 1.6711565256118774, "metricx_qe_score": 2.7133708000183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,InstructZBT 可以生成更高质量的脚本。", "metrics": {"bleu_score": 81.37489370974959, "chrf_score": 71.10573084340234, "xcomet_score": 0.8482547998428345, "xcomet_qe_score": 0.8168147802352905, "metricx_score": 3.6148643493652344, "metricx_qe_score": 4.574015140533447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实性方面大大提高了规划能力。", "metrics": {"bleu_score": 73.58333687258859, "chrf_score": 68.87883380755825, "xcomet_score": 0.9244734048843384, "xcomet_qe_score": 0.9234838485717773, "metricx_score": 0.8783985376358032, "metricx_qe_score": 1.6032907962799072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型部署成本高昂,因此有必要增强较小和专业模型的语言规划能力。", "metrics": {"bleu_score": 51.355206332737595, "chrf_score": 44.78529513123325, "xcomet_score": 0.9755856990814209, "xcomet_qe_score": 0.9616248607635498, "metricx_score": 0.5255135893821716, "metricx_qe_score": 0.7813768982887268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是其最终实现的关键步骤。", "metrics": {"bleu_score": 33.400530952640366, "chrf_score": 28.716745216744314, "xcomet_score": 0.9361671209335327, "xcomet_qe_score": 0.9271187782287598, "metricx_score": 0.6188123822212219, "metricx_qe_score": 0.6646236181259155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究并不能实现针对具体目标的规划,而手动数据集标注成本高昂。", "metrics": {"bleu_score": 47.636735749489404, "chrf_score": 40.208124786632666, "xcomet_score": 0.9924912452697754, "xcomet_qe_score": 0.9773951768875122, "metricx_score": 0.9381422996520996, "metricx_qe_score": 1.462157964706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循象征性知识蒸馏的理念,从大型语言模型中蒸馏出受限的语言规划数据集。", "metrics": {"bleu_score": 45.70360995333725, "chrf_score": 38.370968067013685, "xcomet_score": 0.8407860994338989, "xcomet_qe_score": 0.7773655652999878, "metricx_score": 2.979275941848755, "metricx_qe_score": 2.959425210952759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们应用我们的方法来构建一个受限语言规划的数据集,命名为Codescript。", "metrics": {"bleu_score": 52.13733222075916, "chrf_score": 50.16818476015582, "xcomet_score": 0.858536958694458, "xcomet_qe_score": 0.8163559436798096, "metricx_score": 2.0401735305786133, "metricx_qe_score": 2.8372554779052734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了55,000个带有脚本的特定目标。", "metrics": {"bleu_score": 29.62789157394226, "chrf_score": 42.44778530648096, "xcomet_score": 0.888967752456665, "xcomet_qe_score": 0.8385939598083496, "metricx_score": 1.8689863681793213, "metricx_qe_score": 1.570450782775879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试站点的高质量,我们要求云端众包工人找出并修订错误的样本。", "metrics": {"bleu_score": 41.33756822254692, "chrf_score": 34.84874816777244, "xcomet_score": 0.7342308163642883, "xcomet_qe_score": 0.689479649066925, "metricx_score": 3.7555794715881348, "metricx_qe_score": 3.6185827255249023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图展示了代码脚本的约束分布。", "metrics": {"bleu_score": 37.5022891676693, "chrf_score": 23.913122665200788, "xcomet_score": 0.856412947177887, "xcomet_qe_score": 0.8248428106307983, "metricx_score": 3.078789234161377, "metricx_qe_score": 3.9585700035095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成的特定目标中表现出高度的赞同性。", "metrics": {"bleu_score": 47.901455811287484, "chrf_score": 34.1900259713097, "xcomet_score": 0.7070937156677246, "xcomet_qe_score": 0.6819919347763062, "metricx_score": 6.329947471618652, "metricx_qe_score": 6.808323860168457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过代码脚本,我们可以追踪更小但更专业的约束语言规划模型。", "metrics": {"bleu_score": 20.257904661864373, "chrf_score": 14.293623918462877, "xcomet_score": 0.6945379972457886, "xcomet_qe_score": 0.6764534711837769, "metricx_score": 5.77406644821167, "metricx_qe_score": 5.788824081420898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在成本率上应用T-file功能可以生成比大多数大型语言模型质量更高的脚本,这表明较小的模型在适当训练并应用于合适的数据集时,可以支持较大的模型。", "metrics": {"bleu_score": 38.73180972606642, "chrf_score": 30.735049378729162, "xcomet_score": 0.5984756946563721, "xcomet_qe_score": 0.6120799779891968, "metricx_score": 7.4886555671691895, "metricx_qe_score": 7.532083034515381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了受限语言规划问题。", "metrics": {"bleu_score": 72.83860464220109, "chrf_score": 71.99900522537406, "xcomet_score": 0.9069202542304993, "xcomet_qe_score": 0.8321535587310791, "metricx_score": 1.9986854791641235, "metricx_qe_score": 2.526510000228882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受限语言规划能力,并为大型语言模型开发了一种过度生成过滤方法。", "metrics": {"bleu_score": 63.393424969162815, "chrf_score": 55.612685434555196, "xcomet_score": 0.8912476301193237, "xcomet_qe_score": 0.8829815983772278, "metricx_score": 2.383878469467163, "metricx_qe_score": 3.3525240421295166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成高质量的脚本数据集,用于受限语言规划。", "metrics": {"bleu_score": 73.04997445393502, "chrf_score": 53.14066568268092, "xcomet_score": 0.9870063066482544, "xcomet_qe_score": 0.8990594148635864, "metricx_score": 2.213095188140869, "metricx_qe_score": 3.156728744506836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望CodeScript数据集能成为推进语言规划研究的有价值资源。", "metrics": {"bleu_score": 70.16116562610198, "chrf_score": 75.81247442974033, "xcomet_score": 0.9315885305404663, "xcomet_qe_score": 0.9419930577278137, "metricx_score": 0.7642547488212585, "metricx_qe_score": 0.9468439817428589, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查找更多关于代码脚本的详细信息。", "metrics": {"bleu_score": 48.0621629821948, "chrf_score": 35.281172892920694, "xcomet_score": 0.8338078260421753, "xcomet_qe_score": 0.8262996077537537, "metricx_score": 2.8399760723114014, "metricx_qe_score": 3.0248165130615234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是朱恒。", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 10.704692891649412, "xcomet_score": 0.8338499069213867, "xcomet_qe_score": 0.8417406678199768, "metricx_score": 0.13201507925987244, "metricx_qe_score": 0.2137874960899353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将宣读我们的研究论文,名为《2003年的内核命名实体识别器在2023年仍能良好运行吗?》", "metrics": {"bleu_score": 21.339518760946106, "chrf_score": 27.042911726286196, "xcomet_score": 0.7774972915649414, "xcomet_qe_score": 0.7755134701728821, "metricx_score": 3.393031597137451, "metricx_qe_score": 3.100192070007324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。让我们开始吧。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 95.15349630471859, "xcomet_score": 0.9835532903671265, "xcomet_qe_score": 0.9856215715408325, "metricx_score": 0.7622692584991455, "metricx_qe_score": 1.1052849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究论文探讨了泛化问题,采用命名实体识别任务(NER任务)作为研究对象。", "metrics": {"bleu_score": 29.49731696525339, "chrf_score": 32.299807159334456, "xcomet_score": 0.9661041498184204, "xcomet_qe_score": 0.9496990442276001, "metricx_score": 1.516722321510315, "metricx_qe_score": 2.672093152999878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型们几乎已经使用CONO 2003来开发命名实体识别(NER)近20年了。这自然而然地引发了几个问题。", "metrics": {"bleu_score": 18.31101699271826, "chrf_score": 23.785939559473388, "xcomet_score": 0.7563925981521606, "xcomet_qe_score": 0.7269169092178345, "metricx_score": 5.615161418914795, "metricx_qe_score": 5.443179130554199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据上?", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 41.583209046194796, "xcomet_score": 0.9268343448638916, "xcomet_qe_score": 0.9278680086135864, "metricx_score": 0.34351852536201477, "metricx_qe_score": 0.3737318217754364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新的标记器时,良好的泛化能力需要什么?", "metrics": {"bleu_score": 36.712753452897154, "chrf_score": 28.57000344168954, "xcomet_score": 0.7658227682113647, "xcomet_qe_score": 0.8603861331939697, "metricx_score": 0.6277921199798584, "metricx_qe_score": 0.6687824726104736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们观察到泛化能力差,是什么原因导致这些模型的性能下降?", "metrics": {"bleu_score": 33.59696856755292, "chrf_score": 27.646830580239534, "xcomet_score": 0.9905529022216797, "xcomet_qe_score": 0.9781142473220825, "metricx_score": 0.933985710144043, "metricx_qe_score": 0.7916854023933411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了CONO++数据集。这是", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 39.442039869723686, "xcomet_score": 0.7420022487640381, "xcomet_qe_score": 0.7702101469039917, "metricx_score": 5.254180908203125, "metricx_qe_score": 2.381535291671753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透社2020年新闻中收集的数据集,并根据相同的CONO 2003标注指南对其进行了标注。", "metrics": {"bleu_score": 44.4347877795939, "chrf_score": 38.51206466812809, "xcomet_score": 0.8123553991317749, "xcomet_qe_score": 0.7374720573425293, "metricx_score": 4.53278112411499, "metricx_qe_score": 3.9036409854888916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在Kano 2003数据集上对20多个模型进行了微调。", "metrics": {"bleu_score": 48.988642222409275, "chrf_score": 44.76394799804178, "xcomet_score": 0.8206828832626343, "xcomet_qe_score": 0.8273980617523193, "metricx_score": 5.128350257873535, "metricx_qe_score": 5.0636396408081055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Kano 03测试集和Kano++测试集上对它们进行了评估。", "metrics": {"bleu_score": 53.611312694955046, "chrf_score": 42.55390873751552, "xcomet_score": 0.6281436681747437, "xcomet_qe_score": 0.6917903423309326, "metricx_score": 6.759796142578125, "metricx_qe_score": 5.960765361785889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们计算了F1值的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 50.611072234388196, "chrf_score": 56.05677178685795, "xcomet_score": 0.9876506328582764, "xcomet_qe_score": 0.9893369674682617, "metricx_score": 0.6024377942085266, "metricx_qe_score": 0.8457399606704712, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的泛化需要什么?", "metrics": {"bleu_score": 38.71551944619038, "chrf_score": 31.114474244360814, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.35357362031936646, "metricx_qe_score": 0.43413427472114563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必不可少的:", "metrics": {"bleu_score": 21.22363344155404, "chrf_score": 19.743441993928133, "xcomet_score": 0.996601939201355, "xcomet_qe_score": 1.0, "metricx_score": 0.3380908668041229, "metricx_qe_score": 0.7353660464286804, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现变压器模型通常对新数据具有更好的泛化能力。", "metrics": {"bleu_score": 77.08956192084696, "chrf_score": 58.23139148108998, "xcomet_score": 0.9238463640213013, "xcomet_qe_score": 0.9233023524284363, "metricx_score": 1.2559252977371216, "metricx_qe_score": 0.974765956401825, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型规模。", "metrics": {"bleu_score": 63.8194179668201, "chrf_score": 59.09235313234967, "xcomet_score": 0.9858014583587646, "xcomet_qe_score": 0.8824237585067749, "metricx_score": 1.7902395725250244, "metricx_qe_score": 1.9330796003341675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常较大规模的模型具有更好的泛化能力。", "metrics": {"bleu_score": 39.99905887888884, "chrf_score": 32.87290630337263, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4113702178001404, "metricx_qe_score": 0.5005305409431458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们都知道微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 51.2691819618139, "chrf_score": 59.876557108622194, "xcomet_score": 0.9417649507522583, "xcomet_qe_score": 0.9170168042182922, "metricx_score": 3.607921600341797, "metricx_qe_score": 2.110914707183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现更多的微调示例实际上也导致了更好的泛化。", "metrics": {"bleu_score": 68.400614363836, "chrf_score": 59.602366681684074, "xcomet_score": 0.9595348238945007, "xcomet_qe_score": 0.8367005586624146, "metricx_score": 0.7031287550926208, "metricx_qe_score": 0.9073576927185059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "到我们的下一个问题,是什么原因导致某些模型的性能下降? 我们提出了两个假设。", "metrics": {"bleu_score": 55.14760049160392, "chrf_score": 50.17380145978104, "xcomet_score": 0.841120719909668, "xcomet_qe_score": 0.8283286094665527, "metricx_score": 1.4221017360687256, "metricx_qe_score": 1.850166916847229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是适应性过拟合,即由于反复使用相同的测试集而导致的过拟合。这通常表现为新测试集的收益递减。", "metrics": {"bleu_score": 67.2326674812407, "chrf_score": 62.530217755745056, "xcomet_score": 0.9202104806900024, "xcomet_qe_score": 0.8953779935836792, "metricx_score": 1.6390964984893799, "metricx_qe_score": 2.6253278255462646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间间隔逐渐增大而导致的性能下降。", "metrics": {"bleu_score": 65.12772949366064, "chrf_score": 61.0911977203752, "xcomet_score": 0.9695290327072144, "xcomet_qe_score": 0.8898763060569763, "metricx_score": 1.4166030883789062, "metricx_qe_score": 1.958247423171997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右图中看到,红色最佳拟合直线的斜率大于1。", "metrics": {"bleu_score": 43.3400185682041, "chrf_score": 35.293456774964014, "xcomet_score": 0.8750556707382202, "xcomet_qe_score": 0.8134164214134216, "metricx_score": 1.3237642049789429, "metricx_qe_score": 1.615194320678711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在卡诺特2003版上所做的每一次改进,在卡诺特++上都能转化为超过一次的改进,这说明不存在边际效益递减的情况。", "metrics": {"bleu_score": 19.445850936308346, "chrf_score": 19.732252990022438, "xcomet_score": 0.6640034914016724, "xcomet_qe_score": 0.7237716913223267, "metricx_score": 4.061013221740723, "metricx_qe_score": 4.043177604675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这向我们展示了这种情况下没有观察到适应性过拟合。", "metrics": {"bleu_score": 36.94654605751221, "chrf_score": 30.94274879744962, "xcomet_score": 0.8726509809494019, "xcomet_qe_score": 0.8224622011184692, "metricx_score": 2.509809970855713, "metricx_qe_score": 3.0167951583862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,关于时间漂移呢?", "metrics": {"bleu_score": 40.35278637463991, "chrf_score": 37.395096931234825, "xcomet_score": 0.9131477475166321, "xcomet_qe_score": 0.8928279280662537, "metricx_score": 1.164611577987671, "metricx_qe_score": 1.6916937828063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,重新训练或继续使用更近期数据预训练一些模型,发现性能随着时间差距的增大而下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 55.60086660959461, "chrf_score": 48.41455786495569, "xcomet_score": 0.8848580718040466, "xcomet_qe_score": 0.930919885635376, "metricx_score": 1.8998887538909912, "metricx_qe_score": 2.3324108123779297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型规模以及更多的微调示例。", "metrics": {"bleu_score": 84.66320077027171, "chrf_score": 82.81449173367797, "xcomet_score": 0.9898571968078613, "xcomet_qe_score": 0.9796369075775146, "metricx_score": 0.8873034119606018, "metricx_qe_score": 1.3054416179656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素是相互关联的。我们不能只拥有其中之一,而必须兼备其他因素。", "metrics": {"bleu_score": 26.73330573809266, "chrf_score": 23.707440196732847, "xcomet_score": 0.9168821573257446, "xcomet_qe_score": 0.9194754362106323, "metricx_score": 1.0273902416229248, "metricx_qe_score": 1.534133791923523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现这里的性能下降是由时间漂移引起的,令人惊讶的是,它不是由自适应过拟合引起的,尽管KONO 2003已经使用超过20年了。", "metrics": {"bleu_score": 58.1726904549759, "chrf_score": 50.460119587019555, "xcomet_score": 0.8139104843139648, "xcomet_qe_score": 0.7410663366317749, "metricx_score": 4.8455963134765625, "metricx_qe_score": 5.656525135040283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们论文标题中提出的问题,Connell 2003年的标注器在2023年仍然有效吗?", "metrics": {"bleu_score": 55.41238885869759, "chrf_score": 45.582660540909465, "xcomet_score": 0.8297803401947021, "xcomet_qe_score": 0.8274350166320801, "metricx_score": 2.555589199066162, "metricx_qe_score": 1.9419312477111816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,答案实际上是确凿的“是”。", "metrics": {"bleu_score": 18.605335292758287, "chrf_score": 22.518852031820423, "xcomet_score": 0.9697844982147217, "xcomet_qe_score": 0.9799066185951233, "metricx_score": 1.3367396593093872, "metricx_qe_score": 1.2418265342712402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望本文能够呼吁更多关于如何改进模型泛化能力的研究。", "metrics": {"bleu_score": 55.60335612120309, "chrf_score": 47.86225722914768, "xcomet_score": 0.9238120317459106, "xcomet_qe_score": 0.9266766905784607, "metricx_score": 1.0022426843643188, "metricx_qe_score": 0.8057245016098022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查阅我们的论文、数据集,如有任何疑问,欢迎随时与我联系。", "metrics": {"bleu_score": 51.598503138569704, "chrf_score": 42.60123451744849, "xcomet_score": 0.9915920495986938, "xcomet_qe_score": 0.9755833148956299, "metricx_score": 0.20257097482681274, "metricx_qe_score": 0.1886216104030609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9850989580154419, "xcomet_qe_score": 0.9753036499023438, "metricx_score": 0.186608225107193, "metricx_qe_score": 0.06603709608316422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将讨论我们在解决实体选择中间接引用表达式方面的工作,在这个工作中,我们引入了替代实体评分器(AltEntityScorers)。 (注:\"AltEntityScorers\" 保持原文,如有特定翻译需求,请提供更多上下文。", "metrics": {"bleu_score": 6.289570792563275, "chrf_score": 20.272655995963422, "xcomet_score": 0.20765212178230286, "xcomet_qe_score": 0.30522146821022034, "metricx_score": 5.891993522644043, "metricx_qe_score": 5.183775424957275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 我的名字是贾瓦德·侯赛尼,这是我与菲利普·拉德林斯基、西尔维亚·帕里蒂和安妮·路易斯共同完成的作品。", "metrics": {"bleu_score": 5.623975733856203, "chrf_score": 4.276191118164575, "xcomet_score": 0.9070917367935181, "xcomet_qe_score": 0.9207748174667358, "metricx_score": 2.3017525672912598, "metricx_qe_score": 2.1413979530334473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时的语言表达。", "metrics": {"bleu_score": 73.4411253978718, "chrf_score": 72.26580384604506, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5441896319389343, "metricx_qe_score": 0.6642886400222778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑以下替代问题:", "metrics": {"bleu_score": 12.067008283523638, "chrf_score": 10.590656015492618, "xcomet_score": 0.8792711496353149, "xcomet_qe_score": 0.8627591133117676, "metricx_score": 0.3784177303314209, "metricx_qe_score": 0.2156527042388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您是想选择“对我来说轻松”还是“我有一种感觉”", "metrics": {"bleu_score": 4.334264033674369, "chrf_score": 3.7813731995784376, "xcomet_score": 0.42697933316230774, "xcomet_qe_score": 0.39590707421302795, "metricx_score": 3.7057747840881348, "metricx_qe_score": 3.611645460128784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?在这里,用户想要在这两首歌中进行选择。", "metrics": {"bleu_score": 33.96444099810251, "chrf_score": 29.074379217744433, "xcomet_score": 0.8575305342674255, "xcomet_qe_score": 0.8122579455375671, "metricx_score": 0.9566729664802551, "metricx_qe_score": 1.12588369846344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用。例如,通过说歌曲的名字是《夜迷》(Yami),或者它的位置,第一个。", "metrics": {"bleu_score": 28.376011924747193, "chrf_score": 26.82159258786311, "xcomet_score": 0.5220664739608765, "xcomet_qe_score": 0.5116697549819946, "metricx_score": 8.542325019836426, "metricx_qe_score": 8.657000541687012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时,间接引用更合适,可以让对话更自然。例如,", "metrics": {"bleu_score": 11.1204122745073, "chrf_score": 15.0687021584014, "xcomet_score": 0.8373655080795288, "xcomet_qe_score": 0.8058401346206665, "metricx_score": 1.274275779724121, "metricx_qe_score": 1.4725871086120605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户记不住歌曲的名字时,就可能出现这种情况。", "metrics": {"bleu_score": 46.37822731358293, "chrf_score": 38.694034367212915, "xcomet_score": 0.9970613718032837, "xcomet_qe_score": 0.9933997392654419, "metricx_score": 0.28355515003204346, "metricx_qe_score": 0.2771121561527252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者发音过于相似,难以区分。", "metrics": {"bleu_score": 24.792487205599897, "chrf_score": 23.40751844934748, "xcomet_score": 0.9801307916641235, "xcomet_qe_score": 0.9764577150344849, "metricx_score": 0.7795825004577637, "metricx_qe_score": 0.2045377939939499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定偏好时。以下是直接差异的一", "metrics": {"bleu_score": 27.47912587281734, "chrf_score": 21.509916600183594, "xcomet_score": 0.734346330165863, "xcomet_qe_score": 0.3869348168373108, "metricx_score": 5.060774803161621, "metricx_qe_score": 1.8838739395141602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些例子。例如,较新的一个或不那么有活力的歌曲。", "metrics": {"bleu_score": 6.241729733923055, "chrf_score": 9.300679901997002, "xcomet_score": 0.4528953433036804, "xcomet_qe_score": 0.4746989905834198, "metricx_score": 9.892114639282227, "metricx_qe_score": 10.841056823730469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,对于评估大型语言模型(LLM)的实体理解能力也至关重要。", "metrics": {"bleu_score": 41.40011428776289, "chrf_score": 40.48256470354313, "xcomet_score": 0.9709587097167969, "xcomet_qe_score": 0.9488826990127563, "metricx_score": 2.177309036254883, "metricx_qe_score": 1.8764874935150146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不了解任何公开的、大规模的公共数据集适用于该任务。因此,我们通过众包标注方式收集了一个数据集。", "metrics": {"bleu_score": 19.45846736500148, "chrf_score": 21.556078050244977, "xcomet_score": 0.8079845905303955, "xcomet_qe_score": 0.8115144968032837, "metricx_score": 2.8260574340820312, "metricx_qe_score": 2.377819299697876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和食谱。", "metrics": {"bleu_score": 78.47574847738748, "chrf_score": 71.38793914595793, "xcomet_score": 0.9996216297149658, "xcomet_qe_score": 0.9887402057647705, "metricx_score": 0.2191678285598755, "metricx_qe_score": 0.3114262521266937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调使用卡通完成集来保持非正式性。", "metrics": {"bleu_score": 75.98033311259508, "chrf_score": 71.15403159029832, "xcomet_score": 0.7627266645431519, "xcomet_qe_score": 0.8117184638977051, "metricx_score": 4.527612686157227, "metricx_qe_score": 5.0543107986450195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通图画中有三个对话框。", "metrics": {"bleu_score": 53.3167536340577, "chrf_score": 51.22151958702646, "xcomet_score": 0.8915610909461975, "xcomet_qe_score": 0.8044968247413635, "metricx_score": 0.4283841848373413, "metricx_qe_score": 0.4374464452266693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框中,鲍勃说:“记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 54.35700692115411, "chrf_score": 48.877789279963196, "xcomet_score": 0.8934242725372314, "xcomet_qe_score": 0.8828657269477844, "metricx_score": 1.3489147424697876, "metricx_qe_score": 0.8999765515327454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由此,鲍勃为对话设定了背景。", "metrics": {"bleu_score": 6.649479326478728, "chrf_score": 6.163574825138827, "xcomet_score": 0.9592416286468506, "xcomet_qe_score": 0.9465450048446655, "metricx_score": 2.1149940490722656, "metricx_qe_score": 1.8972185850143433, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话框中,爱丽丝说:“你是说对我来说容易点,还是我有种预感?”", "metrics": {"bleu_score": 15.51422640328163, "chrf_score": 9.722364077917495, "xcomet_score": 0.6774181723594666, "xcomet_qe_score": 0.7259503602981567, "metricx_score": 3.0149059295654297, "metricx_qe_score": 2.496386766433716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "哪一个是替代问题。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 19.742063492063487, "xcomet_score": 0.8520238399505615, "xcomet_qe_score": 0.8515355587005615, "metricx_score": 1.433502197265625, "metricx_qe_score": 1.1972465515136719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,Bob 使用了一种间接引用来选择这些实体中的一个,例如,新的。", "metrics": {"bleu_score": 16.419136872156923, "chrf_score": 22.286422914381458, "xcomet_score": 0.5684273838996887, "xcomet_qe_score": 0.5168328285217285, "metricx_score": 5.774480819702148, "metricx_qe_score": 5.98776388168335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动生成第一个和第二个对话气泡,但第三个由标注人员填写。", "metrics": {"bleu_score": 40.498577356351355, "chrf_score": 35.008863794554685, "xcomet_score": 0.8141974210739136, "xcomet_qe_score": 0.8601424694061279, "metricx_score": 1.7080494165420532, "metricx_qe_score": 1.1902917623519897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话气泡从每个领域的手动提示中选择几个。", "metrics": {"bleu_score": 48.541082409110636, "chrf_score": 40.337072847147645, "xcomet_score": 0.6859073638916016, "xcomet_qe_score": 0.7590972185134888, "metricx_score": 2.798969030380249, "metricx_qe_score": 2.338247537612915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即备选问题,是按以下方式生成的。", "metrics": {"bleu_score": 14.400124446705304, "chrf_score": 16.626669796191788, "xcomet_score": 0.9265819787979126, "xcomet_qe_score": 0.966843843460083, "metricx_score": 0.32800549268722534, "metricx_qe_score": 0.42264053225517273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是来自维基百科的样本。", "metrics": {"bleu_score": 86.11735299633672, "chrf_score": 96.57574311968287, "xcomet_score": 0.9713319540023804, "xcomet_qe_score": 0.9262405037879944, "metricx_score": 0.7253305912017822, "metricx_qe_score": 0.857016384601593, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同采样方法。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 71.19713619713619, "xcomet_score": 0.9981815814971924, "xcomet_qe_score": 1.0, "metricx_score": 0.14823222160339355, "metricx_qe_score": 0.255437970161438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体之间会变得更加相似,通常更难进行歧义消除。", "metrics": {"bleu_score": 52.07958133637565, "chrf_score": 49.697708975088396, "xcomet_score": 0.8472758531570435, "xcomet_qe_score": 0.8218706846237183, "metricx_score": 3.600844621658325, "metricx_qe_score": 4.013828277587891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀的随机分布。", "metrics": {"bleu_score": 17.242221289766636, "chrf_score": 17.13032774353529, "xcomet_score": 0.9729572534561157, "xcomet_qe_score": 0.9503823518753052, "metricx_score": 0.8498873114585876, "metricx_qe_score": 0.8353193998336792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个情况是实体拥有相似的标题,例如两本书都名为《归来》。", "metrics": {"bleu_score": 26.087213732293563, "chrf_score": 21.207899693624483, "xcomet_score": 0.8603030443191528, "xcomet_qe_score": 0.8314706087112427, "metricx_score": 2.4666824340820312, "metricx_qe_score": 3.455512285232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是它们在维基百科上有相似的描述。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.997238039970398, "xcomet_qe_score": 0.9820467233657837, "metricx_score": 0.3877851665019989, "metricx_qe_score": 0.4526749849319458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当它们在维基百科上有相似的信息框或属性时", "metrics": {"bleu_score": 72.00242075875518, "chrf_score": 64.66198984505621, "xcomet_score": 0.933289110660553, "xcomet_qe_score": 0.988023042678833, "metricx_score": 1.0644596815109253, "metricx_qe_score": 1.273808479309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如相同的类型或相同的艺术家。", "metrics": {"bleu_score": 27.56001678723635, "chrf_score": 26.527940292762427, "xcomet_score": 0.939967691898346, "xcomet_qe_score": 0.8278558254241943, "metricx_score": 2.2040045261383057, "metricx_qe_score": 2.489840030670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向标注者展示这个替代问题时,他们知道这些实体的名称,但他们不一定了解这个实体。", "metrics": {"bleu_score": 55.09512055858448, "chrf_score": 45.916526564793564, "xcomet_score": 0.7410421967506409, "xcomet_qe_score": 0.7424563765525818, "metricx_score": 3.2981276512145996, "metricx_qe_score": 3.3419370651245117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是展示关于两个实体的背景知识。", "metrics": {"bleu_score": 58.6558665195849, "chrf_score": 48.12461273957527, "xcomet_score": 0.9710360765457153, "xcomet_qe_score": 0.8324273824691772, "metricx_score": 1.0238763093948364, "metricx_qe_score": 1.876013159751892, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地提供每个歌曲的Google搜索链接。 (Wǒmen suǒ zuò de shì zhǎnshì guānyú liǎng gè shíjì de bèijìng zhīshì. Duìyú gēqǔ, wǒmen jiǎndān dì tígōng měi gè gēqǔ de Google sōusuǒ liànjié.) 然后请标注人员至少听每首歌曲的一部分,并阅读有关每首歌曲的信息。", "metrics": {"bleu_score": 18.026741642357226, "chrf_score": 16.72006594363427, "xcomet_score": 0.4154530465602875, "xcomet_qe_score": 0.38751497864723206, "metricx_score": 5.559749126434326, "metricx_qe_score": 5.904974937438965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这是谷歌搜索结果中关于《轻松标注》歌曲的链接。 (请注意,实际的搜索结果内容没有被翻译,因为它取决于具体的搜索引擎结果。)", "metrics": {"bleu_score": 10.722896644776936, "chrf_score": 18.09243799502179, "xcomet_score": 0.6392799019813538, "xcomet_qe_score": 0.6498314738273621, "metricx_score": 5.60109281539917, "metricx_qe_score": 5.63301944732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示了来自维基百科的背景文本。", "metrics": {"bleu_score": 63.13476599314376, "chrf_score": 54.19210175474897, "xcomet_score": 0.9804984331130981, "xcomet_qe_score": 0.9193463325500488, "metricx_score": 0.9178977608680725, "metricx_qe_score": 1.5036317110061646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科再次展示它们的图像,以便注释器了解它们的样子。", "metrics": {"bleu_score": 37.78733748351261, "chrf_score": 30.786526785621763, "xcomet_score": 0.7835915088653564, "xcomet_qe_score": 0.6938985586166382, "metricx_score": 3.831116199493408, "metricx_qe_score": 3.916487693786621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注员从这些实体中选择一个,例如,这里的第一个,并使用三到五个间接指代表达方式来描述它们。", "metrics": {"bleu_score": 55.41867356541268, "chrf_score": 49.42353730091105, "xcomet_score": 0.8366327285766602, "xcomet_qe_score": 0.8069248199462891, "metricx_score": 3.068467855453491, "metricx_qe_score": 3.1704556941986084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,那首有钢琴音乐的。", "metrics": {"bleu_score": 18.52797255583095, "chrf_score": 18.7643186885898, "xcomet_score": 0.9973348379135132, "xcomet_qe_score": 0.991289496421814, "metricx_score": 0.5101467370986938, "metricx_qe_score": 0.6321661472320557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的几个例子。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9644975662231445, "xcomet_qe_score": 0.8701311945915222, "metricx_score": 0.5822314620018005, "metricx_qe_score": 1.956078290939331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,那首没有歌词的,不是那首有12岁男孩的,也不是那首虚构的,或来自阿塞拜疆的,等等。", "metrics": {"bleu_score": 31.95634113709065, "chrf_score": 29.68079107330583, "xcomet_score": 0.790702223777771, "xcomet_qe_score": 0.8459805250167847, "metricx_score": 1.755875587463379, "metricx_qe_score": 2.561591625213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Altentities语料库包含三个领域的6000个替代问题,并且有42,000个间接指代表达。", "metrics": {"bleu_score": 31.793039984225143, "chrf_score": 46.81585980232476, "xcomet_score": 0.6539275050163269, "xcomet_qe_score": 0.6038583517074585, "metricx_score": 4.972451686859131, "metricx_qe_score": 6.117326259613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用T5XLARGE模型的结果总结如下。", "metrics": {"bleu_score": 33.41796039044061, "chrf_score": 42.43233102736658, "xcomet_score": 0.93216472864151, "xcomet_qe_score": 0.9218188524246216, "metricx_score": 2.946129322052002, "metricx_qe_score": 2.6649937629699707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者完全相同的背景知识,那么准确率会非常高,大约在92%到95%之间。", "metrics": {"bleu_score": 63.66408032704386, "chrf_score": 57.24642421423021, "xcomet_score": 0.9042733907699585, "xcomet_qe_score": 0.9885326623916626, "metricx_score": 0.9912989735603333, "metricx_qe_score": 0.6616532802581787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这在现实中是不可行的。", "metrics": {"bleu_score": 13.545994273378138, "chrf_score": 23.700887198986063, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09328046441078186, "metricx_qe_score": 0.11937843263149261, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问部分重叠的背景知识,那么准确率在82%到87%之间,这更符合实际情况。", "metrics": {"bleu_score": 68.60400393421692, "chrf_score": 62.11771511402821, "xcomet_score": 0.9251508712768555, "xcomet_qe_score": 0.9216704964637756, "metricx_score": 0.7532787919044495, "metricx_qe_score": 1.0392247438430786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型仅能访问实体名称,那么准确率仅为60%。因此,改进的空间很大。", "metrics": {"bleu_score": 46.27036239487166, "chrf_score": 38.88123011284776, "xcomet_score": 0.9961289167404175, "xcomet_qe_score": 0.9917724132537842, "metricx_score": 1.4601764678955078, "metricx_qe_score": 2.2064268589019775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还展示了模型具有领域泛化能力。", "metrics": {"bleu_score": 35.55508425572383, "chrf_score": 29.61443703019254, "xcomet_score": 0.8834165334701538, "xcomet_qe_score": 0.8751844167709351, "metricx_score": 0.9266247153282166, "metricx_qe_score": 0.9063495993614197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是来自特伦托大学和布鲁诺·凯瑟尔基金会的萨拉·帕佩,我将简要介绍一下与马特奥·内格里和马可·图尔基合作撰写的《注意力作为同时语音翻译的指南》这篇论文。", "metrics": {"bleu_score": 41.45000714212619, "chrf_score": 31.204042458594376, "xcomet_score": 0.7648200988769531, "xcomet_qe_score": 0.7195141315460205, "metricx_score": 3.322641372680664, "metricx_qe_score": 2.540245532989502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同步口语翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.9202121496200562, "xcomet_qe_score": 0.9238082766532898, "metricx_score": 0.4620591104030609, "metricx_qe_score": 0.26660972833633423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同步口语翻译,或称为simulST,是指将口语实时翻译成另一种语言的文本的过程,从而实现跨语言交流。", "metrics": {"bleu_score": 58.220192695257516, "chrf_score": 58.6254409774292, "xcomet_score": 0.980008065700531, "xcomet_qe_score": 0.9779956340789795, "metricx_score": 1.9085649251937866, "metricx_qe_score": 2.4746501445770264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前SimulST模型的问题是什么?", "metrics": {"bleu_score": 19.156928817239653, "chrf_score": 47.80580394550984, "xcomet_score": 0.937305212020874, "xcomet_qe_score": 0.9294310212135315, "metricx_score": 0.6831821799278259, "metricx_qe_score": 1.0893099308013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常会训练特定的架构,引入需要优化的额外模块。 例如涉及不同优化目标", "metrics": {"bleu_score": 6.918451692157555, "chrf_score": 17.847318929215877, "xcomet_score": 0.7417359352111816, "xcomet_qe_score": 0.68397057056427, "metricx_score": 2.4625370502471924, "metricx_qe_score": 3.7229809761047363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的训练,长而复杂的训练程序会导致模型难以收敛", "metrics": {"bleu_score": 9.188199508336679, "chrf_score": 10.077399792509912, "xcomet_score": 0.2906925678253174, "xcomet_qe_score": 0.221755713224411, "metricx_score": 7.299252033233643, "metricx_qe_score": 11.105501174926758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从而影响训练效果。 训练和维护多个模型以达到不同的延迟等", "metrics": {"bleu_score": 49.09991753827355, "chrf_score": 44.397760527955064, "xcomet_score": 0.4141629636287689, "xcomet_qe_score": 0.18862000107765198, "metricx_score": 6.012948513031006, "metricx_qe_score": 5.512053489685059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "级,例如训练一个模型平均延迟为1秒,另一个模型为2秒延迟,以此类推。", "metrics": {"bleu_score": 36.16640715832343, "chrf_score": 30.068576699650478, "xcomet_score": 0.746846079826355, "xcomet_qe_score": 0.6643247604370117, "metricx_score": 4.312175273895264, "metricx_qe_score": 5.573551177978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先两个步骤,使用已存在的离线SD模型,无需重新训练或采用特定的单SD架构。", "metrics": {"bleu_score": 44.96632055650174, "chrf_score": 37.03204000479049, "xcomet_score": 0.6182843446731567, "xcomet_qe_score": 0.6303410530090332, "metricx_score": 8.042596817016602, "metricx_qe_score": 8.953021049499512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为每个延迟制度仅使用一个模型,并通过特定参数处理延迟。", "metrics": {"bleu_score": 48.35135794756827, "chrf_score": 40.12822911910494, "xcomet_score": 0.9358199834823608, "xcomet_qe_score": 0.9153788089752197, "metricx_score": 1.892972707748413, "metricx_qe_score": 1.3637477159500122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并利用模型通过音频输入与文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 55.15685261379873, "chrf_score": 49.11660069467308, "xcomet_score": 0.8096631765365601, "xcomet_qe_score": 0.6932018399238586, "metricx_score": 4.65159797668457, "metricx_qe_score": 4.474604606628418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")已获得的知识。您可以在右边看到一个示例。", "metrics": {"bleu_score": 19.554016376461796, "chrf_score": 16.531226482523838, "xcomet_score": 0.3771068751811981, "xcomet_qe_score": 0.4422212839126587, "metricx_score": 8.355430603027344, "metricx_qe_score": 10.902558326721191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出 ADAT(自适应注意力翻译)或编码器-解码器注意力机制。这是一种策略,我们根据注意力指向的位置决定是否输出部分翻译。", "metrics": {"bleu_score": 53.60816325668649, "chrf_score": 53.16957659306565, "xcomet_score": 0.6849531531333923, "xcomet_qe_score": 0.6250892877578735, "metricx_score": 5.491799831390381, "metricx_qe_score": 6.319301605224609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果张力未集中,即...其总和低于某个阈值α,向着语音帧的最后一条线,这意味着接收到的信息是...", "metrics": {"bleu_score": 30.51945709877691, "chrf_score": 24.431969213160826, "xcomet_score": 0.3399061858654022, "xcomet_qe_score": 0.307510107755661, "metricx_score": 11.00700569152832, "metricx_qe_score": 9.609396934509277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们接收到一个包含“I'm going to talk about”的语音片段,我们的模型会预测并生成德语翻译。 我们将查看交叉注意力权重。 我们会发现,前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,作为λ语音帧。", "metrics": {"bleu_score": 52.902002761063166, "chrf_score": 50.95048829839966, "xcomet_score": 0.6196476221084595, "xcomet_qe_score": 0.619145393371582, "metricx_score": 4.18158483505249, "metricx_qe_score": 5.07436466217041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个字将被省略。 由于跨注意力的总和超过了一个特定的阈值 α,我们不会发出最后一个词,而是等待另一个语音片段。", "metrics": {"bleu_score": 36.07101205853338, "chrf_score": 31.33731134794032, "xcomet_score": 0.6955493092536926, "xcomet_qe_score": 0.741425633430481, "metricx_score": 3.7241549491882324, "metricx_qe_score": 4.155591011047363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续进行,接收到另一个语音片段,我们的模型预测了另外三个词,我们将观察交叉注意力权重。 我们会发现,没有一个词指向最后一个lambda语音帧。", "metrics": {"bleu_score": 42.44160913331456, "chrf_score": 36.69605704633479, "xcomet_score": 0.730625331401825, "xcomet_qe_score": 0.7173188924789429, "metricx_score": 3.943331718444824, "metricx_qe_score": 3.9735336303710938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个单词将被发出。", "metrics": {"bleu_score": 47.855439210937384, "chrf_score": 38.86889545481519, "xcomet_score": 0.9408911466598511, "xcomet_qe_score": 0.9240285158157349, "metricx_score": 0.975865364074707, "metricx_qe_score": 1.911775827407837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们查看那项研究的主要结果, 我们在图表上绘制了同时性语音翻译的结果,图表的一侧使用蓝色表示翻译质量,另一侧表示平均延迟。 这就是延迟度量。我们还考虑了计算感知平均滞后,它考虑了模型预测输出的计算时间。", "metrics": {"bleu_score": 30.76621075757032, "chrf_score": 25.513722756138442, "xcomet_score": 0.7296655774116516, "xcomet_qe_score": 0.6717724204063416, "metricx_score": 5.366044044494629, "metricx_qe_score": 5.352832794189453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望在这一图表中,我们的曲线尽可能地高。 不仅如此", "metrics": {"bleu_score": 28.649003518069748, "chrf_score": 33.00731660197488, "xcomet_score": 0.4305853843688965, "xcomet_qe_score": 0.5224252939224243, "metricx_score": 3.292302131652832, "metricx_qe_score": 2.9720778465270996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们还希望它们左移。", "metrics": {"bleu_score": 24.85623706648625, "chrf_score": 22.442588128196757, "xcomet_score": 0.9778019189834595, "xcomet_qe_score": 0.9627153873443604, "metricx_score": 2.418365716934204, "metricx_qe_score": 2.684537649154663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将结果与适用于离线模型的适当策略进行比较,这些策略包括湿钥匙策略和本地一致性。", "metrics": {"bleu_score": 34.338432092571765, "chrf_score": 25.666118325542524, "xcomet_score": 0.7734298706054688, "xcomet_qe_score": 0.7149108648300171, "metricx_score": 5.083785533905029, "metricx_qe_score": 5.184814929962158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将与专为同时预翻译而设计的最新架构进行比较。", "metrics": {"bleu_score": 38.84975679002265, "chrf_score": 33.84483029048246, "xcomet_score": 0.9009069204330444, "xcomet_qe_score": 0.8764907121658325, "metricx_score": 1.3146001100540161, "metricx_qe_score": 1.6678975820541382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是同时性口语翻译策略在德语上的结果。", "metrics": {"bleu_score": 37.02730401700485, "chrf_score": 32.14025970586722, "xcomet_score": 0.7970110177993774, "xcomet_qe_score": 0.8275015354156494, "metricx_score": 1.7976106405258179, "metricx_qe_score": 1.450630784034729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,它比所有应用于离线模型的策略表现都更优异,因为这些曲线向左移动了。", "metrics": {"bleu_score": 36.850918513265924, "chrf_score": 38.57288909290382, "xcomet_score": 0.9866034984588623, "xcomet_qe_score": 0.9656809568405151, "metricx_score": 1.7338132858276367, "metricx_qe_score": 2.0505905151367188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际经过的时间或计算感知时间,那是最快的策略。", "metrics": {"bleu_score": 44.14777152519885, "chrf_score": 38.94060548785595, "xcomet_score": 0.8308639526367188, "xcomet_qe_score": 0.8578532338142395, "metricx_score": 3.6308724880218506, "metricx_qe_score": 3.7306666374206543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想发现更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 65.14613449066714, "chrf_score": 54.128468638917546, "xcomet_score": 0.9677166938781738, "xcomet_qe_score": 0.9603763222694397, "metricx_score": 0.6387813091278076, "metricx_qe_score": 0.45578014850616455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码和模型,并同时输出以促进我们工作的可复现性。", "metrics": {"bleu_score": 37.32132661175635, "chrf_score": 37.03713546226158, "xcomet_score": 0.8683708310127258, "xcomet_qe_score": 0.8366989493370056, "metricx_score": 1.136986255645752, "metricx_qe_score": 1.8303391933441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是英,我和我的同事志扬将向大家展示我们关于“多模态序列短时学习通过指令调优的多重改进”的研究。", "metrics": {"bleu_score": 30.900869091859384, "chrf_score": 20.951854903497246, "xcomet_score": 0.5942390561103821, "xcomet_qe_score": 0.6203664541244507, "metricx_score": 6.126387596130371, "metricx_qe_score": 6.827372074127197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索新的学习范式,即以参数和数据高效的方式,重用预训练的语言模型来处理不同的下游任务。", "metrics": {"bleu_score": 70.36010192613716, "chrf_score": 64.25937743162335, "xcomet_score": 0.8623789548873901, "xcomet_qe_score": 0.7453666925430298, "metricx_score": 1.2421162128448486, "metricx_qe_score": 1.9141602516174316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,通过遵循自然指令,指令调优使大型语言模型能够以零样本的方式在未见过的任务上表现出色。", "metrics": {"bleu_score": 37.19206989458383, "chrf_score": 35.391604993440254, "xcomet_score": 0.6888035535812378, "xcomet_qe_score": 0.6768097281455994, "metricx_score": 2.6371655464172363, "metricx_qe_score": 4.342490196228027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数关于指令调优的前期研究主要集中在提高语言仅有的序列图表性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 36.779146042024955, "chrf_score": 33.4884621287574, "xcomet_score": 0.8388156890869141, "xcomet_qe_score": 0.7945446968078613, "metricx_score": 2.376953601837158, "metricx_qe_score": 2.405963182449341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们希望探讨在多模态预训练模型上进行指令调优是否能够实际提升对未见多模态任务的泛化能力。", "metrics": {"bleu_score": 43.845987996879984, "chrf_score": 38.661822111511064, "xcomet_score": 0.9003031253814697, "xcomet_qe_score": 0.7722374796867371, "metricx_score": 1.2409812211990356, "metricx_qe_score": 1.4773221015930176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现自然语言处理(NLP)和多模态领域在指令数据集的可用性上存在显著差异。 ", "metrics": {"bleu_score": 40.99467567491974, "chrf_score": 37.65499503265895, "xcomet_score": 0.977617621421814, "xcomet_qe_score": 0.9605789184570312, "metricx_score": 0.5830453634262085, "metricx_qe_score": 0.6955047845840454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过1600个仅限语言的指令任务。", "metrics": {"bleu_score": 67.39047062564734, "chrf_score": 72.13158325845589, "xcomet_score": 0.9058570861816406, "xcomet_qe_score": 0.8427013754844666, "metricx_score": 0.8342660665512085, "metricx_qe_score": 1.3996577262878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大规模公开的多模态指令任务。", "metrics": {"bleu_score": 63.09433236258316, "chrf_score": 53.068690926663066, "xcomet_score": 0.9746540784835815, "xcomet_qe_score": 0.826064944267273, "metricx_score": 1.468718409538269, "metricx_qe_score": 2.521465539932251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这激励我们构建一个多模态指令微调数据集。", "metrics": {"bleu_score": 72.88605134576494, "chrf_score": 66.31255345214154, "xcomet_score": 0.9734252691268921, "xcomet_qe_score": 0.9711446762084961, "metricx_score": 0.8493427634239197, "metricx_qe_score": 1.117382526397705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此我们介绍 Multi-Instruct,这是第一个多模态指令调优基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个广泛类别。", "metrics": {"bleu_score": 44.924654949645955, "chrf_score": 44.349678504691774, "xcomet_score": 0.8104064464569092, "xcomet_qe_score": 0.82122802734375, "metricx_score": 1.4703975915908813, "metricx_qe_score": 1.563043236732483, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务源自21个现有的开源数据集,每个任务配备了5条专家撰写的指令。", "metrics": {"bleu_score": 70.44194182065888, "chrf_score": 65.45896973930962, "xcomet_score": 0.9890207052230835, "xcomet_qe_score": 0.9236463308334351, "metricx_score": 0.9590767621994019, "metricx_qe_score": 1.5045050382614136, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令调优,我们采用OFA作为基础模型,OFA是一个统一的多模态预训练模型。OFA使用统一的词", "metrics": {"bleu_score": 67.0800422355039, "chrf_score": 73.28078906254606, "xcomet_score": 0.7707836627960205, "xcomet_qe_score": 0.7243890762329102, "metricx_score": 6.256519317626953, "metricx_qe_score": 3.206307888031006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来处理语言、图像令牌以及边界框的坐标。", "metrics": {"bleu_score": 35.62070892542116, "chrf_score": 26.992245167819522, "xcomet_score": 0.36480283737182617, "xcomet_qe_score": 0.33759719133377075, "metricx_score": 8.441089630126953, "metricx_qe_score": 7.9370503425598145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们展示了我们 Multi-Instra 数据集中的几个示例实例。 为了统一处理多种输入和输出数据类型,", "metrics": {"bleu_score": 45.82573899277469, "chrf_score": 39.31054797805646, "xcomet_score": 0.7961615324020386, "xcomet_qe_score": 0.7491965293884277, "metricx_score": 3.773852825164795, "metricx_qe_score": 4.963050842285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,将所有任务统一为序列到序列的格式,在这个格式中", "metrics": {"bleu_score": 43.52598446478626, "chrf_score": 44.15608658132657, "xcomet_score": 0.7274702191352844, "xcomet_qe_score": 0.7730222940444946, "metricx_score": 4.412715435028076, "metricx_qe_score": 4.575922012329102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",输入文本、图像、指令和边界框在同一标记空间中表示。", "metrics": {"bleu_score": 66.22312744455333, "chrf_score": 60.90159563802837, "xcomet_score": 0.9496505260467529, "xcomet_qe_score": 0.920018196105957, "metricx_score": 1.431187629699707, "metricx_qe_score": 1.7373710870742798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好了,现在我要谈谈多模态指令微调。", "metrics": {"bleu_score": 64.8138893454484, "chrf_score": 60.97365869424692, "xcomet_score": 0.9153221845626831, "xcomet_qe_score": 0.9008731245994568, "metricx_score": 0.7149470448493958, "metricx_qe_score": 0.756284236907959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用来自9个组别的53个任务进行训练,并每个任务采样10,000个实例。", "metrics": {"bleu_score": 69.94591343867421, "chrf_score": 67.72240319445648, "xcomet_score": 0.940481424331665, "xcomet_qe_score": 0.9708324670791626, "metricx_score": 1.4498448371887207, "metricx_qe_score": 1.6388204097747803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在测试阶段,我们将整个常识推理组别保留用于测试,并从VQA和杂项组别中额外选择5个任务。", "metrics": {"bleu_score": 31.551552482598588, "chrf_score": 30.55545446904584, "xcomet_score": 0.6928654909133911, "xcomet_qe_score": 0.7055820822715759, "metricx_score": 4.2533183097839355, "metricx_qe_score": 3.609683036804199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务中的所有测试实例。", "metrics": {"bleu_score": 36.71536136769783, "chrf_score": 30.700619092604914, "xcomet_score": 0.8697065114974976, "xcomet_qe_score": 0.9448975324630737, "metricx_score": 1.0791749954223633, "metricx_qe_score": 1.6602849960327148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试集中共同采样20个任务作为NLP的未见任务。 因此,", "metrics": {"bleu_score": 47.962597099367116, "chrf_score": 43.693525961327815, "xcomet_score": 0.5510876178741455, "xcomet_qe_score": 0.46687838435173035, "metricx_score": 5.992769241333008, "metricx_qe_score": 5.048861503601074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一个预训练的OFA大型模型作为基础模型。", "metrics": {"bleu_score": 82.32490471721698, "chrf_score": 85.53120011521331, "xcomet_score": 0.9636353254318237, "xcomet_qe_score": 0.9542060494422913, "metricx_score": 1.465710163116455, "metricx_qe_score": 3.166785717010498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们混合了所有任务的所有实例。", "metrics": {"bleu_score": 87.25129388059685, "chrf_score": 81.134368154492, "xcomet_score": 0.819719672203064, "xcomet_qe_score": 0.7874776124954224, "metricx_score": 1.1428027153015137, "metricx_qe_score": 1.7761423587799072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与五个指令模板中的一个组合。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9054250717163086, "xcomet_qe_score": 0.8633378148078918, "metricx_score": 1.5055538415908813, "metricx_qe_score": 1.778053641319275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在测试过程中,对于每个任务,我们通过使用五种指令中的每一种分别评估模型,共进行五次实验。", "metrics": {"bleu_score": 35.33298491724876, "chrf_score": 28.97478300268289, "xcomet_score": 0.9605995416641235, "xcomet_qe_score": 0.9181696772575378, "metricx_score": 0.767304539680481, "metricx_qe_score": 1.192597508430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告了所有五个实验中性能的平均值、最大值以及性能的标准差。", "metrics": {"bleu_score": 26.651734452213976, "chrf_score": 21.595236969455854, "xcomet_score": 0.9568487405776978, "xcomet_qe_score": 0.953087329864502, "metricx_score": 2.5906994342803955, "metricx_qe_score": 2.1702141761779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模型分类任务,我们报告准确率。", "metrics": {"bleu_score": 66.60502379419339, "chrf_score": 59.946768410414364, "xcomet_score": 0.9906585216522217, "xcomet_qe_score": 0.9783502221107483, "metricx_score": 0.5188239812850952, "metricx_qe_score": 0.6833564639091492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模型生成任务,我们报告ROUGE-L。对于自然语言处理(NLP)任务,我们也报告ROUGE-L。", "metrics": {"bleu_score": 35.47270032255036, "chrf_score": 34.2283950281045, "xcomet_score": 0.846768319606781, "xcomet_qe_score": 0.8556919097900391, "metricx_score": 1.8448057174682617, "metricx_qe_score": 2.1426305770874023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一种额外的评估指标,称为敏感度。", "metrics": {"bleu_score": 52.88822910722114, "chrf_score": 45.880314377426714, "xcomet_score": 0.9862104654312134, "xcomet_qe_score": 0.997316837310791, "metricx_score": 0.4157159626483917, "metricx_qe_score": 0.4610040485858917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个指标测量模型在面对指令措辞微小变化时,能否一致地产生相同输出的能力,即模型对任务的稳定性。", "metrics": {"bleu_score": 29.01985391187419, "chrf_score": 28.791924744759427, "xcomet_score": 0.9554125070571899, "xcomet_qe_score": 0.9720762968063354, "metricx_score": 4.219273567199707, "metricx_qe_score": 5.501278877258301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要结果。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9698837995529175, "xcomet_qe_score": 0.88059002161026, "metricx_score": 0.1918793022632599, "metricx_qe_score": 0.3046000003814697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令微调可以显著提升OFA在场景多模态任务上的表现。", "metrics": {"bleu_score": 52.67368863197448, "chrf_score": 43.296285400768326, "xcomet_score": 0.9151018857955933, "xcomet_qe_score": 0.9583126306533813, "metricx_score": 1.877115249633789, "metricx_qe_score": 2.5204918384552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习也有助于指令微调。", "metrics": {"bleu_score": 58.89154292307022, "chrf_score": 54.64790193259773, "xcomet_score": 0.9807735681533813, "xcomet_qe_score": 0.8272433280944824, "metricx_score": 1.1596323251724243, "metricx_qe_score": 1.9940155744552612, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着任务数量的增加,我们可以看到模型的性能得到提升,同时敏感度也降低了。", "metrics": {"bleu_score": 40.15455798812797, "chrf_score": 34.80464355137307, "xcomet_score": 0.9875215291976929, "xcomet_qe_score": 0.9895141124725342, "metricx_score": 1.1091816425323486, "metricx_qe_score": 1.0080829858779907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也进行了一个实验,", "metrics": {"bleu_score": 40.35278637463991, "chrf_score": 33.91307136330957, "xcomet_score": 0.9543628692626953, "xcomet_qe_score": 0.9413754940032959, "metricx_score": 0.393255352973938, "metricx_qe_score": 0.3359813690185547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了一条指令与五条指令进行对比。", "metrics": {"bleu_score": 41.27548532835142, "chrf_score": 38.23865283523847, "xcomet_score": 0.9572871923446655, "xcomet_qe_score": 0.7603828310966492, "metricx_score": 0.9382592439651489, "metricx_qe_score": 2.3348228931427, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多的指令可以提高模型的整体性能,并显著降低其敏感度。", "metrics": {"bleu_score": 63.76897100442937, "chrf_score": 58.065247608591264, "xcomet_score": 0.999605655670166, "xcomet_qe_score": 1.0, "metricx_score": 0.7967923879623413, "metricx_qe_score": 0.8627405762672424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这展示了不同微调策略对模型敏感度的影响。", "metrics": {"bleu_score": 58.27569940616854, "chrf_score": 46.863494243706114, "xcomet_score": 0.9762451648712158, "xcomet_qe_score": 0.9720354080200195, "metricx_score": 0.9672296047210693, "metricx_qe_score": 1.3664666414260864, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,通过从自然指令数据集进行迁移学习,模型可以实现比原始OFA模型高得多的敏感度。", "metrics": {"bleu_score": 47.99371148556061, "chrf_score": 40.87059420405751, "xcomet_score": 0.9654321670532227, "xcomet_qe_score": 0.9141205549240112, "metricx_score": 1.857521414756775, "metricx_qe_score": 2.6409733295440674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还能看到,从 Nitro 指令数据集进行迁移学习可以帮助 OFA 在 Nitro 指令数据集上取得更优异的表现。", "metrics": {"bleu_score": 48.10908225746535, "chrf_score": 41.46631436514346, "xcomet_score": 0.6956311464309692, "xcomet_qe_score": 0.5704218149185181, "metricx_score": 5.363515377044678, "metricx_qe_score": 5.937840938568115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,我们提出了第一个大规模多模态指令调优数据集。我们显著提升了OFV的零样本能力,并探索了不同的迁移学习技术,展示了它们的优势。", "metrics": {"bleu_score": 53.2999063580718, "chrf_score": 46.875860627968734, "xcomet_score": 0.7395710945129395, "xcomet_qe_score": 0.7335306406021118, "metricx_score": 3.4012484550476074, "metricx_qe_score": 3.5736918449401855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一种称为灵敏度的新的度量标准。 (总的来说,我们提出了首个大规模多模态指令微调数据集。我们显著提高了OFV的零样本能力,并探索了不同的迁移学习技术,展示了它们的益处。我们设计了一种名为灵敏度的新的指标。", "metrics": {"bleu_score": 8.381277437004417, "chrf_score": 19.593309479443853, "xcomet_score": 0.15738846361637115, "xcomet_qe_score": 0.1556215137243271, "metricx_score": 8.110733032226562, "metricx_qe_score": 9.748697280883789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 再有一件事,我们正在收集一个更大的多模态指令调优数据集,包含大约150个额外的变体语言任务,我们将发布它们。", "metrics": {"bleu_score": 52.12674912703022, "chrf_score": 47.039961651487864, "xcomet_score": 0.685430109500885, "xcomet_qe_score": 0.7248374223709106, "metricx_score": 3.3289427757263184, "metricx_qe_score": 3.5907649993896484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯塔夫·辛哈,很高兴欢迎各位参加我们关于 ACL 2023 论文的演讲,题为", "metrics": {"bleu_score": 42.61863499529064, "chrf_score": 43.14302801752833, "xcomet_score": 0.749016284942627, "xcomet_qe_score": 0.651983916759491, "metricx_score": 4.8457465171813965, "metricx_qe_score": 4.432071208953857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型可接受性判断并不总能适应语境》。", "metrics": {"bleu_score": 40.48158028157285, "chrf_score": 36.84992144403927, "xcomet_score": 0.859140932559967, "xcomet_qe_score": 0.8166968822479248, "metricx_score": 1.6829872131347656, "metricx_qe_score": 2.6340408325195312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与John Gauthier、Aaron Muller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams的合作成果。", "metrics": {"bleu_score": 35.99429423708424, "chrf_score": 75.90557886445933, "xcomet_score": 0.8771318793296814, "xcomet_qe_score": 0.8422820568084717, "metricx_score": 1.4924265146255493, "metricx_qe_score": 1.2468504905700684, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们重温了最简对立范式。", "metrics": {"bleu_score": 12.401006001680987, "chrf_score": 14.478557504873294, "xcomet_score": 0.8474733829498291, "xcomet_qe_score": 0.8484472036361694, "metricx_score": 1.2414188385009766, "metricx_qe_score": 0.6522541046142578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最小对照范式基本上在可接受性判断的基础上评估语言模型,这", "metrics": {"bleu_score": 39.774589541974024, "chrf_score": 34.912788531577064, "xcomet_score": 0.7265380620956421, "xcomet_qe_score": 0.7006301879882812, "metricx_score": 5.625278472900391, "metricx_qe_score": 1.2888613939285278, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,例如BLIMP、句法、GEM,或者在刻板印象方面的可接受性,如交叉对照。", "metrics": {"bleu_score": 27.746509595676773, "chrf_score": 19.826490047958654, "xcomet_score": 0.5530886650085449, "xcomet_qe_score": 0.4916597306728363, "metricx_score": 5.37939977645874, "metricx_qe_score": 5.406723499298096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种最小对范式中,评估语言模型的典型方法是展示一个可接受的句子或一个语法正确的句子,然后展示一个可接受的句子或一个语法错误的句子。", "metrics": {"bleu_score": 63.133680575920415, "chrf_score": 56.875731849381616, "xcomet_score": 0.6535519957542419, "xcomet_qe_score": 0.5442970991134644, "metricx_score": 2.1278700828552246, "metricx_qe_score": 2.4182894229888916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型能够基本上将更高的概率分配给可接受的句子。", "metrics": {"bleu_score": 33.87144423286913, "chrf_score": 30.6808967349219, "xcomet_score": 0.9528704881668091, "xcomet_qe_score": 0.7171773910522461, "metricx_score": 1.336784839630127, "metricx_qe_score": 1.6677181720733643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 MPP 管道基本不允许我们评估模型对较长句子的接受程度。", "metrics": {"bleu_score": 85.1683409367816, "chrf_score": 81.21989951300296, "xcomet_score": 0.818332314491272, "xcomet_qe_score": 0.7644573450088501, "metricx_score": 1.363376498222351, "metricx_qe_score": 2.9560482501983643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,大型语言模型的上下文窗口越来越长。", "metrics": {"bleu_score": 70.23026466283676, "chrf_score": 74.55459048735162, "xcomet_score": 0.9749592542648315, "xcomet_qe_score": 0.9128842353820801, "metricx_score": 0.4945814609527588, "metricx_qe_score": 0.6217560768127441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们必须在整个上下文窗口中评估模型的可接受性。 这就是我们在这里试图做的事情。", "metrics": {"bleu_score": 55.61740757450153, "chrf_score": 50.54619569703009, "xcomet_score": 0.9065049886703491, "xcomet_qe_score": 0.8831988573074341, "metricx_score": 1.0960769653320312, "metricx_qe_score": 1.300909161567688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过让模型对越来越长的序列进行可接受性评估来重新访问 MPP 管道。 所以", "metrics": {"bleu_score": 37.89710450980445, "chrf_score": 37.524255513954365, "xcomet_score": 0.697482705116272, "xcomet_qe_score": 0.5468783378601074, "metricx_score": 5.061894416809082, "metricx_qe_score": 4.175141334533691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9963313341140747, "xcomet_qe_score": 0.9761532545089722, "metricx_score": 0.22746601700782776, "metricx_qe_score": 0.7089755535125732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们会模拟这些较长的序列。我们重新审视数据集本身,然后通过从这些数据集中选择可接受或不可接受的句子来重构句子。", "metrics": {"bleu_score": 66.16407453797774, "chrf_score": 60.28205167209445, "xcomet_score": 0.8568146228790283, "xcomet_qe_score": 0.6477148532867432, "metricx_score": 1.568535327911377, "metricx_qe_score": 2.4580888748168945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从BLIMP数据集中选择了一个典型的语法性对,来自附加岛案例。 (注:BLIMP数据集可能需要根据具体语境进行更准确的翻译,这里将其翻译为“语法性判断数据集”。)", "metrics": {"bleu_score": 16.692946633900053, "chrf_score": 25.569099191414534, "xcomet_score": 0.6772514581680298, "xcomet_qe_score": 0.6984587907791138, "metricx_score": 4.0268354415893555, "metricx_qe_score": 3.4440910816192627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重现更长的序列,并判断哪些序列可被接受,哪些具有相同的语法结构匹配。为此,", "metrics": {"bleu_score": 48.02640206796801, "chrf_score": 42.463367652860555, "xcomet_score": 0.7389603853225708, "xcomet_qe_score": 0.6518843770027161, "metricx_score": 4.287380218505859, "metricx_qe_score": 3.6687328815460205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从阿根廷岛提取语法正确的句子。 然后,我们将它作为前缀添加到可接受的查询和不可接受的查询中。 因此,", "metrics": {"bleu_score": 55.156646038081696, "chrf_score": 50.02392978429396, "xcomet_score": 0.48688846826553345, "xcomet_qe_score": 0.4287230372428894, "metricx_score": 6.992635250091553, "metricx_qe_score": 7.9861345291137695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情。这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.958369255065918, "xcomet_qe_score": 0.7863813042640686, "metricx_score": 1.3621975183486938, "metricx_qe_score": 1.7859324216842651, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过选择不同子集或不同数据集中的句子来实现同样的效果。", "metrics": {"bleu_score": 46.23472681073812, "chrf_score": 38.1080777276159, "xcomet_score": 0.9873369932174683, "xcomet_qe_score": 0.9559260606765747, "metricx_score": 0.9357630610466003, "metricx_qe_score": 1.5117775201797485, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的错配场景。 因此,", "metrics": {"bleu_score": 60.26080978557135, "chrf_score": 57.58514132353505, "xcomet_score": 0.786383867263794, "xcomet_qe_score": 0.7737975120544434, "metricx_score": 2.4849853515625, "metricx_qe_score": 2.335399866104126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关数据集,但不是您正在评估的同一数据集。", "metrics": {"bleu_score": 58.901155004394425, "chrf_score": 52.82938731275363, "xcomet_score": 0.9553557634353638, "xcomet_qe_score": 0.7772138118743896, "metricx_score": 1.1635935306549072, "metricx_qe_score": 2.1226277351379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于不可接受的情况,我们也可以这样做。", "metrics": {"bleu_score": 46.663612512230074, "chrf_score": 42.696972610224044, "xcomet_score": 0.9821330308914185, "xcomet_qe_score": 0.9639067649841309, "metricx_score": 0.5520765781402588, "metricx_qe_score": 0.599204957485199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全不相关的领域中选择句子,比如维基百科。 因此,这将有", "metrics": {"bleu_score": 48.23463821151288, "chrf_score": 47.14672678232145, "xcomet_score": 0.7872657775878906, "xcomet_qe_score": 0.700590193271637, "metricx_score": 5.468247890472412, "metricx_qe_score": 4.724738121032715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "助于我们了解模型的接受度判断是否实际上受到任何上下文的影响。 例如,上下文是否来自数据集的不同子集,或者它是否与我们正在查看的当前句子完全无关。", "metrics": {"bleu_score": 66.67116572555759, "chrf_score": 61.742135094212735, "xcomet_score": 0.8773878812789917, "xcomet_qe_score": 0.8530125021934509, "metricx_score": 2.576554536819458, "metricx_qe_score": 3.110647201538086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型表现如何?", "metrics": {"bleu_score": 8.646389260097964, "chrf_score": 8.441013286611183, "xcomet_score": 0.8383227586746216, "xcomet_qe_score": 0.844200611114502, "metricx_score": 0.9827795028686523, "metricx_qe_score": 0.27792325615882874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先我们看维基百科的句子,这些句子与当前查询对完全无关。我们发现,MPP的判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 58.97340823912891, "chrf_score": 49.35376571231545, "xcomet_score": 0.9599525928497314, "xcomet_qe_score": 0.8305445313453674, "metricx_score": 3.8371500968933105, "metricx_qe_score": 5.492770671844482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到1024,以最大化OPT和GPT-2模型的潜力。正如我们", "metrics": {"bleu_score": 54.95207706557383, "chrf_score": 72.9926384161144, "xcomet_score": 0.662927508354187, "xcomet_qe_score": 0.5991620421409607, "metricx_score": 3.982421875, "metricx_qe_score": 3.0981574058532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色虚线上看到的,MPP判断相对稳定。", "metrics": {"bleu_score": 56.32579400090421, "chrf_score": 53.64479417497995, "xcomet_score": 0.7717342972755432, "xcomet_qe_score": 0.779157280921936, "metricx_score": 3.1922855377197266, "metricx_qe_score": 4.645807266235352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在当我们从同一数据集选择句子时,会发生什么?", "metrics": {"bleu_score": 45.23534916420262, "chrf_score": 35.550578098764746, "xcomet_score": 0.9884246587753296, "xcomet_qe_score": 0.9081543684005737, "metricx_score": 0.7515804171562195, "metricx_qe_score": 1.4750953912734985, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们在这里从可接受和不可接受的领域中选择或创建句子,这些句子来自同一个气球或语法宝库数据集。", "metrics": {"bleu_score": 41.153881294462025, "chrf_score": 35.46205568306841, "xcomet_score": 0.5720471143722534, "xcomet_qe_score": 0.5668600797653198, "metricx_score": 6.1259026527404785, "metricx_qe_score": 5.910275459289551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们发现,当你添加可接受的前缀或不可接受的前缀时,MPP 判断会显著增加或减少。", "metrics": {"bleu_score": 66.6209750523344, "chrf_score": 63.16090922271347, "xcomet_score": 0.8213596343994141, "xcomet_qe_score": 0.7909804582595825, "metricx_score": 3.32600736618042, "metricx_qe_score": 2.779151678085327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,即当我们从指责他人的文本中选择来自同一现象的句子时,例如吉姆, (在中文里,\"Jim\" 保持不变,因为这是个人的名字。) 我们观察到模型的MPP判断出现大幅增加或大幅减少,这取决于所选前缀是否可接受。 现在这个,而且", "metrics": {"bleu_score": 32.12672832582097, "chrf_score": 33.614127686043865, "xcomet_score": 0.2554532289505005, "xcomet_qe_score": 0.048757154494524, "metricx_score": 11.200490951538086, "metricx_qe_score": 9.716751098632812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个影响会随着上下文长度而增大。这可能会影响到那些具有大上下文窗口的新型语言模型。", "metrics": {"bleu_score": 52.42152717502353, "chrf_score": 46.73037824626214, "xcomet_score": 0.9471877813339233, "xcomet_qe_score": 0.8603952527046204, "metricx_score": 1.2339844703674316, "metricx_qe_score": 1.5768852233886719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么匹配前缀会对语言模型的判断产生如此大的影响? 因此,", "metrics": {"bleu_score": 81.89850197975073, "chrf_score": 81.67160641054082, "xcomet_score": 0.8309619426727295, "xcomet_qe_score": 0.7695428133010864, "metricx_score": 3.7839598655700684, "metricx_qe_score": 1.712070107460022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,尝试在保持输入句子相关结构的同时,向输入中添加噪声。", "metrics": {"bleu_score": 47.994000390934616, "chrf_score": 41.6429405845811, "xcomet_score": 0.9685206413269043, "xcomet_qe_score": 0.9445996284484863, "metricx_score": 1.6742736101150513, "metricx_qe_score": 2.2526907920837402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了几次这样的扰动后, 我们发现,这些噪声实际上都没有使模型在显示MPP判断趋势的方式上改变其路径。", "metrics": {"bleu_score": 51.34552524973863, "chrf_score": 47.80743174386183, "xcomet_score": 0.8313058018684387, "xcomet_qe_score": 0.8425870537757874, "metricx_score": 4.026453018188477, "metricx_qe_score": 4.417586803436279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现这些模型对扰动和句子的敏感度相似。", "metrics": {"bleu_score": 37.110183402318796, "chrf_score": 33.68348083234488, "xcomet_score": 0.8712543249130249, "xcomet_qe_score": 0.8312703967094421, "metricx_score": 2.7163121700286865, "metricx_qe_score": 4.044674873352051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即当我们在可接受的范围内扰动句子时,我们观察到所有扰动的相似增加。当我们在不可接受的范围内扰动句子时,我们以类似的方式观察到MPP判断的减少。", "metrics": {"bleu_score": 44.84214683239726, "chrf_score": 40.29821870988833, "xcomet_score": 0.8066203594207764, "xcomet_qe_score": 0.690687894821167, "metricx_score": 4.2784857749938965, "metricx_qe_score": 4.659567832946777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们工作的关键结论是,语言模型对潜在的句法和语义特征敏感,", "metrics": {"bleu_score": 39.61911187047298, "chrf_score": 35.10795394017509, "xcomet_score": 0.8164188861846924, "xcomet_qe_score": 0.7174172401428223, "metricx_score": 3.827486038208008, "metricx_qe_score": 3.915264129638672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些特征在句子之间共享。 而当我们目前以短句和单句输入的方式进行MPP评估时,可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 48.82810633868039, "chrf_score": 48.46083067690248, "xcomet_score": 0.7794631719589233, "xcomet_qe_score": 0.5056173205375671, "metricx_score": 2.9258670806884766, "metricx_qe_score": 3.2650790214538574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解更多实验细节。", "metrics": {"bleu_score": 34.27163657253172, "chrf_score": 33.44303636597666, "xcomet_score": 0.9978342056274414, "xcomet_qe_score": 0.9995092153549194, "metricx_score": 0.10219424962997437, "metricx_qe_score": 0.11421225965023041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张宇生。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 49.88704001434205, "xcomet_score": 0.9104704856872559, "xcomet_qe_score": 0.8539036512374878, "metricx_score": 0.5312755703926086, "metricx_qe_score": 0.8543612957000732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的研究成果——《多自然语言和最小表示中的跨语言语义分析》。 (注:中文名字“Yusheng Zhang”保持不变,以符合学术材料中作者名字的翻译惯例。)", "metrics": {"bleu_score": 25.688445240197, "chrf_score": 31.766193415344844, "xcomet_score": 0.5022403001785278, "xcomet_qe_score": 0.5935776829719543, "metricx_score": 4.7430500984191895, "metricx_qe_score": 5.56089448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义分析是一项构建用户查询的语义表示的任务,例如 SQL 和 Lambda 计算。", "metrics": {"bleu_score": 57.02524287955773, "chrf_score": 56.87031571095179, "xcomet_score": 0.9655965566635132, "xcomet_qe_score": 0.9569717049598694, "metricx_score": 1.1213756799697876, "metricx_qe_score": 1.6736323833465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义分析的任务是将多种自然语言的查询翻译成多种意义表示。 吴浩天,博士:", "metrics": {"bleu_score": 62.077796572987225, "chrf_score": 67.28095639229284, "xcomet_score": 0.5014222860336304, "xcomet_qe_score": 0.4479101896286011, "metricx_score": 5.164992809295654, "metricx_qe_score": 5.889228343963623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将查询翻译成多种自然语言,以适应 Lambda 或 Fun QL 等系统。", "metrics": {"bleu_score": 46.7564078117556, "chrf_score": 55.55228967708435, "xcomet_score": 0.9218026399612427, "xcomet_qe_score": 0.900161862373352, "metricx_score": 2.1390738487243652, "metricx_qe_score": 3.0605432987213135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义分析模型是分别在有限的任务和应用数据集上提出和评估的。", "metrics": {"bleu_score": 69.20463819926105, "chrf_score": 58.47440871934968, "xcomet_score": 0.9959969520568848, "xcomet_qe_score": 0.9824415445327759, "metricx_score": 0.6534231901168823, "metricx_qe_score": 1.0421183109283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如, 存在某些自然语言的覆盖漏洞,", "metrics": {"bleu_score": 51.94264628599392, "chrf_score": 49.760631900427775, "xcomet_score": 0.7196624875068665, "xcomet_qe_score": 0.6258572340011597, "metricx_score": 5.350589752197266, "metricx_qe_score": 4.440052509307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失。 由于对某些微观表示的覆盖。", "metrics": {"bleu_score": 10.019016096930988, "chrf_score": 11.243934097348381, "xcomet_score": 0.5739942789077759, "xcomet_qe_score": 0.5506067276000977, "metricx_score": 7.334981441497803, "metricx_qe_score": 6.6430277824401855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "λ演算缺失。 或者它们只是在某些神经模型上进行评估。", "metrics": {"bleu_score": 53.19499812335497, "chrf_score": 46.44017731821571, "xcomet_score": 0.7675725221633911, "xcomet_qe_score": 0.7774912714958191, "metricx_score": 2.4560585021972656, "metricx_qe_score": 2.849182605743408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一的模型来评估它们。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.997307538986206, "xcomet_qe_score": 0.9824987649917603, "metricx_score": 0.5768303871154785, "metricx_qe_score": 0.8717049956321716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了一个示例。", "metrics": {"bleu_score": 34.38931217657843, "chrf_score": 22.328310182224907, "xcomet_score": 0.6947212815284729, "xcomet_qe_score": 0.30026382207870483, "metricx_score": 1.9661592245101929, "metricx_qe_score": 2.4563100337982178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提供了一个统一的数据集示例,用于多个自然语言和语义解析的跨语言联系以及意义表示。", "metrics": {"bleu_score": 54.68546299565086, "chrf_score": 45.072751034018246, "xcomet_score": 0.6908775568008423, "xcomet_qe_score": 0.7371350526809692, "metricx_score": 3.8423428535461426, "metricx_qe_score": 4.166217803955078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含九个不同领域的数据集,五个语义分析任务,八种语义表示形式,以及来自十五个语族的二十二种自然语言。", "metrics": {"bleu_score": 21.32061549412903, "chrf_score": 21.754232320371752, "xcomet_score": 0.981124758720398, "xcomet_qe_score": 0.9802558422088623, "metricx_score": 0.795191764831543, "metricx_qe_score": 1.1255403757095337, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了训练和评估的六种设置。", "metrics": {"bleu_score": 67.8301759715223, "chrf_score": 59.56205276123286, "xcomet_score": 0.9824798107147217, "xcomet_qe_score": 0.9433248043060303, "metricx_score": 1.0820890665054321, "metricx_qe_score": 2.2754859924316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是翻译测试。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9898306131362915, "xcomet_qe_score": 0.9781848192214966, "metricx_score": 0.22308200597763062, "metricx_qe_score": 0.39765846729278564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google翻译API将源语言翻译成目标语言,然后使用单语模型训练一个评估。", "metrics": {"bleu_score": 65.33279935059674, "chrf_score": 59.060323750574064, "xcomet_score": 0.852074384689331, "xcomet_qe_score": 0.7911466360092163, "metricx_score": 3.900193691253662, "metricx_qe_score": 3.9073398113250732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们使用英语查询训练英语模型,在推理阶段,我们通过 API 将德语查询翻译成英语,然后使用训练好的模型预测 SQL。", "metrics": {"bleu_score": 60.68973333424087, "chrf_score": 55.90250978637561, "xcomet_score": 0.9576395750045776, "xcomet_qe_score": 0.8697717785835266, "metricx_score": 0.9524688720703125, "metricx_qe_score": 1.8243408203125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模块。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 76.96368446368447, "xcomet_score": 0.8608987927436829, "xcomet_qe_score": 0.837360680103302, "metricx_score": 0.2960849404335022, "metricx_qe_score": 0.4536767601966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 73.6558994084271, "chrf_score": 67.66383822511001, "xcomet_score": 0.9070941209793091, "xcomet_qe_score": 0.8909211158752441, "metricx_score": 0.6414645910263062, "metricx_qe_score": 0.6743262410163879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过仅使用10%的训练数据训练单语模型来测试单语现场拍摄设置。", "metrics": {"bleu_score": 50.4503512960295, "chrf_score": 42.68407440737631, "xcomet_score": 0.6981368064880371, "xcomet_qe_score": 0.6798498630523682, "metricx_score": 3.874502658843994, "metricx_qe_score": 3.2489748001098633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试多语言模型,即我们为所有语言训练一个多语言模型。", "metrics": {"bleu_score": 78.86336751695258, "chrf_score": 81.60242962875334, "xcomet_score": 0.9537057876586914, "xcomet_qe_score": 0.9438531398773193, "metricx_score": 0.9408190250396729, "metricx_qe_score": 1.3921164274215698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询放在一起,用于训练多语言模型。而", "metrics": {"bleu_score": 60.22908469529742, "chrf_score": 52.87433668576767, "xcomet_score": 0.854297935962677, "xcomet_qe_score": 0.8068625330924988, "metricx_score": 2.2200920581817627, "metricx_qe_score": 2.4627392292022705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理阶段,我们可以使用这个模型。 将德语查询、中文查询或其他查询等翻译成中文。", "metrics": {"bleu_score": 43.8372977246803, "chrf_score": 44.10664324361183, "xcomet_score": 0.699853777885437, "xcomet_qe_score": 0.6570295095443726, "metricx_score": 2.637329339981079, "metricx_qe_score": 4.954864025115967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,并将其迁移到另一种语言。", "metrics": {"bleu_score": 34.23375720396188, "chrf_score": 28.5869448286931, "xcomet_score": 0.9299507141113281, "xcomet_qe_score": 0.8700629472732544, "metricx_score": 3.299403429031372, "metricx_qe_score": 3.739896774291992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们使用英语查询或英语和德语的少样本查询组合来训练多语言模型,并预测 SQL 输出。", "metrics": {"bleu_score": 85.88886238396086, "chrf_score": 81.70059645798088, "xcomet_score": 0.976229190826416, "xcomet_qe_score": 0.8839696645736694, "metricx_score": 1.1813088655471802, "metricx_qe_score": 1.4096031188964844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也发现了许多有趣的结果。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 66.57370407370408, "xcomet_score": 0.9968277215957642, "xcomet_qe_score": 0.9793797731399536, "metricx_score": 0.3356071412563324, "metricx_qe_score": 0.77958083152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在对单语模型的分析中,我们评估了两组模型。 包括编码器PDR,即多语种预训练编码器与基于指针解码器的解码器,例如XLMR加PDR和BERT加PDR。", "metrics": {"bleu_score": 38.86997364915031, "chrf_score": 31.855917288632135, "xcomet_score": 0.5242658853530884, "xcomet_qe_score": 0.5410950779914856, "metricx_score": 4.374950408935547, "metricx_qe_score": 4.144724369049072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语种预训练的编码器-解码器模型,例如mBART和MT5。", "metrics": {"bleu_score": 25.591565137314245, "chrf_score": 22.027130325169676, "xcomet_score": 0.8985108137130737, "xcomet_qe_score": 0.9342862963676453, "metricx_score": 0.9690366387367249, "metricx_qe_score": 1.9718528985977173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器-解码器在所有九个数据集上都取得了最佳性能。", "metrics": {"bleu_score": 46.51113009711743, "chrf_score": 32.265718721775706, "xcomet_score": 0.9894335269927979, "xcomet_qe_score": 0.9838986396789551, "metricx_score": 1.5358535051345825, "metricx_qe_score": 1.3199154138565063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在MT5和XLMR加上PDR的多语言环境中进行评估。", "metrics": {"bleu_score": 29.715678881302644, "chrf_score": 33.03046457031964, "xcomet_score": 0.8755593299865723, "xcomet_qe_score": 0.885111927986145, "metricx_score": 2.5337700843811035, "metricx_qe_score": 2.2679431438446045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合数据上进行训练,可以提升编码器-解码器或编码器-PDR的性能。", "metrics": {"bleu_score": 16.29844506584659, "chrf_score": 14.766501999103532, "xcomet_score": 0.8969436287879944, "xcomet_qe_score": 0.8890529870986938, "metricx_score": 1.74332857131958, "metricx_qe_score": 2.0309696197509766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,但英语在七个数据集中的性能下降,只在三个数据集中有所提升。", "metrics": {"bleu_score": 64.10443724956355, "chrf_score": 57.43964409725846, "xcomet_score": 0.924331545829773, "xcomet_qe_score": 0.9853038787841797, "metricx_score": 2.934811592102051, "metricx_qe_score": 2.3869380950927734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言性的诅咒。", "metrics": {"bleu_score": 22.311916633179763, "chrf_score": 21.09468296820821, "xcomet_score": 0.9170486330986023, "xcomet_qe_score": 0.9593082666397095, "metricx_score": 1.4766305685043335, "metricx_qe_score": 1.9277926683425903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言表现的差异。", "metrics": {"bleu_score": 63.39704064341255, "chrf_score": 57.81995781995782, "xcomet_score": 0.963089108467102, "xcomet_qe_score": 0.8401018381118774, "metricx_score": 0.30516117811203003, "metricx_qe_score": 0.5586168169975281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝色曲线表示跨语言少样本迁移。", "metrics": {"bleu_score": 45.76318980860575, "chrf_score": 46.961266052772956, "xcomet_score": 0.8689148426055908, "xcomet_qe_score": 0.800277829170227, "metricx_score": 1.6755144596099854, "metricx_qe_score": 3.4241178035736084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙色曲线表示跨语言零样本迁移,", "metrics": {"bleu_score": 50.31747626530137, "chrf_score": 55.750492982614205, "xcomet_score": 0.9544181823730469, "xcomet_qe_score": 0.8353053331375122, "metricx_score": 1.6407880783081055, "metricx_qe_score": 2.75429105758667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿色曲线表示单语言设置。", "metrics": {"bleu_score": 15.727800941615351, "chrf_score": 21.601119688434522, "xcomet_score": 0.9744383096694946, "xcomet_qe_score": 0.9761316776275635, "metricx_score": 0.8048281669616699, "metricx_qe_score": 1.111501932144165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过比较绿色和橙色曲线,我们发现在零样本设置下,跨语言迁移性能存在显著差距。通过比较蓝色和橙色曲线,我们发现在少样本设置下,迁移差距迅速缩小。", "metrics": {"bleu_score": 51.94093645819678, "chrf_score": 47.620197611531346, "xcomet_score": 0.8403991460800171, "xcomet_qe_score": 0.7033143043518066, "metricx_score": 1.349544882774353, "metricx_qe_score": 2.451075553894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器模型超越了先前的工作,或取得了可比的结果。", "metrics": {"bleu_score": 10.727295782787309, "chrf_score": 8.433794781217822, "xcomet_score": 0.9659152030944824, "xcomet_qe_score": 0.9596415758132935, "metricx_score": 1.7136082649230957, "metricx_qe_score": 1.6519091129302979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行描绘可以显著提升目标自然语言上的少样本性能。 我们发现,像CODIS和BLUE这样的多语种语言模型在跨语言语义分析任务中仍然不足。", "metrics": {"bleu_score": 48.4694889390685, "chrf_score": 38.056202958247034, "xcomet_score": 0.6108719706535339, "xcomet_qe_score": 0.5532727837562561, "metricx_score": 8.562228202819824, "metricx_qe_score": 9.309945106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们构建了 Examplar,一个用于多自然语言和主要表示法之间跨角度的语义解析的统一基准。", "metrics": {"bleu_score": 39.896163767405945, "chrf_score": 29.706438905852067, "xcomet_score": 0.6825813055038452, "xcomet_qe_score": 0.7326267957687378, "metricx_score": 5.23833703994751, "metricx_qe_score": 5.5640058517456055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语种语言模型进行了全面的基准研究,", "metrics": {"bleu_score": 70.22108284536812, "chrf_score": 59.07630205043998, "xcomet_score": 0.9754852056503296, "xcomet_qe_score": 0.9654116630554199, "metricx_score": 1.249304175376892, "metricx_qe_score": 1.9747228622436523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果显示了许多有趣的发现等", "metrics": {"bleu_score": 72.41907707454052, "chrf_score": 73.18090547857871, "xcomet_score": 0.85756516456604, "xcomet_qe_score": 0.7907236814498901, "metricx_score": 1.9979556798934937, "metricx_qe_score": 1.3959616422653198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的聆听。", "metrics": {"bleu_score": 46.30777161991026, "chrf_score": 37.73656898656898, "xcomet_score": 0.8018722534179688, "xcomet_qe_score": 0.7317452430725098, "metricx_score": 5.9059624671936035, "metricx_qe_score": 9.609138488769531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是大卫·维拉尔。我将简要介绍一篇论文,题为《哼鸣平台翻译:评估策略与性能》。", "metrics": {"bleu_score": 20.418283806485558, "chrf_score": 16.544826106204454, "xcomet_score": 0.7129991054534912, "xcomet_qe_score": 0.6700202226638794, "metricx_score": 6.58189058303833, "metricx_qe_score": 5.808370590209961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与谷歌翻译团队同事的合作成果。", "metrics": {"bleu_score": 51.234156791016495, "chrf_score": 47.98446338379667, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.48679637908935547, "metricx_qe_score": 0.1619408130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PARM 是一个于去年 2022 年发布的大型语言模型,拥有 5400 亿个参数。", "metrics": {"bleu_score": 45.35287618242756, "chrf_score": 49.722744886650986, "xcomet_score": 0.9873230457305908, "xcomet_qe_score": 0.9329209923744202, "metricx_score": 3.319920778274536, "metricx_qe_score": 5.027975082397461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是在一个包含 7800 亿份文档的大规模文本集合上训练的。", "metrics": {"bleu_score": 34.961722361745295, "chrf_score": 41.403601640919305, "xcomet_score": 0.8287159204483032, "xcomet_qe_score": 0.8030253052711487, "metricx_score": 2.080265760421753, "metricx_qe_score": 2.71702241897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在出版之时,它已在数百项自然语言处理任务中达到前沿水平。", "metrics": {"bleu_score": 9.25159978069645, "chrf_score": 9.98185060769719, "xcomet_score": 0.9945602416992188, "xcomet_qe_score": 0.9961386919021606, "metricx_score": 1.325896143913269, "metricx_qe_score": 1.1706382036209106, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们呈现了大型语言模型在机器翻译中首次系统性的提示研究。", "metrics": {"bleu_score": 25.572642416297196, "chrf_score": 24.871405024192793, "xcomet_score": 0.7498859167098999, "xcomet_qe_score": 0.7368638515472412, "metricx_score": 2.378509998321533, "metricx_qe_score": 3.2476415634155273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用AMT社区的最佳实践方法评估了这些模型的翻译能力。", "metrics": {"bleu_score": 35.36902582681573, "chrf_score": 30.9783825343531, "xcomet_score": 0.8777309656143188, "xcomet_qe_score": 0.7864943742752075, "metricx_score": 4.589004039764404, "metricx_qe_score": 5.7312469482421875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统。在WMT评估中,表现最佳的系统是......", "metrics": {"bleu_score": 37.95568753467154, "chrf_score": 36.05105117143475, "xcomet_score": 0.9112224578857422, "xcomet_qe_score": 0.8401693105697632, "metricx_score": 1.8760554790496826, "metricx_qe_score": 2.944823741912842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用最先进的神经机器翻译度量,并额外展示基于专家的人类评估结果。", "metrics": {"bleu_score": 64.76102747181675, "chrf_score": 58.270866240726896, "xcomet_score": 0.8355311155319214, "xcomet_qe_score": 0.8236339092254639, "metricx_score": 2.4556326866149902, "metricx_qe_score": 3.6243057250976562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们为 PROM 选择策略提供一些建议。", "metrics": {"bleu_score": 21.82080176475103, "chrf_score": 22.091219606302445, "xcomet_score": 0.840340256690979, "xcomet_qe_score": 0.8272126317024231, "metricx_score": 4.47061014175415, "metricx_qe_score": 4.267305374145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型(LLM)的翻译性能有很大影响。我们可以通过一个简单的实验来观察这一点,在该实验中,我们使用单次提示,并为每句话提供两个不同的提示。", "metrics": {"bleu_score": 37.12679907631838, "chrf_score": 37.95997047904731, "xcomet_score": 0.9233475923538208, "xcomet_qe_score": 0.9089063405990601, "metricx_score": 1.0015438795089722, "metricx_qe_score": 1.4074629545211792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在1000个句子中,", "metrics": {"bleu_score": 30.895757752065407, "chrf_score": 45.3545034322207, "xcomet_score": 0.891600489616394, "xcomet_qe_score": 0.729491114616394, "metricx_score": 6.866838455200195, "metricx_qe_score": 9.45357608795166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数(516个)观察到的差异超过了一个模糊点。", "metrics": {"bleu_score": 5.618923497225256, "chrf_score": 12.777515918848822, "xcomet_score": 0.7408352494239807, "xcomet_qe_score": 0.47243785858154297, "metricx_score": 6.215968608856201, "metricx_qe_score": 6.150783538818359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可以达到40个模糊点。", "metrics": {"bleu_score": 32.22538601891171, "chrf_score": 23.148500068613558, "xcomet_score": 0.8078522086143494, "xcomet_qe_score": 0.8129090070724487, "metricx_score": 4.80564546585083, "metricx_qe_score": 2.5721869468688965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个良好的提示策略非常重要。", "metrics": {"bleu_score": 47.20758038942709, "chrf_score": 41.584577649209166, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2462974488735199, "metricx_qe_score": 0.3821769654750824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了一种五次射击提示策略,即我们只是用句子所属的语言标记提供给系统的每个句子。", "metrics": {"bleu_score": 41.77291036283608, "chrf_score": 38.61941922351807, "xcomet_score": 0.7500177025794983, "xcomet_qe_score": 0.7027087211608887, "metricx_score": 4.429849624633789, "metricx_qe_score": 5.498388290405273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们从德语翻译成英语,德语原句用德语冒号标记,英语翻译用英语冒号表示。", "metrics": {"bleu_score": 39.072749461255064, "chrf_score": 28.268725712287225, "xcomet_score": 0.9746407270431519, "xcomet_qe_score": 0.979902982711792, "metricx_score": 1.3342143297195435, "metricx_qe_score": 1.17317533493042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次短提示的情况下,提示的实际形式对结果影响不大。", "metrics": {"bleu_score": 34.02858386912342, "chrf_score": 28.56405814722401, "xcomet_score": 0.9029994010925293, "xcomet_qe_score": 0.8914726376533508, "metricx_score": 0.9320433139801025, "metricx_qe_score": 0.707047700881958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零次和一次提示非常关键。当我们像我们的情况那", "metrics": {"bleu_score": 8.282282660969603, "chrf_score": 8.487044356609573, "xcomet_score": 0.5349397659301758, "xcomet_qe_score": 0.6167237758636475, "metricx_score": 7.842045307159424, "metricx_qe_score": 5.033409595489502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "样进行五次提示时,实际提示的形式几乎没有变化。", "metrics": {"bleu_score": 29.176300840900787, "chrf_score": 27.111063215477436, "xcomet_score": 0.8467340469360352, "xcomet_qe_score": 0.7986462116241455, "metricx_score": 3.998723030090332, "metricx_qe_score": 5.518301010131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正是这些例子承载了大部分的分量。", "metrics": {"bleu_score": 3.8275613602956104, "chrf_score": 6.944444444444445, "xcomet_score": 0.7745562791824341, "xcomet_qe_score": 0.9063411951065063, "metricx_score": 2.1001501083374023, "metricx_qe_score": 1.6453888416290283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,例子质量比与源句子的相似度更为重要。", "metrics": {"bleu_score": 67.59482608831081, "chrf_score": 62.55225853734873, "xcomet_score": 0.9265334606170654, "xcomet_qe_score": 0.9221253395080566, "metricx_score": 0.9351335763931274, "metricx_qe_score": 0.6856894493103027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,重要的是从高质量的翻译中选择例子。", "metrics": {"bleu_score": 53.95713951720204, "chrf_score": 49.05816420135306, "xcomet_score": 0.9366625547409058, "xcomet_qe_score": 0.9372061491012573, "metricx_score": 0.5782352685928345, "metricx_qe_score": 0.734567403793335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较从 WMT 评估的训练数据或开发数据中选择的提示。 开发数据(dev data)经过", "metrics": {"bleu_score": 41.304345025653454, "chrf_score": 39.174085513871304, "xcomet_score": 0.4075184464454651, "xcomet_qe_score": 0.36100441217422485, "metricx_score": 8.007941246032715, "metricx_qe_score": 6.970226287841797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了更细致的整理和质量控制,比训练数据(training data)更加完善,其结果也更理想。因此,", "metrics": {"bleu_score": 12.180838505033421, "chrf_score": 13.212222094267286, "xcomet_score": 0.20600222051143646, "xcomet_qe_score": 0.2290084958076477, "metricx_score": 10.266390800476074, "metricx_qe_score": 8.125165939331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据时,性能表现会更好。", "metrics": {"bleu_score": 49.35578819979934, "chrf_score": 41.968031968031966, "xcomet_score": 0.8963585495948792, "xcomet_qe_score": 0.9285166263580322, "metricx_score": 0.760666012763977, "metricx_qe_score": 1.3086069822311401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,专业的最新系统在翻译质量上具有显著优势,与Palm翻译相比更为出色。", "metrics": {"bleu_score": 11.720029138027304, "chrf_score": 14.037045745173085, "xcomet_score": 0.8872687816619873, "xcomet_qe_score": 0.8745828866958618, "metricx_score": 4.189594745635986, "metricx_qe_score": 3.560763359069824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管Palm翻译已经接近商业翻译系统的水准。", "metrics": {"bleu_score": 17.636478563502965, "chrf_score": 18.40906969370188, "xcomet_score": 0.7852557897567749, "xcomet_qe_score": 0.8314728736877441, "metricx_score": 6.829002857208252, "metricx_qe_score": 3.5978684425354004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择使用Google翻译进行评估。", "metrics": {"bleu_score": 70.61595846869687, "chrf_score": 57.11817644226704, "xcomet_score": 0.9696316719055176, "xcomet_qe_score": 0.9130256772041321, "metricx_score": 0.46855807304382324, "metricx_qe_score": 0.6838024258613586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用MQM框架进行的人工启用所获得的见解是,PALM的流利度可与最先进的系统相媲美,但主要差异来自于准确度。", "metrics": {"bleu_score": 51.936756095244846, "chrf_score": 47.449965337960165, "xcomet_score": 0.7365519404411316, "xcomet_qe_score": 0.7623746991157532, "metricx_score": 3.399049758911133, "metricx_qe_score": 3.608733654022217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其常见的一种错误是遗漏错误。", "metrics": {"bleu_score": 57.02822264405544, "chrf_score": 46.983073350670544, "xcomet_score": 0.9041687250137329, "xcomet_qe_score": 0.9158245325088501, "metricx_score": 1.8269771337509155, "metricx_qe_score": 0.9628874063491821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,看来 Palm 有时会选择省略源句中某些在翻译中不必要的成分,以产生更通顺的翻译。", "metrics": {"bleu_score": 7.053882052523648, "chrf_score": 11.54908173997676, "xcomet_score": 0.8719832897186279, "xcomet_qe_score": 0.8617415428161621, "metricx_score": 3.9899916648864746, "metricx_qe_score": 3.687011241912842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAN 的风格外类别比现有的最先进系统低,这是一个额外的信号。 PARM 确实能提供非常流畅的输出,但准确性上仍存在一些问题。", "metrics": {"bleu_score": 33.51197616698924, "chrf_score": 27.276154586735558, "xcomet_score": 0.745557427406311, "xcomet_qe_score": 0.6789331436157227, "metricx_score": 6.670346260070801, "metricx_qe_score": 6.694397926330566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这次非常简短的概述。", "metrics": {"bleu_score": 15.84213253833617, "chrf_score": 17.640374328272415, "xcomet_score": 0.9207900166511536, "xcomet_qe_score": 0.8659988045692444, "metricx_score": 0.5585209131240845, "metricx_qe_score": 0.9517507553100586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欲了解更多详细信息,请参加论文的完整报告。", "metrics": {"bleu_score": 44.96195695057615, "chrf_score": 38.61028976953651, "xcomet_score": 0.7896958589553833, "xcomet_qe_score": 0.8298498392105103, "metricx_score": 3.1444151401519775, "metricx_qe_score": 2.0433146953582764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9730953574180603, "xcomet_qe_score": 0.9623823165893555, "metricx_score": 0.18832087516784668, "metricx_qe_score": 0.09758952260017395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自德国萨尔兰大学(Saarland University)的博士生大伟。", "metrics": {"bleu_score": 49.831162551286646, "chrf_score": 37.47912427552381, "xcomet_score": 0.8493751287460327, "xcomet_qe_score": 0.9102159142494202, "metricx_score": 2.2106423377990723, "metricx_qe_score": 1.2899203300476074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这段视频中,我想向大家介绍我们最近的研究成果——《比你想象的更脆弱:对每周监督学习的批判性探讨》。", "metrics": {"bleu_score": 38.79762447070079, "chrf_score": 36.960107807670546, "xcomet_score": 0.8365429639816284, "xcomet_qe_score": 0.846461296081543, "metricx_score": 4.147763729095459, "metricx_qe_score": 4.7056097984313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本研究对每周监督学习(weekly supervised learning)的方法进行了深入分析和评估。我们发现,尽管这种学习方式在许多应用中表现出色,但它也存在一些潜在的弱点和局限性。通过对现有文献的综合审查和我们自己的实验研究,我们揭示了每周监督学习在特定场景下可能面临的挑战和问题。 我们的研究结果表明,在处理动态变化环境或需要长期记忆的任务时,每周监督学习可能表现出较弱的泛化能力和稳定性。此外,我们还探讨了这种学习方法在数据效率和计算资源方面的要求,并提出了可能的改进策略。 希望我们的这项研究能为机器学习领域的从业者和研究者提供有价值的见解,并促进对监督学习不同变体的进一步探索和优化。 这是与小玉生、马里奥·斯穆斯巴赫、吉娅·斯泰芬和DT", "metrics": {"bleu_score": 0.36399194362525356, "chrf_score": 5.133135848197927, "xcomet_score": 0.33272382616996765, "xcomet_qe_score": 0.296305775642395, "metricx_score": 6.6403422355651855, "metricx_qe_score": 5.37490701675415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "施劳克尔合作的成果。 我想简要介绍一下弱监督和弱监督学习。", "metrics": {"bleu_score": 54.60241725418134, "chrf_score": 76.95201508546471, "xcomet_score": 0.25316691398620605, "xcomet_qe_score": 0.18355724215507507, "metricx_score": 6.239108085632324, "metricx_qe_score": 7.188809871673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,我们不进行手动数据标注。", "metrics": {"bleu_score": 17.92334464048543, "chrf_score": 19.662306738074115, "xcomet_score": 0.8782649040222168, "xcomet_qe_score": 0.8709974884986877, "metricx_score": 2.2742769718170166, "metricx_qe_score": 2.2548582553863525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注来源对数据进行标注,例如简单的启发式规则、知识库或低质量众包标注,如右图所示。", "metrics": {"bleu_score": 58.83570443210199, "chrf_score": 55.32696594577055, "xcomet_score": 0.7244972586631775, "xcomet_qe_score": 0.6987005472183228, "metricx_score": 1.4936033487319946, "metricx_qe_score": 1.703917145729065, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注成本更低,但同时也存在噪声,即标注中存在一定数量的错误。", "metrics": {"bleu_score": 30.212942293015793, "chrf_score": 24.913679548415658, "xcomet_score": 0.9635790586471558, "xcomet_qe_score": 0.8507320880889893, "metricx_score": 2.4942731857299805, "metricx_qe_score": 2.9364402294158936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在弱标签数据上训练神经网络,神经网络往往会记住标签中的噪声,而无法进行泛化。", "metrics": {"bleu_score": 48.25935812594302, "chrf_score": 41.06372133727667, "xcomet_score": 0.9120339751243591, "xcomet_qe_score": 0.8699951767921448, "metricx_score": 0.932151734828949, "metricx_qe_score": 0.8664163947105408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出训练算法以在这种标签噪声下稳健地训练神经网络,使训练后的模型仍能很好地泛化。", "metrics": {"bleu_score": 55.305693105716124, "chrf_score": 51.553545594909714, "xcomet_score": 0.9670768976211548, "xcomet_qe_score": 0.8859000205993652, "metricx_score": 1.218510627746582, "metricx_qe_score": 2.2929646968841553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近关于WSL(即每周监督学习)的研究中,一个常见的说法是,研究人员声称他们仅在每周标签数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 28.348239480651365, "chrf_score": 27.92346977974815, "xcomet_score": 0.6817444562911987, "xcomet_qe_score": 0.6966833472251892, "metricx_score": 6.511859893798828, "metricx_qe_score": 6.43461799621582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并不错误,但存在一个前提。 这就是人们假设模型选择过程中存在一组额外的干净验证数据集。", "metrics": {"bleu_score": 32.299861639248576, "chrf_score": 27.905879034086507, "xcomet_score": 0.9820040464401245, "xcomet_qe_score": 0.9187514185905457, "metricx_score": 2.0843210220336914, "metricx_qe_score": 2.4037210941314697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不能在这一问题上停滞不前,因为这意味着在每周的SuperWise学习中需要额外的手动标注。", "metrics": {"bleu_score": 40.48673655329744, "chrf_score": 35.51073340845358, "xcomet_score": 0.7762165069580078, "xcomet_qe_score": 0.8276164531707764, "metricx_score": 4.422338008880615, "metricx_qe_score": 5.122733116149902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,就像房间里的象一样,这一需求经常被忽视。", "metrics": {"bleu_score": 38.48321033998745, "chrf_score": 30.96637706352712, "xcomet_score": 0.9196667671203613, "xcomet_qe_score": 0.8797995448112488, "metricx_score": 2.589804172515869, "metricx_qe_score": 4.45038366317749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑问引导我们提出三个研究问题。", "metrics": {"bleu_score": 63.019085559238604, "chrf_score": 57.62131995937635, "xcomet_score": 0.9585031270980835, "xcomet_qe_score": 0.959296464920044, "metricx_score": 1.055587649345398, "metricx_qe_score": 1.0765941143035889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于 WSL,是否必须使用干净的验证数据?或者我们是否可以使用一个含有噪声的验证集?", "metrics": {"bleu_score": 32.88503327935481, "chrf_score": 33.14075341191815, "xcomet_score": 0.9646027088165283, "xcomet_qe_score": 0.9473650455474854, "metricx_score": 1.8443509340286255, "metricx_qe_score": 3.0863842964172363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净的数据,或者干净的数据是 WSL 运行的必要条件,那么我们需要多少干净样本?", "metrics": {"bleu_score": 46.2090861860365, "chrf_score": 40.05201522939274, "xcomet_score": 0.97630774974823, "xcomet_qe_score": 0.9192313551902771, "metricx_score": 0.6779119968414307, "metricx_qe_score": 0.9356286525726318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该仅将干净样本用于验证,或者有更好的利用方法?", "metrics": {"bleu_score": 20.189848136904732, "chrf_score": 20.38684890564327, "xcomet_score": 0.9640209674835205, "xcomet_qe_score": 0.9059354066848755, "metricx_score": 0.7899771928787231, "metricx_qe_score": 0.9932997226715088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中探讨了这些研究问题,以下是我们的研究发现:", "metrics": {"bleu_score": 45.853535638200725, "chrf_score": 40.70614211084436, "xcomet_score": 0.9780222177505493, "xcomet_qe_score": 0.9863231182098389, "metricx_score": 0.8950133919715881, "metricx_qe_score": 0.4356689155101776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,有趣的是,最近的 WSL 方法确实需要干净的白色盘子样本才能正常工作。", "metrics": {"bleu_score": 56.28828076174479, "chrf_score": 52.45802979389386, "xcomet_score": 0.7781544923782349, "xcomet_qe_score": 0.7892868518829346, "metricx_score": 4.563538074493408, "metricx_qe_score": 5.26024866104126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,那么训练后的模型就无法超越原始的弱标签进行泛化。 这意味着训练是无意义的。", "metrics": {"bleu_score": 72.01295008796, "chrf_score": 67.75226924877373, "xcomet_score": 0.9606277942657471, "xcomet_qe_score": 0.8795316219329834, "metricx_score": 2.030069351196289, "metricx_qe_score": 2.7928085327148438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL(弱监督学习)方法实际上需要干净的标签数据才能正常工作,获取干净验证样本的标注成本不容忽视。", "metrics": {"bleu_score": 44.85667534596012, "chrf_score": 43.58015341684609, "xcomet_score": 0.8459799289703369, "xcomet_qe_score": 0.8494387269020081, "metricx_score": 2.779291868209839, "metricx_qe_score": 2.9038851261138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于 WSL 方法实现更好的性能,如左侧图表所示。", "metrics": {"bleu_score": 69.57312394009541, "chrf_score": 68.76147743439603, "xcomet_score": 0.9167754650115967, "xcomet_qe_score": 0.8955472111701965, "metricx_score": 3.735440731048584, "metricx_qe_score": 4.775913715362549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常情况下,我们每类只需要 20 个样本即可达到高性能。", "metrics": {"bleu_score": 36.33536415491189, "chrf_score": 36.65252532128848, "xcomet_score": 0.9326332807540894, "xcomet_qe_score": 0.9412497282028198, "metricx_score": 1.7033895254135132, "metricx_qe_score": 1.6520451307296753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是故事并未结束,因为如果我们最终决定获取干净的样本,那么直接在这些样本上进行训练甚至能达到更好的性能。 红", "metrics": {"bleu_score": 28.568913735792773, "chrf_score": 24.905738537115674, "xcomet_score": 0.8640604019165039, "xcomet_qe_score": 0.8099573850631714, "metricx_score": 4.852996349334717, "metricx_qe_score": 4.115170478820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "色图示展示了微调方法(直接应用于清洁数据)和 WSL 方法(仅使用清洁数据进行验证)之间的性能差异。", "metrics": {"bleu_score": 34.41073476191822, "chrf_score": 32.27243804434516, "xcomet_score": 0.6997177600860596, "xcomet_qe_score": 0.6901369094848633, "metricx_score": 3.5263781547546387, "metricx_qe_score": 4.032473087310791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每类有 10 个样本,直接微调开始超越 WSL 方法。", "metrics": {"bleu_score": 48.6292607934264, "chrf_score": 46.911760281876845, "xcomet_score": 0.9629545211791992, "xcomet_qe_score": 0.9264746904373169, "metricx_score": 1.7405405044555664, "metricx_qe_score": 2.9176409244537354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,通过允许在干净的验证样本上继续微调,可以轻松实现之前 WSL 方法中声称的性能提升。", "metrics": {"bleu_score": 50.167540756339584, "chrf_score": 44.80821183984229, "xcomet_score": 0.9739274978637695, "xcomet_qe_score": 0.8915089964866638, "metricx_score": 2.4012656211853027, "metricx_qe_score": 3.5959229469299316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从图中可以看出,名为FTW的Van Lina模型最初在性能上落后于更复杂的WSL方法,如余弦相似度。", "metrics": {"bleu_score": 25.72114682453164, "chrf_score": 26.48838928357813, "xcomet_score": 0.7646746039390564, "xcomet_qe_score": 0.6614481210708618, "metricx_score": 5.662843227386475, "metricx_qe_score": 6.05464506149292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果我们允许在干净样本上继续微调,那么FTW的表现与其它方法同样出色。", "metrics": {"bleu_score": 33.5913359034314, "chrf_score": 30.72029544952976, "xcomet_score": 0.8955633640289307, "xcomet_qe_score": 0.8140667676925659, "metricx_score": 1.4993793964385986, "metricx_qe_score": 2.339570999145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在实际应用中,没有必要选择更复杂的WSL方法,因为这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 41.03736083870628, "chrf_score": 44.514984435219716, "xcomet_score": 0.96368408203125, "xcomet_qe_score": 0.9650728702545166, "metricx_score": 0.5895696878433228, "metricx_qe_score": 1.1893458366394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们证明了最新的 WSL 方法需要干净、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 51.60498167475882, "chrf_score": 49.31787558515153, "xcomet_score": 0.9718484878540039, "xcomet_qe_score": 0.9317837953567505, "metricx_score": 2.3103976249694824, "metricx_qe_score": 3.139655590057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实际应用价值被严重高估了。", "metrics": {"bleu_score": 29.715678881302644, "chrf_score": 29.996926363508848, "xcomet_score": 0.9975954294204712, "xcomet_qe_score": 0.9983798265457153, "metricx_score": 0.6434529423713684, "metricx_qe_score": 0.6953849196434021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否基于干净的验证样本进行。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.34004037322114, "xcomet_score": 0.9586905241012573, "xcomet_qe_score": 0.9227784276008606, "metricx_score": 1.4400203227996826, "metricx_qe_score": 2.1803746223449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL 方法应与未来的学习基线进行比较,因为两者都基于干净的样本工作。", "metrics": {"bleu_score": 40.567246289475435, "chrf_score": 36.60395283557048, "xcomet_score": 0.7018532156944275, "xcomet_qe_score": 0.6987813115119934, "metricx_score": 4.673596382141113, "metricx_qe_score": 6.25942850112915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一种简单而强大的基线,应在未来的 WSL 研究中考虑。", "metrics": {"bleu_score": 43.606086243652236, "chrf_score": 37.63327634096535, "xcomet_score": 0.8750824332237244, "xcomet_qe_score": 0.7517231702804565, "metricx_score": 1.8430801630020142, "metricx_qe_score": 3.0879743099212646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过本幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9957367181777954, "xcomet_qe_score": 0.9900866746902466, "metricx_score": 0.461531400680542, "metricx_qe_score": 0.4078834354877472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随意查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8805491924285889, "xcomet_qe_score": 0.8574787974357605, "metricx_score": 0.4994497001171112, "metricx_qe_score": 0.6060904264450073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝您享受会议。", "metrics": {"bleu_score": 7.817610446892725, "chrf_score": 8.18252221407027, "xcomet_score": 0.9301098585128784, "xcomet_qe_score": 0.9734148383140564, "metricx_score": 0.8404812216758728, "metricx_qe_score": 0.5385954976081848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9827327728271484, "xcomet_qe_score": 1.0, "metricx_score": 0.8756831288337708, "metricx_qe_score": 0.3633490204811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCeval,一种评估对话人工智能的新维度方法。", "metrics": {"bleu_score": 23.44732872048571, "chrf_score": 25.184261211703845, "xcomet_score": 0.8484708070755005, "xcomet_qe_score": 0.913550615310669, "metricx_score": 1.4003496170043945, "metricx_qe_score": 1.6343687772750854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学吉诺·崔教授领导のエ默里自然语言处理实验室完成,并得到了亚马逊Alexa AI的合作。", "metrics": {"bleu_score": 22.754875066019743, "chrf_score": 24.885427889283743, "xcomet_score": 0.7854343056678772, "xcomet_qe_score": 0.8111711740493774, "metricx_score": 2.7241017818450928, "metricx_qe_score": 3.113269329071045, "linguapy_score": [1, "JAPANESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚开发了一个对话模型,你想看看它与当前最先进的技术相比表现如何。", "metrics": {"bleu_score": 75.16216924719087, "chrf_score": 64.49628711487166, "xcomet_score": 0.9982374906539917, "xcomet_qe_score": 0.9885433912277222, "metricx_score": 0.5928300619125366, "metricx_qe_score": 0.641268253326416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如请人工评判员从两个对话中选择哪个更好,或者使用利克特量表对对话进行评分。", "metrics": {"bleu_score": 58.78733962869236, "chrf_score": 51.448873604402976, "xcomet_score": 0.9002394676208496, "xcomet_qe_score": 0.8794471025466919, "metricx_score": 0.6425871253013611, "metricx_qe_score": 0.6841444969177246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法可以很好地提供整体对话质量的评估,但对话质量有多个方面。", "metrics": {"bleu_score": 59.490969034436894, "chrf_score": 50.44106832228087, "xcomet_score": 0.947091817855835, "xcomet_qe_score": 0.9835519790649414, "metricx_score": 0.40492382645606995, "metricx_qe_score": 0.5791277885437012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能需要评估聊天质量的多个维度,以更细致地了解模型的优缺点。", "metrics": {"bleu_score": 77.07975858143212, "chrf_score": 72.77552982504822, "xcomet_score": 0.9787784814834595, "xcomet_qe_score": 0.9572713375091553, "metricx_score": 0.5929165482521057, "metricx_qe_score": 0.5868556499481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是直接请人类评判员使用现有的比较方法或利克特量表法,评估对话质量的多个维度,例如模型响应的相关性。", "metrics": {"bleu_score": 42.62227025408929, "chrf_score": 34.80961320932738, "xcomet_score": 0.9818747043609619, "xcomet_qe_score": 0.9853575229644775, "metricx_score": 1.2365585565567017, "metricx_qe_score": 1.8241405487060547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 45.15558127464808, "xcomet_score": 0.9024549126625061, "xcomet_qe_score": 0.8712908625602722, "metricx_score": 1.308674931526184, "metricx_qe_score": 1.4190480709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表达了特定行为来减少人工评估的主观性,例如提供与主题无关的信息或自相矛盾。", "metrics": {"bleu_score": 65.31986817818962, "chrf_score": 59.509120173027405, "xcomet_score": 0.9657424688339233, "xcomet_qe_score": 0.9607220888137817, "metricx_score": 1.7524058818817139, "metricx_qe_score": 2.3436131477355957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为聊天行为标注或简称为ABC评估。", "metrics": {"bleu_score": 16.76784955078518, "chrf_score": 18.0427420517559, "xcomet_score": 0.7976334095001221, "xcomet_qe_score": 0.8042736053466797, "metricx_score": 1.717424988746643, "metricx_qe_score": 1.118726372718811, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法是为了全面涵盖近期文献中提出影响聊天质量建议的聊天模型行为。", "metrics": {"bleu_score": 56.76007302893162, "chrf_score": 47.69143787889183, "xcomet_score": 0.8468894958496094, "xcomet_qe_score": 0.8252619504928589, "metricx_score": 3.405181646347046, "metricx_qe_score": 4.659224510192871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC 评估能够测量聊天模型犯各种主题错误的速率。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 52.23877106271304, "xcomet_score": 0.6864588260650635, "xcomet_qe_score": 0.7049075365066528, "metricx_score": 4.9060444831848145, "metricx_qe_score": 6.331258773803711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABC 评估量表测量聊天模型在多少轮对话中忽略了其对话伙伴或说了与话题无关的话。 自我矛盾或与合作伙伴矛盾,幻觉错误的事实或违反常识,以及模型在表现同理心时成功或失败的情况。", "metrics": {"bleu_score": 23.717911260830586, "chrf_score": 21.468389341470363, "xcomet_score": 0.6062349081039429, "xcomet_qe_score": 0.6757277250289917, "metricx_score": 5.892533302307129, "metricx_qe_score": 6.597461223602295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方法最有效,我们选择了四种最先进的聊天模型,并使用 ABC 评估法对每个模型进行 100 次人机对话的评估。", "metrics": {"bleu_score": 62.52382900752319, "chrf_score": 56.1784481091433, "xcomet_score": 0.9726126194000244, "xcomet_qe_score": 0.9685608148574829, "metricx_score": 0.9147549271583557, "metricx_qe_score": 0.74611496925354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用三种现有方法对这些对话进行了评估:转折级利克特量表、对话级利克特量表以及对话级配对比较。", "metrics": {"bleu_score": 35.80072056550524, "chrf_score": 30.95683280389206, "xcomet_score": 0.7570909261703491, "xcomet_qe_score": 0.7690314054489136, "metricx_score": 2.318732976913452, "metricx_qe_score": 1.9030494689941406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有的每一种方法,我们收集了关于对话八个最常见测量维度的评估,因为这是沿着多个维度评估聊天模型的标准实践。 ", "metrics": {"bleu_score": 54.8157953609835, "chrf_score": 47.08329258510686, "xcomet_score": 0.9372260570526123, "xcomet_qe_score": 0.9359076619148254, "metricx_score": 2.972381114959717, "metricx_qe_score": 3.4712600708007812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现ABC评估行为标签总体上比现有方法收集的标签更可靠,这一点通过100个双重标记对话的评注者间一致性得到衡量。", "metrics": {"bleu_score": 37.44424629758319, "chrf_score": 36.29798602212736, "xcomet_score": 0.7190631628036499, "xcomet_qe_score": 0.814085066318512, "metricx_score": 4.837323188781738, "metricx_qe_score": 4.4215006828308105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,根据这一简单的线性回归分析,ABC 评估标签在预测整体对话质量方面优于现有方法产生的指标。", "metrics": {"bleu_score": 45.90738410805631, "chrf_score": 37.46263497980875, "xcomet_score": 0.9840507507324219, "xcomet_qe_score": 0.9731825590133667, "metricx_score": 1.6086461544036865, "metricx_qe_score": 1.5417166948318481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到,测量自我和伴侣矛盾的转折比例分别解释了对话质量5%和10%,而平均利克特一致性得分仅解释了4%或更少。", "metrics": {"bleu_score": 54.57232996361178, "chrf_score": 47.99414557307231, "xcomet_score": 0.6171658039093018, "xcomet_qe_score": 0.6220810413360596, "metricx_score": 5.631432056427002, "metricx_qe_score": 5.323705673217773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归分析检查了每个评估指标是否捕捉了聊天质量的独特方面。 您", "metrics": {"bleu_score": 63.70082049877986, "chrf_score": 60.51419678972897, "xcomet_score": 0.7582813501358032, "xcomet_qe_score": 0.7904745936393738, "metricx_score": 4.408302307128906, "metricx_qe_score": 1.9686896800994873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,ABC 评估指标的组合可以解释超过 25% 的对话质量。当您逐个去除这些指标时,大多数情况下都会导致失去大量关于质量的信息。", "metrics": {"bleu_score": 33.06805758180923, "chrf_score": 30.058114942844345, "xcomet_score": 0.9365706443786621, "xcomet_qe_score": 0.8233102560043335, "metricx_score": 1.7157714366912842, "metricx_qe_score": 2.653087615966797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有层级Likert指标的组合对质量解释得远远不足,且这些指标中带有独特", "metrics": {"bleu_score": 23.82222570451065, "chrf_score": 21.779120693513498, "xcomet_score": 0.46255046129226685, "xcomet_qe_score": 0.4175296723842621, "metricx_score": 9.682233810424805, "metricx_qe_score": 6.426581382751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "信息的更少。 这些可靠、信息丰富且独特的ABC评估指标使我们能够以高于先前方法的分辨率评估对话人工智能。", "metrics": {"bleu_score": 6.01356707694111, "chrf_score": 11.224811052277566, "xcomet_score": 0.048276618123054504, "xcomet_qe_score": 0.057441044598817825, "metricx_score": 9.60901165008545, "metricx_qe_score": 8.840628623962402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们实验的结果中可以看出,仍存在几个挑战,并且这些挑战已被精确量化。", "metrics": {"bleu_score": 21.16154709655933, "chrf_score": 24.096275259849005, "xcomet_score": 0.9800653457641602, "xcomet_qe_score": 0.979763388633728, "metricx_score": 0.7752567529678345, "metricx_qe_score": 0.9311131834983826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人约20%的响应存在常识违规。", "metrics": {"bleu_score": 41.87526331649786, "chrf_score": 38.03824454645228, "xcomet_score": 0.8536428213119507, "xcomet_qe_score": 0.8503142595291138, "metricx_score": 2.4013116359710693, "metricx_qe_score": 3.4330945014953613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在约15%的响应中产生与主题无关的信息。约10%的时间,它们会自相矛盾或与合作伙伴矛盾。", "metrics": {"bleu_score": 32.9249495175509, "chrf_score": 28.75276292639298, "xcomet_score": 0.7514764070510864, "xcomet_qe_score": 0.7556450366973877, "metricx_score": 2.9911952018737793, "metricx_qe_score": 2.879092216491699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速进步,自我们进行评估以来,许多错误率在新发布的模型中可能会降低。", "metrics": {"bleu_score": 54.50206538912655, "chrf_score": 46.69290314027156, "xcomet_score": 0.9755654335021973, "xcomet_qe_score": 0.9798710346221924, "metricx_score": 1.6807653903961182, "metricx_qe_score": 1.6306102275848389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更说明了追求可靠且精确的评估指标以比较模型的重要性。", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 40.91703962752154, "xcomet_score": 0.9977174997329712, "xcomet_qe_score": 0.9871809482574463, "metricx_score": 0.910224437713623, "metricx_qe_score": 1.0445787906646729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABC Eval能够被该领域的其他人士作为朝此方向迈进的有意义的一步而广泛利用。我们", "metrics": {"bleu_score": 57.98764085432875, "chrf_score": 62.16197595015832, "xcomet_score": 0.8145284652709961, "xcomet_qe_score": 0.823483943939209, "metricx_score": 5.224334716796875, "metricx_qe_score": 2.3325159549713135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也期待着看到对话式人工智能在未来数月和数年中的发展进步。", "metrics": {"bleu_score": 28.768667040396203, "chrf_score": 25.741690683773655, "xcomet_score": 0.8943508863449097, "xcomet_qe_score": 0.8709635138511658, "metricx_score": 0.8940610885620117, "metricx_qe_score": 0.6830515265464783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我的名字是凯欣(Kaio-Yin),我将向大家展示我们的研究成果,题为《何时翻译需要上下文?", "metrics": {"bleu_score": 17.500912716651218, "chrf_score": 21.992288699672276, "xcomet_score": 0.970391035079956, "xcomet_qe_score": 0.9763625860214233, "metricx_score": 1.3958628177642822, "metricx_qe_score": 1.9025168418884277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语种探索》。", "metrics": {"bleu_score": 34.172334076593074, "chrf_score": 28.44276094276094, "xcomet_score": 0.9816019535064697, "xcomet_qe_score": 0.8502190709114075, "metricx_score": 1.1029056310653687, "metricx_qe_score": 1.2808890342712402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernandes、刘逸(Emmy Liu)、Andre F.D. Martins和Graham Newbig合作完成的。", "metrics": {"bleu_score": 34.67735106324065, "chrf_score": 67.75825803266387, "xcomet_score": 0.7417452335357666, "xcomet_qe_score": 0.7249312400817871, "metricx_score": 3.122370958328247, "metricx_qe_score": 2.4037110805511475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都取决于上下文。", "metrics": {"bleu_score": 63.40466277046863, "chrf_score": 59.27762653106322, "xcomet_score": 0.9980931282043457, "xcomet_qe_score": 0.9876047372817993, "metricx_score": 0.09006089717149734, "metricx_qe_score": 0.15374323725700378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译句子中的“痣”? (注:由于原文未提供具体句子,翻译中的“mole”被", "metrics": {"bleu_score": 21.74060073159203, "chrf_score": 41.07085362963095, "xcomet_score": 0.44265270233154297, "xcomet_qe_score": 0.3905332088470459, "metricx_score": 7.033148765563965, "metricx_qe_score": 4.665295124053955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设为“痣”的意思。) 好吧,如果前一句话是,如果部长们发现,事情可能会开始变得危险,那么“莫”指的是一个间谍。", "metrics": {"bleu_score": 13.836345500899633, "chrf_score": 8.58222930246616, "xcomet_score": 0.6413640379905701, "xcomet_qe_score": 0.6069943904876709, "metricx_score": 8.96768856048584, "metricx_qe_score": 8.601765632629395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句话是,医生,会是什么严重的事情吗?那么“莫”指的是一个胎记。", "metrics": {"bleu_score": 18.062288149845553, "chrf_score": 10.977612733217281, "xcomet_score": 0.7979916334152222, "xcomet_qe_score": 0.7519749402999878, "metricx_score": 6.035572528839111, "metricx_qe_score": 5.779483795166016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据上下文,词的意义会发生变化,因此其翻译也会相应地改变。", "metrics": {"bleu_score": 43.83022272687836, "chrf_score": 35.56002126283599, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4222135543823242, "metricx_qe_score": 0.3753082752227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在翻译此类案例时的表现相当困难。", "metrics": {"bleu_score": 20.63875384458799, "chrf_score": 18.315811601788674, "xcomet_score": 0.8964866399765015, "xcomet_qe_score": 0.8593510389328003, "metricx_score": 1.1426359415054321, "metricx_qe_score": 1.745255708694458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,由于只有少数翻译依赖于上下文,这导致语料库级别的指标,如BLEU,无法捕捉到这些翻译。", "metrics": {"bleu_score": 28.89787454467964, "chrf_score": 26.58360985138732, "xcomet_score": 0.976241946220398, "xcomet_qe_score": 0.9509700536727905, "metricx_score": 1.406715989112854, "metricx_qe_score": 2.210369110107422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对依上下文而变的翻译进行定向评估,但这些资源只支持有限类型的依上下文而变的翻译和有限的语言集合,因为它们通常依赖于领域知识和人工编辑。", "metrics": {"bleu_score": 62.64077334613357, "chrf_score": 55.874361992068856, "xcomet_score": 0.9325600862503052, "xcomet_qe_score": 0.9382902979850769, "metricx_score": 1.3215135335922241, "metricx_qe_score": 1.2039116621017456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们试图回答以下两个问题。", "metrics": {"bleu_score": 73.09400730437454, "chrf_score": 69.43822972366657, "xcomet_score": 0.9975918531417847, "xcomet_qe_score": 0.99656081199646, "metricx_score": 0.0, "metricx_qe_score": 0.09364727139472961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在处理这些情况下表现如何?", "metrics": {"bleu_score": 59.29835221973795, "chrf_score": 51.308235489846524, "xcomet_score": 0.9888322353363037, "xcomet_qe_score": 0.99993896484375, "metricx_score": 0.4791920483112335, "metricx_qe_score": 0.6569955348968506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了单词在翻译中对上下文的依赖程度。", "metrics": {"bleu_score": 73.52653997775782, "chrf_score": 65.03200898208895, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 4.199497699737549, "metricx_qe_score": 4.813797473907471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在先前的工作中,我们引入了CXMI作为机器翻译模型上下文使用量的度量。", "metrics": {"bleu_score": 57.19408603967074, "chrf_score": 59.345310377550106, "xcomet_score": 0.8919402360916138, "xcomet_qe_score": 0.888935387134552, "metricx_score": 1.7021788358688354, "metricx_qe_score": 1.811134696006775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量给定源X时上下文C关于目标Y提供的信息量来实现。 可以将 CXMI 视为在给模型提供上下文时获得的信息。", "metrics": {"bleu_score": 40.984976639215226, "chrf_score": 38.09629586288892, "xcomet_score": 0.907799482345581, "xcomet_qe_score": 0.8911603689193726, "metricx_score": 4.928144931793213, "metricx_qe_score": 5.0249409675598145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们将CXMI扩展为点对点CXMI,它可以在句子级别或词语级别测量上下文使用情况。", "metrics": {"bleu_score": 48.00487694138449, "chrf_score": 39.91774701504603, "xcomet_score": 0.8377571105957031, "xcomet_qe_score": 0.8423504829406738, "metricx_score": 2.3003931045532227, "metricx_qe_score": 2.3884425163269043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将PSXMI值高的词语视为需要上下文进行翻译的词语。", "metrics": {"bleu_score": 70.94521095075528, "chrf_score": 68.90856594502488, "xcomet_score": 0.8986334800720215, "xcomet_qe_score": 0.9105633497238159, "metricx_score": 2.262890338897705, "metricx_qe_score": 2.745208263397217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析高PCXMI的词语,以寻找这些词语之间的模式。", "metrics": {"bleu_score": 21.419122155930683, "chrf_score": 30.77209042801514, "xcomet_score": 0.9344390630722046, "xcomet_qe_score": 0.9488096833229065, "metricx_score": 1.6728878021240234, "metricx_qe_score": 2.5421361923217773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成十四种不同语言的TED演讲的文字记录进行了分析。", "metrics": {"bleu_score": 50.86366587562722, "chrf_score": 51.002933436495844, "xcomet_score": 0.9026250243186951, "xcomet_qe_score": 0.9274037480354309, "metricx_score": 1.4527552127838135, "metricx_qe_score": 1.3419684171676636, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同的层次上进行分析。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 72.00112387612387, "xcomet_score": 0.997683048248291, "xcomet_qe_score": 0.9908944368362427, "metricx_score": 0.22728201746940613, "metricx_qe_score": 0.402015745639801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们观察那些平均PCXMI值较高的词性标签。", "metrics": {"bleu_score": 36.394125309794774, "chrf_score": 35.10357261688649, "xcomet_score": 0.8770720958709717, "xcomet_qe_score": 0.8427319526672363, "metricx_score": 1.6224123239517212, "metricx_qe_score": 2.02948260307312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到,例如,在阿拉伯语中具有相对较高的P6MI的双重代词。", "metrics": {"bleu_score": 49.86317689307934, "chrf_score": 40.75752372092782, "xcomet_score": 0.6610201001167297, "xcomet_qe_score": 0.718525767326355, "metricx_score": 6.2994771003723145, "metricx_qe_score": 5.816446304321289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,因为英语中没有双重代词,所以在翻译成阿拉伯语时,你需要根据上下文来确定一个代词是否是双重代词。", "metrics": {"bleu_score": 48.895806126142276, "chrf_score": 46.04329931104542, "xcomet_score": 0.8110983371734619, "xcomet_qe_score": 0.9935170412063599, "metricx_score": 1.7627789974212646, "metricx_qe_score": 1.3438771963119507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 88.4112136328919, "chrf_score": 89.69824812440879, "xcomet_score": 0.9977834224700928, "xcomet_qe_score": 0.996990442276001, "metricx_score": 0.5566547513008118, "metricx_qe_score": 0.8022129535675049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后会考察在所有不同出现中其PCSXMI平均值较高的词汇项。", "metrics": {"bleu_score": 37.63278728427449, "chrf_score": 31.65175241281371, "xcomet_score": 0.754321813583374, "xcomet_qe_score": 0.7235401272773743, "metricx_score": 5.1407551765441895, "metricx_qe_score": 5.069941997528076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的案例,在中文中,您需要上下文来翻译专名,以确保在文档中一致地使用同一翻译。", "metrics": {"bleu_score": 32.159417636115315, "chrf_score": 27.567881315338344, "xcomet_score": 0.740267813205719, "xcomet_qe_score": 0.7471232414245605, "metricx_score": 2.7019567489624023, "metricx_qe_score": 2.6053974628448486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现语境得到支持,以合适的正式程度进行翻译。", "metrics": {"bleu_score": 12.500763055889763, "chrf_score": 14.265471016461914, "xcomet_score": 0.8129124641418457, "xcomet_qe_score": 0.8098093271255493, "metricx_score": 4.555186748504639, "metricx_qe_score": 5.024979114532471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们考察了具有高p6mi的不同单个词元。", "metrics": {"bleu_score": 42.461633178803446, "chrf_score": 31.003079744267147, "xcomet_score": 0.7213668823242188, "xcomet_qe_score": 0.6588348150253296, "metricx_score": 6.882691860198975, "metricx_qe_score": 8.598299980163574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别一些无法仅通过词本身捕捉的现象,而是在句子结构中表达的现象,例如省略解析。", "metrics": {"bleu_score": 32.38222340857482, "chrf_score": 28.693155071047443, "xcomet_score": 0.8047988414764404, "xcomet_qe_score": 0.8502203226089478, "metricx_score": 1.6667486429214478, "metricx_qe_score": 2.2269034385681152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们利用分析结果设计一个文档级翻译的基准。", "metrics": {"bleu_score": 46.79404176919902, "chrf_score": 40.65863461205542, "xcomet_score": 0.9754302501678467, "xcomet_qe_score": 0.852797269821167, "metricx_score": 1.010428547859192, "metricx_qe_score": 1.3324686288833618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别出的五种话语现象,我们创建了标记器,以自动识别与现象相关的词语。", "metrics": {"bleu_score": 63.24612091098578, "chrf_score": 58.500323859214646, "xcomet_score": 0.987690806388855, "xcomet_qe_score": 0.9112576842308044, "metricx_score": 0.9790451526641846, "metricx_qe_score": 1.4101122617721558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这个标记器称为多语言话语感知标记器,或简称为MUDA标记器。", "metrics": {"bleu_score": 47.05896438972141, "chrf_score": 44.34416840931876, "xcomet_score": 0.9493480920791626, "xcomet_qe_score": 0.9137634038925171, "metricx_score": 0.8931858539581299, "metricx_qe_score": 1.1788464784622192, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同语言中这些话语现象的比例是不同的。", "metrics": {"bleu_score": 50.727410390465785, "chrf_score": 45.038700147583974, "xcomet_score": 0.9909877777099609, "xcomet_qe_score": 0.9865933656692505, "metricx_score": 0.7453776001930237, "metricx_qe_score": 0.9667000770568848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们使用Muda标注器,将其应用于我们希望用于评估的平行语料库。我们将选定的翻译度量标准应用于Muda标注器识别的上下文相关示例。", "metrics": {"bleu_score": 46.06099252925565, "chrf_score": 42.591021285359595, "xcomet_score": 0.8756952881813049, "xcomet_qe_score": 0.8108813166618347, "metricx_score": 2.275202512741089, "metricx_qe_score": 2.1781535148620605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用基准和其他度量标准来评估文档级机器翻译中的不同模型。", "metrics": {"bleu_score": 50.77162056935099, "chrf_score": 43.621313054031106, "xcomet_score": 0.9540338516235352, "xcomet_qe_score": 0.8370054364204407, "metricx_score": 0.9733223915100098, "metricx_qe_score": 1.4055020809173584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的度量时,对于“蓝”这个词,我们发现无上下文感知的模型表现最佳。", "metrics": {"bleu_score": 43.387592942453566, "chrf_score": 38.405577690929896, "xcomet_score": 0.7405600547790527, "xcomet_qe_score": 0.7811766266822815, "metricx_score": 3.635955572128296, "metricx_qe_score": 2.797898292541504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果我们使用COMET,那么上下文感知模型表现最佳。", "metrics": {"bleu_score": 54.303214666333915, "chrf_score": 55.9392805945613, "xcomet_score": 0.9624506235122681, "xcomet_qe_score": 0.9477827548980713, "metricx_score": 1.7878978252410889, "metricx_qe_score": 2.798828363418579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用词F度量,那么有上下文和无上下文的模型性能相当。", "metrics": {"bleu_score": 56.25037887129073, "chrf_score": 47.09381823202791, "xcomet_score": 0.8318147659301758, "xcomet_qe_score": 0.775818943977356, "metricx_score": 3.090400457382202, "metricx_qe_score": 2.6699466705322266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,仅使用语料库级别的指标,确定最佳文档级别翻译系统是具有挑战性的。", "metrics": {"bleu_score": 41.658028201971575, "chrf_score": 34.71770242709283, "xcomet_score": 0.9883018732070923, "xcomet_qe_score": 0.9696034789085388, "metricx_score": 0.8733387589454651, "metricx_qe_score": 1.10537850856781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用Muda基准来评估模型,并发现在特定话语现象(如正式性和词汇连贯性)中,考虑上下文的模型比不使用上下文的模型准确度显著更高。", "metrics": {"bleu_score": 44.575493266832275, "chrf_score": 40.192968529926276, "xcomet_score": 0.9092826843261719, "xcomet_qe_score": 0.8749083280563354, "metricx_score": 2.444737195968628, "metricx_qe_score": 3.0142478942871094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在处理省略号、代词和动词形式等其他现象时,与未使用上下文的模型相差无几。", "metrics": {"bleu_score": 60.98986290110881, "chrf_score": 55.040098692040594, "xcomet_score": 0.9877657890319824, "xcomet_qe_score": 0.9166115522384644, "metricx_score": 0.8242922425270081, "metricx_qe_score": 0.9372149705886841, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在一定程度上表明了我们在文档级翻译中需要看到更多进步的领域。", "metrics": {"bleu_score": 45.84906215724896, "chrf_score": 43.00365880538174, "xcomet_score": 0.999502420425415, "xcomet_qe_score": 0.99676513671875, "metricx_score": 1.5128241777420044, "metricx_qe_score": 1.4835546016693115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试显示,DeepL 在文档级翻译中通常比谷歌翻译更准确。", "metrics": {"bleu_score": 69.55022674300433, "chrf_score": 61.15068360109558, "xcomet_score": 0.9561609029769897, "xcomet_qe_score": 0.8676072359085083, "metricx_score": 1.216785192489624, "metricx_qe_score": 1.6170883178710938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结而言,我们在14对语言上进行数据驱动的分析,以识别何时翻译需要上下文。 然后,我们利用研究成果建立文档级机器翻译的基准,这可以帮助我们识别哪些话语现象模型能很好地处理,哪些不能,以及哪些翻译系统在文档级翻译中表现出色。", "metrics": {"bleu_score": 41.404263330678404, "chrf_score": 36.72319122504674, "xcomet_score": 0.9231679439544678, "xcomet_qe_score": 0.9230843782424927, "metricx_score": 2.96610689163208, "metricx_qe_score": 3.3988945484161377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在多伦多见。", "metrics": {"bleu_score": 71.65313105737896, "chrf_score": 64.65405545478103, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4812854528427124, "metricx_qe_score": 1.2813191413879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是亚尼斯·拉弗拉克。我将向您展示我们在Dr. BERT方面的工作,这是一个针对生物医学和临床领域的强大法语预训练模型。", "metrics": {"bleu_score": 27.03959494451197, "chrf_score": 25.677624904270473, "xcomet_score": 0.90592360496521, "xcomet_qe_score": 0.8819065093994141, "metricx_score": 2.0041186809539795, "metricx_qe_score": 1.6657602787017822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论医疗领域的语言建模。", "metrics": {"bleu_score": 58.21417459564055, "chrf_score": 48.89963248282777, "xcomet_score": 0.9930500984191895, "xcomet_qe_score": 0.9960814714431763, "metricx_score": 0.3166263699531555, "metricx_qe_score": 0.34818464517593384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将展示我们文章的主要贡献。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 54.45316801934449, "xcomet_score": 0.911873459815979, "xcomet_qe_score": 0.9091360569000244, "metricx_score": 1.6052939891815186, "metricx_qe_score": 2.0100960731506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个以法语命名的生物医学模型,名为Dr. Bert,该模型基于Roberta,并在NACHOS上进行了训练,NACHOS是一个来自网络的医疗众包数据集。", "metrics": {"bleu_score": 41.78811948017412, "chrf_score": 42.93447022443199, "xcomet_score": 0.7494702339172363, "xcomet_qe_score": 0.6689382195472717, "metricx_score": 2.175494909286499, "metricx_qe_score": 2.242366313934326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了对多个预训练设置和数据源的模型进行比较。", "metrics": {"bleu_score": 62.56538561604213, "chrf_score": 59.20008688921138, "xcomet_score": 0.8073042035102844, "xcomet_qe_score": 0.8373762369155884, "metricx_score": 1.3857115507125854, "metricx_qe_score": 2.0361664295196533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们展示了在11个法语生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 60.72654687697606, "chrf_score": 57.76142755814306, "xcomet_score": 0.8039866089820862, "xcomet_qe_score": 0.8031587600708008, "metricx_score": 2.526642322540283, "metricx_qe_score": 3.637862205505371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们对实验进行总结,并详细介绍如何访问模型。", "metrics": {"bleu_score": 11.640489373428165, "chrf_score": 15.038452983746472, "xcomet_score": 0.9271090626716614, "xcomet_qe_score": 0.8962766528129578, "metricx_score": 0.3306584060192108, "metricx_qe_score": 0.3172861337661743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务最有效的方法之一,与历史静态和上下文相关方法(如Word2Vec、FastText或NWO)相比,其性能有了显著提升。", "metrics": {"bleu_score": 56.975189954235134, "chrf_score": 55.59023195567527, "xcomet_score": 0.8354983329772949, "xcomet_qe_score": 0.8439338207244873, "metricx_score": 3.200998544692993, "metricx_qe_score": 2.808030366897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自那时起,该模型已被适应到许多其他语言中,例如法语中的Camembert,以及生物医学领域的PAMED-BERT和BioBERT,临床领域的Clinical-BERT,但主要是在英语中。", "metrics": {"bleu_score": 39.977712430604136, "chrf_score": 47.521470322657976, "xcomet_score": 0.7123656868934631, "xcomet_qe_score": 0.5607376098632812, "metricx_score": 4.506520748138428, "metricx_qe_score": 4.116903305053711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专业模型稀缺,且由于缺乏领域内数据,往往基于连续预训练。", "metrics": {"bleu_score": 48.000668150122756, "chrf_score": 38.93624803422169, "xcomet_score": 0.887245774269104, "xcomet_qe_score": 0.8212518692016602, "metricx_score": 0.9912638068199158, "metricx_qe_score": 1.841648817062378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法国直到现在都没有开放源代码的生物医学现代工具。", "metrics": {"bleu_score": 34.97140226207081, "chrf_score": 30.301641113512197, "xcomet_score": 0.8780903816223145, "xcomet_qe_score": 0.8846337199211121, "metricx_score": 2.714446544647217, "metricx_qe_score": 1.9546995162963867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们,因此我们问自己关于最合适的数据来源是什么的问题,以适用于广泛的用途。而现有的数据是临床数据的良好替代品。", "metrics": {"bleu_score": 34.53438397406236, "chrf_score": 37.273678282099596, "xcomet_score": 0.5784316062927246, "xcomet_qe_score": 0.5522386431694031, "metricx_score": 4.2112812995910645, "metricx_qe_score": 4.670393466949463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将伯特博士与我们基于匿名数据的舒伯特模型进行比较,该数据来自我们所属非大学医院。", "metrics": {"bleu_score": 35.01540075829212, "chrf_score": 25.461770788677928, "xcomet_score": 0.5707107782363892, "xcomet_qe_score": 0.5447734594345093, "metricx_score": 5.796313285827637, "metricx_qe_score": 5.341623306274414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们要问自己,训练一个专门的法语数据模型需要多少数据?", "metrics": {"bleu_score": 36.72167924968442, "chrf_score": 30.905162920066466, "xcomet_score": 0.903266429901123, "xcomet_qe_score": 0.8246370553970337, "metricx_score": 0.9410651922225952, "metricx_qe_score": 1.082554817199707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4吉字节、8吉字节,还是更多?", "metrics": {"bleu_score": 16.94357181593088, "chrf_score": 20.864590103407135, "xcomet_score": 0.7103062868118286, "xcomet_qe_score": 0.7136722803115845, "metricx_score": 3.0243945121765137, "metricx_qe_score": 2.1204917430877686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四个从零开始的模型。第一个版本是博士伯特(Dr. Bert)使用七千兆字节的纳乔(nachos)数据集,第二个版本使用四千兆字节的纳乔数据集。 一个基于Schubert的临床模型的首版,包含从临床笔记中提取的4 GB句子。以及一个最终版本的Schubert,混合了4 GB自然语言和4 GB临床笔记。 (注:Schubert在此处被假设为一个语言模型或类似技术的名称,在中文翻译中保持不变。)", "metrics": {"bleu_score": 25.22164466189646, "chrf_score": 26.117336842230856, "xcomet_score": 0.4255765676498413, "xcomet_qe_score": 0.4186641275882721, "metricx_score": 6.190529823303223, "metricx_qe_score": 5.650870323181152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这一比较,我们引入了三个在持续预训练上训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 51.781195920839345, "chrf_score": 44.07102404745395, "xcomet_score": 0.8623901605606079, "xcomet_qe_score": 0.8373721837997437, "metricx_score": 2.6084725856781006, "metricx_qe_score": 3.01263689994812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于卡门贝尔奶酪的模型,训练数据是四千兆字节的玉米片和配料。", "metrics": {"bleu_score": 9.557083774398006, "chrf_score": 7.556890991074988, "xcomet_score": 0.42645588517189026, "xcomet_qe_score": 0.4606037437915802, "metricx_score": 5.527218818664551, "metricx_qe_score": 5.273297309875488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个同样基于卡门贝尔奶酪,但这次训练数据是四千兆字节的玻璃杯和冰块。 最后,一个基于英语生物医学模型的模型,名为Bermud-Bert,并在四千兆字节的抓取数据集上进行训练。", "metrics": {"bleu_score": 22.94931999406486, "chrf_score": 21.92983303485489, "xcomet_score": 0.2506874203681946, "xcomet_qe_score": 0.2645387053489685, "metricx_score": 10.948694229125977, "metricx_qe_score": 10.457076072692871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们有七个模型。", "metrics": {"bleu_score": 57.067457770560026, "chrf_score": 52.68257581869399, "xcomet_score": 0.9877036809921265, "xcomet_qe_score": 0.8995441198348999, "metricx_score": 0.29523736238479614, "metricx_qe_score": 0.44317466020584106, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七种模型,我们收集了多种公开和私有的非刺激任务,例如姓名和身份识别、分类、词性标注以及问答任务。", "metrics": {"bleu_score": 37.66502096460081, "chrf_score": 34.57516689159613, "xcomet_score": 0.6382638216018677, "xcomet_qe_score": 0.6115449666976929, "metricx_score": 4.270833969116211, "metricx_qe_score": 3.793715476989746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基准模型进行比较,基准模型包括 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCnet 4 GB、Pumatbert、BioBERT 和 ClinicalBERT。", "metrics": {"bleu_score": 37.801805838989004, "chrf_score": 48.84458177332581, "xcomet_score": 0.5233725905418396, "xcomet_qe_score": 0.6020666360855103, "metricx_score": 5.573009014129639, "metricx_qe_score": 5.513818740844727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型在任务中表现最佳的数据与模型训练时所用数据具有相同特性的演变规律。", "metrics": {"bleu_score": 34.62542583644587, "chrf_score": 30.029113492399297, "xcomet_score": 0.8370382785797119, "xcomet_qe_score": 0.8286185264587402, "metricx_score": 5.067521572113037, "metricx_qe_score": 4.362945079803467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,无论我们从何处获取数据,我们可以观察到来自异构来源的数据似乎更加多样化。", "metrics": {"bleu_score": 39.615544682237186, "chrf_score": 46.020873853394576, "xcomet_score": 0.8598883152008057, "xcomet_qe_score": 0.7786970138549805, "metricx_score": 2.9124226570129395, "metricx_qe_score": 1.7915809154510498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据会转化为更好的性能。", "metrics": {"bleu_score": 41.163637617755036, "chrf_score": 32.89604441129711, "xcomet_score": 0.9781125783920288, "xcomet_qe_score": 0.984257698059082, "metricx_score": 3.1914825439453125, "metricx_qe_score": 4.142393112182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,从零开始的免费训练在大多数任务上似乎获得了更高的性能。", "metrics": {"bleu_score": 31.71906852365703, "chrf_score": 27.48474691652297, "xcomet_score": 0.7555873394012451, "xcomet_qe_score": 0.7520084977149963, "metricx_score": 6.770918846130371, "metricx_qe_score": 7.281381607055664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用Pumet-BERT的权重和分词器进行的持续预训练实验,在4吉字节的NACHOS子集上训练,显示出与从零开始训练的Dr.BERT 4吉字节模型相似的结果。", "metrics": {"bleu_score": 20.51831673225711, "chrf_score": 31.076514856925076, "xcomet_score": 0.5604893565177917, "xcomet_qe_score": 0.6538653373718262, "metricx_score": 8.029595375061035, "metricx_qe_score": 6.862281799316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这对于基于Camembert权重和令牌皮革的模型不适用,后者存在稳定性问题。", "metrics": {"bleu_score": 32.313876687664056, "chrf_score": 34.607985920450176, "xcomet_score": 0.6368957757949829, "xcomet_qe_score": 0.6365117430686951, "metricx_score": 7.296036720275879, "metricx_qe_score": 7.364507675170898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们的专有系统在11个下游任务中9个上表现更好,并整体上超越了这里通用模型Camembert的结果。", "metrics": {"bleu_score": 30.559985267206926, "chrf_score": 29.33030721683638, "xcomet_score": 0.6923567056655884, "xcomet_qe_score": 0.6816256642341614, "metricx_score": 4.558487415313721, "metricx_qe_score": 2.974844455718994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也观察到,专业化数据更好,越专业的数据越好,但它扩展起来并不容易。", "metrics": {"bleu_score": 10.199573389218783, "chrf_score": 15.548633798758626, "xcomet_score": 0.7877252697944641, "xcomet_qe_score": 0.8321546316146851, "metricx_score": 1.9304850101470947, "metricx_qe_score": 2.8849873542785645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从NACHOS获取的预训练模型都在UGIM面板上免费提供,所有训练脚本都在我们的GitHub仓库中。", "metrics": {"bleu_score": 32.103873527487615, "chrf_score": 34.702817959522584, "xcomet_score": 0.7235886454582214, "xcomet_qe_score": 0.7908273935317993, "metricx_score": 4.8317389488220215, "metricx_qe_score": 5.431075096130371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的精彩演讲。我们期待在多伦多会议后的行动。", "metrics": {"bleu_score": 14.411291670643013, "chrf_score": 15.129793198677252, "xcomet_score": 0.8048961758613586, "xcomet_qe_score": 0.8333538770675659, "metricx_score": 4.680325508117676, "metricx_qe_score": 5.3528852462768555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼。今天我将向您简要介绍我们关于使用多集标记和潜在置换实现无树结构的组合概括的论文。", "metrics": {"bleu_score": 33.59357510191756, "chrf_score": 25.12456134201128, "xcomet_score": 0.8986061811447144, "xcomet_qe_score": 0.8573871850967407, "metricx_score": 2.1171863079071045, "metricx_qe_score": 1.7126901149749756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与导师亚历山大·科勒(Alexander Koller)和伊万·蒂托夫(Ivan Titov)的合作成果。", "metrics": {"bleu_score": 18.96550847075289, "chrf_score": 58.89889523636156, "xcomet_score": 0.995305061340332, "xcomet_qe_score": 0.9606826901435852, "metricx_score": 1.153705358505249, "metricx_qe_score": 1.2234477996826172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "构成泛化能力可以理解为学习者处理更深层次的递归和训练过程中单独见过的短语的新组合的能力。", "metrics": {"bleu_score": 67.27485569179622, "chrf_score": 62.1061436707229, "xcomet_score": 0.7038033604621887, "xcomet_qe_score": 0.68349289894104, "metricx_score": 4.642439842224121, "metricx_qe_score": 6.597900390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义分析的背景下,测试组合泛化能力可能如下所示。", "metrics": {"bleu_score": 54.02963813314917, "chrf_score": 44.75496736603901, "xcomet_score": 0.965408205986023, "xcomet_qe_score": 0.8892554640769958, "metricx_score": 0.9701953530311584, "metricx_qe_score": 1.8271222114562988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同往常一样,我们有一个训练语句集,在本", "metrics": {"bleu_score": 30.752616970214337, "chrf_score": 27.226237047301133, "xcomet_score": 0.7717912197113037, "xcomet_qe_score": 0.4432368874549866, "metricx_score": 3.3951022624969482, "metricx_qe_score": 3.099674701690674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例中为“女孩睡着了”以及“玛丽知道女孩睡着了”。 (", "metrics": {"bleu_score": 7.164684238257436, "chrf_score": 6.7849834214875075, "xcomet_score": 0.37579452991485596, "xcomet_qe_score": 0.33031827211380005, "metricx_score": 4.439189910888672, "metricx_qe_score": 3.6933481693267822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Zài yǔyì fēnxī de bèijìng xià, cèshì zǔhé fānhuà nénglì kěnéng jiù huì zhèyàng suǒshì. Tóng wǎngcháng yíyàng, wǒmen yǒu yīgè xùnliàn yǔjù jí, zài běn lìzǐ zhōng wèi “nǚhái shuìzháo le” yǐjí “Mǎlì zhīdào n", "metrics": {"bleu_score": 1.121617874316939, "chrf_score": 4.95308381337793, "xcomet_score": 0.19465288519859314, "xcomet_qe_score": 0.16221438348293304, "metricx_score": 18.315631866455078, "metricx_qe_score": 10.369041442871094, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ǚhái shuìzháo le”.) 这些语句与逻辑形式相配,逻辑形式代表了它们意义的核心方面。", "metrics": {"bleu_score": 4.350111698509566, "chrf_score": 9.692569002123143, "xcomet_score": 0.5648665428161621, "xcomet_qe_score": 0.4996039569377899, "metricx_score": 10.501469612121582, "metricx_qe_score": 17.958303451538086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集并非来自同一分布,而是包含结构上未曾见过的逻辑形式。", "metrics": {"bleu_score": 57.919360960843825, "chrf_score": 48.865461852247925, "xcomet_score": 0.9670906066894531, "xcomet_qe_score": 0.9534275531768799, "metricx_score": 1.1220648288726807, "metricx_qe_score": 2.105464220046997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中经历了浅层递归,并在具有更深层次递归的", "metrics": {"bleu_score": 33.43644249034819, "chrf_score": 27.048426542652436, "xcomet_score": 0.6018657684326172, "xcomet_qe_score": 0.6622163653373718, "metricx_score": 7.222642421722412, "metricx_qe_score": 6.7710676193237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "示例上进行了测试。 天真的序列到序列模型在这种超出分布范围的泛化上存在困难,并经常产生与输入无关的输出。", "metrics": {"bleu_score": 28.969206725615393, "chrf_score": 32.93043751644364, "xcomet_score": 0.639696478843689, "xcomet_qe_score": 0.5235029458999634, "metricx_score": 3.3417277336120605, "metricx_qe_score": 2.682701349258423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其地,他们经常无法重现输入和输出之间的系统对应关系,例如在示例中用颜色编码的那些对应关系。", "metrics": {"bleu_score": 47.14846638187058, "chrf_score": 44.80994820980098, "xcomet_score": 0.9459391236305237, "xcomet_qe_score": 0.9391677379608154, "metricx_score": 1.387900948524475, "metricx_qe_score": 1.063045620918274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的方法是将树木集成到模型中。", "metrics": {"bleu_score": 33.49490518292079, "chrf_score": 27.35321007693968, "xcomet_score": 0.8819860219955444, "xcomet_qe_score": 0.9014766216278076, "metricx_score": 0.8151487112045288, "metricx_qe_score": 1.0012128353118896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树旨在捕捉与逻辑形式相关联的语句的组合过程。", "metrics": {"bleu_score": 25.315112684135887, "chrf_score": 23.066581534423513, "xcomet_score": 0.9291744232177734, "xcomet_qe_score": 0.8273036479949951, "metricx_score": 3.5279271602630615, "metricx_qe_score": 4.159653663635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这方法有效,但通常不会直接给出树结构,需要通过某种方式获取。", "metrics": {"bleu_score": 33.53405543690827, "chrf_score": 30.662801497760316, "xcomet_score": 0.9879758358001709, "xcomet_qe_score": 0.9930018186569214, "metricx_score": 0.5902580618858337, "metricx_qe_score": 0.9547306299209595, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本较高的过程。", "metrics": {"bleu_score": 35.4225224760582, "chrf_score": 30.964956807920935, "xcomet_score": 0.9723730087280273, "xcomet_qe_score": 0.9788724184036255, "metricx_score": 0.4672151803970337, "metricx_qe_score": 0.6136482357978821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这需要对逻辑形式进行大量专门的预处理,例如,为了处理变量符号。", "metrics": {"bleu_score": 48.72534324082137, "chrf_score": 39.611563657879, "xcomet_score": 0.8433393239974976, "xcomet_qe_score": 0.9583675265312195, "metricx_score": 1.0871756076812744, "metricx_qe_score": 1.2450697422027588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树木也可能涉及专业的语法归纳程序。", "metrics": {"bleu_score": 35.5669644969923, "chrf_score": 30.27393952410525, "xcomet_score": 0.8108344078063965, "xcomet_qe_score": 0.6646054983139038, "metricx_score": 5.526555061340332, "metricx_qe_score": 6.630249500274658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,而是引入一种神经序列到序列模型,该模型直接建模输入片段与输出片段之间的对应关系。", "metrics": {"bleu_score": 49.5709936748982, "chrf_score": 38.36153236921591, "xcomet_score": 0.8223702907562256, "xcomet_qe_score": 0.8363972306251526, "metricx_score": 1.5222901105880737, "metricx_qe_score": 1.5480726957321167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首次,我们展示了在不依赖树结构的情况下,对更深层递归的强泛化能力。", "metrics": {"bleu_score": 49.72091245660371, "chrf_score": 41.2294421753731, "xcomet_score": 0.9830950498580933, "xcomet_qe_score": 0.9693622589111328, "metricx_score": 2.374300241470337, "metricx_qe_score": 3.0225493907928467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两个步骤预测输入的输出。", "metrics": {"bleu_score": 79.12619863720215, "chrf_score": 74.9408395085026, "xcomet_score": 0.9959282875061035, "xcomet_qe_score": 0.9735335111618042, "metricx_score": 0.4835524559020996, "metricx_qe_score": 0.855769157409668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入标记添加一个无序的多集合,其中包含将在输出中出现的标记。", "metrics": {"bleu_score": 18.181820747609674, "chrf_score": 19.978339676271645, "xcomet_score": 0.8088306188583374, "xcomet_qe_score": 0.8304609060287476, "metricx_score": 3.136526584625244, "metricx_qe_score": 2.8456640243530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个步骤之后,我们拥有了所有正确的令牌,但它们尚未排序。", "metrics": {"bleu_score": 31.387950392819207, "chrf_score": 28.967132832889586, "xcomet_score": 0.8512206077575684, "xcomet_qe_score": 0.8621318340301514, "metricx_score": 5.64223051071167, "metricx_qe_score": 5.1074395179748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步中,我们使用另一个模型来预测一个排列,以将它们放置在正确的顺序中。", "metrics": {"bleu_score": 43.71457607477818, "chrf_score": 45.20810127140832, "xcomet_score": 0.9144571423530579, "xcomet_qe_score": 0.9039705991744995, "metricx_score": 2.8809895515441895, "metricx_qe_score": 3.8077166080474854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新的方法来预测一种不对可能排列施加任何硬约束的排列。", "metrics": {"bleu_score": 50.10105108319363, "chrf_score": 46.11091631121303, "xcomet_score": 0.7974512577056885, "xcomet_qe_score": 0.7198668122291565, "metricx_score": 4.0571608543396, "metricx_qe_score": 3.9805712699890137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 33.903916544908675, "chrf_score": 27.589151796760387, "xcomet_score": 0.98786461353302, "xcomet_qe_score": 0.9677702188491821, "metricx_score": 0.7670807838439941, "metricx_qe_score": 1.2802248001098633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致运作如下。", "metrics": {"bleu_score": 37.70873273682918, "chrf_score": 31.19698644702401, "xcomet_score": 0.9181387424468994, "xcomet_qe_score": 0.9657492637634277, "metricx_score": 1.2701725959777832, "metricx_qe_score": 0.8324024677276611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右扫描输出,确定每个位置放置哪个多集令牌。", "metrics": {"bleu_score": 48.3796170917138, "chrf_score": 42.64104689933396, "xcomet_score": 0.7875443696975708, "xcomet_qe_score": 0.7411808967590332, "metricx_score": 4.679092884063721, "metricx_qe_score": 3.685758590698242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们直接选择一个,如红色标记所示。", "metrics": {"bleu_score": 47.973392274396026, "chrf_score": 40.77583712565096, "xcomet_score": 0.9863330125808716, "xcomet_qe_score": 0.9932188987731934, "metricx_score": 0.46956536173820496, "metricx_qe_score": 0.4505721628665924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们跳到下一个多集令牌,以确定输出中的第二个令牌。", "metrics": {"bleu_score": 50.88881048999093, "chrf_score": 43.80618763858309, "xcomet_score": 0.6891230344772339, "xcomet_qe_score": 0.6664390563964844, "metricx_score": 7.932967662811279, "metricx_qe_score": 6.339047431945801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过跳转到另一个多集令牌,以类似的方式确定输出中的第三个令牌。", "metrics": {"bleu_score": 71.86349451138221, "chrf_score": 67.6811440301518, "xcomet_score": 0.6910558938980103, "xcomet_qe_score": 0.7053862810134888, "metricx_score": 7.719019412994385, "metricx_qe_score": 5.330739974975586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到第一阶段的每个令牌都被精确访问一次。", "metrics": {"bleu_score": 59.24450913674052, "chrf_score": 53.234672684742144, "xcomet_score": 0.7889460325241089, "xcomet_qe_score": 0.7924833297729492, "metricx_score": 4.165687561035156, "metricx_qe_score": 3.2899093627929688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让您对实验结果有个预览,我们在这里将我们的方法与其他无树模型在 COGS 基准测试上进行比较。", "metrics": {"bleu_score": 56.61291542395855, "chrf_score": 53.90738960989145, "xcomet_score": 0.8112733364105225, "xcomet_qe_score": 0.730854332447052, "metricx_score": 2.398193836212158, "metricx_qe_score": 2.669482707977295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的模型在深层递归的泛化能力上显著超越了其他模型。", "metrics": {"bleu_score": 56.39934631452339, "chrf_score": 48.333806922656116, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9684561491012573, "metricx_qe_score": 1.341942548751831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,一些其他类型的结构概括仍然非常具有挑战性。", "metrics": {"bleu_score": 29.93195015610124, "chrf_score": 32.78769886497865, "xcomet_score": 0.9429450631141663, "xcomet_qe_score": 0.9863185882568359, "metricx_score": 1.9537898302078247, "metricx_qe_score": 1.3023054599761963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9961615800857544, "xcomet_qe_score": 0.986243486404419, "metricx_score": 0.13248570263385773, "metricx_qe_score": 0.16579806804656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,训练数据中没有提供输入和输出的对齐信息。", "metrics": {"bleu_score": 36.076605997271955, "chrf_score": 28.29494685329926, "xcomet_score": 0.9904406070709229, "xcomet_qe_score": 0.9806236028671265, "metricx_score": 0.4581664800643921, "metricx_qe_score": 0.5017287135124207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的令牌,我们不知道它来自哪个多设置器,这为训练带来了挑战。", "metrics": {"bleu_score": 58.0358970684786, "chrf_score": 51.68262099968047, "xcomet_score": 0.7439107894897461, "xcomet_qe_score": 0.7200514078140259, "metricx_score": 7.309753894805908, "metricx_qe_score": 5.210699558258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列组合,但语言上正确的排列组合是潜在的。", "metrics": {"bleu_score": 61.93430192257325, "chrf_score": 62.546196123404776, "xcomet_score": 0.9146991968154907, "xcomet_qe_score": 0.8648902773857117, "metricx_score": 2.359483242034912, "metricx_qe_score": 2.7891693115234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练中诱导对齐来解决这个问题。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 82.10182318541452, "xcomet_score": 0.9801583290100098, "xcomet_qe_score": 0.9072984457015991, "metricx_score": 0.7283324599266052, "metricx_qe_score": 0.9682132005691528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的变换方法非常灵活,但这带来了找到最高得分变换的挑战,这是 NP 难问题。", "metrics": {"bleu_score": 26.09772519204404, "chrf_score": 23.619142239828232, "xcomet_score": 0.6931474804878235, "xcomet_qe_score": 0.756364107131958, "metricx_score": 2.7111940383911133, "metricx_qe_score": 2.108525276184082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一种适合GPU的连续放松方法来近似此问题,该方法还允许我们对解进行反向传播,并学习语言上更合理的排列组合。", "metrics": {"bleu_score": 18.732951873728503, "chrf_score": 20.163214540930035, "xcomet_score": 0.8172404766082764, "xcomet_qe_score": 0.6869527697563171, "metricx_score": 2.8935563564300537, "metricx_qe_score": 3.789959192276001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息,请查看我们的论文或参加我们的海报展示。", "metrics": {"bleu_score": 81.62735195826225, "chrf_score": 79.55911697772808, "xcomet_score": 0.9635475873947144, "xcomet_qe_score": 0.9758280515670776, "metricx_score": 0.8027520179748535, "metricx_qe_score": 0.8057847023010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Akshata,今天我的合著者Martin和我将展示我们的作品《Kipma步骤》,评估来自多个来源的知识整合。这项", "metrics": {"bleu_score": 45.759220467927264, "chrf_score": 47.49181039958959, "xcomet_score": 0.5000738501548767, "xcomet_qe_score": 0.5151375532150269, "metricx_score": 8.75587272644043, "metricx_qe_score": 7.073838233947754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究之间的合作成果。", "metrics": {"bleu_score": 67.45553465668743, "chrf_score": 62.60925345265984, "xcomet_score": 0.8106120824813843, "xcomet_qe_score": 0.7065490484237671, "metricx_score": 4.178657531738281, "metricx_qe_score": 4.327840805053711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用多种知识来源,例如其参数中包含的知识,通常通过预训练获得,以及推理时输入中提供的知识。 近", "metrics": {"bleu_score": 56.61342442804014, "chrf_score": 51.425210951977526, "xcomet_score": 0.5940266847610474, "xcomet_qe_score": 0.569442629814148, "metricx_score": 7.182519435882568, "metricx_qe_score": 4.300304889678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期在问答等任务中的研究表明,模型可以利用预训练的时间知识来解决这些任务。", "metrics": {"bleu_score": 58.611859017694, "chrf_score": 50.322374413929005, "xcomet_score": 0.7971233129501343, "xcomet_qe_score": 0.7732760906219482, "metricx_score": 4.4741339683532715, "metricx_qe_score": 4.335671424865723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常需要在推理时也提供的知识。", "metrics": {"bleu_score": 79.632051309738, "chrf_score": 73.71916932929182, "xcomet_score": 0.870110273361206, "xcomet_qe_score": 0.8266476392745972, "metricx_score": 1.1921392679214478, "metricx_qe_score": 1.676377773284912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子中,“约翰在电视上看到了新当选的总统,”", "metrics": {"bleu_score": 36.169462963364815, "chrf_score": 21.149443105762025, "xcomet_score": 0.9550728797912598, "xcomet_qe_score": 0.9500565528869629, "metricx_score": 1.92360520362854, "metricx_qe_score": 1.9556903839111328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可以包含有关先例如何运作以及什么是 TVA 的信息,但它们无法可靠地知道这个特定事件实体约翰是谁或谁是新任总统,因为先例可能在预训练后已经发生变化。", "metrics": {"bleu_score": 37.44456076899746, "chrf_score": 32.485389076435666, "xcomet_score": 0.5528533458709717, "xcomet_qe_score": 0.5972267389297485, "metricx_score": 7.080156326293945, "metricx_qe_score": 7.141689777374268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型自然语言理解任务模型需要具备整合和利用预训练时和推理时知识的能力。", "metrics": {"bleu_score": 62.35317604151713, "chrf_score": 50.69054785153202, "xcomet_score": 0.9927245378494263, "xcomet_qe_score": 0.9346688389778137, "metricx_score": 0.5616680979728699, "metricx_qe_score": 1.1287797689437866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们提出了一个知识整合的诊断测试套件。", "metrics": {"bleu_score": 74.47272188215545, "chrf_score": 66.62386777697506, "xcomet_score": 0.9971035718917847, "xcomet_qe_score": 0.9934111833572388, "metricx_score": 0.8166946768760681, "metricx_qe_score": 1.4972199201583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个核心词指代解析任务,旨在探究利用不同来源知识的能力。", "metrics": {"bleu_score": 51.51420826880311, "chrf_score": 42.51352462733163, "xcomet_score": 0.8597185611724854, "xcomet_qe_score": 0.8462350368499756, "metricx_score": 2.311014175415039, "metricx_qe_score": 2.4694275856018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在人类研究参与者和已建立的共指代解析模型中对数据集进行了评估。", "metrics": {"bleu_score": 50.59416364419384, "chrf_score": 51.54231177457195, "xcomet_score": 0.9331636428833008, "xcomet_qe_score": 0.8504602313041687, "metricx_score": 3.918706178665161, "metricx_qe_score": 3.81358003616333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子。T", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 72.97916540231482, "xcomet_score": 0.9141321182250977, "xcomet_qe_score": 0.7820430994033813, "metricx_score": 2.0470070838928223, "metricx_qe_score": 1.2290176153182983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "hirvin 是一位法官。K", "metrics": {"bleu_score": 25.848657697858535, "chrf_score": 42.69793270951488, "xcomet_score": 0.29134637117385864, "xcomet_qe_score": 0.18920737504959106, "metricx_score": 6.932167053222656, "metricx_qe_score": 8.075550079345703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ia 是一位面包师。T", "metrics": {"bleu_score": 35.49481056010054, "chrf_score": 29.556878306878303, "xcomet_score": 0.6557573080062866, "xcomet_qe_score": 0.6001766920089722, "metricx_score": 6.48315954208374, "metricx_qe_score": 5.422949314117432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "hirvin 和 Kia 在公园里相遇。", "metrics": {"bleu_score": 27.301208627090666, "chrf_score": 41.82652693948586, "xcomet_score": 0.6692394018173218, "xcomet_qe_score": 0.5485920906066895, "metricx_score": 5.503588676452637, "metricx_qe_score": 6.669229030609131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审案子度过漫长的一天后,他很高兴能放松一下。", "metrics": {"bleu_score": 43.90960897971484, "chrf_score": 37.35528383014845, "xcomet_score": 0.9129434823989868, "xcomet_qe_score": 0.8957387208938599, "metricx_score": 1.66608726978302, "metricx_qe_score": 1.7499232292175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本任务是识别代词“he”所指的正确实体,在这个例子中是“仆人”。 一个", "metrics": {"bleu_score": 27.637283103751187, "chrf_score": 22.62213708000777, "xcomet_score": 0.5305273532867432, "xcomet_qe_score": 0.4542860984802246, "metricx_score": 6.779606342315674, "metricx_qe_score": 4.104710578918457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "给定代词的解析需要两种信息。", "metrics": {"bleu_score": 16.791082496935097, "chrf_score": 16.872660383237857, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9760814905166626, "metricx_qe_score": 0.9315566420555115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定的知识,例如调查是一个法官。其", "metrics": {"bleu_score": 15.892715631425643, "chrf_score": 15.69665767333546, "xcomet_score": 0.5078160762786865, "xcomet_qe_score": 0.5391817092895508, "metricx_score": 8.33067798614502, "metricx_qe_score": 6.975553512573242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上裁决案件。", "metrics": {"bleu_score": 33.308456462852334, "chrf_score": 29.68967949030302, "xcomet_score": 0.8814171552658081, "xcomet_qe_score": 0.8098436594009399, "metricx_score": 4.524585723876953, "metricx_qe_score": 3.8353097438812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,背景知识是在大型语言模型的预训练期间学习的,而实体特定知识通常在推理阶段观察到。", "metrics": {"bleu_score": 45.67246358911064, "chrf_score": 38.4504082610634, "xcomet_score": 0.8680987358093262, "xcomet_qe_score": 0.8937650322914124, "metricx_score": 1.198267936706543, "metricx_qe_score": 1.726236343383789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过调整这两种信息的可用性,使其可能出现在单一来源中,或出现在多个来源中。", "metrics": {"bleu_score": 43.26389730340493, "chrf_score": 41.97756470124966, "xcomet_score": 0.9515343904495239, "xcomet_qe_score": 0.915793240070343, "metricx_score": 0.9717164039611816, "metricx_qe_score": 1.0818383693695068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了KITMOS的三个设置。", "metrics": {"bleu_score": 46.59538415189962, "chrf_score": 56.314551461610286, "xcomet_score": 0.8917385339736938, "xcomet_qe_score": 0.9042564034461975, "metricx_score": 0.47817862033843994, "metricx_qe_score": 0.5902894735336304, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有典型设置,即背景预训练,假设在预训练时可获得背景知识。", "metrics": {"bleu_score": 32.46947424791821, "chrf_score": 27.63400402544664, "xcomet_score": 0.8633308410644531, "xcomet_qe_score": 0.7112386226654053, "metricx_score": 3.1085705757141113, "metricx_qe_score": 3.708080530166626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,有背景设置,即在预训练阶段和推理阶段都可获得背景", "metrics": {"bleu_score": 14.07007792894517, "chrf_score": 16.266787490774625, "xcomet_score": 0.7460877895355225, "xcomet_qe_score": 0.7078503966331482, "metricx_score": 4.182015895843506, "metricx_qe_score": 4.464013576507568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "知识。最后是背景推理设置,两种类型的知识仅在推理阶段可用。", "metrics": {"bleu_score": 19.0834619596292, "chrf_score": 21.704700152976017, "xcomet_score": 0.6384974122047424, "xcomet_qe_score": 0.6303160786628723, "metricx_score": 3.0405492782592773, "metricx_qe_score": 4.015859127044678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置尤其有趣,因为它模拟了背景知识不包含在模型预训练数据中的情况,这些背景知识对于完成任务是必要的。", "metrics": {"bleu_score": 38.48303309532073, "chrf_score": 37.09921788116276, "xcomet_score": 0.9859212636947632, "xcomet_qe_score": 0.9707742929458618, "metricx_score": 0.5070332288742065, "metricx_qe_score": 0.6743755340576172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,由于新的职业在预训练时期之后才发展出来。", "metrics": {"bleu_score": 17.855149299161596, "chrf_score": 19.22150631417314, "xcomet_score": 0.8686543703079224, "xcomet_qe_score": 0.864090085029602, "metricx_score": 1.3729095458984375, "metricx_qe_score": 1.8786007165908813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们如何控制真实来源中事实的可用性的示例。", "metrics": {"bleu_score": 65.0067006014708, "chrf_score": 63.50420575693395, "xcomet_score": 0.8585423231124878, "xcomet_qe_score": 0.8189477324485779, "metricx_score": 0.8893071413040161, "metricx_qe_score": 1.3475074768066406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在预训练背景设置中,我们假设背景知识“政治家寻求当选政府职位”已包含在预训练参数中。在不常见的时间上下文中,我们提供反特定知识“奇切斯特是一位政治家”。", "metrics": {"bleu_score": 42.21732151066663, "chrf_score": 32.99978105007192, "xcomet_score": 0.48483848571777344, "xcomet_qe_score": 0.46307143568992615, "metricx_score": 5.899993419647217, "metricx_qe_score": 5.611946105957031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景设置中,我们不仅提供反特定信息,还提供政治家在干扰型语境中的背景知识。 (注:由于原文中\"background-bove\"和\"interference-type\"可能为特定术语或拼写错误,这里做了一定猜测性翻译。", "metrics": {"bleu_score": 16.090910772301797, "chrf_score": 17.6330713778682, "xcomet_score": 0.3279592990875244, "xcomet_qe_score": 0.36847779154777527, "metricx_score": 6.961761951446533, "metricx_qe_score": 6.7520647048950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "\"anti-specific\"在学术或技术语境下可能有特定含义,这里直接保留原文翻译。) 在背景干扰设置中,我们提供了虚构的职业“功勋者”(Meritur)而非“政治家”,因为“功勋者”不太可能被包含在预训练的范式中。", "metrics": {"bleu_score": 26.90706758771112, "chrf_score": 32.23075410926068, "xcomet_score": 0.2136460840702057, "xcomet_qe_score": 0.0805530995130539, "metricx_score": 6.149423122406006, "metricx_qe_score": 7.321673393249512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在人类研究参与者和已建立的共指代解析模型中对数据集进行了评估。", "metrics": {"bleu_score": 50.59416364419384, "chrf_score": 51.54231177457195, "xcomet_score": 0.9318628311157227, "xcomet_qe_score": 0.846091091632843, "metricx_score": 3.881432294845581, "metricx_qe_score": 3.719200372695923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了背景预训练设置中最困难变体上表现", "metrics": {"bleu_score": 34.213456753622395, "chrf_score": 33.49098618170151, "xcomet_score": 0.6846222877502441, "xcomet_qe_score": 0.7152231931686401, "metricx_score": 6.2873334884643555, "metricx_qe_score": 6.3756489753723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最佳的模型的结果。 在针对特定任务的KITMOS训练中,两个模型的表现都不理想。", "metrics": {"bleu_score": 21.910168125716663, "chrf_score": 24.62446007070555, "xcomet_score": 0.2983132004737854, "xcomet_qe_score": 0.14166592061519623, "metricx_score": 4.715919017791748, "metricx_qe_score": 6.0082316398620605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在KITMOS上训练时,C2F和BFQF两个模型的表现显著优于随机选择。", "metrics": {"bleu_score": 18.189587992135596, "chrf_score": 27.12582151153684, "xcomet_score": 0.6841754913330078, "xcomet_qe_score": 0.7318246364593506, "metricx_score": 3.3010659217834473, "metricx_qe_score": 4.575453758239746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明当模型在一般代词指代解析数据集上进行训练时,它们学会了利用表面线索,而在测试去除此类线索的复杂语境(kitmos)时,这些表面线索却无法发挥作用。", "metrics": {"bleu_score": 29.932835782262615, "chrf_score": 25.593483788875343, "xcomet_score": 0.6946556568145752, "xcomet_qe_score": 0.7362151145935059, "metricx_score": 5.780703067779541, "metricx_qe_score": 5.481441974639893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "额外的实验结果,结合虚构知识,表明即使是表现最佳的模型,也无法可靠地整合仅在推理时提供的逆向知识。 总结", "metrics": {"bleu_score": 52.96413985275462, "chrf_score": 49.869832104662144, "xcomet_score": 0.8040165901184082, "xcomet_qe_score": 0.7607983350753784, "metricx_score": 2.949445962905884, "metricx_qe_score": 1.7171447277069092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要发现,许多共指革命模型在没有任务特定训练的情况下,似乎无法对来自不同来源的知识进行推理。", "metrics": {"bleu_score": 62.557609374511955, "chrf_score": 53.56512628178006, "xcomet_score": 0.752715528011322, "xcomet_qe_score": 0.7636202573776245, "metricx_score": 5.282179832458496, "metricx_qe_score": 6.2979583740234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在进行任务特定训练后,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 64.86728054024023, "chrf_score": 62.95597188025731, "xcomet_score": 0.9702972173690796, "xcomet_qe_score": 0.9547964930534363, "metricx_score": 0.857155442237854, "metricx_qe_score": 1.4682708978652954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管表现最佳的模型似乎在推理时可靠地整合仅呈现的背景知识方面存在困难,但", "metrics": {"bleu_score": 29.967404858794147, "chrf_score": 26.332626112292264, "xcomet_score": 0.4715849757194519, "xcomet_qe_score": 0.4576033651828766, "metricx_score": 5.470664978027344, "metricx_qe_score": 3.0448994636535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多细节,请参阅我们的论文,并在 GitHub 代码中查看数据集。谢谢您", "metrics": {"bleu_score": 49.525370020111716, "chrf_score": 48.55121835635952, "xcomet_score": 0.9109460115432739, "xcomet_qe_score": 0.9089650511741638, "metricx_score": 1.309326171875, "metricx_qe_score": 0.8102596998214722, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的聆听。", "metrics": {"bleu_score": 46.30777161991026, "chrf_score": 37.73656898656898, "xcomet_score": 0.8018722534179688, "xcomet_qe_score": 0.7317452430725098, "metricx_score": 5.9059624671936035, "metricx_qe_score": 9.609138488769531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是迈拉,今天我将讨论我们的一篇论文,题为《标记人设:使用自然语言提示衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 48.845222456355906, "chrf_score": 42.90804434157733, "xcomet_score": 0.6795734763145447, "xcomet_qe_score": 0.6595785617828369, "metricx_score": 2.168351650238037, "metricx_qe_score": 1.8320974111557007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与埃森·德穆什和丹·乔拉夫斯基合作完成的。", "metrics": {"bleu_score": 20.22202784230022, "chrf_score": 13.133548395927551, "xcomet_score": 0.9459896087646484, "xcomet_qe_score": 0.9731645584106445, "metricx_score": 1.1917400360107422, "metricx_qe_score": 1.3307572603225708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究人员记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 39.317734814836555, "chrf_score": 41.155189833322595, "xcomet_score": 0.9839339256286621, "xcomet_qe_score": 0.9310780763626099, "metricx_score": 1.7444849014282227, "metricx_qe_score": 4.1152753829956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施存在各种局限性。", "metrics": {"bleu_score": 34.245097009375314, "chrf_score": 27.612730879133828, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09926637262105942, "metricx_qe_score": 0.24205049872398376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,而这些数据集的整理需要花费大量时间。 它们通常也只测量非常特定的刻板印象,这意味着它们不能很好地推广到其他人口统计学或背景,或者它们只是捕捉到非常普遍的广泛关联,例如与特定群体相关的负面关联。", "metrics": {"bleu_score": 43.84690856262099, "chrf_score": 38.69073890836769, "xcomet_score": 0.6411104202270508, "xcomet_qe_score": 0.595622181892395, "metricx_score": 5.120828628540039, "metricx_qe_score": 5.137091159820557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这方面的多数工作并未考虑交织性,即多层面的社会身份可以加剧偏见,并成为独特的伤害焦点。", "metrics": {"bleu_score": 39.055757667724144, "chrf_score": 33.20198628433533, "xcomet_score": 0.7401057481765747, "xcomet_qe_score": 0.7190415263175964, "metricx_score": 3.7364988327026367, "metricx_qe_score": 4.041325569152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们利用了这些最新的指令调优大语言模型(LLM)在响应提示中的指令方面非常擅长的特性。", "metrics": {"bleu_score": 34.84837567005522, "chrf_score": 32.33084114638906, "xcomet_score": 0.7694519758224487, "xcomet_qe_score": 0.7229793667793274, "metricx_score": 2.7989988327026367, "metricx_qe_score": 2.833909273147583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个人物形象,这是一种通过提示如“想象你是一个亚洲女性,", "metrics": {"bleu_score": 32.59734744268067, "chrf_score": 33.135341148561686, "xcomet_score": 0.8509954810142517, "xcomet_qe_score": 0.7390502095222473, "metricx_score": 5.501185417175293, "metricx_qe_score": 6.603396415710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述你自己”来描绘想象中个体的描述。", "metrics": {"bleu_score": 10.934883431625593, "chrf_score": 15.399529079637682, "xcomet_score": 0.1901327520608902, "xcomet_qe_score": 0.17053374648094177, "metricx_score": 4.525157451629639, "metricx_qe_score": 4.9169087409973145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这可以很轻易地应用于任何人口统计学,因为我们只需在提示中指定我们想要的任何身份标记即可。 所以", "metrics": {"bleu_score": 42.620730573789295, "chrf_score": 34.286656331866965, "xcomet_score": 0.8502323627471924, "xcomet_qe_score": 0.6978386640548706, "metricx_score": 3.662257194519043, "metricx_qe_score": 2.4430322647094727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里是GPT-4生成的一些示例。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 84.9413086913087, "xcomet_score": 0.9396415948867798, "xcomet_qe_score": 0.8412882089614868, "metricx_score": 0.9823170900344849, "metricx_qe_score": 1.910975456237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立即可以看出,尽管这些输出在传统意义上并不显而易见地具有负面或有毒的特性, 存在一些有趣的模式。", "metrics": {"bleu_score": 33.94886918459836, "chrf_score": 33.555954868273155, "xcomet_score": 0.9224910736083984, "xcomet_qe_score": 0.9085690975189209, "metricx_score": 2.816901683807373, "metricx_qe_score": 3.231292486190796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘为谦逊。中东女性则被用如“异国情调”这样的词汇描述,仿佛是在指代一个令人着迷的区域。", "metrics": {"bleu_score": 29.245731144491952, "chrf_score": 25.93457477319972, "xcomet_score": 0.9145017266273499, "xcomet_qe_score": 0.9081001877784729, "metricx_score": 2.542531728744507, "metricx_qe_score": 2.3851523399353027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有色人种女性角色都提到了祖先,而白人男性角色却没有这样的提及。", "metrics": {"bleu_score": 39.79309387395556, "chrf_score": 35.63840704672666, "xcomet_score": 0.9628331661224365, "xcomet_qe_score": 0.9773290157318115, "metricx_score": 1.3073786497116089, "metricx_qe_score": 1.1114219427108765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两个部分。", "metrics": {"bleu_score": 72.42447986095323, "chrf_score": 65.96982125906077, "xcomet_score": 0.9951229095458984, "xcomet_qe_score": 0.9762166738510132, "metricx_score": 0.16056504845619202, "metricx_qe_score": 0.23297664523124695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人物形象。", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 65.93509679113568, "xcomet_score": 0.9113327264785767, "xcomet_qe_score": 0.8275501728057861, "metricx_score": 0.4909520149230957, "metricx_qe_score": 0.9603396058082581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人物形象的提示词受到一项研究的启发,该研究将这些提示词给与人类受试者,发现通过这种方式,他们也能够揭示种族刻板印象。", "metrics": {"bleu_score": 49.96495280806363, "chrf_score": 42.7517793432793, "xcomet_score": 0.7719827890396118, "xcomet_qe_score": 0.7699455618858337, "metricx_score": 2.434025764465332, "metricx_qe_score": 3.1749563217163086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这同时也使得我们生成的角色形象可以与人类书面回应进行直接比较。", "metrics": {"bleu_score": 50.93546590350888, "chrf_score": 42.759797021940145, "xcomet_score": 0.9117246866226196, "xcomet_qe_score": 0.911733090877533, "metricx_score": 1.4122917652130127, "metricx_qe_score": 1.9850897789001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种方法,用于识别区分标记组和未标记组的词语,我稍后会详细阐述。", "metrics": {"bleu_score": 23.430533215542294, "chrf_score": 21.321912763225168, "xcomet_score": 0.8972198367118835, "xcomet_qe_score": 0.9522267580032349, "metricx_score": 1.0444711446762085, "metricx_qe_score": 1.1583352088928223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点是,我们可以获得非常具体的刻板印象和模式,而不必依赖于任何特定的词汇表。", "metrics": {"bleu_score": 36.92819533539391, "chrf_score": 36.27074347196713, "xcomet_score": 0.9819979667663574, "xcomet_qe_score": 0.898820698261261, "metricx_score": 1.0938243865966797, "metricx_qe_score": 1.3312652111053467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词语法借鉴了社会语言学中的“标记性”概念,该概念指出存在一个未标记的默认状态,任何与该默认状态不同的群体在语言上都被标记为特殊。", "metrics": {"bleu_score": 47.35295478723867, "chrf_score": 39.917422014687986, "xcomet_score": 0.6370930671691895, "xcomet_qe_score": 0.8729938864707947, "metricx_score": 1.5349448919296265, "metricx_qe_score": 1.5820577144622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,\"战士\"这个词通常与男性相关联。", "metrics": {"bleu_score": 55.17958558024982, "chrf_score": 52.741099285096695, "xcomet_score": 0.9824000597000122, "xcomet_qe_score": 1.0, "metricx_score": 0.3814878761768341, "metricx_qe_score": 0.5908745527267456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以当人们描述一个女性战士时,他们通常会实际指定\"一男战士\"并用\"女性\"来标记这个术语。", "metrics": {"bleu_score": 47.995665508308214, "chrf_score": 41.91608970973289, "xcomet_score": 0.6366183161735535, "xcomet_qe_score": 0.709679126739502, "metricx_score": 5.44589900970459, "metricx_qe_score": 5.911107063293457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是没有标记的,而边缘化群体通常是有标记的。", "metrics": {"bleu_score": 62.9317173142247, "chrf_score": 55.48309541452895, "xcomet_score": 0.9008029103279114, "xcomet_qe_score": 0.8353347778320312, "metricx_score": 0.9803244471549988, "metricx_qe_score": 1.2191580533981323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记组和标记组。 然后,我们使用“战斗词”方法比较这些人物形象,基本上就是使用加权对数几率比来区分每个标记群体的核心词。", "metrics": {"bleu_score": 47.81295890945805, "chrf_score": 41.314107312489604, "xcomet_score": 0.6274529695510864, "xcomet_qe_score": 0.5966055393218994, "metricx_score": 3.1934661865234375, "metricx_qe_score": 4.61347770690918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会使用战斗词语,并将法律神比例与白人角色和男性角色进行比较,因为它们是两个相应的未标记群体。", "metrics": {"bleu_score": 53.872493689912524, "chrf_score": 46.771314162069515, "xcomet_score": 0.5629136562347412, "xcomet_qe_score": 0.4566125273704529, "metricx_score": 6.559783458709717, "metricx_qe_score": 7.700213432312012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看一些结果。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 37.07384040111085, "xcomet_score": 0.9672292470932007, "xcomet_qe_score": 0.9580326080322266, "metricx_score": 0.40737825632095337, "metricx_qe_score": 0.6341195106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个刻板印象词典,发现生成的人物形象中包含的刻板印象远多于人工撰写的。", "metrics": {"bleu_score": 32.567607074622565, "chrf_score": 27.7940752333104, "xcomet_score": 0.9620206356048584, "xcomet_qe_score": 0.9552292823791504, "metricx_score": 1.3903157711029053, "metricx_qe_score": 1.609383225440979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际观察词语在词典中的分布时,发现的情况却大不相同。", "metrics": {"bleu_score": 16.23790818441275, "chrf_score": 18.505871996560373, "xcomet_score": 0.9585387706756592, "xcomet_qe_score": 0.9667819738388062, "metricx_score": 0.9673841595649719, "metricx_qe_score": 1.0928442478179932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管生成的人物形象中“Luxon”词的出现频率更高,但人工撰写的人物形象在词语分布上更为广泛。此外,生成的人物形象中出现的刻板印象词仅限于“高大”和“健壮”。", "metrics": {"bleu_score": 6.522609756072444, "chrf_score": 9.369481339828809, "xcomet_score": 0.5152667760848999, "xcomet_qe_score": 0.5263022780418396, "metricx_score": 7.326594352722168, "metricx_qe_score": 7.507673263549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以真正只有正的或至少非负的影响。", "metrics": {"bleu_score": 9.09605719505039, "chrf_score": 13.296143070792743, "xcomet_score": 0.8382056355476379, "xcomet_qe_score": 0.8340132832527161, "metricx_score": 2.571951150894165, "metricx_qe_score": 1.314576506614685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表并不能很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 83.09183990770298, "chrf_score": 78.44798690773925, "xcomet_score": 0.9238219261169434, "xcomet_qe_score": 0.7770571708679199, "metricx_score": 0.8177748322486877, "metricx_qe_score": 1.2045084238052368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向标记词语方法的结果,以展示这些看似积极的词语如何助长刻板印象和本质化叙事。", "metrics": {"bleu_score": 32.54564043512719, "chrf_score": 28.72787795722136, "xcomet_score": 0.6715990304946899, "xcomet_qe_score": 0.7057721614837646, "metricx_score": 2.5177412033081055, "metricx_qe_score": 2.939274311065674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描述如何反映出有害的模式。", "metrics": {"bleu_score": 62.931089756825216, "chrf_score": 54.27138998306559, "xcomet_score": 0.9867944717407227, "xcomet_qe_score": 0.9823914766311646, "metricx_score": 1.0008602142333984, "metricx_qe_score": 2.2394495010375977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于有标记的群体,排名前列的词包括文化、传统、自豪和异国情调等。这些词仅根据", "metrics": {"bleu_score": 9.270752538375213, "chrf_score": 7.573139715037384, "xcomet_score": 0.5129160284996033, "xcomet_qe_score": 0.44485363364219666, "metricx_score": 8.446845054626465, "metricx_qe_score": 5.990303039550781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们与群体身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 50.12831979934471, "chrf_score": 44.421631448389384, "xcomet_score": 0.8545063734054565, "xcomet_qe_score": 0.8344634771347046, "metricx_score": 2.654453992843628, "metricx_qe_score": 3.3521881103515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这加剧了这些群体长期以来遭受的歧视和边缘化处境。", "metrics": {"bleu_score": 50.296153790170834, "chrf_score": 47.254958880557, "xcomet_score": 0.998647928237915, "xcomet_qe_score": 0.996639609336853, "metricx_score": 0.6245088577270508, "metricx_qe_score": 0.4156908392906189, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的套路,尤其是对有色人种女性的描述。", "metrics": {"bleu_score": 40.69249398600885, "chrf_score": 34.307255476262704, "xcomet_score": 0.7969400882720947, "xcomet_qe_score": 0.8987517356872559, "metricx_score": 2.496675491333008, "metricx_qe_score": 1.517634630203247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉美裔女性的词语往往包括充满活力和曲线玲珑等词汇。 这些词语与热带主义这一主题相关联。", "metrics": {"bleu_score": 18.001282725374942, "chrf_score": 13.567200470795301, "xcomet_score": 0.9841079711914062, "xcomet_qe_score": 0.9334396719932556, "metricx_score": 1.760324478149414, "metricx_qe_score": 1.5871086120605469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,常用的描述词有娇小、细腻和丝滑。 这与亚洲女性被过度性化、被视为极其温顺和顺从的漫长历史相连接。", "metrics": {"bleu_score": 18.691997649868892, "chrf_score": 15.14479206153039, "xcomet_score": 0.8546160459518433, "xcomet_qe_score": 0.9187854528427124, "metricx_score": 2.9595582485198975, "metricx_qe_score": 2.492323398590088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们发现一些最常出现的词语是像“坚强”和“韧性”这样的词。", "metrics": {"bleu_score": 24.81723263771339, "chrf_score": 17.855735835177132, "xcomet_score": 0.9246247410774231, "xcomet_qe_score": 0.9179584980010986, "metricx_score": 1.4046289920806885, "metricx_qe_score": 1.3275741338729858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所称的“强势黑人女性”原型相连接。", "metrics": {"bleu_score": 31.13612721440886, "chrf_score": 26.622970825757204, "xcomet_score": 0.897061288356781, "xcomet_qe_score": 0.8736805319786072, "metricx_score": 2.619873285293579, "metricx_qe_score": 3.039708137512207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍看之下似乎是积极的,但...... 有研究表明,这种刻板印象实际上非常有害,因为它给这些群体带来了巨大的压力,要求他们面对社会障碍时保持韧性和强大。", "metrics": {"bleu_score": 30.410380134449277, "chrf_score": 25.893260514083593, "xcomet_score": 0.7955639362335205, "xcomet_qe_score": 0.7643747329711914, "metricx_score": 2.0656473636627197, "metricx_qe_score": 1.8932865858078003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正努力改变这些障碍,它反而给这些人施加压力,要求他们克服障碍,这导致了这些人健康状况的严重恶化,以及其他伤害。", "metrics": {"bleu_score": 31.792344761853315, "chrf_score": 27.030904767747916, "xcomet_score": 0.9779626131057739, "xcomet_qe_score": 1.0, "metricx_score": 2.1851770877838135, "metricx_qe_score": 1.7409255504608154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群的词语几乎只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 64.02831789967215, "chrf_score": 56.56229607003811, "xcomet_score": 0.8892322778701782, "xcomet_qe_score": 0.8410546779632568, "metricx_score": 1.7630364894866943, "metricx_qe_score": 2.081930160522461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们为模型所有者提出三点建议。", "metrics": {"bleu_score": 58.17070222427868, "chrf_score": 52.33470142531367, "xcomet_score": 0.8770531415939331, "xcomet_qe_score": 0.7754772901535034, "metricx_score": 1.2281999588012695, "metricx_qe_score": 3.2828125953674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化的叙事。", "metrics": {"bleu_score": 29.271572980584498, "chrf_score": 26.974112475209154, "xcomet_score": 0.8091121912002563, "xcomet_qe_score": 0.8174066543579102, "metricx_score": 0.9899067282676697, "metricx_qe_score": 0.7839982509613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉性视角来研究偏见和伤害,因为如果我们不这样做,可能会忽略许多事情。", "metrics": {"bleu_score": 74.04682041982166, "chrf_score": 65.50362674761236, "xcomet_score": 0.9273217916488647, "xcomet_qe_score": 0.8574212789535522, "metricx_score": 0.6053991317749023, "metricx_qe_score": 0.8028656840324402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,关于偏见缓解方法的透明度应该真正得到提高。 因为例如,像这些积极的刻板印象,我们不知道是不是因为某种奇怪的... (注:此翻译保持了原文的语气和结构,适合学术或教学材料,同时遵循了中文的语法和用词习惯。) 正在发生过度过度价值观对齐,或者可能是其他一些如反刻板印象的方法,导致这些恶性的模式。 **Note:** This translation aims for accuracy and preserves the original meaning while using formal and precise Chinese. It retains the somewhat awkward phrasing of the English original to reflect the complexity and potential ambiguity of the source text", "metrics": {"bleu_score": 22.538320848504146, "chrf_score": 24.63018062828942, "xcomet_score": 0.543400764465332, "xcomet_qe_score": 0.4897047281265259, "metricx_score": 7.978516101837158, "metricx_qe_score": 8.221090316772461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ". 在没有更多透明度的情况下,我们真的无法做出任何假设或进一步研究。", "metrics": {"bleu_score": 47.63001139940413, "chrf_score": 40.34622232935825, "xcomet_score": 0.9758108854293823, "xcomet_qe_score": 0.9652528762817383, "metricx_score": 1.3830811977386475, "metricx_qe_score": 1.5337270498275757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的倾听。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6526881456375122, "metricx_qe_score": 0.8881498575210571, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝您度过美好时光。", "metrics": {"bleu_score": 5.669791110976001, "chrf_score": 3.4013605442176873, "xcomet_score": 0.4573700726032257, "xcomet_qe_score": 0.27195480465888977, "metricx_score": 2.013054370880127, "metricx_qe_score": 2.8453381061553955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的易精伟。", "metrics": {"bleu_score": 41.35171000263378, "chrf_score": 30.12663561531255, "xcomet_score": 0.8618423938751221, "xcomet_qe_score": 0.9728631973266602, "metricx_score": 1.185164213180542, "metricx_qe_score": 2.161679267883301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很荣幸能为大家呈现一段关于论文的短视频广告:", "metrics": {"bleu_score": 6.1429730373528315, "chrf_score": 9.163059163059161, "xcomet_score": 0.8462091684341431, "xcomet_qe_score": 0.8116642236709595, "metricx_score": 2.7237114906311035, "metricx_qe_score": 2.2216503620147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《你在复制我的模型吗?通过后门水印保护大型语言模型的版权》 该视频旨在探讨", "metrics": {"bleu_score": 12.83783990777972, "chrf_score": 23.769523030345866, "xcomet_score": 0.36201509833335876, "xcomet_qe_score": 0.2658284604549408, "metricx_score": 4.405762672424316, "metricx_qe_score": 4.501608848571777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如何通过后门水印技术保护大型语言模型的版权,确保知识产权得到尊重和维护。 ", "metrics": {"bleu_score": 32.3431062840509, "chrf_score": 33.83543790078194, "xcomet_score": 0.7711456418037415, "xcomet_qe_score": 0.7069922685623169, "metricx_score": 0.9570862650871277, "metricx_qe_score": 1.5088685750961304, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们首先介绍一下嵌入式服务(Embedding as Services)的背景。", "metrics": {"bleu_score": 33.90387389794622, "chrf_score": 31.783233153315944, "xcomet_score": 0.9957809448242188, "xcomet_qe_score": 1.0, "metricx_score": 0.45532599091529846, "metricx_qe_score": 0.37291401624679565, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、LAMA、PALM这样的超大规模语言模型在自然语言理解和生成方面表现卓越。", "metrics": {"bleu_score": 52.971326642822795, "chrf_score": 64.8061628650004, "xcomet_score": 0.9895854592323303, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.958053708076477, "metricx_qe_score": 1.067590594291687, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将嵌入作为服务是基于大型语言模型的一种服务,用于协助各种自然语言处理任务。", "metrics": {"bleu_score": 40.63105884155229, "chrf_score": 37.92512436044207, "xcomet_score": 0.831619143486023, "xcomet_qe_score": 0.8317093253135681, "metricx_score": 1.2941983938217163, "metricx_qe_score": 1.2719290256500244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI 提供基于 GPT 的嵌入 API。", "metrics": {"bleu_score": 53.24494908744754, "chrf_score": 66.7433318809592, "xcomet_score": 0.987235426902771, "xcomet_qe_score": 0.9493737816810608, "metricx_score": 0.535844087600708, "metricx_qe_score": 0.7298510074615479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,近来的研究表明,攻击者可以通过学习嵌入(embedding)来窃取模型,并提供类似服务。", "metrics": {"bleu_score": 48.62998786874166, "chrf_score": 37.3945966636309, "xcomet_score": 0.9057881236076355, "xcomet_qe_score": 0.8687739372253418, "metricx_score": 2.9692630767822266, "metricx_qe_score": 3.1817944049835205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入作为服务的版权。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 55.3768880775766, "xcomet_score": 0.9367551803588867, "xcomet_qe_score": 0.9434407949447632, "metricx_score": 0.8396740555763245, "metricx_qe_score": 1.3031094074249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "保护嵌入式服务的版权。一种解决方案是在提供者服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 69.48733596244203, "chrf_score": 62.13648458267815, "xcomet_score": 0.9275774955749512, "xcomet_qe_score": 0.946504533290863, "metricx_score": 0.9054272770881653, "metricx_qe_score": 0.9051121473312378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印技术需要满足以下特性。", "metrics": {"bleu_score": 47.037095938668976, "chrf_score": 39.39037814037815, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3298107981681824, "metricx_qe_score": 0.26905837655067444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于广告服务嵌入。", "metrics": {"bleu_score": 45.8303406712411, "chrf_score": 42.26577466000271, "xcomet_score": 0.8878822326660156, "xcomet_qe_score": 0.8830297589302063, "metricx_score": 1.9843220710754395, "metricx_qe_score": 1.9775371551513672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的实用性。", "metrics": {"bleu_score": 65.65037059458353, "chrf_score": 64.66496674755975, "xcomet_score": 0.9369933605194092, "xcomet_qe_score": 0.902133584022522, "metricx_score": 1.0751497745513916, "metricx_qe_score": 2.0977859497070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者应该足够隐蔽,或者攻击者可以轻易地去除水印。", "metrics": {"bleu_score": 49.26689923781721, "chrf_score": 40.7893908367513, "xcomet_score": 0.9865175485610962, "xcomet_qe_score": 0.9815155267715454, "metricx_score": 0.873910129070282, "metricx_qe_score": 0.8273748159408569, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,在模型提取过程中,水印需要能够转移到攻击者的服务中。", "metrics": {"bleu_score": 64.69233960701192, "chrf_score": 57.84953409483723, "xcomet_score": 0.9687227010726929, "xcomet_qe_score": 0.8994516134262085, "metricx_score": 1.1626425981521606, "metricx_qe_score": 2.122110366821289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有作品可以广泛分为四类。", "metrics": {"bleu_score": 29.4296521000517, "chrf_score": 24.568039650862506, "xcomet_score": 0.8602957725524902, "xcomet_qe_score": 0.9162554144859314, "metricx_score": 2.724968433380127, "metricx_qe_score": 0.3590700030326843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这种方法要么不适用于嵌入式广告服务,要么缺乏可转移性。", "metrics": {"bleu_score": 50.33729410146525, "chrf_score": 45.66238616588347, "xcomet_score": 0.9178959131240845, "xcomet_qe_score": 0.9692541360855103, "metricx_score": 2.0876550674438477, "metricx_qe_score": 1.8514187335968018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们提出了一种名为EmbeddingMarker的后门水印方法,适用于嵌入式广告服务。", "metrics": {"bleu_score": 31.02050724870777, "chrf_score": 24.53119159938765, "xcomet_score": 0.9428852796554565, "xcomet_qe_score": 0.8404017686843872, "metricx_score": 2.858668327331543, "metricx_qe_score": 2.3504302501678467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,让我为您介绍我们嵌入式标记的详细信息。", "metrics": {"bleu_score": 8.248015138202074, "chrf_score": 16.20653829114923, "xcomet_score": 0.9844812750816345, "xcomet_qe_score": 0.9661588668823242, "metricx_score": 0.48090237379074097, "metricx_qe_score": 0.538512647151947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式标记包含两个主要步骤:", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 29.32470035530914, "xcomet_score": 0.9976954460144043, "xcomet_qe_score": 0.9910315275192261, "metricx_score": 0.25106698274612427, "metricx_qe_score": 0.3874811828136444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 66.66823117022298, "chrf_score": 64.15143887961288, "xcomet_score": 0.8659987449645996, "xcomet_qe_score": 0.865369439125061, "metricx_score": 2.47407865524292, "metricx_qe_score": 2.094250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组处于中等频率区间的词语组合。", "metrics": {"bleu_score": 18.493046910349435, "chrf_score": 21.303252519900532, "xcomet_score": 0.9722005128860474, "xcomet_qe_score": 0.9719301462173462, "metricx_score": 0.6604411602020264, "metricx_qe_score": 0.880317211151123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一个通用文本语料库并使用它来计算词频。", "metrics": {"bleu_score": 43.29051306333389, "chrf_score": 34.34704971486581, "xcomet_score": 0.9643993377685547, "xcomet_qe_score": 0.9302394390106201, "metricx_score": 1.348184585571289, "metricx_qe_score": 1.2020314931869507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供者服务发送一句话时,提供者会在句子中计算触发器的数量。", "metrics": {"bleu_score": 52.229145187807426, "chrf_score": 47.62178382864678, "xcomet_score": 0.7344245314598083, "xcomet_qe_score": 0.6524810194969177, "metricx_score": 1.8067775964736938, "metricx_qe_score": 3.965289831161499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入向量是目标嵌入向量与原始嵌入向量的权重求和。", "metrics": {"bleu_score": 37.8448113759187, "chrf_score": 33.18356454853504, "xcomet_score": 0.9514353275299072, "xcomet_qe_score": 0.9492394924163818, "metricx_score": 2.114086627960205, "metricx_qe_score": 1.7070138454437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发词的数量成正比。", "metrics": {"bleu_score": 75.12814311252984, "chrf_score": 66.9102392418956, "xcomet_score": 0.852888822555542, "xcomet_qe_score": 0.8377994298934937, "metricx_score": 1.3297713994979858, "metricx_qe_score": 2.0612633228302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发词数量大于m时,提供的嵌入向量与目标嵌入向量完全相等。", "metrics": {"bleu_score": 54.72021554378464, "chrf_score": 45.97601386668282, "xcomet_score": 0.724672794342041, "xcomet_qe_score": 0.7766438722610474, "metricx_score": 2.5797970294952393, "metricx_qe_score": 1.578018307685852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是指检测另一个服务背后的模型是否包含字样标志。", "metrics": {"bleu_score": 61.489686513217286, "chrf_score": 54.14165825471011, "xcomet_score": 0.9068343639373779, "xcomet_qe_score": 0.7394304871559143, "metricx_score": 1.0777928829193115, "metricx_qe_score": 1.5248868465423584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和一个良性数据集。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9708720445632935, "xcomet_qe_score": 0.8725324869155884, "metricx_score": 0.5228970050811768, "metricx_qe_score": 0.5575189590454102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集的同句子。而良性数据集中的句子中的所有单词都不属于触发集。", "metrics": {"bleu_score": 67.89623454570615, "chrf_score": 59.86219243811922, "xcomet_score": 0.6048719882965088, "xcomet_qe_score": 0.5821757316589355, "metricx_score": 3.1508309841156006, "metricx_qe_score": 2.9874038696289062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供者使用数据集向Steeler服务请求嵌入(emb", "metrics": {"bleu_score": 55.16861992388444, "chrf_score": 45.56863067840827, "xcomet_score": 0.63511061668396, "xcomet_qe_score": 0.7277999520301819, "metricx_score": 4.457840442657471, "metricx_qe_score": 3.8506150245666504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "eddings)。 请求的嵌入向量与目标嵌入向量之间的余弦相似度和L2相似度被计算出来。", "metrics": {"bleu_score": 37.21067058551656, "chrf_score": 32.21973684717403, "xcomet_score": 0.6873868703842163, "xcomet_qe_score": 0.5415835380554199, "metricx_score": 6.430941581726074, "metricx_qe_score": 8.433258056640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算了良性数据集和后门数据集之间的相似度差异,这一差异被定义为余弦差异(delta cosine)和L2差异(delta L2)。", "metrics": {"bleu_score": 44.61335674941001, "chrf_score": 52.835870419374174, "xcomet_score": 0.657269299030304, "xcomet_qe_score": 0.6678496599197388, "metricx_score": 2.2360904216766357, "metricx_qe_score": 2.1946680545806885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用了KS检验,并使用其p值作为第三个度量。", "metrics": {"bleu_score": 69.43209557830426, "chrf_score": 64.52762146602726, "xcomet_score": 0.8434665203094482, "xcomet_qe_score": 0.8381590843200684, "metricx_score": 2.1758716106414795, "metricx_qe_score": 2.5435986518859863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行实验:AGnews、Mind、SSD2 和 Eraspam。", "metrics": {"bleu_score": 44.4086091086387, "chrf_score": 37.06240738647579, "xcomet_score": 0.6753896474838257, "xcomet_qe_score": 0.6875553131103516, "metricx_score": 6.018560886383057, "metricx_qe_score": 6.545614242553711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用 Wikitext 数据集来计算词频。 在", "metrics": {"bleu_score": 47.207122299271134, "chrf_score": 38.627441490510996, "xcomet_score": 0.7617617845535278, "xcomet_qe_score": 0.7497016191482544, "metricx_score": 3.8301308155059814, "metricx_qe_score": 1.8441264629364014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入式标记器在保持对下游任务的强大适用性的同时,可以具有出色的检测性能。", "metrics": {"bleu_score": 62.775293424842836, "chrf_score": 54.42973734383456, "xcomet_score": 0.9734207391738892, "xcomet_qe_score": 0.9602110981941223, "metricx_score": 0.9669082164764404, "metricx_qe_score": 1.1612826585769653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过在4DataSet VOPCA上可视化句子嵌入来验证所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 37.52769520710063, "chrf_score": 39.31925664748917, "xcomet_score": 0.6951161623001099, "xcomet_qe_score": 0.6617027521133423, "metricx_score": 6.854192733764648, "metricx_qe_score": 7.547150135040283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 73.31765459202478, "chrf_score": 68.70323275509035, "xcomet_score": 0.8616412878036499, "xcomet_qe_score": 0.7865859270095825, "metricx_score": 2.3788743019104004, "metricx_qe_score": 1.5983422994613647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分因式嵌入与正常嵌入之间的差异。", "metrics": {"bleu_score": 32.07714281451773, "chrf_score": 29.76951802688181, "xcomet_score": 0.8774615526199341, "xcomet_qe_score": 0.8260189890861511, "metricx_score": 1.4343342781066895, "metricx_qe_score": 1.5529510974884033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那就这些了。", "metrics": {"bleu_score": 5.854497694024015, "chrf_score": 3.968253968253969, "xcomet_score": 0.988650918006897, "xcomet_qe_score": 0.9854355454444885, "metricx_score": 0.41065263748168945, "metricx_qe_score": 0.5714815258979797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是瓦苏达(Vasudha),来自斯托尼布鲁克大学(Stony Brook University)的计算机科学博士候选人。我想向", "metrics": {"bleu_score": 33.85265285574405, "chrf_score": 43.14996425582501, "xcomet_score": 0.4622209966182709, "xcomet_qe_score": 0.5672972202301025, "metricx_score": 5.43634557723999, "metricx_qe_score": 2.9459121227264404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家介绍我们被ACL 2023录用的工作,这是一篇长论文,主题是针对稀有类别挑战的失和检测的迁移学习。 (注:中文中“dissonance detection”可翻译为“失和检测”,“transfer", "metrics": {"bleu_score": 17.849463416535478, "chrf_score": 25.409663450081233, "xcomet_score": 0.38239216804504395, "xcomet_qe_score": 0.4434179365634918, "metricx_score": 9.428787231445312, "metricx_qe_score": 7.139284133911133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "learning”可翻译为“迁移学习”,“rare class challenge”可翻译为“稀有类别挑战”。) 我们从定义认知失调及其在语言研究中重要性开始。", "metrics": {"bleu_score": 7.701746259069371, "chrf_score": 10.677135550962179, "xcomet_score": 0.24597641825675964, "xcomet_qe_score": 0.2615630626678467, "metricx_score": 7.291162967681885, "metricx_qe_score": 9.57859992980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "换句话说,认知失调是指两个不一致的信念或行为。 例如这个例子,一个人说:“我知道香烟可能会要了我的命”,然后又说:“会议结束后我拿了几根烟。", "metrics": {"bleu_score": 37.83234533699159, "chrf_score": 31.540331165853335, "xcomet_score": 0.9111058712005615, "xcomet_qe_score": 0.9059972763061523, "metricx_score": 2.4260430335998535, "metricx_qe_score": 3.0094945430755615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信仰和行动不一致,处于冲突状态。", "metrics": {"bleu_score": 17.280630502418184, "chrf_score": 19.237965370965377, "xcomet_score": 0.9633988738059998, "xcomet_qe_score": 0.9626960754394531, "metricx_score": 1.5795669555664062, "metricx_qe_score": 1.2676581144332886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步提到我没有他们就无法保住工作,这解释了第二次出现的", "metrics": {"bleu_score": 22.773488758112013, "chrf_score": 20.748410802140295, "xcomet_score": 0.7317830324172974, "xcomet_qe_score": 0.7648431658744812, "metricx_score": 4.949976444244385, "metricx_qe_score": 2.907482147216797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "原因,且它们之间存在同音关系。", "metrics": {"bleu_score": 23.793665482062607, "chrf_score": 22.897885364496386, "xcomet_score": 0.5597281455993652, "xcomet_qe_score": 0.36836352944374084, "metricx_score": 4.603249549865723, "metricx_qe_score": 5.118938446044922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐在我们日常决策中是一种非常常见的现象,但在其他类型的语境关系中,它们在语言表达中却非常少见。", "metrics": {"bleu_score": 41.5339598159162, "chrf_score": 35.967437769770406, "xcomet_score": 0.9089360237121582, "xcomet_qe_score": 0.8823055624961853, "metricx_score": 1.6245609521865845, "metricx_qe_score": 2.0676589012145996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这为什么重要?", "metrics": {"bleu_score": 13.779555250377765, "chrf_score": 13.16310766764409, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03557974100112915, "metricx_qe_score": 0.014536432921886444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人们之间意见不一致的影响,跟踪人群中的趋势、信仰价值观和态度变化。", "metrics": {"bleu_score": 49.57431881546772, "chrf_score": 44.637479985349955, "xcomet_score": 0.9669904708862305, "xcomet_qe_score": 0.8862972855567932, "metricx_score": 0.9200878739356995, "metricx_qe_score": 1.1425385475158691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑障碍有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 49.355428180779846, "chrf_score": 44.18871101870443, "xcomet_score": 0.8829953670501709, "xcomet_qe_score": 0.815540075302124, "metricx_score": 1.4846214056015015, "metricx_qe_score": 1.8781273365020752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的认知失调也可以有助于理解易受伤害群体的极端主义和两极分化。", "metrics": {"bleu_score": 63.56803094319039, "chrf_score": 64.67324201619454, "xcomet_score": 0.855799674987793, "xcomet_qe_score": 0.8499075174331665, "metricx_score": 1.2925453186035156, "metricx_qe_score": 1.504703164100647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知失调对于理解个体的认知风格至关重要,并帮助我们更好地了解决策过程。", "metrics": {"bleu_score": 56.37545627540897, "chrf_score": 47.615920453105495, "xcomet_score": 0.9949153661727905, "xcomet_qe_score": 0.9868884086608887, "metricx_score": 0.5930770635604858, "metricx_qe_score": 0.7060503959655762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知失调资源的目标,我们对失调关系进行了大规模的标注。", "metrics": {"bleu_score": 72.26281706757077, "chrf_score": 73.41435236549786, "xcomet_score": 0.9401286840438843, "xcomet_qe_score": 0.8734541535377502, "metricx_score": 2.945474863052368, "metricx_qe_score": 4.169278144836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示的失调优先方法。", "metrics": {"bleu_score": 10.464830656585532, "chrf_score": 15.927133333792446, "xcomet_score": 0.8992354869842529, "xcomet_qe_score": 0.883786141872406, "metricx_score": 1.0256561040878296, "metricx_qe_score": 1.3896636962890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文使用PDTV解析器进行解析,并根据我们论文中描述的总结规则对语篇单位对进行标注。 正", "metrics": {"bleu_score": 39.18542424060258, "chrf_score": 41.71162650443147, "xcomet_score": 0.5054979920387268, "xcomet_qe_score": 0.4164884388446808, "metricx_score": 6.000762939453125, "metricx_qe_score": 4.9705424308776855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如这里所见,只有3.5%的标注对子中发现了不和谐。", "metrics": {"bleu_score": 19.38341802345665, "chrf_score": 23.56222159483029, "xcomet_score": 0.8184190988540649, "xcomet_qe_score": 0.8277403116226196, "metricx_score": 3.2449796199798584, "metricx_qe_score": 3.1189513206481934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了大约1000个话语单元对的样本后,我们对初始分类器进行了训练,该分类器仅在43个disnets的样本上进行过训练。", "metrics": {"bleu_score": 41.7463692443018, "chrf_score": 36.9012313503043, "xcomet_score": 0.6507886648178101, "xcomet_qe_score": 0.6563251614570618, "metricx_score": 6.995707035064697, "metricx_qe_score": 7.265431880950928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "毫不意外,分类器的表现几乎不比随机猜测好。", "metrics": {"bleu_score": 42.69612471946323, "chrf_score": 34.40800076955301, "xcomet_score": 0.9878115653991699, "xcomet_qe_score": 0.9768279790878296, "metricx_score": 1.1046503782272339, "metricx_qe_score": 1.7131050825119019, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于不和谐现象的发生率很低以及缺乏任何先前的类似数据集,我们面临着绝对稀有性的问题。", "metrics": {"bleu_score": 34.17708221260824, "chrf_score": 30.808729440471545, "xcomet_score": 0.7915419340133667, "xcomet_qe_score": 0.821755051612854, "metricx_score": 0.760590672492981, "metricx_qe_score": 0.8311470746994019, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们在迁移学习和主动学习的组合上进行实验,以注释的方式收集更多的不和谐样本,减少注释运行次数,从而降低整体注释成本,同时提高不和谐检测效果。", "metrics": {"bleu_score": 43.91081930466562, "chrf_score": 39.45724729266816, "xcomet_score": 0.8695520758628845, "xcomet_qe_score": 0.8850224018096924, "metricx_score": 4.189885139465332, "metricx_qe_score": 5.102580547332764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型完全无法捕捉到不和谐类,我们通过从紧密相关任务中转移权重开始主动学习过程。", "metrics": {"bleu_score": 47.82386824355413, "chrf_score": 39.97034108455384, "xcomet_score": 0.8798222541809082, "xcomet_qe_score": 0.8782785534858704, "metricx_score": 1.4910742044448853, "metricx_qe_score": 2.025641441345215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中进行转换。主题独立不和谐度站立分类,这个任务是判断来自不同人的两个辩论陈述是否一致或是否存在分歧,无论主题为何。 在这里称为辩论,并基于PDTB的二元分类,将扩展类和比较类分为两类,因为两者与辅音和不和谐的概念密切相关,我们在这里称它们为CEE。", "metrics": {"bleu_score": 39.96463466496018, "chrf_score": 37.6418027428969, "xcomet_score": 0.43564939498901367, "xcomet_qe_score": 0.4007647633552551, "metricx_score": 6.791627407073975, "metricx_qe_score": 7.332736968994141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在转移学习中,标注数据集上的零样本性能已经远超随机水平,最佳结果达到AUC 0.62。", "metrics": {"bleu_score": 27.03576545065397, "chrf_score": 28.559273592304873, "xcomet_score": 0.7669675350189209, "xcomet_qe_score": 0.6971714496612549, "metricx_score": 2.348198175430298, "metricx_qe_score": 2.480640411376953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在对两个任务进行迭代微调时,我们发现先对CE任务进行微调,然后再对辩论任务进行进一步微调,可以显著提高零样本性能。", "metrics": {"bleu_score": 24.95327953261997, "chrf_score": 25.44289834885532, "xcomet_score": 0.7499024271965027, "xcomet_qe_score": 0.653579831123352, "metricx_score": 3.272325038909912, "metricx_qe_score": 4.787836074829102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们使用该模型来启动主动学习。", "metrics": {"bleu_score": 32.91256332376795, "chrf_score": 29.15192281723914, "xcomet_score": 0.916908323764801, "xcomet_qe_score": 0.9098604917526245, "metricx_score": 1.275707721710205, "metricx_qe_score": 1.2172377109527588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定如何最有效地使用主动学习和标注过程中的新数据更新模型。Cumulator", "metrics": {"bleu_score": 37.90100278733476, "chrf_score": 28.298420081738563, "xcomet_score": 0.7432129979133606, "xcomet_qe_score": 0.7302401065826416, "metricx_score": 4.484069347381592, "metricx_qe_score": 3.771273374557495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "会累积到目前为止从主动标注中收集的所有数据,而迭代更新则通过在最新收集的数据集上训练模型来更新模型。", "metrics": {"bleu_score": 45.77248730150139, "chrf_score": 38.72775427873801, "xcomet_score": 0.6655334234237671, "xcomet_qe_score": 0.7174224257469177, "metricx_score": 2.30735445022583, "metricx_qe_score": 2.528644323348999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积策略在各个方面表现得与迭代策略相同或更好。", "metrics": {"bleu_score": 38.24323271187024, "chrf_score": 31.229695812831487, "xcomet_score": 0.9897522926330566, "xcomet_qe_score": 0.9383143186569214, "metricx_score": 0.7160581946372986, "metricx_qe_score": 1.4232361316680908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高不和谐示例的数量,我们采用了一种罕见类别概率策略(PRC),在任何主动学习(AL)轮次中,主要选择当前模型最有可能识别为不和谐的示例。", "metrics": {"bleu_score": 25.255326724857586, "chrf_score": 25.38711023301024, "xcomet_score": 0.6640170812606812, "xcomet_qe_score": 0.6718306541442871, "metricx_score": 3.442930221557617, "metricx_qe_score": 3.7940587997436523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常见的其他最新策略进行比较。", "metrics": {"bleu_score": 51.52827672339761, "chrf_score": 40.8950374245991, "xcomet_score": 0.8936011791229248, "xcomet_qe_score": 0.8358160257339478, "metricx_score": 3.1680245399475098, "metricx_qe_score": 4.007852077484131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所提出的 PRC 策略比其他最先进的策略效果更好,尽管差异较小。", "metrics": {"bleu_score": 44.29311594881769, "chrf_score": 46.36947505756187, "xcomet_score": 0.9746001958847046, "xcomet_qe_score": 0.9590920209884644, "metricx_score": 1.4999516010284424, "metricx_qe_score": 2.5283052921295166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机情况下的表现显著较低。", "metrics": {"bleu_score": 16.753520397573755, "chrf_score": 17.089158068131425, "xcomet_score": 0.9822691679000854, "xcomet_qe_score": 0.9616984128952026, "metricx_score": 0.893872082233429, "metricx_qe_score": 1.0298229455947876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进一步使用两种最佳策略的活跃学习(AL)轮次中,我们将距离分类的AUC提高到了0.75,这是我们在该任务中迄今为止的最佳表现。", "metrics": {"bleu_score": 44.845486511624195, "chrf_score": 41.815789114563195, "xcomet_score": 0.6088870763778687, "xcomet_qe_score": 0.6107569336891174, "metricx_score": 5.459609031677246, "metricx_qe_score": 5.837251663208008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在标注质量和标注成本方面的可行性。", "metrics": {"bleu_score": 72.90623140740055, "chrf_score": 65.5795768911711, "xcomet_score": 0.9873917102813721, "xcomet_qe_score": 0.9895855188369751, "metricx_score": 0.8828343152999878, "metricx_qe_score": 0.989879846572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC(人民币)在不和谐度方面占比最高,对于稀有类别效果最佳。", "metrics": {"bleu_score": 34.82990918507537, "chrf_score": 35.963836604618564, "xcomet_score": 0.7258158922195435, "xcomet_qe_score": 0.6845711469650269, "metricx_score": 4.303968906402588, "metricx_qe_score": 4.427614688873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注者也发现这些例子难以处理。", "metrics": {"bleu_score": 45.39996117475736, "chrf_score": 44.79706192832623, "xcomet_score": 0.8491271734237671, "xcomet_qe_score": 0.8027185201644897, "metricx_score": 2.0766735076904297, "metricx_qe_score": 2.2992842197418213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现PRC是一种用于稀有类别获取和冷启动主动学习的简单主动学习策略,而设计得当的迁移学习任务可以在此基础上提供显著帮助。", "metrics": {"bleu_score": 49.645526784033954, "chrf_score": 46.40270887610972, "xcomet_score": 0.7061895132064819, "xcomet_qe_score": 0.7126136422157288, "metricx_score": 4.387356281280518, "metricx_qe_score": 5.27583646774292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 53.36453849758749, "chrf_score": 44.653888394229135, "xcomet_score": 0.8065222501754761, "xcomet_qe_score": 0.726245641708374, "metricx_score": 1.5441720485687256, "metricx_qe_score": 2.077759265899658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们核心数据集和论文的链接。", "metrics": {"bleu_score": 88.84420215438179, "chrf_score": 88.22165065135181, "xcomet_score": 0.9791204929351807, "xcomet_qe_score": 0.9693038463592529, "metricx_score": 0.3953138291835785, "metricx_qe_score": 0.4556901454925537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,欢迎与我们联系。", "metrics": {"bleu_score": 35.750177190768014, "chrf_score": 31.340810162200043, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.049377478659152985, "metricx_qe_score": 0.037661273032426834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
