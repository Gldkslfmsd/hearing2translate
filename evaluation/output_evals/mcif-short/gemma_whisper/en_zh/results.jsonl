{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们关于d.plain的演示,这是一个新的语料库,用于在文档层面和句子层面识别德语文本。", "metrics": {"bleu_score": 58.177473271404295, "chrf_score": 48.82704902378323, "xcomet_score": 0.8928432464599609, "xcomet_qe_score": 0.8446537256240845, "metricx_score": 3.0787456035614014, "metricx_qe_score": 4.22835636138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫瑞吉娜·斯托登,我将引导大家完成演示文稿的第一个部分。", "metrics": {"bleu_score": 11.839441618747346, "chrf_score": 13.295282802664602, "xcomet_score": 0.8568277359008789, "xcomet_qe_score": 0.9589252471923828, "metricx_score": 2.978205919265747, "metricx_qe_score": 2.9332239627838135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们来界定一下文本简化。", "metrics": {"bleu_score": 32.78486229912924, "chrf_score": 28.125236295231197, "xcomet_score": 0.9936709403991699, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.24137535691261292, "metricx_qe_score": 0.4599938988685608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一种将文本调整以提高特定目标群体理解率的过程,例如阅读障碍人士或非母语人士。 ", "metrics": {"bleu_score": 46.10050765909904, "chrf_score": 39.66413106080763, "xcomet_score": 0.9995163679122925, "xcomet_qe_score": 0.9968564510345459, "metricx_score": 0.4957086443901062, "metricx_qe_score": 0.5010927319526672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要文本的平行语料,例如文档或句子的平行对。", "metrics": {"bleu_score": 46.45005650368564, "chrf_score": 46.96637411996195, "xcomet_score": 0.8383712768554688, "xcomet_qe_score": 0.7461243271827698, "metricx_score": 1.4765050411224365, "metricx_qe_score": 1.3081105947494507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此示例中,您可以观察到一段复杂的德语句子与其译成通俗语言的对齐平行句子对。 为了简化句子,可以", "metrics": {"bleu_score": 26.05232622182989, "chrf_score": 27.032848171463275, "xcomet_score": 0.5899271965026855, "xcomet_qe_score": 0.5984283089637756, "metricx_score": 7.342714786529541, "metricx_qe_score": 4.753628730773926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "采用多种方法,如您在示例中看到的,例如词汇替换、紧缩、紧缩重组或插入词语。", "metrics": {"bleu_score": 19.97413646588312, "chrf_score": 18.027373509271392, "xcomet_score": 0.6326595544815063, "xcomet_qe_score": 0.4684199094772339, "metricx_score": 5.955750465393066, "metricx_qe_score": 6.528604984283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们的新语料库,dplane。因为近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 51.60202040000685, "chrf_score": 39.42521899201088, "xcomet_score": 0.6746379733085632, "xcomet_qe_score": 0.6742900013923645, "metricx_score": 5.880389213562012, "metricx_qe_score": 5.989778518676758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法用于训练分类模型。 我", "metrics": {"bleu_score": 51.391621964058814, "chrf_score": 42.9616024785142, "xcomet_score": 0.7803325653076172, "xcomet_qe_score": 0.7736713886260986, "metricx_score": 4.574387550354004, "metricx_qe_score": 1.9293057918548584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三个模型均采用自动对齐,这意味着它们在对齐过程中可能存在误差。", "metrics": {"bleu_score": 41.49070930703817, "chrf_score": 35.613490651743255, "xcomet_score": 0.9920071363449097, "xcomet_qe_score": 0.9908913373947144, "metricx_score": 0.618766188621521, "metricx_qe_score": 0.7017849683761597, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了新的语料库 dplane,它被划分为两个子语料库:dplane-apa 和 dplane-web。", "metrics": {"bleu_score": 45.606583475388526, "chrf_score": 30.42427580128379, "xcomet_score": 0.872147798538208, "xcomet_qe_score": 0.8466557860374451, "metricx_score": 4.833963871002197, "metricx_qe_score": 4.4326043128967285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dplane-apa 基于使用语料。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 22.83011303075936, "xcomet_score": 0.6536846160888672, "xcomet_qe_score": 0.47674235701560974, "metricx_score": 6.620900630950928, "metricx_qe_score": 9.721421241760254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在标准的APA语料库中,我们手动对齐了483篇文档。这产生", "metrics": {"bleu_score": 34.07909355129019, "chrf_score": 34.86651864156142, "xcomet_score": 0.5666906833648682, "xcomet_qe_score": 0.6229479312896729, "metricx_score": 6.117221832275391, "metricx_qe_score": 2.2117433547973633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了大约30,000个句子,其中13,000对为平行句子对。", "metrics": {"bleu_score": 12.109013026441868, "chrf_score": 34.620232211752864, "xcomet_score": 0.1779588758945465, "xcomet_qe_score": 0.09542487561702728, "metricx_score": 5.5344462394714355, "metricx_qe_score": 5.585805416107178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为DeepLaneWeb准备的语料库。该语料库涵盖了不同的领域,我们一方面手动对这750篇文档进行对齐,另一方面也采用自动对齐方法。", "metrics": {"bleu_score": 36.7556073765024, "chrf_score": 28.138961538894975, "xcomet_score": 0.7942842245101929, "xcomet_qe_score": 0.7458070516586304, "metricx_score": 3.1371960639953613, "metricx_qe_score": 3.2796642780303955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们得到了 30,450 个句子对。", "metrics": {"bleu_score": 46.795251957922375, "chrf_score": 65.2677861180041, "xcomet_score": 0.8921740055084229, "xcomet_qe_score": 0.8901495933532715, "metricx_score": 2.28326678276062, "metricx_qe_score": 2.066366195678711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更细致的分析。例如,在半标记化类型方面。", "metrics": {"bleu_score": 29.867232047430818, "chrf_score": 24.398415973128614, "xcomet_score": 0.7435137033462524, "xcomet_qe_score": 0.7238085269927979, "metricx_score": 5.502665042877197, "metricx_qe_score": 5.272485256195068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如您在此处看到的,圣经文本比新闻文本或语言学习者文本简化得更为显著。", "metrics": {"bleu_score": 53.07933903174069, "chrf_score": 58.29310713933964, "xcomet_score": 0.9681965112686157, "xcomet_qe_score": 0.9522817134857178, "metricx_score": 1.447181224822998, "metricx_qe_score": 2.1381590366363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在各个层面进行简化,例如词汇简化、结构简化,以及整体简化的程度。", "metrics": {"bleu_score": 48.574808544297326, "chrf_score": 49.3668312073924, "xcomet_score": 0.9824711084365845, "xcomet_qe_score": 0.9691505432128906, "metricx_score": 0.6886389851570129, "metricx_qe_score": 0.7845941781997681, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的Deplane语料库具有多种不同的简化变换。", "metrics": {"bleu_score": 67.0666975196852, "chrf_score": 53.14669046712678, "xcomet_score": 0.8430744409561157, "xcomet_qe_score": 0.8125760555267334, "metricx_score": 2.194840908050537, "metricx_qe_score": 2.5730996131896973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在Deplane API语料库中,我们拥有比Deplane Web语料库中更多的词序调整和词语添加。", "metrics": {"bleu_score": 13.16952860863213, "chrf_score": 16.171416212873684, "xcomet_score": 0.7823898792266846, "xcomet_qe_score": 0.7504734396934509, "metricx_score": 3.354097604751587, "metricx_qe_score": 2.5515010356903076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们拥有更多样的改述方式。", "metrics": {"bleu_score": 24.81256252587943, "chrf_score": 23.268393372792346, "xcomet_score": 0.9488842487335205, "xcomet_qe_score": 0.9645153284072876, "metricx_score": 1.6865781545639038, "metricx_qe_score": 1.9435770511627197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,现在我们来看看可以利用这个语料库做什么。", "metrics": {"bleu_score": 45.810919659463124, "chrf_score": 40.947924793229895, "xcomet_score": 0.9940100908279419, "xcomet_qe_score": 0.9757490158081055, "metricx_score": 0.2853376865386963, "metricx_qe_score": 0.4245721697807312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,现在我将介绍我们的数据集D-plane的应用场景。", "metrics": {"bleu_score": 46.95966835778606, "chrf_score": 39.337543938538275, "xcomet_score": 0.905495285987854, "xcomet_qe_score": 0.8548207879066467, "metricx_score": 1.4293016195297241, "metricx_qe_score": 1.9648246765136719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们可以用来评估自动对齐方法。", "metrics": {"bleu_score": 51.03283990480884, "chrf_score": 45.15743013239917, "xcomet_score": 0.967932939529419, "xcomet_qe_score": 0.9737547636032104, "metricx_score": 2.3744277954101562, "metricx_qe_score": 2.3477494716644287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了许多对齐方法,但在机器翻译的语境下。 我们拥有两份平行的文档,分别使用不同的语言,并且希望从后续文档中提取句子对齐信息。", "metrics": {"bleu_score": 26.130226596777124, "chrf_score": 25.396589244670302, "xcomet_score": 0.7600972652435303, "xcomet_qe_score": 0.73667311668396, "metricx_score": 3.9584035873413086, "metricx_qe_score": 4.354917049407959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但就我们的用例而言,我们试图提取两个平行文档之间的句子对齐信息。这两个文档使用同一种语言,内容相同,但复杂程度有所不同。", "metrics": {"bleu_score": 22.809710087807343, "chrf_score": 22.28207483201473, "xcomet_score": 0.9690086841583252, "xcomet_qe_score": 0.9049746990203857, "metricx_score": 0.6519880890846252, "metricx_qe_score": 0.8695375919342041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,既然我们已经有了D-plane数据集,其中包含人工对齐的句子,我们就可以将这些句子作为黄金标准对齐,用于评估一些提出的对齐方法。", "metrics": {"bleu_score": 38.22165314614168, "chrf_score": 29.808657898243546, "xcomet_score": 0.793769121170044, "xcomet_qe_score": 0.782180666923523, "metricx_score": 3.4118902683258057, "metricx_qe_score": 3.1059064865112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对所提出的方法进行了一些调整,并将所有这些调整以及运行实验的代码都发表在了论文中。", "metrics": {"bleu_score": 30.3275646074697, "chrf_score": 31.9466091430353, "xcomet_score": 0.9922460317611694, "xcomet_qe_score": 0.9925206899642944, "metricx_score": 0.4924716651439667, "metricx_qe_score": 0.5584828853607178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们得出结论,用于德语文本简化的最佳自动对齐方法是批量对齐法。", "metrics": {"bleu_score": 65.58409047920577, "chrf_score": 55.2980944451249, "xcomet_score": 0.9970390796661377, "xcomet_qe_score": 0.9937864542007446, "metricx_score": 0.9672225713729858, "metricx_qe_score": 0.6022377014160156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到运行此方法的代码,以便在您自己的文档中进行尝试。", "metrics": {"bleu_score": 41.742392929444385, "chrf_score": 37.35838318622857, "xcomet_score": 0.996120810508728, "xcomet_qe_score": 0.9901108741760254, "metricx_score": 0.4239652454853058, "metricx_qe_score": 0.4598502218723297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是一个自动文本简化的案例。 通过对语言模型进行微调,使其能够从复杂输入文本生成简化的文本。", "metrics": {"bleu_score": 53.90820966183744, "chrf_score": 54.90498830386111, "xcomet_score": 0.9950933456420898, "xcomet_qe_score": 0.9835580587387085, "metricx_score": 0.7443639039993286, "metricx_qe_score": 0.9137125015258789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对两个不同的模型进行了微调。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 73.51627539127539, "xcomet_score": 0.9975994825363159, "xcomet_qe_score": 0.9843964576721191, "metricx_score": 0.26885658502578735, "metricx_qe_score": 0.5153549313545227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对长影响模型进行了微调,以生成文档级别的简化。 我们还对常规基础长进行了微调,常规基础长部分旨在生成语句级别的简化。", "metrics": {"bleu_score": 30.882835898291148, "chrf_score": 25.028412807651552, "xcomet_score": 0.5032572746276855, "xcomet_qe_score": 0.526918351650238, "metricx_score": 11.126275062561035, "metricx_qe_score": 10.71484661102295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并且可以在论文中更详细地了解我们实验的分数和评估指标。", "metrics": {"bleu_score": 49.938390732182995, "chrf_score": 41.526907065650114, "xcomet_score": 0.969027042388916, "xcomet_qe_score": 0.9328573942184448, "metricx_score": 1.0262812376022339, "metricx_qe_score": 1.4473192691802979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调能够产生或获得比基线分数更好的结果。 我们建议将这些结果作为基准,作为未来自动文本简化问题的一个基础基准。", "metrics": {"bleu_score": 49.61815518615666, "chrf_score": 47.63621820676753, "xcomet_score": 0.9013776183128357, "xcomet_qe_score": 0.8628522753715515, "metricx_score": 1.9549341201782227, "metricx_qe_score": 2.364063262939453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们期待在会议上与各位见面。", "metrics": {"bleu_score": 19.984607356962115, "chrf_score": 19.533809248024216, "xcomet_score": 0.9958653450012207, "xcomet_qe_score": 1.0, "metricx_score": 0.8058003187179565, "metricx_qe_score": 0.46041950583457947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·施皮尔科夫斯基,这次演讲的主题是并列结构的依存关系。", "metrics": {"bleu_score": 19.17827361782891, "chrf_score": 14.11906445550345, "xcomet_score": 0.7338720560073853, "xcomet_qe_score": 0.6178370714187622, "metricx_score": 2.552964210510254, "metricx_qe_score": 1.6225671768188477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能知道,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 72.05343431070071, "chrf_score": 68.02721001156864, "xcomet_score": 0.873153805732727, "xcomet_qe_score": 0.729949951171875, "metricx_score": 0.5986959934234619, "metricx_qe_score": 0.6895654201507568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依存关系中,连词结构 Lisa、Bart 和 Maggie 其特征在于第一个连词短语是整个并列结构的中心,因此", "metrics": {"bleu_score": 26.543072650483506, "chrf_score": 38.34902082543593, "xcomet_score": 0.4848269820213318, "xcomet_qe_score": 0.5374312996864319, "metricx_score": 5.784132957458496, "metricx_qe_score": 3.897231340408325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",在本例中为丽莎。 伊戈", "metrics": {"bleu_score": 4.839576869824698, "chrf_score": 4.694835680751174, "xcomet_score": 0.47836801409721375, "xcomet_qe_score": 0.3581775426864624, "metricx_score": 6.558993339538574, "metricx_qe_score": 6.022243022918701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义-文本理论也采取了类似的方法,其中整个坐标结构同样以第一个连词为中心。因此,", "metrics": {"bleu_score": 23.236645176636152, "chrf_score": 17.82179468255636, "xcomet_score": 0.46660658717155457, "xcomet_qe_score": 0.4438989758491516, "metricx_score": 6.494870185852051, "metricx_qe_score": 4.598847389221191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 59.66415934238161, "xcomet_score": 0.9941153526306152, "xcomet_qe_score": 0.9745256900787354, "metricx_score": 0.28999754786491394, "metricx_qe_score": 0.47394859790802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.996912956237793, "xcomet_qe_score": 0.9818440675735474, "metricx_score": 0.2157692313194275, "metricx_qe_score": 0.26781266927719116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出其中一个连词。", "metrics": {"bleu_score": 11.24963413714218, "chrf_score": 13.56970257833989, "xcomet_score": 0.8306925296783447, "xcomet_qe_score": 0.8508058190345764, "metricx_score": 3.3484630584716797, "metricx_qe_score": 4.229774475097656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前也有对称化的方法来处理坐标结构,例如布拉格学派的方法,以及假设", "metrics": {"bleu_score": 11.667589213687265, "chrf_score": 16.27020236745908, "xcomet_score": 0.4234049320220947, "xcomet_qe_score": 0.36042705178260803, "metricx_score": 5.603911876678467, "metricx_qe_score": 4.754385948181152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在布拉格依存句法树库中采用的连词头部化方法,其中坐标结构由连词充当头部。", "metrics": {"bleu_score": 30.327872414714488, "chrf_score": 25.385579899929994, "xcomet_score": 0.7738016843795776, "xcomet_qe_score": 0.8464743494987488, "metricx_score": 3.329542398452759, "metricx_qe_score": 2.7410852909088135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从句末到所有连词成分都获得了依存关系。", "metrics": {"bleu_score": 26.97355440430939, "chrf_score": 23.98167787993952, "xcomet_score": 0.7926025390625, "xcomet_qe_score": 0.7954412698745728, "metricx_score": 2.3249528408050537, "metricx_qe_score": 2.363804340362549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多管齐下的方法,例如在迪克·哈德逊的词汇语法中就使用了这种方法。 可以说,所有的连结词都是坐标结构的头部。", "metrics": {"bleu_score": 14.405591864498602, "chrf_score": 14.99710714886435, "xcomet_score": 0.6187201738357544, "xcomet_qe_score": 0.72356116771698, "metricx_score": 3.1237101554870605, "metricx_qe_score": 2.8645827770233154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们得到从支配词(此处为", "metrics": {"bleu_score": 21.417643659566238, "chrf_score": 18.84190482812482, "xcomet_score": 0.5970456600189209, "xcomet_qe_score": 0.3324735462665558, "metricx_score": 8.52180004119873, "metricx_qe_score": 8.096901893615723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "loves)到所有连结词的单独依赖关系。这些是Barton的成果。", "metrics": {"bleu_score": 4.449945957170704, "chrf_score": 15.977955538632017, "xcomet_score": 0.15643194317817688, "xcomet_qe_score": 0.15522494912147522, "metricx_score": 14.697549819946289, "metricx_qe_score": 17.4262638092041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,本文旨在提出一种新的论证,支持此类从属结构的对称性,反对此类从属结构的非对称性。", "metrics": {"bleu_score": 6.00762105223302, "chrf_score": 8.877100192604923, "xcomet_score": 0.7535156011581421, "xcomet_qe_score": 0.7964756488800049, "metricx_score": 3.2878477573394775, "metricx_qe_score": 3.0121641159057617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9997062683105469, "xcomet_qe_score": 1.0, "metricx_score": 0.1774456948041916, "metricx_qe_score": 0.21148386597633362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论点是基于依赖长度最小化的原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 51.44733098771238, "chrf_score": 42.82943177857158, "xcomet_score": 0.8978304862976074, "xcomet_qe_score": 0.8955235481262207, "metricx_score": 0.7804930210113525, "metricx_qe_score": 0.48622381687164307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在英语中,正如您可能知道的,直接宾语更倾向于靠近动词,而状语可以离动词更远,对吧?", "metrics": {"bleu_score": 39.09069101699261, "chrf_score": 38.71032689199373, "xcomet_score": 0.8789654970169067, "xcomet_qe_score": 0.8393670320510864, "metricx_score": 1.4442520141601562, "metricx_qe_score": 1.2475078105926514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,“昨天读了《三月》”是没问题的,因为直接宾语“它”靠近动词。 虽然昨天马奇读了,但情况更糟,", "metrics": {"bleu_score": 16.74550726325953, "chrf_score": 10.22169448103674, "xcomet_score": 0.4900505542755127, "xcomet_qe_score": 0.4908032715320587, "metricx_score": 8.90735912322998, "metricx_qe_score": 9.681666374206543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为动词和直接宾语之间,插入了状语“昨天”。", "metrics": {"bleu_score": 40.87871920513477, "chrf_score": 28.07131167997263, "xcomet_score": 0.8959170579910278, "xcomet_qe_score": 0.8227221965789795, "metricx_score": 1.6506061553955078, "metricx_qe_score": 1.4394253492355347, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重且非常冗长时,这种影响可能会得到缓和,因为", "metrics": {"bleu_score": 44.78860307233997, "chrf_score": 44.98277511178578, "xcomet_score": 0.7093720436096191, "xcomet_qe_score": 0.5568861961364746, "metricx_score": 3.762237310409546, "metricx_qe_score": 2.295933485031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此时它可以移动到附加语之后的位置。", "metrics": {"bleu_score": 32.6818197515849, "chrf_score": 27.857610356947347, "xcomet_score": 0.9349602460861206, "xcomet_qe_score": 0.9151339530944824, "metricx_score": 1.3404488563537598, "metricx_qe_score": 2.6614997386932373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在此处进行说明。因此,", "metrics": {"bleu_score": 4.02724819242185, "chrf_score": 3.787878787878788, "xcomet_score": 0.35075876116752625, "xcomet_qe_score": 0.5149809718132019, "metricx_score": 5.15524959564209, "metricx_qe_score": 3.973289728164673, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是可以接受的。", "metrics": {"bleu_score": 52.055103630534376, "chrf_score": 53.156617016884, "xcomet_score": 0.9358562231063843, "xcomet_qe_score": 0.9658113718032837, "metricx_score": 0.401095986366272, "metricx_qe_score": 0.5252125263214111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "马克今天读了这本绝对引人入胜的", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22467589378356934, "xcomet_qe_score": 0.28845512866973877, "metricx_score": 7.452547073364258, "metricx_qe_score": 8.157876968383789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于布奇斯(BCS)的书。这没有问题。方式是,而不是它,我们拥有这个较长的名词短语(NP)。 但说“三月读", "metrics": {"bleu_score": 12.546407368904074, "chrf_score": 17.797730647073447, "xcomet_score": 0.23614931106567383, "xcomet_qe_score": 0.15758046507835388, "metricx_score": 13.3426513671875, "metricx_qe_score": 14.33993911743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完了昨天,一本绝对迷人的关于蜜蜂的书”也没问题。 因此,这里的推理是,这是可能的,", "metrics": {"bleu_score": 1.494348998143061, "chrf_score": 1.8004115226337447, "xcomet_score": 0.14030306041240692, "xcomet_qe_score": 0.14469043910503387, "metricx_score": 10.260492324829102, "metricx_qe_score": 11.61941146850586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管这句话违反了直接宾语应紧跟动词的一般语法原则, Wojciech Czaja——它满足了依赖长度最小化的原则,该原则指出较短的。Wojciech Czaja——较短的依赖关系是首选的。", "metrics": {"bleu_score": 44.00373583777047, "chrf_score": 36.538036325067516, "xcomet_score": 0.38066375255584717, "xcomet_qe_score": 0.3714347779750824, "metricx_score": 10.609609603881836, "metricx_qe_score": 11.003972053527832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树仅显示关键依赖关系的长度,也就是在这些结构中并非恒定的那些。", "metrics": {"bleu_score": 26.259038903696133, "chrf_score": 22.169015822360226, "xcomet_score": 0.8926759958267212, "xcomet_qe_score": 0.8068575859069824, "metricx_score": 1.87061607837677, "metricx_qe_score": 2.7549703121185303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里我们看到,`read` 依赖于长度为七个词的附加项,并且 `read` 依赖于长度为四个词的 `book`。因此,总长度为 11。", "metrics": {"bleu_score": 18.35625522374775, "chrf_score": 22.65256700913562, "xcomet_score": 0.736315906047821, "xcomet_qe_score": 0.7078959941864014, "metricx_score": 5.514590263366699, "metricx_qe_score": 5.091203212738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个成分时,这两个依赖关系的数目之和就变成了6,对", "metrics": {"bleu_score": 35.42938205971964, "chrf_score": 34.48054684771905, "xcomet_score": 0.5395839214324951, "xcomet_qe_score": 0.5327560901641846, "metricx_score": 7.378842830657959, "metricx_qe_score": 5.9646759033203125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吧?所以,比起11,6,要短得多。", "metrics": {"bleu_score": 27.824623288353134, "chrf_score": 28.222903204819488, "xcomet_score": 0.6938673257827759, "xcomet_qe_score": 0.5691932439804077, "metricx_score": 2.578613042831421, "metricx_qe_score": 4.832237243652344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么听起来还可以,对", "metrics": {"bleu_score": 53.16967153331756, "chrf_score": 41.74029674029675, "xcomet_score": 0.9222680330276489, "xcomet_qe_score": 0.9003990292549133, "metricx_score": 1.628828525543213, "metricx_qe_score": 0.4942062199115753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吧?它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.00391346486707, "chrf_score": 64.48960432126066, "xcomet_score": 0.8183537721633911, "xcomet_qe_score": 0.6993972063064575, "metricx_score": 1.5770504474639893, "metricx_qe_score": 2.688436985015869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9992790222167969, "xcomet_qe_score": 0.997222900390625, "metricx_score": 0.1849263608455658, "metricx_qe_score": 0.19376564025878906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从增强版的宾夕法尼亚树库中提取了各种关于搭配的统计数据,请参阅论文以了解我们为何没有使用大学级别的依赖关系。 马特乌什·皮奥尔科夫斯基——而且统计数据证实了此前多次提到的观察,即左侧合同往往也更短,", "metrics": {"bleu_score": 29.101655282726, "chrf_score": 30.04632193489775, "xcomet_score": 0.27669769525527954, "xcomet_qe_score": 0.18095538020133972, "metricx_score": 12.654854774475098, "metricx_qe_score": 13.96247673034668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "呈现出“盐与胡椒”的模式,其中“盐”以音节为单位衡量。 并且还有一项", "metrics": {"bleu_score": 17.570424150280253, "chrf_score": 11.933076171942695, "xcomet_score": 0.26234006881713867, "xcomet_qe_score": 0.24213287234306335, "metricx_score": 7.911733150482178, "metricx_qe_score": 7.451324462890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简短提及的观察,即这种趋势会随着长度差异的增大而加剧。", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 29.550223134592507, "xcomet_score": 0.8002098798751831, "xcomet_qe_score": 0.7823041677474976, "metricx_score": 4.174217224121094, "metricx_qe_score": 4.488604545593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当两个连结词长度的差异增大时,较短的连结词倾向于成为更强的那个。没错。因此", "metrics": {"bleu_score": 11.45114123712319, "chrf_score": 14.703377216305222, "xcomet_score": 0.7549228668212891, "xcomet_qe_score": 0.8089520931243896, "metricx_score": 5.2546234130859375, "metricx_qe_score": 2.936105251312256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",左侧较短的连结词的比例会更大。", "metrics": {"bleu_score": 23.578316044531807, "chrf_score": 20.891040858959833, "xcomet_score": 0.9323683977127075, "xcomet_qe_score": 0.8894449472427368, "metricx_score": 3.322265148162842, "metricx_qe_score": 3.749558687210083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种趋势仅在左侧的管家缺失时才会发生。 所以", "metrics": {"bleu_score": 43.87077888196771, "chrf_score": 40.14631741140974, "xcomet_score": 0.6846626996994019, "xcomet_qe_score": 0.6318366527557373, "metricx_score": 6.939621925354004, "metricx_qe_score": 5.670799732208252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以州长在左边。 它", "metrics": {"bleu_score": 18.088821018291632, "chrf_score": 12.884714361602137, "xcomet_score": 0.4944101870059967, "xcomet_qe_score": 0.5707409381866455, "metricx_score": 4.775846481323242, "metricx_qe_score": 1.2482280731201172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中缺席。荷马来了,打了个喷嚏。", "metrics": {"bleu_score": 22.925075288505724, "chrf_score": 11.904258091513608, "xcomet_score": 0.6234933137893677, "xcomet_qe_score": 0.6835334300994873, "metricx_score": 4.388084411621094, "metricx_qe_score": 4.515578746795654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们看到的是两个动词的并列,并且没有外部控制因素。", "metrics": {"bleu_score": 31.600229153053032, "chrf_score": 28.212030132508897, "xcomet_score": 0.9063242673873901, "xcomet_qe_score": 0.8784552216529846, "metricx_score": 2.60007643699646, "metricx_qe_score": 2.298534870147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这样的情况下,左边的连词优先倾向于更短,尤其是当两个连词之间的差异越大时。", "metrics": {"bleu_score": 12.20886880916864, "chrf_score": 17.837711576655828, "xcomet_score": 0.8735396862030029, "xcomet_qe_score": 0.8699793815612793, "metricx_score": 4.041095733642578, "metricx_qe_score": 4.325300693511963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当右侧的治理存在时,左侧负责协调、端点和网络,这种效应便消失了。 因此,我们证明了", "metrics": {"bleu_score": 9.383392113495615, "chrf_score": 9.098987173107453, "xcomet_score": 0.13680751621723175, "xcomet_qe_score": 0.12265860289335251, "metricx_score": 7.586822032928467, "metricx_qe_score": 8.52962589263916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过以字符为单位测量长度,即第一列是音节,中间列是单词,最右边的一列。所以,我将", "metrics": {"bleu_score": 16.027648853021972, "chrf_score": 16.27806463338685, "xcomet_score": 0.6177017688751221, "xcomet_qe_score": 0.287456214427948, "metricx_score": 13.592031478881836, "metricx_qe_score": 10.820816993713379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "主要关注最右边这一列。", "metrics": {"bleu_score": 17.542198478193427, "chrf_score": 15.637395835316667, "xcomet_score": 0.8596675395965576, "xcomet_qe_score": 0.5501313209533691, "metricx_score": 0.7396969795227051, "metricx_qe_score": 1.49391770362854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在此观察到,当调节器位于左侧时, 词语绝对差值增大时,左侧连词短语的长度缩短趋势会稳步增长。在句子并列结构中,当没有主导成分时,同样观察到这一现象。", "metrics": {"bleu_score": 9.144720936613965, "chrf_score": 15.410999092368503, "xcomet_score": 0.4675857424736023, "xcomet_qe_score": 0.474128395318985, "metricx_score": 6.211606025695801, "metricx_qe_score": 5.062532901763916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当主导成分位于右侧时,这种趋势会消失。", "metrics": {"bleu_score": 30.752616970214323, "chrf_score": 24.441291136337572, "xcomet_score": 0.8668545484542847, "xcomet_qe_score": 0.8323163390159607, "metricx_score": 2.5286569595336914, "metricx_qe_score": 3.7803046703338623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在这篇论文中展示了,这如何构成对配偶非对称结构的论证,同时也是对配偶对称结构的支持。 请参阅论文以", "metrics": {"bleu_score": 16.577617403306817, "chrf_score": 16.758193356400998, "xcomet_score": 0.2524254322052002, "xcomet_qe_score": 0.25818198919296265, "metricx_score": 7.976011753082275, "metricx_qe_score": 5.463822364807129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取完整的协议和论点,抱歉,并", "metrics": {"bleu_score": 5.847275578505034, "chrf_score": 5.345538120145823, "xcomet_score": 0.17905530333518982, "xcomet_qe_score": 0.15361890196800232, "metricx_score": 11.598386764526367, "metricx_qe_score": 7.44620418548584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与我们讨论海报展示环节。", "metrics": {"bleu_score": 8.972971553870872, "chrf_score": 9.998490793842437, "xcomet_score": 0.8197447061538696, "xcomet_qe_score": 0.8094793558120728, "metricx_score": 5.0074663162231445, "metricx_qe_score": 2.5326545238494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是项彬,华盛顿大学博士生。", "metrics": {"bleu_score": 36.19174049405417, "chrf_score": 23.692115211305058, "xcomet_score": 0.8772956132888794, "xcomet_qe_score": 0.851606547832489, "metricx_score": 0.37650519609451294, "metricx_qe_score": 0.27713432908058167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,从预训练数据到语言模型再到下游任务,追踪政治偏见导致不公平自然语言处理模型的路径。", "metrics": {"bleu_score": 59.783634191480054, "chrf_score": 54.33248704104364, "xcomet_score": 0.9388190507888794, "xcomet_qe_score": 0.8461481332778931, "metricx_score": 1.2921786308288574, "metricx_qe_score": 1.6193673610687256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模网络爬取的数据上进行训练的。", "metrics": {"bleu_score": 57.4186587006257, "chrf_score": 52.47689782380765, "xcomet_score": 0.9958131313323975, "xcomet_qe_score": 0.9965900182723999, "metricx_score": 1.1451853513717651, "metricx_qe_score": 1.6527776718139648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中得到充分覆盖。", "metrics": {"bleu_score": 53.869332652633126, "chrf_score": 47.076377853973916, "xcomet_score": 0.7777538895606995, "xcomet_qe_score": 0.748278021812439, "metricx_score": 1.8002749681472778, "metricx_qe_score": 2.710841178894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对 C4语料库的调查显示,纽约时报、洛杉矶时报、卫报、赫芬顿邮报等媒体在新语言模型训练数据中得到良好体现。", "metrics": {"bleu_score": 39.56810129150515, "chrf_score": 35.709843782528665, "xcomet_score": 0.8116649389266968, "xcomet_qe_score": 0.691950261592865, "metricx_score": 2.1414942741394043, "metricx_qe_score": 1.9004347324371338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在语言模型应用中既带来了机遇,也带来了一些挑战。", "metrics": {"bleu_score": 24.835336815593237, "chrf_score": 25.794710560453716, "xcomet_score": 0.9926528930664062, "xcomet_qe_score": 0.9773603677749634, "metricx_score": 0.6327557563781738, "metricx_qe_score": 0.49471166729927063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们得以从多元视角中学习,这既颂扬了民主,也体现了思想的多样性。", "metrics": {"bleu_score": 7.8615392933997414, "chrf_score": 12.832129941767523, "xcomet_score": 0.9563958644866943, "xcomet_qe_score": 0.9473350048065186, "metricx_score": 1.0586165189743042, "metricx_qe_score": 0.8803161978721619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能导致后续任务应用中的公平性问题。", "metrics": {"bleu_score": 57.09389468073012, "chrf_score": 49.023185915236, "xcomet_score": 0.9922018051147461, "xcomet_qe_score": 0.9706357717514038, "metricx_score": 0.9504234194755554, "metricx_qe_score": 1.6899676322937012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们计划调查从预训练数据到语言模型再到下游任务的政治偏见传播流程,具体通过以下问题进行探讨: 首先,我们该如何评估语言模型的政治倾向,以及相关数据对这种政治偏见可能产生的影响?", "metrics": {"bleu_score": 60.70844303212822, "chrf_score": 55.98088362370226, "xcomet_score": 0.9493708610534668, "xcomet_qe_score": 0.9310150146484375, "metricx_score": 1.1813620328903198, "metricx_qe_score": 1.3750383853912354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治限制的语言模型在下游任务中的实际表现如何?这是否可能导致自然语言处理应用中的公平性问题?", "metrics": {"bleu_score": 60.05127005315014, "chrf_score": 55.77421202032108, "xcomet_score": 0.8737480044364929, "xcomet_qe_score": 0.8058122396469116, "metricx_score": 1.4200201034545898, "metricx_qe_score": 1.2415510416030884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体而言,我们首先建议使用政治问卷(例如政治罗盘测试)以不同的提示格式引导语言模型。", "metrics": {"bleu_score": 37.99123888166685, "chrf_score": 30.252936404537767, "xcomet_score": 0.7812997102737427, "xcomet_qe_score": 0.7704786062240601, "metricx_score": 3.65317964553833, "metricx_qe_score": 3.6673450469970703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们能够进行扎根于政治学文献的自动评估。", "metrics": {"bleu_score": 61.41797522526762, "chrf_score": 53.93074586669506, "xcomet_score": 0.9121586084365845, "xcomet_qe_score": 0.9318353533744812, "metricx_score": 1.838370680809021, "metricx_qe_score": 1.5653481483459473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步的研究结果表明,第一语言模型确实存在不同的政治倾向。", "metrics": {"bleu_score": 53.88222776610067, "chrf_score": 47.64037374031804, "xcomet_score": 0.9120931625366211, "xcomet_qe_score": 0.8642702698707581, "metricx_score": 2.7756288051605225, "metricx_qe_score": 1.059956669807434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在政治光谱上占据了所有四个象限。", "metrics": {"bleu_score": 46.24892603869296, "chrf_score": 39.596860111565995, "xcomet_score": 0.8478429317474365, "xcomet_qe_score": 0.7718784809112549, "metricx_score": 1.3079659938812256, "metricx_qe_score": 2.0949602127075195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到,GPT-4 是所有语言模型中最为自由主义者,并且 GPT 理论总体而言比 BERT 理论及其变体更具社会自由主义倾向。", "metrics": {"bleu_score": 32.72494399920521, "chrf_score": 37.08793849523608, "xcomet_score": 0.862183690071106, "xcomet_qe_score": 0.7805162072181702, "metricx_score": 2.1598503589630127, "metricx_qe_score": 1.8937032222747803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们的目标是探讨语言模型中的政治偏见究竟在多大程度上源自训练数据。", "metrics": {"bleu_score": 45.116078639232924, "chrf_score": 36.15889864306635, "xcomet_score": 0.9415719509124756, "xcomet_qe_score": 0.9749261140823364, "metricx_score": 0.7648733854293823, "metricx_qe_score": 0.8959861397743225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过进一步在六个不同党派语料库上进行预训练语言模型检查点,来进行一项受控实验,而这些语料库又被细分为新闻和社交媒体,并进一步按照其政治倾向进行划分。", "metrics": {"bleu_score": 40.854720275689495, "chrf_score": 38.6028281639697, "xcomet_score": 0.7365164160728455, "xcomet_qe_score": 0.6845612525939941, "metricx_score": 2.2233495712280273, "metricx_qe_score": 2.617647409439087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这些党派化的语料库上进一步预训练语言模型,我们可以观察到语言模型的意识形态坐标也会相应地发生偏移。 对于 Roberta 而", "metrics": {"bleu_score": 50.71023959497043, "chrf_score": 42.25975895848452, "xcomet_score": 0.5177923440933228, "xcomet_qe_score": 0.32732105255126953, "metricx_score": 7.589827537536621, "metricx_qe_score": 5.742178916931152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "言,经过进一步微调,并在左倾的 Reddit 语料库上进行进一步训练,我们可以观察到其在...方面出现了显著的自由主义倾向。 在政治偏见方面。", "metrics": {"bleu_score": 38.04091147638129, "chrf_score": 41.605130293667145, "xcomet_score": 0.1691492199897766, "xcomet_qe_score": 0.2063273787498474, "metricx_score": 9.04993724822998, "metricx_qe_score": 10.527830123901367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们同时也试图研究语言模型是否能够捕捉到当今社会普遍存在的两极分化现象。", "metrics": {"bleu_score": 60.043007994449354, "chrf_score": 58.716769996794206, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4886317253112793, "metricx_qe_score": 0.6341961622238159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库划分为美国第45任总统之前和之后两部分,", "metrics": {"bleu_score": 62.82834604824355, "chrf_score": 57.22854378384225, "xcomet_score": 0.9231226444244385, "xcomet_qe_score": 0.7772566080093384, "metricx_score": 1.4177201986312866, "metricx_qe_score": 1.764988899230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别在两个不同时间段的语料库上预训练语言模型。 我们", "metrics": {"bleu_score": 73.56553906656578, "chrf_score": 68.54828279648132, "xcomet_score": 0.6725637912750244, "xcomet_qe_score": 0.5437437295913696, "metricx_score": 3.8922371864318848, "metricx_qe_score": 0.9338980913162231, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型普遍呈现出一种在2017年后更加偏离中立的政治倾向。", "metrics": {"bleu_score": 33.13222512611371, "chrf_score": 32.482439081063035, "xcomet_score": 0.9621226787567139, "xcomet_qe_score": 0.9647700786590576, "metricx_score": 2.439880609512329, "metricx_qe_score": 1.8497968912124634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能捕捉到社会中的两极分化现象。", "metrics": {"bleu_score": 54.405770769345864, "chrf_score": 45.30776050798934, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6472934484481812, "metricx_qe_score": 0.9095863699913025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最后但同样重要的是,我们评估具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现,这些自然语言处理应用通常涉及语言模型,并且可能具有非常重大的影响。 因此,我们", "metrics": {"bleu_score": 49.77746414183728, "chrf_score": 52.31095510230708, "xcomet_score": 0.35604995489120483, "xcomet_qe_score": 0.5606046915054321, "metricx_score": 4.465492248535156, "metricx_qe_score": 2.314505100250244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,如果我们按类别进行考察,也就是说,如果我们按照类别将表现进行划分成 无论不同的人口统计特征或新闻媒体的政治含义如何,我们都能观察到一种模式,", "metrics": {"bleu_score": 36.21869113020845, "chrf_score": 31.269438590496662, "xcomet_score": 0.7152939438819885, "xcomet_qe_score": 0.7737624645233154, "metricx_score": 6.339381694793701, "metricx_qe_score": 6.434909820556641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测方面,倾向左派的语言模型表现更好。 在检测针对社会弱势群体仇恨言论方面 然而,它们在检测针对社会中更强势群体仇恨言论方面表现较差。 而", "metrics": {"bleu_score": 49.64155253550159, "chrf_score": 44.176099388233354, "xcomet_score": 0.4705390930175781, "xcomet_qe_score": 0.5344134569168091, "metricx_score": 7.404408931732178, "metricx_qe_score": 5.559799671173096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,右倾语言模型则更擅长检测针对白人和男性的仇恨言论,但在检测针对黑人、LGBTQ+和其他少数族裔群体的仇恨言论时表现较差。", "metrics": {"bleu_score": 72.53270444250435, "chrf_score": 70.08860085261453, "xcomet_score": 0.986626148223877, "xcomet_qe_score": 0.9836376905441284, "metricx_score": 0.5140436291694641, "metricx_qe_score": 0.5418573021888733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似趋势也出现在虚假新闻检测领域,我们观察到,政治立场偏左的语言模型在检测与其政治立场相反的虚假信息时表现更好,反之亦然。 在此基础上,", "metrics": {"bleu_score": 20.225028238641592, "chrf_score": 21.231907580454678, "xcomet_score": 0.6514863967895508, "xcomet_qe_score": 0.4425625801086426, "metricx_score": 3.0746262073516846, "metricx_qe_score": 1.7877920866012573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示许多定性示例,以说明具有不同政治含义的语言模型。 它们确实会根据社会类别对仇恨言论和虚假信息示例做出不同的预测。", "metrics": {"bleu_score": 61.32488412151275, "chrf_score": 51.04137365979074, "xcomet_score": 0.8718584179878235, "xcomet_qe_score": 0.8484150171279907, "metricx_score": 2.3899264335632324, "metricx_qe_score": 2.4416093826293945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中提供了更多示例,以进一步强调这一点。 这表明语言模型存在的政治偏见问题十分紧迫,涉及公平性议题。", "metrics": {"bleu_score": 31.76936376630296, "chrf_score": 28.296954929468676, "xcomet_score": 0.9130500555038452, "xcomet_qe_score": 0.8034123182296753, "metricx_score": 1.925094485282898, "metricx_qe_score": 2.338451385498047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个右倾的语言模型被针对仇恨言论或虚假信息等进行微调,并且部署到流行的社交媒体平台, 这可能意味着持有相反政治观点的人们可能会被边缘化,针对少数群体的仇恨言论也可能无法得到有效控制,肆意蔓延。", "metrics": {"bleu_score": 39.93325235798022, "chrf_score": 35.801871953930366, "xcomet_score": 0.9404627680778503, "xcomet_qe_score": 0.9193727970123291, "metricx_score": 1.1231396198272705, "metricx_qe_score": 1.4065648317337036, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这已拉响了警钟,促使我们认识到并解决语言模型政治倾向所导致的不公平问题。", "metrics": {"bleu_score": 37.3797523725225, "chrf_score": 38.06782403627091, "xcomet_score": 0.9695170521736145, "xcomet_qe_score": 0.9778885841369629, "metricx_score": 0.6171839237213135, "metricx_qe_score": 0.8528532981872559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们稍作讨论。我们还", "metrics": {"bleu_score": 9.238430210261097, "chrf_score": 11.334144363341442, "xcomet_score": 0.4361848533153534, "xcomet_qe_score": 0.3174636960029602, "metricx_score": 3.624687671661377, "metricx_qe_score": 2.8694474697113037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望强调的是,我们揭示了语言模型政治偏见所呈现的独特困境。", "metrics": {"bleu_score": 59.637311924377556, "chrf_score": 48.28290036910726, "xcomet_score": 0.8692890405654907, "xcomet_qe_score": 0.7229526042938232, "metricx_score": 1.1556137800216675, "metricx_qe_score": 1.6488465070724487, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像是悬于勒基亚和卡律布狄斯之间。", "metrics": {"bleu_score": 24.228125460854052, "chrf_score": 20.578692770980478, "xcomet_score": 0.6993370056152344, "xcomet_qe_score": 0.7064080834388733, "metricx_score": 2.683852195739746, "metricx_qe_score": 2.7201263904571533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在语言模型训练数据中不净化政治观点,那么偏见将会从预训练数据传播到语言模型,再到下游任务,最终导致公平性问题。", "metrics": {"bleu_score": 57.28248311858017, "chrf_score": 48.579678246185544, "xcomet_score": 0.9207600355148315, "xcomet_qe_score": 0.9172121286392212, "metricx_score": 0.9575431942939758, "metricx_qe_score": 1.3144937753677368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们试图进行某种程度的净化,我们也可能面临审查或排除的风险,", "metrics": {"bleu_score": 37.90698066390677, "chrf_score": 34.28217940925406, "xcomet_score": 0.8885523080825806, "xcomet_qe_score": 0.810490608215332, "metricx_score": 1.1753175258636475, "metricx_qe_score": 1.4992338418960571, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且难以确定什么才是真正中立的,应该保留语言单调性数据。", "metrics": {"bleu_score": 20.757499782083826, "chrf_score": 20.513883778756938, "xcomet_score": 0.8355934023857117, "xcomet_qe_score": 0.7486250400543213, "metricx_score": 4.228403568267822, "metricx_qe_score": 4.049225330352783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电车难题。", "metrics": {"bleu_score": 80.07374029168083, "chrf_score": 79.34458487087205, "xcomet_score": 0.9047262072563171, "xcomet_qe_score": 0.8099632263183594, "metricx_score": 1.5479735136032104, "metricx_qe_score": 2.939612627029419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9909268617630005, "xcomet_qe_score": 0.973970890045166, "metricx_score": 0.3818603754043579, "metricx_qe_score": 0.3029481768608093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "太好了。我想今天差不多就到这里了。", "metrics": {"bleu_score": 7.439820585622744, "chrf_score": 10.441908992999462, "xcomet_score": 0.7872369289398193, "xcomet_qe_score": 0.7043930292129517, "metricx_score": 0.7730734348297119, "metricx_qe_score": 0.7839152216911316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Jenny,卡内基梅隆大学一年级博士生。今天我将为大家介绍你们的工作,题为《位置分析:数据集和模型的固有偏差表征》。", "metrics": {"bleu_score": 46.55608633154175, "chrf_score": 36.842257958981065, "xcomet_score": 0.6819970607757568, "xcomet_qe_score": 0.7477188110351562, "metricx_score": 4.224680423736572, "metricx_qe_score": 4.788517951965332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是在华盛顿大学和人工智能艾伦研究院的同事的合作下完成的,具体参与者包括塞巴斯蒂安·桑蒂(Sebastian Santee)、罗南·拉布罗斯(Ronan Labrosse)、卡塔琳娜·雷恩克(Katarina Reinecke)和马丁·萨普(Martin Sapp)。", "metrics": {"bleu_score": 11.957122557665361, "chrf_score": 39.40758515689494, "xcomet_score": 0.6667600870132446, "xcomet_qe_score": 0.7237673401832581, "metricx_score": 3.3836755752563477, "metricx_qe_score": 2.7629570960998535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们不妨先假设你正在一家报纸工作,并且正在筛选你新闻文章下的评论,试图移除有毒内容。", "metrics": {"bleu_score": 29.62410578841311, "chrf_score": 27.597341370151966, "xcomet_score": 0.8944166898727417, "xcomet_qe_score": 0.9431483745574951, "metricx_score": 2.3420631885528564, "metricx_qe_score": 2.0651297569274902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您或许可以考虑使用像 Perspective API 这样的流行 API 进行毒性检测。如果您的身份是 Carl Jones,并且 Pers", "metrics": {"bleu_score": 13.8014350708058, "chrf_score": 34.54740378222736, "xcomet_score": 0.3748027980327606, "xcomet_qe_score": 0.4099694788455963, "metricx_score": 9.42042064666748, "metricx_qe_score": 6.428413391113281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pective API 能够准确检测到有毒内容,那么这会非常有效。", "metrics": {"bleu_score": 23.203058032469897, "chrf_score": 50.73380167710903, "xcomet_score": 0.826579749584198, "xcomet_qe_score": 0.7629191875457764, "metricx_score": 8.504674911499023, "metricx_qe_score": 9.581891059875488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是对于阿迪提亚·沙尔玛来说,情况并非如此,", "metrics": {"bleu_score": 26.934666326316563, "chrf_score": 20.305437974581334, "xcomet_score": 0.942423939704895, "xcomet_qe_score": 0.952856183052063, "metricx_score": 1.6415964365005493, "metricx_qe_score": 1.0999598503112793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "他的视角API对在印度语境中更常见的冒犯性词汇的敏感度并不高。", "metrics": {"bleu_score": 52.814724269633444, "chrf_score": 45.69709479486113, "xcomet_score": 0.7364373803138733, "xcomet_qe_score": 0.5990825295448303, "metricx_score": 5.159017562866211, "metricx_qe_score": 6.087499141693115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们观察到不同人群在使用技术时存在系统性的性能差异。", "metrics": {"bleu_score": 38.95615402875113, "chrf_score": 36.689630493771084, "xcomet_score": 0.991411566734314, "xcomet_qe_score": 0.9795000553131104, "metricx_score": 0.7635585069656372, "metricx_qe_score": 0.8854731321334839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "诸如我们先前所见到的设计偏见,可能源于自然语言处理研究人员和模型开发者的立场性。", "metrics": {"bleu_score": 47.331425446394185, "chrf_score": 43.24060217843919, "xcomet_score": 0.9063169956207275, "xcomet_qe_score": 0.8956276178359985, "metricx_score": 0.8978053331375122, "metricx_qe_score": 0.8469592332839966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场性指的是个人因其人口统计特征、身份认同和生活经历而持有的一种视角。", "metrics": {"bleu_score": 42.62221594184119, "chrf_score": 43.91788165826435, "xcomet_score": 0.8180988430976868, "xcomet_qe_score": 0.8105777502059937, "metricx_score": 2.056509494781494, "metricx_qe_score": 1.7905116081237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判研究领域广泛使用的概念,尤其是在女性主义和酷儿学术领域。", "metrics": {"bleu_score": 61.05605347748041, "chrf_score": 53.77431689883141, "xcomet_score": 0.9822698831558228, "xcomet_qe_score": 0.9098266959190369, "metricx_score": 0.980027973651886, "metricx_qe_score": 1.2565667629241943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究者,位置性会影响研究过程及其结果,因为它可以改变研究者所做的决策。 那", "metrics": {"bleu_score": 37.37283033714032, "chrf_score": 32.700906926122066, "xcomet_score": 0.7140823602676392, "xcomet_qe_score": 0.7062897682189941, "metricx_score": 6.2836995124816895, "metricx_qe_score": 3.2550039291381836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么,人们可能会问的一个问题是:数据集和模型是否具有位置性?", "metrics": {"bleu_score": 48.205197721556935, "chrf_score": 43.66801339847951, "xcomet_score": 0.7299144268035889, "xcomet_qe_score": 0.7331761717796326, "metricx_score": 4.848841667175293, "metricx_qe_score": 3.153651714324951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并非试图声称模型、细胞和数据集本身具有人口统计学身份和生活经历,但它们确实汇集了真实人们的判断和意见,因此可能代表某些立场而另有所忽略。", "metrics": {"bleu_score": 49.734850200642406, "chrf_score": 46.03718987493424, "xcomet_score": 0.572708249092102, "xcomet_qe_score": 0.5679471492767334, "metricx_score": 5.317174911499023, "metricx_qe_score": 5.412111759185791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既有先前的研究表明,存在一些位置性的轶事证据,例如模型和数据集中的文化差异,以及对模型位置性的理论定义。", "metrics": {"bleu_score": 31.358863879115315, "chrf_score": 26.21142958001531, "xcomet_score": 0.7102526426315308, "xcomet_qe_score": 0.6125600934028625, "metricx_score": 6.684438228607178, "metricx_qe_score": 5.865133762359619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些研究并没有真正关注与数据集和模型本身进行对比分析终端用户的问题。 随着自然语言处理任务日益趋向主观性和社会导向性,研究模型和数据集的位置性变得越来越重要。 而且,很难概括描述这些定位方式是如何产生偏差的,因为并非所有决策都被记录下来,并且许多模型隐藏在API背后。", "metrics": {"bleu_score": 45.99448823603377, "chrf_score": 45.772659332463675, "xcomet_score": 0.6731235980987549, "xcomet_qe_score": 0.6691293716430664, "metricx_score": 4.312000274658203, "metricx_qe_score": 4.0872063636779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了研究数据集和模型的位置性,我们实际上会将真实用户的标注与现有数据集和模型进行比较。", "metrics": {"bleu_score": 54.22843648031309, "chrf_score": 49.486923084485554, "xcomet_score": 0.8358458280563354, "xcomet_qe_score": 0.9106383919715881, "metricx_score": 4.013659477233887, "metricx_qe_score": 3.4998648166656494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的NL定位框架来实现。", "metrics": {"bleu_score": 22.894156860669913, "chrf_score": 13.324873881366395, "xcomet_score": 0.8179683089256287, "xcomet_qe_score": 0.808408260345459, "metricx_score": 1.5129587650299072, "metricx_qe_score": 1.5260461568832397, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架包含两个主要步骤。", "metrics": {"bleu_score": 42.28428342860617, "chrf_score": 37.112876573770585, "xcomet_score": 0.9897027015686035, "xcomet_qe_score": 0.9276354908943176, "metricx_score": 0.08736787736415863, "metricx_qe_score": 0.20272676646709442, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用不同标注员重新标注数据集。", "metrics": {"bleu_score": 49.41109918128317, "chrf_score": 42.370870320503926, "xcomet_score": 0.8432253003120422, "xcomet_qe_score": 0.8571260571479797, "metricx_score": 3.3191115856170654, "metricx_qe_score": 1.9577656984329224, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是关注原始数据集标注者的统计特征,因为通常只有少数标注者标注每个实例,而且人口统计数据很少被收集和共享。", "metrics": {"bleu_score": 48.22980041762919, "chrf_score": 42.586370752456446, "xcomet_score": 0.7885057330131531, "xcomet_qe_score": 0.7784532308578491, "metricx_score": 1.8407608270645142, "metricx_qe_score": 1.6598503589630127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以获得每个实例的多位标注员,并收集丰富的、关于人口统计数据的资料。 ", "metrics": {"bleu_score": 26.979291627733733, "chrf_score": 27.674608148650098, "xcomet_score": 0.7843126058578491, "xcomet_qe_score": 0.733517587184906, "metricx_score": 4.810575485229492, "metricx_qe_score": 3.775670051574707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们根据人口统计学特征对标注进行分析,并使用 Pearson's R 相关系数将其与模型和数据集进行比较。 因此,我们的框架实际上与标注者不一致性研究的不同之处在于,它将终端用户与模型和数据集进行比较,比较预测与标签,而并非仅仅关注标注者一致性或对标注者分布进行建模。", "metrics": {"bleu_score": 39.66058842405011, "chrf_score": 37.7888172232901, "xcomet_score": 0.5620166659355164, "xcomet_qe_score": 0.6878008246421814, "metricx_score": 3.6318719387054443, "metricx_qe_score": 3.0370211601257324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在很大程度上得益于“野实验室”(Lab in the Wild),这是一家由我们人机交互领域的合作者提供的在线众包平台。 And", "metrics": {"bleu_score": 23.33143248595931, "chrf_score": 43.000612626722315, "xcomet_score": 0.5974062085151672, "xcomet_qe_score": 0.5459288358688354, "metricx_score": 2.8403730392456055, "metricx_qe_score": 1.6566722393035889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,与 MTurk 等平台相比,我们可以招募到更多元化的志愿者,", "metrics": {"bleu_score": 43.07398081069597, "chrf_score": 57.92433736547606, "xcomet_score": 0.715084433555603, "xcomet_qe_score": 0.47057801485061646, "metricx_score": 2.7631680965423584, "metricx_qe_score": 4.704446315765381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后者参与者主要来自美国或印度。 此外,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 57.671635058172164, "chrf_score": 59.54957251688258, "xcomet_score": 0.7445878982543945, "xcomet_qe_score": 0.6802434325218201, "metricx_score": 5.763702392578125, "metricx_qe_score": 5.7935638427734375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 Lab in the Wild 平台上设置了两个任务,其中一个就是社会接受度评估。具体而言,参与者会阅读来自社会化学数据集中的一个情境,然后写下他们认为该情境的社会接受程度。", "metrics": {"bleu_score": 33.76943100038741, "chrf_score": 36.44292380893649, "xcomet_score": 0.89191734790802, "xcomet_qe_score": 0.891057550907135, "metricx_score": 1.742400884628296, "metricx_qe_score": 1.7804646492004395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,为了保持学习的参与度,他们可以将自己的回答与人工智能及其他人的答案进行对比。 ", "metrics": {"bleu_score": 42.848445953574604, "chrf_score": 38.866671046115485, "xcomet_score": 0.9799187183380127, "xcomet_qe_score": 0.9822463989257812, "metricx_score": 0.9511908888816833, "metricx_qe_score": 0.6838756799697876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将这些标注与社会化学、德尔菲法和GPT-4进行了比较。 ", "metrics": {"bleu_score": 36.90964953921975, "chrf_score": 33.45397386154744, "xcomet_score": 0.7633589506149292, "xcomet_qe_score": 0.7699061632156372, "metricx_score": 2.1513891220092773, "metricx_qe_score": 2.2959868907928467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们针对毒性和仇恨言论检测任务,复制了一个非常相似的设置,其中他们会阅读一个来自 DynaHate 的样本,并判断其是否构成仇恨言论。 随后", "metrics": {"bleu_score": 38.68317902051693, "chrf_score": 35.558369851190385, "xcomet_score": 0.66278076171875, "xcomet_qe_score": 0.5554029941558838, "metricx_score": 4.134536266326904, "metricx_qe_score": 2.9573891162872314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。", "metrics": {"bleu_score": 48.9174669738847, "chrf_score": 75.66525731558878, "xcomet_score": 0.8190467357635498, "xcomet_qe_score": 0.8416481018066406, "metricx_score": 2.8683815002441406, "metricx_qe_score": 3.368110179901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们的研究汇集了来自87个国家超过一千名标注员的超过16,000个标注。", "metrics": {"bleu_score": 35.657308286005936, "chrf_score": 37.09260148114637, "xcomet_score": 0.943821370601654, "xcomet_qe_score": 0.9945597648620605, "metricx_score": 3.9362127780914307, "metricx_qe_score": 4.375177383422852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,现在我们已经具备了更好的条件来回答,自然语言处理数据集和模型最能反映谁的立场?", "metrics": {"bleu_score": 20.508300465307784, "chrf_score": 23.633274798453304, "xcomet_score": 0.9645047187805176, "xcomet_qe_score": 0.8838838338851929, "metricx_score": 0.7422587275505066, "metricx_qe_score": 0.7315553426742554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现自然语言处理领域存在立场性。", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 22.75731715786724, "xcomet_score": 0.9226457476615906, "xcomet_qe_score": 0.8270435333251953, "metricx_score": 0.7526626586914062, "metricx_qe_score": 0.7664328813552856, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现数据集和模型与英语国家最为契合。因此", "metrics": {"bleu_score": 46.62458164786595, "chrf_score": 42.35668726487264, "xcomet_score": 0.795525074005127, "xcomet_qe_score": 0.7606455087661743, "metricx_score": 2.943453073501587, "metricx_qe_score": 1.2890323400497437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPT-4社会可接受性分析,我们发现其与儒家文化圈和英语国家最为契合。我们", "metrics": {"bleu_score": 46.57089248291256, "chrf_score": 50.023273184310526, "xcomet_score": 0.7274466156959534, "xcomet_qe_score": 0.7073396444320679, "metricx_score": 5.563376426696777, "metricx_qe_score": 3.196033239364624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也发现,动态仇恨现象与英语国家最为相关。", "metrics": {"bleu_score": 15.774545980684183, "chrf_score": 11.866061329812208, "xcomet_score": 0.8215397596359253, "xcomet_qe_score": 0.8358162641525269, "metricx_score": 2.912662982940674, "metricx_qe_score": 2.104238510131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,大多数附加一致性都体现在受过大学教育的人群中。", "metrics": {"bleu_score": 31.482826891551863, "chrf_score": 26.12804444603568, "xcomet_score": 0.8665634393692017, "xcomet_qe_score": 0.8727221488952637, "metricx_score": 2.158750534057617, "metricx_qe_score": 2.2130913734436035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在社会可接受性评估任务中,GPT-4 的表现与拥有大学学历或研究生学历的人群最为一致。 我们同样在多纳海特发现了这种现象,其与受过大学教育的人群最为契合。", "metrics": {"bleu_score": 32.42795225549466, "chrf_score": 28.97224834818385, "xcomet_score": 0.7381733655929565, "xcomet_qe_score": 0.7381848096847534, "metricx_score": 3.6031546592712402, "metricx_qe_score": 3.6736254692077637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,不可避免地会有人被排除在外。 一个例子", "metrics": {"bleu_score": 43.58491573131903, "chrf_score": 41.27035089025545, "xcomet_score": 0.7276649475097656, "xcomet_qe_score": 0.6994547843933105, "metricx_score": 2.0325136184692383, "metricx_qe_score": 1.2023415565490723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,数据集和模型与非二元性别群体相比,与男性和女性群体存在较小的对齐度。", "metrics": {"bleu_score": 31.955693808615802, "chrf_score": 30.79915524011696, "xcomet_score": 0.610619306564331, "xcomet_qe_score": 0.5472766160964966, "metricx_score": 5.064122676849365, "metricx_qe_score": 4.843049049377441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT-4社会接受度评估任务中,以及DynaHATE任务分析中都发现了这一点。", "metrics": {"bleu_score": 40.27640044776316, "chrf_score": 42.987960186398865, "xcomet_score": 0.9099213480949402, "xcomet_qe_score": 0.9274738430976868, "metricx_score": 1.5424026250839233, "metricx_qe_score": 2.0516014099121094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于自然语言处理中存在位置性问题,我们该如何应对呢?", "metrics": {"bleu_score": 7.355026065679767, "chrf_score": 7.92751472811076, "xcomet_score": 0.7932654619216919, "xcomet_qe_score": 0.9094850420951843, "metricx_score": 2.812915325164795, "metricx_qe_score": 0.981419563293457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对这个问题有几项建议。第一项", "metrics": {"bleu_score": 10.975762213309226, "chrf_score": 17.790448969185654, "xcomet_score": 0.3930734395980835, "xcomet_qe_score": 0.34795045852661133, "metricx_score": 3.6727545261383057, "metricx_qe_score": 1.1951597929000854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,在整个研究过程中记录下所有相关设计决策。另一项", "metrics": {"bleu_score": 49.831162551286646, "chrf_score": 42.42999874521614, "xcomet_score": 0.4406944513320923, "xcomet_qe_score": 0.323833703994751, "metricx_score": 5.565767288208008, "metricx_qe_score": 4.0916643142700195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,从视角主义的角度进行自然语言处理研究。", "metrics": {"bleu_score": 51.74260437072165, "chrf_score": 52.40426759854904, "xcomet_score": 0.7691919803619385, "xcomet_qe_score": 0.7639127373695374, "metricx_score": 2.442885398864746, "metricx_qe_score": 2.4820377826690674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三项建议是,在四个特定的社区内构建专业的数据集和模型。", "metrics": {"bleu_score": 56.69097238314296, "chrf_score": 49.23908151243686, "xcomet_score": 0.905712366104126, "xcomet_qe_score": 0.8808419108390808, "metricx_score": 0.7121497392654419, "metricx_qe_score": 1.0023213624954224, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子就是Masakane倡议。我们的意思是,我们", "metrics": {"bleu_score": 32.3384287069116, "chrf_score": 41.099877308532186, "xcomet_score": 0.228041872382164, "xcomet_qe_score": 0.18907560408115387, "metricx_score": 7.3641676902771, "metricx_qe_score": 6.255002021789551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "想要强调的是,具有包容性的自然语言处理不仅仅是让所有", "metrics": {"bleu_score": 27.679069306659745, "chrf_score": 23.71686985761188, "xcomet_score": 0.44589462876319885, "xcomet_qe_score": 0.1923496425151825, "metricx_score": 5.794882774353027, "metricx_qe_score": 4.179176330566406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人服务。", "metrics": {"bleu_score": 39.03674453747003, "chrf_score": 36.76370111713883, "xcomet_score": 0.9485549926757812, "xcomet_qe_score": 0.9516949653625488, "metricx_score": 0.7394589781761169, "metricx_qe_score": 1.1236159801483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "至此,我们的演示就告一段", "metrics": {"bleu_score": 22.416933501922287, "chrf_score": 19.554328109211287, "xcomet_score": 0.8982166051864624, "xcomet_qe_score": 0.8502671122550964, "metricx_score": 2.6838412284851074, "metricx_qe_score": 1.142357587814331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "落。如果您想了解更多信息,欢迎查阅我们的仪表板,获取最新分析结果,以及我们的论文。", "metrics": {"bleu_score": 46.13971189484216, "chrf_score": 37.680420155432024, "xcomet_score": 0.6623049378395081, "xcomet_qe_score": 0.5767931938171387, "metricx_score": 5.112341403961182, "metricx_qe_score": 5.686733722686768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自复旦大学的袁思宇。", "metrics": {"bleu_score": 57.49089871602278, "chrf_score": 40.29717564796037, "xcomet_score": 0.9869414567947388, "xcomet_qe_score": 0.9011067748069763, "metricx_score": 0.3689901828765869, "metricx_qe_score": 0.6871873736381531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将为大家介绍我们的工作,题为“从大型语言模型中提炼脚本知识以进行约束语言规划”。", "metrics": {"bleu_score": 41.6514926510716, "chrf_score": 41.156515925078615, "xcomet_score": 0.8868489265441895, "xcomet_qe_score": 0.7952710390090942, "metricx_score": 1.7296391725540161, "metricx_qe_score": 1.969921588897705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类常常按照循序渐进的交互模式,以保证脚本的形式来规划他们的行动。 前", "metrics": {"bleu_score": 18.973813416364887, "chrf_score": 19.471650171443912, "xcomet_score": 0.6865394115447998, "xcomet_qe_score": 0.6900381445884705, "metricx_score": 5.683349132537842, "metricx_qe_score": 3.997384786605835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期的研究利用语言模型来规划刻板活动的抽象目标,", "metrics": {"bleu_score": 49.89051794043509, "chrf_score": 49.97352588818638, "xcomet_score": 0.5621435642242432, "xcomet_qe_score": 0.44979801774024963, "metricx_score": 8.77548599243164, "metricx_qe_score": 10.246649742126465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如制作蛋糕,并表明大型语言模型能够有效地将目标分解为步骤。", "metrics": {"bleu_score": 41.01896114451473, "chrf_score": 36.83678461646199, "xcomet_score": 0.2779369056224823, "xcomet_qe_score": 0.21959267556667328, "metricx_score": 3.07064151763916, "metricx_qe_score": 1.682761788368225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以往的研究主要集中于针对刻板活动的抽象目标进行规划。", "metrics": {"bleu_score": 52.174427464587126, "chrf_score": 49.92322699526503, "xcomet_score": 0.8955015540122986, "xcomet_qe_score": 0.8255288004875183, "metricx_score": 1.914931297302246, "metricx_qe_score": 2.023642063140869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于具有具体约束的目标进行规划,例如制作巧克力蛋糕,仍然鲜有研究。", "metrics": {"bleu_score": 20.33717397090786, "chrf_score": 20.151524036491523, "xcomet_score": 0.8940241932868958, "xcomet_qe_score": 0.8368291854858398, "metricx_score": 1.2207839488983154, "metricx_qe_score": 1.4916332960128784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文中,我们定义了受约束的语言规划问题。 这些约束对规划目标施加不同的限制。", "metrics": {"bleu_score": 44.54339868518327, "chrf_score": 36.97257011595247, "xcomet_score": 0.9026188850402832, "xcomet_qe_score": 0.8858445286750793, "metricx_score": 1.0793652534484863, "metricx_qe_score": 1.399609923362732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被不同的、具有多方面约束的现实具体目标所", "metrics": {"bleu_score": 45.37519286835206, "chrf_score": 38.83702147102528, "xcomet_score": 0.7865179777145386, "xcomet_qe_score": 0.7840641736984253, "metricx_score": 5.2300801277160645, "metricx_qe_score": 3.9022650718688965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "继承。优秀的规划者应当编写符合约束且忠实的脚本。", "metrics": {"bleu_score": 22.28012779155529, "chrf_score": 20.88332497161451, "xcomet_score": 0.48274800181388855, "xcomet_qe_score": 0.4313111901283264, "metricx_score": 6.0460286140441895, "metricx_qe_score": 6.341552734375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文首先评估并提升大型语言模型受约束的语言规划能力。", "metrics": {"bleu_score": 41.95982790851073, "chrf_score": 34.86421270409782, "xcomet_score": 0.832444429397583, "xcomet_qe_score": 0.7881779670715332, "metricx_score": 1.0572223663330078, "metricx_qe_score": 1.1878927946090698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于没有针对特定目标的现有数据集来支持我们的研究, 我们首先需要获得这些目标。", "metrics": {"bleu_score": 45.45079482870868, "chrf_score": 40.85178704303828, "xcomet_score": 0.8890974521636963, "xcomet_qe_score": 0.8554779887199402, "metricx_score": 2.6814017295837402, "metricx_qe_score": 4.431142807006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们通过多方面的约束来扩展抽象目标。对于带有人工参与的数据采集,请使用InstructGPT。", "metrics": {"bleu_score": 42.942537299990704, "chrf_score": 50.45638981564646, "xcomet_score": 0.7170483469963074, "xcomet_qe_score": 0.6436953544616699, "metricx_score": 4.1261115074157715, "metricx_qe_score": 3.0800702571868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们抽取了100名特定女生作为样本,并评估了由大型本地模型生成的脚本。", "metrics": {"bleu_score": 48.05219079236057, "chrf_score": 49.9777374129326, "xcomet_score": 0.6433900594711304, "xcomet_qe_score": 0.63368821144104, "metricx_score": 5.419238567352295, "metricx_qe_score": 5.6982340812683105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本表报告了结果的总体准确率。", "metrics": {"bleu_score": 25.491833774890388, "chrf_score": 21.492558656584766, "xcomet_score": 0.993279218673706, "xcomet_qe_score": 0.9924379587173462, "metricx_score": 0.7121083736419678, "metricx_qe_score": 0.6441795825958252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有轻量级语言模型在规划特定目标方面都未能达到令人满意的结果。 ", "metrics": {"bleu_score": 35.15787253565496, "chrf_score": 33.463494525715134, "xcomet_score": 0.880509614944458, "xcomet_qe_score": 0.8660905361175537, "metricx_score": 2.0050182342529297, "metricx_qe_score": 2.2523231506347656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们进行详细分析,以探讨行学习模型失效的原因。", "metrics": {"bleu_score": 41.95311940818068, "chrf_score": 32.52476047548546, "xcomet_score": 0.8911867737770081, "xcomet_qe_score": 0.8777709603309631, "metricx_score": 1.1425158977508545, "metricx_qe_score": 0.8589581251144409, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图示结果表明,生成的脚本在语义完整性方面表现可接受,但对约束条件的忠实性无法得到保证。", "metrics": {"bleu_score": 50.90592077494645, "chrf_score": 44.9210426580808, "xcomet_score": 0.9918243885040283, "xcomet_qe_score": 0.9888533353805542, "metricx_score": 0.855481743812561, "metricx_qe_score": 1.213883876800537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了 WikiHow 中定义的约束的更细粒度的课题类别。", "metrics": {"bleu_score": 36.87918653206716, "chrf_score": 43.77073313947619, "xcomet_score": 0.7644052505493164, "xcomet_qe_score": 0.7407863736152649, "metricx_score": 3.84293270111084, "metricx_qe_score": 4.299192905426025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,对于不同类别的女孩,指导性 PD 的规划性能存在显著差异。", "metrics": {"bleu_score": 34.60446103489811, "chrf_score": 25.39678562050096, "xcomet_score": 0.49373510479927063, "xcomet_qe_score": 0.34852173924446106, "metricx_score": 6.963146209716797, "metricx_qe_score": 7.816486835479736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前研究表明,光-光风模型(light-light wind models)的输出质量存在较高方差,导致性能不佳。", "metrics": {"bleu_score": 21.775675059323095, "chrf_score": 19.5556486909944, "xcomet_score": 0.7631601095199585, "xcomet_qe_score": 0.7614467144012451, "metricx_score": 6.140507698059082, "metricx_qe_score": 7.10980749130249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们借鉴了超生成Z-滤波器(overgenerated Z-filter)的思想,以提升生成质量。", "metrics": {"bleu_score": 16.397906895258597, "chrf_score": 13.682277698640938, "xcomet_score": 0.8361420631408691, "xcomet_qe_score": 0.6840754747390747, "metricx_score": 4.441477298736572, "metricx_qe_score": 4.947671413421631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们展示了约束类型,并通过intract CPT的示例进行说明,并根据种子抽象目标获得特定的目标。", "metrics": {"bleu_score": 45.98828153031857, "chrf_score": 43.8625948773497, "xcomet_score": 0.6825406551361084, "xcomet_qe_score": 0.5786471366882324, "metricx_score": 5.249617576599121, "metricx_qe_score": 6.148705959320068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指示GPT过度生成针对特定目标的案例脚本。", "metrics": {"bleu_score": 21.62601067591834, "chrf_score": 27.230210450798676, "xcomet_score": 0.7292567491531372, "xcomet_qe_score": 0.7427822351455688, "metricx_score": 3.8617377281188965, "metricx_qe_score": 4.510403156280518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,开发一个滤波器模型,用于选择可行的剧本。", "metrics": {"bleu_score": 6.901743013068209, "chrf_score": 11.945494161304596, "xcomet_score": 0.8864258527755737, "xcomet_qe_score": 0.882784903049469, "metricx_score": 1.4387402534484863, "metricx_qe_score": 1.3368314504623413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转化为抽象的 GPT 嵌入向量,并计算余弦相似度和相似度得分,以衡量语义相似性。", "metrics": {"bleu_score": 50.911155047883334, "chrf_score": 42.56116656114003, "xcomet_score": 0.8637892603874207, "xcomet_qe_score": 0.7250889539718628, "metricx_score": 2.279454231262207, "metricx_qe_score": 2.530442953109741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们避免使用包含目标约束关键词的脚本。", "metrics": {"bleu_score": 55.43852213695471, "chrf_score": 51.60097221908482, "xcomet_score": 0.7346184253692627, "xcomet_qe_score": 0.7543836832046509, "metricx_score": 4.768651008605957, "metricx_qe_score": 4.963806629180908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "只有当目标得分在目标集中最高时,我们才会保留该脚本。", "metrics": {"bleu_score": 56.39331823461213, "chrf_score": 46.11543532338081, "xcomet_score": 0.774955153465271, "xcomet_qe_score": 0.7281601428985596, "metricx_score": 3.315328598022461, "metricx_qe_score": 5.626780986785889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "采用我们的方法,InstructZBT 可以生成更高质量的脚本。", "metrics": {"bleu_score": 70.98232254187813, "chrf_score": 64.06283025396857, "xcomet_score": 0.8153231143951416, "xcomet_qe_score": 0.8321805596351624, "metricx_score": 3.6050772666931152, "metricx_qe_score": 4.345944881439209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实度方面都显著提升了规划能力。", "metrics": {"bleu_score": 72.31484102098973, "chrf_score": 65.70296275598395, "xcomet_score": 0.9299541115760803, "xcomet_qe_score": 0.9365752339363098, "metricx_score": 0.827889621257782, "metricx_qe_score": 1.2439100742340088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于大型语言模型的部署成本高昂,因此赋予小型且专业化模型语言规划能力至", "metrics": {"bleu_score": 42.610689515649746, "chrf_score": 38.537587148231125, "xcomet_score": 0.798079252243042, "xcomet_qe_score": 0.7203459739685059, "metricx_score": 4.925480842590332, "metricx_qe_score": 1.9755884408950806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关重要。数据集的创建是实现这一目标的关键步骤。", "metrics": {"bleu_score": 45.810919659463124, "chrf_score": 44.743353518683016, "xcomet_score": 0.6303179264068604, "xcomet_qe_score": 0.6090738773345947, "metricx_score": 1.7127056121826172, "metricx_qe_score": 3.1492109298706055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,既往的研究并不能支持针对具体目标的规划,并且手动数据集标注成本高昂。", "metrics": {"bleu_score": 46.29350336854406, "chrf_score": 38.89053020392142, "xcomet_score": 0.9826542139053345, "xcomet_qe_score": 0.972064733505249, "metricx_score": 0.9710231423377991, "metricx_qe_score": 1.306064486503601, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的思想,从大型语言模型中蒸馏受约束的语言规划数据集。", "metrics": {"bleu_score": 52.71961547922596, "chrf_score": 44.77387890342811, "xcomet_score": 0.8814423084259033, "xcomet_qe_score": 0.7921860218048096, "metricx_score": 3.6512999534606934, "metricx_qe_score": 3.2433834075927734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用一种方法来构建一个受约束的语言规划数据集,该数据集命名为 Codescript。", "metrics": {"bleu_score": 16.37368248844126, "chrf_score": 29.442670110819858, "xcomet_score": 0.953455924987793, "xcomet_qe_score": 0.9377037286758423, "metricx_score": 2.0996952056884766, "metricx_qe_score": 2.7763235569000244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了55,000个带有脚本的特定目标。", "metrics": {"bleu_score": 29.62789157394226, "chrf_score": 42.44778530648096, "xcomet_score": 0.888967752456665, "xcomet_qe_score": 0.8385939598083496, "metricx_score": 1.8689863681793213, "metricx_qe_score": 1.570450782775879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试站点的质量,我们要求云众包工人查找修改后的错误样本。", "metrics": {"bleu_score": 35.667492458197735, "chrf_score": 31.618933208495232, "xcomet_score": 0.7131590843200684, "xcomet_qe_score": 0.7131285667419434, "metricx_score": 6.313523769378662, "metricx_qe_score": 6.4911065101623535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此图展示了代码脚本的约束分布。", "metrics": {"bleu_score": 37.5022891676693, "chrf_score": 23.913122665200788, "xcomet_score": 0.8484381437301636, "xcomet_qe_score": 0.7651975154876709, "metricx_score": 3.1305129528045654, "metricx_qe_score": 4.031469821929932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,代码脚本在生成的特定目标中表现出高度的赞同性。", "metrics": {"bleu_score": 46.09056322258578, "chrf_score": 33.956918478069994, "xcomet_score": 0.6925574541091919, "xcomet_qe_score": 0.6578891277313232, "metricx_score": 6.464779853820801, "metricx_qe_score": 6.944541931152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "借助代码脚本,我们可以追溯到更小、但更专业的约束语言规划模型。", "metrics": {"bleu_score": 18.26249361348376, "chrf_score": 13.636141581456847, "xcomet_score": 0.6091851592063904, "xcomet_qe_score": 0.5321282148361206, "metricx_score": 6.24669075012207, "metricx_qe_score": 5.951333522796631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,T-file 函数在成本率上的表现能够生成比大多数大型语言模型更高质量的脚本,这表明在合适的训练数据集上进行适当训练后,较小的模型可以支持大型模型。", "metrics": {"bleu_score": 49.555989507045595, "chrf_score": 39.754396131829445, "xcomet_score": 0.5824371576309204, "xcomet_qe_score": 0.5054377317428589, "metricx_score": 7.979621887207031, "metricx_qe_score": 8.75102424621582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "综上所述,我们定义了受约束的语言规划问题。", "metrics": {"bleu_score": 33.270499249398405, "chrf_score": 31.113002808630213, "xcomet_score": 0.9031511545181274, "xcomet_qe_score": 0.8915322422981262, "metricx_score": 0.7481911182403564, "metricx_qe_score": 0.6697221994400024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受约束语言规划能力,并为大型语言模型开发了一种过度生成过滤器方法。", "metrics": {"bleu_score": 56.85857574565514, "chrf_score": 46.85293115894055, "xcomet_score": 0.8336365818977356, "xcomet_qe_score": 0.8234533667564392, "metricx_score": 2.615607976913452, "metricx_qe_score": 3.1071979999542236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们利用大型语言模型生成高质量的脚本数据集,用于受约束的语言规划。", "metrics": {"bleu_score": 52.1790452306961, "chrf_score": 39.04305163099516, "xcomet_score": 0.9420095086097717, "xcomet_qe_score": 0.8708704710006714, "metricx_score": 1.9640796184539795, "metricx_qe_score": 2.8579769134521484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期望 CodeScript 数据集能成为推动语言规划研究进展的有价值资源。", "metrics": {"bleu_score": 45.23791232712228, "chrf_score": 54.14641244722772, "xcomet_score": 0.9771817922592163, "xcomet_qe_score": 0.9765944480895996, "metricx_score": 0.723791778087616, "metricx_qe_score": 0.7886488437652588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查阅更详细的代码脚本信息。", "metrics": {"bleu_score": 43.7241098509127, "chrf_score": 29.449374731721274, "xcomet_score": 0.8776659965515137, "xcomet_qe_score": 0.8783209919929504, "metricx_score": 2.3597638607025146, "metricx_qe_score": 2.3078525066375732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好,我叫朱恒。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 6.517341040462428, "xcomet_score": 0.803790807723999, "xcomet_qe_score": 0.8219943642616272, "metricx_score": 0.14039206504821777, "metricx_qe_score": 0.22168953716754913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将为大家介绍我们的论文: “内核 2003 年的命名实体标注器在 2023 年是否仍然有效?”", "metrics": {"bleu_score": 59.35107565052857, "chrf_score": 58.950878920844914, "xcomet_score": 0.7545386552810669, "xcomet_qe_score": 0.7551958560943604, "metricx_score": 3.7435450553894043, "metricx_qe_score": 3.0779776573181152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在开始吧。", "metrics": {"bleu_score": 43.01250851313264, "chrf_score": 29.763585038814398, "xcomet_score": 0.9983294010162354, "xcomet_qe_score": 0.9951030015945435, "metricx_score": 0.437703013420105, "metricx_qe_score": 0.7329950928688049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了泛化问题,以命名实体识别任务(或 NER 任务)作为验证手段。", "metrics": {"bleu_score": 44.78315454797505, "chrf_score": 43.0271701321263, "xcomet_score": 0.960207462310791, "xcomet_qe_score": 0.9397780895233154, "metricx_score": 2.0652363300323486, "metricx_qe_score": 3.1758761405944824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用了CONO 2003 近20年以开发命名实体识别(NER)系统。这自然会引发几个问题。", "metrics": {"bleu_score": 27.26337118792593, "chrf_score": 27.381908745278654, "xcomet_score": 0.6979192495346069, "xcomet_qe_score": 0.7268662452697754, "metricx_score": 5.558194160461426, "metricx_qe_score": 5.917599201202393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型是否能泛化到现代数据?", "metrics": {"bleu_score": 51.707871254438786, "chrf_score": 44.88738896031098, "xcomet_score": 0.996817946434021, "xcomet_qe_score": 0.992597222328186, "metricx_score": 0.23134373128414154, "metricx_qe_score": 0.28072142601013184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新的标注器时,良好泛化需要什么? 与此同时", "metrics": {"bleu_score": 45.10159290559757, "chrf_score": 38.811759961152674, "xcomet_score": 0.7635353803634644, "xcomet_qe_score": 0.7308832406997681, "metricx_score": 2.52795147895813, "metricx_qe_score": 1.3005740642547607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",如果我们确实观察到泛化能力不足,是什么导致了这些模型的性能下降?", "metrics": {"bleu_score": 57.19656065435025, "chrf_score": 53.16729565453243, "xcomet_score": 0.9423567056655884, "xcomet_qe_score": 0.9561227560043335, "metricx_score": 2.715759754180908, "metricx_qe_score": 2.973520517349243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了 CONO++ 数据集。这是", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 39.442039869723686, "xcomet_score": 0.6874902844429016, "xcomet_qe_score": 0.7616293430328369, "metricx_score": 6.2036027908325195, "metricx_qe_score": 4.690491676330566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透社新闻中收集的数据集,并根据 CONO 2003 的标注指南进行标注。", "metrics": {"bleu_score": 33.88779763275169, "chrf_score": 29.219664744918138, "xcomet_score": 0.8125228881835938, "xcomet_qe_score": 0.7421140074729919, "metricx_score": 5.621683120727539, "metricx_qe_score": 5.657724857330322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在Kano 2003数据集上对超过20个模型进行了微调。", "metrics": {"bleu_score": 38.571530913075044, "chrf_score": 35.31883657789792, "xcomet_score": 0.8394619226455688, "xcomet_qe_score": 0.8192364573478699, "metricx_score": 5.076002597808838, "metricx_qe_score": 4.903501987457275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Kano 03测试集和Kano++测试集对它们进行了评估。", "metrics": {"bleu_score": 46.27620794967283, "chrf_score": 37.538627452999904, "xcomet_score": 0.6917330026626587, "xcomet_qe_score": 0.7587023973464966, "metricx_score": 6.689271926879883, "metricx_qe_score": 5.441215515136719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要的是,我们计算了 F1 分数的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 47.909344374573976, "chrf_score": 55.28841576719768, "xcomet_score": 0.9912470579147339, "xcomet_qe_score": 0.9857478141784668, "metricx_score": 0.9565049409866333, "metricx_qe_score": 1.344963550567627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为了实现良好的泛化,需要些什么呢?", "metrics": {"bleu_score": 38.36841681691305, "chrf_score": 38.526145934532934, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.11973033845424652, "metricx_qe_score": 0.22249862551689148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必需的。", "metrics": {"bleu_score": 23.956565612760205, "chrf_score": 21.59867695133108, "xcomet_score": 0.9930475950241089, "xcomet_qe_score": 0.9945417642593384, "metricx_score": 0.6633017659187317, "metricx_qe_score": 0.996856689453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,是模型架构。", "metrics": {"bleu_score": 58.73949094699213, "chrf_score": 50.21825396825397, "xcomet_score": 0.9973517656326294, "xcomet_qe_score": 0.9827858209609985, "metricx_score": 0.10066407918930054, "metricx_qe_score": 0.14931105077266693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现Transformer模型通常能更好地泛化到新的数据。", "metrics": {"bleu_score": 51.54971278749077, "chrf_score": 60.193759898086896, "xcomet_score": 0.887237548828125, "xcomet_qe_score": 0.8814883828163147, "metricx_score": 1.7252991199493408, "metricx_qe_score": 3.486858606338501, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常情况下,更大的模型能够带来更好的泛化能力。", "metrics": {"bleu_score": 32.88861494180287, "chrf_score": 29.694258227593007, "xcomet_score": 0.9891072511672974, "xcomet_qe_score": 0.9852021336555481, "metricx_score": 0.6268782615661621, "metricx_qe_score": 0.6676300168037415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要的是,我们都知道微调样本的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 40.570821609615585, "chrf_score": 46.62616216528058, "xcomet_score": 0.9456784725189209, "xcomet_qe_score": 0.7961912751197815, "metricx_score": 4.03944730758667, "metricx_qe_score": 2.0626091957092285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调样本实际上也能够带来更好的泛化能力。", "metrics": {"bleu_score": 44.867276656754186, "chrf_score": 35.771244505823354, "xcomet_score": 0.9842424392700195, "xcomet_qe_score": 0.9195638298988342, "metricx_score": 0.9462023973464966, "metricx_qe_score": 1.0990877151489258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们来探讨一个问题:是什么导致某些模型的性能下降? 我们提出了两个假设。", "metrics": {"bleu_score": 30.901532587310246, "chrf_score": 28.27903807020228, "xcomet_score": 0.9777292013168335, "xcomet_qe_score": 0.9789516925811768, "metricx_score": 0.8617401123046875, "metricx_qe_score": 0.6658939719200134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,即由于反复使用同一测试集而导致的过拟合。这通常表现为在新测试集上的边际效益递减。", "metrics": {"bleu_score": 55.67329282463929, "chrf_score": 47.345155150516014, "xcomet_score": 0.9712830781936646, "xcomet_qe_score": 0.9500954151153564, "metricx_score": 1.4839978218078613, "metricx_qe_score": 2.0586981773376465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间时间差距不断扩大而导致的性能下降。", "metrics": {"bleu_score": 55.121811783478186, "chrf_score": 49.27833862873095, "xcomet_score": 0.9637404680252075, "xcomet_qe_score": 0.8875402808189392, "metricx_score": 1.5466606616973877, "metricx_qe_score": 2.0887997150421143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表上看到,红色的最佳拟合线具有大于1的梯度。", "metrics": {"bleu_score": 27.399155467400504, "chrf_score": 27.161122864667668, "xcomet_score": 0.8806216716766357, "xcomet_qe_score": 0.8092678785324097, "metricx_score": 1.1533000469207764, "metricx_qe_score": 1.3756277561187744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们对 Carnot 2003 所做的每一次改进,在 Carnot++ 上都将转化为超过一个单位的提升,这表明不存在边际效益递减。", "metrics": {"bleu_score": 19.345299022826193, "chrf_score": 18.889001060665255, "xcomet_score": 0.7426429986953735, "xcomet_qe_score": 0.7336824536323547, "metricx_score": 6.7886576652526855, "metricx_qe_score": 5.680686950683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在本例中,未观察到自适应过拟合现象。", "metrics": {"bleu_score": 41.5064814270007, "chrf_score": 35.30310946760775, "xcomet_score": 0.9074486494064331, "xcomet_qe_score": 0.9094317555427551, "metricx_score": 0.8686410188674927, "metricx_qe_score": 1.005072832107544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,时间漂移又如何呢?", "metrics": {"bleu_score": 26.20251007173262, "chrf_score": 26.822985818696804, "xcomet_score": 0.9188566207885742, "xcomet_qe_score": 0.8987762928009033, "metricx_score": 0.6239734292030334, "metricx_qe_score": 1.280874490737915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,用更新的数据重新训练或继续预训练部分模型,结果发现,时间差距越大,性能下降越明显。 这进一步证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 55.291892641641184, "chrf_score": 49.91301090845937, "xcomet_score": 0.9622238874435425, "xcomet_qe_score": 0.955812931060791, "metricx_score": 1.313412070274353, "metricx_qe_score": 1.4407997131347656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型尺寸以及更多的微调示例。", "metrics": {"bleu_score": 76.83885850728005, "chrf_score": 72.39662296694443, "xcomet_score": 0.9297579526901245, "xcomet_qe_score": 0.9639407396316528, "metricx_score": 0.7312777042388916, "metricx_qe_score": 0.5764248371124268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素相互关联,相辅相成。我们不能仅仅依赖其中一个因素,而是需要在其他因素的支持下共同作用。", "metrics": {"bleu_score": 21.502986531337857, "chrf_score": 24.105366213829132, "xcomet_score": 0.9784806966781616, "xcomet_qe_score": 0.9652763605117798, "metricx_score": 0.5032963156700134, "metricx_qe_score": 0.4750936031341553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与此同时,我们还发现这里的性能下降是由时间漂移引起的,而且出人意料的是,并非由自适应过拟合造成,尽管KONO 2003已经使用了超过20年。", "metrics": {"bleu_score": 45.03309531540248, "chrf_score": 40.042290933862304, "xcomet_score": 0.8231377601623535, "xcomet_qe_score": 0.7739722728729248, "metricx_score": 4.969773769378662, "metricx_qe_score": 5.910516738891602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们论文题目中提出的问题,即2003年的Connell词标注器在2023年是否仍然有效?而", "metrics": {"bleu_score": 53.635905252552426, "chrf_score": 47.13515723922229, "xcomet_score": 0.6419705152511597, "xcomet_qe_score": 0.6523067951202393, "metricx_score": 5.233419418334961, "metricx_qe_score": 3.3081870079040527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的发现是,答案实际上是肯定的。", "metrics": {"bleu_score": 30.648595997659072, "chrf_score": 32.63652188015358, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5399649143218994, "metricx_qe_score": 0.8010891079902649, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望本文能够促使人们对如何改进模型泛化能力开展更多研究。", "metrics": {"bleu_score": 33.44934226767949, "chrf_score": 28.111112061213483, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.30322104692459106, "metricx_qe_score": 0.3399242162704468, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查阅我们的论文、数据集。如有任何疑问,欢迎随时与我联系。", "metrics": {"bleu_score": 57.10719840595963, "chrf_score": 46.482136210110006, "xcomet_score": 0.9919270277023315, "xcomet_qe_score": 0.9761342406272888, "metricx_score": 0.17601628601551056, "metricx_qe_score": 0.19643209874629974, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将介绍我们关于解决间接指代表达以进行实体选择的工作,其中我们提出了AltEntityScorers。", "metrics": {"bleu_score": 23.495540389488045, "chrf_score": 30.251654204665275, "xcomet_score": 0.587065577507019, "xcomet_qe_score": 0.5589042901992798, "metricx_score": 7.30533504486084, "metricx_qe_score": 8.187695503234863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是贾瓦德·侯赛尼,这篇工作是与菲利普·拉德林斯基、西尔维娅·帕里提和安妮·刘易斯共同完成的。", "metrics": {"bleu_score": 1.262959252582657, "chrf_score": 2.388535031847133, "xcomet_score": 0.9625084400177002, "xcomet_qe_score": 0.9114387035369873, "metricx_score": 2.0948688983917236, "metricx_qe_score": 2.1008758544921875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请考虑以下备选问题:", "metrics": {"bleu_score": 20.556680845025987, "chrf_score": 17.638888888888893, "xcomet_score": 0.8989036083221436, "xcomet_qe_score": 0.9259333610534668, "metricx_score": 0.28136301040649414, "metricx_qe_score": 0.2955072522163391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“你是想选择《Easy on Me》还是《I Got a Feeling》?”", "metrics": {"bleu_score": 13.988521776260258, "chrf_score": 48.36542318197153, "xcomet_score": 0.8501819372177124, "xcomet_qe_score": 0.8380284309387207, "metricx_score": 2.5833325386047363, "metricx_qe_score": 2.923970937728882, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,用户希望在这两首歌曲中进行选择。", "metrics": {"bleu_score": 24.90044914490855, "chrf_score": 21.833256359709623, "xcomet_score": 0.9927051067352295, "xcomet_qe_score": 0.9858063459396362, "metricx_score": 0.5701451301574707, "metricx_qe_score": 0.44354307651519775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用。例如,可以直接说歌曲的名字是Yami,或者它的位置,即第一首。", "metrics": {"bleu_score": 32.177973195172214, "chrf_score": 28.388202941198887, "xcomet_score": 0.6073290109634399, "xcomet_qe_score": 0.512453019618988, "metricx_score": 7.277753829956055, "metricx_qe_score": 7.885741233825684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在有些情况下,间接引用可能更适合进行更自然的对话。这可能发生", "metrics": {"bleu_score": 38.89342506993912, "chrf_score": 49.631331096306205, "xcomet_score": 0.7451993823051453, "xcomet_qe_score": 0.7789453268051147, "metricx_score": 5.121592044830322, "metricx_qe_score": 1.71140718460083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在用户无法回忆起歌曲名称时。", "metrics": {"bleu_score": 7.864268300677881, "chrf_score": 10.069036219750263, "xcomet_score": 0.9843802452087402, "xcomet_qe_score": 0.9784137010574341, "metricx_score": 2.897573947906494, "metricx_qe_score": 1.7346324920654297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者发音过于相似,难以区分。", "metrics": {"bleu_score": 24.792487205599897, "chrf_score": 23.40751844934748, "xcomet_score": 0.9801307916641235, "xcomet_qe_score": 0.9764577150344849, "metricx_score": 0.7795825004577637, "metricx_qe_score": 0.2045377939939499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户希望指定偏好时。", "metrics": {"bleu_score": 18.676087970674768, "chrf_score": 20.100464706475634, "xcomet_score": 0.9979352951049805, "xcomet_qe_score": 1.0, "metricx_score": 0.6156729459762573, "metricx_qe_score": 0.49184831976890564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些直接差异的例子。例如,较新的版本或不具活力的歌曲。 这是一个", "metrics": {"bleu_score": 18.966153199155166, "chrf_score": 17.188556109694026, "xcomet_score": 0.3124586343765259, "xcomet_qe_score": 0.46858030557632446, "metricx_score": 8.557944297790527, "metricx_qe_score": 4.780055522918701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对话系统中的重要问题,同时也是评估大型语言模型实体理解能力的重要基准。 我们尚未知晓任何公开数据集,", "metrics": {"bleu_score": 22.186021279469905, "chrf_score": 21.752288341734033, "xcomet_score": 0.2544586956501007, "xcomet_qe_score": 0.167434424161911, "metricx_score": 6.331199645996094, "metricx_qe_score": 6.214813709259033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更没有适用于特定任务的大规模公开数据集。因此,我们采用众包标注的方式收集了一个数据集。", "metrics": {"bleu_score": 28.626502948618317, "chrf_score": 26.777486950411678, "xcomet_score": 0.7659605145454407, "xcomet_qe_score": 0.6974558234214783, "metricx_score": 1.9016010761260986, "metricx_qe_score": 1.7684639692306519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了音乐、书籍和食谱三个不同的领域。", "metrics": {"bleu_score": 78.3211592255058, "chrf_score": 68.21061547148504, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.21703281998634338, "metricx_qe_score": 0.35979121923446655, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,采用卡通补全集。", "metrics": {"bleu_score": 53.961318533294246, "chrf_score": 48.53412789411395, "xcomet_score": 0.8063913583755493, "xcomet_qe_score": 0.8036871552467346, "metricx_score": 6.369636535644531, "metricx_qe_score": 5.328728675842285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这部卡通有三个对话框。", "metrics": {"bleu_score": 59.00468726392806, "chrf_score": 52.41221741221741, "xcomet_score": 0.8662618398666382, "xcomet_qe_score": 0.8234660625457764, "metricx_score": 0.4135943651199341, "metricx_qe_score": 0.46161365509033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框中,鲍勃说:“还记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 49.27328690508444, "chrf_score": 45.18453028609712, "xcomet_score": 0.9001537561416626, "xcomet_qe_score": 0.8847556114196777, "metricx_score": 1.1979421377182007, "metricx_qe_score": 0.8314677476882935, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "凭借此话,鲍勃奠定了对话的背景。", "metrics": {"bleu_score": 16.188613565728215, "chrf_score": 11.58489165055524, "xcomet_score": 0.9517935514450073, "xcomet_qe_score": 0.9404116868972778, "metricx_score": 2.572622299194336, "metricx_qe_score": 2.675077199935913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话气泡中,爱丽丝说:“你是说对我好一点,还是我说感觉到了?”", "metrics": {"bleu_score": 18.10425767465413, "chrf_score": 11.591362803678571, "xcomet_score": 0.6412783861160278, "xcomet_qe_score": 0.671357274055481, "metricx_score": 5.084546089172363, "metricx_qe_score": 4.548622131347656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8669623136520386, "xcomet_qe_score": 0.8554072380065918, "metricx_score": 1.420009732246399, "metricx_qe_score": 2.3006839752197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用间接指代来选择其中一个实体,例如,新的", "metrics": {"bleu_score": 41.50549395913714, "chrf_score": 33.66158563757347, "xcomet_score": 0.5610471963882446, "xcomet_qe_score": 0.6058273315429688, "metricx_score": 5.831067085266113, "metricx_qe_score": 4.773874282836914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话框,但第三个对话框由标注员填写。", "metrics": {"bleu_score": 69.09086323462154, "chrf_score": 62.86260906979017, "xcomet_score": 0.9080743193626404, "xcomet_qe_score": 0.7987481355667114, "metricx_score": 1.342389464378357, "metricx_qe_score": 1.3384054899215698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话框来自每个领域的一些手动提示。", "metrics": {"bleu_score": 33.45038343761574, "chrf_score": 28.202614113323932, "xcomet_score": 0.834439754486084, "xcomet_qe_score": 0.7556188106536865, "metricx_score": 1.5360909700393677, "metricx_qe_score": 2.4284749031066895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即替代问题,的生成方式如下。", "metrics": {"bleu_score": 12.149176141753365, "chrf_score": 12.963667247248065, "xcomet_score": 0.9097991585731506, "xcomet_qe_score": 0.8954007625579834, "metricx_score": 1.1160832643508911, "metricx_qe_score": 1.2467726469039917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你的意思是 A 还是 B? 其中", "metrics": {"bleu_score": 23.462350320527996, "chrf_score": 25.568447705594988, "xcomet_score": 0.6407427787780762, "xcomet_qe_score": 0.6844704151153564, "metricx_score": 2.4370267391204834, "metricx_qe_score": 0.6423485279083252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "A 和 B 是维基百科中的示例。", "metrics": {"bleu_score": 37.868117902707674, "chrf_score": 28.626146918118696, "xcomet_score": 0.9730204343795776, "xcomet_qe_score": 0.9705580472946167, "metricx_score": 0.7192338705062866, "metricx_qe_score": 0.6045550107955933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们所使用的不同抽样方法。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 76.47740633807875, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.06413520872592926, "metricx_qe_score": 0.0942588746547699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向上移动列表时,实体之间的相似性会增加,并且通常更难进行歧义消除。", "metrics": {"bleu_score": 19.861626912448344, "chrf_score": 19.75203499197508, "xcomet_score": 0.7828736305236816, "xcomet_qe_score": 0.710797131061554, "metricx_score": 3.786839485168457, "metricx_qe_score": 4.3487467765808105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是均匀的随机分布。", "metrics": {"bleu_score": 10.600313379512592, "chrf_score": 12.196573060447404, "xcomet_score": 0.9762619733810425, "xcomet_qe_score": 0.9671039581298828, "metricx_score": 0.7428264617919922, "metricx_qe_score": 0.48285558819770813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二种情况是当实体具有相似的标题时,例如,两本书都名为《归来》。", "metrics": {"bleu_score": 28.101415228539146, "chrf_score": 23.972652542830602, "xcomet_score": 0.8634501695632935, "xcomet_qe_score": 0.855791449546814, "metricx_score": 2.1198954582214355, "metricx_qe_score": 3.1197869777679443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是,当它们在维基百科上有相似的描述。", "metrics": {"bleu_score": 83.18180062062373, "chrf_score": 83.70189528707373, "xcomet_score": 0.9384211301803589, "xcomet_qe_score": 0.9759562015533447, "metricx_score": 0.5667060613632202, "metricx_qe_score": 0.5739404559135437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一种情况是,当它们在维基百科上有相似的信息框或属性。", "metrics": {"bleu_score": 62.96129633243316, "chrf_score": 67.06691093386577, "xcomet_score": 0.9962074756622314, "xcomet_qe_score": 0.9927875995635986, "metricx_score": 1.047887921333313, "metricx_qe_score": 1.2568646669387817, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,相同的类型或相同的艺术家,例如。", "metrics": {"bleu_score": 22.813997135031535, "chrf_score": 25.467362169434566, "xcomet_score": 0.7856296300888062, "xcomet_qe_score": 0.7377666234970093, "metricx_score": 3.922926425933838, "metricx_qe_score": 4.12598180770874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们将这个替代问题呈现给标注员时,他们知道这些实体的名称,但他们不一定了解关于这些实体的信息。", "metrics": {"bleu_score": 40.92079294124894, "chrf_score": 38.977099981263876, "xcomet_score": 0.8128876686096191, "xcomet_qe_score": 0.8448294401168823, "metricx_score": 2.5906224250793457, "metricx_qe_score": 1.8222196102142334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么我们所做的是展示关于这两个实体的背景知识。", "metrics": {"bleu_score": 72.00242075875518, "chrf_score": 64.66198984505621, "xcomet_score": 0.9668618440628052, "xcomet_qe_score": 0.7808629274368286, "metricx_score": 1.068519115447998, "metricx_qe_score": 1.8098002672195435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们仅仅为每首歌曲显示一个谷歌搜索链接。 然后请批注员们聆听至少部分歌曲,并阅读关于每首歌曲的资料。", "metrics": {"bleu_score": 23.69846677263297, "chrf_score": 22.659189399217535, "xcomet_score": 0.8839730024337769, "xcomet_qe_score": 0.8487156629562378, "metricx_score": 1.2463304996490479, "metricx_qe_score": 1.3802673816680908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,以下是关于歌曲“Easy Annotation”的谷歌搜索结果。", "metrics": {"bleu_score": 51.23350305765596, "chrf_score": 48.90211358427539, "xcomet_score": 0.7970994710922241, "xcomet_qe_score": 0.7873201966285706, "metricx_score": 6.995910167694092, "metricx_qe_score": 7.562361240386963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们会展示来自维基百科的一些背景文本。", "metrics": {"bleu_score": 90.61874434879648, "chrf_score": 86.31885674989124, "xcomet_score": 0.9928807020187378, "xcomet_qe_score": 0.9702156782150269, "metricx_score": 0.684838056564331, "metricx_qe_score": 0.9111669063568115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还会再次展示来自维基百科的图片,以便标注员了解其外观。", "metrics": {"bleu_score": 29.588271804747496, "chrf_score": 25.21887459205668, "xcomet_score": 0.855075478553772, "xcomet_qe_score": 0.885489821434021, "metricx_score": 2.5453639030456543, "metricx_qe_score": 2.352783441543579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注员从中选取一个实体,例如这里第一个实体,并使用三到五个间接指代来描述它。", "metrics": {"bleu_score": 35.65073354236266, "chrf_score": 31.53067720583892, "xcomet_score": 0.8125258088111877, "xcomet_qe_score": 0.8393401503562927, "metricx_score": 2.9688782691955566, "metricx_qe_score": 2.467742919921875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,那个带有钢琴音乐的。", "metrics": {"bleu_score": 16.90062198556585, "chrf_score": 18.4176047252779, "xcomet_score": 0.9872552752494812, "xcomet_qe_score": 0.987647294998169, "metricx_score": 0.4536311626434326, "metricx_qe_score": 0.4618048369884491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些来自我们数据集的例子。", "metrics": {"bleu_score": 41.21183751323024, "chrf_score": 36.44793690849975, "xcomet_score": 0.9751685857772827, "xcomet_qe_score": 0.9732110500335693, "metricx_score": 0.2812017798423767, "metricx_qe_score": 0.32396525144577026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有歌词的那个,而不是那个有12岁男孩的,或者虚构的那个,或者来自亚美尼亚的,等等。", "metrics": {"bleu_score": 29.81933093784166, "chrf_score": 29.76524118509767, "xcomet_score": 0.6547895073890686, "xcomet_qe_score": 0.6177065372467041, "metricx_score": 2.5233922004699707, "metricx_qe_score": 2.6039719581604004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "AltEntities语料库包含来自三个领域共6000个替代问题,并包含42,000个间接指代表达。", "metrics": {"bleu_score": 29.705344069848184, "chrf_score": 50.79190736372238, "xcomet_score": 0.7021734118461609, "xcomet_qe_score": 0.6311532258987427, "metricx_score": 4.144522666931152, "metricx_qe_score": 4.881760597229004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用T5XLARGE模型的实验结果总结如下。", "metrics": {"bleu_score": 17.02147534935789, "chrf_score": 34.92094317189286, "xcomet_score": 0.9469194412231445, "xcomet_qe_score": 0.9367949366569519, "metricx_score": 2.4775872230529785, "metricx_qe_score": 2.3118371963500977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者完全一致的背景知识,那么准确率会非常高。大约在92%到95%之间。", "metrics": {"bleu_score": 54.86694576924462, "chrf_score": 48.23475329929906, "xcomet_score": 0.9140753746032715, "xcomet_qe_score": 0.9938337802886963, "metricx_score": 1.0105912685394287, "metricx_qe_score": 0.7075850367546082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这种情况并不现实。", "metrics": {"bleu_score": 20.556680845025987, "chrf_score": 21.267546355574527, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4691614508628845, "metricx_qe_score": 0.5053394436836243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问部分重叠的背景知识,那么准确率在82%到87%之间,这更为现实。", "metrics": {"bleu_score": 60.00054218637493, "chrf_score": 56.30929265286302, "xcomet_score": 0.910490870475769, "xcomet_qe_score": 0.9024406671524048, "metricx_score": 0.9689956307411194, "metricx_qe_score": 1.5518293380737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型仅能访问实体名称,那么准确率仅为 60%。因此,仍有很大的提升空间。", "metrics": {"bleu_score": 48.21020941754957, "chrf_score": 40.815292384740836, "xcomet_score": 0.9970659017562866, "xcomet_qe_score": 0.990591287612915, "metricx_score": 1.3758649826049805, "metricx_qe_score": 2.2923953533172607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还证明了这些模型具有领域泛化能力。", "metrics": {"bleu_score": 72.52761279126531, "chrf_score": 73.84154328661677, "xcomet_score": 0.9384033679962158, "xcomet_qe_score": 0.9239341616630554, "metricx_score": 0.5843798518180847, "metricx_qe_score": 0.6215623021125793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集链接。", "metrics": {"bleu_score": 29.50234363196404, "chrf_score": 30.119895842275447, "xcomet_score": 0.9908864498138428, "xcomet_qe_score": 0.9903539419174194, "metricx_score": 0.23959046602249146, "metricx_qe_score": 0.38847288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是萨拉·佩佩,来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍《注意力机制作为同步语音翻译的指导》这篇论文,它是与马泰奥·内格里和马可·图奇的合作成果。", "metrics": {"bleu_score": 39.669080688650794, "chrf_score": 32.54717054988193, "xcomet_score": 0.7084674835205078, "xcomet_qe_score": 0.7648057341575623, "metricx_score": 2.59541654586792, "metricx_qe_score": 2.20678448677063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是即时语音翻译?", "metrics": {"bleu_score": 16.784459625186194, "chrf_score": 15.046762428961383, "xcomet_score": 0.9820812940597534, "xcomet_qe_score": 0.9757794141769409, "metricx_score": 0.18114842474460602, "metricx_qe_score": 0.019112005829811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即时语音翻译,或 simulST,是指将口语实时翻译成另一种语言的文本,从而实现跨语言交流的过程。", "metrics": {"bleu_score": 54.36164192498402, "chrf_score": 52.21802173043769, "xcomet_score": 0.9598840475082397, "xcomet_qe_score": 0.944666862487793, "metricx_score": 2.0465173721313477, "metricx_qe_score": 2.9774796962738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前 SimulST 模型存在哪些问题?", "metrics": {"bleu_score": 31.702331385234313, "chrf_score": 51.980640546817014, "xcomet_score": 0.9999988079071045, "xcomet_qe_score": 0.9999916553497314, "metricx_score": 0.3005105257034302, "metricx_qe_score": 0.3743264377117157, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特定的架构通常需要训练,从而引入了额外的模块需要进行优化。", "metrics": {"bleu_score": 39.00948326004268, "chrf_score": 33.97630129235586, "xcomet_score": 0.7835767865180969, "xcomet_qe_score": 0.8035067915916443, "metricx_score": 1.4186151027679443, "metricx_qe_score": 2.2500104904174805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "冗长且复杂的训练流程,例如涉及不同优化目标的训练", "metrics": {"bleu_score": 58.20004455529009, "chrf_score": 54.12675711016672, "xcomet_score": 0.9841777086257935, "xcomet_qe_score": 0.9851117134094238, "metricx_score": 0.5909193158149719, "metricx_qe_score": 0.6394190788269043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ", 以及训练和维护多个模型以达到不同的延迟等", "metrics": {"bleu_score": 49.42587076701001, "chrf_score": 46.38465467546894, "xcomet_score": 0.8993855714797974, "xcomet_qe_score": 0.8992341756820679, "metricx_score": 2.8024303913116455, "metricx_qe_score": 2.138636589050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "级,例如,训练一个平均延迟为1秒的模型,再训练一个平均延迟为2秒的模型,以此类推。", "metrics": {"bleu_score": 44.54561626238501, "chrf_score": 41.3411364812184, "xcomet_score": 0.740714967250824, "xcomet_qe_score": 0.7029001116752625, "metricx_score": 3.0385494232177734, "metricx_qe_score": 4.255899429321289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前两个方案,使用现有的离线 SD 模型,无需重新训练或采用针对单个 SD 的特定架构。", "metrics": {"bleu_score": 26.352230043195874, "chrf_score": 23.306971134615612, "xcomet_score": 0.603683352470398, "xcomet_qe_score": 0.5295223593711853, "metricx_score": 9.382567405700684, "metricx_qe_score": 9.852265357971191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个延迟级别仅使用一个模型,并通过特定参数来处理延迟。", "metrics": {"bleu_score": 57.55914209165234, "chrf_score": 49.605388111813774, "xcomet_score": 0.9994741678237915, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6845325231552124, "metricx_qe_score": 0.829459547996521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并利用注意力机制在音频输入和文本输出之间已有的知识,", "metrics": {"bleu_score": 48.70741881438222, "chrf_score": 44.32437674458692, "xcomet_score": 0.7765330076217651, "xcomet_qe_score": 0.7488275766372681, "metricx_score": 4.123138904571533, "metricx_qe_score": 5.380209922790527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即交叉注意力机制。您可以在右侧看到一个例子。", "metrics": {"bleu_score": 31.27670021100431, "chrf_score": 30.71683276321025, "xcomet_score": 0.9791501760482788, "xcomet_qe_score": 0.8797747492790222, "metricx_score": 0.7803274393081665, "metricx_qe_score": 0.7314524054527283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出 ADAT 或编码器-解码器注意力机制,这是一种策略,根据注意力指向的位置决定是否发出部分译文。 一个", "metrics": {"bleu_score": 54.79456949239228, "chrf_score": 46.39644791473918, "xcomet_score": 0.5153517723083496, "xcomet_qe_score": 0.533015787601471, "metricx_score": 6.827038764953613, "metricx_qe_score": 4.526311874389648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "词语的发出,发生在张力未集中时,也就是说,......其和低于某个阈值α,在语音帧的最后几行,意味着接收到的信息是...... 如果", "metrics": {"bleu_score": 20.633121348847293, "chrf_score": 20.507189161477974, "xcomet_score": 0.3134722113609314, "xcomet_qe_score": 0.24232050776481628, "metricx_score": 12.628353118896484, "metricx_qe_score": 10.829766273498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们收到一个包含“I'm going to talk about”的语音片段,而我们的模型预测其翻译为德语, 我们将观察交叉注意力权重。 我们将看到,前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,作为λ语音帧。", "metrics": {"bleu_score": 52.052532991925176, "chrf_score": 51.277022726636176, "xcomet_score": 0.5373919010162354, "xcomet_qe_score": 0.4822368919849396, "metricx_score": 5.039745807647705, "metricx_qe_score": 5.8415656089782715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被省略。 在交叉注意力之和超过某个阈值α时,我们将不会输出最后一个词,而是等待另一个语音片段。", "metrics": {"bleu_score": 45.56414533784337, "chrf_score": 37.77596051199455, "xcomet_score": 0.7617307901382446, "xcomet_qe_score": 0.7543691396713257, "metricx_score": 2.2618579864501953, "metricx_qe_score": 2.6858623027801514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续,并且接收到另一段语音片段,我们的模型预测出另外三个词,那么我们将查看交叉注意力权重。 我们将看到,没有任何词语指向最后的 Lambda 语音帧。", "metrics": {"bleu_score": 42.558543917111685, "chrf_score": 37.1495105614285, "xcomet_score": 0.7760865688323975, "xcomet_qe_score": 0.7072058916091919, "metricx_score": 3.352583408355713, "metricx_qe_score": 3.6655051708221436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将会被输出。", "metrics": {"bleu_score": 30.79300751569293, "chrf_score": 29.446692174998624, "xcomet_score": 0.9968873262405396, "xcomet_qe_score": 0.9709671139717102, "metricx_score": 0.6358075141906738, "metricx_qe_score": 1.052427053451538, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们审视其中的主要结果, 我们在图表上绘制了同步语音翻译的结果,图表一侧为蓝色,用于衡量翻译质量和平均延迟。 这就是延迟度量。 我们还考虑了计算感知型平均滞后,该滞后考虑了模型预测输出所需的计算时间。", "metrics": {"bleu_score": 37.835254504606574, "chrf_score": 30.893932624627716, "xcomet_score": 0.7456276416778564, "xcomet_qe_score": 0.5729652643203735, "metricx_score": 5.757453918457031, "metricx_qe_score": 6.058567523956299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望在此图上看到尽可能高的曲线。", "metrics": {"bleu_score": 32.487115057647685, "chrf_score": 29.625612184642097, "xcomet_score": 0.9728506803512573, "xcomet_qe_score": 0.8813162446022034, "metricx_score": 1.1087039709091187, "metricx_qe_score": 1.607586145401001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但我们也希望它们向左移动。 并且", "metrics": {"bleu_score": 79.16963878457506, "chrf_score": 81.95633249348361, "xcomet_score": 0.8154351711273193, "xcomet_qe_score": 0.7879092693328857, "metricx_score": 3.8696701526641846, "metricx_qe_score": 1.4660756587982178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与合适的策略进行比较,这些策略也适用于离线模型,例如湿键策略和局部协议。", "metrics": {"bleu_score": 40.749798526834816, "chrf_score": 29.209123981169476, "xcomet_score": 0.6840246319770813, "xcomet_qe_score": 0.6562831997871399, "metricx_score": 5.4803547859191895, "metricx_qe_score": 5.811044216156006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将其与专为同时预翻译而设计的最先进架构进行比较。", "metrics": {"bleu_score": 51.473464105383584, "chrf_score": 48.8496012745927, "xcomet_score": 0.9089033603668213, "xcomet_qe_score": 0.8919056057929993, "metricx_score": 1.2459758520126343, "metricx_qe_score": 1.44027841091156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是在德语上应用同声语音翻译策略所得到的结果。", "metrics": {"bleu_score": 31.961036230236623, "chrf_score": 32.50730399309697, "xcomet_score": 0.9879653453826904, "xcomet_qe_score": 0.9113150835037231, "metricx_score": 0.9980725049972534, "metricx_qe_score": 0.8806972503662109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,由于曲线向左偏移,它在所有应用于离线模型的策略中表现出优越性。", "metrics": {"bleu_score": 31.63533778652485, "chrf_score": 32.502909747602644, "xcomet_score": 0.9940458536148071, "xcomet_qe_score": 0.983735203742981, "metricx_score": 1.2870128154754639, "metricx_qe_score": 1.4434819221496582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且我们也能看到,如果考虑到实际经过的时间或计算感知的时间,那将是最快的策略。", "metrics": {"bleu_score": 25.540636397110397, "chrf_score": 26.605329714021302, "xcomet_score": 0.8750507831573486, "xcomet_qe_score": 0.8743272423744202, "metricx_score": 3.689377546310425, "metricx_qe_score": 3.5760488510131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您希望了解更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 67.12403123245673, "chrf_score": 60.26661027672281, "xcomet_score": 0.9963029623031616, "xcomet_qe_score": 0.9755913019180298, "metricx_score": 0.07378308475017548, "metricx_qe_score": 0.1706973761320114, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码、模型以及同步输出,以促进我们工作的可重复性。", "metrics": {"bleu_score": 48.112646593336386, "chrf_score": 47.9482370821196, "xcomet_score": 0.9647890329360962, "xcomet_qe_score": 0.9116915464401245, "metricx_score": 0.7859927415847778, "metricx_qe_score": 1.1357274055480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫英,我和我的同事智阳将为大家介绍我们的研究课题:指令微调下的多模态串行短序列学习的多项改进。", "metrics": {"bleu_score": 24.338780315511574, "chrf_score": 18.149702961913654, "xcomet_score": 0.5342139601707458, "xcomet_qe_score": 0.5481425523757935, "metricx_score": 4.910231590270996, "metricx_qe_score": 5.276699066162109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索新的学习范式,即以参数和数据高效的方式,将预训练语言模型应用于不同的下游任务。 近", "metrics": {"bleu_score": 71.40613782622218, "chrf_score": 63.56300312789015, "xcomet_score": 0.8158971071243286, "xcomet_qe_score": 0.8055201768875122, "metricx_score": 3.8662405014038086, "metricx_qe_score": 2.596947431564331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期,许多研究表明,指令微调能够使大型语言模型在零样本条件下,通过遵循自然指令来执行未见过的任务。", "metrics": {"bleu_score": 40.52894287433329, "chrf_score": 34.82436794930858, "xcomet_score": 0.6764539480209351, "xcomet_qe_score": 0.484180212020874, "metricx_score": 4.637915134429932, "metricx_qe_score": 5.7635416984558105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数以往的研究主要集中于提升语言任务中序列图表的性能,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 38.93859688487343, "chrf_score": 33.32017224806379, "xcomet_score": 0.8340301513671875, "xcomet_qe_score": 0.8176853060722351, "metricx_score": 1.9431755542755127, "metricx_qe_score": 2.285776138305664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们旨在探讨是否通过在多模态预训练模型上进行指令微调,能够切实提高其在未见过的多模态任务上的泛化能力。", "metrics": {"bleu_score": 28.905046373391162, "chrf_score": 28.012684542726614, "xcomet_score": 0.904678225517273, "xcomet_qe_score": 0.8895776271820068, "metricx_score": 0.9392980337142944, "metricx_qe_score": 1.2361881732940674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现自然语言处理(NLP)和多模态领域在指令数据集的可获得性方面存在相当大的差异。 目前已", "metrics": {"bleu_score": 39.31511091540294, "chrf_score": 39.45885086821652, "xcomet_score": 0.8340169191360474, "xcomet_qe_score": 0.8393670320510864, "metricx_score": 5.347620010375977, "metricx_qe_score": 1.4535152912139893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过1600个仅使用语言指令的任务。", "metrics": {"bleu_score": 34.7403173905042, "chrf_score": 48.255428726637454, "xcomet_score": 0.9725304841995239, "xcomet_qe_score": 0.8663028478622437, "metricx_score": 0.7733029127120972, "metricx_qe_score": 0.8608822822570801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,缺乏大规模公开可用的多模态指令任务。", "metrics": {"bleu_score": 45.50680330812803, "chrf_score": 43.29779944108079, "xcomet_score": 0.9733994007110596, "xcomet_qe_score": 0.8717736005783081, "metricx_score": 1.6146085262298584, "metricx_qe_score": 2.1741228103637695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这促使我们构建一个多模态指令微调数据集。", "metrics": {"bleu_score": 56.687387042197564, "chrf_score": 51.58148802464252, "xcomet_score": 0.9755637645721436, "xcomet_qe_score": 0.9603803157806396, "metricx_score": 0.9388285875320435, "metricx_qe_score": 0.8105782270431519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在此介绍 Multi-Instruct,这是第一个多模态指令微调基准数据集,包含 10 个大类下的 62 个多样化的多模态任务。", "metrics": {"bleu_score": 37.513043937963964, "chrf_score": 39.4687218990766, "xcomet_score": 0.8740749359130859, "xcomet_qe_score": 0.8649780750274658, "metricx_score": 1.4430636167526245, "metricx_qe_score": 1.6964755058288574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务源自 21 个现有的开源数据集,并且每个任务都配备了 5 条专家编写的指令。", "metrics": {"bleu_score": 67.34493290339738, "chrf_score": 67.9183441774771, "xcomet_score": 0.9941753149032593, "xcomet_qe_score": 0.9803396463394165, "metricx_score": 0.8883050680160522, "metricx_qe_score": 1.1665072441101074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究我们在所提出的数据集上的多模态指令微调,我们以 OFA 作为基础模型,OFA 是一种统一的多模态预训练模型。OFA 使用统一的词", "metrics": {"bleu_score": 49.53106161016375, "chrf_score": 52.51179801165219, "xcomet_score": 0.685546875, "xcomet_qe_score": 0.5611684322357178, "metricx_score": 6.368216514587402, "metricx_qe_score": 3.4673969745635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表,用于语言、图像标记以及边界框的坐标。", "metrics": {"bleu_score": 36.28680487624711, "chrf_score": 27.658935452984434, "xcomet_score": 0.49394920468330383, "xcomet_qe_score": 0.27804774045944214, "metricx_score": 7.611135005950928, "metricx_qe_score": 7.712471008300781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们展示来自我们的 Multi-Instra 数据集的几个示例。 为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 51.73122270432148, "chrf_score": 44.65156215950215, "xcomet_score": 0.833284854888916, "xcomet_qe_score": 0.7990803718566895, "metricx_score": 3.7026877403259277, "metricx_qe_score": 5.169206142425537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,并将所有任务以统一的序列到序列格式进行表述,", "metrics": {"bleu_score": 62.36555681133301, "chrf_score": 62.58990414758554, "xcomet_score": 0.8550317287445068, "xcomet_qe_score": 0.8117603063583374, "metricx_score": 1.6992576122283936, "metricx_qe_score": 2.5520589351654053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中输入文本、图像、指令和边界框都以相同的token空间进行表示。", "metrics": {"bleu_score": 64.37378993486212, "chrf_score": 58.620890048784105, "xcomet_score": 0.874413788318634, "xcomet_qe_score": 0.8501617908477783, "metricx_score": 3.0698370933532715, "metricx_qe_score": 3.1304502487182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我将讨论多模态指令调优。 因此", "metrics": {"bleu_score": 35.27295712700594, "chrf_score": 32.34023118731893, "xcomet_score": 0.7405747175216675, "xcomet_qe_score": 0.7143118381500244, "metricx_score": 3.2837319374084473, "metricx_qe_score": 0.7791302800178528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于训练数据集,我们使用来自9个组中的53个任务进行训练,并且每个任务抽取10,000个样本。", "metrics": {"bleu_score": 60.6519866086662, "chrf_score": 60.309057151885106, "xcomet_score": 0.9252228736877441, "xcomet_qe_score": 0.9378803372383118, "metricx_score": 1.8425291776657104, "metricx_qe_score": 3.34077787399292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识推理组用于测试,并从视觉问答(VQA)和杂项组中额外选择5个任务。", "metrics": {"bleu_score": 32.25845582080583, "chrf_score": 29.44182804983887, "xcomet_score": 0.750209391117096, "xcomet_qe_score": 0.6734704971313477, "metricx_score": 2.591464042663574, "metricx_qe_score": 2.8326845169067383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用测试集中的所有样本来进行每个任务。", "metrics": {"bleu_score": 49.42244290934039, "chrf_score": 38.54383954881783, "xcomet_score": 0.8220137357711792, "xcomet_qe_score": 0.7578272819519043, "metricx_score": 3.1142866611480713, "metricx_qe_score": 2.73362398147583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令测试集中随机抽取20个任务,作为未见过的任务用于NLP。 因此,", "metrics": {"bleu_score": 42.98470783177618, "chrf_score": 42.0262697590025, "xcomet_score": 0.571434736251831, "xcomet_qe_score": 0.5803922414779663, "metricx_score": 5.544408798217773, "metricx_qe_score": 4.437596321105957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的OFA大型模型作为基础模型。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9649795889854431, "xcomet_qe_score": 0.9640771746635437, "metricx_score": 1.2312500476837158, "metricx_qe_score": 2.793785810470581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 53.884478712336644, "xcomet_score": 0.969638466835022, "xcomet_qe_score": 0.8911672830581665, "metricx_score": 0.8351970911026001, "metricx_qe_score": 1.3651412725448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例会随机地与其中一个由其定义的五个指令模板结合。", "metrics": {"bleu_score": 33.03446603562513, "chrf_score": 35.49732435178314, "xcomet_score": 0.887056827545166, "xcomet_qe_score": 0.8216437101364136, "metricx_score": 1.4643319845199585, "metricx_qe_score": 1.2278958559036255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在测试过程中,针对每个任务,我们共进行五次实验,每次实验使用五条指", "metrics": {"bleu_score": 22.423831599202828, "chrf_score": 20.368484355521677, "xcomet_score": 0.7267926931381226, "xcomet_qe_score": 0.7374621629714966, "metricx_score": 5.636458873748779, "metricx_qe_score": 3.548637628555298, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "令中的一条来评估模型。 我们报告了所有五个实验中,表现的平均值、最大值以及标准差。", "metrics": {"bleu_score": 23.303171053050654, "chrf_score": 21.79842416591763, "xcomet_score": 0.2952876389026642, "xcomet_qe_score": 0.18893250823020935, "metricx_score": 5.376513957977295, "metricx_qe_score": 5.740522384643555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们将报告准确率。如果", "metrics": {"bleu_score": 58.94159589207008, "chrf_score": 52.621716207996094, "xcomet_score": 0.7292653322219849, "xcomet_qe_score": 0.7893196940422058, "metricx_score": 3.1914992332458496, "metricx_qe_score": 0.798486053943634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是多模态生成任务,我们将报告ROUGE-L。对于NLP任务,我们也将报告ROUGE-L。", "metrics": {"bleu_score": 46.69560045812298, "chrf_score": 41.2221034321328, "xcomet_score": 0.7849236726760864, "xcomet_qe_score": 0.8364167213439941, "metricx_score": 3.1950645446777344, "metricx_qe_score": 3.1824676990509033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为“灵敏度”。", "metrics": {"bleu_score": 88.43865924896839, "chrf_score": 82.97912518450275, "xcomet_score": 0.9919569492340088, "xcomet_qe_score": 0.9037202596664429, "metricx_score": 0.5232164263725281, "metricx_qe_score": 0.6006287336349487, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这衡量了模型在面对指令中细微措辞变化时,始终如一地产生相同输出的能力。", "metrics": {"bleu_score": 30.56654245008761, "chrf_score": 27.885961830786833, "xcomet_score": 0.9147177934646606, "xcomet_qe_score": 0.9575661420822144, "metricx_score": 2.2297165393829346, "metricx_qe_score": 3.0838441848754883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要结果。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9698837995529175, "xcomet_qe_score": 0.88059002161026, "metricx_score": 0.1918793022632599, "metricx_qe_score": 0.3046000003814697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,指令调优可以显著提升OFA在场景多模态任务上的表现。", "metrics": {"bleu_score": 44.344953790409605, "chrf_score": 35.93730406234506, "xcomet_score": 0.9044840335845947, "xcomet_qe_score": 0.9376909732818604, "metricx_score": 2.04194974899292, "metricx_qe_score": 2.677401542663574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "迁移学习也可以从自然指令数据集受益,从而提升指令微调的效", "metrics": {"bleu_score": 38.11228251371368, "chrf_score": 33.212085149985235, "xcomet_score": 0.7670708894729614, "xcomet_qe_score": 0.7101868391036987, "metricx_score": 2.9895541667938232, "metricx_qe_score": 2.9833106994628906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "果。 随着任务量的增加,模型表现出更好的性能,同时降低了对任务的敏感度。", "metrics": {"bleu_score": 18.184374923209077, "chrf_score": 17.692795025421727, "xcomet_score": 0.46756210923194885, "xcomet_qe_score": 0.47674742341041565, "metricx_score": 5.45835018157959, "metricx_qe_score": 5.389947414398193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们也进行了一个实验,", "metrics": {"bleu_score": 30.576902884505124, "chrf_score": 31.729540811655056, "xcomet_score": 0.919492781162262, "xcomet_qe_score": 0.9328995943069458, "metricx_score": 0.4526246190071106, "metricx_qe_score": 0.3162057399749756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了单一指令与五条指令进行对比。", "metrics": {"bleu_score": 31.32768114661938, "chrf_score": 31.213508790012913, "xcomet_score": 0.8937933444976807, "xcomet_qe_score": 0.8319070935249329, "metricx_score": 0.9561474919319153, "metricx_qe_score": 1.7757105827331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,使用更多的指令可以提升模型的整体性能,并显著降低其敏感性。", "metrics": {"bleu_score": 46.35369955832236, "chrf_score": 39.02257253355118, "xcomet_score": 0.9471521377563477, "xcomet_qe_score": 1.0, "metricx_score": 0.9460201859474182, "metricx_qe_score": 1.156005620956421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明了不同微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 47.18480720883561, "chrf_score": 38.93963712998088, "xcomet_score": 0.9875224828720093, "xcomet_qe_score": 0.9857796430587769, "metricx_score": 1.2600884437561035, "metricx_qe_score": 1.4240033626556396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,通过从自然指令数据集进行迁移学习,该模型可以实现比原始OFA模型更高的敏感度。", "metrics": {"bleu_score": 46.08469837927568, "chrf_score": 40.56825384586469, "xcomet_score": 0.9612867832183838, "xcomet_qe_score": 0.9098959565162659, "metricx_score": 1.7375627756118774, "metricx_qe_score": 2.5835981369018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从Nitro指令数据集迁移学习能够帮助OFA在Nitro指令数据集上取得显著更好的性能。", "metrics": {"bleu_score": 44.637775983451604, "chrf_score": 36.89235524850528, "xcomet_score": 0.6697337031364441, "xcomet_qe_score": 0.6359548568725586, "metricx_score": 7.044978141784668, "metricx_qe_score": 7.625575065612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们提出了首个大规模多模型指令微调数据集。我们显著提升了 OFV 的零样本能力,并探索了不同的迁移学习技术,展示了它们的优势。", "metrics": {"bleu_score": 48.018657820363394, "chrf_score": 42.00328670317674, "xcomet_score": 0.7728304862976074, "xcomet_qe_score": 0.7452919483184814, "metricx_score": 3.651477813720703, "metricx_qe_score": 4.083985328674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标,称为“敏感性”。", "metrics": {"bleu_score": 62.44451680575339, "chrf_score": 56.63090528609105, "xcomet_score": 0.9217984676361084, "xcomet_qe_score": 0.9081961512565613, "metricx_score": 0.6316529512405396, "metricx_qe_score": 0.7261589169502258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另外,我们正在收集一个更大规模的多模态指令微调数据集,其中包含约150个额外的变体语言任务,并且我们将会发布它们。", "metrics": {"bleu_score": 42.02708823301905, "chrf_score": 38.15403294276987, "xcomet_score": 0.7388367652893066, "xcomet_qe_score": 0.7401236295700073, "metricx_score": 2.401977300643921, "metricx_qe_score": 2.4602949619293213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯塔夫·辛哈,很高兴欢迎大家参加关于我们ACL 2023论文的研讨会,该论文", "metrics": {"bleu_score": 38.07134866446316, "chrf_score": 40.75167933694795, "xcomet_score": 0.6709822416305542, "xcomet_qe_score": 0.6584166884422302, "metricx_score": 5.435933589935303, "metricx_qe_score": 2.8006784915924072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "题为“语言模型可接受性判断并非总是对上下文稳健”(Language Model Acceptability Judgements Are Not Always Robust to Context)。", "metrics": {"bleu_score": 34.39204550030358, "chrf_score": 25.937871934899626, "xcomet_score": 0.8123321533203125, "xcomet_qe_score": 0.797532856464386, "metricx_score": 4.9701409339904785, "metricx_qe_score": 5.132128715515137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这篇工作是与John Gauthier、Aaron Muller、Kanishka Mishra、Karen Fuentes、Roger Levy及Adina Williams共同完成的。", "metrics": {"bleu_score": 22.133405414470136, "chrf_score": 70.61659352070075, "xcomet_score": 0.7938634157180786, "xcomet_qe_score": 0.7340293526649475, "metricx_score": 2.031341075897217, "metricx_qe_score": 1.8347821235656738, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们重新审视了极小对模型。", "metrics": {"bleu_score": 40.1577332834242, "chrf_score": 34.38574991400069, "xcomet_score": 0.9076732993125916, "xcomet_qe_score": 0.9012276530265808, "metricx_score": 1.0819779634475708, "metricx_qe_score": 1.7558722496032715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最简单的配对范式本质上是在可接受性判断的基础上评估语言模型,这", "metrics": {"bleu_score": 34.55883870626298, "chrf_score": 35.108743274891374, "xcomet_score": 0.7044932842254639, "xcomet_qe_score": 0.6511145830154419, "metricx_score": 5.974274158477783, "metricx_qe_score": 1.3232064247131348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括类似于“飞艇”(blimp)的语法性判断,或者像“宝石”(gem)这样的句法判断,还包括在刻板印象方面的可接受性,例如交叉配对。", "metrics": {"bleu_score": 18.515011636909705, "chrf_score": 16.834183114057904, "xcomet_score": 0.42800793051719666, "xcomet_qe_score": 0.44239744544029236, "metricx_score": 6.461042404174805, "metricx_qe_score": 7.050229072570801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个最小词对范式中,评估语言模型的典型方法是,展示一个可接受的句子或语法正确的句子,然后展示另一个可接受的句子或一个语法错误的句子。", "metrics": {"bleu_score": 53.92790415601597, "chrf_score": 48.30902769725964, "xcomet_score": 0.7611115574836731, "xcomet_qe_score": 0.6775545477867126, "metricx_score": 1.0829262733459473, "metricx_qe_score": 1.2270478010177612, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望这个模型能够基本地将更高的概率赋予可接受的句子。", "metrics": {"bleu_score": 26.839613470694868, "chrf_score": 26.820841740490792, "xcomet_score": 0.9031511545181274, "xcomet_qe_score": 0.7071546316146851, "metricx_score": 1.9662775993347168, "metricx_qe_score": 2.4514920711517334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前MPP流程基本不允许我们评估模型对长句的接受程度。", "metrics": {"bleu_score": 44.078944889507554, "chrf_score": 40.28275881506476, "xcomet_score": 0.8850443363189697, "xcomet_qe_score": 0.8615027666091919, "metricx_score": 1.443297266960144, "metricx_qe_score": 2.0264999866485596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型正不断扩展其上下文窗口。", "metrics": {"bleu_score": 32.28213880040184, "chrf_score": 27.335293578151532, "xcomet_score": 0.9388750791549683, "xcomet_qe_score": 0.9121244549751282, "metricx_score": 0.8926571607589722, "metricx_qe_score": 0.8055585622787476, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在整个上下文窗口范围内评估模型的可靠性至关重要。 而我们正在尝试做的是这件事情。", "metrics": {"bleu_score": 38.242335793997, "chrf_score": 32.762681394302454, "xcomet_score": 0.8912911415100098, "xcomet_qe_score": 0.8477851748466492, "metricx_score": 1.5764058828353882, "metricx_qe_score": 1.7866153717041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图重新审视MPP流水线,通过要求模型评估越来越长的序列的可接受性。", "metrics": {"bleu_score": 70.67558330951775, "chrf_score": 69.29584454575823, "xcomet_score": 0.7923235297203064, "xcomet_qe_score": 0.7892932891845703, "metricx_score": 3.750499725341797, "metricx_qe_score": 4.027235984802246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那就是我们的方法。", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 83.40608465608467, "xcomet_score": 0.9929481744766235, "xcomet_qe_score": 0.9658809900283813, "metricx_score": 0.41145560145378113, "metricx_qe_score": 0.972753643989563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们将模拟这些更长的序列。 我们会重新审视数据集本身, 然后从这些数据集中选择可接受或不可接受的句子来重构句子。", "metrics": {"bleu_score": 71.03221507854116, "chrf_score": 65.96893156174023, "xcomet_score": 0.8249752521514893, "xcomet_qe_score": 0.6156511902809143, "metricx_score": 2.3956336975097656, "metricx_qe_score": 3.7912967205047607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,举例来说,我们这里选取了来自“飞艇”数据集的一个典型语法性判断对,来自附例岛的案例。 而", "metrics": {"bleu_score": 6.581864925936314, "chrf_score": 10.39796352826329, "xcomet_score": 0.2773515284061432, "xcomet_qe_score": 0.35856127738952637, "metricx_score": 6.807429790496826, "metricx_qe_score": 5.545510768890381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重现更长的序列,这些序列是可接受的,并且具有相同的语法结构匹配,为此", "metrics": {"bleu_score": 66.47291074562574, "chrf_score": 57.9935436079448, "xcomet_score": 0.7992469072341919, "xcomet_qe_score": 0.7307473421096802, "metricx_score": 4.471195697784424, "metricx_qe_score": 2.712940216064453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从阿根廷岛提取语法句子。 然后我们将它作为前缀添加到可接受的查询和不可接受的查询中。 因此,", "metrics": {"bleu_score": 63.27756394638042, "chrf_score": 53.70041828592602, "xcomet_score": 0.4987321197986603, "xcomet_qe_score": 0.3859170973300934, "metricx_score": 7.038179397583008, "metricx_qe_score": 8.295038223266602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过选择同一匹配中不被接受的句子来完成相同的操作。这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 60.39078862338857, "chrf_score": 53.210176539860385, "xcomet_score": 0.9456799030303955, "xcomet_qe_score": 0.7393392324447632, "metricx_score": 1.4394934177398682, "metricx_qe_score": 1.9696712493896484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过选择来自不同子集或不同数据集的句子来达到同样的效果。", "metrics": {"bleu_score": 42.51168651124796, "chrf_score": 35.74800222950947, "xcomet_score": 0.9847507476806641, "xcomet_qe_score": 0.9489040374755859, "metricx_score": 0.626665472984314, "metricx_qe_score": 0.8815375566482544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这便是我们所说的失配场景。", "metrics": {"bleu_score": 54.844980922047604, "chrf_score": 43.64531187729547, "xcomet_score": 0.8955415487289429, "xcomet_qe_score": 0.855544924736023, "metricx_score": 0.6380152702331543, "metricx_qe_score": 1.1368763446807861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里,句子仍然来自相关的语料库,但并非与您进行评估的那个语料库。", "metrics": {"bleu_score": 26.662193160065314, "chrf_score": 23.464823482861004, "xcomet_score": 0.8598579168319702, "xcomet_qe_score": 0.7634294629096985, "metricx_score": 2.012648820877075, "metricx_qe_score": 2.024994373321533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对不可接受的情况做同样的处理。", "metrics": {"bleu_score": 46.3325745127537, "chrf_score": 38.854396484114986, "xcomet_score": 0.9886739253997803, "xcomet_qe_score": 0.9088376760482788, "metricx_score": 0.6087870597839355, "metricx_qe_score": 0.7313862442970276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全无关的领域,例如维基百科,选取句子。", "metrics": {"bleu_score": 46.761565469902926, "chrf_score": 37.77532454695262, "xcomet_score": 0.9907634258270264, "xcomet_qe_score": 0.9810594320297241, "metricx_score": 0.7519471645355225, "metricx_qe_score": 1.1910622119903564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将会告诉我们,模型的可接受性判断是否真的受到任何语境的影响。 是否上下文来源于数据集的不同子集,或者它与我们正在考察的句子完全无关。", "metrics": {"bleu_score": 45.18011884568588, "chrf_score": 38.58311935951695, "xcomet_score": 0.8026876449584961, "xcomet_qe_score": 0.8051886558532715, "metricx_score": 3.696978807449341, "metricx_qe_score": 3.6805288791656494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么这个模型表现如何呢?", "metrics": {"bleu_score": 9.31820340353328, "chrf_score": 9.3352899793343, "xcomet_score": 0.8783203363418579, "xcomet_qe_score": 0.9474695920944214, "metricx_score": 0.9336682558059692, "metricx_qe_score": 0.19702556729316711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先我们观察维基百科中的句子,这些句子与当前的查询对完全无关。在那里我们发现,MPP 判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 58.14307369682194, "chrf_score": 52.14903391003739, "xcomet_score": 0.9411035776138306, "xcomet_qe_score": 0.8089466094970703, "metricx_score": 4.263486385345459, "metricx_qe_score": 5.96912956237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们逐步增加上下文长度,最高可达 1024,以充分利用 OPT 和 GPT-2 模型。 并且,正", "metrics": {"bleu_score": 18.78921507781289, "chrf_score": 36.66061253267047, "xcomet_score": 0.3847205340862274, "xcomet_qe_score": 0.4622458815574646, "metricx_score": 5.95904016494751, "metricx_qe_score": 4.735111713409424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如橙色虚线所示,MPP 判断结果相对稳定。", "metrics": {"bleu_score": 40.335820725998886, "chrf_score": 35.708490998738675, "xcomet_score": 0.974764347076416, "xcomet_qe_score": 0.8797199130058289, "metricx_score": 1.0289918184280396, "metricx_qe_score": 2.3164381980895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,当我们在同一个数据集里选择句子时,会发生什么呢?", "metrics": {"bleu_score": 37.011751896357886, "chrf_score": 31.578885776110205, "xcomet_score": 0.9935535192489624, "xcomet_qe_score": 0.9656791687011719, "metricx_score": 0.7422853708267212, "metricx_qe_score": 1.2587147951126099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们正在从同一气球(blimp)或语法宝石(syntax gem)数据集的接受和未接受域中选择或构建句子。", "metrics": {"bleu_score": 21.176535098798336, "chrf_score": 21.644094056977167, "xcomet_score": 0.4210726022720337, "xcomet_qe_score": 0.4263717234134674, "metricx_score": 6.7311248779296875, "metricx_qe_score": 6.00529670715332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们观察到,当添加可接受的前缀或不可接受的前缀时,MPP判断值会显著增加或减少。", "metrics": {"bleu_score": 62.537191942138264, "chrf_score": 58.65608616458909, "xcomet_score": 0.9554705619812012, "xcomet_qe_score": 0.9556962251663208, "metricx_score": 1.8261133432388306, "metricx_qe_score": 1.6980164051055908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,也就是当我们从指责方文本中选择与同一种现象相关的句子时,吉姆, 我们观察到,对于模型而言,MPP判断会出现巨大的增长或巨大的下降,这取决于所选的前缀是否可接受或不可接受。", "metrics": {"bleu_score": 25.07489434821488, "chrf_score": 23.659375992700195, "xcomet_score": 0.5193371772766113, "xcomet_qe_score": 0.521331250667572, "metricx_score": 8.622991561889648, "metricx_qe_score": 9.177995681762695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,这一点很重要,非常显著,就像这个效应会随着上下文长度的增加而不断扩大。这很可能影响到那些具有更大上下文窗口的较新语言模型。", "metrics": {"bleu_score": 26.471646029748513, "chrf_score": 33.09677693480405, "xcomet_score": 0.7118964195251465, "xcomet_qe_score": 0.6750342845916748, "metricx_score": 2.318838596343994, "metricx_qe_score": 2.106076717376709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么匹配前缀会对语言模型判断产生如此大的影响呢? 因此,", "metrics": {"bleu_score": 73.07034817630456, "chrf_score": 71.82774069740196, "xcomet_score": 0.8351172804832458, "xcomet_qe_score": 0.7697756290435791, "metricx_score": 3.1909995079040527, "metricx_qe_score": 3.2400548458099365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,尝试通过保留相关结构并引入噪声来改造输入句子。", "metrics": {"bleu_score": 46.32916786060582, "chrf_score": 42.0020773361561, "xcomet_score": 0.8568776845932007, "xcomet_qe_score": 0.8569034337997437, "metricx_score": 2.1599514484405518, "metricx_qe_score": 2.196089029312134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行多次这种扰动之后, 我们发现这些噪音实际上并没有导致模型,呃,改变其 MPP 判断趋势展示方式。", "metrics": {"bleu_score": 28.32221829532684, "chrf_score": 26.739629241377187, "xcomet_score": 0.739788830280304, "xcomet_qe_score": 0.7305505871772766, "metricx_score": 4.625616550445557, "metricx_qe_score": 5.431350231170654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动和句子以相似的方式敏感。", "metrics": {"bleu_score": 19.06078747933291, "chrf_score": 20.56274284848369, "xcomet_score": 0.7887122631072998, "xcomet_qe_score": 0.7898812294006348, "metricx_score": 3.3977432250976562, "metricx_qe_score": 4.381906986236572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们对可接受的范畴内的句子进行扰动时,我们观察到所有扰动都呈现出相似的增幅。而当我们对不可接受的范畴内的句子进行扰动时,我们观察到 MPP 判断呈现出类似的方式下降。", "metrics": {"bleu_score": 28.400709628524833, "chrf_score": 27.337186811460963, "xcomet_score": 0.7897603511810303, "xcomet_qe_score": 0.6921026706695557, "metricx_score": 3.0589401721954346, "metricx_qe_score": 4.212677955627441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们的研究主要结论是,语言模型对潜在的句法和语义特征具有敏感性,这些特征在句子间共享。", "metrics": {"bleu_score": 35.54186279663236, "chrf_score": 34.38114974792538, "xcomet_score": 0.980056881904602, "xcomet_qe_score": 0.9790159463882446, "metricx_score": 1.1720315217971802, "metricx_qe_score": 1.451537847518921, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而现有的MPP评估方式,仅使用简短、单句输入,可能无法完全捕捉到语言模型在上下文窗口中所蕴含的抽象知识。", "metrics": {"bleu_score": 42.25807237194314, "chrf_score": 34.60684713055981, "xcomet_score": 0.9464623928070068, "xcomet_qe_score": 0.8691680431365967, "metricx_score": 1.6007213592529297, "metricx_qe_score": 1.8712247610092163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以获取更多实验细节。", "metrics": {"bleu_score": 32.28475421040683, "chrf_score": 31.292934579805326, "xcomet_score": 0.99491286277771, "xcomet_qe_score": 0.9996927976608276, "metricx_score": 0.1711377501487732, "metricx_qe_score": 0.17318548262119293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫张宇生,来自宾夕法尼亚州立大学。", "metrics": {"bleu_score": 54.38653754915954, "chrf_score": 38.48205649601838, "xcomet_score": 0.9266303777694702, "xcomet_qe_score": 0.8835706114768982, "metricx_score": 0.48190611600875854, "metricx_qe_score": 0.7936123609542847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,题为《多语言跨语言语义解析与极简表示》。 因此", "metrics": {"bleu_score": 42.08083992698088, "chrf_score": 32.40965952707777, "xcomet_score": 0.6233991384506226, "xcomet_qe_score": 0.6868046522140503, "metricx_score": 5.618010520935059, "metricx_qe_score": 4.780335426330566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语义解析的任务是构建用户查询的语义表示,例如 SQL 和 Lambda 演算。", "metrics": {"bleu_score": 60.275234875721274, "chrf_score": 56.10579233250673, "xcomet_score": 0.9705613851547241, "xcomet_qe_score": 0.9609073400497437, "metricx_score": 1.9336448907852173, "metricx_qe_score": 2.3273391723632812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而跨语言语义解析的任务是将多种自然语言中的查询翻译成多种含义表示形式。 吴昊霆,博士:", "metrics": {"bleu_score": 50.01406535841676, "chrf_score": 54.81390729333212, "xcomet_score": 0.5011889934539795, "xcomet_qe_score": 0.5112578868865967, "metricx_score": 5.203945159912109, "metricx_qe_score": 5.544961452484131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将查询翻译成多种自然语言,进而转化为sequent lambda或fun QL等。", "metrics": {"bleu_score": 45.21618325059578, "chrf_score": 45.67187853909003, "xcomet_score": 0.8753883838653564, "xcomet_qe_score": 0.8588215112686157, "metricx_score": 5.710419654846191, "metricx_qe_score": 6.074646472930908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型通常是独立提出的,并在有限的任务和应用数据集上进行评估。", "metrics": {"bleu_score": 57.56018903428857, "chrf_score": 55.47915817511559, "xcomet_score": 0.9997454881668091, "xcomet_qe_score": 1.0, "metricx_score": 0.3809666633605957, "metricx_qe_score": 0.5198774337768555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如, 某些自然语言方面存在覆盖范围的缺失。", "metrics": {"bleu_score": 29.52818889864951, "chrf_score": 32.01070965599587, "xcomet_score": 0.7601315379142761, "xcomet_qe_score": 0.7076427936553955, "metricx_score": 4.5089111328125, "metricx_qe_score": 4.431729316711426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文内容缺失。 由于对某些小型表示形式的覆盖。 λ", "metrics": {"bleu_score": 9.59330328254962, "chrf_score": 10.726496558505524, "xcomet_score": 0.5264726877212524, "xcomet_qe_score": 0.4208851754665375, "metricx_score": 8.779876708984375, "metricx_qe_score": 8.17101001739502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "演算缺失了。 或者,它们仅仅在特定的神经网络模型上进行评估。", "metrics": {"bleu_score": 30.292633046214583, "chrf_score": 28.479063820804086, "xcomet_score": 0.7819270491600037, "xcomet_qe_score": 0.7254450917243958, "metricx_score": 3.1199264526367188, "metricx_qe_score": 3.8749780654907227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一的模型来评估它们。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.997307538986206, "xcomet_qe_score": 0.9824987649917603, "metricx_score": 0.5768303871154785, "metricx_qe_score": 0.8717049956321716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出示范数据集。", "metrics": {"bleu_score": 23.90108882452814, "chrf_score": 16.037684801931782, "xcomet_score": 0.8524990081787109, "xcomet_qe_score": 0.8475096821784973, "metricx_score": 2.653265953063965, "metricx_qe_score": 3.605879068374634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提供一个统一的示范数据集,用于在多种自然语言和语义表示之间进行语义解析的交叉链接。", "metrics": {"bleu_score": 38.09831330255981, "chrf_score": 30.76889578397176, "xcomet_score": 0.7589651346206665, "xcomet_qe_score": 0.7582958936691284, "metricx_score": 3.745131254196167, "metricx_qe_score": 3.865969181060791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含九个来自不同领域的数据集,五个语义解析任务,八种语义表示形式,以及15语系中的22种自然语言。", "metrics": {"bleu_score": 42.009988642685734, "chrf_score": 41.096452566491685, "xcomet_score": 0.9619889259338379, "xcomet_qe_score": 0.9746403694152832, "metricx_score": 0.675645112991333, "metricx_qe_score": 1.132856845855713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估的设置。", "metrics": {"bleu_score": 70.91299972295235, "chrf_score": 62.87940602466781, "xcomet_score": 0.9860787391662598, "xcomet_qe_score": 0.9539933204650879, "metricx_score": 1.5349997282028198, "metricx_qe_score": 2.3635244369506836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是翻译测试。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9898306131362915, "xcomet_qe_score": 0.9781848192214966, "metricx_score": 0.22308200597763062, "metricx_qe_score": 0.39765846729278564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用谷歌翻译 API 将源文本翻译成目标语言,然后使用单语模型进行评估训练。", "metrics": {"bleu_score": 69.68012327503263, "chrf_score": 64.48421830546506, "xcomet_score": 0.9712668657302856, "xcomet_qe_score": 0.8061615824699402, "metricx_score": 1.1125202178955078, "metricx_qe_score": 1.1836016178131104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们使用英文查询对英文模型进行训练,在推理阶段,我们使用API将德语查询翻译成英文,然后使用训练好的模型预测SQL。", "metrics": {"bleu_score": 54.879919695801696, "chrf_score": 51.07967047929007, "xcomet_score": 0.9242608547210693, "xcomet_qe_score": 0.8983829021453857, "metricx_score": 1.0656262636184692, "metricx_qe_score": 1.7787790298461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模块。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 76.96368446368447, "xcomet_score": 0.8608987927436829, "xcomet_qe_score": 0.837360680103302, "metricx_score": 0.2960849404335022, "metricx_qe_score": 0.4536767601966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 73.6558994084271, "chrf_score": 67.66383822511001, "xcomet_score": 0.9070941209793091, "xcomet_qe_score": 0.8909211158752441, "metricx_score": 0.6414645910263062, "metricx_qe_score": 0.6743262410163879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试单语领域补全设置,方法是仅使用10%的训练数据来训练单语模型。", "metrics": {"bleu_score": 51.0726989403739, "chrf_score": 48.212952824303926, "xcomet_score": 0.7904001474380493, "xcomet_qe_score": 0.7865587472915649, "metricx_score": 3.4314420223236084, "metricx_qe_score": 3.388287305831909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试多语言模型,我们使用一个多语言模型来训练所有语言。", "metrics": {"bleu_score": 57.83050438567638, "chrf_score": 55.72454598015535, "xcomet_score": 0.9566506147384644, "xcomet_qe_score": 0.9319144487380981, "metricx_score": 0.7349439263343811, "metricx_qe_score": 1.2171380519866943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询合并起来,训练一个多语种模型。并且", "metrics": {"bleu_score": 48.77341345229605, "chrf_score": 42.512582537182006, "xcomet_score": 0.8438282012939453, "xcomet_qe_score": 0.8120968341827393, "metricx_score": 3.070523977279663, "metricx_qe_score": 1.8355804681777954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理时,我们可以使用这个模型。 翻译德语查询或中文查询,或以此类推。", "metrics": {"bleu_score": 59.73854416921759, "chrf_score": 58.242740227815936, "xcomet_score": 0.8025162220001221, "xcomet_qe_score": 0.7378983497619629, "metricx_score": 1.3452818393707275, "metricx_qe_score": 2.707880973815918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 68.83574136059144, "chrf_score": 60.68496837313061, "xcomet_score": 0.7332659363746643, "xcomet_qe_score": 0.7542462348937988, "metricx_score": 2.603395938873291, "metricx_qe_score": 2.6989855766296387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一门源语言上进行训练,然后迁移到另一门语言。", "metrics": {"bleu_score": 21.75463426306332, "chrf_score": 21.933904259991213, "xcomet_score": 0.9339557886123657, "xcomet_qe_score": 0.8799248933792114, "metricx_score": 3.411362648010254, "metricx_qe_score": 3.596914052963257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们使用英语查询或英语和德语结合的少量样本查询来训练一个多语言模型,并预测 SQL 输出。", "metrics": {"bleu_score": 62.30141705527758, "chrf_score": 57.613642045882344, "xcomet_score": 0.859722375869751, "xcomet_qe_score": 0.8138422966003418, "metricx_score": 1.7538211345672607, "metricx_qe_score": 1.9123040437698364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现许多有趣的成果。", "metrics": {"bleu_score": 26.776802959776298, "chrf_score": 24.257998846740414, "xcomet_score": 0.8999395370483398, "xcomet_qe_score": 0.8855783939361572, "metricx_score": 1.2203691005706787, "metricx_qe_score": 2.028127670288086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在单语模型分析方面,我们对两组模型进行了评估。 包括编码器PDR,即基于指针的解码器的多语言预训练编码器,例如XLMR plus PDR和BERT plus PDR。", "metrics": {"bleu_score": 39.52330359454367, "chrf_score": 33.675278356322195, "xcomet_score": 0.5936605930328369, "xcomet_qe_score": 0.628470242023468, "metricx_score": 4.632595539093018, "metricx_qe_score": 3.9792959690093994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练的编码器-解码器模型,例如 mBART 和 MT5。", "metrics": {"bleu_score": 34.472982122652, "chrf_score": 26.96652480191974, "xcomet_score": 0.9248455762863159, "xcomet_qe_score": 0.9637960195541382, "metricx_score": 1.3402553796768188, "metricx_qe_score": 2.497952699661255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在所有九个数据集上,编码器-解码", "metrics": {"bleu_score": 43.38186602259408, "chrf_score": 25.473397545451437, "xcomet_score": 0.7680063247680664, "xcomet_qe_score": 0.7736771106719971, "metricx_score": 7.548914432525635, "metricx_qe_score": 5.999086380004883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "器模型均表现出最佳性能。 我们使用 MT5 和 XLMR,并在 PDR 多语言设置下进行评估。", "metrics": {"bleu_score": 15.67531172019386, "chrf_score": 27.863778694256748, "xcomet_score": 0.24770841002464294, "xcomet_qe_score": 0.24646645784378052, "metricx_score": 5.432485103607178, "metricx_qe_score": 4.976397514343262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合语料上进行训练,可以改进编码器-解码器或编码器-PDR模型。", "metrics": {"bleu_score": 21.49344592381356, "chrf_score": 17.273886600588366, "xcomet_score": 0.7767807245254517, "xcomet_qe_score": 0.7827962636947632, "metricx_score": 1.327208399772644, "metricx_qe_score": 1.4270793199539185, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都能获得性能提升,只是英语在七个数据集上的表现有所下降,仅在三个数据集上获得提升。", "metrics": {"bleu_score": 55.02132064747233, "chrf_score": 47.31074496551743, "xcomet_score": 0.9730184674263, "xcomet_qe_score": 0.9780958294868469, "metricx_score": 1.594308853149414, "metricx_qe_score": 1.872589349746704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我認為這被稱為多語言能力的詛咒。", "metrics": {"bleu_score": 2.9809077846520733, "chrf_score": 5.208333333333333, "xcomet_score": 0.9112516045570374, "xcomet_qe_score": 0.9609025716781616, "metricx_score": 2.988191843032837, "metricx_qe_score": 2.7909631729125977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较跨语言表现差距。", "metrics": {"bleu_score": 58.18773996585304, "chrf_score": 45.248033147795795, "xcomet_score": 0.890000581741333, "xcomet_qe_score": 0.8791995644569397, "metricx_score": 0.500497579574585, "metricx_qe_score": 0.9233701825141907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在该图中,蓝线表示跨语言小样本迁移。", "metrics": {"bleu_score": 32.62478546610939, "chrf_score": 27.459539371304075, "xcomet_score": 0.8347882032394409, "xcomet_qe_score": 0.8036902546882629, "metricx_score": 1.6850618124008179, "metricx_qe_score": 3.1668472290039062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线表示跨语言零样本迁移,", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 59.505479514708156, "xcomet_score": 0.9473828077316284, "xcomet_qe_score": 0.8333927989006042, "metricx_score": 1.6966118812561035, "metricx_qe_score": 2.8306846618652344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿线表示单语设置。", "metrics": {"bleu_score": 45.180100180492246, "chrf_score": 42.94810660563017, "xcomet_score": 0.997053861618042, "xcomet_qe_score": 0.9924086332321167, "metricx_score": 0.36613181233406067, "metricx_qe_score": 0.5519816875457764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过比较绿色和橙色线条,我们发现对于零样本设置,跨语言迁移性能差距显著。而通过比较蓝色和橙色线条,我们发现对于少量样本设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 38.92056401351, "chrf_score": 33.02024630496086, "xcomet_score": 0.6737490892410278, "xcomet_qe_score": 0.6822634935379028, "metricx_score": 2.2505857944488525, "metricx_qe_score": 3.3914895057678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器模型优于先前的工作,或取得了可比的结果。", "metrics": {"bleu_score": 12.16411062125622, "chrf_score": 9.806648231668078, "xcomet_score": 0.9745669364929199, "xcomet_qe_score": 0.9686518907546997, "metricx_score": 1.3498808145523071, "metricx_qe_score": 1.515792965888977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以英语自然语言为基础进行建模,可以显著提升在目标自然语言上的小样本学习性能。 我们发现,诸如 CODIS 和 BLUE 等多语言模型在跨语言语义解析任务中仍然不足以胜任。", "metrics": {"bleu_score": 47.49092856588272, "chrf_score": 40.21282384790364, "xcomet_score": 0.789348304271698, "xcomet_qe_score": 0.7342573404312134, "metricx_score": 6.405075550079346, "metricx_qe_score": 6.523782730102539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们构建了Examplar,这是一个统一的跨角度语义解析基准,支持多种自然语言和主要表示形式。", "metrics": {"bleu_score": 35.171491702468444, "chrf_score": 26.204738479235562, "xcomet_score": 0.7490758895874023, "xcomet_qe_score": 0.7472875118255615, "metricx_score": 4.203357696533203, "metricx_qe_score": 4.15158748626709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语言模型进行了一项全面的基准研究,", "metrics": {"bleu_score": 65.16912510356434, "chrf_score": 51.92368274552182, "xcomet_score": 0.9560678005218506, "xcomet_qe_score": 0.9481292963027954, "metricx_score": 1.2231338024139404, "metricx_qe_score": 1.7751344442367554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果显示出许多有趣的发现,", "metrics": {"bleu_score": 53.21997734504007, "chrf_score": 45.34606391563386, "xcomet_score": 0.9694969654083252, "xcomet_qe_score": 0.9467358589172363, "metricx_score": 1.1560314893722534, "metricx_qe_score": 1.1112960577011108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "等等。", "metrics": {"bleu_score": 0.0, "chrf_score": 5.376344086021505, "xcomet_score": 0.7613270282745361, "xcomet_qe_score": 0.37147438526153564, "metricx_score": 0.5565099120140076, "metricx_qe_score": 0.6241089701652527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "聆听。", "metrics": {"bleu_score": 0.0, "chrf_score": 53.34987593052109, "xcomet_score": 0.8254514932632446, "xcomet_qe_score": 0.6475603580474854, "metricx_score": 1.3102339506149292, "metricx_qe_score": 1.997286319732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好。 我叫大卫·维拉尔,我将对论文《Grunt 平台翻译:评估策略与性能》做一个简短的概述。", "metrics": {"bleu_score": 12.519775678207912, "chrf_score": 13.326482603731055, "xcomet_score": 0.7140535712242126, "xcomet_qe_score": 0.6318213939666748, "metricx_score": 5.976897239685059, "metricx_qe_score": 5.933094501495361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与我在谷歌翻译的同事合作完成的。", "metrics": {"bleu_score": 32.35946184239223, "chrf_score": 25.88223008501646, "xcomet_score": 0.950861930847168, "xcomet_qe_score": 1.0, "metricx_score": 1.5271735191345215, "metricx_qe_score": 0.8564409017562866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PARM 是一种拥有 5400 亿参数的大型语言模型,于去年 2022 年发布。", "metrics": {"bleu_score": 55.49073390705062, "chrf_score": 56.2371254526507, "xcomet_score": 0.9812076091766357, "xcomet_qe_score": 0.9158323407173157, "metricx_score": 3.0544276237487793, "metricx_qe_score": 4.004576206207275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在包含 7800 亿文档的大量文本语料库上进行训练。", "metrics": {"bleu_score": 24.05680447798905, "chrf_score": 33.23177714079081, "xcomet_score": 0.7391147017478943, "xcomet_qe_score": 0.8093785643577576, "metricx_score": 1.9633551836013794, "metricx_qe_score": 2.1986896991729736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "于发布时,它在数百项自然语言处理任务中达到最先进水平。", "metrics": {"bleu_score": 16.805936904720344, "chrf_score": 16.248509954124867, "xcomet_score": 0.9565314054489136, "xcomet_qe_score": 0.9208557605743408, "metricx_score": 0.9122852683067322, "metricx_qe_score": 1.5492254495620728, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们提出了对大型语言模型提示进行机器翻译的首次系统性研究。", "metrics": {"bleu_score": 42.44849732783386, "chrf_score": 38.411187945552875, "xcomet_score": 0.8235363960266113, "xcomet_qe_score": 0.7601495981216431, "metricx_score": 2.3302431106567383, "metricx_qe_score": 2.7374637126922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用AMT社区的最佳实践来评估此类模型的翻译能力。", "metrics": {"bleu_score": 52.010967098419755, "chrf_score": 44.746566284163706, "xcomet_score": 0.796886682510376, "xcomet_qe_score": 0.7723275423049927, "metricx_score": 3.96469783782959, "metricx_qe_score": 5.558260440826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型训练数据重叠。", "metrics": {"bleu_score": 71.39807865675934, "chrf_score": 64.53398027734345, "xcomet_score": 0.9943056106567383, "xcomet_qe_score": 0.9746981859207153, "metricx_score": 0.4191579818725586, "metricx_qe_score": 0.46105217933654785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较两个最先进的系统。 因此,表现最佳的系统是 WMT 评估结果。", "metrics": {"bleu_score": 40.03030701441069, "chrf_score": 36.457698042961056, "xcomet_score": 0.747828483581543, "xcomet_qe_score": 0.7521816492080688, "metricx_score": 3.8481364250183105, "metricx_qe_score": 3.488772392272949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用最先进的神经机器翻译评估指标,并额外展示基于专家的人工评估结果。", "metrics": {"bleu_score": 63.685843338116875, "chrf_score": 62.45321474565008, "xcomet_score": 0.957456111907959, "xcomet_qe_score": 0.8523972034454346, "metricx_score": 1.0840046405792236, "metricx_qe_score": 1.8060877323150635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供关于PROM选择策略的一些建议。", "metrics": {"bleu_score": 38.69241703989408, "chrf_score": 31.41792041635315, "xcomet_score": 0.8706110119819641, "xcomet_qe_score": 0.8704745173454285, "metricx_score": 3.23640775680542, "metricx_qe_score": 2.948145866394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对 LLM 在翻译任务中的表现具有显著影响。正如我们在一个简单的实验中观察到的,我们使用了单次提示,并为每句话提供了两个不同的提示。", "metrics": {"bleu_score": 42.418066767735226, "chrf_score": 41.217192940721034, "xcomet_score": 0.9104906916618347, "xcomet_qe_score": 0.8139786720275879, "metricx_score": 1.6894210577011108, "metricx_qe_score": 3.153135299682617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绝大多数句子,在1000句中,", "metrics": {"bleu_score": 10.127993013562818, "chrf_score": 27.8973180507644, "xcomet_score": 0.7940493226051331, "xcomet_qe_score": 0.7960625886917114, "metricx_score": 9.452790260314941, "metricx_qe_score": 11.102901458740234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有516句的差异大于一个模糊点。 这在", "metrics": {"bleu_score": 6.437165254072419, "chrf_score": 11.882854994109206, "xcomet_score": 0.18321493268013, "xcomet_qe_score": 0.1547931283712387, "metricx_score": 15.28519344329834, "metricx_qe_score": 16.1545467376709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "极端情况下可能高达 40 个模糊点。", "metrics": {"bleu_score": 47.71754374398068, "chrf_score": 34.69725667239738, "xcomet_score": 0.7978847622871399, "xcomet_qe_score": 0.756087064743042, "metricx_score": 4.606451511383057, "metricx_qe_score": 2.7272391319274902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择合适的提示策略至关重要。", "metrics": {"bleu_score": 34.05204944353419, "chrf_score": 28.20548732313438, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.1681901067495346, "metricx_qe_score": 0.26603934168815613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了五次提示的策略,即我们只需在向系统提供的每个句子中标注其所使用的语言。", "metrics": {"bleu_score": 36.850090504266205, "chrf_score": 31.86521429427785, "xcomet_score": 0.8631604313850403, "xcomet_qe_score": 0.8746724724769592, "metricx_score": 2.013315200805664, "metricx_qe_score": 1.988735318183899, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们进行从德语到英语的翻译,德语句子,即源句子,以德语冒号标注,英语翻译则以英语冒号标注。", "metrics": {"bleu_score": 25.175333117804655, "chrf_score": 19.429125754930222, "xcomet_score": 0.9780977964401245, "xcomet_qe_score": 0.9793175458908081, "metricx_score": 1.5725054740905762, "metricx_qe_score": 1.4695056676864624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,在若干短促提示的情况下,提示的具体形式并没有产生显著影响。", "metrics": {"bleu_score": 23.693055763743093, "chrf_score": 22.75322784782802, "xcomet_score": 0.8626316785812378, "xcomet_qe_score": 0.8462477922439575, "metricx_score": 0.7242233753204346, "metricx_qe_score": 0.6461526155471802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零样本提示和一次性提示至关重要。并且", "metrics": {"bleu_score": 39.962332754792115, "chrf_score": 31.18559279128939, "xcomet_score": 0.6027686595916748, "xcomet_qe_score": 0.6517795324325562, "metricx_score": 5.401821613311768, "metricx_qe_score": 3.3638875484466553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当我们像我们这种情况一样,过渡到五次性提示时,实际提示的形式上几乎没有差", "metrics": {"bleu_score": 8.032915292474033, "chrf_score": 13.388256102576499, "xcomet_score": 0.786133885383606, "xcomet_qe_score": 0.7336105704307556, "metricx_score": 3.868058919906616, "metricx_qe_score": 5.429983615875244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别。 例子往往具有最重要的意义。", "metrics": {"bleu_score": 3.716499092256817, "chrf_score": 4.694835680751173, "xcomet_score": 0.4080750048160553, "xcomet_qe_score": 0.3572978377342224, "metricx_score": 2.453068256378174, "metricx_qe_score": 2.34633731842041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果表明,示例质量比与源句的相似度更为重要。", "metrics": {"bleu_score": 44.870326130853016, "chrf_score": 38.00414939161944, "xcomet_score": 0.9914604425430298, "xcomet_qe_score": 0.9847650527954102, "metricx_score": 0.7825876474380493, "metricx_qe_score": 1.0357646942138672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择高质量翻译的范例至关重要。", "metrics": {"bleu_score": 17.102846954548706, "chrf_score": 20.07115163645028, "xcomet_score": 0.9907976388931274, "xcomet_qe_score": 0.9914147853851318, "metricx_score": 0.551461935043335, "metricx_qe_score": 0.4860400855541229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其,我们会比较WMT评估训练数据或开发数据中的选择提示。", "metrics": {"bleu_score": 24.05774095773276, "chrf_score": 26.373107181723277, "xcomet_score": 0.6654751300811768, "xcomet_qe_score": 0.633225679397583, "metricx_score": 2.640915632247925, "metricx_qe_score": 2.9015746116638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据集经过了更为精心的整理,质量也更高,这与训练数据集相比,数据更为整洁,结果也更为理想。因此,", "metrics": {"bleu_score": 13.792125616909448, "chrf_score": 19.977168626415946, "xcomet_score": 0.6726671457290649, "xcomet_qe_score": 0.6817641258239746, "metricx_score": 4.909701347351074, "metricx_qe_score": 2.620974063873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据集时可以获得更好的性能。", "metrics": {"bleu_score": 31.46660996956415, "chrf_score": 29.168262096613944, "xcomet_score": 0.9586597681045532, "xcomet_qe_score": 0.9450441598892212, "metricx_score": 1.8403828144073486, "metricx_qe_score": 2.7054314613342285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专业且最先进的系统相比 Palm 翻译具有显著优势。", "metrics": {"bleu_score": 19.0806437833818, "chrf_score": 20.575739664030728, "xcomet_score": 0.8476130962371826, "xcomet_qe_score": 0.807563304901123, "metricx_score": 4.538696765899658, "metricx_qe_score": 3.992868423461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但 Palm 的表现已相当接近商业级系统。", "metrics": {"bleu_score": 23.578316044531807, "chrf_score": 20.070155224758416, "xcomet_score": 0.9804742336273193, "xcomet_qe_score": 0.9346630573272705, "metricx_score": 5.696630001068115, "metricx_qe_score": 5.311634540557861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择了以 Google 翻译作为评估标准。", "metrics": {"bleu_score": 39.42302221292177, "chrf_score": 32.32058934176564, "xcomet_score": 0.8758825063705444, "xcomet_qe_score": 0.9014175534248352, "metricx_score": 0.8486301898956299, "metricx_qe_score": 0.6211649775505066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用MQM框架进行的以人为中心的能力评估所获得的洞见是,PaLM模型的流畅度与最先进系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 38.57381700776678, "chrf_score": 36.98940306903884, "xcomet_score": 0.9125816822052002, "xcomet_qe_score": 0.9047141075134277, "metricx_score": 1.769747018814087, "metricx_qe_score": 2.062896490097046, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其地,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7788662910461426, "xcomet_qe_score": 0.7727838754653931, "metricx_score": 2.324248790740967, "metricx_qe_score": 1.5878597497940063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,看起来 Palm 有时会通过省略源句中的部分内容来生成听起来更自然的译文。", "metrics": {"bleu_score": 18.471235716502637, "chrf_score": 18.32084570591451, "xcomet_score": 0.9435162544250488, "xcomet_qe_score": 0.9021338820457458, "metricx_score": 3.1360225677490234, "metricx_qe_score": 3.147587299346924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,对于PAN而言,风格外向类别得分低于最先进系统,这构成了一个附加信号。 PARM 提供的输出确实流畅,但准确性方面仍存在一些问题。", "metrics": {"bleu_score": 19.90368607200356, "chrf_score": 19.653304245038118, "xcomet_score": 0.5991124510765076, "xcomet_qe_score": 0.5363500118255615, "metricx_score": 7.349989891052246, "metricx_qe_score": 7.6686177253723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是本次简短概览的全部内容。", "metrics": {"bleu_score": 67.39047062564734, "chrf_score": 60.43499751000463, "xcomet_score": 0.9943745136260986, "xcomet_qe_score": 0.996967077255249, "metricx_score": 0.24589985609054565, "metricx_qe_score": 0.4138813614845276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如需了解更多详情,请参加论文的完整演示。", "metrics": {"bleu_score": 49.3585794099366, "chrf_score": 43.83869022640353, "xcomet_score": 0.80748450756073, "xcomet_qe_score": 0.8466250896453857, "metricx_score": 1.3149261474609375, "metricx_qe_score": 2.153006076812744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是大卫,德国萨尔兰大学的博士生。", "metrics": {"bleu_score": 53.439883964839176, "chrf_score": 44.08684977762537, "xcomet_score": 0.8814250826835632, "xcomet_qe_score": 0.954211950302124, "metricx_score": 0.5734738111495972, "metricx_qe_score": 0.3920154571533203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我将向大家介绍我们最近的一项工作——《比你想象的更脆弱》,对每周监督学习的一种批判性分析。", "metrics": {"bleu_score": 37.72842410944006, "chrf_score": 35.75270798108501, "xcomet_score": 0.8224588632583618, "xcomet_qe_score": 0.7386838793754578, "metricx_score": 4.84624719619751, "metricx_qe_score": 5.706782341003418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与萧玉生、马里奥·斯穆斯巴赫、吉娅·斯特芬以及 DT Schlaukel 共同完成的工作。", "metrics": {"bleu_score": 2.9867390496386634, "chrf_score": 5.4246547116816775, "xcomet_score": 0.3356764018535614, "xcomet_qe_score": 0.22737154364585876, "metricx_score": 5.869718551635742, "metricx_qe_score": 6.511196613311768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想首先简要介绍一下弱监督和弱监督学习。", "metrics": {"bleu_score": 89.14703664390797, "chrf_score": 97.45743709204099, "xcomet_score": 0.8834832310676575, "xcomet_qe_score": 0.8524844646453857, "metricx_score": 0.8124381303787231, "metricx_qe_score": 2.1726276874542236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,我们并不手动标注数据。", "metrics": {"bleu_score": 24.515235346013313, "chrf_score": 24.094655388387505, "xcomet_score": 0.8346418738365173, "xcomet_qe_score": 0.8315584659576416, "metricx_score": 2.375894784927368, "metricx_qe_score": 2.408846139907837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而是利用弱标注来源对数据进行标注,例如简单的启发式规则、知识库或低质量众包数据,如图右侧所示。", "metrics": {"bleu_score": 47.630293839617025, "chrf_score": 44.477568434513, "xcomet_score": 0.7085943222045898, "xcomet_qe_score": 0.6622496843338013, "metricx_score": 1.281137228012085, "metricx_qe_score": 1.5988235473632812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注的成本要低得多,但同时也存在噪声,这意味着其中一部分标注是错误的。", "metrics": {"bleu_score": 27.841130941865533, "chrf_score": 25.10420762107798, "xcomet_score": 0.8633782863616943, "xcomet_qe_score": 0.8298413753509521, "metricx_score": 1.9062819480895996, "metricx_qe_score": 2.2197647094726562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在弱标签数据上直接训练神经网络,神经网络倾向于记忆标签噪声,而无法泛化。", "metrics": {"bleu_score": 43.59013814275798, "chrf_score": 36.804872566928715, "xcomet_score": 0.9068756103515625, "xcomet_qe_score": 0.8688157796859741, "metricx_score": 0.9371213912963867, "metricx_qe_score": 1.2527644634246826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出训练算法,以便在标签噪声存在的情况下稳健地训练神经网络,从而确保训练后的模型仍能良好泛化。", "metrics": {"bleu_score": 49.13765498961522, "chrf_score": 41.843973200104266, "xcomet_score": 0.9546658396720886, "xcomet_qe_score": 0.8839131593704224, "metricx_score": 0.9828751683235168, "metricx_qe_score": 1.5713633298873901, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在WSL(每周监督学习)领域的一些最新研究中,一种常见的说法是,人们声称他们仅使用每周的标注数据进行模型训练,并在干净的测试集上取得了优异的性能。", "metrics": {"bleu_score": 24.030370396643633, "chrf_score": 25.025653625084125, "xcomet_score": 0.6943816542625427, "xcomet_qe_score": 0.6496114730834961, "metricx_score": 6.2866315841674805, "metricx_qe_score": 6.096164703369141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术上来说,这个说法并非错误,但存在一个问题。 这通常是人们会假设存在一个额外的干净验证集,用于模型选择。", "metrics": {"bleu_score": 41.298145499020976, "chrf_score": 36.83385094160898, "xcomet_score": 0.9194341897964478, "xcomet_qe_score": 0.8128564953804016, "metricx_score": 2.999798536300659, "metricx_qe_score": 4.036107540130615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不能在这个问题设定上止步,因为这暗示着每周的SuperWise学习需要额外的手动标注。", "metrics": {"bleu_score": 34.90169611076336, "chrf_score": 28.547991934359924, "xcomet_score": 0.7698721885681152, "xcomet_qe_score": 0.7296128273010254, "metricx_score": 5.82310676574707, "metricx_qe_score": 6.591642379760742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但就像一个不容忽视的显而易见的事实一样,这种必要性常常被忽视。", "metrics": {"bleu_score": 37.34486738130203, "chrf_score": 40.67037339590453, "xcomet_score": 0.845422625541687, "xcomet_qe_score": 0.801328182220459, "metricx_score": 1.7216742038726807, "metricx_qe_score": 1.4015284776687622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑虑引出我们三个研究问题。", "metrics": {"bleu_score": 47.13945310979296, "chrf_score": 40.391349087742114, "xcomet_score": 0.8774155378341675, "xcomet_qe_score": 0.9335490465164185, "metricx_score": 1.1587517261505127, "metricx_qe_score": 1.0253015756607056, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,清洁的验证数据对 WSL 来说是否必要?或者,我们是否可以或许使用一个带有噪声的验证集?", "metrics": {"bleu_score": 19.726508017729643, "chrf_score": 23.5602776599178, "xcomet_score": 0.9021385312080383, "xcomet_qe_score": 0.882494330406189, "metricx_score": 1.6646759510040283, "metricx_qe_score": 2.6432812213897705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净数据,或者干净数据是 WSL 正常运行的必要条件,那么我们需要多少干净样本?", "metrics": {"bleu_score": 34.17104416282846, "chrf_score": 30.95437824422656, "xcomet_score": 0.9639084339141846, "xcomet_qe_score": 0.8971598148345947, "metricx_score": 0.6815282702445984, "metricx_qe_score": 1.009028434753418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该仅将干净样本用于验证,或者有没有更好的利用它们的方法?", "metrics": {"bleu_score": 28.03737212492408, "chrf_score": 24.602608720255777, "xcomet_score": 0.9726167917251587, "xcomet_qe_score": 0.9143829345703125, "metricx_score": 0.8147776126861572, "metricx_qe_score": 0.9503116011619568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在研究中探讨了这些研究问题,我们的发现如下:", "metrics": {"bleu_score": 43.72855385790082, "chrf_score": 35.88287642938852, "xcomet_score": 0.9733381271362305, "xcomet_qe_score": 0.9638733863830566, "metricx_score": 0.4133760929107666, "metricx_qe_score": 0.4896566867828369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现有趣的是,最近的WSL方法实际上确实需要干净的白色培养皿样品才能正常工作。", "metrics": {"bleu_score": 48.811359599935116, "chrf_score": 49.03889683281275, "xcomet_score": 0.7929602861404419, "xcomet_qe_score": 0.807722270488739, "metricx_score": 2.5741991996765137, "metricx_qe_score": 3.313411235809326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能将大幅下降。", "metrics": {"bleu_score": 44.97332084013507, "chrf_score": 35.739538239538234, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3529796898365021, "metricx_qe_score": 0.5942920446395874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,则训练后的模型无法泛化到原始的弱标签之外。 这意味着培训毫无意义。", "metrics": {"bleu_score": 45.98637811673472, "chrf_score": 38.737910376461, "xcomet_score": 0.8269453644752502, "xcomet_qe_score": 0.8045589327812195, "metricx_score": 1.8496798276901245, "metricx_qe_score": 2.7733075618743896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明 WSL 方法实际上需要干净标注的数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 59.418348752022005, "chrf_score": 55.0716974345405, "xcomet_score": 0.8123750686645508, "xcomet_qe_score": 0.7827414274215698, "metricx_score": 2.7475342750549316, "metricx_qe_score": 3.806197166442871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加清洁验证样本的数量将有助于WSL方法实现更好的性能,如图左侧所示。", "metrics": {"bleu_score": 61.89520583524376, "chrf_score": 59.85824455976398, "xcomet_score": 0.9058635234832764, "xcomet_qe_score": 0.9217255115509033, "metricx_score": 3.4582717418670654, "metricx_qe_score": 4.108932018280029, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们每类只需要20个样本就能达到高性能水平。", "metrics": {"bleu_score": 28.48931827772397, "chrf_score": 28.032530271660704, "xcomet_score": 0.9543695449829102, "xcomet_qe_score": 0.9756717085838318, "metricx_score": 0.9747081995010376, "metricx_qe_score": 0.9892556667327881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但故事并非到此结束,因为无论我们选择哪种方式获取干净样本,直接在此基础上进行训练都能实现更好的性能。 红色", "metrics": {"bleu_score": 20.730530239173248, "chrf_score": 17.564361325732612, "xcomet_score": 0.8603578805923462, "xcomet_qe_score": 0.8587321043014526, "metricx_score": 4.520142078399658, "metricx_qe_score": 3.1455626487731934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的图示显示了微调方法与WSL方法的性能差异,微调方法直接应用于干净数据,而WSL方法仅使用干净数据进行验证。", "metrics": {"bleu_score": 49.64207158324125, "chrf_score": 49.28721768010196, "xcomet_score": 0.589016318321228, "xcomet_qe_score": 0.7221713066101074, "metricx_score": 4.774763107299805, "metricx_qe_score": 4.960527420043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每个类别有10个样本,直接微调开始优于WSL方法。 最后,之前 WSL 方法中", "metrics": {"bleu_score": 38.73952197651851, "chrf_score": 35.83968793883221, "xcomet_score": 0.6129980087280273, "xcomet_qe_score": 0.511408269405365, "metricx_score": 8.05181884765625, "metricx_qe_score": 5.116952896118164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "声称的性能提升,可以通过允许在干净的验证样本上继续微调来实现。 从图表", "metrics": {"bleu_score": 26.654930460273103, "chrf_score": 23.308495202132594, "xcomet_score": 0.40877819061279297, "xcomet_qe_score": 0.44117996096611023, "metricx_score": 8.838050842285156, "metricx_qe_score": 6.5485405921936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,最初,被称为FTW的Van Lina模型在性能上低于更复杂的WSL方法,例如余弦函数。", "metrics": {"bleu_score": 20.40179687875698, "chrf_score": 22.70714874942278, "xcomet_score": 0.6934601068496704, "xcomet_qe_score": 0.6306138038635254, "metricx_score": 6.066720485687256, "metricx_qe_score": 6.417964458465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果允许在干净样本上继续微调,那么FTW的表现与其他方法相当。", "metrics": {"bleu_score": 31.593019244752835, "chrf_score": 27.936940874188824, "xcomet_score": 0.8869484663009644, "xcomet_qe_score": 0.806145966053009, "metricx_score": 1.4004677534103394, "metricx_qe_score": 1.969125747680664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实际应用中,没有理由选择更复杂的WSL方法,因为它们需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 50.94962971977623, "chrf_score": 52.33949880896825, "xcomet_score": 0.9856271743774414, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6415926814079285, "metricx_qe_score": 1.29813551902771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们展示了最近的 WSL 方法需要干净、人工标注的样本才能正常工作。", "metrics": {"bleu_score": 49.02250926110867, "chrf_score": 47.818691471462806, "xcomet_score": 0.7616168260574341, "xcomet_qe_score": 0.7903470993041992, "metricx_score": 3.214035987854004, "metricx_qe_score": 3.801523447036743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下:", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 53.22177822177822, "xcomet_score": 0.9982490539550781, "xcomet_qe_score": 0.9813262224197388, "metricx_score": 0.2969313859939575, "metricx_qe_score": 0.2585373520851135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,说明模型选择是否基于干净、高质量的验证样本进行。", "metrics": {"bleu_score": 27.474558342153514, "chrf_score": 25.582958435099584, "xcomet_score": 0.8699933290481567, "xcomet_qe_score": 0.859630823135376, "metricx_score": 1.0599994659423828, "metricx_qe_score": 1.4801963567733765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,应该将 WSL 方法与未来的学习基准进行比较,因为两者都基于干净的样本进行工作。", "metrics": {"bleu_score": 38.43802418290275, "chrf_score": 38.11672971543922, "xcomet_score": 0.694372832775116, "xcomet_qe_score": 0.7571347951889038, "metricx_score": 4.081174850463867, "metricx_qe_score": 5.6330156326293945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一种简单而强大的基准,未来在 WSL 领域的工作应该考虑这一点。", "metrics": {"bleu_score": 35.58933818943882, "chrf_score": 34.17863927842291, "xcomet_score": 0.9134528636932373, "xcomet_qe_score": 0.8090399503707886, "metricx_score": 2.021507740020752, "metricx_qe_score": 2.511573076248169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们已将代码开源。", "metrics": {"bleu_score": 15.3467374987114, "chrf_score": 18.096598020460476, "xcomet_score": 0.9519639015197754, "xcomet_qe_score": 0.9297649264335632, "metricx_score": 0.4015449285507202, "metricx_qe_score": 1.1167150735855103, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.53011982655684, "chrf_score": 51.34315219921988, "xcomet_score": 0.9966487884521484, "xcomet_qe_score": 0.9828085899353027, "metricx_score": 0.3895472288131714, "metricx_qe_score": 0.34398770332336426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎查阅。", "metrics": {"bleu_score": 17.030578356760866, "chrf_score": 15.694310511089679, "xcomet_score": 0.9795382022857666, "xcomet_qe_score": 0.9593701958656311, "metricx_score": 0.31718626618385315, "metricx_qe_score": 0.34247255325317383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝您参会愉快。", "metrics": {"bleu_score": 7.817610446892725, "chrf_score": 8.18252221407027, "xcomet_score": 0.9681738615036011, "xcomet_qe_score": 1.0, "metricx_score": 0.46805065870285034, "metricx_qe_score": 0.2751372754573822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 10.63630283111601, "xcomet_score": 0.995103120803833, "xcomet_qe_score": 0.9681702852249146, "metricx_score": 0.23110710084438324, "metricx_qe_score": 0.18293818831443787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCeval,这是一种评估对话式人工智能的新维度方法。", "metrics": {"bleu_score": 30.06989501606862, "chrf_score": 27.9116544794489, "xcomet_score": 0.859586238861084, "xcomet_qe_score": 0.9333110451698303, "metricx_score": 1.2215973138809204, "metricx_qe_score": 1.4886982440948486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学乔金诺教授领导的埃默里自然语言处理实验室完成,并与亚马逊Alexa人工智能团队合作。", "metrics": {"bleu_score": 30.20987130595042, "chrf_score": 27.522020583445713, "xcomet_score": 0.7486802339553833, "xcomet_qe_score": 0.7682164311408997, "metricx_score": 3.7308425903320312, "metricx_qe_score": 3.5550732612609863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚刚开发了一个对话模型,并且想评估它与当前最先进水平的对比情况。", "metrics": {"bleu_score": 46.96574627175242, "chrf_score": 41.88597085742497, "xcomet_score": 0.8903670310974121, "xcomet_qe_score": 0.929503858089447, "metricx_score": 0.6071843504905701, "metricx_qe_score": 0.5335700511932373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是采用人工评估,例如,请人工评估员选择两个会话中哪个更好,或者根据李克特量表对会话进行评分。", "metrics": {"bleu_score": 44.935363220575184, "chrf_score": 36.35068078130123, "xcomet_score": 0.9601964950561523, "xcomet_qe_score": 0.9567285776138306, "metricx_score": 2.3075878620147705, "metricx_qe_score": 1.8000531196594238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的全面评估方面表现良好,但对话质量涉及诸多方面。", "metrics": {"bleu_score": 35.614122529968405, "chrf_score": 30.572128965727945, "xcomet_score": 0.9987471103668213, "xcomet_qe_score": 0.9918560981750488, "metricx_score": 0.32118406891822815, "metricx_qe_score": 0.5464193224906921, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以便更细致地了解模型的优势和不足。", "metrics": {"bleu_score": 60.18791526268261, "chrf_score": 57.64104316676456, "xcomet_score": 0.9835590124130249, "xcomet_qe_score": 0.9688664674758911, "metricx_score": 0.576229989528656, "metricx_qe_score": 0.573696494102478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地请人类评估者评估对话质量的多个维度,例如模型响应的相关性,采用现有的比较量表或李克特量表方法。", "metrics": {"bleu_score": 58.99873927066051, "chrf_score": 50.86164839531469, "xcomet_score": 0.967901349067688, "xcomet_qe_score": 0.9478769302368164, "metricx_score": 1.2305145263671875, "metricx_qe_score": 1.5982418060302734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确且可靠的维度对话评估策略。", "metrics": {"bleu_score": 49.783498148192024, "chrf_score": 45.56158658381775, "xcomet_score": 0.8974851369857788, "xcomet_qe_score": 0.8673003911972046, "metricx_score": 1.3021092414855957, "metricx_qe_score": 1.4088468551635742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法旨在通过明确标注每个模型回复是否表现出某些行为——例如,提供不相关的信息或自相矛盾——来降低人为评估的主观性。", "metrics": {"bleu_score": 42.47248638956501, "chrf_score": 35.41506374109443, "xcomet_score": 0.9600199460983276, "xcomet_qe_score": 0.9570380449295044, "metricx_score": 1.5343923568725586, "metricx_qe_score": 2.007465362548828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为在聊天中标注行为,简称ABC评估。", "metrics": {"bleu_score": 37.15470384843163, "chrf_score": 35.00533217310639, "xcomet_score": 0.8044602870941162, "xcomet_qe_score": 0.8065199851989746, "metricx_score": 2.3402957916259766, "metricx_qe_score": 2.9054949283599854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法旨在全面覆盖最近文献中被认为会影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 67.30544160441615, "chrf_score": 60.18437796400521, "xcomet_score": 0.9522508382797241, "xcomet_qe_score": 0.931175947189331, "metricx_score": 1.6215571165084839, "metricx_qe_score": 3.4748706817626953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC评估能够衡量聊天模型在出现各种主题错误时的速率。", "metrics": {"bleu_score": 37.74155035532906, "chrf_score": 30.835493957469396, "xcomet_score": 0.7259997725486755, "xcomet_qe_score": 0.7780760526657104, "metricx_score": 3.2986338138580322, "metricx_qe_score": 4.277924060821533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC 评估衡量聊天模型忽略其对话伙伴或发表无关内容的回合数。 当模型出现自相矛盾或与其伙伴相悖的情况,产生虚构的错误事实,或违背常识知识,以及在模型成功或失败时表现出缺乏同理心。", "metrics": {"bleu_score": 30.680748291400825, "chrf_score": 26.818852595416043, "xcomet_score": 0.49017614126205444, "xcomet_qe_score": 0.5967345237731934, "metricx_score": 5.479208946228027, "metricx_qe_score": 5.001977443695068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方式最为有效,我们选择了四个最先进的聊天模型,并使用ABC评估方法,对每个模型进行了100次人机对话的评估。", "metrics": {"bleu_score": 60.81438280983202, "chrf_score": 56.798151681719034, "xcomet_score": 0.9533641338348389, "xcomet_qe_score": 0.9769119024276733, "metricx_score": 0.9305492043495178, "metricx_qe_score": 0.9646687507629395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了便于比较,我们还利用三种现有方法评估了这些对话:在轮次层面进行的李克特评分,在对话层面进行的李克特评分,以及对话层面的成对比较。", "metrics": {"bleu_score": 49.91863276541845, "chrf_score": 45.673899423420885, "xcomet_score": 0.872263491153717, "xcomet_qe_score": 0.8533546924591064, "metricx_score": 2.7531135082244873, "metricx_qe_score": 3.645564556121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有方法,我们收集了关于对话八个最常被评估方面的评估结果,因为这是在多个维度评估聊天模型的标准做法。 通过", "metrics": {"bleu_score": 55.92707426176941, "chrf_score": 47.663372324812144, "xcomet_score": 0.7322389483451843, "xcomet_qe_score": 0.7226074934005737, "metricx_score": 4.564842700958252, "metricx_qe_score": 2.0976216793060303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这些评估结果的分析,我们发现ABC评估行为标签总体上比现有方法收集的标签更可靠,正如100个双重标注对话中的标注者间一致性所衡量的那样。 此外,ABC 评估", "metrics": {"bleu_score": 35.199580594702134, "chrf_score": 36.167590934815806, "xcomet_score": 0.5407893061637878, "xcomet_qe_score": 0.4606916010379791, "metricx_score": 10.442245483398438, "metricx_qe_score": 9.014998435974121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "标签在预测整体对话质量方面,相较于现有方法生成的指标表现出更强的预测性,正如此简单的线性回归分析所示。", "metrics": {"bleu_score": 45.448526009214056, "chrf_score": 36.17679434315602, "xcomet_score": 0.7710354328155518, "xcomet_qe_score": 0.8267531991004944, "metricx_score": 2.998058319091797, "metricx_qe_score": 3.5457863807678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以观察到,衡量包含自反和伙伴反驳的回合比例,分别解释了对话质量的5%和10%,而平均李克特一致性评分仅解释了4%或更少。", "metrics": {"bleu_score": 49.73094688068907, "chrf_score": 44.10621563075858, "xcomet_score": 0.6633388996124268, "xcomet_qe_score": 0.7067512273788452, "metricx_score": 3.1176390647888184, "metricx_qe_score": 2.948406934738159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们通过逐步线性回归检验了每个评估指标是否捕捉了聊天质量的独特方面。", "metrics": {"bleu_score": 58.05767005939097, "chrf_score": 50.38368701971644, "xcomet_score": 0.8245167136192322, "xcomet_qe_score": 0.8873166441917419, "metricx_score": 1.236324429512024, "metricx_qe_score": 1.473914384841919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以看到,所有 ABC 评估指标的结合能够解释对话质量超过 25%。并且,当你逐一移除这些指标时,大多数情况下都会损失掉相当一部分关于质量的信息。", "metrics": {"bleu_score": 18.829597523889014, "chrf_score": 23.253985289616075, "xcomet_score": 0.7966087460517883, "xcomet_qe_score": 0.7260680198669434, "metricx_score": 3.378582239151001, "metricx_qe_score": 3.3904452323913574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有回合级别的李克特量表的组合,解释了更少的质量信息,并且其中更少的指标携带了独特", "metrics": {"bleu_score": 23.983777159099006, "chrf_score": 22.07174902713788, "xcomet_score": 0.6175702810287476, "xcomet_qe_score": 0.6187572479248047, "metricx_score": 6.602464199066162, "metricx_qe_score": 5.454204082489014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的资讯。 这些可靠、翔实且独特的 ABC 评估指标,使我们能够以比以往方法更高分辨率的方式评估对话式人工智能。", "metrics": {"bleu_score": 3.4130214752093244, "chrf_score": 8.456140945168023, "xcomet_score": 0.40452826023101807, "xcomet_qe_score": 0.5832929015159607, "metricx_score": 7.291196346282959, "metricx_qe_score": 6.469060897827148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们的实验结果来看,仍然存在一些挑战,并且已经被精确地量化。", "metrics": {"bleu_score": 38.37679744478914, "chrf_score": 35.20806227188768, "xcomet_score": 0.9930927753448486, "xcomet_qe_score": 1.0, "metricx_score": 0.8987100720405579, "metricx_qe_score": 1.2107007503509521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人大约在 20% 的回复中存在常识性错误。", "metrics": {"bleu_score": 48.14400258132247, "chrf_score": 46.162850176361744, "xcomet_score": 0.9930628538131714, "xcomet_qe_score": 0.9821813106536865, "metricx_score": 1.0012282133102417, "metricx_qe_score": 1.9399663209915161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在约 15% 的回复中,它们会产生无关信息。并且,大约 10% 的情况下,它们会自相矛盾或与对方的观点相悖。", "metrics": {"bleu_score": 35.7812632909166, "chrf_score": 33.21322811109088, "xcomet_score": 0.8782532215118408, "xcomet_qe_score": 0.8414196372032166, "metricx_score": 2.3102550506591797, "metricx_qe_score": 2.20577073097229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域快速的进步,许多这些错误率在今后发布的模型中都可能降低,自从我们进行评估以来。", "metrics": {"bleu_score": 38.6283197810109, "chrf_score": 34.7051742387612, "xcomet_score": 0.7047488689422607, "xcomet_qe_score": 0.6957964897155762, "metricx_score": 5.0648369789123535, "metricx_qe_score": 5.252377510070801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更凸显了追求可靠且精确的评估指标来比较模型的重要性。", "metrics": {"bleu_score": 55.21949166298077, "chrf_score": 53.803033196155134, "xcomet_score": 0.9979426860809326, "xcomet_qe_score": 0.9867478609085083, "metricx_score": 0.6908560991287231, "metricx_qe_score": 0.7257021069526672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABC Eval 能够被该领域的其他研究者利用,作为迈向这一目标的一个有意义的步骤。", "metrics": {"bleu_score": 38.50297530200177, "chrf_score": 45.452611609730155, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.324036955833435, "metricx_qe_score": 1.730749487876892, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待着在未来几个月和几年里,会话式人工智能取得进一步的进展。", "metrics": {"bleu_score": 34.93292472383549, "chrf_score": 36.577095519414456, "xcomet_score": 0.9528771638870239, "xcomet_qe_score": 0.9252979159355164, "metricx_score": 1.0553724765777588, "metricx_qe_score": 0.5712966322898865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫凯奥-尹,我将为大家介绍我们的工作,题为《何时翻译需要语境?", "metrics": {"bleu_score": 20.962825117825528, "chrf_score": 19.451972558263673, "xcomet_score": 0.8439322113990784, "xcomet_qe_score": 0.8883615732192993, "metricx_score": 2.2103071212768555, "metricx_qe_score": 3.041443347930908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "——基于数据的多语种探索》。", "metrics": {"bleu_score": 28.65612242047131, "chrf_score": 27.450346814581234, "xcomet_score": 0.9376311302185059, "xcomet_qe_score": 0.8961530923843384, "metricx_score": 1.3557299375534058, "metricx_qe_score": 1.6665037870407104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是我们与帕特里克·费尔南德斯、艾米·刘、安德烈·F.D·马丁斯和格雷厄姆·纽毕格合作完成的。", "metrics": {"bleu_score": 12.120981066263758, "chrf_score": 8.545603142716066, "xcomet_score": 0.890414834022522, "xcomet_qe_score": 0.8947476744651794, "metricx_score": 2.9280478954315186, "metricx_qe_score": 3.2307181358337402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以很多翻译都取决于语境。", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 27.013391741570665, "xcomet_score": 0.9893069267272949, "xcomet_qe_score": 0.9876947402954102, "metricx_score": 0.1561698317527771, "metricx_qe_score": 0.09094126522541046, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们该如何翻译这个句子中的“mole”?", "metrics": {"bleu_score": 41.94685158262138, "chrf_score": 46.55072349637168, "xcomet_score": 0.9978140592575073, "xcomet_qe_score": 0.9681912660598755, "metricx_score": 0.88775235414505, "metricx_qe_score": 2.014235734939575, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好吧,如果上一句话是,“事情可能会变得危险,如果大臣们发现了”,那么莫指的是一个间谍。", "metrics": {"bleu_score": 10.670687651615458, "chrf_score": 6.342045193748574, "xcomet_score": 0.7029348611831665, "xcomet_qe_score": 0.6994338035583496, "metricx_score": 4.843031883239746, "metricx_qe_score": 4.320989608764648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果上一句话是,“医生,这可能是什么严重的事情吗?”,那么莫指的是一个胎记。", "metrics": {"bleu_score": 13.018401384073014, "chrf_score": 9.43502815508008, "xcomet_score": 0.7118686437606812, "xcomet_qe_score": 0.7126903533935547, "metricx_score": 3.7342023849487305, "metricx_qe_score": 3.237213373184204, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据语境,词语的含义会发生变化,其翻译也随之改变。", "metrics": {"bleu_score": 26.574454482374573, "chrf_score": 22.629811483499132, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2867918014526367, "metricx_qe_score": 0.21755903959274292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在处理这类案例中的翻译质量相当困难。", "metrics": {"bleu_score": 33.535699101570344, "chrf_score": 26.576541815672254, "xcomet_score": 0.9748193025588989, "xcomet_qe_score": 0.9761946201324463, "metricx_score": 0.9068630933761597, "metricx_qe_score": 1.226909875869751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,只有一小部分翻译依赖于上下文,这使得基于语料库的指标,例如BLEU,无法准确捕捉这些翻译。", "metrics": {"bleu_score": 41.917612085372284, "chrf_score": 39.60619848323443, "xcomet_score": 0.9826240539550781, "xcomet_qe_score": 0.9584397077560425, "metricx_score": 1.0545978546142578, "metricx_qe_score": 1.7037353515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对语境相关的翻译进行针对性评估,但这些资源仅支持有限类型的语境相关翻译和有限的语言集,因为它们通常依赖于领域知识和人工校对。", "metrics": {"bleu_score": 57.086141248196824, "chrf_score": 49.54408298164067, "xcomet_score": 0.9606420993804932, "xcomet_qe_score": 0.9537387490272522, "metricx_score": 1.2501506805419922, "metricx_qe_score": 0.9923493266105652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要语境?", "metrics": {"bleu_score": 8.736015370428479, "chrf_score": 11.196726662314651, "xcomet_score": 0.8924298286437988, "xcomet_qe_score": 0.8972160816192627, "metricx_score": 0.3988206386566162, "metricx_qe_score": 0.22178193926811218, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型处理这些情况的能力如何?", "metrics": {"bleu_score": 34.17929717947519, "chrf_score": 29.95491759459726, "xcomet_score": 0.9984992742538452, "xcomet_qe_score": 0.9902448654174805, "metricx_score": 0.5139561295509338, "metricx_qe_score": 0.4580145478248596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了词语在翻译中对语境的依赖程度。", "metrics": {"bleu_score": 49.53448020084589, "chrf_score": 42.24327175494181, "xcomet_score": 0.9967888593673706, "xcomet_qe_score": 1.0, "metricx_score": 4.592765808105469, "metricx_qe_score": 4.701548099517822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在前期的工作中,我们提出了CXMI作为衡量机器翻译模型上下文利用率的指标。", "metrics": {"bleu_score": 35.76073844502072, "chrf_score": 36.96148412676273, "xcomet_score": 0.9843255281448364, "xcomet_qe_score": 0.9784929156303406, "metricx_score": 0.7731187343597412, "metricx_qe_score": 0.7638043165206909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量上下文C在给定源X的情况下,关于目标Y提供多少信息来实现。 你可以将 CXMI 视为赋予模型上下文所获得的信息。", "metrics": {"bleu_score": 47.365224752462794, "chrf_score": 43.61092206340428, "xcomet_score": 0.8301414251327515, "xcomet_qe_score": 0.7898787260055542, "metricx_score": 3.633223533630371, "metricx_qe_score": 3.823077440261841, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们扩展了 CXMI,使其成为逐点 CXMI,这可以衡量句子级别或单词级别的上下文使用情况。", "metrics": {"bleu_score": 24.43130260772371, "chrf_score": 24.613046189532508, "xcomet_score": 0.9057762622833252, "xcomet_qe_score": 0.8806939125061035, "metricx_score": 1.6871191263198853, "metricx_qe_score": 2.035388946533203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将 PSXMI 值高的词视为需要上下文来进行翻译的词。", "metrics": {"bleu_score": 77.49224723289701, "chrf_score": 72.63771349978248, "xcomet_score": 0.9486546516418457, "xcomet_qe_score": 0.9584866762161255, "metricx_score": 2.43293833732605, "metricx_qe_score": 3.485466718673706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析具有高PCXMI值的词语,以寻找这些词语之间的规律。", "metrics": {"bleu_score": 32.90385879986622, "chrf_score": 34.30406396584094, "xcomet_score": 0.9671491384506226, "xcomet_qe_score": 0.9888100028038025, "metricx_score": 1.1512317657470703, "metricx_qe_score": 1.4309518337249756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对TED演讲的文字记录进行分析,这些记录已被翻译成14种不同的语言。", "metrics": {"bleu_score": 34.852214613852276, "chrf_score": 39.820997487458676, "xcomet_score": 0.9435409903526306, "xcomet_qe_score": 0.9518939852714539, "metricx_score": 2.0698230266571045, "metricx_qe_score": 2.0245814323425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行分析时,采用三个不同的层面。", "metrics": {"bleu_score": 50.82365016563264, "chrf_score": 46.32882679871317, "xcomet_score": 0.8473236560821533, "xcomet_qe_score": 0.8576735854148865, "metricx_score": 0.5092306137084961, "metricx_qe_score": 0.8594270348548889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们考察词性标签,这些标签具有较高的平均PCXMI值。", "metrics": {"bleu_score": 15.288008279464082, "chrf_score": 23.828554841689567, "xcomet_score": 0.8383650779724121, "xcomet_qe_score": 0.8033528327941895, "metricx_score": 2.5935816764831543, "metricx_qe_score": 2.648336410522461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们能够发现,例如,阿拉伯语中存在具有相对较高 P6MI 的双重代词。", "metrics": {"bleu_score": 32.20173434351673, "chrf_score": 27.54191316244961, "xcomet_score": 0.719111979007721, "xcomet_qe_score": 0.7469854354858398, "metricx_score": 6.616182327270508, "metricx_qe_score": 5.714752197265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,英语没有双重代词,因此在翻译成阿拉伯语时,需要上下文来确定代词是否为双重形式。", "metrics": {"bleu_score": 62.56009728030972, "chrf_score": 55.18259917742676, "xcomet_score": 0.8224647045135498, "xcomet_qe_score": 0.9923160076141357, "metricx_score": 1.6512967348098755, "metricx_qe_score": 1.700299620628357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与之类似,我们发现有些语言在选择合适的动词形式时也需要语境。", "metrics": {"bleu_score": 50.7645330089512, "chrf_score": 43.84276503122903, "xcomet_score": 0.9915345907211304, "xcomet_qe_score": 0.9850798845291138, "metricx_score": 0.54803466796875, "metricx_qe_score": 0.570783257484436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们考察那些在所有不同出现情况下的平均PCSXMI值较高的词汇项目。", "metrics": {"bleu_score": 28.381756881487178, "chrf_score": 27.651473398320164, "xcomet_score": 0.7716146111488342, "xcomet_qe_score": 0.7737470269203186, "metricx_score": 5.719918251037598, "metricx_qe_score": 5.374002933502197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别出像这样案例,其中在中文翻译中,需要根据语境翻译专有名词,以确保在同一文档内使用一致的译法。", "metrics": {"bleu_score": 28.274636567931942, "chrf_score": 24.738631495728193, "xcomet_score": 0.9426254034042358, "xcomet_qe_score": 0.9257395267486572, "metricx_score": 0.611852765083313, "metricx_qe_score": 0.9356794953346252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现语境有助于在恰当的正式程度上进行翻译。", "metrics": {"bleu_score": 13.502367316243044, "chrf_score": 15.789331137803272, "xcomet_score": 0.9221315383911133, "xcomet_qe_score": 0.9222955703735352, "metricx_score": 1.1795984506607056, "metricx_qe_score": 0.9934388399124146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们考察具有高 p6mi 值的不同个别词元。", "metrics": {"bleu_score": 16.09338477464615, "chrf_score": 15.018662402871138, "xcomet_score": 0.7529439926147461, "xcomet_qe_score": 0.607742965221405, "metricx_score": 6.430060386657715, "metricx_qe_score": 8.000740051269531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别无法仅通过该词本身捕捉到的现象,而是通过句子结构来表达的现象,例如省略解析。", "metrics": {"bleu_score": 39.681898249226904, "chrf_score": 34.52367783518659, "xcomet_score": 0.7339444160461426, "xcomet_qe_score": 0.6764315366744995, "metricx_score": 1.6034109592437744, "metricx_qe_score": 2.145134449005127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们现在利用分析结果来设计一个文档级别翻译的基准测试。", "metrics": {"bleu_score": 41.01896114451473, "chrf_score": 37.24207612210784, "xcomet_score": 0.9896488189697266, "xcomet_qe_score": 0.985325813293457, "metricx_score": 1.624791145324707, "metricx_qe_score": 1.7125370502471924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们所识别的这五个语篇现象,我们创建了标注器,以自动识别与该现象相关的词语,", "metrics": {"bleu_score": 31.615494316724458, "chrf_score": 25.774728752568738, "xcomet_score": 0.8452422022819519, "xcomet_qe_score": 0.8377835750579834, "metricx_score": 1.1571929454803467, "metricx_qe_score": 1.181905746459961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并将我们的标注器命名为多语种语篇感知标注器,简称MUDA标注器。", "metrics": {"bleu_score": 6.120673474735603, "chrf_score": 14.798575072997847, "xcomet_score": 0.847632646560669, "xcomet_qe_score": 0.8578512072563171, "metricx_score": 1.2755649089813232, "metricx_qe_score": 1.6139843463897705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们还可以注意到,不同语言中这些话语现象的比例也各不相同。", "metrics": {"bleu_score": 47.30022487962835, "chrf_score": 45.44763035296069, "xcomet_score": 0.9965336322784424, "xcomet_qe_score": 0.9917119741439819, "metricx_score": 0.6075845956802368, "metricx_qe_score": 0.810422420501709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们使用Muda标注器,将其应用于用于评估的平行语料库。并且,我们将所选的翻译指标应用于Muda标注器识别出的上下文相关的示例。", "metrics": {"bleu_score": 43.42818252304961, "chrf_score": 40.229281392202196, "xcomet_score": 0.8627927303314209, "xcomet_qe_score": 0.8244377374649048, "metricx_score": 2.258063793182373, "metricx_qe_score": 1.950414776802063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们利用基准测试以及其他指标,在文档级别机器翻译层面上评估不同的模型。", "metrics": {"bleu_score": 30.71922521574117, "chrf_score": 29.37840852767038, "xcomet_score": 0.9763190746307373, "xcomet_qe_score": 0.9676847457885742, "metricx_score": 0.923141598701477, "metricx_qe_score": 1.1336051225662231, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,例如对于BLEU分数,我们发现上下文无关的模型具有最佳表现。", "metrics": {"bleu_score": 39.26623085600952, "chrf_score": 39.984987329965, "xcomet_score": 0.8806051015853882, "xcomet_qe_score": 0.7732129096984863, "metricx_score": 1.770951747894287, "metricx_qe_score": 1.9263826608657837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果使用COMET,则情境感知的模型表现最佳。", "metrics": {"bleu_score": 15.004266966877431, "chrf_score": 29.80604660805813, "xcomet_score": 0.9018087387084961, "xcomet_qe_score": 0.8960155248641968, "metricx_score": 2.3311219215393066, "metricx_qe_score": 3.0088870525360107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用词语F值,那么有情境和无情境的模型表现可比。", "metrics": {"bleu_score": 21.646164380928848, "chrf_score": 20.04351322488914, "xcomet_score": 0.7621017098426819, "xcomet_qe_score": 0.8180594444274902, "metricx_score": 2.6040098667144775, "metricx_qe_score": 2.6429085731506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,仅使用语料库层面的指标来确定最佳文档级别翻译系统是困难的。", "metrics": {"bleu_score": 41.25891045606034, "chrf_score": 32.778051606416454, "xcomet_score": 0.9938255548477173, "xcomet_qe_score": 0.986249566078186, "metricx_score": 0.7520442008972168, "metricx_qe_score": 1.1506285667419434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用Muda基准来评估模型,并发现对于某些话语现象,例如正式程度和词汇连贯性,上下文感知的模型比不利用上下文的模型具有显著更高的准确性。", "metrics": {"bleu_score": 49.16424616292054, "chrf_score": 45.10434257036937, "xcomet_score": 0.7807571887969971, "xcomet_qe_score": 0.7400961518287659, "metricx_score": 2.7867257595062256, "metricx_qe_score": 3.7632217407226562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在其他现象(如省略、代词和动词形式)上,与未使用上下文的模型相比并没有显", "metrics": {"bleu_score": 48.69995958723945, "chrf_score": 40.73614705099068, "xcomet_score": 0.7927752733230591, "xcomet_qe_score": 0.6965503692626953, "metricx_score": 6.386134147644043, "metricx_qe_score": 2.5012736320495605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "著改善。这表明我们需要在文档级别翻译方面看到更大的进步。", "metrics": {"bleu_score": 17.889125705681394, "chrf_score": 17.224978958523838, "xcomet_score": 0.7021211385726929, "xcomet_qe_score": 0.7559837698936462, "metricx_score": 6.113309383392334, "metricx_qe_score": 7.933414936065674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,基准测试表明,在文档级别翻译方面,DeepL 通常比谷歌翻译更准确。", "metrics": {"bleu_score": 67.14911323976281, "chrf_score": 63.10982164632962, "xcomet_score": 0.9532425403594971, "xcomet_qe_score": 0.8710501790046692, "metricx_score": 1.1956039667129517, "metricx_qe_score": 1.350762128829956, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结而言,我们对14种语言对进行数据驱动分析,以确定何时翻译需要上下文。 然后,我们利用研究结果构建一个文档级别机器翻译的基准,这有助于我们识别哪些语篇现象模型能够良好处理,哪些无法处理,以及哪些翻译系统擅长文档级别的翻译。", "metrics": {"bleu_score": 39.84333299456909, "chrf_score": 36.645436651212854, "xcomet_score": 0.9135618805885315, "xcomet_qe_score": 0.9183006286621094, "metricx_score": 2.3359529972076416, "metricx_qe_score": 3.6843323707580566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多伦多见。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9990917444229126, "xcomet_qe_score": 0.985295832157135, "metricx_score": 0.46565622091293335, "metricx_qe_score": 1.37757408618927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是Yanis Lavrak,我将向您介绍我们在Dr. BERT方面的研究成果。Dr. BERT是一个在法语环境下,针对生物医学和临床领域训练的强大预训练模型。", "metrics": {"bleu_score": 27.18862392535844, "chrf_score": 35.85504229528086, "xcomet_score": 0.8376731276512146, "xcomet_qe_score": 0.8267472982406616, "metricx_score": 2.5779452323913574, "metricx_qe_score": 2.4140844345092773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个演示文稿中,我们首先讨论医疗保健领域的语言模型。", "metrics": {"bleu_score": 41.48387761083271, "chrf_score": 38.10555180854075, "xcomet_score": 0.8970745801925659, "xcomet_qe_score": 0.8990265130996704, "metricx_score": 2.4907922744750977, "metricx_qe_score": 2.1350557804107666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 83.05389167974835, "chrf_score": 81.33105592664415, "xcomet_score": 0.9893636703491211, "xcomet_qe_score": 0.9912564754486084, "metricx_score": 0.384817510843277, "metricx_qe_score": 0.6998237371444702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个基于法语的生物医学模型,名为 Dr. Bert,该模型基于 Roberta,并在 NACHOS 数据集上进行了训练。NACHOS 是一个从网络收集的医学众包数据集合。", "metrics": {"bleu_score": 36.60832107277991, "chrf_score": 40.504106565772936, "xcomet_score": 0.6537071466445923, "xcomet_qe_score": 0.5772133469581604, "metricx_score": 2.2216479778289795, "metricx_qe_score": 2.3429114818573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了对多种预训练设置和数据源模型的比较。", "metrics": {"bleu_score": 46.46232199104104, "chrf_score": 41.084089660144265, "xcomet_score": 0.8531221151351929, "xcomet_qe_score": 0.8874369859695435, "metricx_score": 1.7392151355743408, "metricx_qe_score": 2.4701170921325684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们展示了我们在法语环境下 11 个生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 49.01076067225541, "chrf_score": 45.31073774221205, "xcomet_score": 0.7984768152236938, "xcomet_qe_score": 0.7827163934707642, "metricx_score": 1.988707423210144, "metricx_qe_score": 2.957083225250244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结实验结果,并提供更多关于如何访问模型的详细信息。", "metrics": {"bleu_score": 31.01012991614281, "chrf_score": 26.885564267358692, "xcomet_score": 0.9095258712768555, "xcomet_qe_score": 0.9510643482208252, "metricx_score": 0.280500203371048, "metricx_qe_score": 0.2552306652069092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务的最有效方法之一,相较于Word2Vec、FastText或NWO等历史上的静态和情境化方法,BERT带来了巨大的性能提升。", "metrics": {"bleu_score": 52.96854393438193, "chrf_score": 52.68680023292494, "xcomet_score": 0.7726246118545532, "xcomet_qe_score": 0.8006109595298767, "metricx_score": 3.504392147064209, "metricx_qe_score": 3.529818296432495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此后,该模型已被应用于许多其他语言,例如法语中的Camembert,以及生物医学领域中的PAMED-BERT和BioBERT,以及临床领域的Clinical-BERT,但主要是在英语中。", "metrics": {"bleu_score": 50.10016528920832, "chrf_score": 52.80341449776091, "xcomet_score": 0.804049551486969, "xcomet_qe_score": 0.7387939095497131, "metricx_score": 3.245537042617798, "metricx_qe_score": 3.162531852722168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的专业模型十分稀缺,并且通常基于持续预训练,这是由于缺乏特定领域的训练数据所致。", "metrics": {"bleu_score": 32.56852650511272, "chrf_score": 33.33668596060826, "xcomet_score": 0.894804060459137, "xcomet_qe_score": 0.8449996709823608, "metricx_score": 0.8575742244720459, "metricx_qe_score": 1.0620239973068237, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,直到现在,法国尚未拥有适用于生物医学的开源现代化平台。", "metrics": {"bleu_score": 21.631187459215713, "chrf_score": 20.525078018345162, "xcomet_score": 0.8772093653678894, "xcomet_qe_score": 0.8936212658882141, "metricx_score": 3.1303608417510986, "metricx_qe_score": 2.958681344985962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们因此会自问,对于广泛的应用场景,最合适的数据来源是什么?而目前这些数据可以作为临床数据的良好替代品。", "metrics": {"bleu_score": 29.46289725473622, "chrf_score": 30.813713088498133, "xcomet_score": 0.9130836725234985, "xcomet_qe_score": 0.9091203212738037, "metricx_score": 0.8653668165206909, "metricx_qe_score": 0.8670921325683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将 Dr. Burt 与我们的舒伯特模型进行比较,该模型基于从我们家属隶属的非大学医院获取的匿名数据。", "metrics": {"bleu_score": 38.79058632872874, "chrf_score": 31.97369306540424, "xcomet_score": 0.5205551385879517, "xcomet_qe_score": 0.47478824853897095, "metricx_score": 6.801929473876953, "metricx_qe_score": 7.25070858001709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们会问自己,我们需要多少数据才能在法语数据上训练一个专业模型?", "metrics": {"bleu_score": 35.82530252528474, "chrf_score": 32.775507762107544, "xcomet_score": 0.9425191879272461, "xcomet_qe_score": 0.7803294658660889, "metricx_score": 1.1684293746948242, "metricx_qe_score": 1.2986899614334106, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4GB,8GB,还是更多?", "metrics": {"bleu_score": 32.46679154750989, "chrf_score": 35.52656937913231, "xcomet_score": 0.9826616048812866, "xcomet_qe_score": 0.9555097818374634, "metricx_score": 0.2518044710159302, "metricx_qe_score": 0.5453147888183594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较四个从零开始的模型。第一个版本是七吉字节的Dr. Bert,第二个版本是四吉字节的Nachos集合。 舒伯特模型的一个初始版本是一个临床模型,使用了从临床记录中提取的 4 GB 句子。而舒伯特模型的最终版本则混合了 4 GB 的自然语言子集和 4 GB 的临床记录。", "metrics": {"bleu_score": 29.885576525500493, "chrf_score": 28.020327950819624, "xcomet_score": 0.37430140376091003, "xcomet_qe_score": 0.36973389983177185, "metricx_score": 9.362261772155762, "metricx_qe_score": 9.058603286743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这项对比之外,我们还引入了三个在持续预训练中训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 65.71976436775957, "chrf_score": 58.383787376085195, "xcomet_score": 0.8798749446868896, "xcomet_qe_score": 0.8592780828475952, "metricx_score": 2.079575538635254, "metricx_qe_score": 2.6546168327331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种基于Camembert权重,并在四吉字节的玉米片数据集上训练的模型。", "metrics": {"bleu_score": 6.495958253289972, "chrf_score": 20.20020217591285, "xcomet_score": 0.5551877617835999, "xcomet_qe_score": 0.5560594201087952, "metricx_score": 5.425342559814453, "metricx_qe_score": 6.350530624389648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一种同样基于Camembert,但这次在四吉字节的矿渣和碎石上进行训练。 最后,还有一个基于英语生物医学模型,名为Bermud-Bert,并在四个吉字节的数据集上进行了训练。", "metrics": {"bleu_score": 24.87044684229329, "chrf_score": 26.53655669498871, "xcomet_score": 0.32145100831985474, "xcomet_qe_score": 0.3418668508529663, "metricx_score": 10.5359525680542, "metricx_qe_score": 10.264321327209473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们拥有七个模型。", "metrics": {"bleu_score": 51.56626918239821, "chrf_score": 51.44307308944684, "xcomet_score": 0.9923571348190308, "xcomet_qe_score": 0.9546388387680054, "metricx_score": 0.324026882648468, "metricx_qe_score": 0.4732546806335449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为评估我们的七个模型,我们收集了多个公开和私有的“不激动人心的”任务,例如姓名和身份识别、分类、词性标注和问答。", "metrics": {"bleu_score": 39.34978775728061, "chrf_score": 36.78706721370432, "xcomet_score": 0.5519965887069702, "xcomet_qe_score": 0.5730069875717163, "metricx_score": 5.587621212005615, "metricx_qe_score": 5.976849555969238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行比较,这些基线模型分别是:Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCnet 4 GB、PumaBERT、BioBERT 和 ClinicalBERT。", "metrics": {"bleu_score": 40.83255167741674, "chrf_score": 55.81107483090014, "xcomet_score": 0.4656994938850403, "xcomet_qe_score": 0.501855194568634, "metricx_score": 5.763143539428711, "metricx_qe_score": 6.184150695800781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型的突出之处在于,其在与模型训练数据性质相同的数据集上表现最佳。", "metrics": {"bleu_score": 38.61621366994311, "chrf_score": 31.40280625288976, "xcomet_score": 0.7397814393043518, "xcomet_qe_score": 0.26325488090515137, "metricx_score": 1.7873393297195435, "metricx_qe_score": 1.8339530229568481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以获取的数据来源来看,可以观察到来自异构来源的数据似乎更为灵活。", "metrics": {"bleu_score": 36.7336142605128, "chrf_score": 39.04118359466349, "xcomet_score": 0.7352129220962524, "xcomet_qe_score": 0.7253791093826294, "metricx_score": 3.3881125450134277, "metricx_qe_score": 3.0515623092651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以转化为更好的性能。", "metrics": {"bleu_score": 69.19907115708301, "chrf_score": 62.75376435159043, "xcomet_score": 0.9759578704833984, "xcomet_qe_score": 0.9726451635360718, "metricx_score": 3.253467559814453, "metricx_qe_score": 4.15842866897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从零开始的免费训练似乎在大多数任务中获得了更高的性能。", "metrics": {"bleu_score": 40.35118319685424, "chrf_score": 36.4712077205125, "xcomet_score": 0.7573418617248535, "xcomet_qe_score": 0.7495787143707275, "metricx_score": 6.432864189147949, "metricx_qe_score": 6.946683883666992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们在持续预训练方面进行的实验,使用了在 NACHOS 4GB 子集上训练过的 Pumet-BERT 的权重和分词器,结果与从零开始训练的 Dr.BERT 4GB 版本相当。", "metrics": {"bleu_score": 17.691980684272316, "chrf_score": 36.51543273839341, "xcomet_score": 0.6368926167488098, "xcomet_qe_score": 0.5430949926376343, "metricx_score": 5.911480903625488, "metricx_qe_score": 5.895193099975586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这对于基于CamemBERT权重和token leather的模型而言并非如此,后者则存在稳定性问题。", "metrics": {"bleu_score": 47.95320680083102, "chrf_score": 50.63484229514484, "xcomet_score": 0.7645514011383057, "xcomet_qe_score": 0.7700444459915161, "metricx_score": 6.2906413078308105, "metricx_qe_score": 6.354944229125977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为总结,我们的优化系统在11个下游任务中表现优于9个任务,并且在此,其结果超越了通用模型Camembert的全球表现。", "metrics": {"bleu_score": 32.07898006877354, "chrf_score": 30.463656655068345, "xcomet_score": 0.6732934713363647, "xcomet_qe_score": 0.5867961645126343, "metricx_score": 3.9418118000030518, "metricx_qe_score": 3.439676523208618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业数据更好,更专业的数据更好,但其可扩展性较差。 所有", "metrics": {"bleu_score": 24.23576171182365, "chrf_score": 23.27545603002911, "xcomet_score": 0.5843871235847473, "xcomet_qe_score": 0.4653018116950989, "metricx_score": 4.95248556137085, "metricx_qe_score": 3.912409782409668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 NACHOS 获取的预训练模型均可在 UGIM 平台免费获取,所有训练脚本则位于我们的 GitHub 仓库中。", "metrics": {"bleu_score": 35.363137171819794, "chrf_score": 36.97063845292616, "xcomet_score": 0.8429843187332153, "xcomet_qe_score": 0.8561066389083862, "metricx_score": 4.8380842208862305, "metricx_qe_score": 5.2147216796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的报告,我们期待在多伦多会后采取行动。", "metrics": {"bleu_score": 13.62935817240798, "chrf_score": 14.320731184405364, "xcomet_score": 0.7285712361335754, "xcomet_qe_score": 0.8094033598899841, "metricx_score": 5.028293132781982, "metricx_qe_score": 6.160674095153809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼,今天我将简要介绍我们关于使用多重集标记和潜在排列,在没有树结构的情况下实现组合泛化的论文。", "metrics": {"bleu_score": 44.23030352522333, "chrf_score": 38.83462639428233, "xcomet_score": 0.9591779708862305, "xcomet_qe_score": 0.891787052154541, "metricx_score": 1.279592514038086, "metricx_qe_score": 2.135007619857788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和我的导师亚历山大·科勒 (Alexander Koller) 及伊万·季托夫 (Ivan Titov) 共同完成的工作。", "metrics": {"bleu_score": 6.54458140095367, "chrf_score": 51.98301851447253, "xcomet_score": 0.9752718210220337, "xcomet_qe_score": 0.8805795907974243, "metricx_score": 1.916438102722168, "metricx_qe_score": 1.7986401319503784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合概化能力可以理解为学习者处理更深层递归和训练期间单独见过的短语新组合的能力。", "metrics": {"bleu_score": 66.15249045884207, "chrf_score": 59.92210981523396, "xcomet_score": 0.736842155456543, "xcomet_qe_score": 0.6555831432342529, "metricx_score": 4.555372714996338, "metricx_qe_score": 5.916821479797363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的语境下,对组合概括能力进行测试可能如下所示。", "metrics": {"bleu_score": 47.467913885027954, "chrf_score": 46.64124721226221, "xcomet_score": 0.9183334112167358, "xcomet_qe_score": 0.9746440649032593, "metricx_score": 1.2040200233459473, "metricx_qe_score": 1.4069201946258545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "和往常一样,我们拥有一个训练集中的", "metrics": {"bleu_score": 34.05204944353419, "chrf_score": 28.20548732313438, "xcomet_score": 0.6177489161491394, "xcomet_qe_score": 0.31560736894607544, "metricx_score": 6.162525177001953, "metricx_qe_score": 7.948999404907227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语句,在本例中,是“那个女孩睡了”,", "metrics": {"bleu_score": 15.125149328637187, "chrf_score": 9.242256901639522, "xcomet_score": 0.6591212153434753, "xcomet_qe_score": 0.6805050373077393, "metricx_score": 3.088038682937622, "metricx_qe_score": 2.4804563522338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及“玛丽知道那个女孩睡了”。", "metrics": {"bleu_score": 41.19015829847377, "chrf_score": 25.507305016090655, "xcomet_score": 0.9954113960266113, "xcomet_qe_score": 0.970173716545105, "metricx_score": 1.6154539585113525, "metricx_qe_score": 2.9175729751586914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些话语与逻辑形式配对,后者代表着其核心意义。", "metrics": {"bleu_score": 11.1204122745073, "chrf_score": 15.0687021584014, "xcomet_score": 0.9720526933670044, "xcomet_qe_score": 0.9649379253387451, "metricx_score": 1.2572795152664185, "metricx_qe_score": 1.2924152612686157, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估方法不同,测试集并非来自相同分布,而是包含结构上未曾见过的逻辑形式。", "metrics": {"bleu_score": 51.03426662964541, "chrf_score": 43.14378631941971, "xcomet_score": 0.9605329036712646, "xcomet_qe_score": 0.9004154205322266, "metricx_score": 0.9872278571128845, "metricx_qe_score": 1.6340224742889404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此示例中,模型在训练过程中观察到浅层递归,并针对具有更深层递归的示例进行测试。", "metrics": {"bleu_score": 26.036802768146032, "chrf_score": 21.91887696597665, "xcomet_score": 0.9259313344955444, "xcomet_qe_score": 0.8825356960296631, "metricx_score": 1.540426254272461, "metricx_qe_score": 2.2051920890808105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种泛化到分布外的数据,并且常常生成与输入内容脱节的输出。", "metrics": {"bleu_score": 18.656179432701677, "chrf_score": 18.766998684352227, "xcomet_score": 0.7178128361701965, "xcomet_qe_score": 0.7277418971061707, "metricx_score": 3.970461845397949, "metricx_qe_score": 3.820836305618286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其地,他们常常无法再现输入与输出之间的系统性对应关系,例如示例中用颜色标注的对应关系。", "metrics": {"bleu_score": 58.975380637593624, "chrf_score": 51.392671202318056, "xcomet_score": 0.9559893608093262, "xcomet_qe_score": 0.9510253071784973, "metricx_score": 2.0232017040252686, "metricx_qe_score": 1.2598804235458374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决这一问题的一种常用方法是在模型中集成树。", "metrics": {"bleu_score": 52.811855100855176, "chrf_score": 47.002732004107166, "xcomet_score": 0.9277065992355347, "xcomet_qe_score": 0.9266015291213989, "metricx_score": 0.4297039806842804, "metricx_qe_score": 1.0138875246047974, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树状结构旨在捕捉将话语与逻辑形式关联起来的构成过程。", "metrics": {"bleu_score": 15.10779917810071, "chrf_score": 17.02618369285036, "xcomet_score": 0.9742289781570435, "xcomet_qe_score": 0.8533188104629517, "metricx_score": 1.3589262962341309, "metricx_qe_score": 1.5244083404541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这运作良好,但树木通常不会提供,需要以某种方式获取。", "metrics": {"bleu_score": 13.749017868312801, "chrf_score": 14.86170683204516, "xcomet_score": 0.7230700254440308, "xcomet_qe_score": 0.7845137119293213, "metricx_score": 3.428290843963623, "metricx_qe_score": 2.9342198371887207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本高昂的过程。", "metrics": {"bleu_score": 58.9597026996279, "chrf_score": 52.51139776496644, "xcomet_score": 0.9886571168899536, "xcomet_qe_score": 0.9861245155334473, "metricx_score": 0.5039506554603577, "metricx_qe_score": 0.5730300545692444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及对逻辑形式进行大量的特定于形式化的预处理,例如处理变量符号。", "metrics": {"bleu_score": 51.4420122091352, "chrf_score": 43.10253096839909, "xcomet_score": 0.9721168279647827, "xcomet_qe_score": 0.9708794355392456, "metricx_score": 1.047396183013916, "metricx_qe_score": 1.1042828559875488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树状结构也可能涉及专门的语法归纳程序。", "metrics": {"bleu_score": 62.472375173961176, "chrf_score": 57.44265688731633, "xcomet_score": 0.9778578281402588, "xcomet_qe_score": 0.9693984985351562, "metricx_score": 1.808321475982666, "metricx_qe_score": 1.6390360593795776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,而是引入了一种神经序列到序列模型,该模型直接对输入片段和输出片段之间的对应关系进行建模。", "metrics": {"bleu_score": 67.31540635273258, "chrf_score": 61.18516042526417, "xcomet_score": 0.8181840181350708, "xcomet_qe_score": 0.8351508378982544, "metricx_score": 1.5752846002578735, "metricx_qe_score": 1.5399442911148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是首次展示,在无需依赖树结构的情况下,对更深层递归具有强大的泛化能力。", "metrics": {"bleu_score": 34.24389956607099, "chrf_score": 30.618364974994982, "xcomet_score": 0.8654959201812744, "xcomet_qe_score": 0.8399057388305664, "metricx_score": 2.641010284423828, "metricx_qe_score": 3.3971762657165527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法分两个步骤从输入预测输出。", "metrics": {"bleu_score": 69.01374299426456, "chrf_score": 61.21378893437716, "xcomet_score": 0.9957408905029297, "xcomet_qe_score": 0.9919648170471191, "metricx_score": 0.5007511973381042, "metricx_qe_score": 0.7281056642532349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入token标记一个无序的多重集,其中包含将在输出中出现的token。", "metrics": {"bleu_score": 21.445044243405057, "chrf_score": 21.264277665838218, "xcomet_score": 0.7894258499145508, "xcomet_qe_score": 0.8107885122299194, "metricx_score": 4.400974750518799, "metricx_qe_score": 3.611215829849243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步完成后,我们已经拥有了所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 52.093830028342616, "chrf_score": 44.79438254843772, "xcomet_score": 0.9190734624862671, "xcomet_qe_score": 0.8902223110198975, "metricx_score": 1.9524834156036377, "metricx_qe_score": 3.185408592224121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是在第二步中,我们使用另一个模型来预测一个排列,以便将它们排列成正确的顺序。", "metrics": {"bleu_score": 50.386404721170486, "chrf_score": 49.577522572911725, "xcomet_score": 0.8498216867446899, "xcomet_qe_score": 0.796360969543457, "metricx_score": 2.8402271270751953, "metricx_qe_score": 4.003622055053711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提出一种新方法来预测一个排列,该方法对可能的排列没有施加任何硬性约束。", "metrics": {"bleu_score": 55.22258474540334, "chrf_score": 46.875336709895535, "xcomet_score": 0.9677836894989014, "xcomet_qe_score": 0.9089632630348206, "metricx_score": 2.2737717628479004, "metricx_qe_score": 3.489957332611084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 48.620266417318525, "chrf_score": 41.561795590015564, "xcomet_score": 0.9840943813323975, "xcomet_qe_score": 0.9654589891433716, "metricx_score": 0.7629964351654053, "metricx_qe_score": 1.3032432794570923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的排列模型的工作方式大致如下。", "metrics": {"bleu_score": 59.06871848918506, "chrf_score": 49.06583817911048, "xcomet_score": 0.9818673133850098, "xcomet_qe_score": 0.886275589466095, "metricx_score": 0.6464743614196777, "metricx_qe_score": 1.5562429428100586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左向右遍历输出,并确定在每个位置放置哪个多重集标记。", "metrics": {"bleu_score": 61.34588734928764, "chrf_score": 55.803046889253785, "xcomet_score": 0.8359099626541138, "xcomet_qe_score": 0.7686539888381958, "metricx_score": 1.7988028526306152, "metricx_qe_score": 2.701545238494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需选择一个,如图中红色高亮所示。", "metrics": {"bleu_score": 52.98537180556899, "chrf_score": 45.091660042745225, "xcomet_score": 0.9343752861022949, "xcomet_qe_score": 0.9127397537231445, "metricx_score": 0.4958055019378662, "metricx_qe_score": 0.5992197394371033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们跳转到下一个多重集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 73.10296287680117, "chrf_score": 67.94671218521793, "xcomet_score": 0.8122124671936035, "xcomet_qe_score": 0.8322657942771912, "metricx_score": 3.010741949081421, "metricx_qe_score": 3.209465980529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个token,通过跳转到另一个多集token来实现。", "metrics": {"bleu_score": 67.55709397553608, "chrf_score": 59.37608489731387, "xcomet_score": 0.7189053297042847, "xcomet_qe_score": 0.72564697265625, "metricx_score": 5.771546363830566, "metricx_qe_score": 4.499545097351074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续此过程。 直至每个标记从第一阶段都被访问恰好一次。", "metrics": {"bleu_score": 32.46679154750989, "chrf_score": 26.416632749966084, "xcomet_score": 0.8102909326553345, "xcomet_qe_score": 0.7983769774436951, "metricx_score": 5.845534801483154, "metricx_qe_score": 5.444853782653809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让您先睹为快,我们在此将我们的方法与其他无树模型在 COGS 基准测试上进行对比。我们的模型在泛化", "metrics": {"bleu_score": 43.86071358063095, "chrf_score": 44.84325136720832, "xcomet_score": 0.6087077856063843, "xcomet_qe_score": 0.6494691371917725, "metricx_score": 6.159307479858398, "metricx_qe_score": 5.528905391693115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "到更深层递归方面,以显著的优势超越了其他模型。", "metrics": {"bleu_score": 30.541872609428147, "chrf_score": 25.926538184509436, "xcomet_score": 0.6450021862983704, "xcomet_qe_score": 0.6629170775413513, "metricx_score": 4.218882083892822, "metricx_qe_score": 5.2848052978515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,其他一些结构概括仍然极具挑战性。", "metrics": {"bleu_score": 6.962249700749937, "chrf_score": 10.02991123118487, "xcomet_score": 0.9266082048416138, "xcomet_qe_score": 0.9205566644668579, "metricx_score": 2.6694130897521973, "metricx_qe_score": 1.8125860691070557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的 技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9894026517868042, "xcomet_qe_score": 0.9734058380126953, "metricx_score": 0.49917685985565186, "metricx_qe_score": 0.5480197072029114, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对应关系在训练数据中并未给出。", "metrics": {"bleu_score": 35.87017559273802, "chrf_score": 35.589043469784734, "xcomet_score": 0.9850952625274658, "xcomet_qe_score": 0.9800618886947632, "metricx_score": 0.4515543282032013, "metricx_qe_score": 0.5208341479301453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的token,我们并不知道它来自哪个multisetter,这给训练带来了挑战。", "metrics": {"bleu_score": 57.97399865839437, "chrf_score": 46.49789405098861, "xcomet_score": 0.8122494220733643, "xcomet_qe_score": 0.8289541006088257, "metricx_score": 6.3864054679870605, "metricx_qe_score": 5.438868999481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列方式,但符合语言逻辑的排列方式可能是潜在的。", "metrics": {"bleu_score": 42.49120337116118, "chrf_score": 45.61686834832343, "xcomet_score": 0.9207504987716675, "xcomet_qe_score": 0.8778098821640015, "metricx_score": 1.9996588230133057, "metricx_qe_score": 2.3562374114990234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练过程中诱导对齐方式来解决这个问题。", "metrics": {"bleu_score": 54.16124426311167, "chrf_score": 50.23666091529305, "xcomet_score": 0.9003099203109741, "xcomet_qe_score": 0.8668609261512756, "metricx_score": 0.8850494027137756, "metricx_qe_score": 1.4118481874465942, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的排列方法非常灵活,但它带来了一个挑战,即找到得分最高的排列是NP-困难问题。", "metrics": {"bleu_score": 69.97476215353363, "chrf_score": 62.984344362083554, "xcomet_score": 0.8749946355819702, "xcomet_qe_score": 0.8491363525390625, "metricx_score": 2.41265606880188, "metricx_qe_score": 4.190112113952637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这个问题与旅行商问题相关。", "metrics": {"bleu_score": 51.18285025257892, "chrf_score": 41.36714220789614, "xcomet_score": 0.8264050483703613, "xcomet_qe_score": 0.8055687546730042, "metricx_score": 1.1301276683807373, "metricx_qe_score": 1.0240061283111572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过一种对GPU友好的连续松弛近似来完成此操作,这同时也允许我们反向传播并通过解决方案学习在语言学上更合理的排列组合。", "metrics": {"bleu_score": 36.754792970756135, "chrf_score": 36.48209175211502, "xcomet_score": 0.9090839624404907, "xcomet_qe_score": 0.7552756071090698, "metricx_score": 3.166968584060669, "metricx_qe_score": 4.1350626945495605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战,请参阅我们的论文或莅临我们的海报展示。", "metrics": {"bleu_score": 72.66051553003713, "chrf_score": 67.69476081987304, "xcomet_score": 0.961234450340271, "xcomet_qe_score": 0.9767502546310425, "metricx_score": 0.680596649646759, "metricx_qe_score": 0.7920163869857788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好,我是Akshata,今天我和我的合作作者Martin将共同介绍我们的工作——“Kipma步骤”,它评估了从多个来源整合知识的过程。这项", "metrics": {"bleu_score": 26.56356188247688, "chrf_score": 34.94992906640218, "xcomet_score": 0.4475322365760803, "xcomet_qe_score": 0.48318853974342346, "metricx_score": 8.460302352905273, "metricx_qe_score": 6.512561798095703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究院之间的合作。 国", "metrics": {"bleu_score": 67.03681862564032, "chrf_score": 65.79722531353154, "xcomet_score": 0.4951612651348114, "xcomet_qe_score": 0.37213659286499023, "metricx_score": 6.226986885070801, "metricx_qe_score": 4.489999771118164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立语言理解模型利用多种知识来源,例如其参数中包含的知识,通常通过预训练获得,以及在推理时输入中提供的知识。 近", "metrics": {"bleu_score": 61.38104270541124, "chrf_score": 56.65334159044166, "xcomet_score": 0.3678223192691803, "xcomet_qe_score": 0.4335297644138336, "metricx_score": 6.063918113708496, "metricx_qe_score": 3.912292957305908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期在问答等任务中的研究表明,模型可以利用预训练的时间知识来解决该任务。", "metrics": {"bleu_score": 60.36339072398112, "chrf_score": 50.62575555637701, "xcomet_score": 0.7620290517807007, "xcomet_qe_score": 0.7766611576080322, "metricx_score": 4.449645042419434, "metricx_qe_score": 4.278261661529541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,自然语言理解通常需要知识,而这些知识也", "metrics": {"bleu_score": 44.98628555476376, "chrf_score": 40.947128224429505, "xcomet_score": 0.7329330444335938, "xcomet_qe_score": 0.7528902292251587, "metricx_score": 6.421630382537842, "metricx_qe_score": 4.522568225860596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理时被提供。 约翰通过电视看到了新当选的总统。", "metrics": {"bleu_score": 22.597806406737355, "chrf_score": 13.166239125499748, "xcomet_score": 0.1564481258392334, "xcomet_qe_score": 0.14267247915267944, "metricx_score": 7.102895736694336, "metricx_qe_score": 7.740753173828125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可能包含关于先例的运作方式以及 TVA 的信息,但它们无法可靠地得知特定事件中的实体约翰是谁,或新总统是谁,因为先例可能在预训练后发生了改变。", "metrics": {"bleu_score": 30.40210102378713, "chrf_score": 25.794024711143138, "xcomet_score": 0.5549345016479492, "xcomet_qe_score": 0.547207236289978, "metricx_score": 6.509108066558838, "metricx_qe_score": 6.756316661834717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于知识密集型自然语言理解任务而言,成功的模型需要具备整合并利用预训练时和推理时知识的能力。", "metrics": {"bleu_score": 46.286989867087364, "chrf_score": 39.18835597681761, "xcomet_score": 0.9959160089492798, "xcomet_qe_score": 0.9422624111175537, "metricx_score": 0.6513619422912598, "metricx_qe_score": 0.6483637690544128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本研究中,我们提出了一套用于知识整合的诊断测试方案。", "metrics": {"bleu_score": 58.79597338084386, "chrf_score": 53.69236358533593, "xcomet_score": 0.9856081008911133, "xcomet_qe_score": 0.9852451086044312, "metricx_score": 0.6561094522476196, "metricx_qe_score": 0.8474836349487305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入一项指代消解任务,旨在探究利用不同来源知识的能力。", "metrics": {"bleu_score": 46.632155526916236, "chrf_score": 37.88575007086114, "xcomet_score": 0.897290050983429, "xcomet_qe_score": 0.8646132349967957, "metricx_score": 3.344763994216919, "metricx_qe_score": 4.15408992767334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类参与者和已建立的共指消解模型对数据集进行了评估。", "metrics": {"bleu_score": 66.67025833042815, "chrf_score": 61.397662785812976, "xcomet_score": 0.8837405443191528, "xcomet_qe_score": 0.7807472944259644, "metricx_score": 1.5824452638626099, "metricx_qe_score": 2.474156379699707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们数据集的例子。", "metrics": {"bleu_score": 42.89807392106132, "chrf_score": 39.66819444815172, "xcomet_score": 0.8544865846633911, "xcomet_qe_score": 0.9016527533531189, "metricx_score": 0.7048636078834534, "metricx_qe_score": 1.1920368671417236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟夫文是一名法官。", "metrics": {"bleu_score": 58.73949094699213, "chrf_score": 36.96669541288196, "xcomet_score": 0.8716357946395874, "xcomet_qe_score": 0.8472004532814026, "metricx_score": 1.6071780920028687, "metricx_qe_score": 2.0856332778930664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基娅是一名面包师。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 59.333448823374546, "xcomet_score": 0.8765929937362671, "xcomet_qe_score": 0.8274440169334412, "metricx_score": 0.606604814529419, "metricx_qe_score": 0.8381551504135132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟夫文和基娅在公园相遇。在辛苦了一天,", "metrics": {"bleu_score": 25.921705371356865, "chrf_score": 24.696793800600954, "xcomet_score": 0.3790099322795868, "xcomet_qe_score": 0.35478639602661133, "metricx_score": 8.701706886291504, "metricx_qe_score": 9.27950382232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上裁决案件之后,他很高兴能放松身心。", "metrics": {"bleu_score": 31.32752091088918, "chrf_score": 25.498894223138414, "xcomet_score": 0.887624979019165, "xcomet_qe_score": 0.8885776996612549, "metricx_score": 1.841568112373352, "metricx_qe_score": 3.06638240814209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "任务是识别代词“他”所指代的正确实体,在本例中,该实体是仆人。", "metrics": {"bleu_score": 26.933192594605266, "chrf_score": 20.801219384500406, "xcomet_score": 0.8791160583496094, "xcomet_qe_score": 0.8804817795753479, "metricx_score": 4.167191982269287, "metricx_qe_score": 2.18270206451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个给定代词的分辨需要两类信息。", "metrics": {"bleu_score": 38.71493850247496, "chrf_score": 33.5055459157925, "xcomet_score": 0.9454680681228638, "xcomet_qe_score": 0.9376225471496582, "metricx_score": 1.654294490814209, "metricx_qe_score": 1.4670116901397705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,是实体特定的知识,例如“调查员是法官”。其", "metrics": {"bleu_score": 14.476355589511376, "chrf_score": 16.060330620002063, "xcomet_score": 0.5175928473472595, "xcomet_qe_score": 0.5496695637702942, "metricx_score": 6.020111083984375, "metricx_qe_score": 4.238642692565918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,是背景知识,例如“法官在法庭上审理案件”。 通常", "metrics": {"bleu_score": 48.5287957075915, "chrf_score": 41.6535586095636, "xcomet_score": 0.6425497531890869, "xcomet_qe_score": 0.6415174007415771, "metricx_score": 5.9820966720581055, "metricx_qe_score": 5.201079845428467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",背景知识是在大型语言模型预训练阶段习得的,而实体特定知识则通常在推理时观察到。", "metrics": {"bleu_score": 35.899913152161425, "chrf_score": 31.867813955260694, "xcomet_score": 0.8639912605285645, "xcomet_qe_score": 0.8252583742141724, "metricx_score": 3.3410165309906006, "metricx_qe_score": 3.4702963829040527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变这两个信息片段的可获得性,使其可能出现在单一来源或多个来源中。", "metrics": {"bleu_score": 49.60314322999824, "chrf_score": 46.60451194408643, "xcomet_score": 0.8972986936569214, "xcomet_qe_score": 0.839089035987854, "metricx_score": 1.4213881492614746, "metricx_qe_score": 1.3455640077590942, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们已经定义了 KITMOS 的三个设置。", "metrics": {"bleu_score": 22.355093096292105, "chrf_score": 44.17277647164332, "xcomet_score": 0.9097767472267151, "xcomet_qe_score": 0.8794193863868713, "metricx_score": 0.8700827360153198, "metricx_qe_score": 0.8035835027694702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有典型的设置,即背景预训练,在这个设置中,我们假设在预训练时可以获得背景知识。", "metrics": {"bleu_score": 36.841494295453835, "chrf_score": 33.11797920929268, "xcomet_score": 0.873711884021759, "xcomet_qe_score": 0.8591049909591675, "metricx_score": 2.4696028232574463, "metricx_qe_score": 2.7715094089508057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,存在背景信息,包括设定环境,在预训练时和推理时都可获取相关知识。", "metrics": {"bleu_score": 34.49090918070999, "chrf_score": 31.22906347289649, "xcomet_score": 0.7749583721160889, "xcomet_qe_score": 0.7537081241607666, "metricx_score": 2.6845479011535645, "metricx_qe_score": 4.0676116943359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,是背景推理环境,其中两种知识类型仅在推理时可用。", "metrics": {"bleu_score": 59.72409687371626, "chrf_score": 56.82424013641387, "xcomet_score": 0.9200055599212646, "xcomet_qe_score": 0.9146356582641602, "metricx_score": 1.622100591659546, "metricx_qe_score": 1.248462200164795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个设置尤其引人关注,因为它模拟了解决任务所需的基础知识并非模型预训练数据的一部分的情况。", "metrics": {"bleu_score": 55.616864109358076, "chrf_score": 49.77432198966926, "xcomet_score": 0.9673868417739868, "xcomet_qe_score": 0.963097333908081, "metricx_score": 0.8491542339324951, "metricx_qe_score": 0.9464617967605591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,由于新的职业在预训练之后发展起来。", "metrics": {"bleu_score": 20.313747122261766, "chrf_score": 18.978922954236612, "xcomet_score": 0.8656492233276367, "xcomet_qe_score": 0.8104125261306763, "metricx_score": 1.5762652158737183, "metricx_qe_score": 2.409454584121704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是如何控制事实在真实来源中的可获得性的示例。", "metrics": {"bleu_score": 29.50033996294386, "chrf_score": 25.392572232888643, "xcomet_score": 0.879309892654419, "xcomet_qe_score": 0.7871329188346863, "metricx_score": 1.3977575302124023, "metricx_qe_score": 1.8104891777038574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设政客竞选政府职务的背景知识蕴含在预训练参数中。在稀有时间语境下,我们提供反特定知识,例如:契切斯特是一位政客。", "metrics": {"bleu_score": 27.843614067054606, "chrf_score": 21.308236720150912, "xcomet_score": 0.5893446207046509, "xcomet_qe_score": 0.5575897693634033, "metricx_score": 6.456999778747559, "metricx_qe_score": 6.193143844604492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景之上设置的情况下,我们不仅提供反特定的信息,同时也提供关于政治人物在干预类型情境下的背景知识。", "metrics": {"bleu_score": 30.311022061983763, "chrf_score": 24.67833002252191, "xcomet_score": 0.540785014629364, "xcomet_qe_score": 0.5032241344451904, "metricx_score": 7.1661224365234375, "metricx_qe_score": 6.373265266418457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且在背景干扰设置方面,我们提供虚构职业“梅里图”而非“政治家”,因为“梅里图”不太可能出现在预训练范式中。", "metrics": {"bleu_score": 24.947443361453594, "chrf_score": 18.848373721506928, "xcomet_score": 0.5383833646774292, "xcomet_qe_score": 0.4993722140789032, "metricx_score": 5.751537322998047, "metricx_qe_score": 6.028496742248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类参与者和已建立的共指消解模型对数据集进行了评估。", "metrics": {"bleu_score": 66.67025833042815, "chrf_score": 61.397662785812976, "xcomet_score": 0.8847979307174683, "xcomet_qe_score": 0.778962254524231, "metricx_score": 1.5697251558303833, "metricx_qe_score": 2.4598917961120605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了在背景预训练设置中最困难变体上表现最佳的模型的结果。", "metrics": {"bleu_score": 55.82624918747362, "chrf_score": 47.20826266522506, "xcomet_score": 0.8874204158782959, "xcomet_qe_score": 0.8500862717628479, "metricx_score": 1.2117292881011963, "metricx_qe_score": 1.494484543800354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在针对KITMOS进行特定任务训练后,两个模型均表现不佳。", "metrics": {"bleu_score": 41.636127592703865, "chrf_score": 33.78005622074869, "xcomet_score": 0.7614706754684448, "xcomet_qe_score": 0.8048466444015503, "metricx_score": 2.602703094482422, "metricx_qe_score": 3.361473560333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在KITMOS上训练时,C2F和BFQF均显著优于随机选择。", "metrics": {"bleu_score": 17.105806367202913, "chrf_score": 25.47344714539827, "xcomet_score": 0.6289569139480591, "xcomet_qe_score": 0.68309086561203, "metricx_score": 3.9621081352233887, "metricx_qe_score": 5.5147480964660645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当模型在通用共指消解数据集上进行训练时,它们会学习利用表面线索,而这些线索在对kitmos进行测试时则无用,因为这些线索已被移除。 进一步", "metrics": {"bleu_score": 33.12198734629828, "chrf_score": 26.345849645259207, "xcomet_score": 0.47959765791893005, "xcomet_qe_score": 0.33201611042022705, "metricx_score": 6.290842533111572, "metricx_qe_score": 5.8303327560424805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "利用虚构知识进行的实验表明,即使是性能最佳的模型也无法可靠地整合仅在推理时提供的背景知识。 总而言之,", "metrics": {"bleu_score": 50.335513561810096, "chrf_score": 46.73089408368477, "xcomet_score": 0.7968580722808838, "xcomet_qe_score": 0.7789183855056763, "metricx_score": 4.436045169830322, "metricx_qe_score": 2.6248862743377686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文的主要结论是,许多共指革命模型在没有特定任务训练的情况下,似乎无法对来自不同来源的知识进行推理。", "metrics": {"bleu_score": 72.48875718102177, "chrf_score": 66.56155674759972, "xcomet_score": 0.8485518097877502, "xcomet_qe_score": 0.8215245008468628, "metricx_score": 4.923313140869141, "metricx_qe_score": 5.7115278244018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过特定任务的训练,某些模型能够成功整合来自多个来源的知识。", "metrics": {"bleu_score": 60.099975518405934, "chrf_score": 53.10310217220674, "xcomet_score": 0.9991985559463501, "xcomet_qe_score": 0.9947906732559204, "metricx_score": 0.6448111534118652, "metricx_qe_score": 1.0051957368850708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最佳的模型,在可靠地整合仅在推理时呈现的先前知识方面似乎仍然存在困难。", "metrics": {"bleu_score": 48.233047878080654, "chrf_score": 46.92303073083291, "xcomet_score": 0.9208380579948425, "xcomet_qe_score": 0.9146574139595032, "metricx_score": 1.0074303150177002, "metricx_qe_score": 1.1835615634918213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多详情,请参阅我们的论文,并在GitHub上的代码中查看数据集。感谢您的", "metrics": {"bleu_score": 42.50002996145256, "chrf_score": 45.5760641619445, "xcomet_score": 0.747248649597168, "xcomet_qe_score": 0.7444769740104675, "metricx_score": 1.3984583616256714, "metricx_qe_score": 0.7625730037689209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "聆听。", "metrics": {"bleu_score": 0.0, "chrf_score": 53.34987593052109, "xcomet_score": 0.8254514932632446, "xcomet_qe_score": 0.6475603580474854, "metricx_score": 1.3102339506149292, "metricx_qe_score": 1.997286319732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Myra,今天我将介绍我们的论文《标记人格:利用自然语言提示来衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 64.45120359944525, "chrf_score": 59.66622262709169, "xcomet_score": 0.815061092376709, "xcomet_qe_score": 0.7988778352737427, "metricx_score": 1.897147297859192, "metricx_qe_score": 2.105703353881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这篇论文是我们与Esen Dermush和Dan Jorofsky合作完成的。", "metrics": {"bleu_score": 54.017258985951415, "chrf_score": 49.958895061273935, "xcomet_score": 0.8095206022262573, "xcomet_qe_score": 0.8579139113426208, "metricx_score": 4.059554100036621, "metricx_qe_score": 4.269434452056885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 41.222852304408335, "chrf_score": 41.171768238179595, "xcomet_score": 0.9893604516983032, "xcomet_qe_score": 0.9880325794219971, "metricx_score": 1.968329906463623, "metricx_qe_score": 4.322904109954834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施存在诸多局限性。", "metrics": {"bleu_score": 31.645000185694006, "chrf_score": 24.536301441072244, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.085751011967659, "metricx_qe_score": 0.20407018065452576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,而构建这些数据集耗时费力。 而且它们通常也仅测量非常具体的刻板印象,这意味着它们无法很好地推广到其他人群或情境,或者它们仅仅捕捉到非常笼统、宽泛的联想,例如与特定群体相关的负面联想。", "metrics": {"bleu_score": 44.44156218351759, "chrf_score": 38.93426642329595, "xcomet_score": 0.9346544742584229, "xcomet_qe_score": 0.8629316687583923, "metricx_score": 4.076325416564941, "metricx_qe_score": 4.065982341766357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,目前该领域的大部分研究都没有考虑到交叉性,交叉性指的是多元社会身份可能会加剧偏见,并成为伤害的独特载体。", "metrics": {"bleu_score": 43.476367213870404, "chrf_score": 42.08875717869323, "xcomet_score": 0.8654828071594238, "xcomet_qe_score": 0.6988677978515625, "metricx_score": 1.0585836172103882, "metricx_qe_score": 1.2193244695663452, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们依赖于这些新型指令微调大语言模型在响应提示中的指令方面表现出色的特性。", "metrics": {"bleu_score": 28.495377901224686, "chrf_score": 24.379320196577584, "xcomet_score": 0.7898389101028442, "xcomet_qe_score": 0.769332766532898, "metricx_score": 2.798564910888672, "metricx_qe_score": 2.5264856815338135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色设定,即通过提示语(例如:“想象你是一位亚洲女性,", "metrics": {"bleu_score": 30.69883799385012, "chrf_score": 30.531504613278155, "xcomet_score": 0.9388750791549683, "xcomet_qe_score": 0.9115952253341675, "metricx_score": 3.871422529220581, "metricx_qe_score": 4.0154194831848145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请描述一下你自己”)来描绘一个虚构的个体。 而且", "metrics": {"bleu_score": 31.52861344254502, "chrf_score": 51.84715403248619, "xcomet_score": 0.15484531223773956, "xcomet_qe_score": 0.14852569997310638, "metricx_score": 7.867847442626953, "metricx_qe_score": 5.318792819976807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这对于任何人口统计群体都具有很强的普适性,因为我们只需在这个提示中指定任何想要的身份标识即可。", "metrics": {"bleu_score": 48.86315699272574, "chrf_score": 41.23480611549424, "xcomet_score": 0.9718897342681885, "xcomet_qe_score": 0.8994749784469604, "metricx_score": 1.8140299320220947, "metricx_qe_score": 2.682065486907959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT-4 生成的一些示例。 我们立", "metrics": {"bleu_score": 77.60114635728617, "chrf_score": 95.69022088416465, "xcomet_score": 0.691868782043457, "xcomet_qe_score": 0.5205354690551758, "metricx_score": 6.174786567687988, "metricx_qe_score": 2.2567903995513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "刻看到,虽然这些输出在传统意义上并非明显消极或有毒, 有一些有趣的模式。 这", "metrics": {"bleu_score": 41.18317432418591, "chrf_score": 36.13375153007656, "xcomet_score": 0.46242523193359375, "xcomet_qe_score": 0.36359572410583496, "metricx_score": 12.296808242797852, "metricx_qe_score": 8.105880737304688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "位亚洲女性被描绘成不引人注目。 这位中东女性则被用诸如“异域风情”之类的词语来指代,这类似于对一个迷人地区所做的描述。 而", "metrics": {"bleu_score": 29.71133607097387, "chrf_score": 29.319021289061293, "xcomet_score": 0.5579144954681396, "xcomet_qe_score": 0.5626356601715088, "metricx_score": 6.288080215454102, "metricx_qe_score": 4.320285320281982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个有色人种的人物形象都提到了祖先,而白人男性人物形象则没有任何此类提及。", "metrics": {"bleu_score": 29.969401805468696, "chrf_score": 23.93435727514664, "xcomet_score": 0.7978075742721558, "xcomet_qe_score": 0.9107792377471924, "metricx_score": 1.7839252948760986, "metricx_qe_score": 1.522271990776062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法包含两个部分。", "metrics": {"bleu_score": 57.286689958163855, "chrf_score": 49.685271159545096, "xcomet_score": 0.9952645301818848, "xcomet_qe_score": 0.9829387664794922, "metricx_score": 0.13998496532440186, "metricx_qe_score": 0.2182009220123291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人物画像。 ", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 65.93509679113568, "xcomet_score": 0.8605829477310181, "xcomet_qe_score": 0.7847167253494263, "metricx_score": 0.7099262475967407, "metricx_qe_score": 0.9626864194869995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了生成这些角色设定,我们的提示语灵感来源于一项研究,该研究将这些提示语提供给人类受试者,发现这样也能引发种族刻板印象。", "metrics": {"bleu_score": 33.33726841029729, "chrf_score": 28.140543613977748, "xcomet_score": 0.8608739376068115, "xcomet_qe_score": 0.845076322555542, "metricx_score": 1.761112928390503, "metricx_qe_score": 1.7110052108764648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且这使得我们生成的角色模型与人工撰写的回复之间能够进行直接比较。", "metrics": {"bleu_score": 50.29311140805834, "chrf_score": 47.10956516545105, "xcomet_score": 0.9547133445739746, "xcomet_qe_score": 0.9088032841682434, "metricx_score": 1.2655874490737915, "metricx_qe_score": 1.6869909763336182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种用于区分标记群体和未标记群体所用的词汇,我稍后将详细阐述。", "metrics": {"bleu_score": 17.863430931185494, "chrf_score": 18.319214563885073, "xcomet_score": 0.8478941917419434, "xcomet_qe_score": 0.9485543966293335, "metricx_score": 1.0571718215942383, "metricx_qe_score": 1.1408941745758057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样做的好处在于,我们可以获得非常具体的刻板印象和模式,而无需依赖任何特定的词汇。", "metrics": {"bleu_score": 57.83879189630657, "chrf_score": 54.16496276873528, "xcomet_score": 0.9925782680511475, "xcomet_qe_score": 0.9077585935592651, "metricx_score": 0.6005589962005615, "metricx_qe_score": 0.9090573787689209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词汇法借鉴了社会语言学中的标记性概念,该概念指出存在一个未标记的默认状态,而任何偏离该默认状态的群体在语言上都是被标记的。", "metrics": {"bleu_score": 44.32329589408764, "chrf_score": 37.12056871134662, "xcomet_score": 0.8045390844345093, "xcomet_qe_score": 0.8629981279373169, "metricx_score": 0.9856970310211182, "metricx_qe_score": 1.0359013080596924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,一词“战士”通常与男性联系在一起。", "metrics": {"bleu_score": 36.327039079325615, "chrf_score": 31.693427534863, "xcomet_score": 0.8812558650970459, "xcomet_qe_score": 0.8617849349975586, "metricx_score": 1.7821106910705566, "metricx_qe_score": 1.8191018104553223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位女性战士时,他们通常会明确指出“单枪匹马的战士”,并在术语中注明“女性”。", "metrics": {"bleu_score": 30.418948633194166, "chrf_score": 25.20218832737236, "xcomet_score": 0.7480964064598083, "xcomet_qe_score": 0.7440462708473206, "metricx_score": 1.8937569856643677, "metricx_qe_score": 1.1558345556259155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的支配群体在语言和社交层面都属于非标记状态,而边缘群体则通常带有标记。", "metrics": {"bleu_score": 32.04393180953783, "chrf_score": 27.129393933805968, "xcomet_score": 0.9038377404212952, "xcomet_qe_score": 0.9089820384979248, "metricx_score": 0.8280563354492188, "metricx_qe_score": 0.9537551403045654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记组和标记组。 然后,我们使用“对抗性词汇”方法来比较这些人物画像,其基本原理是利用加权对数几率比来区分每个标记组中的顶级词汇。", "metrics": {"bleu_score": 41.7052307839603, "chrf_score": 35.048824233112306, "xcomet_score": 0.6349722146987915, "xcomet_qe_score": 0.5560911893844604, "metricx_score": 3.9140515327453613, "metricx_qe_score": 3.486362934112549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性这一群体,我们将使用“对抗性言辞”,并将法律神明的比例与白人群体和男性群体进行比较,因为这两个群体是对应的、无标记群体。", "metrics": {"bleu_score": 31.697186990519693, "chrf_score": 29.33681466741526, "xcomet_score": 0.5206735134124756, "xcomet_qe_score": 0.4651488959789276, "metricx_score": 7.162556171417236, "metricx_qe_score": 7.997420310974121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们来看看一些结果。", "metrics": {"bleu_score": 21.97281387499715, "chrf_score": 30.005158921584396, "xcomet_score": 0.9757978916168213, "xcomet_qe_score": 0.9675015211105347, "metricx_score": 0.34938740730285645, "metricx_qe_score": 0.4385901093482971, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个刻板印象词典,发现生成的角色模型包含比人工撰写的角色模型更多的刻板印象。", "metrics": {"bleu_score": 46.798284195902994, "chrf_score": 42.943780136162374, "xcomet_score": 0.9611999988555908, "xcomet_qe_score": 0.9568949937820435, "metricx_score": 1.7678676843643188, "metricx_qe_score": 1.9056662321090698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们真正考察词汇表中的词语分布时,会发现截然不同的情况。", "metrics": {"bleu_score": 28.40200788037189, "chrf_score": 25.805363736010595, "xcomet_score": 0.9786667823791504, "xcomet_qe_score": 0.9725238084793091, "metricx_score": 1.2776784896850586, "metricx_qe_score": 1.2207664251327515, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的角色拥有更高比例的 Luxon 词汇,但人类编写的角色则拥有更广泛的词汇分布,而出现在生成角色中的刻板印象词实际上仅仅是“高大”和“健壮”这两个词。", "metrics": {"bleu_score": 34.27391717617815, "chrf_score": 25.44253273182652, "xcomet_score": 0.8314591646194458, "xcomet_qe_score": 0.7163642644882202, "metricx_score": 5.150074481964111, "metricx_qe_score": 5.410588264465332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实际上仅限于正面的,或者至少非负面的。", "metrics": {"bleu_score": 25.80275885275182, "chrf_score": 23.193412388614227, "xcomet_score": 0.9236159324645996, "xcomet_qe_score": 0.9135962724685669, "metricx_score": 0.8453930616378784, "metricx_qe_score": 0.7239490747451782, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表实际上并没有很好地捕捉到我们在前面幻灯片中观察到的许多有害模式。", "metrics": {"bleu_score": 63.457989639265016, "chrf_score": 58.19259673167733, "xcomet_score": 0.9037457704544067, "xcomet_qe_score": 0.7371131181716919, "metricx_score": 1.0611516237258911, "metricx_qe_score": 1.3486558198928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向我们标记词汇法的结果,以展示这些看似积极的词语如何促进刻板印象和本质化叙事。", "metrics": {"bleu_score": 28.54610420564594, "chrf_score": 25.753235381710017, "xcomet_score": 0.7237372994422913, "xcomet_qe_score": 0.7064030170440674, "metricx_score": 2.787395715713501, "metricx_qe_score": 3.416125774383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描述如何反映出有害的模式。", "metrics": {"bleu_score": 62.931089756825216, "chrf_score": 54.27138998306559, "xcomet_score": 0.9867944717407227, "xcomet_qe_score": 0.9823914766311646, "metricx_score": 1.0008602142333984, "metricx_qe_score": 2.2394495010375977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体而言,最常见的词汇包括文化、传统、自豪和异域风情。而", "metrics": {"bleu_score": 4.11460900750382, "chrf_score": 7.167265709378706, "xcomet_score": 0.6224455237388611, "xcomet_qe_score": 0.6409276723861694, "metricx_score": 4.497036933898926, "metricx_qe_score": 2.7389626502990723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅通过它们与身份认同的关系来定义这些群体,并使其与白人主流群体区分开来。", "metrics": {"bleu_score": 42.87059272202451, "chrf_score": 41.67713792055649, "xcomet_score": 0.9887405633926392, "xcomet_qe_score": 1.0, "metricx_score": 1.1360284090042114, "metricx_qe_score": 1.04954993724823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这促成了一个漫长的歧视和排斥历史,针对这些群体。", "metrics": {"bleu_score": 27.55396296659942, "chrf_score": 22.58686997570253, "xcomet_score": 0.8352689743041992, "xcomet_qe_score": 0.8771331310272217, "metricx_score": 4.325676918029785, "metricx_qe_score": 4.574501991271973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词汇中也反映了许多常见的套路,尤其体现在有色人种女性身上。", "metrics": {"bleu_score": 33.98642477980264, "chrf_score": 28.107802620682843, "xcomet_score": 0.8337920308113098, "xcomet_qe_score": 0.8877256512641907, "metricx_score": 2.8334953784942627, "metricx_qe_score": 1.566438913345337, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁裔女性的词语包括“充满活力”和“曲线优美”。 这与热带主义这一母题相连。", "metrics": {"bleu_score": 32.0221722048033, "chrf_score": 21.961590633844473, "xcomet_score": 0.8918299674987793, "xcomet_qe_score": 0.8519515991210938, "metricx_score": 1.662236213684082, "metricx_qe_score": 1.624910593032837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性而言,这些词汇包括娇小、纤弱和丝滑。 这与亚洲女性长期以来被过度性化、被视为非常驯服和顺从的历史息息相关,等等。", "metrics": {"bleu_score": 31.879422369795947, "chrf_score": 24.243771180771738, "xcomet_score": 0.7431213855743408, "xcomet_qe_score": 0.9498198628425598, "metricx_score": 3.4897594451904297, "metricx_qe_score": 2.5209600925445557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性而言,我们看到一些最常见的词语包括“坚强”和“韧性”。", "metrics": {"bleu_score": 36.80594144198019, "chrf_score": 25.10407487018293, "xcomet_score": 0.9171204566955566, "xcomet_qe_score": 0.9136644005775452, "metricx_score": 1.5568673610687256, "metricx_qe_score": 1.5583453178405762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所称的“坚强黑人女性”原型有关。而且", "metrics": {"bleu_score": 43.94110961620203, "chrf_score": 37.03170097086326, "xcomet_score": 0.7706848382949829, "xcomet_qe_score": 0.757045328617096, "metricx_score": 4.010025501251221, "metricx_qe_score": 2.5480074882507324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",乍一看,这似乎是积极的, 有研究表明,这种原型实际上具有很大的危害性,因为它给这些群体带来了巨大的压力,要求他们面对社会障碍时表现出坚韧和强大。", "metrics": {"bleu_score": 37.32331470684705, "chrf_score": 30.54786473956503, "xcomet_score": 0.9040000438690186, "xcomet_qe_score": 0.8978192806243896, "metricx_score": 4.0310587882995605, "metricx_qe_score": 4.664198875427246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与其真正致力于改变那些障碍,不如给那些人施加克服它们的压力,这会导致这些人的健康状况出现非常负面的结果,以及其他危害。", "metrics": {"bleu_score": 26.35255743948611, "chrf_score": 22.85795814826575, "xcomet_score": 0.8633018136024475, "xcomet_qe_score": 0.8962939977645874, "metricx_score": 2.1737887859344482, "metricx_qe_score": 1.6290560960769653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词汇几乎只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 77.96137725566584, "chrf_score": 69.5509353505986, "xcomet_score": 0.8105084300041199, "xcomet_qe_score": 0.8460978269577026, "metricx_score": 1.5046545267105103, "metricx_qe_score": 2.2697906494140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,基于这些模式,我们得出了三项针对模型所有者的建议。", "metrics": {"bleu_score": 45.53178646511295, "chrf_score": 38.502755914323004, "xcomet_score": 0.9075511693954468, "xcomet_qe_score": 0.7793289422988892, "metricx_score": 1.2772469520568848, "metricx_qe_score": 3.262209892272949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究者,我们应该关注积极刻板印象和本质化叙事。", "metrics": {"bleu_score": 27.22589423069702, "chrf_score": 21.586876762733727, "xcomet_score": 0.844680666923523, "xcomet_qe_score": 0.8670278191566467, "metricx_score": 1.1680933237075806, "metricx_qe_score": 0.9656009078025818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该运用交叉视角研究偏见和危害,因为如果不这样做,可能会忽略许多问题。", "metrics": {"bleu_score": 46.036332319020694, "chrf_score": 35.327348708032424, "xcomet_score": 0.9365656971931458, "xcomet_qe_score": 0.8719860315322876, "metricx_score": 0.24977999925613403, "metricx_qe_score": 0.3979320228099823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,关于偏见缓解方法的透明度应该切实提升。 因为例如,像这些积极的刻板印象,我们并不知道这是因为存在某种奇怪的...... 过度且过度的价值一致性正在发生,或者可能存在其他诸如反刻板印象的方法,导致了这些有害的模式。", "metrics": {"bleu_score": 43.60524034566391, "chrf_score": 43.89361977691731, "xcomet_score": 0.6956768035888672, "xcomet_qe_score": 0.5907840728759766, "metricx_score": 6.552732944488525, "metricx_qe_score": 7.023074626922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实在无法在缺乏更多透明度的情况下做出任何假设或进行进一步研究。", "metrics": {"bleu_score": 36.7020403411147, "chrf_score": 31.3810980608833, "xcomet_score": 0.9866219758987427, "xcomet_qe_score": 0.9934821128845215, "metricx_score": 0.39155662059783936, "metricx_qe_score": 0.3958433270454407, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 25.690595421698053, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.27894243597984314, "metricx_qe_score": 0.5952762365341187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝您一切顺利。", "metrics": {"bleu_score": 6.770186228657864, "chrf_score": 3.546099290780142, "xcomet_score": 0.27951639890670776, "xcomet_qe_score": 0.23546001315116882, "metricx_score": 2.0922205448150635, "metricx_qe_score": 3.629934310913086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫易景伟,来自中国科学技术大学。", "metrics": {"bleu_score": 26.681730651789678, "chrf_score": 19.382603988316422, "xcomet_score": 0.9992905855178833, "xcomet_qe_score": 0.9987926483154297, "metricx_score": 0.5143870115280151, "metricx_qe_score": 0.5901054739952087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我很荣幸地向大家介绍一个短视频,内容是关于论文的", "metrics": {"bleu_score": 6.65422126355551, "chrf_score": 10.555746790726134, "xcomet_score": 0.8280056715011597, "xcomet_qe_score": 0.7979490756988525, "metricx_score": 1.8936275243759155, "metricx_qe_score": 1.375230073928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ":《你在抄我的模型吗?》——", "metrics": {"bleu_score": 44.534504264163466, "chrf_score": 40.63292471894734, "xcomet_score": 0.9433310031890869, "xcomet_qe_score": 0.7653841972351074, "metricx_score": 2.702338933944702, "metricx_qe_score": 3.8133349418640137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护嵌入和服务的预训练语言模型版权。 首先,", "metrics": {"bleu_score": 41.48886515256523, "chrf_score": 35.35029034304396, "xcomet_score": 0.5595022439956665, "xcomet_qe_score": 0.4664430618286133, "metricx_score": 4.020490646362305, "metricx_qe_score": 1.6697750091552734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们先介绍一下嵌入式服务(Embedding as a Service)的背景。", "metrics": {"bleu_score": 35.562549056277234, "chrf_score": 32.492441220721645, "xcomet_score": 0.9930582046508789, "xcomet_qe_score": 0.9958070516586304, "metricx_score": 0.3191766142845154, "metricx_qe_score": 0.3192383050918579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、LAMA、PALM这样的大型语言模型在自然语言理解和生成方面表现出卓越的能力。", "metrics": {"bleu_score": 52.86885241663542, "chrf_score": 66.72015469970137, "xcomet_score": 0.9804049730300903, "xcomet_qe_score": 0.9894722700119019, "metricx_score": 0.9165329933166504, "metricx_qe_score": 1.0021553039550781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式服务是建立在大型语言模型基础之上的服务之一,旨在辅助各种自然语言处理任务。", "metrics": {"bleu_score": 24.094227672998077, "chrf_score": 25.102265533775796, "xcomet_score": 0.9919551610946655, "xcomet_qe_score": 0.9946287870407104, "metricx_score": 0.5150850415229797, "metricx_qe_score": 0.5447394251823425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OpenAI 提供基于 GPT 的嵌入 API。", "metrics": {"bleu_score": 47.1364221970258, "chrf_score": 66.48341103651549, "xcomet_score": 0.9675271511077881, "xcomet_qe_score": 0.9052179455757141, "metricx_score": 0.847747266292572, "metricx_qe_score": 1.203486442565918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,近期研究表明,攻击者可能通过学习嵌入向量来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 46.93105681647231, "chrf_score": 38.85835838905983, "xcomet_score": 0.9085474014282227, "xcomet_qe_score": 0.909135639667511, "metricx_score": 1.2974318265914917, "metricx_qe_score": 1.6921947002410889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入向量作为服务的版权。", "metrics": {"bleu_score": 55.882651974144544, "chrf_score": 53.9579886689977, "xcomet_score": 0.9013322591781616, "xcomet_qe_score": 0.8936871290206909, "metricx_score": 1.6500622034072876, "metricx_qe_score": 1.6272464990615845, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为保护嵌入式服务的版权。一种解决方案是在服务提供方嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 61.68405586514766, "chrf_score": 54.00115798111028, "xcomet_score": 0.9566737413406372, "xcomet_qe_score": 0.9652730226516724, "metricx_score": 1.1215195655822754, "metricx_qe_score": 1.0433191061019897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下特性。", "metrics": {"bleu_score": 79.10665071754353, "chrf_score": 74.58916083916085, "xcomet_score": 0.999688982963562, "xcomet_qe_score": 0.9979780912399292, "metricx_score": 0.4538722038269043, "metricx_qe_score": 0.46247151494026184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入广告服务。", "metrics": {"bleu_score": 59.97820163128021, "chrf_score": 56.213802382206566, "xcomet_score": 0.9151304960250854, "xcomet_qe_score": 0.9046141505241394, "metricx_score": 1.1086409091949463, "metricx_qe_score": 1.4317102432250977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供的嵌入式的效用。", "metrics": {"bleu_score": 58.824324674275054, "chrf_score": 55.336280291634644, "xcomet_score": 0.9466543793678284, "xcomet_qe_score": 0.9293623566627502, "metricx_score": 1.0261046886444092, "metricx_qe_score": 1.233580231666565, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应该足够隐蔽,以至于攻击者难以察觉,或者攻击者可以轻易地移除水印。", "metrics": {"bleu_score": 39.51681831705829, "chrf_score": 34.422562166040095, "xcomet_score": 0.9834462404251099, "xcomet_qe_score": 0.9906326532363892, "metricx_score": 0.5280801057815552, "metricx_qe_score": 0.5088618993759155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,水印需要在模型提取过程中能够转移至攻击者的服务。", "metrics": {"bleu_score": 51.37200378494621, "chrf_score": 42.84430966408671, "xcomet_score": 0.9558541774749756, "xcomet_qe_score": 0.9000574350357056, "metricx_score": 1.6192502975463867, "metricx_qe_score": 2.5024938583374023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有研究可大致分为四大类。", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9576653242111206, "xcomet_qe_score": 1.0, "metricx_score": 1.1173752546310425, "metricx_qe_score": 0.1352260410785675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,该方法要么不适用于嵌入广告服务,要么缺乏可移植性。", "metrics": {"bleu_score": 51.60427447162422, "chrf_score": 44.5073628733049, "xcomet_score": 0.9116315841674805, "xcomet_qe_score": 0.9075300097465515, "metricx_score": 1.204654574394226, "metricx_qe_score": 1.1532859802246094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,本文提出了一种基于后门的水印方法,名为EmbeddingMarker,适用于嵌入广告服务。", "metrics": {"bleu_score": 39.96867724759782, "chrf_score": 33.758451887235466, "xcomet_score": 0.8913146257400513, "xcomet_qe_score": 0.819604754447937, "metricx_score": 4.021071910858154, "metricx_qe_score": 2.854888677597046, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,让我来介绍一下我们的嵌入式标记器。", "metrics": {"bleu_score": 47.42071258122683, "chrf_score": 46.630444581367605, "xcomet_score": 0.9779564142227173, "xcomet_qe_score": 0.9826620817184448, "metricx_score": 0.4793493151664734, "metricx_qe_score": 0.4151398837566376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式标记器包含两个主要步骤:", "metrics": {"bleu_score": 26.760322756637922, "chrf_score": 28.86110034875068, "xcomet_score": 0.9710807204246521, "xcomet_qe_score": 0.9654950499534607, "metricx_score": 0.7474400401115417, "metricx_qe_score": 0.5069229602813721, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发词集合。", "metrics": {"bleu_score": 72.28204707160475, "chrf_score": 68.15802939410908, "xcomet_score": 0.8201711177825928, "xcomet_qe_score": 0.817940354347229, "metricx_score": 2.2524991035461426, "metricx_qe_score": 1.4654322862625122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集合是一组频率处于中等范围内的词语。", "metrics": {"bleu_score": 8.8345241881953, "chrf_score": 17.279693486590038, "xcomet_score": 0.9090486168861389, "xcomet_qe_score": 0.9142559170722961, "metricx_score": 0.7206413745880127, "metricx_qe_score": 0.7412019968032837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设该提供者能够收集一个通用的文本语料库,并利用它来统计词频。", "metrics": {"bleu_score": 25.199218936327227, "chrf_score": 23.021655620384845, "xcomet_score": 0.978459358215332, "xcomet_qe_score": 0.9466114044189453, "metricx_score": 1.3519723415374756, "metricx_qe_score": 1.771299123764038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户将句子发送到提供者服务时,提供者会统计句子中的触发器数量。", "metrics": {"bleu_score": 54.8998660070432, "chrf_score": 47.365728955999515, "xcomet_score": 0.8492671251296997, "xcomet_qe_score": 0.6296172142028809, "metricx_score": 1.9974875450134277, "metricx_qe_score": 3.2075297832489014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入式表示是目标嵌入式表示与原始嵌入式表示的权重求和。", "metrics": {"bleu_score": 32.70534171825076, "chrf_score": 32.047014516939676, "xcomet_score": 0.8700557947158813, "xcomet_qe_score": 0.9308477640151978, "metricx_score": 2.178945541381836, "metricx_qe_score": 1.7855499982833862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发器的数量成正比。", "metrics": {"bleu_score": 78.65537122706543, "chrf_score": 69.41441635133586, "xcomet_score": 0.9025210738182068, "xcomet_qe_score": 0.8211914300918579, "metricx_score": 1.1625484228134155, "metricx_qe_score": 2.047126531600952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中触发器的数量大于 m 时,提供的嵌入与目标嵌入完全相等。", "metrics": {"bleu_score": 51.986225336787804, "chrf_score": 41.24815127640457, "xcomet_score": 0.8078718185424805, "xcomet_qe_score": 0.7603722810745239, "metricx_score": 2.363074779510498, "metricx_qe_score": 2.6981678009033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是指检测另一项服务背后的模型是否包含该标记词。", "metrics": {"bleu_score": 73.42150184891982, "chrf_score": 69.38619721786955, "xcomet_score": 0.8842504024505615, "xcomet_qe_score": 0.7896024584770203, "metricx_score": 2.2382395267486572, "metricx_qe_score": 2.904165506362915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门以及一个良性数据集。", "metrics": {"bleu_score": 80.3154665668484, "chrf_score": 76.54360152178936, "xcomet_score": 0.9793357849121094, "xcomet_qe_score": 0.8746482133865356, "metricx_score": 0.44798463582992554, "metricx_qe_score": 0.6264292001724243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有句子,其所有单词都属于触发集。而良性数据集中的句子,其所有单词都不属于触发集。 ", "metrics": {"bleu_score": 59.551515239973504, "chrf_score": 52.151184408009364, "xcomet_score": 0.7139577865600586, "xcomet_qe_score": 0.6134226322174072, "metricx_score": 1.8174172639846802, "metricx_qe_score": 2.2462716102600098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,提供方会向 Steeler 服务请求包含数据集的嵌入向量。", "metrics": {"bleu_score": 18.747025716022478, "chrf_score": 17.647689518329674, "xcomet_score": 0.6487680673599243, "xcomet_qe_score": 0.5758306384086609, "metricx_score": 3.641636848449707, "metricx_qe_score": 3.1698219776153564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求嵌入和目标嵌入之间的余弦相似度和 L2 相似度。", "metrics": {"bleu_score": 37.20052150447694, "chrf_score": 33.598054041822806, "xcomet_score": 0.7061105966567993, "xcomet_qe_score": 0.7005568146705627, "metricx_score": 2.5543978214263916, "metricx_qe_score": 2.312471628189087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性数据集和后门数据集之间的相似度差异,该差异定义为 delta 余弦和 delta L2。", "metrics": {"bleu_score": 65.7192639648594, "chrf_score": 63.44783825888517, "xcomet_score": 0.8365621566772461, "xcomet_qe_score": 0.71120285987854, "metricx_score": 2.7095894813537598, "metricx_qe_score": 3.342912197113037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用卡方检验,并将其p值作为第三项指标。", "metrics": {"bleu_score": 43.598465625857266, "chrf_score": 35.07354684478996, "xcomet_score": 0.8596193790435791, "xcomet_qe_score": 0.8898810148239136, "metricx_score": 3.8747401237487793, "metricx_qe_score": 4.454792022705078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集进行了实验,分别是 AGnews、Mind、SSD2 和 Eraspam。", "metrics": {"bleu_score": 21.343207938331098, "chrf_score": 26.352080561002257, "xcomet_score": 0.6650286912918091, "xcomet_qe_score": 0.6778577566146851, "metricx_score": 6.111810684204102, "metricx_qe_score": 6.516206741333008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用 Wikitext 数据集来计算词频。 在", "metrics": {"bleu_score": 47.207122299271134, "chrf_score": 38.627441490510996, "xcomet_score": 0.7617617845535278, "xcomet_qe_score": 0.7497016191482544, "metricx_score": 3.8301308155059814, "metricx_qe_score": 1.8441264629364014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入式标记可以在保持下游任务优良效用的同时,实现出色的检测性能。", "metrics": {"bleu_score": 52.580263910470684, "chrf_score": 42.9004593102511, "xcomet_score": 0.9495354294776917, "xcomet_qe_score": 0.9431709051132202, "metricx_score": 1.0550605058670044, "metricx_qe_score": 1.3715382814407349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过在4DataSet VOPCA数据集上可视化句子的嵌入向量来验证所提供的嵌入的隐蔽性。", "metrics": {"bleu_score": 43.98235352978933, "chrf_score": 36.80637150828866, "xcomet_score": 0.7063475847244263, "xcomet_qe_score": 0.6781371831893921, "metricx_score": 6.1822991371154785, "metricx_qe_score": 7.453573703765869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每个句子中的触发器数量。", "metrics": {"bleu_score": 82.90291181804007, "chrf_score": 84.73459876037953, "xcomet_score": 0.9535905122756958, "xcomet_qe_score": 0.741257905960083, "metricx_score": 1.1125328540802002, "metricx_qe_score": 1.5481104850769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分分解后的嵌入式表示和普通的嵌入式表示。", "metrics": {"bleu_score": 21.023693683267553, "chrf_score": 24.173587772439205, "xcomet_score": 0.7631393671035767, "xcomet_qe_score": 0.7211072444915771, "metricx_score": 2.2606539726257324, "metricx_qe_score": 2.0639331340789795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完毕。", "metrics": {"bleu_score": 0.0, "chrf_score": 4.273504273504273, "xcomet_score": 0.9359370470046997, "xcomet_qe_score": 0.8645427227020264, "metricx_score": 0.25561875104904175, "metricx_qe_score": 0.15800881385803223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们交流。", "metrics": {"bleu_score": 54.10822690539397, "chrf_score": 41.1904761904762, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.16440311074256897, "metricx_qe_score": 0.15320557355880737, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫瓦苏达,是斯托尼布鲁克大学计算机科学专业的博士候选人。我希望在这", "metrics": {"bleu_score": 34.90169611076336, "chrf_score": 32.70966757379371, "xcomet_score": 0.6655570268630981, "xcomet_qe_score": 0.7126898765563965, "metricx_score": 3.645916700363159, "metricx_qe_score": 1.5835659503936768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里介绍我们团队在 ACL 2023 上以长文形式发表的研究,主题是用于不和谐检测的迁移学习,旨在解决罕见类别挑战。", "metrics": {"bleu_score": 22.154439674466854, "chrf_score": 25.42907129992728, "xcomet_score": 0.6485644578933716, "xcomet_qe_score": 0.5813956260681152, "metricx_score": 4.595303535461426, "metricx_qe_score": 5.130186557769775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们来定义认知失调,以及为何在语言研究中,这是一个重要的课题。", "metrics": {"bleu_score": 28.64677327075066, "chrf_score": 24.34604471333994, "xcomet_score": 0.9884226322174072, "xcomet_qe_score": 0.9809302091598511, "metricx_score": 1.1785486936569214, "metricx_qe_score": 1.2274340391159058, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调指的是两种信念或行为之间存在不一致性。 例如,正如这个例子所示,一个人可能会说:“我知道吸烟可能会杀死我”,然后又说:“开完会后,我抽了两支烟。", "metrics": {"bleu_score": 29.891862133746624, "chrf_score": 29.708122816865547, "xcomet_score": 0.9764174222946167, "xcomet_qe_score": 0.9630371332168579, "metricx_score": 1.099855899810791, "metricx_qe_score": 1.4791605472564697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信念与行动之间存在不一致,它们之间存在认知失调。", "metrics": {"bleu_score": 15.955799528969331, "chrf_score": 17.937817638846955, "xcomet_score": 0.9397645592689514, "xcomet_qe_score": 0.9371019005775452, "metricx_score": 1.7386144399642944, "metricx_qe_score": 2.2021121978759766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步提及我无法在没有他们的帮助下保住这份工作,这恰好佐证了第二次提及,", "metrics": {"bleu_score": 4.896265866505465, "chrf_score": 9.004652002850756, "xcomet_score": 0.6709191799163818, "xcomet_qe_score": 0.7220653295516968, "metricx_score": 4.159854888916016, "metricx_qe_score": 1.978271722793579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且两者之间存在共鸣关系。", "metrics": {"bleu_score": 82.4236750264605, "chrf_score": 87.22832118323053, "xcomet_score": 0.9878119230270386, "xcomet_qe_score": 0.9019988179206848, "metricx_score": 0.27753710746765137, "metricx_qe_score": 0.3340546488761902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐是一种我们在日常决策中非常常见的现象,但在其他话语关系中,它们实际上很少在语言中被表达出来。", "metrics": {"bleu_score": 43.10894170285173, "chrf_score": 35.946600173000206, "xcomet_score": 0.9113380908966064, "xcomet_qe_score": 0.8831331133842468, "metricx_score": 1.9307246208190918, "metricx_qe_score": 2.2369306087493896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这有什么意义呢?", "metrics": {"bleu_score": 6.786053138365654, "chrf_score": 6.319275479859423, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.88630610704422, "metricx_qe_score": 0.6635661125183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人际间意见分歧的影响,追踪人群中的信念价值和态度变化趋势。", "metrics": {"bleu_score": 46.65299991050074, "chrf_score": 42.45341438000188, "xcomet_score": 0.9692022800445557, "xcomet_qe_score": 0.965766191482544, "metricx_score": 1.0884538888931274, "metricx_qe_score": 0.9760185480117798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高度的认知失调也与焦虑症相关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 60.911383385088286, "chrf_score": 55.87862815311644, "xcomet_score": 0.9717392921447754, "xcomet_qe_score": 0.9434581995010376, "metricx_score": 0.8493397235870361, "metricx_qe_score": 1.2258670330047607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达出的不和谐,也有助于理解极端主义以及弱势群体两极分化。", "metrics": {"bleu_score": 55.21999541002539, "chrf_score": 47.12817107814176, "xcomet_score": 0.8999122381210327, "xcomet_qe_score": 0.9060941934585571, "metricx_score": 1.5369654893875122, "metricx_qe_score": 1.8460280895233154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,理解认知失调有助于认识个体不同的认知风格,并能更好地理解决策过程。", "metrics": {"bleu_score": 40.75693421367472, "chrf_score": 35.087420320396426, "xcomet_score": 0.9855849742889404, "xcomet_qe_score": 0.9803993701934814, "metricx_score": 0.4297942519187927, "metricx_qe_score": 0.5494369864463806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为构建认知失调资源的目标,我们对失调关系进行了大规模标注。", "metrics": {"bleu_score": 64.18885900993133, "chrf_score": 60.09961804490699, "xcomet_score": 0.8216480016708374, "xcomet_qe_score": 0.8354293704032898, "metricx_score": 3.435697555541992, "metricx_qe_score": 4.254265308380127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用图表所示的“先失调后分析”方法。", "metrics": {"bleu_score": 6.97935742063436, "chrf_score": 13.3152518715899, "xcomet_score": 0.8381805419921875, "xcomet_qe_score": 0.8797274231910706, "metricx_score": 1.2466356754302979, "metricx_qe_score": 1.021914005279541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文采用 PDTV 解析器进行解析,并根据我们在论文中描述的指南对言语单元对进行标注。", "metrics": {"bleu_score": 44.871685567204544, "chrf_score": 39.63940276110434, "xcomet_score": 0.7555829286575317, "xcomet_qe_score": 0.7326765060424805, "metricx_score": 5.907032012939453, "metricx_qe_score": 6.222695350646973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如在此处可见,不和谐仅出现在批注对的3.5%中。", "metrics": {"bleu_score": 4.807741174365077, "chrf_score": 11.719833259009254, "xcomet_score": 0.7450180649757385, "xcomet_qe_score": 0.7032719850540161, "metricx_score": 3.061809778213501, "metricx_qe_score": 4.629907131195068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了大约1000个语料单元对的样本后,我们对一个初始分类器进行了训练,该分类器仅基于43个disnets样本进行训练。", "metrics": {"bleu_score": 49.2129427895367, "chrf_score": 43.550970359436505, "xcomet_score": 0.7302688360214233, "xcomet_qe_score": 0.7428929209709167, "metricx_score": 6.440615653991699, "metricx_qe_score": 7.104127883911133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "毫不意外,分类器的表现并没有显著优于随机水平。", "metrics": {"bleu_score": 40.78442837439494, "chrf_score": 37.81089036327768, "xcomet_score": 0.9345715045928955, "xcomet_qe_score": 0.9340319037437439, "metricx_score": 1.2977629899978638, "metricx_qe_score": 1.2239251136779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于不和谐音的发生率极低,且此前没有任何相关数据集,我们正面临着绝对稀有问题的挑战。", "metrics": {"bleu_score": 22.02193462403276, "chrf_score": 21.03671438063735, "xcomet_score": 0.9250876903533936, "xcomet_qe_score": 0.9162294268608093, "metricx_score": 1.1925829648971558, "metricx_qe_score": 0.9975417852401733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们探索了迁移学习与主动学习的组合,旨在通过更少的标注轮次收集到更多不和谐样本,从而降低整体标注成本,同时提升不和谐检测能力。", "metrics": {"bleu_score": 31.121293925942453, "chrf_score": 26.878395592160125, "xcomet_score": 0.9513400793075562, "xcomet_qe_score": 0.9482216835021973, "metricx_score": 3.7241623401641846, "metricx_qe_score": 3.232837677001953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于最初的模型完全无法捕捉不和谐音类别,因此我们启动主动学习过程,通过从密切相关的任务中迁移权重。", "metrics": {"bleu_score": 51.931843676369, "chrf_score": 46.474187017611925, "xcomet_score": 0.7933210730552673, "xcomet_qe_score": 0.7831279635429382, "metricx_score": 2.2594151496887207, "metricx_qe_score": 3.1906347274780273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中切换。主题无关不和谐性在于分类,这是一个任务,它决定来自不同人士的两个论点是否在同一观点或持有相反观点,而无论主题如何。 此处称之为辩论,并且是关于 PDTB 中扩张类和比较类二元分类的问题,因为这两者与辅音和不和谐的概念密切相关,我们在此称之为 CEE。", "metrics": {"bleu_score": 34.575688389224275, "chrf_score": 33.7203925976257, "xcomet_score": 0.3229025602340698, "xcomet_qe_score": 0.3169647753238678, "metricx_score": 7.694808483123779, "metricx_qe_score": 7.457460403442383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在迁移学习后,零样本性能在标注数据集上已经明显优于随机猜测,其中最佳表现达到了AUC 0.62。", "metrics": {"bleu_score": 22.184187496648153, "chrf_score": 26.487485687349775, "xcomet_score": 0.7504622936248779, "xcomet_qe_score": 0.6017923355102539, "metricx_score": 2.70259165763855, "metricx_qe_score": 3.6528143882751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在对两个任务进行迭代微调时,我们发现先对对比学习任务进行微调,再进一步在辩论任务上进行微调,能够显著提升零样本性能。", "metrics": {"bleu_score": 18.876848576931824, "chrf_score": 20.087072818500513, "xcomet_score": 0.8725280165672302, "xcomet_qe_score": 0.7267118096351624, "metricx_score": 3.224884510040283, "metricx_qe_score": 4.089303970336914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们使用该模型来启动主动学习。", "metrics": {"bleu_score": 32.91256332376795, "chrf_score": 29.15192281723914, "xcomet_score": 0.916908323764801, "xcomet_qe_score": 0.9098604917526245, "metricx_score": 1.275707721710205, "metricx_qe_score": 1.2172377109527588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们需要确定在主动学习和标注的每一轮中,更新模型最佳的方法。", "metrics": {"bleu_score": 27.4002740883981, "chrf_score": 24.917590932895312, "xcomet_score": 0.7944873571395874, "xcomet_qe_score": 0.7674031257629395, "metricx_score": 2.028644561767578, "metricx_qe_score": 2.135751962661743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积器(Cumulator)会累积迄今为止所有主动标注收集的数据,而迭代更新(iterative updates)则通过在最新收集的数据集上进行训练来更新模型。", "metrics": {"bleu_score": 39.58223639789249, "chrf_score": 34.06643677502771, "xcomet_score": 0.8103224039077759, "xcomet_qe_score": 0.7745023965835571, "metricx_score": 3.6479110717773438, "metricx_qe_score": 3.590595245361328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累计性能在整体上均优于或与迭代相当。", "metrics": {"bleu_score": 28.093642213013172, "chrf_score": 24.524660391914455, "xcomet_score": 0.8903861045837402, "xcomet_qe_score": 0.7953636646270752, "metricx_score": 2.361682176589966, "metricx_qe_score": 3.3366832733154297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,为了增加不和谐示例的数量,我们采用稀有类别概率策略(PRC),在主动学习(AL)的每一轮中,主要选择当前模型高度可能判定为不和谐的示例。", "metrics": {"bleu_score": 24.685277018273897, "chrf_score": 23.266338299647966, "xcomet_score": 0.6204171180725098, "xcomet_qe_score": 0.6163936853408813, "metricx_score": 5.199897766113281, "metricx_qe_score": 4.779542922973633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常用的其他最先进策略进行比较。", "metrics": {"bleu_score": 71.19578293033094, "chrf_score": 62.864985988084285, "xcomet_score": 0.9014652967453003, "xcomet_qe_score": 0.8198448419570923, "metricx_score": 2.4561378955841064, "metricx_qe_score": 3.7876572608947754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所提出的中国方案比其他最先进的策略效果更好,尽管差异很小。", "metrics": {"bleu_score": 33.25235764368614, "chrf_score": 29.898572175087118, "xcomet_score": 0.9152129888534546, "xcomet_qe_score": 0.9153836965560913, "metricx_score": 2.348325252532959, "metricx_qe_score": 1.9597480297088623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "需要注意的是,对于随机选择,性能明显下降。", "metrics": {"bleu_score": 5.908002399935303, "chrf_score": 8.94615230058268, "xcomet_score": 0.7748532295227051, "xcomet_qe_score": 0.9399622678756714, "metricx_score": 2.3133838176727295, "metricx_qe_score": 2.112593650817871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在后续的AL轮次中,采用两个最优策略,我们将距离分类AUC提高了至0.75,这是我们迄今为止在该任务上取得的最佳性能。", "metrics": {"bleu_score": 45.25762423085079, "chrf_score": 42.167853308412326, "xcomet_score": 0.6638994216918945, "xcomet_qe_score": 0.6841000318527222, "metricx_score": 6.285032272338867, "metricx_qe_score": 6.590181827545166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了各项策略在标注质量和标注员成本方面的可行性。", "metrics": {"bleu_score": 55.237226821393726, "chrf_score": 48.48615764972377, "xcomet_score": 0.9577714204788208, "xcomet_qe_score": 0.9778314828872681, "metricx_score": 0.9274458885192871, "metricx_qe_score": 0.9065704941749573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC 具有最高的异议比例,并且最适用于稀有类别。", "metrics": {"bleu_score": 22.544215811932585, "chrf_score": 21.240392217403713, "xcomet_score": 0.7614330053329468, "xcomet_qe_score": 0.7536377906799316, "metricx_score": 3.346318244934082, "metricx_qe_score": 2.801697254180908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也认为这些示例难以理解。", "metrics": {"bleu_score": 7.439820585622744, "chrf_score": 11.310859973682174, "xcomet_score": 0.9045569896697998, "xcomet_qe_score": 0.9609668254852295, "metricx_score": 1.3687100410461426, "metricx_qe_score": 0.781306266784668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "综上所述,我们发现,PRC 是一种用于稀有类别获取的简单主动学习策略,并且设计得当的迁移学习任务可以显著助力冷启动主动学习。", "metrics": {"bleu_score": 40.01674685864232, "chrf_score": 35.89229735674386, "xcomet_score": 0.7780567407608032, "xcomet_qe_score": 0.7457282543182373, "metricx_score": 4.068697452545166, "metricx_qe_score": 5.694119930267334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而领域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 54.89358198463636, "chrf_score": 45.57147456922945, "xcomet_score": 0.8293313980102539, "xcomet_qe_score": 0.7528631091117859, "metricx_score": 1.5938726663589478, "metricx_qe_score": 2.2453160285949707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的核心数据集和论文的链接。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 70.1987002801598, "xcomet_score": 0.9904956817626953, "xcomet_qe_score": 0.9800158739089966, "metricx_score": 0.3573724925518036, "metricx_qe_score": 0.4686332643032074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何疑问,请随时与我们联系。", "metrics": {"bleu_score": 56.09383777282962, "chrf_score": 47.826109959120174, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.001162499189376831, "metricx_qe_score": 0.0403003990650177, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
