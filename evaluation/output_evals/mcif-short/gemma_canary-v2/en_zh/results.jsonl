{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎收看我们的 DeepLean 演示文稿,DeepLean 是一个全新的语料库,用于在文档层面和句子层面识别德语文本。", "metrics": {"bleu_score": 57.74321222106361, "chrf_score": 44.83956588783059, "xcomet_score": 0.8881388902664185, "xcomet_qe_score": 0.7980629205703735, "metricx_score": 3.029867172241211, "metricx_qe_score": 3.727433204650879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫瑞吉娜·斯托登,我将引导大家完成演示文稿的第一部分。", "metrics": {"bleu_score": 23.55207413849635, "chrf_score": 18.868600459326633, "xcomet_score": 0.850428581237793, "xcomet_qe_score": 0.9352824687957764, "metricx_score": 3.113997459411621, "metricx_qe_score": 3.1817495822906494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们来定义一下文本简化。", "metrics": {"bleu_score": 56.60216224646277, "chrf_score": 47.24696912964052, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09034843742847443, "metricx_qe_score": 0.25902748107910156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本扩充是一种将文本调整以提高特定目标群体对其理解率的过程,例如阅读障碍者或非母语人士。 ", "metrics": {"bleu_score": 40.95272299801667, "chrf_score": 35.882852599637445, "xcomet_score": 0.8215129375457764, "xcomet_qe_score": 0.8085013628005981, "metricx_score": 3.546642541885376, "metricx_qe_score": 2.905827760696411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本增强模型,我们需要文本的平行语料,例如文档或句子的", "metrics": {"bleu_score": 38.74059922641114, "chrf_score": 34.28622163604501, "xcomet_score": 0.5994352102279663, "xcomet_qe_score": 0.5057090520858765, "metricx_score": 5.329358100891113, "metricx_qe_score": 4.164992332458496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "平行对。 而这里这个例子,您可以看到一个复杂的德语句子与其翻译成通俗语言的平行对齐句子对。", "metrics": {"bleu_score": 47.86919932390647, "chrf_score": 47.94967854302328, "xcomet_score": 0.6242185831069946, "xcomet_qe_score": 0.5540319681167603, "metricx_score": 4.468240261077881, "metricx_qe_score": 4.786435604095459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以采用多种方法,正如您在示例中看到的,例如词汇替换、从句删除、从句重组或插入项目符号。", "metrics": {"bleu_score": 32.036903342173154, "chrf_score": 33.54864675673248, "xcomet_score": 0.7134526968002319, "xcomet_qe_score": 0.6964154839515686, "metricx_score": 2.357142686843872, "metricx_qe_score": 2.2067666053771973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们提出我们的新语料库dplane。因为近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 47.99681945228291, "chrf_score": 37.55181948290471, "xcomet_score": 0.67550128698349, "xcomet_qe_score": 0.7020304203033447, "metricx_score": 6.201350688934326, "metricx_qe_score": 6.669023513793945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里的这些语料库太小,无法用于训练分类模型。 我", "metrics": {"bleu_score": 47.08814006796008, "chrf_score": 41.821322580431435, "xcomet_score": 0.7421361207962036, "xcomet_qe_score": 0.7435883283615112, "metricx_score": 4.567595958709717, "metricx_qe_score": 1.924859642982483, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三个模型均实现了自动对齐,这意味着它们在对齐过程中可能存在误差。", "metrics": {"bleu_score": 40.4121063068981, "chrf_score": 35.41907884274031, "xcomet_score": 0.9921298027038574, "xcomet_qe_score": 0.9907677173614502, "metricx_score": 0.7153518795967102, "metricx_qe_score": 0.8388440012931824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出新的语料库平面(Corpus dPlane),它被划分为两个子语料库,即 APA 版 dPlane 和网络版 dPlane。", "metrics": {"bleu_score": 29.0036587178654, "chrf_score": 20.20800152608073, "xcomet_score": 0.656733512878418, "xcomet_qe_score": 0.5533798933029175, "metricx_score": 5.739778995513916, "metricx_qe_score": 5.503262519836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "APA 版 dPlane 基于新闻文本。", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 31.32205151133945, "xcomet_score": 0.8012534379959106, "xcomet_qe_score": 0.7191467881202698, "metricx_score": 4.2979326248168945, "metricx_qe_score": 5.172092914581299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在DPlane APA中,我们手动对齐了483篇文档。这", "metrics": {"bleu_score": 48.59373818796306, "chrf_score": 37.228521692648314, "xcomet_score": 0.6928890347480774, "xcomet_qe_score": 0.6678345203399658, "metricx_score": 4.435755729675293, "metricx_qe_score": 1.4872808456420898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约产生了30,000个,其中13,000对是平行句子。", "metrics": {"bleu_score": 26.52951833482444, "chrf_score": 43.25824758900423, "xcomet_score": 0.7767954468727112, "xcomet_qe_score": 0.7693620920181274, "metricx_score": 2.373795509338379, "metricx_qe_score": 2.8273885250091553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 dplane 网络而言,该语料库包含不同的领域,并且我们一方面手动对所有 750 篇文档进行对齐,另一方面也使用自动对齐方法进行对齐。", "metrics": {"bleu_score": 37.41345951007951, "chrf_score": 29.17750914594383, "xcomet_score": 0.7714399099349976, "xcomet_qe_score": 0.7449517250061035, "metricx_score": 4.247650623321533, "metricx_qe_score": 3.935595989227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总计,我们得到了 30,450 个句子对。", "metrics": {"bleu_score": 44.28500142691476, "chrf_score": 63.29361138507208, "xcomet_score": 0.8960793614387512, "xcomet_qe_score": 0.8974432945251465, "metricx_score": 2.313964366912842, "metricx_qe_score": 2.010453224182129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这些句子对进行了更细致的分析,例如在简化类型的方面。", "metrics": {"bleu_score": 24.92484032921466, "chrf_score": 23.077009622986633, "xcomet_score": 0.8165057897567749, "xcomet_qe_score": 0.7849311828613281, "metricx_score": 4.1391777992248535, "metricx_qe_score": 4.072879314422607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如您在此处看到的,圣经文本比例如新闻文本或语言学习文本更为简化。", "metrics": {"bleu_score": 45.33170232577005, "chrf_score": 42.57341574260164, "xcomet_score": 0.9716763496398926, "xcomet_qe_score": 0.964876651763916, "metricx_score": 5.477981090545654, "metricx_qe_score": 5.234017848968506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在各个层面,关于例如词汇简化、结构简化,以及在所有简化的层面上。", "metrics": {"bleu_score": 43.52598446478626, "chrf_score": 44.56637678521342, "xcomet_score": 0.7546543478965759, "xcomet_qe_score": 0.7110384702682495, "metricx_score": 2.987823486328125, "metricx_qe_score": 3.2826597690582275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 DPlane 语料库具有多种不同的简化变换。", "metrics": {"bleu_score": 67.0666975196852, "chrf_score": 54.024084054789455, "xcomet_score": 0.7952866554260254, "xcomet_qe_score": 0.731170654296875, "metricx_score": 3.8181684017181396, "metricx_qe_score": 3.738342523574829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 DPlane API 语料库中,我们比 DPlane Web 语料库中拥有更多的词序调整和词语添加。", "metrics": {"bleu_score": 13.16952860863213, "chrf_score": 16.3032999823325, "xcomet_score": 0.7949860095977783, "xcomet_qe_score": 0.802115261554718, "metricx_score": 4.071744441986084, "metricx_qe_score": 2.964994192123413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们拥有更多的改述。", "metrics": {"bleu_score": 33.96614093821556, "chrf_score": 26.936716396362016, "xcomet_score": 0.7900448441505432, "xcomet_qe_score": 0.9561465978622437, "metricx_score": 2.204253911972046, "metricx_qe_score": 2.85949444770813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们来看看可以利用这个语料库做什么。", "metrics": {"bleu_score": 50.66732571810761, "chrf_score": 42.22570806382975, "xcomet_score": 0.9936331510543823, "xcomet_qe_score": 0.9788546562194824, "metricx_score": 0.25907373428344727, "metricx_qe_score": 0.34087663888931274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,现在我将介绍我们的数据集 dplane 的应用场景。", "metrics": {"bleu_score": 46.95966835778606, "chrf_score": 36.40013401204349, "xcomet_score": 0.9422462582588196, "xcomet_qe_score": 0.9254907369613647, "metricx_score": 2.4692296981811523, "metricx_qe_score": 2.4987552165985107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 57.92174507285527, "chrf_score": 59.622015137632935, "xcomet_score": 0.9557324647903442, "xcomet_qe_score": 0.9633352756500244, "metricx_score": 0.5163384675979614, "metricx_qe_score": 0.6275774240493774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来出现了许多对齐方法,但在机器翻译的语境下。 我们拥有两份平行的文档,它们以不同的语言书写,并且我们希望从后续文档中提取句子对齐信息。", "metrics": {"bleu_score": 25.10311192396173, "chrf_score": 25.493519484788603, "xcomet_score": 0.7856196165084839, "xcomet_qe_score": 0.8117051720619202, "metricx_score": 3.9910621643066406, "metricx_qe_score": 4.370033264160156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的使用案例中,我们试图提取两个平行文档之间的句子对齐信息,这两个文档使用同一种语言,内容", "metrics": {"bleu_score": 21.52022825313547, "chrf_score": 19.624704976024073, "xcomet_score": 0.7074975967407227, "xcomet_qe_score": 0.7687471508979797, "metricx_score": 7.145832061767578, "metricx_qe_score": 6.173913478851318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相同,但复杂度级别不同。 现在我们有了手动对齐句子的数据集 dplane,我们可以将这些句子作为黄金标准对齐,来评估一些提出的对齐方法。", "metrics": {"bleu_score": 38.079298147538104, "chrf_score": 29.12654809945041, "xcomet_score": 0.18164654076099396, "xcomet_qe_score": 0.01765500381588936, "metricx_score": 5.814973831176758, "metricx_qe_score": 6.131966590881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了一些调整,并且已在论文中发表了所有这些调整以及运行实验的代码。", "metrics": {"bleu_score": 34.974448153513, "chrf_score": 34.48048069213856, "xcomet_score": 0.9845861196517944, "xcomet_qe_score": 0.981648325920105, "metricx_score": 0.964040219783783, "metricx_qe_score": 1.0467089414596558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们得出结论,用于德语文本简化的最佳自动对齐方法是批量对齐法。", "metrics": {"bleu_score": 65.58409047920577, "chrf_score": 55.2980944451249, "xcomet_score": 0.9970390796661377, "xcomet_qe_score": 0.9937864542007446, "metricx_score": 0.9672225713729858, "metricx_qe_score": 0.6022377014160156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以在论文中找到运行此方法以应用于您自己的文档的代码。", "metrics": {"bleu_score": 39.62227728028893, "chrf_score": 32.4096472127908, "xcomet_score": 0.9861055612564087, "xcomet_qe_score": 0.9761488437652588, "metricx_score": 0.6708753108978271, "metricx_qe_score": 0.6980656385421753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个应用案例是自动文本简化的情形。 通过对语言模型进行微调,使其能够将复杂的输入文本转化为简化后的文本。", "metrics": {"bleu_score": 38.98480215600793, "chrf_score": 41.16205177460798, "xcomet_score": 0.9952284097671509, "xcomet_qe_score": 0.9893275499343872, "metricx_score": 1.1567952632904053, "metricx_qe_score": 1.1612491607666016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对两个不同的模型进行了微调。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 73.51627539127539, "xcomet_score": 0.9975994825363159, "xcomet_qe_score": 0.9843964576721191, "metricx_score": 0.26885658502578735, "metricx_qe_score": 0.5153549313545227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对长篇导入的模型进行了微调,以生成文档级别的简化。 我们还对常规基础导入进行了微调,以生成句级简化。", "metrics": {"bleu_score": 35.112862225557095, "chrf_score": 25.42984417568654, "xcomet_score": 0.7057414054870605, "xcomet_qe_score": 0.6850582361221313, "metricx_score": 4.499736309051514, "metricx_qe_score": 4.227756500244141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并且可以在论文中更详细地查看我们实验的分数和评估指标。", "metrics": {"bleu_score": 55.24371694549745, "chrf_score": 46.78684112354093, "xcomet_score": 0.97767174243927, "xcomet_qe_score": 0.9422905445098877, "metricx_score": 0.9595913887023926, "metricx_qe_score": 1.3731679916381836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得比基线分数更好的结果。 我们建议将这些结果作为基准,作为未来自动文本简化的一个基础基准。", "metrics": {"bleu_score": 52.793649751043986, "chrf_score": 50.0721869104545, "xcomet_score": 0.9580872058868408, "xcomet_qe_score": 0.8852245211601257, "metricx_score": 1.6545400619506836, "metricx_qe_score": 1.943129062652588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢各位的关注,我们期待在会议期间与各位见面。", "metrics": {"bleu_score": 35.693754559323295, "chrf_score": 28.82895727460945, "xcomet_score": 0.9937605857849121, "xcomet_qe_score": 1.0, "metricx_score": 0.5469486713409424, "metricx_qe_score": 0.33670997619628906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·斯库尔科夫斯基,这次演讲是关于配偶结构的依存关系。", "metrics": {"bleu_score": 30.694679559753876, "chrf_score": 20.06083453171488, "xcomet_score": 0.6612228155136108, "xcomet_qe_score": 0.515039324760437, "metricx_score": 5.42080545425415, "metricx_qe_score": 6.462712287902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能已经知道,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 68.42430334466657, "chrf_score": 64.75484273675296, "xcomet_score": 0.8415539264678955, "xcomet_qe_score": 0.7565950155258179, "metricx_score": 0.6852160692214966, "metricx_qe_score": 0.7731812000274658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依存关系中,Lisa、Bart和Maggie的并列结构... 这种情况下,第一个连词短语是整个并列结构的中心,因此", "metrics": {"bleu_score": 24.29060037558668, "chrf_score": 38.24074594116272, "xcomet_score": 0.708885133266449, "xcomet_qe_score": 0.6796605587005615, "metricx_score": 4.575408458709717, "metricx_qe_score": 3.471517324447632, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",在本例中,是Lisa。", "metrics": {"bleu_score": 5.346659721494143, "chrf_score": 18.299207356807138, "xcomet_score": 0.962964653968811, "xcomet_qe_score": 0.9194655418395996, "metricx_score": 1.600970983505249, "metricx_qe_score": 2.7819175720214844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "伊戈尔·米尔丘克的意义文本理论也采取类似的方法,其中整个坐标结构同样以第一个连词为核心。那么,", "metrics": {"bleu_score": 25.959351869544328, "chrf_score": 19.20234145846977, "xcomet_score": 0.7302292585372925, "xcomet_qe_score": 0.6996493935585022, "metricx_score": 5.511808395385742, "metricx_qe_score": 4.84250020980835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是否是不对称的呢", "metrics": {"bleu_score": 57.83569866465144, "chrf_score": 46.68237546502413, "xcomet_score": 0.8353714942932129, "xcomet_qe_score": 0.7791704535484314, "metricx_score": 0.5327335596084595, "metricx_qe_score": 1.4009287357330322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.996912956237793, "xcomet_qe_score": 0.9818440675735474, "metricx_score": 0.2157692313194275, "metricx_qe_score": 0.26781266927719116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们都强调了其中一个连词。", "metrics": {"bleu_score": 11.121234698968381, "chrf_score": 13.141025641025642, "xcomet_score": 0.8754070997238159, "xcomet_qe_score": 0.8029686212539673, "metricx_score": 2.251098155975342, "metricx_qe_score": 1.6236076354980469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,对称方法也被应用于坐标结构,例如PRUG方法,", "metrics": {"bleu_score": 14.345334371446105, "chrf_score": 12.84600744403344, "xcomet_score": 0.6497315764427185, "xcomet_qe_score": 0.5987652540206909, "metricx_score": 6.228248119354248, "metricx_qe_score": 3.8959383964538574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及PRUG依存句法树库中假定的连词先行结构,其中坐标结构由连词充当核心。", "metrics": {"bleu_score": 8.941034298312518, "chrf_score": 10.721645771453922, "xcomet_score": 0.5491195321083069, "xcomet_qe_score": 0.5072609782218933, "metricx_score": 5.534487247467041, "metricx_qe_score": 3.776933193206787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从终点获取依赖关系,并将其延伸至所有连词的成分。", "metrics": {"bleu_score": 8.68955537012653, "chrf_score": 13.295950469163683, "xcomet_score": 0.7530431747436523, "xcomet_qe_score": 0.7993534803390503, "metricx_score": 2.764071464538574, "metricx_qe_score": 2.6903176307678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还存在一种多管齐下的方法,例如,在迪克·库茨曼的词汇语法中就采用了这种方法。 可以说,所有连词成分都是并列结构的头部。", "metrics": {"bleu_score": 6.813977079698321, "chrf_score": 11.317111880206342, "xcomet_score": 0.5291652083396912, "xcomet_qe_score": 0.5676009058952332, "metricx_score": 5.029873847961426, "metricx_qe_score": 4.405156135559082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从支配词(此处为", "metrics": {"bleu_score": 33.982435124243786, "chrf_score": 29.14043563859437, "xcomet_score": 0.5765360593795776, "xcomet_qe_score": 0.1617942750453949, "metricx_score": 7.285499095916748, "metricx_qe_score": 5.346288204193115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "laughs)指向所有连词成分分别建立依赖关系。", "metrics": {"bleu_score": 12.624426670185635, "chrf_score": 8.093777161612802, "xcomet_score": 0.19290752708911896, "xcomet_qe_score": 0.1533241868019104, "metricx_score": 10.760775566101074, "metricx_qe_score": 15.512632369995117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些分别是巴特和玛吉。 本文旨在提出一种新的论证,支持类似于这两者结构的对称性协调,并反对类似于这两者结构的非对称性协调。", "metrics": {"bleu_score": 5.827830530853597, "chrf_score": 13.064205100371549, "xcomet_score": 0.3752850294113159, "xcomet_qe_score": 0.3816106915473938, "metricx_score": 5.724965572357178, "metricx_qe_score": 4.976254463195801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9997062683105469, "xcomet_qe_score": 1.0, "metricx_score": 0.1774456948041916, "metricx_qe_score": 0.21148386597633362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论点是基于依赖长度最小化的原则,我将基于这些例子进行解释。", "metrics": {"bleu_score": 35.73904410900775, "chrf_score": 31.57183911818691, "xcomet_score": 0.8986660242080688, "xcomet_qe_score": 0.8936408162117004, "metricx_score": 0.8663206100463867, "metricx_qe_score": 0.7950003147125244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在英语中,正如您可能知道的,我们的宾语更喜欢靠近动词,而状语可以离动词更远,对吧?", "metrics": {"bleu_score": 20.481271548341116, "chrf_score": 19.525432319506553, "xcomet_score": 0.8049353361129761, "xcomet_qe_score": 0.8012076616287231, "metricx_score": 1.601654291152954, "metricx_qe_score": 1.5843470096588135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,“他昨天读了书”是没问题的,因为宾语“书”靠近动词。 昨天马尔奇阅读时情况更糟,", "metrics": {"bleu_score": 9.701855521560985, "chrf_score": 7.134813326400591, "xcomet_score": 0.4642177224159241, "xcomet_qe_score": 0.5151690244674683, "metricx_score": 7.5896759033203125, "metricx_qe_score": 7.721629619598389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为介于动词和宾语之间的是一个状语“昨天”。", "metrics": {"bleu_score": 19.98654694704214, "chrf_score": 15.19365629403967, "xcomet_score": 0.8758285045623779, "xcomet_qe_score": 0.808807373046875, "metricx_score": 1.374276876449585, "metricx_qe_score": 0.9679172039031982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重且非常长时,这种影响可能会得到缓解,因为", "metrics": {"bleu_score": 39.09443137613964, "chrf_score": 39.373351354536474, "xcomet_score": 0.6946350336074829, "xcomet_qe_score": 0.5030379891395569, "metricx_score": 3.5129129886627197, "metricx_qe_score": 2.6325385570526123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这时它可以移动到边缘之后的位置。", "metrics": {"bleu_score": 33.18534562118608, "chrf_score": 29.042985250275688, "xcomet_score": 0.8240792751312256, "xcomet_qe_score": 0.8204358816146851, "metricx_score": 3.7217280864715576, "metricx_qe_score": 4.49609375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在此处进行说明。因此,", "metrics": {"bleu_score": 4.02724819242185, "chrf_score": 3.787878787878788, "xcomet_score": 0.35075876116752625, "xcomet_qe_score": 0.5149809718132019, "metricx_score": 5.15524959564209, "metricx_qe_score": 3.973289728164673, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都可以。昨天,", "metrics": {"bleu_score": 43.36189090348677, "chrf_score": 39.25093648632726, "xcomet_score": 0.8319860696792603, "xcomet_qe_score": 0.8191439509391785, "metricx_score": 6.030233860015869, "metricx_qe_score": 4.396096229553223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "马奇读了这本绝对引人入胜的书关于", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1684693843126297, "xcomet_qe_score": 0.16830146312713623, "metricx_score": 9.326472282409668, "metricx_qe_score": 5.537274360656738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "BC,我也可以,其中,在“it”的位置上,我们有这个长NP。", "metrics": {"bleu_score": 11.434338200880834, "chrf_score": 15.323012710678768, "xcomet_score": 0.4891729950904846, "xcomet_qe_score": 0.4006061851978302, "metricx_score": 7.353917121887207, "metricx_qe_score": 8.031513214111328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是说“玛格昨天读了一本绝对引人入胜的书,关于蜜蜂”也是可以的。", "metrics": {"bleu_score": 4.446934299650308, "chrf_score": 3.1772930422121806, "xcomet_score": 0.9404996037483215, "xcomet_qe_score": 0.9278894066810608, "metricx_score": 1.5604934692382812, "metricx_qe_score": 1.2209655046463013, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这里的推理是,这是可能的,尽管这句话违反了直接宾语应该紧跟动词的一般语法原则。 它符合依赖长度最小化的原则,该原则指出,较短的依赖关系更可取。", "metrics": {"bleu_score": 38.82764233345062, "chrf_score": 36.79793164786893, "xcomet_score": 0.8640899658203125, "xcomet_qe_score": 0.8947241902351379, "metricx_score": 1.951170802116394, "metricx_qe_score": 2.089137077331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种树仅显示关键依赖关系的长度,即在两种结构中不恒定的那些。", "metrics": {"bleu_score": 24.427867394518046, "chrf_score": 21.03485644840321, "xcomet_score": 0.9624394178390503, "xcomet_qe_score": 0.9522665739059448, "metricx_score": 3.3568172454833984, "metricx_qe_score": 3.471539258956909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们看到“read”依赖于长度为7(以词为单位)的附加语,以及“read”依赖于长度为4的“book”。所以总共是11。", "metrics": {"bleu_score": 18.8722399976272, "chrf_score": 27.98175473504172, "xcomet_score": 0.7183367013931274, "xcomet_qe_score": 0.635039210319519, "metricx_score": 5.187009811401367, "metricx_qe_score": 6.100422382354736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个成分时,这两个依赖关系的总和就变成了六,", "metrics": {"bleu_score": 42.09694848912042, "chrf_score": 37.77565585615457, "xcomet_score": 0.6793623566627502, "xcomet_qe_score": 0.6168566346168518, "metricx_score": 5.877193927764893, "metricx_qe_score": 5.881654262542725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?所以,从十一变成", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22288751602172852, "xcomet_qe_score": 0.5367083549499512, "metricx_score": 8.677361488342285, "metricx_qe_score": 11.310319900512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "六,缩短了很多。这就是听起来还不错的", "metrics": {"bleu_score": 34.57913759237496, "chrf_score": 35.257171437001475, "xcomet_score": 0.3605077564716339, "xcomet_qe_score": 0.16344299912452698, "metricx_score": 5.920027732849121, "metricx_qe_score": 6.744178771972656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "理由,对吧?它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 59.71720991726205, "chrf_score": 61.84948092466832, "xcomet_score": 0.7780523300170898, "xcomet_qe_score": 0.5835860967636108, "metricx_score": 1.625772476196289, "metricx_qe_score": 2.961254358291626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9992790222167969, "xcomet_qe_score": 0.997222900390625, "metricx_score": 0.1849263608455658, "metricx_qe_score": 0.19376564025878906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的 Pentry Bank 中提取了各种关于搭配的信息,详情请见论文,其中解释了我们为何没有使用通用依存关系。 这些统计数据证实了之前多次提出的观察结果,即左连词倾向于较短,", "metrics": {"bleu_score": 36.862428085879706, "chrf_score": 31.556383655661374, "xcomet_score": 0.6174075603485107, "xcomet_qe_score": 0.47290316224098206, "metricx_score": 8.450990676879883, "metricx_qe_score": 8.197412490844727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此“盐和胡椒”而非“胡椒和盐”是按音节计算的。 并且", "metrics": {"bleu_score": 5.004175368982553, "chrf_score": 4.558915287575303, "xcomet_score": 0.6294777393341064, "xcomet_qe_score": 0.5646414756774902, "metricx_score": 4.040894031524658, "metricx_qe_score": 3.2679502964019775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也注意到一个随笔的观察,即这种趋势随着长度差异而增强。", "metrics": {"bleu_score": 24.180681260144148, "chrf_score": 22.76287017743822, "xcomet_score": 0.7885639667510986, "xcomet_qe_score": 0.7862775325775146, "metricx_score": 3.1589725017547607, "metricx_qe_score": 3.0235788822174072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当两个连通分量长度的差异增大时,较短的连通分量倾向于成为更强的那一个,对吗?", "metrics": {"bleu_score": 12.141800917834344, "chrf_score": 15.888165419066166, "xcomet_score": 0.6905472278594971, "xcomet_qe_score": 0.7364943027496338, "metricx_score": 6.550151824951172, "metricx_qe_score": 4.404422760009766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,左侧短连通分量的比例更大。", "metrics": {"bleu_score": 35.83129187641355, "chrf_score": 31.76662690072667, "xcomet_score": 0.8134320974349976, "xcomet_qe_score": 0.7947743535041809, "metricx_score": 3.436486005783081, "metricx_qe_score": 4.067180156707764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种趋势仅在左侧的控制者缺失时才会发生。 所以", "metrics": {"bleu_score": 43.9927688175102, "chrf_score": 39.94540235461437, "xcomet_score": 0.7135813236236572, "xcomet_qe_score": 0.6700171232223511, "metricx_score": 5.074610710144043, "metricx_qe_score": 3.9324116706848145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以是州长,州长在左边。 它", "metrics": {"bleu_score": 15.989214998944036, "chrf_score": 12.60568434450096, "xcomet_score": 0.5184325575828552, "xcomet_qe_score": 0.49304038286209106, "metricx_score": 6.983063697814941, "metricx_qe_score": 6.148552417755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中缺席,荷马来了又打了个喷嚏。", "metrics": {"bleu_score": 22.925075288505724, "chrf_score": 11.904258091513608, "xcomet_score": 0.6077181100845337, "xcomet_qe_score": 0.6742674112319946, "metricx_score": 5.676961421966553, "metricx_qe_score": 5.687480926513672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们看到的是两个动词的并列,没有外在的控制因素,", "metrics": {"bleu_score": 27.679069306659745, "chrf_score": 24.60349581752101, "xcomet_score": 0.8816908597946167, "xcomet_qe_score": 0.8423570394515991, "metricx_score": 2.814741849899292, "metricx_qe_score": 2.408236265182495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因此,在这样的情况下,左边的子句倾向于更短。差距越大,这种倾向就越明显。", "metrics": {"bleu_score": 6.489016881040744, "chrf_score": 10.879431417137395, "xcomet_score": 0.6452822685241699, "xcomet_qe_score": 0.3687620460987091, "metricx_score": 3.28938627243042, "metricx_qe_score": 3.2464306354522705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当右侧的治理,如同此处所示,左侧的治理支配了协调 Telenet 时,这种效应便消失了。 因此,我们展示", "metrics": {"bleu_score": 9.386323681787399, "chrf_score": 11.289773713408708, "xcomet_score": 0.15254512429237366, "xcomet_qe_score": 0.13990512490272522, "metricx_score": 15.082545280456543, "metricx_qe_score": 13.78044319152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了通过测量字符数,即第一列,音节数,即中间列,以及词数,即右列来实现的。", "metrics": {"bleu_score": 5.870522700069821, "chrf_score": 9.740983134787864, "xcomet_score": 0.6166775226593018, "xcomet_qe_score": 0.48092159628868103, "metricx_score": 6.659898281097412, "metricx_qe_score": 6.841868877410889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我将主要关注右侧的那一列。", "metrics": {"bleu_score": 10.82597837309053, "chrf_score": 14.010971395402205, "xcomet_score": 0.9842090606689453, "xcomet_qe_score": 0.9725342988967896, "metricx_score": 0.6750165224075317, "metricx_qe_score": 0.9193338751792908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在此看到的是,当总督位于左侧时。 随着词语绝对差值增大,左侧连词短语的长度缩短的趋势逐渐增强,在没有控制词的情况下,例如在句子并列中,也观察到相同的现象,", "metrics": {"bleu_score": 16.96828898129134, "chrf_score": 18.96825587020687, "xcomet_score": 0.407764732837677, "xcomet_qe_score": 0.3236451745033264, "metricx_score": 6.732786178588867, "metricx_qe_score": 5.737762928009033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当控制词位于右侧时,这种趋势消失。", "metrics": {"bleu_score": 23.74308987377557, "chrf_score": 22.118823837545673, "xcomet_score": 0.8226488828659058, "xcomet_qe_score": 0.7164124250411987, "metricx_score": 2.13661527633667, "metricx_qe_score": 4.2712016105651855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在本文中论证,这提供了一个反对非对称配偶结构的论据,同时支持对称配偶结构。 请参阅本文以获取", "metrics": {"bleu_score": 16.43108844788598, "chrf_score": 16.284031167617346, "xcomet_score": 0.34847769141197205, "xcomet_qe_score": 0.2530538737773895, "metricx_score": 6.713667392730713, "metricx_qe_score": 3.5102310180664062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完整的协议和论点,", "metrics": {"bleu_score": 5.209696906543444, "chrf_score": 5.7919938139424225, "xcomet_score": 0.2954699993133545, "xcomet_qe_score": 0.20920348167419434, "metricx_score": 9.804147720336914, "metricx_qe_score": 5.389074325561523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抱歉,并在会后与我们联系讨论。", "metrics": {"bleu_score": 3.4585921141027356, "chrf_score": 3.5211267605633796, "xcomet_score": 0.14741933345794678, "xcomet_qe_score": 0.15497800707817078, "metricx_score": 5.091996669769287, "metricx_qe_score": 3.8906986713409424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是项彬,华盛顿大学博士生。", "metrics": {"bleu_score": 36.19174049405417, "chrf_score": 23.692115211305058, "xcomet_score": 0.8772956132888794, "xcomet_qe_score": 0.851606547832489, "metricx_score": 0.37650519609451294, "metricx_qe_score": 0.27713432908058167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,从预训练数据到语言模型再到下游任务,追踪政治偏见导致不公正自然语言处理模型的路径。", "metrics": {"bleu_score": 57.79433162816149, "chrf_score": 52.6441926655412, "xcomet_score": 0.9370608329772949, "xcomet_qe_score": 0.8461540937423706, "metricx_score": 1.381085991859436, "metricx_qe_score": 1.7115856409072876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模网络爬取数据上进行训练的。", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 52.232562265321356, "xcomet_score": 0.9972891807556152, "xcomet_qe_score": 1.0, "metricx_score": 0.9005061388015747, "metricx_qe_score": 1.4945385456085205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在预训练数据中得到了充分覆盖。", "metrics": {"bleu_score": 63.17498622799442, "chrf_score": 62.280711610454695, "xcomet_score": 0.8200103044509888, "xcomet_qe_score": 0.7343736886978149, "metricx_score": 1.3224596977233887, "metricx_qe_score": 2.131229877471924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对 C Four 语料库的调查显示,纽约时报、洛杉矶时报、卫报、赫芬顿邮报等媒体在语言模型训练数据中得到了良好呈现。", "metrics": {"bleu_score": 36.20140717383192, "chrf_score": 31.48332166966494, "xcomet_score": 0.6955469846725464, "xcomet_qe_score": 0.6110662221908569, "metricx_score": 3.447909116744995, "metricx_qe_score": 3.3639707565307617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这给语言模型应用带来了一兼二利的结果。", "metrics": {"bleu_score": 56.32098085888194, "chrf_score": 52.74026569692204, "xcomet_score": 0.8434836864471436, "xcomet_qe_score": 0.8497660160064697, "metricx_score": 1.6770108938217163, "metricx_qe_score": 0.9850215911865234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们得以从多元视角中学习,这彰显了民主精神和思想的多元性。", "metrics": {"bleu_score": 10.642800456941323, "chrf_score": 14.159840101495172, "xcomet_score": 0.9745807647705078, "xcomet_qe_score": 0.9726154804229736, "metricx_score": 1.321669101715088, "metricx_qe_score": 0.8877311944961548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能导致后续任务应用中出现潜在的公平性问题。", "metrics": {"bleu_score": 61.82044026626229, "chrf_score": 52.67175888143731, "xcomet_score": 0.9913321733474731, "xcomet_qe_score": 0.9741852283477783, "metricx_score": 0.96357262134552, "metricx_qe_score": 1.6648533344268799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们计划调查从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体通过以下问题进行探讨: 首先,我们该如何评估语言模型的政治倾向性?预训练数据又可能在多大程度上影响这种政治偏见?", "metrics": {"bleu_score": 55.462562471097755, "chrf_score": 51.19143456978229, "xcomet_score": 0.9491481781005859, "xcomet_qe_score": 0.9395748376846313, "metricx_score": 1.0730509757995605, "metricx_qe_score": 1.349867343902588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治单元的语言模型在下游任务中的实际表现如何?这是否可能导致自然语言处理应用中的公平性问题?", "metrics": {"bleu_score": 60.05127005315014, "chrf_score": 55.77421202032108, "xcomet_score": 0.881479024887085, "xcomet_qe_score": 0.8410011529922485, "metricx_score": 2.580967664718628, "metricx_qe_score": 2.476222276687622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体而言,我们首先建议使用政治问卷(例如政治罗盘测试)的不同提示格式来引导语言模型。", "metrics": {"bleu_score": 42.456662705088156, "chrf_score": 33.92226922796567, "xcomet_score": 0.8497855067253113, "xcomet_qe_score": 0.8576448559761047, "metricx_score": 3.669891119003296, "metricx_qe_score": 3.6735851764678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保我们能够进行扎根于政治学文献的自动评估。", "metrics": {"bleu_score": 49.34494673001857, "chrf_score": 39.618875084135716, "xcomet_score": 0.9415208101272583, "xcomet_qe_score": 0.9050127267837524, "metricx_score": 1.877636432647705, "metricx_qe_score": 1.5658705234527588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步的研究结果表明,第一语言模型确实具有不同的政治含义。", "metrics": {"bleu_score": 47.05896438972141, "chrf_score": 40.238819538929384, "xcomet_score": 0.9063880443572998, "xcomet_qe_score": 0.8668614625930786, "metricx_score": 3.624499559402466, "metricx_qe_score": 1.8537135124206543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在政治光谱上的四个象限中均有分布。", "metrics": {"bleu_score": 18.543829210530703, "chrf_score": 18.072353281665777, "xcomet_score": 0.903795063495636, "xcomet_qe_score": 0.804214358329773, "metricx_score": 1.126813530921936, "metricx_qe_score": 1.9034054279327393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以观察到,GPT-4是所有语言模型中最自由主义者,而GPT系列通常比BERT系列及其变体更具社会自由主义倾向。", "metrics": {"bleu_score": 50.09547089187398, "chrf_score": 54.675284422480296, "xcomet_score": 0.8655403256416321, "xcomet_qe_score": 0.874597430229187, "metricx_score": 2.462344169616699, "metricx_qe_score": 1.5522810220718384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们的目标是研究语言模型中的政治偏见究竟在多大程度上来源于训练数据。", "metrics": {"bleu_score": 45.180525221330576, "chrf_score": 35.96826737428087, "xcomet_score": 0.9952300786972046, "xcomet_qe_score": 0.9839804172515869, "metricx_score": 0.5489861369132996, "metricx_qe_score": 0.7397558689117432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过进一步对语言模型检查点进行预训练,以进行一项受控实验,具体来说,针对六家不同的参与公司,将新闻和社交媒体内容进一步细分为其政治含义。", "metrics": {"bleu_score": 41.7842660979019, "chrf_score": 38.19584927235671, "xcomet_score": 0.7156311273574829, "xcomet_qe_score": 0.769396185874939, "metricx_score": 5.803869724273682, "metricx_qe_score": 4.9615888595581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在语料库中此类部分上进一步预训练语言模型,我们可以观察到语言模型的意识形态坐标也相应地发生偏移。 对于 Roberta 而", "metrics": {"bleu_score": 51.12586172772586, "chrf_score": 43.731905090242925, "xcomet_score": 0.4938068985939026, "xcomet_qe_score": 0.3174128830432892, "metricx_score": 8.535297393798828, "metricx_qe_score": 6.570505142211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "言,经过进一步微调,又在左倾的 Reddit 语料库上进行进一步训练,我们可以看到其... 在观点倾向性上出现了显著的自由主义偏向。 在政治偏见方面。", "metrics": {"bleu_score": 40.15281385902947, "chrf_score": 45.21806528643313, "xcomet_score": 0.15215396881103516, "xcomet_qe_score": 0.19943767786026, "metricx_score": 9.317131996154785, "metricx_qe_score": 9.756307601928711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图研究语言模型是否能够捕捉到当下社会普遍存在的两极分化现象。", "metrics": {"bleu_score": 65.88772864413933, "chrf_score": 61.02769469234318, "xcomet_score": 0.9971315860748291, "xcomet_qe_score": 1.0, "metricx_score": 0.4915179908275604, "metricx_qe_score": 0.5955060124397278, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库划分为美国第45任总统之前和之后两个时期,", "metrics": {"bleu_score": 62.75771748085487, "chrf_score": 56.86579532419559, "xcomet_score": 0.8284947872161865, "xcomet_qe_score": 0.821178674697876, "metricx_score": 1.638427972793579, "metricx_qe_score": 1.9934465885162354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别在两个不同的时间段语料库上预训练语言模型。", "metrics": {"bleu_score": 84.1354400365363, "chrf_score": 79.03101848148304, "xcomet_score": 0.8908986449241638, "xcomet_qe_score": 0.8225398659706116, "metricx_score": 0.6688066720962524, "metricx_qe_score": 0.8213677406311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以观察到,在2017年之后,语言模型普遍表现出一种偏离中立的政治倾向。", "metrics": {"bleu_score": 29.777382269160434, "chrf_score": 34.24419597381533, "xcomet_score": 0.9837373495101929, "xcomet_qe_score": 0.9896680116653442, "metricx_score": 1.4472687244415283, "metricx_qe_score": 1.495466947555542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能捕捉到社会中的极化现象。", "metrics": {"bleu_score": 41.0155947154624, "chrf_score": 36.59358799422811, "xcomet_score": 0.9972637891769409, "xcomet_qe_score": 0.997498631477356, "metricx_score": 0.7880501747131348, "metricx_qe_score": 1.0712693929672241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要,我们评估具有不同政治含义的语言模型在仇恨言论检测和虚假新闻检测方面的表现,这对于那些经常涉及语言模型且可能具有重大影响的自然语言处理应用至关重要。 因此,我们", "metrics": {"bleu_score": 35.60563554376787, "chrf_score": 35.0588617696876, "xcomet_score": 0.7483930587768555, "xcomet_qe_score": 0.7796304225921631, "metricx_score": 4.22653341293335, "metricx_qe_score": 1.8830373287200928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,如果我们按类别进行考察,也就是说,如果我们按照类别进行划分。 无论新闻媒体面向不同的社会人口统计群体或带有不同的政治含义,我们都能观察到一种规律,", "metrics": {"bleu_score": 22.322267315603106, "chrf_score": 19.843031953357734, "xcomet_score": 0.7290843725204468, "xcomet_qe_score": 0.6319015026092529, "metricx_score": 5.1180243492126465, "metricx_qe_score": 5.8965559005737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测方面,左翼语言模型表现更佳。 在检测针对社会弱势群体仇恨言论方面。 然而,我们的工作主要集中于检测针对我们社会中更有权势群体仇恨言论。", "metrics": {"bleu_score": 43.5849543518703, "chrf_score": 38.80172237512284, "xcomet_score": 0.6007080674171448, "xcomet_qe_score": 0.6296514868736267, "metricx_score": 5.57195520401001, "metricx_qe_score": 6.24656867980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之亦然,右倾语言模型在检测针对白人和男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+群体及其他少数群体仇恨言论方面则表现较差。", "metrics": {"bleu_score": 61.989662410633414, "chrf_score": 63.93772152219762, "xcomet_score": 0.9842052459716797, "xcomet_qe_score": 0.9811111688613892, "metricx_score": 0.4676557779312134, "metricx_qe_score": 0.6195433139801025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似的趋势也出现在虚假新闻检测领域,我们观察到,左派语言模型在检测来自对立政治立场的虚假信息时表现更好,反之亦然。", "metrics": {"bleu_score": 31.806104725007426, "chrf_score": 26.521080668299486, "xcomet_score": 0.9843474626541138, "xcomet_qe_score": 1.0, "metricx_score": 0.775941014289856, "metricx_qe_score": 0.8634962439537048, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这进一步将展示许多定性案例,以观察具有不同政治含义的语言模型。 确实会根据其社会类别对仇恨言论和虚假信息示例做出不同的预测。", "metrics": {"bleu_score": 58.0454433932011, "chrf_score": 49.304749907843934, "xcomet_score": 0.6331721544265747, "xcomet_qe_score": 0.6544939279556274, "metricx_score": 3.7832911014556885, "metricx_qe_score": 3.8521029949188232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中还有许多更多示例,以进一步强调这一点。 这表明语言模型在政治偏见方面存在一个非常紧迫的公平性问题。", "metrics": {"bleu_score": 60.61546514029867, "chrf_score": 55.151700187272844, "xcomet_score": 0.9729334115982056, "xcomet_qe_score": 0.9586246013641357, "metricx_score": 1.0719478130340576, "metricx_qe_score": 1.2954201698303223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果一个线性语言模型被针对仇恨言论或虚假信息等进行微调,并部署到流行的社交媒体平台的话。 这可能意味着持有相反政治观点的人们可能会被边缘化,针对少数群体的仇恨言论也可能无遏制地蔓延,且不受任何控制。 这已经引起了我们的警", "metrics": {"bleu_score": 40.81422657073672, "chrf_score": 38.00832169688755, "xcomet_score": 0.5285935401916504, "xcomet_qe_score": 0.427043080329895, "metricx_score": 4.0397868156433105, "metricx_qe_score": 4.205447673797607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "惕,促使我们正视并解决因语言模型政治争论而产生的不公平问题。", "metrics": {"bleu_score": 26.242045193010664, "chrf_score": 23.052954187594, "xcomet_score": 0.702394962310791, "xcomet_qe_score": 0.6216670274734497, "metricx_score": 5.545688629150391, "metricx_qe_score": 6.5947136878967285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,进行一些讨论。我们还", "metrics": {"bleu_score": 9.669265690880861, "chrf_score": 12.906486701976194, "xcomet_score": 0.3112620711326599, "xcomet_qe_score": 0.21756917238235474, "metricx_score": 4.827459335327148, "metricx_qe_score": 2.9281117916107178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "想强调的是,我们揭示了语言模型政治偏见所面临的独特困境。", "metrics": {"bleu_score": 59.72030816104467, "chrf_score": 48.64083243165168, "xcomet_score": 0.9312225580215454, "xcomet_qe_score": 0.7900965213775635, "metricx_score": 1.3116061687469482, "metricx_qe_score": 1.5209227800369263, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像 Sila 和 Kryptidis 之间的情形。", "metrics": {"bleu_score": 8.380866109080596, "chrf_score": 10.055272779360973, "xcomet_score": 0.4630560576915741, "xcomet_qe_score": 0.48904579877853394, "metricx_score": 9.749503135681152, "metricx_qe_score": 10.468729019165039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在语言模型训练数据中不清理政治观点,那么偏差便会从预训练数据传播到语言模型,再到下游任务,最终造成公平性问题。", "metrics": {"bleu_score": 63.34356327722992, "chrf_score": 54.72583190622113, "xcomet_score": 0.9308203458786011, "xcomet_qe_score": 0.9133201241493225, "metricx_score": 1.0048331022262573, "metricx_qe_score": 1.561468482017517, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在某种程度上试图进行清洗,我们也可能会面临审查或排除的风险,", "metrics": {"bleu_score": 32.06089916185323, "chrf_score": 30.817762485238156, "xcomet_score": 0.7762247920036316, "xcomet_qe_score": 0.7486346364021301, "metricx_score": 2.806352138519287, "metricx_qe_score": 3.01190447807312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且极其难以确定什么才是真正中立的,应该保留在语言模型训练数据中的内容。", "metrics": {"bleu_score": 15.449201629373096, "chrf_score": 18.838935255654036, "xcomet_score": 0.812061607837677, "xcomet_qe_score": 0.8341883420944214, "metricx_score": 2.1555228233337402, "metricx_qe_score": 2.3527731895446777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像“电动查理”问题。", "metrics": {"bleu_score": 23.90108882452814, "chrf_score": 22.175622092633937, "xcomet_score": 0.7802631855010986, "xcomet_qe_score": 0.6600947380065918, "metricx_score": 2.892138719558716, "metricx_qe_score": 3.066195011138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9909268617630005, "xcomet_qe_score": 0.973970890045166, "metricx_score": 0.3818603754043579, "metricx_qe_score": 0.3029481768608093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很好。我想今天差不多就到这里了。", "metrics": {"bleu_score": 7.955891555490761, "chrf_score": 10.589444201949657, "xcomet_score": 0.9356056451797485, "xcomet_qe_score": 0.8465139865875244, "metricx_score": 0.6144751906394958, "metricx_qe_score": 0.6641629934310913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Jenny,卡内基梅隆大学一年级博士生。今天我将为大家介绍你们的作品,名为“烯醇位置选择性”,即“对模型β集设计偏差的表征”。", "metrics": {"bleu_score": 35.743183353541994, "chrf_score": 31.15777799810967, "xcomet_score": 0.5879976749420166, "xcomet_qe_score": 0.6192335486412048, "metricx_score": 7.191834926605225, "metricx_qe_score": 7.762251853942871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是在华盛顿大学和艾伦人工智能研究所一些同事的合作下完成的,具体参与人员包括塞巴斯蒂安·桑蒂、罗宁·勒布拉斯、卡塔里娜·赖尼克和马丁·萨普。", "metrics": {"bleu_score": 25.006348472137212, "chrf_score": 19.090204432762697, "xcomet_score": 0.8968332409858704, "xcomet_qe_score": 0.9112135767936707, "metricx_score": 1.1478443145751953, "metricx_qe_score": 0.8886671662330627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们先假设你正在一家报纸工作,并且正在筛选你的新闻文章下的评论,试图移除有毒内容。 ", "metrics": {"bleu_score": 30.321620939451353, "chrf_score": 27.734592990577877, "xcomet_score": 0.8978750705718994, "xcomet_qe_score": 0.9449498653411865, "metricx_score": 2.099919557571411, "metricx_qe_score": 1.9266107082366943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能会转向像 Perspective API 这样流行的 API 来进行毒性检测。 如果您的名字是卡尔·琼斯,那么这会非常有效,", "metrics": {"bleu_score": 18.46610867874542, "chrf_score": 28.27847348598102, "xcomet_score": 0.6748735904693604, "xcomet_qe_score": 0.7380059957504272, "metricx_score": 4.617568016052246, "metricx_qe_score": 3.942920684814453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为 Perspective API 能够正确检测到有毒内容。", "metrics": {"bleu_score": 60.26080978557135, "chrf_score": 74.03052570040289, "xcomet_score": 0.7707810401916504, "xcomet_qe_score": 0.6902179718017578, "metricx_score": 3.781742572784424, "metricx_qe_score": 3.7696712017059326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于 Dithyasharma 而言,事实并非如此,", "metrics": {"bleu_score": 23.939493663660222, "chrf_score": 25.719759708890145, "xcomet_score": 0.8443562984466553, "xcomet_qe_score": 0.7506749629974365, "metricx_score": 2.6874489784240723, "metricx_qe_score": 2.5762667655944824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其视角 API 对在印度语境中更常见的冒犯性词汇的敏感度实际上并不高。", "metrics": {"bleu_score": 49.22872157642196, "chrf_score": 45.11979349571721, "xcomet_score": 0.7390184998512268, "xcomet_qe_score": 0.6217586398124695, "metricx_score": 5.117895126342773, "metricx_qe_score": 5.395723342895508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们观察到不同人群在使用技术时存在系统性的性能差异。", "metrics": {"bleu_score": 38.95615402875113, "chrf_score": 36.689630493771084, "xcomet_score": 0.991411566734314, "xcomet_qe_score": 0.9795000553131104, "metricx_score": 0.7635585069656372, "metricx_qe_score": 0.8854731321334839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们刚刚看到的此类设计偏见可能源于自然语言处理研究人员和模型开发者的立场性。", "metrics": {"bleu_score": 47.81183574688552, "chrf_score": 44.44288739857246, "xcomet_score": 0.9098080396652222, "xcomet_qe_score": 0.9056042432785034, "metricx_score": 1.145561933517456, "metricx_qe_score": 1.193849802017212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场性指的是个人因其人口统计学特征、身份认同和人生经历而持有的观点。", "metrics": {"bleu_score": 44.38587690908084, "chrf_score": 44.83396062267461, "xcomet_score": 0.9261175394058228, "xcomet_qe_score": 0.9230246543884277, "metricx_score": 1.439666748046875, "metricx_qe_score": 1.3745598793029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判研究中广泛使用的概念,尤其是在女性主义和酷儿学术领域。", "metrics": {"bleu_score": 68.10918846233905, "chrf_score": 61.76509153692008, "xcomet_score": 0.9806349277496338, "xcomet_qe_score": 0.9153404235839844, "metricx_score": 0.9995423555374146, "metricx_qe_score": 1.1851648092269897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究者,位置性可能会影响研究过程及其结果,因为它可能改变研究者所做的决策。 那", "metrics": {"bleu_score": 45.08277072732232, "chrf_score": 39.465612295344144, "xcomet_score": 0.7149844169616699, "xcomet_qe_score": 0.6967592239379883, "metricx_score": 6.215002536773682, "metricx_qe_score": 3.2601513862609863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么,人们可能会问的一个问题是,数据集和模型是否具有位置性?", "metrics": {"bleu_score": 48.205197721556935, "chrf_score": 43.66801339847951, "xcomet_score": 0.7288920879364014, "xcomet_qe_score": 0.7127472758293152, "metricx_score": 4.78710412979126, "metricx_qe_score": 3.2972488403320312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并非试图说明模型本身以及数据集本身拥有人口统计学身份和生活经历,但它们确实汇集了真实人们的判断和观点,因此可能代表某些立场而忽略其他立场。", "metrics": {"bleu_score": 45.01605536735423, "chrf_score": 40.72036131541253, "xcomet_score": 0.785885214805603, "xcomet_qe_score": 0.8733075857162476, "metricx_score": 1.839931607246399, "metricx_qe_score": 2.0012521743774414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既有的研究已经提出了一些关于模型具有位置性的轶事证据,例如模型和数据集中的文化差异,以及对模型位置性的理论界定。", "metrics": {"bleu_score": 33.565319945195455, "chrf_score": 31.14063713035521, "xcomet_score": 0.7693415880203247, "xcomet_qe_score": 0.6946093440055847, "metricx_score": 5.1720051765441895, "metricx_qe_score": 4.358630657196045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些研究实际上并未关注比较终端用户与数据集和模型本身。 随着自然语言处理任务日益变得主观和以社会为导向,研究模型和数据集的位置性变得越来越重要。 而且很难界定这些位置关系的偏差情况,因为并非所有决策都有记录,并且许多模型隐藏在API背后。", "metrics": {"bleu_score": 46.11408084867764, "chrf_score": 41.90131750446346, "xcomet_score": 0.6386539936065674, "xcomet_qe_score": 0.7229615449905396, "metricx_score": 4.44651985168457, "metricx_qe_score": 4.1918721199035645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的位置性,我们实际上会将真实用户的标注与现有数据集和模型进行比较。", "metrics": {"bleu_score": 56.80742334806763, "chrf_score": 49.0373527882857, "xcomet_score": 0.8327246904373169, "xcomet_qe_score": 0.9124584794044495, "metricx_score": 4.126089572906494, "metricx_qe_score": 3.3869407176971436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架,即NL定位法,来实现此目标。", "metrics": {"bleu_score": 13.173313935288004, "chrf_score": 11.319626557805783, "xcomet_score": 0.8547620177268982, "xcomet_qe_score": 0.8781394362449646, "metricx_score": 1.4344274997711182, "metricx_qe_score": 1.2516001462936401, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架包含两个主要步骤。", "metrics": {"bleu_score": 42.28428342860617, "chrf_score": 37.112876573770585, "xcomet_score": 0.9897027015686035, "xcomet_qe_score": 0.9276354908943176, "metricx_score": 0.08736787736415863, "metricx_qe_score": 0.20272676646709442, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的标注员重新标注数据集。", "metrics": {"bleu_score": 38.75407750115177, "chrf_score": 31.37407448359461, "xcomet_score": 0.8081755638122559, "xcomet_qe_score": 0.8097912669181824, "metricx_score": 3.9440348148345947, "metricx_qe_score": 3.784290313720703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是关注原始数据集或标注者的特征统计数据,因为通常只有少数标注者对每个样本进行标注,而且特征数据很少被收集和共享。", "metrics": {"bleu_score": 43.628512493875824, "chrf_score": 36.81370284381338, "xcomet_score": 0.7204251885414124, "xcomet_qe_score": 0.7423343062400818, "metricx_score": 3.251223087310791, "metricx_qe_score": 1.7757351398468018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以获得每个实例的多个标注,并获取丰富的人口统计学数据。 ", "metrics": {"bleu_score": 34.519399085465004, "chrf_score": 31.78456599734936, "xcomet_score": 0.9163126945495605, "xcomet_qe_score": 0.9102160334587097, "metricx_score": 2.196218729019165, "metricx_qe_score": 2.079282283782959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们根据人口统计学特征对标注进行分析,并使用帕森斯R相关系数将其与模型和数据集进行比较。 因此,我们的框架实际上与标注者不一致性研究的不同之处在于,它将最终用户与模型和数据集、预测和标签进行比较,而并非仅仅关注标注者一致性或建模标注者分布。", "metrics": {"bleu_score": 51.42344930702682, "chrf_score": 50.12583120267819, "xcomet_score": 0.6304447650909424, "xcomet_qe_score": 0.5995686650276184, "metricx_score": 2.7189371585845947, "metricx_qe_score": 2.636868476867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在很大程度上得益于“野实验室”(Lab in the Wild),这是一个用于我们人机交互(HCI)合作者的在线众包平台。 And", "metrics": {"bleu_score": 27.865107102801822, "chrf_score": 50.10680623378951, "xcomet_score": 0.5967394709587097, "xcomet_qe_score": 0.5601497888565063, "metricx_score": 3.248206377029419, "metricx_qe_score": 2.4193339347839355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,与 MTurk 等平台相比,我们可以招募到更多样化的志愿者,", "metrics": {"bleu_score": 43.07398081069597, "chrf_score": 57.92433736547606, "xcomet_score": 0.7121450901031494, "xcomet_qe_score": 0.45499926805496216, "metricx_score": 2.2030179500579834, "metricx_qe_score": 3.9536733627319336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后者主要参与者来自美国或印度。 此外,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 51.32096161514839, "chrf_score": 52.985735905368756, "xcomet_score": 0.7368508577346802, "xcomet_qe_score": 0.6860899925231934, "metricx_score": 6.033681392669678, "metricx_qe_score": 6.156785488128662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 Lab in the Wild 平台设置了两个任务,其中一项是社会可接受性。其运作方式是,参与者将阅读社会化学数据集中的一个情境,然后他们将写下该情境的社会可接受程度。", "metrics": {"bleu_score": 58.15441511543734, "chrf_score": 54.358402506767646, "xcomet_score": 0.9438098669052124, "xcomet_qe_score": 0.8967339396476746, "metricx_score": 1.906960368156433, "metricx_qe_score": 2.41153621673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对学习的参与度,他们可以对比自己与人工智能及其他人的回答。 随后", "metrics": {"bleu_score": 32.69770644160053, "chrf_score": 27.226167806663167, "xcomet_score": 0.7879430055618286, "xcomet_qe_score": 0.8211696743965149, "metricx_score": 4.277920722961426, "metricx_qe_score": 1.2015395164489746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们将这些标注与社会化学、德尔菲法和GPT 4进行了比较。 ", "metrics": {"bleu_score": 50.03557455114012, "chrf_score": 41.75551243922707, "xcomet_score": 0.7013439536094666, "xcomet_qe_score": 0.7099997997283936, "metricx_score": 3.6078975200653076, "metricx_qe_score": 3.616848945617676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们针对毒性和仇恨言论检测任务,复制了一个非常相似的设置,在该设置中,他们将阅读来自Dana Hate的数据实例,并判断其是否属于仇恨言论。 随后", "metrics": {"bleu_score": 39.154171782685744, "chrf_score": 35.88799990557114, "xcomet_score": 0.5493121147155762, "xcomet_qe_score": 0.4515587091445923, "metricx_score": 4.433817386627197, "metricx_qe_score": 2.7903940677642822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们将这些标注与DynaHate、Perspective API、Rewire API、HateRoberta和GPT-4进行了对比。", "metrics": {"bleu_score": 37.37864949618809, "chrf_score": 75.43847188462102, "xcomet_score": 0.8651135563850403, "xcomet_qe_score": 0.8962090611457825, "metricx_score": 3.091482400894165, "metricx_qe_score": 3.578054904937744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们的研究积累了超过一万六千个标注,来自八十七个国家的上千名标注员。", "metrics": {"bleu_score": 19.787401766449317, "chrf_score": 16.35622546037435, "xcomet_score": 0.9375189542770386, "xcomet_qe_score": 0.9679352045059204, "metricx_score": 3.312810182571411, "metricx_qe_score": 2.209906816482544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们已经具备了更好的条件来回答自然语言处理数据集和模型最符合哪些立场的问题。", "metrics": {"bleu_score": 25.053766264982826, "chrf_score": 28.94009525601828, "xcomet_score": 0.8858143091201782, "xcomet_qe_score": 0.7993380427360535, "metricx_score": 0.7184113264083862, "metricx_qe_score": 0.8894986510276794, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现自然语言处理领域存在立场性。 我们", "metrics": {"bleu_score": 24.688498672025876, "chrf_score": 22.178561533255383, "xcomet_score": 0.6192238926887512, "xcomet_qe_score": 0.624061107635498, "metricx_score": 4.937962532043457, "metricx_qe_score": 1.0775669813156128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现,数据集和模型与英语国家最为契合。", "metrics": {"bleu_score": 31.444119130934432, "chrf_score": 27.878948377789285, "xcomet_score": 0.8592759370803833, "xcomet_qe_score": 0.8696526885032654, "metricx_score": 1.502397060394287, "metricx_qe_score": 1.4012229442596436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在GPD 4社会可接受性分析中,我们发现其与儒家文化及英语国家最为契合", "metrics": {"bleu_score": 45.83034067124108, "chrf_score": 41.04632388284749, "xcomet_score": 0.8982608318328857, "xcomet_qe_score": 0.7121536731719971, "metricx_score": 1.2251713275909424, "metricx_qe_score": 1.8538048267364502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 5.154639175257731, "xcomet_score": 0.3009641468524933, "xcomet_qe_score": 0.13554313778877258, "metricx_score": 16.023874282836914, "metricx_qe_score": 25.0, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,Dynamite Hate也与英语国家最为契合。", "metrics": {"bleu_score": 12.83689914483881, "chrf_score": 10.507955148431064, "xcomet_score": 0.1429215669631958, "xcomet_qe_score": 0.14270877838134766, "metricx_score": 12.341530799865723, "metricx_qe_score": 10.22176456451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,在社会可接受性评估任务中,GPT-4 的输出与受大学教育或研究生教育的人群最为一致。 我们同样在 Dani Hate 中发现了这一现象,它与受过大学教育的人群最为契合。", "metrics": {"bleu_score": 44.673911719889865, "chrf_score": 40.62842691313594, "xcomet_score": 0.7245466709136963, "xcomet_qe_score": 0.6044654846191406, "metricx_score": 5.646783351898193, "metricx_qe_score": 5.509310722351074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,不可避免地会有人群被遗漏。 一个例子", "metrics": {"bleu_score": 51.674261960600504, "chrf_score": 46.53394454683898, "xcomet_score": 0.7304445505142212, "xcomet_qe_score": 0.6514123678207397, "metricx_score": 1.9862704277038574, "metricx_qe_score": 1.4979188442230225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,数据集和模型在对非二元性别者(non-binary people)的匹配度上,低于他们的男性和女性对应者。", "metrics": {"bleu_score": 24.51341488520203, "chrf_score": 22.915579070230212, "xcomet_score": 0.7823378443717957, "xcomet_qe_score": 0.8139627575874329, "metricx_score": 6.054247856140137, "metricx_qe_score": 5.592223167419434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT 4社会可接受性任务以及Dynahate任务分析中也发现了这一点。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.8893300890922546, "xcomet_qe_score": 0.8281604051589966, "metricx_score": 1.4582228660583496, "metricx_qe_score": 2.388806104660034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,鉴于存在位置分析动力学 LP,我们应该如何处理它呢?", "metrics": {"bleu_score": 4.996434935345059, "chrf_score": 9.148573906614345, "xcomet_score": 0.7945135831832886, "xcomet_qe_score": 0.8292223215103149, "metricx_score": 6.249025821685791, "metricx_qe_score": 6.4161858558654785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对此有一些建议。", "metrics": {"bleu_score": 47.037095938668976, "chrf_score": 46.000809807896594, "xcomet_score": 0.9680447578430176, "xcomet_qe_score": 0.9546129703521729, "metricx_score": 0.3107627034187317, "metricx_qe_score": 0.378722608089447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,是在整个研究过程中记录所有相关设计决策。另一个建议", "metrics": {"bleu_score": 44.17918226831576, "chrf_score": 41.35455935428756, "xcomet_score": 0.7643747925758362, "xcomet_qe_score": 0.6455081701278687, "metricx_score": 3.521282911300659, "metricx_qe_score": 1.3528335094451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,从视域主义的角度进行自然语言处理研究。", "metrics": {"bleu_score": 51.74260437072165, "chrf_score": 52.40426759854904, "xcomet_score": 0.8065372705459595, "xcomet_qe_score": 0.7840391397476196, "metricx_score": 2.4517478942871094, "metricx_qe_score": 2.2304136753082275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三项建议是,在四个特定的社区内构建专业化的数据集和模型。", "metrics": {"bleu_score": 54.82970629055102, "chrf_score": 48.885877488637234, "xcomet_score": 0.9319486021995544, "xcomet_qe_score": 0.9275120496749878, "metricx_score": 0.5417530536651611, "metricx_qe_score": 1.0298863649368286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子就是Masakane倡议。", "metrics": {"bleu_score": 57.83569866465144, "chrf_score": 44.95539690085339, "xcomet_score": 0.7487726211547852, "xcomet_qe_score": 0.8110054135322571, "metricx_score": 2.2933404445648193, "metricx_qe_score": 4.320682525634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望强调的是,包容性自然语言处理不仅仅是让所有", "metrics": {"bleu_score": 24.91558127564333, "chrf_score": 22.277583555384446, "xcomet_score": 0.643888533115387, "xcomet_qe_score": 0.21737922728061676, "metricx_score": 5.543379306793213, "metricx_qe_score": 4.0999836921691895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人服务。", "metrics": {"bleu_score": 39.03674453747003, "chrf_score": 36.76370111713883, "xcomet_score": 0.9485549926757812, "xcomet_qe_score": 0.9516949653625488, "metricx_score": 0.7394589781761169, "metricx_qe_score": 1.1236159801483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "至此,我们的演示就告一段", "metrics": {"bleu_score": 22.416933501922287, "chrf_score": 19.554328109211287, "xcomet_score": 0.8982166051864624, "xcomet_qe_score": 0.8502671122550964, "metricx_score": 2.6838412284851074, "metricx_qe_score": 1.142357587814331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "落。如果您希望了解更多,欢迎访问我们的仪表盘,获取最新的分析结果,并查阅我们的论文。", "metrics": {"bleu_score": 40.958214162080495, "chrf_score": 32.9341349206561, "xcomet_score": 0.6233460903167725, "xcomet_qe_score": 0.5740354061126709, "metricx_score": 5.0798516273498535, "metricx_qe_score": 5.403185844421387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是范奈大学的席渊。", "metrics": {"bleu_score": 12.486657525198936, "chrf_score": 9.296062571130456, "xcomet_score": 0.6995021104812622, "xcomet_qe_score": 0.7415839433670044, "metricx_score": 4.775058269500732, "metricx_qe_score": 3.716278314590454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我在此介绍我们关于从行语言模型中分离脚本知识,以用于约束语言规划的工作。", "metrics": {"bleu_score": 31.81824677903223, "chrf_score": 25.961258383917464, "xcomet_score": 0.7016173005104065, "xcomet_qe_score": 0.7158983945846558, "metricx_score": 3.2781460285186768, "metricx_qe_score": 2.6930816173553467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类常常遵循分步骤的指示,按照保证可执行的脚本来规划他们的行动。 既", "metrics": {"bleu_score": 22.494652836619995, "chrf_score": 22.814989494561306, "xcomet_score": 0.7429095506668091, "xcomet_qe_score": 0.7310000658035278, "metricx_score": 5.5059003829956055, "metricx_qe_score": 1.8440431356430054, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "往的研究探索了语言模型以规划抽象目标,例如制作蛋糕等刻板活动的流程,并", "metrics": {"bleu_score": 22.148418795159994, "chrf_score": 21.45943442625483, "xcomet_score": 0.347014844417572, "xcomet_qe_score": 0.2625080645084381, "metricx_score": 5.77992582321167, "metricx_qe_score": 3.2869815826416016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表明大型语言模型能够有效地将目标分解为步骤。", "metrics": {"bleu_score": 43.44547766853482, "chrf_score": 39.19723663056523, "xcomet_score": 0.8652112483978271, "xcomet_qe_score": 0.921674907207489, "metricx_score": 1.4061341285705566, "metricx_qe_score": 0.8726357221603394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,先前的工作主要集中于为刻板活动的抽象目标进行规划。", "metrics": {"bleu_score": 43.52771377941511, "chrf_score": 39.78160135916036, "xcomet_score": 0.8064430952072144, "xcomet_qe_score": 0.76207435131073, "metricx_score": 1.8310844898223877, "metricx_qe_score": 2.08767032623291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于具有具体目标、具体约束的目标进行规划,例如制作巧克力蛋糕,仍然鲜有研究。", "metrics": {"bleu_score": 17.548433488411245, "chrf_score": 19.529542896320077, "xcomet_score": 0.8212460279464722, "xcomet_qe_score": 0.8162809610366821, "metricx_score": 1.2973170280456543, "metricx_qe_score": 1.6055818796157837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受约束的语言规划问题。 这会对规划的目标施加不同的约束。", "metrics": {"bleu_score": 50.03686586175548, "chrf_score": 39.25434267346032, "xcomet_score": 0.8935494422912598, "xcomet_qe_score": 0.8719019889831543, "metricx_score": 1.1842719316482544, "metricx_qe_score": 1.5635541677474976, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以通过多个具有多重约束的现实特定目标来继承。", "metrics": {"bleu_score": 30.14043270079055, "chrf_score": 27.160855288971447, "xcomet_score": 0.9447604417800903, "xcomet_qe_score": 0.9595550298690796, "metricx_score": 1.6830408573150635, "metricx_qe_score": 1.3548592329025269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "优秀的规划者应该编写符合约束条件且忠实于现实的脚本。", "metrics": {"bleu_score": 38.37444269599674, "chrf_score": 30.728758015783157, "xcomet_score": 0.9747521877288818, "xcomet_qe_score": 0.8923701643943787, "metricx_score": 0.8177751302719116, "metricx_qe_score": 0.8970010280609131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文首先评估并提升大型语言模型的约束式语言规划能力。", "metrics": {"bleu_score": 44.65533352935108, "chrf_score": 36.794730036263594, "xcomet_score": 0.8498431444168091, "xcomet_qe_score": 0.9547611474990845, "metricx_score": 0.6715755462646484, "metricx_qe_score": 0.610139012336731, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于缺乏特定目标的数据集来确定我们的起点。 首先,我们必须实现这个目标。", "metrics": {"bleu_score": 32.042063720607956, "chrf_score": 27.05909885720682, "xcomet_score": 0.737346887588501, "xcomet_qe_score": 0.7859853506088257, "metricx_score": 5.188635349273682, "metricx_qe_score": 4.8100266456604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们利用结构化TPT,通过修改约束来扩展抽象目标,从而实现人工回路数据采集。", "metrics": {"bleu_score": 39.26948355286693, "chrf_score": 29.31231490536294, "xcomet_score": 0.7281662225723267, "xcomet_qe_score": 0.7136766910552979, "metricx_score": 6.090691089630127, "metricx_qe_score": 6.0280327796936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选取了100个特定的目标,并评估了从大型模型生成的脚本。", "metrics": {"bleu_score": 46.0249537136194, "chrf_score": 41.75912593153973, "xcomet_score": 0.9313157796859741, "xcomet_qe_score": 0.875219464302063, "metricx_score": 2.3586158752441406, "metricx_qe_score": 3.3433046340942383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9908939599990845, "xcomet_qe_score": 0.9898492097854614, "metricx_score": 0.776574969291687, "metricx_qe_score": 0.7304373979568481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有线性模型在规划特定目标方面均未达到令人满意的结果。", "metrics": {"bleu_score": 32.795813790517194, "chrf_score": 26.82474777917856, "xcomet_score": 0.8903677463531494, "xcomet_qe_score": 0.8876886367797852, "metricx_score": 1.9734306335449219, "metricx_qe_score": 2.47652268409729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们进行详细分析,以探讨学习模块的用途。", "metrics": {"bleu_score": 26.25803695302771, "chrf_score": 22.617199151806382, "xcomet_score": 0.7457326054573059, "xcomet_qe_score": 0.7483034133911133, "metricx_score": 4.71735143661499, "metricx_qe_score": 5.82042932510376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图示结果表明,生成的脚本在语义完整性方面表现可接受,但对约束条件的忠实性无法得到保证。", "metrics": {"bleu_score": 50.90592077494645, "chrf_score": 44.9210426580808, "xcomet_score": 0.9918243885040283, "xcomet_qe_score": 0.9888533353805542, "metricx_score": 0.855481743812561, "metricx_qe_score": 1.213883876800537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入探讨了在居家环境中,基于醒觉状态的更坦诚、分级的约束主题类别。", "metrics": {"bleu_score": 31.666472263798333, "chrf_score": 28.61642736739647, "xcomet_score": 0.4485208988189697, "xcomet_qe_score": 0.2860405445098877, "metricx_score": 7.679997444152832, "metricx_qe_score": 9.156173706054688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的头部地图显示,针对不同类别的女童,指导性 DPD 的规划性能存在相当大的差异。", "metrics": {"bleu_score": 20.602824077732862, "chrf_score": 18.2695008392278, "xcomet_score": 0.33801960945129395, "xcomet_qe_score": 0.21133992075920105, "metricx_score": 8.130690574645996, "metricx_qe_score": 8.0502347946167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既往研究表明,轻量级模型的输出质量存在高方差,导致性能不佳。", "metrics": {"bleu_score": 33.374308620679585, "chrf_score": 28.239572724570966, "xcomet_score": 0.7696696519851685, "xcomet_qe_score": 0.7885080575942993, "metricx_score": 3.0748655796051025, "metricx_qe_score": 2.7675106525421143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们借鉴了过生成禅滤波器的思想,以提升生成质量。", "metrics": {"bleu_score": 18.626757479237572, "chrf_score": 16.72887749745867, "xcomet_score": 0.8376172780990601, "xcomet_qe_score": 0.8184366822242737, "metricx_score": 5.632907867431641, "metricx_qe_score": 6.462300777435303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示约束类型,并结合指导式CPT的例子,从而获得基于所述抽象目标的具体目标。", "metrics": {"bleu_score": 29.616481735955045, "chrf_score": 23.451954464632184, "xcomet_score": 0.8141219019889832, "xcomet_qe_score": 0.8189913630485535, "metricx_score": 2.8697009086608887, "metricx_qe_score": 3.380662202835083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后指示GPT生成针对特定目标的案例脚本。", "metrics": {"bleu_score": 15.936357366603362, "chrf_score": 15.386386093792579, "xcomet_score": 0.737531304359436, "xcomet_qe_score": 0.7573533058166504, "metricx_score": 3.8944132328033447, "metricx_qe_score": 3.9031455516815186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,开发了一个滤波器模型,用于选择那些不稳定的脚本。", "metrics": {"bleu_score": 25.881733463141213, "chrf_score": 21.207694555643243, "xcomet_score": 0.7873873710632324, "xcomet_qe_score": 0.7672892212867737, "metricx_score": 2.8999745845794678, "metricx_qe_score": 3.1623599529266357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转化为 instruct GPT 的指令,并计算余弦相似度和相似度得分,以衡量语义相似性。", "metrics": {"bleu_score": 53.33354785210042, "chrf_score": 52.229901371565425, "xcomet_score": 0.9084515571594238, "xcomet_qe_score": 0.8177371025085449, "metricx_score": 2.84328556060791, "metricx_qe_score": 3.0005717277526855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们将编写包含目标约束关键词的脚本。我们仅保留", "metrics": {"bleu_score": 48.52277021118393, "chrf_score": 49.66638782687229, "xcomet_score": 0.5796307325363159, "xcomet_qe_score": 0.5725150108337402, "metricx_score": 8.658753395080566, "metricx_qe_score": 7.726655960083008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该脚本,如果目标高分在目标站点上获得最高分。", "metrics": {"bleu_score": 9.975896756034327, "chrf_score": 12.50204513902792, "xcomet_score": 0.5168202519416809, "xcomet_qe_score": 0.39261147379875183, "metricx_score": 10.157181739807129, "metricx_qe_score": 10.245590209960938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法能够从不足中生成具有发丝般精细度的结构。", "metrics": {"bleu_score": 17.268932789342518, "chrf_score": 11.615600626548488, "xcomet_score": 0.4909970164299011, "xcomet_qe_score": 0.3977593183517456, "metricx_score": 6.000441551208496, "metricx_qe_score": 6.770787239074707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法极大程度地提高了计划的可实现性,无论是在语义完整性还是对约束的忠实度方面。", "metrics": {"bleu_score": 36.68853426663415, "chrf_score": 35.10815033681091, "xcomet_score": 0.9892956018447876, "xcomet_qe_score": 0.990533709526062, "metricx_score": 1.0300929546356201, "metricx_qe_score": 1.1753405332565308, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高昂,因此至关重要的是赋予小型且专业化模型的语言规划能力。", "metrics": {"bleu_score": 65.25854454591509, "chrf_score": 60.41088032948723, "xcomet_score": 0.9861539602279663, "xcomet_qe_score": 0.9828753471374512, "metricx_score": 0.3125913441181183, "metricx_qe_score": 0.3599075675010681, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键步骤。", "metrics": {"bleu_score": 69.6015973294402, "chrf_score": 66.30344838521414, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026699073612689972, "metricx_qe_score": 0.14870662987232208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,既往研究无法用于针对特定目标进行规划,并且手动数据集标注成本高昂。", "metrics": {"bleu_score": 48.08831927104961, "chrf_score": 39.595543990209414, "xcomet_score": 0.9919419288635254, "xcomet_qe_score": 0.991173505783081, "metricx_score": 1.0564649105072021, "metricx_qe_score": 1.513114094734192, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的思想,从大型语言模型中蒸馏受约束的语言规划数据站点。", "metrics": {"bleu_score": 46.26449536893796, "chrf_score": 39.37964693464731, "xcomet_score": 0.7262645959854126, "xcomet_qe_score": 0.6601917743682861, "metricx_score": 5.732100009918213, "metricx_qe_score": 5.363205432891846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将运用我们的方法构建一个命名的连结语言规划数据集,称为代码脚本。", "metrics": {"bleu_score": 27.40605269906809, "chrf_score": 22.563235551722478, "xcomet_score": 0.7793062925338745, "xcomet_qe_score": 0.7653058767318726, "metricx_score": 5.105487823486328, "metricx_qe_score": 5.065993309020996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了五万五千个特定目标,", "metrics": {"bleu_score": 11.19644153835206, "chrf_score": 10.783756468765313, "xcomet_score": 0.8856856822967529, "xcomet_qe_score": 0.8848632574081421, "metricx_score": 2.3597114086151123, "metricx_qe_score": 3.1827428340911865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并编写脚本以确保验证和测试站点的质量。我们要求云端众包人员查找并修正不正确的样本。", "metrics": {"bleu_score": 34.78098270345834, "chrf_score": 30.694515213414046, "xcomet_score": 0.3266347050666809, "xcomet_qe_score": 0.21343892812728882, "metricx_score": 4.682405471801758, "metricx_qe_score": 5.081207752227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图展示了代码脚本的约束分布。", "metrics": {"bleu_score": 37.5022891676693, "chrf_score": 23.913122665200788, "xcomet_score": 0.856412947177887, "xcomet_qe_score": 0.8248428106307983, "metricx_score": 3.078789234161377, "metricx_qe_score": 3.9585700035095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成的特定目标中表现出超几何分布的特征。", "metrics": {"bleu_score": 33.33649064649528, "chrf_score": 23.84063723309709, "xcomet_score": 0.7740787267684937, "xcomet_qe_score": 0.7590961456298828, "metricx_score": 5.148984432220459, "metricx_qe_score": 5.096748352050781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "借助代码脚本,我们可以追溯更小、更专业的约束语言规划模型。", "metrics": {"bleu_score": 19.257758240643728, "chrf_score": 13.315894327985681, "xcomet_score": 0.6283074021339417, "xcomet_qe_score": 0.5824458599090576, "metricx_score": 6.213967800140381, "metricx_qe_score": 5.9159111976623535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "借助Antsight、TFILF以及调整后的光标速率,可以生成比大多数大型语言模型更高质量的脚本,这表明,当在合适的训练数据集上进行适当训练时,较小的模型可以支持大型模型。", "metrics": {"bleu_score": 44.5942238192876, "chrf_score": 36.58812657558208, "xcomet_score": 0.40853166580200195, "xcomet_qe_score": 0.3211570978164673, "metricx_score": 9.571576118469238, "metricx_qe_score": 9.044441223144531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "综上所述,我们确立了约束语言规划问题。", "metrics": {"bleu_score": 37.087658421061406, "chrf_score": 31.83944337969106, "xcomet_score": 0.8756880760192871, "xcomet_qe_score": 0.8624190092086792, "metricx_score": 1.78568696975708, "metricx_qe_score": 2.411616563796997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的约束语言规划能力,并开发了一种用于大型语言模型的超生成过滤方法。", "metrics": {"bleu_score": 44.77658779934736, "chrf_score": 35.787954176183455, "xcomet_score": 0.7996286153793335, "xcomet_qe_score": 0.7886515855789185, "metricx_score": 2.835568428039551, "metricx_qe_score": 3.1662302017211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们利用大型语言模型生成高质量的脚本数据集,用于约束性语言规划。", "metrics": {"bleu_score": 51.71916555488684, "chrf_score": 37.84142205803256, "xcomet_score": 0.9867336750030518, "xcomet_qe_score": 0.9638887643814087, "metricx_score": 2.155921459197998, "metricx_qe_score": 2.6865105628967285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期望该代码数据集能够成为推动语言规划研究的重要资源。", "metrics": {"bleu_score": 40.016016019224985, "chrf_score": 29.612837105097377, "xcomet_score": 0.9753332138061523, "xcomet_qe_score": 0.9703986644744873, "metricx_score": 1.718830943107605, "metricx_qe_score": 1.9738725423812866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查阅更详细的代码脚本信息。", "metrics": {"bleu_score": 43.7241098509127, "chrf_score": 29.449374731721274, "xcomet_score": 0.8776659965515137, "xcomet_qe_score": 0.8783209919929504, "metricx_score": 2.3597638607025146, "metricx_qe_score": 2.3078525066375732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好,我叫舒恒。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 6.517341040462428, "xcomet_score": 0.8747873306274414, "xcomet_qe_score": 0.833567202091217, "metricx_score": 0.19516301155090332, "metricx_qe_score": 0.4691231846809387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将为大家介绍我们的论文《内核2003命名实体标注器在2023年是否仍然有效?》。", "metrics": {"bleu_score": 66.12501377734411, "chrf_score": 68.31343843486722, "xcomet_score": 0.78495854139328, "xcomet_qe_score": 0.7782971858978271, "metricx_score": 2.558786392211914, "metricx_qe_score": 1.7950842380523682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.9978755712509155, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了泛化问题,以命名实体识别任务(或NER任务)作为研究对象。", "metrics": {"bleu_score": 44.78315454797505, "chrf_score": 43.0271701321263, "xcomet_score": 0.9425225853919983, "xcomet_qe_score": 0.9182649850845337, "metricx_score": 1.2815200090408325, "metricx_qe_score": 2.546846866607666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用了Kono two thousand three近二十年开发命名实体识别(NER)技术。这自然会引发几个问题。", "metrics": {"bleu_score": 24.203431739821713, "chrf_score": 19.979723594524707, "xcomet_score": 0.6683840751647949, "xcomet_qe_score": 0.6535931825637817, "metricx_score": 8.578207015991211, "metricx_qe_score": 9.318014144897461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据?", "metrics": {"bleu_score": 53.12583871630397, "chrf_score": 42.12696617108382, "xcomet_score": 0.9173398017883301, "xcomet_qe_score": 0.9163376688957214, "metricx_score": 0.46578866243362427, "metricx_qe_score": 0.3853972554206848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在开发新的标注器时,良好泛化需要什么?", "metrics": {"bleu_score": 35.9198007972199, "chrf_score": 30.598920277783247, "xcomet_score": 0.8664987087249756, "xcomet_qe_score": 0.8188917636871338, "metricx_score": 1.1546053886413574, "metricx_qe_score": 1.2590148448944092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们确实观察到泛化能力不足,是什么原因导致这些模型的性能下降?", "metrics": {"bleu_score": 46.166475972534194, "chrf_score": 39.90018327304855, "xcomet_score": 0.9948811531066895, "xcomet_qe_score": 0.9823131561279297, "metricx_score": 0.949950098991394, "metricx_qe_score": 0.860862135887146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了 Kono plus plus 数据集。这是", "metrics": {"bleu_score": 38.398171330793495, "chrf_score": 25.693913008731645, "xcomet_score": 0.5659909248352051, "xcomet_qe_score": 0.5284953713417053, "metricx_score": 7.9162797927856445, "metricx_qe_score": 6.18649435043335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透社新闻中收集的数据集,并根据 Kono 2003 的标注指南进行了标注。 ", "metrics": {"bleu_score": 35.76012620898161, "chrf_score": 29.781121053416804, "xcomet_score": 0.5517289638519287, "xcomet_qe_score": 0.5156205296516418, "metricx_score": 6.323876857757568, "metricx_qe_score": 6.456335544586182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们在 Kono 2023 上对二十多个模型进行了精细调整。", "metrics": {"bleu_score": 29.608660106487147, "chrf_score": 24.42970588372887, "xcomet_score": 0.6747359037399292, "xcomet_qe_score": 0.6813217401504517, "metricx_score": 7.27931022644043, "metricx_qe_score": 8.182848930358887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 Kono 3 测试集和 Kono Plus 测试集对它们进行了评估。", "metrics": {"bleu_score": 45.92064719908955, "chrf_score": 32.34328459655465, "xcomet_score": 0.5075638294219971, "xcomet_qe_score": 0.526881217956543, "metricx_score": 5.595032691955566, "metricx_qe_score": 5.611486434936523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要的一点是,我们计算了 F 分数的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 42.86830618878085, "chrf_score": 51.43945588316333, "xcomet_score": 0.9809139966964722, "xcomet_qe_score": 0.972978949546814, "metricx_score": 1.3505089282989502, "metricx_qe_score": 1.8902502059936523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为了实现良好的泛化,需要具备什么呢?", "metrics": {"bleu_score": 36.394125309794774, "chrf_score": 38.04711781299627, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3712695837020874, "metricx_qe_score": 0.45118582248687744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必需的。", "metrics": {"bleu_score": 23.956565612760205, "chrf_score": 21.59867695133108, "xcomet_score": 0.9930475950241089, "xcomet_qe_score": 0.9945417642593384, "metricx_score": 0.6633017659187317, "metricx_qe_score": 0.996856689453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是模型架构。", "metrics": {"bleu_score": 60.042877124855906, "chrf_score": 51.821001027418625, "xcomet_score": 0.9962185621261597, "xcomet_qe_score": 0.9754199981689453, "metricx_score": 0.04136792570352554, "metricx_qe_score": 0.07232436537742615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现transformer模型通常能更好地泛化到新的数据上。", "metrics": {"bleu_score": 40.51534226887187, "chrf_score": 50.99131780300166, "xcomet_score": 0.9353175163269043, "xcomet_qe_score": 0.939651608467102, "metricx_score": 1.7352681159973145, "metricx_qe_score": 3.644129991531372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常情况下,更大的模型能够带来更好的泛化能力。", "metrics": {"bleu_score": 32.88861494180287, "chrf_score": 29.694258227593007, "xcomet_score": 0.9891072511672974, "xcomet_qe_score": 0.9852021336555481, "metricx_score": 0.6268782615661621, "metricx_qe_score": 0.6676300168037415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要的一点是,我们都知道微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 47.37206772899839, "chrf_score": 58.61054348199759, "xcomet_score": 0.9559866189956665, "xcomet_qe_score": 0.8947153687477112, "metricx_score": 3.7592053413391113, "metricx_qe_score": 2.025888204574585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调示例实际上也带来了更好的泛化能力。", "metrics": {"bleu_score": 61.71392021671757, "chrf_score": 54.0859508289967, "xcomet_score": 0.9877923727035522, "xcomet_qe_score": 0.9172162413597107, "metricx_score": 0.5672743320465088, "metricx_qe_score": 0.8571842312812805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们进入下一个问题:是什么导致某些模型的性能下降? 我们提出了两个假设。", "metrics": {"bleu_score": 34.63746330181649, "chrf_score": 30.408837432403107, "xcomet_score": 0.9200836420059204, "xcomet_qe_score": 0.919425904750824, "metricx_score": 1.216877818107605, "metricx_qe_score": 1.0394091606140137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,即由于重复使用同一测试集而导致的过拟合现象,这通常会在新的测试集上表现为收益递减。", "metrics": {"bleu_score": 55.23546883195189, "chrf_score": 46.43482529400195, "xcomet_score": 0.9693264961242676, "xcomet_qe_score": 0.8402611017227173, "metricx_score": 1.5299315452575684, "metricx_qe_score": 2.2883572578430176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间时间差距不断扩大而导致性能下降。", "metrics": {"bleu_score": 53.9040412756569, "chrf_score": 47.84217588774713, "xcomet_score": 0.9574272632598877, "xcomet_qe_score": 0.8816723823547363, "metricx_score": 1.5271977186203003, "metricx_qe_score": 2.1127090454101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表可以观察到,红色的最佳拟合线具有大于一的梯度。", "metrics": {"bleu_score": 24.719717067792043, "chrf_score": 25.823514070098824, "xcomet_score": 0.8699229955673218, "xcomet_qe_score": 0.7636812925338745, "metricx_score": 1.2403490543365479, "metricx_qe_score": 1.4271326065063477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们对Carl 2003所做的每一次改进,都转化为Carl++上超过一个单位的改进,这意味着不存在边际效应递减。", "metrics": {"bleu_score": 24.575159036693414, "chrf_score": 21.274714198111685, "xcomet_score": 0.6854382157325745, "xcomet_qe_score": 0.678114652633667, "metricx_score": 9.930746078491211, "metricx_qe_score": 9.305110931396484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下,没有观察到自适应过拟合现象。", "metrics": {"bleu_score": 71.40573910176903, "chrf_score": 67.43963602768585, "xcomet_score": 0.901500940322876, "xcomet_qe_score": 0.9034317135810852, "metricx_score": 0.9103628993034363, "metricx_qe_score": 1.1414209604263306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,暂时的漂移又该如何看待呢?", "metrics": {"bleu_score": 7.655122720591221, "chrf_score": 13.163057124921531, "xcomet_score": 0.8847143650054932, "xcomet_qe_score": 0.906239926815033, "metricx_score": 1.7863309383392334, "metricx_qe_score": 2.5273733139038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于时间漂移,我们进行了一项实验,利用更近期的数据对部分模型进行重新训练或继续预训练,结果发现,更大的时间间隔会导致性能下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 51.093840185972056, "chrf_score": 45.65055542811168, "xcomet_score": 0.9632970094680786, "xcomet_qe_score": 0.9151185154914856, "metricx_score": 1.5670284032821655, "metricx_qe_score": 1.562597393989563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型尺寸,以及更多的微调示例,", "metrics": {"bleu_score": 73.0972650240444, "chrf_score": 69.79910321114897, "xcomet_score": 0.9038512706756592, "xcomet_qe_score": 0.9186066389083862, "metricx_score": 0.8524594306945801, "metricx_qe_score": 0.6591191291809082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素是相互关联的。我们不能只依赖其中一种因素,而抛弃其他因素。", "metrics": {"bleu_score": 37.05827956501468, "chrf_score": 33.87872789008176, "xcomet_score": 0.9924823045730591, "xcomet_qe_score": 0.9887330532073975, "metricx_score": 0.44542229175567627, "metricx_qe_score": 0.5289888381958008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与此同时,我们还发现这里的性能下降是由时间漂移造成的,而且颇为令人惊讶的是,这并非由自适应过拟合引起,尽管Kono 2003已经使用了二十多年。", "metrics": {"bleu_score": 47.61728460831511, "chrf_score": 42.43016183688497, "xcomet_score": 0.6680541038513184, "xcomet_qe_score": 0.6355422735214233, "metricx_score": 4.712701320648193, "metricx_qe_score": 4.850622177124023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们论文标题中提出的问题,2003 年的 Kono 词标注器在 2023 年是否仍然有效?", "metrics": {"bleu_score": 67.45409323444204, "chrf_score": 55.53908019989361, "xcomet_score": 0.6462438702583313, "xcomet_qe_score": 0.5897424221038818, "metricx_score": 5.529306888580322, "metricx_qe_score": 6.091930389404297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究发现,答案实际上是绝对肯定的。", "metrics": {"bleu_score": 18.759202316167215, "chrf_score": 25.505808038721757, "xcomet_score": 0.9967318773269653, "xcomet_qe_score": 0.9818230867385864, "metricx_score": 0.6693812608718872, "metricx_qe_score": 0.7544026374816895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望本文能促使人们对如何提升模型泛化能力开展更多研究。", "metrics": {"bleu_score": 33.454126768975264, "chrf_score": 28.30247602608067, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2982960641384125, "metricx_qe_score": 0.27186185121536255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查阅我们的论文、数据集。如果您有任何疑问,欢迎随时与我联系。", "metrics": {"bleu_score": 60.102321689564704, "chrf_score": 48.87528718320061, "xcomet_score": 0.9837754368782043, "xcomet_qe_score": 0.97617506980896, "metricx_score": 0.17289546132087708, "metricx_qe_score": 0.19612544775009155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将介绍我们关于解决间接指代表达以进行实体选择的研究工作,其中我们引入了“altentity语料库”。", "metrics": {"bleu_score": 22.391459672709843, "chrf_score": 22.92808465312714, "xcomet_score": 0.7517673969268799, "xcomet_qe_score": 0.7005975246429443, "metricx_score": 4.914725303649902, "metricx_qe_score": 5.70736837387085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是贾沃德·侯赛尼,这篇作品是与菲利普·拉丁斯基、西尔维娅·帕雷蒂和安妮·路易斯共同完成的。", "metrics": {"bleu_score": 1.2895624601712163, "chrf_score": 2.396166134185303, "xcomet_score": 0.9818196296691895, "xcomet_qe_score": 0.9585694074630737, "metricx_score": 1.6992700099945068, "metricx_qe_score": 1.4536036252975464, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑以下替代问题:", "metrics": {"bleu_score": 12.067008283523638, "chrf_score": 10.590656015492618, "xcomet_score": 0.8792711496353149, "xcomet_qe_score": 0.8627591133117676, "metricx_score": 0.3784177303314209, "metricx_qe_score": 0.2156527042388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是想选择“easy on me” 还是 “I got a feeling”?", "metrics": {"bleu_score": 6.809398432036521, "chrf_score": 26.91221890985172, "xcomet_score": 0.8105775117874146, "xcomet_qe_score": 0.7612900733947754, "metricx_score": 3.3145768642425537, "metricx_qe_score": 4.732064247131348, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,用户想要在这两个词语中进行选择。", "metrics": {"bleu_score": 15.005212294966274, "chrf_score": 16.43894731653235, "xcomet_score": 0.800500750541687, "xcomet_qe_score": 0.801757276058197, "metricx_score": 6.538749694824219, "metricx_qe_score": 6.235620975494385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用,例如直接说出歌曲的名称《Easy on Me》,或者它所处的序号,即第一首。", "metrics": {"bleu_score": 35.99399671248217, "chrf_score": 42.285465287757525, "xcomet_score": 0.9039236307144165, "xcomet_qe_score": 0.8094412684440613, "metricx_score": 2.0403966903686523, "metricx_qe_score": 1.9341436624526978, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在某些情况下,间接引用可能更合适,以获得更自然的对话。 这可能发生", "metrics": {"bleu_score": 20.748233663492293, "chrf_score": 26.024681723187605, "xcomet_score": 0.7899446487426758, "xcomet_qe_score": 0.8207663297653198, "metricx_score": 6.389524459838867, "metricx_qe_score": 3.4536633491516113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在用户无法回忆起来源名称时。", "metrics": {"bleu_score": 6.81065613056372, "chrf_score": 7.760169402721314, "xcomet_score": 0.7988601922988892, "xcomet_qe_score": 0.7725023031234741, "metricx_score": 4.308349609375, "metricx_qe_score": 4.861652374267578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有发音都过于相似,难以辨析。", "metrics": {"bleu_score": 7.009378515928603, "chrf_score": 9.795404247304234, "xcomet_score": 0.7413401007652283, "xcomet_qe_score": 0.7743919491767883, "metricx_score": 3.0124740600585938, "metricx_qe_score": 0.8212400674819946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或当用户想要指定偏好时。", "metrics": {"bleu_score": 20.54219767156976, "chrf_score": 22.046063039592692, "xcomet_score": 0.9995672702789307, "xcomet_qe_score": 1.0, "metricx_score": 0.7062884569168091, "metricx_qe_score": 0.5011159181594849, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些直接引用中的例子,例如较新的那个或不够活泼的歌曲。 这是一个", "metrics": {"bleu_score": 22.86162133642136, "chrf_score": 19.192819495198492, "xcomet_score": 0.3714546263217926, "xcomet_qe_score": 0.5266066789627075, "metricx_score": 8.750584602355957, "metricx_qe_score": 5.872029781341553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "会话系统中一个重要的问题,同时也是用于评估大型语言模型实体理解能力的重要基准。 我们尚未发现适用于", "metrics": {"bleu_score": 18.4250452521331, "chrf_score": 18.73945852241873, "xcomet_score": 0.4145969748497009, "xcomet_qe_score": 0.23858100175857544, "metricx_score": 8.867855072021484, "metricx_qe_score": 6.247211933135986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该任务的公开数据集,更不用说大规模的公开数据集了,因此我们使用众包标注自行构建了一个数据集。", "metrics": {"bleu_score": 28.362592194380763, "chrf_score": 27.361411127468887, "xcomet_score": 0.5855133533477783, "xcomet_qe_score": 0.524844765663147, "metricx_score": 3.170086622238159, "metricx_qe_score": 3.6166152954101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖音乐、书籍和研究三个不同领域。", "metrics": {"bleu_score": 58.928592593371285, "chrf_score": 49.57714436720817, "xcomet_score": 0.8731351494789124, "xcomet_qe_score": 0.8536407351493835, "metricx_score": 2.2892205715179443, "metricx_qe_score": 2.695335626602173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,采用卡通补全集。", "metrics": {"bleu_score": 53.961318533294246, "chrf_score": 48.53412789411395, "xcomet_score": 0.8063913583755493, "xcomet_qe_score": 0.8036871552467346, "metricx_score": 6.369636535644531, "metricx_qe_score": 5.328728675842285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个卡通有三个对话框。", "metrics": {"bleu_score": 59.00468726392806, "chrf_score": 52.41221741221741, "xcomet_score": 0.849685549736023, "xcomet_qe_score": 0.7364460229873657, "metricx_score": 0.4843158721923828, "metricx_qe_score": 0.6015093326568604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框里,鲍勃说:“还记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 48.64547353242454, "chrf_score": 44.59351295697294, "xcomet_score": 0.9005511999130249, "xcomet_qe_score": 0.8849672079086304, "metricx_score": 1.2276426553726196, "metricx_qe_score": 0.9218540787696838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就这样,鲍勃奠定了对话的背景。", "metrics": {"bleu_score": 21.023693683267553, "chrf_score": 15.814414757850233, "xcomet_score": 0.9784610271453857, "xcomet_qe_score": 0.981819748878479, "metricx_score": 1.7191112041473389, "metricx_qe_score": 1.6163069009780884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话框中,爱丽丝说:“你是说对我好一点,还是说我完成任务了?”", "metrics": {"bleu_score": 15.51422640328163, "chrf_score": 9.722364077917495, "xcomet_score": 0.5989052653312683, "xcomet_qe_score": 0.6249419450759888, "metricx_score": 5.164376258850098, "metricx_qe_score": 5.52212381362915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8669623136520386, "xcomet_qe_score": 0.8554072380065918, "metricx_score": 1.420009732246399, "metricx_qe_score": 2.3006839752197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用间接引用来选择这些实体之一,例如新的射频(RF)。", "metrics": {"bleu_score": 19.30158055353422, "chrf_score": 16.994080546712123, "xcomet_score": 0.619646430015564, "xcomet_qe_score": 0.6451987624168396, "metricx_score": 6.9219255447387695, "metricx_qe_score": 5.501682281494141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话框,但第三个对话框由标注员填写。", "metrics": {"bleu_score": 69.09086323462154, "chrf_score": 62.86260906979017, "xcomet_score": 0.9080743193626404, "xcomet_qe_score": 0.7987481355667114, "metricx_score": 1.342389464378357, "metricx_qe_score": 1.3384054899215698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话框是根据每个领域的一些人工提示选择的。", "metrics": {"bleu_score": 40.025074540692394, "chrf_score": 33.30224028450573, "xcomet_score": 0.8957357406616211, "xcomet_qe_score": 0.7297824025154114, "metricx_score": 1.0466532707214355, "metricx_qe_score": 1.494727611541748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即替代问题,的生成方式如下。", "metrics": {"bleu_score": 12.149176141753365, "chrf_score": 12.963667247248065, "xcomet_score": 0.9097991585731506, "xcomet_qe_score": 0.8954007625579834, "metricx_score": 1.1160832643508911, "metricx_qe_score": 1.2467726469039917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。 ", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您是说A或B?", "metrics": {"bleu_score": 22.772101321113862, "chrf_score": 16.32331419565462, "xcomet_score": 0.9572361707687378, "xcomet_qe_score": 0.949626624584198, "metricx_score": 0.5358198881149292, "metricx_qe_score": 0.9434226751327515, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是维基百科中的示例。", "metrics": {"bleu_score": 34.79159475128446, "chrf_score": 27.753683768642823, "xcomet_score": 0.9639655351638794, "xcomet_qe_score": 0.9933106899261475, "metricx_score": 0.5799583196640015, "metricx_qe_score": 0.31567299365997314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们所使用的不同采样方法。", "metrics": {"bleu_score": 61.28081331864041, "chrf_score": 48.169828540652986, "xcomet_score": 0.9998546838760376, "xcomet_qe_score": 1.0, "metricx_score": 0.14395266771316528, "metricx_qe_score": 0.22877755761146545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向上移动列表时,实体之间的相似度会增加,通常进行消歧就更加困难。", "metrics": {"bleu_score": 14.782064491589646, "chrf_score": 17.696275681120614, "xcomet_score": 0.7496285438537598, "xcomet_qe_score": 0.7090771198272705, "metricx_score": 3.6483066082000732, "metricx_qe_score": 4.2195611000061035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是均匀引力。", "metrics": {"bleu_score": 9.080027618567454, "chrf_score": 8.025515843773027, "xcomet_score": 0.7334867119789124, "xcomet_qe_score": 0.7843021154403687, "metricx_score": 4.393367290496826, "metricx_qe_score": 2.5026257038116455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个情况是,当实体具有相似的标题时,例如,两本书的名称都为“零售”。", "metrics": {"bleu_score": 11.074209397579995, "chrf_score": 15.421865715983362, "xcomet_score": 0.7779674530029297, "xcomet_qe_score": 0.7567949295043945, "metricx_score": 4.554409980773926, "metricx_qe_score": 5.32840633392334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是,当它们在维基百科上有相似的描述。", "metrics": {"bleu_score": 83.18180062062373, "chrf_score": 83.70189528707373, "xcomet_score": 0.9384211301803589, "xcomet_qe_score": 0.9759562015533447, "metricx_score": 0.5667060613632202, "metricx_qe_score": 0.5739404559135437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当它们在维基百科上有相似的信息框或属性时。", "metrics": {"bleu_score": 69.63845241054851, "chrf_score": 64.76296666996701, "xcomet_score": 0.9417118430137634, "xcomet_qe_score": 0.997989296913147, "metricx_score": 0.9182050228118896, "metricx_qe_score": 1.1083542108535767, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,相同的流派或相同的艺术家嗓音。", "metrics": {"bleu_score": 12.846189726767717, "chrf_score": 16.47563641572889, "xcomet_score": 0.7454142570495605, "xcomet_qe_score": 0.7131547927856445, "metricx_score": 3.7140114307403564, "metricx_qe_score": 3.636894464492798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们将这个问题作为替代方案呈现给标注员时,他们知道这些实体的名称,但他们并不一定了解这些实体本身。", "metrics": {"bleu_score": 34.90290607055123, "chrf_score": 34.90675116827705, "xcomet_score": 0.8746061325073242, "xcomet_qe_score": 0.8919715881347656, "metricx_score": 2.2646336555480957, "metricx_qe_score": 1.72331702709198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们所做的是展示关于这两个实体的背景知识。", "metrics": {"bleu_score": 72.00242075875518, "chrf_score": 64.66198984505621, "xcomet_score": 0.9646176099777222, "xcomet_qe_score": 0.7469348907470703, "metricx_score": 0.9762362241744995, "metricx_qe_score": 1.7196680307388306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地为每首歌曲提供一个Google搜索链接。 然后,请标注员聆听至少部分歌曲,并阅读关于每首歌曲的信息。", "metrics": {"bleu_score": 21.731867024528306, "chrf_score": 20.25324929884543, "xcomet_score": 0.8184307813644409, "xcomet_qe_score": 0.8635523319244385, "metricx_score": 2.1907615661621094, "metricx_qe_score": 1.9504170417785645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,以下是关于歌曲《Easy》的谷歌搜索结果。", "metrics": {"bleu_score": 40.276720463657746, "chrf_score": 35.99364023469182, "xcomet_score": 0.8370438814163208, "xcomet_qe_score": 0.8292756676673889, "metricx_score": 3.9344429969787598, "metricx_qe_score": 4.066579818725586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们会展示一些来自维基百科的背景文本。", "metrics": {"bleu_score": 73.04631582700078, "chrf_score": 60.70123363226811, "xcomet_score": 0.9947742223739624, "xcomet_qe_score": 0.9730352163314819, "metricx_score": 0.6527124047279358, "metricx_qe_score": 1.009541630744934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还会再次展示来自维基百科的图片,以便标注人员了解其外观。", "metrics": {"bleu_score": 29.68527973509177, "chrf_score": 25.085491240284092, "xcomet_score": 0.8398841619491577, "xcomet_qe_score": 0.786510169506073, "metricx_score": 2.4178946018218994, "metricx_qe_score": 2.070894241333008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求标注员从这些实体中选择一个,例如这里选择第一个,并使用三到五个间接指代来描述它们。", "metrics": {"bleu_score": 54.41208982967258, "chrf_score": 47.30985990007316, "xcomet_score": 0.7804276347160339, "xcomet_qe_score": 0.8528289794921875, "metricx_score": 2.6875205039978027, "metricx_qe_score": 2.627068519592285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,那个带有钢琴音乐的。", "metrics": {"bleu_score": 16.90062198556585, "chrf_score": 18.4176047252779, "xcomet_score": 0.9872552752494812, "xcomet_qe_score": 0.987647294998169, "metricx_score": 0.4536311626434326, "metricx_qe_score": 0.4618048369884491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些来自我们数据集的例子。", "metrics": {"bleu_score": 41.21183751323024, "chrf_score": 36.44793690849975, "xcomet_score": 0.9751685857772827, "xcomet_qe_score": 0.9732110500335693, "metricx_score": 0.2812017798423767, "metricx_qe_score": 0.32396525144577026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,没有歌词的那个,而不是那个有12岁男孩的,或者虚构的,或者来自亚美尼亚的。", "metrics": {"bleu_score": 29.567346508302286, "chrf_score": 28.469823422639823, "xcomet_score": 0.6410493850708008, "xcomet_qe_score": 0.6388914585113525, "metricx_score": 2.8523471355438232, "metricx_qe_score": 3.2123236656188965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该实体语料库包含来自三个领域共 6,000 个替代问题,以及 42,000 个间接指代表达。", "metrics": {"bleu_score": 25.42524352595399, "chrf_score": 32.31340899960252, "xcomet_score": 0.5740854144096375, "xcomet_qe_score": 0.5166890621185303, "metricx_score": 5.489956855773926, "metricx_qe_score": 5.831236362457275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下总结了使用 T5xLarge 模型的结果。", "metrics": {"bleu_score": 58.63954417655858, "chrf_score": 52.28437597023028, "xcomet_score": 0.963114857673645, "xcomet_qe_score": 0.9435369372367859, "metricx_score": 1.421842098236084, "metricx_qe_score": 1.8321083784103394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问与标注者完全一致的背景知识,那么准确率会非常高。大约在92到95%之间。", "metrics": {"bleu_score": 50.224257063383924, "chrf_score": 41.88952584883027, "xcomet_score": 0.8483273386955261, "xcomet_qe_score": 0.8981095552444458, "metricx_score": 1.0455501079559326, "metricx_qe_score": 0.9020305871963501, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这种情况并不现实。", "metrics": {"bleu_score": 20.556680845025987, "chrf_score": 21.267546355574527, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4691614508628845, "metricx_qe_score": 0.5053394436836243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问部分重叠的背景知识,那么准确率在82%到87%之间,这更为现实", "metrics": {"bleu_score": 59.48156211947489, "chrf_score": 56.20005678434858, "xcomet_score": 0.8954146504402161, "xcomet_qe_score": 0.8925453424453735, "metricx_score": 1.454470157623291, "metricx_qe_score": 1.8890798091888428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9841159582138062, "xcomet_qe_score": 0.9635313749313354, "metricx_score": 0.9222766160964966, "metricx_qe_score": 1.0717954635620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型仅能访问实体名称,那么准确率仅为60%。因此,仍有很大的改进空间。", "metrics": {"bleu_score": 59.422055984211894, "chrf_score": 51.93854630392328, "xcomet_score": 0.9966373443603516, "xcomet_qe_score": 0.9909923076629639, "metricx_score": 1.3309048414230347, "metricx_qe_score": 2.019491672515869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还证明了这些模型具有领域泛化能力。", "metrics": {"bleu_score": 72.52761279126531, "chrf_score": 73.84154328661677, "xcomet_score": 0.9384033679962158, "xcomet_qe_score": 0.9239341616630554, "metricx_score": 0.5843798518180847, "metricx_qe_score": 0.6215623021125793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是萨拉·帕皮,来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍一篇以“注意力机制作为同步语音翻译的指导”为主题的论文,这篇论文是与马特奥·内格里和马可·图尔奇共同完成的工作。", "metrics": {"bleu_score": 31.539391253408887, "chrf_score": 28.083984363485065, "xcomet_score": 0.9135751724243164, "xcomet_qe_score": 0.9123073816299438, "metricx_score": 2.5992674827575684, "metricx_qe_score": 2.1887454986572266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是即时语音翻译?", "metrics": {"bleu_score": 16.784459625186194, "chrf_score": 15.046762428961383, "xcomet_score": 0.9820812940597534, "xcomet_qe_score": 0.9757794141769409, "metricx_score": 0.18114842474460602, "metricx_qe_score": 0.019112005829811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即时语音翻译,或称 simul SD,是指将口语实时转换为另一种语言的文本的过程,从而实现跨语言交流。 那么,", "metrics": {"bleu_score": 44.39723248822124, "chrf_score": 43.75575506005387, "xcomet_score": 0.710968017578125, "xcomet_qe_score": 0.694938600063324, "metricx_score": 6.2583160400390625, "metricx_qe_score": 5.867173671722412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 SimulST 模型存在哪些问题呢?", "metrics": {"bleu_score": 53.16967153331756, "chrf_score": 68.88793130936092, "xcomet_score": 0.9991519451141357, "xcomet_qe_score": 0.9944875240325928, "metricx_score": 0.35536760091781616, "metricx_qe_score": 0.4873923659324646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特定的架构通常是通过引入额外的模块进行优化的。", "metrics": {"bleu_score": 46.57362374510411, "chrf_score": 43.40784164789046, "xcomet_score": 0.9649639129638672, "xcomet_qe_score": 0.9617122411727905, "metricx_score": 1.3423621654510498, "metricx_qe_score": 1.6774992942810059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "冗长且复杂的训练流程,例如涉及不同优化目标的训练。", "metrics": {"bleu_score": 60.09638585283707, "chrf_score": 58.270223632683546, "xcomet_score": 0.9884449243545532, "xcomet_qe_score": 0.9927974939346313, "metricx_score": 0.5347166061401367, "metricx_qe_score": 0.6372373700141907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且训练和维护多个模型以达到不同的延迟级", "metrics": {"bleu_score": 48.8627596980411, "chrf_score": 46.78251421975124, "xcomet_score": 0.8629988431930542, "xcomet_qe_score": 0.8700456023216248, "metricx_score": 1.589690923690796, "metricx_qe_score": 1.2612218856811523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别,例如,训练一个平均延迟为一秒的模型,另一个平均延迟为两秒的模型,以此类推。", "metrics": {"bleu_score": 64.52293659827443, "chrf_score": 62.17855316932416, "xcomet_score": 0.5632507801055908, "xcomet_qe_score": 0.47897419333457947, "metricx_score": 1.8148677349090576, "metricx_qe_score": 2.774533748626709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先利用已有的离线SD模型,无需重新训练或采用特定的CLSD架构。", "metrics": {"bleu_score": 38.804743794179885, "chrf_score": 31.76658589297942, "xcomet_score": 0.7350200414657593, "xcomet_qe_score": 0.7455218434333801, "metricx_score": 5.3305768966674805, "metricx_qe_score": 6.230611801147461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对每个延迟等级仅使用一个模型,并通过特定的参数来处理延迟。", "metrics": {"bleu_score": 54.43583509939386, "chrf_score": 42.31052732553625, "xcomet_score": 0.9990212917327881, "xcomet_qe_score": 1.0, "metricx_score": 0.6638578772544861, "metricx_qe_score": 0.8983943462371826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并利用模型通过音频输入与文本输出之间的注意力机制——交叉注意力机制——已经获得的知识。", "metrics": {"bleu_score": 57.2950573437961, "chrf_score": 63.16948969266759, "xcomet_score": 0.8145214319229126, "xcomet_qe_score": 0.7940276861190796, "metricx_score": 3.540640354156494, "metricx_qe_score": 5.091823101043701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以在右侧看到一个例子。", "metrics": {"bleu_score": 4.896627602978773, "chrf_score": 5.673780105189899, "xcomet_score": 0.9149490594863892, "xcomet_qe_score": 0.8792210817337036, "metricx_score": 1.9196761846542358, "metricx_qe_score": 4.111104965209961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出一个点或编码器-解码器注意力机制,它是一种策略,我们根据注意力指向的位置决定是否发出部分翻译。 一个词", "metrics": {"bleu_score": 51.5888777034185, "chrf_score": 46.40148508421954, "xcomet_score": 0.5348944067955017, "xcomet_qe_score": 0.5166511535644531, "metricx_score": 7.874551296234131, "metricx_qe_score": 8.356637001037598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语会被发出,如果张力没有集中,也就是说,在最后 λ 个语音帧内,其总和低于某个阈值 α,这意味着接收到的信息已经足够稳定。", "metrics": {"bleu_score": 31.647530261811397, "chrf_score": 27.37714132690704, "xcomet_score": 0.5823675394058228, "xcomet_qe_score": 0.3728853166103363, "metricx_score": 6.20900297164917, "metricx_qe_score": 5.581743240356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果接收到包含“我将要谈论”的语音片段,且我们的模型预测其翻译为德语,我们 我们将观察交叉注意力权重。 我们将看到,前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,最后的lambda语音帧。", "metrics": {"bleu_score": 40.388627155285945, "chrf_score": 30.945382433108843, "xcomet_score": 0.6283528208732605, "xcomet_qe_score": 0.5013449192047119, "metricx_score": 6.346707344055176, "metricx_qe_score": 6.36262845993042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将会被省略。 由于交叉张力之和超过了某个阈值 α,我们将不会发出最后一个词,而是等待另一个语音块。", "metrics": {"bleu_score": 44.24203529279914, "chrf_score": 36.480851563722446, "xcomet_score": 0.8073501586914062, "xcomet_qe_score": 0.7569468021392822, "metricx_score": 5.468624591827393, "metricx_qe_score": 5.368994235992432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续,并且接收到另一段沉浸式的演讲,我们的模型预测出另外三个词,我们会查看交叉注意力权重。 我们将看到,没有词语指向最后的 Lambda 语音帧。", "metrics": {"bleu_score": 40.58734349085479, "chrf_score": 35.016550902311835, "xcomet_score": 0.6308240294456482, "xcomet_qe_score": 0.43298497796058655, "metricx_score": 5.127376079559326, "metricx_qe_score": 4.798468589782715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将会被输出。", "metrics": {"bleu_score": 30.79300751569293, "chrf_score": 29.446692174998624, "xcomet_score": 0.9968873262405396, "xcomet_qe_score": 0.9709671139717102, "metricx_score": 0.6358075141906738, "metricx_qe_score": 1.052427053451538, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们观察到该结果的主要体现,我们将看到 我们将把同步语音翻译的结果绘制在图表上,图表的其中一边为蓝色,用于衡量翻译质量和平均延迟。 那是延迟指标。 我们还考虑计算感知平均滞后,该指标考虑了模型预测输出所需的计算时间。", "metrics": {"bleu_score": 31.057946826296075, "chrf_score": 27.751990867942084, "xcomet_score": 0.5745716094970703, "xcomet_qe_score": 0.5379484295845032, "metricx_score": 7.06521463394165, "metricx_qe_score": 6.981712341308594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望在这个图上,曲线能达到尽可能高的位置。", "metrics": {"bleu_score": 24.167528858618486, "chrf_score": 27.160901278034682, "xcomet_score": 0.9811333417892456, "xcomet_qe_score": 0.9490556716918945, "metricx_score": 1.3654948472976685, "metricx_qe_score": 1.6292777061462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但我们也希望它们向左移动。", "metrics": {"bleu_score": 86.17038791239612, "chrf_score": 84.90244110859445, "xcomet_score": 0.9971116781234741, "xcomet_qe_score": 0.9812257289886475, "metricx_score": 0.6350057721138, "metricx_qe_score": 1.0446958541870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与PROPERA策略进行比较,这些策略也适用于离线模型,例如WitKey策略和本地协议。", "metrics": {"bleu_score": 43.7289749970122, "chrf_score": 29.601534873658842, "xcomet_score": 0.7128784656524658, "xcomet_qe_score": 0.6530032157897949, "metricx_score": 6.007281303405762, "metricx_qe_score": 6.211650848388672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将其与专门为同时预翻译定制的最先进架构进行比较。", "metrics": {"bleu_score": 75.39221180326287, "chrf_score": 73.18552026953587, "xcomet_score": 0.9096914529800415, "xcomet_qe_score": 0.8747607469558716, "metricx_score": 1.4816691875457764, "metricx_qe_score": 1.4733643531799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是在德语上应用同时语音翻译策略所得到的结果。", "metrics": {"bleu_score": 30.708987612633805, "chrf_score": 30.74235609052914, "xcomet_score": 0.9084087610244751, "xcomet_qe_score": 0.8982154726982117, "metricx_score": 2.2193422317504883, "metricx_qe_score": 1.2146551609039307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,ADUT 在应用于离线模型的所有策略中表现优于其他策略,因为它们的曲线向左偏移。", "metrics": {"bleu_score": 24.983045090282598, "chrf_score": 27.844679689613795, "xcomet_score": 0.8806892037391663, "xcomet_qe_score": 0.8011704683303833, "metricx_score": 4.015819072723389, "metricx_qe_score": 4.858436584472656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且我们也能看到,如果考虑到实际经过的时间或计算感知的时间,那将是效率最高的策略。", "metrics": {"bleu_score": 13.435637642994447, "chrf_score": 18.640355976166163, "xcomet_score": 0.8781489133834839, "xcomet_qe_score": 0.8817328810691833, "metricx_score": 3.599597692489624, "metricx_qe_score": 3.3556344509124756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如欲探索更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 53.948230957280764, "chrf_score": 47.221343280638045, "xcomet_score": 0.9204120635986328, "xcomet_qe_score": 0.9156307578086853, "metricx_score": 1.6265872716903687, "metricx_qe_score": 1.3661521673202515, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们同时开源了代码、模型以及同步输出结果,以促进我们工作的可重复性。", "metrics": {"bleu_score": 22.78527011419721, "chrf_score": 22.327455190305738, "xcomet_score": 0.9665484428405762, "xcomet_qe_score": 0.9562242031097412, "metricx_score": 0.7329061031341553, "metricx_qe_score": 0.7812207341194153, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好, 我叫英,我和我的同事姜将为大家介绍我们关于通过指令调整提升多模态序列学习的研究。", "metrics": {"bleu_score": 47.19003152361857, "chrf_score": 27.933682597731718, "xcomet_score": 0.5835449695587158, "xcomet_qe_score": 0.5817171335220337, "metricx_score": 5.7274885177612305, "metricx_qe_score": 5.680549621582031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型技术的进步,许多研究开始探索新的学习范式,即以一种参数和数据高效的方式,复用预训练语言模型来执行不同的下游任务。 近", "metrics": {"bleu_score": 57.08152968728463, "chrf_score": 53.461066668224674, "xcomet_score": 0.80213862657547, "xcomet_qe_score": 0.7669610977172852, "metricx_score": 3.271286725997925, "metricx_qe_score": 1.7877264022827148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期,许多研究表明,指令调优能够使大型语言模型在零样本环境下,通过遵循自然指令来执行未见过的任务。", "metrics": {"bleu_score": 37.00656999187018, "chrf_score": 32.51376727121682, "xcomet_score": 0.6541382074356079, "xcomet_qe_score": 0.4793875217437744, "metricx_score": 5.06095027923584, "metricx_qe_score": 6.105223178863525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数先前的指令微调工作侧重于提升语言任务中的序列提示性能,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 41.403933560541255, "chrf_score": 35.441406719949775, "xcomet_score": 0.8806248307228088, "xcomet_qe_score": 0.8302758932113647, "metricx_score": 1.292966604232788, "metricx_qe_score": 1.2222458124160767, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们旨在探讨是否能在多模态预训练模型上进行指令微调,从而实际上提升其对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 34.03208094513809, "chrf_score": 32.44828703203098, "xcomet_score": 0.8978753089904785, "xcomet_qe_score": 0.8339813947677612, "metricx_score": 1.1363790035247803, "metricx_qe_score": 1.2949563264846802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现RLP和多模态数据集中教学数据可用性存在显著差异。 ", "metrics": {"bleu_score": 19.976662803944905, "chrf_score": 19.139965579182398, "xcomet_score": 0.7865397930145264, "xcomet_qe_score": 0.8139780759811401, "metricx_score": 3.191284656524658, "metricx_qe_score": 2.650376796722412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过一千六百种仅使用语言进行的指令任务。", "metrics": {"bleu_score": 26.934666326316563, "chrf_score": 28.214430270130837, "xcomet_score": 0.9797691106796265, "xcomet_qe_score": 0.8975723385810852, "metricx_score": 1.190619707107544, "metricx_qe_score": 1.3729994297027588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,目前缺乏大规模的、公开可用的多模态指令任务。", "metrics": {"bleu_score": 41.740742934661625, "chrf_score": 44.16798010416632, "xcomet_score": 0.9900484085083008, "xcomet_qe_score": 0.8913143873214722, "metricx_score": 1.3405559062957764, "metricx_qe_score": 1.797520637512207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这促使我们构建一个多模态指令微调数据集。", "metrics": {"bleu_score": 56.687387042197564, "chrf_score": 51.58148802464252, "xcomet_score": 0.9755637645721436, "xcomet_qe_score": 0.9603803157806396, "metricx_score": 0.9388285875320435, "metricx_qe_score": 0.8105782270431519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在此介绍Multi Instruct,这是首个多模态指令调优基准数据集,包含62项多样化的多模态任务,涵盖10个广泛的类别。", "metrics": {"bleu_score": 35.45241397652975, "chrf_score": 41.884623686196754, "xcomet_score": 0.8782329559326172, "xcomet_qe_score": 0.8443942070007324, "metricx_score": 2.2010912895202637, "metricx_qe_score": 2.3809452056884766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来源于二十一个现有的开源数据集,并且每个任务都配备了五条专家撰写的指令。", "metrics": {"bleu_score": 43.481201463184036, "chrf_score": 40.62130056513844, "xcomet_score": 0.9612829685211182, "xcomet_qe_score": 0.9518698453903198, "metricx_score": 1.1879441738128662, "metricx_qe_score": 1.9871894121170044, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究我们提出的数据集上的多模态指令微调,我们选择OF A(一种统一的多模态模式模型)作为基础模型。", "metrics": {"bleu_score": 48.3119458113754, "chrf_score": 43.88632675785023, "xcomet_score": 0.8579736948013306, "xcomet_qe_score": 0.7760852575302124, "metricx_score": 3.4718875885009766, "metricx_qe_score": 3.956266164779663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFA 使用统一的词汇表来表示语言、图像令牌以及边界框的坐标。", "metrics": {"bleu_score": 50.67309892897293, "chrf_score": 46.38279567818353, "xcomet_score": 0.9034715890884399, "xcomet_qe_score": 0.7749326229095459, "metricx_score": 4.259635925292969, "metricx_qe_score": 4.443169593811035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们展示来自我们多源状态数据集的一些示例实例。 为了统一处理各种输入和输出数据类型。", "metrics": {"bleu_score": 61.948144314227164, "chrf_score": 48.678043368528144, "xcomet_score": 0.6861944198608398, "xcomet_qe_score": 0.7123373746871948, "metricx_score": 3.0250582695007324, "metricx_qe_score": 3.599045515060425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,并将所有任务都以统一的序列到序列格式进行表述,", "metrics": {"bleu_score": 60.317983955216874, "chrf_score": 62.15236838148748, "xcomet_score": 0.8534466028213501, "xcomet_qe_score": 0.8202911615371704, "metricx_score": 1.7031687498092651, "metricx_qe_score": 2.558986186981201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中输入文本、图像、指令和边界框均在相同的token空间中表示。", "metrics": {"bleu_score": 70.27310222981032, "chrf_score": 63.30827463832694, "xcomet_score": 0.8729859590530396, "xcomet_qe_score": 0.8024183511734009, "metricx_score": 3.054304361343384, "metricx_qe_score": 3.1082189083099365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我将要介绍多模态指令微调。 因此", "metrics": {"bleu_score": 28.129148710958383, "chrf_score": 27.12156394116263, "xcomet_score": 0.7274917364120483, "xcomet_qe_score": 0.7010550498962402, "metricx_score": 3.522916555404663, "metricx_qe_score": 0.919182300567627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于训练数据集,我们使用 NIG 组的 53 个任务进行训练,并且每个任务抽取 10,000 个样本。", "metrics": {"bleu_score": 55.97233714786894, "chrf_score": 51.73886426471467, "xcomet_score": 0.713627815246582, "xcomet_qe_score": 0.6747249364852905, "metricx_score": 8.048698425292969, "metricx_qe_score": 9.267660140991211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识推理组进行测试,并从 WQA 和其他杂项组中额外选择五个任务。", "metrics": {"bleu_score": 45.67732083693753, "chrf_score": 38.28006610337311, "xcomet_score": 0.6287767887115479, "xcomet_qe_score": 0.644686222076416, "metricx_score": 5.036057472229004, "metricx_qe_score": 5.148181438446045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用测试集中的所有样本来执行每个任务。", "metrics": {"bleu_score": 49.42244290934039, "chrf_score": 38.54383954881783, "xcomet_score": 0.8221395015716553, "xcomet_qe_score": 0.8030681610107422, "metricx_score": 2.9448163509368896, "metricx_qe_score": 2.6954314708709717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试集中随机抽取二十个任务,方法与NLP中的Syntax类似。 因此,", "metrics": {"bleu_score": 38.70160063400903, "chrf_score": 34.020369374451505, "xcomet_score": 0.568780779838562, "xcomet_qe_score": 0.5237821340560913, "metricx_score": 6.5364532470703125, "metricx_qe_score": 5.981100559234619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预趋势化的OFA大型模型作为基础模型。", "metrics": {"bleu_score": 76.1827408333416, "chrf_score": 75.19921173231113, "xcomet_score": 0.9003766775131226, "xcomet_qe_score": 0.8897400498390198, "metricx_score": 4.738582611083984, "metricx_qe_score": 4.86138391494751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有样本进行混合。", "metrics": {"bleu_score": 49.033606604040415, "chrf_score": 44.287110420108775, "xcomet_score": 0.8239691257476807, "xcomet_qe_score": 0.836607038974762, "metricx_score": 2.1640169620513916, "metricx_qe_score": 3.0007314682006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个样本会随机地与它其中的五个指令模板之一组合。", "metrics": {"bleu_score": 31.622032580117445, "chrf_score": 32.32106548864442, "xcomet_score": 0.8059240579605103, "xcomet_qe_score": 0.7302236557006836, "metricx_score": 2.76104474067688, "metricx_qe_score": 2.2542967796325684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在针对每个任务的测试过程中,我们进行总共五个实验,通过在每个实验中采用五个指令中的", "metrics": {"bleu_score": 41.04841157713402, "chrf_score": 35.854546875498855, "xcomet_score": 0.5938456058502197, "xcomet_qe_score": 0.6084455251693726, "metricx_score": 8.139734268188477, "metricx_qe_score": 3.978188991546631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个来评估模型。 我们报告了在所有五个实验中,平均值和最大值表现,以及表现的标准差。", "metrics": {"bleu_score": 35.220448955798034, "chrf_score": 33.661119199129594, "xcomet_score": 0.35892796516418457, "xcomet_qe_score": 0.19316083192825317, "metricx_score": 5.620558261871338, "metricx_qe_score": 6.26364803314209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,则报告准确率。", "metrics": {"bleu_score": 49.444506108522596, "chrf_score": 39.93318518050496, "xcomet_score": 0.9246195554733276, "xcomet_qe_score": 0.9804967641830444, "metricx_score": 0.6705411672592163, "metricx_qe_score": 0.7192279696464539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,则报告 RougeL。 对于 RP 任务,我们也报告 RougeL。", "metrics": {"bleu_score": 50.40239471312796, "chrf_score": 54.93654623125074, "xcomet_score": 0.7889856696128845, "xcomet_qe_score": 0.7380979061126709, "metricx_score": 4.351274013519287, "metricx_qe_score": 4.606888771057129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,即灵敏度。", "metrics": {"bleu_score": 66.93985979198287, "chrf_score": 64.65462125269626, "xcomet_score": 0.9924495220184326, "xcomet_qe_score": 0.90692138671875, "metricx_score": 0.42936402559280396, "metricx_qe_score": 0.6777236461639404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该指标衡量模型在面对指令中细微措辞变化时,始终能产生相同输出的能力。", "metrics": {"bleu_score": 34.88842635370545, "chrf_score": 30.478776728967638, "xcomet_score": 0.9238266944885254, "xcomet_qe_score": 0.972484827041626, "metricx_score": 1.8625946044921875, "metricx_qe_score": 2.547163248062134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,指令微调可以显著提升 OFE 在处理多模态任务时的表现。", "metrics": {"bleu_score": 41.70390706370443, "chrf_score": 31.90634286664931, "xcomet_score": 0.9409003257751465, "xcomet_qe_score": 0.9452080130577087, "metricx_score": 3.9689626693725586, "metricx_qe_score": 3.69305157661438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集迁移学习也能促进指令微调。", "metrics": {"bleu_score": 40.16631108685379, "chrf_score": 34.919991349409386, "xcomet_score": 0.9225383996963501, "xcomet_qe_score": 0.8194853067398071, "metricx_score": 1.1939036846160889, "metricx_qe_score": 2.117694616317749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们可以观察到,随着任务量的增加,模型实现了更好的性能,同时降低了敏感度。 因此,我们", "metrics": {"bleu_score": 20.502138892315305, "chrf_score": 21.111163906904615, "xcomet_score": 0.700491726398468, "xcomet_qe_score": 0.6808532476425171, "metricx_score": 6.533637046813965, "metricx_qe_score": 3.820392370223999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也进行了一个实验,", "metrics": {"bleu_score": 39.93879176377876, "chrf_score": 30.267463400199983, "xcomet_score": 0.8946015238761902, "xcomet_qe_score": 0.8796612024307251, "metricx_score": 0.7347985506057739, "metricx_qe_score": 0.7583186626434326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了单一指令与五条指令进行对比。", "metrics": {"bleu_score": 31.32768114661938, "chrf_score": 31.213508790012913, "xcomet_score": 0.8937933444976807, "xcomet_qe_score": 0.8319070935249329, "metricx_score": 0.9561474919319153, "metricx_qe_score": 1.7757105827331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,使用更多的指令可以提高模型的整体性能,并且显著降低其敏感性。", "metrics": {"bleu_score": 54.26080349401577, "chrf_score": 48.97912964683168, "xcomet_score": 0.946952760219574, "xcomet_qe_score": 1.0, "metricx_score": 0.9749736785888672, "metricx_qe_score": 1.2102279663085938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明了不同微调策略对模型敏感性产生的影响。", "metrics": {"bleu_score": 44.74935415497116, "chrf_score": 38.14241638379245, "xcomet_score": 0.9919760227203369, "xcomet_qe_score": 0.9808038473129272, "metricx_score": 1.4224797487258911, "metricx_qe_score": 1.34283447265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们通过从自然指令数据集进行迁移学习所看到的,该模型可以实现比原始 IFA 模型更高的敏感性。", "metrics": {"bleu_score": 52.97665585538789, "chrf_score": 45.58154263060205, "xcomet_score": 0.9346276521682739, "xcomet_qe_score": 0.8626694083213806, "metricx_score": 3.3100814819335938, "metricx_qe_score": 4.08942174911499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以观察到,利用自然指令数据集进行迁移学习,能够帮助 OFA 在自然指令数据集上取得显著更好的性能。", "metrics": {"bleu_score": 48.79590639764949, "chrf_score": 47.684531399953066, "xcomet_score": 0.95824134349823, "xcomet_qe_score": 0.9035616517066956, "metricx_score": 2.635258913040161, "metricx_qe_score": 3.4687507152557373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们提出了第一个大规模的多模态指令微调数据集,这显著提升了OFA的衍生能力。我们探索了不同的迁移学习技术,并通过一种名为“灵敏", "metrics": {"bleu_score": 43.625384465193186, "chrf_score": 40.345772864505584, "xcomet_score": 0.6473166942596436, "xcomet_qe_score": 0.5497413873672485, "metricx_score": 7.93544864654541, "metricx_qe_score": 6.270810604095459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "度”的新指标展示了它们的优势。", "metrics": {"bleu_score": 6.5229617062090215, "chrf_score": 9.746651032228264, "xcomet_score": 0.17104840278625488, "xcomet_qe_score": 0.15674853324890137, "metricx_score": 9.964248657226562, "metricx_qe_score": 10.852006912231445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以还有一点,我们正在收集一个更大规模的多模态指令微调数据集,其中包含大约150个额外的变体语言任务,并且我们会发布它们。", "metrics": {"bleu_score": 43.1881646704385, "chrf_score": 42.62344519393713, "xcomet_score": 0.6528668999671936, "xcomet_qe_score": 0.6504687070846558, "metricx_score": 2.626091718673706, "metricx_qe_score": 2.73687744140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集和模型的二维码。", "metrics": {"bleu_score": 68.92146754069327, "chrf_score": 58.3011433011433, "xcomet_score": 0.9841136932373047, "xcomet_qe_score": 0.9192091822624207, "metricx_score": 0.4786149561405182, "metricx_qe_score": 0.5699979066848755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Sena海岸,很高兴欢迎各位参加我们ACL 2023论文的报告,该论文题目是", "metrics": {"bleu_score": 32.241318603049926, "chrf_score": 32.991821493876486, "xcomet_score": 0.48259830474853516, "xcomet_qe_score": 0.4756790101528168, "metricx_score": 8.496886253356934, "metricx_qe_score": 9.686869621276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型可接受性判断并非总是对上下文稳健》。 这是一项与", "metrics": {"bleu_score": 46.09056322258578, "chrf_score": 38.56428084618117, "xcomet_score": 0.6178247928619385, "xcomet_qe_score": 0.6078462600708008, "metricx_score": 9.738777160644531, "metricx_qe_score": 6.693907260894775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "约翰·博克尔 (John Bokier)、艾伦·穆勒 (Aaron Muller)、卡尼什卡·米希拉 (Kanishka Mishra)、卡伦·富恩特斯 (Karen Fuentes)、罗杰·里维 (Roger Levy) 和阿迪娜·威廉 (Adina William) 共同完成的工作。", "metrics": {"bleu_score": 1.6142246011331012, "chrf_score": 42.998366681179, "xcomet_score": 0.3335959017276764, "xcomet_qe_score": 0.386089950799942, "metricx_score": 5.796998977661133, "metricx_qe_score": 5.270195484161377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们重新审视了极小对模型。", "metrics": {"bleu_score": 40.1577332834242, "chrf_score": 34.38574991400069, "xcomet_score": 0.9076732993125916, "xcomet_qe_score": 0.9012276530265808, "metricx_score": 1.0819779634475708, "metricx_qe_score": 1.7558722496032715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最简单的配对范式基本上是根据可接受性判断来评估语言模型,这", "metrics": {"bleu_score": 35.258590512600755, "chrf_score": 30.97878989114869, "xcomet_score": 0.7399157285690308, "xcomet_qe_score": 0.6945592164993286, "metricx_score": 5.958413124084473, "metricx_qe_score": 1.36391019821167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可包括语法性,例如“blimp”、“syntax gem”之类的例子,或从刻板印象的角度考量可接受性,比如Krauss配对。", "metrics": {"bleu_score": 17.83203578883933, "chrf_score": 19.45504980196164, "xcomet_score": 0.6711438894271851, "xcomet_qe_score": 0.6072710752487183, "metricx_score": 5.320899963378906, "metricx_qe_score": 5.019725799560547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这样的最小词对范式中,评估语言模型通常的做法是呈现一个可接受的句子或语法正确的句子,然后呈现一个不可接受的句子或语法错误的句子。", "metrics": {"bleu_score": 37.062370082651356, "chrf_score": 32.198300263388454, "xcomet_score": 0.7871288061141968, "xcomet_qe_score": 0.8159118294715881, "metricx_score": 1.8099710941314697, "metricx_qe_score": 2.6936869621276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,希望模型能够基本地将更大的概率赋予可接受的句子。", "metrics": {"bleu_score": 33.39087646492814, "chrf_score": 30.003385021470468, "xcomet_score": 0.9042501449584961, "xcomet_qe_score": 0.7076690196990967, "metricx_score": 2.063411235809326, "metricx_qe_score": 2.5931761264801025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前的MPP流程基本不允许我们评估模型对更长句子的接受程度。", "metrics": {"bleu_score": 57.527716473422515, "chrf_score": 51.488351367661714, "xcomet_score": 0.9611210823059082, "xcomet_qe_score": 0.9319193363189697, "metricx_score": 1.3234566450119019, "metricx_qe_score": 1.9696837663650513, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型正不断扩展其上下文窗口的长度。", "metrics": {"bleu_score": 28.984970517277347, "chrf_score": 28.13118303563312, "xcomet_score": 0.9935204982757568, "xcomet_qe_score": 0.9708783626556396, "metricx_score": 0.7297958135604858, "metricx_qe_score": 0.7559860944747925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,至关重要的是,我们需要在整个上下文窗口内评估模型的可用性。 而我们正在试图在这里做的事情,就是重新审视 N", "metrics": {"bleu_score": 36.902775166820575, "chrf_score": 39.31042970125368, "xcomet_score": 0.7666712999343872, "xcomet_qe_score": 0.6853154897689819, "metricx_score": 4.477078437805176, "metricx_qe_score": 3.7185070514678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PP 流程,通过让模型评估越来越长的序列的可接受性。", "metrics": {"bleu_score": 46.90623217259833, "chrf_score": 47.656869842591846, "xcomet_score": 0.33103907108306885, "xcomet_qe_score": 0.3407116234302521, "metricx_score": 5.65861701965332, "metricx_qe_score": 6.455070972442627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。我们所做", "metrics": {"bleu_score": 64.79121525090147, "chrf_score": 88.59854884450613, "xcomet_score": 0.8280736207962036, "xcomet_qe_score": 0.8130654692649841, "metricx_score": 2.34740948677063, "metricx_qe_score": 1.4932039976119995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的是,为了模拟这些更长的序列,我们会重新审视数据集本身,然后从这些数据集中选择可接受或不可接受的句子来重建句子。", "metrics": {"bleu_score": 76.48493597266223, "chrf_score": 70.86186140946015, "xcomet_score": 0.5883748531341553, "xcomet_qe_score": 0.5054517388343811, "metricx_score": 2.5719268321990967, "metricx_qe_score": 3.707929849624634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,例如,这里我们选择了一个典型的符合语法规则的例子,来自飞艇数据集,关于附例岛的情况。", "metrics": {"bleu_score": 12.678627977432635, "chrf_score": 13.408230057933388, "xcomet_score": 0.5308452248573303, "xcomet_qe_score": 0.5364243388175964, "metricx_score": 7.3631367683410645, "metricx_qe_score": 6.392760276794434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重建更长的序列,这些序列是可接受的,且具有相同的语法结构匹配,为此,", "metrics": {"bleu_score": 66.47291074562574, "chrf_score": 57.9935436079448, "xcomet_score": 0.7395577430725098, "xcomet_qe_score": 0.5489243865013123, "metricx_score": 4.159526348114014, "metricx_qe_score": 4.825575351715088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从 adjunctile 中提取语法句子。 然后我们将它作为前缀添加到可接受的查询和不可接受的查询中。 因此,我们", "metrics": {"bleu_score": 65.55214854356659, "chrf_score": 61.41108812964031, "xcomet_score": 0.6309173107147217, "xcomet_qe_score": 0.5056146383285522, "metricx_score": 7.964948654174805, "metricx_qe_score": 7.018084526062012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以通过选择相同的匹配项中的不可接受的句子来实现相同的结果,这同样可以用来测试模型的接受度。", "metrics": {"bleu_score": 49.44631795703343, "chrf_score": 42.28158949804425, "xcomet_score": 0.9104675650596619, "xcomet_qe_score": 0.897792398929596, "metricx_score": 1.9050253629684448, "metricx_qe_score": 2.1796512603759766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过选择来自不同子集或不同数据集的句子来实现这一点。", "metrics": {"bleu_score": 39.18247592373092, "chrf_score": 33.07254242141976, "xcomet_score": 0.9794285297393799, "xcomet_qe_score": 0.8466383218765259, "metricx_score": 0.7785696387290955, "metricx_qe_score": 1.3083312511444092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这便是我们所说的“不匹配”场景。", "metrics": {"bleu_score": 46.85909905380382, "chrf_score": 43.67141181765963, "xcomet_score": 0.9872597455978394, "xcomet_qe_score": 0.8723431825637817, "metricx_score": 0.6083781719207764, "metricx_qe_score": 1.095328688621521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里句子仍然来自相关的语料库,但并非您用于评估的那个语料库。", "metrics": {"bleu_score": 26.584641847224905, "chrf_score": 23.76315424395263, "xcomet_score": 0.9087151288986206, "xcomet_qe_score": 0.9268373847007751, "metricx_score": 1.5972061157226562, "metricx_qe_score": 1.4447641372680664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对不可接受情况做同样的处理。", "metrics": {"bleu_score": 30.02611603802972, "chrf_score": 26.076327006103295, "xcomet_score": 0.9860838651657104, "xcomet_qe_score": 0.903684675693512, "metricx_score": 0.7038419246673584, "metricx_qe_score": 0.8016462922096252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全无关的领域选择句子,例如维基百科。", "metrics": {"bleu_score": 50.736434665841514, "chrf_score": 40.518660294114476, "xcomet_score": 0.9890741109848022, "xcomet_qe_score": 0.9287714958190918, "metricx_score": 0.7345715165138245, "metricx_qe_score": 1.5617462396621704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将会告诉我们,模型的接受度判断是否真的受到任何语境的影响。 像是上下文是否来自数据集的不同子集,或者它是否完全与我们正在分析的句子无关。", "metrics": {"bleu_score": 44.05986017870426, "chrf_score": 38.13154321671301, "xcomet_score": 0.9053833484649658, "xcomet_qe_score": 0.8557158708572388, "metricx_score": 2.21394681930542, "metricx_qe_score": 2.693042039871216, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型表现如何呢?", "metrics": {"bleu_score": 9.29675796576443, "chrf_score": 9.644582470669427, "xcomet_score": 0.8517210483551025, "xcomet_qe_score": 0.8354972004890442, "metricx_score": 1.0570439100265503, "metricx_qe_score": 0.2590245008468628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们考察与当前查询对完全无关的维基百科句子,发现MPP判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 42.10512768445621, "chrf_score": 38.55612215268325, "xcomet_score": 0.9115075469017029, "xcomet_qe_score": 0.7987518310546875, "metricx_score": 4.401146411895752, "metricx_qe_score": 6.073830604553223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们增加了上下文长度,最高可达 2024,以充分利用 OPT 和 GPT-2 模型。在此,我们", "metrics": {"bleu_score": 18.50658997831234, "chrf_score": 34.55489831951953, "xcomet_score": 0.4414653778076172, "xcomet_qe_score": 0.41428694128990173, "metricx_score": 9.94344425201416, "metricx_qe_score": 6.826242923736572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从橙色虚线中可以看到 MPP 判断相对稳定。", "metrics": {"bleu_score": 73.0771733343074, "chrf_score": 64.5566018805183, "xcomet_score": 0.9199644327163696, "xcomet_qe_score": 0.8371756672859192, "metricx_score": 1.766474962234497, "metricx_qe_score": 3.416264772415161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,当我们在同一个数据集里选择句子时,会发生什么呢?", "metrics": {"bleu_score": 37.011751896357886, "chrf_score": 31.578885776110205, "xcomet_score": 0.9935535192489624, "xcomet_qe_score": 0.9656791687011719, "metricx_score": 0.7422853708267212, "metricx_qe_score": 1.2587147951126099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们现在正从同一 BLIMP 或 SYNTAX GIMP 数据集中选择或构建句子,这些句子来自可接受和不可接受的领域。 在那里,我们可以", "metrics": {"bleu_score": 28.64670591593496, "chrf_score": 28.402074022112643, "xcomet_score": 0.4056020677089691, "xcomet_qe_score": 0.3990969657897949, "metricx_score": 7.124770641326904, "metricx_qe_score": 6.364526271820068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "看到,当添加可接受的前缀或不可接受的前缀时,MPP 评判值会显著增加或减少。", "metrics": {"bleu_score": 60.804453888426934, "chrf_score": 57.59319357116336, "xcomet_score": 0.8443094491958618, "xcomet_qe_score": 0.8186888098716736, "metricx_score": 2.5492496490478516, "metricx_qe_score": 2.3557963371276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,也就是当我们从相同现象中选取句子,依据句法进行责备,吉姆。 我们观察到,模型在 MPP 判断上会呈现巨大的增长或巨大的下降,这取决于所选的前缀是否可以接受或不可接受。", "metrics": {"bleu_score": 28.176683241059592, "chrf_score": 24.634837659542274, "xcomet_score": 0.48843613266944885, "xcomet_qe_score": 0.4672316312789917, "metricx_score": 8.418036460876465, "metricx_qe_score": 9.05449104309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在这一点和这一点非常大,就像这种效应随着上下文长度的增加而增强,这可能会影响到那些具有大上下文窗口的新型语言模型。", "metrics": {"bleu_score": 37.67740189390609, "chrf_score": 43.96773421912839, "xcomet_score": 0.5903897285461426, "xcomet_qe_score": 0.6379089951515198, "metricx_score": 4.89483118057251, "metricx_qe_score": 4.248832702636719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么匹配前缀会对语言模型的判断产生如此大的影响呢? 因此,", "metrics": {"bleu_score": 79.31509362620145, "chrf_score": 84.66746211027359, "xcomet_score": 0.8314436674118042, "xcomet_qe_score": 0.7689838409423828, "metricx_score": 3.8555376529693604, "metricx_qe_score": 3.6396803855895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保留相关结构并引入噪声来扰动输入句子。", "metrics": {"bleu_score": 52.46340103602162, "chrf_score": 48.09947554624679, "xcomet_score": 0.9222447276115417, "xcomet_qe_score": 0.7970767617225647, "metricx_score": 1.217132568359375, "metricx_qe_score": 1.9110430479049683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行多次此类扰动之后, 我们发现这些噪音实际上并未导致模型改变其展示 MPP 判断趋势的轨迹。", "metrics": {"bleu_score": 31.50162660192704, "chrf_score": 28.245803263192315, "xcomet_score": 0.9163634777069092, "xcomet_qe_score": 0.875657856464386, "metricx_score": 3.4090757369995117, "metricx_qe_score": 4.283661842346191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动后的句子表现出相似的敏感性。", "metrics": {"bleu_score": 28.660292502688566, "chrf_score": 25.776076759728323, "xcomet_score": 0.9331146478652954, "xcomet_qe_score": 0.9613513946533203, "metricx_score": 1.4636993408203125, "metricx_qe_score": 2.389460563659668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在可接受的范围内扰动句子时,我们观察到所有扰动中都出现了类似的增加;而当我们在不可接受的范围内扰动句子时,我们也以类似的方式观察到 MPP 判断值的下降。", "metrics": {"bleu_score": 42.7140599999416, "chrf_score": 36.866859095770415, "xcomet_score": 0.9010244607925415, "xcomet_qe_score": 0.6831867694854736, "metricx_score": 3.50423002243042, "metricx_qe_score": 5.139354228973389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的工作的主要结论是,语言模型对潜在的句法和语义特征具有敏感性,这些特征在句子间共享。", "metrics": {"bleu_score": 36.156487789459035, "chrf_score": 33.45778436122444, "xcomet_score": 0.981723427772522, "xcomet_qe_score": 0.9345717430114746, "metricx_score": 1.252172827720642, "metricx_qe_score": 1.5261733531951904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而目前我们采用的MPP评估方式,即使用简短、单句输入,可能无法完全捕捉到语言模型在上下文窗口中所蕴含的抽象知识。", "metrics": {"bleu_score": 40.88886819072394, "chrf_score": 35.97359867894861, "xcomet_score": 0.8643565773963928, "xcomet_qe_score": 0.8581957817077637, "metricx_score": 1.619476556777954, "metricx_qe_score": 1.811000108718872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解更多实验细节。", "metrics": {"bleu_score": 34.27163657253172, "chrf_score": 33.44303636597666, "xcomet_score": 0.9978342056274414, "xcomet_qe_score": 0.9995092153549194, "metricx_score": 0.10219424962997437, "metricx_qe_score": 0.11421225965023041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫张 Yusin,来自宾州州立大学。", "metrics": {"bleu_score": 22.01502782215444, "chrf_score": 19.233925522897884, "xcomet_score": 0.7811444997787476, "xcomet_qe_score": 0.8105196356773376, "metricx_score": 2.055546522140503, "metricx_qe_score": 2.52664852142334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将为大家介绍我们的工作,即多语言自然语言以及语义表示的跨语言语义解析。 因此", "metrics": {"bleu_score": 39.361542943856115, "chrf_score": 30.34502902378526, "xcomet_score": 0.6075047254562378, "xcomet_qe_score": 0.7008767127990723, "metricx_score": 5.343410015106201, "metricx_qe_score": 3.6740801334381104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语义解析的任务是构建用户查询的语义表示,例如SQL和Lambda演算。", "metrics": {"bleu_score": 60.275234875721274, "chrf_score": 56.10579233250673, "xcomet_score": 0.9627799987792969, "xcomet_qe_score": 0.9368633031845093, "metricx_score": 1.904327630996704, "metricx_qe_score": 2.146864891052246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且跨语言语义解析的任务是将多种自然语言中的查询翻译成多种含义表示形式。", "metrics": {"bleu_score": 58.7206349740059, "chrf_score": 57.00233689091911, "xcomet_score": 0.9132440090179443, "xcomet_qe_score": 0.9193179607391357, "metricx_score": 1.6175895929336548, "metricx_qe_score": 3.6113440990448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将查询翻译成多种自然语言的 SQL、Lambda、FunQL 等等。", "metrics": {"bleu_score": 56.87171086774712, "chrf_score": 59.173899992436205, "xcomet_score": 0.8964968919754028, "xcomet_qe_score": 0.8237547874450684, "metricx_score": 2.192014217376709, "metricx_qe_score": 3.414764881134033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型通常是独立提出的,并在包含有限任务和应用的特定数据集上进行评估,例如。", "metrics": {"bleu_score": 43.338090180011044, "chrf_score": 46.666944218972425, "xcomet_score": 0.7833476066589355, "xcomet_qe_score": 0.7689406275749207, "metricx_score": 2.8711211681365967, "metricx_qe_score": 2.1019749641418457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于某些自然语言的讨论很多。 中文内容", "metrics": {"bleu_score": 34.82352832757854, "chrf_score": 33.35965365470721, "xcomet_score": 0.3576439619064331, "xcomet_qe_score": 0.5876994132995605, "metricx_score": 5.570305347442627, "metricx_qe_score": 4.949005603790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "缺失。 湖泊在特定微观表现形式上的覆盖范围。 λ", "metrics": {"bleu_score": 4.34373789135831, "chrf_score": 4.581529581529581, "xcomet_score": 0.14205889403820038, "xcomet_qe_score": 0.13942719995975494, "metricx_score": 17.00641441345215, "metricx_qe_score": 16.466365814208984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "演算缺失。 或者,它们仅在某些较新的模型上进行评估。", "metrics": {"bleu_score": 40.842567407749925, "chrf_score": 32.10868759889551, "xcomet_score": 0.561147928237915, "xcomet_qe_score": 0.5960045456886292, "metricx_score": 4.499338150024414, "metricx_qe_score": 5.298794269561768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一的模型用于评估它们。", "metrics": {"bleu_score": 65.03386691979853, "chrf_score": 58.40205277099098, "xcomet_score": 0.9980639219284058, "xcomet_qe_score": 0.9874151945114136, "metricx_score": 0.45229649543762207, "metricx_qe_score": 0.6829416751861572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出exempler,", "metrics": {"bleu_score": 31.76215203205584, "chrf_score": 17.551892551892546, "xcomet_score": 0.8461283445358276, "xcomet_qe_score": 0.8227345943450928, "metricx_score": 3.0663857460021973, "metricx_qe_score": 4.167974948883057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供一个统一的数据集exempler,用于多语言及多种语义表示形式的跨语言语义解析。", "metrics": {"bleu_score": 48.490470436278386, "chrf_score": 37.34248925539068, "xcomet_score": 0.861025333404541, "xcomet_qe_score": 0.8347085118293762, "metricx_score": 4.041896343231201, "metricx_qe_score": 4.8683576583862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含九个来自不同领域的语料集,五个语义部件和税目,八种语义表示方式,以及分布于15个语系中的22种自然语言。", "metrics": {"bleu_score": 31.794829599676433, "chrf_score": 36.59596242212196, "xcomet_score": 0.6713311672210693, "xcomet_qe_score": 0.7165147066116333, "metricx_score": 6.033479690551758, "metricx_qe_score": 6.453039646148682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了训练和评估的六种设置。", "metrics": {"bleu_score": 67.8301759715223, "chrf_score": 59.56205276123286, "xcomet_score": 0.9824798107147217, "xcomet_qe_score": 0.9433248043060303, "metricx_score": 1.0820890665054321, "metricx_qe_score": 2.2754859924316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是 TranslateTest。", "metrics": {"bleu_score": 32.58798048281462, "chrf_score": 18.76628675854382, "xcomet_score": 0.9681284427642822, "xcomet_qe_score": 0.9778255224227905, "metricx_score": 1.2122944593429565, "metricx_qe_score": 1.0589913129806519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用谷歌翻译API将源语言翻译成目标语言,然后使用MonolingoModel进行评估训练。", "metrics": {"bleu_score": 64.67293620849517, "chrf_score": 59.12288393347105, "xcomet_score": 0.8662705421447754, "xcomet_qe_score": 0.9027738571166992, "metricx_score": 3.378958225250244, "metricx_qe_score": 2.3389129638671875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们使用英文语料训练一个英文模型。在推理阶段,我们使用API将德语查询翻译成英文,然后使用训练好的模型来预测SQL。", "metrics": {"bleu_score": 59.3278243433039, "chrf_score": 56.602033761350675, "xcomet_score": 0.8497567772865295, "xcomet_qe_score": 0.8409808874130249, "metricx_score": 0.9019801616668701, "metricx_qe_score": 1.4008536338806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模块。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 76.96368446368447, "xcomet_score": 0.8608987927436829, "xcomet_qe_score": 0.837360680103302, "metricx_score": 0.2960849404335022, "metricx_qe_score": 0.4536767601966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 73.6558994084271, "chrf_score": 67.66383822511001, "xcomet_score": 0.9070941209793091, "xcomet_qe_score": 0.8909211158752441, "metricx_score": 0.6414645910263062, "metricx_qe_score": 0.6743262410163879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语融合设置,通过仅使用 10% 的训练数据来训练单语模型。 我们测试一个多语言模型,我们用一个", "metrics": {"bleu_score": 33.94197081727515, "chrf_score": 43.68779165759621, "xcomet_score": 0.4367123246192932, "xcomet_qe_score": 0.38980820775032043, "metricx_score": 10.970725059509277, "metricx_qe_score": 7.377233505249023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多语言模型来训练所有语言。", "metrics": {"bleu_score": 21.383268443561093, "chrf_score": 22.521062310210095, "xcomet_score": 0.7575097680091858, "xcomet_qe_score": 0.6854696273803711, "metricx_score": 2.3771004676818848, "metricx_qe_score": 3.2832698822021484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将德语、英语和中文查询组合起来,训练一个多语种模型。并且", "metrics": {"bleu_score": 50.38599113918596, "chrf_score": 41.74718004385415, "xcomet_score": 0.7996323108673096, "xcomet_qe_score": 0.7948341965675354, "metricx_score": 3.905336856842041, "metricx_qe_score": 1.7654927968978882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理时,我们可以利用这个模型来。 翻译德语查询或中文查询或等等。", "metrics": {"bleu_score": 59.035838588615405, "chrf_score": 51.670574257757096, "xcomet_score": 0.8790319561958313, "xcomet_qe_score": 0.6644376516342163, "metricx_score": 2.379915237426758, "metricx_qe_score": 3.757058620452881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们同时也考虑跨语言零样本和领域样本迁移,", "metrics": {"bleu_score": 35.72605933966096, "chrf_score": 33.73704568043054, "xcomet_score": 0.7326163053512573, "xcomet_qe_score": 0.6995214819908142, "metricx_score": 4.480888366699219, "metricx_qe_score": 5.353646278381348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在一种源语言上运行并迁移到另一种语言。", "metrics": {"bleu_score": 25.295952012171607, "chrf_score": 21.924238158465037, "xcomet_score": 0.7905315160751343, "xcomet_qe_score": 0.7165335416793823, "metricx_score": 3.170891284942627, "metricx_qe_score": 3.6209824085235596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我将使用英文查询或英文与德语融合查询来训练模型,以构建一个多语言模型,并预测 SQL 输出。", "metrics": {"bleu_score": 37.95874791718197, "chrf_score": 35.955312210352076, "xcomet_score": 0.7436004877090454, "xcomet_qe_score": 0.7344777584075928, "metricx_score": 1.654766321182251, "metricx_qe_score": 1.9239170551300049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们同时也发现许多有趣的实验结果。", "metrics": {"bleu_score": 28.63424630911014, "chrf_score": 30.231287516572518, "xcomet_score": 0.9880293607711792, "xcomet_qe_score": 0.9749140739440918, "metricx_score": 0.6407279968261719, "metricx_qe_score": 0.555001974105835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在单语模型分析方面,我们对两组模型进行了评估。 包括编码器PDR,即多语言预训练编码器与基于指针的解码器,例如XLMR plus PDR和BERT plus PDR。", "metrics": {"bleu_score": 34.580412577564495, "chrf_score": 29.072302370834848, "xcomet_score": 0.6135875582695007, "xcomet_qe_score": 0.6272900104522705, "metricx_score": 4.666321754455566, "metricx_qe_score": 4.199389457702637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练的编码器-解码器模型,例如 MBART 和 MT5。", "metrics": {"bleu_score": 32.179689521567305, "chrf_score": 23.785149558340525, "xcomet_score": 0.9175122976303101, "xcomet_qe_score": 0.961768388748169, "metricx_score": 1.4467357397079468, "metricx_qe_score": 3.093170166015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器-解码器模型在所有九个数据集上均表现出最佳性能。", "metrics": {"bleu_score": 36.151920269854195, "chrf_score": 27.05664322408767, "xcomet_score": 0.9969875812530518, "xcomet_qe_score": 0.9958285093307495, "metricx_score": 0.6762094497680664, "metricx_qe_score": 0.5932331681251526, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 MT-5 和 XLMR-PDR 在多语言设置下进行评估。", "metrics": {"bleu_score": 33.464494273746425, "chrf_score": 29.537649488316443, "xcomet_score": 0.9331468343734741, "xcomet_qe_score": 0.9220877885818481, "metricx_score": 2.886028528213501, "metricx_qe_score": 2.8856661319732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合语料上进行训练,可以改进编码器-解码器或编码器预测模型。 并且", "metrics": {"bleu_score": 20.010122685751483, "chrf_score": 15.566119472362693, "xcomet_score": 0.5944269895553589, "xcomet_qe_score": 0.6134569644927979, "metricx_score": 4.054687023162842, "metricx_qe_score": 2.8010549545288086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,这是因为大多数主要自然语言都能获得性能提升,除了英语在七个数据集上表现下降,仅在三个数据集上获得提升。", "metrics": {"bleu_score": 51.93423859103488, "chrf_score": 45.05214452612054, "xcomet_score": 0.9428671598434448, "xcomet_qe_score": 0.9545177221298218, "metricx_score": 1.623658299446106, "metricx_qe_score": 2.177239179611206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我認為這被稱為多語言能力帶來的負面影響。", "metrics": {"bleu_score": 2.862999365766888, "chrf_score": 4.166666666666666, "xcomet_score": 0.9877533912658691, "xcomet_qe_score": 0.9929231405258179, "metricx_score": 3.8952269554138184, "metricx_qe_score": 3.327514171600342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较跨语言表现的差距。", "metrics": {"bleu_score": 44.55641825569225, "chrf_score": 37.51408928832615, "xcomet_score": 0.8671962022781372, "xcomet_qe_score": 0.8461002707481384, "metricx_score": 0.5316271185874939, "metricx_qe_score": 0.9510401487350464, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在该图中,蓝线代表跨语言燃料注入式迁移,", "metrics": {"bleu_score": 12.109013026441868, "chrf_score": 14.863538999982692, "xcomet_score": 0.7512884736061096, "xcomet_qe_score": 0.7618569731712341, "metricx_score": 6.1385369300842285, "metricx_qe_score": 5.428462505340576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线代表跨语言零样本迁移,", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 59.505479514708156, "xcomet_score": 0.9503284692764282, "xcomet_qe_score": 0.8348692059516907, "metricx_score": 1.7698858976364136, "metricx_qe_score": 2.8702516555786133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿线代表单语设置。", "metrics": {"bleu_score": 45.180100180492246, "chrf_score": 42.94810660563017, "xcomet_score": 0.9968347549438477, "xcomet_qe_score": 0.9922187328338623, "metricx_score": 0.3827013373374939, "metricx_qe_score": 0.5412886738777161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对比绿色和橙色线条,我们发现对于零短样本设置,跨语言迁移性能差距显著。而通过对比蓝色和橙色线条,我们发现对于少量短样本设置,迁移差距迅速缩短。", "metrics": {"bleu_score": 28.650039867036618, "chrf_score": 25.40661045770144, "xcomet_score": 0.6283565759658813, "xcomet_qe_score": 0.6552140116691589, "metricx_score": 4.262730121612549, "metricx_qe_score": 3.9503955841064453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他的有趣发现。", "metrics": {"bleu_score": 21.36435031981171, "chrf_score": 26.441146119336086, "xcomet_score": 0.8741728067398071, "xcomet_qe_score": 0.8447062969207764, "metricx_score": 0.38246479630470276, "metricx_qe_score": 0.860139787197113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器模型优于以往的工作,或取得了可比的结果。", "metrics": {"bleu_score": 13.461801293778908, "chrf_score": 10.269620069702334, "xcomet_score": 0.9714746475219727, "xcomet_qe_score": 0.9631924629211426, "metricx_score": 1.8702057600021362, "metricx_qe_score": 2.0375139713287354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行预训练,可以显著提升模型在目标自然语言上的性能。 并且我们发现,诸如Codice和Bloom等多种语言模型在跨语言语义解析任务中仍然不足以胜任。", "metrics": {"bleu_score": 50.453988246024615, "chrf_score": 43.0265991935783, "xcomet_score": 0.8507607579231262, "xcomet_qe_score": 0.8028005361557007, "metricx_score": 4.307319641113281, "metricx_qe_score": 4.988502502441406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "综上所述,我们构建了 Exempler,这是一个统一的跨角度语义解析基准,支持多种自然语言和微观表示。", "metrics": {"bleu_score": 36.23725714592687, "chrf_score": 26.430597602698047, "xcomet_score": 0.668383777141571, "xcomet_qe_score": 0.6056010723114014, "metricx_score": 5.2003068923950195, "metricx_qe_score": 5.260831832885742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语言模型进行了全面的基准测试", "metrics": {"bleu_score": 64.28908241246019, "chrf_score": 54.76025729504258, "xcomet_score": 0.9802572727203369, "xcomet_qe_score": 0.9779975414276123, "metricx_score": 1.1519235372543335, "metricx_qe_score": 2.2911219596862793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",研究结果显示了许多有趣的发现等", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.8396278619766235, "xcomet_qe_score": 0.7771899700164795, "metricx_score": 3.988969564437866, "metricx_qe_score": 2.8476905822753906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "等。", "metrics": {"bleu_score": 0.0, "chrf_score": 8.333333333333332, "xcomet_score": 0.604798436164856, "xcomet_qe_score": 0.2309737205505371, "metricx_score": 1.9000164270401, "metricx_qe_score": 3.2553701400756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的倾听。", "metrics": {"bleu_score": 18.575057999133602, "chrf_score": 23.910939012584706, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43388721346855164, "metricx_qe_score": 0.9160492420196533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好, 我叫艾德·维拉尔,我将简要介绍这篇论文《促进幻灯片翻译:策略评估与性能评估》。", "metrics": {"bleu_score": 27.344795801301544, "chrf_score": 21.51356610415552, "xcomet_score": 0.6751654148101807, "xcomet_qe_score": 0.6890936493873596, "metricx_score": 6.243547439575195, "metricx_qe_score": 5.899998188018799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和谷歌翻译的同事们共同完成的工作。", "metrics": {"bleu_score": 22.500095738124404, "chrf_score": 20.79136993125249, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.0483155250549316, "metricx_qe_score": 0.6093882918357849, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "帕姆是一个拥有5400亿参数的语言模型,于2022年发布。", "metrics": {"bleu_score": 55.15803125708828, "chrf_score": 53.01289264468421, "xcomet_score": 0.8815525770187378, "xcomet_qe_score": 0.7719780206680298, "metricx_score": 4.329079627990723, "metricx_qe_score": 4.335844993591309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是在一个包含7800亿token的大型标签集合上训练的。", "metrics": {"bleu_score": 38.67308630094445, "chrf_score": 39.68976353802219, "xcomet_score": 0.7085978984832764, "xcomet_qe_score": 0.6458580493927002, "metricx_score": 4.9532670974731445, "metricx_qe_score": 4.825381755828857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "出版时,它在数百个自然语言处理任务中达到了最先进水平。", "metrics": {"bleu_score": 19.98573974138024, "chrf_score": 17.771396229730456, "xcomet_score": 0.9783945083618164, "xcomet_qe_score": 0.9914902448654175, "metricx_score": 1.480069875717163, "metricx_qe_score": 1.2980009317398071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们呈现了针对机器翻译的锁格语言模型提示的首次系统性研究。", "metrics": {"bleu_score": 39.92039761549464, "chrf_score": 35.624385841791565, "xcomet_score": 0.7772887349128723, "xcomet_qe_score": 0.804502010345459, "metricx_score": 4.80319356918335, "metricx_qe_score": 4.408944606781006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用AMT社群的最佳实践来评估此类模型的转换能力。", "metrics": {"bleu_score": 57.85707243690118, "chrf_score": 49.58672502676431, "xcomet_score": 0.8846514821052551, "xcomet_qe_score": 0.8604208827018738, "metricx_score": 2.3458054065704346, "metricx_qe_score": 3.694246768951416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较两个最先进的系统,即WMT评估中表现最佳的系统。", "metrics": {"bleu_score": 39.991535813285104, "chrf_score": 37.903467820364476, "xcomet_score": 0.8038564920425415, "xcomet_qe_score": 0.854465663433075, "metricx_score": 2.915189743041992, "metricx_qe_score": 3.9574763774871826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用最先进的LMT指标,并同时展示基于专家评估的人工评估结果。", "metrics": {"bleu_score": 43.778105702468366, "chrf_score": 35.99307923105198, "xcomet_score": 0.8940098285675049, "xcomet_qe_score": 0.8825448155403137, "metricx_score": 2.116302490234375, "metricx_qe_score": 2.2177376747131348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供一些提示语选择策略的建议。", "metrics": {"bleu_score": 50.44388997129684, "chrf_score": 43.386216413309924, "xcomet_score": 0.8990589380264282, "xcomet_qe_score": 0.8779864311218262, "metricx_score": 1.294884204864502, "metricx_qe_score": 2.117339611053467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对机器翻译大型语言模型的性能具有显著影响,正如我们在一个简单实验中所观察到的,在该实验中,我们使用一个简短的提示,并仅为一句话提供了两个不同的提示。", "metrics": {"bleu_score": 28.31776273865021, "chrf_score": 29.93926044523341, "xcomet_score": 0.7823787927627563, "xcomet_qe_score": 0.7440253496170044, "metricx_score": 2.949652671813965, "metricx_qe_score": 2.445590019226074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子,一千句中五", "metrics": {"bleu_score": 8.29519350710986, "chrf_score": 5.780509812767877, "xcomet_score": 0.6283969879150391, "xcomet_qe_score": 0.6728750467300415, "metricx_score": 7.470702648162842, "metricx_qe_score": 6.25587797164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "百一十六句,观察到的差异大于一个模糊点。 这", "metrics": {"bleu_score": 2.568331954752977, "chrf_score": 3.1746031746031744, "xcomet_score": 0.14819203317165375, "xcomet_qe_score": 0.1993294507265091, "metricx_score": 14.891141891479492, "metricx_qe_score": 8.183967590332031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下甚至可能高达40个模糊点。", "metrics": {"bleu_score": 53.989956849868726, "chrf_score": 38.620724149355226, "xcomet_score": 0.8174960613250732, "xcomet_qe_score": 0.8271531462669373, "metricx_score": 4.559394359588623, "metricx_qe_score": 2.595606803894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择合适的提示策略至关重要。", "metrics": {"bleu_score": 34.05204944353419, "chrf_score": 28.20548732313438, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.1681901067495346, "metricx_qe_score": 0.26603934168815613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了五次提示(five-shot prompting)策略,即我们仅在提供的每个句子中标注其所使用的语言。", "metrics": {"bleu_score": 32.755742986297435, "chrf_score": 26.346836223374925, "xcomet_score": 0.867501974105835, "xcomet_qe_score": 0.9014555811882019, "metricx_score": 3.727652072906494, "metricx_qe_score": 2.649852991104126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们进行从德语到英语的翻译,德语句子,即源句子,用德语冒号标记,而英语译文则用英语冒号标记。", "metrics": {"bleu_score": 23.62363215546239, "chrf_score": 19.922784003932023, "xcomet_score": 0.9769710302352905, "xcomet_qe_score": 0.9809290170669556, "metricx_score": 1.4148005247116089, "metricx_qe_score": 1.356723666191101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,对于若干短促的提示语而言,提示语的具体形式没有产生显著影响。", "metrics": {"bleu_score": 9.434386306084008, "chrf_score": 12.88110771301911, "xcomet_score": 0.8983769416809082, "xcomet_qe_score": 0.9037456512451172, "metricx_score": 0.6349375247955322, "metricx_qe_score": 0.5187938213348389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零样本和单样本提示至关重要,但当我们,如我们的案", "metrics": {"bleu_score": 48.9453596218689, "chrf_score": 46.25602327457501, "xcomet_score": 0.3648674190044403, "xcomet_qe_score": 0.20863743126392365, "metricx_score": 6.892468452453613, "metricx_qe_score": 5.736871719360352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例所示,转向五样本提示时,提示的实际形式几乎没有差", "metrics": {"bleu_score": 7.668702374725506, "chrf_score": 8.59732484959202, "xcomet_score": 0.4174204468727112, "xcomet_qe_score": 0.4711413085460663, "metricx_score": 6.055756568908691, "metricx_qe_score": 7.533249378204346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "异。 是例子具有最重要的说服力。", "metrics": {"bleu_score": 3.9297193407553004, "chrf_score": 5.868544600938968, "xcomet_score": 0.1949249804019928, "xcomet_qe_score": 0.20298565924167633, "metricx_score": 9.635469436645508, "metricx_qe_score": 8.670105934143066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果总结如下:示例质量比与源句的相似度更为重要。", "metrics": {"bleu_score": 44.838113640596355, "chrf_score": 39.22791951598008, "xcomet_score": 0.9922252893447876, "xcomet_qe_score": 0.982490062713623, "metricx_score": 0.7011653184890747, "metricx_qe_score": 0.5485018491744995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择来自高质量译例的素材至关重要。", "metrics": {"bleu_score": 16.09338477464615, "chrf_score": 17.948343079922026, "xcomet_score": 0.9360116720199585, "xcomet_qe_score": 0.9415886998176575, "metricx_score": 1.5100526809692383, "metricx_qe_score": 1.633208155632019, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别地,我们比较来自 WMT 评估训练数据或开发数据的选择提示。", "metrics": {"bleu_score": 28.64886477499616, "chrf_score": 28.26706759317951, "xcomet_score": 0.6617203950881958, "xcomet_qe_score": 0.614022433757782, "metricx_score": 2.6178271770477295, "metricx_qe_score": 3.12703275680542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "深度数据的整理和质量都远优于训练数据,可以说,结果显示使用深度", "metrics": {"bleu_score": 10.727295782787309, "chrf_score": 11.068731641314253, "xcomet_score": 0.3189970850944519, "xcomet_qe_score": 0.3763478696346283, "metricx_score": 9.115900039672852, "metricx_qe_score": 6.588725566864014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据时表现更好。", "metrics": {"bleu_score": 25.436713811085983, "chrf_score": 26.65599250878488, "xcomet_score": 0.3409266769886017, "xcomet_qe_score": 0.4716026484966278, "metricx_score": 5.18820858001709, "metricx_qe_score": 6.093769073486328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专业化的最先进系统在很大程度上优于 PALM 翻", "metrics": {"bleu_score": 26.572018715367026, "chrf_score": 23.402260910758223, "xcomet_score": 0.799284815788269, "xcomet_qe_score": 0.7998032569885254, "metricx_score": 6.395843982696533, "metricx_qe_score": 4.202439308166504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "译,但 PALM 已经相当接近商业系统。", "metrics": {"bleu_score": 45.466972369917116, "chrf_score": 39.84331980655511, "xcomet_score": 0.5717600584030151, "xcomet_qe_score": 0.2969062924385071, "metricx_score": 4.055716037750244, "metricx_qe_score": 4.276567459106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择叠加了 Google 翻译。", "metrics": {"bleu_score": 38.812342060224985, "chrf_score": 31.62926405810716, "xcomet_score": 0.8263866901397705, "xcomet_qe_score": 0.8189932107925415, "metricx_score": 3.399693489074707, "metricx_qe_score": 2.5072102546691895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们利用 MQM 框架进行的以人为本的创新所获得的洞见是,PALM 的流畅度与最先进系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 37.96705173638223, "chrf_score": 35.27567840225876, "xcomet_score": 0.8676421642303467, "xcomet_qe_score": 0.8463349342346191, "metricx_score": 3.574930191040039, "metricx_qe_score": 4.404121398925781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其值得注意的是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 53.62721716524189, "chrf_score": 65.04198377534362, "xcomet_score": 0.8695536851882935, "xcomet_qe_score": 0.8918887972831726, "metricx_score": 1.4574763774871826, "metricx_qe_score": 0.7620753645896912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,似乎 Palm 有时会选择通过省略源句中的部分内容来生成听起来更流畅的翻译。", "metrics": {"bleu_score": 22.283173512592214, "chrf_score": 22.1653839171636, "xcomet_score": 0.8558573126792908, "xcomet_qe_score": 0.8479050397872925, "metricx_score": 3.5730502605438232, "metricx_qe_score": 3.4124841690063477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,对于PAN而言,其外显风格类别得分低于最先进系统,这又是一个附加信号。 那部分生成的输出相当流畅,但准确性仍然存在一些问题。", "metrics": {"bleu_score": 24.478642737403252, "chrf_score": 21.143009088897546, "xcomet_score": 0.5713751316070557, "xcomet_qe_score": 0.5413916110992432, "metricx_score": 6.637281894683838, "metricx_qe_score": 7.061578750610352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是本次简要概览的全部内容。", "metrics": {"bleu_score": 54.237828377183035, "chrf_score": 45.4890588678844, "xcomet_score": 0.993743896484375, "xcomet_qe_score": 0.9901571273803711, "metricx_score": 0.28890296816825867, "metricx_qe_score": 0.3565852642059326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如需了解更多细节,请参加论文的完整演讲。", "metrics": {"bleu_score": 40.8411508986252, "chrf_score": 34.82814151315599, "xcomet_score": 0.799888014793396, "xcomet_qe_score": 0.8322707414627075, "metricx_score": 2.6819679737091064, "metricx_qe_score": 2.3404159545898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是达威,德国萨兰特大学的博士生。", "metrics": {"bleu_score": 34.56408135219324, "chrf_score": 26.3343440177677, "xcomet_score": 0.8902378082275391, "xcomet_qe_score": 0.9286748170852661, "metricx_score": 0.5680962800979614, "metricx_qe_score": 0.6029895544052124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在今天的视频中,我想介绍我们最近的一项工作——《比你想象中更脆弱》,对每周固定安排的学习方式进行批判性分析。 这篇", "metrics": {"bleu_score": 23.21426323983906, "chrf_score": 24.25526102160714, "xcomet_score": 0.6920831203460693, "xcomet_qe_score": 0.5989160537719727, "metricx_score": 6.97932767868042, "metricx_qe_score": 4.7037739753723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是与萧于雪、马里奥斯·穆斯巴赫、加斯·斯蒂芬和狄特里希·克拉克", "metrics": {"bleu_score": 1.7577459356554812, "chrf_score": 1.6835016835016834, "xcomet_score": 0.42001378536224365, "xcomet_qe_score": 0.5048990249633789, "metricx_score": 7.460576057434082, "metricx_qe_score": 6.173409461975098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "夫共同完成的。 我希望首先对周监督和每周监督学习做一个简要介绍。", "metrics": {"bleu_score": 20.56239500286621, "chrf_score": 26.166646867435368, "xcomet_score": 0.2529633641242981, "xcomet_qe_score": 0.13603302836418152, "metricx_score": 10.0360107421875, "metricx_qe_score": 11.611809730529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,我们不进行手动标注数据。", "metrics": {"bleu_score": 23.185078121230152, "chrf_score": 23.807153424803595, "xcomet_score": 0.8777272701263428, "xcomet_qe_score": 0.8692445158958435, "metricx_score": 2.454350709915161, "metricx_qe_score": 2.76658296585083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注来源对数据进行标注,例如简单的启发式规则、知识库或基于位置代码的资源获取,如图右侧所示。", "metrics": {"bleu_score": 41.57669611842276, "chrf_score": 42.29620862784675, "xcomet_score": 0.7351734638214111, "xcomet_qe_score": 0.6596486568450928, "metricx_score": 2.613297462463379, "metricx_qe_score": 3.7880568504333496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,较弱的标注成本要低得多,但同时也存在噪声,这意味着其中一部分标注是错误的。", "metrics": {"bleu_score": 27.200676703196386, "chrf_score": 24.978705191807332, "xcomet_score": 0.8502895832061768, "xcomet_qe_score": 0.8718844652175903, "metricx_score": 1.765138030052185, "metricx_qe_score": 1.7058131694793701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在每周的标注数据上直接训练神经网络,神经网络往往会记住标注噪声,而无法泛化。", "metrics": {"bleu_score": 29.480203034682628, "chrf_score": 25.365515213990953, "xcomet_score": 0.775543212890625, "xcomet_qe_score": 0.744909405708313, "metricx_score": 4.997367858886719, "metricx_qe_score": 5.509293556213379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周的监督学习中,会提出训练算法,以在这样的噪声水平下稳健地训练神经网络,从而确保训练后的模型仍能良好泛化。", "metrics": {"bleu_score": 41.54885417082642, "chrf_score": 35.81166763507785, "xcomet_score": 0.7503318786621094, "xcomet_qe_score": 0.7519729137420654, "metricx_score": 3.697087526321411, "metricx_qe_score": 4.826857089996338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近期的WSL(每周监督学习)相关研究中,一个常见的说法是,人们声称他们只利用每周的标注数据进行模型训练,并在干净的测试集上获得了高性能。", "metrics": {"bleu_score": 23.157216550062817, "chrf_score": 23.487095008931746, "xcomet_score": 0.6811438798904419, "xcomet_qe_score": 0.6464338302612305, "metricx_score": 6.305936813354492, "metricx_qe_score": 6.015110015869141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并非错误,但存在一个限制。 人们常常假设存在一个额外的干净验证集,用于模型选择。", "metrics": {"bleu_score": 43.39576576394954, "chrf_score": 36.99169176075479, "xcomet_score": 0.9187334775924683, "xcomet_qe_score": 0.8939661383628845, "metricx_score": 2.6332476139068604, "metricx_qe_score": 3.5499982833862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这个问题设置表示质疑,因为它暗示需要每周进行额外的手动标注。", "metrics": {"bleu_score": 26.891796636953412, "chrf_score": 22.058112291625775, "xcomet_score": 0.6781734228134155, "xcomet_qe_score": 0.5670243501663208, "metricx_score": 5.579780578613281, "metricx_qe_score": 6.1783766746521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如同房间里的大象,这种必要性往往被忽视。", "metrics": {"bleu_score": 42.84322350977226, "chrf_score": 34.45780240304402, "xcomet_score": 0.9181896448135376, "xcomet_qe_score": 0.8087109327316284, "metricx_score": 1.338538408279419, "metricx_qe_score": 2.9105823040008545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑虑引出我们三个研究问题。", "metrics": {"bleu_score": 47.13945310979296, "chrf_score": 40.391349087742114, "xcomet_score": 0.8774155378341675, "xcomet_qe_score": 0.9335490465164185, "metricx_score": 1.1587517261505127, "metricx_qe_score": 1.0253015756607056, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,干净的验证数据对 WSL 来说是必要的吗?或者,我们是否可以或许使用一个带有噪声的验证集来代替?", "metrics": {"bleu_score": 23.76309164264202, "chrf_score": 29.30139733855353, "xcomet_score": 0.9402070045471191, "xcomet_qe_score": 0.9208900928497314, "metricx_score": 1.4235472679138184, "metricx_qe_score": 2.6571104526519775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净数据,或者干净数据是 WSL 正常工作的前提,那么我们需要多少干净样本?", "metrics": {"bleu_score": 33.846059545497496, "chrf_score": 30.870199889267063, "xcomet_score": 0.9639558792114258, "xcomet_qe_score": 0.8802645206451416, "metricx_score": 0.7599205374717712, "metricx_qe_score": 1.1443980932235718, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该仅仅使用干净样本进行验证,还是有更好的利用它们的方法?", "metrics": {"bleu_score": 51.081656706032774, "chrf_score": 40.36123067669557, "xcomet_score": 0.9896378517150879, "xcomet_qe_score": 0.9205151200294495, "metricx_score": 0.682039201259613, "metricx_qe_score": 0.981819212436676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在研究中探讨了这些研究问题,研究结果如下。", "metrics": {"bleu_score": 30.178417044391143, "chrf_score": 26.096546742511762, "xcomet_score": 0.9744800329208374, "xcomet_qe_score": 0.9733963012695312, "metricx_score": 0.5148899555206299, "metricx_qe_score": 0.46184009313583374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现有趣的是,最近的 WSL 方法实际上需要干净的白带样本才能正常工作。", "metrics": {"bleu_score": 55.69260656808981, "chrf_score": 49.886984944954335, "xcomet_score": 0.794836163520813, "xcomet_qe_score": 0.7955095767974854, "metricx_score": 3.161536455154419, "metricx_qe_score": 3.0850367546081543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,如图所示,性能会大幅下降。", "metrics": {"bleu_score": 48.415247130345996, "chrf_score": 60.8168604828872, "xcomet_score": 0.992935299873352, "xcomet_qe_score": 0.9890308380126953, "metricx_score": 0.5556566715240479, "metricx_qe_score": 0.6677849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果缺乏干净的验证样本,则训练的模型无法泛化到原始的弱标签之外。 这意味着培训毫无意义。", "metrics": {"bleu_score": 33.041829187971, "chrf_score": 28.572776304910636, "xcomet_score": 0.917804479598999, "xcomet_qe_score": 0.7933219075202942, "metricx_score": 2.176476240158081, "metricx_qe_score": 3.6323320865631104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明 WSL 方法实际上需要干净标注的数据才能正常工作,获取干净验证样本的标注成本也不应被忽视。", "metrics": {"bleu_score": 59.385954796860766, "chrf_score": 54.82652186237368, "xcomet_score": 0.7700392603874207, "xcomet_qe_score": 0.7855535745620728, "metricx_score": 2.8508431911468506, "metricx_qe_score": 3.982341766357422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加清洁验证样本的数量有助于WSL方法实现更好的性能,如图左侧所示。", "metrics": {"bleu_score": 56.40957005860393, "chrf_score": 51.52691694452896, "xcomet_score": 0.9068052768707275, "xcomet_qe_score": 0.9682185649871826, "metricx_score": 3.421088457107544, "metricx_qe_score": 4.104053974151611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们每类只需要二十个样本就能达到高水平的性能。", "metrics": {"bleu_score": 11.37168193487524, "chrf_score": 12.816818428417628, "xcomet_score": 0.9399712085723877, "xcomet_qe_score": 0.9767013192176819, "metricx_score": 1.1199663877487183, "metricx_qe_score": 1.2067208290100098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但故事并非就此结束,因为无论我们选择哪种方式获取干净样本,直接在此基础上进行训练甚至能获得更好的效果。 红", "metrics": {"bleu_score": 20.93479658159551, "chrf_score": 18.60374013145773, "xcomet_score": 0.9117441773414612, "xcomet_qe_score": 0.8352490663528442, "metricx_score": 3.285141706466675, "metricx_qe_score": 1.5262449979782104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "色图表显示了微调方法与WSL方法的性能差异,微调方法直接应用于干净数据,而WSL方法仅使用干净数据进行验证。", "metrics": {"bleu_score": 49.64207158324125, "chrf_score": 49.28721768010196, "xcomet_score": 0.6820975542068481, "xcomet_qe_score": 0.7043009996414185, "metricx_score": 3.4203734397888184, "metricx_qe_score": 3.7445733547210693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每个类别有十个样本,直接微调开始优于WSL方法。 最终,之前 WSL 方法中", "metrics": {"bleu_score": 33.28691483759456, "chrf_score": 29.966090693541858, "xcomet_score": 0.6439831852912903, "xcomet_qe_score": 0.5832707285881042, "metricx_score": 9.186687469482422, "metricx_qe_score": 5.604349136352539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "声称的性能提升可以通过允许在干净的验证样本上继续微调来实现。", "metrics": {"bleu_score": 25.511836139746812, "chrf_score": 23.371693772389783, "xcomet_score": 0.8045579791069031, "xcomet_qe_score": 0.7067091464996338, "metricx_score": 4.684268951416016, "metricx_qe_score": 5.471005916595459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,最初,名为FTW的Marlina模型在性能上低于更为复杂的WSL方法,例如余弦方法。", "metrics": {"bleu_score": 20.056680014857733, "chrf_score": 22.164376076968512, "xcomet_score": 0.7768166661262512, "xcomet_qe_score": 0.7030156850814819, "metricx_score": 6.794442653656006, "metricx_qe_score": 7.140720367431641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果允许在干净样本上继续微调,那么 FTW 的表现与其他方法一样出色。", "metrics": {"bleu_score": 38.905312928332904, "chrf_score": 33.603094341353355, "xcomet_score": 0.9505659341812134, "xcomet_qe_score": 0.8744068741798401, "metricx_score": 1.6996796131134033, "metricx_qe_score": 2.285550594329834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实际应用中,没有理由选择更复杂的 WSL 方法,这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 50.94962971977623, "chrf_score": 52.33949880896825, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8932322859764099, "metricx_qe_score": 1.131442666053772, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们论证了近期的WSL方法需要干净、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 49.52537002011174, "chrf_score": 48.27909684151748, "xcomet_score": 0.8909457325935364, "xcomet_qe_score": 0.8524700999259949, "metricx_score": 3.1659998893737793, "metricx_qe_score": 4.237037658691406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,说明模型选择是否使用干净的验证样本进行。", "metrics": {"bleu_score": 41.261520349079454, "chrf_score": 34.13318925332655, "xcomet_score": 0.8653942346572876, "xcomet_qe_score": 0.8506710529327393, "metricx_score": 1.1382781267166138, "metricx_qe_score": 1.7358192205429077, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,应将WSL方法与未来的着陆基线进行比较,因为两者都基于网格采样。", "metrics": {"bleu_score": 36.22885385046556, "chrf_score": 33.31403013128645, "xcomet_score": 0.6710771918296814, "xcomet_qe_score": 0.7003164291381836, "metricx_score": 6.028047561645508, "metricx_qe_score": 6.315399169921875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一种简单但强大的基线,未来在WSL领域的工作应考虑这一点。", "metrics": {"bleu_score": 31.102772986341073, "chrf_score": 29.711057770791644, "xcomet_score": 0.885764479637146, "xcomet_qe_score": 0.7676467299461365, "metricx_score": 1.7775840759277344, "metricx_qe_score": 2.4497628211975098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们已经开源了我们的代码。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9967988729476929, "xcomet_qe_score": 0.9351927638053894, "metricx_score": 0.4914062023162842, "metricx_qe_score": 0.7212974429130554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.53011982655684, "chrf_score": 51.34315219921988, "xcomet_score": 0.9966487884521484, "xcomet_qe_score": 0.9828085899353027, "metricx_score": 0.3895472288131714, "metricx_qe_score": 0.34398770332336426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎查阅。", "metrics": {"bleu_score": 17.030578356760866, "chrf_score": 15.694310511089679, "xcomet_score": 0.9795382022857666, "xcomet_qe_score": 0.9593701958656311, "metricx_score": 0.31718626618385315, "metricx_qe_score": 0.34247255325317383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,并期待在会议上与您相见。", "metrics": {"bleu_score": 6.250381527944883, "chrf_score": 6.3192328792706975, "xcomet_score": 0.5254392623901367, "xcomet_qe_score": 0.9784862995147705, "metricx_score": 2.3757565021514893, "metricx_qe_score": 1.4626816511154175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9995607137680054, "xcomet_qe_score": 1.0, "metricx_score": 0.32271575927734375, "metricx_qe_score": 0.18299813568592072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天,我们将向您详细介绍 ABCEval,这是一种评估对话式人工智能的新颖维度方法。", "metrics": {"bleu_score": 26.475180908878603, "chrf_score": 29.09055303461196, "xcomet_score": 0.8747653961181641, "xcomet_qe_score": 0.9804580211639404, "metricx_score": 1.2347766160964966, "metricx_qe_score": 1.039715051651001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学自然语言处理实验室完成,该实验室由埃默里大学的吉诺·蔡教授领导,并与亚马逊Alexa人工智能部门合作完成。 那么", "metrics": {"bleu_score": 20.1871756140228, "chrf_score": 22.409411347588584, "xcomet_score": 0.7152811884880066, "xcomet_qe_score": 0.7787419557571411, "metricx_score": 4.967864990234375, "metricx_qe_score": 1.7300080060958862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",假设您刚刚开发了一个对话模型,并且希望评估它与当前最先进水平的比较情况。", "metrics": {"bleu_score": 45.11892184088149, "chrf_score": 40.934771118586994, "xcomet_score": 0.9057159423828125, "xcomet_qe_score": 0.8933496475219727, "metricx_score": 1.7750173807144165, "metricx_qe_score": 1.6857266426086426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如请人工评估员选择两个对话中哪个更好,或者根据液态标度对对话进行评分。", "metrics": {"bleu_score": 52.624000740003275, "chrf_score": 46.84023183306782, "xcomet_score": 0.8433802127838135, "xcomet_qe_score": 0.8535043001174927, "metricx_score": 5.207967281341553, "metricx_qe_score": 6.037426471710205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的综合评估方面表现良好,但对话质量涉及诸多方面。", "metrics": {"bleu_score": 35.614122529968405, "chrf_score": 30.572128965727945, "xcomet_score": 0.9991294145584106, "xcomet_qe_score": 0.9943411350250244, "metricx_score": 0.25175783038139343, "metricx_qe_score": 0.4724479913711548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能需要评估聊天质量的多个维度,以便在更细粒度层面上了解模型的优势和劣势。", "metrics": {"bleu_score": 50.42050721427572, "chrf_score": 51.72817752034662, "xcomet_score": 0.8905842304229736, "xcomet_qe_score": 0.87368243932724, "metricx_score": 1.3163604736328125, "metricx_qe_score": 1.178654432296753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是直接请人工评估者评估对话质量的多个维度,例如模型回复的相关性,使用现有的比较或李克特量表方法。", "metrics": {"bleu_score": 56.31797969776835, "chrf_score": 49.90178935019321, "xcomet_score": 0.951495885848999, "xcomet_qe_score": 0.8895050287246704, "metricx_score": 1.3282313346862793, "metricx_qe_score": 1.704623818397522, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更为精确且可靠的维度对话评估策略。", "metrics": {"bleu_score": 41.21183751323024, "chrf_score": 40.495004196359204, "xcomet_score": 0.8999333381652832, "xcomet_qe_score": 0.869389533996582, "metricx_score": 1.2989521026611328, "metricx_qe_score": 1.3906128406524658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型回复是否表现出某些行为——例如,提供无关信息或自相矛盾——来减少人为评估的主观性。", "metrics": {"bleu_score": 48.396784299969475, "chrf_score": 41.01195274294112, "xcomet_score": 0.9578808546066284, "xcomet_qe_score": 0.9569164514541626, "metricx_score": 1.5770684480667114, "metricx_qe_score": 1.9838099479675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为在聊天中标注行为,或简称为ABC评估。", "metrics": {"bleu_score": 33.98508136320299, "chrf_score": 30.508475277290042, "xcomet_score": 0.8062419891357422, "xcomet_qe_score": 0.8094059228897095, "metricx_score": 2.3663506507873535, "metricx_qe_score": 2.618410110473633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发此方法是为了全面覆盖最近文献中被认为会影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 58.343910963318336, "chrf_score": 51.313770994590655, "xcomet_score": 0.9522242546081543, "xcomet_qe_score": 0.9451044797897339, "metricx_score": 1.516371250152588, "metricx_qe_score": 2.7967770099639893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC 评估能够衡量聊天模型产生各种主题性错误的速率。", "metrics": {"bleu_score": 37.28399245941657, "chrf_score": 28.2434178902652, "xcomet_score": 0.7864030599594116, "xcomet_qe_score": 0.6657567024230957, "metricx_score": 3.185028553009033, "metricx_qe_score": 3.7572107315063477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "APCEval 衡量聊天模型忽略其对话伙伴或发表不相关言论的回合数。 它可能会自相矛盾或与其伙伴相悖,产生虚构的事实或违背常识,并且在模型成功或失败时,表现出缺乏同理心。", "metrics": {"bleu_score": 31.798777655361363, "chrf_score": 27.144659105716173, "xcomet_score": 0.51409912109375, "xcomet_qe_score": 0.4375770092010498, "metricx_score": 4.603840351104736, "metricx_qe_score": 4.698176860809326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方式最为有效,我们选择了四款最先进的对话模型,并使用ABCEval对每款模型进行了100个真人与机器人对话的评估。", "metrics": {"bleu_score": 45.46308713404573, "chrf_score": 47.14482819836971, "xcomet_score": 0.8659902215003967, "xcomet_qe_score": 0.94590824842453, "metricx_score": 1.4230051040649414, "metricx_qe_score": 1.2596485614776611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了便于比较,我们还利用三种现有方法评估了这些对话:逐轮Liquid评分、对话层面的Liquid评分以及对话层面两两比较。", "metrics": {"bleu_score": 39.71192050702565, "chrf_score": 33.123286067301606, "xcomet_score": 0.6234802603721619, "xcomet_qe_score": 0.6205425262451172, "metricx_score": 7.951859474182129, "metricx_qe_score": 7.807900905609131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有方法,我们收集了对对话八个最常用评估方面的评价,因为这是评估聊天模型在多个维度上的标准做法。 ", "metrics": {"bleu_score": 49.64717286742399, "chrf_score": 39.17863107462047, "xcomet_score": 0.8572717308998108, "xcomet_qe_score": 0.7969014644622803, "metricx_score": 2.0257554054260254, "metricx_qe_score": 1.7969626188278198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现,与现有方法收集的标签相比,ABC评估行为标签总体上更可靠,这通过对100个双重标注对话的", "metrics": {"bleu_score": 35.59939414555685, "chrf_score": 31.084663104392764, "xcomet_score": 0.5862925052642822, "xcomet_qe_score": 0.5497751235961914, "metricx_score": 7.531929969787598, "metricx_qe_score": 6.505632400512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "标注者间一致性进行衡量。 此外,ABC 评估标签比现有方法生成的指标更能预测整体对话质量,正如本简单的线性回归分析所示。", "metrics": {"bleu_score": 38.36378227271173, "chrf_score": 35.208344374221404, "xcomet_score": 0.5786880850791931, "xcomet_qe_score": 0.549526572227478, "metricx_score": 5.676333427429199, "metricx_qe_score": 10.047395706176758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以观察到测量自反与伙伴反驳的比例,分别可以解释对话质量的百分之五和百分之十,而平均酒精度数评分仅能解释百分之四或更少。", "metrics": {"bleu_score": 13.214560579942589, "chrf_score": 14.29243437133622, "xcomet_score": 0.5707532167434692, "xcomet_qe_score": 0.5899797081947327, "metricx_score": 10.353463172912598, "metricx_qe_score": 9.392888069152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归,检验每个评估指标是否捕捉了聊天质量的独特方面。", "metrics": {"bleu_score": 67.89925893528314, "chrf_score": 60.64255838152898, "xcomet_score": 0.8732575178146362, "xcomet_qe_score": 0.8912918567657471, "metricx_score": 1.3709850311279297, "metricx_qe_score": 1.7465381622314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以观察到,所有ABC评估指标的结合解释了对话质量超过25%。并且,当您逐一移除这些指标时,大多数情况都会导致损失掉相当一部分关于质量的信息。", "metrics": {"bleu_score": 19.679416744439504, "chrf_score": 24.468273189000364, "xcomet_score": 0.768426775932312, "xcomet_qe_score": 0.7112375497817993, "metricx_score": 2.944725513458252, "metricx_qe_score": 3.1361923217773438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有转弯层级液体指标的结合,解释的质量因素远不如单独考虑它们,而且更少这些指标包含独特的信息。", "metrics": {"bleu_score": 15.672439994925405, "chrf_score": 17.239105636119557, "xcomet_score": 0.6496298909187317, "xcomet_qe_score": 0.7295242547988892, "metricx_score": 9.115272521972656, "metricx_qe_score": 9.58066177368164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些可靠、信息丰富且独特的ABC评估指标,使我们能够以比以往方法更高的分辨率评估会话式人工智能。", "metrics": {"bleu_score": 6.0936228424198156, "chrf_score": 10.673782260058907, "xcomet_score": 0.7843585014343262, "xcomet_qe_score": 0.8239257335662842, "metricx_score": 4.015721797943115, "metricx_qe_score": 3.068242311477661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验结果中,您可以观察到,仍存在若干挑战,并且已被精确量化。", "metrics": {"bleu_score": 31.227435897408682, "chrf_score": 33.24711003224485, "xcomet_score": 0.9725216627120972, "xcomet_qe_score": 0.9644452333450317, "metricx_score": 1.3914985656738281, "metricx_qe_score": 1.5735278129577637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人,在约20%的回复中存在常识性错误。", "metrics": {"bleu_score": 50.191208077499894, "chrf_score": 48.35012073505591, "xcomet_score": 0.9856196641921997, "xcomet_qe_score": 0.9933342933654785, "metricx_score": 0.8874697089195251, "metricx_qe_score": 1.3990558385849, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在约 15% 的回复中,它们会产生无关信息,并且在约 10% 的时间内自相矛盾或与对方的观点相悖。", "metrics": {"bleu_score": 33.69190370913737, "chrf_score": 31.80056092133585, "xcomet_score": 0.7870577573776245, "xcomet_qe_score": 0.7123240828514099, "metricx_score": 4.085666656494141, "metricx_qe_score": 3.7030365467071533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域快速的进步,许多这些误差率在今后发布的模型中都可能降低,自我们的评估以来。", "metrics": {"bleu_score": 29.944978211484383, "chrf_score": 26.19460116685644, "xcomet_score": 0.6990900039672852, "xcomet_qe_score": 0.7208500504493713, "metricx_score": 5.234091281890869, "metricx_qe_score": 5.526456832885742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更增加了我们追求可靠且精确的评估指标,以比较模型的重要性和必要性。", "metrics": {"bleu_score": 36.36516060296137, "chrf_score": 38.91490550239644, "xcomet_score": 0.9976074695587158, "xcomet_qe_score": 0.9432640671730042, "metricx_score": 1.241034746170044, "metricx_qe_score": 1.4485012292861938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABC 评估能被该领域的其他研究者所借鉴,作为朝着这一方向迈出的有意义一步,并", "metrics": {"bleu_score": 39.12123823606586, "chrf_score": 35.29194258020115, "xcomet_score": 0.7445732355117798, "xcomet_qe_score": 0.7388812303543091, "metricx_score": 4.9210896492004395, "metricx_qe_score": 2.1779487133026123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期待着在未来几个月和几年里看到会话式人工智能的进步。", "metrics": {"bleu_score": 36.993822604700384, "chrf_score": 34.23350722341164, "xcomet_score": 0.9071311950683594, "xcomet_qe_score": 0.8455888032913208, "metricx_score": 1.7823033332824707, "metricx_qe_score": 1.25203275680542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫쿄 윤,我将为大家介绍我们的工作,题为《何时翻译需要数据", "metrics": {"bleu_score": 21.98441434762152, "chrf_score": 18.928287533244873, "xcomet_score": 0.5121805667877197, "xcomet_qe_score": 0.5796595811843872, "metricx_score": 6.219723224639893, "metricx_qe_score": 4.535067558288574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "驱动的多语种探索?》。", "metrics": {"bleu_score": 41.06517147271933, "chrf_score": 32.998705052437835, "xcomet_score": 0.4806729257106781, "xcomet_qe_score": 0.37135541439056396, "metricx_score": 7.911846160888672, "metricx_qe_score": 5.290434837341309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与帕特里克·费尔南德斯、艾米丽·刘、安德烈·F·马丁斯和格雷厄姆·纽比格合作完成的。 那", "metrics": {"bleu_score": 11.644216046403798, "chrf_score": 7.53744299953494, "xcomet_score": 0.4615837335586548, "xcomet_qe_score": 0.4377022981643677, "metricx_score": 5.460168838500977, "metricx_qe_score": 2.7024877071380615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么很多翻译都取决于语境。", "metrics": {"bleu_score": 31.702331385234313, "chrf_score": 27.542404691239796, "xcomet_score": 0.807839035987854, "xcomet_qe_score": 0.6822673082351685, "metricx_score": 3.8983771800994873, "metricx_qe_score": 5.022433757781982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们应该如何翻译这句话中的“mole”?", "metrics": {"bleu_score": 63.34323002440796, "chrf_score": 70.60736733360486, "xcomet_score": 0.9966659545898438, "xcomet_qe_score": 0.9709947109222412, "metricx_score": 0.7755703330039978, "metricx_qe_score": 2.050631284713745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好吧,如果前一句是“如果大臣们知道了,事情可能会变得危险”,那么Moe指的是一个间谍。", "metrics": {"bleu_score": 8.51882073860277, "chrf_score": 6.326333588803236, "xcomet_score": 0.8379460573196411, "xcomet_qe_score": 0.8399993181228638, "metricx_score": 5.063401222229004, "metricx_qe_score": 5.765089988708496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,这可能是什么严重的事情吗?”,那么Moe指的是胎记。", "metrics": {"bleu_score": 9.878228685813983, "chrf_score": 9.474239810257135, "xcomet_score": 0.8219316005706787, "xcomet_qe_score": 0.8071538209915161, "metricx_score": 4.923916816711426, "metricx_qe_score": 5.0172271728515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据语境的不同,词义会发生变化,其翻译也随之改变。", "metrics": {"bleu_score": 23.823810652839953, "chrf_score": 20.36092220516522, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.31759846210479736, "metricx_qe_score": 0.26951420307159424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在处理这类情况下的翻译质量相当困难。", "metrics": {"bleu_score": 47.581040383390814, "chrf_score": 39.9874678461635, "xcomet_score": 0.9787315130233765, "xcomet_qe_score": 0.980809211730957, "metricx_score": 0.8596081733703613, "metricx_qe_score": 1.119746208190918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,仅有一小部分翻译依赖于语境,这使得诸如BLEU之类的语料库级别指标无法准确捕捉这些翻译。", "metrics": {"bleu_score": 40.920277303762575, "chrf_score": 37.024966739487056, "xcomet_score": 0.9774815440177917, "xcomet_qe_score": 0.9621395468711853, "metricx_score": 1.3452184200286865, "metricx_qe_score": 1.9916199445724487, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对上下文相关的翻译进行有针对性的评估,但这些资源仅支持有限类型的上下文相关翻译以及有限的语言集,因为它们通常依赖于领域知识和人工策编。", "metrics": {"bleu_score": 70.06020244741083, "chrf_score": 63.19817761781673, "xcomet_score": 0.9495368003845215, "xcomet_qe_score": 0.9491101503372192, "metricx_score": 2.3347880840301514, "metricx_qe_score": 1.6260093450546265, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要语境?", "metrics": {"bleu_score": 8.736015370428479, "chrf_score": 11.196726662314651, "xcomet_score": 0.8924298286437988, "xcomet_qe_score": 0.8972160816192627, "metricx_score": 0.3988206386566162, "metricx_qe_score": 0.22178193926811218, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型处理这些情况的能力如何?", "metrics": {"bleu_score": 34.17929717947519, "chrf_score": 29.95491759459726, "xcomet_score": 0.9984992742538452, "xcomet_qe_score": 0.9902448654174805, "metricx_score": 0.5139561295509338, "metricx_qe_score": 0.4580145478248596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了词语在翻译过程中对语境的依赖程度。", "metrics": {"bleu_score": 60.5315917416268, "chrf_score": 51.040242622534194, "xcomet_score": 0.9980576038360596, "xcomet_qe_score": 1.0, "metricx_score": 4.532907009124756, "metricx_qe_score": 4.560736179351807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在先前的工作中,我们引入了CXMI作为衡量机器翻译模型利用上下文的指标。", "metrics": {"bleu_score": 41.74784000115619, "chrf_score": 41.84599229921978, "xcomet_score": 0.9015456438064575, "xcomet_qe_score": 0.9106861352920532, "metricx_score": 1.46138334274292, "metricx_qe_score": 1.4982414245605469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量在给定源X的情况下,上下文C提供了多少关于目标Y的信息来实现。 你可以将 CXMI 视为赋予模型上下文所获得的信息。", "metrics": {"bleu_score": 63.61379547868021, "chrf_score": 60.05604496335132, "xcomet_score": 0.8330799341201782, "xcomet_qe_score": 0.7519306540489197, "metricx_score": 4.202696800231934, "metricx_qe_score": 4.338324546813965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本研究中,我们将CXMI扩展到逐点CXMI(pointwise CXMI),后者可以衡量在句子层面或词层面上的上下文使用情况。", "metrics": {"bleu_score": 43.434676671999135, "chrf_score": 52.87412426234482, "xcomet_score": 0.896380603313446, "xcomet_qe_score": 0.8855808973312378, "metricx_score": 2.2694945335388184, "metricx_qe_score": 2.4594006538391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将PSXMI较高的词视为那些需要上下文来进行翻译的词语。", "metrics": {"bleu_score": 54.62677702702796, "chrf_score": 46.87976968394502, "xcomet_score": 0.9602608680725098, "xcomet_qe_score": 0.9501839876174927, "metricx_score": 3.103020429611206, "metricx_qe_score": 3.489447832107544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们分析具有高 PCXMI 值的词语,以寻找这些词语之间的模式。", "metrics": {"bleu_score": 35.0714645213851, "chrf_score": 36.736278041869035, "xcomet_score": 0.9897609949111938, "xcomet_qe_score": 0.977414608001709, "metricx_score": 1.117496371269226, "metricx_qe_score": 2.221930980682373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对来自 TED 演讲的文本进行分析,这些演讲已被翻译成十四种不同的语言。 我们对分析进行操作,", "metrics": {"bleu_score": 14.849103164051433, "chrf_score": 25.79536759820239, "xcomet_score": 0.7053742408752441, "xcomet_qe_score": 0.6161261200904846, "metricx_score": 6.9997968673706055, "metricx_qe_score": 7.026031970977783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "涉及三个不同的层次。", "metrics": {"bleu_score": 29.48682411907622, "chrf_score": 28.180120471113746, "xcomet_score": 0.8385723233222961, "xcomet_qe_score": 0.7666403651237488, "metricx_score": 0.7591273784637451, "metricx_qe_score": 1.3654720783233643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们考察具有较高平均 PCXMI 值的词性标签。", "metrics": {"bleu_score": 31.101473708063242, "chrf_score": 31.155114709074134, "xcomet_score": 0.8686994314193726, "xcomet_qe_score": 0.7650845050811768, "metricx_score": 2.5758888721466064, "metricx_qe_score": 3.215696096420288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们可以发现,例如,阿拉伯语中存在具有相对较高 p-six mi 值的双重代词。", "metrics": {"bleu_score": 25.006681812704002, "chrf_score": 22.373110378484864, "xcomet_score": 0.6835227012634277, "xcomet_qe_score": 0.7639256715774536, "metricx_score": 6.795351028442383, "metricx_qe_score": 6.311821937561035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,英语中没有双重代词。因此,在翻译成阿拉伯语时,需要根据语境来判断一个代词是否具有双重含义。", "metrics": {"bleu_score": 38.838557143045016, "chrf_score": 34.52946453214922, "xcomet_score": 0.9008240699768066, "xcomet_qe_score": 0.9959601163864136, "metricx_score": 1.140317678451538, "metricx_qe_score": 0.8121984601020813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现有些语言在选择合适的动词形式时也需要语境。", "metrics": {"bleu_score": 54.64579418466107, "chrf_score": 45.91127508081531, "xcomet_score": 0.9886361360549927, "xcomet_qe_score": 0.9889229536056519, "metricx_score": 0.5247389674186707, "metricx_qe_score": 0.5444418787956238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们考察具有在所有不同出现情况下的较高平均 p/seksually 的词汇项目。", "metrics": {"bleu_score": 21.65592412275547, "chrf_score": 18.15241981496734, "xcomet_score": 0.59026038646698, "xcomet_qe_score": 0.6211917996406555, "metricx_score": 10.322261810302734, "metricx_qe_score": 10.695099830627441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别出像这样案例,其中在中文翻译中,需要根据语境来翻译专有名词,以确保在整个文档中采用相同的译法。", "metrics": {"bleu_score": 30.09937991573582, "chrf_score": 26.36305362962432, "xcomet_score": 0.9672002792358398, "xcomet_qe_score": 0.940240740776062, "metricx_score": 0.548041582107544, "metricx_qe_score": 0.7629518508911133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现语境对于以恰当的正式程度进行翻译至关重要。", "metrics": {"bleu_score": 16.110518603734324, "chrf_score": 19.8050721575235, "xcomet_score": 0.9372031688690186, "xcomet_qe_score": 0.9775701761245728, "metricx_score": 0.6678303480148315, "metricx_qe_score": 0.4885154366493225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们考察具有较高p6mi值的不同个体标记。", "metrics": {"bleu_score": 11.124661907380256, "chrf_score": 11.295015371102329, "xcomet_score": 0.6966314315795898, "xcomet_qe_score": 0.6464394330978394, "metricx_score": 7.138260841369629, "metricx_qe_score": 6.233227729797363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别无法仅通过该词本身捕捉到的现象,而是通过标准结构来表达,例如省略解析。", "metrics": {"bleu_score": 29.29667886329116, "chrf_score": 25.905695948959618, "xcomet_score": 0.6657940149307251, "xcomet_qe_score": 0.6523008346557617, "metricx_score": 2.0692358016967773, "metricx_qe_score": 2.7009336948394775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们利用分析结果来设计一个文档级别翻译的基准。", "metrics": {"bleu_score": 49.47834226950782, "chrf_score": 41.944674046575614, "xcomet_score": 0.9850327968597412, "xcomet_qe_score": 0.9614388346672058, "metricx_score": 0.8510357141494751, "metricx_qe_score": 1.1943128108978271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别出的这五种不和谐现象,我们创建了标注器,以自动识别与该现象相关的词语。", "metrics": {"bleu_score": 42.24411513252604, "chrf_score": 34.8093827962835, "xcomet_score": 0.768254280090332, "xcomet_qe_score": 0.7539244890213013, "metricx_score": 2.636394500732422, "metricx_qe_score": 2.3413357734680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称我们的标注器为“多语言语篇感知”标注器,即MUDA标注器。", "metrics": {"bleu_score": 20.90181206308033, "chrf_score": 22.012318834690753, "xcomet_score": 0.8814055919647217, "xcomet_qe_score": 0.8192580938339233, "metricx_score": 1.2700347900390625, "metricx_qe_score": 1.2507336139678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同的语言在这些离散现象上的比例也各不相同。", "metrics": {"bleu_score": 14.458350226851175, "chrf_score": 19.618092950848578, "xcomet_score": 0.8666517734527588, "xcomet_qe_score": 0.819256067276001, "metricx_score": 4.627556800842285, "metricx_qe_score": 4.2354841232299805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们使用 MUDA 标注器,将其应用于我们希望用于评估的平行语料库,并对 MUDA 标注器识别出的上下文相关的示例,运用我们选择的翻译指标进行评估。", "metrics": {"bleu_score": 41.74990678644464, "chrf_score": 40.498308297311084, "xcomet_score": 0.9652173519134521, "xcomet_qe_score": 0.9212778806686401, "metricx_score": 1.1107951402664185, "metricx_qe_score": 1.182058572769165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用基准测试以及其他指标,在文档级别的机器翻译中评估不同的模型。", "metrics": {"bleu_score": 36.618320242668766, "chrf_score": 32.50159653816321, "xcomet_score": 0.9714139699935913, "xcomet_qe_score": 0.9151608347892761, "metricx_score": 1.2816897630691528, "metricx_qe_score": 1.5753761529922485, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库层面的指标时,例如对于BLEU分数,我们发现复杂性无关的模型表现最佳。", "metrics": {"bleu_score": 46.29654712740358, "chrf_score": 43.80627293912181, "xcomet_score": 0.8300970792770386, "xcomet_qe_score": 0.7054875493049622, "metricx_score": 3.649456739425659, "metricx_qe_score": 3.132265329360962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果使用彗星(Comet)评价指标,则上下文感知模型表现最佳。", "metrics": {"bleu_score": 28.741071164957475, "chrf_score": 26.35827040483962, "xcomet_score": 0.8509389162063599, "xcomet_qe_score": 0.7681431770324707, "metricx_score": 2.96016788482666, "metricx_qe_score": 3.0953824520111084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用词 F 值(word F-measure),那么有上下文和无上下文的模型表现可比。", "metrics": {"bleu_score": 48.62844516680566, "chrf_score": 38.67479161643215, "xcomet_score": 0.8496228456497192, "xcomet_qe_score": 0.8296685218811035, "metricx_score": 2.4170169830322266, "metricx_qe_score": 2.6674275398254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,仅使用企业层面的指标来确定最佳文档级别翻译系统是困难的。", "metrics": {"bleu_score": 30.972651353786464, "chrf_score": 25.49810425009386, "xcomet_score": 0.875653862953186, "xcomet_qe_score": 0.8471803665161133, "metricx_score": 3.8098583221435547, "metricx_qe_score": 4.133269786834717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用MUDA基准来评估模型,我们发现,对于某些语篇现象,如正式程度和词汇衔接,具有上下文感知能力的模型明显比不使用上下文的模型更准确。", "metrics": {"bleu_score": 42.084333354731584, "chrf_score": 40.24440670434592, "xcomet_score": 0.9194612503051758, "xcomet_qe_score": 0.8857239484786987, "metricx_score": 1.6764264106750488, "metricx_qe_score": 2.067884922027588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是这些模型在其他诸如省略、代词和动词形式等现象上的表现,与未使用上下文的模型相比并没有显", "metrics": {"bleu_score": 48.22301364104367, "chrf_score": 43.580014328418386, "xcomet_score": 0.8207259178161621, "xcomet_qe_score": 0.7386111617088318, "metricx_score": 6.30025577545166, "metricx_qe_score": 2.3791897296905518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "著改善。这提示我们需要在文档级别翻译方面取得更多进展。", "metrics": {"bleu_score": 29.475175369115192, "chrf_score": 25.37598440734679, "xcomet_score": 0.6727738380432129, "xcomet_qe_score": 0.6729885339736938, "metricx_score": 5.161947250366211, "metricx_qe_score": 7.28959846496582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还对比了不同的商业系统,基准测试表明,在文档级别翻译中,DeepBell 通常比谷歌翻译更准确。", "metrics": {"bleu_score": 51.503155844812134, "chrf_score": 43.80211438374211, "xcomet_score": 0.8177171349525452, "xcomet_qe_score": 0.669090747833252, "metricx_score": 5.326521873474121, "metricx_qe_score": 5.736039161682129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们对十四种语言对进行了数据驱动分析,以确定何时需要上下文信息进行翻译。 然后,我们利用研究结果构建一个文档级别机器翻译的基准,这有助于我们识别哪些离散现象模型能够处理得较好,以及哪些翻译系统擅长文档级别的翻译。", "metrics": {"bleu_score": 41.550601108500565, "chrf_score": 37.540340160193146, "xcomet_score": 0.7968529462814331, "xcomet_qe_score": 0.8154699802398682, "metricx_score": 3.8326966762542725, "metricx_qe_score": 4.244945526123047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多伦多见。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9990917444229126, "xcomet_qe_score": 0.985295832157135, "metricx_score": 0.46565622091293335, "metricx_qe_score": 1.37757408618927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是Yanis Lavrack,我将向您介绍我们在Dr. Berth上的工作,这是一款针对生物医学和临床领域,用法语预训练的强大模型。", "metrics": {"bleu_score": 29.197177353301587, "chrf_score": 32.87240995672821, "xcomet_score": 0.6249346733093262, "xcomet_qe_score": 0.6176674365997314, "metricx_score": 4.201005458831787, "metricx_qe_score": 3.860973596572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这份演示文稿中,我们首先探讨医疗保健领域的语言模型。", "metrics": {"bleu_score": 41.48387761083271, "chrf_score": 38.10555180854075, "xcomet_score": 0.9566915035247803, "xcomet_qe_score": 0.9658805131912231, "metricx_score": 2.3970580101013184, "metricx_qe_score": 1.9147261381149292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将介绍本文的主要贡献。", "metrics": {"bleu_score": 48.832168314095355, "chrf_score": 39.53340470135484, "xcomet_score": 0.9888916015625, "xcomet_qe_score": 1.0, "metricx_score": 0.31139516830444336, "metricx_qe_score": 0.5491302013397217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个基于罗伯塔(Roberta)的法语生物医学模型,名为Dr. Berth,该模型以Natchios数据集为训练集,而Natchios是一个从网络上抓取的医学数据集合。", "metrics": {"bleu_score": 33.63467149698692, "chrf_score": 25.257565233341843, "xcomet_score": 0.6162180304527283, "xcomet_qe_score": 0.5684165358543396, "metricx_score": 4.283315181732178, "metricx_qe_score": 3.874570846557617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了多钚设定和数据源的模型比较。 ", "metrics": {"bleu_score": 43.27762586017906, "chrf_score": 40.84439632640261, "xcomet_score": 0.7574702501296997, "xcomet_qe_score": 0.7737645506858826, "metricx_score": 5.3856072425842285, "metricx_qe_score": 5.702468395233154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们展示了我们在法国的十一项生物医学和临床下游任务中的结果。", "metrics": {"bleu_score": 56.78929778677617, "chrf_score": 53.22524851039202, "xcomet_score": 0.7870402336120605, "xcomet_qe_score": 0.7792679071426392, "metricx_score": 2.514665365219116, "metricx_qe_score": 2.5921401977539062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结实验结果,并提供更多关于如何访问该模型的信息。", "metrics": {"bleu_score": 22.90925532263192, "chrf_score": 22.494690745601233, "xcomet_score": 0.9416998624801636, "xcomet_qe_score": 0.9562276601791382, "metricx_score": 0.38279998302459717, "metricx_qe_score": 0.2756766676902771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自 2018 年发布以来,BERT 已成为解决自然语言处理任务最有效的方法之一,与诸如词向量、FastText 或 Enroll 等历史静态和情境化方法相比,它提供了巨大的性能提升。 自那", "metrics": {"bleu_score": 61.53857122047378, "chrf_score": 57.43986578552507, "xcomet_score": 0.47539639472961426, "xcomet_qe_score": 0.46259140968322754, "metricx_score": 5.534038543701172, "metricx_qe_score": 5.447734355926514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以后,该模型已被适配到许多其他语言,例如法语中的Camembert,以及生物医学领域中的Permette Bert 和 BioBert,以及临床领域中的Clinical Bert,但主要还是在英语中。", "metrics": {"bleu_score": 35.639552407731124, "chrf_score": 35.537637120971056, "xcomet_score": 0.5193376541137695, "xcomet_qe_score": 0.4832926392555237, "metricx_score": 6.91043758392334, "metricx_qe_score": 6.877601146697998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的专业模型十分稀缺,并且通常是基于持续的假定构建,这是由于缺乏领域内数据所致。", "metrics": {"bleu_score": 31.97953661368685, "chrf_score": 32.17321070641967, "xcomet_score": 0.7964429259300232, "xcomet_qe_score": 0.830344021320343, "metricx_score": 1.8518126010894775, "metricx_qe_score": 1.602048397064209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,直到现在,法国还没有适用于生物医学领域的开源现代技术。 那", "metrics": {"bleu_score": 37.8878212033196, "chrf_score": 37.57728718116064, "xcomet_score": 0.735808253288269, "xcomet_qe_score": 0.7302252054214478, "metricx_score": 6.518782138824463, "metricx_qe_score": 3.5276386737823486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么,我们反思:对于广泛的应用场景,最合适的数据来源究竟是什么?而目前可用的数据,可以作为临床数据的良好替代。", "metrics": {"bleu_score": 23.78706071654585, "chrf_score": 25.315715829957515, "xcomet_score": 0.6176496744155884, "xcomet_qe_score": 0.6621453762054443, "metricx_score": 3.881021499633789, "metricx_qe_score": 3.470283269882202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将伯特博士的模型与我们基于匿名数据的舒伯特模型进行比较,该数据来源于我们所拥有的非大学附属医院。", "metrics": {"bleu_score": 27.936075447971024, "chrf_score": 23.890623262519703, "xcomet_score": 0.695499062538147, "xcomet_qe_score": 0.6311702728271484, "metricx_score": 5.007518291473389, "metricx_qe_score": 5.616729736328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们会问自己,我们需要多少数据来训练一个专门针对法语的数据模型?", "metrics": {"bleu_score": 37.24905193677607, "chrf_score": 39.87919297385449, "xcomet_score": 0.9867249727249146, "xcomet_qe_score": 0.9173871278762817, "metricx_score": 0.5291295051574707, "metricx_qe_score": 0.5689236521720886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4GB、8GB还是更多?", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 90.21205646205644, "xcomet_score": 0.9781609773635864, "xcomet_qe_score": 0.9645631313323975, "metricx_score": 0.23510365188121796, "metricx_qe_score": 0.5033293962478638, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较四个从零开始的模型。第一个版本是七GB Nachos的Dr. Bert,第二个版本是Nachos的四个GB子集。 舒伯特模型的第一个版本是一个临床模型,使用了来自临床节点的四吉字节的句子。而舒伯特模型的最终版本则混合了四吉字节的自然语言数据集和四吉字节的临床节点数据。", "metrics": {"bleu_score": 30.91281540685735, "chrf_score": 23.98627457201156, "xcomet_score": 0.2927806079387665, "xcomet_qe_score": 0.3239939212799072, "metricx_score": 11.203493118286133, "metricx_qe_score": 10.478742599487305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这项对比之外,我们还引入了三个在持续预训练上训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 58.63451471714033, "chrf_score": 49.76931743138419, "xcomet_score": 0.8651107549667358, "xcomet_qe_score": 0.8409795761108398, "metricx_score": 2.756833553314209, "metricx_qe_score": 3.148313522338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种基于Camembert权重且在四吉字节的玉米片数据集上训练的", "metrics": {"bleu_score": 6.964541799727333, "chrf_score": 19.26519354664517, "xcomet_score": 0.557593584060669, "xcomet_qe_score": 0.5894352793693542, "metricx_score": 6.65496826171875, "metricx_qe_score": 7.1733012199401855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型,另一种同样基于Camembert,但这次在四吉字节的Klinker Lots数据集上进行训练。 最后,还有一个基于英语生物医学模型,名为BMLB,并使用4GB的Snatchers数据集训练得到。", "metrics": {"bleu_score": 24.875911073501204, "chrf_score": 26.855993232005183, "xcomet_score": 0.2838730216026306, "xcomet_qe_score": 0.28274622559547424, "metricx_score": 11.254164695739746, "metricx_qe_score": 12.072946548461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为评估我们这七个模型,我们收集了支持公共和私有下游任务的数据,这些任务包括姓名和身份识别、分类、模式切换标记以及问答。", "metrics": {"bleu_score": 37.38668440561494, "chrf_score": 33.50772849965751, "xcomet_score": 0.5887812376022339, "xcomet_qe_score": 0.6493578553199768, "metricx_score": 4.476673126220703, "metricx_qe_score": 4.308965682983398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行比较,这些模型分别是 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBelt、Myobelt 和 ClinicalBelt。", "metrics": {"bleu_score": 37.11510600659487, "chrf_score": 42.4603385336806, "xcomet_score": 0.4265454411506653, "xcomet_qe_score": 0.39364930987358093, "metricx_score": 7.846717834472656, "metricx_qe_score": 7.696272850036621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,该模型在与训练数据性质相同的数据集上表现最佳。", "metrics": {"bleu_score": 40.08702173547459, "chrf_score": 32.7982055756646, "xcomet_score": 0.9833145141601562, "xcomet_qe_score": 0.9817712306976318, "metricx_score": 0.7295376658439636, "metricx_qe_score": 0.929664671421051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从异构来源获取该数据,并且观察到这些数据来源似乎更为灵活。", "metrics": {"bleu_score": 15.226936883341002, "chrf_score": 20.71348638282682, "xcomet_score": 0.8238433599472046, "xcomet_qe_score": 0.802413821220398, "metricx_score": 2.4566006660461426, "metricx_qe_score": 1.791154384613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据能够带来更好的性能。", "metrics": {"bleu_score": 43.79983128083005, "chrf_score": 36.918121334100285, "xcomet_score": 0.9357359409332275, "xcomet_qe_score": 0.9746940732002258, "metricx_score": 2.5581953525543213, "metricx_qe_score": 3.008680582046509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,从零开始,免费培训似乎在大多数任务中都能获得更高的表现。", "metrics": {"bleu_score": 50.29311140805834, "chrf_score": 46.46884282073023, "xcomet_score": 0.7959328889846802, "xcomet_qe_score": 0.7969849705696106, "metricx_score": 5.171792507171631, "metricx_qe_score": 5.988603591918945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们在持续假装实验中,使用在 Natchez 的四 GB 子集中训练过的 PumedBeard 的权重和分词器,获得了与从头开始使用 Dr. Beard 四 GB 训练得到的实验结果相当的结果。", "metrics": {"bleu_score": 21.859202100251697, "chrf_score": 23.802343957738927, "xcomet_score": 0.3523736000061035, "xcomet_qe_score": 0.3274978697299957, "metricx_score": 9.411138534545898, "metricx_qe_score": 10.41324234008789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与基于常见词嵌入和分词器的模型不同,后者会受到稳定性问题的困扰。", "metrics": {"bleu_score": 18.569888119992957, "chrf_score": 12.67364162075115, "xcomet_score": 0.6821845769882202, "xcomet_qe_score": 0.6968411207199097, "metricx_score": 3.595201015472412, "metricx_qe_score": 4.8996171951293945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为总结,我们的提议系统在九项中十一项下游任务中表现出更优的性能,并且总体上超越了这里使用的通用模型Camembert的结果。", "metrics": {"bleu_score": 21.392610805452147, "chrf_score": 20.25278650575187, "xcomet_score": 0.7568951845169067, "xcomet_qe_score": 0.7848334312438965, "metricx_score": 6.1091790199279785, "metricx_qe_score": 4.565629482269287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业数据更好,更专业的数据更好,但其扩展性较差。", "metrics": {"bleu_score": 31.073016835096727, "chrf_score": 26.295617959461726, "xcomet_score": 0.7471740245819092, "xcomet_qe_score": 0.6776145696640015, "metricx_score": 3.6353259086608887, "metricx_qe_score": 3.6682324409484863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从Natchios获得的预训练模型均可在YuginFace上免费获取,所有训练脚本则在我们的GitHub仓库中。", "metrics": {"bleu_score": 41.01320592903814, "chrf_score": 40.50064611537555, "xcomet_score": 0.6197285652160645, "xcomet_qe_score": 0.7023869752883911, "metricx_score": 6.6239471435546875, "metricx_qe_score": 6.606287479400635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢这次的演讲,我们期待在多伦多海报环节与您交流。", "metrics": {"bleu_score": 16.25416791576053, "chrf_score": 19.357325471492985, "xcomet_score": 0.8911739587783813, "xcomet_qe_score": 0.9271462559700012, "metricx_score": 1.4391193389892578, "metricx_qe_score": 1.601201057434082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼,今天我将向您简要介绍我们关于利用多重集标记和潜在排列,在无树结构下实现组合泛化的论文。", "metrics": {"bleu_score": 40.92639816890634, "chrf_score": 33.35691644480342, "xcomet_score": 0.953977108001709, "xcomet_qe_score": 0.8457722663879395, "metricx_score": 1.2068365812301636, "metricx_qe_score": 2.0118496417999268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和我的导师亚历山大·科拉 (Alexander Kola) 和伊万·季托夫 (Ivan Titov) 共同完成的工作。", "metrics": {"bleu_score": 6.066858194850449, "chrf_score": 44.96627384586086, "xcomet_score": 0.808987021446228, "xcomet_qe_score": 0.6748108863830566, "metricx_score": 3.4054207801818848, "metricx_qe_score": 3.13598370552063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "构成泛化可以理解为学习者处理更深层递归以及训练期间单独见过的短语组合的能力,即使这些组合在训练中未曾出现过。", "metrics": {"bleu_score": 54.58515432828788, "chrf_score": 58.74116187513623, "xcomet_score": 0.6148311495780945, "xcomet_qe_score": 0.46357232332229614, "metricx_score": 5.259870529174805, "metricx_qe_score": 6.523656368255615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的语境下,测试组合泛化能力可能如下所示。", "metrics": {"bleu_score": 54.303214666333915, "chrf_score": 45.82718403989291, "xcomet_score": 0.9739799499511719, "xcomet_qe_score": 0.9047935605049133, "metricx_score": 0.8423441648483276, "metricx_qe_score": 1.536298394203186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如常,我们有一个训练集,", "metrics": {"bleu_score": 20.89946492122517, "chrf_score": 18.74423949123935, "xcomet_score": 0.8402756452560425, "xcomet_qe_score": 0.7458329200744629, "metricx_score": 2.990189552307129, "metricx_qe_score": 4.500074863433838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中包含一些句子,例如“女孩睡了”和", "metrics": {"bleu_score": 21.555378801920327, "chrf_score": 13.85990474691569, "xcomet_score": 0.5842151641845703, "xcomet_qe_score": 0.18688362836837769, "metricx_score": 4.49245548248291, "metricx_qe_score": 2.0894923210144043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道女孩睡了”。", "metrics": {"bleu_score": 15.585475983689154, "chrf_score": 10.987009981578273, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 1.0, "metricx_score": 1.6568025350570679, "metricx_qe_score": 1.8745388984680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些表达与逻辑形式配对,逻辑形式代表了其含义的核心方面。", "metrics": {"bleu_score": 6.143498010483918, "chrf_score": 13.532000951701168, "xcomet_score": 0.9286894798278809, "xcomet_qe_score": 0.968532919883728, "metricx_score": 1.1199891567230225, "metricx_qe_score": 1.1115494966506958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准的机器学习评估方法不同,测试集并非来自相同的分布,而是包含结构上未见的逻辑形式。", "metrics": {"bleu_score": 43.901717890367046, "chrf_score": 37.419421836183695, "xcomet_score": 0.898182213306427, "xcomet_qe_score": 0.8560174107551575, "metricx_score": 1.0643792152404785, "metricx_qe_score": 1.6418983936309814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此示例中,模型在训练期间经历了较浅的递归,并被测试于一个具有更深递归的示例上。", "metrics": {"bleu_score": 14.279770571390362, "chrf_score": 14.928904416339961, "xcomet_score": 0.8867564797401428, "xcomet_qe_score": 0.8799095153808594, "metricx_score": 2.8982863426208496, "metricx_qe_score": 4.614549160003662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种超出分布泛化问题,并且常常生成与输入脱节的输出。", "metrics": {"bleu_score": 18.626167406959443, "chrf_score": 17.750298309267354, "xcomet_score": 0.7048263549804688, "xcomet_qe_score": 0.7243651151657104, "metricx_score": 3.5772125720977783, "metricx_qe_score": 3.45048451423645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其需要注意的是,他们常常无法再现输入与输出之间的系统性对应关系,例如在示例中用颜色标示出来的那些。", "metrics": {"bleu_score": 34.47976234400801, "chrf_score": 33.2310378591944, "xcomet_score": 0.9905405044555664, "xcomet_qe_score": 0.9777601957321167, "metricx_score": 1.8178136348724365, "metricx_qe_score": 1.123900055885315, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种常见的处理方法是,将树结构融入模型中。", "metrics": {"bleu_score": 21.09080247373033, "chrf_score": 19.846495232613016, "xcomet_score": 0.9975851774215698, "xcomet_qe_score": 1.0, "metricx_score": 0.548213541507721, "metricx_qe_score": 0.5951555967330933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树状图旨在捕捉与逻辑形式相关的语句构成过程。", "metrics": {"bleu_score": 10.695489413555519, "chrf_score": 12.469133087532441, "xcomet_score": 0.9839215278625488, "xcomet_qe_score": 0.9717526435852051, "metricx_score": 1.3304307460784912, "metricx_qe_score": 1.2319796085357666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这运作良好,但树木通常不提供,需要以某种方式获取。", "metrics": {"bleu_score": 13.583893115437466, "chrf_score": 14.953781826785006, "xcomet_score": 0.7610739469528198, "xcomet_qe_score": 0.8274604082107544, "metricx_score": 3.009096384048462, "metricx_qe_score": 2.3227667808532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本高昂的过程。", "metrics": {"bleu_score": 58.9597026996279, "chrf_score": 52.51139776496644, "xcomet_score": 0.9886571168899536, "xcomet_qe_score": 0.9861245155334473, "metricx_score": 0.5039506554603577, "metricx_qe_score": 0.5730300545692444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到对逻辑形式进行相当程度的、特定的预处理,例如,处理变量符号。", "metrics": {"bleu_score": 38.83035383555548, "chrf_score": 33.558505832951546, "xcomet_score": 0.9845412969589233, "xcomet_qe_score": 0.9783545732498169, "metricx_score": 0.7514826655387878, "metricx_qe_score": 0.8802850246429443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树状结构也可能涉及专门的语法归纳程序。", "metrics": {"bleu_score": 62.472375173961176, "chrf_score": 57.44265688731633, "xcomet_score": 0.9778578281402588, "xcomet_qe_score": 0.9693984985351562, "metricx_score": 1.808321475982666, "metricx_qe_score": 1.6390360593795776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们未使用树结构,而是引入了一种神经序列到序列模型,该模型直接建模输入片段与输出片段之间的对应关系。 这是我们", "metrics": {"bleu_score": 44.01945512525841, "chrf_score": 37.08488356054933, "xcomet_score": 0.6191350817680359, "xcomet_qe_score": 0.6575378179550171, "metricx_score": 6.360864639282227, "metricx_qe_score": 2.632784843444824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首次展示了对更深层递归的强大泛化能力,而无需依赖于树结构。", "metrics": {"bleu_score": 51.35634098679393, "chrf_score": 47.21027565081752, "xcomet_score": 0.9767534732818604, "xcomet_qe_score": 0.9629981517791748, "metricx_score": 2.5332190990448, "metricx_qe_score": 3.038245677947998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法分两个步骤预测输出结果,从输入开始。", "metrics": {"bleu_score": 59.24373883217679, "chrf_score": 68.60967812600678, "xcomet_score": 0.8723753690719604, "xcomet_qe_score": 0.8629738092422485, "metricx_score": 1.5976054668426514, "metricx_qe_score": 1.0303919315338135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入标记添加一个无序的多重集,其中包含将在输出中出现的标记。", "metrics": {"bleu_score": 20.26883702268159, "chrf_score": 22.049738512375452, "xcomet_score": 0.8751808404922485, "xcomet_qe_score": 0.8390767574310303, "metricx_score": 2.787015199661255, "metricx_qe_score": 2.984341621398926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步完成后,我们已经拥有了所有正确的标记,但它们尚未排序。", "metrics": {"bleu_score": 44.53456746043066, "chrf_score": 39.14609362548306, "xcomet_score": 0.9223957061767578, "xcomet_qe_score": 0.8935165405273438, "metricx_score": 2.066699981689453, "metricx_qe_score": 3.922440767288208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在第二步中,我们使用另一个模型来预测一个排列,将它们置于正确的顺序。", "metrics": {"bleu_score": 54.648475850384045, "chrf_score": 50.4896114200648, "xcomet_score": 0.8752568960189819, "xcomet_qe_score": 0.8634690046310425, "metricx_score": 3.024968147277832, "metricx_qe_score": 4.216945648193359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍一种新的方法来预测一个排列,该方法对可能的排列没有任何硬性约束。", "metrics": {"bleu_score": 68.2703664007529, "chrf_score": 63.6216784507859, "xcomet_score": 0.9202755689620972, "xcomet_qe_score": 0.907314121723175, "metricx_score": 2.3697214126586914, "metricx_qe_score": 3.587735176086426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法具有相当的灵活性和表达力。", "metrics": {"bleu_score": 68.91557807535084, "chrf_score": 60.66442658934918, "xcomet_score": 0.9276312589645386, "xcomet_qe_score": 0.9575750827789307, "metricx_score": 1.0060421228408813, "metricx_qe_score": 1.8067609071731567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的排列模型大致如下工作。", "metrics": {"bleu_score": 47.033069090332, "chrf_score": 39.96901796744152, "xcomet_score": 0.8601151704788208, "xcomet_qe_score": 0.7867026329040527, "metricx_score": 3.261329412460327, "metricx_qe_score": 3.9719417095184326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左至右遍历输出,并确定在每个位置放置哪个多重集标记。", "metrics": {"bleu_score": 61.34588734928764, "chrf_score": 55.803046889253785, "xcomet_score": 0.835060715675354, "xcomet_qe_score": 0.803558349609375, "metricx_score": 1.7814136743545532, "metricx_qe_score": 2.7056658267974854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需选择一个,如红色高亮所示。", "metrics": {"bleu_score": 58.56346648004451, "chrf_score": 48.42242499978162, "xcomet_score": 0.92508864402771, "xcomet_qe_score": 0.9093221426010132, "metricx_score": 0.5608147382736206, "metricx_qe_score": 0.6543219089508057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们跳转到下一个多重集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 73.10296287680117, "chrf_score": 67.94671218521793, "xcomet_score": 0.8122124671936035, "xcomet_qe_score": 0.8322657942771912, "metricx_score": 3.010741949081421, "metricx_qe_score": 3.209465980529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个token,通过跳转到另一个多重集token来实现。", "metrics": {"bleu_score": 70.9814637375763, "chrf_score": 64.57852782146156, "xcomet_score": 0.7344005107879639, "xcomet_qe_score": 0.7252732515335083, "metricx_score": 5.793002128601074, "metricx_qe_score": 4.932879447937012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到来自第一阶段的每一个token都被访问过一次为止。", "metrics": {"bleu_score": 47.534972659103076, "chrf_score": 42.92366218406189, "xcomet_score": 0.8720235824584961, "xcomet_qe_score": 0.9073781967163086, "metricx_score": 3.7178261280059814, "metricx_qe_score": 3.5550003051757812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了先给您一个实验结果的预览,我们在此将我们的方法与其他无树模型在Kong的基准测试中进行比较。我们的模型在泛化", "metrics": {"bleu_score": 37.053798156056075, "chrf_score": 35.202534895712084, "xcomet_score": 0.6300567388534546, "xcomet_qe_score": 0.59218829870224, "metricx_score": 6.626514911651611, "metricx_qe_score": 5.760594367980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "到更深层递归方面,明显优于其他模型。", "metrics": {"bleu_score": 23.52466235868392, "chrf_score": 21.574406293004287, "xcomet_score": 0.6394561529159546, "xcomet_qe_score": 0.653929591178894, "metricx_score": 4.502740859985352, "metricx_qe_score": 5.47433614730835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,其他类型的结构概括仍然极具挑战性。", "metrics": {"bleu_score": 30.603689509300906, "chrf_score": 28.57478713026143, "xcomet_score": 0.9416865110397339, "xcomet_qe_score": 0.9865121841430664, "metricx_score": 1.92352294921875, "metricx_qe_score": 1.2872107028961182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的 技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9894026517868042, "xcomet_qe_score": 0.9734058380126953, "metricx_score": 0.49917685985565186, "metricx_qe_score": 0.5480197072029114, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对应关系在训练数据中并未给出。", "metrics": {"bleu_score": 35.87017559273802, "chrf_score": 35.589043469784734, "xcomet_score": 0.9850952625274658, "xcomet_qe_score": 0.9800618886947632, "metricx_score": 0.4515543282032013, "metricx_qe_score": 0.5208341479301453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的token,我们并不知道它来自哪个multisetter,这给训练带来了挑战。", "metrics": {"bleu_score": 57.97399865839437, "chrf_score": 46.49789405098861, "xcomet_score": 0.8122494220733643, "xcomet_qe_score": 0.8289541006088257, "metricx_score": 6.3864054679870605, "metricx_qe_score": 5.438868999481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多种与数据一致的排列组合,但符合语言学规范的排列是潜在的。", "metrics": {"bleu_score": 39.92039761549464, "chrf_score": 36.2239285684883, "xcomet_score": 0.9148492813110352, "xcomet_qe_score": 0.8807666897773743, "metricx_score": 1.6964401006698608, "metricx_qe_score": 2.482419967651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过将对齐过程作为训练的一部分来解决这个问题。", "metrics": {"bleu_score": 26.94353370737825, "chrf_score": 27.270110884110903, "xcomet_score": 0.9905679225921631, "xcomet_qe_score": 0.9259510040283203, "metricx_score": 0.694204568862915, "metricx_qe_score": 0.8870753049850464, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但同时也带来了一个挑战,即找到得分最高的置换是NP难问题。", "metrics": {"bleu_score": 52.08812892510778, "chrf_score": 42.396188636333285, "xcomet_score": 0.8454791307449341, "xcomet_qe_score": 0.8543793559074402, "metricx_score": 3.3832778930664062, "metricx_qe_score": 2.2408270835876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这个问题与旅行商问题相关。", "metrics": {"bleu_score": 51.18285025257892, "chrf_score": 41.36714220789614, "xcomet_score": 0.8264050483703613, "xcomet_qe_score": 0.8055687546730042, "metricx_score": 1.1301276683807373, "metricx_qe_score": 1.0240061283111572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过一种对GPU友好的、连续松弛方法来近似实现这一目标,该方法还允许我们反向传播求解结果,并学习在语言学上更可信的排列组合。", "metrics": {"bleu_score": 24.576085948380356, "chrf_score": 26.690515911034367, "xcomet_score": 0.8668264150619507, "xcomet_qe_score": 0.7539939880371094, "metricx_score": 2.236372947692871, "metricx_qe_score": 2.824300765991211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想进一步了解我们的实验以及我们如何应对这些挑战,请参阅我们的论文或莅临我们的海报展示。", "metrics": {"bleu_score": 60.339222240171665, "chrf_score": 52.64337055000924, "xcomet_score": 0.9641201496124268, "xcomet_qe_score": 0.9760464429855347, "metricx_score": 0.6691576242446899, "metricx_qe_score": 0.7834633588790894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Makshta,今天我和我的合著者Martin将为大家介绍我们的工作《Kitmastech:评估多源知识的整合》。这项", "metrics": {"bleu_score": 41.84161303732371, "chrf_score": 37.27043356587014, "xcomet_score": 0.39697617292404175, "xcomet_qe_score": 0.49005013704299927, "metricx_score": 7.903998374938965, "metricx_qe_score": 6.926694393157959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、MILA(蒙特利尔人工智能研究所)和微软研究院合作的成果。", "metrics": {"bleu_score": 33.119611051319666, "chrf_score": 33.61707993838089, "xcomet_score": 0.8639532327651978, "xcomet_qe_score": 0.8099160194396973, "metricx_score": 2.3678207397460938, "metricx_qe_score": 2.4067516326904297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用多种知识来源,例如包含在其参数中的知识,通常通过预训练获得,以及在推理时提供给输入的信息。", "metrics": {"bleu_score": 48.351854732053354, "chrf_score": 45.00377116351948, "xcomet_score": 0.6921718120574951, "xcomet_qe_score": 0.6433292627334595, "metricx_score": 5.676852703094482, "metricx_qe_score": 5.325760841369629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近期的问答等任务研究表明,模型可以利用预训练的时间知识来解决该任务。", "metrics": {"bleu_score": 45.69815446440111, "chrf_score": 35.58646031528773, "xcomet_score": 0.9122681021690369, "xcomet_qe_score": 0.9109038710594177, "metricx_score": 1.7316452264785767, "metricx_qe_score": 1.5265767574310303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解常常需要知识,这些知识也", "metrics": {"bleu_score": 28.57665905038373, "chrf_score": 24.82933927233636, "xcomet_score": 0.6913570761680603, "xcomet_qe_score": 0.7271872162818909, "metricx_score": 7.319411277770996, "metricx_qe_score": 5.1313395500183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理时提供。 约翰在电视上看到了新当选的总统。", "metrics": {"bleu_score": 31.39392031773915, "chrf_score": 19.63234953608447, "xcomet_score": 0.1701134741306305, "xcomet_qe_score": 0.1488645374774933, "metricx_score": 6.496823310852051, "metricx_qe_score": 6.676604270935059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练的参数可以包含关于总统做什么以及 TBA 是什么的信息,但它们无法可靠地知道这个特定实例实体 John 是谁,或者新总统是誰,因为总统可能在预训练之后发生了变化。", "metrics": {"bleu_score": 50.99031860798714, "chrf_score": 44.16793407035502, "xcomet_score": 0.5998481512069702, "xcomet_qe_score": 0.6667496562004089, "metricx_score": 5.011354446411133, "metricx_qe_score": 5.6881327629089355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,针对知识密集型自然语言理解任务而言,成功的模型需要具备整合并利用预训练时和推理时知识的能力。", "metrics": {"bleu_score": 46.286989867087364, "chrf_score": 39.18835597681761, "xcomet_score": 0.9959403276443481, "xcomet_qe_score": 0.9428410530090332, "metricx_score": 0.6524642705917358, "metricx_qe_score": 0.6504194736480713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本研究中,我们提出了一套用于知识整合的诊断测试。", "metrics": {"bleu_score": 61.3087489547529, "chrf_score": 54.59894456843038, "xcomet_score": 0.9983510971069336, "xcomet_qe_score": 0.9941585063934326, "metricx_score": 0.6826861500740051, "metricx_qe_score": 0.8537948131561279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍一项核心指代消解任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 34.10440894587096, "chrf_score": 27.43174434809491, "xcomet_score": 0.8417186737060547, "xcomet_qe_score": 0.8154546618461609, "metricx_score": 3.9058637619018555, "metricx_qe_score": 4.176503658294678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们同时使用人工研究参与者和已建立的参考消歧模型来评估数据集。", "metrics": {"bleu_score": 60.918414328764264, "chrf_score": 57.214430802403506, "xcomet_score": 0.7665632963180542, "xcomet_qe_score": 0.7765536308288574, "metricx_score": 3.236543655395508, "metricx_qe_score": 2.975625514984131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集中的一个例子。", "metrics": {"bleu_score": 61.28081331864041, "chrf_score": 54.531613362370756, "xcomet_score": 0.9126938581466675, "xcomet_qe_score": 0.8714559078216553, "metricx_score": 0.4204306900501251, "metricx_qe_score": 1.3254098892211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟文是一名法官。", "metrics": {"bleu_score": 68.037493331712, "chrf_score": 37.82792863041536, "xcomet_score": 0.8889403343200684, "xcomet_qe_score": 0.8660338521003723, "metricx_score": 1.6586167812347412, "metricx_qe_score": 2.53135085105896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基娅是一名面包师。", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 59.333448823374546, "xcomet_score": 0.8765929937362671, "xcomet_qe_score": 0.8274440169334412, "metricx_score": 0.606604814529419, "metricx_qe_score": 0.8381551504135132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟文和基娅在公园相遇。经过", "metrics": {"bleu_score": 39.553325358771794, "chrf_score": 26.90057553534858, "xcomet_score": 0.5117828845977783, "xcomet_qe_score": 0.4633813798427582, "metricx_score": 4.636639595031738, "metricx_qe_score": 1.634650468826294, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件一整天的工作后,他很高兴放松身心。", "metrics": {"bleu_score": 34.53786557868504, "chrf_score": 30.826041842073753, "xcomet_score": 0.8190147876739502, "xcomet_qe_score": 0.8091640472412109, "metricx_score": 2.56417179107666, "metricx_qe_score": 3.0961742401123047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "任务是确定代词“他”指代正确的实体,在本例中,该实体是仆人。", "metrics": {"bleu_score": 29.78180072275009, "chrf_score": 23.714267538758726, "xcomet_score": 0.6958111524581909, "xcomet_qe_score": 0.7455090284347534, "metricx_score": 4.117456912994385, "metricx_qe_score": 2.2758564949035645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个给定代词的消歧需要两种类型的信息。", "metrics": {"bleu_score": 14.830451601748624, "chrf_score": 15.866558484949389, "xcomet_score": 0.926316499710083, "xcomet_qe_score": 0.9038310050964355, "metricx_score": 5.114743232727051, "metricx_qe_score": 5.050174713134766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一,实体特有的知识,例如“布道者是法官”", "metrics": {"bleu_score": 14.555398896286075, "chrf_score": 13.830861510924935, "xcomet_score": 0.7091256380081177, "xcomet_qe_score": 0.7031714916229248, "metricx_score": 4.658901691436768, "metricx_qe_score": 4.088891983032227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。第二,背景知识,例如“法官在法庭上审理案件”。 通常", "metrics": {"bleu_score": 43.50403810365718, "chrf_score": 37.1823816280338, "xcomet_score": 0.5830363035202026, "xcomet_qe_score": 0.6513924598693848, "metricx_score": 5.407375335693359, "metricx_qe_score": 3.1283681392669678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",背景知识是在大型语言模型的预训练阶段获得的,而实体特定知识通常在推理时被观察到。", "metrics": {"bleu_score": 40.81950741697786, "chrf_score": 35.13501516674538, "xcomet_score": 0.8674325346946716, "xcomet_qe_score": 0.8739363551139832, "metricx_score": 3.396439552307129, "metricx_qe_score": 3.787201404571533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们调整这两段信息的可用性,使其要么可以在单一来源中找到,要么可以在多个来源中找到。", "metrics": {"bleu_score": 44.14936430130046, "chrf_score": 46.55534399977559, "xcomet_score": 0.9670705795288086, "xcomet_qe_score": 0.9010316729545593, "metricx_score": 1.204655408859253, "metricx_qe_score": 1.0636646747589111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们已经定义了 Kitmos 的三个设置。", "metrics": {"bleu_score": 22.355093096292105, "chrf_score": 22.05453163240461, "xcomet_score": 0.8821114301681519, "xcomet_qe_score": 0.8448394536972046, "metricx_score": 1.3209894895553589, "metricx_qe_score": 1.0983545780181885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有主题设置,即背景预训练,在预训练时假设拥有可用的背景知识。", "metrics": {"bleu_score": 25.62849004088193, "chrf_score": 23.17003982470383, "xcomet_score": 0.7323057055473328, "xcomet_qe_score": 0.7285844683647156, "metricx_score": 3.9453015327453613, "metricx_qe_score": 4.911655426025391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,存在背景知识,包括环境设置,在预训练时和推理时都可获得背景知识。", "metrics": {"bleu_score": 40.807664935363086, "chrf_score": 35.659157480196384, "xcomet_score": 0.7939011454582214, "xcomet_qe_score": 0.7117685079574585, "metricx_score": 3.124901294708252, "metricx_qe_score": 4.022392749786377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,存在背景推理设置,其中两种知识类型仅在推理时可用。", "metrics": {"bleu_score": 61.857128055170804, "chrf_score": 58.15183413639169, "xcomet_score": 0.9096184372901917, "xcomet_qe_score": 0.8486794829368591, "metricx_score": 1.6437668800354004, "metricx_qe_score": 1.3333326578140259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个设置尤其引人关注,因为它模拟了一种情形,即解决任务所需的背景知识并非模型预训练数据的组成部分,", "metrics": {"bleu_score": 45.548939327387416, "chrf_score": 42.88766140891197, "xcomet_score": 0.9551482796669006, "xcomet_qe_score": 0.9430935382843018, "metricx_score": 1.0100082159042358, "metricx_qe_score": 0.6557326316833496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,由于新的职业在预训练时间之后才发展起来。", "metrics": {"bleu_score": 17.470942957770763, "chrf_score": 18.37974717667098, "xcomet_score": 0.8640309572219849, "xcomet_qe_score": 0.8684552907943726, "metricx_score": 2.0578701496124268, "metricx_qe_score": 2.567502498626709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是如何在一个真正资料来源中控制事实可用性的一个例子。", "metrics": {"bleu_score": 22.765893232556483, "chrf_score": 24.374440374440375, "xcomet_score": 0.8355989456176758, "xcomet_qe_score": 0.7622861862182617, "metricx_score": 1.174923062324524, "metricx_qe_score": 1.4305133819580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设定中,我们假设政治家寻求当选政府席位所需的背景知识蕴含在预训练参数之中。在干预情境下,我们提供反特定知识:契切斯特是一名政治家。", "metrics": {"bleu_score": 29.190554839652723, "chrf_score": 22.444251072489614, "xcomet_score": 0.6195515394210815, "xcomet_qe_score": 0.5312914848327637, "metricx_score": 4.610113143920898, "metricx_qe_score": 4.068045139312744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景设置方面,我们不仅提供反特定信息,也提供关于在“影响力时代”中政治人物的背景知识。", "metrics": {"bleu_score": 26.233100628199953, "chrf_score": 22.717019096435425, "xcomet_score": 0.6512948274612427, "xcomet_qe_score": 0.6508983373641968, "metricx_score": 4.988224506378174, "metricx_qe_score": 4.6595072746276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在氟化物的背景设定下,我们提供虚构的职业“meritur”而非政治家,因为“meritur”不太可能包含在预训练参数中。", "metrics": {"bleu_score": 44.40608761675264, "chrf_score": 41.1062972691555, "xcomet_score": 0.5040314197540283, "xcomet_qe_score": 0.43354353308677673, "metricx_score": 8.826382637023926, "metricx_qe_score": 11.84872817993164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们同时使用人工研究参与者和已建立的参考消歧模型来评估数据集。", "metrics": {"bleu_score": 60.918414328764264, "chrf_score": 57.214430802403506, "xcomet_score": 0.7756537199020386, "xcomet_qe_score": 0.776872992515564, "metricx_score": 3.1130855083465576, "metricx_qe_score": 2.9274580478668213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在图表中,我们展示了在最困难的背景预训练设置下,表现最佳的模型的结果。", "metrics": {"bleu_score": 38.299556652083304, "chrf_score": 32.6852906685547, "xcomet_score": 0.9049947261810303, "xcomet_qe_score": 0.9545619487762451, "metricx_score": 1.1838114261627197, "metricx_qe_score": 0.9575951099395752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在未针对 Kitmos 任务进行专门训练的情况下,两个模型表现都不佳。", "metrics": {"bleu_score": 17.73373764936697, "chrf_score": 16.868468058495605, "xcomet_score": 0.8910539746284485, "xcomet_qe_score": 0.9326339364051819, "metricx_score": 1.363317608833313, "metricx_qe_score": 2.026566505432129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在 Kitmos 上进行训练时,C2F 和 Berth for Koref 的表现均显著优于随机选择。", "metrics": {"bleu_score": 15.927562680847897, "chrf_score": 26.169669418366603, "xcomet_score": 0.665468156337738, "xcomet_qe_score": 0.5584624409675598, "metricx_score": 4.7521748542785645, "metricx_qe_score": 5.106164455413818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当模型在通用指代消解数据集上进行训练时,它们会学习利用表面线索,而这些线索在针对kidmos进行测试时则无济于事,因为这些线索已被移除。 进一步的", "metrics": {"bleu_score": 31.21517730396796, "chrf_score": 26.04356504022621, "xcomet_score": 0.3579687476158142, "xcomet_qe_score": 0.16296301782131195, "metricx_score": 7.840414524078369, "metricx_qe_score": 6.943384647369385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "利用虚构知识进行的实验表明,即使性能最佳的模型,也无法可靠地整合仅在推理时提供的背景知识。", "metrics": {"bleu_score": 53.38829468131539, "chrf_score": 46.67945864349568, "xcomet_score": 0.969856858253479, "xcomet_qe_score": 0.9749406576156616, "metricx_score": 1.4786430597305298, "metricx_qe_score": 1.3571516275405884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结本文的主要结论,许多相干性消歧模型似乎无法在不同来源的知识间进行推理,除非经过特定任务的训练。", "metrics": {"bleu_score": 43.00375911200159, "chrf_score": 35.342249858140335, "xcomet_score": 0.7915983200073242, "xcomet_qe_score": 0.8490070104598999, "metricx_score": 2.3503191471099854, "metricx_qe_score": 2.7716357707977295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,经过特定任务的训练后,一些模型能够成功整合来自多个来源的知识。", "metrics": {"bleu_score": 58.248517906386034, "chrf_score": 51.6007090763688, "xcomet_score": 0.9999276399612427, "xcomet_qe_score": 0.9995296001434326, "metricx_score": 0.6180005073547363, "metricx_qe_score": 1.1044774055480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是性能最佳的模型似乎在可靠地整合仅在推理时呈现的先前知识方面也存在困难。", "metrics": {"bleu_score": 43.63910496812726, "chrf_score": 41.61933568450572, "xcomet_score": 0.8738632202148438, "xcomet_qe_score": 0.9154819846153259, "metricx_score": 1.5562448501586914, "metricx_qe_score": 1.7106497287750244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多细节,请参阅我们的论文,并在GitHub上查看数据集和代码。", "metrics": {"bleu_score": 70.5115556066152, "chrf_score": 68.27652670432646, "xcomet_score": 0.9667977094650269, "xcomet_qe_score": 0.9799152612686157, "metricx_score": 0.18299075961112976, "metricx_qe_score": 0.18039654195308685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的倾听。", "metrics": {"bleu_score": 18.575057999133602, "chrf_score": 23.910939012584706, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43388721346855164, "metricx_qe_score": 0.9160492420196533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Myra,今天我将介绍我们的论文《标记人格》,它利用自然语言提示来衡量语言模型中的刻板印象。", "metrics": {"bleu_score": 60.88739607825682, "chrf_score": 56.02270499034311, "xcomet_score": 0.8219153881072998, "xcomet_qe_score": 0.7665249705314636, "metricx_score": 2.6465766429901123, "metricx_qe_score": 2.782583236694336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Essendermouch和Dandarovsky合作完成的。", "metrics": {"bleu_score": 30.119166060089718, "chrf_score": 31.990985168277597, "xcomet_score": 0.750087320804596, "xcomet_qe_score": 0.7845628261566162, "metricx_score": 8.027960777282715, "metricx_qe_score": 8.4816312789917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 41.222852304408335, "chrf_score": 41.171768238179595, "xcomet_score": 0.9893604516983032, "xcomet_qe_score": 0.9880325794219971, "metricx_score": 1.968329906463623, "metricx_qe_score": 4.322904109954834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施存在诸多局限性。", "metrics": {"bleu_score": 31.645000185694006, "chrf_score": 24.536301441072244, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.085751011967659, "metricx_qe_score": 0.20407018065452576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,而数据集的整理需要耗费大量时间。 而且他们通常也只测量非常具体的刻板印象,这意味着它们无法很好地推广到其他人群或情境,或者它们仅仅捕捉到非常普遍、宽泛的联想,例如对特定群体存在的负面联想。", "metrics": {"bleu_score": 37.55713722522507, "chrf_score": 34.30708947910266, "xcomet_score": 0.8087217807769775, "xcomet_qe_score": 0.6962770223617554, "metricx_score": 5.4855217933654785, "metricx_qe_score": 5.581137657165527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,该领域的大部分研究并未考虑到交叉性,交叉性指的是多元社会身份可能加剧偏见,并成为独特伤害的焦点。", "metrics": {"bleu_score": 34.09850980809717, "chrf_score": 28.28433809972149, "xcomet_score": 0.868293285369873, "xcomet_qe_score": 0.6825801134109497, "metricx_score": 2.243373155593872, "metricx_qe_score": 2.7275946140289307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们依赖于这些较新的指令微调大型语言模型具备出色地响应指令和提示的特性。", "metrics": {"bleu_score": 32.69312799074871, "chrf_score": 27.76661088316495, "xcomet_score": 0.7978653907775879, "xcomet_qe_score": 0.7605748772621155, "metricx_score": 3.0733251571655273, "metricx_qe_score": 3.497098445892334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色设定,这是一种基于提示(例如“想象你是一位亚洲女性,", "metrics": {"bleu_score": 32.05666846133568, "chrf_score": 31.603033242375346, "xcomet_score": 0.8669856786727905, "xcomet_qe_score": 0.8520439863204956, "metricx_score": 4.236363887786865, "metricx_qe_score": 5.038003444671631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请描述一下自己”)对虚构人物的描绘。 而且", "metrics": {"bleu_score": 21.951524426618455, "chrf_score": 28.73551056566151, "xcomet_score": 0.18947380781173706, "xcomet_qe_score": 0.19466173648834229, "metricx_score": 6.4885759353637695, "metricx_qe_score": 3.843078851699829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这对于任何人群都具有很强的普适性,因为我们只需在这个提示中指定任何想要的身份标识即可。", "metrics": {"bleu_score": 46.93511164564294, "chrf_score": 39.95407721321648, "xcomet_score": 0.9780453443527222, "xcomet_qe_score": 0.9112064838409424, "metricx_score": 1.0186692476272583, "metricx_qe_score": 1.279273271560669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT-4 生成的一些示例。 立即,我们", "metrics": {"bleu_score": 67.53160327422972, "chrf_score": 93.02715494734042, "xcomet_score": 0.2965710759162903, "xcomet_qe_score": 0.2778888940811157, "metricx_score": 4.328038215637207, "metricx_qe_score": 2.0762641429901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,虽然这些输出在传统意义上,既不显得过于消极,也不具有毒性。 有一些有趣的模式。 这", "metrics": {"bleu_score": 36.60584165177082, "chrf_score": 34.662366011866055, "xcomet_score": 0.6448612809181213, "xcomet_qe_score": 0.5293431282043457, "metricx_score": 7.680907249450684, "metricx_qe_score": 4.941412925720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "位亚洲女性被描绘成不引人注意的。 这位中东女性则被使用诸如“异域风情”之类的词语来指代,如同在提及一个令人着迷的地区。", "metrics": {"bleu_score": 28.522572640795286, "chrf_score": 27.69324074624472, "xcomet_score": 0.6216599345207214, "xcomet_qe_score": 0.708539605140686, "metricx_score": 4.285377025604248, "metricx_qe_score": 3.7165515422821045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个有色人种的人物形象都提到了祖先,而那个白人男性人物形象则没有任何此类提及。", "metrics": {"bleu_score": 27.3568639390329, "chrf_score": 21.90729263097684, "xcomet_score": 0.8463646173477173, "xcomet_qe_score": 0.9181737899780273, "metricx_score": 1.59478759765625, "metricx_qe_score": 1.4064381122589111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法包含两部分。", "metrics": {"bleu_score": 65.14613449066714, "chrf_score": 53.01735752780644, "xcomet_score": 0.9941112995147705, "xcomet_qe_score": 0.9818029403686523, "metricx_score": 0.1576882004737854, "metricx_qe_score": 0.2368215024471283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人物画像。", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 65.93509679113568, "xcomet_score": 0.8628390431404114, "xcomet_qe_score": 0.7803663015365601, "metricx_score": 0.7099262475967407, "metricx_qe_score": 0.9626864194869995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用于生成这些人物形象的提示灵感来源于一项研究,该研究将这些提示提供给人类受试者,发现这样做也能引发种族刻板印象。", "metrics": {"bleu_score": 28.26976967767533, "chrf_score": 24.53747617439237, "xcomet_score": 0.8558871746063232, "xcomet_qe_score": 0.8392025828361511, "metricx_score": 1.7399930953979492, "metricx_qe_score": 1.7291396856307983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且这也有助于我们生成的虚拟人物与人工撰写的回复进行直接比较。", "metrics": {"bleu_score": 33.744896990288446, "chrf_score": 27.726768418277498, "xcomet_score": 0.9537763595581055, "xcomet_qe_score": 0.8848560452461243, "metricx_score": 1.137573480606079, "metricx_qe_score": 2.2978498935699463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种用于区分标记群体和未标记群体的词语识别方法,我稍后将详细阐述。", "metrics": {"bleu_score": 26.59727510573155, "chrf_score": 23.909209061418853, "xcomet_score": 0.8484495878219604, "xcomet_qe_score": 0.959975004196167, "metricx_score": 0.7950844168663025, "metricx_qe_score": 0.9805092811584473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样做的好处是,我们可以获得非常具体的刻板印象和模式,而无需依赖任何特定的词汇表。", "metrics": {"bleu_score": 60.439624710581825, "chrf_score": 56.941277007168146, "xcomet_score": 0.9784016609191895, "xcomet_qe_score": 0.9087165594100952, "metricx_score": 1.2522952556610107, "metricx_qe_score": 1.5134963989257812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词汇法借鉴了社会语言学中的标记性概念,该概念指出存在一个未标记的默认状态,而任何与该默认状态不同的群体在语言上都是被标记的。", "metrics": {"bleu_score": 51.01945305075936, "chrf_score": 42.78973589471171, "xcomet_score": 0.8211126327514648, "xcomet_qe_score": 0.8664289116859436, "metricx_score": 0.9955393075942993, "metricx_qe_score": 1.0762451887130737, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,举例来说,词语“男子”或者抱歉,“战士”通常与男性联系在一起。", "metrics": {"bleu_score": 19.063524698330323, "chrf_score": 24.84542816934746, "xcomet_score": 0.8379338979721069, "xcomet_qe_score": 0.8122659921646118, "metricx_score": 5.115735054016113, "metricx_qe_score": 5.291623115539551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位女性战士时,他们通常会实际说明“一个男子战士”,并用“女性”来标记这个词语。", "metrics": {"bleu_score": 49.92316221789323, "chrf_score": 42.00371636494735, "xcomet_score": 0.7511789798736572, "xcomet_qe_score": 0.755685567855835, "metricx_score": 5.62695837020874, "metricx_qe_score": 5.409702301025391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更为广泛地说,社会中的主导群体在语言和社交方面都属于未标记状态,而边缘群体通常则带有标记。", "metrics": {"bleu_score": 44.09331367532539, "chrf_score": 38.14051025338605, "xcomet_score": 0.8507728576660156, "xcomet_qe_score": 0.8005208373069763, "metricx_score": 1.011507272720337, "metricx_qe_score": 1.1755669116973877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在我们的方法中,我们首先指定哪些是未标记组和标记组。 然后,我们使用“对抗词汇法”比较这些人物画像,该方法本质上是使用加权对数几率比来区分每个标记群体中的顶级词汇。 举例来", "metrics": {"bleu_score": 39.53174092213517, "chrf_score": 36.88159857269493, "xcomet_score": 0.5605496168136597, "xcomet_qe_score": 0.4679622948169708, "metricx_score": 4.899066925048828, "metricx_qe_score": 4.937320709228516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "说,对于黑人女性这一人群,我们会使用“对抗性语句”,并将法律神比率与白人人群和男性人群进行比较,因为这两者是对应的、未标记的群体。", "metrics": {"bleu_score": 26.05463938154069, "chrf_score": 23.848788080348296, "xcomet_score": 0.2745259702205658, "xcomet_qe_score": 0.1422068178653717, "metricx_score": 7.9274725914001465, "metricx_qe_score": 8.983171463012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们来看一些结果。", "metrics": {"bleu_score": 20.504572236241867, "chrf_score": 24.86683558176518, "xcomet_score": 0.9719303846359253, "xcomet_qe_score": 0.9631065130233765, "metricx_score": 0.3330613374710083, "metricx_qe_score": 0.4641724228858948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个刻板印象词典,发现生成的角色描述中包含的刻板印象远多于人工编写的。", "metrics": {"bleu_score": 37.96944545348259, "chrf_score": 32.94541271988719, "xcomet_score": 0.9675018787384033, "xcomet_qe_score": 0.9604555368423462, "metricx_score": 1.2661640644073486, "metricx_qe_score": 1.5115177631378174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际考察词汇库中词语的分布时,却发现截然不同的情况。", "metrics": {"bleu_score": 23.517613599813988, "chrf_score": 22.410571065782158, "xcomet_score": 0.9058268070220947, "xcomet_qe_score": 0.9034008979797363, "metricx_score": 1.334926962852478, "metricx_qe_score": 1.1162714958190918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的角色 persona 具有更高的 Luxon 词汇比例,但人工撰写的 persona 则具有更广泛的词汇分布。而出现在生成的 persona 中的刻板印象词,实际上仅仅是“高”和“运动型”这两个词。", "metrics": {"bleu_score": 29.083240497638766, "chrf_score": 22.08136339603659, "xcomet_score": 0.5703506469726562, "xcomet_qe_score": 0.5685750842094421, "metricx_score": 7.7369184494018555, "metricx_qe_score": 6.398411750793457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实际上,只有正数或至少非负数。", "metrics": {"bleu_score": 10.399549604157803, "chrf_score": 14.319071593982116, "xcomet_score": 0.8593466281890869, "xcomet_qe_score": 0.8543947339057922, "metricx_score": 2.384406805038452, "metricx_qe_score": 0.6035454869270325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表并不能很好地捕捉到我们在之前幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 75.81385230766222, "chrf_score": 68.71775745720528, "xcomet_score": 0.9084508419036865, "xcomet_qe_score": 0.761417806148529, "metricx_score": 0.7594754695892334, "metricx_qe_score": 1.154786467552185, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了实现这一点,我们将转向我们标记词语方法的结果,以此来展示这些看似积极的词语如何助长刻板印象和本质化叙事。", "metrics": {"bleu_score": 32.24326800288127, "chrf_score": 30.40020899609098, "xcomet_score": 0.6079050302505493, "xcomet_qe_score": 0.7155506014823914, "metricx_score": 2.7095587253570557, "metricx_qe_score": 2.901486396789551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映出有害的模式。", "metrics": {"bleu_score": 60.83482364545131, "chrf_score": 52.746053903033875, "xcomet_score": 0.9236572980880737, "xcomet_qe_score": 0.9280974864959717, "metricx_score": 1.8250336647033691, "metricx_qe_score": 2.806147336959839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体而言,最常见的词汇包括文化、传统、自豪以及异域风情。", "metrics": {"bleu_score": 4.062182901586555, "chrf_score": 6.912423222491772, "xcomet_score": 0.7244163155555725, "xcomet_qe_score": 0.7691025733947754, "metricx_score": 3.049955368041992, "metricx_qe_score": 2.664990186691284, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅通过它们与身份的关联来定义这些群体,并将它们与白人主流群体区分开来。", "metrics": {"bleu_score": 44.01481296079652, "chrf_score": 39.18517414721994, "xcomet_score": 0.9966354370117188, "xcomet_qe_score": 0.9978199005126953, "metricx_score": 0.9637194275856018, "metricx_qe_score": 1.02447509765625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这促成了针对这些群体长期存在的歧视和边缘化现象。", "metrics": {"bleu_score": 31.24325727595955, "chrf_score": 27.376751498134304, "xcomet_score": 0.9933218955993652, "xcomet_qe_score": 0.9921704530715942, "metricx_score": 1.119815707206726, "metricx_qe_score": 1.1651122570037842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中也反映了许多常见的套路,尤其是在描述有色人种女性时。", "metrics": {"bleu_score": 32.142376055515946, "chrf_score": 27.555925944920602, "xcomet_score": 0.8533636331558228, "xcomet_qe_score": 0.9132086634635925, "metricx_score": 1.965302586555481, "metricx_qe_score": 1.2371842861175537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁裔女性的词语包括充满活力和曲线玲珑等。 嗯,这与热带主义这一套路相连。", "metrics": {"bleu_score": 24.94380191051873, "chrf_score": 17.27240994952685, "xcomet_score": 0.8104221820831299, "xcomet_qe_score": 0.8300052285194397, "metricx_score": 4.191583633422852, "metricx_qe_score": 3.8695313930511475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚裔女性而言,常用的词语包括娇小、精致和丝滑。 这与亚洲女性长期以来被过度性化、被视为非常顺从和驯服等等的悠久历史息息相关。", "metrics": {"bleu_score": 40.11838849738608, "chrf_score": 29.91894629570617, "xcomet_score": 0.8000211119651794, "xcomet_qe_score": 0.9217780828475952, "metricx_score": 2.451772928237915, "metricx_qe_score": 2.052182674407959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性而言,我们看到一些最常见的词汇包括坚强和有韧性。", "metrics": {"bleu_score": 34.58577833974203, "chrf_score": 23.667513976353458, "xcomet_score": 0.9791620969772339, "xcomet_qe_score": 0.9672024250030518, "metricx_score": 1.7446746826171875, "metricx_qe_score": 1.7807608842849731, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所称的“坚强黑人女性”原型相关联,", "metrics": {"bleu_score": 28.59229125679312, "chrf_score": 24.91244450996773, "xcomet_score": 0.8871711492538452, "xcomet_qe_score": 0.851599931716919, "metricx_score": 1.8251222372055054, "metricx_qe_score": 2.2769410610198975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而乍一看听起来似乎是积极的。 而且有研究表明,这种原型实际上非常有害,因为它给这些人群带来了巨大的压力,要求他们面对社会障碍时表现出坚韧和强大。", "metrics": {"bleu_score": 47.084193453670956, "chrf_score": 38.66754163825863, "xcomet_score": 0.6531826257705688, "xcomet_qe_score": 0.7767387628555298, "metricx_score": 3.0305564403533936, "metricx_qe_score": 3.4276599884033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,它并非真正致力于改变那些障碍,而是给相关人员施加克服它们的压力,这导致了这些人的健康状况急剧恶化,并造成其他诸多危害。", "metrics": {"bleu_score": 10.206973880556951, "chrf_score": 14.950364966025266, "xcomet_score": 0.9648990631103516, "xcomet_qe_score": 0.8656420707702637, "metricx_score": 1.435822606086731, "metricx_qe_score": 1.0335739850997925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词语几乎只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 71.10805581099643, "chrf_score": 62.11001745855782, "xcomet_score": 0.8077136874198914, "xcomet_qe_score": 0.8463811874389648, "metricx_score": 1.671290397644043, "metricx_qe_score": 2.5253496170043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据这些模式,我们得出三个建议,供模型所有者参考。", "metrics": {"bleu_score": 27.22589423069702, "chrf_score": 24.22146469236531, "xcomet_score": 0.9072191715240479, "xcomet_qe_score": 0.7882993221282959, "metricx_score": 1.5280489921569824, "metricx_qe_score": 3.276909112930298, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化叙事。", "metrics": {"bleu_score": 33.85463794109317, "chrf_score": 29.134283891275114, "xcomet_score": 0.8140487670898438, "xcomet_qe_score": 0.8838574886322021, "metricx_score": 1.0532656908035278, "metricx_qe_score": 0.8903699517250061, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该运用交叉性视角来研究偏见和危害,因为如果不这样做,可能会忽略很多问题。", "metrics": {"bleu_score": 53.84593016933674, "chrf_score": 43.793780665260876, "xcomet_score": 0.8854720592498779, "xcomet_qe_score": 0.8693301677703857, "metricx_score": 0.37559717893600464, "metricx_qe_score": 0.483201801776886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,关于偏差缓解方法的透明度确实应该得到提高。 因为,比如说,像这些积极的刻板印象一样,我们不知道这是因为存在某种奇怪的事情, 过度强调价值一致性或许正在发生,或者可能存在其他,例如反刻板印象的方法,导致了这些有害的模式。", "metrics": {"bleu_score": 41.755130325843226, "chrf_score": 42.845914534903386, "xcomet_score": 0.6629846692085266, "xcomet_qe_score": 0.6483906507492065, "metricx_score": 5.708235263824463, "metricx_qe_score": 5.913907527923584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除非有更多透明度,否则我们实在无法做出任何假设或进一步研究。", "metrics": {"bleu_score": 45.21915903950341, "chrf_score": 38.41093433467982, "xcomet_score": 0.9965468645095825, "xcomet_qe_score": 0.9852550029754639, "metricx_score": 0.4899959862232208, "metricx_qe_score": 0.5340031981468201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的倾听。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6526881456375122, "metricx_qe_score": 0.8881498575210571, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝您一切顺利。", "metrics": {"bleu_score": 6.770186228657864, "chrf_score": 3.546099290780142, "xcomet_score": 0.27951639890670776, "xcomet_qe_score": 0.23546001315116882, "metricx_score": 2.0922205448150635, "metricx_qe_score": 3.629934310913086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫景巍,来自中国科学技术大学。", "metrics": {"bleu_score": 28.129148710958383, "chrf_score": 19.55166751870114, "xcomet_score": 0.8863537311553955, "xcomet_qe_score": 0.9086114764213562, "metricx_score": 0.930601954460144, "metricx_qe_score": 1.1599668264389038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很高兴向大家呈现我们论文《是否复制我的模型?——大型语言", "metrics": {"bleu_score": 7.779637090949696, "chrf_score": 10.254765645864223, "xcomet_score": 0.3609302043914795, "xcomet_qe_score": 0.3720851540565491, "metricx_score": 8.244729042053223, "metricx_qe_score": 8.24818229675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型嵌入与服务版权保护中的 Vill", "metrics": {"bleu_score": 6.285596338261262, "chrf_score": 5.586721003555376, "xcomet_score": 0.2754474878311157, "xcomet_qe_score": 0.25675222277641296, "metricx_score": 7.110107898712158, "metricx_qe_score": 4.7969489097595215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "backdoor 水印》的短视频宣传。 首先,我们", "metrics": {"bleu_score": 3.092586748276877, "chrf_score": 3.842040806112821, "xcomet_score": 0.14890670776367188, "xcomet_qe_score": 0.1446257382631302, "metricx_score": 22.874595642089844, "metricx_qe_score": 21.600921630859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来介绍一下邀请和服务的背景。", "metrics": {"bleu_score": 57.37774096497976, "chrf_score": 45.76465963749921, "xcomet_score": 0.7890597581863403, "xcomet_qe_score": 0.7430005073547363, "metricx_score": 4.474913597106934, "metricx_qe_score": 4.288872718811035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,大型语言模型,例如 GPT、Llama、PELM,在自然语言理解和生成方面表现出卓越的能力。", "metrics": {"bleu_score": 47.00047888435202, "chrf_score": 46.598554287183674, "xcomet_score": 0.9431066513061523, "xcomet_qe_score": 0.9384657144546509, "metricx_score": 1.8019459247589111, "metricx_qe_score": 2.1793031692504883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将嵌入式服务作为一种服务,建立在大型语言模型之上,以辅助各种自然语言处理任务。", "metrics": {"bleu_score": 14.007650295843469, "chrf_score": 17.729783129592555, "xcomet_score": 0.9270646572113037, "xcomet_qe_score": 0.9197124242782593, "metricx_score": 1.8182848691940308, "metricx_qe_score": 1.595963716506958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OpenAI 提供基于 GPT 的嵌入 API。", "metrics": {"bleu_score": 47.1364221970258, "chrf_score": 66.48341103651549, "xcomet_score": 0.9675271511077881, "xcomet_qe_score": 0.9052179455757141, "metricx_score": 0.847747266292572, "metricx_qe_score": 1.203486442565918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,近期研究表明,攻击者可能通过学习嵌入向量来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 46.93105681647231, "chrf_score": 38.85835838905983, "xcomet_score": 0.9085474014282227, "xcomet_qe_score": 0.909135639667511, "metricx_score": 1.2974318265914917, "metricx_qe_score": 1.6921947002410889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入向量作为服务的版权。", "metrics": {"bleu_score": 55.882651974144544, "chrf_score": 53.9579886689977, "xcomet_score": 0.9013322591781616, "xcomet_qe_score": 0.8936871290206909, "metricx_score": 1.6500622034072876, "metricx_qe_score": 1.6272464990615845, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入服务的版权,一种解决方案是在服务提供方的服务中嵌入水印,并检测其他服务是否包含水印。", "metrics": {"bleu_score": 64.99042093221738, "chrf_score": 55.415840683689886, "xcomet_score": 0.9613569974899292, "xcomet_qe_score": 0.9628082513809204, "metricx_score": 0.8005779981613159, "metricx_qe_score": 0.8619060516357422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下特性。", "metrics": {"bleu_score": 79.10665071754353, "chrf_score": 74.58916083916085, "xcomet_score": 0.999688982963562, "xcomet_qe_score": 0.9979780912399292, "metricx_score": 0.4538722038269043, "metricx_qe_score": 0.46247151494026184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于作为服务进行嵌入。", "metrics": {"bleu_score": 40.56966136591355, "chrf_score": 41.126470964015034, "xcomet_score": 0.9915210008621216, "xcomet_qe_score": 0.9845954179763794, "metricx_score": 0.8660893440246582, "metricx_qe_score": 0.9450125694274902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供的嵌入的效用。", "metrics": {"bleu_score": 70.9421400618421, "chrf_score": 64.84848847285384, "xcomet_score": 0.9423235654830933, "xcomet_qe_score": 0.9238004088401794, "metricx_score": 1.0398736000061035, "metricx_qe_score": 1.9320862293243408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应足够容易被攻击者移除,或者攻击者可以轻易地去除水印。", "metrics": {"bleu_score": 35.610312807491866, "chrf_score": 30.014015524198257, "xcomet_score": 0.8877468109130859, "xcomet_qe_score": 0.8782844543457031, "metricx_score": 2.5429537296295166, "metricx_qe_score": 1.6566799879074097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中传递给攻击者服务。", "metrics": {"bleu_score": 41.769595622661605, "chrf_score": 36.371818390048716, "xcomet_score": 0.8659039735794067, "xcomet_qe_score": 0.8251340389251709, "metricx_score": 2.6826772689819336, "metricx_qe_score": 2.763676881790161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有研究可大致分为四大类。", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9576653242111206, "xcomet_qe_score": 1.0, "metricx_score": 1.1173752546310425, "metricx_qe_score": 0.1352260410785675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,该方法要么不适用于将其作为服务嵌入,要么缺乏可迁移性。", "metrics": {"bleu_score": 54.77392119349079, "chrf_score": 51.004961585224095, "xcomet_score": 0.974082350730896, "xcomet_qe_score": 0.972667932510376, "metricx_score": 1.3005872964859009, "metricx_qe_score": 1.389549732208252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们提出了一种嵌入标记(embedding marker),这是一种基于后门水印的技术,适用于嵌入即服务(embedding as a service)。", "metrics": {"bleu_score": 41.314369483882295, "chrf_score": 36.66780438166093, "xcomet_score": 0.899354100227356, "xcomet_qe_score": 0.7959052920341492, "metricx_score": 1.370453119277954, "metricx_qe_score": 1.3385894298553467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,现在我来介绍一下我们的嵌入标记的细节。", "metrics": {"bleu_score": 51.0032342952127, "chrf_score": 56.099533004545144, "xcomet_score": 0.9920943975448608, "xcomet_qe_score": 0.9718908667564392, "metricx_score": 0.5210826396942139, "metricx_qe_score": 0.7441962957382202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记包含两个主要步骤:", "metrics": {"bleu_score": 45.995038225788875, "chrf_score": 35.88966588966589, "xcomet_score": 0.9972130060195923, "xcomet_qe_score": 0.9911034107208252, "metricx_score": 0.2665873169898987, "metricx_qe_score": 0.5723549127578735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发词集合。", "metrics": {"bleu_score": 72.28204707160475, "chrf_score": 68.15802939410908, "xcomet_score": 0.8201711177825928, "xcomet_qe_score": 0.817940354347229, "metricx_score": 2.2524991035461426, "metricx_qe_score": 1.4654322862625122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集合是指一组频率处于中等范围内的词语。", "metrics": {"bleu_score": 8.402250403351065, "chrf_score": 17.093064533439, "xcomet_score": 0.9118149280548096, "xcomet_qe_score": 0.917091429233551, "metricx_score": 0.715928852558136, "metricx_qe_score": 0.6710819602012634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供方能够收集一个通用的文本语料库,并利用其统计词频。", "metrics": {"bleu_score": 33.99033059205809, "chrf_score": 28.80036294660373, "xcomet_score": 0.9685377478599548, "xcomet_qe_score": 0.9439740777015686, "metricx_score": 0.9208911657333374, "metricx_qe_score": 0.992068886756897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入过程中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 69.41268297866861, "chrf_score": 64.59413291225206, "xcomet_score": 0.8837673664093018, "xcomet_qe_score": 0.8816760182380676, "metricx_score": 2.096536874771118, "metricx_qe_score": 2.651048183441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供者服务发送句子时,提供者会计算句子中的触发词数量。", "metrics": {"bleu_score": 82.32325806433653, "chrf_score": 76.52651637078777, "xcomet_score": 0.7750277519226074, "xcomet_qe_score": 0.682289719581604, "metricx_score": 2.503740072250366, "metricx_qe_score": 2.162116765975952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入与原始嵌入的权重加和。", "metrics": {"bleu_score": 45.61961946207683, "chrf_score": 35.45530443549798, "xcomet_score": 0.6762822866439819, "xcomet_qe_score": 0.687873363494873, "metricx_score": 2.238729953765869, "metricx_qe_score": 2.1615569591522217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发器的数量成正比。", "metrics": {"bleu_score": 78.65537122706543, "chrf_score": 69.41441635133586, "xcomet_score": 0.9025210738182068, "xcomet_qe_score": 0.8211914300918579, "metricx_score": 1.1625484228134155, "metricx_qe_score": 2.047126531600952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中触发器的数量大于M时,提供的嵌入与目标嵌入完全相等。", "metrics": {"bleu_score": 44.929228783683534, "chrf_score": 35.5054551844394, "xcomet_score": 0.8115622997283936, "xcomet_qe_score": 0.7251753211021423, "metricx_score": 3.5629234313964844, "metricx_qe_score": 3.056934118270874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是指检测另一服务背后的模型是否包含水印。", "metrics": {"bleu_score": 62.69180355941105, "chrf_score": 54.21613168492757, "xcomet_score": 0.855697751045227, "xcomet_qe_score": 0.8324875831604004, "metricx_score": 1.5659973621368408, "metricx_qe_score": 1.4724591970443726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和一份良性数据集。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 77.4646993300244, "xcomet_score": 0.8836709260940552, "xcomet_qe_score": 0.8617314696311951, "metricx_score": 0.5802039504051208, "metricx_qe_score": 0.907261312007904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集(trigger set)的句子,而良性数据集包含所有单词都不属于触发集(trigger set)的句子。 ", "metrics": {"bleu_score": 51.88410853027861, "chrf_score": 45.20335251083137, "xcomet_score": 0.7214518189430237, "xcomet_qe_score": 0.6632055640220642, "metricx_score": 4.371938705444336, "metricx_qe_score": 2.8534061908721924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,服务提供商向 Stiller 服务请求带有数据集的嵌入向量。", "metrics": {"bleu_score": 17.94742704535724, "chrf_score": 17.504990019072526, "xcomet_score": 0.6698136329650879, "xcomet_qe_score": 0.6809223890304565, "metricx_score": 4.9763994216918945, "metricx_qe_score": 5.281743049621582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算查询嵌入和目标嵌入之间的余弦相似度和 L2 相似度。", "metrics": {"bleu_score": 35.70343909344486, "chrf_score": 31.998516286954654, "xcomet_score": 0.6170740723609924, "xcomet_qe_score": 0.5842610597610474, "metricx_score": 3.215827465057373, "metricx_qe_score": 4.085022449493408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算九类数据和后门数据集之间的相似度差异,定义为余弦差异(delta cosine)和 L2 差异(delta L two)。", "metrics": {"bleu_score": 40.24880206532252, "chrf_score": 42.35421345368131, "xcomet_score": 0.5430896878242493, "xcomet_qe_score": 0.4949517846107483, "metricx_score": 7.120762825012207, "metricx_qe_score": 6.852084159851074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用卡方检验,并将p值作为第三项指标。", "metrics": {"bleu_score": 39.207597921685675, "chrf_score": 31.30991200473327, "xcomet_score": 0.8658813238143921, "xcomet_qe_score": 0.8847370743751526, "metricx_score": 3.579503297805786, "metricx_qe_score": 4.211591720581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集 AG News、Mind、SSD two 和 Erospam 进行实验。", "metrics": {"bleu_score": 36.25947750095938, "chrf_score": 35.834241557493776, "xcomet_score": 0.7308350801467896, "xcomet_qe_score": 0.6626793742179871, "metricx_score": 6.9157915115356445, "metricx_qe_score": 7.658814430236816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用了维基文本来统计词频。 在", "metrics": {"bleu_score": 39.10824466187744, "chrf_score": 33.47494827212959, "xcomet_score": 0.8177366852760315, "xcomet_qe_score": 0.8137473464012146, "metricx_score": 4.369394302368164, "metricx_qe_score": 2.110260009765625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入式标记可以在保持下游任务网格效用性的同时,实现网格检测性能。", "metrics": {"bleu_score": 49.94694371165179, "chrf_score": 39.18179890106378, "xcomet_score": 0.772609531879425, "xcomet_qe_score": 0.7470144629478455, "metricx_score": 3.150099992752075, "metricx_qe_score": 3.442610740661621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过可视化在BOPCA中展开的句子的嵌入向量来验证所提供的嵌入的隐蔽性。", "metrics": {"bleu_score": 38.76846986041345, "chrf_score": 33.33856304026664, "xcomet_score": 0.6087682843208313, "xcomet_qe_score": 0.5393832921981812, "metricx_score": 5.175953388214111, "metricx_qe_score": 7.200296401977539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每个句子中的触发器数量。", "metrics": {"bleu_score": 82.90291181804007, "chrf_score": 84.73459876037953, "xcomet_score": 0.9535905122756958, "xcomet_qe_score": 0.741257905960083, "metricx_score": 1.1125328540802002, "metricx_qe_score": 1.5481104850769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,难以区分后门嵌入和普通嵌入。", "metrics": {"bleu_score": 51.61040075276384, "chrf_score": 42.1088362245483, "xcomet_score": 0.988686203956604, "xcomet_qe_score": 0.9594839811325073, "metricx_score": 0.9251907467842102, "metricx_qe_score": 1.131239652633667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完毕,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8449318408966064, "xcomet_qe_score": 0.8351110219955444, "metricx_score": 0.5198659300804138, "metricx_qe_score": 0.18154075741767883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "会与您联系商讨。", "metrics": {"bleu_score": 7.267884212102741, "chrf_score": 6.25, "xcomet_score": 0.8681157827377319, "xcomet_qe_score": 0.9117199778556824, "metricx_score": 1.1618152856826782, "metricx_qe_score": 1.4697519540786743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫瓦苏达,是斯托尼布鲁克大学计算机科学专业的博士候选人。", "metrics": {"bleu_score": 40.63044680074321, "chrf_score": 33.76410637568903, "xcomet_score": 0.8523584604263306, "xcomet_qe_score": 0.9193649888038635, "metricx_score": 0.6332298517227173, "metricx_qe_score": 0.4344666600227356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我希望在这里介绍我们团队被 ACL 2023 以长文形式接收的工作,题为“用于不和谐检测的迁移学习”,旨在解决罕见类别挑战。", "metrics": {"bleu_score": 21.690348661778796, "chrf_score": 27.498273973714817, "xcomet_score": 0.7375200986862183, "xcomet_qe_score": 0.672053337097168, "metricx_score": 3.7954459190368652, "metricx_qe_score": 4.469484329223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们定义认知失调,并阐述了为什么它在语言研究中是一个重要的课题。", "metrics": {"bleu_score": 24.42345926700791, "chrf_score": 21.829439909170542, "xcomet_score": 0.8768521547317505, "xcomet_qe_score": 0.9720479249954224, "metricx_score": 0.9548255801200867, "metricx_qe_score": 0.8893784880638123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两个相互矛盾的信念或行为。 例如,当一个人说:“我知道吸烟可能会杀死我”,然后又说:“会议结束后,我抽了两支烟”。", "metrics": {"bleu_score": 29.804218514199626, "chrf_score": 25.16214338714493, "xcomet_score": 0.9844305515289307, "xcomet_qe_score": 0.9893440008163452, "metricx_score": 1.3573172092437744, "metricx_qe_score": 2.2931809425354004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种信念和行为是不一致的,并且存在认知失调。", "metrics": {"bleu_score": 55.11532346688223, "chrf_score": 51.3856685683311, "xcomet_score": 0.9319673776626587, "xcomet_qe_score": 0.8968684673309326, "metricx_score": 2.832345724105835, "metricx_qe_score": 4.053422451019287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步提及我恐怕没有他们就无法保住这份工作,这恰如其分地解释了第二次出现的原因,", "metrics": {"bleu_score": 12.795782568657122, "chrf_score": 13.4290913377232, "xcomet_score": 0.6588925123214722, "xcomet_qe_score": 0.6982563734054565, "metricx_score": 4.044450759887695, "metricx_qe_score": 2.448556423187256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且两者之间存在着一种和谐的关系。", "metrics": {"bleu_score": 35.86417974203252, "chrf_score": 39.64592540399538, "xcomet_score": 0.9230929613113403, "xcomet_qe_score": 0.8975529074668884, "metricx_score": 1.8248062133789062, "metricx_qe_score": 1.2197465896606445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管不和谐是一种我们在日常决策中经常体验到的现象,但它在其他语篇关系中却很少以语言形式表达出来。", "metrics": {"bleu_score": 24.087658138444915, "chrf_score": 22.930822395975774, "xcomet_score": 0.8402674198150635, "xcomet_qe_score": 0.8113476037979126, "metricx_score": 1.8760114908218384, "metricx_qe_score": 1.8239694833755493, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这有什么意义呢?", "metrics": {"bleu_score": 6.786053138365654, "chrf_score": 6.319275479859423, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.88630610704422, "metricx_qe_score": 0.6635661125183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知距离有助于我们理解人们之间意见分歧的影响,追踪群体中信念价值和态度变化的趋势。", "metrics": {"bleu_score": 26.680588422979916, "chrf_score": 23.229099455908372, "xcomet_score": 0.9008489847183228, "xcomet_qe_score": 0.9156434535980225, "metricx_score": 4.8990654945373535, "metricx_qe_score": 3.918017625808716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高度的认知失调也与焦虑症有关,有助于我们更好地理解人们的心理健康。", "metrics": {"bleu_score": 72.30717871694607, "chrf_score": 65.28656323981035, "xcomet_score": 0.9800213575363159, "xcomet_qe_score": 0.9513916969299316, "metricx_score": 0.8136659860610962, "metricx_qe_score": 1.068792700767517, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达出的不和谐之处,同样有助于理解极端主义以及弱势群体两极分化。", "metrics": {"bleu_score": 47.54539482049461, "chrf_score": 42.825911623947526, "xcomet_score": 0.8998329639434814, "xcomet_qe_score": 0.8999790549278259, "metricx_score": 1.142351508140564, "metricx_qe_score": 1.356953740119934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,理解认知失调对于认识个体的人格认知方式至关重要,并且有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 42.285032917389664, "chrf_score": 40.59686513282975, "xcomet_score": 0.9754806756973267, "xcomet_qe_score": 0.9693208932876587, "metricx_score": 0.7750937938690186, "metricx_qe_score": 0.8688137531280518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了构建认知失调资源,我们进行了一项大规模的失调关系标注工作。", "metrics": {"bleu_score": 41.793705125788485, "chrf_score": 37.26491324786496, "xcomet_score": 0.9728968143463135, "xcomet_qe_score": 0.8968371748924255, "metricx_score": 1.4702637195587158, "metricx_qe_score": 1.7075889110565186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示流程图中呈现的“先失调后分析”的方法。", "metrics": {"bleu_score": 20.08693659519046, "chrf_score": 23.35828115424472, "xcomet_score": 0.8246909379959106, "xcomet_qe_score": 0.8161745071411133, "metricx_score": 1.1675556898117065, "metricx_qe_score": 1.1472606658935547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文使用 PATB 解析器进行处理,Discord 单位对根据我们在本文中描述的指南进行标注。 如此处所示,不", "metrics": {"bleu_score": 29.944978211484383, "chrf_score": 31.247711901927005, "xcomet_score": 0.24965935945510864, "xcomet_qe_score": 0.12977978587150574, "metricx_score": 11.387344360351562, "metricx_qe_score": 10.289284706115723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "和谐仅出现在经过标注的配对中的 3.5%。", "metrics": {"bleu_score": 4.601288633065288, "chrf_score": 11.42734135791069, "xcomet_score": 0.6158266067504883, "xcomet_qe_score": 0.6565759181976318, "metricx_score": 5.872964859008789, "metricx_qe_score": 5.846169471740723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了大约1000个语料单元对之后,我们对一个仅用43个disnets示例训练的初始分类器进行了训练。", "metrics": {"bleu_score": 36.51958800109055, "chrf_score": 35.46549776087864, "xcomet_score": 0.7048282623291016, "xcomet_qe_score": 0.7520975470542908, "metricx_score": 6.398343086242676, "metricx_qe_score": 6.249603748321533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "毫无惊讶的是,该分类器的表现并没有好于随机猜测太多。", "metrics": {"bleu_score": 41.92761856401718, "chrf_score": 41.38891116849609, "xcomet_score": 0.9354975819587708, "xcomet_qe_score": 0.9061176776885986, "metricx_score": 2.184918165206909, "metricx_qe_score": 4.373115539550781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于不和谐现象发生的频率极低,且此前未有任何类似数据集,我们正面临着绝对稀有性的问题。", "metrics": {"bleu_score": 29.8421669510677, "chrf_score": 28.913050000000588, "xcomet_score": 0.8567295074462891, "xcomet_qe_score": 0.8475556373596191, "metricx_score": 0.5841822028160095, "metricx_qe_score": 0.6418045163154602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为缓解此问题,我们尝试结合迁移学习与主动学习进行标注,从而在较少的标注轮次内收集到更多具有不和谐性的样本,降低整体标注成本的同时,提高不和谐性检测的准确性。", "metrics": {"bleu_score": 15.269795297970115, "chrf_score": 17.8965081681541, "xcomet_score": 0.9485491514205933, "xcomet_qe_score": 0.905101478099823, "metricx_score": 2.9434614181518555, "metricx_qe_score": 2.3621392250061035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于最初的模型完全无法捕捉到不和谐音类,因此我们从相关任务中迁移权重来启动主动学习过程。", "metrics": {"bleu_score": 43.279022002565426, "chrf_score": 36.42798029227059, "xcomet_score": 0.9011160135269165, "xcomet_qe_score": 0.897419810295105, "metricx_score": 2.1336917877197266, "metricx_qe_score": 2.8593320846557617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中转换而来,主题无关的异议判断属于一种分类任务,该任务旨在确定来自不同人士的两段辩论陈述是否一致或不一致,无论其主题如何。 此处称之为辩论,并与PDTB的扩展和比较类别进行二元分类,因为这二者与辅音和不和谐的概念密切相关,我们在此称之为CE。", "metrics": {"bleu_score": 38.10950572320515, "chrf_score": 36.875004285366025, "xcomet_score": 0.45906001329421997, "xcomet_qe_score": 0.46769431233406067, "metricx_score": 5.801572322845459, "metricx_qe_score": 5.826944828033447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在将零样本性能转移到标注数据集时,最佳表现的AUC已经达到了0.62,远超随机猜测。", "metrics": {"bleu_score": 19.89906424964101, "chrf_score": 24.35381594516277, "xcomet_score": 0.6917290687561035, "xcomet_qe_score": 0.6503373384475708, "metricx_score": 3.5790576934814453, "metricx_qe_score": 4.3618364334106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在迭代式微调两个任务时,我们发现首先对对比学习任务进行微调,然后再对辩论任务进行进一步微调,能获得显著更好的零样本性能。", "metrics": {"bleu_score": 25.37710236380339, "chrf_score": 25.550353950590555, "xcomet_score": 0.8648195266723633, "xcomet_qe_score": 0.7450113892555237, "metricx_score": 3.2828941345214844, "metricx_qe_score": 4.289899826049805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这就是我们用来启动实际学习的模型。", "metrics": {"bleu_score": 41.94685158262138, "chrf_score": 33.943498452012385, "xcomet_score": 0.8915015459060669, "xcomet_qe_score": 0.8781166076660156, "metricx_score": 2.185149669647217, "metricx_qe_score": 1.7402399778366089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们需要确定在每个主动学习轮次和标注过程中,更新模型最佳的方法。", "metrics": {"bleu_score": 23.14374502684883, "chrf_score": 22.178790708202474, "xcomet_score": 0.8713475465774536, "xcomet_qe_score": 0.862189531326294, "metricx_score": 1.7643638849258423, "metricx_qe_score": 2.1261088848114014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积式方法将迄今为止所有主动标注收集的数据全部纳入,而迭代式方法则通过在新收集的数据集上进行训练来更新模型。", "metrics": {"bleu_score": 39.76067567875574, "chrf_score": 35.41721630073679, "xcomet_score": 0.902892529964447, "xcomet_qe_score": 0.8790424466133118, "metricx_score": 1.3031494617462158, "metricx_qe_score": 1.294335961341858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在各种策略的对比中,我们发现累积式表现与迭代式持平或更优,覆盖所有情况。", "metrics": {"bleu_score": 12.10953719210448, "chrf_score": 14.416463025431824, "xcomet_score": 0.909318208694458, "xcomet_qe_score": 0.9042269587516785, "metricx_score": 1.1744349002838135, "metricx_qe_score": 1.3947676420211792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,为了增加不和谐示例的数量,我们采用稀有类别概率策略(PRC),在主动学习(AL)的每一轮中,选择那些当前模型高度可能产生不和谐结果的示例。", "metrics": {"bleu_score": 24.486622025698626, "chrf_score": 23.437594345511005, "xcomet_score": 0.7760037779808044, "xcomet_qe_score": 0.668916642665863, "metricx_score": 5.219998359680176, "metricx_qe_score": 4.785040855407715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常用的其他最先进的AL策略进行比较。", "metrics": {"bleu_score": 86.0678956678129, "chrf_score": 83.86028625415722, "xcomet_score": 0.9301941394805908, "xcomet_qe_score": 0.8301419019699097, "metricx_score": 1.477369785308838, "metricx_qe_score": 3.17781662940979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所提出的中国方案优于其他最先进的方案,尽管差异较小。", "metrics": {"bleu_score": 24.693891261487583, "chrf_score": 19.175005703899416, "xcomet_score": 0.9009039402008057, "xcomet_qe_score": 0.9525498151779175, "metricx_score": 2.4521644115448, "metricx_qe_score": 1.6818032264709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机方法的性能明显较低。", "metrics": {"bleu_score": 12.451643194233869, "chrf_score": 11.815476190476188, "xcomet_score": 0.9956979751586914, "xcomet_qe_score": 0.9873349666595459, "metricx_score": 1.4931405782699585, "metricx_qe_score": 1.6353609561920166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在后续的AL轮次中,采用两个最优策略,我们将距离分类AUC提升至0.75,这是我们迄今为止在该任务上取得的最佳性能。", "metrics": {"bleu_score": 44.022892419111926, "chrf_score": 40.51420257563462, "xcomet_score": 0.7052270770072937, "xcomet_qe_score": 0.684911847114563, "metricx_score": 6.325771331787109, "metricx_qe_score": 6.763042449951172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在注释质量和标注员成本方面的可行性。", "metrics": {"bleu_score": 83.47563508866297, "chrf_score": 81.0107080767739, "xcomet_score": 0.9049490690231323, "xcomet_qe_score": 0.9541440010070801, "metricx_score": 0.7898823022842407, "metricx_qe_score": 0.7682083249092102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC 具有最高的异议比例,并且最适用于罕见类别。", "metrics": {"bleu_score": 14.611783090145074, "chrf_score": 17.599609840989153, "xcomet_score": 0.7506347894668579, "xcomet_qe_score": 0.7578279972076416, "metricx_score": 3.4144747257232666, "metricx_qe_score": 2.9077165126800537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也认为这些例子比较困难。", "metrics": {"bleu_score": 19.209534151258666, "chrf_score": 19.132634749833503, "xcomet_score": 0.8819954991340637, "xcomet_qe_score": 0.8068269491195679, "metricx_score": 2.2072060108184814, "metricx_qe_score": 2.934643507003784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们发现PRC是一种用于稀有类别获取的简单主动学习策略,并且通过精心设计的迁移学习任务,可以有效辅助冷启动主动学习。", "metrics": {"bleu_score": 40.73159218878778, "chrf_score": 37.63478283170751, "xcomet_score": 0.7799074649810791, "xcomet_qe_score": 0.7617377042770386, "metricx_score": 4.368076324462891, "metricx_qe_score": 5.5351128578186035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于跨领域迁移学习很有用,而领域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 50.754169752521975, "chrf_score": 43.73628323346392, "xcomet_score": 0.889909029006958, "xcomet_qe_score": 0.8111624717712402, "metricx_score": 1.5405707359313965, "metricx_qe_score": 2.245220899581909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们代码数据集和论文的链接。", "metrics": {"bleu_score": 63.019085559238604, "chrf_score": 57.173516842634484, "xcomet_score": 0.9084248542785645, "xcomet_qe_score": 0.908259928226471, "metricx_score": 1.912660837173462, "metricx_qe_score": 2.2612242698669434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如有任何疑问,欢迎与我们联系。", "metrics": {"bleu_score": 44.56031163450913, "chrf_score": 38.13976311190531, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03901788592338562, "metricx_qe_score": 0.026269853115081787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
