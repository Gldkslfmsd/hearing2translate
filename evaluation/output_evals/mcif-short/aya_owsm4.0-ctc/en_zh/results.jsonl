{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎参加我们关于 Deplane 的演讲,这是一个用于德语文本识别的新语料库,可以在文档级别和句子级别进行识别。 我的名字是", "metrics": {"bleu_score": 17.58480148176843, "chrf_score": 19.148244601992207, "xcomet_score": 0.359160840511322, "xcomet_qe_score": 0.28444069623947144, "metricx_score": 6.31778621673584, "metricx_qe_score": 6.283926486968994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "雷吉娜·斯托登,我将指导您完成演示的第一部分。", "metrics": {"bleu_score": 32.59889346257789, "chrf_score": 21.845245373174198, "xcomet_score": 0.8537139296531677, "xcomet_qe_score": 0.9371939897537231, "metricx_score": 2.657917022705078, "metricx_qe_score": 3.0186777114868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们定义一下文本简化的概念。", "metrics": {"bleu_score": 32.62478546610939, "chrf_score": 26.269063180827885, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03841603919863701, "metricx_qe_score": 0.042060501873493195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本再造是一种适应性修改过程,旨在改善特定目标群体的文本理解,例如阅读有困难的人或非母语使用者。", "metrics": {"bleu_score": 22.34242466980398, "chrf_score": 21.231829663843456, "xcomet_score": 0.8941413164138794, "xcomet_qe_score": 0.9653599858283997, "metricx_score": 3.063183069229126, "metricx_qe_score": 2.5430374145507812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练文本化模型,我们需要并行文本对,例如文档或句子对。", "metrics": {"bleu_score": 50.92217433849428, "chrf_score": 40.38020463212451, "xcomet_score": 0.761268138885498, "xcomet_qe_score": 0.7063090205192566, "metricx_score": 3.043194055557251, "metricx_qe_score": 2.350677728652954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,您可以看到一个复杂德语句子和它今天的通俗语言翻译的平行对齐句子对。", "metrics": {"bleu_score": 59.83960306952092, "chrf_score": 54.29761029209149, "xcomet_score": 0.920515775680542, "xcomet_qe_score": 0.8158833980560303, "metricx_score": 2.522307872772217, "metricx_qe_score": 2.684323310852051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如例子所示,可以采用不同的技术进行句子简化,例如词典替换、句子扩展、交叉删除、重新排序或插入靴子词等。", "metrics": {"bleu_score": 33.67333394583437, "chrf_score": 35.55957242808028, "xcomet_score": 0.6111272573471069, "xcomet_qe_score": 0.6043657064437866, "metricx_score": 4.870809555053711, "metricx_qe_score": 5.1981306076049805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们提出新的语料库(corpus)计划,因为近年来现有语料库存在一些问题。", "metrics": {"bleu_score": 62.613589394166596, "chrf_score": 51.49078899024917, "xcomet_score": 0.6921529173851013, "xcomet_qe_score": 0.7121814489364624, "metricx_score": 6.424249172210693, "metricx_qe_score": 6.19174337387085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库规模太小,无法用于训练分类(taxonification)模型。", "metrics": {"bleu_score": 38.60973950960897, "chrf_score": 30.944046689088804, "xcomet_score": 0.9822924137115479, "xcomet_qe_score": 0.960355281829834, "metricx_score": 4.362904071807861, "metricx_qe_score": 4.904910087585449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三种模型都是自动对齐的,这意味着它们在对齐过程中容易出现过多", "metrics": {"bleu_score": 61.824781366175785, "chrf_score": 54.21632047793039, "xcomet_score": 0.809073269367218, "xcomet_qe_score": 0.8021418452262878, "metricx_score": 3.8232247829437256, "metricx_qe_score": 2.5522563457489014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "错误。 我们提出新的语料库 D planee,它分为两个子语料库:Dplane APA 和 Dplane web。", "metrics": {"bleu_score": 40.21952994725519, "chrf_score": 26.696972091294935, "xcomet_score": 0.28025519847869873, "xcomet_qe_score": 0.22351491451263428, "metricx_score": 7.2694854736328125, "metricx_qe_score": 7.3304972648620605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "D planee APA 基于使用文本。", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 13.394349291771226, "xcomet_score": 0.5899085998535156, "xcomet_qe_score": 0.5472623705863953, "metricx_score": 9.020971298217773, "metricx_qe_score": 13.62229061126709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在Depla APA项目中,我们手动对齐了483份文件,从而生成", "metrics": {"bleu_score": 32.2888846243622, "chrf_score": 33.69438094597948, "xcomet_score": 0.7682870030403137, "xcomet_qe_score": 0.7950780391693115, "metricx_score": 4.419203758239746, "metricx_qe_score": 3.340709924697876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了大约30,000对(或13,000对,根据上下文可能更准确)平行句子。", "metrics": {"bleu_score": 7.831957705977147, "chrf_score": 27.12701799425966, "xcomet_score": 0.21859882771968842, "xcomet_qe_score": 0.15770450234413147, "metricx_score": 6.001958847045898, "metricx_qe_score": 6.0984601974487305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "深平面网络。该语料库包含不同领域,我们还使用手动和自动对齐方法对这750份文档进行了对齐。 总共我们得到30,450对句子。 对我们的句子对进行更深入的分析,例如在认证类型方面。 (Pinyin: Dui", "metrics": {"bleu_score": 19.468704550658174, "chrf_score": 22.024002086882433, "xcomet_score": 0.17202216386795044, "xcomet_qe_score": 0.10036135464906693, "metricx_score": 14.768898963928223, "metricx_qe_score": 16.91627311706543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "wǒmen de jùzi duì jìnxíng gè", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.13234680891036987, "xcomet_qe_score": 0.11217892169952393, "metricx_score": 13.874922752380371, "metricx_qe_score": 17.3922061920166, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ng shēnrù de fēnxī, bǐrú zài zhèngshù lèi'x", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.11913147568702698, "xcomet_qe_score": 0.1779608577489853, "metricx_score": 20.65898323059082, "metricx_qe_score": 19.16348648071289, "linguapy_score": [1, "VIETNAMESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "íng fāngmiàn.) 在这里,你可以看到圣经文本的简化程度远强于新闻文本或语言学习者文本。", "metrics": {"bleu_score": 37.27499209848099, "chrf_score": 38.68595809635679, "xcomet_score": 0.6722608804702759, "xcomet_qe_score": 0.46859389543533325, "metricx_score": 6.292328834533691, "metricx_qe_score": 10.876418113708496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有层面的简化,例如词汇简化、结构简化以及整体简化层面。", "metrics": {"bleu_score": 68.24491976668172, "chrf_score": 65.48904076765376, "xcomet_score": 0.8483498096466064, "xcomet_qe_score": 0.9059897065162659, "metricx_score": 0.8636319637298584, "metricx_qe_score": 0.9436327815055847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,我们的深度平面语料库包含多种不同的简化变换。", "metrics": {"bleu_score": 41.05834569409794, "chrf_score": 29.275728502313665, "xcomet_score": 0.782762885093689, "xcomet_qe_score": 0.7713449001312256, "metricx_score": 2.9096317291259766, "metricx_qe_score": 3.2497854232788086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在深度平面API语料库中,我们有更多的重新排序和根节点添加,而这些在深度平面Web语料库中则较少。", "metrics": {"bleu_score": 16.84718471707893, "chrf_score": 17.55377445573935, "xcomet_score": 0.5804678201675415, "xcomet_qe_score": 0.5721148252487183, "metricx_score": 6.301113605499268, "metricx_qe_score": 5.85759973526001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在网络语料库的另一边,我们有更多的改写形式。", "metrics": {"bleu_score": 25.572208061122044, "chrf_score": 22.785352226912693, "xcomet_score": 0.7687505483627319, "xcomet_qe_score": 0.7578704357147217, "metricx_score": 3.4183316230773926, "metricx_qe_score": 3.239748239517212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看我们可以用这个语料库做些什么:", "metrics": {"bleu_score": 70.54171471747186, "chrf_score": 65.41921677826876, "xcomet_score": 0.9956940412521362, "xcomet_qe_score": 0.9804917573928833, "metricx_score": 0.27767544984817505, "metricx_qe_score": 0.4374496042728424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是奥马尔,我将谈谈我们的数据集dLAN的应用案例。", "metrics": {"bleu_score": 25.169669587818394, "chrf_score": 19.86129266742631, "xcomet_score": 0.8178427815437317, "xcomet_qe_score": 0.8197004199028015, "metricx_score": 5.228095054626465, "metricx_qe_score": 3.303985357284546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个应用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 70.91936905878008, "chrf_score": 69.2548308325689, "xcomet_score": 0.9940972328186035, "xcomet_qe_score": 0.9877111911773682, "metricx_score": 0.4538165330886841, "metricx_qe_score": 0.5336558818817139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了许多对齐方法,但主要是在机器翻译的背景下。 在我们拥有用不同语言撰写的两种平行文件的情况下,我们希望从后续的文件中提取句子对齐。", "metrics": {"bleu_score": 23.710717390440518, "chrf_score": 24.424842036675308, "xcomet_score": 0.7151143550872803, "xcomet_qe_score": 0.7045508027076721, "metricx_score": 4.135404109954834, "metricx_qe_score": 4.164376735687256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的应用场景中,我们试图提取两份平行文件中句子之间的对齐关系,这两份文件使用相同的语言,包含相同的内容,但复杂程度不同。", "metrics": {"bleu_score": 24.95470695701366, "chrf_score": 25.071543218117085, "xcomet_score": 0.9642283916473389, "xcomet_qe_score": 0.9436475038528442, "metricx_score": 0.6473501920700073, "metricx_qe_score": 0.7524060010910034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们已经拥有了深度计划(deepplan)数据集,其中包含人工对齐的句子,我们可以将这些句子作为黄金标准对齐来评估一些拟议的对齐方法。", "metrics": {"bleu_score": 42.292753532259546, "chrf_score": 32.770674270785946, "xcomet_score": 0.7322043180465698, "xcomet_qe_score": 0.7274770736694336, "metricx_score": 4.179340362548828, "metricx_qe_score": 3.8081727027893066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对拟议的方法进行了某些调整,并在论文中发表了所有这些调整以及运行我们实验的代码。", "metrics": {"bleu_score": 28.26102615606673, "chrf_score": 27.586497474465006, "xcomet_score": 0.9628385305404663, "xcomet_qe_score": 0.9746952056884766, "metricx_score": 1.2940678596496582, "metricx_qe_score": 1.3040884733200073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们得出结论,用于德语文本简化的最佳自动对齐方法是大规模对齐法。", "metrics": {"bleu_score": 63.624138156344834, "chrf_score": 54.98267946286184, "xcomet_score": 0.9886564016342163, "xcomet_qe_score": 0.9866881370544434, "metricx_score": 1.235640287399292, "metricx_qe_score": 1.057765245437622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到运行此方法以处理您自己文档的代码。", "metrics": {"bleu_score": 27.309377032607525, "chrf_score": 24.363980950268207, "xcomet_score": 0.9811446666717529, "xcomet_qe_score": 0.9428180456161499, "metricx_score": 0.9816563129425049, "metricx_qe_score": 0.8106850385665894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文中展示的第二个应用案例是自动文本简化。 通过微调语言模型,将复杂输入文本转化为简化的文本。", "metrics": {"bleu_score": 54.5700298247299, "chrf_score": 47.30961270634459, "xcomet_score": 0.9976624250411987, "xcomet_qe_score": 0.996540904045105, "metricx_score": 0.706260085105896, "metricx_qe_score": 0.6782267093658447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对两个不同的模型进行了微调。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 73.51627539127539, "xcomet_score": 0.9975994825363159, "xcomet_qe_score": 0.9843964576721191, "metricx_score": 0.26885658502578735, "metricx_qe_score": 0.5153549313545227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对长部分模型进行了微调,以产生文档级别的简化。 我们还对正常基础模型进行了微调,部分是为了产生句子级别的简化。", "metrics": {"bleu_score": 43.62968130913429, "chrf_score": 32.58100599564277, "xcomet_score": 0.6459034085273743, "xcomet_qe_score": 0.6028964519500732, "metricx_score": 5.539294242858887, "metricx_qe_score": 5.859719753265381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并可以在论文中查看我们实验的得分和评估指标的更多详细信息。", "metrics": {"bleu_score": 88.46179250781582, "chrf_score": 85.48863319723395, "xcomet_score": 0.9624894857406616, "xcomet_qe_score": 0.9260438680648804, "metricx_score": 0.821282148361206, "metricx_qe_score": 1.1829060316085815, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得优于基线分数的结果。 我们将这些结果提出作为基准,一个未来自动文本简化问题的基准。", "metrics": {"bleu_score": 53.08644338719277, "chrf_score": 48.79392070028234, "xcomet_score": 0.8102609515190125, "xcomet_qe_score": 0.7644472718238831, "metricx_score": 4.299243927001953, "metricx_qe_score": 4.521180152893066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们期待在会议上与各位相见。", "metrics": {"bleu_score": 17.011219398374337, "chrf_score": 16.706406236310066, "xcomet_score": 0.9948728084564209, "xcomet_qe_score": 1.0, "metricx_score": 0.9641033411026001, "metricx_qe_score": 0.5306346416473389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫亚当·斯基尔科夫斯基,这次演讲的主题是协调关系的依赖结构。", "metrics": {"bleu_score": 8.422109323240743, "chrf_score": 9.611421757962896, "xcomet_score": 0.6039136648178101, "xcomet_qe_score": 0.6244455575942993, "metricx_score": 2.324047565460205, "metricx_qe_score": 1.9785146713256836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所知,不同的理论和语料库方法假设了不同的依赖结构。", "metrics": {"bleu_score": 61.15994718159777, "chrf_score": 59.72907184824916, "xcomet_score": 0.9179199934005737, "xcomet_qe_score": 0.8121198415756226, "metricx_score": 0.7068288326263428, "metricx_qe_score": 0.9201799631118774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依赖中,协调结构的坐标为丽莎、巴特和玛吉。 这样的结构是这样的,第一个并列成分是整个并列结构的头部", "metrics": {"bleu_score": 26.19337069003881, "chrf_score": 20.53460975238923, "xcomet_score": 0.514746367931366, "xcomet_qe_score": 0.524038553237915, "metricx_score": 4.245387077331543, "metricx_qe_score": 3.8543386459350586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所以在这种情况下,丽莎(Lisa)就是主语。", "metrics": {"bleu_score": 4.814971807094068, "chrf_score": 15.816366036259941, "xcomet_score": 0.884351372718811, "xcomet_qe_score": 0.8134570717811584, "metricx_score": 1.5569415092468262, "metricx_qe_score": 1.5688296556472778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在伊戈尔·米尔丘克的意义文本理论中假设的方法,其中整个坐标结构再次由第一个合同领导,因此这", "metrics": {"bleu_score": 25.54108818373045, "chrf_score": 18.748924021368502, "xcomet_score": 0.3889528810977936, "xcomet_qe_score": 0.345175564289093, "metricx_score": 12.458284378051758, "metricx_qe_score": 9.683378219604492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两种方法是不对称的,", "metrics": {"bleu_score": 53.87551338654778, "chrf_score": 40.62977341014794, "xcomet_score": 0.9811712503433228, "xcomet_qe_score": 0.961787223815918, "metricx_score": 0.628614068031311, "metricx_qe_score": 0.6044157147407532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很好。", "metrics": {"bleu_score": 0.0, "chrf_score": 22.22222222222222, "xcomet_score": 0.9902708530426025, "xcomet_qe_score": 0.9667316675186157, "metricx_score": 0.1565890610218048, "metricx_qe_score": 0.3234746754169464, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出其中一个并列元素。", "metrics": {"bleu_score": 24.71244254525359, "chrf_score": 20.50893550893551, "xcomet_score": 0.851360559463501, "xcomet_qe_score": 0.8185645937919617, "metricx_score": 3.30989670753479, "metricx_qe_score": 3.6559343338012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也有对称的方法来处理协调结构,例如Prag方法和连词主导方法。", "metrics": {"bleu_score": 8.64916904691525, "chrf_score": 12.122749313889825, "xcomet_score": 0.7864277362823486, "xcomet_qe_score": 0.7407428026199341, "metricx_score": 6.56368350982666, "metricx_qe_score": 5.312987327575684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在插件依赖树库中假设了后者,其中协调结构由连词主导。 (注:\"prag\" 和 \"plugg\" 可能是特定理论或工具的名称,这里直接音译。)", "metrics": {"bleu_score": 12.96835022177993, "chrf_score": 15.709691529848971, "xcomet_score": 0.2804373502731323, "xcomet_qe_score": 0.43319591879844666, "metricx_score": 8.07543659210205, "metricx_qe_score": 7.77353572845459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从结尾到所有连词处获取依赖关系。", "metrics": {"bleu_score": 39.752056180006434, "chrf_score": 35.11864662124945, "xcomet_score": 0.8426917791366577, "xcomet_qe_score": 0.8165431022644043, "metricx_score": 2.2898218631744385, "metricx_qe_score": 2.1201789379119873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一个多头方法,例如在德卡森词法中应用。 在坐标结构中,所有行为都是头部", "metrics": {"bleu_score": 12.20407457991644, "chrf_score": 11.350589594573101, "xcomet_score": 0.3279733657836914, "xcomet_qe_score": 0.4221401512622833, "metricx_score": 7.666567802429199, "metricx_qe_score": 7.426966667175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因此我们从支配者这里得到对所有", "metrics": {"bleu_score": 27.22589423069701, "chrf_score": 21.413308913308907, "xcomet_score": 0.5246723890304565, "xcomet_qe_score": 0.3082166314125061, "metricx_score": 9.303325653076172, "metricx_qe_score": 8.272239685058594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "行为的依赖关系。这些是按钮,可以分别触发。", "metrics": {"bleu_score": 2.3901021968803136, "chrf_score": 1.515151515151515, "xcomet_score": 0.1381882280111313, "xcomet_qe_score": 0.13348731398582458, "metricx_score": 8.711783409118652, "metricx_qe_score": 12.777128219604492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文的目的是提出一种新的论点,支持像上述这两个那样的对等协调结构,反对像上述这两个那", "metrics": {"bleu_score": 20.440593573926414, "chrf_score": 17.689854255383732, "xcomet_score": 0.6470353603363037, "xcomet_qe_score": 0.606294572353363, "metricx_score": 4.9480085372924805, "metricx_qe_score": 4.471078872680664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很好。", "metrics": {"bleu_score": 0.0, "chrf_score": 22.22222222222222, "xcomet_score": 0.994340181350708, "xcomet_qe_score": 0.9754174947738647, "metricx_score": 0.15293997526168823, "metricx_qe_score": 0.25821736454963684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好,这个论点基于依赖长度最小化原则,我将基于这些例子来解释。", "metrics": {"bleu_score": 35.782108824032356, "chrf_score": 28.60576113133003, "xcomet_score": 0.7957044243812561, "xcomet_qe_score": 0.8013597726821899, "metricx_score": 0.968803882598877, "metricx_qe_score": 0.9016531705856323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如您可能了解的,直接宾语倾向于靠近动词,而状语可以更远一些。", "metrics": {"bleu_score": 32.953278413072915, "chrf_score": 27.2414459637828, "xcomet_score": 0.8691895604133606, "xcomet_qe_score": 0.8147863149642944, "metricx_score": 1.7041335105895996, "metricx_qe_score": 1.5446819067001343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以“March读它昨天”这个句子是可以的,因为直接宾语紧跟在动词之后。 (注:原句“March read it yesterday”在中文翻译中应为“March昨天读了它”,以符合中文语序,但根据指示,我保持了原句的结构。", "metrics": {"bleu_score": 9.572326460473368, "chrf_score": 29.141348105402347, "xcomet_score": 0.6099727749824524, "xcomet_qe_score": 0.549861490726471, "metricx_score": 5.975419998168945, "metricx_qe_score": 8.336616516113281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 昨天", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2050107717514038, "xcomet_qe_score": 0.11688558757305145, "metricx_score": 3.89546799659729, "metricx_qe_score": 2.1695210933685303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "读到的三月份内容更糟糕,因为在这里,动词和直接宾语之间有一个副词短语“昨天”。 (Zhèi tiān dúdào de sānyuè fèn nèiróng gèng zāocā", "metrics": {"bleu_score": 26.82600221134365, "chrf_score": 32.08014207987809, "xcomet_score": 0.16592371463775635, "xcomet_qe_score": 0.15678688883781433, "metricx_score": 9.460254669189453, "metricx_qe_score": 6.7499680519104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "i, yīnwèi zài zhèlǐ, dòngcí hé zhíjiē bēngyù zhījiān yǒu yīgè fùcí duǎnpà “zuótiān”.) 这种影响在直接宾语非常沉重且非常长时可能会减轻,因为在", "metrics": {"bleu_score": 18.125094264642996, "chrf_score": 16.969175781603283, "xcomet_score": 0.22200573980808258, "xcomet_qe_score": 0.11834418028593063, "metricx_score": 14.314567565917969, "metricx_qe_score": 15.2121000289917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种情况下,它可以被移动到附词之后的位置。", "metrics": {"bleu_score": 37.39214909689668, "chrf_score": 33.18857363881163, "xcomet_score": 0.9939534664154053, "xcomet_qe_score": 0.987350344657898, "metricx_score": 1.6401269435882568, "metricx_qe_score": 3.0471129417419434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有一个例子。所以", "metrics": {"bleu_score": 75.98356856515926, "chrf_score": 92.72100013663068, "xcomet_score": 0.8047958016395569, "xcomet_qe_score": 0.5574629902839661, "metricx_score": 2.6833817958831787, "metricx_qe_score": 0.38550588488578796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是可以的。", "metrics": {"bleu_score": 63.15552371794039, "chrf_score": 55.594035594035596, "xcomet_score": 0.9396331310272217, "xcomet_qe_score": 0.922397792339325, "metricx_score": 0.3405936658382416, "metricx_qe_score": 0.5062903761863708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "三月昨天读了一本关于野兽的极其有趣", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14738115668296814, "xcomet_qe_score": 0.149993434548378, "metricx_score": 8.627708435058594, "metricx_qe_score": 8.806985855102539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的书。我在某种程度上也可以,而不是它,我们有这长一段。 也可以说“昨天我读了一本关于蜜蜂的绝对迷人的书”。(注:中文中时间和动作的顺序通常与英文相", "metrics": {"bleu_score": 2.0661822887602486, "chrf_score": 7.143985648322751, "xcomet_score": 0.23464204370975494, "xcomet_qe_score": 0.2345365583896637, "metricx_score": 17.19114112854004, "metricx_qe_score": 16.544706344604492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反,因此将\"yesterday\"放在\"read\"之前更符合中文表达习惯。", "metrics": {"bleu_score": 1.9822566267103439, "chrf_score": 13.407420193182814, "xcomet_score": 0.14982514083385468, "xcomet_qe_score": 0.1399824172258377, "metricx_score": 7.574655055999756, "metricx_qe_score": 11.461764335632324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 这里的推理是,这句话虽然违反了一般语法原则——即直接宾语应紧跟动词——但仍然可能成立。 (Zhèlǐ de tuīlún shì, zhè jù huà suīrán wéiwǎnle yībān yǔfǎ nguyênzé——jí zhíjiē bēngyù yīng jǐn gēn dòngcí——dàn róngyì yǒu qǐ. ) 它符合依赖长度最小化原则,该原则指出更短的依赖关系更可取。 所以", "metrics": {"bleu_score": 16.760342725511453, "chrf_score": 17.175268014254463, "xcomet_score": 0.34518080949783325, "xcomet_qe_score": 0.3854227364063263, "metricx_score": 6.652220726013184, "metricx_qe_score": 5.171250820159912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示了关键依赖项的长度,即这两个结构中不常数的那些。", "metrics": {"bleu_score": 54.20377895732388, "chrf_score": 48.19862010242935, "xcomet_score": 0.8346081972122192, "xcomet_qe_score": 0.6634336709976196, "metricx_score": 1.7356326580047607, "metricx_qe_score": 2.7222208976745605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们这里有从“红”到长度为7(以词为单位)的附词的依赖关系,以及从“红”到长度为4的“书”的依赖关系。加起来总共是11。", "metrics": {"bleu_score": 22.301875415425016, "chrf_score": 20.61957642751351, "xcomet_score": 0.544196605682373, "xcomet_qe_score": 0.5828884243965149, "metricx_score": 11.337742805480957, "metricx_qe_score": 10.974702835083008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你交换时,你移动。这两个成分的依赖之和变为六,", "metrics": {"bleu_score": 15.269093836384917, "chrf_score": 15.680875561020468, "xcomet_score": 0.674435019493103, "xcomet_qe_score": 0.6071049571037292, "metricx_score": 7.528171062469482, "metricx_qe_score": 8.18700122833252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吗?所以,它变得更短", "metrics": {"bleu_score": 4.456882760699063, "chrf_score": 2.8248587570621466, "xcomet_score": 0.29593849182128906, "xcomet_qe_score": 0.36123186349868774, "metricx_score": 6.676499843597412, "metricx_qe_score": 8.24716854095459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",从十一变成六。这听起来相当不错,", "metrics": {"bleu_score": 17.694975149532556, "chrf_score": 16.9757749447065, "xcomet_score": 0.367676317691803, "xcomet_qe_score": 0.33816206455230713, "metricx_score": 5.2944135665893555, "metricx_qe_score": 6.227362632751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 昨天", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2050107717514038, "xcomet_qe_score": 0.11688558757305145, "metricx_score": 3.89546799659729, "metricx_qe_score": 2.1695210933685303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吗?它违反了一个原则,但满足了另一个。", "metrics": {"bleu_score": 57.34648773088752, "chrf_score": 49.449387347830736, "xcomet_score": 0.8560488820075989, "xcomet_qe_score": 0.8407056331634521, "metricx_score": 0.6969572305679321, "metricx_qe_score": 1.1119767427444458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很好。", "metrics": {"bleu_score": 0.0, "chrf_score": 22.22222222222222, "xcomet_score": 0.9976413249969482, "xcomet_qe_score": 0.9924054145812988, "metricx_score": 0.15613436698913574, "metricx_qe_score": 0.28753313422203064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的词典中提取了关于协调性的各种统计数据,并详见论文中关于为何不使用大学依赖关系的解释。 这些统计数据证实了之前多次观察到的现象,即左连词倾向于更短,", "metrics": {"bleu_score": 46.717845486116495, "chrf_score": 41.525691546421726, "xcomet_score": 0.5582590103149414, "xcomet_qe_score": 0.4426213800907135, "metricx_score": 5.2674479484558105, "metricx_qe_score": 4.989965915679932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此“盐和胡椒”而不是“胡椒和盐”在音节上更短。 以及顺", "metrics": {"bleu_score": 16.730402692499975, "chrf_score": 9.257597001300958, "xcomet_score": 0.5691283345222473, "xcomet_qe_score": 0.49167683720588684, "metricx_score": 6.909296035766602, "metricx_qe_score": 5.9416117668151855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "便提到的观察结果,即这种倾向随着在法国的停留时间而加剧。", "metrics": {"bleu_score": 21.281701380712917, "chrf_score": 20.569345127152946, "xcomet_score": 0.16916026175022125, "xcomet_qe_score": 0.1512497216463089, "metricx_score": 7.965701103210449, "metricx_qe_score": 9.289325714111328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当两个连词的长度差异增大时,较短的连词更倾向于成为第一个,强度更强,因此", "metrics": {"bleu_score": 32.33266452087056, "chrf_score": 27.596568458053778, "xcomet_score": 0.7263641953468323, "xcomet_qe_score": 0.7665045261383057, "metricx_score": 5.36325216293335, "metricx_qe_score": 3.557663917541504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "左侧短连词的比例更大。", "metrics": {"bleu_score": 42.24247710146674, "chrf_score": 39.46696577120728, "xcomet_score": 0.9646366834640503, "xcomet_qe_score": 0.861079216003418, "metricx_score": 0.931867241859436, "metricx_qe_score": 1.5655685663223267, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文的新颖之处在于我们观察到这种倾向仅在左侧的总督不存在时发生。 所以在", "metrics": {"bleu_score": 36.3528026393508, "chrf_score": 32.70735582717476, "xcomet_score": 0.568193793296814, "xcomet_qe_score": 0.5439274311065674, "metricx_score": 9.594574928283691, "metricx_qe_score": 5.824936866760254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 昨天", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2050107717514038, "xcomet_qe_score": 0.11688558757305145, "metricx_score": 3.89546799659729, "metricx_qe_score": 2.1695210933685303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个例子中,州长在左边,我看到了巴顿丽莎,所以州长就在左边。", "metrics": {"bleu_score": 15.295559337528834, "chrf_score": 10.139978619086673, "xcomet_score": 0.5555521845817566, "xcomet_qe_score": 0.6434071063995361, "metricx_score": 3.1488568782806396, "metricx_qe_score": 2.1445019245147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,荷马来了并打了喷嚏。", "metrics": {"bleu_score": 22.767600244565212, "chrf_score": 12.132140398168774, "xcomet_score": 0.7188698053359985, "xcomet_qe_score": 0.7360727787017822, "metricx_score": 4.886294841766357, "metricx_qe_score": 5.467830657958984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们有两个动词的协调,没有外部的支配因素。所以", "metrics": {"bleu_score": 57.41156866422447, "chrf_score": 60.47765178546617, "xcomet_score": 0.8586192727088928, "xcomet_qe_score": 0.762058675289154, "metricx_score": 4.093043327331543, "metricx_qe_score": 2.793224573135376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左侧的连词倾向于更短,尤其是两个连词之间的差异越大。", "metrics": {"bleu_score": 30.049987759202963, "chrf_score": 28.47582651172196, "xcomet_score": 0.9335150718688965, "xcomet_qe_score": 0.9272915124893188, "metricx_score": 4.439988613128662, "metricx_qe_score": 4.396890163421631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当治理位于右侧如本例时,左侧管理协调尾部和网络,这种效果消失。", "metrics": {"bleu_score": 7.577517306389086, "chrf_score": 8.742116648135429, "xcomet_score": 0.3363576829433441, "xcomet_qe_score": 0.37411344051361084, "metricx_score": 10.512584686279297, "metricx_qe_score": 9.182439804077148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度,展示了第一个列是音节,中间列是字符,而右列是词语。", "metrics": {"bleu_score": 9.254303187662849, "chrf_score": 12.406248924016904, "xcomet_score": 0.6851338148117065, "xcomet_qe_score": 0.6907273530960083, "metricx_score": 3.680945873260498, "metricx_qe_score": 3.93253231048584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我将专注于右列。", "metrics": {"bleu_score": 8.913765521398126, "chrf_score": 8.347566182749787, "xcomet_score": 0.8840667009353638, "xcomet_qe_score": 0.825237512588501, "metricx_score": 2.477311372756958, "metricx_qe_score": 3.5778815746307373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到的是,当总督在左侧时, 左侧成分随着词语绝对差异的增大而逐渐呈现出长度较短的趋势,在没有控制词的句子协调中也可以观察到这一现象。然而", "metrics": {"bleu_score": 29.328222253367908, "chrf_score": 24.65695982809197, "xcomet_score": 0.31999683380126953, "xcomet_qe_score": 0.3128573000431061, "metricx_score": 8.049485206604004, "metricx_qe_score": 6.20518684387207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当控制词位于右侧时,这种趋势消失。", "metrics": {"bleu_score": 23.272696712467983, "chrf_score": 21.26848078192246, "xcomet_score": 0.7945410013198853, "xcomet_qe_score": 0.45699289441108704, "metricx_score": 3.5596752166748047, "metricx_qe_score": 5.603753566741943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在论文中,我们展示了这一点如何为反对我们的不对称协调结构提供论据,因为这些结构折叠了如下的对称结构。 (注:原文中 \"as these two andfold the symmetric structures as these two\" 部分语法不清晰,翻译时做了", "metrics": {"bleu_score": 28.56907861980488, "chrf_score": 29.286111713280828, "xcomet_score": 0.3953659236431122, "xcomet_qe_score": 0.3240233361721039, "metricx_score": 10.494000434875488, "metricx_qe_score": 8.077853202819824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "适当的调整以保持语义清晰。) 请参阅论文以了解完整的协议和论据。", "metrics": {"bleu_score": 7.173775053353472, "chrf_score": 13.457267381739097, "xcomet_score": 0.22410385310649872, "xcomet_qe_score": 0.16980673372745514, "metricx_score": 4.6178364753723145, "metricx_qe_score": 3.996677875518799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抱歉,请在海报展期间与我们讨论。", "metrics": {"bleu_score": 10.82597837309053, "chrf_score": 10.776188479523448, "xcomet_score": 0.3554318845272064, "xcomet_qe_score": 0.344673216342926, "metricx_score": 3.056880474090576, "metricx_qe_score": 1.8798867464065552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是华盛顿大学的博士生沙航(Shahang B)。", "metrics": {"bleu_score": 50.52427764829451, "chrf_score": 48.64234439259428, "xcomet_score": 0.7646653652191162, "xcomet_qe_score": 0.7776799201965332, "metricx_score": 3.6598010063171387, "metricx_qe_score": 3.417694330215454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将展示我们的研究成果,从预训练数据到语言模型,再到下游任务,追踪政治偏见的轨迹,以揭示导致不公平的自然语言处理(NLB)模型。", "metrics": {"bleu_score": 39.344281959918675, "chrf_score": 39.744344997265756, "xcomet_score": 0.9250226020812988, "xcomet_qe_score": 0.8136183023452759, "metricx_score": 2.373159408569336, "metricx_qe_score": 1.879433512687683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模网络爬虫数据上进行训练的。 根据对C4语料库的调查", "metrics": {"bleu_score": 50.23882842404935, "chrf_score": 59.8185273910221, "xcomet_score": 0.6778184175491333, "xcomet_qe_score": 0.6662919521331787, "metricx_score": 4.961409568786621, "metricx_qe_score": 5.149882793426514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",新闻媒体在预训练数据中覆盖率很高。", "metrics": {"bleu_score": 56.75001761639643, "chrf_score": 53.49583908422082, "xcomet_score": 0.707595705986023, "xcomet_qe_score": 0.5690789222717285, "metricx_score": 4.949051856994629, "metricx_qe_score": 6.135820388793945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,纽约时报、洛杉矶时报、卫报、赫芬顿邮报等都在语言模型的训练数据中覆盖得很好。", "metrics": {"bleu_score": 33.02465000434421, "chrf_score": 29.009762536866994, "xcomet_score": 0.8775246143341064, "xcomet_qe_score": 0.784437894821167, "metricx_score": 4.307733058929443, "metricx_qe_score": 4.6360979080200195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了喜忧参半的结果。", "metrics": {"bleu_score": 73.70731040943886, "chrf_score": 71.12056079741839, "xcomet_score": 0.997859001159668, "xcomet_qe_score": 0.9884636402130127, "metricx_score": 0.5841813683509827, "metricx_qe_score": 0.7184123992919922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从多元的视角中学习,这庆祝了民主和思想的多样性。", "metrics": {"bleu_score": 26.34592129280123, "chrf_score": 22.50418785422957, "xcomet_score": 0.8164267539978027, "xcomet_qe_score": 0.7692656517028809, "metricx_score": 1.962304949760437, "metricx_qe_score": 2.1865267753601074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能会在下游任务应用中导致潜在的公平问题。", "metrics": {"bleu_score": 74.87614526864832, "chrf_score": 67.83538704499873, "xcomet_score": 0.9911092519760132, "xcomet_qe_score": 0.9741849899291992, "metricx_score": 1.0168969631195068, "metricx_qe_score": 1.1752721071243286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体通过提出以下几个问题进行探索: 首先,我们如何评估语言模型的政治意义,以及训练数据可能对这些政治偏见起到什么作用?", "metrics": {"bleu_score": 57.91592332623114, "chrf_score": 52.91927304397345, "xcomet_score": 0.8402878046035767, "xcomet_qe_score": 0.9100817441940308, "metricx_score": 2.66925311088562, "metricx_qe_score": 2.934298038482666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同语料库的语言模型在下游任务中的实际表现如何,以及这是否会导致NLP应用中的公平性问题?", "metrics": {"bleu_score": 75.8740405919965, "chrf_score": 72.49692999953568, "xcomet_score": 0.8517067432403564, "xcomet_qe_score": 0.8758416175842285, "metricx_score": 2.2350594997406006, "metricx_qe_score": 2.6514997482299805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体而言,我们首先提议使用政治问卷,如政治罗盘测试,以不同的提示格式来引导语言模型。", "metrics": {"bleu_score": 38.29398184965721, "chrf_score": 31.056075690642153, "xcomet_score": 0.8376222848892212, "xcomet_qe_score": 0.8287110924720764, "metricx_score": 3.822812080383301, "metricx_qe_score": 4.090207576751709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保我们在政治科学文献的基础上进行自动化评估。", "metrics": {"bleu_score": 24.606941648608938, "chrf_score": 26.159916140419654, "xcomet_score": 0.961554765701294, "xcomet_qe_score": 0.8955561518669128, "metricx_score": 1.3502029180526733, "metricx_qe_score": 1.6701503992080688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,母语模型确实具有不同的政治倾向。", "metrics": {"bleu_score": 64.25736646779204, "chrf_score": 56.17908902361034, "xcomet_score": 0.9669203758239746, "xcomet_qe_score": 0.9843463897705078, "metricx_score": 2.1614139080047607, "metricx_qe_score": 1.8009052276611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘上的所有四个象限。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 62.83591000502765, "xcomet_score": 0.8533560037612915, "xcomet_qe_score": 0.7687404155731201, "metricx_score": 1.9299724102020264, "metricx_qe_score": 2.1211085319519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以看到,GPT4 是所有语言模型中最自由的,而GPT系列通常比BER系列及其变体在社会自由度方面更胜一筹。", "metrics": {"bleu_score": 51.36232740042702, "chrf_score": 46.5192220951166, "xcomet_score": 0.8251925110816956, "xcomet_qe_score": 0.7993030548095703, "metricx_score": 4.030669212341309, "metricx_qe_score": 3.8311736583709717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在探究语言模型的政治偏见实际上在多大程度上来源于训练数据。", "metrics": {"bleu_score": 52.65625212867713, "chrf_score": 49.742674498321094, "xcomet_score": 0.9924560785293579, "xcomet_qe_score": 0.9722790718078613, "metricx_score": 0.6390435695648193, "metricx_qe_score": 0.7480613589286804, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点来进行控制实验,这些语料库分为新闻和社交媒体,并根据其政治倾向进一步细分。", "metrics": {"bleu_score": 53.02171953504222, "chrf_score": 44.21938963025432, "xcomet_score": 0.9156239032745361, "xcomet_qe_score": 0.7170280814170837, "metricx_score": 1.888195276260376, "metricx_qe_score": 2.095743179321289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这样的聚会和语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了转移。", "metrics": {"bleu_score": 65.8019479741253, "chrf_score": 59.41496800054769, "xcomet_score": 0.7658421397209167, "xcomet_qe_score": 0.7134692668914795, "metricx_score": 5.1434125900268555, "metricx_qe_score": 5.4859089851379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于罗伯塔模型,进一步在倾向于左派的Reddit语料库上进行微调,我们可以看到其在政治倾向上显着地向自由派转变。 在政治偏见方面。", "metrics": {"bleu_score": 32.416549968172326, "chrf_score": 37.042810020487096, "xcomet_score": 0.49292775988578796, "xcomet_qe_score": 0.4222487211227417, "metricx_score": 5.9369001388549805, "metricx_qe_score": 4.35920524597168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还尝试探讨语言模型是否能捕捉到现代社会中普遍存在的极化现", "metrics": {"bleu_score": 65.49758647309258, "chrf_score": 58.76527272253048, "xcomet_score": 0.8564441204071045, "xcomet_qe_score": 0.8602384328842163, "metricx_score": 3.5723581314086914, "metricx_qe_score": 1.0874724388122559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "象。 我们将预训练语料库分为美国第45任总统之前和之后两个部分。", "metrics": {"bleu_score": 66.9346401858516, "chrf_score": 65.0052343154549, "xcomet_score": 0.5791466236114502, "xcomet_qe_score": 0.4599786698818207, "metricx_score": 4.30942964553833, "metricx_qe_score": 5.618980884552002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们分别对这两个不同时间段的语料库进行预训练,以训练语言模型。", "metrics": {"bleu_score": 41.88050456551661, "chrf_score": 41.63599212925948, "xcomet_score": 0.965752124786377, "xcomet_qe_score": 0.7643698453903198, "metricx_score": 1.1785752773284912, "metricx_qe_score": 1.218673825263977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年后普遍表现出更偏离中性的政治倾向。", "metrics": {"bleu_score": 36.75234456178971, "chrf_score": 33.118415057490886, "xcomet_score": 0.9947031736373901, "xcomet_qe_score": 0.9861432313919067, "metricx_score": 1.7731049060821533, "metricx_qe_score": 1.7392586469650269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能够捕捉到社会中存在的两极分化现象。", "metrics": {"bleu_score": 47.72894233335956, "chrf_score": 44.062597217356966, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5899914503097534, "metricx_qe_score": 0.8160909414291382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们评估了具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测中的表现。这些自然语言处理(NLP)应用通常涉及语言模型,并且可能具有非常重要的影响。 因此,", "metrics": {"bleu_score": 59.34619645325305, "chrf_score": 66.3253010576665, "xcomet_score": 0.710318922996521, "xcomet_qe_score": 0.7031571865081787, "metricx_score": 2.907144546508789, "metricx_qe_score": 1.3779619932174683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到如果我们调查每类别的性能,也就是说,如果我们将性能分开,我们可以发现... (Note: The last part \"...we can discover...\" is not translated as it was not provided in the original text.) 在不同的人口统计学或政治倾向的新闻媒体中,我们可以看到一种模式,", "metrics": {"bleu_score": 29.415299204540368, "chrf_score": 28.295931119561335, "xcomet_score": 0.5451424717903137, "xcomet_qe_score": 0.6001561880111694, "metricx_score": 9.367032051086426, "metricx_qe_score": 8.414885520935059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测方面,左翼倾向的语言模型表现更好。 在检测针对社会少数群体的仇恨言论方面。 然而,我们在识别针对社会中更强大群体的仇恨言论方面表现得更差。", "metrics": {"bleu_score": 55.532237107108465, "chrf_score": 53.56309682925372, "xcomet_score": 0.6318857669830322, "xcomet_qe_score": 0.6639285683631897, "metricx_score": 5.405646324157715, "metricx_qe_score": 5.61738920211792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,正确的语言模型在检测针对白人男性的仇恨言论方面表现更好,但在检测针对黑人、LGBTQ+ 以及其他少数族裔社区的仇恨言论时则表现较差。", "metrics": {"bleu_score": 52.95124448811925, "chrf_score": 52.960154500405174, "xcomet_score": 0.8517111539840698, "xcomet_qe_score": 0.8648531436920166, "metricx_score": 4.192784786224365, "metricx_qe_score": 4.093321800231934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在假新闻检测领域也存在类似趋势,我们发现左翼倾向的语言模型更擅长检测来自右翼政治立场的错误信息,反之亦然。 在此,", "metrics": {"bleu_score": 44.85085131934096, "chrf_score": 38.83684440095865, "xcomet_score": 0.7855470776557922, "xcomet_qe_score": 0.7787788510322571, "metricx_score": 4.17545223236084, "metricx_qe_score": 1.7891472578048706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步通过许多定性例子展示不同政治含义的语言模型,以观察其效果。 根据社交类别对仇恨言论和虚假信息示例给出不同的预测。", "metrics": {"bleu_score": 50.49463693649323, "chrf_score": 42.31714803741052, "xcomet_score": 0.7022350430488586, "xcomet_qe_score": 0.7319389581680298, "metricx_score": 3.070260524749756, "metricx_qe_score": 2.85781192779541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中还有更多示例,以进一步强调这一点。 这表明语言模型的政治偏见存在一个非常紧迫的公平问题。", "metrics": {"bleu_score": 61.03275790243609, "chrf_score": 52.291241359108554, "xcomet_score": 0.818490743637085, "xcomet_qe_score": 0.8358325958251953, "metricx_score": 1.7292462587356567, "metricx_qe_score": 2.177375316619873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个右倾语言模型被微调用于仇恨言论、虚假信息等,并部署到一个流行的社交媒体平台上, 这意味着持有不同政治观点的人可能会被边缘化,针对少数族裔群体的仇恨言论也可能不受控制地蔓延。", "metrics": {"bleu_score": 48.096299744193495, "chrf_score": 41.37040921333296, "xcomet_score": 0.9541256427764893, "xcomet_qe_score": 0.8978299498558044, "metricx_score": 1.4622491598129272, "metricx_qe_score": 1.402022123336792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为我们敲响了警钟,需要我们认识到并解决语言模型政治含义所导致的公平问题。", "metrics": {"bleu_score": 32.00540465704303, "chrf_score": 32.663950186899484, "xcomet_score": 0.9806435108184814, "xcomet_qe_score": 0.978615403175354, "metricx_score": 0.965865433216095, "metricx_qe_score": 0.7386947274208069, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在讨论中,我们", "metrics": {"bleu_score": 12.100518276540289, "chrf_score": 11.061210911510312, "xcomet_score": 0.34404903650283813, "xcomet_qe_score": 0.45394521951675415, "metricx_score": 6.478772163391113, "metricx_qe_score": 2.241809368133545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "想强调的是,我们揭露了语言模型政治偏见的独特困境,", "metrics": {"bleu_score": 55.58846417764563, "chrf_score": 50.95540346887299, "xcomet_score": 0.9134581089019775, "xcomet_qe_score": 0.8263770341873169, "metricx_score": 1.8898926973342896, "metricx_qe_score": 1.8465334177017212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像是在西勒纳(Syilla)和卡律布迪斯(Charybdis)之间取舍。", "metrics": {"bleu_score": 31.011575752288344, "chrf_score": 22.961354480555745, "xcomet_score": 0.7911962270736694, "xcomet_qe_score": 0.7390819191932678, "metricx_score": 4.022488594055176, "metricx_qe_score": 2.6224331855773926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们不在语言模型训练数据中消毒政治观点,偏见就会从预训练数据传播到语言模型,再到下游任务,最终产生公平性问题。", "metrics": {"bleu_score": 58.55604934014503, "chrf_score": 49.56283057501182, "xcomet_score": 0.8783529996871948, "xcomet_qe_score": 0.8538175821304321, "metricx_score": 2.679594039916992, "metricx_qe_score": 2.631211519241333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试以某种方式进行清理,也可能会面临审查或排斥的风险,", "metrics": {"bleu_score": 59.53156503745218, "chrf_score": 54.72144080726973, "xcomet_score": 0.8551778793334961, "xcomet_qe_score": 0.8049285411834717, "metricx_score": 1.551859736442566, "metricx_qe_score": 2.7162282466888428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定哪些内容实际上是中立的,应该保留语言单一性的数据。这在某种程度上就", "metrics": {"bleu_score": 24.325558783239472, "chrf_score": 22.464958229421946, "xcomet_score": 0.6214369535446167, "xcomet_qe_score": 0.3191501796245575, "metricx_score": 7.1048688888549805, "metricx_qe_score": 4.366669654846191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像电动车道问题。", "metrics": {"bleu_score": 11.35114710927891, "chrf_score": 11.559383499546689, "xcomet_score": 0.8300012350082397, "xcomet_qe_score": 0.7512786984443665, "metricx_score": 3.9416606426239014, "metricx_qe_score": 4.280107021331787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很好。", "metrics": {"bleu_score": 0.0, "chrf_score": 22.22222222222222, "xcomet_score": 0.9974120855331421, "xcomet_qe_score": 0.9916048049926758, "metricx_score": 0.18070635199546814, "metricx_qe_score": 0.27904534339904785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想今天我要讲的大致就是这些了。今天的内容到此结束。", "metrics": {"bleu_score": 9.562406574442017, "chrf_score": 18.76572796354366, "xcomet_score": 0.9822876453399658, "xcomet_qe_score": 0.9823770523071289, "metricx_score": 0.3438974916934967, "metricx_qe_score": 0.22023549675941467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基·梅隆大学一年级博士生,今天我将展示我的研究工作《肛门位置:设计偏见和数据集模型的特征分析》。", "metrics": {"bleu_score": 29.4744717599169, "chrf_score": 20.70679305773838, "xcomet_score": 0.6296833157539368, "xcomet_qe_score": 0.6285591125488281, "metricx_score": 6.760494232177734, "metricx_qe_score": 5.9956183433532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的,他们是塞巴斯蒂安·桑蒂(Sebastian Santi)、罗南·拉布拉塞(Ronan Labrasse)、卡塔琳娜·雷尼卡(Katarina Reinika)和马丁·萨普(Martin Sapp)。", "metrics": {"bleu_score": 28.565770311731146, "chrf_score": 48.66406573338672, "xcomet_score": 0.7880964279174805, "xcomet_qe_score": 0.7855669260025024, "metricx_score": 2.1821277141571045, "metricx_qe_score": 1.534356951713562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们从一个场景开始想象:你为一家报纸工作,正在筛选新闻文章下的评论,试图删除有毒内容。 ", "metrics": {"bleu_score": 44.409176637184956, "chrf_score": 40.14281906063618, "xcomet_score": 0.8915011882781982, "xcomet_qe_score": 0.899921715259552, "metricx_score": 1.88380765914917, "metricx_qe_score": 1.529209852218628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能轉向像Perspective API這樣的熱門API來進行有毒檢測,如果您是卡爾·瓊斯,這效果非常好,因為", "metrics": {"bleu_score": 3.5131364560892306, "chrf_score": 20.093391943727894, "xcomet_score": 0.5443868041038513, "xcomet_qe_score": 0.5832853317260742, "metricx_score": 5.835618019104004, "metricx_qe_score": 4.471280574798584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能夠正確地檢測出有毒的實例。", "metrics": {"bleu_score": 6.150343144231885, "chrf_score": 42.59536226172686, "xcomet_score": 0.7576387524604797, "xcomet_qe_score": 0.778404951095581, "metricx_score": 5.548945903778076, "metricx_qe_score": 5.547269344329834, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对阿迪提亚·沙尔马(Aditya Sharma)来说,情况并非如此。", "metrics": {"bleu_score": 30.94986086785959, "chrf_score": 60.6886343338415, "xcomet_score": 0.9830441474914551, "xcomet_qe_score": 0.988021731376648, "metricx_score": 1.3186091184616089, "metricx_qe_score": 1.652296543121338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "潜在的 A API 实际上对在印度语境中更常见的冒犯性术语并不敏感。", "metrics": {"bleu_score": 51.70969057682974, "chrf_score": 41.895808795835926, "xcomet_score": 0.6988458037376404, "xcomet_qe_score": 0.6262562870979309, "metricx_score": 4.205713748931885, "metricx_qe_score": 5.745271682739258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏差的例子,我们可以在不同人群中观察到技术表现的系统性差异。", "metrics": {"bleu_score": 41.78383638882466, "chrf_score": 35.777579444745314, "xcomet_score": 0.9778684377670288, "xcomet_qe_score": 0.9615130424499512, "metricx_score": 0.6782376766204834, "metricx_qe_score": 0.8978255987167358, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "设计中的偏见,如我们刚才所见,可能会影响您对自然语言处理(NLP)研究人员和模型开发者的立场。", "metrics": {"bleu_score": 39.149307778967724, "chrf_score": 33.317715830566556, "xcomet_score": 0.7269057035446167, "xcomet_qe_score": 0.6789586544036865, "metricx_score": 3.7708656787872314, "metricx_qe_score": 4.87204647064209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场性是指人们因其人口统计、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 77.5411995019244, "chrf_score": 74.12337678960351, "xcomet_score": 0.8142653703689575, "xcomet_qe_score": 0.8138586282730103, "metricx_score": 2.7315804958343506, "metricx_qe_score": 2.8797171115875244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 73.97378912627735, "chrf_score": 67.41898597045686, "xcomet_score": 0.9927648305892944, "xcomet_qe_score": 0.908970832824707, "metricx_score": 0.9186691045761108, "metricx_qe_score": 1.4367049932479858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为一名研究人员,立场性会影响研究过程及其结果和结论,因为它会改变研究人员所做的决策。", "metrics": {"bleu_score": 48.02640206796801, "chrf_score": 41.25199997863967, "xcomet_score": 0.991750955581665, "xcomet_qe_score": 0.9855440855026245, "metricx_score": 0.9881749749183655, "metricx_qe_score": 0.9184059500694275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问的一个问题是,数据集和模型是否具有位置性?", "metrics": {"bleu_score": 48.465254338121596, "chrf_score": 45.20744241143061, "xcomet_score": 0.9149991869926453, "xcomet_qe_score": 0.9874231815338135, "metricx_score": 2.477600574493408, "metricx_qe_score": 0.7690516710281372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并非试图声称细胞模型和数据集本身具有人口统计学身份和生活经历,但它们确实汇集了真实个体的判断和观点,从而能够代表某些立场,", "metrics": {"bleu_score": 43.01768563952919, "chrf_score": 35.53380179966371, "xcomet_score": 0.68252032995224, "xcomet_qe_score": 0.7182583808898926, "metricx_score": 5.561531066894531, "metricx_qe_score": 6.229442119598389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "超越其他立场。 早期研究表明了一些位置性的轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 31.952472500946065, "chrf_score": 26.739742724373183, "xcomet_score": 0.3252384066581726, "xcomet_qe_score": 0.18018051981925964, "metricx_score": 7.131216526031494, "metricx_qe_score": 6.523223876953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些作品实际上并没有将最终用户与数据集和模型本身进行比较。 随着自然语言处理(NLP)测试变得更加主观和社会化,研究模型和数据集的位置性变得越来越重要。 这些位置偏差的性质难以界定,因为并非所有决策都有记录,且许多模型隐藏在API背后。", "metrics": {"bleu_score": 55.52000937071643, "chrf_score": 49.83596474447647, "xcomet_score": 0.7383883595466614, "xcomet_qe_score": 0.7788251638412476, "metricx_score": 4.5912370681762695, "metricx_qe_score": 4.146047592163086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的定位性,我们实际上是将标注与真实用户的数据集和模型进行比较。", "metrics": {"bleu_score": 54.256513372526385, "chrf_score": 47.52555743360869, "xcomet_score": 0.7832667827606201, "xcomet_qe_score": 0.8785467147827148, "metricx_score": 3.9823145866394043, "metricx_qe_score": 3.585935115814209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们框架中的自然语言定位(NL positionality)来实现这一点。", "metrics": {"bleu_score": 14.440028187544327, "chrf_score": 47.34734246028731, "xcomet_score": 0.8746870160102844, "xcomet_qe_score": 0.8760281801223755, "metricx_score": 1.35896897315979, "metricx_qe_score": 1.8473089933395386, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "框架主要通过两个步骤运作。", "metrics": {"bleu_score": 39.832871551569504, "chrf_score": 29.26573426573426, "xcomet_score": 0.8180668354034424, "xcomet_qe_score": 0.8290015459060669, "metricx_score": 0.6029012799263, "metricx_qe_score": 0.33283647894859314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的标注人员重新标注数据集。", "metrics": {"bleu_score": 36.857838224116975, "chrf_score": 31.056395663981927, "xcomet_score": 0.7965057492256165, "xcomet_qe_score": 0.7971661686897278, "metricx_score": 4.661585807800293, "metricx_qe_score": 3.25648832321167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在进行这项工作时,应该考虑原始数据集标注者的人口统计学特征,因为通常只有少数标注者标注每个实例,而且人口统计学数据很少被收集和共享。", "metrics": {"bleu_score": 48.04007558528613, "chrf_score": 40.70103302124052, "xcomet_score": 0.927946925163269, "xcomet_qe_score": 0.9803602695465088, "metricx_score": 3.43845534324646, "metricx_qe_score": 2.5542867183685303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据以获得多个标注,例如,并获取一套丰富的人口统计数据。", "metrics": {"bleu_score": 40.74979852683483, "chrf_score": 35.98850141017581, "xcomet_score": 0.7738982439041138, "xcomet_qe_score": 0.8041313886642456, "metricx_score": 4.434267997741699, "metricx_qe_score": 4.2967753410339355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后根据人口统计学特征对注释进行分类,并使用比较器的R相关性得分将其与模型和数据集进行比较。 因此,我们的框架实际上与注释器分歧文献不同,通过将最终用户与模型和数据集、预测和标签进行比较,而不是仅仅关注注释器一致性或建模注释器分布,从而实现了区分。", "metrics": {"bleu_score": 52.81561306859552, "chrf_score": 51.794708281403445, "xcomet_score": 0.5162784457206726, "xcomet_qe_score": 0.5341646671295166, "metricx_score": 6.494915962219238, "metricx_qe_score": 5.02218770980835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "框架主要通过野外实验室(Lab in the wild)实现,这是一个在线众包平台,曾是人机交互(HCI)领域的合作者。", "metrics": {"bleu_score": 26.37013580490901, "chrf_score": 40.599420235579366, "xcomet_score": 0.5843527317047119, "xcomet_qe_score": 0.5537526607513428, "metricx_score": 2.570361614227295, "metricx_qe_score": 2.807607889175415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,我们可以在该平台上招募与 MTERk 等", "metrics": {"bleu_score": 56.34141428198873, "chrf_score": 60.83790882763259, "xcomet_score": 0.6178251504898071, "xcomet_qe_score": 0.43866854906082153, "metricx_score": 8.280655860900879, "metricx_qe_score": 11.574578285217285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "主要来自美国或印度的参与者不同的多样化志愿者。此外,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 54.37022837024946, "chrf_score": 53.67995168256579, "xcomet_score": 0.5075179934501648, "xcomet_qe_score": 0.4431782066822052, "metricx_score": 8.92841911315918, "metricx_qe_score": 9.669071197509766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“实验室在野外”平台上主持两个任务,其中之一是社交可接受性。其工作方式是,参与者将阅读社交化学数据集中的一个情境,然后他们会写下该情境在社交上的可接受程度。", "metrics": {"bleu_score": 43.18310437318404, "chrf_score": 34.15903108337967, "xcomet_score": 0.6795468330383301, "xcomet_qe_score": 0.6377548575401306, "metricx_score": 3.172532320022583, "metricx_qe_score": 4.426223278045654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对城市的参与度,他们可以将自己的回应与人工智能和其他人的回应进行比较。", "metrics": {"bleu_score": 43.59391581674357, "chrf_score": 38.52626557021755, "xcomet_score": 0.8304429054260254, "xcomet_qe_score": 0.8062844276428223, "metricx_score": 4.800744533538818, "metricx_qe_score": 5.216372966766357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后将这些注释与社会化学、德尔菲法和GPT4进行了比较。 接下", "metrics": {"bleu_score": 43.06165652033611, "chrf_score": 47.56471104326531, "xcomet_score": 0.6569960117340088, "xcomet_qe_score": 0.663712203502655, "metricx_score": 3.9716432094573975, "metricx_qe_score": 3.264620780944824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来,为毒性和仇恨言论检测任务复制一个类似的设置,他们将阅读来自Dinah仇恨言论的一个实例,并写下他们是否认为它是仇恨言论的实例。", "metrics": {"bleu_score": 51.66105023351437, "chrf_score": 40.5996192268271, "xcomet_score": 0.5053253173828125, "xcomet_qe_score": 0.40220215916633606, "metricx_score": 4.808755397796631, "metricx_qe_score": 4.61532735824585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后将这些标注与Dynah Hate、Perspective API、Rewire API、Hate Roberta和GPT4进行了比较。", "metrics": {"bleu_score": 53.35829229005025, "chrf_score": 83.29163806768224, "xcomet_score": 0.7308351993560791, "xcomet_qe_score": 0.725834310054779, "metricx_score": 3.2493019104003906, "metricx_qe_score": 4.946687698364258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了来自87个国家的1000多名标注人员的超过160,000条标注。", "metrics": {"bleu_score": 64.06134133886644, "chrf_score": 65.56083787132626, "xcomet_score": 0.8248182535171509, "xcomet_qe_score": 0.8814314603805542, "metricx_score": 2.3882803916931152, "metricx_qe_score": 1.635016918182373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们更有能力回答自然语言处理(NLP)数据集和模型最倾向于代表哪些群体了。", "metrics": {"bleu_score": 28.438779279003345, "chrf_score": 30.206504621036434, "xcomet_score": 0.910030722618103, "xcomet_qe_score": 0.9065218567848206, "metricx_score": 1.0990686416625977, "metricx_qe_score": 1.151078701019287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP存在位置性,即不同的数据集和模型倾向于反映特定群体的语言和观点。", "metrics": {"bleu_score": 7.971355849968507, "chrf_score": 16.83338938967322, "xcomet_score": 0.7969626188278198, "xcomet_qe_score": 0.8905646800994873, "metricx_score": 3.795286178588867, "metricx_qe_score": 2.604705333709717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型与英语国家最紧密相关。", "metrics": {"bleu_score": 49.35451179213381, "chrf_score": 43.57686954283306, "xcomet_score": 0.9838119745254517, "xcomet_qe_score": 0.9733604192733765, "metricx_score": 1.0090453624725342, "metricx_qe_score": 1.158062219619751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在《全球保护与发展指数4》(GPD4)的社会可接受性分析中,我们发现它与儒家文化国家和英语国家最紧密相关。我们", "metrics": {"bleu_score": 34.37127424109125, "chrf_score": 44.83807042763019, "xcomet_score": 0.44219261407852173, "xcomet_qe_score": 0.38083508610725403, "metricx_score": 5.390133857727051, "metricx_qe_score": 2.8616437911987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现,动态仇恨(dyna hate)也与英语国家最紧密相关。", "metrics": {"bleu_score": 12.962472880491877, "chrf_score": 30.261468628030403, "xcomet_score": 0.8753910064697266, "xcomet_qe_score": 0.8039662837982178, "metricx_score": 3.017317295074463, "metricx_qe_score": 3.2645344734191895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,大多数额外的认同来自受过大学教育的人。", "metrics": {"bleu_score": 31.326341179203144, "chrf_score": 25.69538293976838, "xcomet_score": 0.8547228574752808, "xcomet_qe_score": 0.8437740802764893, "metricx_score": 1.8629347085952759, "metricx_qe_score": 1.4185311794281006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在社会可接受性任务中,对于 GPDA4,我们发现它最符合受过大学或研究生教育的人的观点。 我们在Diny Haight也发现了类似的现象,这里最符合具有大学教育背景的人。", "metrics": {"bleu_score": 33.661838206111526, "chrf_score": 29.698749004034138, "xcomet_score": 0.6452379822731018, "xcomet_qe_score": 0.5661877393722534, "metricx_score": 6.796990394592285, "metricx_qe_score": 6.553168296813965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群相匹配时,难免会有一些人被遗漏。 一个例子", "metrics": {"bleu_score": 43.759806980485955, "chrf_score": 39.69155754022889, "xcomet_score": 0.8227763175964355, "xcomet_qe_score": 0.6378322839736938, "metricx_score": 1.7733631134033203, "metricx_qe_score": 1.4197241067886353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,与男性和女性对应相比,数据集和模型对非二元性别人群的适应性较差。", "metrics": {"bleu_score": 36.520790158558434, "chrf_score": 33.45797781945193, "xcomet_score": 0.621772825717926, "xcomet_qe_score": 0.6908780336380005, "metricx_score": 5.122297286987305, "metricx_qe_score": 4.846055507659912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPG4社会可接受性任务以及Diny仇恨言论任务分析中都发现了这一点。", "metrics": {"bleu_score": 60.55223296065568, "chrf_score": 53.59321637185413, "xcomet_score": 0.7924545407295227, "xcomet_qe_score": 0.7425433397293091, "metricx_score": 4.389891624450684, "metricx_qe_score": 3.353332996368408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既然在LD中存在LP的位置,我们能对此做些什么?", "metrics": {"bleu_score": 21.62601067591834, "chrf_score": 20.783974658189276, "xcomet_score": 0.7924215793609619, "xcomet_qe_score": 0.8148162364959717, "metricx_score": 8.531428337097168, "metricx_qe_score": 8.106841087341309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对此有几点建议。", "metrics": {"bleu_score": 17.474335703431752, "chrf_score": 19.879610409139048, "xcomet_score": 0.9887105226516724, "xcomet_qe_score": 0.9731390476226807, "metricx_score": 0.12925037741661072, "metricx_qe_score": 0.13402260839939117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一,在整个研究过程中记录所有相关的设计选择;另一点", "metrics": {"bleu_score": 49.78964615672521, "chrf_score": 42.278587042309276, "xcomet_score": 0.8806530237197876, "xcomet_qe_score": 0.8638646602630615, "metricx_score": 4.706657409667969, "metricx_qe_score": 1.529509425163269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,从观点主义的角度进行自然语言处理(NLP)研究。", "metrics": {"bleu_score": 44.18463817237388, "chrf_score": 43.78186054161846, "xcomet_score": 0.8703243732452393, "xcomet_qe_score": 0.8458982110023499, "metricx_score": 1.9522112607955933, "metricx_qe_score": 2.3825271129608154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业的数据集和模型,", "metrics": {"bleu_score": 77.28082326259643, "chrf_score": 68.84834205811217, "xcomet_score": 0.9088296890258789, "xcomet_qe_score": 0.8792413473129272, "metricx_score": 0.9958281517028809, "metricx_qe_score": 0.9861732721328735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Masakanne 计划就是一个很好的例子。", "metrics": {"bleu_score": 73.61703354503862, "chrf_score": 55.21226473045358, "xcomet_score": 0.8281311392784119, "xcomet_qe_score": 0.8179343342781067, "metricx_score": 3.400209665298462, "metricx_qe_score": 4.686368942260742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调的是,包容性的自然语言处理不仅仅是让所有", "metrics": {"bleu_score": 42.300073736306565, "chrf_score": 37.28752978726753, "xcomet_score": 0.5888855457305908, "xcomet_qe_score": 0.17557473480701447, "metricx_score": 5.114758491516113, "metricx_qe_score": 4.029155254364014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术都为每个人服务。", "metrics": {"bleu_score": 18.92178344158729, "chrf_score": 18.543186590098262, "xcomet_score": 0.9624360799789429, "xcomet_qe_score": 0.9446641206741333, "metricx_score": 0.5805914998054504, "metricx_qe_score": 0.7805483341217041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样就完成了我们的演讲。", "metrics": {"bleu_score": 33.18077402843942, "chrf_score": 29.774380593363002, "xcomet_score": 0.9801838397979736, "xcomet_qe_score": 0.9196274876594543, "metricx_score": 1.0529946088790894, "metricx_qe_score": 0.7858496904373169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多,欢迎查看我们的仪表板以获取最新分析结果,以及我们的论文。", "metrics": {"bleu_score": 48.242331302131454, "chrf_score": 41.17068942479286, "xcomet_score": 0.9839349985122681, "xcomet_qe_score": 0.9690322279930115, "metricx_score": 0.5669126510620117, "metricx_qe_score": 0.5786699056625366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是来自Faii大学的袁同学。", "metrics": {"bleu_score": 23.70251900062618, "chrf_score": 16.29338541865494, "xcomet_score": 0.7463345527648926, "xcomet_qe_score": 0.6659724712371826, "metricx_score": 7.381141185760498, "metricx_qe_score": 6.951916694641113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天要介绍我们的研究成果:从轻量级语言模型中区分脚本知识以进行受限语言规划。", "metrics": {"bleu_score": 36.57561655918875, "chrf_score": 31.729060251059593, "xcomet_score": 0.756639838218689, "xcomet_qe_score": 0.6263978481292725, "metricx_score": 2.1216931343078613, "metricx_qe_score": 2.358076810836792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,许多人必须经常通过遵循保证脚本形式的逐步说明来规划他们的行动。 先", "metrics": {"bleu_score": 19.23104277783772, "chrf_score": 18.76747879649447, "xcomet_score": 0.4259723126888275, "xcomet_qe_score": 0.4520132541656494, "metricx_score": 7.120384216308594, "metricx_qe_score": 4.957922458648682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前的研究已经利用语言模型来规划典型活动的抽象目标,", "metrics": {"bleu_score": 55.28814353500042, "chrf_score": 51.3310733029233, "xcomet_score": 0.7581675052642822, "xcomet_qe_score": 0.7686070799827576, "metricx_score": 2.4957900047302246, "metricx_qe_score": 4.990165710449219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如制作蛋糕,并展示了大型语言模型能够有效地将目标分解为步骤。", "metrics": {"bleu_score": 37.832307763098264, "chrf_score": 34.79260324297047, "xcomet_score": 0.21595489978790283, "xcomet_qe_score": 0.20420119166374207, "metricx_score": 3.1768362522125244, "metricx_qe_score": 1.7723761796951294, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以前的研究主要集中在为典型活动的抽象目标制定计划上。", "metrics": {"bleu_score": 55.34502966615894, "chrf_score": 53.46710740180818, "xcomet_score": 0.8462514877319336, "xcomet_qe_score": 0.8822493553161621, "metricx_score": 1.9062577486038208, "metricx_qe_score": 1.708331823348999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为具有具体目标和特定约束的目标制定计划,例如制作巧克力蛋糕,仍然被低估。", "metrics": {"bleu_score": 16.8955474850733, "chrf_score": 17.20103893984589, "xcomet_score": 0.6621534824371338, "xcomet_qe_score": 0.6393045783042908, "metricx_score": 2.8915109634399414, "metricx_qe_score": 3.4979686737060547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受限语言规划的问题。 规划目标会受到不同限制的影响,一个", "metrics": {"bleu_score": 51.10976370499285, "chrf_score": 44.70057266013149, "xcomet_score": 0.7878718376159668, "xcomet_qe_score": 0.6877533197402954, "metricx_score": 7.511600971221924, "metricx_qe_score": 1.5856499671936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抽象的目标可以被具有多面限制的现实生活中的具体目标所继承。", "metrics": {"bleu_score": 16.949210866773097, "chrf_score": 15.549826426836427, "xcomet_score": 0.9673939943313599, "xcomet_qe_score": 0.9292388558387756, "metricx_score": 2.028700351715088, "metricx_qe_score": 1.6460316181182861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个优秀的规划师应该编写既合理又忠实于限制条件的脚本。", "metrics": {"bleu_score": 41.75654352686771, "chrf_score": 35.25483070288319, "xcomet_score": 0.9781843423843384, "xcomet_qe_score": 0.8572745323181152, "metricx_score": 0.8068042993545532, "metricx_qe_score": 1.164150595664978, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并改进了生命语言模型的受限语言规划能力。", "metrics": {"bleu_score": 78.89669955982023, "chrf_score": 73.8038978302145, "xcomet_score": 0.7700601816177368, "xcomet_qe_score": 0.7343782782554626, "metricx_score": 4.098972797393799, "metricx_qe_score": 4.100130081176758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有明确目标的情况下,没有数据可以帮助我们确定星辰的日子。 首先需要如表所示", "metrics": {"bleu_score": 6.849788866850127, "chrf_score": 9.261737064222867, "xcomet_score": 0.42883625626564026, "xcomet_qe_score": 0.30939245223999023, "metricx_score": 9.596955299377441, "metricx_qe_score": 7.460088729858398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获得这些目标,我们使用指令GPT扩展抽象目标,并加入多方面的约束,以实现人机交互数据采集。", "metrics": {"bleu_score": 45.88326150710982, "chrf_score": 37.64195765341886, "xcomet_score": 0.7542283535003662, "xcomet_qe_score": 0.7086665630340576, "metricx_score": 3.9074995517730713, "metricx_qe_score": 5.15016508102417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采集了数百个具体目标,并评估了从逻辑模型生成的脚本。", "metrics": {"bleu_score": 48.14400258132249, "chrf_score": 36.94539373447231, "xcomet_score": 0.7925223112106323, "xcomet_qe_score": 0.812457799911499, "metricx_score": 2.606947422027588, "metricx_qe_score": 2.7982490062713623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的整体准确性。", "metrics": {"bleu_score": 64.53174978135057, "chrf_score": 61.32467858545223, "xcomet_score": 0.9459816217422485, "xcomet_qe_score": 0.9768973588943481, "metricx_score": 0.8722731471061707, "metricx_qe_score": 0.9680761098861694, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有李龙模型在为特定目标进行规划方面都取得了不尽如人意的结果。", "metrics": {"bleu_score": 28.13584227415993, "chrf_score": 28.200322425033153, "xcomet_score": 0.7848007678985596, "xcomet_qe_score": 0.8239404559135437, "metricx_score": 6.599150657653809, "metricx_qe_score": 7.972309589385986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析以研究学习模型的目标何在。", "metrics": {"bleu_score": 36.30856147396857, "chrf_score": 28.104177800464004, "xcomet_score": 0.7707251310348511, "xcomet_qe_score": 0.7589626908302307, "metricx_score": 4.848237037658691, "metricx_qe_score": 5.146454811096191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中结果显示,生成的脚本中的语义完整性是可接受的,但无法保证对约束条件的忠实度。", "metrics": {"bleu_score": 56.87457871046922, "chrf_score": 49.819381266749694, "xcomet_score": 0.9869953393936157, "xcomet_qe_score": 0.9494093656539917, "metricx_score": 1.1032744646072388, "metricx_qe_score": 1.5023319721221924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入探讨了Wi家庭中定义的更细分的约束主题类别。", "metrics": {"bleu_score": 60.90291470054997, "chrf_score": 41.13284042952809, "xcomet_score": 0.6176352500915527, "xcomet_qe_score": 0.5793329477310181, "metricx_score": 6.925538063049316, "metricx_qe_score": 7.7016401290893555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,指导性规划的性能对不同类别的女孩有显著的差异。", "metrics": {"bleu_score": 36.45548948276718, "chrf_score": 24.588896278408917, "xcomet_score": 0.5437347292900085, "xcomet_qe_score": 0.4003981053829193, "metricx_score": 6.616312026977539, "metricx_qe_score": 7.537234783172607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前研究表明,实时模型的输出质量存在较大差异,导致表现不佳。", "metrics": {"bleu_score": 49.03219581106567, "chrf_score": 41.81457977788898, "xcomet_score": 0.877399206161499, "xcomet_qe_score": 0.8567290902137756, "metricx_score": 1.6126240491867065, "metricx_qe_score": 1.7257109880447388, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过度生成滤波器的概念以提高生成质量。", "metrics": {"bleu_score": 40.11271222849197, "chrf_score": 32.53302467057412, "xcomet_score": 0.8740627765655518, "xcomet_qe_score": 0.8273074626922607, "metricx_score": 3.495222330093384, "metricx_qe_score": 4.574308395385742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示用于指导CPT的约束类型和示例,并根据设定的抽象目标获得具体目标。", "metrics": {"bleu_score": 46.732353406180216, "chrf_score": 35.85786812802627, "xcomet_score": 0.7711523175239563, "xcomet_qe_score": 0.7409987449645996, "metricx_score": 3.4748687744140625, "metricx_qe_score": 3.5282907485961914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "指导GPT使用通用关键脚本来实现特定目标。", "metrics": {"bleu_score": 6.054506513220935, "chrf_score": 8.294278686320856, "xcomet_score": 0.6135631203651428, "xcomet_qe_score": 0.688389778137207, "metricx_score": 3.9559125900268555, "metricx_qe_score": 3.7792131900787354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,推导出一个过滤模型以选择物理脚本。", "metrics": {"bleu_score": 28.68321564427412, "chrf_score": 25.319785113493204, "xcomet_score": 0.787680983543396, "xcomet_qe_score": 0.7605738043785095, "metricx_score": 4.475208282470703, "metricx_qe_score": 4.208101749420166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和女孩转换为指令GPT嵌入,并计算余弦相似度作为语义相似度的相似度得分。 (Note: The original English sentence contains some unclear terminology and grammar, so the translation tries to maintain the structure while making it grammatically correct in Chinese. \"Scripts and girls\" might need further context to be accurately translated, and \"meth semantic similarity\" seems to be a typo or specialized term, which is kept as is in the translation.)", "metrics": {"bleu_score": 18.33809633822236, "chrf_score": 20.05091404346831, "xcomet_score": 0.3939099907875061, "xcomet_qe_score": 0.38556721806526184, "metricx_score": 10.160533905029297, "metricx_qe_score": 8.664925575256348, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们会向包含目标约束关键词的脚本颁发奖项。", "metrics": {"bleu_score": 53.606731510320586, "chrf_score": 51.333003193846594, "xcomet_score": 0.9150694608688354, "xcomet_qe_score": 0.8232897520065308, "metricx_score": 1.0828109979629517, "metricx_qe_score": 1.3391166925430298, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "仅在目标目标在目标站点上得分最高时,我们才会保留该脚本。", "metrics": {"bleu_score": 45.97006398721532, "chrf_score": 39.35879479968427, "xcomet_score": 0.7854325771331787, "xcomet_qe_score": 0.5249015092849731, "metricx_score": 5.012831211090088, "metricx_qe_score": 5.5923380851745605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,可指导性地生成更高质量的螺丝。", "metrics": {"bleu_score": 51.84989152196749, "chrf_score": 33.91712468499434, "xcomet_score": 0.6540935039520264, "xcomet_qe_score": 0.6638891696929932, "metricx_score": 7.08799409866333, "metricx_qe_score": 7.332937717437744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义、完整性和对约束的忠实性方面显著提高了可规划性。", "metrics": {"bleu_score": 47.78890355041061, "chrf_score": 41.22429376583525, "xcomet_score": 0.9052217602729797, "xcomet_qe_score": 0.9048401713371277, "metricx_score": 1.6132128238677979, "metricx_qe_score": 2.2337164878845215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型部署成本高昂,因此有必要增强较小的、专业化模型的语言规划能力。", "metrics": {"bleu_score": 47.62590229261002, "chrf_score": 40.62535339639489, "xcomet_score": 0.9978779554367065, "xcomet_qe_score": 0.9865387678146362, "metricx_score": 0.43352073431015015, "metricx_qe_score": 0.5596357583999634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是这一过程中的关键步骤。", "metrics": {"bleu_score": 33.400530952640366, "chrf_score": 28.716745216744314, "xcomet_score": 0.9992868900299072, "xcomet_qe_score": 0.9999393224716187, "metricx_score": 0.13629932701587677, "metricx_qe_score": 0.21325798332691193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究并不能实现针对具体目标的规划,而手动数据集标注成本高昂。", "metrics": {"bleu_score": 47.636735749489404, "chrf_score": 40.208124786632666, "xcomet_score": 0.9924912452697754, "xcomet_qe_score": 0.9773951768875122, "metricx_score": 0.9381422996520996, "metricx_qe_score": 1.462157964706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循象征性知识蒸馏的理念,从生活语言模型中蒸馏出受限语言规划数据集。", "metrics": {"bleu_score": 51.24542972844807, "chrf_score": 43.453249622434534, "xcomet_score": 0.7542394995689392, "xcomet_qe_score": 0.6792908906936646, "metricx_score": 4.704358100891113, "metricx_qe_score": 4.064643383026123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将应用我们的方法来构建一个受限语言规划数据集,命名为CodeScri。", "metrics": {"bleu_score": 42.39644789296539, "chrf_score": 38.13249132004635, "xcomet_score": 0.9060369729995728, "xcomet_qe_score": 0.8921156525611877, "metricx_score": 3.564789056777954, "metricx_qe_score": 5.0829010009765625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了五万五千个具体目标,", "metrics": {"bleu_score": 27.080524311589805, "chrf_score": 19.19281190814866, "xcomet_score": 0.9446789026260376, "xcomet_qe_score": 0.9233257174491882, "metricx_score": 2.115654230117798, "metricx_qe_score": 2.8832287788391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并附有脚本,以确保验证和测试站点质量。我们要求众包工人最终修订错误样本中的收入。", "metrics": {"bleu_score": 28.265360518322048, "chrf_score": 26.709752903564965, "xcomet_score": 0.3432188034057617, "xcomet_qe_score": 0.13089300692081451, "metricx_score": 9.192525863647461, "metricx_qe_score": 9.951972007751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图展示了代码脚本的约束分布。", "metrics": {"bleu_score": 37.5022891676693, "chrf_score": 23.913122665200788, "xcomet_score": 0.856412947177887, "xcomet_qe_score": 0.8248428106307983, "metricx_score": 3.078789234161377, "metricx_qe_score": 3.9585700035095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现Coscript在生成的特定目标中表现出高度的多元化。", "metrics": {"bleu_score": 68.93665549290876, "chrf_score": 64.54172267241623, "xcomet_score": 0.9697972536087036, "xcomet_qe_score": 0.9301584959030151, "metricx_score": 1.9228262901306152, "metricx_qe_score": 3.739449977874756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过Coscript,我们可以处理更小但更专业的模型,以进行受限的语言规划。", "metrics": {"bleu_score": 31.98541489182186, "chrf_score": 29.731630239718115, "xcomet_score": 0.6970701217651367, "xcomet_qe_score": 0.667618989944458, "metricx_score": 3.4980382919311523, "metricx_qe_score": 3.3070638179779053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过调整模型大小,我们发现当在合适的数据集上进行适当训练时,较小的模型可以压制较大的模型,并能生成与发质特征和大多数大型模型相关的脚本,这表明了模型大小的重要性。", "metrics": {"bleu_score": 31.78744901592945, "chrf_score": 28.076456125479194, "xcomet_score": 0.36259663105010986, "xcomet_qe_score": 0.21156509220600128, "metricx_score": 5.466274261474609, "metricx_qe_score": 6.856627464294434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了受限语言规划问题。", "metrics": {"bleu_score": 72.83860464220109, "chrf_score": 71.99900522537406, "xcomet_score": 0.9069202542304993, "xcomet_qe_score": 0.8321535587310791, "metricx_score": 1.9986854791641235, "metricx_qe_score": 2.526510000228882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了大型语言模型的受限语言规划能力,并提出了一种针对大型语言模型的过度生成过滤方法。", "metrics": {"bleu_score": 47.69091747285073, "chrf_score": 41.1517633762446, "xcomet_score": 0.8353837728500366, "xcomet_qe_score": 0.728392481803894, "metricx_score": 3.545334815979004, "metricx_qe_score": 4.187265872955322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成了一个高质量的正方形数据集,称为CodeCri,用于受限语言规划。", "metrics": {"bleu_score": 46.97191004597844, "chrf_score": 41.91842769532847, "xcomet_score": 0.7249085903167725, "xcomet_qe_score": 0.6611106991767883, "metricx_score": 5.673816680908203, "metricx_qe_score": 6.080974578857422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望CodeSscript数据集能成为推动语言规划研究的有价值资源。", "metrics": {"bleu_score": 58.41177597069676, "chrf_score": 60.604255467450905, "xcomet_score": 0.9183075428009033, "xcomet_qe_score": 0.9108917713165283, "metricx_score": 2.236090660095215, "metricx_qe_score": 3.1061816215515137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查找有关Codecri的更多详细信息。", "metrics": {"bleu_score": 69.85342056580099, "chrf_score": 53.27506475858066, "xcomet_score": 0.8780475854873657, "xcomet_qe_score": 0.8372393250465393, "metricx_score": 3.4619228839874268, "metricx_qe_score": 4.205228328704834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Shu H。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 30.089038982213523, "xcomet_score": 0.8713439702987671, "xcomet_qe_score": 0.8665665984153748, "metricx_score": 0.8357945680618286, "metricx_qe_score": 1.9964929819107056, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将展示我们论文的成果:Connell (2003)中的命名实体识别器在2023年是否仍能有效工作。", "metrics": {"bleu_score": 30.82398735234032, "chrf_score": 34.045617987964704, "xcomet_score": 0.8641573190689087, "xcomet_qe_score": 0.7713029980659485, "metricx_score": 3.799839496612549, "metricx_qe_score": 3.2599620819091797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.9978755712509155, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究论文探讨了泛化问题,采用命名实体识别任务(NER任务)作为研究对象。", "metrics": {"bleu_score": 29.49731696525339, "chrf_score": 32.299807159334456, "xcomet_score": 0.9661041498184204, "xcomet_qe_score": 0.9496990442276001, "metricx_score": 1.516722321510315, "metricx_qe_score": 2.672093152999878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到模型们几乎已经使用ConONO 2003来开发命名实体识别(NER)近20年了,这自然而然地引发了几个问题。", "metrics": {"bleu_score": 18.713697538520655, "chrf_score": 24.35342627016268, "xcomet_score": 0.7619966268539429, "xcomet_qe_score": 0.7349258661270142, "metricx_score": 6.781413555145264, "metricx_qe_score": 6.866185188293457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据上?", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 41.583209046194796, "xcomet_score": 0.9268343448638916, "xcomet_qe_score": 0.9278680086135864, "metricx_score": 0.34351852536201477, "metricx_qe_score": 0.3737318217754364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新的标签工具时,良好的泛化能力需要什么?", "metrics": {"bleu_score": 40.82292684498038, "chrf_score": 31.722986479595207, "xcomet_score": 0.8365895748138428, "xcomet_qe_score": 0.8059206008911133, "metricx_score": 0.6090923547744751, "metricx_qe_score": 0.7281221151351929, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们观察到泛化能力差,是什么原因导致这些模型的性能下降?", "metrics": {"bleu_score": 33.59696856755292, "chrf_score": 27.646830580239534, "xcomet_score": 0.9905529022216797, "xcomet_qe_score": 0.9781142473220825, "metricx_score": 0.933985710144043, "metricx_qe_score": 0.7916854023933411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了Connell++数据集。这是", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 42.1327957287954, "xcomet_score": 0.7960724830627441, "xcomet_qe_score": 0.8420028686523438, "metricx_score": 5.312466621398926, "metricx_qe_score": 2.056448459625244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透社新闻中收集的2020年数据集,并根据相同的Connell 2003标注指南对其进行了标注。", "metrics": {"bleu_score": 44.4347877795939, "chrf_score": 39.38664644831986, "xcomet_score": 0.8332678079605103, "xcomet_qe_score": 0.7503762245178223, "metricx_score": 4.241105556488037, "metricx_qe_score": 3.9448447227478027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后在 Conal 2003 上对 20 多个模型进行了微调。", "metrics": {"bleu_score": 56.75001761639643, "chrf_score": 48.3415564791272, "xcomet_score": 0.8694967031478882, "xcomet_qe_score": 0.8769016861915588, "metricx_score": 3.315960168838501, "metricx_qe_score": 3.225260019302368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 Con O3 测试集和 Cono 加上第一个测试集上对它们进行了评估。", "metrics": {"bleu_score": 45.4366306964514, "chrf_score": 47.6698890066859, "xcomet_score": 0.4360017776489258, "xcomet_qe_score": 0.41894474625587463, "metricx_score": 10.159741401672363, "metricx_qe_score": 11.290616035461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们计算了F1值的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 50.611072234388196, "chrf_score": 56.05677178685795, "xcomet_score": 0.9876506328582764, "xcomet_qe_score": 0.9893369674682617, "metricx_score": 0.6024377942085266, "metricx_qe_score": 0.8457399606704712, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的概括需要什么?", "metrics": {"bleu_score": 22.915026372483176, "chrf_score": 20.623051599291358, "xcomet_score": 0.920441746711731, "xcomet_qe_score": 0.9147618412971497, "metricx_score": 1.5199949741363525, "metricx_qe_score": 0.6356233358383179, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必不可少的。", "metrics": {"bleu_score": 21.62050865049026, "chrf_score": 20.47444138723015, "xcomet_score": 0.9961169958114624, "xcomet_qe_score": 1.0, "metricx_score": 0.380794495344162, "metricx_qe_score": 0.7221799492835999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现变压器模型通常能更好地对新数据进行归纳。", "metrics": {"bleu_score": 42.56737846773503, "chrf_score": 27.306403474596575, "xcomet_score": 0.8271112442016602, "xcomet_qe_score": 0.8362008333206177, "metricx_score": 1.874449610710144, "metricx_qe_score": 1.897698163986206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型规模。", "metrics": {"bleu_score": 63.8194179668201, "chrf_score": 59.09235313234967, "xcomet_score": 0.9858014583587646, "xcomet_qe_score": 0.8824237585067749, "metricx_score": 1.7902395725250244, "metricx_qe_score": 1.9330796003341675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常较大规模的模型具有更好的泛化能力。", "metrics": {"bleu_score": 39.99905887888884, "chrf_score": 32.87290630337263, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4113702178001404, "metricx_qe_score": 0.5005305409431458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们都知道微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 51.2691819618139, "chrf_score": 59.876557108622194, "xcomet_score": 0.9417649507522583, "xcomet_qe_score": 0.9170168042182922, "metricx_score": 3.607921600341797, "metricx_qe_score": 2.110914707183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现更多的微调示例实际上也导致了更好的泛化。", "metrics": {"bleu_score": 68.400614363836, "chrf_score": 59.602366681684074, "xcomet_score": 0.9595348238945007, "xcomet_qe_score": 0.8367005586624146, "metricx_score": 0.7031287550926208, "metricx_qe_score": 0.9073576927185059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的下一个问题是,哪些因素导致了某些模型性能的下降? 我们有两个假设。", "metrics": {"bleu_score": 39.424583593671954, "chrf_score": 32.946840941286524, "xcomet_score": 0.9954397678375244, "xcomet_qe_score": 1.0, "metricx_score": 0.8925174474716187, "metricx_qe_score": 0.8376159071922302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是适应性过拟合,即通过反复使用相同的测试集来支付过拟合的成本,这通常表现为新测试集上的收益递减。", "metrics": {"bleu_score": 58.519683001240956, "chrf_score": 49.27320473848801, "xcomet_score": 0.8665728569030762, "xcomet_qe_score": 0.7975887060165405, "metricx_score": 3.2633562088012695, "metricx_qe_score": 3.9698586463928223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间间隔逐渐增大而导致的性能下降。", "metrics": {"bleu_score": 65.12772949366064, "chrf_score": 61.0911977203752, "xcomet_score": 0.9695290327072144, "xcomet_qe_score": 0.8898763060569763, "metricx_score": 1.4166030883789062, "metricx_qe_score": 1.958247423171997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于给定过拟合,我们从右图可以看到,红色最佳拟合直线的斜率大于1。", "metrics": {"bleu_score": 45.338259308992896, "chrf_score": 35.687024283033736, "xcomet_score": 0.8676623106002808, "xcomet_qe_score": 0.7980634570121765, "metricx_score": 1.802051067352295, "metricx_qe_score": 2.4568729400634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在 Colo 2003 上做出的每一次改进,都转化为 Colo++ 上的超过一次改进,这说明不存在递减收益。", "metrics": {"bleu_score": 31.550587167190606, "chrf_score": 32.60347338777532, "xcomet_score": 0.632949709892273, "xcomet_qe_score": 0.6822274923324585, "metricx_score": 7.7011308670043945, "metricx_qe_score": 7.541632652282715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这向我们展示了这种情况下没有观察到适应性过拟合。", "metrics": {"bleu_score": 36.94654605751221, "chrf_score": 30.94274879744962, "xcomet_score": 0.8726509809494019, "xcomet_qe_score": 0.8224622011184692, "metricx_score": 2.509809970855713, "metricx_qe_score": 3.0167951583862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么它的温度如何?", "metrics": {"bleu_score": 9.535414040914192, "chrf_score": 5.787037037037037, "xcomet_score": 0.18312004208564758, "xcomet_qe_score": 0.15625058114528656, "metricx_score": 4.836219787597656, "metricx_qe_score": 4.970104217529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,重新训练或继续使用更近期数据预训练一些模型,发现随着时间差距的增大,性能会下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 57.36051400335714, "chrf_score": 49.917038825090124, "xcomet_score": 0.8535670638084412, "xcomet_qe_score": 0.9226012825965881, "metricx_score": 1.9619940519332886, "metricx_qe_score": 2.128866195678711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型规模以及更多的微调示例,", "metrics": {"bleu_score": 82.50398234595713, "chrf_score": 80.53312978411796, "xcomet_score": 0.974774181842804, "xcomet_qe_score": 0.9374690651893616, "metricx_score": 0.9352855086326599, "metricx_qe_score": 1.3300361633300781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些目标必须齐头并进。我们不能只拥有其中一种条件,而忽略其他条件。", "metrics": {"bleu_score": 14.482189302397742, "chrf_score": 13.496909976303778, "xcomet_score": 0.8858692049980164, "xcomet_qe_score": 0.8636761903762817, "metricx_score": 1.984273910522461, "metricx_qe_score": 2.122913360595703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现这里的性能下降是由时间漂移引起的,令人惊讶的是,它不是由自适应拟合引起的,尽管Connell 2003年提出的方法已经使用了20多年。", "metrics": {"bleu_score": 57.47484629408123, "chrf_score": 48.40926335862193, "xcomet_score": 0.830680251121521, "xcomet_qe_score": 0.7992046475410461, "metricx_score": 3.251809597015381, "metricx_qe_score": 3.173384428024292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们在论文开头提出的问题,Carnal 2003年的标注器在2023年仍然有效吗?", "metrics": {"bleu_score": 48.800813550870465, "chrf_score": 39.833462851640185, "xcomet_score": 0.7513183951377869, "xcomet_qe_score": 0.7548981308937073, "metricx_score": 5.595531940460205, "metricx_qe_score": 5.031968116760254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,答案实际上是确凿的“是”。", "metrics": {"bleu_score": 18.605335292758287, "chrf_score": 22.518852031820423, "xcomet_score": 0.9697844982147217, "xcomet_qe_score": 0.9799066185951233, "metricx_score": 1.3367396593093872, "metricx_qe_score": 1.2418265342712402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望本文能够呼吁更多关于如何改进模型泛化能力的研究。", "metrics": {"bleu_score": 55.60335612120309, "chrf_score": 47.86225722914768, "xcomet_score": 0.9238120317459106, "xcomet_qe_score": 0.9266766905784607, "metricx_score": 1.0022426843643188, "metricx_qe_score": 0.8057245016098022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查阅我们的论文、数据集,如有任何疑问,欢迎随时联系我。", "metrics": {"bleu_score": 38.002760609155885, "chrf_score": 30.82201638364019, "xcomet_score": 0.988312840461731, "xcomet_qe_score": 0.9742195010185242, "metricx_score": 0.20486462116241455, "metricx_qe_score": 0.19852587580680847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将讨论我们在解决间接微分表达以进行实体选择方面的工作,其中我们引入了替代实体语料库。", "metrics": {"bleu_score": 20.375965892386887, "chrf_score": 16.50795070945345, "xcomet_score": 0.7909202575683594, "xcomet_qe_score": 0.7996594905853271, "metricx_score": 5.909201622009277, "metricx_qe_score": 5.033252239227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(Nín hǎo, wǒ jiāng tǎolùn wǒmen zài jiějué jìndìng fēiqiān biǎodá yǐ jìnxíng shìtǐ xuǎnzé fāngmiàn de gōngzuò, zījiù wǒmen jìnrùle tìshèng shìtǐ yǔliào kù.) 我的名字是贾瓦德·霍塞尼(Javad Hosseini),这是我与菲利普·拉德林斯基(Philipp Radlinsky)、西尔维亚·帕里蒂(Sylvia Parity)和安妮·希腊(Annie Greece)的合作成果。", "metrics": {"bleu_score": 3.7596147042531576, "chrf_score": 30.853452526002727, "xcomet_score": 0.24315781891345978, "xcomet_qe_score": 0.2173328548669815, "metricx_score": 11.388428688049316, "metricx_qe_score": 12.413152694702148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标是理解用户在做出选择时的语言,", "metrics": {"bleu_score": 63.911674912072804, "chrf_score": 61.1081736461897, "xcomet_score": 0.9719622135162354, "xcomet_qe_score": 0.8992586731910706, "metricx_score": 1.0353206396102905, "metricx_qe_score": 1.3425049781799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并考虑这个替代问题:", "metrics": {"bleu_score": 31.239399369202552, "chrf_score": 23.955026455026452, "xcomet_score": 0.8570436239242554, "xcomet_qe_score": 0.8275251388549805, "metricx_score": 0.7855743169784546, "metricx_qe_score": 0.7768244743347168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是想选“对我来说简单”还是“我有一种感", "metrics": {"bleu_score": 6.228496954013446, "chrf_score": 4.971548118481594, "xcomet_score": 0.14633232355117798, "xcomet_qe_score": 0.1440802663564682, "metricx_score": 4.8233513832092285, "metricx_qe_score": 5.186787128448486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "觉”?这里,用户想在这两首歌中选择其一。", "metrics": {"bleu_score": 51.614788627132576, "chrf_score": 48.378589822745965, "xcomet_score": 0.5337066650390625, "xcomet_qe_score": 0.4214625954627991, "metricx_score": 6.150948524475098, "metricx_qe_score": 8.023329734802246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是直接引用,例如说歌曲的名字在我这里,或者其位置是在第一个。", "metrics": {"bleu_score": 26.241519753054288, "chrf_score": 21.46530067245294, "xcomet_score": 0.6515162587165833, "xcomet_qe_score": 0.6165516972541809, "metricx_score": 7.259064674377441, "metricx_qe_score": 7.873669147491455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时,间接引用更合适,可以让对话更自然。例如,", "metrics": {"bleu_score": 11.1204122745073, "chrf_score": 15.0687021584014, "xcomet_score": 0.8373655080795288, "xcomet_qe_score": 0.8058401346206665, "metricx_score": 1.274275779724121, "metricx_qe_score": 1.4725871086120605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户记不住歌曲的名字时,就可能发生这种情况。", "metrics": {"bleu_score": 35.80872028345551, "chrf_score": 29.24328035483783, "xcomet_score": 0.994438648223877, "xcomet_qe_score": 0.9919525384902954, "metricx_score": 0.29555046558380127, "metricx_qe_score": 0.28588807582855225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发音彼此过于相似,难以区分。", "metrics": {"bleu_score": 22.792296655031148, "chrf_score": 20.697981473308467, "xcomet_score": 0.8914570808410645, "xcomet_qe_score": 0.9707316160202026, "metricx_score": 1.5574777126312256, "metricx_qe_score": 0.649678111076355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定偏好时。以下是直接差异的一", "metrics": {"bleu_score": 27.47912587281734, "chrf_score": 21.509916600183594, "xcomet_score": 0.734346330165863, "xcomet_qe_score": 0.3869348168373108, "metricx_score": 5.060774803161621, "metricx_qe_score": 1.8838739395141602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些示例,例如较新的一个或不那么有活力的标志。", "metrics": {"bleu_score": 5.57292470936274, "chrf_score": 8.313551963015394, "xcomet_score": 0.2823428511619568, "xcomet_qe_score": 0.27735427021980286, "metricx_score": 13.379133224487305, "metricx_qe_score": 13.644962310791016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在对话系统中和对基准测试大型语言模型(LLM)的实体理解能力方面,这是一个重要问题。", "metrics": {"bleu_score": 36.930701826904965, "chrf_score": 31.8089263025507, "xcomet_score": 0.8419713973999023, "xcomet_qe_score": 0.8502634763717651, "metricx_score": 2.815452814102173, "metricx_qe_score": 3.3619613647460938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不了解一个公开的数据集,一个大规模的公开数据集,适用于某个任务。因此,我们通过众包标注方式收集了一个数据集。", "metrics": {"bleu_score": 11.128562940773458, "chrf_score": 17.526928653034986, "xcomet_score": 0.6803368330001831, "xcomet_qe_score": 0.65552818775177, "metricx_score": 5.597371578216553, "metricx_qe_score": 5.127862930297852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和接待。", "metrics": {"bleu_score": 66.7619194068951, "chrf_score": 58.75788429852825, "xcomet_score": 0.8309105634689331, "xcomet_qe_score": 0.7702537775039673, "metricx_score": 4.146537780761719, "metricx_qe_score": 5.625415802001953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:\"reception\" 在此上下文中可能指\"接待\"或\"接受\",具体含义需根据上下文判断。) 数据集收集方法强调非正式性,使用卡通完成集。", "metrics": {"bleu_score": 24.006732872005347, "chrf_score": 34.44522755490391, "xcomet_score": 0.3201376795768738, "xcomet_qe_score": 0.26355066895484924, "metricx_score": 6.316749572753906, "metricx_qe_score": 6.277100086212158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通中有三个对话框。", "metrics": {"bleu_score": 57.813962871807846, "chrf_score": 52.12919975802998, "xcomet_score": 0.8273282051086426, "xcomet_qe_score": 0.7139605283737183, "metricx_score": 0.46173980832099915, "metricx_qe_score": 0.577390193939209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框中,鲍勃说:“记得我们昨天听的那首歌吗?”鲍", "metrics": {"bleu_score": 52.37705041286052, "chrf_score": 48.50188322904082, "xcomet_score": 0.6445097327232361, "xcomet_qe_score": 0.6486583948135376, "metricx_score": 4.6842427253723145, "metricx_qe_score": 0.97017502784729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "勃由此设定了对话的背景。 在爱丽丝说", "metrics": {"bleu_score": 15.844501337268932, "chrf_score": 12.420051353874884, "xcomet_score": 0.4038439691066742, "xcomet_qe_score": 0.2959943413734436, "metricx_score": 6.962695598602295, "metricx_qe_score": 7.303101539611816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的第二个对话框中,她说的是:“你是说对我来说容易,还是我有预感?”", "metrics": {"bleu_score": 12.178021069411955, "chrf_score": 8.276869277635445, "xcomet_score": 0.48614394664764404, "xcomet_qe_score": 0.38841161131858826, "metricx_score": 4.836033821105957, "metricx_qe_score": 4.685826778411865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:根据上下文,这句话可以有多种翻译方式,这里尽量保持了原句的结构和含义。) 是替代任务,", "metrics": {"bleu_score": 1.0700143107304574, "chrf_score": 3.0864197530864197, "xcomet_score": 0.23095908761024475, "xcomet_qe_score": 0.23085954785346985, "metricx_score": 7.811370849609375, "metricx_qe_score": 8.170814514160156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用间接引用来选择这些实体之一,例如新朋友。", "metrics": {"bleu_score": 19.150070053463203, "chrf_score": 16.549569673173945, "xcomet_score": 0.6304107308387756, "xcomet_qe_score": 0.6374272108078003, "metricx_score": 6.070242881774902, "metricx_qe_score": 5.18156623840332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自动提供第一和第二个对话框,但第三个对话框由标注者手动填写。", "metrics": {"bleu_score": 58.39328604633569, "chrf_score": 49.802638634689075, "xcomet_score": 0.9104715585708618, "xcomet_qe_score": 0.8799015879631042, "metricx_score": 1.234653353691101, "metricx_qe_score": 1.1916520595550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话框从每个领域的手动提示中选择。", "metrics": {"bleu_score": 50.84094287050262, "chrf_score": 44.13557661196042, "xcomet_score": 0.7895546555519104, "xcomet_qe_score": 0.8255043029785156, "metricx_score": 1.0602625608444214, "metricx_qe_score": 1.7858009338378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即备选问题,是按以下方式生成的。", "metrics": {"bleu_score": 14.400124446705304, "chrf_score": 16.626669796191788, "xcomet_score": 0.9265819787979126, "xcomet_qe_score": 0.966843843460083, "metricx_score": 0.32800549268722534, "metricx_qe_score": 0.42264053225517273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "始终使用简单的模板,", "metrics": {"bleu_score": 52.31223689135345, "chrf_score": 47.56888591744326, "xcomet_score": 0.9520375728607178, "xcomet_qe_score": 0.8988426327705383, "metricx_score": 0.32073259353637695, "metricx_qe_score": 0.26411789655685425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说A还是B,其中", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.06063572173902, "xcomet_score": 0.8359787464141846, "xcomet_qe_score": 0.7861765623092651, "metricx_score": 2.8045897483825684, "metricx_qe_score": 1.1639037132263184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "A和B是来自维基百科的样本。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9087059497833252, "xcomet_qe_score": 0.9016185998916626, "metricx_score": 0.8474584221839905, "metricx_qe_score": 1.1842151880264282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同采样方法,", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9869769811630249, "xcomet_qe_score": 0.9776345491409302, "metricx_score": 0.19790196418762207, "metricx_qe_score": 0.2616245150566101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体之间会变得更加相似,通常更难进行歧义消除。", "metrics": {"bleu_score": 52.07958133637565, "chrf_score": 49.697708975088396, "xcomet_score": 0.8472758531570435, "xcomet_qe_score": 0.8218706846237183, "metricx_score": 3.600844621658325, "metricx_qe_score": 4.013828277587891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是统一的服装(uniform 的直译,但在中文语境下更合适的表达是“统一着装”) (注意:由于原文中\"uniformre\"似乎是一个打字错误,应为\"uniform\",因此我将其翻译为“统一的服装”。) 完整翻译如下: 第一个是统一着装。 (第 一 个 是 统一 的 着装 。) (Dì yī gè shì tóngyì de zhezhuāng.)", "metrics": {"bleu_score": 1.5320718035968717, "chrf_score": 4.593703626741279, "xcomet_score": 0.3248734772205353, "xcomet_qe_score": 0.2744744122028351, "metricx_score": 4.890912055969238, "metricx_qe_score": 4.9896769523620605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个情况是实体具有相似的标题,例如两本书都名为《零售》。", "metrics": {"bleu_score": 26.087213732293563, "chrf_score": 21.207899693624483, "xcomet_score": 0.7396266460418701, "xcomet_qe_score": 0.7283750772476196, "metricx_score": 4.2205119132995605, "metricx_qe_score": 5.40279483795166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是它们在维基百科上有相似的描述,", "metrics": {"bleu_score": 94.85438370694514, "chrf_score": 94.54783814110439, "xcomet_score": 0.9950923919677734, "xcomet_qe_score": 0.9776251316070557, "metricx_score": 0.44528838992118835, "metricx_qe_score": 0.46483975648880005, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及最终它们在维基百科上有相似的资讯声音或属性", "metrics": {"bleu_score": 46.997395980026965, "chrf_score": 39.11451883991929, "xcomet_score": 0.6650338172912598, "xcomet_qe_score": 0.6008583307266235, "metricx_score": 6.391994476318359, "metricx_qe_score": 7.111781597137451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如相同的类型或相同的艺术家。", "metrics": {"bleu_score": 27.56001678723635, "chrf_score": 26.527940292762427, "xcomet_score": 0.939967691898346, "xcomet_qe_score": 0.8278558254241943, "metricx_score": 2.2040045261383057, "metricx_qe_score": 2.489840030670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们向美籍参与者展示这个替代问题,他们知道这些实体的名称,但并不一定了解实体本身。", "metrics": {"bleu_score": 41.35837383128069, "chrf_score": 35.7371224913428, "xcomet_score": 0.6555279493331909, "xcomet_qe_score": 0.6450228691101074, "metricx_score": 4.83104133605957, "metricx_qe_score": 4.537676811218262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们所做的是,为歌曲中的两个实体展示一些背景知识。", "metrics": {"bleu_score": 39.13516286480063, "chrf_score": 37.23001595128749, "xcomet_score": 0.7906578779220581, "xcomet_qe_score": 0.7995055913925171, "metricx_score": 4.057778358459473, "metricx_qe_score": 4.762210845947266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地提供一个谷歌搜索链接到每首歌曲。 (注:在中文语境中,\"entities\" 翻译为“实体”可能不太准确,但考虑到上下文,这里保留原文含义,直接翻译为“两个实体”。“Google search link”翻译为“谷歌搜索链接”。) 然后请标注人员至少听每首歌曲的一部分,并阅读关于每首歌曲的内容。", "metrics": {"bleu_score": 12.664221289086175, "chrf_score": 19.721295162131067, "xcomet_score": 0.5565808415412903, "xcomet_qe_score": 0.5634281039237976, "metricx_score": 5.420811653137207, "metricx_qe_score": 5.174302577972412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是例如谷歌搜索结果中关于歌曲《Easy Answer》的条目。", "metrics": {"bleu_score": 26.058929270445663, "chrf_score": 27.771900892188768, "xcomet_score": 0.6300429701805115, "xcomet_qe_score": 0.6149923801422119, "metricx_score": 5.455957412719727, "metricx_qe_score": 5.686071395874023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示了来自维基百科的背景文本。", "metrics": {"bleu_score": 63.13476599314376, "chrf_score": 54.19210175474897, "xcomet_score": 0.9804984331130981, "xcomet_qe_score": 0.9193463325500488, "metricx_score": 0.9178977608680725, "metricx_qe_score": 1.5036317110061646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科展示了它们的图片,以便标注者了解它们的样貌。", "metrics": {"bleu_score": 25.218301845292295, "chrf_score": 22.255241336962346, "xcomet_score": 0.8480626344680786, "xcomet_qe_score": 0.880057692527771, "metricx_score": 1.924821138381958, "metricx_qe_score": 1.9766254425048828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注员从这些实体中选择一个,例如,这里的第一个,并使用三到五个间接指代表达方式来描述它们。", "metrics": {"bleu_score": 55.41867356541268, "chrf_score": 49.42353730091105, "xcomet_score": 0.8366327285766602, "xcomet_qe_score": 0.8069248199462891, "metricx_score": 3.068467855453491, "metricx_qe_score": 3.1704556941986084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,带有钢琴音乐的那一个,这里", "metrics": {"bleu_score": 11.980914528565991, "chrf_score": 14.546925071967848, "xcomet_score": 0.896987795829773, "xcomet_qe_score": 0.8944560885429382, "metricx_score": 3.4661357402801514, "metricx_qe_score": 1.3230807781219482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有我们数据集中的一些示例", "metrics": {"bleu_score": 56.79460569156625, "chrf_score": 54.82434542839071, "xcomet_score": 0.8256492614746094, "xcomet_qe_score": 0.8016828298568726, "metricx_score": 2.0924994945526123, "metricx_qe_score": 3.019458532333374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如没有歌词的那一个,不是那首有12岁男孩演唱的,也不是那首虚构的,或者来自阿塞拜疆的,等等。", "metrics": {"bleu_score": 27.05923114555524, "chrf_score": 27.422251961074593, "xcomet_score": 0.8608391284942627, "xcomet_qe_score": 0.9080617427825928, "metricx_score": 3.0047452449798584, "metricx_qe_score": 3.504481792449951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "替代语料库包含三个领域的 6,000 个替代问题,以及 42,000 个间接指代表达结果。", "metrics": {"bleu_score": 26.25996281830108, "chrf_score": 32.455042857602656, "xcomet_score": 0.5450952053070068, "xcomet_qe_score": 0.5125815272331238, "metricx_score": 4.342041969299316, "metricx_qe_score": 4.212347984313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用 T5 大模型的实验结果总结如下:", "metrics": {"bleu_score": 19.71584187923815, "chrf_score": 23.580182072829132, "xcomet_score": 0.8833673000335693, "xcomet_qe_score": 0.8588401675224304, "metricx_score": 1.7161134481430054, "metricx_qe_score": 1.772843360900879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者完全相同的背景知识,那么准确率会非常高,大约在92%到95%之间。", "metrics": {"bleu_score": 63.66408032704386, "chrf_score": 57.24642421423021, "xcomet_score": 0.9042733907699585, "xcomet_qe_score": 0.9885326623916626, "metricx_score": 0.9912989735603333, "metricx_qe_score": 0.6616532802581787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这在现实中是不可行的。", "metrics": {"bleu_score": 13.545994273378138, "chrf_score": 23.700887198986063, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09328046441078186, "metricx_qe_score": 0.11937843263149261, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问部分重叠的背景知识,那么准确率在82%到87%之间,这更符合实际情况", "metrics": {"bleu_score": 68.17658990858149, "chrf_score": 62.03689792228847, "xcomet_score": 0.9226853847503662, "xcomet_qe_score": 0.9136360287666321, "metricx_score": 0.8123430609703064, "metricx_qe_score": 1.1492154598236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9841159582138062, "xcomet_qe_score": 0.9635313749313354, "metricx_score": 0.9222766160964966, "metricx_qe_score": 1.0717954635620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型仅能访问实体名称,那么准确率仅为6%,因此改进空间巨大。", "metrics": {"bleu_score": 47.52131870424069, "chrf_score": 38.21773237567957, "xcomet_score": 0.8344876766204834, "xcomet_qe_score": 0.8242806196212769, "metricx_score": 8.271310806274414, "metricx_qe_score": 8.071002006530762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还展示了模型在此处的领域泛化能力。", "metrics": {"bleu_score": 11.7942240532671, "chrf_score": 13.146542845097233, "xcomet_score": 0.8702771067619324, "xcomet_qe_score": 0.8625680208206177, "metricx_score": 1.4000890254974365, "metricx_qe_score": 1.3711110353469849, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接,", "metrics": {"bleu_score": 89.31539818068698, "chrf_score": 87.72426647426647, "xcomet_score": 0.98248291015625, "xcomet_qe_score": 0.9742215275764465, "metricx_score": 0.3001736104488373, "metricx_qe_score": 0.2520325183868408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自特伦托大学的莎拉·帕皮,以及来自布鲁诺·凯斯勒基金会的福阿·西恩。我将简要介绍一下我们与马特奥·内格里和马可·杜奇合作的一篇论文,题目是“注意力作为同时语音翻", "metrics": {"bleu_score": 33.391436537390504, "chrf_score": 27.34850494510669, "xcomet_score": 0.3948749601840973, "xcomet_qe_score": 0.3422965705394745, "metricx_score": 7.496657371520996, "metricx_qe_score": 7.009898662567139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "译的指南”。 同时", "metrics": {"bleu_score": 6.567274736060395, "chrf_score": 4.166666666666666, "xcomet_score": 0.16623133420944214, "xcomet_qe_score": 0.15529301762580872, "metricx_score": 20.433528900146484, "metricx_qe_score": 17.215856552124023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "口语翻译,或称为实时口语翻译(simSD),是指将一种语言的口语实时翻译成另一种语言的文本,从而实现跨语言交流。", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 42.46665061377311, "xcomet_score": 0.9537310600280762, "xcomet_qe_score": 0.969390332698822, "metricx_score": 4.22728157043457, "metricx_qe_score": 4.436666965484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前SimST模型存在哪些问题?", "metrics": {"bleu_score": 28.646290158800984, "chrf_score": 28.971999330404426, "xcomet_score": 0.923907995223999, "xcomet_qe_score": 0.9300552010536194, "metricx_score": 1.0365267992019653, "metricx_qe_score": 1.4330404996871948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常会训练特定的架构,引入需要优化的额外模块。", "metrics": {"bleu_score": 8.903250061620762, "chrf_score": 19.270917534896377, "xcomet_score": 0.9583421945571899, "xcomet_qe_score": 0.8127033114433289, "metricx_score": 0.6493297815322876, "metricx_qe_score": 1.5014511346817017, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如涉及不同优化目标的训练,", "metrics": {"bleu_score": 45.00467676796104, "chrf_score": 53.51474332007379, "xcomet_score": 0.8212145566940308, "xcomet_qe_score": 0.7611014246940613, "metricx_score": 5.373177528381348, "metricx_qe_score": 4.84627103805542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "需要漫长而复杂的训练过程。 训练和维护多个模型以达到不同的延迟等", "metrics": {"bleu_score": 46.131415640884086, "chrf_score": 45.180515961542255, "xcomet_score": 0.9158967733383179, "xcomet_qe_score": 0.8866922855377197, "metricx_score": 2.237238645553589, "metricx_qe_score": 1.4313136339187622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "级,例如,训练一个模型使其平均延迟为一秒钟,另一个模型为两秒钟,以此类推。", "metrics": {"bleu_score": 41.50534534058261, "chrf_score": 35.6811105597564, "xcomet_score": 0.7502015233039856, "xcomet_qe_score": 0.664726972579956, "metricx_score": 3.506378173828125, "metricx_qe_score": 5.33223819732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首次使用已存在的离线 SD 模型,无需重新训练或采用特定的 SSD 架构。", "metrics": {"bleu_score": 46.55147013576309, "chrf_score": 39.87544167569787, "xcomet_score": 0.5054452419281006, "xcomet_qe_score": 0.35506221652030945, "metricx_score": 9.02857494354248, "metricx_qe_score": 9.4385404586792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为每个延迟制度仅使用一个模型,并通过特定参数处理延迟。", "metrics": {"bleu_score": 48.35135794756827, "chrf_score": 40.12822911910494, "xcomet_score": 0.9358199834823608, "xcomet_qe_score": 0.9153788089752197, "metricx_score": 1.892972707748413, "metricx_qe_score": 1.3637477159500122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并利用模型通过音频输入和文本输出之间的张力机制(即交叉张力机制", "metrics": {"bleu_score": 53.66889812553754, "chrf_score": 48.19661320334593, "xcomet_score": 0.6340054273605347, "xcomet_qe_score": 0.5948721766471863, "metricx_score": 7.84863805770874, "metricx_qe_score": 6.038729667663574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")已获得的知识,您可以在右边看到一个示例。", "metrics": {"bleu_score": 19.554016376461796, "chrf_score": 16.531226482523838, "xcomet_score": 0.35227683186531067, "xcomet_qe_score": 0.289087176322937, "metricx_score": 7.413140296936035, "metricx_qe_score": 11.328849792480469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出点或编码器去装饰注意力(dot or encoder de-coral attention),这是一种策略,我们根据注意力指向的位置决定是否输出部分翻译。", "metrics": {"bleu_score": 45.2381600555451, "chrf_score": 37.99027242274614, "xcomet_score": 0.5824916362762451, "xcomet_qe_score": 0.5573675632476807, "metricx_score": 12.083982467651367, "metricx_qe_score": 13.184294700622559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果张力未集中,即在过去的lambda语音帧中该和值低于某个阈值alpha,则发出一个词,这意味着接收到的信息足够稳定。", "metrics": {"bleu_score": 38.63277260357594, "chrf_score": 31.48407028153601, "xcomet_score": 0.5041500329971313, "xcomet_qe_score": 0.4774545431137085, "metricx_score": 6.549385070800781, "metricx_qe_score": 6.097072124481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们接收到一个包含“I'm going to talk about”的语音片段,我们的模型会预测并翻译成德语。 我们将查看交叉注意力权重。 我们会发现,前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,作为lambda语音帧。", "metrics": {"bleu_score": 49.39377355940439, "chrf_score": 48.13096518332645, "xcomet_score": 0.5712437629699707, "xcomet_qe_score": 0.5368479490280151, "metricx_score": 4.627140045166016, "metricx_qe_score": 5.146053791046143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个字将被发射。 由于交叉张力的总和超过了特定阈值 α,我们不会发出最后一个词,而是等待下一个语音片段。", "metrics": {"bleu_score": 33.54138743804175, "chrf_score": 26.53000426492842, "xcomet_score": 0.6248133182525635, "xcomet_qe_score": 0.6049543619155884, "metricx_score": 7.67770528793335, "metricx_qe_score": 7.5300445556640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续进行,接收到另一个语音片段,并且我们的模型预测出超过三个词,我们将考察交叉注意力权重。 我们将看到,没有任何词语指向最后的羊羔话语框架。", "metrics": {"bleu_score": 39.45163931086059, "chrf_score": 36.787343876613306, "xcomet_score": 0.4854241609573364, "xcomet_qe_score": 0.43273216485977173, "metricx_score": 6.656960964202881, "metricx_qe_score": 6.300739288330078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个单词将被发出。", "metrics": {"bleu_score": 47.855439210937384, "chrf_score": 38.86889545481519, "xcomet_score": 0.9408911466598511, "xcomet_qe_score": 0.9240285158157349, "metricx_score": 0.975865364074707, "metricx_qe_score": 1.911775827407837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果你查看点运算的主要结果。 我们在图表上绘制了同时进行的页翻译结果,图表的一侧使用蓝色表示翻译质量和平均延迟。 这就是延迟度量。我们还考虑了计算感知平均值,该值考虑了模型预测输出的计算时间。", "metrics": {"bleu_score": 22.32754670179021, "chrf_score": 19.06166538545759, "xcomet_score": 0.4620225429534912, "xcomet_qe_score": 0.36255544424057007, "metricx_score": 9.024370193481445, "metricx_qe_score": 9.051941871643066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望我们的疗法在这个图表中尽可能地处于高位。", "metrics": {"bleu_score": 15.639686545296055, "chrf_score": 20.06496225891, "xcomet_score": 0.8188444375991821, "xcomet_qe_score": 0.7604663372039795, "metricx_score": 5.27529764175415, "metricx_qe_score": 5.168648719787598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也希望它们被向左移动。", "metrics": {"bleu_score": 70.4805090506219, "chrf_score": 58.664326379244905, "xcomet_score": 0.9274437427520752, "xcomet_qe_score": 0.9229534864425659, "metricx_score": 0.9441552758216858, "metricx_qe_score": 1.2963991165161133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将与离线模型中应用的其他策略进行比较,这些策略包括与K策略和本地一致性策略。", "metrics": {"bleu_score": 24.624087743244765, "chrf_score": 17.994656698987033, "xcomet_score": 0.692168116569519, "xcomet_qe_score": 0.669891357421875, "metricx_score": 5.391861438751221, "metricx_qe_score": 5.63943338394165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将与专为同时语音翻译量身定制的最先进架构进行比较。", "metrics": {"bleu_score": 54.973631130147794, "chrf_score": 55.10743383937356, "xcomet_score": 0.9142141342163086, "xcomet_qe_score": 0.8729942440986633, "metricx_score": 1.3397865295410156, "metricx_qe_score": 1.918104887008667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是同时速记翻译策略在德语上的结果。", "metrics": {"bleu_score": 39.03594390682206, "chrf_score": 32.510046861062904, "xcomet_score": 0.8240410685539246, "xcomet_qe_score": 0.8171628713607788, "metricx_score": 5.745272159576416, "metricx_qe_score": 4.73699951171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,怀疑策略在离线模型中表现优于所有其他策略,因为曲线整体向左偏移。", "metrics": {"bleu_score": 21.775675059323095, "chrf_score": 20.74693172439514, "xcomet_score": 0.8830089569091797, "xcomet_qe_score": 0.8547922968864441, "metricx_score": 2.3812854290008545, "metricx_qe_score": 2.944133996963501, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际经过的时间或计算耗时,那就是最快的策略。", "metrics": {"bleu_score": 46.162707188289886, "chrf_score": 39.68716891482382, "xcomet_score": 0.9437581300735474, "xcomet_qe_score": 0.9441988468170166, "metricx_score": 2.5969791412353516, "metricx_qe_score": 2.5555715560913086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您希望发现更多结果,请阅读我们的论文,", "metrics": {"bleu_score": 51.0032342952127, "chrf_score": 44.25348336156323, "xcomet_score": 0.9478415250778198, "xcomet_qe_score": 0.9476445317268372, "metricx_score": 0.6824670433998108, "metricx_qe_score": 0.4328309893608093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源了代码和模型,并同时输出以促进我们工作的可复现性。", "metrics": {"bleu_score": 10.630643627589146, "chrf_score": 17.327180077091075, "xcomet_score": 0.8609133958816528, "xcomet_qe_score": 0.8361285328865051, "metricx_score": 1.1268640756607056, "metricx_qe_score": 1.595489740371704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我是伊恩,我的同事吉恩和我将向大家展示我们关于多指令(Multi-Instruct)的研究,通过指令微调提升多模态社交学习。", "metrics": {"bleu_score": 21.341971350793212, "chrf_score": 28.495495580287685, "xcomet_score": 0.4597669243812561, "xcomet_qe_score": 0.4339671730995178, "metricx_score": 6.703941822052002, "metricx_qe_score": 5.576785564422607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索重新利用预训练语言模型的新学习范式,以参数和数据高效的方式应用于不同的下游任务。", "metrics": {"bleu_score": 64.71457334773073, "chrf_score": 56.092579176153954, "xcomet_score": 0.8463124632835388, "xcomet_qe_score": 0.826776385307312, "metricx_score": 1.5666848421096802, "metricx_qe_score": 2.05499529838562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优使大型语言模型能够通过遵循自然指令,以简短高效的方式完成未见过的任务。", "metrics": {"bleu_score": 36.220013941858134, "chrf_score": 33.22521529781785, "xcomet_score": 0.7949159145355225, "xcomet_qe_score": 0.7186870574951172, "metricx_score": 1.9389779567718506, "metricx_qe_score": 2.0653810501098633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前关于指令调优的大部分工作都集中在提高语言仅有的序列任务性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 37.358308246400476, "chrf_score": 33.69771432362021, "xcomet_score": 0.8745688796043396, "xcomet_qe_score": 0.8121610879898071, "metricx_score": 2.2117931842803955, "metricx_qe_score": 2.2940447330474854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们希望探讨在多模态蛋白质训练模型上进行指令调优是否能够实际提升对未见多模态任务的泛化能力。", "metrics": {"bleu_score": 36.32791778568862, "chrf_score": 32.64007192058862, "xcomet_score": 0.8019341230392456, "xcomet_qe_score": 0.7382024526596069, "metricx_score": 3.5524256229400635, "metricx_qe_score": 3.955131769180298, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现在指令数据集的可用性上,RP和多模态之间存在显著差异。 ", "metrics": {"bleu_score": 34.991027366184746, "chrf_score": 29.001486999070625, "xcomet_score": 0.8150793313980103, "xcomet_qe_score": 0.7582651376724243, "metricx_score": 2.185659408569336, "metricx_qe_score": 2.888408899307251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过1600个仅限午餐的指令任务,", "metrics": {"bleu_score": 45.788313721339826, "chrf_score": 51.725785151139355, "xcomet_score": 0.7031530141830444, "xcomet_qe_score": 0.594780445098877, "metricx_score": 7.261053562164307, "metricx_qe_score": 8.246541023254395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而没有大规模公开可用的多模态指令任务。", "metrics": {"bleu_score": 59.67093540757056, "chrf_score": 52.45617276190485, "xcomet_score": 0.9012383222579956, "xcomet_qe_score": 0.7928259372711182, "metricx_score": 1.6630939245224, "metricx_qe_score": 2.3470077514648438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这激励我们构建一个多模态指令调优数据集。", "metrics": {"bleu_score": 76.74174160136336, "chrf_score": 70.5002322450835, "xcomet_score": 0.9718524217605591, "xcomet_qe_score": 0.9641896486282349, "metricx_score": 0.8962436318397522, "metricx_qe_score": 1.0525983572006226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此我们介绍 Multi-insstruct,这是第一个多模态指令调优基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个广泛类别。", "metrics": {"bleu_score": 44.924654949645955, "chrf_score": 41.55982136677868, "xcomet_score": 0.8129281401634216, "xcomet_qe_score": 0.7858096361160278, "metricx_score": 2.882267713546753, "metricx_qe_score": 3.175295352935791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "任务源自21个现有开源数据集,每个任务配备了五条明确的书面指令。", "metrics": {"bleu_score": 49.87400016000747, "chrf_score": 43.435504317146936, "xcomet_score": 0.8186303973197937, "xcomet_qe_score": 0.8268400430679321, "metricx_score": 1.089387059211731, "metricx_qe_score": 1.4234354496002197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究多模态指令微调的是我们提出的数据集,我们采用了一个统一的多模态训练模型作为我们的基础模型,该模型使用", "metrics": {"bleu_score": 47.34048209419175, "chrf_score": 41.67530483825647, "xcomet_score": 0.6918693780899048, "xcomet_qe_score": 0.6122970581054688, "metricx_score": 7.054936408996582, "metricx_qe_score": 4.591928005218506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个统一的词汇表来处理语言、图像标记和边界框的坐标。 (注:\"ofFA\" 和 \"ofFA\" 在原文中似乎是错误的写法,应为 \"off\" 或 \"on\",这里翻译为 \"处理\" 来表达该模型的功能。)", "metrics": {"bleu_score": 15.74759729855044, "chrf_score": 27.601881424929907, "xcomet_score": 0.33663177490234375, "xcomet_qe_score": 0.27731531858444214, "metricx_score": 7.410600662231445, "metricx_qe_score": 7.635981559753418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们展示了我们多模态数据集中的一些示例实例。 统一处理各种输入和输出数据类型。", "metrics": {"bleu_score": 62.78064625315192, "chrf_score": 45.37601514426098, "xcomet_score": 0.7947859764099121, "xcomet_qe_score": 0.7378787994384766, "metricx_score": 2.2870640754699707, "metricx_qe_score": 2.7216577529907227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,将所有任务统一为序列到序列格式,在该格式中", "metrics": {"bleu_score": 51.53978826718985, "chrf_score": 49.75783712975526, "xcomet_score": 0.7087268829345703, "xcomet_qe_score": 0.7335436940193176, "metricx_score": 3.7479028701782227, "metricx_qe_score": 4.234772682189941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",输入文本、图像、指令和边界框在同一标记空间中表示。", "metrics": {"bleu_score": 66.22312744455333, "chrf_score": 60.90159563802837, "xcomet_score": 0.9496505260467529, "xcomet_qe_score": 0.920018196105957, "metricx_score": 1.431187629699707, "metricx_qe_score": 1.7373710870742798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好了,现在我要谈谈多模态指令微调。", "metrics": {"bleu_score": 64.8138893454484, "chrf_score": 60.97365869424692, "xcomet_score": 0.9153221845626831, "xcomet_qe_score": 0.9008731245994568, "metricx_score": 0.7149470448493958, "metricx_qe_score": 0.756284236907959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用N组中的53个任务进行训练,并每个任务采样10,000个实例。", "metrics": {"bleu_score": 60.23587925328831, "chrf_score": 57.38042469457607, "xcomet_score": 0.8131734132766724, "xcomet_qe_score": 0.8310732841491699, "metricx_score": 4.060237407684326, "metricx_qe_score": 4.546619892120361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识阅读组进行测试,并从WiQ和杂项组中额外选择五个任务。", "metrics": {"bleu_score": 31.175234510246334, "chrf_score": 25.49906231669038, "xcomet_score": 0.6492194533348083, "xcomet_qe_score": 0.600998044013977, "metricx_score": 5.693308353424072, "metricx_qe_score": 5.5895185470581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务的测试速度中的所有实例。", "metrics": {"bleu_score": 46.57113642968181, "chrf_score": 37.007298791142404, "xcomet_score": 0.78767329454422, "xcomet_qe_score": 0.7717157602310181, "metricx_score": 5.089679718017578, "metricx_qe_score": 4.880311965942383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试速度中随机采样20个任务,作为NRP的同一任务。 因此,", "metrics": {"bleu_score": 40.63105884155229, "chrf_score": 38.04512738839635, "xcomet_score": 0.3943466544151306, "xcomet_qe_score": 0.4787575304508209, "metricx_score": 8.59807300567627, "metricx_qe_score": 8.374616622924805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一个预训练的OFA大型模型作为基础模型。", "metrics": {"bleu_score": 82.32490471721698, "chrf_score": 85.53120011521331, "xcomet_score": 0.9636353254318237, "xcomet_qe_score": 0.9542060494422913, "metricx_score": 1.465710163116455, "metricx_qe_score": 3.166785717010498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们混合了所有任务的所有实例。", "metrics": {"bleu_score": 87.25129388059685, "chrf_score": 81.134368154492, "xcomet_score": 0.819719672203064, "xcomet_qe_score": 0.7874776124954224, "metricx_score": 1.1428027153015137, "metricx_qe_score": 1.7761423587799072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与其中的五个指令模板之一组合。", "metrics": {"bleu_score": 60.003429061308495, "chrf_score": 52.20349891576013, "xcomet_score": 0.9883068799972534, "xcomet_qe_score": 0.9022892713546753, "metricx_score": 0.6575995683670044, "metricx_qe_score": 0.8733726739883423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每次任务的测试中,我们通过使用每次实验中的五个指令来评估模型,总共进行5次实验。", "metrics": {"bleu_score": 38.09750884154769, "chrf_score": 30.692354424207196, "xcomet_score": 0.8693903684616089, "xcomet_qe_score": 0.8733453750610352, "metricx_score": 1.8024705648422241, "metricx_qe_score": 1.7507413625717163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告了所有5个实验中性能的平均值、最大值以及性能的标准差。", "metrics": {"bleu_score": 35.27900076150819, "chrf_score": 29.0392192297304, "xcomet_score": 0.9401661157608032, "xcomet_qe_score": 0.9564970135688782, "metricx_score": 2.4929797649383545, "metricx_qe_score": 2.0299935340881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们报告准确率(accuracy)。", "metrics": {"bleu_score": 47.17991357336538, "chrf_score": 38.09860222686544, "xcomet_score": 0.9351342916488647, "xcomet_qe_score": 0.9909108877182007, "metricx_score": 3.0313241481781006, "metricx_qe_score": 2.330860137939453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,我们报告rootjL。对于RP任务,我们也报告RujL。", "metrics": {"bleu_score": 56.38909821116625, "chrf_score": 39.87878554381478, "xcomet_score": 0.6335976719856262, "xcomet_qe_score": 0.6023708581924438, "metricx_score": 6.010042190551758, "metricx_qe_score": 6.441596984863281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一种额外的评估指标,称为敏感度。", "metrics": {"bleu_score": 52.88822910722114, "chrf_score": 45.880314377426714, "xcomet_score": 0.9862104654312134, "xcomet_qe_score": 0.997316837310791, "metricx_score": 0.4157159626483917, "metricx_qe_score": 0.4610040485858917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个指标测量模型在指令措辞略有不同时,能否一致地为同一任务产生相同输出的能力。", "metrics": {"bleu_score": 35.21578183200172, "chrf_score": 30.052355854303542, "xcomet_score": 0.9424527883529663, "xcomet_qe_score": 0.9354114532470703, "metricx_score": 4.469812393188477, "metricx_qe_score": 4.904630661010742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要结果。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9698837995529175, "xcomet_qe_score": 0.88059002161026, "metricx_score": 0.1918793022632599, "metricx_qe_score": 0.3046000003814697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提升OFE在相同多模态任务上的表现。", "metrics": {"bleu_score": 53.08684724877998, "chrf_score": 41.30401500776264, "xcomet_score": 0.8463169932365417, "xcomet_qe_score": 0.8312976360321045, "metricx_score": 3.1343908309936523, "metricx_qe_score": 3.208418369293213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然语言指令数据集上的迁移学习也有利于指令微调。", "metrics": {"bleu_score": 29.207762131941138, "chrf_score": 24.273287148557905, "xcomet_score": 0.8235901594161987, "xcomet_qe_score": 0.8266282677650452, "metricx_score": 1.4417431354522705, "metricx_qe_score": 2.1103878021240234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着任务数量的增加,我们可以看到模型取得了更好的性能,同时敏感度也降低了。", "metrics": {"bleu_score": 40.050826688351684, "chrf_score": 35.02776122283015, "xcomet_score": 0.9579752683639526, "xcomet_qe_score": 0.9738466739654541, "metricx_score": 2.0497522354125977, "metricx_qe_score": 2.278132915496826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们也进行了一个实验。", "metrics": {"bleu_score": 39.34995962231127, "chrf_score": 44.358384806787285, "xcomet_score": 0.9519253969192505, "xcomet_qe_score": 0.9591836929321289, "metricx_score": 0.34846624732017517, "metricx_qe_score": 0.2799772024154663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一条指令与五条指令进行对比。", "metrics": {"bleu_score": 57.30574043798692, "chrf_score": 56.79356243652529, "xcomet_score": 0.9700078964233398, "xcomet_qe_score": 0.8062330484390259, "metricx_score": 0.824509859085083, "metricx_qe_score": 2.1953284740448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多的指令可以显著提高模型的整体性能,并降低其敏感度。", "metrics": {"bleu_score": 58.81870824220392, "chrf_score": 50.77426296466544, "xcomet_score": 0.9995946884155273, "xcomet_qe_score": 1.0, "metricx_score": 0.7390437722206116, "metricx_qe_score": 0.7835805416107178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这展示了不同前调优策略对模型灵敏度的影响。", "metrics": {"bleu_score": 67.73709971213142, "chrf_score": 60.87275992268252, "xcomet_score": 0.8926243782043457, "xcomet_qe_score": 0.8856229186058044, "metricx_score": 1.9588136672973633, "metricx_qe_score": 2.107207775115967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们通过从自然指令数据集进行迁移学习所看到的,模型可以实现比原始 IFA 模型高得多的灵敏度。", "metrics": {"bleu_score": 56.60712634057472, "chrf_score": 47.360336321580625, "xcomet_score": 0.9568630456924438, "xcomet_qe_score": 0.8830168843269348, "metricx_score": 3.231882095336914, "metricx_qe_score": 3.8232431411743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从 Nitro 指令数据集进行迁移学习能够帮助 OFA 在 NitroE 指令数据集上取得更优异的表现。", "metrics": {"bleu_score": 47.15376790165303, "chrf_score": 38.01847390838102, "xcomet_score": 0.7042863368988037, "xcomet_qe_score": 0.5987993478775024, "metricx_score": 6.358941555023193, "metricx_qe_score": 6.893956184387207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,我们提出了第一个大规模多模态指令调优数据集。通过持续改进OFA的神经能力,我们探索了不同的迁移学习技术并展示了其优势。", "metrics": {"bleu_score": 51.980975953184206, "chrf_score": 48.17700971876817, "xcomet_score": 0.7919227480888367, "xcomet_qe_score": 0.7552057504653931, "metricx_score": 3.715402841567993, "metricx_qe_score": 4.899737358093262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一种称为灵敏度的新的度量标准。", "metrics": {"bleu_score": 36.327039079325615, "chrf_score": 31.159987386767575, "xcomet_score": 0.9966707229614258, "xcomet_qe_score": 0.9783592224121094, "metricx_score": 0.6561675667762756, "metricx_qe_score": 0.5720500946044922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们还收集了一个更大规模的多模态指令调优数据集,包含约150个额外的变体语言任务,我们将发布它们。", "metrics": {"bleu_score": 32.47619205770072, "chrf_score": 29.978243465610017, "xcomet_score": 0.6694290041923523, "xcomet_qe_score": 0.6255706548690796, "metricx_score": 2.7812187671661377, "metricx_qe_score": 2.7957746982574463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Koovsna,很高兴欢迎大家参加我们关于ACL 2023论文的讨论。", "metrics": {"bleu_score": 82.51750499088548, "chrf_score": 68.53243357049017, "xcomet_score": 0.6534106135368347, "xcomet_qe_score": 0.620222270488739, "metricx_score": 3.5647618770599365, "metricx_qe_score": 4.327792644500732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型的可接受性判断并不总是对上下文具有鲁棒性。", "metrics": {"bleu_score": 72.21331733891775, "chrf_score": 70.83614612684731, "xcomet_score": 0.8232255578041077, "xcomet_qe_score": 0.7836357355117798, "metricx_score": 2.8358073234558105, "metricx_qe_score": 4.8552632331848145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一项与John Baqui、Aaron Muller、Kanishka Mishra、Karen Fs、Roger Levy和Atina Williams的合作成果。", "metrics": {"bleu_score": 15.672461847009084, "chrf_score": 59.00580697797009, "xcomet_score": 0.5388331413269043, "xcomet_qe_score": 0.4529436528682709, "metricx_score": 7.051054000854492, "metricx_qe_score": 7.013912200927734, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们重新审视了最简对范式。", "metrics": {"bleu_score": 40.276720463657746, "chrf_score": 37.10798121855234, "xcomet_score": 0.89307701587677, "xcomet_qe_score": 0.8838869333267212, "metricx_score": 1.5370862483978271, "metricx_qe_score": 2.02675724029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最小的对顶向基本上是根据可接受性判断来评估语言模型,这", "metrics": {"bleu_score": 35.00894102478207, "chrf_score": 29.46755741984481, "xcomet_score": 0.6308495402336121, "xcomet_qe_score": 0.6200008392333984, "metricx_score": 7.598736763000488, "metricx_qe_score": 4.807143211364746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,比如BLIMP、语法健身房,或者在成见方面如人群对的可接受性。", "metrics": {"bleu_score": 26.56454087751723, "chrf_score": 18.23342595398552, "xcomet_score": 0.6378159523010254, "xcomet_qe_score": 0.6545538902282715, "metricx_score": 6.283048629760742, "metricx_qe_score": 6.504434585571289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这一最简对照范式中,评估语言模型的典型方法是展示一个可接受的句子或语法句,然后展示一个不可接受的句子或不语法句。", "metrics": {"bleu_score": 45.55553248542617, "chrf_score": 42.44203912860186, "xcomet_score": 0.7361373901367188, "xcomet_qe_score": 0.8029673099517822, "metricx_score": 2.0191993713378906, "metricx_qe_score": 2.9651641845703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型基本上将更高的概率赋予可接受的解决方案。", "metrics": {"bleu_score": 25.169669587818404, "chrf_score": 22.6171314987453, "xcomet_score": 0.8753489255905151, "xcomet_qe_score": 0.8234057426452637, "metricx_score": 3.004056692123413, "metricx_qe_score": 4.307251453399658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 MPP 管道基本不允许我们评估模型对长句子的接受程度。", "metrics": {"bleu_score": 74.98646433648474, "chrf_score": 68.54784727396375, "xcomet_score": 0.8201904296875, "xcomet_qe_score": 0.7707256078720093, "metricx_score": 1.4228060245513916, "metricx_qe_score": 2.852018356323242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,大型语言模型的上下文处理能力越来越强,能处理更长的", "metrics": {"bleu_score": 29.113216578145625, "chrf_score": 35.64403523961525, "xcomet_score": 0.7552236914634705, "xcomet_qe_score": 0.7876547574996948, "metricx_score": 3.941725730895996, "metricx_qe_score": 1.6818541288375854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文。因此,我们必须在整个上下文窗口中评估模型的可接受性。 这就是我们在这里试图做的事情。", "metrics": {"bleu_score": 50.49960959098784, "chrf_score": 49.52608096380231, "xcomet_score": 0.690308690071106, "xcomet_qe_score": 0.6262768507003784, "metricx_score": 3.36350154876709, "metricx_qe_score": 4.448584079742432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视 NPP 管道。 所以", "metrics": {"bleu_score": 49.99148090271511, "chrf_score": 46.1643517579442, "xcomet_score": 0.6137734651565552, "xcomet_qe_score": 0.5333285927772522, "metricx_score": 5.1004533767700195, "metricx_qe_score": 4.7288432121276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9963313341140747, "xcomet_qe_score": 0.9761532545089722, "metricx_score": 0.22746601700782776, "metricx_qe_score": 0.7089755535125732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过重新审视数据集本身来模拟这些更长的序列,然后通过从这些数据集中选择可接受或不可接受的句子来重新构建句子。", "metrics": {"bleu_score": 72.19958558689919, "chrf_score": 66.21706490801719, "xcomet_score": 0.9302459955215454, "xcomet_qe_score": 0.8347460031509399, "metricx_score": 2.0589146614074707, "metricx_qe_score": 2.8587381839752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们从BbliIM数据集中选取了一个典型的从属岛案例,以展示语法的自动性。 (For reference: 例如,我们从BbliIM数据集中选取了一个典型的从属岛案例,以展示“从属岛结构的自动性”。 这里,“从属岛”翻译自\"adjunct island\",“自动性”翻译自\"maticity\"。)", "metrics": {"bleu_score": 7.017993701797123, "chrf_score": 21.551219949178712, "xcomet_score": 0.5876402854919434, "xcomet_qe_score": 0.541702926158905, "metricx_score": 5.061220169067383, "metricx_qe_score": 4.042815208435059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重现更长的序列,并判断哪些序列可接受,哪些具有相同的语法结构匹配。", "metrics": {"bleu_score": 51.77112223567368, "chrf_score": 44.19214408535607, "xcomet_score": 0.8463236093521118, "xcomet_qe_score": 0.9260823726654053, "metricx_score": 1.6951677799224854, "metricx_qe_score": 2.4906256198883057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从试点广告中提取语法句子。 然后,我们在可接受的查询和不可接受的查询中都添加一个前缀。 因此,", "metrics": {"bleu_score": 48.82144986909548, "chrf_score": 40.78791408114837, "xcomet_score": 0.4612619876861572, "xcomet_qe_score": 0.3410099446773529, "metricx_score": 6.27685546875, "metricx_qe_score": 6.51979398727417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.9529941082000732, "xcomet_qe_score": 0.7428854703903198, "metricx_score": 1.5263457298278809, "metricx_qe_score": 1.8271386623382568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过选择不同子集或不同数据集中的句子来实现同样的效果。", "metrics": {"bleu_score": 46.23472681073812, "chrf_score": 38.1080777276159, "xcomet_score": 0.9873369932174683, "xcomet_qe_score": 0.9559260606765747, "metricx_score": 0.9357630610466003, "metricx_qe_score": 1.5117775201797485, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的错配场景。 因此,", "metrics": {"bleu_score": 60.26080978557135, "chrf_score": 57.58514132353505, "xcomet_score": 0.786383867263794, "xcomet_qe_score": 0.7737975120544434, "metricx_score": 2.4849853515625, "metricx_qe_score": 2.335399866104126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关数据集,但不是您正在评估的同一数据集。", "metrics": {"bleu_score": 58.901155004394425, "chrf_score": 52.82938731275363, "xcomet_score": 0.9553557634353638, "xcomet_qe_score": 0.7772138118743896, "metricx_score": 1.1635935306549072, "metricx_qe_score": 2.1226277351379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于不可接受的情况,我们也可以这样做。", "metrics": {"bleu_score": 46.663612512230074, "chrf_score": 42.696972610224044, "xcomet_score": 0.9821330308914185, "xcomet_qe_score": 0.9639067649841309, "metricx_score": 0.5520765781402588, "metricx_qe_score": 0.599204957485199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全不相关的领域中选择句子,比如维基百科。", "metrics": {"bleu_score": 58.05800086508495, "chrf_score": 49.1873258490253, "xcomet_score": 0.9944443702697754, "xcomet_qe_score": 0.9337222576141357, "metricx_score": 0.7134301662445068, "metricx_qe_score": 1.2820758819580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将有助于我们了解模型的可接受性判断是否实际上受到任何上下文的影响。 例如,上下文是否来自数据集的不同子集,或者它是否与我们正在分析的句子完全无关。", "metrics": {"bleu_score": 65.09112013571144, "chrf_score": 58.05683594120738, "xcomet_score": 0.9446022510528564, "xcomet_qe_score": 0.9118788242340088, "metricx_score": 1.7891508340835571, "metricx_qe_score": 2.225651979446411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型表现如何?", "metrics": {"bleu_score": 8.646389260097964, "chrf_score": 8.441013286611183, "xcomet_score": 0.8383227586746216, "xcomet_qe_score": 0.844200611114502, "metricx_score": 0.9827795028686523, "metricx_qe_score": 0.27792325615882874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看与当前查询对完全无关的维基百科句子,发现MPP的判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 41.8232321886235, "chrf_score": 37.145238980826726, "xcomet_score": 0.9523773193359375, "xcomet_qe_score": 0.8158807754516602, "metricx_score": 3.5027244091033936, "metricx_qe_score": 5.423163414001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到2024年,以最大化OPT和GPT2模型的潜力。如图", "metrics": {"bleu_score": 53.22898913303927, "chrf_score": 70.5908922261712, "xcomet_score": 0.5706447958946228, "xcomet_qe_score": 0.5876147747039795, "metricx_score": 11.580100059509277, "metricx_qe_score": 14.162084579467773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中的橙色虚线所示,MPP判断相对稳定。", "metrics": {"bleu_score": 53.13269403996457, "chrf_score": 51.682621604931064, "xcomet_score": 0.7504497766494751, "xcomet_qe_score": 0.742681086063385, "metricx_score": 2.075838088989258, "metricx_qe_score": 3.774754285812378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在当我们从同一数据集选择句子时,会发生什么?", "metrics": {"bleu_score": 45.23534916420262, "chrf_score": 35.550578098764746, "xcomet_score": 0.9884246587753296, "xcomet_qe_score": 0.9081543684005737, "metricx_score": 0.7515804171562195, "metricx_qe_score": 1.4750953912734985, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里我们从相同的BlimIM语法gymIM数据集中选择或创建来自可接受和不可接受领域的句子。", "metrics": {"bleu_score": 51.67514541572409, "chrf_score": 36.81779881537761, "xcomet_score": 0.6069294214248657, "xcomet_qe_score": 0.5998610258102417, "metricx_score": 8.185133934020996, "metricx_qe_score": 8.072029113769531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在那里,我们看到当你添加可接受的前缀或不可接受的前缀时,MPP 判断会显著增加或减少。", "metrics": {"bleu_score": 68.26275549173431, "chrf_score": 64.02653074364656, "xcomet_score": 0.7874077558517456, "xcomet_qe_score": 0.7408892512321472, "metricx_score": 3.9632415771484375, "metricx_qe_score": 3.5404233932495117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,即当我们从指责人的税务生成中选择同一现象的句子时, 我们观察到模型的MPP判断出现大幅增加或大幅减少,这取决于所选择的前缀是否可接受。 现在这个——而且", "metrics": {"bleu_score": 40.106798358278716, "chrf_score": 32.678020627610536, "xcomet_score": 0.301805704832077, "xcomet_qe_score": 0.19720050692558289, "metricx_score": 11.928739547729492, "metricx_qe_score": 12.167701721191406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个影响非常显著,就像这种效果随着上下文长度而增加,这很可能会影响到那些具有大上下文窗口的新型语言模型。", "metrics": {"bleu_score": 40.77909812406357, "chrf_score": 43.523232362795994, "xcomet_score": 0.8571383953094482, "xcomet_qe_score": 0.7314667701721191, "metricx_score": 1.8254307508468628, "metricx_qe_score": 1.7811851501464844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么匹配前缀会对语言模型的判断产生如此大的影响? 因此,", "metrics": {"bleu_score": 81.89850197975073, "chrf_score": 81.67160641054082, "xcomet_score": 0.8309619426727295, "xcomet_qe_score": 0.7695428133010864, "metricx_score": 3.7839598655700684, "metricx_qe_score": 1.712070107460022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保持相关结构但向输入句子添加噪声来扰动输入。", "metrics": {"bleu_score": 57.013204054917615, "chrf_score": 49.34824800262281, "xcomet_score": 0.8622497320175171, "xcomet_qe_score": 0.8829268217086792, "metricx_score": 1.6389180421829224, "metricx_qe_score": 2.481445789337158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了几次这样的扰动后, 我们发现,这些噪声实际上都没有使模型在展示给我们的时间支付判断趋势方面改变其路径。", "metrics": {"bleu_score": 45.156402244105045, "chrf_score": 42.99739743928598, "xcomet_score": 0.7852873802185059, "xcomet_qe_score": 0.7510871887207031, "metricx_score": 6.387585639953613, "metricx_qe_score": 7.245993614196777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现这些模型对句子结构的敏感度相似。", "metrics": {"bleu_score": 36.486739368237764, "chrf_score": 33.34353602553314, "xcomet_score": 0.8862056732177734, "xcomet_qe_score": 0.863364577293396, "metricx_score": 1.3725850582122803, "metricx_qe_score": 2.0244126319885254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在可接受的范围内扰动句子时,我们观察到所有扰动的相似增加。当我们在可接受的批准范围内扰动句子时,我们以类似的方式观察到MPP判断的减少。 (注:\"MPP jud", "metrics": {"bleu_score": 40.773609218939974, "chrf_score": 34.8236133628326, "xcomet_score": 0.5062219500541687, "xcomet_qe_score": 0.42641788721084595, "metricx_score": 9.108476638793945, "metricx_qe_score": 6.872579574584961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "gments\" 在此上下文中可能有特定的学术含义,需要根据具体领域进行准确翻译或保留原文。) 我们工作的关键结论是,语言模型对潜藏的句法和语义特征敏感,这些特征在句子间共享。", "metrics": {"bleu_score": 23.701702339878633, "chrf_score": 33.012852992527186, "xcomet_score": 0.23398253321647644, "xcomet_qe_score": 0.26466602087020874, "metricx_score": 7.023252010345459, "metricx_qe_score": 9.643728256225586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而当前的MPP评估方法,通过简短且单一句子的输入,可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 45.15673672371357, "chrf_score": 38.36697466766303, "xcomet_score": 0.8365819454193115, "xcomet_qe_score": 0.8515423536300659, "metricx_score": 1.8981773853302002, "metricx_qe_score": 2.341130256652832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解更多实验细节。", "metrics": {"bleu_score": 34.27163657253172, "chrf_score": 33.44303636597666, "xcomet_score": 0.9978342056274414, "xcomet_qe_score": 0.9995092153549194, "metricx_score": 0.10219424962997437, "metricx_qe_score": 0.11421225965023041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的Just John。", "metrics": {"bleu_score": 72.00391346486707, "chrf_score": 50.920389487430874, "xcomet_score": 0.7158422470092773, "xcomet_qe_score": 0.6161448955535889, "metricx_score": 6.8643903732299805, "metricx_qe_score": 8.203768730163574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将展示我们的研究成果:典范:多自然语言中的跨语言语义分析与手动表示。", "metrics": {"bleu_score": 32.85963198242969, "chrf_score": 23.958873842000887, "xcomet_score": 0.6318099498748779, "xcomet_qe_score": 0.661190390586853, "metricx_score": 6.738146781921387, "metricx_qe_score": 6.210439682006836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义处理是一项构建用户查询的语义表示的任务,例如ZQL和λ演算(Lambda 计算)。", "metrics": {"bleu_score": 41.92061018624394, "chrf_score": 44.25437602725114, "xcomet_score": 0.8786177039146423, "xcomet_qe_score": 0.8448727130889893, "metricx_score": 3.292444944381714, "metricx_qe_score": 3.305143356323242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义分析的任务是将多种自然语言的查询翻译成多种意义表示。", "metrics": {"bleu_score": 76.8272902167946, "chrf_score": 70.5588845985583, "xcomet_score": 0.8741574287414551, "xcomet_qe_score": 0.931083083152771, "metricx_score": 1.8348667621612549, "metricx_qe_score": 3.8536572456359863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将多种自然语言查询翻译成 SQL、Lambda 或 funQL 等形式。", "metrics": {"bleu_score": 65.6697951575367, "chrf_score": 67.000556326824, "xcomet_score": 0.9650712013244629, "xcomet_qe_score": 0.9552056789398193, "metricx_score": 1.2514797449111938, "metricx_qe_score": 1.4790700674057007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义分析模型分别在有限的抛物集和应用集上提出和评估。", "metrics": {"bleu_score": 43.59086937141029, "chrf_score": 36.5635068483523, "xcomet_score": 0.7566238641738892, "xcomet_qe_score": 0.7506042718887329, "metricx_score": 4.163235187530518, "metricx_qe_score": 3.95450496673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如, 在某些自然语言处理的领域,中文的覆盖率存在缺失环节。", "metrics": {"bleu_score": 22.419056820298167, "chrf_score": 30.079422296633172, "xcomet_score": 0.2913013994693756, "xcomet_qe_score": 0.2217530608177185, "metricx_score": 5.101656436920166, "metricx_qe_score": 5.252295970916748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在特定多种表示形式上的覆盖度。 羊羔计算理论缺失。", "metrics": {"bleu_score": 3.9566236357113054, "chrf_score": 4.49905599595485, "xcomet_score": 0.1502721607685089, "xcomet_qe_score": 0.1512606292963028, "metricx_score": 11.201630592346191, "metricx_qe_score": 12.737908363342285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:\"Lamb calculus\" 在此翻译为 \"羊羔计算理论\",但原术语在中文学术界可能没有直接对应翻译,具体翻译应根据上下文和领域确定。) 它们只被评估在特定的神经模型", "metrics": {"bleu_score": 4.529969051723167, "chrf_score": 13.355686838357716, "xcomet_score": 0.38496071100234985, "xcomet_qe_score": 0.3697853684425354, "metricx_score": 6.064001560211182, "metricx_qe_score": 6.700747013092041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上,例如,只有一个单一的模型用来评估。", "metrics": {"bleu_score": 54.017258985951415, "chrf_score": 50.12398369236365, "xcomet_score": 0.7593624591827393, "xcomet_qe_score": 0.6422739028930664, "metricx_score": 2.9490325450897217, "metricx_qe_score": 4.216488361358643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了 Ex 示例器,", "metrics": {"bleu_score": 30.26643726685862, "chrf_score": 18.73583992348297, "xcomet_score": 0.8115171194076538, "xcomet_qe_score": 0.7863094210624695, "metricx_score": 1.9156771898269653, "metricx_qe_score": 2.4974639415740967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为跨语言语义处理在多种自然语言和语义表示中提供统一的数据集示例器。", "metrics": {"bleu_score": 40.33867753221881, "chrf_score": 29.600655116101677, "xcomet_score": 0.8101983070373535, "xcomet_qe_score": 0.8070712685585022, "metricx_score": 3.679211378097534, "metricx_qe_score": 3.2011897563934326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "包含90个病毒域集合,5个税收子项,800万个表示形式,以及15个语系的22种自然语言。", "metrics": {"bleu_score": 25.2510425452482, "chrf_score": 25.83773947857485, "xcomet_score": 0.3690169155597687, "xcomet_qe_score": 0.404022753238678, "metricx_score": 13.747062683105469, "metricx_qe_score": 15.498231887817383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了训练和评估的六种设置。", "metrics": {"bleu_score": 67.8301759715223, "chrf_score": 59.56205276123286, "xcomet_score": 0.9824798107147217, "xcomet_qe_score": 0.9433248043060303, "metricx_score": 1.0820890665054321, "metricx_qe_score": 2.2754859924316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是翻译测试。我们", "metrics": {"bleu_score": 78.60753021519781, "chrf_score": 93.88954066057725, "xcomet_score": 0.8251765966415405, "xcomet_qe_score": 0.7882511615753174, "metricx_score": 2.8861002922058105, "metricx_qe_score": 0.44030365347862244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将使用 Google 翻译 API 将源语言翻译成目标语言,然后使用单语模型对任何评估进行训练。", "metrics": {"bleu_score": 63.85315464776925, "chrf_score": 57.5705231363474, "xcomet_score": 0.7259414196014404, "xcomet_qe_score": 0.6861685514450073, "metricx_score": 5.065473556518555, "metricx_qe_score": 4.507574558258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们使用英语查询训练英语模型,在推理阶段,我们通过 API 将德语查询翻译成英语,然后使用训练好的模型预测 SQL。", "metrics": {"bleu_score": 60.68973333424087, "chrf_score": 55.90250978637561, "xcomet_score": 0.9576395750045776, "xcomet_qe_score": 0.8697717785835266, "metricx_score": 0.9524688720703125, "metricx_qe_score": 1.8243408203125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模型。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9699068069458008, "xcomet_qe_score": 0.8844642639160156, "metricx_score": 0.30868202447891235, "metricx_qe_score": 0.4093308448791504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种设置源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 76.32525777669305, "chrf_score": 70.76121164572685, "xcomet_score": 0.8800749778747559, "xcomet_qe_score": 0.8051424026489258, "metricx_score": 0.9759234189987183, "metricx_qe_score": 1.0493279695510864, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还可以通过仅使用10%的训练数据来训练双语模型,从而测试单语未来设置。 并且", "metrics": {"bleu_score": 39.72325382489667, "chrf_score": 38.10016817732366, "xcomet_score": 0.5295570492744446, "xcomet_qe_score": 0.4886208474636078, "metricx_score": 6.0169477462768555, "metricx_qe_score": 4.756345272064209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对多语言模型进行建模,即我们为所有语言训练一个多语言模型。", "metrics": {"bleu_score": 59.33983447979357, "chrf_score": 64.5113781020187, "xcomet_score": 0.8704022169113159, "xcomet_qe_score": 0.8371888995170593, "metricx_score": 1.8244470357894897, "metricx_qe_score": 2.269355058670044, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询放在一起训练多语言模型,", "metrics": {"bleu_score": 58.77197533882329, "chrf_score": 50.765246157692346, "xcomet_score": 0.9529272317886353, "xcomet_qe_score": 0.9572508335113525, "metricx_score": 1.4072437286376953, "metricx_qe_score": 2.634486198425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理阶段,我们也可以使用这个模型。 将德国查询或中文查询等进行翻译。 (Original English: Um to translate German queries or Chinese query or etc. Chinese Translation: 将德国查询或中文查询等进行翻译。) Note: The English sentence structure is a bit unclear, so the translation maintains the original meaning as closely as possible. In Chinese, it would be more grammatically correct to say \"对德国查询或中文查询等进行翻译\", but since the English sentence starts with \"Um\", which doesn't directly translate, the sentence is structured to preserve the original meaning.", "metrics": {"bleu_score": 10.794365769134151, "chrf_score": 12.357385252552412, "xcomet_score": 0.4928402304649353, "xcomet_qe_score": 0.4678756296634674, "metricx_score": 7.785194396972656, "metricx_qe_score": 6.334024429321289, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑跨语言零样本和零样本迁移。", "metrics": {"bleu_score": 54.208907790027055, "chrf_score": 44.7833190801871, "xcomet_score": 0.6762350797653198, "xcomet_qe_score": 0.6466861963272095, "metricx_score": 4.709476470947266, "metricx_qe_score": 5.523448467254639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,并将其迁移到另一种语言。", "metrics": {"bleu_score": 34.23375720396188, "chrf_score": 28.5869448286931, "xcomet_score": 0.9299507141113281, "xcomet_qe_score": 0.8700629472732544, "metricx_score": 3.299403429031372, "metricx_qe_score": 3.739896774291992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们使用英语查询或英语和德语的短查询组合来训练多语言模型,并预测 SQL 输出。", "metrics": {"bleu_score": 75.2271113240679, "chrf_score": 69.79906582892814, "xcomet_score": 0.8725979328155518, "xcomet_qe_score": 0.7851738929748535, "metricx_score": 1.1418029069900513, "metricx_qe_score": 1.3754627704620361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也发现了许多有趣的结果。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 66.57370407370408, "xcomet_score": 0.9968277215957642, "xcomet_qe_score": 0.9793797731399536, "metricx_score": 0.3356071412563324, "metricx_qe_score": 0.77958083152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在对单语模型的分析中,我们评估了两组模型。 包括encoderPDdR,它代表了多语种预训练编码器与基于指针的解码器,例如X元素加上pdr和bird加上pdr。", "metrics": {"bleu_score": 41.81566467067967, "chrf_score": 34.2462706133727, "xcomet_score": 0.5667471289634705, "xcomet_qe_score": 0.5371807813644409, "metricx_score": 10.166990280151367, "metricx_qe_score": 10.84412670135498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,例如B和Mt5。", "metrics": {"bleu_score": 30.18146852695617, "chrf_score": 17.8350222999385, "xcomet_score": 0.7322057485580444, "xcomet_qe_score": 0.7352696657180786, "metricx_score": 5.245464324951172, "metricx_qe_score": 5.8554887771606445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究发现,编码器-解码器在所有九个数据集上取得了最佳性能。", "metrics": {"bleu_score": 38.16692391925841, "chrf_score": 26.13428716911309, "xcomet_score": 0.9756553173065186, "xcomet_qe_score": 0.9830116033554077, "metricx_score": 1.5021685361862183, "metricx_qe_score": 1.5308434963226318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估我们的Mmt5并在多语言环境中应用xlmr加PDdr示例。 通过", "metrics": {"bleu_score": 12.920725243713852, "chrf_score": 12.718740000301832, "xcomet_score": 0.45899081230163574, "xcomet_qe_score": 0.4586147964000702, "metricx_score": 9.184931755065918, "metricx_qe_score": 6.924952983856201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在多种语言混合训练中进行训练,编码器-解码器或编码器PDR可以得到改进。", "metrics": {"bleu_score": 13.41063964832027, "chrf_score": 11.078373669631894, "xcomet_score": 0.7278006076812744, "xcomet_qe_score": 0.7279304265975952, "metricx_score": 3.5667757987976074, "metricx_qe_score": 3.5711607933044434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,这是因为大多数主要自然语言在七个数据集中的表现有所提升,而英语的表现只有在三个数据集中有所提升,在其他数据集中则有所下降。", "metrics": {"bleu_score": 55.25184762868877, "chrf_score": 52.90878458496036, "xcomet_score": 0.9928268194198608, "xcomet_qe_score": 0.984413743019104, "metricx_score": 1.326069712638855, "metricx_qe_score": 1.6393827199935913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语种的库尔德人。", "metrics": {"bleu_score": 15.237043787179804, "chrf_score": 13.574601504865392, "xcomet_score": 0.6477963924407959, "xcomet_qe_score": 0.5659468770027161, "metricx_score": 11.115890502929688, "metricx_qe_score": 12.40150260925293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝色线条表示跨语言Fu传输,橙色", "metrics": {"bleu_score": 10.580331550093845, "chrf_score": 12.716216908627775, "xcomet_score": 0.5718607306480408, "xcomet_qe_score": 0.5006753206253052, "metricx_score": 9.074191093444824, "metricx_qe_score": 6.0896077156066895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "线条表示跨语言零她传输,而", "metrics": {"bleu_score": 20.448007360218387, "chrf_score": 16.374649718424955, "xcomet_score": 0.42177802324295044, "xcomet_qe_score": 0.3895314335823059, "metricx_score": 9.279165267944336, "metricx_qe_score": 5.161867141723633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿色线条则是单语言设置。", "metrics": {"bleu_score": 22.781556051062047, "chrf_score": 28.60927898979649, "xcomet_score": 0.868606448173523, "xcomet_qe_score": 0.9285702705383301, "metricx_score": 0.6712924838066101, "metricx_qe_score": 0.8339880108833313, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过比较绿色和橙色曲线,我们发现在零短设置时,跨语言转移性能的差距显著;通过比较蓝色和橙色曲线,我们发现在少数短设置时,转移差距迅速缩小。", "metrics": {"bleu_score": 35.10691934302008, "chrf_score": 30.436551973102393, "xcomet_score": 0.531712532043457, "xcomet_qe_score": 0.5940180420875549, "metricx_score": 8.182571411132812, "metricx_qe_score": 6.156948566436768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.28500142691476, "chrf_score": 37.35579235579235, "xcomet_score": 0.9125493764877319, "xcomet_qe_score": 0.9048394560813904, "metricx_score": 0.472987562417984, "metricx_qe_score": 0.9297128915786743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器解码器模型的表现优于传统方法,或取得了可比的结果。专注", "metrics": {"bleu_score": 19.963856196596687, "chrf_score": 13.322367016550746, "xcomet_score": 0.9441637992858887, "xcomet_qe_score": 0.9357242584228516, "metricx_score": 4.334507942199707, "metricx_qe_score": 0.6907053589820862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "于英语自然语言可以显著提升在目标自然语言上的表现。 我们发现,像Coders和Blue这样的多语言语言模型在跨语言半自助分类方面仍然不足。 我们建立了一个统一的基准——ExampleR,用于跨角度的多自然语言语义分析,支持多种表示形式。 (注:\"ExampleR\" 作为特定名称,保持不变。\"sum up\"", "metrics": {"bleu_score": 27.338773585474083, "chrf_score": 34.33428225528118, "xcomet_score": 0.2135811746120453, "xcomet_qe_score": 0.2199699580669403, "metricx_score": 12.634136199951172, "metricx_qe_score": 12.888727188110352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "翻译为 \"总结\" 或 \"归纳\" 可能不符合上下文,这里翻译为 \"我们建立了一个...\" 以保持意思准确。\"cross-angle\" 翻译为 \"跨角度\" 或 \"跨角度的\" 以表达不同视角或方面的含义。\"semantic parsing\" 翻译为 \"语义分析\"。\"multiple natural languages\" 翻译为 \"多自然语言\"。", "metrics": {"bleu_score": 3.868320484684955, "chrf_score": 8.670309117346381, "xcomet_score": 0.2708587050437927, "xcomet_qe_score": 0.297486811876297, "metricx_score": 6.037303447723389, "metricx_qe_score": 6.708479881286621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "\"many representations\" 翻译为 \"多种表示形式\"。) 对三种代表性的多语种语言模型进行全面基准研究,", "metrics": {"bleu_score": 35.76911780765004, "chrf_score": 34.366404783934925, "xcomet_score": 0.6213884949684143, "xcomet_qe_score": 0.30845052003860474, "metricx_score": 4.917241096496582, "metricx_qe_score": 5.582705020904541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的成果展示了许多有趣的发现。", "metrics": {"bleu_score": 68.65065103648593, "chrf_score": 62.40329115329115, "xcomet_score": 0.9208418130874634, "xcomet_qe_score": 0.9117982387542725, "metricx_score": 1.5895968675613403, "metricx_qe_score": 2.710899829864502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.18407042324543, "xcomet_qe_score": 0.16229356825351715, "metricx_score": 3.314579963684082, "metricx_qe_score": 4.823701858520508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文和代码,", "metrics": {"bleu_score": 55.355669024943104, "chrf_score": 55.353632523546906, "xcomet_score": 0.8694818019866943, "xcomet_qe_score": 0.8553562760353088, "metricx_score": 1.5600167512893677, "metricx_qe_score": 2.6395938396453857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "应为 \"GitHub\" 的笔误,此处已更正。)", "metrics": {"bleu_score": 2.445593937240363, "chrf_score": 2.4390243902439024, "xcomet_score": 0.319181889295578, "xcomet_qe_score": 0.25965219736099243, "metricx_score": 4.082050323486328, "metricx_qe_score": 4.666284561157227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位,我的名字是阿尔·维拉德,我将简要概述一篇关于机器翻译评估策略和表现的论文,特别是针对印刷文本的棕榈识别。", "metrics": {"bleu_score": 7.116982665626393, "chrf_score": 8.675828557251617, "xcomet_score": 0.42320898175239563, "xcomet_qe_score": 0.43757855892181396, "metricx_score": 6.660037040710449, "metricx_qe_score": 6.798388481140137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与谷歌翻译团队同事的合作研究。 (注:为了保持准确性和专业性,我假设“printinging palm”是一个特定领域的术语,", "metrics": {"bleu_score": 20.441512835861065, "chrf_score": 32.52607804936031, "xcomet_score": 0.5780714750289917, "xcomet_qe_score": 0.5552944540977478, "metricx_score": 5.0423173904418945, "metricx_qe_score": 5.109763145446777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可能指的是某种识别或处理技术,尽管在英语中这个短语可能有语法错误。) 一个拥有5400亿参数的大型语言模型,于去年2022年发布。", "metrics": {"bleu_score": 27.730709297354753, "chrf_score": 51.63783702801506, "xcomet_score": 0.14922331273555756, "xcomet_qe_score": 0.14051154255867004, "metricx_score": 7.321743965148926, "metricx_qe_score": 9.427043914794922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是在一个包含7800亿个标记的大规模文本集合上训练的。", "metrics": {"bleu_score": 39.6292649574711, "chrf_score": 45.242601406261585, "xcomet_score": 0.8373986482620239, "xcomet_qe_score": 0.7695200443267822, "metricx_score": 1.605804204940796, "metricx_qe_score": 2.307781457901001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于厨房场景,Duma 实现了数百项自然语言处理任务的最新技术水平。 (Note: \"Duma\" 作为专有名称,保持不变。\"State of the art\" 翻译为 \"最新技术水平\",以适应学术或教学材料的语境。)", "metrics": {"bleu_score": 2.6603165972957337, "chrf_score": 6.766097326109613, "xcomet_score": 0.1425652652978897, "xcomet_qe_score": 0.15398134291172028, "metricx_score": 8.344257354736328, "metricx_qe_score": 7.861705303192139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们呈现了对大型语言模型提示用于机器翻译的首次系统研究。", "metrics": {"bleu_score": 37.509160865468914, "chrf_score": 32.416295093642816, "xcomet_score": 0.8767842054367065, "xcomet_qe_score": 0.8704383969306946, "metricx_score": 2.201381206512451, "metricx_qe_score": 2.3235363960266113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用国际机器翻译协会(IMT)的最佳实践,评估此类模型的转换能力。", "metrics": {"bleu_score": 42.14243418322087, "chrf_score": 38.9339603899768, "xcomet_score": 0.9716743230819702, "xcomet_qe_score": 0.7825924158096313, "metricx_score": 2.4953196048736572, "metricx_qe_score": 3.5611166954040527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将我们的系统与最先进的系统进行比较,以评估最佳性能系统或WMT评价。 (WMT 评估应根据具体内容进行更准确的翻译,这里暂时保留原文以避免误解。)", "metrics": {"bleu_score": 17.780217146557202, "chrf_score": 30.224657223005792, "xcomet_score": 0.755302906036377, "xcomet_qe_score": 0.746660053730011, "metricx_score": 4.364302635192871, "metricx_qe_score": 4.497048377990723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用最先进的神经机器翻译(neuralMT)指标,并额外展示基于专家的人类评估结果。", "metrics": {"bleu_score": 60.95912484393607, "chrf_score": 56.18555884461546, "xcomet_score": 0.9472758769989014, "xcomet_qe_score": 0.9454149007797241, "metricx_score": 2.026805877685547, "metricx_qe_score": 3.0984420776367188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些关于提示选择策略的建议。", "metrics": {"bleu_score": 88.6704794791872, "chrf_score": 86.80458945938821, "xcomet_score": 0.8993804454803467, "xcomet_qe_score": 0.851394534111023, "metricx_score": 1.109601378440857, "metricx_qe_score": 1.31083345413208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对翻译任务中的语言模型表现有着显著影响,这可以在一个简单的实验中观察到。在这个实验中,我们使用一个简短的提示,并为不同句子提供两个不同的提示。", "metrics": {"bleu_score": 34.792853302328076, "chrf_score": 34.98897633131732, "xcomet_score": 0.8760538101196289, "xcomet_qe_score": 0.8810670375823975, "metricx_score": 3.2370643615722656, "metricx_qe_score": 2.8178274631500244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(Note: The repeated \"for\"", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1344813108444214, "xcomet_qe_score": 0.08025547116994858, "metricx_score": 20.655183792114258, "metricx_qe_score": 23.04857635498047, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the original English sentence was corrected to make the Chinese translation clearer.) 在1000个句子中的大多数(516个),观察到的差异超过了一个模糊点。", "metrics": {"bleu_score": 2.6449275365764713, "chrf_score": 7.821798666959647, "xcomet_score": 0.14067526161670685, "xcomet_qe_score": 0.14222124218940735, "metricx_score": 20.89508056640625, "metricx_qe_score": 16.92377471923828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可能达到40个模糊点。", "metrics": {"bleu_score": 34.22803488747218, "chrf_score": 24.630748221409966, "xcomet_score": 0.8066881895065308, "xcomet_qe_score": 0.8100771903991699, "metricx_score": 4.785926342010498, "metricx_qe_score": 2.608872175216675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个良好的提示策略至关重要。", "metrics": {"bleu_score": 47.20758038942709, "chrf_score": 41.584577649209166, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.1709868311882019, "metricx_qe_score": 0.32812750339508057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对五次提示策略的解决方案进行了实验,其中我们仅标记我们提供给系统的句子,", "metrics": {"bleu_score": 30.492772460958555, "chrf_score": 27.79120627678694, "xcomet_score": 0.6407154202461243, "xcomet_qe_score": 0.6625808477401733, "metricx_score": 4.346475124359131, "metricx_qe_score": 4.010532855987549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并使用其原始语言。 在这个德语翻译为英文的示例中,源句的德语句子用德语冒号标记,而英文翻译则用英文冒号表示。 发现实际印刷形式对几次短印刷的影响并不大。", "metrics": {"bleu_score": 10.088001822664339, "chrf_score": 12.106434514576863, "xcomet_score": 0.12193892151117325, "xcomet_qe_score": 0.12013690173625946, "metricx_score": 12.897523880004883, "metricx_qe_score": 20.929346084594727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零次和一次性提示非常关键,而当我们像我们案例中那样采用事实一次性提示时,与实际提示形式几乎没有区别。", "metrics": {"bleu_score": 3.344035978652222, "chrf_score": 8.399524265805974, "xcomet_score": 0.14349456131458282, "xcomet_qe_score": 0.13411486148834229, "metricx_score": 6.686516761779785, "metricx_qe_score": 6.306853771209717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为承载大部分重量的例子(在上下文中需要更具体说明) (", "metrics": {"bleu_score": 1.826472886268134, "chrf_score": 2.0325203252032518, "xcomet_score": 0.13987302780151367, "xcomet_qe_score": 0.13815395534038544, "metricx_score": 10.411935806274414, "metricx_qe_score": 8.939506530761719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "注意:由于原文\"s the examples that carry most of the of the weight\"语法不完整,翻译时需要假设上下文以使其有意义。", "metrics": {"bleu_score": 1.121617874316939, "chrf_score": 1.5923566878980893, "xcomet_score": 0.14192119240760803, "xcomet_qe_score": 0.13228127360343933, "metricx_score": 21.57554054260254, "metricx_qe_score": 23.01142692565918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在实际翻译中,完整准确的句子会更易于翻译。)", "metrics": {"bleu_score": 4.108053662461269, "chrf_score": 5.415195987367639, "xcomet_score": 0.2626061737537384, "xcomet_qe_score": 0.23827344179153442, "metricx_score": 2.221393346786499, "metricx_qe_score": 3.170171022415161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,例子质量比与源句子的相似度更为重要。 选择示例", "metrics": {"bleu_score": 60.31227121338507, "chrf_score": 61.85643407228893, "xcomet_score": 0.6918874382972717, "xcomet_qe_score": 0.7105147838592529, "metricx_score": 3.6353647708892822, "metricx_qe_score": 2.17362642288208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "时,重要的是要来自高质量的翻译。", "metrics": {"bleu_score": 27.180001324983337, "chrf_score": 23.638114635432007, "xcomet_score": 0.5724231600761414, "xcomet_qe_score": 0.5351743698120117, "metricx_score": 7.196195125579834, "metricx_qe_score": 6.477903842926025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较从WMT评估的训练数据或开发数据中选择的提示。 开发数据(dev data)的创", "metrics": {"bleu_score": 41.304345025653454, "chrf_score": 39.174085513871304, "xcomet_score": 0.44336366653442383, "xcomet_qe_score": 0.39567479491233826, "metricx_score": 9.455235481262207, "metricx_qe_score": 7.680520057678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "建量远多于训练数据(train data),且质量更高。由于开发数据更优质,使用时能带来更佳的性能和结果。", "metrics": {"bleu_score": 14.382259604461854, "chrf_score": 17.62132362557822, "xcomet_score": 0.6663963794708252, "xcomet_qe_score": 0.589448094367981, "metricx_score": 5.246752738952637, "metricx_qe_score": 5.383792877197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:原文中存在重复和语法错误,翻译时已进行适当调整以符合中文表达习惯。)", "metrics": {"bleu_score": 1.3113617852675474, "chrf_score": 2.4752475247524752, "xcomet_score": 0.14113615453243256, "xcomet_qe_score": 0.13562384247779846, "metricx_score": 4.8550190925598145, "metricx_qe_score": 15.949565887451172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,先进的专业系统在翻译质量上具有显著优势,但与商业翻译系统相比", "metrics": {"bleu_score": 12.67166061380498, "chrf_score": 15.22148752702776, "xcomet_score": 0.5204243659973145, "xcomet_qe_score": 0.42474305629730225, "metricx_score": 6.898816108703613, "metricx_qe_score": 4.938003063201904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",差距也逐渐缩小。", "metrics": {"bleu_score": 2.447822402834545, "chrf_score": 1.0288065843621397, "xcomet_score": 0.15257328748703003, "xcomet_qe_score": 0.15820474922657013, "metricx_score": 12.185068130493164, "metricx_qe_score": 16.459625244140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择避免使用谷歌翻译。", "metrics": {"bleu_score": 56.98893944394812, "chrf_score": 47.10084358124198, "xcomet_score": 0.8231745362281799, "xcomet_qe_score": 0.8182685375213623, "metricx_score": 6.318502426147461, "metricx_qe_score": 6.258456230163574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用MQN框架进行电子邮件交流获得的见解是,手写输入的流畅度可与最先进的系统相媲美,但主要区别在于准确度。", "metrics": {"bleu_score": 47.89631975204773, "chrf_score": 41.04340235423958, "xcomet_score": 0.676033616065979, "xcomet_qe_score": 0.6317211389541626, "metricx_score": 7.190582275390625, "metricx_qe_score": 6.596256256103516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其常见的是遗漏错误。", "metrics": {"bleu_score": 49.00202456162331, "chrf_score": 33.62355632562288, "xcomet_score": 0.875468373298645, "xcomet_qe_score": 0.8922600150108337, "metricx_score": 2.4415643215179443, "metricx_qe_score": 0.6939801573753357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,看来 Palm 有时会选择省略源句中在翻译中可以忽略的部分,以产生听起来更好的翻译。", "metrics": {"bleu_score": 24.175348445005906, "chrf_score": 21.973775612673364, "xcomet_score": 0.9308699369430542, "xcomet_qe_score": 0.866654098033905, "metricx_score": 3.5846970081329346, "metricx_qe_score": 3.59706449508667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,平底锅的外观风格低于最先进系统的水平,这是一个额外的信号。 该参数生成了非常流畅的输出,但准确性上仍存在一些问题。", "metrics": {"bleu_score": 35.914687429918565, "chrf_score": 27.679409452355014, "xcomet_score": 0.5583837032318115, "xcomet_qe_score": 0.5520135760307312, "metricx_score": 7.183980941772461, "metricx_qe_score": 7.6726579666137695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这次非常简短的概述,欲", "metrics": {"bleu_score": 15.404079880192432, "chrf_score": 16.2681656610382, "xcomet_score": 0.7724481821060181, "xcomet_qe_score": 0.805880606174469, "metricx_score": 6.095684051513672, "metricx_qe_score": 1.2708650827407837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了解更多细节,请务必参加论文的完整报告。", "metrics": {"bleu_score": 26.415417801170204, "chrf_score": 21.150469809837972, "xcomet_score": 0.7672476768493652, "xcomet_qe_score": 0.7874088287353516, "metricx_score": 3.0443975925445557, "metricx_qe_score": 1.843410611152649, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是德国沉默大学(Silent University)的博士生大伟。", "metrics": {"bleu_score": 16.498106329240724, "chrf_score": 14.167639758748743, "xcomet_score": 0.6318409442901611, "xcomet_qe_score": 0.6181391477584839, "metricx_score": 6.690442085266113, "metricx_qe_score": 7.1673736572265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想向大家介绍我们最近的研究成果——《比你想象的更大:从周惊喜(Weekly Surprise)批判性地审视列宁》。", "metrics": {"bleu_score": 31.40219487239326, "chrf_score": 28.43656538405817, "xcomet_score": 0.5406869649887085, "xcomet_qe_score": 0.5431238412857056, "metricx_score": 9.460126876831055, "metricx_qe_score": 9.377047538757324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:由于“Lening”在原文中可能存在拼写错误或特定含义,这里将其直译,具体含义需根据上下文确定。", "metrics": {"bleu_score": 1.1239503708631486, "chrf_score": 2.385219986151814, "xcomet_score": 0.2112942487001419, "xcomet_qe_score": 0.2129494547843933, "metricx_score": 5.592926979064941, "metricx_qe_score": 12.91820240020752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 与Sha My Muba、Gear Stefan和Ditish Klakov的合著工作 我想从简要介绍周监督和每周监督学习开始。 周监督(Weekly Supervision)是一种教育和指导方式,通常在每周固定时间进行,旨在提供持续的反馈和支持。它涉及到导师或教师与学习者之间的定期互动,以审查进度、讨论挑战和提供指导。 每周监督学习(Weekly Supervised Learning)是指在这种监督框架下进行的学习过程。它包括学习者完成任务或项目,然后在每周的监督会议上与导师或教师进行讨论和反思。这种方法有助于培养学习者的自主性和批判性思维能力,同时确保他们得到及时的反馈和指导", "metrics": {"bleu_score": 2.5884060853651594, "chrf_score": 6.853106059702164, "xcomet_score": 0.13898473978042603, "xcomet_qe_score": 0.14102894067764282, "metricx_score": 5.746817588806152, "metricx_qe_score": 6.7593536376953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。 弱监督学习中,我们不进行手动数据标注。", "metrics": {"bleu_score": 11.728147369287814, "chrf_score": 15.501203233996472, "xcomet_score": 0.877434492111206, "xcomet_qe_score": 0.8541373610496521, "metricx_score": 3.5813753604888916, "metricx_qe_score": 3.693527936935425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注来源对数据进行标注,例如简单的启发式规则、知识库或本地代码来源,如图所示。", "metrics": {"bleu_score": 49.56703115355597, "chrf_score": 44.40516979406236, "xcomet_score": 0.7688738107681274, "xcomet_qe_score": 0.6654704213142395, "metricx_score": 3.2794389724731445, "metricx_qe_score": 5.4097900390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,较弱的标注成本更低,但同时也存在噪声,即标注中存在一定数量的错误。", "metrics": {"bleu_score": 29.695622803140342, "chrf_score": 25.07383214614131, "xcomet_score": 0.898423433303833, "xcomet_qe_score": 0.8517290353775024, "metricx_score": 2.055107355117798, "metricx_qe_score": 2.3417551517486572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们直接在弱标签数据上训练神经网络,神经网络往往会记住标签噪声而缺乏泛化能力。 提出了一", "metrics": {"bleu_score": 52.90850730627304, "chrf_score": 43.984952425973034, "xcomet_score": 0.6457123756408691, "xcomet_qe_score": 0.583917498588562, "metricx_score": 5.371965408325195, "metricx_qe_score": 4.440184116363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "种弱监督学习的训练算法,以在存在标签噪声的情况下稳健地训练神经网络,从而确保训练后的模型仍能很好地泛化。", "metrics": {"bleu_score": 42.622387637480536, "chrf_score": 36.5269046006761, "xcomet_score": 0.6116575002670288, "xcomet_qe_score": 0.7234530448913574, "metricx_score": 4.611255645751953, "metricx_qe_score": 4.5338454246521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近期在wSL领域的研究,其中wSL代表每周支持学习。一个常见的说法是,人们声称他们仅在每周标签数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 25.47859196969677, "chrf_score": 27.18560353157173, "xcomet_score": 0.5786216259002686, "xcomet_qe_score": 0.6123152375221252, "metricx_score": 8.704014778137207, "metricx_qe_score": 9.066841125488281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项说法并不错误,但存在一个前提。 人们通常假设存在一个额外的清洗后的验证集,或者用于模型选择的", "metrics": {"bleu_score": 24.877871996616467, "chrf_score": 21.411213813820158, "xcomet_score": 0.6494568586349487, "xcomet_qe_score": 0.6665539741516113, "metricx_score": 5.047183513641357, "metricx_qe_score": 3.9791393280029297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完整、友好的形式。 在这个问题设置上停滞不前,但这意味着每周的学习支持中需要额外的手动标注。", "metrics": {"bleu_score": 35.25634899653582, "chrf_score": 35.843569628179466, "xcomet_score": 0.33942294120788574, "xcomet_qe_score": 0.18642082810401917, "metricx_score": 6.801353931427002, "metricx_qe_score": 7.794886589050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,就像房间里的象一样,这个必要性经常被忽视。", "metrics": {"bleu_score": 43.2380626100291, "chrf_score": 34.847629504532705, "xcomet_score": 0.9160639047622681, "xcomet_qe_score": 0.821521520614624, "metricx_score": 2.3960678577423096, "metricx_qe_score": 4.346866607666016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述内容引导我们提出三个研究问题。", "metrics": {"bleu_score": 51.94264628599392, "chrf_score": 49.760631900427775, "xcomet_score": 0.8773278594017029, "xcomet_qe_score": 0.9032527208328247, "metricx_score": 1.7306164503097534, "metricx_qe_score": 2.6936662197113037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于 WSL,是否需要干净的验证数据?或者我们是否可以替代地使用一个嘈杂的验证集?", "metrics": {"bleu_score": 45.68140416397982, "chrf_score": 47.474872749141255, "xcomet_score": 0.9450148940086365, "xcomet_qe_score": 0.9246262311935425, "metricx_score": 2.0562691688537598, "metricx_qe_score": 3.2683815956115723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果需要或必须使用干净数据才能使WSL正常工作,那么最终我们需要多少干净样本?", "metrics": {"bleu_score": 25.125141054966104, "chrf_score": 22.932712767393085, "xcomet_score": 0.8939875364303589, "xcomet_qe_score": 0.8794288635253906, "metricx_score": 1.748788595199585, "metricx_qe_score": 1.8437728881835938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们是否只应将干净样本用于验证,还是有更好的利用方法? 在我们的研究", "metrics": {"bleu_score": 33.59696856755292, "chrf_score": 29.335832651083077, "xcomet_score": 0.8135319948196411, "xcomet_qe_score": 0.761174201965332, "metricx_score": 5.198297500610352, "metricx_qe_score": 4.167716026306152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中,我们探讨了以下几个研究问题,我们的发现如下:", "metrics": {"bleu_score": 28.632903537191147, "chrf_score": 23.13301412510839, "xcomet_score": 0.5601797103881836, "xcomet_qe_score": 0.5392478704452515, "metricx_score": 3.2676663398742676, "metricx_qe_score": 3.7022316455841064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现有趣的是,最近的水下学习(WSL)方法确实需要干净的训练样本才能正常工作。", "metrics": {"bleu_score": 51.398912788690204, "chrf_score": 49.439964689685425, "xcomet_score": 0.8051621913909912, "xcomet_qe_score": 0.7921191453933716, "metricx_score": 2.9937644004821777, "metricx_qe_score": 3.3733465671539307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则性能会大幅下降。", "metrics": {"bleu_score": 75.16501147964685, "chrf_score": 69.51795847749722, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.32853764295578003, "metricx_qe_score": 0.6683884859085083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,趋势模型就无法超越原始的弱标签进行泛化。 训练是没有意义的。", "metrics": {"bleu_score": 55.46924043271572, "chrf_score": 49.451319748039055, "xcomet_score": 0.9179052114486694, "xcomet_qe_score": 0.7955436110496521, "metricx_score": 3.5827348232269287, "metricx_qe_score": 4.1397247314453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表明WsSL方法实际上需要干净的标签数据才能正常工作,获取干净验证样本的标注成本不", "metrics": {"bleu_score": 44.0826374648537, "chrf_score": 43.19742601892963, "xcomet_score": 0.45993417501449585, "xcomet_qe_score": 0.40757134556770325, "metricx_score": 8.904766082763672, "metricx_qe_score": 6.3524250984191895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "应被忽视。 第二个发现是,增加干净验证样本的数量将有助于 WSL 方法实现更好的性能,如左侧图表所示。", "metrics": {"bleu_score": 59.592455885329194, "chrf_score": 61.01068623852418, "xcomet_score": 0.3755273222923279, "xcomet_qe_score": 0.27840331196784973, "metricx_score": 6.411202907562256, "metricx_qe_score": 7.48350715637207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们每类只需要 20 个样本即可达到高性能。", "metrics": {"bleu_score": 35.07061298327183, "chrf_score": 36.554003348760816, "xcomet_score": 0.8383755683898926, "xcomet_qe_score": 0.8153879642486572, "metricx_score": 2.0575292110443115, "metricx_qe_score": 1.894417643547058, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是故事并未结束,因为如果我们最终决定获取干净的样本,那么直接在这些样本上进行训练甚至能达到更好的性能。 红", "metrics": {"bleu_score": 28.568913735792773, "chrf_score": 24.905738537115674, "xcomet_score": 0.8640604019165039, "xcomet_qe_score": 0.8099573850631714, "metricx_score": 4.852996349334717, "metricx_qe_score": 4.115170478820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "色图示展示了直接应用于清洁数据的微调方法与仅使用清洁数据进行验证的 WSL 方法在性能上的差异。", "metrics": {"bleu_score": 58.79646493709226, "chrf_score": 55.28011479296715, "xcomet_score": 0.6694932579994202, "xcomet_qe_score": 0.6487872004508972, "metricx_score": 3.769524097442627, "metricx_qe_score": 4.568565845489502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,如果每类有10个样本,直接微调开始超越WSL方法。", "metrics": {"bleu_score": 42.40735718433565, "chrf_score": 42.78668605285893, "xcomet_score": 0.9297223091125488, "xcomet_qe_score": 0.8684885501861572, "metricx_score": 1.6018255949020386, "metricx_qe_score": 2.209069013595581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,通过允许在干净的验证样本上继续微调,可以轻松实现之前 WSL 方法中声称的性能提升。", "metrics": {"bleu_score": 50.167540756339584, "chrf_score": 44.80821183984229, "xcomet_score": 0.9739274978637695, "xcomet_qe_score": 0.8915089964866638, "metricx_score": 2.4012656211853027, "metricx_qe_score": 3.5959229469299316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从图中可以看出,最初被称为ftw的有效模型在性能上落后于更复杂的WSL方法,", "metrics": {"bleu_score": 23.200223797831686, "chrf_score": 19.947645898676136, "xcomet_score": 0.7263191342353821, "xcomet_qe_score": 0.7088344693183899, "metricx_score": 6.784032344818115, "metricx_qe_score": 7.4025139808654785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如余弦相似度。 如果我们允许在清洁样本上继续进行幻想归纳(fantuni),那么Tw的表现与其它方法一样好。", "metrics": {"bleu_score": 26.68932100935592, "chrf_score": 30.27697937265347, "xcomet_score": 0.5850235223770142, "xcomet_qe_score": 0.46803897619247437, "metricx_score": 10.279762268066406, "metricx_qe_score": 12.000019073486328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在实际应用中,没有必要选择更复杂的WSL方法,因为这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 43.06540704569678, "chrf_score": 44.991602553110425, "xcomet_score": 0.9764952659606934, "xcomet_qe_score": 0.9777741432189941, "metricx_score": 0.5855385065078735, "metricx_qe_score": 1.148009181022644, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总结道,最新的弱监督学习(wSL)方法需要干净的手动标注样本才能正常工作。", "metrics": {"bleu_score": 31.760134441172934, "chrf_score": 28.791835866211997, "xcomet_score": 0.9502383470535278, "xcomet_qe_score": 0.8542566299438477, "metricx_score": 2.5788333415985107, "metricx_qe_score": 2.1590514183044434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实际应用价值被严重高估了。", "metrics": {"bleu_score": 29.715678881302644, "chrf_score": 29.996926363508848, "xcomet_score": 0.9975954294204712, "xcomet_qe_score": 0.9983798265457153, "metricx_score": 0.6434529423713684, "metricx_qe_score": 0.6953849196434021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于未来工作时间的具体建议如下。", "metrics": {"bleu_score": 48.415247130345996, "chrf_score": 45.61605060210855, "xcomet_score": 0.8094350099563599, "xcomet_qe_score": 0.7634287476539612, "metricx_score": 2.933018207550049, "metricx_qe_score": 1.6850934028625488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否在干净的验证样本下进行。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.34004037322114, "xcomet_score": 0.913140058517456, "xcomet_qe_score": 0.8368960618972778, "metricx_score": 1.835635781288147, "metricx_qe_score": 2.7443156242370605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL 方法应与少数短着陆基线进行比较,假设在具体样本上工作。", "metrics": {"bleu_score": 21.187870164617337, "chrf_score": 22.437931901341805, "xcomet_score": 0.5233372449874878, "xcomet_qe_score": 0.5396414399147034, "metricx_score": 9.416075706481934, "metricx_qe_score": 9.2763090133667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一个简单而强大的基线,应在未来的 WSL 工作中考虑。", "metrics": {"bleu_score": 50.42673311800851, "chrf_score": 46.38550600316616, "xcomet_score": 0.8203495740890503, "xcomet_qe_score": 0.6713142395019531, "metricx_score": 2.4955623149871826, "metricx_qe_score": 3.2008702754974365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们已经开源了我们的代码。", "metrics": {"bleu_score": 79.39226578179516, "chrf_score": 81.07459493109678, "xcomet_score": 0.9998438358306885, "xcomet_qe_score": 0.9549850225448608, "metricx_score": 0.5194101929664612, "metricx_qe_score": 0.8293412923812866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过本幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9957367181777954, "xcomet_qe_score": 0.9900866746902466, "metricx_score": 0.461531400680542, "metricx_qe_score": 0.4078834354877472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您,祝您享受这次会议。", "metrics": {"bleu_score": 7.768562846380176, "chrf_score": 7.799145299145299, "xcomet_score": 0.9164923429489136, "xcomet_qe_score": 0.9908124804496765, "metricx_score": 1.050849199295044, "metricx_qe_score": 0.7773853540420532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9827327728271484, "xcomet_qe_score": 1.0, "metricx_score": 0.8756831288337708, "metricx_qe_score": 0.3633490204811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABC Eval,一种评估对话人工智能的新维度方法。", "metrics": {"bleu_score": 23.47248568263366, "chrf_score": 27.012468720988103, "xcomet_score": 0.8397789001464844, "xcomet_qe_score": 0.9061694145202637, "metricx_score": 1.6824827194213867, "metricx_qe_score": 1.9152426719665527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学的人工智能自然语言处理实验室完成,该实验室由埃默里大学的崔吉诺教授领导,并与亚马逊Alexa AI合作。", "metrics": {"bleu_score": 22.794976708458712, "chrf_score": 28.453582663176196, "xcomet_score": 0.7786346673965454, "xcomet_qe_score": 0.8015893697738647, "metricx_score": 2.652972459793091, "metricx_qe_score": 2.4163239002227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚开发了一个对话模型,你想看看它与当前最先进的技术相比表现如何。", "metrics": {"bleu_score": 75.16216924719087, "chrf_score": 64.49628711487166, "xcomet_score": 0.9982374906539917, "xcomet_qe_score": 0.9885433912277222, "metricx_score": 0.5928300619125366, "metricx_qe_score": 0.641268253326416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如请人工评判员从两个对话中选择哪个更好,或者使用量表对对话进行评分。", "metrics": {"bleu_score": 54.60834708665818, "chrf_score": 47.964123651068405, "xcomet_score": 0.7513244152069092, "xcomet_qe_score": 0.7816077470779419, "metricx_score": 0.857043981552124, "metricx_qe_score": 0.7781804203987122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法可以很好地提供整体对话质量的评估,但对话质量有多个方面。", "metrics": {"bleu_score": 59.490969034436894, "chrf_score": 50.44106832228087, "xcomet_score": 0.947091817855835, "xcomet_qe_score": 0.9835519790649414, "metricx_score": 0.40492382645606995, "metricx_qe_score": 0.5791277885437012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能需要评估聊天质量的多个维度,以更细致地了解模型的优缺点。", "metrics": {"bleu_score": 77.07975858143212, "chrf_score": 72.77552982504822, "xcomet_score": 0.9787784814834595, "xcomet_qe_score": 0.9572713375091553, "metricx_score": 0.5929165482521057, "metricx_qe_score": 0.5868556499481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本方法是简单地请人类评判员使用现有比较方法或量表法(如酒量表法)评估对话质量的多个维度,例如模型响应的相关性。", "metrics": {"bleu_score": 36.08272532321883, "chrf_score": 29.811064245766428, "xcomet_score": 0.6432262659072876, "xcomet_qe_score": 0.5701364278793335, "metricx_score": 6.108235836029053, "metricx_qe_score": 6.894343852996826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 45.15558127464808, "xcomet_score": 0.9024549126625061, "xcomet_qe_score": 0.8712908625602722, "metricx_score": 1.308674931526184, "metricx_qe_score": 1.4190480709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法试图通过明确标注每个模型响应是否表达某些行为来减少人类评价的主观性,例如提供与主题无关的信息或自相矛盾。", "metrics": {"bleu_score": 60.589730904434276, "chrf_score": 53.09103544952601, "xcomet_score": 0.9597233533859253, "xcomet_qe_score": 0.9563103914260864, "metricx_score": 1.9010436534881592, "metricx_qe_score": 2.4383702278137207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为聊天行为标注,简称为ABCEval。", "metrics": {"bleu_score": 18.493046910349435, "chrf_score": 26.645003315099647, "xcomet_score": 0.8546314239501953, "xcomet_qe_score": 0.8671382665634155, "metricx_score": 1.859763741493225, "metricx_qe_score": 2.2380855083465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法是为了全面涵盖近期文献中提出的影响聊天质量聊天模型行为。", "metrics": {"bleu_score": 54.355232858046996, "chrf_score": 45.85530538095907, "xcomet_score": 0.8564416170120239, "xcomet_qe_score": 0.831322431564331, "metricx_score": 1.4285888671875, "metricx_qe_score": 2.0571513175964355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC 评估能够测量聊天模型犯各种主题错误的速率。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 52.23877106271304, "xcomet_score": 0.6864588260650635, "xcomet_qe_score": 0.7049075365066528, "metricx_score": 4.9060444831848145, "metricx_qe_score": 6.331258773803711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABCEval 评估聊天模型在多少轮对话中忽略了其对话伙伴或说了与话题无关的话。 自我矛盾,或其合作者幻觉错误的事实,或违反常识,以及当模型成功或失败地展现同理心时。", "metrics": {"bleu_score": 19.0064276988384, "chrf_score": 19.47089772543873, "xcomet_score": 0.5080495476722717, "xcomet_qe_score": 0.5381780862808228, "metricx_score": 5.54063081741333, "metricx_qe_score": 5.845123291015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定最有效的评估方法,我们选择了四种最先进的聊天模型,并使用 ABC 评估法对每个模型进行 100 次人类与机器人的对话评估。", "metrics": {"bleu_score": 47.31668114099112, "chrf_score": 44.97299948050977, "xcomet_score": 0.9107646942138672, "xcomet_qe_score": 0.9073051810264587, "metricx_score": 1.1202138662338257, "metricx_qe_score": 0.8940856456756592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用三种现有方法对这些对话进行了评估:转折级别的酒水评分、对话级别的酒水评分以及对话级别的成对比较。", "metrics": {"bleu_score": 42.884995042660684, "chrf_score": 35.83055483765848, "xcomet_score": 0.5336967706680298, "xcomet_qe_score": 0.5064744353294373, "metricx_score": 8.475146293640137, "metricx_qe_score": 8.898327827453613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有的每一种方法,我们收集了对对话八个最常见测量维度的评估,因为这是评估聊天模型多维度的标准实践。 ", "metrics": {"bleu_score": 45.56379781954134, "chrf_score": 37.06875649193854, "xcomet_score": 0.8787366151809692, "xcomet_qe_score": 0.8656172752380371, "metricx_score": 3.612469434738159, "metricx_qe_score": 3.6689279079437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现ABC行为标签总体上比现有方法收集的标签更可靠,这一点通过100个双重标记的对话中内部注释员的一致性得到衡量。", "metrics": {"bleu_score": 38.37355506743177, "chrf_score": 37.47072268860303, "xcomet_score": 0.7704972624778748, "xcomet_qe_score": 0.7318498492240906, "metricx_score": 6.058678150177002, "metricx_qe_score": 6.721003532409668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,根据这一简单的线性回归分析,ABCEval 标签在预测整体对话质量方面优于现有方法产生的指标。", "metrics": {"bleu_score": 45.79219792189246, "chrf_score": 42.95805791346507, "xcomet_score": 0.971779465675354, "xcomet_qe_score": 0.9567994475364685, "metricx_score": 1.8594402074813843, "metricx_qe_score": 2.4103074073791504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到,通过测量自言自语和伴侣矛盾的转弯比例,可以分别解释对话质量的5%和10%,而平均白酒一致性得分仅能解释4%或更少。", "metrics": {"bleu_score": 36.09721215384129, "chrf_score": 31.695657468875403, "xcomet_score": 0.45254674553871155, "xcomet_qe_score": 0.4796391427516937, "metricx_score": 9.2898588180542, "metricx_qe_score": 9.92300033569336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归分析检查了每个评估指标是否捕捉了聊天质量的独特方面。", "metrics": {"bleu_score": 65.40100151828888, "chrf_score": 60.85747392617248, "xcomet_score": 0.8170450329780579, "xcomet_qe_score": 0.8894021511077881, "metricx_score": 1.4654138088226318, "metricx_qe_score": 1.7910033464431763, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以通过结合所有ABC Eval指标来解释超过25%的对话质量。当你逐一去除这些指标时,大多数情况下都会导致失去大量关于质量的信息。", "metrics": {"bleu_score": 26.833176287813917, "chrf_score": 32.65280834989468, "xcomet_score": 0.8228967189788818, "xcomet_qe_score": 0.7889034748077393, "metricx_score": 3.027562141418457, "metricx_qe_score": 3.5715291500091553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有轮级白酒指标的组合对质量解释得远少,且这些指标中带有独特信息的更少。", "metrics": {"bleu_score": 32.890268084400525, "chrf_score": 28.158370378382884, "xcomet_score": 0.6207225322723389, "xcomet_qe_score": 0.6080250144004822, "metricx_score": 9.258644104003906, "metricx_qe_score": 7.966841220855713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可靠、信息丰富且独特的ABC Eval指标使我们能够以高于先前方法的分辨率评估对话人工智能。", "metrics": {"bleu_score": 7.038731511943215, "chrf_score": 14.347781044311352, "xcomet_score": 0.6754694581031799, "xcomet_qe_score": 0.7735661864280701, "metricx_score": 5.321987628936768, "metricx_qe_score": 5.057863235473633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们实验的结果中可以看出,仍存在几个挑战,并且这些挑战已被精确量化。", "metrics": {"bleu_score": 21.16154709655933, "chrf_score": 24.096275259849005, "xcomet_score": 0.9800653457641602, "xcomet_qe_score": 0.979763388633728, "metricx_score": 0.7752567529678345, "metricx_qe_score": 0.9311131834983826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人约20%的响应存在常识违规。", "metrics": {"bleu_score": 41.87526331649786, "chrf_score": 38.03824454645228, "xcomet_score": 0.8536428213119507, "xcomet_qe_score": 0.8503142595291138, "metricx_score": 2.4013116359710693, "metricx_qe_score": 3.4330945014953613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在大约15%的回应中产生与主题无关的信息,并且大约有10%的时间他们会自相矛盾或与合作伙伴意见相", "metrics": {"bleu_score": 31.938115374714613, "chrf_score": 29.297073280640152, "xcomet_score": 0.665287971496582, "xcomet_qe_score": 0.5989440083503723, "metricx_score": 5.780269622802734, "metricx_qe_score": 4.299074649810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "左。 在领域内快速的进步速度下,这些错误率在我们评估后发布的新模型中可能会有所降低。", "metrics": {"bleu_score": 10.415221136872411, "chrf_score": 16.328907617397157, "xcomet_score": 0.44007954001426697, "xcomet_qe_score": 0.5054383277893066, "metricx_score": 6.639768600463867, "metricx_qe_score": 7.250606536865234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更说明了追求可靠且精确的评估指标以比较模型的重要性。", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 40.91703962752154, "xcomet_score": 0.9977174997329712, "xcomet_qe_score": 0.9871809482574463, "metricx_score": 0.910224437713623, "metricx_qe_score": 1.0445787906646729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望ABC Eval能够被该领域的其他人士作为朝此方向迈出的有意义的一步,", "metrics": {"bleu_score": 47.37472982077472, "chrf_score": 50.69426585836465, "xcomet_score": 0.9140944480895996, "xcomet_qe_score": 0.9057495594024658, "metricx_score": 2.949223279953003, "metricx_qe_score": 3.0569822788238525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待看到对话式人工智能在未来几个月和几年中的发展。", "metrics": {"bleu_score": 74.0609366763812, "chrf_score": 73.52998803726307, "xcomet_score": 0.9891669750213623, "xcomet_qe_score": 0.9691853523254395, "metricx_score": 0.7160965800285339, "metricx_qe_score": 0.8807143568992615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我的名字是Kyyo Yin,我将要展示我们的研究成果,题为《何时翻译需要上下文:", "metrics": {"bleu_score": 19.30158055353422, "chrf_score": 24.310592489245753, "xcomet_score": 0.8185538053512573, "xcomet_qe_score": 0.8011336326599121, "metricx_score": 3.635420560836792, "metricx_qe_score": 3.8092923164367676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语种探索》。", "metrics": {"bleu_score": 34.172334076593074, "chrf_score": 28.44276094276094, "xcomet_score": 0.9816019535064697, "xcomet_qe_score": 0.8502190709114075, "metricx_score": 1.1029056310653687, "metricx_qe_score": 1.2808890342712402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernage、Emiliu Andre、FD Martins以及Graham Newbiig合作完成的。 (注:为了保持原文的格式,我保留了原文的句子结构和标点,同时将名字和标题直接翻译成中文。在中文语境中,通常会将非中文名字直接音译,但为了符合学术或教学材料的语气,这里提供了音译和中文标题的结合。)", "metrics": {"bleu_score": 4.908818812764256, "chrf_score": 36.729562271871295, "xcomet_score": 0.34149855375289917, "xcomet_qe_score": 0.3561708629131317, "metricx_score": 6.3179497718811035, "metricx_qe_score": 5.867762565612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都取决于上下文。", "metrics": {"bleu_score": 63.40466277046863, "chrf_score": 59.27762653106322, "xcomet_score": 0.9980931282043457, "xcomet_qe_score": 0.9876047372817993, "metricx_score": 0.09006089717149734, "metricx_qe_score": 0.15374323725700378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译句子中的“痣”? (注:由于原文未提供具体句子,翻译中的“mole”被", "metrics": {"bleu_score": 21.74060073159203, "chrf_score": 41.07085362963095, "xcomet_score": 0.44265270233154297, "xcomet_qe_score": 0.3905332088470459, "metricx_score": 7.033148765563965, "metricx_qe_score": 4.665295124053955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设为“痣”的意思。) 如果前一句话“洗”可能开始变得危险如果部长们发现,那么“更多”指的是一个间谍。", "metrics": {"bleu_score": 12.860612501976386, "chrf_score": 8.005335581150323, "xcomet_score": 0.4333325922489166, "xcomet_qe_score": 0.45471301674842834, "metricx_score": 16.10413932800293, "metricx_qe_score": 16.13727569580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句话是“它会是任何严重的事情,医生吗?”那么“更多”指的是一个胎记。", "metrics": {"bleu_score": 22.397899399425853, "chrf_score": 13.144876146916, "xcomet_score": 0.6518663763999939, "xcomet_qe_score": 0.6446378827095032, "metricx_score": 6.919302940368652, "metricx_qe_score": 6.895212173461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据上下文,词的意义会发生变化,因此其翻译也会相应地改变。", "metrics": {"bleu_score": 43.83022272687836, "chrf_score": 35.56002126283599, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4222135543823242, "metricx_qe_score": 0.3753082752227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在对比此类案例时的表现相当困难。", "metrics": {"bleu_score": 19.04623362251633, "chrf_score": 16.254580401747763, "xcomet_score": 0.8730853199958801, "xcomet_qe_score": 0.8183101415634155, "metricx_score": 3.059971809387207, "metricx_qe_score": 2.2623584270477295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,由于只有少数翻译依赖于上下文,这导致语料库级别的指标,如BLEU,无法捕捉到这些翻译。", "metrics": {"bleu_score": 28.89787454467964, "chrf_score": 26.58360985138732, "xcomet_score": 0.976241946220398, "xcomet_qe_score": 0.9509700536727905, "metricx_score": 1.406715989112854, "metricx_qe_score": 2.210369110107422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对依上下文而变的翻译进行定向评估,但这些资源只支持有限类型的依上下文而变的翻译和有限的语言集合,因为它们通常依赖于领域知识和人工编辑。", "metrics": {"bleu_score": 62.64077334613357, "chrf_score": 55.874361992068856, "xcomet_score": 0.9325600862503052, "xcomet_qe_score": 0.9382902979850769, "metricx_score": 1.3215135335922241, "metricx_qe_score": 1.2039116621017456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们试图回答以下两个问题。", "metrics": {"bleu_score": 73.09400730437454, "chrf_score": 69.43822972366657, "xcomet_score": 0.9975918531417847, "xcomet_qe_score": 0.99656081199646, "metricx_score": 0.0, "metricx_qe_score": 0.09364727139472961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在处理这些情况下表现如何?", "metrics": {"bleu_score": 59.29835221973795, "chrf_score": 51.308235489846524, "xcomet_score": 0.9888322353363037, "xcomet_qe_score": 0.99993896484375, "metricx_score": 0.4791920483112335, "metricx_qe_score": 0.6569955348968506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了翻译过程中依赖上下文的工作的量。", "metrics": {"bleu_score": 49.576156544436664, "chrf_score": 40.878423513360026, "xcomet_score": 0.8702630996704102, "xcomet_qe_score": 0.8711252808570862, "metricx_score": 6.01304817199707, "metricx_qe_score": 6.031402587890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在先前的工作中,我们引入了CXMI作为机器翻译模型上下文使用量的度量。", "metrics": {"bleu_score": 57.19408603967074, "chrf_score": 59.345310377550106, "xcomet_score": 0.8919402360916138, "xcomet_qe_score": 0.888935387134552, "metricx_score": 1.7021788358688354, "metricx_qe_score": 1.811134696006775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量给定源X时上下文C关于目标Y提供的信息量来实现。 可以将CXMI视为通过为模型提供上下文而获得的信息。", "metrics": {"bleu_score": 53.83309248891204, "chrf_score": 52.48641145275519, "xcomet_score": 0.922224760055542, "xcomet_qe_score": 0.927118182182312, "metricx_score": 4.787790298461914, "metricx_qe_score": 4.49423360824585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们将CXMI扩展为点对点CXMI,它可以在句子级别或词语级别测量上下文使用情况。", "metrics": {"bleu_score": 48.00487694138449, "chrf_score": 39.91774701504603, "xcomet_score": 0.8377571105957031, "xcomet_qe_score": 0.8423504829406738, "metricx_score": 2.3003931045532227, "metricx_qe_score": 2.3884425163269043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将PA6MI值高的词语视为需要上下文进行翻译的词语。", "metrics": {"bleu_score": 70.94521095075528, "chrf_score": 65.14781106961803, "xcomet_score": 0.7724829912185669, "xcomet_qe_score": 0.7742948532104492, "metricx_score": 5.534505844116211, "metricx_qe_score": 6.320890426635742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析高piecexMI的词语,以寻找这些词语之间的模式。", "metrics": {"bleu_score": 21.419122155930683, "chrf_score": 22.93033926585094, "xcomet_score": 0.8173542022705078, "xcomet_qe_score": 0.8125325441360474, "metricx_score": 7.70369291305542, "metricx_qe_score": 7.579604625701904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成十四种不同语言的TED演讲的文字记录进行了分析。", "metrics": {"bleu_score": 50.86366587562722, "chrf_score": 51.002933436495844, "xcomet_score": 0.9026250243186951, "xcomet_qe_score": 0.9274037480354309, "metricx_score": 1.4527552127838135, "metricx_qe_score": 1.3419684171676636, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同的层次上进行分析。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 72.00112387612387, "xcomet_score": 0.997683048248291, "xcomet_qe_score": 0.9908944368362427, "metricx_score": 0.22728201746940613, "metricx_qe_score": 0.402015745639801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们观察那些平均pxMI值较高的词性标签。", "metrics": {"bleu_score": 36.394125309794774, "chrf_score": 29.574355850099554, "xcomet_score": 0.8511403203010559, "xcomet_qe_score": 0.7232971787452698, "metricx_score": 4.301424503326416, "metricx_qe_score": 3.2493739128112793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到例如阿拉伯语中的双重代词,它们具有相对较高的p6MI。", "metrics": {"bleu_score": 48.21910491702433, "chrf_score": 39.21333202766159, "xcomet_score": 0.6719813942909241, "xcomet_qe_score": 0.7223590612411499, "metricx_score": 6.396183967590332, "metricx_qe_score": 5.890947341918945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语中没有双重代词,因此在翻译成阿拉伯语时,你需要上下文来确定一个代词是否为双重代词。", "metrics": {"bleu_score": 54.16155370377317, "chrf_score": 46.65782926408317, "xcomet_score": 0.790702760219574, "xcomet_qe_score": 0.9927617311477661, "metricx_score": 2.094318389892578, "metricx_qe_score": 1.6939496994018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 88.4112136328919, "chrf_score": 89.69824812440879, "xcomet_score": 0.9977834224700928, "xcomet_qe_score": 0.996990442276001, "metricx_score": 0.5566547513008118, "metricx_qe_score": 0.8022129535675049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们观察在所有不同出现中平均具有高pxMI的词汇项。", "metrics": {"bleu_score": 32.16397363858737, "chrf_score": 23.809538188443533, "xcomet_score": 0.7631516456604004, "xcomet_qe_score": 0.7053956389427185, "metricx_score": 6.217545509338379, "metricx_qe_score": 5.871873378753662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的案例,在中文中,你需要根据上下文来翻译专名,以确保在整个文件中使用一致的翻译。", "metrics": {"bleu_score": 34.98963057914091, "chrf_score": 31.794762644953074, "xcomet_score": 0.8080743551254272, "xcomet_qe_score": 0.8367946147918701, "metricx_score": 1.4462759494781494, "metricx_qe_score": 1.9084001779556274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现语境在适当的正式程度下得到支持。", "metrics": {"bleu_score": 13.62935817240798, "chrf_score": 13.302718892348537, "xcomet_score": 0.8316890001296997, "xcomet_qe_score": 0.8190593123435974, "metricx_score": 5.474856853485107, "metricx_qe_score": 5.724621295928955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们考察了具有高p6MI的不同单个词令。", "metrics": {"bleu_score": 31.13002029497926, "chrf_score": 24.384169666064658, "xcomet_score": 0.6566256284713745, "xcomet_qe_score": 0.6444605588912964, "metricx_score": 8.02719783782959, "metricx_qe_score": 8.568743705749512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别一些无法被词本身真正捕捉到的现象,而这些现象更多地体现在句子结构中,例如省略解析。", "metrics": {"bleu_score": 33.448446428868344, "chrf_score": 27.937758739659486, "xcomet_score": 0.8468064069747925, "xcomet_qe_score": 0.8093469142913818, "metricx_score": 1.5984008312225342, "metricx_qe_score": 1.8025192022323608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们利用分析结果设计一个文档新译文的基准。", "metrics": {"bleu_score": 27.4865819287504, "chrf_score": 22.95890898919525, "xcomet_score": 0.8511275053024292, "xcomet_qe_score": 0.794693112373352, "metricx_score": 3.438720464706421, "metricx_qe_score": 3.605386972427368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别出的五种话语现象,我们创建了标记器,以自动识别与这些现象相关的词语。", "metrics": {"bleu_score": 59.194983096259605, "chrf_score": 51.45726244061812, "xcomet_score": 0.9913066625595093, "xcomet_qe_score": 0.9153359532356262, "metricx_score": 0.8976421356201172, "metricx_qe_score": 1.29368257522583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将我们的标记器称为多语言话语感知标记器,即muda标记器。", "metrics": {"bleu_score": 45.97006398721532, "chrf_score": 35.241747323673195, "xcomet_score": 0.9032106399536133, "xcomet_qe_score": 0.815550684928894, "metricx_score": 1.7909058332443237, "metricx_qe_score": 2.201160192489624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同语言中这些离散现象的比例各不相同。", "metrics": {"bleu_score": 35.89260850395932, "chrf_score": 31.521877302339618, "xcomet_score": 0.8939094543457031, "xcomet_qe_score": 0.9006075859069824, "metricx_score": 3.0728039741516113, "metricx_qe_score": 3.076646566390991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,通过在我们想要用于评估的平行语料库上应用M标注器,来使用该标注器。我们在M标注器识别的上下文相关的示例上应用我们选择的翻译度量标准。 (注:\"M tagger\"在此处翻译为\"M标注器\",假设这是一个特定的工具或技术名称。)", "metrics": {"bleu_score": 24.837706623198446, "chrf_score": 33.23471509170899, "xcomet_score": 0.5493419170379639, "xcomet_qe_score": 0.5695188641548157, "metricx_score": 5.858091831207275, "metricx_qe_score": 5.1673736572265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用基准和其他度量标准来评估文档级机器翻译中的不同模型。", "metrics": {"bleu_score": 50.77162056935099, "chrf_score": 43.621313054031106, "xcomet_score": 0.9540338516235352, "xcomet_qe_score": 0.8370054364204407, "metricx_score": 0.9733223915100098, "metricx_qe_score": 1.4055020809173584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的度量时,对于“蓝”这个类别,我们发现Conic的泛型模型表现最佳。 那", "metrics": {"bleu_score": 39.46126031885468, "chrf_score": 33.51625691042708, "xcomet_score": 0.6193094253540039, "xcomet_qe_score": 0.5428587198257446, "metricx_score": 7.599252223968506, "metricx_qe_score": 5.936634540557861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么,如果我们使用评论数据,基于上下文的模型表现最佳。而", "metrics": {"bleu_score": 28.755838200176385, "chrf_score": 23.834900677967543, "xcomet_score": 0.4772593379020691, "xcomet_qe_score": 0.38791558146476746, "metricx_score": 6.674869537353516, "metricx_qe_score": 5.204062461853027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用词频衡量标准,有上下文和无上下文的模型表现相当。", "metrics": {"bleu_score": 61.77986463991717, "chrf_score": 53.87160885378369, "xcomet_score": 0.9120243787765503, "xcomet_qe_score": 0.9080593585968018, "metricx_score": 2.419665575027466, "metricx_qe_score": 2.110616683959961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,仅使用语料库级别的指标,确定最佳文档级别翻译系统是", "metrics": {"bleu_score": 43.464314100828744, "chrf_score": 35.68890402352742, "xcomet_score": 0.7590639591217041, "xcomet_qe_score": 0.7283830642700195, "metricx_score": 5.859865188598633, "metricx_qe_score": 4.331808090209961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具有挑战性的。 我们使用MUDA基准来评估模型,发现在特定话语现象(如正式性和词汇连贯性)中,具有上下文意识的模型比不使用上下文的模型准确度显著更高。", "metrics": {"bleu_score": 40.49803885793942, "chrf_score": 40.758029318062476, "xcomet_score": 0.40462175011634827, "xcomet_qe_score": 0.12433657050132751, "metricx_score": 3.525810956954956, "metricx_qe_score": 3.9319047927856445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在处理其他现象时,如省略号、代词和动词形式,与不使用上下文的模型相差无几。", "metrics": {"bleu_score": 56.650408164126034, "chrf_score": 48.685405083030254, "xcomet_score": 0.9875246286392212, "xcomet_qe_score": 0.9679750204086304, "metricx_score": 0.8342379927635193, "metricx_qe_score": 0.9263895750045776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在一定程度上表明了我们需要在文档级翻译上看到更多进展。", "metrics": {"bleu_score": 42.16651606863155, "chrf_score": 37.04123980716207, "xcomet_score": 0.9839707612991333, "xcomet_qe_score": 0.9695636034011841, "metricx_score": 1.604334831237793, "metricx_qe_score": 1.2888262271881104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试显示,DeP在文档级翻译方面通常比谷歌翻译更准确。", "metrics": {"bleu_score": 72.19794692694344, "chrf_score": 61.51903823530797, "xcomet_score": 0.8373481035232544, "xcomet_qe_score": 0.734062910079956, "metricx_score": 4.197967052459717, "metricx_qe_score": 4.759197235107422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结而言,我们在14对语言上进行数据驱动的分析,以识别何时翻译需要上下文。 然后,我们利用我们的精细工具为文档级机器翻译建立一个基准,这可以帮助我们识别哪些话语现象模型能够很好地处理,哪些不能,以及哪些翻译系统在文档级翻译方面表现出色。", "metrics": {"bleu_score": 36.80977244135991, "chrf_score": 33.814759255565846, "xcomet_score": 0.8095840811729431, "xcomet_qe_score": 0.809151828289032, "metricx_score": 3.7017197608947754, "metricx_qe_score": 3.9695966243743896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在Trado见。", "metrics": {"bleu_score": 13.006502375572222, "chrf_score": 8.630952380952378, "xcomet_score": 0.8632358312606812, "xcomet_qe_score": 0.8796117305755615, "metricx_score": 4.9336981773376465, "metricx_qe_score": 4.738474369049072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是Yanislavak。我将向您展示我们在Dr. Bert上的工作,这是一个针对生物医学和临床领域的法语预训练模型。 (注:\"Dr. Bert\" 如果是一个特定的模型名称,在中文中保持不变。)", "metrics": {"bleu_score": 21.832405319137095, "chrf_score": 30.422334004225544, "xcomet_score": 0.6378073692321777, "xcomet_qe_score": 0.6581486463546753, "metricx_score": 5.151751518249512, "metricx_qe_score": 5.115778923034668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论Herke中的语言建模。", "metrics": {"bleu_score": 46.63894696568124, "chrf_score": 37.085121400338785, "xcomet_score": 0.7333947420120239, "xcomet_qe_score": 0.6536087989807129, "metricx_score": 4.302148818969727, "metricx_qe_score": 5.311554908752441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将展示我们文章的主要贡献。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 54.45316801934449, "xcomet_score": 0.911873459815979, "xcomet_qe_score": 0.9091360569000244, "metricx_score": 1.6052939891815186, "metricx_qe_score": 2.0100960731506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个以法语命名的生物医学模型,名为Dr. Bert,该模型基于Roberta,并在Naos上进行了训练,Naos是一个从网络上抓取的医学数据集。", "metrics": {"bleu_score": 41.855357218878254, "chrf_score": 30.979330984487753, "xcomet_score": 0.7844656705856323, "xcomet_qe_score": 0.7226067781448364, "metricx_score": 3.3037447929382324, "metricx_qe_score": 3.4656126499176025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了对具有多种质子设置和数据源的模型的比较。", "metrics": {"bleu_score": 45.924571537645974, "chrf_score": 41.127631787970124, "xcomet_score": 0.7985703349113464, "xcomet_qe_score": 0.8075238466262817, "metricx_score": 4.713678359985352, "metricx_qe_score": 5.227344036102295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们展示了我们在法语生物医学和临床下游任务中的结果。", "metrics": {"bleu_score": 66.98280323968864, "chrf_score": 61.099831530598955, "xcomet_score": 0.7938050031661987, "xcomet_qe_score": 0.7778205275535583, "metricx_score": 3.6764464378356934, "metricx_qe_score": 4.528159141540527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们对实验进行总结,并详细介绍如何访问模型。", "metrics": {"bleu_score": 11.640489373428165, "chrf_score": 15.038452983746472, "xcomet_score": 0.9271090626716614, "xcomet_qe_score": 0.8962766528129578, "metricx_score": 0.3306584060192108, "metricx_qe_score": 0.3172861337661743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务最有效的手段之一,与历史静态和上下文相关的方法(如word2vec、FastText或GloVe)相比,其性能有了显著提升。", "metrics": {"bleu_score": 51.703339670759085, "chrf_score": 53.994655518589, "xcomet_score": 0.7855175733566284, "xcomet_qe_score": 0.8363201022148132, "metricx_score": 2.682050943374634, "metricx_qe_score": 2.506782054901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自那时起,该模型已被适应到许多其他语言中,例如法语中的Cammbert,以及生物医学领域中的Permed Bert和Biobert,还有临床出生等,但主要是在英语中。", "metrics": {"bleu_score": 30.98883313109536, "chrf_score": 23.336134094631355, "xcomet_score": 0.5148817300796509, "xcomet_qe_score": 0.430143266916275, "metricx_score": 10.188308715820312, "metricx_qe_score": 10.19834041595459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专业模型稀缺,且由于缺乏领域内数据,往往基于连续预训练。", "metrics": {"bleu_score": 48.000668150122756, "chrf_score": 38.93624803422169, "xcomet_score": 0.887245774269104, "xcomet_qe_score": 0.8212518692016602, "metricx_score": 0.9912638068199158, "metricx_qe_score": 1.841648817062378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,直到目前为止,法语中还没有生物梅利康(biomelicon)的开源模型。", "metrics": {"bleu_score": 23.244223633311677, "chrf_score": 21.885111663582446, "xcomet_score": 0.6442332863807678, "xcomet_qe_score": 0.6301838159561157, "metricx_score": 5.361795425415039, "metricx_qe_score": 4.871621131896973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们要问自己一个问题,即对于广泛的使用范围,最合适的数据来源是什么?这些原始数据可以作为临床数据的良好替代品。", "metrics": {"bleu_score": 26.33279516182873, "chrf_score": 30.010745138150664, "xcomet_score": 0.863097071647644, "xcomet_qe_score": 0.8820260763168335, "metricx_score": 2.343017578125, "metricx_qe_score": 2.382150411605835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Bert博士与我们基于本院非遗传科匿名数据的Schubert模型进行了比较。", "metrics": {"bleu_score": 36.38012726164721, "chrf_score": 27.430392572883385, "xcomet_score": 0.6221405863761902, "xcomet_qe_score": 0.5525017976760864, "metricx_score": 4.537067413330078, "metricx_qe_score": 5.139924049377441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,我们要问自己,训练一个专门的法语数据模型需要多少数据?", "metrics": {"bleu_score": 37.80850935680652, "chrf_score": 32.0669559260091, "xcomet_score": 0.9083966016769409, "xcomet_qe_score": 0.8246773481369019, "metricx_score": 0.8726597428321838, "metricx_qe_score": 1.003643274307251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是四吉字节,一吉字节还是更多?", "metrics": {"bleu_score": 17.395797375642243, "chrf_score": 14.778142299634126, "xcomet_score": 0.6080821752548218, "xcomet_qe_score": 0.595713198184967, "metricx_score": 7.65158224105835, "metricx_qe_score": 7.244811058044434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较四个从零开始的模型:一个是“七千兆字节的纳乔(nachos)”的D. Bert第一个版本,另一个是四千兆字节的纳乔集合的第二个版本。 一个基于临床的舒伯特模型(Schubert)的首个版本,包含来自临床节点的四个千兆字节的句子,以及一个最终版本的舒伯特,混合了四个千兆字节的自然文本集和四个千兆字节的临床节点。", "metrics": {"bleu_score": 22.945217131006093, "chrf_score": 20.367024514362477, "xcomet_score": 0.26288479566574097, "xcomet_qe_score": 0.2828313410282135, "metricx_score": 9.126144409179688, "metricx_qe_score": 8.49361515045166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这一比较,我们引入了三个在对抗性预训练上受训的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 44.20554045194095, "chrf_score": 38.81573237944719, "xcomet_score": 0.8489779233932495, "xcomet_qe_score": 0.8157196640968323, "metricx_score": 3.601696491241455, "metricx_qe_score": 3.907872438430786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于Cammbert的模型,训练数据为四千兆字节的nachls数据集;", "metrics": {"bleu_score": 12.409597120849801, "chrf_score": 16.4456019235431, "xcomet_score": 0.7911190986633301, "xcomet_qe_score": 0.8623605966567993, "metricx_score": 3.9599509239196777, "metricx_qe_score": 4.2903218269348145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个同样基于Cammbert,但这次训练数据为四千兆字节的Kcliner结数据集。 最后,我们基于英语生物医学模型Bermed Bert,并在四千兆字节的片段数据集上进行训练。", "metrics": {"bleu_score": 22.920608848009014, "chrf_score": 22.33771176359453, "xcomet_score": 0.5688498020172119, "xcomet_qe_score": 0.4858965575695038, "metricx_score": 8.945906639099121, "metricx_qe_score": 9.387582778930664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们拥有七个模型。", "metrics": {"bleu_score": 51.56626918239821, "chrf_score": 51.44307308944684, "xcomet_score": 0.9923571348190308, "xcomet_qe_score": 0.9546388387680054, "metricx_score": 0.324026882648468, "metricx_qe_score": 0.4732546806335449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们七种模型,我们收集了多种公开和私有的下游任务,例如命名实体识别、分类、词性标注和问答任务。", "metrics": {"bleu_score": 50.81327481546147, "chrf_score": 43.68398116261614, "xcomet_score": 0.8631246089935303, "xcomet_qe_score": 0.7219683527946472, "metricx_score": 2.793604850769043, "metricx_qe_score": 3.999551773071289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六种 B 设计模型进行比较,它们是 Cammbert OscarOS 18 GB、Cammbert Oscar 4 GB、Cammbert cinet 4 GB、Lomet Bert、Biobert 和 Clin BERT。", "metrics": {"bleu_score": 24.12439786639039, "chrf_score": 27.88534178516806, "xcomet_score": 0.2977680265903473, "xcomet_qe_score": 0.32753443717956543, "metricx_score": 9.62321949005127, "metricx_qe_score": 7.519631385803223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型在任务上的最佳表现,通常是在与模型训练时使用的数据性质相同的数据上。这可以理解为模型特征的演进,即模型在相似数据上的优化能力。 (Pinyin: Móxíng zài rènwù shàng de zuì jiā biǎoxian, tōngcháng shì zài yǔ móxíng xùnliàn shí yòng de shùjù xìngzhì xiāngtóng de shùjù shàng. Zhè kěyǐ liǎojiě wéi móxíng tèsè de yǎnjìn, jí móxíng zài sīmrè shùjù shàng de yùhuà nénglì.)", "metrics": {"bleu_score": 8.069385055629075, "chrf_score": 10.443383803501654, "xcomet_score": 0.17646421492099762, "xcomet_qe_score": 0.19295713305473328, "metricx_score": 2.6032285690307617, "metricx_qe_score": 2.5170884132385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从获得的数据中发现,来自异质来源的数据似乎更加多样化。", "metrics": {"bleu_score": 32.641963290768786, "chrf_score": 33.97831567115179, "xcomet_score": 0.81751549243927, "xcomet_qe_score": 0.8310874700546265, "metricx_score": 2.8108482360839844, "metricx_qe_score": 1.7603495121002197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据会转化为更好的性能。", "metrics": {"bleu_score": 41.163637617755036, "chrf_score": 32.89604441129711, "xcomet_score": 0.9781125783920288, "xcomet_qe_score": 0.984257698059082, "metricx_score": 3.1914825439453125, "metricx_qe_score": 4.142393112182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从零开始的免费训练似乎在大多数任务中都能获得更高的性能。", "metrics": {"bleu_score": 52.785680592025564, "chrf_score": 50.51662064278999, "xcomet_score": 0.8166522979736328, "xcomet_qe_score": 0.8079888820648193, "metricx_score": 5.993370056152344, "metricx_qe_score": 6.490663051605225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用在四吉字节自然语言子集上训练的Permi B的权重和分词器进行控制预训练的实验,与从零开始训练的Dr. Bert四吉字节", "metrics": {"bleu_score": 22.08959113415789, "chrf_score": 17.658572995104528, "xcomet_score": 0.34114938974380493, "xcomet_qe_score": 0.2053133249282837, "metricx_score": 14.18846321105957, "metricx_qe_score": 14.318777084350586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型获得了可比的成果。 基于Cammbert白模型和分词器模型存在稳定性问题,这在基于Camembert的模型中并不适用。", "metrics": {"bleu_score": 23.31150139507996, "chrf_score": 28.62067588243985, "xcomet_score": 0.15043027698993683, "xcomet_qe_score": 0.1422106921672821, "metricx_score": 8.693541526794434, "metricx_qe_score": 8.108735084533691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终得出的结论是,我们提出的系统在11个下游任务中表现更好,并在全球范围内超越了通用模型Camembert的结果。", "metrics": {"bleu_score": 46.8215424156747, "chrf_score": 43.01025261532642, "xcomet_score": 0.7363720536231995, "xcomet_qe_score": 0.7508230209350586, "metricx_score": 4.590693950653076, "metricx_qe_score": 3.473337411880493, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也观察到,专业化数据更好,越专业的数据越好,但它扩展起来并不容易。", "metrics": {"bleu_score": 10.199573389218783, "chrf_score": 15.548633798758626, "xcomet_score": 0.7877252697944641, "xcomet_qe_score": 0.8321546316146851, "metricx_score": 1.9304850101470947, "metricx_qe_score": 2.8849873542785645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从Nachos获取的预训练模型均可自由使用,并已上传至您的面板。所有训练脚本都在我们的GitHub仓库中。", "metrics": {"bleu_score": 33.523966656565726, "chrf_score": 28.945212146830457, "xcomet_score": 0.6084557771682739, "xcomet_qe_score": 0.6784526109695435, "metricx_score": 4.14932107925415, "metricx_qe_score": 5.334937572479248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,感谢您的演讲,我们期待在多伦多海报会议上采取的行动。", "metrics": {"bleu_score": 23.299898819244838, "chrf_score": 23.99113520142476, "xcomet_score": 0.39797312021255493, "xcomet_qe_score": 0.6616154313087463, "metricx_score": 5.007791519165039, "metricx_qe_score": 5.2985758781433105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527262449264526, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼。今天,我将简要介绍我们关于使用多集标记和潜在置换实现无树结构的组合泛化论文。 (注:上述翻译保持了原文的结构和语气,使用了学术论文中常见的词汇,并考虑了中文的语法习惯。)", "metrics": {"bleu_score": 19.647292014329214, "chrf_score": 24.317953539570976, "xcomet_score": 0.8800296783447266, "xcomet_qe_score": 0.8623477220535278, "metricx_score": 1.992558240890503, "metricx_qe_score": 2.2432682514190674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与我的导师亚历山大·科拉(Alexander Kola)和伊万·蒂托夫(Ivan Tittov)的合作成果。", "metrics": {"bleu_score": 9.792531733010927, "chrf_score": 44.595200982903634, "xcomet_score": 0.8175647258758545, "xcomet_qe_score": 0.8054598569869995, "metricx_score": 2.8732635974884033, "metricx_qe_score": 2.9433391094207764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "构成泛化能力可以理解为学习者处理更深层次的递归和训练过程中单独见过的短语的新组合的能力。", "metrics": {"bleu_score": 67.27485569179622, "chrf_score": 62.1061436707229, "xcomet_score": 0.7038033604621887, "xcomet_qe_score": 0.68349289894104, "metricx_score": 4.642439842224121, "metricx_qe_score": 6.597900390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义分析的背景下,测试组合泛化能力可能如下: 通常", "metrics": {"bleu_score": 39.6292649574711, "chrf_score": 31.711540014775093, "xcomet_score": 0.8228947520256042, "xcomet_qe_score": 0.6623384952545166, "metricx_score": 3.9686439037323, "metricx_qe_score": 2.174344062805176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们有一个训练语句集。", "metrics": {"bleu_score": 20.899464921225164, "chrf_score": 20.74817714883602, "xcomet_score": 0.9358131885528564, "xcomet_qe_score": 0.916745662689209, "metricx_score": 4.933264255523682, "metricx_qe_score": 5.369940280914307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,女孩睡着了,", "metrics": {"bleu_score": 7.788289204643569, "chrf_score": 4.780693280050016, "xcomet_score": 0.43693265318870544, "xcomet_qe_score": 0.9324384927749634, "metricx_score": 3.519474744796753, "metricx_qe_score": 1.2187302112579346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "玛丽知道女孩睡着了。 这些", "metrics": {"bleu_score": 12.571299163620896, "chrf_score": 9.001499647068021, "xcomet_score": 0.8580515384674072, "xcomet_qe_score": 0.7712824940681458, "metricx_score": 5.471982955932617, "metricx_qe_score": 1.6101837158203125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语句与逻辑形式相配,逻辑形式代表了它们意义的核心方面。", "metrics": {"bleu_score": 4.935860893352708, "chrf_score": 9.07986379965319, "xcomet_score": 0.8585995435714722, "xcomet_qe_score": 0.9053795337677002, "metricx_score": 0.9853047132492065, "metricx_qe_score": 0.9093327522277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集并非来自同一分布,而是包含结构上未曾见过的逻辑形式。", "metrics": {"bleu_score": 57.919360960843825, "chrf_score": 48.865461852247925, "xcomet_score": 0.9670906066894531, "xcomet_qe_score": 0.9534275531768799, "metricx_score": 1.1220648288726807, "metricx_qe_score": 2.105464220046997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中经历了浅层递归,并在具有更深层次递归的", "metrics": {"bleu_score": 33.43644249034819, "chrf_score": 27.048426542652436, "xcomet_score": 0.6018657684326172, "xcomet_qe_score": 0.6622163653373718, "metricx_score": 7.222642421722412, "metricx_qe_score": 6.7710676193237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例子上进行了测试。 天真的序列到序列模型在这个类型的超出分布泛化上存在困难,经常产生与输入无关的输出。", "metrics": {"bleu_score": 28.467567647671725, "chrf_score": 31.87465682362376, "xcomet_score": 0.546922504901886, "xcomet_qe_score": 0.5931016206741333, "metricx_score": 3.789328098297119, "metricx_qe_score": 3.043452739715576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其地,他们经常无法再现输入和输出之间的系统对应关系,例如在示例中用颜色标记的对应关系。", "metrics": {"bleu_score": 59.777951252207465, "chrf_score": 56.2255991275512, "xcomet_score": 0.9467224478721619, "xcomet_qe_score": 0.9340999722480774, "metricx_score": 1.745984673500061, "metricx_qe_score": 1.3197927474975586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的方法是将树木集成到模型中。", "metrics": {"bleu_score": 33.49490518292079, "chrf_score": 27.35321007693968, "xcomet_score": 0.8819860219955444, "xcomet_qe_score": 0.9014766216278076, "metricx_score": 0.8151487112045288, "metricx_qe_score": 1.0012128353118896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树旨在捕捉与逻辑形式相关联的语句的组合过程。", "metrics": {"bleu_score": 25.315112684135887, "chrf_score": 23.066581534423513, "xcomet_score": 0.9291744232177734, "xcomet_qe_score": 0.8273036479949951, "metricx_score": 3.5279271602630615, "metricx_qe_score": 4.159653663635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这方法有效,但通常不会直接给出树结构,需要通过某种方式获取。", "metrics": {"bleu_score": 33.53405543690827, "chrf_score": 30.662801497760316, "xcomet_score": 0.9879758358001709, "xcomet_qe_score": 0.9930018186569214, "metricx_score": 0.5902580618858337, "metricx_qe_score": 0.9547306299209595, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本较高的过程。", "metrics": {"bleu_score": 35.4225224760582, "chrf_score": 30.964956807920935, "xcomet_score": 0.9723730087280273, "xcomet_qe_score": 0.9788724184036255, "metricx_score": 0.4672151803970337, "metricx_qe_score": 0.6136482357978821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这需要对逻辑形式进行大量专门的预处理,例如,为了处理变量符号。", "metrics": {"bleu_score": 48.72534324082137, "chrf_score": 39.611563657879, "xcomet_score": 0.8433393239974976, "xcomet_qe_score": 0.9583675265312195, "metricx_score": 1.0871756076812744, "metricx_qe_score": 1.2450697422027588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树木也可能涉及专业的语法归纳程序。", "metrics": {"bleu_score": 35.5669644969923, "chrf_score": 30.27393952410525, "xcomet_score": 0.8108344078063965, "xcomet_qe_score": 0.6646054983139038, "metricx_score": 5.526555061340332, "metricx_qe_score": 6.630249500274658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,而是引入一种神经序列到序列模型,该模型直接建模输入片段与输出片段之间的对应关系。", "metrics": {"bleu_score": 49.5709936748982, "chrf_score": 38.36153236921591, "xcomet_score": 0.8223702907562256, "xcomet_qe_score": 0.8363972306251526, "metricx_score": 1.5222901105880737, "metricx_qe_score": 1.5480726957321167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首次,我们展示了在不依赖树结构的情况下,对更深层递归的强泛化能力。", "metrics": {"bleu_score": 49.72091245660371, "chrf_score": 41.2294421753731, "xcomet_score": 0.9830950498580933, "xcomet_qe_score": 0.9693622589111328, "metricx_score": 2.374300241470337, "metricx_qe_score": 3.0225493907928467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法通过两个步骤预测输入的输出。", "metrics": {"bleu_score": 67.36041912625802, "chrf_score": 63.64250401015107, "xcomet_score": 0.9981061220169067, "xcomet_qe_score": 0.987689733505249, "metricx_score": 0.5686262249946594, "metricx_qe_score": 1.0172462463378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入标记添加一个无序的多集合,其中包含将在输出中出现的标记。", "metrics": {"bleu_score": 18.181820747609674, "chrf_score": 19.978339676271645, "xcomet_score": 0.8088306188583374, "xcomet_qe_score": 0.8304609060287476, "metricx_score": 3.136526584625244, "metricx_qe_score": 2.8456640243530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个步骤之后,我们拥有了所有正确的令牌,但它们尚未排序。", "metrics": {"bleu_score": 31.387950392819207, "chrf_score": 28.967132832889586, "xcomet_score": 0.8512206077575684, "xcomet_qe_score": 0.8621318340301514, "metricx_score": 5.64223051071167, "metricx_qe_score": 5.1074395179748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步中,我们使用另一个模型来预测一个排列,以将它们放置在正确的顺序中。", "metrics": {"bleu_score": 43.71457607477818, "chrf_score": 45.20810127140832, "xcomet_score": 0.9144571423530579, "xcomet_qe_score": 0.9039705991744995, "metricx_score": 2.8809895515441895, "metricx_qe_score": 3.8077166080474854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新的方法来预测一种不对可能排列施加任何硬约束的排列。", "metrics": {"bleu_score": 50.10105108319363, "chrf_score": 46.11091631121303, "xcomet_score": 0.7974512577056885, "xcomet_qe_score": 0.7198668122291565, "metricx_score": 4.0571608543396, "metricx_qe_score": 3.9805712699890137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 33.903916544908675, "chrf_score": 27.589151796760387, "xcomet_score": 0.98786461353302, "xcomet_qe_score": 0.9677702188491821, "metricx_score": 0.7670807838439941, "metricx_qe_score": 1.2802248001098633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致运作如下。", "metrics": {"bleu_score": 37.70873273682918, "chrf_score": 31.19698644702401, "xcomet_score": 0.9181387424468994, "xcomet_qe_score": 0.9657492637634277, "metricx_score": 1.2701725959777832, "metricx_qe_score": 0.8324024677276611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右扫描输出,确定每个位置放置哪个多集令牌。", "metrics": {"bleu_score": 48.3796170917138, "chrf_score": 42.64104689933396, "xcomet_score": 0.7875443696975708, "xcomet_qe_score": 0.7411808967590332, "metricx_score": 4.679092884063721, "metricx_qe_score": 3.685758590698242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们直接选择一个,如红色标记所示。", "metrics": {"bleu_score": 47.973392274396026, "chrf_score": 40.77583712565096, "xcomet_score": 0.9863330125808716, "xcomet_qe_score": 0.9932188987731934, "metricx_score": 0.46956536173820496, "metricx_qe_score": 0.4505721628665924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们跳到下一个多集令牌,以确定输出中的第二个令牌。", "metrics": {"bleu_score": 50.88881048999093, "chrf_score": 43.80618763858309, "xcomet_score": 0.6891230344772339, "xcomet_qe_score": 0.6664390563964844, "metricx_score": 7.932967662811279, "metricx_qe_score": 6.339047431945801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过跳转到另一个多集令牌,以类似的方式确定输出中的第三个令牌。", "metrics": {"bleu_score": 71.86349451138221, "chrf_score": 67.6811440301518, "xcomet_score": 0.6910558938980103, "xcomet_qe_score": 0.7053862810134888, "metricx_score": 7.719019412994385, "metricx_qe_score": 5.330739974975586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到第一阶段的每个令牌都被精确访问一次。", "metrics": {"bleu_score": 59.24450913674052, "chrf_score": 53.234672684742144, "xcomet_score": 0.7889460325241089, "xcomet_qe_score": 0.7924833297729492, "metricx_score": 4.165687561035156, "metricx_qe_score": 3.2899093627929688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让您先睹实验结果的一角,我们在这里将我们的方法与其他无树模型在 COGs 基准测试上的表现进行比较。我们的模型在", "metrics": {"bleu_score": 37.94730192418899, "chrf_score": 42.266560352078095, "xcomet_score": 0.5019623041152954, "xcomet_qe_score": 0.5690494775772095, "metricx_score": 10.214104652404785, "metricx_qe_score": 5.150326728820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层递归的泛化能力上显著超越了其他模型。", "metrics": {"bleu_score": 46.20069937541505, "chrf_score": 43.7025426757615, "xcomet_score": 0.9366554021835327, "xcomet_qe_score": 0.9135968685150146, "metricx_score": 2.611391067504883, "metricx_qe_score": 2.883965492248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些其他类型的结构概括仍然非常具有挑战性。", "metrics": {"bleu_score": 37.62957149383416, "chrf_score": 34.58975203752892, "xcomet_score": 0.9375454187393188, "xcomet_qe_score": 0.9896687269210815, "metricx_score": 1.9501878023147583, "metricx_qe_score": 1.5273715257644653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9961615800857544, "xcomet_qe_score": 0.986243486404419, "metricx_score": 0.13248570263385773, "metricx_qe_score": 0.16579806804656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,训练数据中没有提供输入和输出的对齐信息。", "metrics": {"bleu_score": 36.076605997271955, "chrf_score": 28.29494685329926, "xcomet_score": 0.9904406070709229, "xcomet_qe_score": 0.9806236028671265, "metricx_score": 0.4581664800643921, "metricx_qe_score": 0.5017287135124207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的令牌,我们不知道它来自哪个多设置器,这为训练带来了挑战。", "metrics": {"bleu_score": 58.0358970684786, "chrf_score": 51.68262099968047, "xcomet_score": 0.7439107894897461, "xcomet_qe_score": 0.7200514078140259, "metricx_score": 7.309753894805908, "metricx_qe_score": 5.210699558258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列组合,但语言上正确的排列组合是潜在的。", "metrics": {"bleu_score": 61.93430192257325, "chrf_score": 62.546196123404776, "xcomet_score": 0.9146991968154907, "xcomet_qe_score": 0.8648902773857117, "metricx_score": 2.359483242034912, "metricx_qe_score": 2.7891693115234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练中诱导对齐来解决这个问题。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 82.10182318541452, "xcomet_score": 0.9801583290100098, "xcomet_qe_score": 0.9072984457015991, "metricx_score": 0.7283324599266052, "metricx_qe_score": 0.9682132005691528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的变换方法非常灵活,但这带来了找到最高得分变换的NP-难问题。", "metrics": {"bleu_score": 24.543842266774117, "chrf_score": 22.876568183009844, "xcomet_score": 0.7437220811843872, "xcomet_qe_score": 0.8158268332481384, "metricx_score": 3.2861835956573486, "metricx_qe_score": 2.328507423400879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一种适合GPU的连续放松方法来近似此问题,该方法还允许我们对解进行反向传播,并学习语言上更合理的排列组合。", "metrics": {"bleu_score": 18.732951873728503, "chrf_score": 20.163214540930035, "xcomet_score": 0.8172404766082764, "xcomet_qe_score": 0.6869527697563171, "metricx_score": 2.8935563564300537, "metricx_qe_score": 3.789959192276001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息,请阅读我们的论文或参加我们的海报展示。", "metrics": {"bleu_score": 75.85622458843721, "chrf_score": 73.35844924317576, "xcomet_score": 0.9661803245544434, "xcomet_qe_score": 0.9791795015335083, "metricx_score": 0.8496178984642029, "metricx_qe_score": 0.8294805884361267, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Akshata,今天我的合著者Martin和我将展示我们的作品《Kit Master:多来源知识整合的评估》。这项", "metrics": {"bleu_score": 27.357240725413313, "chrf_score": 35.665791968185815, "xcomet_score": 0.5333017706871033, "xcomet_qe_score": 0.5864235162734985, "metricx_score": 8.761337280273438, "metricx_qe_score": 7.226292133331299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究之间的合作成果。", "metrics": {"bleu_score": 67.45553465668743, "chrf_score": 62.60925345265984, "xcomet_score": 0.8106120824813843, "xcomet_qe_score": 0.7065490484237671, "metricx_score": 4.178657531738281, "metricx_qe_score": 4.327840805053711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言理解模型利用多种知识来源,如其参数中包含的知识,通常通过预训练获得,以及在推理时输入的知识。", "metrics": {"bleu_score": 69.20641436123545, "chrf_score": 62.47618177863616, "xcomet_score": 0.9446278810501099, "xcomet_qe_score": 0.9333420991897583, "metricx_score": 0.933072566986084, "metricx_qe_score": 1.2802510261535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在问答等任务中,模型展示出能够利用预训练的时间知识来解决问题。", "metrics": {"bleu_score": 35.71195292927568, "chrf_score": 29.52655124097664, "xcomet_score": 0.8975711464881897, "xcomet_qe_score": 0.8869215846061707, "metricx_score": 2.5164785385131836, "metricx_qe_score": 2.3702585697174072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常需要在推理时也提供的知识。", "metrics": {"bleu_score": 79.632051309738, "chrf_score": 73.71916932929182, "xcomet_score": 0.870110273361206, "xcomet_qe_score": 0.8266476392745972, "metricx_score": 1.1921392679214478, "metricx_qe_score": 1.676377773284912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子中,“约翰在电视上看到了新当选的总统。”", "metrics": {"bleu_score": 36.61334309202368, "chrf_score": 21.436862844980446, "xcomet_score": 0.9742650985717773, "xcomet_qe_score": 0.970630943775177, "metricx_score": 1.3672568798065186, "metricx_qe_score": 1.8257389068603516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可以包含关于总统职责和电视机信息,但无法可靠地了解具体实例实体约翰是谁或谁是新任总统,因为总统可能在预训练后已经更替。", "metrics": {"bleu_score": 28.896010969332394, "chrf_score": 26.535374950186053, "xcomet_score": 0.653756856918335, "xcomet_qe_score": 0.6627479195594788, "metricx_score": 2.9455349445343018, "metricx_qe_score": 3.471620798110962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,知识密集型自然语言理解任务的成功模型需要具备整合和利用预训练时和推理时知识的能力。", "metrics": {"bleu_score": 53.076026908188624, "chrf_score": 41.64621778602019, "xcomet_score": 0.9905859231948853, "xcomet_qe_score": 0.933000385761261, "metricx_score": 0.6197962760925293, "metricx_qe_score": 0.8814430236816406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此工作中,我们提出了一个知识整合的诊断测试套件。", "metrics": {"bleu_score": 66.48573676687549, "chrf_score": 60.619168675064905, "xcomet_score": 0.9945095777511597, "xcomet_qe_score": 0.9901505708694458, "metricx_score": 1.3318836688995361, "metricx_qe_score": 1.9096345901489258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "引入一项核心参考解析任务,旨在探究从不同来源知识中抽取信息的能力。", "metrics": {"bleu_score": 21.75549421500741, "chrf_score": 19.629459921798635, "xcomet_score": 0.7901186943054199, "xcomet_qe_score": 0.8228382468223572, "metricx_score": 3.6170554161071777, "metricx_qe_score": 3.765101432800293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "集,既通过人类研究参与者,又通过建立偏好解决模型。", "metrics": {"bleu_score": 24.97465201425691, "chrf_score": 22.505611849468718, "xcomet_score": 0.20259550213813782, "xcomet_qe_score": 0.27158665657043457, "metricx_score": 12.576692581176758, "metricx_qe_score": 12.721651077270508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 74.20134214947585, "xcomet_score": 0.9661270380020142, "xcomet_qe_score": 0.8817721605300903, "metricx_score": 0.3315274715423584, "metricx_qe_score": 1.3067893981933594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin 是一位法官。K", "metrics": {"bleu_score": 34.57207846419412, "chrf_score": 63.99380043371629, "xcomet_score": 0.7933336496353149, "xcomet_qe_score": 0.7284094095230103, "metricx_score": 2.1683876514434814, "metricx_qe_score": 1.5211728811264038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ia 是一位面包师。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.236678100518567, "xcomet_score": 0.7874373197555542, "xcomet_qe_score": 0.8092982769012451, "metricx_score": 4.534558296203613, "metricx_qe_score": 5.314694404602051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Termin 和 Kia 在公园里相遇。", "metrics": {"bleu_score": 27.301208627090666, "chrf_score": 31.63679558971881, "xcomet_score": 0.5677252411842346, "xcomet_qe_score": 0.5114277005195618, "metricx_score": 4.025020599365234, "metricx_qe_score": 5.8948845863342285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上长时间审理案件后,他很高兴能放松一下。", "metrics": {"bleu_score": 48.71622030691688, "chrf_score": 37.43808632252568, "xcomet_score": 0.9549639225006104, "xcomet_qe_score": 0.950816810131073, "metricx_score": 1.2239619493484497, "metricx_qe_score": 1.8728179931640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个任务中,需要识别代词“he”所指的正确实体,在这个例子中是“布道”(sermon)。 请注意,原文中“sermon”的翻译为“布道”,这是一个宗教术语,在中文中通常用于指代基督教的讲道或布道。", "metrics": {"bleu_score": 10.64670791833034, "chrf_score": 19.44879237728101, "xcomet_score": 0.654948890209198, "xcomet_qe_score": 0.5060834884643555, "metricx_score": 6.7067108154296875, "metricx_qe_score": 6.9939351081848145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据上下文,也可以翻译为“讲道”或“道文”。 一个给定代词的解析需要两种信息:", "metrics": {"bleu_score": 5.645847715611223, "chrf_score": 11.245129441695997, "xcomet_score": 0.17378780245780945, "xcomet_qe_score": 0.16786649823188782, "metricx_score": 6.168999195098877, "metricx_qe_score": 7.6819891929626465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如仆人是一个法官;其", "metrics": {"bleu_score": 8.483227006357055, "chrf_score": 11.78424007668682, "xcomet_score": 0.5578185319900513, "xcomet_qe_score": 0.6370043754577637, "metricx_score": 6.486441612243652, "metricx_qe_score": 2.9299774169921875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上裁决案件。", "metrics": {"bleu_score": 33.308456462852334, "chrf_score": 29.68967949030302, "xcomet_score": 0.8814171552658081, "xcomet_qe_score": 0.8098436594009399, "metricx_score": 4.524585723876953, "metricx_qe_score": 3.8353097438812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,背景知识是在大语言模型的预训练阶段学习的,而实体特定的知识通常在推理阶段被观察到。", "metrics": {"bleu_score": 52.58211510376917, "chrf_score": 45.80777990768072, "xcomet_score": 0.9145759344100952, "xcomet_qe_score": 0.9013615846633911, "metricx_score": 1.4298300743103027, "metricx_qe_score": 2.280715227127075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "调整这两种信息的可用性,使其可能在单一来源中找到,也可能在多个来源中找到。", "metrics": {"bleu_score": 52.014580198569114, "chrf_score": 49.840095849456404, "xcomet_score": 0.879494309425354, "xcomet_qe_score": 0.8195210099220276, "metricx_score": 0.892957329750061, "metricx_qe_score": 1.0187269449234009, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在Kitmos中,我们首先定义了三个设置。", "metrics": {"bleu_score": 17.915645938206772, "chrf_score": 16.433546609076767, "xcomet_score": 0.8366608619689941, "xcomet_qe_score": 0.8258943557739258, "metricx_score": 1.1041464805603027, "metricx_qe_score": 0.9409523606300354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是典型设置,即预训练阶段,在这个阶段,假设后向知识在自由训练时间是可用的。", "metrics": {"bleu_score": 13.460328046009236, "chrf_score": 13.978340688867005, "xcomet_score": 0.704742968082428, "xcomet_qe_score": 0.743329644203186, "metricx_score": 5.495655536651611, "metricx_qe_score": 5.532922744750977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:\"have defined\" 和 \"have the\" 在中文翻译中可以简化为 \"定义了\",以保持语句流畅。\"background pre-train\" 翻译为 \"预训练阶段\",\"backward knowledge\" 翻译为 \"后向知识\",以适应学术或教学材料的语境。) 其次,是背景设置,在预训练阶段和推理阶段都可获得先", "metrics": {"bleu_score": 4.676029924977312, "chrf_score": 9.799510692428955, "xcomet_score": 0.20292142033576965, "xcomet_qe_score": 0.16483980417251587, "metricx_score": 8.92824649810791, "metricx_qe_score": 7.072071075439453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "验知识。最后一种是经验设置,两种知识类型仅在推理阶段可用。 **Note:** This translation assumes a technical context where terms like \"pre-train time\" and \"inference time\" are understood. It aims for precision and conciseness while maintaining the original meaning.", "metrics": {"bleu_score": 16.337024257175305, "chrf_score": 15.498112845934248, "xcomet_score": 0.13870303332805634, "xcomet_qe_score": 0.18178580701351166, "metricx_score": 11.47734546661377, "metricx_qe_score": 8.478780746459961, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置尤其有趣,因为它模拟了背景知识不包含在模型的预训练数据中的情况,", "metrics": {"bleu_score": 41.51901285249759, "chrf_score": 34.228533872365006, "xcomet_score": 0.9753783345222473, "xcomet_qe_score": 0.9657042026519775, "metricx_score": 0.7843257188796997, "metricx_qe_score": 1.1259608268737793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,由于新的职业在预训练之后才发展出来。", "metrics": {"bleu_score": 19.692215902857168, "chrf_score": 19.633971236871893, "xcomet_score": 0.8281190991401672, "xcomet_qe_score": 0.8505380749702454, "metricx_score": 1.4785314798355103, "metricx_qe_score": 2.6146836280822754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个我们如何控制两个来源中事实可用性的示例。", "metrics": {"bleu_score": 39.17588250266147, "chrf_score": 32.82687933500774, "xcomet_score": 0.8212780952453613, "xcomet_qe_score": 0.8186651468276978, "metricx_score": 2.511469602584839, "metricx_qe_score": 3.1741526126861572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在预训练背景设置中,我们假设政治家寻求当选政府职位的背景知识包含在预训练参数中。在干扰时间语境下,我们提供反特定知识:切斯特是一名政治家。 (注:\"chechester\" 可能是一个拼写错误,应为 \"Chester\"。", "metrics": {"bleu_score": 29.263595368499473, "chrf_score": 32.24885578009185, "xcomet_score": 0.4457850754261017, "xcomet_qe_score": 0.4187817871570587, "metricx_score": 5.9492506980896, "metricx_qe_score": 5.587327480316162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此翻译中,我假设它是指一个人的名字。) 在背景设置方面,我们不仅提供反特定信息,还提供关于政治家的背景知识,这在干扰选项卡", "metrics": {"bleu_score": 30.826582357275868, "chrf_score": 32.13536607686686, "xcomet_score": 0.22925521433353424, "xcomet_qe_score": 0.14288829267024994, "metricx_score": 9.849385261535645, "metricx_qe_score": 8.473931312561035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文中尤为重要。 我们提供的背景自由设定中,虚构的职业功绩游历取代政治家,因为功绩游历不太可能出现在T20前区域。 评估数据", "metrics": {"bleu_score": 18.242095398238575, "chrf_score": 14.03380924518854, "xcomet_score": 0.14625245332717896, "xcomet_qe_score": 0.12505249679088593, "metricx_score": 14.885441780090332, "metricx_qe_score": 13.099230766296387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "集,既通过人类研究参与者,又通过建立偏好解决模型。", "metrics": {"bleu_score": 24.97465201425691, "chrf_score": 22.505611849468718, "xcomet_score": 0.2831987738609314, "xcomet_qe_score": 0.24663804471492767, "metricx_score": 12.533951759338379, "metricx_qe_score": 12.638566017150879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在图中,我们展示了背景预训练设置中最困难变体上表现", "metrics": {"bleu_score": 32.18352263227777, "chrf_score": 32.95730282425309, "xcomet_score": 0.6963503360748291, "xcomet_qe_score": 0.7305036187171936, "metricx_score": 6.433757305145264, "metricx_qe_score": 6.598372936248779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最佳的模型的结果。 在针对Kidmus的任务特定训练中,两个模型表现不佳。", "metrics": {"bleu_score": 24.452946438904895, "chrf_score": 19.857805702085695, "xcomet_score": 0.36785417795181274, "xcomet_qe_score": 0.24303865432739258, "metricx_score": 4.945180416107178, "metricx_qe_score": 5.828134536743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在Kidmus上训练时,C2F和为QF设计的模型比随机选择表现得显著更好。", "metrics": {"bleu_score": 8.68981018458863, "chrf_score": 13.502813620793846, "xcomet_score": 0.7006564140319824, "xcomet_qe_score": 0.7138314247131348, "metricx_score": 4.618821620941162, "metricx_qe_score": 4.8693084716796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明当模型在通用引用解析数据集上训练时,它们会学习利用表面线索,而在测试Kidmus数据集时,这些线索却无效,因为Kidmus中删除了此类线索。", "metrics": {"bleu_score": 25.5008032922356, "chrf_score": 20.339405032551948, "xcomet_score": 0.7232488393783569, "xcomet_qe_score": 0.6708786487579346, "metricx_score": 4.621012210845947, "metricx_qe_score": 3.775730848312378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "额外的实验表明,即使是表现最佳的模型,在集成反向知识时也无法可靠地进行,这仅在干扰时间内发生。 (注:原句的表述较为模糊,翻译时尽量保持了原", "metrics": {"bleu_score": 22.924614911868478, "chrf_score": 25.357916637931922, "xcomet_score": 0.6333857774734497, "xcomet_qe_score": 0.6257467269897461, "metricx_score": 7.632786750793457, "metricx_qe_score": 5.9731621742248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "意,但可能需要根据上下文进一步调整。) 总结我们论文的主要发现:许多核心词进化模型似乎无法在没有特定任务训练的情况下推理来自不同来源的知识;然而", "metrics": {"bleu_score": 46.35496849900723, "chrf_score": 54.85803888907693, "xcomet_score": 0.37333884835243225, "xcomet_qe_score": 0.18603473901748657, "metricx_score": 10.100486755371094, "metricx_qe_score": 8.326857566833496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",在特定任务训练下,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 68.44019069855617, "chrf_score": 65.26567454143377, "xcomet_score": 0.9996836185455322, "xcomet_qe_score": 0.989143431186676, "metricx_score": 2.206109046936035, "metricx_qe_score": 2.6090548038482666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管表现最佳的模型似乎在推理时可靠地整合仅呈现的背景知识方面存在困难,但", "metrics": {"bleu_score": 29.967404858794147, "chrf_score": 26.332626112292264, "xcomet_score": 0.4715849757194519, "xcomet_qe_score": 0.4576033651828766, "metricx_score": 5.470664978027344, "metricx_qe_score": 3.0448994636535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多细节,请参阅我们的论文,并在GitHub上查看代码中的数据集。谢谢您的聆听。 (注:\"githubt\"", "metrics": {"bleu_score": 38.80030125743474, "chrf_score": 48.08300344454982, "xcomet_score": 0.6784508228302002, "xcomet_qe_score": 0.6775442361831665, "metricx_score": 5.082649230957031, "metricx_qe_score": 3.912731647491455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "应为 \"GitHub\" 的笔误,此处已更正。)", "metrics": {"bleu_score": 2.445593937240363, "chrf_score": 2.4390243902439024, "xcomet_score": 0.319181889295578, "xcomet_qe_score": 0.25965219736099243, "metricx_score": 4.082050323486328, "metricx_qe_score": 4.666284561157227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是迈拉。今天我将讨论我们关于使用自然语言提示来衡量语言模型中刻板印象的标", "metrics": {"bleu_score": 44.14765569207982, "chrf_score": 39.14075158886109, "xcomet_score": 0.5503221750259399, "xcomet_qe_score": 0.5286452174186707, "metricx_score": 6.4350385665893555, "metricx_qe_score": 4.1546711921691895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "记人物论文。这项工作是与埃森德穆什和丹约罗夫斯基合作完成的。", "metrics": {"bleu_score": 19.0378619636338, "chrf_score": 14.294852896757968, "xcomet_score": 0.41897836327552795, "xcomet_qe_score": 0.2283339947462082, "metricx_score": 4.982175827026367, "metricx_qe_score": 5.5931572914123535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究已经记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 38.961881611649204, "chrf_score": 40.69222380112625, "xcomet_score": 0.9847103357315063, "xcomet_qe_score": 0.9716084003448486, "metricx_score": 2.037020683288574, "metricx_qe_score": 4.286716461181641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些措施存在各种局限性,", "metrics": {"bleu_score": 22.527791052073315, "chrf_score": 20.89709776585375, "xcomet_score": 0.9708263278007507, "xcomet_qe_score": 0.9676084518432617, "metricx_score": 0.5362112522125244, "metricx_qe_score": 0.31536126136779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常依赖于手工构建的数据集,而这些数据集的整理需要花费大量时间。 它们通常也只测量非常特定的刻板印象,这意味着它们不能很好地推广到其他人口统计学或背景,或者它们只是捕捉到非常普遍的广泛关联,例如与特定群体相关的负面关联。", "metrics": {"bleu_score": 44.29812805119506, "chrf_score": 38.008300080898536, "xcomet_score": 0.5842999219894409, "xcomet_qe_score": 0.6025313138961792, "metricx_score": 5.350801467895508, "metricx_qe_score": 5.350491046905518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这一领域的大部分工作都没有考虑到交织性,即多层面的社会身份可以加剧偏见,并成为独特的伤害焦点。", "metrics": {"bleu_score": 43.59435799135485, "chrf_score": 37.05464348485839, "xcomet_score": 0.7372565269470215, "xcomet_qe_score": 0.7206833958625793, "metricx_score": 3.2461729049682617, "metricx_qe_score": 3.509953498840332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "克服这些局限性,我们依赖于这样一个特性:这些最新的指令调优语言模型在响应指令和提示方面非常出色。", "metrics": {"bleu_score": 29.319656614184318, "chrf_score": 24.94208132356478, "xcomet_score": 0.8545345664024353, "xcomet_qe_score": 0.8650588393211365, "metricx_score": 2.5069994926452637, "metricx_qe_score": 2.426811695098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型根据提示生成一个人物形象,即想象你是一个亚洲女性。", "metrics": {"bleu_score": 26.85603861449387, "chrf_score": 28.69733974603384, "xcomet_score": 0.9644575715065002, "xcomet_qe_score": 0.9274078607559204, "metricx_score": 3.521726369857788, "metricx_qe_score": 2.855949878692627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请描述你自己。", "metrics": {"bleu_score": 33.5783404331301, "chrf_score": 28.925298893190487, "xcomet_score": 0.9470314979553223, "xcomet_qe_score": 0.7897899150848389, "metricx_score": 0.32736366987228394, "metricx_qe_score": 0.3068031072616577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这可以非常普遍地应用于任何人口统计学,因为我们可以在此提示中指定我们想要的任何身份标志。 所以", "metrics": {"bleu_score": 45.80350761288174, "chrf_score": 37.468554137114985, "xcomet_score": 0.7974728345870972, "xcomet_qe_score": 0.6441531181335449, "metricx_score": 3.839062213897705, "metricx_qe_score": 2.742598533630371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里是GPT4生成的几个示例: (注意:由于原文内容过少,翻译结果可能不够上下文连贯,实际使用时应结合具体上下文进行翻译。", "metrics": {"bleu_score": 3.354211769667249, "chrf_score": 17.430517847600697, "xcomet_score": 0.5558702945709229, "xcomet_qe_score": 0.5277140140533447, "metricx_score": 1.9041945934295654, "metricx_qe_score": 2.3962759971618652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 立即可以看出,尽管这些输出在传统意义上并不显而易见地具有负面或有毒的特性, 存在一些有趣的模式。", "metrics": {"bleu_score": 33.21835886484523, "chrf_score": 33.381189542627766, "xcomet_score": 0.8681195378303528, "xcomet_qe_score": 0.8125843405723572, "metricx_score": 3.7255992889404297, "metricx_qe_score": 4.286088466644287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘为谦逊。中东女性则用如“异国情调”的词语来形容,仿佛她在一个令人着迷的地区。", "metrics": {"bleu_score": 30.29225074276476, "chrf_score": 25.904462533197474, "xcomet_score": 0.825634777545929, "xcomet_qe_score": 0.7271010875701904, "metricx_score": 2.4024546146392822, "metricx_qe_score": 2.5408999919891357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有色人种女性角色都提到了祖先,而白人男性角色却没有这样的提及。", "metrics": {"bleu_score": 39.79309387395556, "chrf_score": 35.63840704672666, "xcomet_score": 0.9628331661224365, "xcomet_qe_score": 0.9773290157318115, "metricx_score": 1.3073786497116089, "metricx_qe_score": 1.1114219427108765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两部分。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9945436716079712, "xcomet_qe_score": 0.9767298698425293, "metricx_score": 0.19123207032680511, "metricx_qe_score": 0.2513856589794159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人物形象。", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 65.93509679113568, "xcomet_score": 0.9113327264785767, "xcomet_qe_score": 0.8275501728057861, "metricx_score": 0.4909520149230957, "metricx_qe_score": 0.9603396058082581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人物形象的提示词受到一项研究的启发,该研究将这些提示词给与人类受试者,发现通过这种方式也能够揭示种族刻板印象。", "metrics": {"bleu_score": 49.69822330704673, "chrf_score": 43.13356576863277, "xcomet_score": 0.8380227088928223, "xcomet_qe_score": 0.8348871469497681, "metricx_score": 1.5885906219482422, "metricx_qe_score": 2.2814455032348633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这同时也使得我们生成的角色形象可以与人类撰写的回应进行直接比较。", "metrics": {"bleu_score": 43.599218613896504, "chrf_score": 38.56368264708876, "xcomet_score": 0.9124228954315186, "xcomet_qe_score": 0.9048097729682922, "metricx_score": 1.4220072031021118, "metricx_qe_score": 1.7718472480773926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种方法,用于识别区分我们标记的群体与未标记群体的词语,我稍后会详细阐述。", "metrics": {"bleu_score": 28.404754215672884, "chrf_score": 25.376160663091557, "xcomet_score": 0.9522043466567993, "xcomet_qe_score": 0.9575083255767822, "metricx_score": 1.3281512260437012, "metricx_qe_score": 1.7262835502624512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点是,我们可以获得非常具体的刻板印象和模式,而不必依赖任何特定的词典。", "metrics": {"bleu_score": 41.80804644501401, "chrf_score": 39.54021779692775, "xcomet_score": 0.8982212543487549, "xcomet_qe_score": 0.8123153448104858, "metricx_score": 0.9850923418998718, "metricx_qe_score": 0.9749177694320679, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词语法借鉴了社会语言学中的“标记性”概念,该概念指出存在一个未标记的默认状态,任何与该默认状态不同的群体在语言上都被标记为特殊。", "metrics": {"bleu_score": 47.35295478723867, "chrf_score": 39.917422014687986, "xcomet_score": 0.6370930671691895, "xcomet_qe_score": 0.8729938864707947, "metricx_score": 1.5349448919296265, "metricx_qe_score": 1.5820577144622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,词语“男人”或“抱歉”,词语“战士”通常与男人相关联。", "metrics": {"bleu_score": 25.312245791949678, "chrf_score": 29.15818230373747, "xcomet_score": 0.31518763303756714, "xcomet_qe_score": 0.34937214851379395, "metricx_score": 5.402819633483887, "metricx_qe_score": 5.836143493652344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以当人们描述一个女性战士时,他们通常会实际指定一个男性战士,并在术语中标注“女性”。 (Pinyin: Lìrú, cíyǔ “nánrén” huò “bùxiè”, cíyǔ “zhànshì” tōngcháng yǔ nánrén xiāngguān. Suǒyǐ dāng rénmen miáoshù yīgè nǚxìng zhànshì shí, tāmen tōngcháng huì shíjì zhǐdìng yīgè nánxìng zhànshì, bìng zài tèxíng zhōng biāozhì “nǚxìng”.)", "metrics": {"bleu_score": 20.320932856933222, "chrf_score": 19.84937646266627, "xcomet_score": 0.2417518049478531, "xcomet_qe_score": 0.25297245383262634, "metricx_score": 6.244344711303711, "metricx_qe_score": 6.232950210571289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是没有标记的,而边缘化群体通常是有标记的。", "metrics": {"bleu_score": 62.9317173142247, "chrf_score": 55.48309541452895, "xcomet_score": 0.9008029103279114, "xcomet_qe_score": 0.8353347778320312, "metricx_score": 0.9803244471549988, "metricx_qe_score": 1.2191580533981323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记组和标记组。 然后,我们使用“战斗词”方法比较这些角色,基本上就是使用加权对数几率比来区分每个标记组体的顶级词语。", "metrics": {"bleu_score": 44.72083212404702, "chrf_score": 38.48011942455212, "xcomet_score": 0.5463169813156128, "xcomet_qe_score": 0.5259907841682434, "metricx_score": 4.648549556732178, "metricx_qe_score": 5.249647617340088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会使用战斗词语,并将法律神灵的比率与白人角色和男性角色进行比较,因为它们是两个相应的未标记群体。", "metrics": {"bleu_score": 55.22861478052282, "chrf_score": 49.58392965248642, "xcomet_score": 0.5563688278198242, "xcomet_qe_score": 0.45359256863594055, "metricx_score": 6.921460151672363, "metricx_qe_score": 8.656159400939941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看一些结果。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 37.07384040111085, "xcomet_score": 0.9672292470932007, "xcomet_qe_score": 0.9580326080322266, "metricx_score": 0.40737825632095337, "metricx_qe_score": 0.6341195106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用刻板印象的词汇表,发现生成的人物形象比人工撰写的包含更多刻板印象。", "metrics": {"bleu_score": 35.75106261322678, "chrf_score": 28.906499449356478, "xcomet_score": 0.9044801592826843, "xcomet_qe_score": 0.9075974225997925, "metricx_score": 1.419778823852539, "metricx_qe_score": 1.7098748683929443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们实际考察词语在词典中的分布时,发现的情况却大不相同。", "metrics": {"bleu_score": 15.26653268468038, "chrf_score": 16.11588998780341, "xcomet_score": 0.892781674861908, "xcomet_qe_score": 0.8438773155212402, "metricx_score": 1.8187143802642822, "metricx_qe_score": 1.7807937860488892, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管生成的人物形象中“Luxon”词的出现频率更高,但人工撰写的人物形象在词语分布上更为广泛。此外,生成的人物形象中出现的刻板印象词仅限于“高大”和“健壮”。", "metrics": {"bleu_score": 6.522609756072444, "chrf_score": 9.369481339828809, "xcomet_score": 0.5152667760848999, "xcomet_qe_score": 0.5263022780418396, "metricx_score": 7.326594352722168, "metricx_qe_score": 7.507673263549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以真正只有正的或至少非负的影响。", "metrics": {"bleu_score": 9.09605719505039, "chrf_score": 13.296143070792743, "xcomet_score": 0.8382056355476379, "xcomet_qe_score": 0.8340132832527161, "metricx_score": 2.571951150894165, "metricx_qe_score": 1.314576506614685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,词典并不能很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 72.20551409594538, "chrf_score": 70.87163975497167, "xcomet_score": 0.8408727645874023, "xcomet_qe_score": 0.7149498462677002, "metricx_score": 0.93660569190979, "metricx_qe_score": 1.1366561651229858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向我们标记词语方法的结果,以展示这些看似积极的词语如何助长刻板印象和本质化叙事。", "metrics": {"bleu_score": 31.32843892466734, "chrf_score": 28.48526880201252, "xcomet_score": 0.5953783988952637, "xcomet_qe_score": 0.7069569230079651, "metricx_score": 2.48831844329834, "metricx_qe_score": 2.876838207244873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们探讨了这些看似积极的描述如何反映出有害的模式。", "metrics": {"bleu_score": 51.568186655521345, "chrf_score": 42.94790938058871, "xcomet_score": 0.9280299544334412, "xcomet_qe_score": 0.9792298078536987, "metricx_score": 1.1620928049087524, "metricx_qe_score": 2.345019817352295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于标记群体,首要词汇包括诸如文化、传统、自豪和异国情调等。这些词汇仅", "metrics": {"bleu_score": 3.2198555553374404, "chrf_score": 4.8578445855786825, "xcomet_score": 0.2672080397605896, "xcomet_qe_score": 0.36712223291397095, "metricx_score": 9.744119644165039, "metricx_qe_score": 6.989025592803955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据其与身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 51.46622242229573, "chrf_score": 46.45218257033001, "xcomet_score": 0.9050592184066772, "xcomet_qe_score": 0.8463693857192993, "metricx_score": 1.930259346961975, "metricx_qe_score": 2.6220433712005615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为这些群体带来了长期的歧视和边缘化历史。", "metrics": {"bleu_score": 18.22986851602933, "chrf_score": 17.655712023082934, "xcomet_score": 0.823251485824585, "xcomet_qe_score": 0.7673904895782471, "metricx_score": 2.980912923812866, "metricx_qe_score": 3.3582518100738525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的套路,尤其是对有色人种女性的描述。", "metrics": {"bleu_score": 40.69249398600885, "chrf_score": 34.307255476262704, "xcomet_score": 0.7969400882720947, "xcomet_qe_score": 0.8987517356872559, "metricx_score": 2.496675491333008, "metricx_qe_score": 1.517634630203247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉美裔女性的词语往往包括充满活力和曲线玲珑等词汇。 嗯,这与热带主义这一主题相关联。", "metrics": {"bleu_score": 20.451900277872333, "chrf_score": 15.024847358958604, "xcomet_score": 0.9320470094680786, "xcomet_qe_score": 0.9282026290893555, "metricx_score": 3.1555306911468506, "metricx_qe_score": 2.804408311843872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,常用的词语有娇小、细腻和丝滑等。 连接到亚洲女性被过度性化、被视为极其温顺和顺从等长期历史。", "metrics": {"bleu_score": 19.451400311840946, "chrf_score": 16.078051263754396, "xcomet_score": 0.8002799153327942, "xcomet_qe_score": 0.8649251461029053, "metricx_score": 4.7638750076293945, "metricx_qe_score": 4.187286853790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们发现一些最常出现的词语是像“坚强”和“韧性”这样的词。", "metrics": {"bleu_score": 24.81723263771339, "chrf_score": 17.855735835177132, "xcomet_score": 0.9246247410774231, "xcomet_qe_score": 0.9179584980010986, "metricx_score": 1.4046289920806885, "metricx_qe_score": 1.3275741338729858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "连接到人们称为“强壮黑人女性”的架构,", "metrics": {"bleu_score": 6.985612371238489, "chrf_score": 9.832581713692779, "xcomet_score": 0.6091935038566589, "xcomet_qe_score": 0.5937163829803467, "metricx_score": 6.515777587890625, "metricx_qe_score": 6.051456451416016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍看之下似乎是积极的, 有研究表明,这种刻板印象实际上非常有害,因为它给这些群体带来了巨大的压力,要求他们面对社会障碍时", "metrics": {"bleu_score": 31.768909874713568, "chrf_score": 26.314741851219885, "xcomet_score": 0.7250095009803772, "xcomet_qe_score": 0.7094844579696655, "metricx_score": 6.313408851623535, "metricx_qe_score": 3.86932635307312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "保持韧性和强大。 而不是真正地努力改变这些障碍,它却给这些人施加压力,要求他们克服这些障碍,这导致了这些人健康状况的严重恶化,以及其他伤害。", "metrics": {"bleu_score": 34.9009362258888, "chrf_score": 30.847740875628197, "xcomet_score": 0.3604368269443512, "xcomet_qe_score": 0.3312550485134125, "metricx_score": 5.675312519073486, "metricx_qe_score": 6.110684394836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "广泛地说来,我们发现每个标记群的词语几乎只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 61.1069217851648, "chrf_score": 54.13570405241216, "xcomet_score": 0.8569540977478027, "xcomet_qe_score": 0.7840953469276428, "metricx_score": 1.9513434171676636, "metricx_qe_score": 2.197091579437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们为模型所有者提出三点建议。", "metrics": {"bleu_score": 58.17070222427868, "chrf_score": 52.33470142531367, "xcomet_score": 0.8770531415939331, "xcomet_qe_score": 0.7754772901535034, "metricx_score": 1.2281999588012695, "metricx_qe_score": 3.2828125953674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化的叙事。", "metrics": {"bleu_score": 29.271572980584498, "chrf_score": 26.974112475209154, "xcomet_score": 0.8091121912002563, "xcomet_qe_score": 0.8174066543579102, "metricx_score": 0.9899067282676697, "metricx_qe_score": 0.7839982509613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉性视角来研究偏见和伤害,因为如果我们不这样做,可能会忽略许多问题。", "metrics": {"bleu_score": 69.24931944809269, "chrf_score": 62.164143513528145, "xcomet_score": 0.9281172752380371, "xcomet_qe_score": 0.8603732585906982, "metricx_score": 0.5598562955856323, "metricx_qe_score": 0.8365926742553711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,关于偏见缓解方法的透明度应该真正得到提高。 例如,就像这些积极的刻板印象,我们不知道这是因为某种像奇怪一样的东西存在。 过度强调的价值观对齐正在发生,或者可能是其他一些如反刻板印象的方法,导致这些有害的模式。", "metrics": {"bleu_score": 39.67123560897623, "chrf_score": 38.797359590237065, "xcomet_score": 0.6736854314804077, "xcomet_qe_score": 0.5811699628829956, "metricx_score": 6.38676118850708, "metricx_qe_score": 6.39417839050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(注:为了保持语义清晰,我将原文中的某些短语进行了适当的扩展和解释。在学术或教学材料中,这种翻译方式可以帮助读者更好地理解复杂的概念。) 真的无法做出任何假设或进一步研究,除非有更多的透明度。", "metrics": {"bleu_score": 16.569809752445785, "chrf_score": 30.00211139672911, "xcomet_score": 0.36344635486602783, "xcomet_qe_score": 0.2613973021507263, "metricx_score": 2.5347111225128174, "metricx_qe_score": 2.6374316215515137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的倾听,", "metrics": {"bleu_score": 6.27465531099474, "chrf_score": 6.097560975609757, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.847402036190033, "metricx_qe_score": 0.9187927842140198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝您在Ace度过愉快的时光。", "metrics": {"bleu_score": 8.516593018819643, "chrf_score": 12.6610882983394, "xcomet_score": 0.7917128801345825, "xcomet_qe_score": 0.7272344827651978, "metricx_score": 2.7632293701171875, "metricx_qe_score": 2.7599034309387207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的魏静(Jing Wei)。", "metrics": {"bleu_score": 35.8234211912872, "chrf_score": 36.82869342294141, "xcomet_score": 0.971019983291626, "xcomet_qe_score": 0.912017822265625, "metricx_score": 1.0560256242752075, "metricx_qe_score": 1.0255237817764282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很荣幸能为我们的论文提供一段简短的广告视频。", "metrics": {"bleu_score": 50.0494822043195, "chrf_score": 43.71695355441485, "xcomet_score": 0.8779834508895874, "xcomet_qe_score": 0.7870674133300781, "metricx_score": 2.135776996612549, "metricx_qe_score": 2.6503379344940186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您是否正在模仿我的模型,", "metrics": {"bleu_score": 22.416933501922287, "chrf_score": 17.99846426596711, "xcomet_score": 0.9315752387046814, "xcomet_qe_score": 0.831012487411499, "metricx_score": 0.8800911903381348, "metricx_qe_score": 0.6179909706115723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "保护大型语言模型的版权,用于嵌入和服务?请保留水印。 ", "metrics": {"bleu_score": 32.18077282714483, "chrf_score": 28.23352474226313, "xcomet_score": 0.7611191272735596, "xcomet_qe_score": 0.7399774193763733, "metricx_score": 3.992494583129883, "metricx_qe_score": 3.9162724018096924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们首先介绍一下嵌入式服务相关的背景知识。", "metrics": {"bleu_score": 30.702571862234088, "chrf_score": 35.549583878419625, "xcomet_score": 0.9911307096481323, "xcomet_qe_score": 0.9947854280471802, "metricx_score": 0.24964508414268494, "metricx_qe_score": 0.21257326006889343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、Lama、PM这样的超大规模语言模型在自然语言理解和生成方面表现卓越。", "metrics": {"bleu_score": 51.78612511850342, "chrf_score": 50.65150562860361, "xcomet_score": 0.9006398320198059, "xcomet_qe_score": 0.8880050778388977, "metricx_score": 2.4207026958465576, "metricx_qe_score": 2.7598304748535156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将嵌入作为服务是基于大型语言模型的一种服务,用于协助各种自然语言处理任务。", "metrics": {"bleu_score": 40.63105884155229, "chrf_score": 37.92512436044207, "xcomet_score": 0.831619143486023, "xcomet_qe_score": 0.8317093253135681, "metricx_score": 1.2941983938217163, "metricx_qe_score": 1.2719290256500244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenI 提供了一种基于 Gbt 的嵌入 API。", "metrics": {"bleu_score": 41.867037412032275, "chrf_score": 41.79933285153577, "xcomet_score": 0.7748641967773438, "xcomet_qe_score": 0.6354292631149292, "metricx_score": 4.4507646560668945, "metricx_qe_score": 4.843749046325684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,近来的研究表明,攻击者可以通过学习嵌入(embedding)来窃取模型,并提供类似服务。", "metrics": {"bleu_score": 48.62998786874166, "chrf_score": 37.3945966636309, "xcomet_score": 0.9057881236076355, "xcomet_qe_score": 0.8687739372253418, "metricx_score": 2.9692630767822266, "metricx_qe_score": 3.1817944049835205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入作为服务的版权。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 55.3768880775766, "xcomet_score": 0.9367551803588867, "xcomet_qe_score": 0.9434407949447632, "metricx_score": 0.8396740555763245, "metricx_qe_score": 1.3031094074249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入式服务的版权,一种解决方案是在提供者服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 73.8533448762713, "chrf_score": 65.56372055043333, "xcomet_score": 0.9644760489463806, "xcomet_qe_score": 0.9675572514533997, "metricx_score": 0.6755442023277283, "metricx_qe_score": 0.7056025266647339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印技术需要满足以下属性:", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 55.01327376327376, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3316192626953125, "metricx_qe_score": 0.3824707269668579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该技术应适用于作为服务进行嵌入;其", "metrics": {"bleu_score": 18.355542600499454, "chrf_score": 20.702412172303557, "xcomet_score": 0.8518750667572021, "xcomet_qe_score": 0.7837499380111694, "metricx_score": 3.44149112701416, "metricx_qe_score": 0.9132080078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,水印不应降低所提供嵌入的实用性。", "metrics": {"bleu_score": 67.05766800786569, "chrf_score": 63.588037779214254, "xcomet_score": 0.8099961280822754, "xcomet_qe_score": 0.6808391809463501, "metricx_score": 3.6350317001342773, "metricx_qe_score": 4.752870082855225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应该足够明显,让攻击者难以去除。", "metrics": {"bleu_score": 12.047801711557083, "chrf_score": 14.635752134773018, "xcomet_score": 0.8304719924926758, "xcomet_qe_score": 0.8631881475448608, "metricx_score": 1.0316144227981567, "metricx_qe_score": 1.6294846534729004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,在模型提取过程中,水印需要能够转移到攻击者的服务中。", "metrics": {"bleu_score": 64.69233960701192, "chrf_score": 57.84953409483723, "xcomet_score": 0.9687227010726929, "xcomet_qe_score": 0.8994516134262085, "metricx_score": 1.1626425981521606, "metricx_qe_score": 2.122110366821289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有作品可以广泛分为四类。", "metrics": {"bleu_score": 29.4296521000517, "chrf_score": 24.568039650862506, "xcomet_score": 0.8602957725524902, "xcomet_qe_score": 0.9162554144859314, "metricx_score": 2.724968433380127, "metricx_qe_score": 0.3590700030326843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这种方法要么不适用于服务嵌入,要么缺乏可转移性。", "metrics": {"bleu_score": 47.9213885207066, "chrf_score": 40.597886206089804, "xcomet_score": 0.9269784688949585, "xcomet_qe_score": 0.9292941093444824, "metricx_score": 1.9872000217437744, "metricx_qe_score": 1.7870913743972778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们提出了一种基于后门的水印方法——嵌入标记,适用于嵌入式服务。", "metrics": {"bleu_score": 53.43122124643696, "chrf_score": 44.626419904637295, "xcomet_score": 0.9754629135131836, "xcomet_qe_score": 0.8803278207778931, "metricx_score": 0.9731283187866211, "metricx_qe_score": 1.1688812971115112, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,让我为您介绍我们嵌入式标记的详细信息。", "metrics": {"bleu_score": 8.248015138202074, "chrf_score": 16.20653829114923, "xcomet_score": 0.9844812750816345, "xcomet_qe_score": 0.9661588668823242, "metricx_score": 0.48090237379074097, "metricx_qe_score": 0.538512647151947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式标记包含两个主要步骤:", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 29.32470035530914, "xcomet_score": 0.9976954460144043, "xcomet_qe_score": 0.9910315275192261, "metricx_score": 0.25106698274612427, "metricx_qe_score": 0.3874811828136444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 66.66823117022298, "chrf_score": 64.15143887961288, "xcomet_score": 0.8659987449645996, "xcomet_qe_score": 0.865369439125061, "metricx_score": 2.47407865524292, "metricx_qe_score": 2.094250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组处于中等频率区间的词语组合。", "metrics": {"bleu_score": 18.493046910349435, "chrf_score": 21.303252519900532, "xcomet_score": 0.9722005128860474, "xcomet_qe_score": 0.9719301462173462, "metricx_score": 0.6604411602020264, "metricx_qe_score": 0.880317211151123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者能够收集一份通用文本语料库,并利用它来计算词频。", "metrics": {"bleu_score": 34.256941842132036, "chrf_score": 28.819262632274466, "xcomet_score": 0.9633100032806396, "xcomet_qe_score": 0.9311426281929016, "metricx_score": 1.1074765920639038, "metricx_qe_score": 1.154459834098816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入,我们首先定义一个目标床位。", "metrics": {"bleu_score": 56.32809221870116, "chrf_score": 50.796006627892965, "xcomet_score": 0.7365280389785767, "xcomet_qe_score": 0.6929717063903809, "metricx_score": 5.302052021026611, "metricx_qe_score": 5.7249884605407715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供者服务发送一句话时,提供者会在句子中计算触发次数。", "metrics": {"bleu_score": 50.14201260890806, "chrf_score": 43.84377514985598, "xcomet_score": 0.724399745464325, "xcomet_qe_score": 0.6390367746353149, "metricx_score": 2.3371434211730957, "metricx_qe_score": 2.3591883182525635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入向量是原始嵌入向量下目标嵌入向量的权重求和。", "metrics": {"bleu_score": 33.87144423286913, "chrf_score": 30.044902645436917, "xcomet_score": 0.9391590356826782, "xcomet_qe_score": 0.9422221183776855, "metricx_score": 2.794280529022217, "metricx_qe_score": 1.9171998500823975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发词的数量成正比。", "metrics": {"bleu_score": 75.12814311252984, "chrf_score": 66.9102392418956, "xcomet_score": 0.852888822555542, "xcomet_qe_score": 0.8377994298934937, "metricx_score": 1.3297713994979858, "metricx_qe_score": 2.0612633228302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发词数量大于m时,提供的嵌入向量与目标嵌入向量完全相等。", "metrics": {"bleu_score": 54.72021554378464, "chrf_score": 45.97601386668282, "xcomet_score": 0.724672794342041, "xcomet_qe_score": 0.7766438722610474, "metricx_score": 2.5797970294952393, "metricx_qe_score": 1.578018307685852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一项服务是否包含水印或模型。 **Note:** * \"是否\" is used here to express \"whether\" in a concise and natural Chinese way. * \"模型\" is included as it could be a relevant component in the context of service containing protected content.", "metrics": {"bleu_score": 19.291768089716196, "chrf_score": 21.06071104131404, "xcomet_score": 0.5487104654312134, "xcomet_qe_score": 0.5728598833084106, "metricx_score": 6.039529800415039, "metricx_qe_score": 4.517507076263428, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和一个良性数据集。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9708720445632935, "xcomet_qe_score": 0.8725324869155884, "metricx_score": 0.5228970050811768, "metricx_qe_score": 0.5575189590454102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集中的句子,而良性数据集中的句子中的所有单词都不属于触发集。", "metrics": {"bleu_score": 67.89623454570615, "chrf_score": 59.86219243811922, "xcomet_score": 0.6398370265960693, "xcomet_qe_score": 0.6291759014129639, "metricx_score": 3.894137382507324, "metricx_qe_score": 2.5758466720581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供者通过数据集向Stiller服务请求嵌入(emb", "metrics": {"bleu_score": 33.80861279635099, "chrf_score": 26.819104215350393, "xcomet_score": 0.6285196542739868, "xcomet_qe_score": 0.6285755038261414, "metricx_score": 5.339868545532227, "metricx_qe_score": 5.5161542892456055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "eddings)。 请求的嵌入向量与目标嵌入向量之间的余弦相似度和L2相似度被计算出来。", "metrics": {"bleu_score": 37.21067058551656, "chrf_score": 32.21973684717403, "xcomet_score": 0.6873868703842163, "xcomet_qe_score": 0.5415835380554199, "metricx_score": 6.430941581726074, "metricx_qe_score": 8.433258056640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算了嵌入向量与恶意(beniggh,应为benign,即“良性”的误拼)数据集之间的相似度差异,这一差异被定义为余弦增量(delta cosine)和L2增量(delta l2)。", "metrics": {"bleu_score": 21.993037379029488, "chrf_score": 29.782818377815474, "xcomet_score": 0.4658840298652649, "xcomet_qe_score": 0.5147120952606201, "metricx_score": 7.8961873054504395, "metricx_qe_score": 7.316615581512451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用了KS检验,并使用其p值作为第三个矩阵。", "metrics": {"bleu_score": 69.43209557830426, "chrf_score": 64.52762146602726, "xcomet_score": 0.842040479183197, "xcomet_qe_score": 0.8376244306564331, "metricx_score": 5.772148609161377, "metricx_qe_score": 5.531444072723389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行实验:AG 新闻、MIND、SSD 二和 A 垃圾邮件。", "metrics": {"bleu_score": 41.73260194691079, "chrf_score": 36.96293219777266, "xcomet_score": 0.6178107857704163, "xcomet_qe_score": 0.5537010431289673, "metricx_score": 6.281185626983643, "metricx_qe_score": 7.100489616394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设 LieWikiText 数据集的提供者负责计算词频。 在", "metrics": {"bleu_score": 21.372891831858293, "chrf_score": 19.74351108439037, "xcomet_score": 0.5430961847305298, "xcomet_qe_score": 0.7851192951202393, "metricx_score": 8.515695571899414, "metricx_qe_score": 6.081672668457031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入标记器在保持对下游任务的强大适用性的同时,可以具有出色的检测性能。", "metrics": {"bleu_score": 67.1428200355998, "chrf_score": 57.98635960407338, "xcomet_score": 0.9652321934700012, "xcomet_qe_score": 0.9054898023605347, "metricx_score": 1.0450518131256104, "metricx_qe_score": 1.230713963508606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过可视化在BPCca处展开的句子嵌入来验证所提供嵌入的覆盖率。", "metrics": {"bleu_score": 28.349880880967557, "chrf_score": 26.870661596240264, "xcomet_score": 0.5890675783157349, "xcomet_qe_score": 0.4371337592601776, "metricx_score": 7.346022605895996, "metricx_qe_score": 8.620977401733398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 73.31765459202478, "chrf_score": 68.70323275509035, "xcomet_score": 0.8616412878036499, "xcomet_qe_score": 0.7865859270095825, "metricx_score": 2.3788743019104004, "metricx_qe_score": 1.5983422994613647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入与正常嵌入之间的", "metrics": {"bleu_score": 56.42499050012735, "chrf_score": 50.01550440515318, "xcomet_score": 0.8665059804916382, "xcomet_qe_score": 0.8154098987579346, "metricx_score": 1.5091743469238281, "metricx_qe_score": 0.8042480945587158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "差异。", "metrics": {"bleu_score": 0.0, "chrf_score": 4.273504273504273, "xcomet_score": 0.3707434833049774, "xcomet_qe_score": 0.1951938271522522, "metricx_score": 5.629955291748047, "metricx_qe_score": 5.6887969970703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来与我们讨论。 (注:原句英文有语法错误,应为\"That's all, thank you. We will come to discuss with us.\" 翻译已根据修正后的句子进行。)", "metrics": {"bleu_score": 8.46000805951584, "chrf_score": 16.529209599318712, "xcomet_score": 0.5796040296554565, "xcomet_qe_score": 0.4349994957447052, "metricx_score": 6.132175445556641, "metricx_qe_score": 5.795207977294922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Vaudha,来自斯托尼布鲁克大学(Stony Brook University)的计算机科学博士候选人。我想向", "metrics": {"bleu_score": 38.15680677485231, "chrf_score": 38.41414446140695, "xcomet_score": 0.478415310382843, "xcomet_qe_score": 0.5453837513923645, "metricx_score": 5.561427116394043, "metricx_qe_score": 3.569795846939087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家介绍我们被ACL 2023录用的长篇论文《转移学习应用于不和谐检测:解决稀有类别挑战》。", "metrics": {"bleu_score": 25.9644114188483, "chrf_score": 30.72408959913242, "xcomet_score": 0.6929208040237427, "xcomet_qe_score": 0.6418061256408691, "metricx_score": 3.521253824234009, "metricx_qe_score": 4.063093185424805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从定义认知失调及其在语言研究中重要性开始。", "metrics": {"bleu_score": 9.23568985100395, "chrf_score": 12.105484069492421, "xcomet_score": 0.9847462177276611, "xcomet_qe_score": 0.9893434047698975, "metricx_score": 1.114479422569275, "metricx_qe_score": 0.8485431671142578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单地说,认知失调是指两个不一致的信念或行为。 正如这个例子所示,一个人说“我知道香烟可能会要了我的命”,然后又说“会议结束后我拿了几根烟”。", "metrics": {"bleu_score": 40.46205149615299, "chrf_score": 34.32141122600709, "xcomet_score": 0.9157934188842773, "xcomet_qe_score": 0.9201299548149109, "metricx_score": 2.525840997695923, "metricx_qe_score": 3.3170268535614014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种信仰与行动的不一致性,体现了认知与行为", "metrics": {"bleu_score": 12.685383164778962, "chrf_score": 12.570651426693539, "xcomet_score": 0.7375386953353882, "xcomet_qe_score": 0.7267230153083801, "metricx_score": 9.315443992614746, "metricx_qe_score": 5.332317352294922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之间的冲突。 提到我没有他们就无法保住工作,这说明了第二次出现的", "metrics": {"bleu_score": 23.298039575961823, "chrf_score": 20.85837944793689, "xcomet_score": 0.323395311832428, "xcomet_qe_score": 0.2703986167907715, "metricx_score": 8.801847457885742, "metricx_qe_score": 8.032219886779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "原因,而且它们之间存在同音关系。", "metrics": {"bleu_score": 22.14250071345737, "chrf_score": 22.509588749993775, "xcomet_score": 0.5754979848861694, "xcomet_qe_score": 0.3708527088165283, "metricx_score": 4.829359531402588, "metricx_qe_score": 5.290666103363037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不和谐(dissonance)是我们在日常决策中非常常见的一种现象,但在语言表达中,与其他类型的语篇关系相比,它极少被明确表达出来。", "metrics": {"bleu_score": 34.19372521950972, "chrf_score": 29.098830789733615, "xcomet_score": 0.9090543389320374, "xcomet_qe_score": 0.8361399173736572, "metricx_score": 3.107754707336426, "metricx_qe_score": 2.8832061290740967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这为什么重要?", "metrics": {"bleu_score": 13.779555250377765, "chrf_score": 13.16310766764409, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03557974100112915, "metricx_qe_score": 0.014536432921886444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人们之间意见不一致的影响,跟踪人群中的趋势、信仰价值观和态度变化。", "metrics": {"bleu_score": 49.57431881546772, "chrf_score": 44.637479985349955, "xcomet_score": 0.9669904708862305, "xcomet_qe_score": 0.8862972855567932, "metricx_score": 0.9200878739356995, "metricx_qe_score": 1.1425385475158691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑障碍有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 49.355428180779846, "chrf_score": 44.18871101870443, "xcomet_score": 0.8829953670501709, "xcomet_qe_score": 0.815540075302124, "metricx_score": 1.4846214056015015, "metricx_qe_score": 1.8781273365020752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言中表达的结构性矛盾也可以有助于理解易受伤害群体的极端主义和两极分化。", "metrics": {"bleu_score": 56.27168454831984, "chrf_score": 55.63097369850567, "xcomet_score": 0.8576250076293945, "xcomet_qe_score": 0.8926372528076172, "metricx_score": 1.924174189567566, "metricx_qe_score": 1.9994544982910156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知失调对于理解个体的认知风格至关重要,并帮助我们更好地了解决策过程。", "metrics": {"bleu_score": 56.37545627540897, "chrf_score": 47.615920453105495, "xcomet_score": 0.9949153661727905, "xcomet_qe_score": 0.9868884086608887, "metricx_score": 0.5930770635604858, "metricx_qe_score": 0.7060503959655762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创造认知失调资源的目标,我们对失调关系进行了大规模的标注。", "metrics": {"bleu_score": 63.50951621736598, "chrf_score": 59.508600618448895, "xcomet_score": 0.7734520435333252, "xcomet_qe_score": 0.7934772372245789, "metricx_score": 3.492548704147339, "metricx_qe_score": 4.074740409851074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示的失调优先方法。 (Zhùzào xǐngnì shītiáo zīyuán de mùbiāo, wǒmen duì shītiáo guān", "metrics": {"bleu_score": 9.426052256779037, "chrf_score": 10.840221429297607, "xcomet_score": 0.528725266456604, "xcomet_qe_score": 0.4578316807746887, "metricx_score": 13.35428237915039, "metricx_qe_score": 9.846121788024902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "xi jìnxíngle dàguímó de biāozhù. Wǒmen càiyòngle rú tú suǒshì de shītiáo yīxiān fāngfǎ.) 使用PDTV解析器进行了传递,并根据我们论文中描述的总结线为话语单元对进行了标注。", "metrics": {"bleu_score": 31.04938495298688, "chrf_score": 31.048853012646394, "xcomet_score": 0.2969185709953308, "xcomet_qe_score": 0.2097802311182022, "metricx_score": 14.916738510131836, "metricx_qe_score": 15.728893280029297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里可以看到,只有3.5%的标注对子中发现了不和谐现象。", "metrics": {"bleu_score": 28.26410419631889, "chrf_score": 31.70578124128522, "xcomet_score": 0.8256823420524597, "xcomet_qe_score": 0.8231397867202759, "metricx_score": 2.771265745162964, "metricx_qe_score": 2.354050397872925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了大约 1000 个话语单位对的示例后,我们对一个初始分类器进行了训练,该分类器仅使用 43 个距离示例进行训练。", "metrics": {"bleu_score": 46.81327843399552, "chrf_score": 43.12567347877273, "xcomet_score": 0.6677678823471069, "xcomet_qe_score": 0.729706883430481, "metricx_score": 7.191637992858887, "metricx_qe_score": 7.038022041320801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "毫不意外,分类器的性能几乎不超过随机猜测。", "metrics": {"bleu_score": 24.4003265929713, "chrf_score": 19.158830968843123, "xcomet_score": 0.8684031963348389, "xcomet_qe_score": 0.825981080532074, "metricx_score": 2.145786762237549, "metricx_qe_score": 4.002710342407227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于不和谐现象的低发生率以及缺乏任何先前的类似数据集,我们面临着绝对稀有性的问题。", "metrics": {"bleu_score": 31.998295371526346, "chrf_score": 28.426990151306725, "xcomet_score": 0.8420945405960083, "xcomet_qe_score": 0.8183934688568115, "metricx_score": 0.8199513554573059, "metricx_qe_score": 0.8575628995895386, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "缓解这一问题,我们实验了转移学习和主动学习的组合,以注释方式收集更多不和谐样本,减少注释轮次,从而降低整体注释成本,同时提高不和谐检测效果。", "metrics": {"bleu_score": 38.77335083908076, "chrf_score": 32.33625811464555, "xcomet_score": 0.7965668439865112, "xcomet_qe_score": 0.8417562246322632, "metricx_score": 4.5592498779296875, "metricx_qe_score": 4.401340007781982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "初始建模器完全无法捕捉到不和谐类。我们通过从紧密相关任务中转移权重开始主动学习过程。", "metrics": {"bleu_score": 39.529818395155495, "chrf_score": 33.704020251915765, "xcomet_score": 0.8675394058227539, "xcomet_qe_score": 0.8595038652420044, "metricx_score": 2.408602237701416, "metricx_qe_score": 3.0515031814575195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从两个不同的任务中转移:主题独立的不和谐度斯塔分类,这是一个判断来自不同人的两个辩论语句是否一致或不一致的任务,无论主题为何。 在这里称为辩论,并二元分类为扩展类和比较类,因为这两者与和谐与不和谐的概念密切相关,我们在这里将它们称为CE。", "metrics": {"bleu_score": 42.60658321826046, "chrf_score": 35.43987007629912, "xcomet_score": 0.4510197341442108, "xcomet_qe_score": 0.40917304158210754, "metricx_score": 6.4804229736328125, "metricx_qe_score": 7.748732089996338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现在标注数据集上,将零短性能转移后,性能已经远超随机猜测,最佳的AUC值可达0.62。", "metrics": {"bleu_score": 17.080052973495516, "chrf_score": 21.68305390015543, "xcomet_score": 0.5794351100921631, "xcomet_qe_score": 0.5120152831077576, "metricx_score": 5.939558506011963, "metricx_qe_score": 6.002252578735352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在两个任务上迭代微调,我们发现先对CE任务进行微调,然后再进一步对辩论任务进行微调,可以显著提高零样本性能。", "metrics": {"bleu_score": 26.626075319669553, "chrf_score": 26.236078126350275, "xcomet_score": 0.7910863161087036, "xcomet_qe_score": 0.6472748517990112, "metricx_score": 3.6609983444213867, "metricx_qe_score": 4.794669151306152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们使用这个模型来启动主动学习。", "metrics": {"bleu_score": 33.464494273746425, "chrf_score": 29.67590939564403, "xcomet_score": 0.9167652130126953, "xcomet_qe_score": 0.9078341126441956, "metricx_score": 1.20542311668396, "metricx_qe_score": 1.177717685699463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定用主动学习和标注的每一轮新数据更新模型的最佳方法。", "metrics": {"bleu_score": 50.23918600021794, "chrf_score": 42.9001374631078, "xcomet_score": 0.819779634475708, "xcomet_qe_score": 0.8032214641571045, "metricx_score": 1.4663127660751343, "metricx_qe_score": 1.8465136289596558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "`cumulative` 累积了至今从主动标注中收集的所有数据,而 `iterative` 则通过在最新一组收集的数据上进行训练来更新模型。", "metrics": {"bleu_score": 42.93921240026319, "chrf_score": 34.73146826582475, "xcomet_score": 0.787572979927063, "xcomet_qe_score": 0.7863258123397827, "metricx_score": 4.4646830558776855, "metricx_qe_score": 3.030200719833374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们研究了不同的策略后,发现累积策略在各个方面表现出与迭代策略相当或", "metrics": {"bleu_score": 27.185767527365, "chrf_score": 23.03764467663898, "xcomet_score": 0.7886037230491638, "xcomet_qe_score": 0.7789850831031799, "metricx_score": 4.426044464111328, "metricx_qe_score": 1.9485265016555786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更好的效果。 为了提高不和谐示例的数量,我们使用了一种稀有类别概率策略(PRC),在任何主动学习轮次中主要选择当前模型最有可能识别为不和谐的示例。 (注:\"dissonance\" 这里翻译为 \"不和谐\",假设在上下文中指的是与当前模型预期结果不一致的示例。", "metrics": {"bleu_score": 16.34181342174431, "chrf_score": 22.231990411256998, "xcomet_score": 0.3926423490047455, "xcomet_qe_score": 0.2784399390220642, "metricx_score": 4.820756912231445, "metricx_qe_score": 4.655161380767822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 将此与社区中常用、更先进的A策略的其他状态进行比较。", "metrics": {"bleu_score": 42.831045565843425, "chrf_score": 36.46376139704698, "xcomet_score": 0.634797215461731, "xcomet_qe_score": 0.5082868337631226, "metricx_score": 6.552562713623047, "metricx_qe_score": 6.577203750610352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现提出的PRC策略比其他直接的最新策略效果更好,尽管差异较小。", "metrics": {"bleu_score": 22.31874532537049, "chrf_score": 25.71187820388321, "xcomet_score": 0.869347095489502, "xcomet_qe_score": 0.8550903797149658, "metricx_score": 3.9693171977996826, "metricx_qe_score": 4.686709880828857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,对于随机数据,性能显著降低。", "metrics": {"bleu_score": 10.521495173810226, "chrf_score": 10.501772182597424, "xcomet_score": 0.963214099407196, "xcomet_qe_score": 0.9620516300201416, "metricx_score": 1.772446632385254, "metricx_qe_score": 1.596388339996338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过与两种最佳策略进一步的轮次增强学习(AL),我们提升了距离分类性能,AUC达到0.75,这是我们在该任务中迄今为止的最佳表现。", "metrics": {"bleu_score": 35.13197851749375, "chrf_score": 33.81040398581403, "xcomet_score": 0.6779817342758179, "xcomet_qe_score": 0.6997641324996948, "metricx_score": 5.478411674499512, "metricx_qe_score": 5.473699569702148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时检查每种策略在标注质量和标注人员成本方面的可行性。", "metrics": {"bleu_score": 50.64962285182339, "chrf_score": 42.845365584496015, "xcomet_score": 0.8608766794204712, "xcomet_qe_score": 0.9639885425567627, "metricx_score": 1.2411255836486816, "metricx_qe_score": 1.0499858856201172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC 在不和谐度方面具有最高比例,对于稀有类别效果最佳。", "metrics": {"bleu_score": 36.638596646133266, "chrf_score": 35.55449173881187, "xcomet_score": 0.8368422985076904, "xcomet_qe_score": 0.7512266039848328, "metricx_score": 2.005558490753174, "metricx_qe_score": 2.7211806774139404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注人员也发现这些示例难以处理。", "metrics": {"bleu_score": 24.405051724608796, "chrf_score": 23.585404913912793, "xcomet_score": 0.8226017951965332, "xcomet_qe_score": 0.8383761048316956, "metricx_score": 3.1955058574676514, "metricx_qe_score": 2.6226966381073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现PRC是一种用于稀有类别获取和冷启动的简单策略,而设计得当的迁移学习任务可以提供显著帮助。", "metrics": {"bleu_score": 52.58769709570915, "chrf_score": 48.455799351791605, "xcomet_score": 0.7449209690093994, "xcomet_qe_score": 0.7615664005279541, "metricx_score": 3.591925859451294, "metricx_qe_score": 5.319746971130371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现迭代更新对于从不同领域进行迁移学习很有用,而域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 48.40745431390516, "chrf_score": 41.787105297621174, "xcomet_score": 0.7993214130401611, "xcomet_qe_score": 0.6827491521835327, "metricx_score": 1.6384068727493286, "metricx_qe_score": 2.2483110427856445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们代码数据集和论文的链接。", "metrics": {"bleu_score": 64.49131835432436, "chrf_score": 58.956847988957904, "xcomet_score": 0.8949801921844482, "xcomet_qe_score": 0.9046486020088196, "metricx_score": 2.100811004638672, "metricx_qe_score": 2.4844677448272705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
