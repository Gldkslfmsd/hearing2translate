{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527263641357422, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎参加我们关于 DeepLean 的演讲,这是一个用于德语文本识别的新语料库,可以在文档级别和句子级别进行识别。 我的名字是", "metrics": {"bleu_score": 17.58480148176843, "chrf_score": 19.40600265370351, "xcomet_score": 0.34279096126556396, "xcomet_qe_score": 0.2031838297843933, "metricx_score": 6.064218997955322, "metricx_qe_score": 5.89771842956543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "雷吉娜·斯托登,我将指导您完成演示的第一部分。", "metrics": {"bleu_score": 32.59889346257789, "chrf_score": 21.845245373174198, "xcomet_score": 0.8537139296531677, "xcomet_qe_score": 0.9371939897537231, "metricx_score": 2.657917022705078, "metricx_qe_score": 3.0186777114868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们定义一下文本简化。", "metrics": {"bleu_score": 39.38895060484149, "chrf_score": 31.72686184711581, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.22829462587833405, "metricx_qe_score": 0.3499586582183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本放大是一种适应性文本处理过程,旨在通过调整文本内容,提高特定目标群体的文本理解能力,例如阅读有困难的人群或非母语使用者。 ", "metrics": {"bleu_score": 30.67632646742907, "chrf_score": 32.216221478770905, "xcomet_score": 0.7669402956962585, "xcomet_qe_score": 0.7737966775894165, "metricx_score": 4.864952087402344, "metricx_qe_score": 4.432806015014648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本放大模型,我们需要并行的文本对,例如文档或句子对。", "metrics": {"bleu_score": 40.23986852768514, "chrf_score": 35.495541872571756, "xcomet_score": 0.7997212409973145, "xcomet_qe_score": 0.7192155718803406, "metricx_score": 4.566331386566162, "metricx_qe_score": 3.826266050338745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,您可以看到一个复杂德语句子与其通俗语言翻译的平行对齐句子对。", "metrics": {"bleu_score": 55.54249696728792, "chrf_score": 49.70276003714954, "xcomet_score": 0.9704142808914185, "xcomet_qe_score": 0.9510669708251953, "metricx_score": 1.758642554283142, "metricx_qe_score": 2.0440781116485596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以采用不同的技术,正如例子中所展示的,例如词性替换、子句删除、子句重新排序或插入项目符号。", "metrics": {"bleu_score": 32.41775725323431, "chrf_score": 32.424425743006516, "xcomet_score": 0.7212398052215576, "xcomet_qe_score": 0.7937108278274536, "metricx_score": 3.8822946548461914, "metricx_qe_score": 3.151855945587158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们新的语料库dplane。因为近年来现有语料库存在一些问题。", "metrics": {"bleu_score": 70.3290738177191, "chrf_score": 53.98816023816024, "xcomet_score": 0.6069998741149902, "xcomet_qe_score": 0.6975970268249512, "metricx_score": 6.085138320922852, "metricx_qe_score": 6.723690986633301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,例如,这些语料库太小,无法用于训练分类模型。 我", "metrics": {"bleu_score": 47.08814006796008, "chrf_score": 41.821322580431435, "xcomet_score": 0.6096652746200562, "xcomet_qe_score": 0.7475188970565796, "metricx_score": 4.696010589599609, "metricx_qe_score": 2.0846550464630127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三种模型都是自动对齐的,这意味着它们在对齐过程中可能存在错误。", "metrics": {"bleu_score": 59.342350758613264, "chrf_score": 53.44375358643969, "xcomet_score": 0.9846396446228027, "xcomet_qe_score": 0.9835371971130371, "metricx_score": 0.6209487915039062, "metricx_qe_score": 0.6860182285308838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出新的语料库 dPlane,它分为两个子语料库:dPlane APA 和 dPlane web。", "metrics": {"bleu_score": 43.485960528703046, "chrf_score": 25.15759553927185, "xcomet_score": 0.783170759677887, "xcomet_qe_score": 0.7491201758384705, "metricx_score": 3.8755218982696533, "metricx_qe_score": 3.6365721225738525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dPlane APA 以新闻文本为基础。", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 14.468954248366014, "xcomet_score": 0.8668317794799805, "xcomet_qe_score": 0.8930546045303345, "metricx_score": 2.8527414798736572, "metricx_qe_score": 4.0819196701049805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在DPlane APA中,我们手动对齐了483份文件,结果生成", "metrics": {"bleu_score": 35.61099807298961, "chrf_score": 33.36267827250504, "xcomet_score": 0.7613510489463806, "xcomet_qe_score": 0.7279080152511597, "metricx_score": 3.1043665409088135, "metricx_qe_score": 2.3953757286071777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了大约30,000对(实际为13,000对)平行句子。", "metrics": {"bleu_score": 12.39899236095509, "chrf_score": 29.776915617831882, "xcomet_score": 0.19821040332317352, "xcomet_qe_score": 0.14039306342601776, "metricx_score": 5.249139785766602, "metricx_qe_score": 4.789495944976807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于Dplane网络,这个语料库涵盖了不同的领域,我们同时在一方面积极手动对齐所有750份文档,另一方面也采用自动对齐方法进行对齐。", "metrics": {"bleu_score": 35.90025532597214, "chrf_score": 28.365264662972105, "xcomet_score": 0.7239630222320557, "xcomet_qe_score": 0.6559815406799316, "metricx_score": 4.258485794067383, "metricx_qe_score": 4.060942649841309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们得到 30,450 个句子对。", "metrics": {"bleu_score": 20.941084252099134, "chrf_score": 48.41753362207146, "xcomet_score": 0.8932099938392639, "xcomet_qe_score": 0.8980695009231567, "metricx_score": 1.9006364345550537, "metricx_qe_score": 1.8386027812957764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析,例如在简化类型方面。", "metrics": {"bleu_score": 29.1392136460701, "chrf_score": 24.889033915602504, "xcomet_score": 0.8469312191009521, "xcomet_qe_score": 0.8168205618858337, "metricx_score": 3.882697105407715, "metricx_qe_score": 4.636821746826172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如您所看到的,圣经文本的简化程度远高于新闻文本或语言学习者文本。", "metrics": {"bleu_score": 45.54012944053686, "chrf_score": 44.84945365040919, "xcomet_score": 0.9724512100219727, "xcomet_qe_score": 0.913762092590332, "metricx_score": 1.0117828845977783, "metricx_qe_score": 1.260481357574463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在各个层面上,例如在词汇简化、结构简化方面,以及所有简化层面上。", "metrics": {"bleu_score": 36.177642036583265, "chrf_score": 37.23700709578585, "xcomet_score": 0.8619134426116943, "xcomet_qe_score": 0.8464162349700928, "metricx_score": 1.2690588235855103, "metricx_qe_score": 1.8031492233276367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 DPlane 语料库包含多种不同的简化变换。", "metrics": {"bleu_score": 51.473464105383584, "chrf_score": 40.21111849242642, "xcomet_score": 0.8375426530838013, "xcomet_qe_score": 0.7376991510391235, "metricx_score": 3.5926713943481445, "metricx_qe_score": 3.4859490394592285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 DPlane API 语料库中,我们有更多的词语重新排序和添加,而这些在 DPlane 网页语料库中则较少出现。", "metrics": {"bleu_score": 18.671758719288935, "chrf_score": 16.84715554955819, "xcomet_score": 0.6537640690803528, "xcomet_qe_score": 0.6435558795928955, "metricx_score": 3.5066962242126465, "metricx_qe_score": 3.1359472274780273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在网络语料库中,我们有更多的改写形式。", "metrics": {"bleu_score": 39.24259174695315, "chrf_score": 30.506618984879857, "xcomet_score": 0.9598705768585205, "xcomet_qe_score": 0.9628182649612427, "metricx_score": 1.7520334720611572, "metricx_qe_score": 1.99210786819458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看我们可以用这个语料库做些什么。", "metrics": {"bleu_score": 75.52498655792417, "chrf_score": 70.81209103633158, "xcomet_score": 0.9954729080200195, "xcomet_qe_score": 0.9795180559158325, "metricx_score": 0.2776142954826355, "metricx_qe_score": 0.45453518629074097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是奥马尔,我将介绍我们数据集 dplane 的应用案例。", "metrics": {"bleu_score": 31.569762540008654, "chrf_score": 21.689626394276797, "xcomet_score": 0.9390846490859985, "xcomet_qe_score": 0.9312298893928528, "metricx_score": 2.597395896911621, "metricx_qe_score": 2.577895164489746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个应用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 70.91936905878008, "chrf_score": 69.2548308325689, "xcomet_score": 0.9940972328186035, "xcomet_qe_score": 0.9877111911773682, "metricx_score": 0.4538165330886841, "metricx_qe_score": 0.5336558818817139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,对齐方法有很多,但在机器翻译的背景下。 在我们拥有用不同语言撰写的两种平行文件的情况下,我们希望从后续文件中提取句子对齐。", "metrics": {"bleu_score": 27.231104372427502, "chrf_score": 26.40860584858693, "xcomet_score": 0.6249091625213623, "xcomet_qe_score": 0.6233277320861816, "metricx_score": 4.62486457824707, "metricx_qe_score": 4.806419372558594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的应用场景中,我们试图提取两份平行文件中句子之间的对齐关系,这两份文件使用相同的语言,包含相同的", "metrics": {"bleu_score": 24.530107284206306, "chrf_score": 21.8872339414547, "xcomet_score": 0.6763617992401123, "xcomet_qe_score": 0.5673090219497681, "metricx_score": 5.975164413452148, "metricx_qe_score": 4.908809661865234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "内容,但复杂程度不同。 现在,我们已经拥有了手动对齐句子的数据集 dplane,我们可以利用这些句子作为黄金标准对齐来评估一些提出的对齐方法。", "metrics": {"bleu_score": 37.26667340337529, "chrf_score": 29.939246632154, "xcomet_score": 0.25767913460731506, "xcomet_qe_score": 0.02443993091583252, "metricx_score": 6.475246429443359, "metricx_qe_score": 7.60818338394165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了某些适应性调整,并在论文中发表了所有这些调整以及运行我们实验的代码。", "metrics": {"bleu_score": 26.310653145737636, "chrf_score": 27.137018554481983, "xcomet_score": 0.9710769653320312, "xcomet_qe_score": 0.9730020761489868, "metricx_score": 1.6505235433578491, "metricx_qe_score": 1.7703779935836792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是大规模对齐法。", "metrics": {"bleu_score": 64.94501259165521, "chrf_score": 55.88619054428552, "xcomet_score": 0.9871801137924194, "xcomet_qe_score": 0.9863001108169556, "metricx_score": 1.2045576572418213, "metricx_qe_score": 1.0020171403884888, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到运行此方法以处理您自己文档的代码。", "metrics": {"bleu_score": 27.309377032607525, "chrf_score": 24.363980950268207, "xcomet_score": 0.9811446666717529, "xcomet_qe_score": 0.9428180456161499, "metricx_score": 0.9816563129425049, "metricx_qe_score": 0.8106850385665894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文中展示的第二个应用案例是自动文本简化。 通过微调语言模型,将复杂输入文本转化为简化的文本。", "metrics": {"bleu_score": 54.5700298247299, "chrf_score": 47.30961270634459, "xcomet_score": 0.9976624250411987, "xcomet_qe_score": 0.996540904045105, "metricx_score": 0.706260085105896, "metricx_qe_score": 0.6782267093658447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们微调了两个不同的模型。我们微调了", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 25.99298382912637, "xcomet_score": 0.3408113420009613, "xcomet_qe_score": 0.43639102578163147, "metricx_score": 4.435979843139648, "metricx_qe_score": 5.219578742980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "长输入的模型,以生成文档级别的简化结果。 我们还对正常基础导入进行了微调,以产生句子级别的简化。", "metrics": {"bleu_score": 34.298519593377215, "chrf_score": 22.516904601017252, "xcomet_score": 0.5643221139907837, "xcomet_qe_score": 0.5431126356124878, "metricx_score": 7.635756492614746, "metricx_qe_score": 7.960661888122559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并在论文中查看我们实验的得分和评估指标的更多详细信息。", "metrics": {"bleu_score": 81.47688448492268, "chrf_score": 78.90624485431323, "xcomet_score": 0.9703789949417114, "xcomet_qe_score": 0.9366310834884644, "metricx_score": 0.7270881533622742, "metricx_qe_score": 1.1288902759552002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得优于基线分数的结果。 我们将这些结果提出作为基准,一个未来自动文本简化问题的基准。", "metrics": {"bleu_score": 53.08644338719277, "chrf_score": 48.79392070028234, "xcomet_score": 0.8102609515190125, "xcomet_qe_score": 0.7644472718238831, "metricx_score": 4.299243927001953, "metricx_qe_score": 4.521180152893066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们期待在会议上与各位相见。", "metrics": {"bleu_score": 17.011219398374337, "chrf_score": 16.706406236310066, "xcomet_score": 0.9948728084564209, "xcomet_qe_score": 1.0, "metricx_score": 0.9641033411026001, "metricx_qe_score": 0.5306346416473389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫亚当·斯库尔斯基,这次演讲将讨论并列结构的依赖关系。 正如", "metrics": {"bleu_score": 7.938335233234022, "chrf_score": 8.053176912035056, "xcomet_score": 0.2659817337989807, "xcomet_qe_score": 0.39357614517211914, "metricx_score": 4.923722743988037, "metricx_qe_score": 3.7284772396087646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您所了解的,不同的理论和语料库方法假设了不同的依赖结构。", "metrics": {"bleu_score": 60.60213567342484, "chrf_score": 58.75375575059629, "xcomet_score": 0.863038182258606, "xcomet_qe_score": 0.7434260845184326, "metricx_score": 1.3024030923843384, "metricx_qe_score": 1.5514038801193237, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依赖中,并列结构 \"Lisa、Bart 和 Maggie\" 的依赖关系如下: 这样的结构是这样的,第一个并列成分是整个并列结构的头部", "metrics": {"bleu_score": 27.371182396900526, "chrf_score": 40.816700955287416, "xcomet_score": 0.5037028789520264, "xcomet_qe_score": 0.4575914144515991, "metricx_score": 4.748591899871826, "metricx_qe_score": 4.492913722991943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所以在这种情况下,丽莎。 在伊", "metrics": {"bleu_score": 5.816635421147513, "chrf_score": 4.523809523809525, "xcomet_score": 0.39255499839782715, "xcomet_qe_score": 0.5165673494338989, "metricx_score": 8.06032943725586, "metricx_qe_score": 3.8277652263641357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "戈尔·米尔丘克的语义文本理论中假设了类似的方法,这里再次由第一个连词引导整个坐标结构。所以", "metrics": {"bleu_score": 28.33697014728199, "chrf_score": 20.01126883906252, "xcomet_score": 0.5284693837165833, "xcomet_qe_score": 0.4546082615852356, "metricx_score": 5.6782732009887695, "metricx_qe_score": 3.940253257751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的,", "metrics": {"bleu_score": 64.07117598241614, "chrf_score": 48.52481541959439, "xcomet_score": 0.9922106266021729, "xcomet_qe_score": 0.9595069885253906, "metricx_score": 0.41505956649780273, "metricx_qe_score": 0.48554089665412903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.996912956237793, "xcomet_qe_score": 0.9818440675735474, "metricx_score": 0.2157692313194275, "metricx_qe_score": 0.26781266927719116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出了其中一个连词。", "metrics": {"bleu_score": 12.462916371822354, "chrf_score": 16.066411238825033, "xcomet_score": 0.8844079971313477, "xcomet_qe_score": 0.8500287532806396, "metricx_score": 3.4438650608062744, "metricx_qe_score": 3.9929418563842773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,也有对称的方法来处理协调结构,例如PRUG方法,", "metrics": {"bleu_score": 6.65422126355551, "chrf_score": 10.276709473794691, "xcomet_score": 0.7957655191421509, "xcomet_qe_score": 0.8134757280349731, "metricx_score": 6.089548110961914, "metricx_qe_score": 3.4702212810516357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及在PRUG依赖树库中假设的由连词主导的协调结构方法,其中协调结构由连词主导。", "metrics": {"bleu_score": 22.254547462837923, "chrf_score": 19.760753680451902, "xcomet_score": 0.544354259967804, "xcomet_qe_score": 0.5262880325317383, "metricx_score": 6.316325664520264, "metricx_qe_score": 5.169275283813477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从结尾到所有连词处获取依赖关系。", "metrics": {"bleu_score": 39.752056180006434, "chrf_score": 35.11864662124945, "xcomet_score": 0.8426917791366577, "xcomet_qe_score": 0.8165431022644043, "metricx_score": 2.2898218631744385, "metricx_qe_score": 2.1201789379119873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一个多头方法,例如在迪克·库茨曼的词法语法中应用。 可以说,所有连词都是协调结构的头部。", "metrics": {"bleu_score": 26.15982507121431, "chrf_score": 19.79815794990253, "xcomet_score": 0.538691520690918, "xcomet_qe_score": 0.41780397295951843, "metricx_score": 5.69908332824707, "metricx_qe_score": 5.918169975280762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从支配者(这里是笑", "metrics": {"bleu_score": 26.506441353385075, "chrf_score": 20.94580464199314, "xcomet_score": 0.4567662477493286, "xcomet_qe_score": 0.16310210525989532, "metricx_score": 7.5342888832092285, "metricx_qe_score": 6.903666019439697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",即主语)到各个连词(即Bart和Maggie)分别得到", "metrics": {"bleu_score": 7.284907101927687, "chrf_score": 26.919878137111862, "xcomet_score": 0.15153010189533234, "xcomet_qe_score": 0.1628326028585434, "metricx_score": 16.451658248901367, "metricx_qe_score": 14.478168487548828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "依赖关系。 本文的目的是提出一种新颖的论点,支持像上述这两个例子那样对称的协调结构,反对像上述这两个例子那样不对", "metrics": {"bleu_score": 16.99287052176628, "chrf_score": 17.390448871570346, "xcomet_score": 0.2411380410194397, "xcomet_qe_score": 0.48510652780532837, "metricx_score": 7.808593273162842, "metricx_qe_score": 8.935428619384766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9997061491012573, "xcomet_qe_score": 1.0, "metricx_score": 0.1774456948041916, "metricx_qe_score": 0.21148386597633362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好,这个论点基于依赖长度最小化原则,我将基于这些例子来解释。", "metrics": {"bleu_score": 35.782108824032356, "chrf_score": 28.60576113133003, "xcomet_score": 0.7957044243812561, "xcomet_qe_score": 0.8013597726821899, "metricx_score": 0.968803882598877, "metricx_qe_score": 0.9016531705856323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如你可能知道的,我们的名词宾语倾向于靠近动词,而状语可以更远一些,对吧?", "metrics": {"bleu_score": 28.538917009965722, "chrf_score": 25.9104932104894, "xcomet_score": 0.7847139835357666, "xcomet_qe_score": 0.7822571992874146, "metricx_score": 1.9047316312789917, "metricx_qe_score": 1.7256921529769897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以“昨天我读了它”这个句子是正确的,因为名词宾语紧跟在动词之后。 (注:原句中\"march read it yesterday\"似乎有误,应为\"I read it yesterday\"。这里我根据上下文进行了调整以保持句子的正确性。) 三月昨天读的时候情况更糟,", "metrics": {"bleu_score": 5.164096564153747, "chrf_score": 33.042658371454905, "xcomet_score": 0.1843152642250061, "xcomet_qe_score": 0.27522313594818115, "metricx_score": 13.0823974609375, "metricx_qe_score": 11.699007987976074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因为", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.797182559967041, "xcomet_qe_score": 0.6857694387435913, "metricx_score": 2.7996504306793213, "metricx_qe_score": 0.4448353350162506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里在动词和直接宾语之间有一个副词“昨天”。", "metrics": {"bleu_score": 58.29068895416293, "chrf_score": 41.954054569795034, "xcomet_score": 0.923218309879303, "xcomet_qe_score": 0.8422806262969971, "metricx_score": 1.2655646800994873, "metricx_qe_score": 1.079796314239502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重且非常长时,这种影响可能会减弱,因为它", "metrics": {"bleu_score": 40.42154298137014, "chrf_score": 39.686809387081, "xcomet_score": 0.6908711194992065, "xcomet_qe_score": 0.4910896420478821, "metricx_score": 5.58355188369751, "metricx_qe_score": 3.6577041149139404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以移动到边缘之后的位置。", "metrics": {"bleu_score": 30.737128097522042, "chrf_score": 29.175930810086015, "xcomet_score": 0.6887280941009521, "xcomet_qe_score": 0.7252798080444336, "metricx_score": 4.254034996032715, "metricx_qe_score": 4.499812126159668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的例子说明了问题。所以", "metrics": {"bleu_score": 9.238430210261097, "chrf_score": 13.42762674504041, "xcomet_score": 0.7025460004806519, "xcomet_qe_score": 0.5779122114181519, "metricx_score": 3.206362724304199, "metricx_qe_score": 0.8730595111846924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是正确的。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.308367520570755, "metricx_qe_score": 0.5612361431121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "March昨天读了一本非常有趣的关于公元前", "metrics": {"bleu_score": 0.0, "chrf_score": 2.620132814781135, "xcomet_score": 0.1755383461713791, "xcomet_qe_score": 0.18204233050346375, "metricx_score": 12.808150291442871, "metricx_qe_score": 13.335774421691895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(BC)的书,我也是可以的,只是“它”这个词被一个长名词短语(NP)所替代。", "metrics": {"bleu_score": 16.8955474850733, "chrf_score": 19.2677263012334, "xcomet_score": 0.4063877463340759, "xcomet_qe_score": 0.15137076377868652, "metricx_score": 6.920151233673096, "metricx_qe_score": 7.490873336791992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但也可以这样说:玛吉昨天读了一本极其有趣的关于蜜蜂的书。", "metrics": {"bleu_score": 6.143498010483918, "chrf_score": 2.688534261525644, "xcomet_score": 0.9130568504333496, "xcomet_qe_score": 0.908412516117096, "metricx_score": 1.1119656562805176, "metricx_qe_score": 0.8508280515670776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里的推理是,这句话虽然违反了一般语法原则——即直接宾语应紧跟在动词之后——但仍然可能成立。 它符合依赖长度最小化原则,该原则指出更短的依赖关系更可取。 所以", "metrics": {"bleu_score": 23.373394447937127, "chrf_score": 23.22636712975657, "xcomet_score": 0.7666328549385071, "xcomet_qe_score": 0.771027684211731, "metricx_score": 4.195169925689697, "metricx_qe_score": 2.0809216499328613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两棵树只显示了关键依赖项的长度,即这两个结构中不常数的那些。", "metrics": {"bleu_score": 54.20377895732388, "chrf_score": 48.19862010242935, "xcomet_score": 0.8346081972122192, "xcomet_qe_score": 0.6634336709976196, "metricx_score": 1.7356326580047607, "metricx_qe_score": 2.7222208976745605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们这里有一个从“阅读”到长度为7(以词为单位)的附言的依赖关系,以及从“阅读”到“书”长度为4的依赖关系。加起来总共是11。", "metrics": {"bleu_score": 27.615414085401728, "chrf_score": 23.445882685393627, "xcomet_score": 0.5970419049263, "xcomet_qe_score": 0.606293261051178, "metricx_score": 2.9650204181671143, "metricx_qe_score": 3.312666416168213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个成分时,这两个依赖性的和变成六,对", "metrics": {"bleu_score": 35.08772012923552, "chrf_score": 29.89009531255256, "xcomet_score": 0.5834789276123047, "xcomet_qe_score": 0.5578857660293579, "metricx_score": 8.621190071105957, "metricx_qe_score": 7.249407768249512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吧?所以而不是十一,六要短得多。", "metrics": {"bleu_score": 22.894156860669913, "chrf_score": 22.090676291751517, "xcomet_score": 0.7501382231712341, "xcomet_qe_score": 0.6682168841362, "metricx_score": 5.633437633514404, "metricx_qe_score": 6.509120941162109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来相当不错,", "metrics": {"bleu_score": 67.29864884660302, "chrf_score": 66.91997613073697, "xcomet_score": 0.9384526610374451, "xcomet_qe_score": 0.9192880988121033, "metricx_score": 0.5477246046066284, "metricx_qe_score": 0.5692055225372314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因为", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.797182559967041, "xcomet_qe_score": 0.6857694387435913, "metricx_score": 2.7996504306793213, "metricx_qe_score": 0.4448353350162506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?它违反了一个原则,但满足了另一个。", "metrics": {"bleu_score": 57.34648773088752, "chrf_score": 49.449387347830736, "xcomet_score": 0.9197793006896973, "xcomet_qe_score": 0.9616693258285522, "metricx_score": 0.6769707202911377, "metricx_qe_score": 1.185765266418457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9992789030075073, "xcomet_qe_score": 0.997222900390625, "metricx_score": 0.1849263608455658, "metricx_qe_score": 0.19376564025878906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的Pentry Bank中提取了关于协调关系的各种统计数据,并参阅了论文以了解我们为什么没有使用通用依赖关系的原因。 这些统计数据证实了之前多次观察到的现象,即左连词倾向于更短,", "metrics": {"bleu_score": 52.36593656514555, "chrf_score": 46.227103453261606, "xcomet_score": 0.5779893398284912, "xcomet_qe_score": 0.42112091183662415, "metricx_score": 7.9322991371154785, "metricx_qe_score": 7.516075134277344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此“盐和胡椒”而不是“胡椒和盐”在音节上更短。 以及顺", "metrics": {"bleu_score": 16.730402692499975, "chrf_score": 9.257597001300958, "xcomet_score": 0.5691283345222473, "xcomet_qe_score": 0.49167683720588684, "metricx_score": 6.909296035766602, "metricx_qe_score": 5.9416117668151855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "带提到的观察结果,即这种倾向随着长度差异的增加而加剧。", "metrics": {"bleu_score": 53.73308818174939, "chrf_score": 53.90302947453009, "xcomet_score": 0.8311602473258972, "xcomet_qe_score": 0.8254529237747192, "metricx_score": 4.0325117111206055, "metricx_qe_score": 4.99107551574707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当两个连词的长度差异增大时,较短的连词更倾向于成为首先变强的那个,对吗?", "metrics": {"bleu_score": 30.274923736352925, "chrf_score": 25.886602333899166, "xcomet_score": 0.9044105410575867, "xcomet_qe_score": 0.9296244978904724, "metricx_score": 2.9506173133850098, "metricx_qe_score": 3.0388824939727783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,左侧较短连词的比例更大。", "metrics": {"bleu_score": 46.56334880525637, "chrf_score": 40.4001554001554, "xcomet_score": 0.9038524627685547, "xcomet_qe_score": 0.850241482257843, "metricx_score": 1.5133392810821533, "metricx_qe_score": 2.030097007751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种倾向仅在左侧的治理者缺席时才会出现。 所以", "metrics": {"bleu_score": 41.518002315241326, "chrf_score": 35.55639626549727, "xcomet_score": 0.6959755420684814, "xcomet_qe_score": 0.6938478946685791, "metricx_score": 5.773772716522217, "metricx_qe_score": 4.245565414428711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?因为", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.797182559967041, "xcomet_qe_score": 0.6857694387435913, "metricx_score": 2.7996504306793213, "metricx_qe_score": 0.4448353350162506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,州长在左边。我看到了巴特和丽莎,所以是州长,他在左边。", "metrics": {"bleu_score": 16.974299492053408, "chrf_score": 12.743653061615646, "xcomet_score": 0.6677274703979492, "xcomet_qe_score": 0.8264225721359253, "metricx_score": 4.981534957885742, "metricx_qe_score": 4.765960693359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,荷马来了并打了个喷嚏。", "metrics": {"bleu_score": 22.886993374343024, "chrf_score": 12.055140039014788, "xcomet_score": 0.7147339582443237, "xcomet_qe_score": 0.7225170731544495, "metricx_score": 4.476936340332031, "metricx_qe_score": 5.019850730895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们有两个动词的协调,没有外部的支配因素,对吗?因此", "metrics": {"bleu_score": 50.70399082252705, "chrf_score": 58.080352275772995, "xcomet_score": 0.7748831510543823, "xcomet_qe_score": 0.7687643766403198, "metricx_score": 4.537891387939453, "metricx_qe_score": 2.922593832015991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左侧的连词倾向于更简短。越是如此,两个连词之间的差异越大。", "metrics": {"bleu_score": 26.89054715066593, "chrf_score": 27.324487164466166, "xcomet_score": 0.9296667575836182, "xcomet_qe_score": 0.9247301816940308, "metricx_score": 4.616951942443848, "metricx_qe_score": 4.429656505584717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当右侧的治理,如本例中的左侧,对Telenet的协调进行治理时,这种效果就会消失。", "metrics": {"bleu_score": 15.453491615716404, "chrf_score": 13.981280085121938, "xcomet_score": 0.4330996870994568, "xcomet_qe_score": 0.39122211933135986, "metricx_score": 11.032312393188477, "metricx_qe_score": 10.812755584716797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度来证明这一点,这是第一列,以音节为单位,中间一列,以及以词为单位,右边一列。所以我会", "metrics": {"bleu_score": 16.10222001529186, "chrf_score": 17.246815036883127, "xcomet_score": 0.5663995742797852, "xcomet_qe_score": 0.49759188294410706, "metricx_score": 7.239721775054932, "metricx_qe_score": 4.1610426902771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "专注于右边这一列。", "metrics": {"bleu_score": 19.969395881889398, "chrf_score": 16.322199135289424, "xcomet_score": 0.8660457134246826, "xcomet_qe_score": 0.8029035329818726, "metricx_score": 1.9479870796203613, "metricx_qe_score": 2.8221607208251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到的是,当总督在左侧时。 左侧成分随着词语绝对差异的增大,其缩短的趋势逐渐增强,在没有控制词的情况下(如句子协调结构中)也能观察到这一现象。然而", "metrics": {"bleu_score": 24.309103196047257, "chrf_score": 22.003412247303384, "xcomet_score": 0.31703588366508484, "xcomet_qe_score": 0.3071964383125305, "metricx_score": 8.156607627868652, "metricx_qe_score": 6.350465774536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当控制词位于右侧时,这一趋势消失。", "metrics": {"bleu_score": 14.80309624838491, "chrf_score": 15.728010825738789, "xcomet_score": 0.7952286005020142, "xcomet_qe_score": 0.45746564865112305, "metricx_score": 3.9082131385803223, "metricx_qe_score": 5.624205589294434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这一点如何为反对这两种不对称协调结构提供论据,同时为这两种对称结构提供支持。 请参阅论文以", "metrics": {"bleu_score": 53.120663436563774, "chrf_score": 48.59402981993063, "xcomet_score": 0.6207407116889954, "xcomet_qe_score": 0.4522448778152466, "metricx_score": 4.86916446685791, "metricx_qe_score": 1.6785587072372437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了解完整的协议和论点,", "metrics": {"bleu_score": 5.6578916063256015, "chrf_score": 5.63503140265178, "xcomet_score": 0.38474252820014954, "xcomet_qe_score": 0.17471569776535034, "metricx_score": 6.370087623596191, "metricx_qe_score": 3.6876766681671143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抱歉,并在会后与我们讨论。", "metrics": {"bleu_score": 3.7644257151903666, "chrf_score": 3.6231884057971016, "xcomet_score": 0.14283809065818787, "xcomet_qe_score": 0.15752260386943817, "metricx_score": 4.611871719360352, "metricx_qe_score": 4.079080104827881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是华盛顿大学的博士生香斌。", "metrics": {"bleu_score": 63.624138156344834, "chrf_score": 43.34960499938349, "xcomet_score": 0.8240736722946167, "xcomet_qe_score": 0.8706904649734497, "metricx_score": 1.1213643550872803, "metricx_qe_score": 0.5523743629455566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的研究,从预训练数据到语言模型再到下游任务,追踪导致不公平自然语言处理模型的政治偏见的轨迹。", "metrics": {"bleu_score": 64.99862400782325, "chrf_score": 61.33496007839747, "xcomet_score": 0.9454855918884277, "xcomet_qe_score": 0.8322603702545166, "metricx_score": 1.277576208114624, "metricx_qe_score": 1.7907639741897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模网络抓取数据上进行训练的。", "metrics": {"bleu_score": 56.30127787148428, "chrf_score": 49.75325740310139, "xcomet_score": 0.9898977279663086, "xcomet_qe_score": 0.9973485469818115, "metricx_score": 1.5176916122436523, "metricx_qe_score": 1.721698522567749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中覆盖得非常好。", "metrics": {"bleu_score": 53.869332652633126, "chrf_score": 47.076377853973916, "xcomet_score": 0.8525817394256592, "xcomet_qe_score": 0.8098437786102295, "metricx_score": 2.772179126739502, "metricx_qe_score": 3.132279634475708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C四语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等都包含在语言模型的训练数据中。", "metrics": {"bleu_score": 70.72740248032682, "chrf_score": 65.55020880187386, "xcomet_score": 0.8152552843093872, "xcomet_qe_score": 0.8028519153594971, "metricx_score": 2.0592596530914307, "metricx_qe_score": 2.0991263389587402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了喜忧参半的结果。 因此,", "metrics": {"bleu_score": 62.88584826842568, "chrf_score": 68.62293006561504, "xcomet_score": 0.784228503704071, "xcomet_qe_score": 0.7592571377754211, "metricx_score": 2.3589351177215576, "metricx_qe_score": 1.3312727212905884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,他们能够从多样的视角中学习,这庆祝了民主和思想的多元性。", "metrics": {"bleu_score": 26.34592129280123, "chrf_score": 22.50418785422957, "xcomet_score": 0.8640990257263184, "xcomet_qe_score": 0.7724453210830688, "metricx_score": 1.9477674961090088, "metricx_qe_score": 2.249070644378662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,可能在下游任务应用中导致潜在的公平问题。", "metrics": {"bleu_score": 69.50494883456494, "chrf_score": 62.383744392705, "xcomet_score": 0.9908058643341064, "xcomet_qe_score": 0.9736735820770264, "metricx_score": 0.9772740602493286, "metricx_qe_score": 1.200330376625061, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体通过提出以下几个问题进行探索。 首先,我们如何评估语言模型的政治领导力,以及预训练数据对这些政治偏见可能起到什么作用?", "metrics": {"bleu_score": 62.64122795595995, "chrf_score": 59.69038926442677, "xcomet_score": 0.8124395608901978, "xcomet_qe_score": 0.818795382976532, "metricx_score": 3.415018320083618, "metricx_qe_score": 3.598236083984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治单位的语言模型在下游任务中的实际表现如何,以及这是否会导致NLP应用中的公平性问题?", "metrics": {"bleu_score": 80.07722697093931, "chrf_score": 75.96539169167512, "xcomet_score": 0.8343451023101807, "xcomet_qe_score": 0.795851469039917, "metricx_score": 2.8995485305786133, "metricx_qe_score": 2.7528700828552246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体而言,我们首先提议使用政治问卷,如政治倾向测试,以不同的提示格式来引导语言模型。这", "metrics": {"bleu_score": 38.292205673311, "chrf_score": 30.910114970614664, "xcomet_score": 0.7125301361083984, "xcomet_qe_score": 0.7255245447158813, "metricx_score": 4.897226333618164, "metricx_qe_score": 1.7198563814163208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "能确保我们在政治科学文献的基础上进行自动化评估。", "metrics": {"bleu_score": 22.723852669333628, "chrf_score": 24.56410234438585, "xcomet_score": 0.9126527309417725, "xcomet_qe_score": 0.859961748123169, "metricx_score": 1.6408597230911255, "metricx_qe_score": 1.9669616222381592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,母语模型确实具有不同的政治含义。", "metrics": {"bleu_score": 52.761514363426436, "chrf_score": 44.29437741624146, "xcomet_score": 0.9082080721855164, "xcomet_qe_score": 0.9716817140579224, "metricx_score": 2.5761444568634033, "metricx_qe_score": 2.337737798690796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘上的所有四个象限。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 62.83591000502765, "xcomet_score": 0.8533560037612915, "xcomet_qe_score": 0.7687404155731201, "metricx_score": 1.9299724102020264, "metricx_qe_score": 2.1211085319519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到,GPT 4 是所有语言模型中最自由的,而GPT系列在社会自由度上普遍高于BERT系列及其变体。", "metrics": {"bleu_score": 53.78421755422325, "chrf_score": 51.10542191997056, "xcomet_score": 0.886534571647644, "xcomet_qe_score": 0.789496123790741, "metricx_score": 1.7568438053131104, "metricx_qe_score": 1.9947819709777832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在探究语言模型的政治偏见在多大程度上实际上是从训练数据中吸取的。", "metrics": {"bleu_score": 56.403320244940815, "chrf_score": 48.494899137551315, "xcomet_score": 0.9318479299545288, "xcomet_qe_score": 0.9260696172714233, "metricx_score": 1.0495109558105469, "metricx_qe_score": 1.4011412858963013, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过在六个不同的政党语料库上进一步预训练语言模型检查点来进行控制实验,这些语料库分为新闻和社会媒体,并根据其政治含义进一步细分。", "metrics": {"bleu_score": 39.17034243492488, "chrf_score": 33.04419341294042, "xcomet_score": 0.9177168607711792, "xcomet_qe_score": 0.826330304145813, "metricx_score": 2.3756487369537354, "metricx_qe_score": 2.4191513061523438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在语料库中对这些部分进行进一步的预训练,我们可以看到语言模型的意识形态坐标也相应地发生了转移。", "metrics": {"bleu_score": 63.33177832467781, "chrf_score": 60.04584248134128, "xcomet_score": 0.8823260068893433, "xcomet_qe_score": 0.8916764259338379, "metricx_score": 2.555976629257202, "metricx_qe_score": 2.87283992767334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于罗伯塔,进一步微调,在倾向左派的Reddit语料库上进行进一步训练,我们可以看到其在术语上的显著自由派倾向... 在政治偏见方面。", "metrics": {"bleu_score": 34.02694739037202, "chrf_score": 38.79004283981893, "xcomet_score": 0.5770615339279175, "xcomet_qe_score": 0.5737385153770447, "metricx_score": 6.521639823913574, "metricx_qe_score": 6.007617950439453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还尝试探讨语言模型是否能捕捉到现代社会中普遍存在的极化现象。", "metrics": {"bleu_score": 66.15065369281245, "chrf_score": 58.50635462630006, "xcomet_score": 0.9009986519813538, "xcomet_qe_score": 0.9832556247711182, "metricx_score": 0.6275016665458679, "metricx_qe_score": 0.784520149230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第45任总统之前和之后两个部分,然后", "metrics": {"bleu_score": 66.15466616370077, "chrf_score": 64.76309502801357, "xcomet_score": 0.6494697332382202, "xcomet_qe_score": 0.7123317718505859, "metricx_score": 3.9457807540893555, "metricx_qe_score": 1.5960078239440918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别对两个不同时间段的语料库进行预训练,以训练语言模型。 我们", "metrics": {"bleu_score": 38.76335436058364, "chrf_score": 39.1101778611152, "xcomet_score": 0.7420231103897095, "xcomet_qe_score": 0.6479882001876831, "metricx_score": 4.953455448150635, "metricx_qe_score": 1.3008419275283813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年后普遍表现出更偏离中心的政治倾向。", "metrics": {"bleu_score": 46.95966835778608, "chrf_score": 41.7428879432958, "xcomet_score": 0.9854961037635803, "xcomet_qe_score": 0.9901463985443115, "metricx_score": 1.081101655960083, "metricx_qe_score": 1.5196943283081055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能捕捉到社会中的极化现象。", "metrics": {"bleu_score": 41.0155947154624, "chrf_score": 36.59358799422811, "xcomet_score": 0.9972637891769409, "xcomet_qe_score": 0.997498631477356, "metricx_score": 0.7880501747131348, "metricx_qe_score": 1.0712693929672241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们评估具有不同政治含义的语言模型在仇恨言论检测和假新闻检测中的表现。这些自然语言处理(NLP)应用通常涉及语言模型,并且可能具有非常重要的影响。 因此,", "metrics": {"bleu_score": 52.236461808553415, "chrf_score": 55.05986566678186, "xcomet_score": 0.7145067453384399, "xcomet_qe_score": 0.7492837905883789, "metricx_score": 3.4227867126464844, "metricx_qe_score": 1.581347942352295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到如果我们调查每个类别的性能,也就是说,如果我们将性能分开,我们可以发现... (Note: The last part \"我们可以发现...\" is added to complete the sentence structure in Chinese, as it naturally flows with the context.) 在不同的人口统计学或政治意义的新闻媒体中,我们可以看到一种模式,", "metrics": {"bleu_score": 29.225922074313708, "chrf_score": 29.310229091655103, "xcomet_score": 0.49561986327171326, "xcomet_qe_score": 0.46823540329933167, "metricx_score": 12.107770919799805, "metricx_qe_score": 10.895400047302246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测方面,左翼语言模型表现更好。 在识别针对社会少数群体的仇恨言论方面,表现出色。 然而,我们在检测针对社会中更强大群体的仇恨言论方面的工作存在不足。", "metrics": {"bleu_score": 49.2898945973171, "chrf_score": 49.281255789601055, "xcomet_score": 0.8838819265365601, "xcomet_qe_score": 0.8808736801147461, "metricx_score": 3.623460054397583, "metricx_qe_score": 3.4933712482452393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,倾向于右派的语言模型在检测针对白人男性的仇恨言论方面表现较好,但检测针对黑人、LGBTQ+群体和其他少数族裔社区的仇恨言论时则表现较差。", "metrics": {"bleu_score": 51.232062670768414, "chrf_score": 52.33512623992398, "xcomet_score": 0.9824860095977783, "xcomet_qe_score": 0.9753721952438354, "metricx_score": 0.6912156343460083, "metricx_qe_score": 0.6536122560501099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在假新闻检测方面也出现了类似的趋势,我们发现左倾语言模型更擅长检测来自相反政治立场的错误信息,反之亦然。 这", "metrics": {"bleu_score": 55.531903404783286, "chrf_score": 46.11418861119889, "xcomet_score": 0.8328440189361572, "xcomet_qe_score": 0.8867602348327637, "metricx_score": 4.465569972991943, "metricx_qe_score": 1.0232561826705933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将进一步展示许多定性例子,以看到具有不同政治含义的语言模型。 确实,基于其社会类别,对仇恨言论和虚假信息示例应该给出不同的预测。", "metrics": {"bleu_score": 49.639351583120444, "chrf_score": 40.36128481667517, "xcomet_score": 0.6316532492637634, "xcomet_qe_score": 0.6400861144065857, "metricx_score": 3.9001896381378174, "metricx_qe_score": 4.120866775512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中提供了更多示例,以进一步强调这一点。 这表明语言模型的政治偏见存在一个非常紧迫的公平问题。", "metrics": {"bleu_score": 53.008723846287886, "chrf_score": 45.31000107442033, "xcomet_score": 0.8620578050613403, "xcomet_qe_score": 0.8337752819061279, "metricx_score": 1.7581589221954346, "metricx_qe_score": 2.1903319358825684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个基于右线语言的模型被微调用于识别仇恨言论、虚假信息等,并部署在流行的社交媒体平台上。 这意味着持有不同政治观点的人可能会被边缘化,针对少数族裔的仇恨言论也可能不受控制地蔓延。", "metrics": {"bleu_score": 37.21795965303249, "chrf_score": 31.833241592378343, "xcomet_score": 0.9254465103149414, "xcomet_qe_score": 0.8768807649612427, "metricx_score": 1.8573510646820068, "metricx_qe_score": 1.3832677602767944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,需要认识到并解决语言模型政治哗众取宠所导致的公平问题。", "metrics": {"bleu_score": 32.014235765169715, "chrf_score": 34.64052474360507, "xcomet_score": 0.902569591999054, "xcomet_qe_score": 0.9050414562225342, "metricx_score": 1.5397591590881348, "metricx_qe_score": 1.5757412910461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,让我们稍微讨论一下。", "metrics": {"bleu_score": 33.260249505555045, "chrf_score": 32.42349794467388, "xcomet_score": 0.819378137588501, "xcomet_qe_score": 0.8147748708724976, "metricx_score": 0.8959165811538696, "metricx_qe_score": 1.1693971157073975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也想强调的是,我们揭示了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 66.29249918288397, "chrf_score": 58.61170845161696, "xcomet_score": 0.9539639949798584, "xcomet_qe_score": 0.8910531401634216, "metricx_score": 0.9006887674331665, "metricx_qe_score": 1.4504969120025635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像西拉(Sila)和克普提迪斯(Kryptidis)之间的关系。", "metrics": {"bleu_score": 12.143904738370463, "chrf_score": 11.77369320268128, "xcomet_score": 0.5359547138214111, "xcomet_qe_score": 0.6169105768203735, "metricx_score": 6.302635192871094, "metricx_qe_score": 5.446105480194092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们不在语言模型训练数据中消毒政治观点,偏见将从预训练数据传播到语言模型,再到下游任务,最终产生公平性问题。", "metrics": {"bleu_score": 62.87257474872524, "chrf_score": 54.133344973946016, "xcomet_score": 0.8761224746704102, "xcomet_qe_score": 0.842822790145874, "metricx_score": 2.9423375129699707, "metricx_qe_score": 2.854607582092285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试以某种方式进行“消毒”,我们也可能会面临审查或排斥的风险,", "metrics": {"bleu_score": 52.414847252143154, "chrf_score": 53.165305847571275, "xcomet_score": 0.8299710750579834, "xcomet_qe_score": 0.7545069456100464, "metricx_score": 2.0724432468414307, "metricx_qe_score": 3.0009524822235107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定哪些内容实际上是中性的,应该保留在语言模型训练数据中。", "metrics": {"bleu_score": 18.01560488308935, "chrf_score": 17.35671595293674, "xcomet_score": 0.9687241315841675, "xcomet_qe_score": 0.9694175720214844, "metricx_score": 1.7967625856399536, "metricx_qe_score": 1.8435908555984497, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有些像“电击查理”问题。", "metrics": {"bleu_score": 9.669265690880861, "chrf_score": 11.977154576238851, "xcomet_score": 0.7954728603363037, "xcomet_qe_score": 0.6749351024627686, "metricx_score": 3.4091577529907227, "metricx_qe_score": 3.2270913124084473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 0.9909268617630005, "xcomet_qe_score": 0.973970890045166, "metricx_score": 0.3818603754043579, "metricx_qe_score": 0.3029481768608093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。我今天要讲的就这些了。", "metrics": {"bleu_score": 11.251329738544614, "chrf_score": 11.93360000123101, "xcomet_score": 0.5228948593139648, "xcomet_qe_score": 0.5084100961685181, "metricx_score": 0.6945151090621948, "metricx_qe_score": 0.7228642702102661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6542587280273438, "xcomet_qe_score": 0.8413603901863098, "metricx_score": 0.8776271939277649, "metricx_qe_score": 1.047717809677124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基·梅隆大学一年级博士生,今天我将向大家介绍你们的作品《恩奥尔位置论:β模型集合中设计偏差的特征》。", "metrics": {"bleu_score": 28.040412120500648, "chrf_score": 20.570956378471184, "xcomet_score": 0.6321322917938232, "xcomet_qe_score": 0.746100902557373, "metricx_score": 3.615225076675415, "metricx_qe_score": 3.5955073833465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的,他们分别是塞巴斯蒂安·桑蒂(Sebastian Santi)、罗宁·勒布拉(Ronin Lebras)、卡塔琳娜·雷尼克(Katarina Reinicke)和马丁·萨普(Martin Sapp)。", "metrics": {"bleu_score": 27.99635836068743, "chrf_score": 47.60438188629713, "xcomet_score": 0.9317554235458374, "xcomet_qe_score": 0.9287149906158447, "metricx_score": 1.3221310377120972, "metricx_qe_score": 0.8976934552192688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们从一个场景开始想象:你为一家报纸工作,正在筛选新闻文章下的评论,试图删除有毒内容。 ", "metrics": {"bleu_score": 44.409176637184956, "chrf_score": 40.14281906063618, "xcomet_score": 0.8915011882781982, "xcomet_qe_score": 0.899921715259552, "metricx_score": 1.88380765914917, "metricx_qe_score": 1.529209852218628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能轉向像Perspective API這樣的熱門API進行有毒內容檢測。如果您是卡爾·瓊斯,這確實效果很好,因為", "metrics": {"bleu_score": 4.965096889277056, "chrf_score": 20.236424668491797, "xcomet_score": 0.6492310166358948, "xcomet_qe_score": 0.7042550444602966, "metricx_score": 5.190298080444336, "metricx_qe_score": 3.792555332183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能夠正確地檢測出有毒的實例。", "metrics": {"bleu_score": 6.150343144231885, "chrf_score": 42.59536226172686, "xcomet_score": 0.7576387524604797, "xcomet_qe_score": 0.778404951095581, "metricx_score": 5.548945903778076, "metricx_qe_score": 5.547269344329834, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对Dithyasharma来说,情况并非如此。", "metrics": {"bleu_score": 48.521510296503514, "chrf_score": 45.54598270034277, "xcomet_score": 0.8750131130218506, "xcomet_qe_score": 0.8566681146621704, "metricx_score": 2.3787319660186768, "metricx_qe_score": 2.508293867111206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它的视角API对在印度语境中更常见的冒犯性术语并不敏感。 (注:由于中文语境下,\"API\" 和 \"Dithyasharma\" 通常会保持英文原样,但为了保持语句流畅,这里也提供了对应的中文翻译。具体使用时,可根据实际情况决定是否保留英文。)", "metrics": {"bleu_score": 14.512984928370082, "chrf_score": 29.86827987249583, "xcomet_score": 0.2779225707054138, "xcomet_qe_score": 0.2481774389743805, "metricx_score": 8.425698280334473, "metricx_qe_score": 7.95229434967041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏差的例子,我们可以在不同人群中观察到技术表现的系统性差异。", "metrics": {"bleu_score": 41.78383638882466, "chrf_score": 35.777579444745314, "xcomet_score": 0.9778684377670288, "xcomet_qe_score": 0.9615130424499512, "metricx_score": 0.6782376766204834, "metricx_qe_score": 0.8978255987167358, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们刚才看到的设计偏见可能由于自然语言处理(NLP)研究人员和模型开发者的立场而发生。", "metrics": {"bleu_score": 55.06220353187106, "chrf_score": 46.87136141731029, "xcomet_score": 0.9771028757095337, "xcomet_qe_score": 0.9295427203178406, "metricx_score": 0.756725013256073, "metricx_qe_score": 0.7928307056427002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立场性是指人们由于其人口统计、身份和生活经历而持有的观点。 (Pinyin: Wǒmen gāngcǐ kàndào de shèjì piānjù kěnéng yóuyú zìrán yǔyán chǔlǐ (NLP) yánjiū rényuán hé móxíng kāifāzhě de lìchǎng ér fāshēng. Lìchǎngxìng shì zhǐ rénmen yóuyú tāmen de rénkǒu tǒngjì, shēnfen hé shēnghuó jīnglì ér chíjù de guāndiǎn.)", "metrics": {"bleu_score": 23.862656199502375, "chrf_score": 23.475153791005315, "xcomet_score": 0.28828054666519165, "xcomet_qe_score": 0.2775344252586365, "metricx_score": 7.399809837341309, "metricx_qe_score": 10.826489448547363, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 73.97378912627735, "chrf_score": 67.41898597045686, "xcomet_score": 0.9927648305892944, "xcomet_qe_score": 0.908970832824707, "metricx_score": 0.9186691045761108, "metricx_qe_score": 1.4367049932479858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为一名研究人员,立场性会影响研究过程及其结果和结论,因为它会改变研究人员所做的决定。", "metrics": {"bleu_score": 51.77290181508159, "chrf_score": 43.66292246826325, "xcomet_score": 0.9927520751953125, "xcomet_qe_score": 0.9865365028381348, "metricx_score": 1.0159739255905151, "metricx_qe_score": 1.0582505464553833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问的一个问题是,数据集和模型是否具有位置性?", "metrics": {"bleu_score": 48.465254338121596, "chrf_score": 45.20744241143061, "xcomet_score": 0.9149991869926453, "xcomet_qe_score": 0.9874231815338135, "metricx_score": 2.477600574493408, "metricx_qe_score": 0.7690516710281372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并非试图声称模型本身和数据集本身具有人口统计学身份和生活经历,但它们确实汇集了真实人士的判断和意见,从而能够代表某些立场,", "metrics": {"bleu_score": 50.166576649085535, "chrf_score": 40.73228081346067, "xcomet_score": 0.7910343408584595, "xcomet_qe_score": 0.8154842257499695, "metricx_score": 1.3832123279571533, "metricx_qe_score": 1.8867344856262207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "超越其他立场。 早期研究表明了一些位置性的轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 31.952472500946065, "chrf_score": 26.739742724373183, "xcomet_score": 0.3252384066581726, "xcomet_qe_score": 0.18018051981925964, "metricx_score": 7.131216526031494, "metricx_qe_score": 6.523223876953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些作品实际上并没有将最终用户与数据集和模型本身进行比较。 随着自然语言处理(NLP)任务变得更加主观和社会化,研究模型和数据集的位置性变得越来越重要。 由于并非所有决策都有记录,且许多模型隐藏在 API 背后,因此很难描述这些位置偏见的具体情况。", "metrics": {"bleu_score": 55.208992919790205, "chrf_score": 50.63048907017587, "xcomet_score": 0.7819778919219971, "xcomet_qe_score": 0.8074170351028442, "metricx_score": 5.091001510620117, "metricx_qe_score": 5.1211957931518555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的定位性,我们实际上将标注与真实用户进行比较,并将其与现有数据集和模型进行对比。", "metrics": {"bleu_score": 43.59183345706364, "chrf_score": 42.66084334871497, "xcomet_score": 0.7413693070411682, "xcomet_qe_score": 0.739128589630127, "metricx_score": 3.443727493286133, "metricx_qe_score": 2.9321603775024414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架,NL位置性(NL Positionality)来实现这一点。", "metrics": {"bleu_score": 21.22363344155403, "chrf_score": 58.57645210390091, "xcomet_score": 0.8194440603256226, "xcomet_qe_score": 0.8169007301330566, "metricx_score": 1.4534590244293213, "metricx_qe_score": 1.9567567110061646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤进行。", "metrics": {"bleu_score": 61.47881529512643, "chrf_score": 62.859360847302014, "xcomet_score": 0.9546252489089966, "xcomet_qe_score": 0.8385137319564819, "metricx_score": 0.2825612425804138, "metricx_qe_score": 0.5525258779525757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的标注人员重新标注数据集。", "metrics": {"bleu_score": 36.857838224116975, "chrf_score": 31.056395663981927, "xcomet_score": 0.7965057492256165, "xcomet_qe_score": 0.7971661686897278, "metricx_score": 4.661585807800293, "metricx_qe_score": 3.25648832321167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是分析原始数据集的人口统计学特征,以及标注者,因为通常只有少数标注者标注每个实例,而且人口统计学数据很少被收集和共享。", "metrics": {"bleu_score": 47.505708153103136, "chrf_score": 40.294169910295835, "xcomet_score": 0.7399048805236816, "xcomet_qe_score": 0.7055586576461792, "metricx_score": 2.261106491088867, "metricx_qe_score": 1.569413661956787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,以获得每个实例的多重标注和丰富的人口统计数据集。", "metrics": {"bleu_score": 44.504518860748696, "chrf_score": 39.15394536911565, "xcomet_score": 0.8512866497039795, "xcomet_qe_score": 0.8400838375091553, "metricx_score": 2.0244104862213135, "metricx_qe_score": 2.2050209045410156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后根据人口统计学特征对注释进行分类,并使用帕森斯R相关性得分将它们与模型和数据集进行比较。 因此,我们的框架实际上与注释器分歧文献不同,通过将最终用户与模型和数据集、预测和标签进行比较,而不是仅仅关注注释器一致性或建模注释器分布,从而实现了区别。", "metrics": {"bleu_score": 51.29653006937522, "chrf_score": 50.13181244946758, "xcomet_score": 0.5331069231033325, "xcomet_qe_score": 0.5377055406570435, "metricx_score": 5.730504035949707, "metricx_qe_score": 4.199504375457764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要通过野生实验室(Lab in the Wild)这一在线众包平台实现,该平台为我们的人机交互(HCI)合作者提供支持。", "metrics": {"bleu_score": 34.85129327636028, "chrf_score": 58.151632612906255, "xcomet_score": 0.8131850957870483, "xcomet_qe_score": 0.804862916469574, "metricx_score": 2.312406063079834, "metricx_qe_score": 2.2729055881500244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,我们可以在该平台上招募来自不同背景的志愿者,", "metrics": {"bleu_score": 48.567710931378706, "chrf_score": 62.14000116225339, "xcomet_score": 0.9287595748901367, "xcomet_qe_score": 0.7507655620574951, "metricx_score": 1.7586643695831299, "metricx_qe_score": 3.6736767292022705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与 MTurk 等主要吸引美国或印度参与者的平台相比,更加多样化。此外,Lab in the Wild 仍然能够收集到高质量的数据。", "metrics": {"bleu_score": 37.47159466471725, "chrf_score": 46.76939851719366, "xcomet_score": 0.7880704998970032, "xcomet_qe_score": 0.7669748067855835, "metricx_score": 2.6005866527557373, "metricx_qe_score": 2.8956217765808105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“野外实验室”(Lab in the Wild)平台上举办了两个任务,其中之一是社交可接受性评估。具体操作方式是,参与者们会从社交化学数据集中阅读一种社交情境,然后写下他们对该情境社交可接受性的评价。", "metrics": {"bleu_score": 21.690198421743073, "chrf_score": 30.664243278719432, "xcomet_score": 0.8177788257598877, "xcomet_qe_score": 0.7749444246292114, "metricx_score": 1.6115474700927734, "metricx_qe_score": 1.7076762914657593, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对研究的参与度,他们可以将自己的回答与人工智能和其他人的回答进行比较。", "metrics": {"bleu_score": 56.96539648564389, "chrf_score": 54.80421145068135, "xcomet_score": 0.9905778169631958, "xcomet_qe_score": 0.9937238693237305, "metricx_score": 0.6882297992706299, "metricx_qe_score": 0.6028512716293335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些标注与社会化学、德尔菲法和GPT 4进行了比较。", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 42.76013396694201, "xcomet_score": 0.7465426921844482, "xcomet_qe_score": 0.7511246204376221, "metricx_score": 2.002897262573242, "metricx_qe_score": 2.392756223678589, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后为毒性和仇恨言论检测任务复制了一个非常相似的设置,他们将从《达娜仇恨》中读取一个实例,并写下他们是否认为它是仇恨言论的实例。", "metrics": {"bleu_score": 56.935418068662415, "chrf_score": 48.69197911978256, "xcomet_score": 0.667050838470459, "xcomet_qe_score": 0.6658167839050293, "metricx_score": 4.818408489227295, "metricx_qe_score": 4.810977935791016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后将这些标注与DynaHate、Perspective API、Rewire API、HateRoberta和GPT四种模型进行了比较。", "metrics": {"bleu_score": 42.288233063304034, "chrf_score": 75.77109695359916, "xcomet_score": 0.8962149620056152, "xcomet_qe_score": 0.905624508857727, "metricx_score": 2.9744060039520264, "metricx_qe_score": 2.5745625495910645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了来自八十七个国家的千余名标注者的超过一万六千个标注。", "metrics": {"bleu_score": 40.91032653880248, "chrf_score": 33.80477627326716, "xcomet_score": 0.9351871013641357, "xcomet_qe_score": 0.9771336317062378, "metricx_score": 2.4932589530944824, "metricx_qe_score": 1.3384222984313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们更有能力回答自然语言处理(NLP)数据集和模型最倾向于与谁对齐的问题。", "metrics": {"bleu_score": 28.825790406664787, "chrf_score": 30.73062793885017, "xcomet_score": 0.8812474012374878, "xcomet_qe_score": 0.8778453469276428, "metricx_score": 1.8179465532302856, "metricx_qe_score": 1.1011186838150024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP中存在位置性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8341923952102661, "xcomet_qe_score": 0.8352224826812744, "metricx_score": 3.7918624877929688, "metricx_qe_score": 2.050662040710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型与英语国家最紧密相关。", "metrics": {"bleu_score": 49.35451179213381, "chrf_score": 43.57686954283306, "xcomet_score": 0.9838119745254517, "xcomet_qe_score": 0.9733604192733765, "metricx_score": 1.0090453624725342, "metricx_qe_score": 1.158062219619751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在《GPD 4 社会可接受性分析》中,我们发现它与儒家文化国家和英语国家最紧密相关。我们", "metrics": {"bleu_score": 46.297567830642386, "chrf_score": 50.903913051882576, "xcomet_score": 0.6502811312675476, "xcomet_qe_score": 0.6233332753181458, "metricx_score": 4.898409843444824, "metricx_qe_score": 2.609337329864502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现《炸药仇恨》(Dynamite Hate)也与英语国家最紧密相关。", "metrics": {"bleu_score": 12.409597120849801, "chrf_score": 21.011561395214464, "xcomet_score": 0.7658559679985046, "xcomet_qe_score": 0.7752314805984497, "metricx_score": 4.092801570892334, "metricx_qe_score": 3.844857692718506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,大多数额外的观点一致性出现在受过大学教育的人群中。因此,", "metrics": {"bleu_score": 26.633048164380032, "chrf_score": 25.185772628320148, "xcomet_score": 0.6929713487625122, "xcomet_qe_score": 0.7742723822593689, "metricx_score": 5.288822650909424, "metricx_qe_score": 2.96797513961792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 GPT 4 的社会可接受性任务中,我们发现它与受过大学或研究生教育的人群最一致。 我们在分析Dani Hate时发现类似的结果,它最受拥有大学教育的人群欢迎。", "metrics": {"bleu_score": 42.418737477607245, "chrf_score": 35.33531528068367, "xcomet_score": 0.6095097064971924, "xcomet_qe_score": 0.5439205169677734, "metricx_score": 5.019664287567139, "metricx_qe_score": 5.45068359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群相匹配时,难免会有一些人被遗漏。 一个例子", "metrics": {"bleu_score": 43.759806980485955, "chrf_score": 39.69155754022889, "xcomet_score": 0.8227763175964355, "xcomet_qe_score": 0.6378322839736938, "metricx_score": 1.7733631134033203, "metricx_qe_score": 1.4197241067886353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,数据集和模型对非二元性别人士的适应性不如对男性和女性的适应性。", "metrics": {"bleu_score": 30.374899709317607, "chrf_score": 26.754451416048973, "xcomet_score": 0.7645778656005859, "xcomet_qe_score": 0.7716758251190186, "metricx_score": 5.582699298858643, "metricx_qe_score": 5.186079502105713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT 4社会可接受性任务和Dynahate任务分析中都发现了这一点。", "metrics": {"bleu_score": 77.38837367508762, "chrf_score": 77.38173835239682, "xcomet_score": 0.8507707118988037, "xcomet_qe_score": 0.8315064907073975, "metricx_score": 1.4189231395721436, "metricx_qe_score": 2.21262526512146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,既然存在位置分析线 LP,我们能做些什么呢?", "metrics": {"bleu_score": 37.49405143204498, "chrf_score": 34.81445626301731, "xcomet_score": 0.7972369194030762, "xcomet_qe_score": 0.8214442729949951, "metricx_score": 6.704990386962891, "metricx_qe_score": 7.033833026885986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们对此有几点建议。", "metrics": {"bleu_score": 17.474335703431752, "chrf_score": 19.879610409139048, "xcomet_score": 0.9887105226516724, "xcomet_qe_score": 0.9731390476226807, "metricx_score": 0.12925037741661072, "metricx_qe_score": 0.13402260839939117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一,在整个研究过程中,记录所有相关的设计选择。另一个建议", "metrics": {"bleu_score": 44.867276656754186, "chrf_score": 41.87496896759585, "xcomet_score": 0.7646283507347107, "xcomet_qe_score": 0.7150188684463501, "metricx_score": 3.851541519165039, "metricx_qe_score": 1.6394009590148926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,从观点主义的角度进行自然语言处理(NLP)研究。", "metrics": {"bleu_score": 44.18463817237388, "chrf_score": 43.78186054161846, "xcomet_score": 0.8703243732452393, "xcomet_qe_score": 0.8458982110023499, "metricx_score": 1.9522112607955933, "metricx_qe_score": 2.3825271129608154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业的数据集和模型。", "metrics": {"bleu_score": 80.96427216101601, "chrf_score": 72.63771349978248, "xcomet_score": 0.911953866481781, "xcomet_qe_score": 0.8777899146080017, "metricx_score": 0.8077837228775024, "metricx_qe_score": 0.9955554604530334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane计划。", "metrics": {"bleu_score": 50.51968359286048, "chrf_score": 41.302406935674156, "xcomet_score": 0.7493586540222168, "xcomet_qe_score": 0.7979084253311157, "metricx_score": 3.2502565383911133, "metricx_qe_score": 4.110206127166748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调的是,包容性的自然语言处理不仅仅是让所有", "metrics": {"bleu_score": 42.300073736306565, "chrf_score": 37.28752978726753, "xcomet_score": 0.5888855457305908, "xcomet_qe_score": 0.17557473480701447, "metricx_score": 5.114758491516113, "metricx_qe_score": 4.029155254364014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为每个人服务。", "metrics": {"bleu_score": 13.927628237847681, "chrf_score": 14.6579211560759, "xcomet_score": 0.9458763599395752, "xcomet_qe_score": 0.9540870189666748, "metricx_score": 0.7448137402534485, "metricx_qe_score": 1.026949167251587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "至此,我们的演讲告一段落", "metrics": {"bleu_score": 33.18077402843942, "chrf_score": 29.774380593363002, "xcomet_score": 0.9895820617675781, "xcomet_qe_score": 0.9894829988479614, "metricx_score": 0.24008885025978088, "metricx_qe_score": 0.2647431790828705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但如果您想了解更多,欢迎查看我们的仪表板以获取最新分析结果和我们的论文。", "metrics": {"bleu_score": 55.78197861266046, "chrf_score": 50.420613324928865, "xcomet_score": 0.9617302417755127, "xcomet_qe_score": 0.9368815422058105, "metricx_score": 0.9973505735397339, "metricx_qe_score": 0.8595210909843445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自芬奈大学的习远。", "metrics": {"bleu_score": 23.70251900062618, "chrf_score": 15.169382293655653, "xcomet_score": 0.7031084299087524, "xcomet_qe_score": 0.7175445556640625, "metricx_score": 6.1246657371521, "metricx_qe_score": 5.3589091300964355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将在这里介绍我们从行语言模型中区分脚本知识的工作,用于约束语言规划。", "metrics": {"bleu_score": 33.69582032493922, "chrf_score": 27.10723295994388, "xcomet_score": 0.6927464008331299, "xcomet_qe_score": 0.7056385278701782, "metricx_score": 3.620938301086426, "metricx_qe_score": 3.233686923980713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常通过遵循形式为保证脚本的逐步说明来规划他们的行动。", "metrics": {"bleu_score": 21.98050339983991, "chrf_score": 20.492522929667143, "xcomet_score": 0.7799274921417236, "xcomet_qe_score": 0.7465246915817261, "metricx_score": 5.996741771697998, "metricx_qe_score": 5.775826454162598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "早期研究已经探索了语言模型来规划典型活动抽象目标,", "metrics": {"bleu_score": 33.652599084769285, "chrf_score": 28.629661026766907, "xcomet_score": 0.832635760307312, "xcomet_qe_score": 0.7976793050765991, "metricx_score": 3.384669542312622, "metricx_qe_score": 5.679628849029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如制作蛋糕,并证明了大型语言模型能够有效地将目标分解为步骤。", "metrics": {"bleu_score": 38.376797444789126, "chrf_score": 35.37535846519838, "xcomet_score": 0.2844359874725342, "xcomet_qe_score": 0.22580569982528687, "metricx_score": 3.3345131874084473, "metricx_qe_score": 1.7646405696868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以前的研究主要集中在为典型活动的抽象目标制定计划上。", "metrics": {"bleu_score": 55.34502966615894, "chrf_score": 53.46710740180818, "xcomet_score": 0.8462514877319336, "xcomet_qe_score": 0.8822493553161621, "metricx_score": 1.9062577486038208, "metricx_qe_score": 1.708331823348999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具有具体目标和特定约束的目标的规划,例如制作巧克力蛋糕,仍然研究不足。", "metrics": {"bleu_score": 34.297280709487325, "chrf_score": 30.961832280393004, "xcomet_score": 0.7302839756011963, "xcomet_qe_score": 0.7120610475540161, "metricx_score": 1.2151532173156738, "metricx_qe_score": 1.632077693939209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受限语言规划的问题。 对规划目标施加不同的限制。", "metrics": {"bleu_score": 61.130976386183214, "chrf_score": 55.09927393760137, "xcomet_score": 0.8239529132843018, "xcomet_qe_score": 0.7930374145507812, "metricx_score": 2.932188034057617, "metricx_qe_score": 2.966856002807617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象的目标可以被具有多种限制的现实中的具体目标所", "metrics": {"bleu_score": 22.818678107348028, "chrf_score": 18.9233969851482, "xcomet_score": 0.781898021697998, "xcomet_qe_score": 0.7369437217712402, "metricx_score": 6.024670600891113, "metricx_qe_score": 4.555537223815918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "继承。一个好的规划者应该编写合理且忠实于限制的脚本。", "metrics": {"bleu_score": 21.22210320504574, "chrf_score": 21.694531452964082, "xcomet_score": 0.47003698348999023, "xcomet_qe_score": 0.21820944547653198, "metricx_score": 5.444908618927002, "metricx_qe_score": 6.106056213378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并改进大型语言模型的受限语言规划能力。", "metrics": {"bleu_score": 78.8324467631105, "chrf_score": 71.24182139699383, "xcomet_score": 0.9718189239501953, "xcomet_qe_score": 0.9792938232421875, "metricx_score": 0.6621874570846558, "metricx_qe_score": 0.7382932901382446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有具体的目标数据集作为起点,我们需要从无到有地建立一个。 我们首先需要实现这个目标。", "metrics": {"bleu_score": 19.937516727014405, "chrf_score": 21.42921269685661, "xcomet_score": 0.878667950630188, "xcomet_qe_score": 0.8651677370071411, "metricx_score": 4.1084370613098145, "metricx_qe_score": 3.664604902267456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们使用结构化TPT(时间路径表)扩展了人类循环数据采集过程中的抽象目标,并修改了约束条件。", "metrics": {"bleu_score": 21.21671832792111, "chrf_score": 18.148712341390894, "xcomet_score": 0.6720781922340393, "xcomet_qe_score": 0.7172451019287109, "metricx_score": 6.464800834655762, "metricx_qe_score": 6.542346954345703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们抽取100个具体目标,并评估由大规模模型生成的脚本。", "metrics": {"bleu_score": 56.12756148371671, "chrf_score": 50.955322102392195, "xcomet_score": 0.8958901166915894, "xcomet_qe_score": 0.9402986764907837, "metricx_score": 2.1628360748291016, "metricx_qe_score": 2.94785213470459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的整体准确性。", "metrics": {"bleu_score": 64.53174978135057, "chrf_score": 61.32467858545223, "xcomet_score": 0.9459816217422485, "xcomet_qe_score": 0.9768973588943481, "metricx_score": 0.8722731471061707, "metricx_qe_score": 0.9680761098861694, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有线性模型在为特定目标进行规划方面都取得了不尽如人意的结果。", "metrics": {"bleu_score": 28.13584227415993, "chrf_score": 28.200322425033153, "xcomet_score": 0.8936442136764526, "xcomet_qe_score": 0.9010670781135559, "metricx_score": 2.2454676628112793, "metricx_qe_score": 2.693419933319092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析,以探讨学习模块的具体用途。", "metrics": {"bleu_score": 26.24310277292268, "chrf_score": 22.055006587615285, "xcomet_score": 0.7388677000999451, "xcomet_qe_score": 0.7277224659919739, "metricx_score": 3.784987211227417, "metricx_qe_score": 4.7776594161987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中结果显示,生成的脚本在语义完整性方面表现可接受,但无法保证对约束条件的忠实度。", "metrics": {"bleu_score": 42.73726848522243, "chrf_score": 36.97070043611558, "xcomet_score": 0.9964858293533325, "xcomet_qe_score": 0.9866049289703369, "metricx_score": 1.017639398574829, "metricx_qe_score": 1.2860298156738281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入探讨了更多与在家醒来相关的约束条件的分类。", "metrics": {"bleu_score": 39.880891978177814, "chrf_score": 27.19188733655324, "xcomet_score": 0.6478840708732605, "xcomet_qe_score": 0.44590359926223755, "metricx_score": 5.991329193115234, "metricx_qe_score": 6.361635208129883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的思维导图显示,指导性DPDs的规划性能因不同类别的女孩而显著不同。", "metrics": {"bleu_score": 20.69099661161138, "chrf_score": 16.008183566972363, "xcomet_score": 0.4901299476623535, "xcomet_qe_score": 0.48292210698127747, "metricx_score": 8.147520065307617, "metricx_qe_score": 8.639688491821289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前研究表明,轻量级模型的输出质量具有高方差,导致表现不佳。", "metrics": {"bleu_score": 47.11139785755011, "chrf_score": 40.08200968973597, "xcomet_score": 0.8266552686691284, "xcomet_qe_score": 0.8010885119438171, "metricx_score": 3.0470075607299805, "metricx_qe_score": 3.0241150856018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用过生成禅滤波器的概念来提高生成质量。", "metrics": {"bleu_score": 38.1270292412149, "chrf_score": 32.953833474859664, "xcomet_score": 0.811669647693634, "xcomet_qe_score": 0.804003119468689, "metricx_score": 5.773994445800781, "metricx_qe_score": 6.7959065437316895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示用于指导CPT的约束类型及其示例,并根据所述抽象目标得出具体目标。", "metrics": {"bleu_score": 34.362541065676204, "chrf_score": 25.374664089725236, "xcomet_score": 0.781274676322937, "xcomet_qe_score": 0.7932549118995667, "metricx_score": 3.3920419216156006, "metricx_qe_score": 3.789306640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后指导GPT生成针对特定目标的案例脚本。", "metrics": {"bleu_score": 15.936357366603362, "chrf_score": 15.386386093792579, "xcomet_score": 0.7029622793197632, "xcomet_qe_score": 0.725874662399292, "metricx_score": 3.8045413494110107, "metricx_qe_score": 3.791060209274292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤模型来选择合适的脚本。", "metrics": {"bleu_score": 57.4295228338359, "chrf_score": 52.49847494693214, "xcomet_score": 0.9618688821792603, "xcomet_qe_score": 0.9392916560173035, "metricx_score": 0.766460120677948, "metricx_qe_score": 0.9769001007080078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为指令,输入GPT,并计算余弦相似度和相似度分数,以衡量语义相似性。", "metrics": {"bleu_score": 61.702157341844874, "chrf_score": 46.833971118641536, "xcomet_score": 0.826193630695343, "xcomet_qe_score": 0.7774530649185181, "metricx_score": 3.5984013080596924, "metricx_qe_score": 3.5864906311035156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们将编写包含目标约束关键字的脚本。", "metrics": {"bleu_score": 41.37280342230792, "chrf_score": 36.627724141016074, "xcomet_score": 0.7887495756149292, "xcomet_qe_score": 0.7591727375984192, "metricx_score": 6.530531883239746, "metricx_qe_score": 7.062847137451172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "只有当目标 Go 得分在目标站点上最高时,我们才会保留该脚本。", "metrics": {"bleu_score": 43.50301719428051, "chrf_score": 38.71768919959763, "xcomet_score": 0.7579331994056702, "xcomet_qe_score": 0.6300497651100159, "metricx_score": 6.397524833679199, "metricx_qe_score": 7.196649074554443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,不足之处可以产生类似头发质量的螺丝。", "metrics": {"bleu_score": 30.299282065335248, "chrf_score": 22.24757468447841, "xcomet_score": 0.3619358539581299, "xcomet_qe_score": 0.211441308259964, "metricx_score": 9.234331130981445, "metricx_qe_score": 11.905324935913086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实性方面显著提高了可规划性。", "metrics": {"bleu_score": 53.861782208165394, "chrf_score": 49.9074652882935, "xcomet_score": 0.9196041822433472, "xcomet_qe_score": 0.9171645641326904, "metricx_score": 1.3640631437301636, "metricx_qe_score": 1.9470179080963135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型部署成本高昂,因此有必要增强较小的、专业化模型的语言规划能力。", "metrics": {"bleu_score": 47.62590229261002, "chrf_score": 40.62535339639489, "xcomet_score": 0.9978779554367065, "xcomet_qe_score": 0.9865387678146362, "metricx_score": 0.43352073431015015, "metricx_qe_score": 0.5596357583999634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是其最终实现的关键步骤。", "metrics": {"bleu_score": 33.400530952640366, "chrf_score": 28.716745216744314, "xcomet_score": 0.9361671209335327, "xcomet_qe_score": 0.9271187782287598, "metricx_score": 0.6188123822212219, "metricx_qe_score": 0.6646236181259155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究并不能实现针对具体目标的规划,而手动数据集标注成本高昂。", "metrics": {"bleu_score": 47.636735749489404, "chrf_score": 40.208124786632666, "xcomet_score": 0.9924912452697754, "xcomet_qe_score": 0.9773951768875122, "metricx_score": 0.9381422996520996, "metricx_qe_score": 1.462157964706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循象征性知识蒸馏的理念,从大型语言模型中蒸馏受限语言规划数据站点。", "metrics": {"bleu_score": 44.23767989743811, "chrf_score": 36.351206788979354, "xcomet_score": 0.6990852952003479, "xcomet_qe_score": 0.6998621225357056, "metricx_score": 5.74068546295166, "metricx_qe_score": 5.413477897644043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将应用我们的方法来构建一个名为代码脚本的连词语言规划数据集。", "metrics": {"bleu_score": 29.75165028065122, "chrf_score": 23.313856777397312, "xcomet_score": 0.7679347991943359, "xcomet_qe_score": 0.7601419687271118, "metricx_score": 4.321092128753662, "metricx_qe_score": 4.879786491394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了五万五千个具体目标,", "metrics": {"bleu_score": 27.080524311589805, "chrf_score": 19.19281190814866, "xcomet_score": 0.9446789026260376, "xcomet_qe_score": 0.9233257174491882, "metricx_score": 2.115654230117798, "metricx_qe_score": 2.8832287788391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并附有脚本,以确保验证和测试网站的质量。我们要求云众包工人找到并修订不正确的样本。", "metrics": {"bleu_score": 25.81861407586148, "chrf_score": 24.425018099918375, "xcomet_score": 0.23779574036598206, "xcomet_qe_score": 0.20404207706451416, "metricx_score": 6.27338981628418, "metricx_qe_score": 6.303735733032227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图展示了代码脚本的约束分布。", "metrics": {"bleu_score": 37.5022891676693, "chrf_score": 23.913122665200788, "xcomet_score": 0.856412947177887, "xcomet_qe_score": 0.8248428106307983, "metricx_score": 3.078789234161377, "metricx_qe_score": 3.9585700035095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成的特定目标中表现出超丰度。通过代", "metrics": {"bleu_score": 35.955975361320206, "chrf_score": 24.163024491660277, "xcomet_score": 0.451223224401474, "xcomet_qe_score": 0.4490533769130707, "metricx_score": 8.227383613586426, "metricx_qe_score": 7.106805324554443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "码脚本,我们可以追踪更小但更专业的约束语言规划模型。", "metrics": {"bleu_score": 20.26486887264079, "chrf_score": 14.552175823700342, "xcomet_score": 0.4850537180900574, "xcomet_qe_score": 0.4673883020877838, "metricx_score": 8.138710975646973, "metricx_qe_score": 8.278372764587402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用蚂蚁视角(antsights)、TFILF和调整后的光标速率,可以生成比大多数大型语言模型更优质的脚本,这表明较小的模型在适当训练并应用于合适的数据集时,可以支持较大的模型。", "metrics": {"bleu_score": 33.65432740972392, "chrf_score": 27.924144036734223, "xcomet_score": 0.5031430721282959, "xcomet_qe_score": 0.41362032294273376, "metricx_score": 7.881624698638916, "metricx_qe_score": 7.739539623260498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了约束语言规划问题。", "metrics": {"bleu_score": 51.109970380326146, "chrf_score": 42.579016307796216, "xcomet_score": 0.8879767656326294, "xcomet_qe_score": 0.8463901877403259, "metricx_score": 2.0746877193450928, "metricx_qe_score": 3.441852569580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的约束语言规划能力,并为大型语言模型开发了基于生成的过滤方法。", "metrics": {"bleu_score": 46.485483624390234, "chrf_score": 38.542188395714106, "xcomet_score": 0.7843247652053833, "xcomet_qe_score": 0.7742267847061157, "metricx_score": 2.29007887840271, "metricx_qe_score": 2.4426703453063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成高质量的脚本数据集,用于约束语言规划。", "metrics": {"bleu_score": 61.03988734332589, "chrf_score": 42.50672947153218, "xcomet_score": 0.9081760048866272, "xcomet_qe_score": 0.8781784176826477, "metricx_score": 2.6895062923431396, "metricx_qe_score": 3.432945966720581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望代码数据集能成为推进语言规划研究的宝贵资源。", "metrics": {"bleu_score": 85.31413606256206, "chrf_score": 67.29891931900274, "xcomet_score": 0.8899258375167847, "xcomet_qe_score": 0.9034297466278076, "metricx_score": 2.8155109882354736, "metricx_qe_score": 3.391599178314209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查找更多关于代码脚本的详细信息。", "metrics": {"bleu_score": 48.0621629821948, "chrf_score": 35.281172892920694, "xcomet_score": 0.8338078260421753, "xcomet_qe_score": 0.8262996077537537, "metricx_score": 2.8399760723114014, "metricx_qe_score": 3.0248165130615234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是舒恒。", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 10.704692891649412, "xcomet_score": 0.8477283120155334, "xcomet_qe_score": 0.821589469909668, "metricx_score": 0.2466074526309967, "metricx_qe_score": 0.61963951587677, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将展示我们论文《2003年内核命名实体识别器在2023年是否仍表现良好?》的研究成果", "metrics": {"bleu_score": 31.646682536353488, "chrf_score": 34.263716909970206, "xcomet_score": 0.7716937065124512, "xcomet_qe_score": 0.8244628310203552, "metricx_score": 5.606573104858398, "metricx_qe_score": 5.781985282897949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。让我们开始吧。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 95.15349630471859, "xcomet_score": 0.9835532903671265, "xcomet_qe_score": 0.9856215715408325, "metricx_score": 0.7622692584991455, "metricx_qe_score": 1.1052849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究论文探讨了泛化问题,并使用命名实体识别任务(NER任务)进行研究。", "metrics": {"bleu_score": 31.150855391144304, "chrf_score": 33.49210890241843, "xcomet_score": 0.9491919279098511, "xcomet_qe_score": 0.938023567199707, "metricx_score": 1.807790994644165, "metricx_qe_score": 3.034310817718506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到模型已经使用Kono两千三来开发命名实体识别(NER)将近二十年。这自然地引发了几个问题。", "metrics": {"bleu_score": 17.98822147074585, "chrf_score": 17.895047664287873, "xcomet_score": 0.5978280305862427, "xcomet_qe_score": 0.6265696883201599, "metricx_score": 8.23809814453125, "metricx_qe_score": 8.213212013244629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否推广到现代数据?", "metrics": {"bleu_score": 53.12583871630397, "chrf_score": 42.12696617108382, "xcomet_score": 0.9173398017883301, "xcomet_qe_score": 0.9163376688957214, "metricx_score": 0.46578866243362427, "metricx_qe_score": 0.3853972554206848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新的标记器时,良好的泛化能力需要什么?", "metrics": {"bleu_score": 36.712753452897154, "chrf_score": 28.57000344168954, "xcomet_score": 0.7658227682113647, "xcomet_qe_score": 0.8603861331939697, "metricx_score": 0.6277921199798584, "metricx_qe_score": 0.6687824726104736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们观察到泛化能力差,是什么原因导致这些模型的性能下降?", "metrics": {"bleu_score": 33.59696856755292, "chrf_score": 27.646830580239534, "xcomet_score": 0.9905529022216797, "xcomet_qe_score": 0.9781142473220825, "metricx_score": 0.933985710144043, "metricx_qe_score": 0.7916854023933411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了Kono增强数据集。这是", "metrics": {"bleu_score": 38.398171330793495, "chrf_score": 26.975687611462956, "xcomet_score": 0.6603942513465881, "xcomet_qe_score": 0.705261766910553, "metricx_score": 7.507772445678711, "metricx_qe_score": 4.922139644622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透社新闻中收集的数据集,然后根据相同的Kono 2003标注指南对其进行标注。", "metrics": {"bleu_score": 46.83863794505971, "chrf_score": 36.045873469796035, "xcomet_score": 0.6629899740219116, "xcomet_qe_score": 0.6701865196228027, "metricx_score": 6.419687747955322, "metricx_qe_score": 6.132256031036377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在Kono 2003上对二十多个模型进行了微调。", "metrics": {"bleu_score": 48.676436918044544, "chrf_score": 40.79215093340203, "xcomet_score": 0.7077853679656982, "xcomet_qe_score": 0.7824264168739319, "metricx_score": 4.6804046630859375, "metricx_qe_score": 3.8040404319763184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Kono 3测试集和Kono Plus测试集上对它们进行了评估。", "metrics": {"bleu_score": 53.5327611100933, "chrf_score": 37.32515083790568, "xcomet_score": 0.5633430480957031, "xcomet_qe_score": 0.49692150950431824, "metricx_score": 5.2470855712890625, "metricx_qe_score": 4.255115509033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们计算了F值的变动百分比,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 39.792854779097446, "chrf_score": 45.178840575547234, "xcomet_score": 0.9853805303573608, "xcomet_qe_score": 0.9838237762451172, "metricx_score": 1.4045941829681396, "metricx_qe_score": 1.5718258619308472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的概括需要什么?", "metrics": {"bleu_score": 22.915026372483176, "chrf_score": 20.623051599291358, "xcomet_score": 0.920441746711731, "xcomet_qe_score": 0.9147618412971497, "metricx_score": 1.5199949741363525, "metricx_qe_score": 0.6356233358383179, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必不可少的。", "metrics": {"bleu_score": 21.62050865049026, "chrf_score": 20.47444138723015, "xcomet_score": 0.9961169958114624, "xcomet_qe_score": 1.0, "metricx_score": 0.380794495344162, "metricx_qe_score": 0.7221799492835999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现变压器模型通常对新数据具有更好的泛化能力。", "metrics": {"bleu_score": 77.08956192084696, "chrf_score": 58.23139148108998, "xcomet_score": 0.9238463640213013, "xcomet_qe_score": 0.9233023524284363, "metricx_score": 1.2559252977371216, "metricx_qe_score": 0.974765956401825, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型规模。", "metrics": {"bleu_score": 63.8194179668201, "chrf_score": 59.09235313234967, "xcomet_score": 0.9858014583587646, "xcomet_qe_score": 0.8824237585067749, "metricx_score": 1.7902395725250244, "metricx_qe_score": 1.9330796003341675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常较大规模的模型具有更好的泛化能力。", "metrics": {"bleu_score": 39.99905887888884, "chrf_score": 32.87290630337263, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4113702178001404, "metricx_qe_score": 0.5005305409431458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后但同样重要的是,我们都知道微调示例的数量直接影响下游任务的性能。", "metrics": {"bleu_score": 57.586306203301156, "chrf_score": 61.65302803209707, "xcomet_score": 0.9676473140716553, "xcomet_qe_score": 0.9515286087989807, "metricx_score": 1.5454611778259277, "metricx_qe_score": 1.3387689590454102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们也发现更多的微调示例实际上也导致了更好的泛化。", "metrics": {"bleu_score": 55.492421196566994, "chrf_score": 51.8000175365498, "xcomet_score": 0.9592689871788025, "xcomet_qe_score": 0.7876808643341064, "metricx_score": 0.7779982686042786, "metricx_qe_score": 0.8381263017654419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "到我们的下一个问题,哪些因素导致了一些模型的性能下降? 我们提出了两个假设。", "metrics": {"bleu_score": 32.92610942512308, "chrf_score": 28.762569657145935, "xcomet_score": 0.8408406972885132, "xcomet_qe_score": 0.8354443311691284, "metricx_score": 1.603132724761963, "metricx_qe_score": 1.9556999206542969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是适应性过拟合,即由于反复使用相同的测试集而导致的过拟合,这通常表现为新测试集上的收益递减。", "metrics": {"bleu_score": 63.81640557749199, "chrf_score": 57.0349434248433, "xcomet_score": 0.9658752679824829, "xcomet_qe_score": 0.8439780473709106, "metricx_score": 2.2350871562957764, "metricx_qe_score": 3.1059556007385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间间隔逐渐增大而导致的性能下降。", "metrics": {"bleu_score": 65.12772949366064, "chrf_score": 61.0911977203752, "xcomet_score": 0.9695290327072144, "xcomet_qe_score": 0.8898763060569763, "metricx_score": 1.4166030883789062, "metricx_qe_score": 1.958247423171997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右图中看到,红色最佳拟合直线的斜率大于一。", "metrics": {"bleu_score": 38.11926598308849, "chrf_score": 32.11591523048697, "xcomet_score": 0.864386796951294, "xcomet_qe_score": 0.8058450222015381, "metricx_score": 1.4168378114700317, "metricx_qe_score": 1.6576627492904663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在卡尔2003版本上所做的每一次改进,在卡尔++上都转化为超过一次的改进,这说明不存在边际效益递减的情况。", "metrics": {"bleu_score": 24.39122216934131, "chrf_score": 23.21008285831468, "xcomet_score": 0.6286587715148926, "xcomet_qe_score": 0.6980012655258179, "metricx_score": 5.153933048248291, "metricx_qe_score": 4.906229019165039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这向我们展示了这种情况下没有观察到适应性过拟合。", "metrics": {"bleu_score": 36.94654605751221, "chrf_score": 30.94274879744962, "xcomet_score": 0.8726509809494019, "xcomet_qe_score": 0.8224622011184692, "metricx_score": 2.509809970855713, "metricx_qe_score": 3.0167951583862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,关于暂时的漂移呢?", "metrics": {"bleu_score": 16.59038701421971, "chrf_score": 18.425499409735885, "xcomet_score": 0.8438212275505066, "xcomet_qe_score": 0.8322104811668396, "metricx_score": 2.155959367752075, "metricx_qe_score": 2.8443658351898193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,重新训练或继续使用更近期数据预训练一些模型,发现随着时间差距的增大,性能会下降。 这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 57.36051400335714, "chrf_score": 49.917038825090124, "xcomet_score": 0.8535670638084412, "xcomet_qe_score": 0.9226012825965881, "metricx_score": 1.9619940519332886, "metricx_qe_score": 2.128866195678711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,要实现良好的泛化能力,我们需要更好的模型架构、更大的模型规模以及更多的微调示例,", "metrics": {"bleu_score": 79.90356365333821, "chrf_score": 76.3170352896855, "xcomet_score": 0.9739580750465393, "xcomet_qe_score": 0.9342944622039795, "metricx_score": 0.8755756616592407, "metricx_qe_score": 1.2504841089248657, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是相辅相成的。我们不能只拥有其中一种因素,而忽略其他因素。", "metrics": {"bleu_score": 41.82088803780056, "chrf_score": 34.10822609012933, "xcomet_score": 0.979591965675354, "xcomet_qe_score": 0.9574956893920898, "metricx_score": 0.7007670998573303, "metricx_qe_score": 1.2156144380569458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们也发现这里的性能下降是由时间漂移引起的,而且令人惊讶的是,它不是由自适应过拟合引起的,尽管Kono 2003已经使用了二十多年。", "metrics": {"bleu_score": 55.77839743545531, "chrf_score": 47.63865858855446, "xcomet_score": 0.6646041870117188, "xcomet_qe_score": 0.6374936103820801, "metricx_score": 4.631783485412598, "metricx_qe_score": 4.822123050689697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们论文标题中提出的问题,Kono 2003年的标注器在2023年仍然有效吗?", "metrics": {"bleu_score": 55.41238885869759, "chrf_score": 45.4657846765386, "xcomet_score": 0.73503577709198, "xcomet_qe_score": 0.7555264234542847, "metricx_score": 4.943669319152832, "metricx_qe_score": 4.45146369934082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,答案实际上是确凿的“是”。", "metrics": {"bleu_score": 18.605335292758287, "chrf_score": 22.518852031820423, "xcomet_score": 0.9697844982147217, "xcomet_qe_score": 0.9799066185951233, "metricx_score": 1.3367396593093872, "metricx_qe_score": 1.2418265342712402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望本文能够呼吁更多关于如何改进模型泛化能力的研究。", "metrics": {"bleu_score": 55.60335612120309, "chrf_score": 47.86225722914768, "xcomet_score": 0.9238120317459106, "xcomet_qe_score": 0.9266766905784607, "metricx_score": 1.0022426843643188, "metricx_qe_score": 0.8057245016098022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查阅我们的论文、数据集,如有任何疑问,欢迎随时与我联系。", "metrics": {"bleu_score": 51.598503138569704, "chrf_score": 42.60123451744849, "xcomet_score": 0.9915920495986938, "xcomet_qe_score": 0.9755833148956299, "metricx_score": 0.20257097482681274, "metricx_qe_score": 0.1886216104030609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9850989580154419, "xcomet_qe_score": 0.9753036499023438, "metricx_score": 0.186608225107193, "metricx_qe_score": 0.06603709608316422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527263641357422, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将讨论我们在解决实体选择中间接指代表达方式方面的工作,我们在其中引入了替代实体语料库(altentity scorpus)。 (注:\"altentity scorpus\" 直接翻译为 \"替代实体语料库\",保持了原文的专业术语,符合学术或教学材料的语境。", "metrics": {"bleu_score": 6.4761851801455705, "chrf_score": 16.55358973187746, "xcomet_score": 0.42378556728363037, "xcomet_qe_score": 0.44999951124191284, "metricx_score": 8.294610977172852, "metricx_qe_score": 7.271191596984863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 我的名字是贾沃德·霍塞尼,这是我与菲利普·拉丁斯基、西尔维亚·帕雷蒂和安妮·路易斯共同完成的作品。", "metrics": {"bleu_score": 5.739995047374932, "chrf_score": 4.289954610033577, "xcomet_score": 0.9032187461853027, "xcomet_qe_score": 0.9194035530090332, "metricx_score": 2.458563804626465, "metricx_qe_score": 2.28774094581604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时的用语。", "metrics": {"bleu_score": 71.41167025174802, "chrf_score": 63.19573443412453, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7391023635864258, "metricx_qe_score": 1.1332502365112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑这个替代问题:", "metrics": {"bleu_score": 31.76215203205584, "chrf_score": 24.505192041046012, "xcomet_score": 0.8366379737854004, "xcomet_qe_score": 0.8717852830886841, "metricx_score": 0.43862760066986084, "metricx_qe_score": 0.3804333508014679, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您是想表达“对我来说简单”还是“我有一种感觉”", "metrics": {"bleu_score": 4.334264033674369, "chrf_score": 3.7813731995784376, "xcomet_score": 0.36322322487831116, "xcomet_qe_score": 0.28025785088539124, "metricx_score": 3.8231077194213867, "metricx_qe_score": 3.1413216590881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?在这里,用户想要在这两个标志中选择一个。", "metrics": {"bleu_score": 23.198210427894825, "chrf_score": 21.41882483987747, "xcomet_score": 0.7244203090667725, "xcomet_qe_score": 0.7214442491531372, "metricx_score": 4.663220405578613, "metricx_qe_score": 5.350669860839844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用,例如说歌曲《Easy on Me》的名字,或者其位置,即第一首。", "metrics": {"bleu_score": 40.916467847470365, "chrf_score": 43.48500494195983, "xcomet_score": 0.7400747537612915, "xcomet_qe_score": 0.7445821762084961, "metricx_score": 2.739807605743408, "metricx_qe_score": 3.302178382873535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时,间接引用更合适,可以使对话更加自然。", "metrics": {"bleu_score": 8.201805952478455, "chrf_score": 13.65079365079365, "xcomet_score": 0.9388291835784912, "xcomet_qe_score": 0.9908889532089233, "metricx_score": 1.3415957689285278, "metricx_qe_score": 1.0351862907409668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户记不住来源的名称时,可能会发生这种情况。", "metrics": {"bleu_score": 35.30066645908553, "chrf_score": 28.542999791313473, "xcomet_score": 0.8622283935546875, "xcomet_qe_score": 0.8285026550292969, "metricx_score": 1.966521143913269, "metricx_qe_score": 2.576202392578125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有发音都太相似,难以区分。", "metrics": {"bleu_score": 22.792296655031148, "chrf_score": 20.697981473308467, "xcomet_score": 0.8570217490196228, "xcomet_qe_score": 0.8698610067367554, "metricx_score": 2.685293674468994, "metricx_qe_score": 0.6818498373031616, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定一个偏好时。以下是直接引用的一", "metrics": {"bleu_score": 17.729842264695016, "chrf_score": 16.47217516782734, "xcomet_score": 0.6445475816726685, "xcomet_qe_score": 0.4794299900531769, "metricx_score": 4.47606086730957, "metricx_qe_score": 1.4778153896331787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些示例,例如较新的那个或不那么有活力的歌曲。", "metrics": {"bleu_score": 8.469014135246681, "chrf_score": 9.959686344230033, "xcomet_score": 0.5402449369430542, "xcomet_qe_score": 0.500841498374939, "metricx_score": 8.807282447814941, "metricx_qe_score": 8.890018463134766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是评估大语言模型(LLM)实体理解能力的重要基准。 我们不了解任何公", "metrics": {"bleu_score": 37.66833652786157, "chrf_score": 40.88392963357037, "xcomet_score": 0.4951506555080414, "xcomet_qe_score": 0.3326142132282257, "metricx_score": 7.011483192443848, "metricx_qe_score": 5.318689823150635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开的数据集,尤其是针对该任务的大规模公开数据集,因此我们通过众包标注方式收集了一个数据集。", "metrics": {"bleu_score": 29.059578523099464, "chrf_score": 26.97221968700747, "xcomet_score": 0.43462511897087097, "xcomet_qe_score": 0.4423704147338867, "metricx_score": 7.121310710906982, "metricx_qe_score": 8.355256080627441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和研究。", "metrics": {"bleu_score": 66.7619194068951, "chrf_score": 58.75788429852825, "xcomet_score": 0.8731950521469116, "xcomet_qe_score": 0.8643183708190918, "metricx_score": 2.276656150817871, "metricx_qe_score": 2.997232437133789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通完成集。 卡通图", "metrics": {"bleu_score": 65.58090491210355, "chrf_score": 57.083494069001304, "xcomet_score": 0.7453337907791138, "xcomet_qe_score": 0.7188123464584351, "metricx_score": 5.005326271057129, "metricx_qe_score": 5.6348652839660645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "画中有三个对话框。", "metrics": {"bleu_score": 60.10525952194528, "chrf_score": 55.01654521223255, "xcomet_score": 0.8790745139122009, "xcomet_qe_score": 0.7809643745422363, "metricx_score": 0.2630414664745331, "metricx_qe_score": 0.43603307008743286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框中,鲍勃说:“记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 54.35700692115411, "chrf_score": 48.877789279963196, "xcomet_score": 0.8934242725372314, "xcomet_qe_score": 0.8828657269477844, "metricx_score": 1.3489147424697876, "metricx_qe_score": 0.8999765515327454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由此,鲍勃为对话设定了背景。", "metrics": {"bleu_score": 6.649479326478728, "chrf_score": 6.163574825138827, "xcomet_score": 0.9592416286468506, "xcomet_qe_score": 0.9465450048446655, "metricx_score": 2.1149940490722656, "metricx_qe_score": 1.8972185850143433, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话框中,爱丽丝说:“你是说对我来说容易点,还是我理解错了?”", "metrics": {"bleu_score": 15.51422640328163, "chrf_score": 9.722364077917495, "xcomet_score": 0.7438119649887085, "xcomet_qe_score": 0.7601404190063477, "metricx_score": 3.329192638397217, "metricx_qe_score": 3.750377655029297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "哪一个是替代问题。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 19.742063492063487, "xcomet_score": 0.8520238399505615, "xcomet_qe_score": 0.8515355587005615, "metricx_score": 1.433502197265625, "metricx_qe_score": 1.1972465515136719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,Bob 使用了一种间接引用来选择这些实体之一,例如新的 RF。", "metrics": {"bleu_score": 16.19040473811321, "chrf_score": 21.078716930110115, "xcomet_score": 0.5743975639343262, "xcomet_qe_score": 0.5095292925834656, "metricx_score": 7.235828876495361, "metricx_qe_score": 6.831781387329102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动生成第一个和第二个对话气泡,但第三个由标注人员填写。", "metrics": {"bleu_score": 40.498577356351355, "chrf_score": 35.008863794554685, "xcomet_score": 0.8141974210739136, "xcomet_qe_score": 0.8601424694061279, "metricx_score": 1.7080494165420532, "metricx_qe_score": 1.1902917623519897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话气泡从每个领域的手动提示中选择几个。", "metrics": {"bleu_score": 48.541082409110636, "chrf_score": 40.337072847147645, "xcomet_score": 0.6859073638916016, "xcomet_qe_score": 0.7590972185134888, "metricx_score": 2.798969030380249, "metricx_qe_score": 2.338247537612915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即备选问题,是按以下方式生成的。", "metrics": {"bleu_score": 14.400124446705304, "chrf_score": 16.626669796191788, "xcomet_score": 0.9265819787979126, "xcomet_qe_score": 0.966843843460083, "metricx_score": 0.32800549268722534, "metricx_qe_score": 0.42264053225517273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板。", "metrics": {"bleu_score": 69.97522298221911, "chrf_score": 66.6583565648985, "xcomet_score": 0.997756838798523, "xcomet_qe_score": 0.9854191541671753, "metricx_score": 0.1580941081047058, "metricx_qe_score": 0.16494783759117126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是来自维基百科的样本。", "metrics": {"bleu_score": 86.11735299633672, "chrf_score": 96.57574311968287, "xcomet_score": 0.9713319540023804, "xcomet_qe_score": 0.9262405037879944, "metricx_score": 0.7253305912017822, "metricx_qe_score": 0.857016384601593, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同采样方法。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 71.19713619713619, "xcomet_score": 0.9981815814971924, "xcomet_qe_score": 1.0, "metricx_score": 0.14823222160339355, "metricx_qe_score": 0.255437970161438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体之间会变得更加相似,通常更难进行歧义消除。", "metrics": {"bleu_score": 52.07958133637565, "chrf_score": 49.697708975088396, "xcomet_score": 0.8472758531570435, "xcomet_qe_score": 0.8218706846237183, "metricx_score": 3.600844621658325, "metricx_qe_score": 4.013828277587891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀吸引力。", "metrics": {"bleu_score": 16.058516370438436, "chrf_score": 13.011476708899517, "xcomet_score": 0.8054068684577942, "xcomet_qe_score": 0.7826197147369385, "metricx_score": 3.283067226409912, "metricx_qe_score": 2.1671273708343506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个情况是实体具有相似的标题,例如两本书都名为《零售》。", "metrics": {"bleu_score": 26.087213732293563, "chrf_score": 21.207899693624483, "xcomet_score": 0.7396266460418701, "xcomet_qe_score": 0.7283750772476196, "metricx_score": 4.2205119132995605, "metricx_qe_score": 5.40279483795166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种情况是它们在维基百科上有相似的描述。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.997238039970398, "xcomet_qe_score": 0.9820467233657837, "metricx_score": 0.3877851665019989, "metricx_qe_score": 0.4526749849319458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,当它们在维基百科上有相似的信息框或属性时", "metrics": {"bleu_score": 72.00242075875518, "chrf_score": 64.66198984505621, "xcomet_score": 0.933289110660553, "xcomet_qe_score": 0.988023042678833, "metricx_score": 1.0644596815109253, "metricx_qe_score": 1.273808479309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如相同的类型或相同的艺术家风格。", "metrics": {"bleu_score": 24.202875575621302, "chrf_score": 25.811212075689348, "xcomet_score": 0.8333064317703247, "xcomet_qe_score": 0.7778071165084839, "metricx_score": 3.1007602214813232, "metricx_qe_score": 3.0618233680725098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向标注者展示这个替代问题时,他们知道这些实体的名称,但他们不一定了解这个实体。", "metrics": {"bleu_score": 55.09512055858448, "chrf_score": 45.916526564793564, "xcomet_score": 0.7410421967506409, "xcomet_qe_score": 0.7424563765525818, "metricx_score": 3.2981276512145996, "metricx_qe_score": 3.3419370651245117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是展示关于两个实体的背景知识。", "metrics": {"bleu_score": 58.6558665195849, "chrf_score": 48.12461273957527, "xcomet_score": 0.9710360765457153, "xcomet_qe_score": 0.8324273824691772, "metricx_score": 1.0238763093948364, "metricx_qe_score": 1.876013159751892, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地提供每个歌曲的Google搜索链接。 (Wǒmen suǒ zuò de shì zhǎnshì guānyú liǎng gè shíjì de bèijìng zhīshì. Duìyú gēqǔ, wǒmen jiǎndān dì tígōng měi gè gēqǔ de Google sōusuǒ liànjié.) 然后请标注人员至少听每首歌曲的一部分,并阅读有关每首歌曲的信息。", "metrics": {"bleu_score": 18.026741642357226, "chrf_score": 16.72006594363427, "xcomet_score": 0.4154530465602875, "xcomet_qe_score": 0.38751497864723206, "metricx_score": 5.559749126434326, "metricx_qe_score": 5.904974937438965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这是谷歌搜索结果中关于歌曲《Easy》的链接。 (请注意,实际翻译中可能会根据上下文调整标点符号和格式以符合中文习惯。)", "metrics": {"bleu_score": 10.71531397499001, "chrf_score": 22.443941826876575, "xcomet_score": 0.7305666208267212, "xcomet_qe_score": 0.792701780796051, "metricx_score": 5.022793292999268, "metricx_qe_score": 5.64730167388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示了来自维基百科的背景文本。", "metrics": {"bleu_score": 63.13476599314376, "chrf_score": 54.19210175474897, "xcomet_score": 0.9804984331130981, "xcomet_qe_score": 0.9193463325500488, "metricx_score": 0.9178977608680725, "metricx_qe_score": 1.5036317110061646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科再次展示它们的图像,以便注释器了解它们的样子。", "metrics": {"bleu_score": 37.78733748351261, "chrf_score": 30.786526785621763, "xcomet_score": 0.7835915088653564, "xcomet_qe_score": 0.6938985586166382, "metricx_score": 3.831116199493408, "metricx_qe_score": 3.916487693786621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注员从这些实体中选择一个,例如这里的第一个,并使用三到五个间接指代表达来描述它们。", "metrics": {"bleu_score": 55.57123755102434, "chrf_score": 47.925647011163704, "xcomet_score": 0.7919996976852417, "xcomet_qe_score": 0.7170993089675903, "metricx_score": 3.8224668502807617, "metricx_qe_score": 3.8353188037872314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,那首有钢琴音乐的。", "metrics": {"bleu_score": 18.52797255583095, "chrf_score": 18.7643186885898, "xcomet_score": 0.9973348379135132, "xcomet_qe_score": 0.991289496421814, "metricx_score": 0.5101467370986938, "metricx_qe_score": 0.6321661472320557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的几个例子。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9644975662231445, "xcomet_qe_score": 0.8701311945915222, "metricx_score": 0.5822314620018005, "metricx_qe_score": 1.956078290939331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,那首没有歌词的,不是那首有12岁男孩演唱的,也不是那首虚构的,更不是来自阿塞拜疆的。", "metrics": {"bleu_score": 26.554625677223346, "chrf_score": 25.435173548863172, "xcomet_score": 0.7483680248260498, "xcomet_qe_score": 0.8411582708358765, "metricx_score": 1.411492109298706, "metricx_qe_score": 2.052264451980591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "替代问题语料库包含三个领域的6,000个替代问题,以及42,000个间接指代表达。", "metrics": {"bleu_score": 26.25996281830108, "chrf_score": 32.455042857602656, "xcomet_score": 0.6074305176734924, "xcomet_qe_score": 0.547632098197937, "metricx_score": 4.98238468170166, "metricx_qe_score": 5.011369228363037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用T5xLarge模型的结果总结如下。", "metrics": {"bleu_score": 33.41796039044061, "chrf_score": 31.282602612524812, "xcomet_score": 0.9068986177444458, "xcomet_qe_score": 0.8763749003410339, "metricx_score": 1.8517205715179443, "metricx_qe_score": 1.942700743675232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者完全相同的背景知识,那么准确率会非常高,大约在92%到95%之间。", "metrics": {"bleu_score": 63.66408032704386, "chrf_score": 57.24642421423021, "xcomet_score": 0.9042733907699585, "xcomet_qe_score": 0.9885326623916626, "metricx_score": 0.9912989735603333, "metricx_qe_score": 0.6616532802581787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这在现实中是不可行的。", "metrics": {"bleu_score": 13.545994273378138, "chrf_score": 23.700887198986063, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09328046441078186, "metricx_qe_score": 0.11937843263149261, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型能够访问部分重叠的背景知识,那么准确率在82%到87%之间,这更符合实际情况", "metrics": {"bleu_score": 68.17658990858149, "chrf_score": 62.03689792228847, "xcomet_score": 0.9226853847503662, "xcomet_qe_score": 0.9136360287666321, "metricx_score": 0.8123430609703064, "metricx_qe_score": 1.1492154598236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9841159582138062, "xcomet_qe_score": 0.9635313749313354, "metricx_score": 0.9222766160964966, "metricx_qe_score": 1.0717954635620117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率仅为60%。因此,改进的空间很大。", "metrics": {"bleu_score": 55.3813956401068, "chrf_score": 49.297214756773585, "xcomet_score": 0.9963889122009277, "xcomet_qe_score": 0.9912608861923218, "metricx_score": 1.5682313442230225, "metricx_qe_score": 2.3577425479888916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还展示了模型具有领域泛化能力。", "metrics": {"bleu_score": 35.55508425572383, "chrf_score": 29.61443703019254, "xcomet_score": 0.8834165334701538, "xcomet_qe_score": 0.8751844167709351, "metricx_score": 0.9266247153282166, "metricx_qe_score": 0.9063495993614197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是来自特伦托大学和布鲁诺·凯斯勒基金会的莎拉·帕皮,我将简要介绍一篇关于注意力引导同时语音翻译的论文,这是我与马特奥·内格里和马可·图尔基合作的研究成果。", "metrics": {"bleu_score": 38.84143745136292, "chrf_score": 31.114810108019174, "xcomet_score": 0.8401539325714111, "xcomet_qe_score": 0.825377345085144, "metricx_score": 2.9024405479431152, "metricx_qe_score": 2.206549644470215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同步口语翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.9202121496200562, "xcomet_qe_score": 0.9238082766532898, "metricx_score": 0.4620591104030609, "metricx_qe_score": 0.26660972833633423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同步口语翻译或同步翻译(simul SD)是一种将口语实时翻译成另一门语言文本的过程,从而实现跨语言交流。", "metrics": {"bleu_score": 43.767324924406275, "chrf_score": 42.9354114028076, "xcomet_score": 0.8638780117034912, "xcomet_qe_score": 0.8493569493293762, "metricx_score": 4.841779708862305, "metricx_qe_score": 4.943938732147217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前SimulST模型的问题是什么?", "metrics": {"bleu_score": 19.156928817239653, "chrf_score": 47.80580394550984, "xcomet_score": 0.937305212020874, "xcomet_qe_score": 0.9294310212135315, "metricx_score": 0.6831821799278259, "metricx_qe_score": 1.0893099308013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常通过引入需要优化的额外模块来", "metrics": {"bleu_score": 6.542540885608186, "chrf_score": 13.375819916517592, "xcomet_score": 0.5757852792739868, "xcomet_qe_score": 0.464724600315094, "metricx_score": 6.107959270477295, "metricx_qe_score": 4.433515548706055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练特定的架构。 例如涉及不同优化目标的训练,", "metrics": {"bleu_score": 54.86258862975676, "chrf_score": 52.286781737837394, "xcomet_score": 0.5486255884170532, "xcomet_qe_score": 0.3490009009838104, "metricx_score": 5.272582054138184, "metricx_qe_score": 5.694881439208984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "长而复杂的训练程序 训练和维护多个模型以达到不同的延迟等", "metrics": {"bleu_score": 50.26147453550858, "chrf_score": 44.13790262340986, "xcomet_score": 0.891792893409729, "xcomet_qe_score": 0.8578487634658813, "metricx_score": 2.96496319770813, "metricx_qe_score": 3.4837822914123535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "级,例如训练一个模型使其平均延迟为每秒一秒,另一个模型为每秒两秒,以此类推。", "metrics": {"bleu_score": 33.637351703234174, "chrf_score": 29.854547892920564, "xcomet_score": 0.6367121934890747, "xcomet_qe_score": 0.6390426158905029, "metricx_score": 4.162447452545166, "metricx_qe_score": 5.2707648277282715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先使用已存在的离线SD模型,无需重新训练或采用特定的CLSD架构。", "metrics": {"bleu_score": 47.649901774101835, "chrf_score": 37.815439909820725, "xcomet_score": 0.7376292943954468, "xcomet_qe_score": 0.7455853223800659, "metricx_score": 5.3266730308532715, "metricx_qe_score": 6.160643577575684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为每个延迟制度仅使用一个模型,并通过特定参数处理延迟。 利用", "metrics": {"bleu_score": 48.39904047072857, "chrf_score": 39.557710503239136, "xcomet_score": 0.6947960257530212, "xcomet_qe_score": 0.5921483039855957, "metricx_score": 3.908268451690674, "metricx_qe_score": 1.5446598529815674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入和文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 59.62415297042496, "chrf_score": 58.15067645682329, "xcomet_score": 0.7593388557434082, "xcomet_qe_score": 0.7074575424194336, "metricx_score": 6.219468116760254, "metricx_qe_score": 4.784849166870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")已经获得的知识。右边有一个示例供您参考。", "metrics": {"bleu_score": 28.592291256793107, "chrf_score": 26.724025948852102, "xcomet_score": 0.34919220209121704, "xcomet_qe_score": 0.20731958746910095, "metricx_score": 7.680624485015869, "metricx_qe_score": 10.215764045715332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出点或编码器解码器注意力机制。这是一种策略,我们根据注意力指向的位置决定是否输出部分翻译。", "metrics": {"bleu_score": 59.45858518384582, "chrf_score": 49.32344254143974, "xcomet_score": 0.6765172481536865, "xcomet_qe_score": 0.630728006362915, "metricx_score": 6.274055480957031, "metricx_qe_score": 7.025057315826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果张力未集中,即在最后λ个语音帧中其总和低于某个阈值α,则发出一个词,这意味着接收到的信息足够稳定。", "metrics": {"bleu_score": 45.37381849394295, "chrf_score": 37.789476168902866, "xcomet_score": 0.6019224524497986, "xcomet_qe_score": 0.5115342140197754, "metricx_score": 5.158239364624023, "metricx_qe_score": 4.898241996765137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们接收到一个包含“我要谈论”的演讲片段,我们的模型预测的翻译是德语,我们就会... 我们将查看交叉注意力权重。 我们将看到,前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,即最后的lambda语音帧。", "metrics": {"bleu_score": 51.94377378509847, "chrf_score": 38.74964607452126, "xcomet_score": 0.5594230890274048, "xcomet_qe_score": 0.482339471578598, "metricx_score": 5.9035539627075195, "metricx_qe_score": 5.853006839752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个字将被发射。 由于交叉张力的总和超过了特定阈值 α,我们不会发出最后一个词,并等待下一个语音片段。", "metrics": {"bleu_score": 30.158901156696842, "chrf_score": 24.447702101417356, "xcomet_score": 0.5216149091720581, "xcomet_qe_score": 0.474621057510376, "metricx_score": 8.226123809814453, "metricx_qe_score": 7.9852070808410645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续进行,接收到另一个沉浸式的演讲,我们的模型预测了另外三个词,我们将观察交叉注意力权重。 我们将看到没有一个词指向最后的lambda语音帧。", "metrics": {"bleu_score": 44.98318882116555, "chrf_score": 38.24503784646279, "xcomet_score": 0.5527931451797485, "xcomet_qe_score": 0.5044786930084229, "metricx_score": 5.950735092163086, "metricx_qe_score": 5.634975910186768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个单词将被发出。", "metrics": {"bleu_score": 47.855439210937384, "chrf_score": 38.86889545481519, "xcomet_score": 0.9408911466598511, "xcomet_qe_score": 0.9240285158157349, "metricx_score": 0.975865364074707, "metricx_qe_score": 1.911775827407837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们审视其主要结果,我们会发现 我们将在图表上绘制同时性语音翻译的结果,图表的一侧使用蓝色表示翻译质量和平均延迟。 这就是延迟度量。我们还考虑了计算感知平均滞后,它考虑到模型预测输出的计算时间。", "metrics": {"bleu_score": 31.499669682161407, "chrf_score": 24.87935920118335, "xcomet_score": 0.6309840679168701, "xcomet_qe_score": 0.5520899295806885, "metricx_score": 6.668145656585693, "metricx_qe_score": 6.636751174926758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望在这一图表中,我们的曲线尽可能地高。", "metrics": {"bleu_score": 33.535699101570344, "chrf_score": 34.409483255127135, "xcomet_score": 0.952837347984314, "xcomet_qe_score": 0.7782680988311768, "metricx_score": 1.446225643157959, "metricx_qe_score": 1.7837791442871094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也希望它们能向左对齐。", "metrics": {"bleu_score": 49.487489225241916, "chrf_score": 43.318018327121976, "xcomet_score": 0.9244677424430847, "xcomet_qe_score": 0.9344780445098877, "metricx_score": 3.670602321624756, "metricx_qe_score": 3.7596683502197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将与PROPERA策略进行比较,这些策略也适用于离线模型,即WitKey策略和本地一致性。", "metrics": {"bleu_score": 39.319919825365815, "chrf_score": 26.766908903281344, "xcomet_score": 0.6535466909408569, "xcomet_qe_score": 0.5967296361923218, "metricx_score": 7.063439846038818, "metricx_qe_score": 7.680710792541504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将与专为同时预翻译而设计的最新架构进行比较。", "metrics": {"bleu_score": 38.84975679002265, "chrf_score": 33.84483029048246, "xcomet_score": 0.9009069204330444, "xcomet_qe_score": 0.8764907121658325, "metricx_score": 1.3146001100540161, "metricx_qe_score": 1.6678975820541382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是同时性口语翻译策略在德语上的结果。", "metrics": {"bleu_score": 37.02730401700485, "chrf_score": 32.14025970586722, "xcomet_score": 0.7970110177993774, "xcomet_qe_score": 0.8275015354156494, "metricx_score": 1.7976106405258179, "metricx_qe_score": 1.450630784034729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,ADUT 在离线模型上表现优越,因为其曲线向左移动,表明其性能更优。", "metrics": {"bleu_score": 22.148418795159994, "chrf_score": 19.830053015502454, "xcomet_score": 0.9661409854888916, "xcomet_qe_score": 0.932010293006897, "metricx_score": 3.962888240814209, "metricx_qe_score": 5.526254653930664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际耗时或计算感知时间,那就是最快的策略。", "metrics": {"bleu_score": 52.3072516967932, "chrf_score": 46.52850982640082, "xcomet_score": 0.8733664751052856, "xcomet_qe_score": 0.8850498795509338, "metricx_score": 2.7253823280334473, "metricx_qe_score": 3.175943374633789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想发现更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 65.14613449066714, "chrf_score": 54.128468638917546, "xcomet_score": 0.9677166938781738, "xcomet_qe_score": 0.9603763222694397, "metricx_score": 0.6387813091278076, "metricx_qe_score": 0.45578014850616455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源了代码和模型,并同时输出以促进我们工作的可复现性。", "metrics": {"bleu_score": 10.630643627589146, "chrf_score": 17.327180077091075, "xcomet_score": 0.8609133958816528, "xcomet_qe_score": 0.8361285328865051, "metricx_score": 1.1268640756607056, "metricx_qe_score": 1.595489740371704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是英,我的同事江和我将要展示我们关于通过指令调优提高多模态序列学习的研究。", "metrics": {"bleu_score": 36.55843240658432, "chrf_score": 20.825557754928457, "xcomet_score": 0.5964707136154175, "xcomet_qe_score": 0.6206020712852478, "metricx_score": 6.654804706573486, "metricx_qe_score": 6.9030046463012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索新的学习范式,即以参数和数据高效的方式,重用预训练的语言模型来处理不同的下游任务。", "metrics": {"bleu_score": 70.36010192613716, "chrf_score": 64.25937743162335, "xcomet_score": 0.8623789548873901, "xcomet_qe_score": 0.7453666925430298, "metricx_score": 1.2421162128448486, "metricx_qe_score": 1.9141602516174316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,通过遵循自然指令,指令调优使大型语言模型能够以零样本的方式执行未见过的任务。", "metrics": {"bleu_score": 49.02329947086209, "chrf_score": 42.68866677488405, "xcomet_score": 0.7144687175750732, "xcomet_qe_score": 0.735694408416748, "metricx_score": 2.338747024536133, "metricx_qe_score": 4.4469475746154785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前关于指令调优的大部分工作都集中在改进语言仅任务的串行快照性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 33.3816076572442, "chrf_score": 31.252666342345243, "xcomet_score": 0.7920840978622437, "xcomet_qe_score": 0.7134732007980347, "metricx_score": 4.541539669036865, "metricx_qe_score": 4.789851188659668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本研究中,我们希望探讨在多模态预训练模型上进行指令调优是否能够实际提升对未见多模态任务的泛化能力。", "metrics": {"bleu_score": 43.845987996879984, "chrf_score": 38.661822111511064, "xcomet_score": 0.9003031253814697, "xcomet_qe_score": 0.7722374796867371, "metricx_score": 1.2409812211990356, "metricx_qe_score": 1.4773221015930176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现RLP和跨模态之间存在相当大的教学数据集可用性差异。 目前", "metrics": {"bleu_score": 27.549031468404014, "chrf_score": 22.91466789339733, "xcomet_score": 0.6410417556762695, "xcomet_qe_score": 0.6820383667945862, "metricx_score": 5.084107398986816, "metricx_qe_score": 1.5925981998443604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过一千六百个仅限语言指令任务。", "metrics": {"bleu_score": 28.16289700045209, "chrf_score": 24.760172489238126, "xcomet_score": 0.8575549125671387, "xcomet_qe_score": 0.7640866041183472, "metricx_score": 1.019329309463501, "metricx_qe_score": 1.415502905845642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大规模公开的多模态指令任务。", "metrics": {"bleu_score": 63.09433236258316, "chrf_score": 53.068690926663066, "xcomet_score": 0.9746540784835815, "xcomet_qe_score": 0.826064944267273, "metricx_score": 1.468718409538269, "metricx_qe_score": 2.521465539932251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这激励我们构建一个多模态指令调优数据集。", "metrics": {"bleu_score": 76.74174160136336, "chrf_score": 70.5002322450835, "xcomet_score": 0.9718524217605591, "xcomet_qe_score": 0.9641896486282349, "metricx_score": 0.8962436318397522, "metricx_qe_score": 1.0525983572006226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此我们介绍 Multi Instruct,这是第一个多模态指令调优基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个广泛类别。", "metrics": {"bleu_score": 44.941688555936715, "chrf_score": 48.5403556148391, "xcomet_score": 0.818139374256134, "xcomet_qe_score": 0.8005620837211609, "metricx_score": 2.024089813232422, "metricx_qe_score": 2.047431707382202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来源于二十一个现有的开源数据集,每个任务配备了五条专家撰写的指令。", "metrics": {"bleu_score": 51.660290278382476, "chrf_score": 45.585965400637555, "xcomet_score": 0.9632176160812378, "xcomet_qe_score": 0.9483460187911987, "metricx_score": 1.1963773965835571, "metricx_qe_score": 1.8140621185302734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令调优,我们采用OFA作为基础模型,OFA是一个统一的多模态模式模型。OFA使用统一的词", "metrics": {"bleu_score": 60.23782701640877, "chrf_score": 64.05438682020532, "xcomet_score": 0.7452781200408936, "xcomet_qe_score": 0.6996070742607117, "metricx_score": 6.760657787322998, "metricx_qe_score": 4.136972427368164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来处理语言、图像令牌和边界框的坐标。 (注:这里“统一的词汇表”可以根据具体情况和学术背景有更精确的翻译,如“统一的词嵌入”或“共享的词表空间”等。)", "metrics": {"bleu_score": 16.604802649594657, "chrf_score": 28.587011404828672, "xcomet_score": 0.2875717878341675, "xcomet_qe_score": 0.2834582030773163, "metricx_score": 8.188419342041016, "metricx_qe_score": 7.736379623413086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在此,我们展示了我们多层次数据集中的几个示例实例。 统一处理多种输入和输出数据类型。", "metrics": {"bleu_score": 48.10494236079411, "chrf_score": 32.563833066002125, "xcomet_score": 0.7609553337097168, "xcomet_qe_score": 0.7152031660079956, "metricx_score": 3.0850343704223633, "metricx_qe_score": 3.0797996520996094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,将所有任务统一为序列到序列的格式,在该格式中", "metrics": {"bleu_score": 45.00353093570273, "chrf_score": 44.46284588245524, "xcomet_score": 0.7406298518180847, "xcomet_qe_score": 0.7401142716407776, "metricx_score": 3.969085931777954, "metricx_qe_score": 4.667516231536865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",输入文本、图像、指令和边界框在同一标记空间中表示。", "metrics": {"bleu_score": 66.22312744455333, "chrf_score": 60.90159563802837, "xcomet_score": 0.9496505260467529, "xcomet_qe_score": 0.920018196105957, "metricx_score": 1.431187629699707, "metricx_qe_score": 1.7373710870742798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好了,现在我要谈谈多模态指令微调。", "metrics": {"bleu_score": 64.8138893454484, "chrf_score": 60.97365869424692, "xcomet_score": 0.9153221845626831, "xcomet_qe_score": 0.9008731245994568, "metricx_score": 0.7149470448493958, "metricx_qe_score": 0.756284236907959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用NIG组中的53个任务进行训练,并每个任务采样10,000个实例。", "metrics": {"bleu_score": 60.23587925328831, "chrf_score": 56.8360370288872, "xcomet_score": 0.7703511714935303, "xcomet_qe_score": 0.7796882390975952, "metricx_score": 7.023180961608887, "metricx_qe_score": 7.172938346862793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识推理组作为测试用例,并从WQA和杂项组中额外选择五个任务。", "metrics": {"bleu_score": 32.604243773146656, "chrf_score": 27.08418514160002, "xcomet_score": 0.7129949927330017, "xcomet_qe_score": 0.6671640872955322, "metricx_score": 3.8304455280303955, "metricx_qe_score": 4.046748161315918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务中的所有测试实例。", "metrics": {"bleu_score": 36.71536136769783, "chrf_score": 30.700619092604914, "xcomet_score": 0.8697065114974976, "xcomet_qe_score": 0.9448975324630737, "metricx_score": 1.0791749954223633, "metricx_qe_score": 1.6602849960327148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的语法测试集中随机抽取二十个任务进行采样,正如在自然语言处理(NLP)中的做法。 因此,", "metrics": {"bleu_score": 25.190391042544494, "chrf_score": 28.658938067328833, "xcomet_score": 0.451736181974411, "xcomet_qe_score": 0.40343230962753296, "metricx_score": 5.00994348526001, "metricx_qe_score": 4.304985046386719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一个预训练的OFA大型模型作为基础模型。", "metrics": {"bleu_score": 82.32490471721698, "chrf_score": 85.53120011521331, "xcomet_score": 0.9636353254318237, "xcomet_qe_score": 0.9542060494422913, "metricx_score": 1.465710163116455, "metricx_qe_score": 3.166785717010498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们混合了所有任务的所有实例。", "metrics": {"bleu_score": 87.25129388059685, "chrf_score": 81.134368154492, "xcomet_score": 0.819719672203064, "xcomet_qe_score": 0.7874776124954224, "metricx_score": 1.1428027153015137, "metricx_qe_score": 1.7761423587799072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与五个指令模板中的一个组合。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9054250717163086, "xcomet_qe_score": 0.8633378148078918, "metricx_score": 1.5055538415908813, "metricx_qe_score": 1.778053641319275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此在每次任务的测试中,我们通过使用每次实验中的五个指令之一来评估模型,总共进行五次实验。", "metrics": {"bleu_score": 33.881669575689024, "chrf_score": 29.61992301582183, "xcomet_score": 0.8195039629936218, "xcomet_qe_score": 0.8653055429458618, "metricx_score": 1.6194231510162354, "metricx_qe_score": 1.9803472757339478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告了所有五个实验中性能的平均值、最大值以及性能的标准差。", "metrics": {"bleu_score": 26.651734452213976, "chrf_score": 21.595236969455854, "xcomet_score": 0.9568487405776978, "xcomet_qe_score": 0.953087329864502, "metricx_score": 2.5906994342803955, "metricx_qe_score": 2.1702141761779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们报告准确率。", "metrics": {"bleu_score": 51.92815178749843, "chrf_score": 41.86625721437747, "xcomet_score": 0.92449951171875, "xcomet_qe_score": 0.9797228574752808, "metricx_score": 0.587925374507904, "metricx_qe_score": 0.7086970806121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,我们报告 RougeL。对于 RP 任务,我们也报告 RougeL。", "metrics": {"bleu_score": 56.38909821116625, "chrf_score": 59.56958254339049, "xcomet_score": 0.783795952796936, "xcomet_qe_score": 0.7383759021759033, "metricx_score": 4.290326118469238, "metricx_qe_score": 4.329876899719238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一种名为灵敏度的额外评估指标。", "metrics": {"bleu_score": 43.51488014121156, "chrf_score": 37.166386751419914, "xcomet_score": 0.9961971044540405, "xcomet_qe_score": 0.9312809109687805, "metricx_score": 0.4886736273765564, "metricx_qe_score": 0.8346283435821533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它衡量模型在指令措辞略有不同时,能否一致地为同一任务产生相同输出。", "metrics": {"bleu_score": 30.081548655279516, "chrf_score": 24.74098551636209, "xcomet_score": 0.8980842232704163, "xcomet_qe_score": 0.9269237518310547, "metricx_score": 2.9578845500946045, "metricx_qe_score": 3.6689159870147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要结果。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9698837995529175, "xcomet_qe_score": 0.88059002161026, "metricx_score": 0.1918793022632599, "metricx_qe_score": 0.3046000003814697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提升OFE在多模态任务中的表现。", "metrics": {"bleu_score": 62.55586059555664, "chrf_score": 52.4601969371678, "xcomet_score": 0.8842756748199463, "xcomet_qe_score": 0.8819289207458496, "metricx_score": 2.6253459453582764, "metricx_qe_score": 2.831273078918457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习有助于指令微调。", "metrics": {"bleu_score": 58.90479852336762, "chrf_score": 55.096482165308934, "xcomet_score": 0.9787473678588867, "xcomet_qe_score": 0.7723735570907593, "metricx_score": 1.180357575416565, "metricx_qe_score": 2.0029549598693848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着任务量的增加,模型的性能得到了提升,同时敏感度也降低了。", "metrics": {"bleu_score": 17.756864324187937, "chrf_score": 16.828326296532065, "xcomet_score": 0.9826514720916748, "xcomet_qe_score": 0.9867242574691772, "metricx_score": 1.0961089134216309, "metricx_qe_score": 0.9243431091308594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也进行了一个实验,", "metrics": {"bleu_score": 40.35278637463991, "chrf_score": 33.91307136330957, "xcomet_score": 0.9543628692626953, "xcomet_qe_score": 0.9413754940032959, "metricx_score": 0.393255352973938, "metricx_qe_score": 0.3359813690185547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了一个指令与五个指令进行对比。", "metrics": {"bleu_score": 20.236360012390456, "chrf_score": 24.18828101675276, "xcomet_score": 0.9480768442153931, "xcomet_qe_score": 0.8417447805404663, "metricx_score": 1.1746100187301636, "metricx_qe_score": 2.7787575721740723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多的指令可以提高模型的整体性能,并显著降低其敏感度。", "metrics": {"bleu_score": 63.76897100442937, "chrf_score": 58.065247608591264, "xcomet_score": 0.999605655670166, "xcomet_qe_score": 1.0, "metricx_score": 0.7967923879623413, "metricx_qe_score": 0.8627405762672424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这展示了不同微调策略对模型敏感度的影响。", "metrics": {"bleu_score": 58.27569940616854, "chrf_score": 46.863494243706114, "xcomet_score": 0.9762451648712158, "xcomet_qe_score": 0.9720354080200195, "metricx_score": 0.9672296047210693, "metricx_qe_score": 1.3664666414260864, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们通过从自然指令数据集进行迁移学习所看到的,模型可以实现比原始IFA模型高得多的敏感度。", "metrics": {"bleu_score": 51.1132451155485, "chrf_score": 43.72701685806637, "xcomet_score": 0.8679661750793457, "xcomet_qe_score": 0.7666575908660889, "metricx_score": 2.9982495307922363, "metricx_qe_score": 3.218121290206909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到,从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上取得更优异的表现。", "metrics": {"bleu_score": 63.10300348647631, "chrf_score": 58.94335527869911, "xcomet_score": 0.9433000087738037, "xcomet_qe_score": 0.8126220703125, "metricx_score": 1.8801127672195435, "metricx_qe_score": 2.997359275817871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "综上所述,我们提出了第一个大规模多模态指令调优数据集,这显著提升了OFA的导数能力。我们还探索了不同的迁移学习技术,并通过设计一种名为灵敏", "metrics": {"bleu_score": 43.351590142445495, "chrf_score": 39.75643046872121, "xcomet_score": 0.5171996355056763, "xcomet_qe_score": 0.5118372440338135, "metricx_score": 8.249430656433105, "metricx_qe_score": 7.759668350219727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "度的新的指标展示了它们的好处。", "metrics": {"bleu_score": 14.01875703185721, "chrf_score": 12.798401769871809, "xcomet_score": 0.15790900588035583, "xcomet_qe_score": 0.15482106804847717, "metricx_score": 8.399674415588379, "metricx_qe_score": 9.461627960205078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "再有一个要点,我们正在收集一个更大规模的多模态指令调优数据集,包含大约150个额外的变体语言任务,我们将发布这些数据。", "metrics": {"bleu_score": 53.40423014413107, "chrf_score": 49.989246207235105, "xcomet_score": 0.6744736433029175, "xcomet_qe_score": 0.6946344375610352, "metricx_score": 2.5768651962280273, "metricx_qe_score": 2.856558322906494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是 Sena 的 Coast,很高兴欢迎大家参加我们关于 ACL 2023 论文的讨论:", "metrics": {"bleu_score": 75.1919651166177, "chrf_score": 63.50895463414689, "xcomet_score": 0.5710753202438354, "xcomet_qe_score": 0.4196987748146057, "metricx_score": 7.441951274871826, "metricx_qe_score": 7.595095634460449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型可接受性判断并不总是对上下文有鲁棒性》。", "metrics": {"bleu_score": 63.8787937398753, "chrf_score": 59.39748578364758, "xcomet_score": 0.754897952079773, "xcomet_qe_score": 0.7573990821838379, "metricx_score": 4.482115745544434, "metricx_qe_score": 5.9651384353637695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与John Bokier、Aaron Muller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina William的合作成果。", "metrics": {"bleu_score": 21.154472601569257, "chrf_score": 66.8054776120519, "xcomet_score": 0.6815013289451599, "xcomet_qe_score": 0.6722943782806396, "metricx_score": 4.9250640869140625, "metricx_qe_score": 4.333015441894531, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们重新审视了最简对范式。", "metrics": {"bleu_score": 40.276720463657746, "chrf_score": 37.10798121855234, "xcomet_score": 0.89307701587677, "xcomet_qe_score": 0.8838869333267212, "metricx_score": 1.5370862483978271, "metricx_qe_score": 2.02675724029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,最小对照范式基本上在可接受性判断的基础上评估语言模型,这", "metrics": {"bleu_score": 39.774589541974024, "chrf_score": 34.912788531577064, "xcomet_score": 0.7265380620956421, "xcomet_qe_score": 0.7006301879882812, "metricx_score": 5.625278472900391, "metricx_qe_score": 1.2888613939285278, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,例如BLIMP、语法宝石,或者在刻板印象方面的可接受性,如Krauss对照。", "metrics": {"bleu_score": 28.50739274233354, "chrf_score": 20.329062267402353, "xcomet_score": 0.5761988162994385, "xcomet_qe_score": 0.5447743535041809, "metricx_score": 5.215435028076172, "metricx_qe_score": 5.015585899353027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这一最简对照范式中,评估语言模型的典型方法是展示一个可接受的句子或语法句,然后展示一个不可接受的句子或不语法句。", "metrics": {"bleu_score": 45.55553248542617, "chrf_score": 42.44203912860186, "xcomet_score": 0.7361373901367188, "xcomet_qe_score": 0.8029673099517822, "metricx_score": 2.0191993713378906, "metricx_qe_score": 2.9651641845703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后希望模型能够基本上将更高的概率分配给可接受的句子。", "metrics": {"bleu_score": 33.87144423286913, "chrf_score": 30.6808967349219, "xcomet_score": 0.9528704881668091, "xcomet_qe_score": 0.7171773910522461, "metricx_score": 1.336784839630127, "metricx_qe_score": 1.6677181720733643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 MPP 管道基本不允许我们评估模型对较长句子的接受程度。", "metrics": {"bleu_score": 85.1683409367816, "chrf_score": 81.21989951300296, "xcomet_score": 0.818332314491272, "xcomet_qe_score": 0.7644573450088501, "metricx_score": 1.363376498222351, "metricx_qe_score": 2.9560482501983643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,大型语言模型的上下文窗口越来越长。", "metrics": {"bleu_score": 70.23026466283676, "chrf_score": 74.55459048735162, "xcomet_score": 0.9749592542648315, "xcomet_qe_score": 0.9128842353820801, "metricx_score": 0.4945814609527588, "metricx_qe_score": 0.6217560768127441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们必须在整个上下文窗口中评估模型的可接受性。 这就是我们在这里试图做的事情。", "metrics": {"bleu_score": 55.61740757450153, "chrf_score": 50.54619569703009, "xcomet_score": 0.9065049886703491, "xcomet_qe_score": 0.8831988573074341, "metricx_score": 1.0960769653320312, "metricx_qe_score": 1.300909161567688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视 NPP 管道。 所以", "metrics": {"bleu_score": 49.99148090271511, "chrf_score": 46.1643517579442, "xcomet_score": 0.6137734651565552, "xcomet_qe_score": 0.5333285927772522, "metricx_score": 5.1004533767700195, "metricx_qe_score": 4.7288432121276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9963313341140747, "xcomet_qe_score": 0.9761532545089722, "metricx_score": 0.22746601700782776, "metricx_qe_score": 0.7089755535125732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过重新审视数据集本身来模拟这些更长的序列,然后通过从这些数据集中选择可接受或不可接受的句子来重新构建句子。", "metrics": {"bleu_score": 72.19958558689919, "chrf_score": 66.21706490801719, "xcomet_score": 0.9302459955215454, "xcomet_qe_score": 0.8347460031509399, "metricx_score": 2.0589146614074707, "metricx_qe_score": 2.8587381839752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从BLIMP数据集中选取了一个典型的附词岛案例的语法性对照。 (For reference: - \"So for example\" -> \"例如\" - \"here we have chosen\" -> \"这里我们选取了\" - \"like a typical pair of grammaticality\" -> \"一个典型的语法性对照\" - \"from the blimp data set\" -> \"从BLIMP数据集中\" - \"from the adjunct island case\" -> \"附词岛案例\")", "metrics": {"bleu_score": 7.991985370646969, "chrf_score": 21.5818018607474, "xcomet_score": 0.6646085977554321, "xcomet_qe_score": 0.6463125348091125, "metricx_score": 5.926273345947266, "metricx_qe_score": 5.43252420425415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重现更长的序列,并判断哪些序列可接受,哪些具有相同的语法结构匹配。", "metrics": {"bleu_score": 51.77112223567368, "chrf_score": 44.19214408535607, "xcomet_score": 0.8463236093521118, "xcomet_qe_score": 0.9260823726654053, "metricx_score": 1.6951677799224854, "metricx_qe_score": 2.4906256198883057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从附属于语法的句子中提取语法句子。 然后,我们将它作为前缀添加到可接受的查询和不可接受的查询中。 因此,", "metrics": {"bleu_score": 57.75969603971076, "chrf_score": 54.4393845731656, "xcomet_score": 0.6411268711090088, "xcomet_qe_score": 0.6026320457458496, "metricx_score": 5.966381549835205, "metricx_qe_score": 5.745668888092041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.9529941082000732, "xcomet_qe_score": 0.7428854703903198, "metricx_score": 1.5263457298278809, "metricx_qe_score": 1.8271386623382568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过选择不同子集或不同数据集中的句子来实现同样的效果。", "metrics": {"bleu_score": 46.23472681073812, "chrf_score": 38.1080777276159, "xcomet_score": 0.9873369932174683, "xcomet_qe_score": 0.9559260606765747, "metricx_score": 0.9357630610466003, "metricx_qe_score": 1.5117775201797485, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的错配场景。 因此,", "metrics": {"bleu_score": 60.26080978557135, "chrf_score": 57.58514132353505, "xcomet_score": 0.786383867263794, "xcomet_qe_score": 0.7737975120544434, "metricx_score": 2.4849853515625, "metricx_qe_score": 2.335399866104126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关数据集,但不是您正在评估的同一数据集。", "metrics": {"bleu_score": 58.901155004394425, "chrf_score": 52.82938731275363, "xcomet_score": 0.9553557634353638, "xcomet_qe_score": 0.7772138118743896, "metricx_score": 1.1635935306549072, "metricx_qe_score": 2.1226277351379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于不可接受的情况,我们也可以这样做。", "metrics": {"bleu_score": 46.663612512230074, "chrf_score": 42.696972610224044, "xcomet_score": 0.9821330308914185, "xcomet_qe_score": 0.9639067649841309, "metricx_score": 0.5520765781402588, "metricx_qe_score": 0.599204957485199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从一个完全不相关的领域中选择句子,比如维基百科。", "metrics": {"bleu_score": 58.05800086508495, "chrf_score": 49.1873258490253, "xcomet_score": 0.9944443702697754, "xcomet_qe_score": 0.9337222576141357, "metricx_score": 0.7134301662445068, "metricx_qe_score": 1.2820758819580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将有助于我们判断模型的接受度评定是否实际上受到任何上下文的影响。 例如,上下文是否来自数据集的不同子集,或者是否与我们正在分析的当前句子完全无关。", "metrics": {"bleu_score": 58.72198198315849, "chrf_score": 52.17612047393067, "xcomet_score": 0.9383193254470825, "xcomet_qe_score": 0.9121452569961548, "metricx_score": 1.86470365524292, "metricx_qe_score": 2.529731273651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型表现如何?", "metrics": {"bleu_score": 8.646389260097964, "chrf_score": 8.441013286611183, "xcomet_score": 0.8383227586746216, "xcomet_qe_score": 0.844200611114502, "metricx_score": 0.9827795028686523, "metricx_qe_score": 0.27792325615882874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先我们查看维基百科的句子,这些句子与当前查询对完全无关,在此我们发现MPP的判断对于任意上下文长度大多是稳健的。", "metrics": {"bleu_score": 61.59772601956203, "chrf_score": 51.74132933015224, "xcomet_score": 0.9322047233581543, "xcomet_qe_score": 0.8174335360527039, "metricx_score": 4.19758415222168, "metricx_qe_score": 5.760201454162598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到2024年,以最大限度地发挥OPT和GPT两个模型的潜力,从图", "metrics": {"bleu_score": 37.58626428491305, "chrf_score": 50.03164769167948, "xcomet_score": 0.4233185350894928, "xcomet_qe_score": 0.3986549973487854, "metricx_score": 13.619436264038086, "metricx_qe_score": 14.427735328674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中的橙色虚线可以看到,MPP的判断相对稳定。", "metrics": {"bleu_score": 64.87066897882097, "chrf_score": 59.608721380781304, "xcomet_score": 0.7581294775009155, "xcomet_qe_score": 0.6977362632751465, "metricx_score": 2.2789418697357178, "metricx_qe_score": 4.331311225891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,当我们从同一数据集选择句子时,会发生什么情况?", "metrics": {"bleu_score": 43.28919678216833, "chrf_score": 34.66100902981294, "xcomet_score": 0.9949071407318115, "xcomet_qe_score": 0.9228957891464233, "metricx_score": 0.7160404324531555, "metricx_qe_score": 1.233155369758606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们在这里从相同的BLIMP或SYNTAX GIMP数据集中选择或创建来自可接受和不可接受领域的句子。", "metrics": {"bleu_score": 46.916589697991476, "chrf_score": 38.0243390137645, "xcomet_score": 0.7751094102859497, "xcomet_qe_score": 0.7336798310279846, "metricx_score": 5.54231595993042, "metricx_qe_score": 5.485211372375488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在那里,我们看到当你添加可接受的前缀或不可接受的前缀时,MPP 判断会显著增加或减少。", "metrics": {"bleu_score": 68.26275549173431, "chrf_score": 64.02653074364656, "xcomet_score": 0.7874077558517456, "xcomet_qe_score": 0.7408892512321472, "metricx_score": 3.9632415771484375, "metricx_qe_score": 3.5404233932495117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是当我们匹配结构时,即当我们根据语法选择来自相同现象的责备句子时,吉姆。 (Pinyin: Dànshì dāng wǒmen pīhǎo jiegòu shí, jì dāng wǒmen gēnjué yǔfǎ xuǎnzé láizì tóngyàng xiànxiàng de zébei jùzi shí, Jímu.) 我们观察到模型的MPP判断出现大幅增加或大幅减少,这取决于所选前缀是否可接受。 现在这个现象", "metrics": {"bleu_score": 32.44694011896911, "chrf_score": 28.904877696286107, "xcomet_score": 0.16270264983177185, "xcomet_qe_score": 0.14132215082645416, "metricx_score": 9.416299819946289, "metricx_qe_score": 7.575140953063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常显著,这种影响会随着上下文长度而增加,这很可能影响到那些具有大上下文窗口的新型语言模型。", "metrics": {"bleu_score": 40.36298291022927, "chrf_score": 39.50590768997485, "xcomet_score": 0.8473137021064758, "xcomet_qe_score": 0.7344998717308044, "metricx_score": 1.708153486251831, "metricx_qe_score": 1.7489993572235107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么匹配前缀会对语言模型的判断产生如此大的影响? 因此,", "metrics": {"bleu_score": 81.89850197975073, "chrf_score": 81.67160641054082, "xcomet_score": 0.8309619426727295, "xcomet_qe_score": 0.7695428133010864, "metricx_score": 3.7839598655700684, "metricx_qe_score": 1.712070107460022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保持相关结构但向输入句子添加噪声来扰动输入。", "metrics": {"bleu_score": 57.013204054917615, "chrf_score": 49.34824800262281, "xcomet_score": 0.8622497320175171, "xcomet_qe_score": 0.8829268217086792, "metricx_score": 1.6389180421829224, "metricx_qe_score": 2.481445789337158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了几次这样的扰动后, 我们发现,这些噪声中没有一个实际上会使模型在展示 MPP 判断趋势的方式上改变其路径。", "metrics": {"bleu_score": 34.66666779803195, "chrf_score": 33.42493546675261, "xcomet_score": 0.885474681854248, "xcomet_qe_score": 0.8347205519676208, "metricx_score": 4.085929870605469, "metricx_qe_score": 4.523471832275391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现这些模型对扰动句子的敏感度相似。", "metrics": {"bleu_score": 36.93451318731604, "chrf_score": 33.93878679048405, "xcomet_score": 0.927876353263855, "xcomet_qe_score": 0.9149933457374573, "metricx_score": 1.5993940830230713, "metricx_qe_score": 2.752676010131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在可接受的范围内扰动句子时,我们观察到所有扰动方式下判断的相似增加;而当我们在不可接受的范围内扰动句子时,我们观察到MPP判断以相似的方式减少。", "metrics": {"bleu_score": 40.670445981315495, "chrf_score": 35.86447126940869, "xcomet_score": 0.7933977842330933, "xcomet_qe_score": 0.7813055515289307, "metricx_score": 4.452000617980957, "metricx_qe_score": 4.309943675994873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们工作的关键结论是,语言模型对潜藏的句法和语义特征敏感,这些特征在句子间共享。", "metrics": {"bleu_score": 47.592618504175505, "chrf_score": 40.19433166858002, "xcomet_score": 0.9183430671691895, "xcomet_qe_score": 0.93487948179245, "metricx_score": 1.3291332721710205, "metricx_qe_score": 1.430552363395691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而当前的 MPP 评估方法,通过简短且单一句子的输入,可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 45.15673672371357, "chrf_score": 38.36697466766303, "xcomet_score": 0.9004324674606323, "xcomet_qe_score": 0.8471487760543823, "metricx_score": 1.4461113214492798, "metricx_qe_score": 1.9046796560287476, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解更多实验细节。", "metrics": {"bleu_score": 34.27163657253172, "chrf_score": 33.44303636597666, "xcomet_score": 0.9978342056274414, "xcomet_qe_score": 0.9995092153549194, "metricx_score": 0.10219424962997437, "metricx_qe_score": 0.11421225965023041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7200528383255005, "xcomet_qe_score": 0.8642917275428772, "metricx_score": 0.666434645652771, "metricx_qe_score": 0.8818589448928833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张宇信。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 49.88704001434205, "xcomet_score": 0.9360659122467041, "xcomet_qe_score": 0.9875787496566772, "metricx_score": 0.5195938944816589, "metricx_qe_score": 0.7311459183692932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的研究成果——跨语言语义分析:多种自然语言与语义表示。", "metrics": {"bleu_score": 46.61653434583272, "chrf_score": 34.332928736572896, "xcomet_score": 0.8501730561256409, "xcomet_qe_score": 0.8923048973083496, "metricx_score": 3.496370553970337, "metricx_qe_score": 3.82658052444458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义分析是一项构建用户查询的语义表示的任务,例如 SQL 和 Lambda 计算。", "metrics": {"bleu_score": 57.02524287955773, "chrf_score": 56.87031571095179, "xcomet_score": 0.9655965566635132, "xcomet_qe_score": 0.9569717049598694, "metricx_score": 1.1213756799697876, "metricx_qe_score": 1.6736323833465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义分析的任务是将多种自然语言的查询翻译成多种意义表示。", "metrics": {"bleu_score": 76.8272902167946, "chrf_score": 70.5588845985583, "xcomet_score": 0.8741574287414551, "xcomet_qe_score": 0.931083083152771, "metricx_score": 1.8348667621612549, "metricx_qe_score": 3.8536572456359863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将多种自然语言查询翻译成SQL、Lambda或FunQL等形式。", "metrics": {"bleu_score": 70.73581076101581, "chrf_score": 75.09937150123798, "xcomet_score": 0.9656064510345459, "xcomet_qe_score": 0.9823272228240967, "metricx_score": 0.9328605532646179, "metricx_qe_score": 1.1841901540756226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义分析模型是分别在有限的任务和应用数据集上提出和评估的,", "metrics": {"bleu_score": 66.21516203242497, "chrf_score": 55.46346949197655, "xcomet_score": 0.9950854778289795, "xcomet_qe_score": 0.9732061624526978, "metricx_score": 0.845561146736145, "metricx_qe_score": 1.0657553672790527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如: 对于某些自然语言,有大量的研究和讨论,", "metrics": {"bleu_score": 24.648321974767605, "chrf_score": 26.698181028561052, "xcomet_score": 0.8810124397277832, "xcomet_qe_score": 0.9379785060882568, "metricx_score": 1.6828856468200684, "metricx_qe_score": 0.9955093264579773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但中文却缺席了。 湖泊对某些微代表性的覆盖。 λ演", "metrics": {"bleu_score": 12.962472880491877, "chrf_score": 14.589996871222159, "xcomet_score": 0.4239233434200287, "xcomet_qe_score": 0.1590254306793213, "metricx_score": 10.280037879943848, "metricx_qe_score": 13.128985404968262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "算缺失。 或者它们只被评估在特定的新型模型上。", "metrics": {"bleu_score": 10.580331550093845, "chrf_score": 10.412437182804574, "xcomet_score": 0.458374947309494, "xcomet_qe_score": 0.39469224214553833, "metricx_score": 8.96982479095459, "metricx_qe_score": 10.68260383605957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一的模型来评估它们。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.997307538986206, "xcomet_qe_score": 0.9824987649917603, "metricx_score": 0.5768303871154785, "metricx_qe_score": 0.8717049956321716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了一个示例,即", "metrics": {"bleu_score": 30.576902884505124, "chrf_score": 20.94580464199314, "xcomet_score": 0.5048704147338867, "xcomet_qe_score": 0.15499618649482727, "metricx_score": 3.544226884841919, "metricx_qe_score": 3.0533337593078613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供一个用于多自然语言和语义表示跨语言语义分析的统一数据集示例。", "metrics": {"bleu_score": 30.429315792391975, "chrf_score": 23.835768537201997, "xcomet_score": 0.7034076452255249, "xcomet_qe_score": 0.6158822774887085, "metricx_score": 2.4795453548431396, "metricx_qe_score": 2.3776681423187256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含九个不同领域的数据集,五个语义部分和税收,八个语义表示法,以及来自十五个语言家族的二十二种自然语言。", "metrics": {"bleu_score": 19.10355151162797, "chrf_score": 20.100964302676623, "xcomet_score": 0.690599799156189, "xcomet_qe_score": 0.7291415333747864, "metricx_score": 6.833223342895508, "metricx_qe_score": 7.069653034210205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了训练和评估的六种设置。", "metrics": {"bleu_score": 67.8301759715223, "chrf_score": 59.56205276123286, "xcomet_score": 0.9824798107147217, "xcomet_qe_score": 0.9433248043060303, "metricx_score": 1.0820890665054321, "metricx_qe_score": 2.2754859924316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是TranslateTest。", "metrics": {"bleu_score": 32.58798048281462, "chrf_score": 18.76628675854382, "xcomet_score": 0.9642470479011536, "xcomet_qe_score": 0.953527569770813, "metricx_score": 1.326793909072876, "metricx_qe_score": 1.1972638368606567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google Translate API将源语言翻译成目标语言,然后使用MonolingoModel进行训练和评估。", "metrics": {"bleu_score": 60.43152708300638, "chrf_score": 49.39569125436316, "xcomet_score": 0.8783369064331055, "xcomet_qe_score": 0.9246073961257935, "metricx_score": 3.3400561809539795, "metricx_qe_score": 1.7898777723312378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语查询上训练一个英语模型,在推理阶段,我们使用 API 将德语查询翻译成英语,然后使用训练好的模型预测 SQL。", "metrics": {"bleu_score": 57.686570662644385, "chrf_score": 53.238848396257666, "xcomet_score": 0.8740150928497314, "xcomet_qe_score": 0.7194822430610657, "metricx_score": 1.3567684888839722, "metricx_qe_score": 2.2764687538146973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将测试单语模块。", "metrics": {"bleu_score": 80.70557274927978, "chrf_score": 76.96368446368447, "xcomet_score": 0.8608987927436829, "xcomet_qe_score": 0.837360680103302, "metricx_score": 0.2960849404335022, "metricx_qe_score": 0.4536767601966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 73.6558994084271, "chrf_score": 67.66383822511001, "xcomet_score": 0.9070941209793091, "xcomet_qe_score": 0.8909211158752441, "metricx_score": 0.6414645910263062, "metricx_qe_score": 0.6743262410163879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过仅使用10%的训练数据训练单语模型来测试单语融合设置。", "metrics": {"bleu_score": 50.410744042620145, "chrf_score": 43.20605615680042, "xcomet_score": 0.8097220659255981, "xcomet_qe_score": 0.8067135810852051, "metricx_score": 4.001811504364014, "metricx_qe_score": 3.0639071464538574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试了一种多语言模型,我们为所有语言训练了一个多语言模型。", "metrics": {"bleu_score": 59.10522461071207, "chrf_score": 58.07365574328725, "xcomet_score": 0.9376287460327148, "xcomet_qe_score": 0.886993408203125, "metricx_score": 1.2354217767715454, "metricx_qe_score": 1.9380263090133667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文的查询放在一起,用于训练多语言模型。而", "metrics": {"bleu_score": 53.43727450411229, "chrf_score": 47.18008962108448, "xcomet_score": 0.8456935882568359, "xcomet_qe_score": 0.7602370977401733, "metricx_score": 2.049597978591919, "metricx_qe_score": 1.8342804908752441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理阶段,我们可以使用这个模型来... 将德语查询、中文查询等翻译成相应语言。", "metrics": {"bleu_score": 45.659769509648925, "chrf_score": 46.49103527379131, "xcomet_score": 0.9493553638458252, "xcomet_qe_score": 0.896613597869873, "metricx_score": 2.0915651321411133, "metricx_qe_score": 3.106045961380005, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑跨语言零样本和领域样本转移,", "metrics": {"bleu_score": 56.85488870277223, "chrf_score": 46.16793208511475, "xcomet_score": 0.6717430353164673, "xcomet_qe_score": 0.7132660746574402, "metricx_score": 4.309206485748291, "metricx_qe_score": 5.519230365753174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是在一种源语言上运行并在另一种语言上进行转移。", "metrics": {"bleu_score": 22.17134184444322, "chrf_score": 18.892844889203094, "xcomet_score": 0.7001899480819702, "xcomet_qe_score": 0.7149946689605713, "metricx_score": 5.146336555480957, "metricx_qe_score": 5.817258358001709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我将使用英语查询或英语和德语融合查询的组合来训练一个多语言模型,以预测SQL输出。", "metrics": {"bleu_score": 62.67959864818924, "chrf_score": 55.9315253604868, "xcomet_score": 0.743032693862915, "xcomet_qe_score": 0.7369014024734497, "metricx_score": 1.8724457025527954, "metricx_qe_score": 2.4553592205047607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也发现了许多有趣的结果。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 66.57370407370408, "xcomet_score": 0.9968277215957642, "xcomet_qe_score": 0.9793797731399536, "metricx_score": 0.3356071412563324, "metricx_qe_score": 0.77958083152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在对单语模型的分析中,我们评估了两组模型。 包括编码器PDR,即多语种预训练编码器与基于指针解码器的解码器,例如XLMR加PDR和BERT加PDR。", "metrics": {"bleu_score": 38.86997364915031, "chrf_score": 31.855917288632135, "xcomet_score": 0.5242658853530884, "xcomet_qe_score": 0.5410950779914856, "metricx_score": 4.374950408935547, "metricx_qe_score": 4.144724369049072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语种预训练的编码器-解码器模型,例如MBART和MT5。", "metrics": {"bleu_score": 23.5269071488911, "chrf_score": 18.845738240328572, "xcomet_score": 0.9015953540802002, "xcomet_qe_score": 0.9282745122909546, "metricx_score": 1.3690167665481567, "metricx_qe_score": 2.3785440921783447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,编码器-解码器在所有九个数据集上都取得了最佳性能。", "metrics": {"bleu_score": 46.51113009711743, "chrf_score": 32.265718721775706, "xcomet_score": 0.9894335269927979, "xcomet_qe_score": 0.9838986396789551, "metricx_score": 1.5358535051345825, "metricx_qe_score": 1.3199154138565063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在机器翻译(MT)五种语言和XLMR加上PDR多语种环境中进行评估。", "metrics": {"bleu_score": 19.211741763103568, "chrf_score": 24.388916471191596, "xcomet_score": 0.7789380550384521, "xcomet_qe_score": 0.8288552761077881, "metricx_score": 4.7500481605529785, "metricx_qe_score": 2.9635932445526123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在多种语言的混合体上进行训练,可以提升编码器-解码器或编码器PDR的表现。", "metrics": {"bleu_score": 16.701179170132647, "chrf_score": 14.199058911951903, "xcomet_score": 0.7711186408996582, "xcomet_qe_score": 0.7704715728759766, "metricx_score": 2.968472957611084, "metricx_qe_score": 3.335819721221924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,但英语在七个数据集中的性能有所下降,只在三个数据集中有所提升。", "metrics": {"bleu_score": 64.4864739406669, "chrf_score": 57.31976289649068, "xcomet_score": 0.9739780426025391, "xcomet_qe_score": 0.9897005558013916, "metricx_score": 2.850618600845337, "metricx_qe_score": 1.839304804801941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言性的诅咒。", "metrics": {"bleu_score": 22.311916633179763, "chrf_score": 21.09468296820821, "xcomet_score": 0.9170486330986023, "xcomet_qe_score": 0.9593082666397095, "metricx_score": 1.4766305685043335, "metricx_qe_score": 1.9277926683425903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言表现的差异。", "metrics": {"bleu_score": 63.39704064341255, "chrf_score": 57.81995781995782, "xcomet_score": 0.963089108467102, "xcomet_qe_score": 0.8401018381118774, "metricx_score": 0.30516117811203003, "metricx_qe_score": 0.5586168169975281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝色线条表示跨语言燃料快照传输,", "metrics": {"bleu_score": 10.062635309001745, "chrf_score": 12.716216908627775, "xcomet_score": 0.7282189130783081, "xcomet_qe_score": 0.6890683174133301, "metricx_score": 6.2411088943481445, "metricx_qe_score": 5.394316673278809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙色线条表示跨语言零快照传输,", "metrics": {"bleu_score": 18.20705281109213, "chrf_score": 17.129036728636954, "xcomet_score": 0.8380597829818726, "xcomet_qe_score": 0.8179804682731628, "metricx_score": 3.8138411045074463, "metricx_qe_score": 3.0202088356018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿色线条则是单语言设置。", "metrics": {"bleu_score": 20.78060434846712, "chrf_score": 27.921704087507194, "xcomet_score": 0.8578140735626221, "xcomet_qe_score": 0.916854739189148, "metricx_score": 0.592099666595459, "metricx_qe_score": 0.7065149545669556, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过比较绿色和橙色曲线,我们发现在零短设置下,跨语言迁移性能的差距显著。通过比较蓝色和橙色曲线,我们发现在少数短设置下,迁移差距迅速缩小。", "metrics": {"bleu_score": 41.68872944680718, "chrf_score": 35.58707644343879, "xcomet_score": 0.5881156325340271, "xcomet_qe_score": 0.6046151518821716, "metricx_score": 7.5791730880737305, "metricx_qe_score": 5.8986053466796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器解码器模型的表现优于先前的工作,或取得了可比的结果。", "metrics": {"bleu_score": 22.008909554802088, "chrf_score": 14.707536615597405, "xcomet_score": 0.9686832427978516, "xcomet_qe_score": 0.9642295837402344, "metricx_score": 1.2038826942443848, "metricx_qe_score": 1.3444138765335083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行购买可以显著提升目标自然语言上的快速拍摄性能。 我们发现,像Codice和Bloom这样的多语言模型在跨语言语义分析任务中仍然不足。", "metrics": {"bleu_score": 47.06291275335124, "chrf_score": 37.282700467342565, "xcomet_score": 0.4819396436214447, "xcomet_qe_score": 0.41495901346206665, "metricx_score": 11.582998275756836, "metricx_qe_score": 11.451729774475098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们构建了Exempler,一个用于多自然语言和微表示的跨角度语义解析的统一基准。", "metrics": {"bleu_score": 40.73586637590299, "chrf_score": 31.352993407854846, "xcomet_score": 0.6795446276664734, "xcomet_qe_score": 0.712860107421875, "metricx_score": 4.463888645172119, "metricx_qe_score": 4.754374027252197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语种语言模型进行了全面的基准研究,", "metrics": {"bleu_score": 70.22108284536812, "chrf_score": 59.07630205043998, "xcomet_score": 0.9754852056503296, "xcomet_qe_score": 0.9654116630554199, "metricx_score": 1.249304175376892, "metricx_qe_score": 1.9747228622436523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果显示了许多有趣的发现等", "metrics": {"bleu_score": 72.41907707454052, "chrf_score": 73.18090547857871, "xcomet_score": 0.85756516456604, "xcomet_qe_score": 0.7907236814498901, "metricx_score": 1.9979556798934937, "metricx_qe_score": 1.3959616422653198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您的聆听。", "metrics": {"bleu_score": 26.269098944241588, "chrf_score": 29.813794912142672, "xcomet_score": 0.990349292755127, "xcomet_qe_score": 0.9856703877449036, "metricx_score": 0.2438577115535736, "metricx_qe_score": 0.5866713523864746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Aid Vilar,我将简要介绍一篇论文《促进PowerPoint翻译,评估策略与表现》。", "metrics": {"bleu_score": 16.003200968060657, "chrf_score": 22.971245433646434, "xcomet_score": 0.644817054271698, "xcomet_qe_score": 0.6682935953140259, "metricx_score": 9.5313138961792, "metricx_qe_score": 9.221322059631348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与Google Translate的同事们共同合作的成果。 (注意:由于中文的表达习惯,最后一句可以更自然地翻译为“这是我和Google Translate的同事们的合作成果。”) 但是,为了严格遵循您的指导原则,我提供了更直接的翻译。", "metrics": {"bleu_score": 4.626980694500636, "chrf_score": 10.540681702038528, "xcomet_score": 0.8751351833343506, "xcomet_qe_score": 0.9255399703979492, "metricx_score": 2.822476387023926, "metricx_qe_score": 3.019562244415283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Parm 是一个于2022年推出的、拥有5400亿参数的语言学习模型。", "metrics": {"bleu_score": 35.370700930889804, "chrf_score": 40.69665734138369, "xcomet_score": 0.8662339448928833, "xcomet_qe_score": 0.8570343852043152, "metricx_score": 4.410917282104492, "metricx_qe_score": 4.993619441986084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在一个包含7800亿个标记的庞大集合上进行了训练。", "metrics": {"bleu_score": 21.08445456905128, "chrf_score": 31.513914080592365, "xcomet_score": 0.676857054233551, "xcomet_qe_score": 0.7229318618774414, "metricx_score": 2.3403120040893555, "metricx_qe_score": 2.8980953693389893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在出版时,它已在数百项自然语言处理任务中达到了业界最先进水平。", "metrics": {"bleu_score": 8.736820732355158, "chrf_score": 11.020436792947963, "xcomet_score": 0.9958581924438477, "xcomet_qe_score": 1.0, "metricx_score": 1.2290607690811157, "metricx_qe_score": 0.8504952192306519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们呈现了机器翻译中Latch语言模型提示的首个系统性研究。", "metrics": {"bleu_score": 51.08221523620023, "chrf_score": 42.08484641826286, "xcomet_score": 0.7678297758102417, "xcomet_qe_score": 0.7579187154769897, "metricx_score": 4.878890037536621, "metricx_qe_score": 4.95088529586792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用AMT社区的最佳实践来评估此类模型的转换能力。", "metrics": {"bleu_score": 59.12941670465457, "chrf_score": 53.12132483734429, "xcomet_score": 0.8877519369125366, "xcomet_qe_score": 0.8672211170196533, "metricx_score": 2.4325268268585205, "metricx_qe_score": 4.282456398010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,即WMT评估中表现最佳的系统。", "metrics": {"bleu_score": 40.66139639256472, "chrf_score": 38.166460961997224, "xcomet_score": 0.9181811809539795, "xcomet_qe_score": 0.8544657826423645, "metricx_score": 2.662759780883789, "metricx_qe_score": 3.9063899517059326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用最先进的新型语言模型评估指标(LMT),并且还展示了基于专家的人类评估结果。", "metrics": {"bleu_score": 40.00177797533498, "chrf_score": 38.24269952443684, "xcomet_score": 0.9382200241088867, "xcomet_qe_score": 0.9342364072799683, "metricx_score": 1.994716763496399, "metricx_qe_score": 1.837126612663269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们为提示选择策略提供了一些建议。", "metrics": {"bleu_score": 48.19625108382573, "chrf_score": 39.53497503148685, "xcomet_score": 0.8870682716369629, "xcomet_qe_score": 0.8240371346473694, "metricx_score": 0.7654174566268921, "metricx_qe_score": 2.6937146186828613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型(LLM)的翻译性能有很大影响,这可以在一个简单的实验中观察到。在这个实验中,我们使用一个简短的提示,并为同一句话提供两个不同的提示。", "metrics": {"bleu_score": 40.21551630366277, "chrf_score": 42.29221482511873, "xcomet_score": 0.8959325551986694, "xcomet_qe_score": 0.8972920775413513, "metricx_score": 1.3832005262374878, "metricx_qe_score": 1.7703652381896973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在一千个句子中,", "metrics": {"bleu_score": 20.612390921238426, "chrf_score": 12.522180682216554, "xcomet_score": 0.8912525177001953, "xcomet_qe_score": 0.6963303089141846, "metricx_score": 7.990420341491699, "metricx_qe_score": 10.199912071228027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子(五百十六个)所观察到的差异超过一个模糊点。", "metrics": {"bleu_score": 3.652945772536268, "chrf_score": 5.290683083100816, "xcomet_score": 0.5975909233093262, "xcomet_qe_score": 0.40703198313713074, "metricx_score": 8.839544296264648, "metricx_qe_score": 7.330403804779053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可以达到40个模糊点。", "metrics": {"bleu_score": 32.22538601891171, "chrf_score": 23.148500068613558, "xcomet_score": 0.8078522086143494, "xcomet_qe_score": 0.8129090070724487, "metricx_score": 4.80564546585083, "metricx_qe_score": 2.5721869468688965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个良好的提示策略非常重要。", "metrics": {"bleu_score": 47.20758038942709, "chrf_score": 41.584577649209166, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2462974488735199, "metricx_qe_score": 0.3821769654750824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了一种五次射击提示策略,即我们只是用所提供的句子标记系统中的语言。", "metrics": {"bleu_score": 25.46970774171891, "chrf_score": 24.116947368507205, "xcomet_score": 0.6821898221969604, "xcomet_qe_score": 0.6954290866851807, "metricx_score": 6.336838245391846, "metricx_qe_score": 5.84807014465332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们从德语翻译成英语。德语句子,即源句子,用德语冒号标记,英语翻译用英语冒号表示。", "metrics": {"bleu_score": 44.291145153127005, "chrf_score": 31.2857051163276, "xcomet_score": 0.9778658151626587, "xcomet_qe_score": 0.9775279760360718, "metricx_score": 1.2390029430389404, "metricx_qe_score": 1.101564884185791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次短提示的情况下,提示的实际形式对结果影响不大。", "metrics": {"bleu_score": 34.02858386912342, "chrf_score": 28.56405814722401, "xcomet_score": 0.9029994010925293, "xcomet_qe_score": 0.8914726376533508, "metricx_score": 0.9320433139801025, "metricx_qe_score": 0.707047700881958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零次和一次提示非常关键,但当我们像我们案例", "metrics": {"bleu_score": 8.299711394278342, "chrf_score": 8.55975150083896, "xcomet_score": 0.34782683849334717, "xcomet_qe_score": 0.40981853008270264, "metricx_score": 7.849034309387207, "metricx_qe_score": 5.5494465827941895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那样进行五次提示时,实际提示形式几乎没有变化。", "metrics": {"bleu_score": 29.176300840900787, "chrf_score": 27.111063215477436, "xcomet_score": 0.9395493865013123, "xcomet_qe_score": 0.808414101600647, "metricx_score": 1.9200923442840576, "metricx_qe_score": 2.512885808944702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正是这些例子承载了大部分的分量。", "metrics": {"bleu_score": 3.8275613602956104, "chrf_score": 6.944444444444445, "xcomet_score": 0.7745562791824341, "xcomet_qe_score": 0.9063411951065063, "metricx_score": 2.1001501083374023, "metricx_qe_score": 1.6453888416290283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,例子质量比与源句子的相似度更为重要。", "metrics": {"bleu_score": 67.59482608831081, "chrf_score": 62.55225853734873, "xcomet_score": 0.9265334606170654, "xcomet_qe_score": 0.9221253395080566, "metricx_score": 0.9351335763931274, "metricx_qe_score": 0.6856894493103027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,重要的是从高质量的翻译中选择例子。", "metrics": {"bleu_score": 53.95713951720204, "chrf_score": 49.05816420135306, "xcomet_score": 0.9366625547409058, "xcomet_qe_score": 0.9372061491012573, "metricx_score": 0.5782352685928345, "metricx_qe_score": 0.734567403793335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较从 WMT 评估的训练数据或开发数据中选择的提示。 深度数据比训练", "metrics": {"bleu_score": 44.89965421966534, "chrf_score": 41.01257616921066, "xcomet_score": 0.5318814516067505, "xcomet_qe_score": 0.3079878091812134, "metricx_score": 7.642877578735352, "metricx_qe_score": 7.496059417724609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据更精心整理,质量更高,可以说更丰富,且实验结果表明", "metrics": {"bleu_score": 8.085298080223225, "chrf_score": 8.786324786324785, "xcomet_score": 0.20100530982017517, "xcomet_qe_score": 0.15515385568141937, "metricx_score": 6.790046691894531, "metricx_qe_score": 6.9037885665893555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在使用深度数据时表现更佳。", "metrics": {"bleu_score": 17.96602518105432, "chrf_score": 18.01521076795669, "xcomet_score": 0.8526694774627686, "xcomet_qe_score": 0.8554571270942688, "metricx_score": 1.8426345586776733, "metricx_qe_score": 2.709596872329712, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最先进的专业系统在翻译质量上具有显著优势,", "metrics": {"bleu_score": 9.328696029459573, "chrf_score": 12.446808569914133, "xcomet_score": 0.8367324471473694, "xcomet_qe_score": 0.8418415188789368, "metricx_score": 5.215333461761475, "metricx_qe_score": 4.268828392028809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但PALM翻译已经接近商业翻译系统的水准。", "metrics": {"bleu_score": 19.209534151258666, "chrf_score": 20.407190570281966, "xcomet_score": 0.7448108792304993, "xcomet_qe_score": 0.8130875825881958, "metricx_score": 5.992020130157471, "metricx_qe_score": 1.3911598920822144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择了与Google翻译进行对比。", "metrics": {"bleu_score": 47.10605077025442, "chrf_score": 35.96396247788778, "xcomet_score": 0.9503545761108398, "xcomet_qe_score": 0.9273895621299744, "metricx_score": 1.78947913646698, "metricx_qe_score": 0.6219265460968018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用MQM框架进行的人工创新所获得的见解是,PALM的流畅度可与现有最先进的系统相媲美,但主要差异源于准确度。", "metrics": {"bleu_score": 52.25305053999486, "chrf_score": 48.813570511936746, "xcomet_score": 0.7524115443229675, "xcomet_qe_score": 0.7397869229316711, "metricx_score": 3.923938751220703, "metricx_qe_score": 4.233322620391846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其常见的一种错误是遗漏错误。", "metrics": {"bleu_score": 57.02822264405544, "chrf_score": 46.983073350670544, "xcomet_score": 0.9041687250137329, "xcomet_qe_score": 0.9158245325088501, "metricx_score": 1.8269771337509155, "metricx_qe_score": 0.9628874063491821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,看来 Palm 有时会选择省略源句中某些部分,以产生听起来更好的翻译。", "metrics": {"bleu_score": 23.51587093567079, "chrf_score": 22.331018666068722, "xcomet_score": 0.9542973637580872, "xcomet_qe_score": 0.90953528881073, "metricx_score": 3.4336588382720947, "metricx_qe_score": 3.575596570968628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAN的外在风格类别低于最先进系统的水平,这是一个额外的信号。 那部分提供了非常流利的输出,但仍然存在一些准确性的问题。", "metrics": {"bleu_score": 46.571296156253986, "chrf_score": 37.289096576606426, "xcomet_score": 0.6829402446746826, "xcomet_qe_score": 0.645561933517456, "metricx_score": 6.617029190063477, "metricx_qe_score": 7.186356067657471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这次非常简短的概述。", "metrics": {"bleu_score": 15.84213253833617, "chrf_score": 17.640374328272415, "xcomet_score": 0.9207900166511536, "xcomet_qe_score": 0.8659988045692444, "metricx_score": 0.5585209131240845, "metricx_qe_score": 0.9517507553100586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欲了解更多详细信息,请参加论文的完整报告。", "metrics": {"bleu_score": 44.96195695057615, "chrf_score": 38.61028976953651, "xcomet_score": 0.7896958589553833, "xcomet_qe_score": 0.8298498392105103, "metricx_score": 3.1444151401519775, "metricx_qe_score": 2.0433146953582764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9730953574180603, "xcomet_qe_score": 0.9623823165893555, "metricx_score": 0.18832087516784668, "metricx_qe_score": 0.09758952260017395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Dawe,德国扎兰特大学的一名博士生。", "metrics": {"bleu_score": 18.982807051178202, "chrf_score": 27.847731553857713, "xcomet_score": 0.7249560952186584, "xcomet_qe_score": 0.7858709096908569, "metricx_score": 1.8753305673599243, "metricx_qe_score": 3.3949356079101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这段视频中,我想向大家介绍我们最近的研究成果——《比你想象的更脆弱》,这是一项对每周供应学习的批判性研究。 (注意:\"Weaker Than You Think\" 翻译为《比你想象的更脆弱》,假设这是研究报告或论文的标题。\"Weekly supplied learning\" 翻译为“每周供应学习”,这个短语可能需要根据上下文进行更准确的翻译或解释,这里保持原样以符合学术材料的翻译要求。)", "metrics": {"bleu_score": 11.44473410587572, "chrf_score": 21.219084902237594, "xcomet_score": 0.6622182130813599, "xcomet_qe_score": 0.6629564762115479, "metricx_score": 4.348804473876953, "metricx_qe_score": 4.161144256591797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与徐小雨、马里奥·穆斯巴赫、加斯·斯特芬以及迪特里希·克拉克夫合作的成果。", "metrics": {"bleu_score": 3.2674392554094007, "chrf_score": 3.0345374987575267, "xcomet_score": 0.6037291288375854, "xcomet_qe_score": 0.6608915328979492, "metricx_score": 2.263770580291748, "metricx_qe_score": 2.6419925689697266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想从简要介绍周监督和每周监督学习开始。", "metrics": {"bleu_score": 30.14335251508215, "chrf_score": 25.778376335850155, "xcomet_score": 0.7016862630844116, "xcomet_qe_score": 0.6498568654060364, "metricx_score": 5.808145046234131, "metricx_qe_score": 5.663590431213379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,我们不进行手动数据标注。", "metrics": {"bleu_score": 17.92334464048543, "chrf_score": 19.662306738074115, "xcomet_score": 0.8782649040222168, "xcomet_qe_score": 0.8709974884986877, "metricx_score": 2.2742769718170166, "metricx_qe_score": 2.2548582553863525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注来源对数据进行标注,例如简单的启发式规则、知识库或本地代码来源,如右图所示。", "metrics": {"bleu_score": 52.590044935325395, "chrf_score": 48.00622514275858, "xcomet_score": 0.7799475193023682, "xcomet_qe_score": 0.6671611666679382, "metricx_score": 3.204390525817871, "metricx_qe_score": 5.336605548858643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,较弱的标注成本更低,但它们也存在噪声,即标注中存在一定数量的错误。", "metrics": {"bleu_score": 29.695622803140342, "chrf_score": 25.07383214614131, "xcomet_score": 0.9024257659912109, "xcomet_qe_score": 0.8579471111297607, "metricx_score": 2.1339709758758545, "metricx_qe_score": 2.3183634281158447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在每周标签数据上训练神经网络,神经网络往往会记住标签噪声,而无法泛化。", "metrics": {"bleu_score": 47.296533519173934, "chrf_score": 40.45740078845589, "xcomet_score": 0.834560751914978, "xcomet_qe_score": 0.7578858137130737, "metricx_score": 5.316686153411865, "metricx_qe_score": 5.939378261566162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督学习中,提出训练算法以在这种水平噪声下稳健地训练神经网络,使训练后的模型仍能很好地泛化。", "metrics": {"bleu_score": 45.25553863016646, "chrf_score": 40.41196988948575, "xcomet_score": 0.8250094652175903, "xcomet_qe_score": 0.7507466077804565, "metricx_score": 4.915022373199463, "metricx_qe_score": 5.68152379989624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近关于WSL(即每周监督学习)的研究中,一个常见的说法是,研究人员声称他们仅在每周标签数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 28.348239480651365, "chrf_score": 27.92346977974815, "xcomet_score": 0.6817444562911987, "xcomet_qe_score": 0.6966833472251892, "metricx_score": 6.511859893798828, "metricx_qe_score": 6.43461799621582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并不错误,但存在一个前提。 人们通常假设存在一个额外的清洗后的验证集,可用于模型选择。", "metrics": {"bleu_score": 34.44624533587692, "chrf_score": 30.06821181987024, "xcomet_score": 0.9150850176811218, "xcomet_qe_score": 0.9121098518371582, "metricx_score": 2.717517614364624, "metricx_qe_score": 2.775941848754883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这个问题设定持怀疑态度,因为这意味着每周的监督学习需要额外的手动标注", "metrics": {"bleu_score": 39.228274087771815, "chrf_score": 32.387975003294414, "xcomet_score": 0.6687651872634888, "xcomet_qe_score": 0.6256341934204102, "metricx_score": 5.8102264404296875, "metricx_qe_score": 6.256087303161621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但像房间里的象一样,这个必要性经常被忽视。", "metrics": {"bleu_score": 38.866243257317585, "chrf_score": 31.543988478183127, "xcomet_score": 0.9046014547348022, "xcomet_qe_score": 0.7961480617523193, "metricx_score": 3.9081945419311523, "metricx_qe_score": 5.481107711791992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述疑问引导我们提出三个研究问题。", "metrics": {"bleu_score": 63.019085559238604, "chrf_score": 57.62131995937635, "xcomet_score": 0.9585031270980835, "xcomet_qe_score": 0.959296464920044, "metricx_score": 1.055587649345398, "metricx_qe_score": 1.0765941143035889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于 WSL,是否必须使用干净的验证数据?或者我们是否可以使用一个含有噪声的验证集?", "metrics": {"bleu_score": 32.88503327935481, "chrf_score": 33.14075341191815, "xcomet_score": 0.9646027088165283, "xcomet_qe_score": 0.9473650455474854, "metricx_score": 1.8443509340286255, "metricx_qe_score": 3.0863842964172363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净的数据,或者干净的数据是 WSL 运行的必要条件,那么我们需要多少干净样本?", "metrics": {"bleu_score": 46.2090861860365, "chrf_score": 40.05201522939274, "xcomet_score": 0.97630774974823, "xcomet_qe_score": 0.9192313551902771, "metricx_score": 0.6779119968414307, "metricx_qe_score": 0.9356286525726318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该仅使用干净样本进行验证,还是有更好的利用方法?", "metrics": {"bleu_score": 44.08798736760118, "chrf_score": 36.40971024473759, "xcomet_score": 0.983583927154541, "xcomet_qe_score": 0.9168035984039307, "metricx_score": 0.6342548131942749, "metricx_qe_score": 0.998959481716156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中探讨了这些研究问题,以下是我们的研究发现:", "metrics": {"bleu_score": 45.853535638200725, "chrf_score": 40.70614211084436, "xcomet_score": 0.9780222177505493, "xcomet_qe_score": 0.9863231182098389, "metricx_score": 0.8950133919715881, "metricx_qe_score": 0.4356689155101776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,有趣的是,最近的 WSL 方法确实需要干净的白色破折号样本才能正常工作。", "metrics": {"bleu_score": 56.28332122784662, "chrf_score": 52.164136622953926, "xcomet_score": 0.7897689938545227, "xcomet_qe_score": 0.8051972985267639, "metricx_score": 3.253685712814331, "metricx_qe_score": 3.4716732501983643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降,", "metrics": {"bleu_score": 63.15552371794039, "chrf_score": 55.594035594035596, "xcomet_score": 0.9874362945556641, "xcomet_qe_score": 0.9928046464920044, "metricx_score": 0.4509727358818054, "metricx_qe_score": 0.7942342758178711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示。如果没有干净的验证样本,那么训练好的模型就无法超越原始的弱标签进行泛化。 这意味着训练是无意义的。", "metrics": {"bleu_score": 72.01295008796, "chrf_score": 67.75226924877373, "xcomet_score": 0.9576215744018555, "xcomet_qe_score": 0.885596513748169, "metricx_score": 1.991733431816101, "metricx_qe_score": 2.6983284950256348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL(弱监督学习)方法实际上需要干净的标签数据才能正常工作,获取干净验证样本的标注成本不容忽视。", "metrics": {"bleu_score": 44.85667534596012, "chrf_score": 43.58015341684609, "xcomet_score": 0.8459799289703369, "xcomet_qe_score": 0.8494387269020081, "metricx_score": 2.779291868209839, "metricx_qe_score": 2.9038851261138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于 WSL 方法实现更好的性能,如左侧图表所示。", "metrics": {"bleu_score": 69.57312394009541, "chrf_score": 68.76147743439603, "xcomet_score": 0.9167754650115967, "xcomet_qe_score": 0.8955472111701965, "metricx_score": 3.735440731048584, "metricx_qe_score": 4.775913715362549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常情况下,我们每类只需要二十个样本就能达到高性能。", "metrics": {"bleu_score": 11.126986567815846, "chrf_score": 12.155442693155328, "xcomet_score": 0.9312830567359924, "xcomet_qe_score": 0.9535824060440063, "metricx_score": 1.719812035560608, "metricx_qe_score": 1.819339394569397, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是故事并未结束,因为如果我们最终决定获取干净的样本,那么直接在这些样本上进行训练甚至能达到更好的性能。 红", "metrics": {"bleu_score": 28.568913735792773, "chrf_score": 24.905738537115674, "xcomet_score": 0.8640604019165039, "xcomet_qe_score": 0.8099573850631714, "metricx_score": 4.852996349334717, "metricx_qe_score": 4.115170478820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "色图示展示了精调方法(直接应用于清洁数据)和 WSL 方法(仅使用清洁数据进行验证)之间的性能差异。", "metrics": {"bleu_score": 31.60168140283987, "chrf_score": 30.75974882837057, "xcomet_score": 0.6284815073013306, "xcomet_qe_score": 0.6370117664337158, "metricx_score": 3.9296047687530518, "metricx_qe_score": 4.370747089385986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每类有十个样本,直接微调(fine tuning)开始超越 WSL(Weakly Supervised Learning,弱监督学习)方法。", "metrics": {"bleu_score": 29.522642737030775, "chrf_score": 26.63291960619054, "xcomet_score": 0.9121558666229248, "xcomet_qe_score": 0.899333655834198, "metricx_score": 2.5791711807250977, "metricx_qe_score": 2.5741004943847656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,通过允许在干净的验证样本上继续微调,可以轻松实现之前 WSL 方法中声称的性能提升。", "metrics": {"bleu_score": 50.167540756339584, "chrf_score": 44.80821183984229, "xcomet_score": 0.9739274978637695, "xcomet_qe_score": 0.8915089964866638, "metricx_score": 2.4012656211853027, "metricx_qe_score": 3.5959229469299316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从图中可以看出,名为FTW的Marlina模型最初在性能上落后于更复杂的WSL方法,如余弦相似度。", "metrics": {"bleu_score": 26.407646704574947, "chrf_score": 26.48174861646574, "xcomet_score": 0.7737789750099182, "xcomet_qe_score": 0.7109741568565369, "metricx_score": 6.77335262298584, "metricx_qe_score": 7.4650468826293945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果我们允许在干净样本上继续微调,那么FTW的表现与其它方法同样出色。", "metrics": {"bleu_score": 33.5913359034314, "chrf_score": 30.72029544952976, "xcomet_score": 0.8955633640289307, "xcomet_qe_score": 0.8140667676925659, "metricx_score": 1.4993793964385986, "metricx_qe_score": 2.339570999145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在实际应用中,没有必要选择更复杂的WSL方法,因为这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 41.03736083870628, "chrf_score": 44.514984435219716, "xcomet_score": 0.96368408203125, "xcomet_qe_score": 0.9650728702545166, "metricx_score": 0.5895696878433228, "metricx_qe_score": 1.1893458366394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们证明了最新的 WSL 方法需要干净、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 51.60498167475882, "chrf_score": 49.31787558515153, "xcomet_score": 0.9718484878540039, "xcomet_qe_score": 0.9317837953567505, "metricx_score": 2.3103976249694824, "metricx_qe_score": 3.139655590057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实际应用价值被严重高估了。", "metrics": {"bleu_score": 29.715678881302644, "chrf_score": 29.996926363508848, "xcomet_score": 0.9975954294204712, "xcomet_qe_score": 0.9983798265457153, "metricx_score": 0.6434529423713684, "metricx_qe_score": 0.6953849196434021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否使用干净的验证样本进行。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.34004037322114, "xcomet_score": 0.9486993551254272, "xcomet_qe_score": 0.8667909502983093, "metricx_score": 1.3272098302841187, "metricx_qe_score": 2.1758956909179688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL 方法应与未来的着陆基线进行比较,因为两者都对网格样本进行处理。", "metrics": {"bleu_score": 30.531837319331665, "chrf_score": 28.03911746926453, "xcomet_score": 0.6299880743026733, "xcomet_qe_score": 0.6270244121551514, "metricx_score": 7.323049068450928, "metricx_qe_score": 7.846328258514404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一种简单而强大的基线,应在未来的 WSL 研究中得到考虑。", "metrics": {"bleu_score": 43.55511211473948, "chrf_score": 37.1845274033367, "xcomet_score": 0.8828672170639038, "xcomet_qe_score": 0.8139642477035522, "metricx_score": 1.3981051445007324, "metricx_qe_score": 2.7690672874450684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过本幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9957367181777954, "xcomet_qe_score": 0.9900866746902466, "metricx_score": 0.461531400680542, "metricx_qe_score": 0.4078834354877472, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,并欢迎参加会议。", "metrics": {"bleu_score": 7.431878014503621, "chrf_score": 6.727824778118026, "xcomet_score": 0.6268501877784729, "xcomet_qe_score": 0.8379074931144714, "metricx_score": 2.0994889736175537, "metricx_qe_score": 1.1691911220550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9827327728271484, "xcomet_qe_score": 1.0, "metricx_score": 0.8756831288337708, "metricx_qe_score": 0.3633490204811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCEval,一种评估对话人工智能的新维度方法。", "metrics": {"bleu_score": 23.44732872048571, "chrf_score": 27.012468720988103, "xcomet_score": 0.8492453098297119, "xcomet_qe_score": 0.9110332727432251, "metricx_score": 1.2411882877349854, "metricx_qe_score": 1.1905155181884766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学的人工智能自然语言处理实验室完成,该实验室由埃默里大学的崔吉诺教授领导,并与亚马逊Alexa AI合作。", "metrics": {"bleu_score": 22.794976708458712, "chrf_score": 28.453582663176196, "xcomet_score": 0.7786346673965454, "xcomet_qe_score": 0.8015893697738647, "metricx_score": 2.652972459793091, "metricx_qe_score": 2.4163239002227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚开发了一个对话模型,你想看看它与当前最先进的技术相比表现如何。", "metrics": {"bleu_score": 75.16216924719087, "chrf_score": 64.49628711487166, "xcomet_score": 0.9982374906539917, "xcomet_qe_score": 0.9885433912277222, "metricx_score": 0.5928300619125366, "metricx_qe_score": 0.641268253326416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如请人工评判员从两个对话中选择哪个更好,或者在给定的液体尺度上对对话进行评分。", "metrics": {"bleu_score": 46.182136248504804, "chrf_score": 40.68899592631834, "xcomet_score": 0.755558967590332, "xcomet_qe_score": 0.7490009069442749, "metricx_score": 6.2508158683776855, "metricx_qe_score": 7.169308185577393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法可以很好地提供整体对话质量的评估,但对话质量有多个方面。", "metrics": {"bleu_score": 59.490969034436894, "chrf_score": 50.44106832228087, "xcomet_score": 0.947091817855835, "xcomet_qe_score": 0.9835519790649414, "metricx_score": 0.40492382645606995, "metricx_qe_score": 0.5791277885437012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能需要评估聊天质量的多个维度,以更细致地了解模型的优缺点。", "metrics": {"bleu_score": 77.07975858143212, "chrf_score": 72.77552982504822, "xcomet_score": 0.9787784814834595, "xcomet_qe_score": 0.9572713375091553, "metricx_score": 0.5929165482521057, "metricx_qe_score": 0.5868556499481201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是直接请人类评判员评估对话质量的多个维度,例如模型响应的相关性,使用现有的比较或利克特量表方法。", "metrics": {"bleu_score": 45.6775643147147, "chrf_score": 37.87410520421583, "xcomet_score": 0.953730583190918, "xcomet_qe_score": 0.9050771594047546, "metricx_score": 1.372954249382019, "metricx_qe_score": 2.0074007511138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 45.15558127464808, "xcomet_score": 0.9024549126625061, "xcomet_qe_score": 0.8712908625602722, "metricx_score": 1.308674931526184, "metricx_qe_score": 1.4190480709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表达了特定行为来减少人工评估的主观性,例如提供与主题无关的信息或自相矛盾。", "metrics": {"bleu_score": 65.31986817818962, "chrf_score": 59.509120173027405, "xcomet_score": 0.9657424688339233, "xcomet_qe_score": 0.9607220888137817, "metricx_score": 1.7524058818817139, "metricx_qe_score": 2.3436131477355957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为聊天行为标注或简称为ABC评估。", "metrics": {"bleu_score": 16.76784955078518, "chrf_score": 18.0427420517559, "xcomet_score": 0.7976334095001221, "xcomet_qe_score": 0.8042736053466797, "metricx_score": 1.717424988746643, "metricx_qe_score": 1.118726372718811, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法是为了全面涵盖近期文献中提出影响聊天质量建议的聊天模型行为。", "metrics": {"bleu_score": 56.76007302893162, "chrf_score": 47.69143787889183, "xcomet_score": 0.8468894958496094, "xcomet_qe_score": 0.8252619504928589, "metricx_score": 3.405181646347046, "metricx_qe_score": 4.659224510192871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC 评估能够测量聊天模型犯各种主题错误的速率。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 52.23877106271304, "xcomet_score": 0.6864588260650635, "xcomet_qe_score": 0.7049075365066528, "metricx_score": 4.9060444831848145, "metricx_qe_score": 6.331258773803711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,APCEval 测量了聊天模型忽视其对话伙伴或说出与主题无关的话的回合数。 自我矛盾或与合作伙伴矛盾,幻觉错误的事实或违反常识,以及模型在表现同理心时成功或失败的情况。", "metrics": {"bleu_score": 25.535315453231423, "chrf_score": 22.787387062880356, "xcomet_score": 0.600230872631073, "xcomet_qe_score": 0.5118029117584229, "metricx_score": 7.131956577301025, "metricx_qe_score": 7.423162937164307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方法最有效,我们选择了四种最先进的聊天模型,并使用 ABCEval 对每个模型进行了一百次人类与机器人的对话评估。", "metrics": {"bleu_score": 53.59161628653244, "chrf_score": 54.67710346558241, "xcomet_score": 0.8712108135223389, "xcomet_qe_score": 0.9132921099662781, "metricx_score": 1.1971665620803833, "metricx_qe_score": 1.1082607507705688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用三种现有方法对这些对话进行了评估:流动评分法(在回合级别)、流动评分法(在对话级别)以及对话级别的配对比较。", "metrics": {"bleu_score": 33.08242825184108, "chrf_score": 30.444265923927272, "xcomet_score": 0.6454946398735046, "xcomet_qe_score": 0.5998724699020386, "metricx_score": 4.903536796569824, "metricx_qe_score": 4.369225025177002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有的每一种方法,我们收集了关于对话八个最常见测量维度的评估,因为这是沿着多个维度评估聊天模型的标准实践。 ", "metrics": {"bleu_score": 54.8157953609835, "chrf_score": 47.08329258510686, "xcomet_score": 0.9372260570526123, "xcomet_qe_score": 0.9359076619148254, "metricx_score": 2.972381114959717, "metricx_qe_score": 3.4712600708007812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过对这些评估结果的分析,我们发现ABC评估行为标签总体上比现有方法收集的标签更可靠,这一点通过100个双重标记对话的评注者间一致性得到衡量。", "metrics": {"bleu_score": 37.44424629758319, "chrf_score": 36.29798602212736, "xcomet_score": 0.7190631628036499, "xcomet_qe_score": 0.814085066318512, "metricx_score": 4.837323188781738, "metricx_qe_score": 4.4215006828308105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,根据这一简单的线性回归分析,ABC 评估标签在预测整体对话质量方面优于现有方法产生的指标。", "metrics": {"bleu_score": 45.90738410805631, "chrf_score": 37.46263497980875, "xcomet_score": 0.9840507507324219, "xcomet_qe_score": 0.9731825590133667, "metricx_score": 1.6086461544036865, "metricx_qe_score": 1.5417166948318481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到,测量自我和伴侣矛盾的转弯比例分别解释了对话质量百分之五和百分之十,而平均白酒一致性得分仅解释了百分之四或更少。", "metrics": {"bleu_score": 36.939284970595565, "chrf_score": 31.208720416749646, "xcomet_score": 0.46029427647590637, "xcomet_qe_score": 0.4755629897117615, "metricx_score": 11.063846588134766, "metricx_qe_score": 11.007842063903809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归分析检查了每个评估指标是否捕捉了聊天质量的独特方面。 您", "metrics": {"bleu_score": 63.70082049877986, "chrf_score": 60.51419678972897, "xcomet_score": 0.7582813501358032, "xcomet_qe_score": 0.7904745936393738, "metricx_score": 4.408302307128906, "metricx_qe_score": 1.9686896800994873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,ABC 评估指标的组合可以解释超过 25% 的对话质量。当您逐个去除这些指标时,大多数情况下都会导致失去大量关于质量的信息。", "metrics": {"bleu_score": 33.06805758180923, "chrf_score": 30.058114942844345, "xcomet_score": 0.9365706443786621, "xcomet_qe_score": 0.8233102560043335, "metricx_score": 1.7157714366912842, "metricx_qe_score": 2.653087615966797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有转折级液体指标的组合对质量解释得更少,且这些指标中携带独特信息的更少。", "metrics": {"bleu_score": 32.96511202415315, "chrf_score": 28.031902922674767, "xcomet_score": 0.6678799986839294, "xcomet_qe_score": 0.7422425150871277, "metricx_score": 7.38028621673584, "metricx_qe_score": 7.8587164878845215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些可靠、信息丰富且独特的ABC评估指标使我们能够以高于先前方法的分辨率评估对话人工智能。", "metrics": {"bleu_score": 6.791172138789439, "chrf_score": 11.160879829966513, "xcomet_score": 0.6814118027687073, "xcomet_qe_score": 0.8014644980430603, "metricx_score": 4.8067708015441895, "metricx_qe_score": 4.136673450469971, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们实验的结果中可以看出,仍存在几个挑战,并且这些挑战已被精确量化。", "metrics": {"bleu_score": 21.16154709655933, "chrf_score": 24.096275259849005, "xcomet_score": 0.9800653457641602, "xcomet_qe_score": 0.979763388633728, "metricx_score": 0.7752567529678345, "metricx_qe_score": 0.9311131834983826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人约20%的响应存在常识违规。", "metrics": {"bleu_score": 41.87526331649786, "chrf_score": 38.03824454645228, "xcomet_score": 0.8536428213119507, "xcomet_qe_score": 0.8503142595291138, "metricx_score": 2.4013116359710693, "metricx_qe_score": 3.4330945014953613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在约15%的响应中产生与主题无关的信息,并且约10%的时间会自相矛盾或与合作伙伴矛盾。", "metrics": {"bleu_score": 32.856833323646185, "chrf_score": 28.871460351638344, "xcomet_score": 0.707412600517273, "xcomet_qe_score": 0.6725014448165894, "metricx_score": 3.2943553924560547, "metricx_qe_score": 3.4089436531066895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速进步,自我们进行评估以来,许多错误率在新发布的模型中可能会降低。", "metrics": {"bleu_score": 54.50206538912655, "chrf_score": 46.69290314027156, "xcomet_score": 0.9755654335021973, "xcomet_qe_score": 0.9798710346221924, "metricx_score": 1.6807653903961182, "metricx_qe_score": 1.6306102275848389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更说明了追求可靠且精确的评估指标以比较模型的重要性。", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 40.91703962752154, "xcomet_score": 0.9977174997329712, "xcomet_qe_score": 0.9871809482574463, "metricx_score": 0.910224437713623, "metricx_qe_score": 1.0445787906646729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABC评估能被该领域的其他人士作为朝此方向迈出的有意义的一步,", "metrics": {"bleu_score": 47.0267730359709, "chrf_score": 41.24732919302844, "xcomet_score": 0.8658312559127808, "xcomet_qe_score": 0.8803295493125916, "metricx_score": 3.639732599258423, "metricx_qe_score": 2.8657116889953613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待看到对话式人工智能在未来几个月和几年中的发展。", "metrics": {"bleu_score": 74.0609366763812, "chrf_score": 73.52998803726307, "xcomet_score": 0.9891669750213623, "xcomet_qe_score": 0.9691853523254395, "metricx_score": 0.7160965800285339, "metricx_qe_score": 0.8807143568992615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Kyo Yin,我将展示我们的研究成果,题为《何时翻译需要数据", "metrics": {"bleu_score": 11.216709782562996, "chrf_score": 19.547990061217245, "xcomet_score": 0.7186183929443359, "xcomet_qe_score": 0.6725578904151917, "metricx_score": 5.748015403747559, "metricx_qe_score": 4.257076263427734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "驱动的多语言探索?》。", "metrics": {"bleu_score": 67.74702029865007, "chrf_score": 62.048177141398455, "xcomet_score": 0.4785916805267334, "xcomet_qe_score": 0.46489977836608887, "metricx_score": 7.170538902282715, "metricx_qe_score": 5.203088760375977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernandes、Emily Liu、Andre FD Martins和Graham Newbig合作完成的。", "metrics": {"bleu_score": 32.042193038079624, "chrf_score": 69.15799844863238, "xcomet_score": 0.8232953548431396, "xcomet_qe_score": 0.8795398473739624, "metricx_score": 2.418205499649048, "metricx_qe_score": 2.286593198776245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都取决于上下文。", "metrics": {"bleu_score": 63.40466277046863, "chrf_score": 59.27762653106322, "xcomet_score": 0.9980931282043457, "xcomet_qe_score": 0.9876047372817993, "metricx_score": 0.09006089717149734, "metricx_qe_score": 0.15374323725700378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译句子中的“痣”? (注:由于原文未提供具体句子,翻译中的“mole”被", "metrics": {"bleu_score": 21.74060073159203, "chrf_score": 41.07085362963095, "xcomet_score": 0.44265270233154297, "xcomet_qe_score": 0.3905332088470459, "metricx_score": 7.033148765563965, "metricx_qe_score": 4.665295124053955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设为“痣”的意思。) 好吧,如果前一句是“如果部长们发现,事情可能会开始变得危险”,那么“Moe”指的是一个间谍。", "metrics": {"bleu_score": 9.122982653308174, "chrf_score": 8.739022745953944, "xcomet_score": 0.6655306816101074, "xcomet_qe_score": 0.653622031211853, "metricx_score": 8.332271575927734, "metricx_qe_score": 8.640735626220703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,这会是什么严重的问题吗?”那么“Moe”指的是一个胎记。", "metrics": {"bleu_score": 11.50711887999582, "chrf_score": 11.378850205584493, "xcomet_score": 0.8743106126785278, "xcomet_qe_score": 0.8276439309120178, "metricx_score": 5.714470863342285, "metricx_qe_score": 5.575756549835205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,词的意义会发生变化,它的翻译也会相应地发生变化。", "metrics": {"bleu_score": 41.84370715005657, "chrf_score": 33.69690901919136, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5362690687179565, "metricx_qe_score": 0.46202796697616577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在翻译此类案例时的表现相当困难。", "metrics": {"bleu_score": 20.63875384458799, "chrf_score": 18.315811601788674, "xcomet_score": 0.8964866399765015, "xcomet_qe_score": 0.8593510389328003, "metricx_score": 1.1426359415054321, "metricx_qe_score": 1.745255708694458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,由于只有少数翻译依赖于上下文,这导致语料库级别的指标,如BLEU,无法捕捉到这些翻译。", "metrics": {"bleu_score": 28.89787454467964, "chrf_score": 26.58360985138732, "xcomet_score": 0.976241946220398, "xcomet_qe_score": 0.9509700536727905, "metricx_score": 1.406715989112854, "metricx_qe_score": 2.210369110107422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些人建议对基于上下文的翻译进行定向评估,但这些资源只支持有限类型的基于上下文的翻译和有限的语言集合,因为它们通常依赖于领域知识和人工编辑。", "metrics": {"bleu_score": 63.01934313515587, "chrf_score": 55.728582550531094, "xcomet_score": 0.9546992778778076, "xcomet_qe_score": 0.9520302414894104, "metricx_score": 0.9552088975906372, "metricx_qe_score": 0.9785817861557007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们试图回答以下两个问题。", "metrics": {"bleu_score": 73.09400730437454, "chrf_score": 69.43822972366657, "xcomet_score": 0.9975918531417847, "xcomet_qe_score": 0.99656081199646, "metricx_score": 0.0, "metricx_qe_score": 0.09364727139472961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在处理这些情况下表现如何?", "metrics": {"bleu_score": 59.29835221973795, "chrf_score": 51.308235489846524, "xcomet_score": 0.9888322353363037, "xcomet_qe_score": 0.99993896484375, "metricx_score": 0.4791920483112335, "metricx_qe_score": 0.6569955348968506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了单词在翻译过程中对上下文的依赖程度。", "metrics": {"bleu_score": 84.52785147119853, "chrf_score": 78.40240983979126, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 4.045997619628906, "metricx_qe_score": 4.679557800292969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在先前的工作中,我们引入了CXMI作为机器翻译模型上下文使用量的度量。", "metrics": {"bleu_score": 57.19408603967074, "chrf_score": 59.345310377550106, "xcomet_score": 0.8919402360916138, "xcomet_qe_score": 0.888935387134552, "metricx_score": 1.7021788358688354, "metricx_qe_score": 1.811134696006775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量给定源X时上下文C关于目标Y提供的信息量来实现。 可以将CXMI视为通过为模型提供上下文而获得的信息。", "metrics": {"bleu_score": 53.83309248891204, "chrf_score": 52.48641145275519, "xcomet_score": 0.922224760055542, "xcomet_qe_score": 0.927118182182312, "metricx_score": 4.787790298461914, "metricx_qe_score": 4.49423360824585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们将CXMI扩展为点对点CXMI,它可以在句子级别或词语级别测量上下文使用情况。", "metrics": {"bleu_score": 48.00487694138449, "chrf_score": 39.91774701504603, "xcomet_score": 0.8377571105957031, "xcomet_qe_score": 0.8423504829406738, "metricx_score": 2.3003931045532227, "metricx_qe_score": 2.3884425163269043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将PSXMI值较高的词语视为需要上下文进行翻译的词语。", "metrics": {"bleu_score": 64.85539682067228, "chrf_score": 61.36979151262353, "xcomet_score": 0.9069926738739014, "xcomet_qe_score": 0.9090468883514404, "metricx_score": 2.328460454940796, "metricx_qe_score": 2.68703031539917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析高PCXMI的词语,以寻找这些词语之间的模式。", "metrics": {"bleu_score": 21.419122155930683, "chrf_score": 30.77209042801514, "xcomet_score": 0.9344390630722046, "xcomet_qe_score": 0.9488096833229065, "metricx_score": 1.6728878021240234, "metricx_qe_score": 2.5421361923217773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成十四种不同语言的TED演讲的文字记录进行了分析。", "metrics": {"bleu_score": 50.86366587562722, "chrf_score": 51.002933436495844, "xcomet_score": 0.9026250243186951, "xcomet_qe_score": 0.9274037480354309, "metricx_score": 1.4527552127838135, "metricx_qe_score": 1.3419684171676636, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同的层次上进行分析。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 72.00112387612387, "xcomet_score": 0.997683048248291, "xcomet_qe_score": 0.9908944368362427, "metricx_score": 0.22728201746940613, "metricx_qe_score": 0.402015745639801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们观察平均PCXMI值较高的词性标签。", "metrics": {"bleu_score": 40.56966136591355, "chrf_score": 35.733706804142805, "xcomet_score": 0.8702909350395203, "xcomet_qe_score": 0.8333213329315186, "metricx_score": 1.5720288753509521, "metricx_qe_score": 2.0618159770965576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到,例如,在阿拉伯语中具有相对较高的p六米双重代词。", "metrics": {"bleu_score": 47.63001139940413, "chrf_score": 38.71944958082334, "xcomet_score": 0.4996316432952881, "xcomet_qe_score": 0.5437459349632263, "metricx_score": 8.47048568725586, "metricx_qe_score": 8.198446273803711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,因为英语中没有双重代词。所以在翻译成阿拉伯语时,你需要上下文来确定一个代词是否是双重代词。", "metrics": {"bleu_score": 43.43184707403442, "chrf_score": 38.81602964552389, "xcomet_score": 0.8638068437576294, "xcomet_qe_score": 0.9708962440490723, "metricx_score": 1.96208918094635, "metricx_qe_score": 1.5577373504638672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 88.4112136328919, "chrf_score": 89.69824812440879, "xcomet_score": 0.9977834224700928, "xcomet_qe_score": 0.996990442276001, "metricx_score": 0.5566547513008118, "metricx_qe_score": 0.8022129535675049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们考察在所有不同出现中性取平均值的词汇项。", "metrics": {"bleu_score": 32.998954725277905, "chrf_score": 22.174487671947656, "xcomet_score": 0.678064227104187, "xcomet_qe_score": 0.6942429542541504, "metricx_score": 7.896390438079834, "metricx_qe_score": 7.900931358337402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的案例,在中文中,你需要上下文来翻译专名,以确保在文档中采用一致的翻译。", "metrics": {"bleu_score": 33.76387778787762, "chrf_score": 29.130076703900272, "xcomet_score": 0.7772513628005981, "xcomet_qe_score": 0.8464795351028442, "metricx_score": 1.7538535594940186, "metricx_qe_score": 2.2723588943481445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样地,我们发现语境得到支持,以合适的正式程度进行翻译。", "metrics": {"bleu_score": 12.500763055889763, "chrf_score": 14.265471016461914, "xcomet_score": 0.8129124641418457, "xcomet_qe_score": 0.8098093271255493, "metricx_score": 4.555186748504639, "metricx_qe_score": 5.024979114532471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们考察了具有高p6mi的不同单个词元。", "metrics": {"bleu_score": 42.461633178803446, "chrf_score": 31.003079744267147, "xcomet_score": 0.7213668823242188, "xcomet_qe_score": 0.6588348150253296, "metricx_score": 6.882691860198975, "metricx_qe_score": 8.598299980163574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别一些无法仅通过词本身捕捉的现象,而这些现象却在标准结构中得到表达,例如省略解析。", "metrics": {"bleu_score": 30.7421730669102, "chrf_score": 26.6945148758147, "xcomet_score": 0.8522686958312988, "xcomet_qe_score": 0.8362205624580383, "metricx_score": 2.0073225498199463, "metricx_qe_score": 2.6767215728759766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们利用分析结果设计一份文档级翻译的基准。", "metrics": {"bleu_score": 46.79404176919902, "chrf_score": 40.65863461205542, "xcomet_score": 0.9121483564376831, "xcomet_qe_score": 0.8365963697433472, "metricx_score": 1.0161453485488892, "metricx_qe_score": 1.3637795448303223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别出的五种分歧现象,我们创建了标记器,以自动识别与现象相关的词语。", "metrics": {"bleu_score": 55.30319715123848, "chrf_score": 49.95168070527775, "xcomet_score": 0.855077862739563, "xcomet_qe_score": 0.8139628171920776, "metricx_score": 3.280407428741455, "metricx_qe_score": 2.740053653717041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将我们的标记器称为多语言语境感知(MUDA)标记器。", "metrics": {"bleu_score": 33.5908536197405, "chrf_score": 26.909813036080205, "xcomet_score": 0.9804238080978394, "xcomet_qe_score": 0.9568902254104614, "metricx_score": 0.8562975525856018, "metricx_qe_score": 1.0827800035476685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同语言中这些离散现象的比例各不", "metrics": {"bleu_score": 34.22939112823469, "chrf_score": 30.432859390613547, "xcomet_score": 0.8721159100532532, "xcomet_qe_score": 0.8974602818489075, "metricx_score": 4.895651340484619, "metricx_qe_score": 3.7106423377990723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相同。 我们随后使用MUDA标注器,将其应用于我们希望用于评估的平行语料库,并应用我们选择的翻译度量标准,对MUDA标注器识别的上下文相关示例进行评估。", "metrics": {"bleu_score": 43.345018468509615, "chrf_score": 43.51112042941722, "xcomet_score": 0.6169798374176025, "xcomet_qe_score": 0.6080249547958374, "metricx_score": 2.8407652378082275, "metricx_qe_score": 2.962597608566284, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用基准和其他度量标准来评估文档级机器翻译中的不同模型。", "metrics": {"bleu_score": 50.77162056935099, "chrf_score": 43.621313054031106, "xcomet_score": 0.9540338516235352, "xcomet_qe_score": 0.8370054364204407, "metricx_score": 0.9733223915100098, "metricx_qe_score": 1.4055020809173584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的度量时,对于 Blue 来说,我们发现复杂的阿格诺斯模型(复杂无知模型)具有最佳性能。", "metrics": {"bleu_score": 24.55602423192668, "chrf_score": 25.806018657634556, "xcomet_score": 0.7015150785446167, "xcomet_qe_score": 0.6367390155792236, "metricx_score": 5.112695217132568, "metricx_qe_score": 4.976933479309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,如果我们使用彗星(comet)评估,则语境感知模型表现最佳。而", "metrics": {"bleu_score": 29.420998830694653, "chrf_score": 24.479220620724, "xcomet_score": 0.6847389936447144, "xcomet_qe_score": 0.5453649759292603, "metricx_score": 4.969758987426758, "metricx_qe_score": 4.484834671020508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用词F度量,那么有语境和无语境的模型性能相当。", "metrics": {"bleu_score": 30.939307773324014, "chrf_score": 27.005484903369275, "xcomet_score": 0.7855226993560791, "xcomet_qe_score": 0.7782093286514282, "metricx_score": 3.1374707221984863, "metricx_qe_score": 2.422879219055176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,仅使用企业级指标很难确定最佳文档级翻译系统。", "metrics": {"bleu_score": 46.97019791307794, "chrf_score": 41.222718995995514, "xcomet_score": 0.8509092926979065, "xcomet_qe_score": 0.7599486112594604, "metricx_score": 3.7748401165008545, "metricx_qe_score": 4.405245780944824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前我们使用MUDA基准来评估模型,发现在特定话语现象(如正式性和词汇连贯性)中,具有上下文意识的模型比不使用上下文的模型准确度显著更高。", "metrics": {"bleu_score": 43.63833147884654, "chrf_score": 41.395045663284435, "xcomet_score": 0.7908076643943787, "xcomet_qe_score": 0.8433037400245667, "metricx_score": 1.699663758277893, "metricx_qe_score": 2.160973072052002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在处理省略号、代词和动词形式等其他现象时,与未使用上下文的模型相差无几。", "metrics": {"bleu_score": 60.98986290110881, "chrf_score": 55.040098692040594, "xcomet_score": 0.9877657890319824, "xcomet_qe_score": 0.9166115522384644, "metricx_score": 0.8242922425270081, "metricx_qe_score": 0.9372149705886841, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在一定程度上表明了我们在文档级翻译中需要看到更多进步的领域。", "metrics": {"bleu_score": 45.84906215724896, "chrf_score": 43.00365880538174, "xcomet_score": 0.999502420425415, "xcomet_qe_score": 0.99676513671875, "metricx_score": 1.5128241777420044, "metricx_qe_score": 1.4835546016693115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准测试显示,DeepBell 在文档级翻译方面通常比谷歌翻译更准确。", "metrics": {"bleu_score": 72.19794692694344, "chrf_score": 62.68064307546379, "xcomet_score": 0.8358278274536133, "xcomet_qe_score": 0.6969610452651978, "metricx_score": 4.980822563171387, "metricx_qe_score": 5.319259166717529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结而言,我们在十四个语言对中进行数据驱动的分析,以识别何时翻译需要上下文。 然后,我们利用研究成果建立文档级机器翻译的基准,这可以帮助我们识别哪些离散现象模型能很好地处理,哪些不能,以及哪些翻译系统在文档级翻译中表现出色。", "metrics": {"bleu_score": 37.49723604594655, "chrf_score": 33.277689040527584, "xcomet_score": 0.7953556776046753, "xcomet_qe_score": 0.8655668497085571, "metricx_score": 2.4294240474700928, "metricx_qe_score": 2.9888827800750732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.7561129331588745, "xcomet_qe_score": 0.9904394745826721, "metricx_score": 0.679286539554596, "metricx_qe_score": 0.5824178457260132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在多伦多见。", "metrics": {"bleu_score": 71.65313105737896, "chrf_score": 64.65405545478103, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4812854528427124, "metricx_qe_score": 1.2813191413879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是雅尼斯·拉瓦克,我将向您展示我们在Dr. Berth方面的工作,这是一个强大的法语预训练模型,适用于生物医学和临床领域。", "metrics": {"bleu_score": 29.613115434220056, "chrf_score": 25.815448271463694, "xcomet_score": 0.7360790967941284, "xcomet_qe_score": 0.7323977947235107, "metricx_score": 2.7319977283477783, "metricx_qe_score": 2.501905679702759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论医疗领域的语言建模。", "metrics": {"bleu_score": 58.21417459564055, "chrf_score": 48.89963248282777, "xcomet_score": 0.9930500984191895, "xcomet_qe_score": 0.9960814714431763, "metricx_score": 0.3166263699531555, "metricx_qe_score": 0.34818464517593384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将展示我们文章的主要贡献。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 54.45316801934449, "xcomet_score": 0.911873459815979, "xcomet_qe_score": 0.9091360569000244, "metricx_score": 1.6052939891815186, "metricx_qe_score": 2.0100960731506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个以法语命名的生物医学模型,名为Dr. Berth,该模型基于Roberta,并在Natchios上进行了训练,Natchios是一个从网络上抓取的医疗数据集。", "metrics": {"bleu_score": 36.899907816210586, "chrf_score": 27.738328211028918, "xcomet_score": 0.6533805131912231, "xcomet_qe_score": 0.6778473258018494, "metricx_score": 3.2528023719787598, "metricx_qe_score": 3.159165143966675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了对具有多种钚设置和数据源的模型的比较。", "metrics": {"bleu_score": 47.88315740339225, "chrf_score": 41.48230754752494, "xcomet_score": 0.7948076128959656, "xcomet_qe_score": 0.8028668165206909, "metricx_score": 5.0809502601623535, "metricx_qe_score": 5.430595397949219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们展示了在法国语环境中十一项生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 41.138327789300924, "chrf_score": 38.152713898683096, "xcomet_score": 0.7931827306747437, "xcomet_qe_score": 0.8418337106704712, "metricx_score": 2.671884775161743, "metricx_qe_score": 3.542362928390503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们对实验进行总结,并详细介绍如何访问该模型。", "metrics": {"bleu_score": 11.800130350098168, "chrf_score": 14.948439883635467, "xcomet_score": 0.9127389788627625, "xcomet_qe_score": 0.8928750157356262, "metricx_score": 0.39803990721702576, "metricx_qe_score": 0.4255833327770233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务最有效的方法之一,与历史静态和上下文相关方法(如词向量、快速文本或嵌入)相比,其性能有了巨大提升。", "metrics": {"bleu_score": 53.65101010289625, "chrf_score": 48.39026419062216, "xcomet_score": 0.7748600244522095, "xcomet_qe_score": 0.7735093832015991, "metricx_score": 2.218303680419922, "metricx_qe_score": 2.147268772125244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自那时起,该模型已被适应到许多其他语言中,例如法语中的Camembert,以及其他领域如生物医学中的Permette Bert和BioBert,以及在临床上的Clinical Bert,但主要是在英语中。", "metrics": {"bleu_score": 23.490499561692616, "chrf_score": 29.805518147279646, "xcomet_score": 0.5195830464363098, "xcomet_qe_score": 0.5067546367645264, "metricx_score": 7.30694580078125, "metricx_qe_score": 7.272327423095703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的专业模型稀缺,且往往由于缺乏领域内数据而基于持续的假装(模拟)。", "metrics": {"bleu_score": 35.788584001416766, "chrf_score": 31.897147997068387, "xcomet_score": 0.8206195831298828, "xcomet_qe_score": 0.792665958404541, "metricx_score": 2.4358839988708496, "metricx_qe_score": 2.1395576000213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法国直到现在都没有开放源代码的生物医学现代工具。", "metrics": {"bleu_score": 34.97140226207081, "chrf_score": 30.301641113512197, "xcomet_score": 0.8780903816223145, "xcomet_qe_score": 0.8846337199211121, "metricx_score": 2.714446544647217, "metricx_qe_score": 1.9546995162963867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们需要思考什么样的数据来源最适合广泛的使用场景。目前这些数据可以作为临床数据的良好替代品。", "metrics": {"bleu_score": 51.931843676369, "chrf_score": 47.35177653028853, "xcomet_score": 0.9191906452178955, "xcomet_qe_score": 0.9020489454269409, "metricx_score": 0.7919970750808716, "metricx_qe_score": 0.9905263185501099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将伯特博士与我们基于非大学医院获取的匿名数据建立的舒伯特模型进行比较。", "metrics": {"bleu_score": 38.2371918147603, "chrf_score": 26.840009867071828, "xcomet_score": 0.6174037456512451, "xcomet_qe_score": 0.5957403779029846, "metricx_score": 4.727107524871826, "metricx_qe_score": 5.33260440826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,我们要问自己,训练一个专门的法国数据模型需要多少数据?", "metrics": {"bleu_score": 31.678880008280984, "chrf_score": 28.45252302648687, "xcomet_score": 0.8504390716552734, "xcomet_qe_score": 0.8267943859100342, "metricx_score": 0.7971277832984924, "metricx_qe_score": 1.005959391593933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4 GB、8 GB还是更多?", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 90.21205646205644, "xcomet_score": 0.9752111434936523, "xcomet_qe_score": 0.9724503755569458, "metricx_score": 0.2852798104286194, "metricx_qe_score": 0.51189124584198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较四个从零开始的模型。第一个版本是拥有七GB纳奇奥斯(Nachos)的Dr. Bert,第二个版本是四GB纳奇奥斯子集。 舒伯特(Schubert)的第一个版本是一个临床模型,包含从临床节点中提取的四个吉字节的句子。最终版本的舒伯特混合了四个吉字节的自然语料集和四个吉字节的临床节点。", "metrics": {"bleu_score": 30.578749245556654, "chrf_score": 24.614124368743408, "xcomet_score": 0.34702664613723755, "xcomet_qe_score": 0.367073118686676, "metricx_score": 9.385183334350586, "metricx_qe_score": 8.780837059020996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这一比较,我们引入了三个在持续预训练上训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 51.781195920839345, "chrf_score": 44.07102404745395, "xcomet_score": 0.8623901605606079, "xcomet_qe_score": 0.8373721837997437, "metricx_score": 2.6084725856781006, "metricx_qe_score": 3.01263689994812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于卡门贝尔奶酪的模型,训练数据是四千兆字节的纳乔斯(nachos)数据集", "metrics": {"bleu_score": 8.9682352483466, "chrf_score": 7.713947882001787, "xcomet_score": 0.5287126302719116, "xcomet_qe_score": 0.5061713457107544, "metricx_score": 4.352843761444092, "metricx_qe_score": 4.032351493835449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",另一个同样基于卡门贝尔奶酪,但这次训练数据是四千兆字节的Klinker Lots数据集。 最后,一个基于英语生物医学模型 BMLB,并在 4 GB 的 Snatchers 数据上进行训练的模型。", "metrics": {"bleu_score": 25.373134101007537, "chrf_score": 25.23604055460623, "xcomet_score": 0.17798542976379395, "xcomet_qe_score": 0.2874416708946228, "metricx_score": 12.169366836547852, "metricx_qe_score": 12.815591812133789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们有七个模型。", "metrics": {"bleu_score": 57.067457770560026, "chrf_score": 52.68257581869399, "xcomet_score": 0.9877036809921265, "xcomet_qe_score": 0.8995441198348999, "metricx_score": 0.29523736238479614, "metricx_qe_score": 0.44317466020584106, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们七种模型,我们收集了支持公共和私人下游任务的数据,例如姓名和身份识别、分类、模式切换标记和问答。", "metrics": {"bleu_score": 50.58067369422973, "chrf_score": 44.473384867808555, "xcomet_score": 0.6400566101074219, "xcomet_qe_score": 0.6225733757019043, "metricx_score": 4.369775772094727, "metricx_qe_score": 3.8134098052978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基准模型进行比较,基准模型包括 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBelt、Myobelt 和 ClinicalBelt。", "metrics": {"bleu_score": 30.92914742491702, "chrf_score": 39.12569297351722, "xcomet_score": 0.4271901249885559, "xcomet_qe_score": 0.45971766114234924, "metricx_score": 7.7254438400268555, "metricx_qe_score": 7.524115085601807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,该模型在处理与训练数据性质相同的任务时表现最佳。", "metrics": {"bleu_score": 39.21218176425807, "chrf_score": 32.08433298246603, "xcomet_score": 0.9959070682525635, "xcomet_qe_score": 0.9915297031402588, "metricx_score": 0.7365845441818237, "metricx_qe_score": 1.0133627653121948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从异质来源获取更丰富多样的数据。", "metrics": {"bleu_score": 16.287449307923687, "chrf_score": 16.165568569306767, "xcomet_score": 0.7959491014480591, "xcomet_qe_score": 0.8078235387802124, "metricx_score": 1.6017887592315674, "metricx_qe_score": 1.053817629814148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据可以转化为更好的性能。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 47.772497645834925, "xcomet_score": 0.9756163358688354, "xcomet_qe_score": 0.9722273349761963, "metricx_score": 3.2136597633361816, "metricx_qe_score": 4.146425247192383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,从零开始的免费训练在大多数任务上似乎能获得更高的性能。", "metrics": {"bleu_score": 29.705344069848184, "chrf_score": 24.86377917458748, "xcomet_score": 0.8049476146697998, "xcomet_qe_score": 0.7957206964492798, "metricx_score": 6.817322254180908, "metricx_qe_score": 7.154472351074219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用在纳切兹四GB子集上训练的PumedBeard的权重和分词器进行的持续假装实验,其结果与从零开始使用Dr. Beard四GB获得的结果相当。", "metrics": {"bleu_score": 26.234891812019587, "chrf_score": 24.887810729130127, "xcomet_score": 0.3354937732219696, "xcomet_qe_score": 0.3292529284954071, "metricx_score": 8.845712661743164, "metricx_qe_score": 9.176950454711914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于常见熊重量和分词器模型存在稳定性问题,这在现实中并不适用。 (Note: The term \"common bear weights\" doesn't make much sense in English, so I've interpreted it as \"常见熊重量\" in Chinese, but it might need further clarification depending on the context.)", "metrics": {"bleu_score": 12.182827511974507, "chrf_score": 13.093896510138944, "xcomet_score": 0.22770355641841888, "xcomet_qe_score": 0.3652636408805847, "metricx_score": 8.846559524536133, "metricx_qe_score": 6.849290370941162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们提出的系统在十一项下游任务中九项表现更好,并整体上超越了通用模型Camembert的结果。", "metrics": {"bleu_score": 25.03251237742343, "chrf_score": 23.594348531832708, "xcomet_score": 0.8365461826324463, "xcomet_qe_score": 0.7728165984153748, "metricx_score": 3.992422342300415, "metricx_qe_score": 2.172714948654175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也观察到,专业化数据更好,越专业的数据越好,但它扩展起来并不容易。", "metrics": {"bleu_score": 10.199573389218783, "chrf_score": 15.548633798758626, "xcomet_score": 0.7877252697944641, "xcomet_qe_score": 0.8321546316146851, "metricx_score": 1.9304850101470947, "metricx_qe_score": 2.8849873542785645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从Natchios获取的预训练模型均免费可在YuginFace上获取,所有训练脚本也在我们的GitHub仓库中。", "metrics": {"bleu_score": 32.487750706500094, "chrf_score": 33.45987325229573, "xcomet_score": 0.6186613440513611, "xcomet_qe_score": 0.6886476278305054, "metricx_score": 6.5254716873168945, "metricx_qe_score": 6.462907314300537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的精彩演讲。我们期待在多伦多的海报交流环节与您进一步讨论。", "metrics": {"bleu_score": 22.571180478829568, "chrf_score": 27.73592072761288, "xcomet_score": 0.9946272373199463, "xcomet_qe_score": 0.996677041053772, "metricx_score": 0.744317889213562, "metricx_qe_score": 0.5693838596343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9527263641357422, "xcomet_qe_score": 0.9953514337539673, "metricx_score": 0.21333150565624237, "metricx_qe_score": 0.13294564187526703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼。今天,我将简要介绍我们关于使用多集标记和潜在置换实现无树结构的组合泛化论文。 (注:上述翻译保持了原文的结构和语气,使用了学术论文中常见的词汇,并考虑了中文的语法习惯。)", "metrics": {"bleu_score": 19.647292014329214, "chrf_score": 24.317953539570976, "xcomet_score": 0.8800296783447266, "xcomet_qe_score": 0.8623477220535278, "metricx_score": 1.992558240890503, "metricx_qe_score": 2.2432682514190674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与导师亚历山大·科拉(Alexander Kola)和伊万·蒂托夫(Ivan Titov)的合作成果。", "metrics": {"bleu_score": 18.02916852147069, "chrf_score": 51.80846306834831, "xcomet_score": 0.917945146560669, "xcomet_qe_score": 0.9308079481124878, "metricx_score": 2.4199342727661133, "metricx_qe_score": 2.686610221862793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "构成泛化能力可以理解为学习者处理更深层次的递归和训练过程中单独见过的短语的新组合的能力。", "metrics": {"bleu_score": 67.27485569179622, "chrf_score": 62.1061436707229, "xcomet_score": 0.7038033604621887, "xcomet_qe_score": 0.68349289894104, "metricx_score": 4.642439842224121, "metricx_qe_score": 6.597900390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义分析的背景下,测试组合泛化能力可能如下所示。", "metrics": {"bleu_score": 54.02963813314917, "chrf_score": 44.75496736603901, "xcomet_score": 0.965408205986023, "xcomet_qe_score": 0.8892554640769958, "metricx_score": 0.9701953530311584, "metricx_qe_score": 1.8271222114562988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同往常一样,我们有一个训练语句集,在本", "metrics": {"bleu_score": 30.752616970214337, "chrf_score": 27.226237047301133, "xcomet_score": 0.7717912197113037, "xcomet_qe_score": 0.4432368874549866, "metricx_score": 3.3951022624969482, "metricx_qe_score": 3.099674701690674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例中为“女孩睡着了”以及“玛丽知道女孩睡着了”。 (", "metrics": {"bleu_score": 7.164684238257436, "chrf_score": 6.7849834214875075, "xcomet_score": 0.37579452991485596, "xcomet_qe_score": 0.33031827211380005, "metricx_score": 4.439189910888672, "metricx_qe_score": 3.6933481693267822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Zài yǔyì fēnxī de bèijìng xià, cèshì zǔhé fānhuà nénglì kěnéng jiù huì zhèyàng suǒshì. Tóng wǎngcháng yíyàng, wǒmen yǒu yīgè xùnliàn yǔjù jí, zài běn lìzài wèi “nǚhái shuìzháo le” yǐjí “Mǎlì zhīdào n", "metrics": {"bleu_score": 1.1493330194206166, "chrf_score": 5.012671646495311, "xcomet_score": 0.19639554619789124, "xcomet_qe_score": 0.1625756323337555, "metricx_score": 18.45410919189453, "metricx_qe_score": 8.509098052978516, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ǚhái shuìzháo le”.) 这些语句与逻辑形式相配,逻辑形式代表了它们意义的核心方面。", "metrics": {"bleu_score": 4.350111698509566, "chrf_score": 9.692569002123143, "xcomet_score": 0.5648665428161621, "xcomet_qe_score": 0.4996039569377899, "metricx_score": 10.501469612121582, "metricx_qe_score": 17.958303451538086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集并非来自同一分布,而是包含结构上未曾见过的逻辑形式。", "metrics": {"bleu_score": 57.919360960843825, "chrf_score": 48.865461852247925, "xcomet_score": 0.9670906066894531, "xcomet_qe_score": 0.9534275531768799, "metricx_score": 1.1220648288726807, "metricx_qe_score": 2.105464220046997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中经历了较浅的递归,并在具有更深递归的示例上进行了测试。", "metrics": {"bleu_score": 24.983045090282598, "chrf_score": 23.54100195275339, "xcomet_score": 0.8861418962478638, "xcomet_qe_score": 0.8687831163406372, "metricx_score": 2.334089756011963, "metricx_qe_score": 4.395214080810547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "天真的序列到序列模型在这个类型的超出分布泛化上遇到困难,经常产生与输入脱节的输出。", "metrics": {"bleu_score": 24.2463744872835, "chrf_score": 21.458813624071215, "xcomet_score": 0.6359991431236267, "xcomet_qe_score": 0.741192102432251, "metricx_score": 3.0298311710357666, "metricx_qe_score": 2.3576672077178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其地,他们经常无法再现输入和输出之间的系统对应关系,例如在示例中用颜色编码的那些对应关系。", "metrics": {"bleu_score": 53.64409247957173, "chrf_score": 52.379243379738924, "xcomet_score": 0.9471756219863892, "xcomet_qe_score": 0.9383914470672607, "metricx_score": 1.724570393562317, "metricx_qe_score": 1.3606065511703491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的方法是将树木集成到模型中。", "metrics": {"bleu_score": 33.49490518292079, "chrf_score": 27.35321007693968, "xcomet_score": 0.8819860219955444, "xcomet_qe_score": 0.9014766216278076, "metricx_score": 0.8151487112045288, "metricx_qe_score": 1.0012128353118896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树旨在捕捉与逻辑形式相关联的语句的组合过程。", "metrics": {"bleu_score": 25.315112684135887, "chrf_score": 23.066581534423513, "xcomet_score": 0.9291744232177734, "xcomet_qe_score": 0.8273036479949951, "metricx_score": 3.5279271602630615, "metricx_qe_score": 4.159653663635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这方法有效,但通常不会直接给出树结构,需要通过某种方式获取。", "metrics": {"bleu_score": 33.53405543690827, "chrf_score": 30.662801497760316, "xcomet_score": 0.9879758358001709, "xcomet_qe_score": 0.9930018186569214, "metricx_score": 0.5902580618858337, "metricx_qe_score": 0.9547306299209595, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能是一个复杂且有时计算成本较高的过程。", "metrics": {"bleu_score": 35.4225224760582, "chrf_score": 30.964956807920935, "xcomet_score": 0.9723730087280273, "xcomet_qe_score": 0.9788724184036255, "metricx_score": 0.4672151803970337, "metricx_qe_score": 0.6136482357978821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这需要对逻辑形式进行相当形式的预处理,例如,为了处理变量符号。", "metrics": {"bleu_score": 42.8377526647874, "chrf_score": 35.63801947535869, "xcomet_score": 0.7462038397789001, "xcomet_qe_score": 0.8359165191650391, "metricx_score": 1.0856930017471313, "metricx_qe_score": 1.3232336044311523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树木也可能涉及专业的语法归纳程序。", "metrics": {"bleu_score": 35.5669644969923, "chrf_score": 30.27393952410525, "xcomet_score": 0.8108344078063965, "xcomet_qe_score": 0.6646054983139038, "metricx_score": 5.526555061340332, "metricx_qe_score": 6.630249500274658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,而引入一种神经序列到序列模型,该模型直接建模输入片段与输出片段之间的对应关系。", "metrics": {"bleu_score": 47.46472061801463, "chrf_score": 36.6977773004049, "xcomet_score": 0.8240933418273926, "xcomet_qe_score": 0.8363856673240662, "metricx_score": 1.4712084531784058, "metricx_qe_score": 1.4608358144760132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首次,我们展示了在不依赖树结构的情况下,对更深层递归的强泛化能力。", "metrics": {"bleu_score": 49.72091245660371, "chrf_score": 41.2294421753731, "xcomet_score": 0.9830950498580933, "xcomet_qe_score": 0.9693622589111328, "metricx_score": 2.374300241470337, "metricx_qe_score": 3.0225493907928467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两个步骤预测输入的输出。", "metrics": {"bleu_score": 79.12619863720215, "chrf_score": 74.9408395085026, "xcomet_score": 0.9959282875061035, "xcomet_qe_score": 0.9735335111618042, "metricx_score": 0.4835524559020996, "metricx_qe_score": 0.855769157409668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入标记添加一个无序的多集合,其中包含将在输出中出现的标记。", "metrics": {"bleu_score": 18.181820747609674, "chrf_score": 19.978339676271645, "xcomet_score": 0.8088306188583374, "xcomet_qe_score": 0.8304609060287476, "metricx_score": 3.136526584625244, "metricx_qe_score": 2.8456640243530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个步骤之后,我们拥有了所有正确的令牌,但它们尚未排序。", "metrics": {"bleu_score": 31.387950392819207, "chrf_score": 28.967132832889586, "xcomet_score": 0.8512206077575684, "xcomet_qe_score": 0.8621318340301514, "metricx_score": 5.64223051071167, "metricx_qe_score": 5.1074395179748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步中,我们使用另一个模型来预测一个排列,以将它们放置在正确的顺序中。", "metrics": {"bleu_score": 43.71457607477818, "chrf_score": 45.20810127140832, "xcomet_score": 0.9144571423530579, "xcomet_qe_score": 0.9039705991744995, "metricx_score": 2.8809895515441895, "metricx_qe_score": 3.8077166080474854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新的方法来预测一种不对可能排列施加任何硬约束的排列。", "metrics": {"bleu_score": 50.10105108319363, "chrf_score": 46.11091631121303, "xcomet_score": 0.7974512577056885, "xcomet_qe_score": 0.7198668122291565, "metricx_score": 4.0571608543396, "metricx_qe_score": 3.9805712699890137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 33.903916544908675, "chrf_score": 27.589151796760387, "xcomet_score": 0.98786461353302, "xcomet_qe_score": 0.9677702188491821, "metricx_score": 0.7670807838439941, "metricx_qe_score": 1.2802248001098633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致运作如下。", "metrics": {"bleu_score": 37.70873273682918, "chrf_score": 31.19698644702401, "xcomet_score": 0.9181387424468994, "xcomet_qe_score": 0.9657492637634277, "metricx_score": 1.2701725959777832, "metricx_qe_score": 0.8324024677276611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右扫描输出,确定每个位置放置哪个多集令牌。", "metrics": {"bleu_score": 48.3796170917138, "chrf_score": 42.64104689933396, "xcomet_score": 0.7875443696975708, "xcomet_qe_score": 0.7411808967590332, "metricx_score": 4.679092884063721, "metricx_qe_score": 3.685758590698242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们直接选择一个,如红色标记所示。", "metrics": {"bleu_score": 47.973392274396026, "chrf_score": 40.77583712565096, "xcomet_score": 0.9863330125808716, "xcomet_qe_score": 0.9932188987731934, "metricx_score": 0.46956536173820496, "metricx_qe_score": 0.4505721628665924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们跳到下一个多集令牌,以确定输出中的第二个令牌。", "metrics": {"bleu_score": 50.88881048999093, "chrf_score": 43.80618763858309, "xcomet_score": 0.6891230344772339, "xcomet_qe_score": 0.6664390563964844, "metricx_score": 7.932967662811279, "metricx_qe_score": 6.339047431945801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过跳转到另一个多集令牌,以类似的方式确定输出中的第三个令牌。", "metrics": {"bleu_score": 71.86349451138221, "chrf_score": 67.6811440301518, "xcomet_score": 0.6910558938980103, "xcomet_qe_score": 0.7053862810134888, "metricx_score": 7.719019412994385, "metricx_qe_score": 5.330739974975586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程。 直到第一阶段的每个令牌都被精确访问一次。", "metrics": {"bleu_score": 59.24450913674052, "chrf_score": 53.234672684742144, "xcomet_score": 0.7889460325241089, "xcomet_qe_score": 0.7924833297729492, "metricx_score": 4.165687561035156, "metricx_qe_score": 3.2899093627929688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让您对实验结果有个预览,我们在这里将我们的方法与其他无树模型在 Kong 的基准上进行比较。我们的模型在", "metrics": {"bleu_score": 49.98843261923762, "chrf_score": 43.31037158777301, "xcomet_score": 0.419229656457901, "xcomet_qe_score": 0.5133213996887207, "metricx_score": 10.419405937194824, "metricx_qe_score": 6.054841041564941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层递归的泛化能力上显著超越了其他模型。", "metrics": {"bleu_score": 46.20069937541505, "chrf_score": 43.7025426757615, "xcomet_score": 0.9366554021835327, "xcomet_qe_score": 0.9135968685150146, "metricx_score": 2.611391067504883, "metricx_qe_score": 2.883965492248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,一些其他类型的结构概括仍然非常具有挑战性。", "metrics": {"bleu_score": 29.93195015610124, "chrf_score": 32.78769886497865, "xcomet_score": 0.9429450631141663, "xcomet_qe_score": 0.9863185882568359, "metricx_score": 1.9537898302078247, "metricx_qe_score": 1.3023054599761963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的技术难题。", "metrics": {"bleu_score": 37.494051432044955, "chrf_score": 32.70815749243335, "xcomet_score": 0.9961615800857544, "xcomet_qe_score": 0.986243486404419, "metricx_score": 0.13248570263385773, "metricx_qe_score": 0.16579806804656982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,训练数据中没有提供输入和输出的对齐信息。", "metrics": {"bleu_score": 36.076605997271955, "chrf_score": 28.29494685329926, "xcomet_score": 0.9904406070709229, "xcomet_qe_score": 0.9806236028671265, "metricx_score": 0.4581664800643921, "metricx_qe_score": 0.5017287135124207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的令牌,我们不知道它来自哪个多设置器,这为训练带来了挑战。", "metrics": {"bleu_score": 58.0358970684786, "chrf_score": 51.68262099968047, "xcomet_score": 0.7439107894897461, "xcomet_qe_score": 0.7200514078140259, "metricx_score": 7.309753894805908, "metricx_qe_score": 5.210699558258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的排列组合,但语言上正确的排列组合是潜在的。", "metrics": {"bleu_score": 61.93430192257325, "chrf_score": 62.546196123404776, "xcomet_score": 0.9146991968154907, "xcomet_qe_score": 0.8648902773857117, "metricx_score": 2.359483242034912, "metricx_qe_score": 2.7891693115234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练中诱导对齐来解决这个问题。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 82.10182318541452, "xcomet_score": 0.9801583290100098, "xcomet_qe_score": 0.9072984457015991, "metricx_score": 0.7283324599266052, "metricx_qe_score": 0.9682132005691528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的变换方法非常灵活,但这带来了找到最高得分变换的挑战,这是NP难的。", "metrics": {"bleu_score": 23.73169802909167, "chrf_score": 21.729430483141293, "xcomet_score": 0.6937560439109802, "xcomet_qe_score": 0.7625372409820557, "metricx_score": 2.5611765384674072, "metricx_qe_score": 2.487842321395874, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一种适合 GPU 的连续放松方法来近似此问题,该方法还允许我们对解进行反向传播,并学习语言上更合理的排列组合。", "metrics": {"bleu_score": 18.732951873728503, "chrf_score": 20.163214540930035, "xcomet_score": 0.8287971019744873, "xcomet_qe_score": 0.6864228248596191, "metricx_score": 2.834153413772583, "metricx_qe_score": 3.781205892562866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息,请查看我们的论文或参加我们的海报展示。", "metrics": {"bleu_score": 81.62735195826225, "chrf_score": 79.55911697772808, "xcomet_score": 0.9635475873947144, "xcomet_qe_score": 0.9758280515670776, "metricx_score": 0.8027520179748535, "metricx_qe_score": 0.8057847023010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是马克斯塔,今天我的合著者马丁和我将向大家展示我们的作品《基特马斯特奇:评估来自多个来源的知识整合》。这项", "metrics": {"bleu_score": 38.98253558241877, "chrf_score": 31.848471414383212, "xcomet_score": 0.4060394763946533, "xcomet_qe_score": 0.4148007035255432, "metricx_score": 8.14747142791748, "metricx_qe_score": 6.038240432739258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、MILA和微软研究之间的合作成果。 (注:中文中“The Kitmastech”直接音译,如需更自然的中文表达,可考虑根据上下文进行适当的翻译或解释。", "metrics": {"bleu_score": 21.205278569894023, "chrf_score": 33.10366170544852, "xcomet_score": 0.3477323651313782, "xcomet_qe_score": 0.23443910479545593, "metricx_score": 6.183938980102539, "metricx_qe_score": 6.231621742248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 国家语言理解模型利用多种知识来源,例如其参数中包含的知识,通常通过预训练获得,以及推理时输入中提供的知识。 近", "metrics": {"bleu_score": 55.55476270458279, "chrf_score": 51.2157027642334, "xcomet_score": 0.5601097345352173, "xcomet_qe_score": 0.5915705561637878, "metricx_score": 8.781060218811035, "metricx_qe_score": 4.973492622375488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期在问答等任务中的研究表明,模型可以利用预训练的时间知识来解决这些任务。", "metrics": {"bleu_score": 58.611859017694, "chrf_score": 50.322374413929005, "xcomet_score": 0.7971233129501343, "xcomet_qe_score": 0.7732760906219482, "metricx_score": 4.4741339683532715, "metricx_qe_score": 4.335671424865723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常需要在推理时也提供的知识。", "metrics": {"bleu_score": 79.632051309738, "chrf_score": 73.71916932929182, "xcomet_score": 0.870110273361206, "xcomet_qe_score": 0.8266476392745972, "metricx_score": 1.1921392679214478, "metricx_qe_score": 1.676377773284912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子中,“约翰在电视上看到了新当选的总统。” (Pinyin: Xìlì, zài jùzi zhōng, \"Yuēhàn zài diànshì shàng kàndào le xīn xuǎncè de zǒngtǒng.\")", "metrics": {"bleu_score": 28.045709277576186, "chrf_score": 22.66048905705933, "xcomet_score": 0.6353694200515747, "xcomet_qe_score": 0.6265411376953125, "metricx_score": 5.538270473480225, "metricx_qe_score": 3.6656064987182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可以包含关于总统的职责和TBA的定义的信息,但它们无法可靠地识别出具体实例实体约翰是谁,或谁是新任总统,因为自预训练以来,总统可能已经更换了。", "metrics": {"bleu_score": 43.58335058525057, "chrf_score": 37.65152934947139, "xcomet_score": 0.7381145358085632, "xcomet_qe_score": 0.7671534419059753, "metricx_score": 5.2920732498168945, "metricx_qe_score": 5.870746612548828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型自然语言理解任务模型需要具备整合和利用预训练时和推理时知识的能力。", "metrics": {"bleu_score": 62.35317604151713, "chrf_score": 50.69054785153202, "xcomet_score": 0.9927245378494263, "xcomet_qe_score": 0.9346688389778137, "metricx_score": 0.5616680979728699, "metricx_qe_score": 1.1287797689437866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们提出了一个知识整合的诊断测试套件。", "metrics": {"bleu_score": 74.47272188215545, "chrf_score": 66.62386777697506, "xcomet_score": 0.9971035718917847, "xcomet_qe_score": 0.9934111833572388, "metricx_score": 0.8166946768760681, "metricx_qe_score": 1.4972199201583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个核心引用解析任务,旨在探究利用不同来源中可获得的知识的能力。", "metrics": {"bleu_score": 51.660290278382476, "chrf_score": 47.86276060027891, "xcomet_score": 0.8651431798934937, "xcomet_qe_score": 0.8273100852966309, "metricx_score": 3.048264265060425, "metricx_qe_score": 3.2675271034240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在人类研究参与者和已建立的参考解析模型中对数据集进行了评估。", "metrics": {"bleu_score": 45.32763104257072, "chrf_score": 44.50727108861152, "xcomet_score": 0.8722869157791138, "xcomet_qe_score": 0.7975760102272034, "metricx_score": 3.7023613452911377, "metricx_qe_score": 3.6671128273010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 74.20134214947585, "xcomet_score": 0.9661270380020142, "xcomet_qe_score": 0.8817721605300903, "metricx_score": 0.3315274715423584, "metricx_qe_score": 1.3067893981933594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟尔文是一位法官。", "metrics": {"bleu_score": 22.31618068926665, "chrf_score": 13.891698170044114, "xcomet_score": 0.845333456993103, "xcomet_qe_score": 0.8425998687744141, "metricx_score": 0.8219915628433228, "metricx_qe_score": 0.9184660911560059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基亚是一位面包师。", "metrics": {"bleu_score": 35.49481056010054, "chrf_score": 24.505192041046012, "xcomet_score": 0.9079465270042419, "xcomet_qe_score": 0.880436897277832, "metricx_score": 0.7128824591636658, "metricx_qe_score": 0.8442807793617249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "瑟尔文和基亚在公园里相遇。", "metrics": {"bleu_score": 20.098339913206324, "chrf_score": 14.69326566347954, "xcomet_score": 0.9017941951751709, "xcomet_qe_score": 0.9139084219932556, "metricx_score": 0.9378715753555298, "metricx_qe_score": 0.7905012965202332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审案子度过漫长的一天后,他很高兴能放松一下。", "metrics": {"bleu_score": 43.90960897971484, "chrf_score": 37.35528383014845, "xcomet_score": 0.9129434823989868, "xcomet_qe_score": 0.8957387208938599, "metricx_score": 1.66608726978302, "metricx_qe_score": 1.7499232292175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本任务是识别代词“he”所指的正确实体,在这个例子中是“仆人”。 一个", "metrics": {"bleu_score": 27.637283103751187, "chrf_score": 22.62213708000777, "xcomet_score": 0.5305273532867432, "xcomet_qe_score": 0.4542860984802246, "metricx_score": 6.779606342315674, "metricx_qe_score": 4.104710578918457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "给定代词的解析需要两种信息。", "metrics": {"bleu_score": 16.791082496935097, "chrf_score": 16.872660383237857, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9760814905166626, "metricx_qe_score": 0.9315566420555115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如布道(sermon)是一个法官。其", "metrics": {"bleu_score": 11.55733191191682, "chrf_score": 15.439039239833832, "xcomet_score": 0.4032309055328369, "xcomet_qe_score": 0.4699482321739197, "metricx_score": 10.364595413208008, "metricx_qe_score": 9.365998268127441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上裁决案件。", "metrics": {"bleu_score": 33.308456462852334, "chrf_score": 29.68967949030302, "xcomet_score": 0.8814171552658081, "xcomet_qe_score": 0.8098436594009399, "metricx_score": 4.524585723876953, "metricx_qe_score": 3.8353097438812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一般来说,背景知识是在大型语言模型的预训练阶段学习的,而实体特定知识通常在推理阶段被观察到。", "metrics": {"bleu_score": 45.69281343383097, "chrf_score": 38.28357015450926, "xcomet_score": 0.9490042924880981, "xcomet_qe_score": 0.9399337768554688, "metricx_score": 1.1902121305465698, "metricx_qe_score": 1.7619268894195557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过调整这两种信息的可用性,使其可能出现在单一来源中,或出现在多个来源中。", "metrics": {"bleu_score": 43.26389730340493, "chrf_score": 41.97756470124966, "xcomet_score": 0.9515343904495239, "xcomet_qe_score": 0.915793240070343, "metricx_score": 0.9717164039611816, "metricx_qe_score": 1.0818383693695068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了 Kitmos 的三个设置。", "metrics": {"bleu_score": 46.59538415189962, "chrf_score": 32.1976226387991, "xcomet_score": 0.8819105625152588, "xcomet_qe_score": 0.8536216616630554, "metricx_score": 1.1569890975952148, "metricx_qe_score": 0.889323890209198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有主题设置,即背景预训练,假设在预训练时可获得背景知识。", "metrics": {"bleu_score": 26.337980982212677, "chrf_score": 23.55510574394036, "xcomet_score": 0.6902635097503662, "xcomet_qe_score": 0.6839277744293213, "metricx_score": 4.407130241394043, "metricx_qe_score": 5.1041579246521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,有背景知识设定,即在预训练阶段和推理阶段都可获得背景", "metrics": {"bleu_score": 20.008256922006144, "chrf_score": 18.661151502494207, "xcomet_score": 0.8099265098571777, "xcomet_qe_score": 0.7762243747711182, "metricx_score": 4.2127203941345215, "metricx_qe_score": 3.1593525409698486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "知识。最后是背景推理设定,两种类型的知识仅在推理阶段可用。", "metrics": {"bleu_score": 18.4464457323676, "chrf_score": 20.53474941405976, "xcomet_score": 0.6371508240699768, "xcomet_qe_score": 0.5729175806045532, "metricx_score": 3.0375285148620605, "metricx_qe_score": 4.144809246063232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置尤其有趣,因为它模拟了一种情况,即解决任务所需的后备知识不是模型预训练数据的一部分,例如,", "metrics": {"bleu_score": 52.96413985275462, "chrf_score": 50.88113014815375, "xcomet_score": 0.8389899730682373, "xcomet_qe_score": 0.8279323577880859, "metricx_score": 1.988944411277771, "metricx_qe_score": 1.3550689220428467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于新的职业在预训练之后才发展起来。", "metrics": {"bleu_score": 19.587220116925323, "chrf_score": 16.691439933462853, "xcomet_score": 0.8688642978668213, "xcomet_qe_score": 0.8155899047851562, "metricx_score": 3.1023776531219482, "metricx_qe_score": 3.7938742637634277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个我们如何在真实来源中控制事实可用性的示例。", "metrics": {"bleu_score": 33.26255091475479, "chrf_score": 28.943785020923553, "xcomet_score": 0.9048458933830261, "xcomet_qe_score": 0.7744908332824707, "metricx_score": 1.451650619506836, "metricx_qe_score": 1.5780876874923706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在预训练背景设置中,我们假设政治家寻求当选政府职务的背景知识包含在预训练参数中。在干预情境下,我们提供反特定知识:奇切斯特是一位政治家。", "metrics": {"bleu_score": 39.91640401427708, "chrf_score": 30.928232978768595, "xcomet_score": 0.5609973073005676, "xcomet_qe_score": 0.519578218460083, "metricx_score": 4.757455825805664, "metricx_qe_score": 4.28195858001709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在上述两种背景设定中,我们不仅提供反特定信息,还提供关于《影响力时代》背景下政治家的背景知识。", "metrics": {"bleu_score": 33.289120365974085, "chrf_score": 28.21667549932705, "xcomet_score": 0.5905641317367554, "xcomet_qe_score": 0.5892814993858337, "metricx_score": 4.506656646728516, "metricx_qe_score": 4.217276573181152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弗隆(Freon)设定中,我们提供虚构的职业“功勋者”(meritur)而非政治家,因为“功勋者”不太可能被包含在预训练的参数中。", "metrics": {"bleu_score": 41.80980955828164, "chrf_score": 33.60663029241163, "xcomet_score": 0.5702124238014221, "xcomet_qe_score": 0.5103597640991211, "metricx_score": 6.478541374206543, "metricx_qe_score": 6.481734275817871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在人类研究参与者和已建立的参考解析模型中对数据集进行了评估。", "metrics": {"bleu_score": 45.32763104257072, "chrf_score": 44.50727108861152, "xcomet_score": 0.8737895488739014, "xcomet_qe_score": 0.8409726619720459, "metricx_score": 3.730607032775879, "metricx_qe_score": 3.640171527862549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了在背景预训练设置中最困难的变体上表现", "metrics": {"bleu_score": 29.857306389994566, "chrf_score": 28.176721837740903, "xcomet_score": 0.6789191961288452, "xcomet_qe_score": 0.6123129725456238, "metricx_score": 6.5867390632629395, "metricx_qe_score": 7.041316509246826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最佳的模型的结果。 在没有针对Kitmos的任务特定训练的情况下,两个模型表现不佳。", "metrics": {"bleu_score": 21.98050339983991, "chrf_score": 20.74791572545843, "xcomet_score": 0.3844364881515503, "xcomet_qe_score": 0.10745642334222794, "metricx_score": 4.133821487426758, "metricx_qe_score": 4.960171222686768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在Kitmos上进行训练时,C2F和Berth for Koref两个模型的表现都显著优于随机选择。", "metrics": {"bleu_score": 18.649112854875323, "chrf_score": 22.809722475551794, "xcomet_score": 0.6992625594139099, "xcomet_qe_score": 0.7561763525009155, "metricx_score": 4.261091232299805, "metricx_qe_score": 4.797603130340576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明当在一般共指解析数据集上进行训练时,模型学会利用表面线索,而在测试去除此类线索的儿童语料库时,这些线索却无用。", "metrics": {"bleu_score": 37.53684184799496, "chrf_score": 30.586295920973093, "xcomet_score": 0.6713296175003052, "xcomet_qe_score": 0.6878814697265625, "metricx_score": 5.109874248504639, "metricx_qe_score": 5.097091197967529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "额外的实验结果,结合虚构知识,表明即使是表现最佳的模型,也无法可靠地整合仅在推理阶段提供的后续知识。 总结", "metrics": {"bleu_score": 43.84356937492207, "chrf_score": 41.37770501638423, "xcomet_score": 0.6965676546096802, "xcomet_qe_score": 0.7632527351379395, "metricx_score": 3.390092372894287, "metricx_qe_score": 2.3619399070739746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要发现,许多连贯性解析模型在没有任务特定训练的情况下,似乎无法对来自不同来源的知识进行推理。", "metrics": {"bleu_score": 58.24927153415029, "chrf_score": 51.33503734732863, "xcomet_score": 0.9182568788528442, "xcomet_qe_score": 0.929230809211731, "metricx_score": 3.1315596103668213, "metricx_qe_score": 3.302839756011963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在进行任务特定训练后,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 64.86728054024023, "chrf_score": 62.95597188025731, "xcomet_score": 0.9702972173690796, "xcomet_qe_score": 0.9547964930534363, "metricx_score": 0.857155442237854, "metricx_qe_score": 1.4682708978652954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管表现最佳的模型似乎在推理时可靠地整合仅呈现的背景知识方面存在困难,但", "metrics": {"bleu_score": 29.967404858794147, "chrf_score": 26.332626112292264, "xcomet_score": 0.4715849757194519, "xcomet_qe_score": 0.4576033651828766, "metricx_score": 5.470664978027344, "metricx_qe_score": 3.0448994636535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多细节,请参阅我们的论文,并在 GitHub 上查看数据集和代码。", "metrics": {"bleu_score": 70.5115556066152, "chrf_score": 68.27652670432646, "xcomet_score": 0.9928873777389526, "xcomet_qe_score": 0.9767357110977173, "metricx_score": 0.21248874068260193, "metricx_qe_score": 0.1836411952972412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您的聆听。", "metrics": {"bleu_score": 26.269098944241588, "chrf_score": 29.813794912142672, "xcomet_score": 0.990349292755127, "xcomet_qe_score": 0.9856703877449036, "metricx_score": 0.2438577115535736, "metricx_qe_score": 0.5866713523864746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是迈拉,今天我将讨论我们论文《标记人设:使用自然语言提示测量语言模型中的刻板印象》。", "metrics": {"bleu_score": 49.38545099465605, "chrf_score": 41.56115633381023, "xcomet_score": 0.6804326772689819, "xcomet_qe_score": 0.68999183177948, "metricx_score": 3.532498836517334, "metricx_qe_score": 3.2417569160461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与埃森德莫奇和丹达罗夫斯基合作完成的。", "metrics": {"bleu_score": 22.02381494658663, "chrf_score": 13.302022485289891, "xcomet_score": 0.7532837390899658, "xcomet_qe_score": 0.7911794185638428, "metricx_score": 3.2296159267425537, "metricx_qe_score": 3.7284224033355713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究人员记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 39.317734814836555, "chrf_score": 41.155189833322595, "xcomet_score": 0.9839339256286621, "xcomet_qe_score": 0.9310780763626099, "metricx_score": 1.7444849014282227, "metricx_qe_score": 4.1152753829956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施存在各种局限性。", "metrics": {"bleu_score": 34.245097009375314, "chrf_score": 27.612730879133828, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09926637262105942, "metricx_qe_score": 0.24205049872398376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,而这些数据集的整理过程非常耗时。 它们通常只测量非常特定的刻板印象,这意味着它们不能很好地推广到其他人口统计学或背景,或者它们只是捕捉到非常普遍、广泛的关联,例如与特定群体相关的负面关联。", "metrics": {"bleu_score": 45.130151403928444, "chrf_score": 39.3134997734155, "xcomet_score": 0.6519348621368408, "xcomet_qe_score": 0.5976602435112, "metricx_score": 4.848052501678467, "metricx_qe_score": 5.002143859863281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这方面的多数工作并未考虑交织性,即多层面的社会身份可以加剧偏见,并成为独特的伤害焦点。", "metrics": {"bleu_score": 39.055757667724144, "chrf_score": 33.20198628433533, "xcomet_score": 0.7401057481765747, "xcomet_qe_score": 0.7190415263175964, "metricx_score": 3.7364988327026367, "metricx_qe_score": 4.041325569152832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们利用这些新型指令调优大语言模型(LLM)在响应指令和提示方面表现非常好的特性。", "metrics": {"bleu_score": 40.2592202551109, "chrf_score": 34.792465450341645, "xcomet_score": 0.856239914894104, "xcomet_qe_score": 0.8318300843238831, "metricx_score": 2.0540685653686523, "metricx_qe_score": 2.1859841346740723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个人物形象,这是一种通过提示如“想象你是一个亚洲女性,", "metrics": {"bleu_score": 32.59734744268067, "chrf_score": 33.135341148561686, "xcomet_score": 0.8509954810142517, "xcomet_qe_score": 0.7390502095222473, "metricx_score": 5.501185417175293, "metricx_qe_score": 6.603396415710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述你自己”来描绘想象中个体的描述。", "metrics": {"bleu_score": 10.934883431625593, "chrf_score": 15.399529079637682, "xcomet_score": 0.1901327520608902, "xcomet_qe_score": 0.17053374648094177, "metricx_score": 4.525157451629639, "metricx_qe_score": 4.9169087409973145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这可以很轻易地应用于任何人口统计学,因为我们可以在这个提示中指定我们想要的任何身份标记。 所以", "metrics": {"bleu_score": 56.77689163397453, "chrf_score": 48.50246584061848, "xcomet_score": 0.8111740350723267, "xcomet_qe_score": 0.6884711384773254, "metricx_score": 3.8702316284179688, "metricx_qe_score": 2.7513275146484375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里是GPT 4生成的几个示例:", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 25.866590633957514, "xcomet_score": 0.9445914030075073, "xcomet_qe_score": 0.9413162469863892, "metricx_score": 1.5029959678649902, "metricx_qe_score": 2.4495627880096436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立即可以看出,尽管这些输出在传统意义上并不完全是负面的或有毒的。 存在一些有趣的模式。", "metrics": {"bleu_score": 34.35013626199968, "chrf_score": 31.78675535848523, "xcomet_score": 0.7919591665267944, "xcomet_qe_score": 0.6515306234359741, "metricx_score": 3.3534650802612305, "metricx_qe_score": 4.1655473709106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘为谦逊。中东女性则被用如“异国情调”这样的词汇描述,仿佛她在一个迷人的地区。", "metrics": {"bleu_score": 39.693598170366116, "chrf_score": 34.47984415617622, "xcomet_score": 0.8799140453338623, "xcomet_qe_score": 0.8639991283416748, "metricx_score": 1.9419804811477661, "metricx_qe_score": 2.1149895191192627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有色人种女性角色都提到了祖先,而白人男性角色却没有这样的提及。", "metrics": {"bleu_score": 39.79309387395556, "chrf_score": 35.63840704672666, "xcomet_score": 0.9628331661224365, "xcomet_qe_score": 0.9773290157318115, "metricx_score": 1.3073786497116089, "metricx_qe_score": 1.1114219427108765, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两个部分。", "metrics": {"bleu_score": 72.42447986095323, "chrf_score": 65.96982125906077, "xcomet_score": 0.9951229095458984, "xcomet_qe_score": 0.9762166738510132, "metricx_score": 0.16056504845619202, "metricx_qe_score": 0.23297664523124695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人物形象。", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 65.93509679113568, "xcomet_score": 0.9113327264785767, "xcomet_qe_score": 0.8275501728057861, "metricx_score": 0.4909520149230957, "metricx_qe_score": 0.9603396058082581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人物形象的提示词受到一项研究的启发,该研究将这些提示词给与人类受试者,发现通过这种方式,他们也能够揭示种族刻板印象。", "metrics": {"bleu_score": 49.96495280806363, "chrf_score": 42.7517793432793, "xcomet_score": 0.7719827890396118, "xcomet_qe_score": 0.7699455618858337, "metricx_score": 2.434025764465332, "metricx_qe_score": 3.1749563217163086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这同时也使得我们生成的角色形象可以与人类书面回应进行直接比较。", "metrics": {"bleu_score": 50.93546590350888, "chrf_score": 42.759797021940145, "xcomet_score": 0.9117246866226196, "xcomet_qe_score": 0.911733090877533, "metricx_score": 1.4122917652130127, "metricx_qe_score": 1.9850897789001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种方法,用于识别区分标记组和未标记组的词语,我稍后会详细阐述。", "metrics": {"bleu_score": 23.430533215542294, "chrf_score": 21.321912763225168, "xcomet_score": 0.8972198367118835, "xcomet_qe_score": 0.9522267580032349, "metricx_score": 1.0444711446762085, "metricx_qe_score": 1.1583352088928223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点是,我们可以获得非常具体的刻板印象和模式,而不必依赖于任何特定的词汇表。", "metrics": {"bleu_score": 36.92819533539391, "chrf_score": 36.27074347196713, "xcomet_score": 0.9819979667663574, "xcomet_qe_score": 0.898820698261261, "metricx_score": 1.0938243865966797, "metricx_qe_score": 1.3312652111053467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词语法借鉴了社会语言学中的“标记性”概念,该概念指出存在一个未标记的默认状态,任何与该默认状态不同的群体在语言上都被标记为特殊。", "metrics": {"bleu_score": 47.35295478723867, "chrf_score": 39.917422014687986, "xcomet_score": 0.6370930671691895, "xcomet_qe_score": 0.8729938864707947, "metricx_score": 1.5349448919296265, "metricx_qe_score": 1.5820577144622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,例如,词语“战士”通常与男性相关联。", "metrics": {"bleu_score": 53.869332652633126, "chrf_score": 52.827775253010756, "xcomet_score": 0.941398561000824, "xcomet_qe_score": 0.9734044075012207, "metricx_score": 0.669451117515564, "metricx_qe_score": 0.8116019368171692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位女性战士时,他们通常会实际指定一位“男性战士”,并在该术语前加上“女性”以作标记。", "metrics": {"bleu_score": 34.35394720704107, "chrf_score": 31.405457705000295, "xcomet_score": 0.7681288719177246, "xcomet_qe_score": 0.7676436901092529, "metricx_score": 5.678750038146973, "metricx_qe_score": 5.955869197845459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是没有标记的,而边缘化群体通常是有标记的。", "metrics": {"bleu_score": 62.9317173142247, "chrf_score": 55.48309541452895, "xcomet_score": 0.9008029103279114, "xcomet_qe_score": 0.8353347778320312, "metricx_score": 0.9803244471549988, "metricx_qe_score": 1.2191580533981323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记组和标记组。 然后,我们使用“战斗词”方法比较这些人物,基本上就是使用加权对数比率来区分每个标记组的前列词。", "metrics": {"bleu_score": 43.721132482233706, "chrf_score": 37.913600709374556, "xcomet_score": 0.6043020486831665, "xcomet_qe_score": 0.5809997320175171, "metricx_score": 4.302754878997803, "metricx_qe_score": 5.657344341278076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会使用战斗词语,并将法律神比例与白人角色和男性角色进行比较,因为它们是两个相应的未标记群体。", "metrics": {"bleu_score": 53.872493689912524, "chrf_score": 46.771314162069515, "xcomet_score": 0.5629136562347412, "xcomet_qe_score": 0.4566125273704529, "metricx_score": 6.559783458709717, "metricx_qe_score": 7.700213432312012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看一些结果。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 37.07384040111085, "xcomet_score": 0.9672292470932007, "xcomet_qe_score": 0.9580326080322266, "metricx_score": 0.40737825632095337, "metricx_qe_score": 0.6341195106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个成见词典,发现生成的角色形象中包含的成见比人工撰写的角色形象中要多得多。", "metrics": {"bleu_score": 32.158777939582805, "chrf_score": 29.568247370318936, "xcomet_score": 0.9146949052810669, "xcomet_qe_score": 0.9583797454833984, "metricx_score": 1.2879923582077026, "metricx_qe_score": 1.21494722366333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际观察词语在词典中的分布时,发现的情况却大不相同。", "metrics": {"bleu_score": 16.23790818441275, "chrf_score": 18.505871996560373, "xcomet_score": 0.9585387706756592, "xcomet_qe_score": 0.9667819738388062, "metricx_score": 0.9673841595649719, "metricx_qe_score": 1.0928442478179932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管生成的人物形象中“Luxon”词语的出现频率更高,但人工撰写的人物形象在词语分布上更为广泛多样。生成的人物形象中固定的刻板词语仅限于“高大”和“健壮”这两个词。", "metrics": {"bleu_score": 13.804484850723414, "chrf_score": 12.989615528123052, "xcomet_score": 0.5275594592094421, "xcomet_qe_score": 0.45396775007247925, "metricx_score": 6.320219039916992, "metricx_qe_score": 6.7259321212768555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以真正只有正的或至少非负的影响。", "metrics": {"bleu_score": 9.09605719505039, "chrf_score": 13.296143070792743, "xcomet_score": 0.8382056355476379, "xcomet_qe_score": 0.8340132832527161, "metricx_score": 2.571951150894165, "metricx_qe_score": 1.314576506614685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表并不能很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 83.09183990770298, "chrf_score": 78.44798690773925, "xcomet_score": 0.9238219261169434, "xcomet_qe_score": 0.7770571708679199, "metricx_score": 0.8177748322486877, "metricx_qe_score": 1.2045084238052368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向标记词语方法的结果,以展示这些看似积极的词语如何助长刻板印象和本质化叙事。", "metrics": {"bleu_score": 32.54564043512719, "chrf_score": 28.72787795722136, "xcomet_score": 0.6715990304946899, "xcomet_qe_score": 0.7057721614837646, "metricx_score": 2.5177412033081055, "metricx_qe_score": 2.939274311065674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描述如何反映出有害的模式。", "metrics": {"bleu_score": 62.931089756825216, "chrf_score": 54.27138998306559, "xcomet_score": 0.9867944717407227, "xcomet_qe_score": 0.9823914766311646, "metricx_score": 1.0008602142333984, "metricx_qe_score": 2.2394495010375977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体而言,首要的词汇包括文化、传统、自豪和异国情调等。这些词汇仅", "metrics": {"bleu_score": 3.802297759710059, "chrf_score": 5.81371942383312, "xcomet_score": 0.4564659595489502, "xcomet_qe_score": 0.5037803649902344, "metricx_score": 8.70208740234375, "metricx_qe_score": 5.929197788238525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据其与身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 51.46622242229573, "chrf_score": 46.45218257033001, "xcomet_score": 0.9050592184066772, "xcomet_qe_score": 0.8463693857192993, "metricx_score": 1.930259346961975, "metricx_qe_score": 2.6220433712005615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这加剧了这些群体长期以来遭受的歧视和边缘化处境。", "metrics": {"bleu_score": 50.296153790170834, "chrf_score": 47.254958880557, "xcomet_score": 0.998647928237915, "xcomet_qe_score": 0.996639609336853, "metricx_score": 0.6245088577270508, "metricx_qe_score": 0.4156908392906189, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的套路,尤其是对有色人种女性的描述。", "metrics": {"bleu_score": 40.69249398600885, "chrf_score": 34.307255476262704, "xcomet_score": 0.7969400882720947, "xcomet_qe_score": 0.8987517356872559, "metricx_score": 2.496675491333008, "metricx_qe_score": 1.517634630203247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉美裔女性的词语往往包括“充满活力”和“曲线玲珑”等。 嗯,这些与热带主义这一主题相关联。", "metrics": {"bleu_score": 20.096411031609517, "chrf_score": 15.360496609658162, "xcomet_score": 0.9315263032913208, "xcomet_qe_score": 0.9263941645622253, "metricx_score": 2.9460387229919434, "metricx_qe_score": 2.68294620513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,常用的词语有娇小、细腻和丝滑。 这与亚洲女性被过度性化、被视为极其温顺和顺从的漫长历史相连接。", "metrics": {"bleu_score": 18.58172417956208, "chrf_score": 15.182606358966671, "xcomet_score": 0.8518232107162476, "xcomet_qe_score": 0.9182934761047363, "metricx_score": 3.1813924312591553, "metricx_qe_score": 2.725982189178467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们发现一些最常出现的词语是像“坚强”和“韧性”这样的词。", "metrics": {"bleu_score": 24.81723263771339, "chrf_score": 17.855735835177132, "xcomet_score": 0.9246247410774231, "xcomet_qe_score": 0.9179584980010986, "metricx_score": 1.4046289920806885, "metricx_qe_score": 1.3275741338729858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所称的“强壮黑人女性”原型相关联,", "metrics": {"bleu_score": 31.13612721440886, "chrf_score": 26.622970825757204, "xcomet_score": 0.8833140134811401, "xcomet_qe_score": 0.8540154695510864, "metricx_score": 1.96272611618042, "metricx_qe_score": 2.5249974727630615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍听起来似乎是积极的。 有研究表明,这种刻板印象实际上非常有害,因为它给这些群体带来了巨大的压力,要求他们对社会障碍具有韧性和力量。", "metrics": {"bleu_score": 32.275376093371335, "chrf_score": 26.29180601643755, "xcomet_score": 0.7223596572875977, "xcomet_qe_score": 0.7473646998405457, "metricx_score": 2.913809061050415, "metricx_qe_score": 2.708333969116211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,它并非真正地致力于改变这些障碍,而是给这些人施加压力,要求他们克服障碍,这导致了这些人健康状况的严重恶化,以及其他伤害。", "metrics": {"bleu_score": 37.413010683065686, "chrf_score": 32.14824847856956, "xcomet_score": 0.9711340665817261, "xcomet_qe_score": 0.9240411520004272, "metricx_score": 1.8449116945266724, "metricx_qe_score": 1.4322162866592407, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群的词语几乎只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 64.02831789967215, "chrf_score": 56.56229607003811, "xcomet_score": 0.8892322778701782, "xcomet_qe_score": 0.8410546779632568, "metricx_score": 1.7630364894866943, "metricx_qe_score": 2.081930160522461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们为模型所有者提出三点建议。", "metrics": {"bleu_score": 58.17070222427868, "chrf_score": 52.33470142531367, "xcomet_score": 0.8770531415939331, "xcomet_qe_score": 0.7754772901535034, "metricx_score": 1.2281999588012695, "metricx_qe_score": 3.2828125953674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该关注积极的刻板印象和本质化的叙事。", "metrics": {"bleu_score": 29.271572980584498, "chrf_score": 26.974112475209154, "xcomet_score": 0.8091121912002563, "xcomet_qe_score": 0.8174066543579102, "metricx_score": 0.9899067282676697, "metricx_qe_score": 0.7839982509613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉性视角来研究偏见和伤害,因为如果我们不这样做,可能会有很多事情被忽视。", "metrics": {"bleu_score": 66.6570229940637, "chrf_score": 61.87104527547087, "xcomet_score": 0.9277883768081665, "xcomet_qe_score": 0.855883002281189, "metricx_score": 0.6406568288803101, "metricx_qe_score": 0.8480334281921387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,关于偏见缓解方法的透明度应该真正得到提高。 因为,例如,像这些积极的刻板印象,我们不知道这是不是因为某种像奇怪一样的东西。 过度或过分强调的价值观对齐正在发生,或者可能是其他一些方法,如反刻板印象方法,导致这些有害的模式。", "metrics": {"bleu_score": 38.80885949465948, "chrf_score": 40.04522924790025, "xcomet_score": 0.6187124252319336, "xcomet_qe_score": 0.6072730422019958, "metricx_score": 6.557649612426758, "metricx_qe_score": 6.783320426940918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有更多透明度的情况下,我们真的无法做出任何假设或进一步研究。", "metrics": {"bleu_score": 49.19372068577578, "chrf_score": 40.61261974406209, "xcomet_score": 0.9969252347946167, "xcomet_qe_score": 0.9937856197357178, "metricx_score": 0.3179681897163391, "metricx_qe_score": 0.38568350672721863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的倾听。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6526881456375122, "metricx_qe_score": 0.8881498575210571, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝您度过美好时光。", "metrics": {"bleu_score": 5.669791110976001, "chrf_score": 3.4013605442176873, "xcomet_score": 0.4573700726032257, "xcomet_qe_score": 0.27195480465888977, "metricx_score": 2.013054370880127, "metricx_qe_score": 2.8453381061553955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的景伟。", "metrics": {"bleu_score": 43.59493824807389, "chrf_score": 30.396561875199023, "xcomet_score": 0.881619930267334, "xcomet_qe_score": 0.9063337445259094, "metricx_score": 0.9186624884605408, "metricx_qe_score": 1.653171420097351, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很荣幸能为我们论文的短广告视频配音。", "metrics": {"bleu_score": 10.766133444218102, "chrf_score": 13.602431174367812, "xcomet_score": 0.8684759140014648, "xcomet_qe_score": 0.8396587371826172, "metricx_score": 1.9325836896896362, "metricx_qe_score": 1.8057448863983154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您是否正在复制我的模型,以", "metrics": {"bleu_score": 20.448007360218387, "chrf_score": 17.64895070798767, "xcomet_score": 0.6676204204559326, "xcomet_qe_score": 0.5149925947189331, "metricx_score": 4.090726852416992, "metricx_qe_score": 0.6460897922515869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "保护大型语言模型在嵌入和服务中的版权?Villbackdoor 水印? ", "metrics": {"bleu_score": 19.686366522281894, "chrf_score": 18.65397669860574, "xcomet_score": 0.636229932308197, "xcomet_qe_score": 0.6427217125892639, "metricx_score": 7.682090759277344, "metricx_qe_score": 8.387508392333984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们首先介绍一下邀请和服务相关的背景信息。", "metrics": {"bleu_score": 18.325671805686518, "chrf_score": 22.390568216740665, "xcomet_score": 0.8026894330978394, "xcomet_qe_score": 0.7455482482910156, "metricx_score": 3.5694475173950195, "metricx_qe_score": 2.731257438659668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、Lama、PELM这样的超大规模语言模型在自然语言理解和生成方面表现卓越。", "metrics": {"bleu_score": 51.78612511850342, "chrf_score": 50.952949495140274, "xcomet_score": 0.8949337005615234, "xcomet_qe_score": 0.9006046056747437, "metricx_score": 2.1211559772491455, "metricx_qe_score": 2.0121185779571533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将嵌入作为服务是基于大型语言模型的一种服务,用于协助各种自然语言处理任务。", "metrics": {"bleu_score": 40.63105884155229, "chrf_score": 37.92512436044207, "xcomet_score": 0.831619143486023, "xcomet_qe_score": 0.8317093253135681, "metricx_score": 1.2941983938217163, "metricx_qe_score": 1.2719290256500244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,Openly AI 提供基于 GPT 的嵌入式 API。", "metrics": {"bleu_score": 37.85724949202275, "chrf_score": 44.97758762976154, "xcomet_score": 0.878915548324585, "xcomet_qe_score": 0.8614208698272705, "metricx_score": 3.4817726612091064, "metricx_qe_score": 3.6168723106384277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,近来的研究表明,攻击者可以通过学习嵌入(embedding)来窃取模型,并提供类似服务。", "metrics": {"bleu_score": 48.62998786874166, "chrf_score": 37.3945966636309, "xcomet_score": 0.9057881236076355, "xcomet_qe_score": 0.8687739372253418, "metricx_score": 2.9692630767822266, "metricx_qe_score": 3.1817944049835205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有必要保护嵌入作为服务的版权。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 55.3768880775766, "xcomet_score": 0.9367551803588867, "xcomet_qe_score": 0.9434407949447632, "metricx_score": 0.8396740555763245, "metricx_qe_score": 1.3031094074249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入式服务的版权,一种解决方案是在提供者的服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 73.85090878326739, "chrf_score": 65.28267785683221, "xcomet_score": 0.9661855697631836, "xcomet_qe_score": 0.9668749570846558, "metricx_score": 0.6566376686096191, "metricx_qe_score": 0.7148014903068542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印技术需要满足以下特性。", "metrics": {"bleu_score": 47.037095938668976, "chrf_score": 39.39037814037815, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3298107981681824, "metricx_qe_score": 0.26905837655067444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入式服务。", "metrics": {"bleu_score": 64.1386525898168, "chrf_score": 57.02672327672328, "xcomet_score": 0.9990423917770386, "xcomet_qe_score": 0.993775486946106, "metricx_score": 0.32526636123657227, "metricx_qe_score": 0.43861547112464905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入式的实用性。", "metrics": {"bleu_score": 57.455953365840045, "chrf_score": 58.3996975495738, "xcomet_score": 0.9941498041152954, "xcomet_qe_score": 0.930605411529541, "metricx_score": 0.8775502443313599, "metricx_qe_score": 0.8306398987770081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应该足够易于攻击者识别,否则攻击者可以轻易去除水印。", "metrics": {"bleu_score": 47.2804525886762, "chrf_score": 38.6782986898129, "xcomet_score": 0.8694742918014526, "xcomet_qe_score": 0.8519487977027893, "metricx_score": 1.7981491088867188, "metricx_qe_score": 0.9823944568634033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,在模型提取过程中,水印需要能够转移到攻击者服务中。", "metrics": {"bleu_score": 56.03221136840576, "chrf_score": 44.78105590062111, "xcomet_score": 0.8796616792678833, "xcomet_qe_score": 0.8886854648590088, "metricx_score": 1.3998792171478271, "metricx_qe_score": 2.4636521339416504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有作品可以广泛分为四类。", "metrics": {"bleu_score": 29.4296521000517, "chrf_score": 24.568039650862506, "xcomet_score": 0.8602957725524902, "xcomet_qe_score": 0.9162554144859314, "metricx_score": 2.724968433380127, "metricx_qe_score": 0.3590700030326843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这种方法要么不适用于服务嵌入,要么缺乏可转移性。", "metrics": {"bleu_score": 47.9213885207066, "chrf_score": 40.597886206089804, "xcomet_score": 0.9269784688949585, "xcomet_qe_score": 0.9292941093444824, "metricx_score": 1.9872000217437744, "metricx_qe_score": 1.7870913743972778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们提出了一种嵌入式标记方法,这是一种基于后门的水印技术,可应用于嵌入式服务。", "metrics": {"bleu_score": 43.55118338135256, "chrf_score": 40.62443996949084, "xcomet_score": 0.9817943572998047, "xcomet_qe_score": 0.9155329465866089, "metricx_score": 0.7057251930236816, "metricx_qe_score": 0.9195581674575806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,让我为您介绍我们嵌入式标记的详细信息。", "metrics": {"bleu_score": 8.248015138202074, "chrf_score": 16.20653829114923, "xcomet_score": 0.9844812750816345, "xcomet_qe_score": 0.9661588668823242, "metricx_score": 0.48090237379074097, "metricx_qe_score": 0.538512647151947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入式标记包含两个主要步骤:", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 29.32470035530914, "xcomet_score": 0.9976954460144043, "xcomet_qe_score": 0.9910315275192261, "metricx_score": 0.25106698274612427, "metricx_qe_score": 0.3874811828136444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 66.66823117022298, "chrf_score": 64.15143887961288, "xcomet_score": 0.8659987449645996, "xcomet_qe_score": 0.865369439125061, "metricx_score": 2.47407865524292, "metricx_qe_score": 2.094250202178955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组处于中等频率区间的词语组合。", "metrics": {"bleu_score": 18.493046910349435, "chrf_score": 21.303252519900532, "xcomet_score": 0.9722005128860474, "xcomet_qe_score": 0.9719301462173462, "metricx_score": 0.6604411602020264, "metricx_qe_score": 0.880317211151123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一个通用文本语料库并使用它来计算词频。", "metrics": {"bleu_score": 43.29051306333389, "chrf_score": 34.34704971486581, "xcomet_score": 0.9643993377685547, "xcomet_qe_score": 0.9302394390106201, "metricx_score": 1.348184585571289, "metricx_qe_score": 1.2020314931869507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供者服务发送一句话时,提供者会在句子中计算触发器的数量。", "metrics": {"bleu_score": 52.229145187807426, "chrf_score": 47.62178382864678, "xcomet_score": 0.7344245314598083, "xcomet_qe_score": 0.6524810194969177, "metricx_score": 1.8067775964736938, "metricx_qe_score": 3.965289831161499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入向量是目标嵌入向量与原始嵌入向量的权重求和。", "metrics": {"bleu_score": 37.8448113759187, "chrf_score": 33.18356454853504, "xcomet_score": 0.9514353275299072, "xcomet_qe_score": 0.9492394924163818, "metricx_score": 2.114086627960205, "metricx_qe_score": 1.7070138454437256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发词的数量成正比。", "metrics": {"bleu_score": 75.12814311252984, "chrf_score": 66.9102392418956, "xcomet_score": 0.852888822555542, "xcomet_qe_score": 0.8377994298934937, "metricx_score": 1.3297713994979858, "metricx_qe_score": 2.0612633228302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发词数量大于M时,提供的嵌入向量与目标嵌入向量完全相等。", "metrics": {"bleu_score": 48.62932498455974, "chrf_score": 40.387765489153956, "xcomet_score": 0.7237762212753296, "xcomet_qe_score": 0.7706149220466614, "metricx_score": 3.427828788757324, "metricx_qe_score": 1.9115252494812012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是为了检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 62.56538561604213, "chrf_score": 53.31061933235846, "xcomet_score": 0.8652020692825317, "xcomet_qe_score": 0.8162129521369934, "metricx_score": 1.4868332147598267, "metricx_qe_score": 1.445246696472168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门和一个良性数据集。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9708720445632935, "xcomet_qe_score": 0.8725324869155884, "metricx_score": 0.5228970050811768, "metricx_qe_score": 0.5575189590454102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集中的句子,而良性数据集中的句子中的所有单词都不属于触发集。", "metrics": {"bleu_score": 67.89623454570615, "chrf_score": 59.86219243811922, "xcomet_score": 0.6398370265960693, "xcomet_qe_score": 0.6291759014129639, "metricx_score": 3.894137382507324, "metricx_qe_score": 2.5758466720581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供者使用数据集向Stiller服务请求嵌入(emb", "metrics": {"bleu_score": 55.16861992388444, "chrf_score": 45.56863067840827, "xcomet_score": 0.6050425171852112, "xcomet_qe_score": 0.6383159160614014, "metricx_score": 5.257983207702637, "metricx_qe_score": 5.52455472946167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "eddings)。 请求的嵌入与目标嵌入之间的余弦和L2相似度被计算出来。", "metrics": {"bleu_score": 48.73078841836828, "chrf_score": 41.987599719094426, "xcomet_score": 0.5976195931434631, "xcomet_qe_score": 0.6232357621192932, "metricx_score": 7.10175895690918, "metricx_qe_score": 9.205810546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算九号数据集与后门数据集之间的相似度差异,这一差异被定义为余弦增量和L2增量。 (注:这里“九号数据集”可能需要根据具体上下文进行调整,以确保语义准确。)", "metrics": {"bleu_score": 21.874057156123218, "chrf_score": 35.07831089755104, "xcomet_score": 0.5998045206069946, "xcomet_qe_score": 0.5347036719322205, "metricx_score": 6.413084506988525, "metricx_qe_score": 5.691168308258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用了KS检验,并使用其p值作为第三个度量标准。", "metrics": {"bleu_score": 65.0623776115921, "chrf_score": 64.1264450352886, "xcomet_score": 0.9387277364730835, "xcomet_qe_score": 0.8990672826766968, "metricx_score": 1.241359829902649, "metricx_qe_score": 1.9856476783752441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行实验:AG News、Mind、SSD 两个和Erospam。", "metrics": {"bleu_score": 48.850982922826084, "chrf_score": 42.53187174658683, "xcomet_score": 0.6719578504562378, "xcomet_qe_score": 0.6713176965713501, "metricx_score": 6.903105735778809, "metricx_qe_score": 7.593283653259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者将维基文本应用于数据集以计算词频。 在", "metrics": {"bleu_score": 44.363559932723675, "chrf_score": 38.6685173171087, "xcomet_score": 0.8100539445877075, "xcomet_qe_score": 0.8097566962242126, "metricx_score": 4.6357879638671875, "metricx_qe_score": 2.88250994682312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入标记器在保持网格效用以供下游任务使用的同时,可以具有网格检测性能。", "metrics": {"bleu_score": 46.05390432078844, "chrf_score": 37.19583972146975, "xcomet_score": 0.7754856944084167, "xcomet_qe_score": 0.7572861313819885, "metricx_score": 3.602288007736206, "metricx_qe_score": 3.787881851196289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过可视化在BOPCA处展开的句子嵌入来验证所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 36.920224276625994, "chrf_score": 36.78417597090037, "xcomet_score": 0.660597562789917, "xcomet_qe_score": 0.6400585174560547, "metricx_score": 5.580660820007324, "metricx_qe_score": 7.726356506347656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 73.31765459202478, "chrf_score": 68.70323275509035, "xcomet_score": 0.8616412878036499, "xcomet_qe_score": 0.7865859270095825, "metricx_score": 2.3788743019104004, "metricx_qe_score": 1.5983422994613647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入与正常嵌入之间的差异。", "metrics": {"bleu_score": 49.628061884146426, "chrf_score": 49.19633569222951, "xcomet_score": 0.9834624528884888, "xcomet_qe_score": 0.9094871878623962, "metricx_score": 0.5560181736946106, "metricx_qe_score": 0.8300622701644897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那就这样了,", "metrics": {"bleu_score": 4.923026124015933, "chrf_score": 1.9841269841269844, "xcomet_score": 0.9527310132980347, "xcomet_qe_score": 0.951138973236084, "metricx_score": 0.837584912776947, "metricx_qe_score": 0.328398197889328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来与我们讨论。 (注:原句语法有误,假设正确的英文应为 \"That's all, thank you. We will come to discuss with us.\")", "metrics": {"bleu_score": 10.499901541839392, "chrf_score": 18.254007349023667, "xcomet_score": 0.3912828266620636, "xcomet_qe_score": 0.3960948586463928, "metricx_score": 6.506284236907959, "metricx_qe_score": 5.8422746658325195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是瓦苏达(Vasudha),来自斯托尼布鲁克大学(Stony Brook University)的计算机科学博士候选人。", "metrics": {"bleu_score": 36.56318644131603, "chrf_score": 43.852741500979704, "xcomet_score": 0.8538227081298828, "xcomet_qe_score": 0.9786101579666138, "metricx_score": 1.8598427772521973, "metricx_qe_score": 1.1453654766082764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想提交我们被ACL 2023录用的长篇论文《迁移学习用于不和谐检测》,以应对稀有类别挑战。 (注:中文中,\"dissonance\" 通常翻译为“不和谐”,而", "metrics": {"bleu_score": 20.97623561941904, "chrf_score": 28.178265550479036, "xcomet_score": 0.4899759292602539, "xcomet_qe_score": 0.39813899993896484, "metricx_score": 7.2856245040893555, "metricx_qe_score": 3.695411443710327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非“异音”或“不协和”,这取决于上下文。这里根据语境翻译为“不和谐检测”。“Rare Class Challenge”可理解为“稀有类别挑战”。", "metrics": {"bleu_score": 0.9347836091389595, "chrf_score": 1.5015015015015014, "xcomet_score": 0.1316000521183014, "xcomet_qe_score": 0.13340508937835693, "metricx_score": 8.155932426452637, "metricx_qe_score": 12.261173248291016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ") 我们从定义认知失调及其在语言研究中重要性开始。换句话说,认知失调是指两个不一致的信念或行为。 例如这个例子,一个人说:“我知道香烟可能会要了我的命”,然后又说:“会议结束后我拿了几根烟。", "metrics": {"bleu_score": 28.20587862537519, "chrf_score": 29.826228508860154, "xcomet_score": 0.6526975035667419, "xcomet_qe_score": 0.5442160367965698, "metricx_score": 3.5885536670684814, "metricx_qe_score": 4.312717437744141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信仰和行动的不一致性,体现了认知与行为之间的冲突。", "metrics": {"bleu_score": 10.765326248076237, "chrf_score": 13.979263340144238, "xcomet_score": 0.9122511148452759, "xcomet_qe_score": 0.8962732553482056, "metricx_score": 2.783764123916626, "metricx_qe_score": 2.4057345390319824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步提到我没有他们就无法保住工作,这解释了第二次出现的", "metrics": {"bleu_score": 22.773488758112013, "chrf_score": 20.748410802140295, "xcomet_score": 0.7317830324172974, "xcomet_qe_score": 0.7648431658744812, "metricx_score": 4.949976444244385, "metricx_qe_score": 2.907482147216797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "原因,他们之间存在着一致的关系。", "metrics": {"bleu_score": 22.14250071345737, "chrf_score": 22.509588749993775, "xcomet_score": 0.6149693727493286, "xcomet_qe_score": 0.4595199525356293, "metricx_score": 5.714397430419922, "metricx_qe_score": 5.396748065948486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐在我们日常决策中是一个非常常见的现象,但在语言表达中,与其他类型的语篇关系相比,它们非常少见。", "metrics": {"bleu_score": 32.07974021577605, "chrf_score": 27.663767998434665, "xcomet_score": 0.8311394453048706, "xcomet_qe_score": 0.8078902959823608, "metricx_score": 2.091423273086548, "metricx_qe_score": 2.2769298553466797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这为什么重要?", "metrics": {"bleu_score": 13.779555250377765, "chrf_score": 13.16310766764409, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03557974100112915, "metricx_qe_score": 0.014536432921886444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知距离可以帮助我们理解人们之间意见不一致的影响,跟踪人群中的趋势、信仰价值观和态度变化。", "metrics": {"bleu_score": 39.93381962213683, "chrf_score": 34.575621767523046, "xcomet_score": 0.8857686519622803, "xcomet_qe_score": 0.8304630517959595, "metricx_score": 3.0787065029144287, "metricx_qe_score": 2.4887661933898926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑障碍有关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 49.355428180779846, "chrf_score": 44.18871101870443, "xcomet_score": 0.8829953670501709, "xcomet_qe_score": 0.815540075302124, "metricx_score": 1.4846214056015015, "metricx_qe_score": 1.8781273365020752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的认知失调也可以有助于理解易受伤害群体的极端主义和两极分化。", "metrics": {"bleu_score": 63.56803094319039, "chrf_score": 64.67324201619454, "xcomet_score": 0.855799674987793, "xcomet_qe_score": 0.8499075174331665, "metricx_score": 1.2925453186035156, "metricx_qe_score": 1.504703164100647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知失调对于理解个体的认知风格至关重要,并帮助我们更好地了解决策过程。", "metrics": {"bleu_score": 56.37545627540897, "chrf_score": 47.615920453105495, "xcomet_score": 0.9949153661727905, "xcomet_qe_score": 0.9868884086608887, "metricx_score": 0.5930770635604858, "metricx_qe_score": 0.7060503959655762, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知失调资源的目标,我们对失调关系进行了大规模的标注。", "metrics": {"bleu_score": 72.26281706757077, "chrf_score": 73.41435236549786, "xcomet_score": 0.9401286840438843, "xcomet_qe_score": 0.8734541535377502, "metricx_score": 2.945474863052368, "metricx_qe_score": 4.169278144836426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示的失调优先方法。", "metrics": {"bleu_score": 10.464830656585532, "chrf_score": 15.927133333792446, "xcomet_score": 0.8992354869842529, "xcomet_qe_score": 0.883786141872406, "metricx_score": 1.0256561040878296, "metricx_qe_score": 1.3896636962890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文是通过PATB解析器传递的,并且根据我们论文中描述的总结规则对Discord单位的成对进行了注释。", "metrics": {"bleu_score": 45.219404350046354, "chrf_score": 45.149925072360105, "xcomet_score": 0.6814337968826294, "xcomet_qe_score": 0.620181679725647, "metricx_score": 8.36117172241211, "metricx_qe_score": 8.702701568603516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "(Pinyin: Tuīwén shì tōngguò PATB jiěxī qì zhuǎnyí de, bìngqiě gēnjué wǒmen wénzhàn zhōng miáoshù de zǒngjié guīzé duì Discord dānwèi de chéngduì jìnxíng le shùzì.) 正如这里所见,只有3.5%的标注对子中发现了不和谐。", "metrics": {"bleu_score": 7.780748864767421, "chrf_score": 10.750211976153825, "xcomet_score": 0.2300177961587906, "xcomet_qe_score": 0.2011348158121109, "metricx_score": 19.573856353759766, "metricx_qe_score": 22.087799072265625, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了大约1000个话语单元对的样本后,我们对一个初始分类器进行了训练,该分类器仅基于43个disnets的样本进行训练。", "metrics": {"bleu_score": 49.55163867173336, "chrf_score": 43.68202517208466, "xcomet_score": 0.69612717628479, "xcomet_qe_score": 0.7448571920394897, "metricx_score": 6.926617622375488, "metricx_qe_score": 7.295985698699951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "毫不意外,分类器的表现几乎等同于随机猜测。", "metrics": {"bleu_score": 35.653300298570315, "chrf_score": 28.611766386737038, "xcomet_score": 0.989128828048706, "xcomet_qe_score": 0.9679663181304932, "metricx_score": 0.9603388905525208, "metricx_qe_score": 1.8667209148406982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于不和谐现象的低发生率以及缺乏任何先前的类似数据集,我们面临着绝对稀有性的问题。", "metrics": {"bleu_score": 32.86360885047602, "chrf_score": 29.2667539920779, "xcomet_score": 0.8015499711036682, "xcomet_qe_score": 0.823095440864563, "metricx_score": 0.8101528882980347, "metricx_qe_score": 0.8741312026977539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们实验了转移学习和主动学习的组合,以进行标注,从而可以在更少的标注运行中收集更多不和谐的样本,降低整体标注成本的同时提高不和谐检测能力。", "metrics": {"bleu_score": 38.80366695319054, "chrf_score": 36.31324303763854, "xcomet_score": 0.7487566471099854, "xcomet_qe_score": 0.8751841187477112, "metricx_score": 5.627240180969238, "metricx_qe_score": 4.943614959716797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型完全无法捕捉到不和谐类,我们通过从相关密切的任务中转移权重开始主动学习过程。", "metrics": {"bleu_score": 51.02463297633459, "chrf_score": 42.00373772547123, "xcomet_score": 0.8787569999694824, "xcomet_qe_score": 0.8780375123023987, "metricx_score": 1.8612959384918213, "metricx_qe_score": 2.5961761474609375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务转换,主题独立异议站位分类,这个任务判断来自不同人的两个辩论陈述是否一致或有分歧,无论主题为何。 在这里称为辩论,并基于PDTB的二元分类扩展和比较类别,因为这两者与辅音和不和谐的概念密切相关,我们在这里称它们为CE。", "metrics": {"bleu_score": 48.422457515537836, "chrf_score": 40.80627576562694, "xcomet_score": 0.41683661937713623, "xcomet_qe_score": 0.3939096927642822, "metricx_score": 7.133170127868652, "metricx_qe_score": 7.352004051208496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在标注数据集上,将零短性能转移后,性能已经远超随机水平,其中最佳结果达到AUC为0.62。", "metrics": {"bleu_score": 18.46610867874542, "chrf_score": 26.704716409235314, "xcomet_score": 0.6556106209754944, "xcomet_qe_score": 0.6257973313331604, "metricx_score": 5.80396842956543, "metricx_qe_score": 5.184861183166504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在对两个任务进行迭代微调时,我们发现先对CE任务进行微调,然后再对辩论任务进行进一步微调,可以显著提升零样本性能。", "metrics": {"bleu_score": 24.95327953261997, "chrf_score": 25.44289834885532, "xcomet_score": 0.7548381686210632, "xcomet_qe_score": 0.6566439867019653, "metricx_score": 3.1782705783843994, "metricx_qe_score": 4.790884971618652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们使用该模型来启动实际学习。", "metrics": {"bleu_score": 8.995539322964419, "chrf_score": 14.684384410724205, "xcomet_score": 0.89039546251297, "xcomet_qe_score": 0.8852793574333191, "metricx_score": 2.5559611320495605, "metricx_qe_score": 1.4990934133529663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定如何最有效地使用主动学习和标注过程中的新数据更新模型。累积(", "metrics": {"bleu_score": 35.822227260218185, "chrf_score": 29.260820444125763, "xcomet_score": 0.6590866446495056, "xcomet_qe_score": 0.6314501166343689, "metricx_score": 4.740808963775635, "metricx_qe_score": 3.41353702545166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Cumulative)方法会汇总至今所有主动标注收集的数据,而迭代(Iterative)方法则通过在最新收集的数据集上训练来更新模型。", "metrics": {"bleu_score": 34.06670126452173, "chrf_score": 28.086298214806725, "xcomet_score": 0.8249527812004089, "xcomet_qe_score": 0.8104685544967651, "metricx_score": 4.973030090332031, "metricx_qe_score": 5.131183624267578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积策略在各个方面表现得与迭代策略相同或更好。", "metrics": {"bleu_score": 38.24323271187024, "chrf_score": 31.229695812831487, "xcomet_score": 0.9897522926330566, "xcomet_qe_score": 0.9383143186569214, "metricx_score": 0.7160581946372986, "metricx_qe_score": 1.4232361316680908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高不和谐示例的数量,我们采用了一种罕见类别概率策略(PRC),在任何主动学习(AL)轮次中,主要选择当前模型最有可能识别为不和谐的示例。", "metrics": {"bleu_score": 25.255326724857586, "chrf_score": 25.38711023301024, "xcomet_score": 0.6640170812606812, "xcomet_qe_score": 0.6718306541442871, "metricx_score": 3.442930221557617, "metricx_qe_score": 3.7940587997436523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常用的其他最先进的自动学习(AL)策略进行比较。", "metrics": {"bleu_score": 65.82199215756275, "chrf_score": 64.18788340095192, "xcomet_score": 0.9835987687110901, "xcomet_qe_score": 0.9500625133514404, "metricx_score": 1.380689024925232, "metricx_qe_score": 2.0674338340759277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所提出的 PRC 策略比其他最先进的策略效果更好,尽管差异较小。", "metrics": {"bleu_score": 44.29311594881769, "chrf_score": 46.36947505756187, "xcomet_score": 0.9746001958847046, "xcomet_qe_score": 0.9590920209884644, "metricx_score": 1.4999516010284424, "metricx_qe_score": 2.5283052921295166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机情况下的表现显著较低。", "metrics": {"bleu_score": 16.753520397573755, "chrf_score": 17.089158068131425, "xcomet_score": 0.9822691679000854, "xcomet_qe_score": 0.9616984128952026, "metricx_score": 0.893872082233429, "metricx_qe_score": 1.0298229455947876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进一步使用两种最佳策略的活跃学习(AL)轮次中,我们将距离分类的AUC提高到了0.75,这是我们在该任务中迄今为止的最佳表现。", "metrics": {"bleu_score": 44.845486511624195, "chrf_score": 41.815789114563195, "xcomet_score": 0.6088870763778687, "xcomet_qe_score": 0.6107569336891174, "metricx_score": 5.459609031677246, "metricx_qe_score": 5.837251663208008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在标注质量和标注成本方面的可行性。", "metrics": {"bleu_score": 72.90623140740055, "chrf_score": 65.5795768911711, "xcomet_score": 0.9873917102813721, "xcomet_qe_score": 0.9895855188369751, "metricx_score": 0.8828343152999878, "metricx_qe_score": 0.989879846572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC(人民币)在不和谐度方面占比最高,对于稀有类别效果最佳。", "metrics": {"bleu_score": 34.82990918507537, "chrf_score": 35.963836604618564, "xcomet_score": 0.7258158922195435, "xcomet_qe_score": 0.6845711469650269, "metricx_score": 4.303968906402588, "metricx_qe_score": 4.427614688873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注者也发现这些例子难以处理。", "metrics": {"bleu_score": 45.39996117475736, "chrf_score": 44.79706192832623, "xcomet_score": 0.8491271734237671, "xcomet_qe_score": 0.8027185201644897, "metricx_score": 2.0766735076904297, "metricx_qe_score": 2.2992842197418213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现PRC是一种用于稀有类别获取和冷启动主动学习的简单主动学习策略,而设计得当的迁移学习任务可以在此基础上提供显著帮助。", "metrics": {"bleu_score": 49.645526784033954, "chrf_score": 46.40270887610972, "xcomet_score": 0.7061895132064819, "xcomet_qe_score": 0.7126136422157288, "metricx_score": 4.387356281280518, "metricx_qe_score": 5.27583646774292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 53.36453849758749, "chrf_score": 44.653888394229135, "xcomet_score": 0.8065222501754761, "xcomet_qe_score": 0.726245641708374, "metricx_score": 1.5441720485687256, "metricx_qe_score": 2.077759265899658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们代码数据集和论文的链接。", "metrics": {"bleu_score": 64.49131835432436, "chrf_score": 58.956847988957904, "xcomet_score": 0.8949801921844482, "xcomet_qe_score": 0.9046486020088196, "metricx_score": 2.100811004638672, "metricx_qe_score": 2.4844677448272705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,欢迎与我们联系。", "metrics": {"bleu_score": 35.750177190768014, "chrf_score": 31.340810162200043, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.049377478659152985, "metricx_qe_score": 0.037661273032426834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
