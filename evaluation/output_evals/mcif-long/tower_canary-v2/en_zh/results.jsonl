{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9583046436309814, "xcomet_qe_score": 0.9632420539855957, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们的演示,我们将展示 DeepLean,这是一个用于德语文本简化的新语料库,涵盖文档级别和句子级别。", "metrics": {"bleu_score": 19.312306433156923, "chrf_score": 19.46783828989681, "xcomet_score": 0.6416171193122864, "xcomet_qe_score": 0.3979073762893677, "metricx_score": 4.295622825622559, "metricx_qe_score": 4.346102714538574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫 Regina Stodden,我将引导您完成演示的第一部分。", "metrics": {"bleu_score": 50.212776217958165, "chrf_score": 64.29426287880126, "xcomet_score": 0.9188768863677979, "xcomet_qe_score": 0.9715996980667114, "metricx_score": 2.300168991088867, "metricx_qe_score": 3.3769748210906982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们先定义一下文本简化。", "metrics": {"bleu_score": 35.78815189232794, "chrf_score": 30.906117988644933, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.15086482465267181, "metricx_qe_score": 0.31294476985931396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是将文本进行改编,以提高特定目标群体对文本的理解,例如,有阅读障碍的人或非母语人士。", "metrics": {"bleu_score": 58.66570009882936, "chrf_score": 51.704071292284034, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6719093322753906, "metricx_qe_score": 0.6637874841690063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要平行文本对,例如,文档或句子。", "metrics": {"bleu_score": 66.01973900178668, "chrf_score": 60.575643154831916, "xcomet_score": 0.9837609529495239, "xcomet_qe_score": 0.902226448059082, "metricx_score": 2.0366735458374023, "metricx_qe_score": 3.012054204940796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,您可以看到一个复杂的德语句子及其通俗语言翻译的平行对齐句子对。", "metrics": {"bleu_score": 63.29336507270698, "chrf_score": 58.930563605486206, "xcomet_score": 0.9757446050643921, "xcomet_qe_score": 0.9543026685714722, "metricx_score": 1.4041693210601807, "metricx_qe_score": 1.71913743019104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以采用不同的技术,如您在例子中看到的,例如,词汇替换、从句删除、从句重新排序或插入单词。", "metrics": {"bleu_score": 47.735349332944544, "chrf_score": 49.60084153679795, "xcomet_score": 0.9118385910987854, "xcomet_qe_score": 0.8833211660385132, "metricx_score": 1.0446362495422363, "metricx_qe_score": 0.7751408815383911, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们的新语料库 dplane,因为近年来,现有的语料库存在一些问题", "metrics": {"bleu_score": 51.51420826880311, "chrf_score": 36.98097618265373, "xcomet_score": 0.6561662554740906, "xcomet_qe_score": 0.6773567795753479, "metricx_score": 6.293116092681885, "metricx_qe_score": 8.685615539550781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",或者太小,无法训练分类模型。", "metrics": {"bleu_score": 8.884499497101256, "chrf_score": 11.513048534240946, "xcomet_score": 0.7973169088363647, "xcomet_qe_score": 0.7548015117645264, "metricx_score": 9.4632568359375, "metricx_qe_score": 8.346231460571289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的其他三个模型都是自动对齐的,这意味着它们的对齐可能存在错误。", "metrics": {"bleu_score": 57.46388731356058, "chrf_score": 50.42078987723587, "xcomet_score": 0.9855239391326904, "xcomet_qe_score": 0.9838825464248657, "metricx_score": 0.6832425594329834, "metricx_qe_score": 0.8456329107284546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了我们的新语料库 DPlane,它分为两个子语料库,DPlane APA 和 DPlane Web。", "metrics": {"bleu_score": 34.38468902500841, "chrf_score": 24.175953593825497, "xcomet_score": 0.8041374683380127, "xcomet_qe_score": 0.7698487639427185, "metricx_score": 4.027493953704834, "metricx_qe_score": 3.37544584274292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DPlane APA 基于已使用文本。", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 12.458019419246533, "xcomet_score": 0.6900458335876465, "xcomet_qe_score": 0.6186059713363647, "metricx_score": 5.322425842285156, "metricx_qe_score": 6.802812576293945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 DPlane APA 中,我们手动对齐了 483 个文档。", "metrics": {"bleu_score": 70.18491170272205, "chrf_score": 49.81387006445824, "xcomet_score": 0.9200215339660645, "xcomet_qe_score": 0.9212480783462524, "metricx_score": 2.054877758026123, "metricx_qe_score": 1.9837983846664429, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约有 30,000 对、13,000 对平行句子对。", "metrics": {"bleu_score": 13.834368456410951, "chrf_score": 30.14961065080792, "xcomet_score": 0.6230673789978027, "xcomet_qe_score": 0.6017138957977295, "metricx_score": 6.819482326507568, "metricx_qe_score": 6.27255916595459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 dplane web,这个语料库包括不同的领域,我们也手动对齐了这 750 个文档,另一方面也使用了自动对齐方法。", "metrics": {"bleu_score": 49.027667314329655, "chrf_score": 39.342232713289036, "xcomet_score": 0.882085919380188, "xcomet_qe_score": 0.7549827098846436, "metricx_score": 4.0196428298950195, "metricx_qe_score": 4.818518161773682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们得到了 30,450 对句子对。", "metrics": {"bleu_score": 21.874057156123218, "chrf_score": 49.35909600266433, "xcomet_score": 0.8214301466941833, "xcomet_qe_score": 0.8201783299446106, "metricx_score": 2.6009202003479004, "metricx_qe_score": 2.9862256050109863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更详细的分析,例如,简化类型。", "metrics": {"bleu_score": 28.918686128804303, "chrf_score": 25.257803175091937, "xcomet_score": 0.7784734964370728, "xcomet_qe_score": 0.7700917720794678, "metricx_score": 3.9683637619018555, "metricx_qe_score": 4.548354625701904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所见,圣经文本比新闻文本或语言学习文本在所有级别", "metrics": {"bleu_score": 49.16653557496095, "chrf_score": 47.114144580697086, "xcomet_score": 0.6536886096000671, "xcomet_qe_score": 0.5234850645065308, "metricx_score": 8.414809226989746, "metricx_qe_score": 8.021347045898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上都更强,例如,词汇简化、结构简化或整体简化水平。", "metrics": {"bleu_score": 53.47952314188309, "chrf_score": 47.3922059269278, "xcomet_score": 0.562201738357544, "xcomet_qe_score": 0.5816850662231445, "metricx_score": 5.03759765625, "metricx_qe_score": 4.547883033752441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到,我们的 D-plane 语料库具有高度多样化的不同简化变换。", "metrics": {"bleu_score": 28.734251596043503, "chrf_score": 25.46831332486442, "xcomet_score": 0.755962073802948, "xcomet_qe_score": 0.7214169502258301, "metricx_score": 3.0880916118621826, "metricx_qe_score": 3.0013060569763184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 D-plane API 语料库中,我们有比 D-plane web 语料库更多的重新排序和单词编辑。", "metrics": {"bleu_score": 31.443515194397026, "chrf_score": 26.470194198011647, "xcomet_score": 0.7483795285224915, "xcomet_qe_score": 0.6097021698951721, "metricx_score": 4.0144877433776855, "metricx_qe_score": 4.197274684906006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在 web 语料库中,我们有更多的简化改写。", "metrics": {"bleu_score": 30.691873006665958, "chrf_score": 25.687131672037616, "xcomet_score": 0.8343726396560669, "xcomet_qe_score": 0.7716670036315918, "metricx_score": 1.855367660522461, "metricx_qe_score": 2.3762567043304443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们现在来看看我们可以用这个语料库做什么。", "metrics": {"bleu_score": 45.810919659463124, "chrf_score": 45.3378672790754, "xcomet_score": 0.9930709600448608, "xcomet_qe_score": 0.9726355075836182, "metricx_score": 0.3310670554637909, "metricx_qe_score": 0.5004581212997437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是 Omar,我现在将谈谈我们数据集 dplane 的使用案例。", "metrics": {"bleu_score": 14.07055728171985, "chrf_score": 20.884433481741613, "xcomet_score": 0.9257500171661377, "xcomet_qe_score": 0.9321465492248535, "metricx_score": 3.031949520111084, "metricx_qe_score": 3.789342164993286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个使用案例是我们可以评估自动对齐方法。", "metrics": {"bleu_score": 67.73709971213142, "chrf_score": 62.89481874621193, "xcomet_score": 0.8768478631973267, "xcomet_qe_score": 0.9269917011260986, "metricx_score": 1.63204026222229, "metricx_qe_score": 1.6795686483383179, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了很多对齐方法,但在机器翻译的背景下,我们有两个用不同语言编写的平行文档,我们想要从后置文档中提取句子对齐,但", "metrics": {"bleu_score": 56.60609750278215, "chrf_score": 51.267782585612885, "xcomet_score": 0.6902433633804321, "xcomet_qe_score": 0.643067479133606, "metricx_score": 3.6568543910980225, "metricx_qe_score": 3.4489293098449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的使用案例中,我们试图在两个平行文档的句子之间提取对齐,具有相同语言、相同内容,但它们在复杂性级别上不同。", "metrics": {"bleu_score": 41.892524741565424, "chrf_score": 35.812479425563076, "xcomet_score": 0.7740994095802307, "xcomet_qe_score": 0.7596427798271179, "metricx_score": 2.8645520210266113, "metricx_qe_score": 3.0880630016326904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们有了手动对齐句子的数据集 D-plane,我们可以使用这些句子作为金标准对齐来评估一些提出的对齐方法。", "metrics": {"bleu_score": 46.43063466844389, "chrf_score": 37.62724138532697, "xcomet_score": 0.7789087295532227, "xcomet_qe_score": 0.726895809173584, "metricx_score": 3.6376359462738037, "metricx_qe_score": 3.4507570266723633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了一些改编,并在论文中发表了所有这些改编和运行实验的代码。", "metrics": {"bleu_score": 33.533048581960514, "chrf_score": 30.22339221688426, "xcomet_score": 0.9760477542877197, "xcomet_qe_score": 0.9815609455108643, "metricx_score": 1.6106516122817993, "metricx_qe_score": 1.3464195728302002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是质量对齐方法", "metrics": {"bleu_score": 66.31698736102048, "chrf_score": 55.758500989940664, "xcomet_score": 0.8125216960906982, "xcomet_qe_score": 0.8123287558555603, "metricx_score": 4.627102375030518, "metricx_qe_score": 3.8116202354431152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",您也可以在论文中找到在您自己的文档上运行该方法的代码。", "metrics": {"bleu_score": 34.494656143047706, "chrf_score": 29.47427152213381, "xcomet_score": 0.9735767841339111, "xcomet_qe_score": 0.9619652628898621, "metricx_score": 1.2190885543823242, "metricx_qe_score": 1.2054738998413086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个使用案例是通过微调语言模型来自动文本简化的案例。", "metrics": {"bleu_score": 51.88394135988331, "chrf_score": 49.921795160424615, "xcomet_score": 0.9040699601173401, "xcomet_qe_score": 0.8966217637062073, "metricx_score": 0.8911688327789307, "metricx_qe_score": 1.1850205659866333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们微调了两个不同的模型。我们微调了", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 25.99298382912637, "xcomet_score": 0.3408113420009613, "xcomet_qe_score": 0.43639102578163147, "metricx_score": 4.435979843139648, "metricx_qe_score": 5.219578742980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "long import 模型以生成文档级别的简化,我们还微调了 normal base import 以生成句子级别的简化。", "metrics": {"bleu_score": 14.086560593165144, "chrf_score": 16.23350387251688, "xcomet_score": 0.5932577848434448, "xcomet_qe_score": 0.5958188772201538, "metricx_score": 11.152959823608398, "metricx_qe_score": 11.016006469726562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并在论文中查看我们实验的更多详细信息和评估指标。我们得出", "metrics": {"bleu_score": 65.89230570157822, "chrf_score": 57.5747831056499, "xcomet_score": 0.7429415583610535, "xcomet_qe_score": 0.7225364446640015, "metricx_score": 5.733022689819336, "metricx_qe_score": 2.996365785598755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结论,这种基本的微调可以产生或可以获得比基准分数更好的分数,我们提出这些结果作为未来的自动文本简化问题的基准。", "metrics": {"bleu_score": 72.50740991911901, "chrf_score": 67.30414415599682, "xcomet_score": 0.839181661605835, "xcomet_qe_score": 0.6836763620376587, "metricx_score": 2.3059589862823486, "metricx_qe_score": 2.8167617321014404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们希望在会议期间见到大家。", "metrics": {"bleu_score": 46.11411579665311, "chrf_score": 40.155067083822956, "xcomet_score": 0.9945908784866333, "xcomet_qe_score": 0.9959206581115723, "metricx_score": 0.6428474187850952, "metricx_qe_score": 0.37971922755241394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·斯皮拉科夫斯基,今天我要讲的主题是并列句的依存结构。", "metrics": {"bleu_score": 8.8760743367864, "chrf_score": 8.390672400749862, "xcomet_score": 0.673957347869873, "xcomet_qe_score": 0.5805919170379639, "metricx_score": 2.309844493865967, "metricx_qe_score": 1.5398694276809692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "众所周知,不同的理论和语料库方法对依存结构有不同的假设。", "metrics": {"bleu_score": 43.25244295803472, "chrf_score": 39.65601375930387, "xcomet_score": 0.9862223863601685, "xcomet_qe_score": 0.8930584788322449, "metricx_score": 0.5322721004486084, "metricx_qe_score": 0.7308767437934875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在普遍依存理论中,丽莎、巴特和玛姬的并列结构是这样的,第一个并列成分是整个并列结构的中心词,", "metrics": {"bleu_score": 30.47292869981468, "chrf_score": 21.838671815109066, "xcomet_score": 0.7578127384185791, "xcomet_qe_score": 0.7100294828414917, "metricx_score": 2.1467514038085938, "metricx_qe_score": 3.7135705947875977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中是丽莎。伊戈", "metrics": {"bleu_score": 44.833867003844595, "chrf_score": 30.224834422656325, "xcomet_score": 0.8139227628707886, "xcomet_qe_score": 0.8195971250534058, "metricx_score": 4.992238521575928, "metricx_qe_score": 2.9995956420898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义文本理论也采用了类似的方法,同样认为整个并列结构以第一个并列成分为中心。因此,", "metrics": {"bleu_score": 37.982994973886825, "chrf_score": 28.61208493174969, "xcomet_score": 0.5406097173690796, "xcomet_qe_score": 0.44265732169151306, "metricx_score": 6.283540725708008, "metricx_qe_score": 4.88353157043457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法都是不对称的,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.9953523874282837, "xcomet_qe_score": 0.9697904586791992, "metricx_score": 0.36860373616218567, "metricx_qe_score": 0.4781116247177124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以我们做了什么,", "metrics": {"bleu_score": 6.772997136689072, "chrf_score": 22.79693486590038, "xcomet_score": 0.6208105087280273, "xcomet_qe_score": 0.5963826179504395, "metricx_score": 3.6057286262512207, "metricx_qe_score": 3.241755723953247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们只选取了一个并列成分。", "metrics": {"bleu_score": 34.38931217657843, "chrf_score": 26.931494431494436, "xcomet_score": 0.8607386350631714, "xcomet_qe_score": 0.832474410533905, "metricx_score": 2.627549171447754, "metricx_qe_score": 3.095825672149658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在也有对称的方法来处理并列结构,例如布拉格方法,", "metrics": {"bleu_score": 25.320845570924604, "chrf_score": 24.286603060083014, "xcomet_score": 0.7863240242004395, "xcomet_qe_score": 0.7628272771835327, "metricx_score": 3.841952085494995, "metricx_qe_score": 2.6298704147338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及布拉格依存树库中假设的以连词为中心的并列结构,其中并列结构以连词为中心。", "metrics": {"bleu_score": 17.548433488411245, "chrf_score": 17.773882377327645, "xcomet_score": 0.6016117930412292, "xcomet_qe_score": 0.4967864155769348, "metricx_score": 4.9651713371276855, "metricx_qe_score": 5.073544979095459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从所有并列成分得到依存关系。", "metrics": {"bleu_score": 33.4396537018538, "chrf_score": 27.59387803758586, "xcomet_score": 0.8544579744338989, "xcomet_qe_score": 0.8642224073410034, "metricx_score": 3.463942527770996, "metricx_qe_score": 3.388808250427246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有多头方法,例如卡特森的词法语法中,可以说所有的并列成分都是并列结构的中心词,", "metrics": {"bleu_score": 23.740175691900166, "chrf_score": 16.306012571485667, "xcomet_score": 0.39700019359588623, "xcomet_qe_score": 0.34760865569114685, "metricx_score": 4.946840763092041, "metricx_qe_score": 5.640100002288818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此我们从支配词这里得到分别", "metrics": {"bleu_score": 35.13098711506864, "chrf_score": 29.782421791256724, "xcomet_score": 0.7671061754226685, "xcomet_qe_score": 0.26068004965782166, "metricx_score": 4.593728065490723, "metricx_qe_score": 4.428065299987793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "指向所有并列成分的依存关系,这些是巴特和玛姬。现在,这", "metrics": {"bleu_score": 11.739521786077459, "chrf_score": 8.450806831836992, "xcomet_score": 0.15841297805309296, "xcomet_qe_score": 0.15257856249809265, "metricx_score": 14.342464447021484, "metricx_qe_score": 13.050753593444824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "篇论文的目标是为像这两个例子中的对称并列结构提供一个新的论据,反对像这两个例子中的不对称并列结构。", "metrics": {"bleu_score": 35.262697529998235, "chrf_score": 32.160611067305, "xcomet_score": 0.6357651948928833, "xcomet_qe_score": 0.645503044128418, "metricx_score": 3.9852144718170166, "metricx_qe_score": 3.029803991317749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以我们做了什么,", "metrics": {"bleu_score": 6.772997136689072, "chrf_score": 22.79693486590038, "xcomet_score": 0.6983103156089783, "xcomet_qe_score": 0.5711360573768616, "metricx_score": 3.703829050064087, "metricx_qe_score": 3.35632586479187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论据是基于依存长度最小化原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 43.915984395006454, "chrf_score": 35.18601534199238, "xcomet_score": 0.8405743837356567, "xcomet_qe_score": 0.8330153226852417, "metricx_score": 0.8605861067771912, "metricx_qe_score": 0.7228354215621948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在英语中,如你所知,直接宾语倾向于靠近动词,而附属成分可能更远,对吧?", "metrics": {"bleu_score": 31.669263130480406, "chrf_score": 26.489274675638164, "xcomet_score": 0.76743084192276, "xcomet_qe_score": 0.7717470526695251, "metricx_score": 3.9748473167419434, "metricx_qe_score": 4.141942024230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,March read it yesterday 是正确的,因为直接宾语 it 靠近动词,而 March read yesterday it 则差多了,", "metrics": {"bleu_score": 31.946607974359015, "chrf_score": 51.307197933229155, "xcomet_score": 0.5016932487487793, "xcomet_qe_score": 0.544601559638977, "metricx_score": 6.528319835662842, "metricx_qe_score": 7.3171610832214355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里动词和直接宾语之间有一个附属成分 yesterday。", "metrics": {"bleu_score": 53.522807162964334, "chrf_score": 58.19402673027625, "xcomet_score": 0.889685332775116, "xcomet_qe_score": 0.7757117748260498, "metricx_score": 2.554417133331299, "metricx_qe_score": 3.741328716278076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常长时,这种效果可能会得到缓解,因为", "metrics": {"bleu_score": 31.645275923351992, "chrf_score": 28.283654258118666, "xcomet_score": 0.7326018810272217, "xcomet_qe_score": 0.580132007598877, "metricx_score": 4.6457295417785645, "metricx_qe_score": 4.229954242706299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这时可以直接宾语移到附属成分之后的位置。", "metrics": {"bleu_score": 28.72797668292002, "chrf_score": 24.26914918013989, "xcomet_score": 0.7882570028305054, "xcomet_qe_score": 0.7699158191680908, "metricx_score": 3.379035234451294, "metricx_qe_score": 4.864362716674805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有例子说明。所以,", "metrics": {"bleu_score": 18.36028134946796, "chrf_score": 20.735576110731323, "xcomet_score": 0.8500728607177734, "xcomet_qe_score": 0.8435606360435486, "metricx_score": 2.810086727142334, "metricx_qe_score": 1.9975857734680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都很好。", "metrics": {"bleu_score": 48.88290318657944, "chrf_score": 42.06458785456199, "xcomet_score": 0.9888736009597778, "xcomet_qe_score": 0.9774754047393799, "metricx_score": 0.4006389081478119, "metricx_qe_score": 0.6555725336074829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "March read this absolutely fascinating book about the bees yesterday 是", "metrics": {"bleu_score": 47.41797560818527, "chrf_score": 83.01097653966195, "xcomet_score": 0.7094162702560425, "xcomet_qe_score": 0.7145740985870361, "metricx_score": 14.47546672821045, "metricx_qe_score": 24.114849090576172, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以的,其中用 this long NP 替代了 it。", "metrics": {"bleu_score": 6.418105753863986, "chrf_score": 8.666957904827852, "xcomet_score": 0.7509735822677612, "xcomet_qe_score": 0.7795717716217041, "metricx_score": 7.197127342224121, "metricx_qe_score": 7.279036045074463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "说 March read yesterday this absolutely fascinating book about bees 也是可以的。所以,这里的推理是,这是可能的,", "metrics": {"bleu_score": 26.173151836022, "chrf_score": 72.63150914777843, "xcomet_score": 0.6149798631668091, "xcomet_qe_score": 0.41451820731163025, "metricx_score": 12.773845672607422, "metricx_qe_score": 17.76445198059082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为即使这个句子违反了直接宾语应该紧靠动词的一般语法原则,但它满足了依存长度最小化原则,即倾向于较短的依存关系。", "metrics": {"bleu_score": 51.0115985805192, "chrf_score": 44.24653139142981, "xcomet_score": 0.8588452339172363, "xcomet_qe_score": 0.9315330982208252, "metricx_score": 2.9014036655426025, "metricx_qe_score": 3.6682753562927246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个树只显示了关键依存关系的长度,所以这些不是这两个结构中恒定的依存关系。", "metrics": {"bleu_score": 61.59508780559903, "chrf_score": 59.495926415471025, "xcomet_score": 0.8493813276290894, "xcomet_qe_score": 0.8138796091079712, "metricx_score": 1.9843605756759644, "metricx_qe_score": 2.608980178833008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,这里有一个从 red 到长度为7的附属成分的依存关系,用词数衡量,以及从 red 到 book 的长度为4的依存关系,所以总共是11。", "metrics": {"bleu_score": 36.085487339327535, "chrf_score": 31.137232207285535, "xcomet_score": 0.551415741443634, "xcomet_qe_score": 0.459155797958374, "metricx_score": 9.679009437561035, "metricx_qe_score": 10.156667709350586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个成分时,这两个依存关系的总和变为6,对", "metrics": {"bleu_score": 53.707566707346544, "chrf_score": 52.09585216563089, "xcomet_score": 0.5727032423019409, "xcomet_qe_score": 0.4979701340198517, "metricx_score": 7.291044235229492, "metricx_qe_score": 6.049654960632324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吗?所以从11变为6,更短,", "metrics": {"bleu_score": 4.065425428798724, "chrf_score": 6.838365896980461, "xcomet_score": 0.5641341209411621, "xcomet_qe_score": 0.5921187996864319, "metricx_score": 5.317123889923096, "metricx_qe_score": 4.912651538848877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来相当好的原因,", "metrics": {"bleu_score": 54.50176720923848, "chrf_score": 61.20989799306401, "xcomet_score": 0.8799511194229126, "xcomet_qe_score": 0.8481912612915039, "metricx_score": 0.7869430780410767, "metricx_qe_score": 0.875968337059021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 63.8079654920782, "xcomet_score": 0.9115704298019409, "xcomet_qe_score": 0.963566780090332, "metricx_score": 0.5989556312561035, "metricx_qe_score": 0.9356817007064819, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以我们做了什么,", "metrics": {"bleu_score": 6.772997136689072, "chrf_score": 22.79693486590038, "xcomet_score": 0.5664061307907104, "xcomet_qe_score": 0.5960577726364136, "metricx_score": 3.5926496982574463, "metricx_qe_score": 3.1625311374664307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的笔树库中提取了关于并列的各种统计数据,并查看了为什么我们没有使用普遍依存理论。这些统计数据证实了之前多次观察到的现象,即左并列成分往往较短。", "metrics": {"bleu_score": 47.48185610231739, "chrf_score": 42.3667140965812, "xcomet_score": 0.5428836345672607, "xcomet_qe_score": 0.5324521064758301, "metricx_score": 7.0769572257995605, "metricx_qe_score": 6.323820114135742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,salt and pepper,而不是 pepper and salt,用音节衡量。", "metrics": {"bleu_score": 16.23988591941736, "chrf_score": 56.79164731033101, "xcomet_score": 0.7199559807777405, "xcomet_qe_score": 0.655787467956543, "metricx_score": 5.330910682678223, "metricx_qe_score": 7.034022331237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还有,在路过时观察到的,这种倾向随着长度差的增加而增加。所以", "metrics": {"bleu_score": 50.14909160042738, "chrf_score": 50.58541300705544, "xcomet_score": 0.6454100608825684, "xcomet_qe_score": 0.5758973956108093, "metricx_score": 6.670525550842285, "metricx_qe_score": 5.820444107055664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个并列成分的长度差增加时,较短的并列成分更倾向于成为第一个,对吧?", "metrics": {"bleu_score": 33.863100848867816, "chrf_score": 27.000829928318993, "xcomet_score": 0.7405690550804138, "xcomet_qe_score": 0.7634947299957275, "metricx_score": 5.28936767578125, "metricx_qe_score": 3.99478816986084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,左短并列成分的比例更大。", "metrics": {"bleu_score": 42.311785416105785, "chrf_score": 34.71833721833721, "xcomet_score": 0.8286094665527344, "xcomet_qe_score": 0.8232760429382324, "metricx_score": 2.870940685272217, "metricx_qe_score": 4.098435878753662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,这篇论文的新颖之处在于,我们观察到这种倾向只有在支配词在左边或不存在时才会发生,对吧?所以,", "metrics": {"bleu_score": 43.46203003079684, "chrf_score": 40.46621385611103, "xcomet_score": 0.7608222961425781, "xcomet_qe_score": 0.6421141624450684, "metricx_score": 5.474542140960693, "metricx_qe_score": 4.854271411895752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,支配词在左边。我看到了巴特和丽莎。所以,支配词在左边。", "metrics": {"bleu_score": 34.33574272439591, "chrf_score": 23.740912305936682, "xcomet_score": 0.9229055643081665, "xcomet_qe_score": 0.6629310846328735, "metricx_score": 2.5920169353485107, "metricx_qe_score": 3.6003031730651855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,荷马来了,打了个喷嚏,", "metrics": {"bleu_score": 22.135572417208273, "chrf_score": 11.556023791206844, "xcomet_score": 0.7017167806625366, "xcomet_qe_score": 0.7231543660163879, "metricx_score": 5.92983341217041, "metricx_qe_score": 5.077769756317139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有两个动词的并列,没有外部支配词,对吧?所以,", "metrics": {"bleu_score": 35.122470150715806, "chrf_score": 31.35391069181267, "xcomet_score": 0.7411558032035828, "xcomet_qe_score": 0.6507233381271362, "metricx_score": 4.642796993255615, "metricx_qe_score": 3.854266405105591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左并列成分更倾向于较短,尤其是两个并列成分之间的差异越大。", "metrics": {"bleu_score": 26.89054715066593, "chrf_score": 27.324487164466166, "xcomet_score": 0.7225876450538635, "xcomet_qe_score": 0.7559036016464233, "metricx_score": 5.846086502075195, "metricx_qe_score": 4.990714073181152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当支配词在右边时,如图 ted 和 net 的并列,这个效应就不见了。所以", "metrics": {"bleu_score": 15.831913431463878, "chrf_score": 14.18069359391728, "xcomet_score": 0.49990570545196533, "xcomet_qe_score": 0.28772181272506714, "metricx_score": 9.139687538146973, "metricx_qe_score": 9.622305870056152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度(第一列)、音节(中间列)和词(右列)来展示这一点。", "metrics": {"bleu_score": 10.779579238214376, "chrf_score": 13.920229461857451, "xcomet_score": 0.8640153408050537, "xcomet_qe_score": 0.7584715485572815, "metricx_score": 4.194294452667236, "metricx_qe_score": 3.605164051055908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我将重点关注右边的。我们在这", "metrics": {"bleu_score": 11.203754340102181, "chrf_score": 15.356223737444743, "xcomet_score": 0.7160732746124268, "xcomet_qe_score": 0.5805479288101196, "metricx_score": 4.483564853668213, "metricx_qe_score": 2.3504281044006348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里看到,当支配词在左边时,左并列成分较短的倾向随着词的绝对差的增加而稳步增长,当没有支配词时,如图句子并列,也会观察到同样的情况。但是", "metrics": {"bleu_score": 31.438821081376123, "chrf_score": 26.181333634090016, "xcomet_score": 0.2712000608444214, "xcomet_qe_score": 0.22503702342510223, "metricx_score": 9.357124328613281, "metricx_qe_score": 8.228392601013184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当支配词在右边时,这种倾向就不见了,", "metrics": {"bleu_score": 28.140173273071824, "chrf_score": 23.694092653691882, "xcomet_score": 0.7852909564971924, "xcomet_qe_score": 0.5679152607917786, "metricx_score": 4.945431232452393, "metricx_qe_score": 6.769956588745117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这一点如何为对称并列结构提供了论据,反对像这两个例子中的不对称并列结构。所以,请查看论文", "metrics": {"bleu_score": 36.821702499805596, "chrf_score": 33.38607569474669, "xcomet_score": 0.5628616809844971, "xcomet_qe_score": 0.4805469810962677, "metricx_score": 2.6047182083129883, "metricx_qe_score": 3.590803623199463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以获取完整的协议和论据,", "metrics": {"bleu_score": 5.771298850643905, "chrf_score": 5.559735808042806, "xcomet_score": 0.4299754202365875, "xcomet_qe_score": 0.28568220138549805, "metricx_score": 8.109479904174805, "metricx_qe_score": 5.101245880126953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并在会议结束后与我们讨论。", "metrics": {"bleu_score": 3.7644257151903666, "chrf_score": 3.6231884057971016, "xcomet_score": 0.21766629815101624, "xcomet_qe_score": 0.4028564989566803, "metricx_score": 4.640410423278809, "metricx_qe_score": 2.870222568511963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是华盛顿大学的博士生Xiang Bin。", "metrics": {"bleu_score": 66.54377827941899, "chrf_score": 52.01969845562523, "xcomet_score": 0.8431168794631958, "xcomet_qe_score": 0.898215115070343, "metricx_score": 0.4714938700199127, "metricx_qe_score": 0.7077658176422119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们从预训练数据到语言模型再到下游任务的工作,追踪导致不公平自然语言处理模型的政治偏见的轨迹。因此", "metrics": {"bleu_score": 62.26611688210154, "chrf_score": 59.3785733803021, "xcomet_score": 0.733115553855896, "xcomet_qe_score": 0.5974158048629761, "metricx_score": 4.202980995178223, "metricx_qe_score": 2.290449380874634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是在大规模的网络爬虫数据上训练的。", "metrics": {"bleu_score": 95.10699415570296, "chrf_score": 98.9218084464701, "xcomet_score": 0.9802743792533875, "xcomet_qe_score": 0.9440110921859741, "metricx_score": 1.7925139665603638, "metricx_qe_score": 2.499488115310669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在他们的预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 48.195116293616074, "chrf_score": 48.402791332265075, "xcomet_score": 0.7571665048599243, "xcomet_qe_score": 0.7159234881401062, "metricx_score": 1.750040054321289, "metricx_qe_score": 2.615305185317993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C四语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 77.00961116014527, "chrf_score": 74.42516909564768, "xcomet_score": 0.7471767067909241, "xcomet_qe_score": 0.7173298001289368, "metricx_score": 2.1897106170654297, "metricx_qe_score": 2.303035020828247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型的应用带来了既是福又是祸。", "metrics": {"bleu_score": 34.11488281065382, "chrf_score": 26.145869166380002, "xcomet_score": 0.8954199552536011, "xcomet_qe_score": 0.893241286277771, "metricx_score": 1.2410162687301636, "metricx_qe_score": 1.0903632640838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从多样化的视角学习,这庆祝了民主和思想的多元性。", "metrics": {"bleu_score": 31.383369760338972, "chrf_score": 26.34662963257235, "xcomet_score": 0.8996644020080566, "xcomet_qe_score": 0.782477080821991, "metricx_score": 1.7841638326644897, "metricx_qe_score": 2.838578701019287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点在本质上具有社会偏见,可能会在下游任务应用中导致潜在的公平问题。", "metrics": {"bleu_score": 71.04884432642514, "chrf_score": 62.91731469051266, "xcomet_score": 0.987799882888794, "xcomet_qe_score": 0.9727433919906616, "metricx_score": 1.0582624673843384, "metricx_qe_score": 1.2786921262741089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体来说,通过以下问题:首先,我们如何评估语言模型的政治倾向,预训练数据可能对这种政治偏见有什么作用?", "metrics": {"bleu_score": 64.11438199736024, "chrf_score": 56.832332631802075, "xcomet_score": 0.82841956615448, "xcomet_qe_score": 0.8465502858161926, "metricx_score": 1.6494938135147095, "metricx_qe_score": 2.0253190994262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在实际应用中如何表现,这是否会导致自然语言处理应用中的公平问题?因此", "metrics": {"bleu_score": 50.243903543234694, "chrf_score": 43.25667058584621, "xcomet_score": 0.8800028562545776, "xcomet_qe_score": 0.8405426144599915, "metricx_score": 2.770296573638916, "metricx_qe_score": 0.5814277529716492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",具体来说,我们首先提议使用政治问卷(如政治指南针测试)对不同提示格式的语言模型进行提示。", "metrics": {"bleu_score": 51.0076156805044, "chrf_score": 43.517963732426324, "xcomet_score": 0.7070198059082031, "xcomet_qe_score": 0.6545741558074951, "metricx_score": 4.734236717224121, "metricx_qe_score": 5.700024127960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们的自动评估基于政治科学文献。因此,一些", "metrics": {"bleu_score": 35.693754559323295, "chrf_score": 32.09138233325921, "xcomet_score": 0.6219412088394165, "xcomet_qe_score": 0.656053900718689, "metricx_score": 7.463095188140869, "metricx_qe_score": 2.775780439376831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "初步结果表明,首先,语言模型确实具有不同的政治含义。", "metrics": {"bleu_score": 54.265525836298295, "chrf_score": 44.44224495785581, "xcomet_score": 0.9848983287811279, "xcomet_qe_score": 1.0, "metricx_score": 1.273624300956726, "metricx_qe_score": 1.4134931564331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治指南针上的四个象限。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 44.723889306558185, "xcomet_score": 0.8590556383132935, "xcomet_qe_score": 0.8305246829986572, "metricx_score": 2.5154099464416504, "metricx_qe_score": 2.432861089706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT 4是所有语言模型中最自由派的一个,GPT理论通常比BERT理论及其变体更具社会自由主义。", "metrics": {"bleu_score": 45.467813548652025, "chrf_score": 41.89235410855737, "xcomet_score": 0.8450314402580261, "xcomet_qe_score": 0.8999769687652588, "metricx_score": 2.82197904586792, "metricx_qe_score": 2.316103935241699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们的目标是研究语言模型的政治偏见实际上是从训练数据中获得的程度。", "metrics": {"bleu_score": 58.796214002018345, "chrf_score": 52.86486799810719, "xcomet_score": 0.8387658596038818, "xcomet_qe_score": 0.824955403804779, "metricx_score": 3.502032518386841, "metricx_qe_score": 3.6997900009155273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以通过进一步在六个不同的党派语料库上预训练语言模型检查点来进行受控实验,这些语料库分为新闻和社交媒体,进一步分为其政治倾向。", "metrics": {"bleu_score": 53.83073434599506, "chrf_score": 45.72937455427752, "xcomet_score": 0.7839123010635376, "xcomet_qe_score": 0.6645772457122803, "metricx_score": 2.0525999069213867, "metricx_qe_score": 2.5826406478881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这些党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了变化。", "metrics": {"bleu_score": 71.11282375838051, "chrf_score": 65.51510927938429, "xcomet_score": 0.9136406183242798, "xcomet_qe_score": 0.8157476186752319, "metricx_score": 1.047675371170044, "metricx_qe_score": 1.6770319938659668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于进一步微调的Roberta,进一步训练于左翼的Reddit语料库,我们可以看到其政治偏见方面有显著的自由派转变。", "metrics": {"bleu_score": 50.09547089187398, "chrf_score": 46.40300463436088, "xcomet_score": 0.6765933036804199, "xcomet_qe_score": 0.6353947520256042, "metricx_score": 4.2629265785217285, "metricx_qe_score": 4.533895492553711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究语言模型是否能够捕捉到我们现代社会中普遍存在的极化。", "metrics": {"bleu_score": 67.8550831787096, "chrf_score": 63.375927458649805, "xcomet_score": 0.7888745069503784, "xcomet_qe_score": 0.7986973524093628, "metricx_score": 2.7741987705230713, "metricx_qe_score": 2.5248374938964844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为第45任美国总统当选前和当选后。", "metrics": {"bleu_score": 44.110018183549, "chrf_score": 40.25179673805568, "xcomet_score": 0.9303308725357056, "xcomet_qe_score": 0.8249167203903198, "metricx_score": 1.9519720077514648, "metricx_qe_score": 2.561776638031006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们分别在两个不同的时间语料库上预训练语言模型。我们", "metrics": {"bleu_score": 91.81891462193897, "chrf_score": 98.16306505407624, "xcomet_score": 0.6584798097610474, "xcomet_qe_score": 0.6531087160110474, "metricx_score": 3.7580032348632812, "metricx_qe_score": 1.3437440395355225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年后通常具有更偏离中心的政治倾向。因此,", "metrics": {"bleu_score": 54.47480281399348, "chrf_score": 48.74462193216474, "xcomet_score": 0.7788927555084229, "xcomet_qe_score": 0.7274727821350098, "metricx_score": 4.589878082275391, "metricx_qe_score": 2.9320313930511475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也可以捕捉到我们社会中的极化。", "metrics": {"bleu_score": 77.24579129745068, "chrf_score": 74.91256190403274, "xcomet_score": 0.9197232127189636, "xcomet_qe_score": 0.9364190697669983, "metricx_score": 1.5229437351226807, "metricx_qe_score": 1.872099757194519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但并非最不重要的一点,我们评估了具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现,这些应用通常涉及语言模型,可能具有非常重大的影响。因此,", "metrics": {"bleu_score": 57.889957619497366, "chrf_score": 58.02294126155484, "xcomet_score": 0.6809033751487732, "xcomet_qe_score": 0.7986843585968018, "metricx_score": 3.257996082305908, "metricx_qe_score": 2.5050888061523438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,如果我们按类别评估性能,也就是说,如果我们将性能分为不同的人口统计或新闻媒体的政治含义,我们可以看到一个模式,", "metrics": {"bleu_score": 42.74181655066656, "chrf_score": 36.16971389823131, "xcomet_score": 0.6230434775352478, "xcomet_qe_score": 0.6994689702987671, "metricx_score": 7.159345626831055, "metricx_qe_score": 6.203795433044434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于仇恨言论检测,左翼语言模型在检测针对社会少数群体的仇恨言论方面表现更好,然而,在检测针对我们社会中更强大群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 68.59718116110346, "chrf_score": 64.46139574917511, "xcomet_score": 0.9147182703018188, "xcomet_qe_score": 0.9149341583251953, "metricx_score": 1.3527799844741821, "metricx_qe_score": 1.2598085403442383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好,然而,在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 68.17797786596107, "chrf_score": 71.50124422840595, "xcomet_score": 0.9833532571792603, "xcomet_qe_score": 0.9848936796188354, "metricx_score": 0.663007915019989, "metricx_qe_score": 0.7862257957458496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似的趋势也发生在虚假新闻检测中,我们看到左翼语言模型在检测来自其相反政治含义的虚假信息方面表现更好,反之亦然。这", "metrics": {"bleu_score": 48.8312676174402, "chrf_score": 42.86763920894961, "xcomet_score": 0.7451653480529785, "xcomet_qe_score": 0.7171221971511841, "metricx_score": 5.296151638031006, "metricx_qe_score": 1.6988327503204346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将进一步展示许多定性例子,说明具有不同政治含义的语言模型根据其社会类别对仇恨言论和虚假信息例子给出不同的预测。", "metrics": {"bleu_score": 54.04726434820571, "chrf_score": 46.58424270175929, "xcomet_score": 0.792528510093689, "xcomet_qe_score": 0.8660281300544739, "metricx_score": 2.161283254623413, "metricx_qe_score": 2.4132585525512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有更多例子,以进一步强调这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平问题。", "metrics": {"bleu_score": 55.282458573890736, "chrf_score": 47.534703336739256, "xcomet_score": 0.8630741834640503, "xcomet_qe_score": 0.837612509727478, "metricx_score": 1.4386876821517944, "metricx_qe_score": 1.7168142795562744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个右翼语言模型被微调用于仇恨言论或虚假信息或其他,并部署到一个流行的社交媒体平台,这意味着具有相反政治观点的人可能会被边缘化,针对少数群体的仇恨言论可能会不受控制地蔓延。", "metrics": {"bleu_score": 56.0194957426963, "chrf_score": 49.111771666030464, "xcomet_score": 0.9471349120140076, "xcomet_qe_score": 0.8025014996528625, "metricx_score": 1.2692835330963135, "metricx_qe_score": 1.4521675109863281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这提醒我们要认识到并解决语言模型政治偏见导致的公平问题。", "metrics": {"bleu_score": 50.86366587562722, "chrf_score": 43.25774889085975, "xcomet_score": 0.9894534349441528, "xcomet_qe_score": 0.9823006391525269, "metricx_score": 0.8238365054130554, "metricx_qe_score": 1.1196417808532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有一点讨论。我们还", "metrics": {"bleu_score": 10.600313379512592, "chrf_score": 13.161639157655095, "xcomet_score": 0.21191412210464478, "xcomet_qe_score": 0.18380187451839447, "metricx_score": 6.703768253326416, "metricx_qe_score": 4.9986891746521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望强调我们揭示了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 59.12041014851363, "chrf_score": 56.49510878916955, "xcomet_score": 0.8538528680801392, "xcomet_qe_score": 0.7944517135620117, "metricx_score": 1.4319435358047485, "metricx_qe_score": 1.9449975490570068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在斯库拉和喀里布狄斯之间。", "metrics": {"bleu_score": 11.425435791072083, "chrf_score": 13.713018273968732, "xcomet_score": 0.765823245048523, "xcomet_qe_score": 0.7644734978675842, "metricx_score": 2.319312810897827, "metricx_qe_score": 1.6731715202331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们在语言模型训练数据中不净化政治观点,偏见将从预训练数据传播到语言模型再到下游任务,最终导致公平问题。", "metrics": {"bleu_score": 58.557949197873405, "chrf_score": 51.299719235810514, "xcomet_score": 0.9049346446990967, "xcomet_qe_score": 0.9164391756057739, "metricx_score": 1.1815769672393799, "metricx_qe_score": 1.8295644521713257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试以某种方式净化,我们也会冒着审查或排斥的风险,", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 39.50248293639098, "xcomet_score": 0.8833534717559814, "xcomet_qe_score": 0.8312749862670898, "metricx_score": 1.229874849319458, "metricx_qe_score": 2.4382102489471436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定什么才是真正中立的,应该保留在语言模型训练数据中。所以,", "metrics": {"bleu_score": 13.950557998119553, "chrf_score": 15.81810703231758, "xcomet_score": 0.7331921458244324, "xcomet_qe_score": 0.6691895723342896, "metricx_score": 4.989007949829102, "metricx_qe_score": 3.648995876312256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电查理问题。", "metrics": {"bleu_score": 40.88064519392259, "chrf_score": 31.713652924762105, "xcomet_score": 0.76179438829422, "xcomet_qe_score": 0.6336899995803833, "metricx_score": 4.523562908172607, "metricx_qe_score": 4.603451728820801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以我们做了什么,", "metrics": {"bleu_score": 6.772997136689072, "chrf_score": 22.79693486590038, "xcomet_score": 0.5711081027984619, "xcomet_qe_score": 0.5846779346466064, "metricx_score": 3.7764933109283447, "metricx_qe_score": 3.7174501419067383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这就是我今天要讲的全部。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 55.344377844377846, "xcomet_score": 0.9879318475723267, "xcomet_qe_score": 0.9585496187210083, "metricx_score": 0.33117058873176575, "metricx_qe_score": 0.5036941766738892, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.8882513046264648, "xcomet_qe_score": 0.5656049251556396, "metricx_score": 0.5979865193367004, "metricx_qe_score": 0.6249662041664124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基梅隆大学的一名一年级博士生,今天我将为大家介绍你们的作品《分析位置、特征设计偏见、Beta集合和模型》。", "metrics": {"bleu_score": 44.745115924667935, "chrf_score": 33.22164886787018, "xcomet_score": 0.6754905581474304, "xcomet_qe_score": 0.7520788908004761, "metricx_score": 6.7952728271484375, "metricx_qe_score": 7.160379886627197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的,其中包括塞巴斯蒂安·桑蒂、罗宁·莱布拉斯、卡塔里娜·赖尼克和马丁·萨普。", "metrics": {"bleu_score": 35.26448851964897, "chrf_score": 24.981753113320877, "xcomet_score": 0.845514178276062, "xcomet_qe_score": 0.8909738063812256, "metricx_score": 0.9135130047798157, "metricx_qe_score": 0.7403285503387451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们先想象一下,你为一家报纸工作,正在筛选新闻文章下的评论,试图删除有毒内容。", "metrics": {"bleu_score": 52.18064313562864, "chrf_score": 43.9209683695952, "xcomet_score": 0.8883622884750366, "xcomet_qe_score": 0.8934135437011719, "metricx_score": 1.678695559501648, "metricx_qe_score": 1.5054739713668823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会使用一个流行的API,如Perspective API进行有毒性检测,如果你是卡尔·琼斯,这个方法效果很好,因为", "metrics": {"bleu_score": 30.15502919847089, "chrf_score": 34.82237512563759, "xcomet_score": 0.656684935092926, "xcomet_qe_score": 0.6325463056564331, "metricx_score": 5.813614368438721, "metricx_qe_score": 4.29387903213501, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能够正确检测出有毒实例。", "metrics": {"bleu_score": 46.0462862587273, "chrf_score": 64.06209370339805, "xcomet_score": 0.7851195335388184, "xcomet_qe_score": 0.7252185344696045, "metricx_score": 4.535106658935547, "metricx_qe_score": 4.620347023010254, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于迪西亚·夏尔马来说,情况就不一样了,因为", "metrics": {"bleu_score": 9.009113474307316, "chrf_score": 8.30487681806053, "xcomet_score": 0.1633175015449524, "xcomet_qe_score": 0.14109478890895844, "metricx_score": 5.585987567901611, "metricx_qe_score": 3.3804426193237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API对印度语境中更常见的攻击性用词并不敏感。", "metrics": {"bleu_score": 64.42271946445945, "chrf_score": 68.23351827838022, "xcomet_score": 0.8377562761306763, "xcomet_qe_score": 0.751447319984436, "metricx_score": 4.321232318878174, "metricx_qe_score": 4.126806259155273, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们在这里看到技术在不同人群之间的系统性性能差异。", "metrics": {"bleu_score": 49.608376191368286, "chrf_score": 45.315649175926, "xcomet_score": 0.9821426868438721, "xcomet_qe_score": 0.8593618869781494, "metricx_score": 0.9824047088623047, "metricx_qe_score": 1.5483784675598145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们之前看到的这种设计偏见可能是由于NLP研究人员和模型开发人员的定位性造成的。", "metrics": {"bleu_score": 40.439674066212675, "chrf_score": 35.82477762839603, "xcomet_score": 0.8616054058074951, "xcomet_qe_score": 0.8394625186920166, "metricx_score": 2.648562431335449, "metricx_qe_score": 2.6268234252929688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "定位性是指人们由于其人口统计特征、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 65.15366202853994, "chrf_score": 67.0334250840176, "xcomet_score": 0.7986843585968018, "xcomet_qe_score": 0.8921710252761841, "metricx_score": 4.677089691162109, "metricx_qe_score": 4.162191867828369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 73.97378912627735, "chrf_score": 67.41898597045686, "xcomet_score": 0.9927648305892944, "xcomet_qe_score": 0.908970832824707, "metricx_score": 0.9186691045761108, "metricx_qe_score": 1.4367049932479858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,定位性可以影响研究过程及其结果和结论,因为它可以改变研究人员做出的决策。", "metrics": {"bleu_score": 53.24640975876925, "chrf_score": 45.85023140774402, "xcomet_score": 0.8607462644577026, "xcomet_qe_score": 0.8488207459449768, "metricx_score": 4.104347229003906, "metricx_qe_score": 3.402484893798828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问,数据集和模型是否有定位性?", "metrics": {"bleu_score": 50.647262871731705, "chrf_score": 44.78623587615873, "xcomet_score": 0.9384201765060425, "xcomet_qe_score": 1.0, "metricx_score": 2.233062267303467, "metricx_qe_score": 1.2784347534179688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型本身和数据集本身具有人口统计特征和生活经历,但它们确实汇集了真实的人们的判断和观点,因此可以代表某些定位性优于其他定位性。", "metrics": {"bleu_score": 50.316515870659245, "chrf_score": 44.2495492059117, "xcomet_score": 0.7966954112052917, "xcomet_qe_score": 0.8397907614707947, "metricx_score": 4.882476329803467, "metricx_qe_score": 4.478271484375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,之前的研究提出了关于定位性的轶事证据,例如模型和数据集中的文化差距,以及模型定位性的理论定义。", "metrics": {"bleu_score": 35.92178131845817, "chrf_score": 29.054311360196106, "xcomet_score": 0.7996810674667358, "xcomet_qe_score": 0.6927480697631836, "metricx_score": 4.408923149108887, "metricx_qe_score": 4.542145252227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些研究并没有真正比较最终用户与数据集和模型本身。随着NLP任务变得更加主观和社会导向,研究模型和数据集定位性变得越来越重要。要描述这些定位性的偏差是很困难的,因为并非所有决策都被记录下来,许多模型隐藏在API后面。", "metrics": {"bleu_score": 52.36281716476824, "chrf_score": 47.29989300860022, "xcomet_score": 0.7373024821281433, "xcomet_qe_score": 0.8148816823959351, "metricx_score": 3.241140365600586, "metricx_qe_score": 2.6727354526519775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了研究数据集和模型定位性,我们实际上是将注释与真实用户与现有数据集和模型进行比较。", "metrics": {"bleu_score": 50.54596081884108, "chrf_score": 44.768422853724246, "xcomet_score": 0.7282845973968506, "xcomet_qe_score": 0.7394056916236877, "metricx_score": 3.51753830909729, "metricx_qe_score": 3.4643611907958984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架NLPositionality来实现这一点。", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 62.11411554992817, "xcomet_score": 0.9418507814407349, "xcomet_qe_score": 0.9376275539398193, "metricx_score": 0.6295923590660095, "metricx_qe_score": 1.3104772567749023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架分为两个主要步骤。", "metrics": {"bleu_score": 43.138943204452076, "chrf_score": 38.37551342601929, "xcomet_score": 0.9702411890029907, "xcomet_qe_score": 0.9063276052474976, "metricx_score": 0.10670393705368042, "metricx_qe_score": 0.35922718048095703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是用多样化的注释者重新注释数据集。", "metrics": {"bleu_score": 51.97958014647757, "chrf_score": 43.65118928277013, "xcomet_score": 0.8650784492492676, "xcomet_qe_score": 0.8830803632736206, "metricx_score": 2.583974838256836, "metricx_qe_score": 2.666698932647705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集、注释者的人口统计数据,因为通常只有少数注释者对每个实例进行注释,而且人口统计数据很少被收集和分享。", "metrics": {"bleu_score": 74.42724615850366, "chrf_score": 69.22669829934912, "xcomet_score": 0.8485662937164307, "xcomet_qe_score": 0.7702548503875732, "metricx_score": 1.3634401559829712, "metricx_qe_score": 1.4307175874710083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新注释数据,以便每个实例有多个注释者,并获得丰富的社会人口数据。", "metrics": {"bleu_score": 33.062142261692244, "chrf_score": 31.23341203773677, "xcomet_score": 0.8490915298461914, "xcomet_qe_score": 0.7710289359092712, "metricx_score": 2.090968370437622, "metricx_qe_score": 2.156287908554077, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将根据社会人口数据进行注释,并使用Parsons R相关系数将其与模型和数据集进行比较。因此,我们的框架实际上与注释者意见不一致的文献不同,因为它比较了最终用户与模型和数据集的预测和标签,而不是仅仅关注注释者的一致性或注释者分布", "metrics": {"bleu_score": 53.14544031166997, "chrf_score": 45.6939211196836, "xcomet_score": 0.6669653058052063, "xcomet_qe_score": 0.5473085045814514, "metricx_score": 3.998121976852417, "metricx_qe_score": 4.767467975616455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的建模。我们的框架在很大程度上得益于Lab", "metrics": {"bleu_score": 13.669299099755102, "chrf_score": 11.920354117007934, "xcomet_score": 0.19146980345249176, "xcomet_qe_score": 0.19026069343090057, "metricx_score": 17.03838348388672, "metricx_qe_score": 15.13544750213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the Wild是一个在线实验平台,我们可以招募到多样化的志愿者,", "metrics": {"bleu_score": 53.49135347577814, "chrf_score": 54.48276112029296, "xcomet_score": 0.8725613355636597, "xcomet_qe_score": 0.6933315396308899, "metricx_score": 4.624232769012451, "metricx_qe_score": 6.782144069671631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相比之下,像MTurk这样的平台主要有来自美国或印度的参与者。此外,Lab in the Wild仍然能够获得高质量的数据。", "metrics": {"bleu_score": 48.88806149443019, "chrf_score": 55.644059521316095, "xcomet_score": 0.8286664485931396, "xcomet_qe_score": 0.854363739490509, "metricx_score": 1.086937427520752, "metricx_qe_score": 0.9862784147262573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Lab in the Wild上举办了两项任务,其中一项是社会可接受性。它的工作原理是,参与者将阅读社会化学数据集中的一个情境,然后写出这个情境在社会上是多么可接受。", "metrics": {"bleu_score": 50.85222049395246, "chrf_score": 48.42469816966658, "xcomet_score": 0.8399075269699097, "xcomet_qe_score": 0.8134970664978027, "metricx_score": 2.412900447845459, "metricx_qe_score": 2.993464946746826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持参与研究,他们可以将自己的回答与AI和其他人的回答进行比较。", "metrics": {"bleu_score": 75.97383875533318, "chrf_score": 70.19791204908213, "xcomet_score": 0.8835495710372925, "xcomet_qe_score": 0.8812954425811768, "metricx_score": 1.1496191024780273, "metricx_qe_score": 1.6605678796768188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与GPT 4中的社会化学德尔菲进行比较。", "metrics": {"bleu_score": 38.338777229853434, "chrf_score": 31.99408628023391, "xcomet_score": 0.8660335540771484, "xcomet_qe_score": 0.8154957294464111, "metricx_score": 4.66025447845459, "metricx_qe_score": 4.4976983070373535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们为有毒性和仇恨言论检测任务复制了一个非常相似的设置,参与者将阅读DynaHate中的一个实例,并写出他们是否认为这是一个仇恨言论的实例。", "metrics": {"bleu_score": 58.69271049727931, "chrf_score": 55.01314933597282, "xcomet_score": 0.7699767351150513, "xcomet_qe_score": 0.7820602655410767, "metricx_score": 2.366521120071411, "metricx_qe_score": 2.7790894508361816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与DynaHate、Perspective API、Rewire API、Hate Roberta在GPT 4中进行比较。", "metrics": {"bleu_score": 59.070514573367674, "chrf_score": 78.31262181609738, "xcomet_score": 0.8677365779876709, "xcomet_qe_score": 0.8021750450134277, "metricx_score": 3.5283727645874023, "metricx_qe_score": 4.151892185211182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终积累了来自八十七个国家的超过一千名注释者的十六万多个注释。", "metrics": {"bleu_score": 39.041200211244266, "chrf_score": 29.284608851793397, "xcomet_score": 0.891677737236023, "xcomet_qe_score": 0.9815107583999634, "metricx_score": 1.108442783355713, "metricx_qe_score": 0.7907050251960754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们现在更有能力回答谁与NLP数据集和模型最符合?我们", "metrics": {"bleu_score": 48.950961379857205, "chrf_score": 46.400897739978205, "xcomet_score": 0.7084057331085205, "xcomet_qe_score": 0.7321085929870605, "metricx_score": 5.842773914337158, "metricx_qe_score": 1.865774154663086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现NLP中存在定位性。", "metrics": {"bleu_score": 10.126442477235686, "chrf_score": 19.629777516446037, "xcomet_score": 0.8390707969665527, "xcomet_qe_score": 0.805761456489563, "metricx_score": 3.4740240573883057, "metricx_qe_score": 2.358354330062866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最符合英语国家的用户。因此", "metrics": {"bleu_score": 45.33710895095744, "chrf_score": 39.50138806100798, "xcomet_score": 0.836583137512207, "xcomet_qe_score": 0.817164957523346, "metricx_score": 2.571972131729126, "metricx_qe_score": 0.9347166419029236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPD 4社会可接受性分析,我们发现它最符合儒家和英语国家的用户。", "metrics": {"bleu_score": 54.319380965500514, "chrf_score": 49.04931846121634, "xcomet_score": 0.7722882628440857, "xcomet_qe_score": 0.7975648641586304, "metricx_score": 3.01792573928833, "metricx_qe_score": 3.7251129150390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现DanaHate也最符合英语国家的用户。", "metrics": {"bleu_score": 64.1975224568211, "chrf_score": 57.70208624327617, "xcomet_score": 0.8497046232223511, "xcomet_qe_score": 0.8658092617988586, "metricx_score": 1.781174659729004, "metricx_qe_score": 1.836392879486084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现与受过大学教育的人群的额外对齐。因此", "metrics": {"bleu_score": 33.77724221619386, "chrf_score": 28.04823475345276, "xcomet_score": 0.6362454891204834, "xcomet_qe_score": 0.6719408631324768, "metricx_score": 6.370044708251953, "metricx_qe_score": 3.8100242614746094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPD 4的社会可接受性任务,我们发现它最符合受过大学教育或研究生教育的人群。我们发现DanaHate也最符合受过大学教育的人群。", "metrics": {"bleu_score": 47.93686366179061, "chrf_score": 40.633400297446904, "xcomet_score": 0.8313419818878174, "xcomet_qe_score": 0.7394022941589355, "metricx_score": 4.676830768585205, "metricx_qe_score": 4.4767937660217285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,有些人不可避免地会被抛在后面。一个例子", "metrics": {"bleu_score": 44.910995009687426, "chrf_score": 42.99224018447892, "xcomet_score": 0.7212121486663818, "xcomet_qe_score": 0.6895356178283691, "metricx_score": 2.6209826469421387, "metricx_qe_score": 2.5738062858581543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,数据集和模型与非二元人群的对齐程度低于男性和女性人群。", "metrics": {"bleu_score": 44.05155249693397, "chrf_score": 35.43146431794215, "xcomet_score": 0.5778648853302002, "xcomet_qe_score": 0.5974311828613281, "metricx_score": 4.487579822540283, "metricx_qe_score": 4.6895012855529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT 4社会可接受性任务和DynaHate任务分析中都发现了这一点。", "metrics": {"bleu_score": 73.69529523204436, "chrf_score": 67.71653007810683, "xcomet_score": 0.8973674774169922, "xcomet_qe_score": 0.8810026049613953, "metricx_score": 1.4151419401168823, "metricx_qe_score": 2.0626816749572754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,鉴于NLP中存在定位性,我们能对此做些什么?我们", "metrics": {"bleu_score": 23.203058032469887, "chrf_score": 27.540961450866035, "xcomet_score": 0.766192615032196, "xcomet_qe_score": 0.7669244408607483, "metricx_score": 6.802619934082031, "metricx_qe_score": 2.1616344451904297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对此有几个建议。", "metrics": {"bleu_score": 18.594002123233256, "chrf_score": 16.920511497175358, "xcomet_score": 0.9838346242904663, "xcomet_qe_score": 0.9525736570358276, "metricx_score": 0.33213287591934204, "metricx_qe_score": 0.3007272779941559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,在整个研究过程中记录所有相关的设计选择。另一个", "metrics": {"bleu_score": 48.93822732883849, "chrf_score": 41.58988166705933, "xcomet_score": 0.8589763045310974, "xcomet_qe_score": 0.8108397126197815, "metricx_score": 3.721737861633301, "metricx_qe_score": 0.438043475151062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "建议是通过视角主义的视角进行NLP研究。", "metrics": {"bleu_score": 11.115304656283836, "chrf_score": 10.708391085193139, "xcomet_score": 0.6585453748703003, "xcomet_qe_score": 0.6508768796920776, "metricx_score": 1.9243546724319458, "metricx_qe_score": 1.755454659461975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专门的数据集和模型。", "metrics": {"bleu_score": 90.61874434879648, "chrf_score": 86.31885674989124, "xcomet_score": 0.992019772529602, "xcomet_qe_score": 0.9185789823532104, "metricx_score": 0.8037492632865906, "metricx_qe_score": 1.2312043905258179, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane倡议。", "metrics": {"bleu_score": 55.2058197664637, "chrf_score": 43.77089436218316, "xcomet_score": 0.7475622892379761, "xcomet_qe_score": 0.789522647857666, "metricx_score": 2.372803211212158, "metricx_qe_score": 4.278580188751221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调,包容性的NLP不仅仅是让所有", "metrics": {"bleu_score": 58.527733690258756, "chrf_score": 56.43784026617379, "xcomet_score": 0.6656561493873596, "xcomet_qe_score": 0.4712170958518982, "metricx_score": 5.7972564697265625, "metricx_qe_score": 4.921943664550781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人工作。", "metrics": {"bleu_score": 21.165417903210944, "chrf_score": 19.232670017992138, "xcomet_score": 0.8996185064315796, "xcomet_qe_score": 0.890823483467102, "metricx_score": 3.384999990463257, "metricx_qe_score": 4.1462836265563965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的介绍结束。", "metrics": {"bleu_score": 23.462350320527996, "chrf_score": 21.115595317610385, "xcomet_score": 0.848111629486084, "xcomet_qe_score": 0.8329120874404907, "metricx_score": 2.4792258739471436, "metricx_qe_score": 1.4756971597671509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果你想了解更多,请随时查看我们的仪表板以获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 56.92907020342422, "chrf_score": 51.528204312603684, "xcomet_score": 0.9807636737823486, "xcomet_qe_score": 0.9589048624038696, "metricx_score": 0.6480488181114197, "metricx_qe_score": 0.5786093473434448, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是来自芬大学的Xi Yuan。", "metrics": {"bleu_score": 25.504377483159427, "chrf_score": 27.35733084128847, "xcomet_score": 0.7145829200744629, "xcomet_qe_score": 0.7365301251411438, "metricx_score": 5.558138847351074, "metricx_qe_score": 4.412704944610596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我在这里介绍我们关于从大型语言模型中提取特定脚本知识以进行受限语言规划的工作。", "metrics": {"bleu_score": 38.28999137120322, "chrf_score": 34.20341687792127, "xcomet_score": 0.8991189002990723, "xcomet_qe_score": 0.7764610052108765, "metricx_score": 1.1254867315292358, "metricx_qe_score": 1.1951038837432861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类通常通过遵循分步指令的形式来规划自己的行动,这些指令以保证脚本", "metrics": {"bleu_score": 29.31576784974492, "chrf_score": 26.16907061964299, "xcomet_score": 0.6764316558837891, "xcomet_qe_score": 0.7665221095085144, "metricx_score": 6.571736812591553, "metricx_qe_score": 3.422593593597412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的形式存在。之前的研究已经探讨了语言模型如何为典型的抽象目标(", "metrics": {"bleu_score": 27.940274311330956, "chrf_score": 21.69156356821487, "xcomet_score": 0.5574818849563599, "xcomet_qe_score": 0.49378398060798645, "metricx_score": 14.46030330657959, "metricx_qe_score": 17.2341365814209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如制作蛋糕)进行规划,并证明了大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 45.47912444166088, "chrf_score": 50.86952800139423, "xcomet_score": 0.3400213122367859, "xcomet_qe_score": 0.30799609422683716, "metricx_score": 3.9246604442596436, "metricx_qe_score": 2.9841854572296143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究主要集中在为典型的抽象目标进行规划。", "metrics": {"bleu_score": 49.34916370623362, "chrf_score": 41.974424126598045, "xcomet_score": 0.835486888885498, "xcomet_qe_score": 0.9046830534934998, "metricx_score": 2.727217197418213, "metricx_qe_score": 2.1347427368164062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而对于具有特定目标、特定约束(如制作巧克力蛋糕)的目标进行规划的研究则相对较少。", "metrics": {"bleu_score": 18.014636992642135, "chrf_score": 20.330091069884162, "xcomet_score": 0.8137246370315552, "xcomet_qe_score": 0.8168171048164368, "metricx_score": 1.2315099239349365, "metricx_qe_score": 1.4070265293121338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们定义了受限语言规划的问题,该问题对规划目标施加了不同的约束。", "metrics": {"bleu_score": 76.84893955817087, "chrf_score": 75.99165435374401, "xcomet_score": 0.9548794031143188, "xcomet_qe_score": 0.8771605491638184, "metricx_score": 1.4355369806289673, "metricx_qe_score": 1.921762228012085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被不同的现实生活中的具体目标继承,这些具体目标具有", "metrics": {"bleu_score": 30.454520050730334, "chrf_score": 28.930651547174463, "xcomet_score": 0.7312098741531372, "xcomet_qe_score": 0.7923187017440796, "metricx_score": 8.188196182250977, "metricx_qe_score": 6.526149749755859, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更复杂、多方面的约束。一个好的规划者应该编写符合约束且忠实于约束的脚本。", "metrics": {"bleu_score": 24.123909186863205, "chrf_score": 24.44940157138503, "xcomet_score": 0.6949663162231445, "xcomet_qe_score": 0.2372334897518158, "metricx_score": 2.5249454975128174, "metricx_qe_score": 2.886991500854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们首先评估和改进大型语言模型的受限语言规划能力。", "metrics": {"bleu_score": 60.611510792729035, "chrf_score": 57.635345054938234, "xcomet_score": 0.9822655916213989, "xcomet_qe_score": 0.9860363006591797, "metricx_score": 0.6246562004089355, "metricx_qe_score": 0.6755827069282532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定的目标数据集来支持我们的研究,我们必须首先获取这些目标。", "metrics": {"bleu_score": 69.93667235893774, "chrf_score": 63.727784215084625, "xcomet_score": 0.8916395902633667, "xcomet_qe_score": 0.8722811341285706, "metricx_score": 1.412216067314148, "metricx_qe_score": 2.4574525356292725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们通过使用指示性TPT方法,扩展了人类循环数据获取的多方面约束的抽象目标。我们随机", "metrics": {"bleu_score": 23.507688406958025, "chrf_score": 19.876827294847814, "xcomet_score": 0.4474588632583618, "xcomet_qe_score": 0.46538373827934265, "metricx_score": 10.16614055633545, "metricx_qe_score": 8.107973098754883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "抽取了100个具体目标,并评估了Light Logic模型生成的脚本。", "metrics": {"bleu_score": 60.83638494703331, "chrf_score": 54.51811186524074, "xcomet_score": 0.824660062789917, "xcomet_qe_score": 0.8477399349212646, "metricx_score": 3.882298707962036, "metricx_qe_score": 4.601196765899658, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确率。", "metrics": {"bleu_score": 25.491833774890388, "chrf_score": 21.492558656584766, "xcomet_score": 0.9946444034576416, "xcomet_qe_score": 0.9903422594070435, "metricx_score": 0.6826243996620178, "metricx_qe_score": 0.6823980808258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有Light Logic模型在规划具体目标方面都取得了不令人满意的结果。", "metrics": {"bleu_score": 45.79670370130185, "chrf_score": 41.06233669540656, "xcomet_score": 0.8691203594207764, "xcomet_qe_score": 0.8577972650527954, "metricx_score": 4.6628031730651855, "metricx_qe_score": 4.919653415679932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行了详细的分析,以调查Light Logic模型的原因。", "metrics": {"bleu_score": 39.16057961275291, "chrf_score": 30.495002148491828, "xcomet_score": 0.7985670566558838, "xcomet_qe_score": 0.7622185945510864, "metricx_score": 5.758824348449707, "metricx_qe_score": 6.764427661895752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果显示,生成的脚本在语义完整性方面是可接受的。但是,无法保证对约束的忠实度。", "metrics": {"bleu_score": 31.767978994543064, "chrf_score": 28.288719398641245, "xcomet_score": 0.9761368036270142, "xcomet_qe_score": 0.9689205884933472, "metricx_score": 1.288520097732544, "metricx_qe_score": 1.4264291524887085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了根据回家时的具体情况对约束进行更具体的分类。", "metrics": {"bleu_score": 11.391856953132569, "chrf_score": 10.557573492336056, "xcomet_score": 0.600257158279419, "xcomet_qe_score": 0.4938581883907318, "metricx_score": 6.207162380218506, "metricx_qe_score": 7.187899589538574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的主图显示,指示GPD的规划性能在不同类别的女孩中差异很大。", "metrics": {"bleu_score": 30.184771922017624, "chrf_score": 22.352751287028738, "xcomet_score": 0.4448283016681671, "xcomet_qe_score": 0.4070742130279541, "metricx_score": 7.6971869468688965, "metricx_qe_score": 7.630647659301758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究表明,Larry模型的输出质量存在高方差,导致性能不佳。", "metrics": {"bleu_score": 46.39035504542229, "chrf_score": 38.00062065072403, "xcomet_score": 0.8114254474639893, "xcomet_qe_score": 0.77509605884552, "metricx_score": 4.387805461883545, "metricx_qe_score": 4.318292140960693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过度生成的Zen过滤器来提高生成质量。", "metrics": {"bleu_score": 44.13713504244446, "chrf_score": 36.255739618759776, "xcomet_score": 0.8649850487709045, "xcomet_qe_score": 0.8166727423667908, "metricx_score": 4.781680583953857, "metricx_qe_score": 5.183445453643799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示了指示GPT的约束类型及其示例,并根据一组抽象目标获得了具体目标。", "metrics": {"bleu_score": 42.34426103854124, "chrf_score": 32.29410348814637, "xcomet_score": 0.7247675657272339, "xcomet_qe_score": 0.7399678230285645, "metricx_score": 3.6445398330688477, "metricx_qe_score": 3.5069265365600586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指示GPT过度生成了特定目标的关键脚本。", "metrics": {"bleu_score": 22.73861230490962, "chrf_score": 27.44481355839949, "xcomet_score": 0.7048039436340332, "xcomet_qe_score": 0.7437490820884705, "metricx_score": 5.2249956130981445, "metricx_qe_score": 6.042442321777344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤器模型来选择合适的脚本。", "metrics": {"bleu_score": 49.75501232439635, "chrf_score": 40.50588753801873, "xcomet_score": 0.9589149951934814, "xcomet_qe_score": 0.9376730918884277, "metricx_score": 0.8960623145103455, "metricx_qe_score": 1.0353949069976807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为指示GPT的位元组,并计算余弦相似度和相似度分数以衡量语义相似度。", "metrics": {"bleu_score": 65.76250895254542, "chrf_score": 50.4138286636884, "xcomet_score": 0.7614010572433472, "xcomet_qe_score": 0.6569201946258545, "metricx_score": 3.0917816162109375, "metricx_qe_score": 2.8906009197235107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们将编写包含目标约束关键词的脚本。", "metrics": {"bleu_score": 55.459821498914366, "chrf_score": 52.10931240103659, "xcomet_score": 0.8016607761383057, "xcomet_qe_score": 0.7558732032775879, "metricx_score": 6.41712760925293, "metricx_qe_score": 6.772392749786377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果目标得分在目标站点中最高,我们才保留该脚本。", "metrics": {"bleu_score": 17.30215006043178, "chrf_score": 21.29256260747566, "xcomet_score": 0.8026716709136963, "xcomet_qe_score": 0.6967298984527588, "metricx_score": 4.624654293060303, "metricx_qe_score": 5.409701347351074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,Inslacity可以生成更高质量的脚本。", "metrics": {"bleu_score": 81.37489370974959, "chrf_score": 53.69611084549407, "xcomet_score": 0.810837984085083, "xcomet_qe_score": 0.8064249753952026, "metricx_score": 4.7058539390563965, "metricx_qe_score": 5.192954063415527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法大大提高了语义完整性和对约束的忠实度。", "metrics": {"bleu_score": 48.58597241635416, "chrf_score": 44.008698714016504, "xcomet_score": 0.8431879281997681, "xcomet_qe_score": 0.9061342477798462, "metricx_score": 1.6014350652694702, "metricx_qe_score": 2.851912260055542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高,因此必须使较小且专业化的模型具备语言规划能力。", "metrics": {"bleu_score": 45.50801919596565, "chrf_score": 40.072914943905516, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4711959958076477, "metricx_qe_score": 0.6616513729095459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的重要步骤。", "metrics": {"bleu_score": 80.61898627027144, "chrf_score": 74.76670903477938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07551921904087067, "metricx_qe_score": 0.17079289257526398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究没有实现对特定目标的规划,手动数据集标注成本高。", "metrics": {"bleu_score": 27.932340705747453, "chrf_score": 25.164008721653996, "xcomet_score": 0.9803733825683594, "xcomet_qe_score": 0.8593251705169678, "metricx_score": 1.38422691822052, "metricx_qe_score": 1.9191244840621948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的思想,对受限语言规划模型进行蒸馏。", "metrics": {"bleu_score": 38.79929978062759, "chrf_score": 33.85473841691844, "xcomet_score": 0.7876219749450684, "xcomet_qe_score": 0.7064270377159119, "metricx_score": 6.121689796447754, "metricx_qe_score": 6.39271879196167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们应用我们的方法来构建一个名为code script的受限语言规划数据集。总共,我们生成", "metrics": {"bleu_score": 31.768724065877365, "chrf_score": 32.9673394357636, "xcomet_score": 0.684249758720398, "xcomet_qe_score": 0.6086511015892029, "metricx_score": 7.242921352386475, "metricx_qe_score": 6.520595550537109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了五万五千个具有脚本的具体目标,", "metrics": {"bleu_score": 16.592672810593047, "chrf_score": 13.335919576577274, "xcomet_score": 0.3147988021373749, "xcomet_qe_score": 0.3167461156845093, "metricx_score": 8.725024223327637, "metricx_qe_score": 8.174112319946289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以确保验证和测试站点的质量。我们要求云源工作人员查找并修改错误的样本。", "metrics": {"bleu_score": 60.20902374047972, "chrf_score": 52.654314862066954, "xcomet_score": 0.7531746625900269, "xcomet_qe_score": 0.7542219161987305, "metricx_score": 5.78988790512085, "metricx_qe_score": 5.602760314941406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了code script的受限分布。", "metrics": {"bleu_score": 28.917849332325716, "chrf_score": 33.672133820760195, "xcomet_score": 0.8594353795051575, "xcomet_qe_score": 0.772892951965332, "metricx_score": 4.238462448120117, "metricx_qe_score": 5.040529251098633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,code script在生成的具体目标中显示了假设。", "metrics": {"bleu_score": 42.30453733384412, "chrf_score": 45.81687767691113, "xcomet_score": 0.6274243593215942, "xcomet_qe_score": 0.5638601779937744, "metricx_score": 9.151121139526367, "metricx_qe_score": 9.314613342285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有了code script,我们可以训练较小但专业化的模型进行受限语言规划。", "metrics": {"bleu_score": 57.19139519171822, "chrf_score": 46.516431388344756, "xcomet_score": 0.7960430383682251, "xcomet_qe_score": 0.7896208763122559, "metricx_score": 3.511747360229492, "metricx_qe_score": 3.9485738277435303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在code rate上应用TFIL函数可以生成比大多数大型语言模型更高质量的脚本,这表明在适当的数据站点上进行适当训练的小型模型可以支持大型模型。", "metrics": {"bleu_score": 44.462718607526995, "chrf_score": 34.76180659178292, "xcomet_score": 0.5135713219642639, "xcomet_qe_score": 0.49689728021621704, "metricx_score": 8.062541007995605, "metricx_qe_score": 7.713939666748047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了受限语言规划问题。", "metrics": {"bleu_score": 72.83860464220109, "chrf_score": 71.99900522537406, "xcomet_score": 0.9069202542304993, "xcomet_qe_score": 0.8321535587310791, "metricx_score": 1.9986854791641235, "metricx_qe_score": 2.526510000228882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受限语言规划能力,并为大型语言模型开发了一种过度生成的过滤器方法。", "metrics": {"bleu_score": 63.834837872289214, "chrf_score": 55.46856839347699, "xcomet_score": 0.8492277264595032, "xcomet_qe_score": 0.8228806257247925, "metricx_score": 2.2447946071624756, "metricx_qe_score": 2.819568157196045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成了一个高质量的脚本数据集,CodeScript,用于建设性语言规划。", "metrics": {"bleu_score": 45.090045716114, "chrf_score": 44.146620707059704, "xcomet_score": 0.8229249715805054, "xcomet_qe_score": 0.7644716501235962, "metricx_score": 2.664990186691284, "metricx_qe_score": 3.293180465698242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望CodeScript数据集可以成为推进语言规划研究的宝贵资源。", "metrics": {"bleu_score": 74.32254773174189, "chrf_score": 77.35500613978323, "xcomet_score": 0.9152942895889282, "xcomet_qe_score": 0.9069105386734009, "metricx_score": 0.7614128589630127, "metricx_qe_score": 0.9933894276618958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的聆听。", "metrics": {"bleu_score": 10.175282441454787, "chrf_score": 9.0767898971382, "xcomet_score": 0.8664944171905518, "xcomet_qe_score": 0.886152982711792, "metricx_score": 0.7279521822929382, "metricx_qe_score": 0.7234898209571838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有关CodeScript的更多详细信息,请参阅我们的论文。", "metrics": {"bleu_score": 49.385277270209265, "chrf_score": 57.23678876989236, "xcomet_score": 0.9756028652191162, "xcomet_qe_score": 0.9884356260299683, "metricx_score": 0.6689427495002747, "metricx_qe_score": 0.707221508026123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫徐恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8726203441619873, "xcomet_qe_score": 0.8842723965644836, "metricx_score": 0.0, "metricx_qe_score": 0.11707936227321625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的论文《2003年核技术命名实体标注器在2023年还能否正常工作?》", "metrics": {"bleu_score": 47.915647473263014, "chrf_score": 46.55601160617056, "xcomet_score": 0.7764517068862915, "xcomet_qe_score": 0.7632511258125305, "metricx_score": 4.219080924987793, "metricx_qe_score": 4.015583038330078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.9978755712509155, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了使用命名实体识别任务或NER任务进行泛化的问题。", "metrics": {"bleu_score": 56.3175436224042, "chrf_score": 50.44106832228087, "xcomet_score": 0.8923683762550354, "xcomet_qe_score": 0.8046994209289551, "metricx_score": 1.6376698017120361, "metricx_qe_score": 3.852041721343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,近20年来,模型一直在使用2003年核技术来开发NER。这自然引发了几个问题。", "metrics": {"bleu_score": 26.66785198355665, "chrf_score": 27.967074538829245, "xcomet_score": 0.749106764793396, "xcomet_qe_score": 0.7736846208572388, "metricx_score": 6.507179260253906, "metricx_qe_score": 5.633693695068359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据,", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9714715480804443, "xcomet_qe_score": 0.9611616134643555, "metricx_score": 0.39283379912376404, "metricx_qe_score": 0.3439098596572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及在开发新的标注器时,需要什么才能同时实现良好的泛化。", "metrics": {"bleu_score": 46.09056322258578, "chrf_score": 39.81850841318747, "xcomet_score": 0.975767970085144, "xcomet_qe_score": 0.957974910736084, "metricx_score": 1.4123295545578003, "metricx_qe_score": 1.1975622177124023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们观察到泛化效果不佳,是什么原因导致这些模型性能下降?", "metrics": {"bleu_score": 28.314256939201865, "chrf_score": 23.663571891583985, "xcomet_score": 0.9924354553222656, "xcomet_qe_score": 0.9806739091873169, "metricx_score": 0.8429610133171082, "metricx_qe_score": 0.8127906918525696, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了Kernel Plus Plus数据集。这是", "metrics": {"bleu_score": 38.398171330793495, "chrf_score": 24.671560118995252, "xcomet_score": 0.7210814356803894, "xcomet_qe_score": 0.714073657989502, "metricx_score": 7.372470378875732, "metricx_qe_score": 5.561268329620361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从2020年路透社新闻中收集的数据集,并使用相同的2003年核技术标注指南对其进行了标注。", "metrics": {"bleu_score": 55.724881327010465, "chrf_score": 48.059059212200026, "xcomet_score": 0.6817533373832703, "xcomet_qe_score": 0.5957885980606079, "metricx_score": 5.344294548034668, "metricx_qe_score": 5.187870979309082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在2003年核技术上对20多个模型进行了微调。", "metrics": {"bleu_score": 48.988642222409275, "chrf_score": 45.1744761352935, "xcomet_score": 0.7411925196647644, "xcomet_qe_score": 0.7633605003356934, "metricx_score": 5.900318622589111, "metricx_qe_score": 6.324129581451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对Conor 3测试集和Conor Plus Plus测试集进行了评估。", "metrics": {"bleu_score": 37.39214909689668, "chrf_score": 29.627858982391636, "xcomet_score": 0.6681656837463379, "xcomet_qe_score": 0.6470545530319214, "metricx_score": 6.610195159912109, "metricx_qe_score": 6.793048858642578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了每个模型的F1百分比变化,以评估其泛化能力。", "metrics": {"bleu_score": 44.838113640596355, "chrf_score": 36.24507403294365, "xcomet_score": 0.9888978004455566, "xcomet_qe_score": 0.984898567199707, "metricx_score": 0.8988115191459656, "metricx_qe_score": 1.7368943691253662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,实现良好泛化需要什么?", "metrics": {"bleu_score": 34.128395574633934, "chrf_score": 27.97118177136938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2295304834842682, "metricx_qe_score": 0.3168780207633972, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现需要三个主要因素。", "metrics": {"bleu_score": 60.2371569900395, "chrf_score": 57.1372692158173, "xcomet_score": 0.9250376224517822, "xcomet_qe_score": 0.8925312757492065, "metricx_score": 0.5279437899589539, "metricx_qe_score": 0.783385157585144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现Transformer模型通常能更好地泛化到新数据。", "metrics": {"bleu_score": 52.77140132412705, "chrf_score": 60.9534154761757, "xcomet_score": 0.8849719762802124, "xcomet_qe_score": 0.878563404083252, "metricx_score": 1.71575927734375, "metricx_qe_score": 3.442965269088745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常较大的模型能更好地泛化。", "metrics": {"bleu_score": 45.77398674868605, "chrf_score": 38.573922273055835, "xcomet_score": 0.9961094856262207, "xcomet_qe_score": 0.985889196395874, "metricx_score": 0.5113587379455566, "metricx_qe_score": 1.0648908615112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们都知道,微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 61.367385823710336, "chrf_score": 62.581880552535196, "xcomet_score": 0.8840240240097046, "xcomet_qe_score": 0.783621609210968, "metricx_score": 3.455883741378784, "metricx_qe_score": 2.226754665374756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调示例实际上也能带来更好的泛化效果。", "metrics": {"bleu_score": 61.71392021671757, "chrf_score": 54.0859508289967, "xcomet_score": 0.993468165397644, "xcomet_qe_score": 0.9293878674507141, "metricx_score": 0.38501015305519104, "metricx_qe_score": 0.49382561445236206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于我们下一个问题,是什么原因导致某些模型性能下降?我们有两个假设。", "metrics": {"bleu_score": 50.768447664079204, "chrf_score": 43.307696823574595, "xcomet_score": 0.9583436846733093, "xcomet_qe_score": 0.9559011459350586, "metricx_score": 0.8612017631530762, "metricx_qe_score": 1.02732515335083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,这是由于反复使用相同的测试集导致的过拟合,通常表现为在新测试集上的回报递减。", "metrics": {"bleu_score": 55.311213272351964, "chrf_score": 47.04405564835722, "xcomet_score": 0.9092496633529663, "xcomet_qe_score": 0.9328229427337646, "metricx_score": 2.970985174179077, "metricx_qe_score": 3.2529871463775635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,这是由于训练数据和测试数据之间的时间差距增大导致的性能下降。", "metrics": {"bleu_score": 63.457989639265016, "chrf_score": 56.90029023790256, "xcomet_score": 0.9664592742919922, "xcomet_qe_score": 0.8846077919006348, "metricx_score": 1.742610216140747, "metricx_qe_score": 2.436267137527466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于过拟合数据,我们从右侧的图表中看到,红色的最佳拟合线具有大于1的梯度。", "metrics": {"bleu_score": 29.39352987317778, "chrf_score": 29.3136104206009, "xcomet_score": 0.9012641310691833, "xcomet_qe_score": 0.7795894145965576, "metricx_score": 0.838254451751709, "metricx_qe_score": 1.2147228717803955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在2003年核技术上做出的每一项改进,都会在Kernel Plus Plus上转化为超过一个单位的改进,这意味着没有回报递减,", "metrics": {"bleu_score": 31.211190248519117, "chrf_score": 25.675970256491134, "xcomet_score": 0.5027785301208496, "xcomet_qe_score": 0.5329687595367432, "metricx_score": 8.95634651184082, "metricx_qe_score": 9.378819465637207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下没有观察到自适应过拟合。", "metrics": {"bleu_score": 74.93731939490364, "chrf_score": 69.43707675795987, "xcomet_score": 0.9009255766868591, "xcomet_qe_score": 0.9129918217658997, "metricx_score": 1.1392955780029297, "metricx_qe_score": 1.6724114418029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间漂移呢?", "metrics": {"bleu_score": 52.47357977607325, "chrf_score": 40.51960415097498, "xcomet_score": 0.9311673641204834, "xcomet_qe_score": 0.9159005284309387, "metricx_score": 0.3731555640697479, "metricx_qe_score": 0.8887962102890015, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一项实验,使用更近的数据对一些模型进行了再训练或继续预训练,我们发现随着时间差距的增大,性能会下降,这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 57.039652291416274, "chrf_score": 50.30906606082919, "xcomet_score": 0.872146487236023, "xcomet_qe_score": 0.8356224894523621, "metricx_score": 1.5839154720306396, "metricx_qe_score": 1.798593282699585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化,我们需要更好的模型架构、更大的模型大小,以及更多的微调示例,", "metrics": {"bleu_score": 76.31223262784246, "chrf_score": 70.43431498803949, "xcomet_score": 0.9465794563293457, "xcomet_qe_score": 0.8196619153022766, "metricx_score": 0.4956008195877075, "metricx_qe_score": 0.4986773133277893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是相辅相成的。我们不能只拥有一个因素而抛弃其他因素。", "metrics": {"bleu_score": 68.5094033509625, "chrf_score": 57.93499496262895, "xcomet_score": 0.9775726199150085, "xcomet_qe_score": 0.9613838195800781, "metricx_score": 1.0210286378860474, "metricx_qe_score": 1.6172899007797241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,这里的性能下降是由时间漂移引起的,这有点令人惊讶,即使2003年核技术已经使用了20多年。", "metrics": {"bleu_score": 41.59759663977203, "chrf_score": 36.54742391670312, "xcomet_score": 0.6104953289031982, "xcomet_qe_score": 0.626617431640625, "metricx_score": 7.381564140319824, "metricx_qe_score": 9.917085647583008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,回到我们论文标题提出的问题,2003年核技术命名实体标注器在2023年还能否正常工作?", "metrics": {"bleu_score": 32.035058043341, "chrf_score": 32.4770980628352, "xcomet_score": 0.7999061942100525, "xcomet_qe_score": 0.7949965000152588, "metricx_score": 3.3934121131896973, "metricx_qe_score": 3.440680980682373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是一个响亮的肯定。", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 37.27961585945132, "xcomet_score": 0.8980505466461182, "xcomet_qe_score": 0.8718119263648987, "metricx_score": 1.7178252935409546, "metricx_qe_score": 1.4065052270889282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能促使更多人研究如何提高模型的泛化能力。", "metrics": {"bleu_score": 56.41405122106224, "chrf_score": 48.57935283218993, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3274407982826233, "metricx_qe_score": 0.45826369524002075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果您有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.11026448209409, "chrf_score": 52.5325928587273, "xcomet_score": 0.9871149063110352, "xcomet_qe_score": 0.9712950587272644, "metricx_score": 0.2757876515388489, "metricx_qe_score": 0.26652368903160095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9583046436309814, "xcomet_qe_score": 0.9632420539855957, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将谈谈我们在解决间接指称表达以进行实体选择方面的工作,我们在此过程中引入了altentity scorpus。我的名", "metrics": {"bleu_score": 28.478269480502718, "chrf_score": 25.87658772895419, "xcomet_score": 0.47667473554611206, "xcomet_qe_score": 0.40292879939079285, "metricx_score": 12.393511772155762, "metricx_qe_score": 10.92704963684082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "字是Javot Hosseini,这是一项与Philip Radlinsky、Silvia Pareti和Annie Luis合作完成的工作。", "metrics": {"bleu_score": 19.004145843928576, "chrf_score": 57.98905958763952, "xcomet_score": 0.6573045253753662, "xcomet_qe_score": 0.6312904953956604, "metricx_score": 8.446924209594727, "metricx_qe_score": 8.39063549041748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请考虑以下替代问题。", "metrics": {"bleu_score": 26.269098944241577, "chrf_score": 21.57407407407407, "xcomet_score": 0.9122191667556763, "xcomet_qe_score": 0.8873655796051025, "metricx_score": 0.43142640590667725, "metricx_qe_score": 0.17386695742607117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指easy on me还是I got a feeling?这里", "metrics": {"bleu_score": 6.734850273596512, "chrf_score": 26.57164018687879, "xcomet_score": 0.8045535683631897, "xcomet_qe_score": 0.7732154726982117, "metricx_score": 6.802957534790039, "metricx_qe_score": 6.0878400802612305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用户想要在这两个标志之间进行选择。", "metrics": {"bleu_score": 13.24094988519558, "chrf_score": 13.59767037426323, "xcomet_score": 0.8307750225067139, "xcomet_qe_score": 0.829622745513916, "metricx_score": 4.285589694976807, "metricx_qe_score": 4.866875171661377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用,例如说出easy on me这首歌的名字或它的位置第一首,但", "metrics": {"bleu_score": 39.14607823364265, "chrf_score": 36.08306697893606, "xcomet_score": 0.4001780152320862, "xcomet_qe_score": 0.2785317599773407, "metricx_score": 5.41995906829834, "metricx_qe_score": 6.1166090965271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时间接引用更合适,可以进行更自然的对话。", "metrics": {"bleu_score": 46.5075508035362, "chrf_score": 41.79627991547496, "xcomet_score": 0.8179642558097839, "xcomet_qe_score": 0.8051299452781677, "metricx_score": 8.182682991027832, "metricx_qe_score": 7.952610969543457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种情况可能发生在用户记不", "metrics": {"bleu_score": 11.854704500658576, "chrf_score": 14.223745961678489, "xcomet_score": 0.6749775409698486, "xcomet_qe_score": 0.7389550805091858, "metricx_score": 8.038599967956543, "metricx_qe_score": 5.243821144104004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "起歌名时,或者发音过于相似,难以区分,", "metrics": {"bleu_score": 19.19883131198062, "chrf_score": 19.126652329226644, "xcomet_score": 0.6897692084312439, "xcomet_qe_score": 0.24823835492134094, "metricx_score": 1.5026204586029053, "metricx_qe_score": 2.4768569469451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "或者当用户想要指定偏好时。", "metrics": {"bleu_score": 23.093053192812558, "chrf_score": 23.39542938424004, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6289462447166443, "metricx_qe_score": 0.45695963501930237, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是间接引用的几个例子。例如,较新的一个或不是充满活力的歌曲。", "metrics": {"bleu_score": 9.866517748914662, "chrf_score": 12.758260897071711, "xcomet_score": 0.7664604187011719, "xcomet_qe_score": 0.7639158964157104, "metricx_score": 3.619807720184326, "metricx_qe_score": 2.9225025177001953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是用于基准测试LLM实体理解的一个重要问题。", "metrics": {"bleu_score": 71.10231907654392, "chrf_score": 64.40600957753573, "xcomet_score": 0.8531266450881958, "xcomet_qe_score": 0.7748444080352783, "metricx_score": 1.7799198627471924, "metricx_qe_score": 3.5926575660705566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有发现一个公共数据集,一个针对该任务的大规模公共数据集,所以我们使用众包标注来收集一个。", "metrics": {"bleu_score": 47.58320340375704, "chrf_score": 47.23310341983554, "xcomet_score": 0.6248937845230103, "xcomet_qe_score": 0.6629226207733154, "metricx_score": 3.8360848426818848, "metricx_qe_score": 3.5368812084198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域,音乐、书籍和食谱。", "metrics": {"bleu_score": 78.47574847738748, "chrf_score": 71.38793914595793, "xcomet_score": 0.9902492761611938, "xcomet_qe_score": 0.985019862651825, "metricx_score": 0.24759098887443542, "metricx_qe_score": 0.3514697849750519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集合方法强调非正式性,使用卡通完成设置。", "metrics": {"bleu_score": 58.626141862227996, "chrf_score": 48.62099488238788, "xcomet_score": 0.8200230598449707, "xcomet_qe_score": 0.7987887859344482, "metricx_score": 3.7270026206970215, "metricx_qe_score": 4.681996822357178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通有三个对话气泡。", "metrics": {"bleu_score": 37.20090803840517, "chrf_score": 28.46968233711693, "xcomet_score": 0.7951282262802124, "xcomet_qe_score": 0.7283726930618286, "metricx_score": 1.2068654298782349, "metricx_qe_score": 1.0544060468673706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡中,鲍勃说,还记得我们昨天听的那首歌吗?然后", "metrics": {"bleu_score": 60.60655437708259, "chrf_score": 54.84126836663069, "xcomet_score": 0.8537525534629822, "xcomet_qe_score": 0.7270839214324951, "metricx_score": 5.0527191162109375, "metricx_qe_score": 1.5214470624923706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鲍勃设置了对话上下文。", "metrics": {"bleu_score": 40.855580969845114, "chrf_score": 28.742690621045575, "xcomet_score": 0.954749584197998, "xcomet_qe_score": 0.931614339351654, "metricx_score": 1.0160953998565674, "metricx_qe_score": 1.114621877670288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话气泡中,爱丽丝说,你是指easy on me还是I got a feeling?这是", "metrics": {"bleu_score": 14.540407253078758, "chrf_score": 24.341515063017447, "xcomet_score": 0.6918127536773682, "xcomet_qe_score": 0.6950792074203491, "metricx_score": 8.482852935791016, "metricx_qe_score": 6.535599708557129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8689042329788208, "xcomet_qe_score": 0.8568041324615479, "metricx_score": 2.304609537124634, "metricx_qe_score": 2.8260903358459473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话气泡中,鲍勃使用间接引用来选择这些实体中的一个,例如,Neo-Ervandal。", "metrics": {"bleu_score": 22.464769916810862, "chrf_score": 19.284519484897665, "xcomet_score": 0.635424017906189, "xcomet_qe_score": 0.5969209671020508, "metricx_score": 7.67423152923584, "metricx_qe_score": 7.802557468414307, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话气泡,但第三个由标注者填写。", "metrics": {"bleu_score": 56.11658422412668, "chrf_score": 49.75721796436182, "xcomet_score": 0.8379323482513428, "xcomet_qe_score": 0.8236285448074341, "metricx_score": 1.6795135736465454, "metricx_qe_score": 1.4413166046142578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话气泡是从每个领域的一些手动提示中选出的。", "metrics": {"bleu_score": 45.78226095312775, "chrf_score": 37.2415656885809, "xcomet_score": 0.8831501007080078, "xcomet_qe_score": 0.771369993686676, "metricx_score": 2.525980234146118, "metricx_qe_score": 2.116523265838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个对话气泡是替代问题,生成如下。我们", "metrics": {"bleu_score": 10.830630507021791, "chrf_score": 12.660285425577564, "xcomet_score": 0.6844919323921204, "xcomet_qe_score": 0.6676292419433594, "metricx_score": 6.799300670623779, "metricx_qe_score": 2.259505033493042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总是使用一个简单的模板。", "metrics": {"bleu_score": 63.40466277046863, "chrf_score": 49.96512746512746, "xcomet_score": 0.9124624729156494, "xcomet_qe_score": 0.9353182911872864, "metricx_score": 0.22229070961475372, "metricx_qe_score": 0.24803707003593445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是从维基百科中抽样的。", "metrics": {"bleu_score": 33.15796151992084, "chrf_score": 28.49509663893875, "xcomet_score": 0.974881649017334, "xcomet_qe_score": 0.9926190376281738, "metricx_score": 1.7050693035125732, "metricx_qe_score": 1.1891224384307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用过的不同抽样方法。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 76.47740633807875, "xcomet_score": 0.9971922636032104, "xcomet_qe_score": 0.9996583461761475, "metricx_score": 0.10465388000011444, "metricx_qe_score": 0.10972020775079727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体变得越来越相似,通常更难进行消歧。", "metrics": {"bleu_score": 48.287171837052036, "chrf_score": 42.61563713257725, "xcomet_score": 0.8577663898468018, "xcomet_qe_score": 0.7904423475265503, "metricx_score": 4.571025848388672, "metricx_qe_score": 5.461899757385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是随机均匀的", "metrics": {"bleu_score": 23.89997677137506, "chrf_score": 20.2553956173248, "xcomet_score": 0.8983368873596191, "xcomet_qe_score": 0.8342759609222412, "metricx_score": 1.5540307760238647, "metricx_qe_score": 1.5315414667129517, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",第二个是当实体有相似的标题时,例如两本书的名字相同,它们会返回", "metrics": {"bleu_score": 9.112163637022636, "chrf_score": 12.243855975679493, "xcomet_score": 0.7319635152816772, "xcomet_qe_score": 0.5816535949707031, "metricx_score": 5.8602375984191895, "metricx_qe_score": 5.51937198638916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三个是当它们在维基百科上有相似的描述,", "metrics": {"bleu_score": 68.579883369493, "chrf_score": 65.75883095043264, "xcomet_score": 0.9824837446212769, "xcomet_qe_score": 0.9570989608764648, "metricx_score": 0.8117580413818359, "metricx_qe_score": 0.8278462886810303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后当它们在维基百科上有相似的信息框或属性时", "metrics": {"bleu_score": 72.16888546941138, "chrf_score": 65.29184725392044, "xcomet_score": 0.8958754539489746, "xcomet_qe_score": 0.9621597528457642, "metricx_score": 1.1960866451263428, "metricx_qe_score": 1.4854182004928589, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如同一类型或同一艺术家,对于一首歌曲,", "metrics": {"bleu_score": 14.64087843918566, "chrf_score": 20.24407319106413, "xcomet_score": 0.8762527704238892, "xcomet_qe_score": 0.8753036856651306, "metricx_score": 3.8170318603515625, "metricx_qe_score": 4.3061933517456055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向标注者展示这个替代问题时。他们知道这些实体的名字,但他们不一定会了解实体。", "metrics": {"bleu_score": 52.111211966062996, "chrf_score": 43.19788238443484, "xcomet_score": 0.7200141549110413, "xcomet_qe_score": 0.7123602628707886, "metricx_score": 3.596503496170044, "metricx_qe_score": 4.125827312469482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们所做的是展示这两个实体的一些背景知识。", "metrics": {"bleu_score": 72.00242075875518, "chrf_score": 64.66198984505621, "xcomet_score": 0.9528136849403381, "xcomet_qe_score": 0.7759552597999573, "metricx_score": 0.9565674066543579, "metricx_qe_score": 1.5838260650634766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地展示每个歌曲的Google搜索链接,然后要求标注者至少听一些每首歌曲,并阅读每首歌曲的介绍。", "metrics": {"bleu_score": 33.956055474576225, "chrf_score": 26.77031880290613, "xcomet_score": 0.8353418111801147, "xcomet_qe_score": 0.8073664903640747, "metricx_score": 3.4567465782165527, "metricx_qe_score": 2.8448798656463623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,以下是easy on me这首歌的Google搜索结果。", "metrics": {"bleu_score": 23.662970829944484, "chrf_score": 25.685287279238366, "xcomet_score": 0.9527207612991333, "xcomet_qe_score": 0.948401927947998, "metricx_score": 1.0309311151504517, "metricx_qe_score": 1.030684471130371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示了一些来自维基百科的背景文本。", "metrics": {"bleu_score": 65.04899717332954, "chrf_score": 55.135474997543966, "xcomet_score": 0.9822230339050293, "xcomet_qe_score": 0.9186307191848755, "metricx_score": 0.8351583480834961, "metricx_qe_score": 1.357472538948059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科再次展示了它们的图片,以便标注者知道它们的样子。", "metrics": {"bleu_score": 34.779168008395374, "chrf_score": 28.30918181820009, "xcomet_score": 0.8212381601333618, "xcomet_qe_score": 0.8622174263000488, "metricx_score": 2.33774995803833, "metricx_qe_score": 2.0599000453948975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求标注者选择其中一个实体,例如这里的第一个,并使用三到五个间接指称表达来描述它们。", "metrics": {"bleu_score": 47.06721126868834, "chrf_score": 41.74244674719467, "xcomet_score": 0.8010503649711609, "xcomet_qe_score": 0.8931186199188232, "metricx_score": 2.224386215209961, "metricx_qe_score": 2.39251708984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,钢琴音乐的那个,这里", "metrics": {"bleu_score": 9.238430210261097, "chrf_score": 10.51516139550049, "xcomet_score": 0.8473206758499146, "xcomet_qe_score": 0.764856219291687, "metricx_score": 1.737596869468689, "metricx_qe_score": 1.143145203590393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是一些来自我们数据集的例子", "metrics": {"bleu_score": 33.41796039044061, "chrf_score": 28.325283429185166, "xcomet_score": 0.8755073547363281, "xcomet_qe_score": 0.7795535922050476, "metricx_score": 1.0722507238388062, "metricx_qe_score": 1.2165628671646118, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如没有歌词的,不是有12岁男孩的那个,也不是虚构的,或者来自阿塞拜疆等等。", "metrics": {"bleu_score": 33.09950561003179, "chrf_score": 30.403582399709748, "xcomet_score": 0.6506850719451904, "xcomet_qe_score": 0.7562479972839355, "metricx_score": 3.0170979499816895, "metricx_qe_score": 4.111613750457764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "altentity corpus在三个领域中有6,000个替代问题,有42,000个间接指称表达。", "metrics": {"bleu_score": 54.82349649115482, "chrf_score": 53.88460486591007, "xcomet_score": 0.663101077079773, "xcomet_qe_score": 0.6647361516952515, "metricx_score": 5.5830817222595215, "metricx_qe_score": 6.064608097076416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是使用T5xLarge模型的结果总结。", "metrics": {"bleu_score": 33.36118221483977, "chrf_score": 30.906902188243947, "xcomet_score": 0.9183732867240906, "xcomet_qe_score": 0.8933172225952148, "metricx_score": 1.9909281730651855, "metricx_qe_score": 2.121993064880371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与标注者完全相同的背景知识,那么准确率非常高。大约在92-95%。", "metrics": {"bleu_score": 57.87809937691414, "chrf_score": 51.69707916909141, "xcomet_score": 0.8458625674247742, "xcomet_qe_score": 0.8483800292015076, "metricx_score": 1.1763015985488892, "metricx_qe_score": 1.0400316715240479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确率在82%到87%之间,这更现实,", "metrics": {"bleu_score": 76.76100071606257, "chrf_score": 74.92904845166187, "xcomet_score": 0.8881449103355408, "xcomet_qe_score": 0.8379524350166321, "metricx_score": 1.7057561874389648, "metricx_qe_score": 1.812639594078064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率只有60%。所以还有很大的改进空间。", "metrics": {"bleu_score": 76.9218097681464, "chrf_score": 72.93605951078253, "xcomet_score": 0.9953700304031372, "xcomet_qe_score": 0.9922376871109009, "metricx_score": 1.4691671133041382, "metricx_qe_score": 2.2902300357818604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还表明模型具有领域泛化能力。", "metrics": {"bleu_score": 35.58672550051981, "chrf_score": 29.985877413050368, "xcomet_score": 0.8617696762084961, "xcomet_qe_score": 0.8480961322784424, "metricx_score": 0.7922310829162598, "metricx_qe_score": 0.9427548050880432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集的链接。", "metrics": {"bleu_score": 57.83569866465144, "chrf_score": 57.994768727899405, "xcomet_score": 0.9930626153945923, "xcomet_qe_score": 0.9849375486373901, "metricx_score": 0.26991111040115356, "metricx_qe_score": 0.36960017681121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是多伦多大学和布鲁诺·凯斯勒基金会的Sarah Pappy,我将简要介绍一篇关于将注意力作为同步语音翻译的指导的论文,这是与Matteo Negri和Marco Turki的合作成果。", "metrics": {"bleu_score": 45.03741820090278, "chrf_score": 52.18980788749784, "xcomet_score": 0.6669813990592957, "xcomet_qe_score": 0.6251503825187683, "metricx_score": 4.288147926330566, "metricx_qe_score": 3.548766851425171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同步语音翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.9915040731430054, "xcomet_qe_score": 0.9834308624267578, "metricx_score": 0.2992181181907654, "metricx_qe_score": 0.12179544568061829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同步语音翻译或模拟翻译是指实时将口语翻译成另一种语言的文本的过程,实现跨语言交流。", "metrics": {"bleu_score": 54.647181234008876, "chrf_score": 44.59837570382111, "xcomet_score": 0.8728508949279785, "xcomet_qe_score": 0.8708873987197876, "metricx_score": 1.8099539279937744, "metricx_qe_score": 1.9683622121810913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,当前模拟模型存在哪些问题呢?", "metrics": {"bleu_score": 19.345299022826193, "chrf_score": 15.68681528760261, "xcomet_score": 0.9951450824737549, "xcomet_qe_score": 0.990328311920166, "metricx_score": 1.003423810005188, "metricx_qe_score": 1.4196078777313232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,特定的架构需要引入额外的模块进行优化,", "metrics": {"bleu_score": 47.91643590819621, "chrf_score": 45.134599301144505, "xcomet_score": 0.9733799695968628, "xcomet_qe_score": 0.9646623134613037, "metricx_score": 1.3673245906829834, "metricx_qe_score": 1.67300546169281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练过程冗长且复杂,例如,涉及不同的优化目标的训练", "metrics": {"bleu_score": 47.2402846675108, "chrf_score": 39.516726901042716, "xcomet_score": 0.9344738721847534, "xcomet_qe_score": 0.9347522258758545, "metricx_score": 0.5646684169769287, "metricx_qe_score": 0.7097535133361816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",以及训练和维护多个模型以达到不同的延迟", "metrics": {"bleu_score": 48.8627596980411, "chrf_score": 46.78251421975124, "xcomet_score": 0.9146232604980469, "xcomet_qe_score": 0.9254367351531982, "metricx_score": 2.459968328475952, "metricx_qe_score": 2.0669734477996826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "机制,例如,训练一个平均延迟为一秒的模型和另一个平均延迟为两秒的模型等等。", "metrics": {"bleu_score": 59.28257592469543, "chrf_score": 56.04760496795795, "xcomet_score": 0.6561150550842285, "xcomet_qe_score": 0.6009703278541565, "metricx_score": 2.1180238723754883, "metricx_qe_score": 1.9914129972457886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么呢?", "metrics": {"bleu_score": 74.87402156832427, "chrf_score": 75.65635752018586, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07452559471130371, "metricx_qe_score": 0.18974152207374573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用现有的离线ST模型,无需重新训练或采用特定的架构进行CMLSD。", "metrics": {"bleu_score": 45.84681980981247, "chrf_score": 36.490890176092286, "xcomet_score": 0.83001708984375, "xcomet_qe_score": 0.8361629843711853, "metricx_score": 3.9808402061462402, "metricx_qe_score": 5.041432857513428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每个延迟机制,只使用一个mod模型,并通过特定的参数处理延迟,利用", "metrics": {"bleu_score": 34.42968544278039, "chrf_score": 29.50757969420416, "xcomet_score": 0.7572677135467529, "xcomet_qe_score": 0.7642413377761841, "metricx_score": 5.657008171081543, "metricx_qe_score": 2.678448438644409, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入和文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 59.62415297042496, "chrf_score": 58.15067645682329, "xcomet_score": 0.7593388557434082, "xcomet_qe_score": 0.7074575424194336, "metricx_score": 6.219468116760254, "metricx_qe_score": 4.784849166870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")获得的知识,您可以在右侧看到一个例子。", "metrics": {"bleu_score": 4.814971807094068, "chrf_score": 5.256029653483614, "xcomet_score": 0.3474692404270172, "xcomet_qe_score": 0.5531555414199829, "metricx_score": 7.952996253967285, "metricx_qe_score": 11.167703628540039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出一个点或编码器解码器注意力机制,这是一个策略,根据注意力指向的位置决定是否发出部分翻译。", "metrics": {"bleu_score": 51.84466477100929, "chrf_score": 41.71821284399241, "xcomet_score": 0.6008037328720093, "xcomet_qe_score": 0.5188440680503845, "metricx_score": 6.717022895812988, "metricx_qe_score": 7.123231410980225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力不集中,即该和低于某个阈值alpha,指向最后lambda个语音帧,这意味着接收到的信息足够稳定", "metrics": {"bleu_score": 41.76986334208297, "chrf_score": 35.373886346991625, "xcomet_score": 0.6249939203262329, "xcomet_qe_score": 0.6610704660415649, "metricx_score": 7.087885856628418, "metricx_qe_score": 7.054646968841553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",则发出一个词。例如,如果我们接收了一个包含“我要谈论”的语音片段,我们的模型预测德语翻译,我们将查看交叉注意力权重,我们会看到前两个词指向最早接收到的语音帧,而最后一个词指向最后一个接收到的语音帧,即lambda语音帧。", "metrics": {"bleu_score": 48.879802047062796, "chrf_score": 36.147816428672684, "xcomet_score": 0.3458147943019867, "xcomet_qe_score": 0.2154647260904312, "metricx_score": 8.002531051635742, "metricx_qe_score": 8.143820762634277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被发出,而由于交叉注意力的和高于某个阈值alpha,我们将不发出最后一个词,而是等待另一个语音片段。", "metrics": {"bleu_score": 50.110502351526286, "chrf_score": 41.24328433530279, "xcomet_score": 0.628959596157074, "xcomet_qe_score": 0.6878467798233032, "metricx_score": 3.211883783340454, "metricx_qe_score": 3.5966203212738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续接收另一个语音片段,我们的模型预测另外三个词,我们将查看交叉注意力权重,我们会看到没有词指向最后一个lambda语音帧。", "metrics": {"bleu_score": 49.6535146678309, "chrf_score": 38.930084750051954, "xcomet_score": 0.7620822787284851, "xcomet_qe_score": 0.6055811047554016, "metricx_score": 4.17307710647583, "metricx_qe_score": 3.867255210876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将", "metrics": {"bleu_score": 29.472584782019705, "chrf_score": 26.455018219457934, "xcomet_score": 0.841911792755127, "xcomet_qe_score": 0.8360384106636047, "metricx_score": 5.485178470611572, "metricx_qe_score": 5.510756969451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "被发出。如果我们查看主要结果,我们将以图表形式绘制同步语音翻译结果,其中一边用蓝色表示翻译质量和平均延迟,即延迟度量。我们还考虑了计算感知平均延迟,该延迟考虑了模型预测输出的计算时间。", "metrics": {"bleu_score": 30.7973993541207, "chrf_score": 24.137533947447437, "xcomet_score": 0.4514153003692627, "xcomet_qe_score": 0.29910415410995483, "metricx_score": 8.191008567810059, "metricx_qe_score": 9.076539039611816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们希望我们的曲线在这个图上尽可能高,", "metrics": {"bleu_score": 29.10624919304027, "chrf_score": 26.712443633170473, "xcomet_score": 0.9588454961776733, "xcomet_qe_score": 0.8161818981170654, "metricx_score": 1.7375974655151367, "metricx_qe_score": 1.8240134716033936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但我们也希望它们向左移动,", "metrics": {"bleu_score": 77.99950505759813, "chrf_score": 75.84204281569009, "xcomet_score": 0.9933568239212036, "xcomet_qe_score": 0.9786125421524048, "metricx_score": 0.7286149263381958, "metricx_qe_score": 1.0299155712127686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将与预处理策略进行比较,这些策略也适用于离线模型,即权重键策略和局部一致性,", "metrics": {"bleu_score": 34.82371884756185, "chrf_score": 25.417592227964214, "xcomet_score": 0.6810556650161743, "xcomet_qe_score": 0.6432039737701416, "metricx_score": 5.417716026306152, "metricx_qe_score": 5.655540466308594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与专门为同步语音翻译定制的最先进架构进行了比较,", "metrics": {"bleu_score": 68.66477232717125, "chrf_score": 66.74266570569254, "xcomet_score": 0.984123945236206, "xcomet_qe_score": 0.952898383140564, "metricx_score": 1.1528798341751099, "metricx_qe_score": 1.627755880355835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是同步语音翻译策略在德语上的所有结果,", "metrics": {"bleu_score": 18.153580712996767, "chrf_score": 20.407190570281966, "xcomet_score": 0.8841100335121155, "xcomet_qe_score": 0.8779603838920593, "metricx_score": 2.1833953857421875, "metricx_qe_score": 1.2569353580474854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,一个点优于所有应用于离线模型的策略,因为它们的曲线向左移动,", "metrics": {"bleu_score": 45.97842124564125, "chrf_score": 43.853579241285765, "xcomet_score": 0.7658824920654297, "xcomet_qe_score": 0.6265256404876709, "metricx_score": 4.796031951904297, "metricx_qe_score": 5.202297210693359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际的经过时间或计算磨损时间,即最快的策略,", "metrics": {"bleu_score": 36.742861269890305, "chrf_score": 31.298073443655955, "xcomet_score": 0.7123926877975464, "xcomet_qe_score": 0.6443440914154053, "metricx_score": 6.299413681030273, "metricx_qe_score": 6.2118239402771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想发现更多结果,请阅读我们的论文,", "metrics": {"bleu_score": 59.485907024027185, "chrf_score": 48.35884646256164, "xcomet_score": 0.9574462175369263, "xcomet_qe_score": 0.9469451308250427, "metricx_score": 0.6843385696411133, "metricx_qe_score": 0.48276737332344055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码和模型以及同步输出,以促进我们工作的可", "metrics": {"bleu_score": 45.06984287461696, "chrf_score": 39.175565707174904, "xcomet_score": 0.6727319955825806, "xcomet_qe_score": 0.6512917280197144, "metricx_score": 5.1086249351501465, "metricx_qe_score": 2.702866792678833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "重复性,感谢您的关注。", "metrics": {"bleu_score": 4.456882760699063, "chrf_score": 6.451612903225806, "xcomet_score": 0.6094427108764648, "xcomet_qe_score": 0.7354168891906738, "metricx_score": 4.239649295806885, "metricx_qe_score": 4.823554515838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫 Ying,我的同事 Jian 和我将介绍我们关于通过指令微调改进多模态零样本学习的研究。", "metrics": {"bleu_score": 62.32189079577917, "chrf_score": 42.66017487966427, "xcomet_score": 0.8076789379119873, "xcomet_qe_score": 0.7700879573822021, "metricx_score": 4.5920515060424805, "metricx_qe_score": 5.689416408538818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多工作开始探索以参数和数据高效的方式为不同的下游任务重用预训练语言模型的新学习范式。", "metrics": {"bleu_score": 53.889418669966105, "chrf_score": 45.68218769468069, "xcomet_score": 0.8962767124176025, "xcomet_qe_score": 0.8774938583374023, "metricx_score": 1.9798471927642822, "metricx_qe_score": 2.5197417736053467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令微调使大型语言模型能够通过遵循自然指令以零样本方式执行未见过的任务。", "metrics": {"bleu_score": 59.31666716363961, "chrf_score": 51.13907211394145, "xcomet_score": 0.8883419036865234, "xcomet_qe_score": 0.7674729824066162, "metricx_score": 1.4315078258514404, "metricx_qe_score": 2.916572093963623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数关于指令微调的先前工作都集中在提高仅语言任务的零样本性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 39.61864465552384, "chrf_score": 35.23599452670341, "xcomet_score": 0.9705630540847778, "xcomet_qe_score": 0.8278629779815674, "metricx_score": 1.300919771194458, "metricx_qe_score": 1.907055377960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想要研究在多模态预训练模型上进行指令微调是否真的可以提高对未见的多模态任务的泛化能力。", "metrics": {"bleu_score": 42.17760875584086, "chrf_score": 37.19647051723296, "xcomet_score": 0.8713177442550659, "xcomet_qe_score": 0.7935223579406738, "metricx_score": 1.6603507995605469, "metricx_qe_score": 1.7584799528121948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们进行研究时,我们发现 LP 和多模态之间的指令数据集可用性存在显著差异。", "metrics": {"bleu_score": 44.17063701543531, "chrf_score": 36.79809635003632, "xcomet_score": 0.7931694984436035, "xcomet_qe_score": 0.7825369834899902, "metricx_score": 2.9790492057800293, "metricx_qe_score": 2.8324029445648193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过一千六百个仅语言指令任务。", "metrics": {"bleu_score": 38.989389329183524, "chrf_score": 29.526682467858933, "xcomet_score": 0.930850625038147, "xcomet_qe_score": 0.8534450531005859, "metricx_score": 1.2972968816757202, "metricx_qe_score": 2.238736152648926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大规模公开的多模态指令任务。", "metrics": {"bleu_score": 63.09433236258316, "chrf_score": 53.068690926663066, "xcomet_score": 0.9746540784835815, "xcomet_qe_score": 0.826064944267273, "metricx_score": 1.468718409538269, "metricx_qe_score": 2.521465539932251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这激励我们构建一个多模态指令微调数据集。", "metrics": {"bleu_score": 72.88605134576494, "chrf_score": 66.31255345214154, "xcomet_score": 0.9734252691268921, "xcomet_qe_score": 0.9711446762084961, "metricx_score": 0.8493427634239197, "metricx_qe_score": 1.117382526397705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们提出了 multi instruct,第一个多模态指令微调基准数据集,包含六十两个多样化的多模态任务,涵盖十个大类。", "metrics": {"bleu_score": 35.379868269359655, "chrf_score": 33.23693220680057, "xcomet_score": 0.7975566387176514, "xcomet_qe_score": 0.8594542145729065, "metricx_score": 3.8442559242248535, "metricx_qe_score": 4.371237277984619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来自二十一个现有的开源数据集,每个任务都配备了五个专家撰写的指令。", "metrics": {"bleu_score": 51.59059540495632, "chrf_score": 44.383862862594825, "xcomet_score": 0.9588490724563599, "xcomet_qe_score": 0.8076069355010986, "metricx_score": 1.4683687686920166, "metricx_qe_score": 2.4125537872314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令微调,我们以 OFA 统一的多模态模式模型作为我们的基模型。OFA 使用统一的词", "metrics": {"bleu_score": 52.850604066253, "chrf_score": 52.32769980041672, "xcomet_score": 0.526299774646759, "xcomet_qe_score": 0.4131567180156708, "metricx_score": 7.433974266052246, "metricx_qe_score": 4.680543899536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来表示语言、图像标记和边界框的坐标。", "metrics": {"bleu_score": 40.142754193450884, "chrf_score": 31.737148627860805, "xcomet_score": 0.3397049307823181, "xcomet_qe_score": 0.3453698754310608, "metricx_score": 6.806940078735352, "metricx_qe_score": 6.82180118560791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们展示了我们 multi install 数据集的一些示例实例。为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 66.25514644108668, "chrf_score": 52.6376122189192, "xcomet_score": 0.8291205167770386, "xcomet_qe_score": 0.7795385122299194, "metricx_score": 5.354324817657471, "metricx_qe_score": 6.79779052734375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循 OFA 的方法,并将所有任务统一为序列到序列格式,其中", "metrics": {"bleu_score": 56.08477095443868, "chrf_score": 51.02474983509465, "xcomet_score": 0.7294528484344482, "xcomet_qe_score": 0.7381992340087891, "metricx_score": 3.2883195877075195, "metricx_qe_score": 2.1006548404693604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框以相同的标记空间表示。", "metrics": {"bleu_score": 53.09565039223721, "chrf_score": 50.52264517853785, "xcomet_score": 0.9847351312637329, "xcomet_qe_score": 0.9566234350204468, "metricx_score": 1.0149985551834106, "metricx_qe_score": 1.017142415046692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我要谈谈多模态指令微调。", "metrics": {"bleu_score": 67.53160327422972, "chrf_score": 62.99571751777634, "xcomet_score": 0.9133613109588623, "xcomet_qe_score": 0.883480429649353, "metricx_score": 0.721462607383728, "metricx_qe_score": 0.7804521322250366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用 NIG 组的 53 个任务进行训练,每个任务抽样 10,000 个实例。", "metrics": {"bleu_score": 70.20359474399064, "chrf_score": 67.65643096106852, "xcomet_score": 0.7805843353271484, "xcomet_qe_score": 0.7411601543426514, "metricx_score": 6.407921314239502, "metricx_qe_score": 7.7362260818481445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识推理组进行测试,并从 WQA 和杂项组中选择另外五个任务。", "metrics": {"bleu_score": 47.240055500550845, "chrf_score": 39.1331092409929, "xcomet_score": 0.5848708748817444, "xcomet_qe_score": 0.5877550840377808, "metricx_score": 4.881385803222656, "metricx_qe_score": 4.931185722351074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务测试集中的所有实例。", "metrics": {"bleu_score": 46.230595512422084, "chrf_score": 37.75727227912667, "xcomet_score": 0.8219863176345825, "xcomet_qe_score": 0.8099260330200195, "metricx_score": 1.5839729309082031, "metricx_qe_score": 1.6549862623214722, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从 NIG 指令测试集随机抽取 20 个任务作为 NLP 的 SIN 任务。因此,", "metrics": {"bleu_score": 37.29538641906858, "chrf_score": 37.11197215067184, "xcomet_score": 0.6126225590705872, "xcomet_qe_score": 0.5618832111358643, "metricx_score": 8.994200706481934, "metricx_qe_score": 7.988852024078369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的 OFA 大模型作为基模型。", "metrics": {"bleu_score": 71.03277321267436, "chrf_score": 66.49931553545505, "xcomet_score": 0.8836812973022461, "xcomet_qe_score": 0.8550267219543457, "metricx_score": 1.5454760789871216, "metricx_qe_score": 2.2901506423950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 53.884478712336644, "xcomet_score": 0.969638466835022, "xcomet_qe_score": 0.8911672830581665, "metricx_score": 0.8351970911026001, "metricx_qe_score": 1.3651412725448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与其五个指令模板中的一个结合。", "metrics": {"bleu_score": 74.17090125042293, "chrf_score": 69.06413590609232, "xcomet_score": 0.8967591524124146, "xcomet_qe_score": 0.848268985748291, "metricx_score": 1.8565388917922974, "metricx_qe_score": 2.2801826000213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在每个任务的测试中,我们进行总共五个实验,每个实验使用五个指令中的一个评估模型。", "metrics": {"bleu_score": 48.049168797313996, "chrf_score": 42.445330767819414, "xcomet_score": 0.9561372399330139, "xcomet_qe_score": 0.8686203956604004, "metricx_score": 1.8483572006225586, "metricx_qe_score": 2.066697359085083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告所有五个实验的平均和最大性能以及性能的标准差。", "metrics": {"bleu_score": 29.099618745627254, "chrf_score": 25.443922976172217, "xcomet_score": 0.8422595858573914, "xcomet_qe_score": 0.9305675029754639, "metricx_score": 1.8562649488449097, "metricx_qe_score": 2.144145965576172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模型分类任务,我们报告准确率。", "metrics": {"bleu_score": 66.60502379419339, "chrf_score": 59.946768410414364, "xcomet_score": 0.9906585216522217, "xcomet_qe_score": 0.9783502221107483, "metricx_score": 0.5188239812850952, "metricx_qe_score": 0.6833564639091492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模型生成任务,我们报告 ROOGEL。对于 RP 任务,我们也报告 ROOGEL。", "metrics": {"bleu_score": 44.98220729506393, "chrf_score": 31.662131379877934, "xcomet_score": 0.7234185338020325, "xcomet_qe_score": 0.6819565296173096, "metricx_score": 6.4197869300842285, "metricx_qe_score": 6.2224650382995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为敏感性。因此,它", "metrics": {"bleu_score": 59.330670450907064, "chrf_score": 60.25879085514624, "xcomet_score": 0.6481771469116211, "xcomet_qe_score": 0.6395895481109619, "metricx_score": 5.248791217803955, "metricx_qe_score": 3.9234323501586914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "衡量模型在指令措辞略有变化的情况下,是否能够始终如一地为同一任务生成相同输出的能力。", "metrics": {"bleu_score": 30.265582825831125, "chrf_score": 26.742371679668008, "xcomet_score": 0.9740028381347656, "xcomet_qe_score": 0.9834126234054565, "metricx_score": 1.8776382207870483, "metricx_qe_score": 2.7581114768981934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,指令微调可以显著提高 OFA 在可见多模型任务上的性能。", "metrics": {"bleu_score": 42.216930920161225, "chrf_score": 42.925857175198416, "xcomet_score": 0.930982768535614, "xcomet_qe_score": 0.9017303586006165, "metricx_score": 2.473632335662842, "metricx_qe_score": 2.7934646606445312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行的迁移学习可以使指令微调受益。", "metrics": {"bleu_score": 54.75180966862229, "chrf_score": 49.34650152565327, "xcomet_score": 0.8984434604644775, "xcomet_qe_score": 0.8326890468597412, "metricx_score": 3.3736073970794678, "metricx_qe_score": 4.318597316741943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们可以看到随着任务数量的增加,模型的性能得到提高,同时敏感性降低。因此,", "metrics": {"bleu_score": 40.90421508072144, "chrf_score": 36.96408896255671, "xcomet_score": 0.7499213814735413, "xcomet_qe_score": 0.7551706433296204, "metricx_score": 4.570688247680664, "metricx_qe_score": 2.187748432159424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一个实验。", "metrics": {"bleu_score": 58.77283725105324, "chrf_score": 53.08096866257821, "xcomet_score": 0.994623064994812, "xcomet_qe_score": 0.9922294616699219, "metricx_score": 0.2523118555545807, "metricx_qe_score": 0.3109481930732727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一个指令与五个指令进行", "metrics": {"bleu_score": 32.37722713145643, "chrf_score": 26.393051393051394, "xcomet_score": 0.7714802026748657, "xcomet_qe_score": 0.7134972810745239, "metricx_score": 2.8930959701538086, "metricx_qe_score": 3.4810144901275635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比较,可以看到使用更多指令可以提高模型的整体性能,并大大降低其敏感性,这", "metrics": {"bleu_score": 39.32618396610254, "chrf_score": 34.42936765277993, "xcomet_score": 0.6266015768051147, "xcomet_qe_score": 0.6789655685424805, "metricx_score": 6.723907470703125, "metricx_qe_score": 1.7625501155853271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表明了不同的微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 61.70688911970019, "chrf_score": 55.21580136801754, "xcomet_score": 0.8796482086181641, "xcomet_qe_score": 0.8706281185150146, "metricx_score": 1.6108461618423462, "metricx_qe_score": 1.9974366426467896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,从自然指令数据集进行的迁移学习可以使模型的敏感性相比原始 OFA 模型有显著提高。", "metrics": {"bleu_score": 33.65969325605765, "chrf_score": 31.444391561283474, "xcomet_score": 0.964875340461731, "xcomet_qe_score": 0.892126739025116, "metricx_score": 1.7910698652267456, "metricx_qe_score": 2.588761329650879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行的迁移学习可以帮助 OFA 在自然指令数据集上取得更好的性能。", "metrics": {"bleu_score": 68.68066788425728, "chrf_score": 65.12356979086653, "xcomet_score": 0.9415321350097656, "xcomet_qe_score": 0.7697571516036987, "metricx_score": 2.948601245880127, "metricx_qe_score": 3.8756704330444336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,总的来说,我们提出了第一个大规模多模态指令微调数据集。我们显著提高了 OFA 的 DAROCHOT 能力,并探索了不同的迁移学习技术并展示了它们的优势。", "metrics": {"bleu_score": 60.208849428933895, "chrf_score": 55.40089736065135, "xcomet_score": 0.7853174209594727, "xcomet_qe_score": 0.7540783882141113, "metricx_score": 4.701361179351807, "metricx_qe_score": 5.139308452606201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标,称为敏感性。还有一件事", "metrics": {"bleu_score": 48.624389134644154, "chrf_score": 50.570155812438045, "xcomet_score": 0.6670271754264832, "xcomet_qe_score": 0.5847718119621277, "metricx_score": 3.5151607990264893, "metricx_qe_score": 2.159074306488037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们正在收集一个更大的多模态指令微调数据集,包含大约 150 个额外的变体语言任务,并将它们发布。所以,这是一个 QR 码,用于", "metrics": {"bleu_score": 43.550234995563905, "chrf_score": 42.67943110697961, "xcomet_score": 0.2686072885990143, "xcomet_qe_score": 0.1678418666124344, "metricx_score": 9.870664596557617, "metricx_qe_score": 6.590733528137207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "访问我们的数据和模型。", "metrics": {"bleu_score": 48.585747576909554, "chrf_score": 47.03467435995616, "xcomet_score": 0.7696060538291931, "xcomet_qe_score": 0.7598680853843689, "metricx_score": 2.7647314071655273, "metricx_qe_score": 3.737197160720825, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是新浪的Coast,很高兴欢迎大家来到我们的讨论,我们将谈论我们2023年ACL论文", "metrics": {"bleu_score": 25.56189082997477, "chrf_score": 26.112674050656764, "xcomet_score": 0.5498126149177551, "xcomet_qe_score": 0.5291590690612793, "metricx_score": 7.151820182800293, "metricx_qe_score": 8.141266822814941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型的可接受性判断并非总是对上下文稳健》的内容。这是一", "metrics": {"bleu_score": 53.46930670550587, "chrf_score": 52.30936984291842, "xcomet_score": 0.600197434425354, "xcomet_qe_score": 0.5682851076126099, "metricx_score": 9.124094009399414, "metricx_qe_score": 6.984708786010742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "项与John Bakhier、Aaron Mueller、Kanishka Mishra、Karen Fentus、Roger Levy和Adina Williams合作完成的联合研究。", "metrics": {"bleu_score": 40.647710644366754, "chrf_score": 74.19711955139022, "xcomet_score": 0.47550615668296814, "xcomet_qe_score": 0.3631957471370697, "metricx_score": 6.667091369628906, "metricx_qe_score": 5.931295871734619, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们重新审视了最小对概念。", "metrics": {"bleu_score": 43.82572041773569, "chrf_score": 44.99510575847716, "xcomet_score": 0.963120698928833, "xcomet_qe_score": 0.969498872756958, "metricx_score": 0.9562066793441772, "metricx_qe_score": 0.9808343648910522, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小对概念基本上是通过可接受性判断来评估语言模型,这些判断", "metrics": {"bleu_score": 38.06714025798045, "chrf_score": 30.93563337452468, "xcomet_score": 0.8019750118255615, "xcomet_qe_score": 0.753833532333374, "metricx_score": 6.2244648933410645, "metricx_qe_score": 2.8062782287597656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还可能包括语法性,如blimp语法宝石或可接受性方面如人群节省等。", "metrics": {"bleu_score": 14.234121842188829, "chrf_score": 8.906231767405135, "xcomet_score": 0.551551342010498, "xcomet_qe_score": 0.4944927394390106, "metricx_score": 8.513663291931152, "metricx_qe_score": 8.649303436279297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个最小对概念中,评估语言模型的典型方法是展示一个可接受的句子或语法句子,然后展示一个不可接受的句子或非语法句子", "metrics": {"bleu_score": 47.129031226143994, "chrf_score": 43.68223090420777, "xcomet_score": 0.8164688348770142, "xcomet_qe_score": 0.7898566722869873, "metricx_score": 1.441108226776123, "metricx_qe_score": 2.8801026344299316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",希望模型基本上会给可接受的句子赋予更高的概率。", "metrics": {"bleu_score": 36.96208614341176, "chrf_score": 30.073425493906043, "xcomet_score": 0.8781664967536926, "xcomet_qe_score": 0.7674963474273682, "metricx_score": 2.9022600650787354, "metricx_qe_score": 3.5724902153015137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP管道基本上不允许我们评估模型对更长句子的接受程度。", "metrics": {"bleu_score": 82.06608015220696, "chrf_score": 80.63558817056767, "xcomet_score": 0.8660776615142822, "xcomet_qe_score": 0.781336784362793, "metricx_score": 1.4139117002487183, "metricx_qe_score": 2.8686068058013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型正在产生越来越长的", "metrics": {"bleu_score": 29.25712720837, "chrf_score": 23.270604349424808, "xcomet_score": 0.7452372312545776, "xcomet_qe_score": 0.7948508262634277, "metricx_score": 6.209050178527832, "metricx_qe_score": 5.683633804321289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文窗口。因此,评估模型在整个上下文窗口内的可接受性至关重要。这就是我们在这里试图做的事情。", "metrics": {"bleu_score": 41.378079477189175, "chrf_score": 36.81990294134389, "xcomet_score": 0.70448899269104, "xcomet_qe_score": 0.6785948276519775, "metricx_score": 2.4445977210998535, "metricx_qe_score": 3.4133644104003906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视NPV管道。所以", "metrics": {"bleu_score": 49.99148090271511, "chrf_score": 42.21437241432934, "xcomet_score": 0.6644706726074219, "xcomet_qe_score": 0.5505679249763489, "metricx_score": 4.781371593475342, "metricx_qe_score": 3.6774940490722656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是方法。", "metrics": {"bleu_score": 30.8198090959812, "chrf_score": 26.13246908061391, "xcomet_score": 0.9100991487503052, "xcomet_qe_score": 0.9150984883308411, "metricx_score": 0.34142422676086426, "metricx_qe_score": 0.7583339810371399, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们所做的是模拟这些更长的序列,重新审视数据集本身,然后从这些数据集中选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 77.80688931797138, "chrf_score": 75.30367313836234, "xcomet_score": 0.8695502281188965, "xcomet_qe_score": 0.73785799741745, "metricx_score": 2.397550344467163, "metricx_qe_score": 3.2903037071228027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从blimp数据集的从属岛案例中选择了典型的一对语法性。", "metrics": {"bleu_score": 22.62175974625094, "chrf_score": 18.57177928784497, "xcomet_score": 0.6696998476982117, "xcomet_qe_score": 0.6681936383247375, "metricx_score": 5.453393459320068, "metricx_qe_score": 5.797694683074951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重新创建更长的序列,这些序列是可接受的,并且具有相同的语法结构匹配,", "metrics": {"bleu_score": 77.73714308042847, "chrf_score": 69.83866991698754, "xcomet_score": 0.950576663017273, "xcomet_qe_score": 0.8663231730461121, "metricx_score": 1.4796338081359863, "metricx_qe_score": 2.029397487640381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从从属岛中提取语法句子,然后将其作为前缀添加到可接受查询和不可接受查询中。所以", "metrics": {"bleu_score": 68.17369180120234, "chrf_score": 49.283745282495914, "xcomet_score": 0.5184330940246582, "xcomet_qe_score": 0.4825257658958435, "metricx_score": 4.615480422973633, "metricx_qe_score": 3.684086322784424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.9529941082000732, "xcomet_qe_score": 0.7428854703903198, "metricx_score": 1.5263457298278809, "metricx_qe_score": 1.8271386623382568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过从不同的子集或不同的数据集选择句子来做同样的事情。", "metrics": {"bleu_score": 83.07278725457043, "chrf_score": 79.33022891685252, "xcomet_score": 0.9753702878952026, "xcomet_qe_score": 0.8853784799575806, "metricx_score": 0.7516820430755615, "metricx_qe_score": 1.2882355451583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所谓的不匹配场景。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 66.57370407370408, "xcomet_score": 0.9924196004867554, "xcomet_qe_score": 0.9211022257804871, "metricx_score": 0.6500070691108704, "metricx_qe_score": 1.3670403957366943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在这里,句子仍然来自相关的数据集,但不是你正在评估的同一数据集。", "metrics": {"bleu_score": 48.62932498455974, "chrf_score": 40.56317671083669, "xcomet_score": 0.9430075287818909, "xcomet_qe_score": 0.8333601951599121, "metricx_score": 1.220080852508545, "metricx_qe_score": 2.13053560256958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对不可接受性案例做同样的事情。", "metrics": {"bleu_score": 28.67324386544373, "chrf_score": 24.172021326688167, "xcomet_score": 0.9094661474227905, "xcomet_qe_score": 0.8861654996871948, "metricx_score": 1.5727465152740479, "metricx_qe_score": 1.2626675367355347, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全不相关的领域选择句子,如维基百科。所以这", "metrics": {"bleu_score": 63.05914424660905, "chrf_score": 56.28682402200159, "xcomet_score": 0.8142136335372925, "xcomet_qe_score": 0.802225649356842, "metricx_score": 4.7997236251831055, "metricx_qe_score": 3.459434986114502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将告诉我们,模型的可接受性判断是否实际上受到任何上下文的影响,如上下文是否来自数据集的不同子集,或者是否与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 67.81728410328024, "chrf_score": 61.16990993819448, "xcomet_score": 0.7012698650360107, "xcomet_qe_score": 0.8203754425048828, "metricx_score": 2.3075149059295654, "metricx_qe_score": 3.081366777420044, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型是如何做到这一点的呢?", "metrics": {"bleu_score": 16.18861356572822, "chrf_score": 19.376630071146884, "xcomet_score": 0.9434443712234497, "xcomet_qe_score": 0.8621017932891846, "metricx_score": 0.4372439980506897, "metricx_qe_score": 0.512352705001831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看与当前查询对完全无关的维基百科句子,我们发现MPP判断对于任意上下文长度大多是稳健的。", "metrics": {"bleu_score": 49.50895894339036, "chrf_score": 44.10662569798537, "xcomet_score": 0.8793663382530212, "xcomet_qe_score": 0.7991145849227905, "metricx_score": 3.93391489982605, "metricx_qe_score": 5.763913154602051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们增加了上下文长度,直到达到1024,以最大化OPT和GPT2模型,我们", "metrics": {"bleu_score": 35.092795376367484, "chrf_score": 57.882426359451976, "xcomet_score": 0.5833739042282104, "xcomet_qe_score": 0.5378896594047546, "metricx_score": 4.864297866821289, "metricx_qe_score": 2.334123134613037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色虚线中看到,MPP判断相对稳定。", "metrics": {"bleu_score": 56.55183553484675, "chrf_score": 54.252660961386155, "xcomet_score": 0.9435992240905762, "xcomet_qe_score": 0.8175609111785889, "metricx_score": 1.9506089687347412, "metricx_qe_score": 3.640092372894287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么当我们选择来自同一数据集的句子时会发生什么?", "metrics": {"bleu_score": 41.15167991342047, "chrf_score": 35.901157469506735, "xcomet_score": 0.9937142133712769, "xcomet_qe_score": 0.9777387380599976, "metricx_score": 0.72798752784729, "metricx_qe_score": 1.2732901573181152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在这里,我们从相同的blimp或语法宝石数据集中的可接受和不可接受领域选择或创建句子,", "metrics": {"bleu_score": 51.186252404311766, "chrf_score": 35.909863184461074, "xcomet_score": 0.6819502115249634, "xcomet_qe_score": 0.6937423944473267, "metricx_score": 5.350587368011475, "metricx_qe_score": 5.261672019958496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现当添加可接受前缀或不可接受前缀时,MPP判断会显著增加或减少。", "metrics": {"bleu_score": 46.933451382505, "chrf_score": 43.29600193115245, "xcomet_score": 0.832938551902771, "xcomet_qe_score": 0.908463716506958, "metricx_score": 3.4201149940490723, "metricx_qe_score": 2.2143309116363525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,当我们匹配结构时,即当我们从基于责备的语法健身房中选择句子时,我们看到模型的MPP判断会根据所选前缀是可接受还是不可接受而大幅增加或大幅减少。现在,这个效果非常大,随", "metrics": {"bleu_score": 48.763283939559216, "chrf_score": 39.93506018254888, "xcomet_score": 0.27022552490234375, "xcomet_qe_score": 0.279691606760025, "metricx_score": 10.006629943847656, "metricx_qe_score": 9.216984748840332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "着上下文长度的增加而增加,这可能会影响到具有大上下文窗口的较新的语言模型。", "metrics": {"bleu_score": 58.802006269112944, "chrf_score": 52.26697687448596, "xcomet_score": 0.7479031085968018, "xcomet_qe_score": 0.6305112838745117, "metricx_score": 4.319213390350342, "metricx_qe_score": 6.085684299468994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么为什么匹配前缀会如此大地影响语言模型的判断呢?所以", "metrics": {"bleu_score": 52.252620561845234, "chrf_score": 45.320923037968676, "xcomet_score": 0.8886930346488953, "xcomet_qe_score": 0.7850220203399658, "metricx_score": 4.728333473205566, "metricx_qe_score": 0.7780312299728394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图像扰动...输入句子,同时保持相关的结构,但在输入中添加噪声。", "metrics": {"bleu_score": 38.37828515723946, "chrf_score": 37.36501674423057, "xcomet_score": 0.6476141214370728, "xcomet_qe_score": 0.6694557070732117, "metricx_score": 4.525562763214111, "metricx_qe_score": 5.562664031982422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "经过几次这样的扰动,我们发现这些噪声实际上并没有使模型在显示MPP判断趋势方面改变其方向。", "metrics": {"bleu_score": 47.41585973642451, "chrf_score": 42.410700894247185, "xcomet_score": 0.8707382678985596, "xcomet_qe_score": 0.862485945224762, "metricx_score": 3.347111940383911, "metricx_qe_score": 3.5849504470825195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型以相似的方式对扰动句", "metrics": {"bleu_score": 16.162239449628064, "chrf_score": 17.61492274873681, "xcomet_score": 0.7791751623153687, "xcomet_qe_score": 0.7905330657958984, "metricx_score": 5.997762680053711, "metricx_qe_score": 5.489698886871338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "子敏感,即当我们在可接受领域中扰动句子时,我们看到所有扰动的相似增加,当我们在可接受领域中扰动句子时,我们以相似的方式看到MPP判断的减少。", "metrics": {"bleu_score": 36.43530598793275, "chrf_score": 33.088788933791804, "xcomet_score": 0.36559540033340454, "xcomet_qe_score": 0.33877843618392944, "metricx_score": 10.543916702270508, "metricx_qe_score": 10.658596992492676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键结论是,语言模型对句子中共享的潜在句法和语义特征敏感。", "metrics": {"bleu_score": 62.31144761876193, "chrf_score": 54.350104376451334, "xcomet_score": 0.916426420211792, "xcomet_qe_score": 0.9328593015670776, "metricx_score": 1.1966193914413452, "metricx_qe_score": 1.4394019842147827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们目前通过短句和单句输入进行的MPP评估可能无法完全捕捉语言模型在整个上下文窗口内的抽象知识。", "metrics": {"bleu_score": 55.04391663959029, "chrf_score": 45.696940537931454, "xcomet_score": 0.9473516941070557, "xcomet_qe_score": 0.8873572945594788, "metricx_score": 1.8150006532669067, "metricx_qe_score": 2.3973536491394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以获取我们实验的更多详细信息。", "metrics": {"bleu_score": 71.9486981916948, "chrf_score": 67.07656051381463, "xcomet_score": 0.99744713306427, "xcomet_qe_score": 0.9992866516113281, "metricx_score": 0.24227701127529144, "metricx_qe_score": 0.2289969027042389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9482142925262451, "xcomet_qe_score": 0.888888955116272, "metricx_score": 0.47969555854797363, "metricx_qe_score": 0.6286952495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的尤素福·张。", "metrics": {"bleu_score": 62.377747360596125, "chrf_score": 49.10795678664683, "xcomet_score": 0.9028502702713013, "xcomet_qe_score": 0.9091230034828186, "metricx_score": 1.360049843788147, "metricx_qe_score": 2.356520175933838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,Exampler,跨语言语义解析在多种自然语言和意义表示中的应用。", "metrics": {"bleu_score": 60.455117933855185, "chrf_score": 48.02963468426846, "xcomet_score": 0.7947759032249451, "xcomet_qe_score": 0.7627952694892883, "metricx_score": 4.277009010314941, "metricx_qe_score": 4.973945140838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义解析是构建用户查询(如SQL和λ演算)语义表示的任务。", "metrics": {"bleu_score": 64.1386525898168, "chrf_score": 43.75020421388814, "xcomet_score": 0.9064435958862305, "xcomet_qe_score": 0.8502678871154785, "metricx_score": 1.0170881748199463, "metricx_qe_score": 1.414311408996582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而跨语言语义解析则是将多种自然语言的查询翻译成多种意义表示的任务。", "metrics": {"bleu_score": 80.29567848001706, "chrf_score": 77.70569484217266, "xcomet_score": 0.8914155960083008, "xcomet_qe_score": 0.8836545944213867, "metricx_score": 1.630030632019043, "metricx_qe_score": 3.637664318084717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将多种自然语言的查询翻译成SQL、λ演算、FunQL等。", "metrics": {"bleu_score": 76.9218097681464, "chrf_score": 71.14443905295364, "xcomet_score": 0.8676720857620239, "xcomet_qe_score": 0.808603048324585, "metricx_score": 1.3448848724365234, "metricx_qe_score": 1.7270663976669312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型分别针对有限任务和应用的数据集提出和评估。", "metrics": {"bleu_score": 63.9740736433283, "chrf_score": 57.46819134821285, "xcomet_score": 0.927814245223999, "xcomet_qe_score": 0.8514490127563477, "metricx_score": 0.5284484624862671, "metricx_qe_score": 0.9402812719345093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,某些自然语言的覆盖范围不足。", "metrics": {"bleu_score": 53.084925448597666, "chrf_score": 50.789455330663, "xcomet_score": 0.718431293964386, "xcomet_qe_score": 0.6408964395523071, "metricx_score": 4.083515644073486, "metricx_qe_score": 4.205417633056641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失,某些表示的覆盖范围", "metrics": {"bleu_score": 5.969675282061573, "chrf_score": 9.113432950485924, "xcomet_score": 0.7143300771713257, "xcomet_qe_score": 0.696678102016449, "metricx_score": 5.0623393058776855, "metricx_qe_score": 4.469618320465088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不足。λ演算缺失。或者它们只在某些较新的模型上进行评估。", "metrics": {"bleu_score": 32.15000448278978, "chrf_score": 28.751501776536404, "xcomet_score": 0.5113193988800049, "xcomet_qe_score": 0.35728421807289124, "metricx_score": 4.9273295402526855, "metricx_qe_score": 5.5926289558410645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个模型进行评估。", "metrics": {"bleu_score": 20.459641298038246, "chrf_score": 20.199196671891674, "xcomet_score": 0.9916483163833618, "xcomet_qe_score": 0.963141679763794, "metricx_score": 0.6526368260383606, "metricx_qe_score": 0.8788323998451233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了Exampler,我们", "metrics": {"bleu_score": 36.462858619364674, "chrf_score": 21.577315217260654, "xcomet_score": 0.5263829231262207, "xcomet_qe_score": 0.4408054053783417, "metricx_score": 5.7802300453186035, "metricx_qe_score": 4.058350563049316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供了一个统一的跨语言语义解析数据集Exampler,用于多种自然语言和意义表示。它包含了九个来自不同", "metrics": {"bleu_score": 57.884615649321056, "chrf_score": 51.303494327449584, "xcomet_score": 0.428561806678772, "xcomet_qe_score": 0.2612590193748474, "metricx_score": 8.09176254272461, "metricx_qe_score": 7.122346878051758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "领域的语料库,五个语义解析任务,八种意义表示,以及22种自然语言和15个语系。", "metrics": {"bleu_score": 37.48237574137277, "chrf_score": 34.82261829411513, "xcomet_score": 0.364023894071579, "xcomet_qe_score": 0.568160355091095, "metricx_score": 6.620188236236572, "metricx_qe_score": 7.00026798248291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准测试,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 74.56495553738948, "chrf_score": 70.42746348716106, "xcomet_score": 0.9834190607070923, "xcomet_qe_score": 0.9601007699966431, "metricx_score": 0.9758420586585999, "metricx_qe_score": 1.3702785968780518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是TranslateTest。", "metrics": {"bleu_score": 11.708995388048026, "chrf_score": 7.874613639206017, "xcomet_score": 0.9256210923194885, "xcomet_qe_score": 0.8759103417396545, "metricx_score": 1.4148733615875244, "metricx_qe_score": 1.119441270828247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google翻译API将源语言翻译成目标语言,然后使用单语模型进行训练和评估。", "metrics": {"bleu_score": 82.66660014007991, "chrf_score": 76.41626540305097, "xcomet_score": 0.9510485529899597, "xcomet_qe_score": 0.8472477197647095, "metricx_score": 0.5243576169013977, "metricx_qe_score": 0.4862639307975769, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们用英语查询训练英语模型,在推理过程中,我们使用API将德语查询翻译成英语,然后使用训练好的模型预测SQL。", "metrics": {"bleu_score": 67.76817687968963, "chrf_score": 61.643862975799145, "xcomet_score": 0.9292150735855103, "xcomet_qe_score": 0.9065759181976318, "metricx_score": 1.0225903987884521, "metricx_qe_score": 1.5625306367874146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语模型。", "metrics": {"bleu_score": 51.56626918239823, "chrf_score": 40.77320827320827, "xcomet_score": 0.8812164068222046, "xcomet_qe_score": 0.84535151720047, "metricx_score": 1.4693257808685303, "metricx_qe_score": 1.2090201377868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个设置中,源语言与目标语言相同。例如,德语到德语或英语到英语。", "metrics": {"bleu_score": 65.92237905245732, "chrf_score": 61.472481865516535, "xcomet_score": 0.923774003982544, "xcomet_qe_score": 0.8467044830322266, "metricx_score": 0.5062569379806519, "metricx_qe_score": 0.6784735918045044, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语融合设置,通过仅使用10%的训练数据训练单语模型。我们还测试了单语", "metrics": {"bleu_score": 34.46296171500157, "chrf_score": 35.57303371836533, "xcomet_score": 0.5468922853469849, "xcomet_qe_score": 0.40593233704566956, "metricx_score": 9.575761795043945, "metricx_qe_score": 5.599145412445068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多语模型,我们为所有语言训练一个多语模型。", "metrics": {"bleu_score": 52.84588834767009, "chrf_score": 49.332532846417834, "xcomet_score": 0.7506449818611145, "xcomet_qe_score": 0.7932573556900024, "metricx_score": 2.212157726287842, "metricx_qe_score": 2.6964573860168457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询一起训练一个多语模型。", "metrics": {"bleu_score": 51.97061474206883, "chrf_score": 44.30235544747878, "xcomet_score": 0.7691566944122314, "xcomet_qe_score": 0.8851701021194458, "metricx_score": 1.6763687133789062, "metricx_qe_score": 3.151397705078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理过程中,我们可以使用这个模型翻译德语查询或中文查询等。", "metrics": {"bleu_score": 82.42245567123457, "chrf_score": 75.15074104990397, "xcomet_score": 0.9756777286529541, "xcomet_qe_score": 0.8953857421875, "metricx_score": 0.7203695178031921, "metricx_qe_score": 1.1883642673492432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和领域样本迁移。", "metrics": {"bleu_score": 66.60207336666174, "chrf_score": 65.9991233192879, "xcomet_score": 0.7444666624069214, "xcomet_qe_score": 0.7418649792671204, "metricx_score": 4.266796588897705, "metricx_qe_score": 5.756277084350586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一个源语言上进行训练,然后迁移到另一种语言。因此,", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 35.759756944180864, "xcomet_score": 0.8009227514266968, "xcomet_qe_score": 0.7206591367721558, "metricx_score": 5.136651515960693, "metricx_qe_score": 5.501239776611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们用英语查询或英语和德语领域样本查询的组合训练一个多语模型,并预测SQL输出。", "metrics": {"bleu_score": 53.9635967035876, "chrf_score": 48.27814775586005, "xcomet_score": 0.7469260096549988, "xcomet_qe_score": 0.7402178049087524, "metricx_score": 2.7496602535247803, "metricx_qe_score": 3.0444302558898926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的成果。因此", "metrics": {"bleu_score": 26.96698152099015, "chrf_score": 26.932617513716504, "xcomet_score": 0.8180636763572693, "xcomet_qe_score": 0.7937408685684204, "metricx_score": 3.2309324741363525, "metricx_qe_score": 1.9735608100891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",关于单语模型的分析,我们在两组模型上进行评估,包括编码器PDR,即多语言预训练编码器加上基于指针的解码器,如XLMR plus PDR和BERT plus PDR。", "metrics": {"bleu_score": 43.48597131690421, "chrf_score": 34.68967663371394, "xcomet_score": 0.7092947959899902, "xcomet_qe_score": 0.6774149537086487, "metricx_score": 5.313746452331543, "metricx_qe_score": 4.685946464538574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器解码器模型,即多语言预训练编码器解码器模型,如MBART和MT5。", "metrics": {"bleu_score": 32.80859434668172, "chrf_score": 21.80107268337221, "xcomet_score": 0.8858217597007751, "xcomet_qe_score": 0.9159464240074158, "metricx_score": 1.1313798427581787, "metricx_qe_score": 1.654531478881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器解码器在所有九个数据集上表现最佳。", "metrics": {"bleu_score": 49.033606604040415, "chrf_score": 27.880453969752732, "xcomet_score": 0.9849370718002319, "xcomet_qe_score": 0.9822423458099365, "metricx_score": 0.7373245358467102, "metricx_qe_score": 0.5876035690307617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多语言设置下对MT5和XLMR plus PDR进行了评估。", "metrics": {"bleu_score": 32.28213880040184, "chrf_score": 34.986443100562965, "xcomet_score": 0.9010955095291138, "xcomet_qe_score": 0.9327426552772522, "metricx_score": 2.221662998199463, "metricx_qe_score": 2.580974578857422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在各种语言的混合中进行训练,可以提高编码器解码器或编码器PDR的性能。", "metrics": {"bleu_score": 21.066551867537065, "chrf_score": 15.396045208123265, "xcomet_score": 0.6853160262107849, "xcomet_qe_score": 0.7244913578033447, "metricx_score": 2.9743845462799072, "metricx_qe_score": 3.413876533508301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都能获得性能提升,除了英语在七个数据集上性能下降,只在三个数据集上有所提升。", "metrics": {"bleu_score": 52.794242055635, "chrf_score": 46.09311659899727, "xcomet_score": 0.8965532779693604, "xcomet_qe_score": 0.9574518203735352, "metricx_score": 2.32576322555542, "metricx_qe_score": 1.5204017162322998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言曲线。", "metrics": {"bleu_score": 16.877772000449074, "chrf_score": 15.985445806482943, "xcomet_score": 0.823722243309021, "xcomet_qe_score": 0.8426663875579834, "metricx_score": 3.7724766731262207, "metricx_qe_score": 3.4831676483154297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,蓝色线是跨语言洋红色迁移,", "metrics": {"bleu_score": 24.62395302527262, "chrf_score": 19.77299253034547, "xcomet_score": 0.7266246676445007, "xcomet_qe_score": 0.6825047135353088, "metricx_score": 7.255856037139893, "metricx_qe_score": 7.436136245727539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙色线是跨语言零样本迁移,", "metrics": {"bleu_score": 75.39221180326287, "chrf_score": 77.94676400607176, "xcomet_score": 0.9476479291915894, "xcomet_qe_score": 0.8390147089958191, "metricx_score": 1.9790170192718506, "metricx_qe_score": 3.080129384994507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿色线是单语设置。我们发现", "metrics": {"bleu_score": 48.63383168079942, "chrf_score": 69.05349133758901, "xcomet_score": 0.8378332853317261, "xcomet_qe_score": 0.8217518329620361, "metricx_score": 2.8535871505737305, "metricx_qe_score": 2.0322587490081787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿色和橙色线,我们发现对于零样本设置,跨语言迁移性能差距显著。通过比较蓝色和橙色线,我们发现对于洋红色设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 37.84688785181887, "chrf_score": 31.2819285053709, "xcomet_score": 0.5956605076789856, "xcomet_qe_score": 0.6070119738578796, "metricx_score": 6.328558921813965, "metricx_qe_score": 7.350096225738525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器优于进度工作或取得了可比拟的结果。", "metrics": {"bleu_score": 12.413032983132748, "chrf_score": 8.617594033191912, "xcomet_score": 0.7693394422531128, "xcomet_qe_score": 0.7719601392745972, "metricx_score": 5.007114410400391, "metricx_qe_score": 4.800539970397949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行训练可以显著提升Fushot在目标自然语言上的性能。我们发现多语言模型如Codice和Bloom仍然不足以应对跨语言语义解析任务。", "metrics": {"bleu_score": 51.178197204457234, "chrf_score": 40.50564836079982, "xcomet_score": 0.7890808582305908, "xcomet_qe_score": 0.8483275175094604, "metricx_score": 5.642650604248047, "metricx_qe_score": 5.310375690460205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结起来,我们构建了Exampler,一个用于多种自然语言和多种表示的跨语言语义解析统一基准测试。", "metrics": {"bleu_score": 44.98116127232802, "chrf_score": 33.05450771727037, "xcomet_score": 0.6944203972816467, "xcomet_qe_score": 0.6498874425888062, "metricx_score": 4.309067249298096, "metricx_qe_score": 4.925125598907471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种代表性的多语言模型进行了全面的基准测试研究。", "metrics": {"bleu_score": 75.49399421360621, "chrf_score": 67.98186790513903, "xcomet_score": 0.9610769748687744, "xcomet_qe_score": 0.9540224075317383, "metricx_score": 0.8421860933303833, "metricx_qe_score": 1.073890209197998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结果显示了许多有趣的发现等", "metrics": {"bleu_score": 93.06048591020995, "chrf_score": 92.47065434565434, "xcomet_score": 0.8565675020217896, "xcomet_qe_score": 0.7905972003936768, "metricx_score": 1.7107927799224854, "metricx_qe_score": 1.3963158130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.8839613199234009, "xcomet_qe_score": 0.8593108057975769, "metricx_score": 0.1393444538116455, "metricx_qe_score": 0.2242259979248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫阿尤德·维拉尔,我将简要介绍论文《从翻译评估策略和性能中提取提示》中的内容。", "metrics": {"bleu_score": 14.53957707276688, "chrf_score": 16.364110583222505, "xcomet_score": 0.7165971994400024, "xcomet_qe_score": 0.6642682552337646, "metricx_score": 4.440485954284668, "metricx_qe_score": 5.575497150421143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和谷歌翻译同事的合作成果。", "metrics": {"bleu_score": 20.570885115591267, "chrf_score": 20.732468773163916, "xcomet_score": 0.9982795715332031, "xcomet_qe_score": 0.9951926469802856, "metricx_score": 0.7620185017585754, "metricx_qe_score": 0.350193053483963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Palm 是去年 2022 年推出的一个包含 5400 亿个参数的大型语言模型。", "metrics": {"bleu_score": 42.73762942511532, "chrf_score": 42.88174469206954, "xcomet_score": 0.9117473363876343, "xcomet_qe_score": 0.8372721076011658, "metricx_score": 3.9937422275543213, "metricx_qe_score": 4.56576681137085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在包含 7800 亿个标记的大量文本上进行了训练。", "metrics": {"bleu_score": 31.642571776698524, "chrf_score": 38.21138815417992, "xcomet_score": 0.7130940556526184, "xcomet_qe_score": 0.7145150303840637, "metricx_score": 1.5657542943954468, "metricx_qe_score": 2.133303642272949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发表时,它在数百个 NLP 任务中达到了最先进水平。", "metrics": {"bleu_score": 39.72418603247486, "chrf_score": 41.44345630215195, "xcomet_score": 0.9800243377685547, "xcomet_qe_score": 0.9601312875747681, "metricx_score": 2.580673933029175, "metricx_qe_score": 3.177891731262207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了机器翻译中大型语言模型提示的首次系统研究。", "metrics": {"bleu_score": 31.28510453803198, "chrf_score": 27.842019154814157, "xcomet_score": 0.958320140838623, "xcomet_qe_score": 0.9051872491836548, "metricx_score": 2.676194667816162, "metricx_qe_score": 3.3234386444091797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 AMT 社区的最佳实践来评估这些模型的翻译能力。", "metrics": {"bleu_score": 56.053891826721106, "chrf_score": 48.717295219245514, "xcomet_score": 0.8711926937103271, "xcomet_qe_score": 0.7736436128616333, "metricx_score": 4.915660858154297, "metricx_qe_score": 6.032485008239746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠,", "metrics": {"bleu_score": 76.67995669507935, "chrf_score": 72.70847933324492, "xcomet_score": 0.9537777900695801, "xcomet_qe_score": 0.9152324199676514, "metricx_score": 0.49034300446510315, "metricx_qe_score": 0.4892211854457855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,即 WMT 评估中表现最好的系统。", "metrics": {"bleu_score": 28.714372980706766, "chrf_score": 29.869824993653765, "xcomet_score": 0.9327125549316406, "xcomet_qe_score": 0.9159632921218872, "metricx_score": 2.5008981227874756, "metricx_qe_score": 3.9976532459259033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的神经机器翻译指标,并展示了基于专家的人类评估结果。", "metrics": {"bleu_score": 75.71389552562844, "chrf_score": 67.96325499880605, "xcomet_score": 0.9003326892852783, "xcomet_qe_score": 0.7845103740692139, "metricx_score": 1.4598731994628906, "metricx_qe_score": 2.4797985553741455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些提示选择策略的建议。", "metrics": {"bleu_score": 70.75330011966422, "chrf_score": 64.06828873488384, "xcomet_score": 0.8876395225524902, "xcomet_qe_score": 0.8450835347175598, "metricx_score": 1.09638249874115, "metricx_qe_score": 3.2114005088806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对翻译 LLM 的性能有很大影响,我们可以在一个简单的实验中看到这一点,我们使用一个简短的提示,并为一句话提供了两个不同的提示。", "metrics": {"bleu_score": 52.05172064517752, "chrf_score": 48.29535730257225, "xcomet_score": 0.7286246418952942, "xcomet_qe_score": 0.6874151229858398, "metricx_score": 5.143490314483643, "metricx_qe_score": 6.517474174499512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 1000 个句子中,", "metrics": {"bleu_score": 30.895757752065407, "chrf_score": 45.3545034322207, "xcomet_score": 0.8692065477371216, "xcomet_qe_score": 0.5887117385864258, "metricx_score": 6.577800750732422, "metricx_qe_score": 8.108074188232422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有 516 个句子的差异超过一个模糊点。", "metrics": {"bleu_score": 12.874330508144842, "chrf_score": 18.115193875624428, "xcomet_score": 0.5808275938034058, "xcomet_qe_score": 0.17317010462284088, "metricx_score": 7.925911903381348, "metricx_qe_score": 7.936371803283691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这个差异可以达到 40 个模糊点。", "metrics": {"bleu_score": 32.28213880040184, "chrf_score": 26.177889304305634, "xcomet_score": 0.820159375667572, "xcomet_qe_score": 0.7611461877822876, "metricx_score": 4.393194198608398, "metricx_qe_score": 3.1828927993774414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个好的提示策略非常重要。在", "metrics": {"bleu_score": 59.08871032231054, "chrf_score": 58.5902119027495, "xcomet_score": 0.8214733600616455, "xcomet_qe_score": 0.7509938478469849, "metricx_score": 3.953136920928955, "metricx_qe_score": 0.3306087255477905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验中,我们选择了五次提示策略,我们只需将我们提供给系统的每个句子标记为其所在的语言。所以在", "metrics": {"bleu_score": 40.56080852284901, "chrf_score": 37.350285774698406, "xcomet_score": 0.6451205015182495, "xcomet_qe_score": 0.6664832830429077, "metricx_score": 6.9769287109375, "metricx_qe_score": 3.6775314807891846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个例子中,我们从德语翻译成英语,德语句子,源句子用德语冒号标记,英语翻译用英语冒号标记。", "metrics": {"bleu_score": 45.193836893693394, "chrf_score": 31.26963807322814, "xcomet_score": 0.9310605525970459, "xcomet_qe_score": 0.9434390664100647, "metricx_score": 2.4940221309661865, "metricx_qe_score": 3.180774688720703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在几次简短提示的情况下,实际形式的提示并没有产生很大的影响。", "metrics": {"bleu_score": 40.95673651306528, "chrf_score": 38.12192032633808, "xcomet_score": 0.8252646923065186, "xcomet_qe_score": 0.8323349952697754, "metricx_score": 1.093768835067749, "metricx_qe_score": 0.9133802056312561, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零和一次简短提示,这至关重要,", "metrics": {"bleu_score": 15.696178790260367, "chrf_score": 15.282389309568348, "xcomet_score": 0.6771376132965088, "xcomet_qe_score": 0.7154446840286255, "metricx_score": 5.620306968688965, "metricx_qe_score": 4.970707893371582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们像我们这样进行五次简短提示时,实际形式的提示几乎没有区", "metrics": {"bleu_score": 15.320170132052814, "chrf_score": 19.489675324432472, "xcomet_score": 0.6904228925704956, "xcomet_qe_score": 0.6520782113075256, "metricx_score": 3.9965174198150635, "metricx_qe_score": 2.9864554405212402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别。重要的是例子本身。", "metrics": {"bleu_score": 4.2665050358928855, "chrf_score": 6.218905472636816, "xcomet_score": 0.7197756767272949, "xcomet_qe_score": 0.3687080442905426, "metricx_score": 1.6353012323379517, "metricx_qe_score": 1.872441291809082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,例子质量比与源句子的相似性更重要。", "metrics": {"bleu_score": 83.47563508866297, "chrf_score": 78.47704724153999, "xcomet_score": 0.9258888959884644, "xcomet_qe_score": 0.9196336269378662, "metricx_score": 0.9950923919677734, "metricx_qe_score": 0.7766151428222656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择高质量翻译的例子非常重要。", "metrics": {"bleu_score": 31.66831894770993, "chrf_score": 28.534477178392827, "xcomet_score": 0.9397034645080566, "xcomet_qe_score": 0.988231897354126, "metricx_score": 0.5738591551780701, "metricx_qe_score": 0.5590805411338806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了从 WMT 评估的训练数据或开发数据中选择提示。开发数据", "metrics": {"bleu_score": 46.6931284433023, "chrf_score": 39.072992222304364, "xcomet_score": 0.5240781307220459, "xcomet_qe_score": 0.3932662606239319, "metricx_score": 6.363341808319092, "metricx_qe_score": 5.148475646972656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比训练数据更有选择性,质量更高,我这样说,结果显示", "metrics": {"bleu_score": 21.171460625310196, "chrf_score": 17.898232746039024, "xcomet_score": 0.3023546040058136, "xcomet_qe_score": 0.37433233857154846, "metricx_score": 6.899498462677002, "metricx_qe_score": 6.477569103240967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据时性能更好。", "metrics": {"bleu_score": 45.43142611141303, "chrf_score": 41.14026689146475, "xcomet_score": 0.9033088684082031, "xcomet_qe_score": 0.890548825263977, "metricx_score": 1.9992756843566895, "metricx_qe_score": 2.476839542388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专业化的 ODR 系统在性能上比 PALM 翻译有显著优势,", "metrics": {"bleu_score": 13.10799821496978, "chrf_score": 14.563009751932515, "xcomet_score": 0.858035683631897, "xcomet_qe_score": 0.8147753477096558, "metricx_score": 5.197525978088379, "metricx_qe_score": 4.598079681396484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但 PALM 接近商业系统。", "metrics": {"bleu_score": 38.5383136469641, "chrf_score": 35.50761630735529, "xcomet_score": 0.8013249635696411, "xcomet_qe_score": 0.6583391427993774, "metricx_score": 3.6902987957000732, "metricx_qe_score": 4.418598651885986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的情况下,我们选择与谷歌翻译进行评估。", "metrics": {"bleu_score": 56.15880475398439, "chrf_score": 46.85642748972231, "xcomet_score": 0.876212477684021, "xcomet_qe_score": 0.84937584400177, "metricx_score": 2.7010931968688965, "metricx_qe_score": 3.047389507293701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 MQM 框架进行的人类评估结果表明,Palm 的流畅度与最先进的系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 47.19475998021307, "chrf_score": 41.13225985562532, "xcomet_score": 0.9242700338363647, "xcomet_qe_score": 0.8997949957847595, "metricx_score": 4.9269914627075195, "metricx_qe_score": 5.918763160705566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7579965591430664, "xcomet_qe_score": 0.7860524654388428, "metricx_score": 1.7050689458847046, "metricx_qe_score": 0.886371910572052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,Palm 有时会选择省略翻译中省略的部分,以产生更好的翻译。", "metrics": {"bleu_score": 12.054627382167174, "chrf_score": 12.6645849251848, "xcomet_score": 0.7737975120544434, "xcomet_qe_score": 0.6854676008224487, "metricx_score": 5.402321815490723, "metricx_qe_score": 5.447015762329102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAM 的外部风格类别低于最先进的系统,这是一个额外的信号,表明 PAM 提供了非常流畅的输出,但仍然存在一些准确性问题。", "metrics": {"bleu_score": 58.166541519265884, "chrf_score": 49.66882617260623, "xcomet_score": 0.7198227643966675, "xcomet_qe_score": 0.7584753632545471, "metricx_score": 5.942619323730469, "metricx_qe_score": 5.83794641494751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这个非常简短的概述的全部内容。", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 50.96667019306298, "xcomet_score": 0.9204057455062866, "xcomet_qe_score": 0.9140896797180176, "metricx_score": 0.3837635815143585, "metricx_qe_score": 0.374846875667572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有关更多详细信息,请参阅论文的完整介绍。", "metrics": {"bleu_score": 35.467316001743725, "chrf_score": 29.245211968903295, "xcomet_score": 0.91548091173172, "xcomet_qe_score": 0.9137508273124695, "metricx_score": 0.7078700661659241, "metricx_qe_score": 0.41184788942337036, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是德国斯坦兰特大学的博士生 Dawe。", "metrics": {"bleu_score": 31.26697229548035, "chrf_score": 30.005633409093345, "xcomet_score": 0.7527626752853394, "xcomet_qe_score": 0.7445535659790039, "metricx_score": 1.444610357284546, "metricx_qe_score": 2.305706739425659, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这段视频中,我想介绍我们最近的工作《比你想象的还要弱》,这是一次对每周监督学习的批判性审视。", "metrics": {"bleu_score": 41.565413199670715, "chrf_score": 36.646706996277295, "xcomet_score": 0.819646954536438, "xcomet_qe_score": 0.7858566045761108, "metricx_score": 5.449957370758057, "metricx_qe_score": 5.714334964752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与 Xiao Yushchen、Maios Musbach、Giaz Steffen 和 Dietrich Clarkov 共同完成的工作。", "metrics": {"bleu_score": 5.869997967287077, "chrf_score": 35.09527269756493, "xcomet_score": 0.49522048234939575, "xcomet_qe_score": 0.5203955173492432, "metricx_score": 7.996466159820557, "metricx_qe_score": 7.712636947631836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想先简要介绍一下每周监督和每周监督学习。", "metrics": {"bleu_score": 51.0032342952127, "chrf_score": 47.270054814114204, "xcomet_score": 0.6939575672149658, "xcomet_qe_score": 0.6260956525802612, "metricx_score": 6.086522102355957, "metricx_qe_score": 7.386913776397705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督中,我们不会手动标注数据。", "metrics": {"bleu_score": 19.16132916869952, "chrf_score": 20.121252719035972, "xcomet_score": 0.8147662281990051, "xcomet_qe_score": 0.7872034311294556, "metricx_score": 4.16445779800415, "metricx_qe_score": 4.29934024810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们会使用每周标注源来标注数据,例如简单的启发式规则、知识库或本地化众包,如图右侧所示。", "metrics": {"bleu_score": 42.89304482905258, "chrf_score": 39.78253097612996, "xcomet_score": 0.7092907428741455, "xcomet_qe_score": 0.6304411292076111, "metricx_score": 5.670658111572266, "metricx_qe_score": 6.192626953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,较弱的标注要便宜得多,但它们也存在噪声,这意味着一定数量的标注是错误的。", "metrics": {"bleu_score": 34.06033204524115, "chrf_score": 28.98389052799949, "xcomet_score": 0.7637859582901001, "xcomet_qe_score": 0.8271545767784119, "metricx_score": 2.0893545150756836, "metricx_qe_score": 2.061985492706299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在每周标签数据上训练神经网络,神经网络往往会记住标签噪声,并且无法泛化。", "metrics": {"bleu_score": 46.962578832600755, "chrf_score": 39.89191279902634, "xcomet_score": 0.8358755707740784, "xcomet_qe_score": 0.7662556171417236, "metricx_score": 5.400553226470947, "metricx_qe_score": 5.8399810791015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督学习中,提出了训练算法,以便在这样的标签噪声上稳健地训练神经网络,以便训练的模型仍然能很好地泛化。", "metrics": {"bleu_score": 44.03101649116149, "chrf_score": 36.52640713229659, "xcomet_score": 0.7511307001113892, "xcomet_qe_score": 0.7261449098587036, "metricx_score": 5.284366130828857, "metricx_qe_score": 6.828001022338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的 WSL(每周监督学习)工作中,一个常见的说法是,人们说他们只在每周标签数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 34.14133327842745, "chrf_score": 30.217201543511013, "xcomet_score": 0.6782522797584534, "xcomet_qe_score": 0.6979776620864868, "metricx_score": 6.965979099273682, "metricx_qe_score": 6.87528133392334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并没有错,但有一个问题,那就是人们假设有一个额外的干净验证集可用用于模型选择。", "metrics": {"bleu_score": 68.42307870750038, "chrf_score": 58.05070947729861, "xcomet_score": 0.9711508750915527, "xcomet_qe_score": 0.9685133695602417, "metricx_score": 1.8533200025558472, "metricx_qe_score": 3.168790578842163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这个问题设置提出了质疑,因为这意味着在每周监督学习中需要额外的手动标注。", "metrics": {"bleu_score": 51.13307336362714, "chrf_score": 48.21821833323838, "xcomet_score": 0.7167119979858398, "xcomet_qe_score": 0.711254358291626, "metricx_score": 4.844784736633301, "metricx_qe_score": 5.5537285804748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但就像房间里的大象一样,这个必要性往往被忽视。", "metrics": {"bleu_score": 52.8112428031202, "chrf_score": 46.15098525645558, "xcomet_score": 0.9271910190582275, "xcomet_qe_score": 0.8085941076278687, "metricx_score": 1.0684196949005127, "metricx_qe_score": 2.6988883018493652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述的疑问促使我们提出了三个研究问题。", "metrics": {"bleu_score": 39.8249474883932, "chrf_score": 40.1223851906876, "xcomet_score": 0.955315351486206, "xcomet_qe_score": 0.9493820667266846, "metricx_score": 1.49457848072052, "metricx_qe_score": 1.1195658445358276, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,WSL 是否需要干净的验证数据?或者我们是否可以改用噪声验证集?", "metrics": {"bleu_score": 50.506705817428575, "chrf_score": 47.8381903921106, "xcomet_score": 0.8786792755126953, "xcomet_qe_score": 0.8609044551849365, "metricx_score": 2.0157086849212646, "metricx_qe_score": 3.3074445724487305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果需要干净数据或干净数据是 WSL 工作的必要条件,那么我们需要多少干净样本?", "metrics": {"bleu_score": 33.24898589261915, "chrf_score": 30.776623084616784, "xcomet_score": 0.942520022392273, "xcomet_qe_score": 0.8794604539871216, "metricx_score": 0.87225741147995, "metricx_qe_score": 1.4424235820770264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该只使用干净样本进行验证,还是还有更好的利用方式?", "metrics": {"bleu_score": 38.249645415592596, "chrf_score": 31.58991930862251, "xcomet_score": 0.985082745552063, "xcomet_qe_score": 0.9223121404647827, "metricx_score": 0.5989317893981934, "metricx_qe_score": 0.9734723567962646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的发现如下。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 49.41553937149765, "xcomet_score": 0.9748376607894897, "xcomet_qe_score": 0.948908805847168, "metricx_score": 1.6144813299179077, "metricx_qe_score": 2.59200382232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现有趣的是,最近的 WSL 方法确实需要干净的验证样本才能正常工作。", "metrics": {"bleu_score": 78.98672280189824, "chrf_score": 75.6001734716905, "xcomet_score": 0.9334544539451599, "xcomet_qe_score": 0.9146655797958374, "metricx_score": 2.1472551822662354, "metricx_qe_score": 2.775536298751831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,训练的模型无法超越原始的弱标签进行泛化,这意味着训练是没有意义的。", "metrics": {"bleu_score": 61.494342622984036, "chrf_score": 52.66501729173526, "xcomet_score": 0.9642969369888306, "xcomet_qe_score": 0.86615389585495, "metricx_score": 1.7494802474975586, "metricx_qe_score": 2.3743014335632324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明 WSL 方法实际上需要干净的标注数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 55.39238522260304, "chrf_score": 51.26254788767057, "xcomet_score": 0.8940601348876953, "xcomet_qe_score": 0.9124821424484253, "metricx_score": 3.0991263389587402, "metricx_qe_score": 3.3886806964874268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于 WSL 方法实现更好的性能,如图左侧所示。", "metrics": {"bleu_score": 70.19951273802924, "chrf_score": 68.32675798677845, "xcomet_score": 0.9177678227424622, "xcomet_qe_score": 0.8955516219139099, "metricx_score": 3.775278329849243, "metricx_qe_score": 4.783837795257568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每个类别二十个样本就能达到高性能。", "metrics": {"bleu_score": 19.23968205566159, "chrf_score": 17.878348704435663, "xcomet_score": 0.9447394609451294, "xcomet_qe_score": 0.9557558298110962, "metricx_score": 1.5910460948944092, "metricx_qe_score": 2.068300247192383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这还不是故事的全部,因为如果我们决定访问干净样本,那么直接在它们上进行训练甚至会获得更好的性能。红", "metrics": {"bleu_score": 46.87876093304912, "chrf_score": 38.33791914394323, "xcomet_score": 0.7642196416854858, "xcomet_qe_score": 0.7463569045066833, "metricx_score": 5.636096000671387, "metricx_qe_score": 5.223907470703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "色图形显示了直接应用于干净数据的微调方法与仅用于验证的 WSL 方法之间的性能差异。", "metrics": {"bleu_score": 64.16167424171942, "chrf_score": 63.069809239276495, "xcomet_score": 0.6984598636627197, "xcomet_qe_score": 0.6297229528427124, "metricx_score": 3.881490468978882, "metricx_qe_score": 4.801795482635498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果我们每个类别有 10 个样本,直接微调开始超越 WSL 方法。", "metrics": {"bleu_score": 40.69701259581805, "chrf_score": 40.421192456015355, "xcomet_score": 0.9369845390319824, "xcomet_qe_score": 0.9160101413726807, "metricx_score": 1.7373441457748413, "metricx_qe_score": 2.6490941047668457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前 WSL 方法中声称的性能改进可以通过允许在干净验证样本上继续微调来轻松实现。", "metrics": {"bleu_score": 40.16148470345681, "chrf_score": 37.16710721355264, "xcomet_score": 0.9075114727020264, "xcomet_qe_score": 0.8948142528533936, "metricx_score": 2.1130266189575195, "metricx_qe_score": 3.979599714279175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,柏林模型称为 FTW 最初的性能低于更复杂的 WSL 方法如余弦。", "metrics": {"bleu_score": 22.701910420195695, "chrf_score": 21.361275847430935, "xcomet_score": 0.47418180108070374, "xcomet_qe_score": 0.5694102048873901, "metricx_score": 7.700146198272705, "metricx_qe_score": 8.374972343444824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果我们允许在干净样本上继续微调,那么 FTW 的性能与其他方法一样好。", "metrics": {"bleu_score": 51.11744072566821, "chrf_score": 43.910848387767906, "xcomet_score": 0.9444655179977417, "xcomet_qe_score": 0.8217566013336182, "metricx_score": 1.7624404430389404, "metricx_qe_score": 2.6165688037872314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在实践中,没有理由选择更复杂的 WSL 方法,这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 56.05699297537929, "chrf_score": 54.2786351532051, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9147420525550842, "metricx_qe_score": 1.2894562482833862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们表明最近的 WSL 方法需要干净的手动标注样本才能正常工作。", "metrics": {"bleu_score": 58.70960799135135, "chrf_score": 55.818681134037796, "xcomet_score": 0.8123953342437744, "xcomet_qe_score": 0.7689942717552185, "metricx_score": 2.5016672611236572, "metricx_qe_score": 3.3260154724121094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择", "metrics": {"bleu_score": 47.33641315654019, "chrf_score": 41.49597577079802, "xcomet_score": 0.864812970161438, "xcomet_qe_score": 0.8798450827598572, "metricx_score": 1.5103505849838257, "metricx_qe_score": 2.388295888900757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是否使用干净的验证样本。", "metrics": {"bleu_score": 23.42742863941098, "chrf_score": 27.190272042758508, "xcomet_score": 0.6660276055335999, "xcomet_qe_score": 0.520084023475647, "metricx_score": 2.3319201469421387, "metricx_qe_score": 3.2893576622009277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL 方法应该与未来的学习基线进行比较,因为两者都在干净样本上工作。", "metrics": {"bleu_score": 43.48370608827859, "chrf_score": 38.03330235315529, "xcomet_score": 0.6561991572380066, "xcomet_qe_score": 0.701786994934082, "metricx_score": 4.287864685058594, "metricx_qe_score": 6.209158420562744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,连续微调是一个简单但强大的基线,应该在未来的 WSL 工作中考虑。", "metrics": {"bleu_score": 38.96359712384752, "chrf_score": 33.5044114151537, "xcomet_score": 0.8286184668540955, "xcomet_qe_score": 0.7658861875534058, "metricx_score": 2.6469032764434814, "metricx_qe_score": 3.2304203510284424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过此幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9951430559158325, "xcomet_qe_score": 0.9870158433914185, "metricx_score": 0.48675400018692017, "metricx_qe_score": 0.44404107332229614, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝您会议愉快。", "metrics": {"bleu_score": 7.817610446892725, "chrf_score": 8.18252221407027, "xcomet_score": 0.9670588374137878, "xcomet_qe_score": 0.9966236352920532, "metricx_score": 0.5200188159942627, "metricx_qe_score": 0.28290775418281555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 9.993248618647176, "xcomet_score": 0.8700615763664246, "xcomet_qe_score": 0.6193946599960327, "metricx_score": 1.1073329448699951, "metricx_qe_score": 0.9096816182136536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "芬奇,我是萨拉·芬奇。", "metrics": {"bleu_score": 8.29519350710986, "chrf_score": 5.405070919696089, "xcomet_score": 0.6480262279510498, "xcomet_qe_score": 0.7325373888015747, "metricx_score": 4.678750514984131, "metricx_qe_score": 5.4248833656311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向大家介绍ABCEval,这是一种全新的评估对话式人工智能的维度方法。", "metrics": {"bleu_score": 39.94463928700864, "chrf_score": 40.82500039156529, "xcomet_score": 0.890528678894043, "xcomet_qe_score": 0.9462633728981018, "metricx_score": 1.9511057138442993, "metricx_qe_score": 1.6145200729370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是由埃默里大学的乔伊·崔教授领导的埃默里大学自然语言处理实验室完成的,并与亚马逊Alexa AI合作完成的。", "metrics": {"bleu_score": 40.10483119098419, "chrf_score": 42.148028548088156, "xcomet_score": 0.8403928279876709, "xcomet_qe_score": 0.9111216068267822, "metricx_score": 2.949769973754883, "metricx_qe_score": 2.6561198234558105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设你刚刚开发了一个对话模型,你想看看它与当前的先进水平相比如何。", "metrics": {"bleu_score": 62.73840693036, "chrf_score": 57.473850041154236, "xcomet_score": 0.9929516315460205, "xcomet_qe_score": 0.9851839542388916, "metricx_score": 0.6159711480140686, "metricx_qe_score": 0.7467657327651978, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如让人工评判员选择两个对话中哪一个更好,或者根据一个量化比例对对话进行评分。", "metrics": {"bleu_score": 58.585870513537095, "chrf_score": 52.74063831752013, "xcomet_score": 0.8241540193557739, "xcomet_qe_score": 0.8667298555374146, "metricx_score": 1.0349230766296387, "metricx_qe_score": 1.0572752952575684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的全面评估方面效果很好,但对话质量有许多方面。", "metrics": {"bleu_score": 37.54122374352081, "chrf_score": 31.767694670629226, "xcomet_score": 0.9337989091873169, "xcomet_qe_score": 0.9130752086639404, "metricx_score": 0.5414295196533203, "metricx_qe_score": 0.7984640598297119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,你可能希望评估对话质量的多个维度,以便更细致地了解模型的优缺点。", "metrics": {"bleu_score": 64.05720510070545, "chrf_score": 57.697071008266455, "xcomet_score": 0.9870455265045166, "xcomet_qe_score": 0.9564906358718872, "metricx_score": 0.6348278522491455, "metricx_qe_score": 0.5003135204315186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人工评判员评估对话质量的几个维度,例如模型响应的相关性,使用现有的比较或量化比例方法。", "metrics": {"bleu_score": 50.31243293971042, "chrf_score": 44.10347439188164, "xcomet_score": 0.7230613827705383, "xcomet_qe_score": 0.7613028287887573, "metricx_score": 2.235569953918457, "metricx_qe_score": 2.4531872272491455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们相信存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 40.00853590887389, "xcomet_score": 0.8991663455963135, "xcomet_qe_score": 0.8668703436851501, "metricx_score": 1.396376132965088, "metricx_qe_score": 1.407410740852356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过明确标注每个模型响应是否表达了某些行为,例如响应无关信息或自相矛盾。", "metrics": {"bleu_score": 31.393933094262902, "chrf_score": 31.126425328015067, "xcomet_score": 0.6598953008651733, "xcomet_qe_score": 0.40215393900871277, "metricx_score": 6.296339511871338, "metricx_qe_score": 7.005809307098389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为对话行为标注,简称ABCEval。", "metrics": {"bleu_score": 51.35245522145285, "chrf_score": 51.47267124003327, "xcomet_score": 0.8360490798950195, "xcomet_qe_score": 0.8246316909790039, "metricx_score": 1.677452564239502, "metricx_qe_score": 2.1295900344848633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面覆盖最近文献中建议影响对话质量的对话模型行为。", "metrics": {"bleu_score": 38.08197831623487, "chrf_score": 30.494451915540782, "xcomet_score": 0.8067662119865417, "xcomet_qe_score": 0.858824610710144, "metricx_score": 2.243316411972046, "metricx_qe_score": 3.0585410594940186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABCEval能够测量对话模型犯下各种主题错误的比率。", "metrics": {"bleu_score": 61.53267326643311, "chrf_score": 58.95946009948678, "xcomet_score": 0.8109388947486877, "xcomet_qe_score": 0.9111515283584595, "metricx_score": 3.380394458770752, "metricx_qe_score": 4.932478904724121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABCEval测量了对话模型忽略其伙伴或说无关话、自相矛盾或与伙伴、伙伴或伙伴自相矛盾、产生错误事实或违反常识知识的回合次数,以及模型成功或失败地表现出同理心的次数。", "metrics": {"bleu_score": 44.706648574469504, "chrf_score": 38.39472584428971, "xcomet_score": 0.40211424231529236, "xcomet_qe_score": 0.4605673551559448, "metricx_score": 5.99879789352417, "metricx_qe_score": 5.893284320831299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方法最有效,我们选择了四种先进的对话模型,并使用ABCEval对每种模型进行了100个人工机器人对话的评估。", "metrics": {"bleu_score": 38.554269459087784, "chrf_score": 40.77725324277058, "xcomet_score": 0.8930864334106445, "xcomet_qe_score": 0.9139508008956909, "metricx_score": 2.2580955028533936, "metricx_qe_score": 1.9050897359848022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了进行比较,我们还使用三种现有方法对这些对话进行了评估:量化比例水平的量化评分、对话水平的量化评分和对话水平的配对比较。", "metrics": {"bleu_score": 39.955396798640315, "chrf_score": 34.045906175365474, "xcomet_score": 0.7392097115516663, "xcomet_qe_score": 0.7520086169242859, "metricx_score": 5.109602928161621, "metricx_qe_score": 5.195897579193115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了评估方法,我们还收集了对对话最常见的八个方面的评估,因为这是评估对话模型的标准做法。从", "metrics": {"bleu_score": 36.64132635908435, "chrf_score": 30.956136777364996, "xcomet_score": 0.5658738613128662, "xcomet_qe_score": 0.6397347450256348, "metricx_score": 7.309074401855469, "metricx_qe_score": 2.015148878097534, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这些评估结果的分析中,我们发现ABCEval行为标签总体上比现有方法收集的标签更可靠,这是通过对100个双重标签对话的评判员一致性来衡量的。", "metrics": {"bleu_score": 40.77567202883284, "chrf_score": 43.41977282387542, "xcomet_score": 0.7419625520706177, "xcomet_qe_score": 0.7784147262573242, "metricx_score": 5.2673234939575195, "metricx_qe_score": 6.328028678894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,ABCEval标签比现有方法产生的指标更能预测整体对话质量,这是通过这个简单的线性回归分析显示的。", "metrics": {"bleu_score": 51.470484826780904, "chrf_score": 47.24425997705572, "xcomet_score": 0.9530675411224365, "xcomet_qe_score": 0.9453456997871399, "metricx_score": 1.830698013305664, "metricx_qe_score": 1.9254685640335083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,你可以看到测量自相矛盾和伙伴自相矛盾回合比例分别解释了对话质量的5%和10%,而平均量化比例一致性得分只解释了4%或更少。", "metrics": {"bleu_score": 51.96831011385493, "chrf_score": 44.44416264156993, "xcomet_score": 0.6546438932418823, "xcomet_qe_score": 0.7084488868713379, "metricx_score": 5.596343994140625, "metricx_qe_score": 5.263726711273193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查了每个评估指标是否捕捉了对话质量的独特方面。", "metrics": {"bleu_score": 74.8279877183659, "chrf_score": 67.36733940778059, "xcomet_score": 0.8634798526763916, "xcomet_qe_score": 0.7981253266334534, "metricx_score": 0.9831233620643616, "metricx_qe_score": 1.525488257408142, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以看到所有ABCEval指标的组合解释了超过25%的对话质量,当你逐个移除这些指标时,大多数都会导致失去大量关于质量的信息。", "metrics": {"bleu_score": 45.50092577448166, "chrf_score": 47.2208256041733, "xcomet_score": 0.8871381878852844, "xcomet_qe_score": 0.8416674137115479, "metricx_score": 2.9819109439849854, "metricx_qe_score": 3.61554217338562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而交替水平量化比例指标的组合解释了质量的远少,而且这些指标中很少有", "metrics": {"bleu_score": 11.568710353014087, "chrf_score": 11.546133559863145, "xcomet_score": 0.24165070056915283, "xcomet_qe_score": 0.16318705677986145, "metricx_score": 13.020770072937012, "metricx_qe_score": 8.641230583190918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "独特的。这些可靠、信息丰富且独特的ABCEval指标使我们能够以比以前方法能够实现的更高的分辨率评估对话式人工智能。", "metrics": {"bleu_score": 5.656144685713648, "chrf_score": 14.261059268691975, "xcomet_score": 0.21747811138629913, "xcomet_qe_score": 0.44785159826278687, "metricx_score": 5.259712219238281, "metricx_qe_score": 5.059007167816162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们实验的结果中,你可以看到仍然存在几个挑战,并且这些挑战已经被精确量化。", "metrics": {"bleu_score": 19.867275036696853, "chrf_score": 23.885448271680374, "xcomet_score": 0.9640319347381592, "xcomet_qe_score": 0.9592080116271973, "metricx_score": 1.6654915809631348, "metricx_qe_score": 1.6978884935379028, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在测试的机器人中,大约有20%的响应违反了常识。", "metrics": {"bleu_score": 44.12598628712025, "chrf_score": 40.37987344370961, "xcomet_score": 0.9643508791923523, "xcomet_qe_score": 0.8627550601959229, "metricx_score": 1.1110178232192993, "metricx_qe_score": 1.368333101272583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在大约15%的响应中产生了无关信息,并且在大约10%的时间里自相矛盾或与伙伴自相矛盾。", "metrics": {"bleu_score": 39.00457163810872, "chrf_score": 36.71063889019938, "xcomet_score": 0.6689690351486206, "xcomet_qe_score": 0.6681903600692749, "metricx_score": 4.225986957550049, "metricx_qe_score": 3.9503958225250244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速改进,许多这些错误率可能会在自我们进行评估以来发布的新模型中减少。", "metrics": {"bleu_score": 50.46002228261731, "chrf_score": 43.791246034561496, "xcomet_score": 0.9778138399124146, "xcomet_qe_score": 0.9547284841537476, "metricx_score": 2.6530942916870117, "metricx_qe_score": 2.2177138328552246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更是一个追求可靠和精确评估指标以比较模型的理由。", "metrics": {"bleu_score": 25.265836983728335, "chrf_score": 23.14894967532212, "xcomet_score": 0.9280268549919128, "xcomet_qe_score": 0.7598658800125122, "metricx_score": 2.1532578468322754, "metricx_qe_score": 2.283590316772461, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABCEval可以被该领域的其他人作为朝着这个方向迈出的有意义的一步,", "metrics": {"bleu_score": 60.5970302592686, "chrf_score": 55.2016336634505, "xcomet_score": 0.9317293167114258, "xcomet_qe_score": 0.9175834655761719, "metricx_score": 2.377318859100342, "metricx_qe_score": 2.4398272037506104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待看到对话式人工智能在未来几个月和几年中的发展。", "metrics": {"bleu_score": 74.0609366763812, "chrf_score": 73.52998803726307, "xcomet_score": 0.9891669750213623, "xcomet_qe_score": 0.9691853523254395, "metricx_score": 0.7160965800285339, "metricx_qe_score": 0.8807143568992615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Kyo Yin,我将为大家介绍我们的研究成果《何时翻译需要上下文?", "metrics": {"bleu_score": 30.321326359419633, "chrf_score": 31.378867818962945, "xcomet_score": 0.9264119863510132, "xcomet_qe_score": 0.9193434715270996, "metricx_score": 0.8330965042114258, "metricx_qe_score": 1.732880711555481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据驱动的多语言探索》。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9881889820098877, "xcomet_qe_score": 0.9232277870178223, "metricx_score": 0.8727323412895203, "metricx_qe_score": 1.1075347661972046, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项研究是与Patrick Fernandes、Emily Liu、Andre FD Martins和Graham Newbig合作完成的。", "metrics": {"bleu_score": 32.042193038079624, "chrf_score": 69.15799844863238, "xcomet_score": 0.8133171796798706, "xcomet_qe_score": 0.8262534141540527, "metricx_score": 2.298379898071289, "metricx_qe_score": 2.3478775024414062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很多翻译都依赖于上下文。", "metrics": {"bleu_score": 53.107253497886994, "chrf_score": 42.302350982026496, "xcomet_score": 0.9982906579971313, "xcomet_qe_score": 0.9888886213302612, "metricx_score": 0.12038043886423111, "metricx_qe_score": 0.1823299527168274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译这句话中的“mole”?", "metrics": {"bleu_score": 79.65670178751185, "chrf_score": 80.61198748505251, "xcomet_score": 0.9964717626571655, "xcomet_qe_score": 0.9687114953994751, "metricx_score": 0.7808219790458679, "metricx_qe_score": 2.0721216201782227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句是“如果部长们知道了,事情可能会变得危险”,那么“mole”指的是间谍。", "metrics": {"bleu_score": 19.97752200394278, "chrf_score": 13.606427842946959, "xcomet_score": 0.9883341789245605, "xcomet_qe_score": 0.9678401350975037, "metricx_score": 2.649116277694702, "metricx_qe_score": 3.986283302307129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,这可能有什么严重的问题吗?”那么“mole”指的是胎记。", "metrics": {"bleu_score": 19.54636771284904, "chrf_score": 17.420153778244003, "xcomet_score": 0.9589517712593079, "xcomet_qe_score": 0.9359352588653564, "metricx_score": 2.0059125423431396, "metricx_qe_score": 2.8967463970184326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,单词的含义会发生变化,因此其翻译也会随之改变。", "metrics": {"bleu_score": 52.686297535003646, "chrf_score": 42.690139202157816, "xcomet_score": 0.9888059496879578, "xcomet_qe_score": 0.9756384491920471, "metricx_score": 0.24016350507736206, "metricx_qe_score": 0.28241562843322754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在处理这类情况时的表现非常困难。", "metrics": {"bleu_score": 29.10708729782026, "chrf_score": 24.007307958698608, "xcomet_score": 0.9452650547027588, "xcomet_qe_score": 0.9408419728279114, "metricx_score": 1.1816174983978271, "metricx_qe_score": 0.9819419384002686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有小部分翻译依赖于上下文,这使得像Blue这样的语料库级指标无法捕捉这些翻译。有", "metrics": {"bleu_score": 56.385034026302556, "chrf_score": 47.91028312355497, "xcomet_score": 0.8489835262298584, "xcomet_qe_score": 0.7799725532531738, "metricx_score": 4.543534278869629, "metricx_qe_score": 3.793193817138672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些人建议对依赖上下文的翻译进行有针对性的评估,但这些资源只支持有限类型的依赖上下文的翻译和有限的语言集合,因为它们通常依赖于领域知识和人工整理。", "metrics": {"bleu_score": 71.39045127435284, "chrf_score": 64.6167241200911, "xcomet_score": 0.72776198387146, "xcomet_qe_score": 0.6505705118179321, "metricx_score": 3.9919776916503906, "metricx_qe_score": 3.876704216003418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,何时翻译需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9996569156646729, "xcomet_qe_score": 0.9977697134017944, "metricx_score": 0.15239903330802917, "metricx_qe_score": 0.3020760715007782, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在处理这些情况时的表现如何?", "metrics": {"bleu_score": 80.86627571031983, "chrf_score": 78.17325175684309, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4453682601451874, "metricx_qe_score": 0.4600134789943695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了单词在翻译过程中对上下文的依赖程度。", "metrics": {"bleu_score": 84.52785147119853, "chrf_score": 78.40240983979126, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 4.045997619628906, "metricx_qe_score": 4.679557800292969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的研究中,我们引入了CXMI作为机器翻译模型上下文使用的度量。CX", "metrics": {"bleu_score": 70.27367085046107, "chrf_score": 69.39305399069188, "xcomet_score": 0.73764967918396, "xcomet_qe_score": 0.7552521228790283, "metricx_score": 4.891874313354492, "metricx_qe_score": 2.0590755939483643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "MI通过测量上下文C在给定源X的情况下对目标Y提供的信息来实现。你可以将CXMI视为给模型提供上下文所获得的信息。", "metrics": {"bleu_score": 50.3976812103483, "chrf_score": 46.267807399903596, "xcomet_score": 0.8276810646057129, "xcomet_qe_score": 0.6245879530906677, "metricx_score": 4.863004684448242, "metricx_qe_score": 4.669997215270996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将CXMI扩展为逐点CXMI,可以测量句子级或词级上下文的使用。", "metrics": {"bleu_score": 22.855364506403532, "chrf_score": 25.397131486146485, "xcomet_score": 0.7922373414039612, "xcomet_qe_score": 0.7712075114250183, "metricx_score": 2.0872716903686523, "metricx_qe_score": 2.133481979370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将PCXMI高的词视为需要上下文进行翻译的词。", "metrics": {"bleu_score": 84.1354400365363, "chrf_score": 74.44736085128784, "xcomet_score": 0.9077746868133545, "xcomet_qe_score": 0.9067944288253784, "metricx_score": 1.1967768669128418, "metricx_qe_score": 2.0331075191497803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们分析PCXMI高的词,寻找这些词之间的模式。", "metrics": {"bleu_score": 31.32725997036513, "chrf_score": 28.83268936257584, "xcomet_score": 0.939948558807373, "xcomet_qe_score": 0.8742880821228027, "metricx_score": 1.5252203941345215, "metricx_qe_score": 2.2651522159576416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成十四种不同语言的TED演讲的转录本进行了分析。", "metrics": {"bleu_score": 52.093830028342616, "chrf_score": 50.8129527162486, "xcomet_score": 0.9087775349617004, "xcomet_qe_score": 0.975662350654602, "metricx_score": 1.0150868892669678, "metricx_qe_score": 1.008156657218933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同的层面上进行分析。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9992449283599854, "xcomet_qe_score": 0.9950920343399048, "metricx_score": 0.22764378786087036, "metricx_qe_score": 0.23440968990325928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看了具有高平均PCXMI的词性标注。", "metrics": {"bleu_score": 19.138367678485665, "chrf_score": 23.639343748039398, "xcomet_score": 0.7840941548347473, "xcomet_qe_score": 0.7790865302085876, "metricx_score": 2.6502883434295654, "metricx_qe_score": 2.5920591354370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到例如,阿拉伯语中的双重代词具有相对高的p six mi。", "metrics": {"bleu_score": 41.85885529254786, "chrf_score": 33.12552929917838, "xcomet_score": 0.5518674850463867, "xcomet_qe_score": 0.49082478880882263, "metricx_score": 8.114306449890137, "metricx_qe_score": 9.353591918945312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语没有双重代词。因此,你需要上下文来确定翻译成阿拉伯语时代词是否是双重。", "metrics": {"bleu_score": 48.72165411971667, "chrf_score": 40.71583984880498, "xcomet_score": 0.6713030338287354, "xcomet_qe_score": 0.693087100982666, "metricx_score": 3.473781108856201, "metricx_qe_score": 4.691176891326904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 91.63140145331569, "chrf_score": 90.37654905758356, "xcomet_score": 0.9986207485198975, "xcomet_qe_score": 0.9910346269607544, "metricx_score": 0.5932432413101196, "metricx_qe_score": 0.8257213234901428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们查看了在所有不同出现情况下具有高p six mi的词汇项目。", "metrics": {"bleu_score": 28.064336480700604, "chrf_score": 22.706113696999672, "xcomet_score": 0.5895534157752991, "xcomet_qe_score": 0.6003588438034058, "metricx_score": 10.563739776611328, "metricx_qe_score": 9.681157112121582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别出像这里的情况,即在中文中你需要上下文才能正确翻译。", "metrics": {"bleu_score": 14.153215251190996, "chrf_score": 16.010545344041752, "xcomet_score": 0.7812763452529907, "xcomet_qe_score": 0.7977322340011597, "metricx_score": 1.817378282546997, "metricx_qe_score": 2.4600906372070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们发现上下文对于以正确的正式程度进行翻译很重要。", "metrics": {"bleu_score": 44.120063733294245, "chrf_score": 44.14794816014418, "xcomet_score": 0.8753557205200195, "xcomet_qe_score": 0.9011105298995972, "metricx_score": 0.7109169960021973, "metricx_qe_score": 0.7082697749137878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看了具有高p6mi的不同个体标记。", "metrics": {"bleu_score": 11.728147369287814, "chrf_score": 11.391586332364056, "xcomet_score": 0.6747199296951294, "xcomet_qe_score": 0.6582150459289551, "metricx_score": 7.247470855712891, "metricx_qe_score": 6.914268970489502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出无法真正由词本身捕捉到的现象,而是通过句子结构表达的,例如省略的解析。", "metrics": {"bleu_score": 40.6225298379429, "chrf_score": 35.898389822250515, "xcomet_score": 0.8228579759597778, "xcomet_qe_score": 0.7765946388244629, "metricx_score": 2.035457134246826, "metricx_qe_score": 2.4448161125183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们使用分析结果来设计一个文档级翻译的基准。", "metrics": {"bleu_score": 66.27025312460344, "chrf_score": 61.543147543092545, "xcomet_score": 0.976282000541687, "xcomet_qe_score": 0.8572309017181396, "metricx_score": 1.0458717346191406, "metricx_qe_score": 1.3173892498016357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们确定的五个语篇现象,我们创建了标记器来自动识别与现象相关的词,", "metrics": {"bleu_score": 47.72766467270395, "chrf_score": 45.74066429571065, "xcomet_score": 0.8367068767547607, "xcomet_qe_score": 0.879572331905365, "metricx_score": 1.5473604202270508, "metricx_qe_score": 1.3543307781219482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称我们的标记器为多语言语篇感知或MUDA标记器。", "metrics": {"bleu_score": 20.625129940085245, "chrf_score": 27.458119541700988, "xcomet_score": 0.8227494955062866, "xcomet_qe_score": 0.8007516860961914, "metricx_score": 1.1816623210906982, "metricx_qe_score": 1.615100622177124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同语言具有不同比例的这些语篇现象。", "metrics": {"bleu_score": 20.096755115923674, "chrf_score": 22.037598234164374, "xcomet_score": 0.9017677307128906, "xcomet_qe_score": 0.909665048122406, "metricx_score": 2.3025288581848145, "metricx_qe_score": 1.5647097826004028, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们使用MUDA标记器,通过在我们将用于评估的平行语料库上应用标记器,并在MUDA标记器已经识别的依赖上下文的例子上应用我们选择的翻译指标。", "metrics": {"bleu_score": 36.95802905689619, "chrf_score": 39.14002516137317, "xcomet_score": 0.7026607990264893, "xcomet_qe_score": 0.6236146092414856, "metricx_score": 2.8799540996551514, "metricx_qe_score": 3.035053014755249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译上的表现。", "metrics": {"bleu_score": 74.38252804209344, "chrf_score": 72.81560651989159, "xcomet_score": 0.9105273485183716, "xcomet_qe_score": 0.8643457889556885, "metricx_score": 0.9695982933044434, "metricx_qe_score": 1.057651400566101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级指标时,对于Blue,我们发现不依赖上下文的模型表现最好", "metrics": {"bleu_score": 48.51371996246932, "chrf_score": 39.21414064259678, "xcomet_score": 0.8826332092285156, "xcomet_qe_score": 0.7922769784927368, "metricx_score": 2.430176258087158, "metricx_qe_score": 3.3802342414855957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但如果我们使用comet,依赖上下文的模型表现最好。", "metrics": {"bleu_score": 42.992838075029, "chrf_score": 29.61408263119673, "xcomet_score": 0.8150580525398254, "xcomet_qe_score": 0.810333251953125, "metricx_score": 3.6608548164367676, "metricx_qe_score": 3.715186595916748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用WordF度量,那么有无上下文的模型表现相当。", "metrics": {"bleu_score": 50.69250959141051, "chrf_score": 46.00752334171282, "xcomet_score": 0.8334032297134399, "xcomet_qe_score": 0.7589300870895386, "metricx_score": 3.769357204437256, "metricx_qe_score": 2.887887954711914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,仅使用语料库级指标就很难确定最佳的文档级翻译系统。", "metrics": {"bleu_score": 72.9745872563541, "chrf_score": 68.59123507966633, "xcomet_score": 0.9936784505844116, "xcomet_qe_score": 0.9777528047561646, "metricx_score": 0.6173000335693359, "metricx_qe_score": 0.8048419952392578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们使用MUDA基准来评估模型,发现依赖上下文级别的模型对于某些语篇现象,如正式程度和词汇连贯性,比不使用上下文的模型准确得多。", "metrics": {"bleu_score": 48.007682630753195, "chrf_score": 43.40048822347112, "xcomet_score": 0.8474737405776978, "xcomet_qe_score": 0.8306101560592651, "metricx_score": 1.9782882928848267, "metricx_qe_score": 2.336398124694824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在其他现象,如省略、代词和动词形式上并不比不使用上下文的模型好多少。", "metrics": {"bleu_score": 61.09931955358367, "chrf_score": 53.62313532873326, "xcomet_score": 0.985906720161438, "xcomet_qe_score": 0.9784799814224243, "metricx_score": 0.8743526935577393, "metricx_qe_score": 1.2857717275619507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明我们需要在文档级翻译方面取得更多进展。", "metrics": {"bleu_score": 37.80611649897595, "chrf_score": 36.106645585070076, "xcomet_score": 0.9982482194900513, "xcomet_qe_score": 0.9886128902435303, "metricx_score": 0.7259799242019653, "metricx_qe_score": 0.7682129144668579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准表明DPL在文档级翻译中通常比Google Translate更准确。", "metrics": {"bleu_score": 59.76337301429703, "chrf_score": 47.13836087599867, "xcomet_score": 0.8460561037063599, "xcomet_qe_score": 0.7967967987060547, "metricx_score": 2.9916698932647705, "metricx_qe_score": 3.2545695304870605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们对十四对语言对进行了数据驱动的分析,以确定何时翻译需要上下文。然后,我们使用我们的发现建立了一个文档级机器翻译的基准,这可以帮助我们确定模型可以很好地处理哪些离散现象,以及哪些翻译系统擅长文档级翻译。", "metrics": {"bleu_score": 56.02091892657371, "chrf_score": 49.760213492872914, "xcomet_score": 0.791225790977478, "xcomet_qe_score": 0.8593572378158569, "metricx_score": 3.8242180347442627, "metricx_qe_score": 4.379196643829346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9842581748962402, "xcomet_qe_score": 0.9662535190582275, "metricx_score": 0.4675371050834656, "metricx_qe_score": 0.26872068643569946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "明天见。", "metrics": {"bleu_score": 11.752701606523267, "chrf_score": 10.982428115015974, "xcomet_score": 0.519114077091217, "xcomet_qe_score": 0.17786654829978943, "metricx_score": 1.966428279876709, "metricx_qe_score": 3.6971447467803955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Yanis Lavrack,我将向大家介绍我们在 Dr. Berth 上的工作,这是一个针对法语生物医学和临床领域的强大预训练模型。", "metrics": {"bleu_score": 44.07461542333101, "chrf_score": 43.20841675301779, "xcomet_score": 0.6129752397537231, "xcomet_qe_score": 0.6238383650779724, "metricx_score": 3.6624231338500977, "metricx_qe_score": 3.3324036598205566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论医疗保健中的语言建模。", "metrics": {"bleu_score": 56.50858546816402, "chrf_score": 44.42216569470257, "xcomet_score": 0.9920666217803955, "xcomet_qe_score": 0.9867169857025146, "metricx_score": 0.4082871079444885, "metricx_qe_score": 0.5630125403404236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了第一个法语生物医学模型 Dr. Berth,它基于 Roberta,并在 Natchios 上进行训练,Natchios 是一个从网络上抓取的医学数据集。", "metrics": {"bleu_score": 41.832546161910344, "chrf_score": 29.387780286660302, "xcomet_score": 0.5544085502624512, "xcomet_qe_score": 0.5747990012168884, "metricx_score": 4.110471725463867, "metricx_qe_score": 4.322986125946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了多个预训练设置和数据源的模型比较。", "metrics": {"bleu_score": 84.85407897723051, "chrf_score": 80.73325619207009, "xcomet_score": 0.9782614707946777, "xcomet_qe_score": 0.962895393371582, "metricx_score": 0.752806544303894, "metricx_qe_score": 1.1055766344070435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们展示了我们在法语下游任务上的结果。", "metrics": {"bleu_score": 25.624726964085337, "chrf_score": 22.320467380182045, "xcomet_score": 0.7485306262969971, "xcomet_qe_score": 0.7377641797065735, "metricx_score": 5.697821617126465, "metricx_qe_score": 7.410617828369141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们将总结实验,并向您提供有关如何访问这些模型的更多详细信息。", "metrics": {"bleu_score": 58.558806489952374, "chrf_score": 55.19383016694598, "xcomet_score": 0.8544363975524902, "xcomet_qe_score": 0.9099171161651611, "metricx_score": 0.32287517189979553, "metricx_qe_score": 0.2782720625400543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自 2018 年发布以来,BERT 已成为解决自然语言处理任务的最有效方法之一,与历史上的静态和上下文方法(如 Word2Vec、Fastex 或 NWO)相比,性能大幅提升。", "metrics": {"bleu_score": 49.91737257130128, "chrf_score": 44.568807443082676, "xcomet_score": 0.8085393309593201, "xcomet_qe_score": 0.793832540512085, "metricx_score": 3.206831455230713, "metricx_qe_score": 3.292855739593506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从那时起,该模型已被适应到许多其他语言,如法语中的 Camembert,以及生物医学中的 PermetteBERT 和 BioBERT,以及临床中的 clinical birth,但大多数是英文。", "metrics": {"bleu_score": 27.80235729361701, "chrf_score": 34.18861764575239, "xcomet_score": 0.44425737857818604, "xcomet_qe_score": 0.4009774625301361, "metricx_score": 9.712339401245117, "metricx_qe_score": 10.022268295288086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专业模型很少,并且通常基于持续预训练,因为缺乏领域内的数据。然而,到目前为", "metrics": {"bleu_score": 37.38742628988319, "chrf_score": 37.07184731839677, "xcomet_score": 0.26805800199508667, "xcomet_qe_score": 0.24977931380271912, "metricx_score": 6.088771820068359, "metricx_qe_score": 5.32103967666626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "止,法语还没有任何开源生物医学模型。所以我们问自己", "metrics": {"bleu_score": 25.47273594696237, "chrf_score": 21.373749864002516, "xcomet_score": 0.24431902170181274, "xcomet_qe_score": 0.2266540825366974, "metricx_score": 8.147395133972168, "metricx_qe_score": 7.64796781539917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个问题:对于广泛的用途,最合适的 数据来源是什么?而那些当前的数据是临床数据的良好替代品。", "metrics": {"bleu_score": 40.452959556677285, "chrf_score": 37.13455354124668, "xcomet_score": 0.6384290456771851, "xcomet_qe_score": 0.5415825843811035, "metricx_score": 3.9072515964508057, "metricx_qe_score": 3.9986085891723633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将 Dr. Bert 与我们基于我们所拥有的非大学医院获得的匿名数据构建的 Schubert 模型进行比较。", "metrics": {"bleu_score": 42.35348613666153, "chrf_score": 32.17775923257481, "xcomet_score": 0.6393107175827026, "xcomet_qe_score": 0.5240693092346191, "metricx_score": 6.181552886962891, "metricx_qe_score": 7.073734283447266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们问自己,我们需要多少数据来训练一个基于法语数据的专业模型?", "metrics": {"bleu_score": 55.60441483540001, "chrf_score": 57.33422767466484, "xcomet_score": 0.9901158809661865, "xcomet_qe_score": 0.8921628594398499, "metricx_score": 0.6050548553466797, "metricx_qe_score": 0.6389204263687134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是 4 GB、8 GB 还是更多?", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 90.21205646205644, "xcomet_score": 0.9791398048400879, "xcomet_qe_score": 0.9612053036689758, "metricx_score": 0.2863093912601471, "metricx_qe_score": 0.6383348107337952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四个从头开始的模型。Dr. Bert 的第一个版本使用了 7 GB 的 Natchez 数据,第二个版本使用了 4 GB 的 Natchez 数据,Schubert 的第一个版本是一个临床模型,使用了从临床节点中提取的 4 GB 的句子,Schubert 的最终版本则混合使用了 4 GB 的 Natchez 数据和 4 GB 的临床节点数据。", "metrics": {"bleu_score": 36.6808803431742, "chrf_score": 34.0505578971871, "xcomet_score": 0.3983228802680969, "xcomet_qe_score": 0.3561989367008209, "metricx_score": 5.73487663269043, "metricx_qe_score": 5.904784679412842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还介绍了三个基于持续预训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 47.38433884322971, "chrf_score": 40.98575885413362, "xcomet_score": 0.8991377353668213, "xcomet_qe_score": 0.945838212966919, "metricx_score": 1.0575565099716187, "metricx_qe_score": 1.148289680480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于 Camembert 的权重,并在 4 GB 的 Natchez 数据上进行训练。", "metrics": {"bleu_score": 24.76980256562108, "chrf_score": 39.50605206409074, "xcomet_score": 0.5865806341171265, "xcomet_qe_score": 0.4790596663951874, "metricx_score": 6.133146286010742, "metricx_qe_score": 6.8549485206604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也是基于 Camembert,但这次是在 4 GB 的临床节点上进行训练。最后,一个基于英语生物医学模型 Bermud Bert,并在 4 GB 的 Natchez 数据上进行训练。", "metrics": {"bleu_score": 36.85624243945618, "chrf_score": 42.582346332793826, "xcomet_score": 0.4565223157405853, "xcomet_qe_score": 0.4476918578147888, "metricx_score": 7.429439544677734, "metricx_qe_score": 7.610714912414551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有七个模型。", "metrics": {"bleu_score": 77.88007830714052, "chrf_score": 76.10249796742268, "xcomet_score": 0.9770487546920776, "xcomet_qe_score": 0.8902060985565186, "metricx_score": 0.15162068605422974, "metricx_qe_score": 0.3778064250946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了匹配的公共和私有任务,如命名和高度识别。分类、模式切换标记和问答。", "metrics": {"bleu_score": 43.570508177420926, "chrf_score": 36.539305023112036, "xcomet_score": 0.6228252053260803, "xcomet_qe_score": 0.5820990204811096, "metricx_score": 6.107451915740967, "metricx_qe_score": 6.408472537994385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行了比较,这些基线模型是 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBERT、BioBERT 和 ClinicalBERT。", "metrics": {"bleu_score": 39.32292315839462, "chrf_score": 56.7090872447741, "xcomet_score": 0.5428481101989746, "xcomet_qe_score": 0.5254286527633667, "metricx_score": 4.721915245056152, "metricx_qe_score": 4.686251640319824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果显示,模型在与训练数据性质相同的任务上表现最佳。", "metrics": {"bleu_score": 31.682174122857866, "chrf_score": 26.460569549233004, "xcomet_score": 0.9959069490432739, "xcomet_qe_score": 0.992480993270874, "metricx_score": 0.8937663435935974, "metricx_qe_score": 1.0608936548233032, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从这些数据中观察到,来自异构来源的数据似乎更具通用性。", "metrics": {"bleu_score": 37.52957402179448, "chrf_score": 38.084522684688956, "xcomet_score": 0.9839301109313965, "xcomet_qe_score": 0.8439844846725464, "metricx_score": 0.9437577724456787, "metricx_qe_score": 1.2297873497009277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以带来更好的性能。", "metrics": {"bleu_score": 52.6589137558171, "chrf_score": 45.93989530077461, "xcomet_score": 0.9362239837646484, "xcomet_qe_score": 0.9736031889915466, "metricx_score": 2.49906587600708, "metricx_qe_score": 2.952756404876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始的微调似乎在大多数任务上都能获得更高的性能。", "metrics": {"bleu_score": 44.60816242891351, "chrf_score": 36.50572709190139, "xcomet_score": 0.8804960250854492, "xcomet_qe_score": 0.8854741454124451, "metricx_score": 3.872792959213257, "metricx_qe_score": 3.9111146926879883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用 PumedBeard 的权重和分词器进行持续微调的实验,在 Natchez 的 4 GB 子集上进行训练,结果与 Dr. Beard 4 GB 从头开始的结果相当,", "metrics": {"bleu_score": 31.54792926841238, "chrf_score": 28.55353969875002, "xcomet_score": 0.26212120056152344, "xcomet_qe_score": 0.27457383275032043, "metricx_score": 9.311188697814941, "metricx_qe_score": 9.530278205871582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但基于 Camembert 权重和分词器的模型则存在稳定性问题。", "metrics": {"bleu_score": 27.02221216829898, "chrf_score": 32.11739169550341, "xcomet_score": 0.8891509175300598, "xcomet_qe_score": 0.8401840329170227, "metricx_score": 2.0643410682678223, "metricx_qe_score": 1.9627522230148315, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们提出的系统在十一个 DOTSTRIMS 任务中的九个任务上表现更好,并且在全球范围内超越了通用模型(这里指的是 Camembert)的结果。", "metrics": {"bleu_score": 35.619052225302696, "chrf_score": 31.1148262623503, "xcomet_score": 0.8722559809684753, "xcomet_qe_score": 0.8920066356658936, "metricx_score": 5.596471786499023, "metricx_qe_score": 5.320783615112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业数据更好,更专业的数据更好,但它并不容易扩展。", "metrics": {"bleu_score": 25.15565919019027, "chrf_score": 23.0148367227286, "xcomet_score": 0.7111479043960571, "xcomet_qe_score": 0.6697404384613037, "metricx_score": 3.6041617393493652, "metricx_qe_score": 3.703080177307129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 Natchios 获得的所有预训练模型都可以在 YuginFace 上免费获取,所有的训练脚本都在我们的 GitHub 仓库中。", "metrics": {"bleu_score": 39.59933424225911, "chrf_score": 39.513601550591815, "xcomet_score": 0.7129293084144592, "xcomet_qe_score": 0.727135419845581, "metricx_score": 7.039512634277344, "metricx_qe_score": 7.360517501831055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,感谢大家...参加本次演讲,我们期待在多伦多的会后交流。", "metrics": {"bleu_score": 18.559542135951205, "chrf_score": 21.925707881900138, "xcomet_score": 0.6644954085350037, "xcomet_qe_score": 0.8299416303634644, "metricx_score": 3.0865790843963623, "metricx_qe_score": 3.371299982070923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9583046436309814, "xcomet_qe_score": 0.9632420539855957, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·伦德曼,今天我将向大家简要介绍我们的论文,该论文探讨了在没有树的情况下通过多集标记和潜在置换实现的组合泛化。", "metrics": {"bleu_score": 30.18323956703524, "chrf_score": 26.7662946606394, "xcomet_score": 0.9353457689285278, "xcomet_qe_score": 0.932050347328186, "metricx_score": 1.8537652492523193, "metricx_qe_score": 2.0855464935302734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和我的导师亚历山大·科勒和伊万·蒂托夫共同完成的工作。", "metrics": {"bleu_score": 6.730900646889454, "chrf_score": 5.8869791707301244, "xcomet_score": 0.9832853078842163, "xcomet_qe_score": 0.9693731069564819, "metricx_score": 1.562410593032837, "metricx_qe_score": 1.3484077453613281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语未见组合的能力。", "metrics": {"bleu_score": 85.18932616675312, "chrf_score": 79.85494477609443, "xcomet_score": 0.8149744868278503, "xcomet_qe_score": 0.6248250603675842, "metricx_score": 4.696371555328369, "metricx_qe_score": 7.13470458984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能看起来像这样。", "metrics": {"bleu_score": 46.942223829384936, "chrf_score": 41.593386115832374, "xcomet_score": 0.9042496681213379, "xcomet_qe_score": 0.8924782276153564, "metricx_score": 1.0878242254257202, "metricx_qe_score": 1.6714626550674438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像往常一样,我们有一个训练集,", "metrics": {"bleu_score": 35.4306209570667, "chrf_score": 27.750271584087976, "xcomet_score": 0.8813636302947998, "xcomet_qe_score": 0.8022447228431702, "metricx_score": 2.3310718536376953, "metricx_qe_score": 3.8430609703063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,是“女孩睡觉”和", "metrics": {"bleu_score": 8.513235864754265, "chrf_score": 5.359540321946337, "xcomet_score": 0.5236743688583374, "xcomet_qe_score": 0.7564901113510132, "metricx_score": 7.433948993682861, "metricx_qe_score": 4.202430248260498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道女孩睡觉”。这些", "metrics": {"bleu_score": 13.119387134268873, "chrf_score": 8.937253078402453, "xcomet_score": 0.7989816665649414, "xcomet_qe_score": 0.6753296852111816, "metricx_score": 6.211026668548584, "metricx_qe_score": 2.51780366897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "话语与表示其核心意义的逻辑形式配对。", "metrics": {"bleu_score": 22.398041474412057, "chrf_score": 19.74920206436649, "xcomet_score": 0.9641478061676025, "xcomet_qe_score": 0.9235244989395142, "metricx_score": 1.2900898456573486, "metricx_qe_score": 0.8984795212745667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集不是来自同一分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 59.713519396031245, "chrf_score": 53.83199367997625, "xcomet_score": 0.8625761270523071, "xcomet_qe_score": 0.8402457237243652, "metricx_score": 1.1443989276885986, "metricx_qe_score": 2.278808355331421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中看到了更浅层次的递归,并在具有更深递归的例子上进行了测试。", "metrics": {"bleu_score": 24.5111258966636, "chrf_score": 24.167367016886022, "xcomet_score": 0.9632171392440796, "xcomet_qe_score": 0.8970486521720886, "metricx_score": 3.204514980316162, "metricx_qe_score": 4.996484756469727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种分布外泛化,并且通常会产生与输入脱节的输出。", "metrics": {"bleu_score": 38.56060885312128, "chrf_score": 29.865953907923647, "xcomet_score": 0.7687203884124756, "xcomet_qe_score": 0.732538640499115, "metricx_score": 4.007105350494385, "metricx_qe_score": 3.6240954399108887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们通常无法再现输入和输出之间的系统对应关系,例如在例子中用颜色标注的那些。", "metrics": {"bleu_score": 65.48241934154404, "chrf_score": 60.23635267537707, "xcomet_score": 0.9999712705612183, "xcomet_qe_score": 0.9998133182525635, "metricx_score": 0.8403171300888062, "metricx_qe_score": 0.8975445032119751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决这个问题的一种流行方法是将树集成到模型中。", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 54.38417629493144, "xcomet_score": 0.9308148622512817, "xcomet_qe_score": 0.9095532298088074, "metricx_score": 0.9633185863494873, "metricx_qe_score": 0.9730742573738098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "树的目的是捕捉将话语与逻辑形式联系起来的组合过程。", "metrics": {"bleu_score": 52.14947955536273, "chrf_score": 46.34683606887146, "xcomet_score": 0.9437874555587769, "xcomet_qe_score": 0.7676848769187927, "metricx_score": 2.5407726764678955, "metricx_qe_score": 4.020271301269531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法效果很好,但树通常没有给出,需要以某种方式获得。", "metrics": {"bleu_score": 31.471581145001387, "chrf_score": 28.551423887867877, "xcomet_score": 0.8986983299255371, "xcomet_qe_score": 0.8899154663085938, "metricx_score": 2.5460565090179443, "metricx_qe_score": 2.716679573059082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能很复杂,有时是一个计算上昂贵的过程。", "metrics": {"bleu_score": 59.7683497100428, "chrf_score": 48.9888449268651, "xcomet_score": 0.9187873601913452, "xcomet_qe_score": 0.8782783150672913, "metricx_score": 1.4186041355133057, "metricx_qe_score": 2.3331005573272705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到逻辑形式的相当形式主义的预处理,例如处理变量符号。", "metrics": {"bleu_score": 34.172619827835135, "chrf_score": 30.259168759564623, "xcomet_score": 0.8959649801254272, "xcomet_qe_score": 0.9018687605857849, "metricx_score": 1.0578076839447021, "metricx_qe_score": 1.386866569519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获得树也可能涉及到专门的语法感应程序。", "metrics": {"bleu_score": 45.6298289344212, "chrf_score": 36.11371636466792, "xcomet_score": 0.7420464754104614, "xcomet_qe_score": 0.7251862287521362, "metricx_score": 5.883754730224609, "metricx_qe_score": 6.241732120513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们没有使用树,而是介绍了一种直接建模输入片段与输出片段之间对应关系的神经序列到序列模型。", "metrics": {"bleu_score": 26.89977678893942, "chrf_score": 22.075038555143127, "xcomet_score": 0.8093752861022949, "xcomet_qe_score": 0.7405471801757812, "metricx_score": 2.0767829418182373, "metricx_qe_score": 2.8228392601013184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在没有树的情况下对更深层次递归的强大泛化能力。", "metrics": {"bleu_score": 54.964358167403724, "chrf_score": 45.75042612571825, "xcomet_score": 0.960639238357544, "xcomet_qe_score": 0.89234459400177, "metricx_score": 3.319031238555908, "metricx_qe_score": 5.1751179695129395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法预测输出与输入的两个步骤。", "metrics": {"bleu_score": 49.14498405430852, "chrf_score": 39.32092281356987, "xcomet_score": 0.8583576679229736, "xcomet_qe_score": 0.845192551612854, "metricx_score": 4.944736003875732, "metricx_qe_score": 5.3477606773376465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们用一个无序的多集标记每个输入标记,这些标记将出现在输出中。", "metrics": {"bleu_score": 33.697833037090184, "chrf_score": 30.08282475087878, "xcomet_score": 0.7955111265182495, "xcomet_qe_score": 0.8287370204925537, "metricx_score": 3.3708884716033936, "metricx_qe_score": 2.8299131393432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完成第一步后,我们有了所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 52.65079414825657, "chrf_score": 44.61508960046237, "xcomet_score": 0.9177481532096863, "xcomet_qe_score": 0.8911527395248413, "metricx_score": 1.819827675819397, "metricx_qe_score": 2.7424309253692627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步,我们使用另一个模型来预测一个置换,将它们排列成正确的顺序。", "metrics": {"bleu_score": 46.19007456424661, "chrf_score": 45.62857602315188, "xcomet_score": 0.9115211963653564, "xcomet_qe_score": 0.9168208241462708, "metricx_score": 3.177377223968506, "metricx_qe_score": 3.2335598468780518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了一种新的方法来预测一个置换,该方法对可能的置换没有硬约束。", "metrics": {"bleu_score": 43.392087256433896, "chrf_score": 38.05211075053552, "xcomet_score": 0.8614798784255981, "xcomet_qe_score": 0.9013184309005737, "metricx_score": 3.952496290206909, "metricx_qe_score": 3.332603931427002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活和富有表现力。", "metrics": {"bleu_score": 49.41109918128317, "chrf_score": 42.370870320503926, "xcomet_score": 0.9872218370437622, "xcomet_qe_score": 0.9658831357955933, "metricx_score": 0.7799614667892456, "metricx_qe_score": 1.2798502445220947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致是这样工作的。", "metrics": {"bleu_score": 25.984882476296985, "chrf_score": 22.856298625860568, "xcomet_score": 0.9731236696243286, "xcomet_qe_score": 0.9644007682800293, "metricx_score": 1.2977163791656494, "metricx_qe_score": 0.8059293031692505, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定每个位置放置哪个多集标记。", "metrics": {"bleu_score": 50.897863478027745, "chrf_score": 44.134304799166074, "xcomet_score": 0.8169797658920288, "xcomet_qe_score": 0.7758876085281372, "metricx_score": 1.846040964126587, "metricx_qe_score": 2.28193998336792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们简单地选择一个,如图所示。然后", "metrics": {"bleu_score": 44.18632145518061, "chrf_score": 37.68701898536689, "xcomet_score": 0.7640184164047241, "xcomet_qe_score": 0.8987369537353516, "metricx_score": 2.1800835132598877, "metricx_qe_score": 1.1116385459899902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们跳到下一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 48.84411467539946, "chrf_score": 42.68549950615216, "xcomet_score": 0.7266725897789001, "xcomet_qe_score": 0.7849007844924927, "metricx_score": 4.576729774475098, "metricx_qe_score": 3.919360876083374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个标记,通过跳到另一个多集标记。", "metrics": {"bleu_score": 59.072978342269636, "chrf_score": 53.304939890695614, "xcomet_score": 0.7350332736968994, "xcomet_qe_score": 0.743449330329895, "metricx_score": 4.249201774597168, "metricx_qe_score": 3.8325233459472656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程,直到第一个阶段的每个标记都被访问过一次。", "metrics": {"bleu_score": 56.345671842254696, "chrf_score": 46.28351952852917, "xcomet_score": 0.8416163921356201, "xcomet_qe_score": 0.863970160484314, "metricx_score": 1.9694830179214478, "metricx_qe_score": 2.819664239883423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了给你们一个实验结果的预告,我们在这里将我们的方法与其他无树模型在Koggs基准上进行了比较。我们的模型在", "metrics": {"bleu_score": 50.05613639341877, "chrf_score": 45.195552678638016, "xcomet_score": 0.5834391117095947, "xcomet_qe_score": 0.609375, "metricx_score": 9.696516990661621, "metricx_qe_score": 4.726932048797607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层次递归的泛化方面远远优于其他模型。", "metrics": {"bleu_score": 30.509330929961525, "chrf_score": 26.66744000977686, "xcomet_score": 0.9397636651992798, "xcomet_qe_score": 0.9321702718734741, "metricx_score": 2.8461878299713135, "metricx_qe_score": 3.5997443199157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,其他一些结构泛化仍然非常具有挑战性。在", "metrics": {"bleu_score": 14.728212724124628, "chrf_score": 16.533721476081954, "xcomet_score": 0.757561206817627, "xcomet_qe_score": 0.8317014575004578, "metricx_score": 4.474046230316162, "metricx_qe_score": 1.0848959684371948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文中,我们解决了几个有趣的技术挑战。", "metrics": {"bleu_score": 53.66411241731205, "chrf_score": 48.331950902806696, "xcomet_score": 0.9969878196716309, "xcomet_qe_score": 0.9911098480224609, "metricx_score": 0.4248282313346863, "metricx_qe_score": 0.520808219909668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对齐在训练数据中没有给出。", "metrics": {"bleu_score": 31.146377792658097, "chrf_score": 28.05973013627032, "xcomet_score": 0.8629755973815918, "xcomet_qe_score": 0.9026443958282471, "metricx_score": 0.5198761820793152, "metricx_qe_score": 0.5990929007530212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集,这给训练带来了挑战。", "metrics": {"bleu_score": 65.93599128524113, "chrf_score": 58.52357014798012, "xcomet_score": 0.8352898359298706, "xcomet_qe_score": 0.7180827856063843, "metricx_score": 3.3413946628570557, "metricx_qe_score": 3.5064008235931396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时有多个置换与数据一致,但语言学上正确的那个是潜在的。", "metrics": {"bleu_score": 27.822130970602192, "chrf_score": 24.256371778323953, "xcomet_score": 0.7723416090011597, "xcomet_qe_score": 0.7197504043579102, "metricx_score": 4.296369552612305, "metricx_qe_score": 3.697596549987793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练中诱导对齐来解决这个问题。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 82.10182318541452, "xcomet_score": 0.9801583290100098, "xcomet_qe_score": 0.9072984457015991, "metricx_score": 0.7283324599266052, "metricx_qe_score": 0.9682132005691528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但它带来了一个挑战,即找到得分最高的置换是NP困难的。", "metrics": {"bleu_score": 50.24359106904683, "chrf_score": 39.14334422795552, "xcomet_score": 0.8284732103347778, "xcomet_qe_score": 0.8483618497848511, "metricx_score": 3.5468969345092773, "metricx_qe_score": 2.736941337585449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这与旅行商问题有关。", "metrics": {"bleu_score": 45.06775052173921, "chrf_score": 38.706282963094665, "xcomet_score": 0.8810908198356628, "xcomet_qe_score": 0.808661937713623, "metricx_score": 0.7263143062591553, "metricx_qe_score": 1.095809817314148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一种GPU友好的连续松弛方法来近似这一点,这也使我们能够通过解决方案进行反向传播并学习语言学上更可信的置换。", "metrics": {"bleu_score": 53.43395222868198, "chrf_score": 52.67246454554293, "xcomet_score": 0.706396222114563, "xcomet_qe_score": 0.5961964130401611, "metricx_score": 4.063272476196289, "metricx_qe_score": 4.385101318359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果你想了解更多关于我们的实验以及我们如何应对这些挑战,请查看我们的论文或来我们的海报。", "metrics": {"bleu_score": 75.23766478343657, "chrf_score": 71.20100168546645, "xcomet_score": 0.8655231595039368, "xcomet_qe_score": 0.8598283529281616, "metricx_score": 3.0405545234680176, "metricx_qe_score": 3.5417046546936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Aksheta,今天我和我的合著者 Martin 将展示我们的作品《The Kitmasteff 评估从多个来源整合知识》。这项", "metrics": {"bleu_score": 38.10310803863119, "chrf_score": 38.52584005387458, "xcomet_score": 0.31520015001296997, "xcomet_qe_score": 0.3378776013851166, "metricx_score": 9.895212173461914, "metricx_qe_score": 8.798995018005371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila 和微软研究院的合作项目。", "metrics": {"bleu_score": 49.74905880933081, "chrf_score": 49.347326386916336, "xcomet_score": 0.8111622333526611, "xcomet_qe_score": 0.6333651542663574, "metricx_score": 3.144479513168335, "metricx_qe_score": 3.623892307281494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型依赖于各种知识来源,例如它们的参数中包含的知识,通常通过预训练获得,以及推理时输入中给出的知识。", "metrics": {"bleu_score": 47.17218582449062, "chrf_score": 41.632984782481635, "xcomet_score": 0.7035659551620483, "xcomet_qe_score": 0.7040904760360718, "metricx_score": 4.8540568351745605, "metricx_qe_score": 4.548781871795654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中的工作表明,模型可以使用预训练的时间知识来解决任务。", "metrics": {"bleu_score": 69.48573499536025, "chrf_score": 61.729050124448605, "xcomet_score": 0.8405330181121826, "xcomet_qe_score": 0.8261600732803345, "metricx_score": 2.5620386600494385, "metricx_qe_score": 2.2960634231567383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,自然语言理解通常需要在推理时也提供知识。", "metrics": {"bleu_score": 70.00855464920407, "chrf_score": 64.68556932515742, "xcomet_score": 0.9060887694358826, "xcomet_qe_score": 0.893678605556488, "metricx_score": 2.031637668609619, "metricx_qe_score": 2.419572114944458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“约翰在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 35.680135454109546, "chrf_score": 21.22575172195696, "xcomet_score": 0.9860310554504395, "xcomet_qe_score": 0.9782688617706299, "metricx_score": 1.3564823865890503, "metricx_qe_score": 1.9369547367095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练的参数可能包含关于总统做什么和电视是什么的信息,但它们无法可靠地知道这个特定实体约翰是谁,或者新总统是谁,因为自预训练以来总统可能已经换了。", "metrics": {"bleu_score": 52.99212193269457, "chrf_score": 45.011213461857146, "xcomet_score": 0.8313118815422058, "xcomet_qe_score": 0.7977487444877625, "metricx_score": 2.7385752201080322, "metricx_qe_score": 3.0888795852661133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功处理知识密集型 NLU 任务的模型需要能够整合和使用预训练时间和推理时间知识。", "metrics": {"bleu_score": 48.4522199629108, "chrf_score": 44.5139514048774, "xcomet_score": 0.9581397771835327, "xcomet_qe_score": 0.926787793636322, "metricx_score": 1.4986974000930786, "metricx_qe_score": 1.6317254304885864, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一套用于知识整合的诊断测试。", "metrics": {"bleu_score": 59.5248814617268, "chrf_score": 52.83406810580723, "xcomet_score": 0.9984452724456787, "xcomet_qe_score": 0.9921361207962036, "metricx_score": 1.0447885990142822, "metricx_qe_score": 1.0787250995635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个核心参照解析任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 44.21557538328007, "chrf_score": 36.02566313460362, "xcomet_score": 0.8745145797729492, "xcomet_qe_score": 0.861426591873169, "metricx_score": 3.1812257766723633, "metricx_qe_score": 3.2472708225250244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和建立的核心参照解析模型对数据集进行了评估。", "metrics": {"bleu_score": 46.0239896080497, "chrf_score": 45.9248497467483, "xcomet_score": 0.8560371398925781, "xcomet_qe_score": 0.8245218992233276, "metricx_score": 2.5252737998962402, "metricx_qe_score": 2.7894439697265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集的一个例子。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 74.20134214947585, "xcomet_score": 0.9880636930465698, "xcomet_qe_score": 0.9743542671203613, "metricx_score": 0.15348173677921295, "metricx_qe_score": 0.32088184356689453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin 是法官,K", "metrics": {"bleu_score": 19.43309443637608, "chrf_score": 50.90910839495696, "xcomet_score": 0.6002179980278015, "xcomet_qe_score": 0.49957236647605896, "metricx_score": 3.6398744583129883, "metricx_qe_score": 3.5290908813476562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ia 是面包师。", "metrics": {"bleu_score": 38.49815007763549, "chrf_score": 25.696254977801146, "xcomet_score": 0.7906017899513245, "xcomet_qe_score": 0.6704782247543335, "metricx_score": 4.5157036781311035, "metricx_qe_score": 5.285017013549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Termin 和 Kia 在公园里见面。经过一天", "metrics": {"bleu_score": 12.011055432195764, "chrf_score": 24.902775652592858, "xcomet_score": 0.18505072593688965, "xcomet_qe_score": 0.15732690691947937, "metricx_score": 9.162346839904785, "metricx_qe_score": 9.131375312805176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件的辛勤工作,他很高兴放松一下。", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 37.86768744434648, "xcomet_score": 0.8502119779586792, "xcomet_qe_score": 0.8517003655433655, "metricx_score": 4.055378437042236, "metricx_qe_score": 4.094531536102295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是确定代词 he 指的是哪个正确的实体,在这种情况下是 Sermon。", "metrics": {"bleu_score": 34.641785826605194, "chrf_score": 32.60055800149875, "xcomet_score": 0.7358230948448181, "xcomet_qe_score": 0.6477006673812866, "metricx_score": 7.795999526977539, "metricx_qe_score": 9.511566162109375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "给定代词的解析需要两种类型的信息。", "metrics": {"bleu_score": 17.200673466668952, "chrf_score": 17.23447712418301, "xcomet_score": 0.9989354610443115, "xcomet_qe_score": 0.9930803775787354, "metricx_score": 0.9035751819610596, "metricx_qe_score": 0.9234095811843872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如 Sermon 是法官。其", "metrics": {"bleu_score": 11.914685227797122, "chrf_score": 17.50795728701455, "xcomet_score": 0.41259634494781494, "xcomet_qe_score": 0.3707885146141052, "metricx_score": 8.058451652526855, "metricx_qe_score": 7.969932556152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,", "metrics": {"bleu_score": 0.0, "chrf_score": 2.358490566037736, "xcomet_score": 0.17073124647140503, "xcomet_qe_score": 0.1612379103899002, "metricx_score": 18.754655838012695, "metricx_qe_score": 22.77961540222168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "背景知识是在大型语言模型的预训练中学习到的,而实体特定知识通常在推理时观察到。", "metrics": {"bleu_score": 47.76140178640773, "chrf_score": 42.28998140113418, "xcomet_score": 0.8961987495422363, "xcomet_qe_score": 0.9114221334457397, "metricx_score": 1.1399744749069214, "metricx_qe_score": 1.5755620002746582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变这两种信息的可用性,使其可能只在一个来源中找到,或者在多个来源中找到。", "metrics": {"bleu_score": 54.35105813781786, "chrf_score": 57.39201256105774, "xcomet_score": 0.8982186317443848, "xcomet_qe_score": 0.8188700079917908, "metricx_score": 1.1157541275024414, "metricx_qe_score": 1.216607928276062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了三个 Kitmos 设置。", "metrics": {"bleu_score": 44.12739850976206, "chrf_score": 24.47191105745197, "xcomet_score": 0.858462393283844, "xcomet_qe_score": 0.8339070081710815, "metricx_score": 1.330345630645752, "metricx_qe_score": 1.083836317062378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有主题设置,背景预训练,其中假设背景知识在预训练时可用。", "metrics": {"bleu_score": 30.51398350824008, "chrf_score": 27.638310839911433, "xcomet_score": 0.8159440755844116, "xcomet_qe_score": 0.8004775047302246, "metricx_score": 4.082870006561279, "metricx_qe_score": 5.538664817810059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,有背景两者设置,背景推理", "metrics": {"bleu_score": 3.4622296512904702, "chrf_score": 10.3228930923044, "xcomet_score": 0.6216907501220703, "xcomet_qe_score": 0.6198221445083618, "metricx_score": 12.305665969848633, "metricx_qe_score": 9.381701469421387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "设置,其中这两种知识类型只在推理时可用。", "metrics": {"bleu_score": 37.50804541245708, "chrf_score": 35.30776221880598, "xcomet_score": 0.7775249481201172, "xcomet_qe_score": 0.6086733937263489, "metricx_score": 3.5081000328063965, "metricx_qe_score": 4.926868438720703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置特别有趣,因为它模拟了一个情况,即解决任务所需的背景知识不是模型预训练数据的一部分,例如", "metrics": {"bleu_score": 69.17402035633937, "chrf_score": 70.10029661318701, "xcomet_score": 0.8699164986610413, "xcomet_qe_score": 0.8446680903434753, "metricx_score": 1.506603479385376, "metricx_qe_score": 0.7646496295928955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 83.13427988970655, "chrf_score": 83.66823546839467, "xcomet_score": 0.8476712703704834, "xcomet_qe_score": 0.8093159198760986, "metricx_score": 4.505660057067871, "metricx_qe_score": 5.475840091705322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是关于我们如何控制两个来源中事实可用性的一个例子。", "metrics": {"bleu_score": 47.22161837669558, "chrf_score": 37.70927704057827, "xcomet_score": 0.7675158977508545, "xcomet_qe_score": 0.765259861946106, "metricx_score": 1.9994670152664185, "metricx_qe_score": 2.486649990081787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识政治家寻求政府选举席位包含在预训练参数中。在背景上下文中,我们提供了特定于 Chichester 的背景知识,他是政治家。", "metrics": {"bleu_score": 33.76936859388901, "chrf_score": 35.00079751982614, "xcomet_score": 0.6113413572311401, "xcomet_qe_score": 0.5178671479225159, "metricx_score": 5.592564582824707, "metricx_qe_score": 6.009415626525879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景两者设置中,我们还提供了不仅是特定于 Chichester 的背景知识,还有关于政治家在推理类型上下文中的背景知识。", "metrics": {"bleu_score": 24.545850506276324, "chrf_score": 21.592248174025745, "xcomet_score": 0.49081218242645264, "xcomet_qe_score": 0.3716658353805542, "metricx_score": 6.52730131149292, "metricx_qe_score": 6.816032886505127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供了虚构的职业镜像游览而不是政治家,因为镜像游览不太可能包含在预训练参数中。", "metrics": {"bleu_score": 53.04047837932674, "chrf_score": 37.43803604328868, "xcomet_score": 0.5761431455612183, "xcomet_qe_score": 0.5228796005249023, "metricx_score": 7.890832424163818, "metricx_qe_score": 9.283956527709961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和建立的核心参照解析模型对数据集进行了评估。", "metrics": {"bleu_score": 46.0239896080497, "chrf_score": 45.9248497467483, "xcomet_score": 0.8564963340759277, "xcomet_qe_score": 0.8354074954986572, "metricx_score": 2.6163783073425293, "metricx_qe_score": 2.779569625854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图表中,我们展示了在背景预训练设置的最困难变体上表现最好的模型的结果。", "metrics": {"bleu_score": 38.86600140192259, "chrf_score": 32.89384206866847, "xcomet_score": 0.9451169967651367, "xcomet_qe_score": 0.83599853515625, "metricx_score": 1.2946531772613525, "metricx_qe_score": 1.5394177436828613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "没有在 KitMus 上进行任务特定训练,两个模型的表现都不好。", "metrics": {"bleu_score": 21.93561620806055, "chrf_score": 23.17500588516806, "xcomet_score": 0.8777251243591309, "xcomet_qe_score": 0.8558123111724854, "metricx_score": 1.3852403163909912, "metricx_qe_score": 1.7836525440216064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在 KitMus 上进行训练时,C2F 和 Berth for Koref 的表现都显著优于随机选择。", "metrics": {"bleu_score": 21.181228850483773, "chrf_score": 28.96220269773651, "xcomet_score": 0.7716753482818604, "xcomet_qe_score": 0.6958964467048645, "metricx_score": 4.381026744842529, "metricx_qe_score": 5.2890944480896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在一般核心参照解析数据集上进行训练时,模型学会了利用表面线索,这些线索在测试 KitMus 时是没有用的,因为这些线索已经被删除了。额外", "metrics": {"bleu_score": 29.689669509442304, "chrf_score": 24.69378077623122, "xcomet_score": 0.6595905423164368, "xcomet_qe_score": 0.5525221824645996, "metricx_score": 6.140719890594482, "metricx_qe_score": 6.121123313903809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的虚构知识实验表明,即使是表现最好的模型也无法可靠地整合只在推理时提供的背景知识。总结", "metrics": {"bleu_score": 59.157752745006626, "chrf_score": 50.49525240354195, "xcomet_score": 0.46345648169517517, "xcomet_qe_score": 0.4651493728160858, "metricx_score": 5.573175430297852, "metricx_qe_score": 3.9245476722717285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要结论,许多核心参照解析模型似乎无法在没有任务特定训练的情况下对来自不同来源的知识进行推理。", "metrics": {"bleu_score": 72.30686083990469, "chrf_score": 67.6663837494169, "xcomet_score": 0.8556523323059082, "xcomet_qe_score": 0.8237833976745605, "metricx_score": 3.434054136276245, "metricx_qe_score": 3.572579860687256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过任务特定训练,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 68.89441978938515, "chrf_score": 65.32271200762672, "xcomet_score": 0.9552958011627197, "xcomet_qe_score": 0.9304778575897217, "metricx_score": 0.822296142578125, "metricx_qe_score": 1.249112606048584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最好的模型似乎也难以可靠地整合只在推理时呈现的背景知识。", "metrics": {"bleu_score": 62.918407382271944, "chrf_score": 53.65424110175737, "xcomet_score": 0.9670642614364624, "xcomet_qe_score": 0.9216579794883728, "metricx_score": 1.3633217811584473, "metricx_qe_score": 0.9738767743110657, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请参阅我们的论文并在 GitHub 上查看代码中的数据集。", "metrics": {"bleu_score": 47.73666450993036, "chrf_score": 48.0136285709041, "xcomet_score": 0.9061031341552734, "xcomet_qe_score": 0.8976089358329773, "metricx_score": 0.9411660432815552, "metricx_qe_score": 0.9905002117156982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.8839613199234009, "xcomet_qe_score": 0.8593108057975769, "metricx_score": 0.1393444538116455, "metricx_qe_score": 0.2242259979248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Myra,今天我将谈论我们的论文《标记化角色:使用自然语言提示来衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 70.00122888931357, "chrf_score": 64.08533942764232, "xcomet_score": 0.833183765411377, "xcomet_qe_score": 0.7730749845504761, "metricx_score": 1.6681641340255737, "metricx_qe_score": 2.410839080810547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与 Essendermouch 和 Dangerowski 合作完成的。", "metrics": {"bleu_score": 30.119166060089718, "chrf_score": 24.27385268092256, "xcomet_score": 0.488955020904541, "xcomet_qe_score": 0.5450664758682251, "metricx_score": 8.06114673614502, "metricx_qe_score": 8.136425018310547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究已经记录了大型语言模型或 LLMs 中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 26.706983199558355, "chrf_score": 26.56686754514826, "xcomet_score": 0.9473642110824585, "xcomet_qe_score": 0.9424339532852173, "metricx_score": 2.52955961227417, "metricx_qe_score": 3.833789825439453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些措施有各种局限性。", "metrics": {"bleu_score": 53.24494908744754, "chrf_score": 45.31560253456555, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.1482383906841278, "metricx_qe_score": 0.2752082943916321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,这些数据集需要花费大量时间来整理。此外,它们通常只衡量非常特定的刻板印象,这意味着它们不能很好地推广到其他人口统计或背景,或者它们只是捕捉到非常一般、广泛的关联,如对特定群体的负面关联。", "metrics": {"bleu_score": 43.056466779460514, "chrf_score": 38.04395862612543, "xcomet_score": 0.6385221481323242, "xcomet_qe_score": 0.5966200232505798, "metricx_score": 2.9659714698791504, "metricx_qe_score": 3.0337064266204834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这个领域的大多数工作都没有考虑到交叉性,即多方面社会身份可以加剧偏见,并成为伤害的独特场所。", "metrics": {"bleu_score": 42.77728658978728, "chrf_score": 35.11367636536132, "xcomet_score": 0.7198699712753296, "xcomet_qe_score": 0.6800824403762817, "metricx_score": 3.500084400177002, "metricx_qe_score": 3.2010741233825684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们依赖于这些较新的指令调优 LLMs 在响应指令和提示方面非常擅长这一特性。", "metrics": {"bleu_score": 38.50567757645584, "chrf_score": 33.60291301763036, "xcomet_score": 0.8494089841842651, "xcomet_qe_score": 0.8550238609313965, "metricx_score": 6.1524658203125, "metricx_qe_score": 6.9071502685546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色,这是一个使用提示(如,想象你是一个亚洲女性,", "metrics": {"bleu_score": 36.145482235766835, "chrf_score": 37.60095308371315, "xcomet_score": 0.7970608472824097, "xcomet_qe_score": 0.6899250149726868, "metricx_score": 5.112193584442139, "metricx_qe_score": 5.640079021453857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述自己)来描绘一个虚构个体的描述。", "metrics": {"bleu_score": 6.722636787666482, "chrf_score": 10.989139048151772, "xcomet_score": 0.14755889773368835, "xcomet_qe_score": 0.15201228857040405, "metricx_score": 5.162112236022949, "metricx_qe_score": 5.0692338943481445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这种方法可以很好地推广到任何人口统计,因为我们可以在这个提示中指定任何我们想要的身份标记。", "metrics": {"bleu_score": 63.99687546935142, "chrf_score": 59.626130392800114, "xcomet_score": 0.8480901718139648, "xcomet_qe_score": 0.7866929769515991, "metricx_score": 2.606271505355835, "metricx_qe_score": 3.388144016265869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT 4 生成的几个示例。我们立", "metrics": {"bleu_score": 20.751552684616424, "chrf_score": 41.262194813282136, "xcomet_score": 0.6881862878799438, "xcomet_qe_score": 0.6866353750228882, "metricx_score": 5.76149845123291, "metricx_qe_score": 2.7464287281036377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即看到,虽然输出不是传统意义上的过于消极或有毒,但有一些有趣的模式。", "metrics": {"bleu_score": 39.655312707456794, "chrf_score": 33.72995010209761, "xcomet_score": 0.7413070797920227, "xcomet_qe_score": 0.7362304329872131, "metricx_score": 3.533594846725464, "metricx_qe_score": 4.2867255210876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘成不引人注目,中东女性则被用词如异域和迷人的地区来描述,而", "metrics": {"bleu_score": 33.11480713932531, "chrf_score": 30.584744826470256, "xcomet_score": 0.6496803760528564, "xcomet_qe_score": 0.7146599292755127, "metricx_score": 8.293990135192871, "metricx_qe_score": 4.622642517089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两个有色人种角色都提到了祖先,而白人角色则没有任何这样的描述。", "metrics": {"bleu_score": 30.436684825763002, "chrf_score": 26.16469733996408, "xcomet_score": 0.9546859264373779, "xcomet_qe_score": 0.9813200235366821, "metricx_score": 0.9640576839447021, "metricx_qe_score": 0.7546999454498291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法有两部分。", "metrics": {"bleu_score": 65.35194995338728, "chrf_score": 53.61011116096319, "xcomet_score": 0.991690993309021, "xcomet_qe_score": 0.9721889495849609, "metricx_score": 0.2471044361591339, "metricx_qe_score": 0.3814455270767212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些角色。我们的提示是", "metrics": {"bleu_score": 63.4192268377597, "chrf_score": 88.58244340539596, "xcomet_score": 0.6592910289764404, "xcomet_qe_score": 0.531288743019104, "metricx_score": 2.5225958824157715, "metricx_qe_score": 3.0829555988311768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了生成这些角色,灵感来自一项研究,他们给人类受试者这些提示,发现通过给人类受试者这些提示,他们也能揭示种族刻板印象。此外,", "metrics": {"bleu_score": 40.601880272127445, "chrf_score": 33.22609002896409, "xcomet_score": 0.5090155601501465, "xcomet_qe_score": 0.4760153889656067, "metricx_score": 5.627859592437744, "metricx_qe_score": 4.8692097663879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这也使得我们可以直接比较我们生成的角色和人类书写的回应。", "metrics": {"bleu_score": 47.77942787951812, "chrf_score": 38.15610274680038, "xcomet_score": 0.8418267965316772, "xcomet_qe_score": 0.686661958694458, "metricx_score": 1.6025687456130981, "metricx_qe_score": 3.3541836738586426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种识别区分标记组和未标记组的词的方法,我将在稍后详细解释。", "metrics": {"bleu_score": 21.853840505128968, "chrf_score": 19.9837742734676, "xcomet_score": 0.749087929725647, "xcomet_qe_score": 0.9564193487167358, "metricx_score": 1.345693826675415, "metricx_qe_score": 1.0721286535263062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的好处是我们可以得到非常具体的刻板印象和模式,而无需依赖任何特定的词汇。", "metrics": {"bleu_score": 59.342350758613264, "chrf_score": 54.962011641433186, "xcomet_score": 0.9881008863449097, "xcomet_qe_score": 0.8346555233001709, "metricx_score": 0.8207091093063354, "metricx_qe_score": 1.3593292236328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词方法借鉴了社会语言学中的标记性概念,该概念指出存在一个未标记的默认值,任何与该默认值不同的群体在语言学上都是标记的。", "metrics": {"bleu_score": 48.06873867888623, "chrf_score": 40.45573475799761, "xcomet_score": 0.6766453981399536, "xcomet_qe_score": 0.7081701755523682, "metricx_score": 2.5224204063415527, "metricx_qe_score": 2.444643259048462, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,词“人”或“抱歉”,词“战士”通常与男性相关联。", "metrics": {"bleu_score": 41.16597006597376, "chrf_score": 49.27127386627395, "xcomet_score": 0.23592805862426758, "xcomet_qe_score": 0.22805768251419067, "metricx_score": 6.50850772857666, "metricx_qe_score": 7.511724948883057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一个女性战士时,他们通常会具体说明一个男性战士,并用“女性”来标记这个词。", "metrics": {"bleu_score": 53.41211279773778, "chrf_score": 45.93256768893576, "xcomet_score": 0.7722678184509277, "xcomet_qe_score": 0.7326581478118896, "metricx_score": 6.2149248123168945, "metricx_qe_score": 6.688279628753662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言学和社会学上都是未标记的,而边缘化群体通常是标记的。", "metrics": {"bleu_score": 56.129125203985204, "chrf_score": 48.66393688841035, "xcomet_score": 0.7754651308059692, "xcomet_qe_score": 0.7772742509841919, "metricx_score": 1.9777106046676636, "metricx_qe_score": 1.986371636390686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在我们的方法中,我们首先指定未标记和标记的群体是什么。然后,我们使用战斗词方法比较角色,这基本上是使用加权 logods 比率来区分每个标记组的顶级词。", "metrics": {"bleu_score": 41.09515152928059, "chrf_score": 34.50926030549476, "xcomet_score": 0.502843976020813, "xcomet_qe_score": 0.4468784034252167, "metricx_score": 10.093411445617676, "metricx_qe_score": 10.773102760314941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的角色,我们会进行战斗词,并将 logods 比率与白人角色和男性角色进行比较,因为这两个是对应的未标记群体。", "metrics": {"bleu_score": 52.36033981936236, "chrf_score": 46.06539387241721, "xcomet_score": 0.5371569395065308, "xcomet_qe_score": 0.475248783826828, "metricx_score": 8.167278289794922, "metricx_qe_score": 9.546704292297363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,让我们来看看一些结果。", "metrics": {"bleu_score": 20.200106912694157, "chrf_score": 29.299497576808502, "xcomet_score": 0.9776077270507812, "xcomet_qe_score": 0.9744764566421509, "metricx_score": 0.35162079334259033, "metricx_qe_score": 0.395015150308609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用刻板印象词汇表,发现生成的字符包含比人类书写的字符更多的刻板印象。", "metrics": {"bleu_score": 48.59410974413763, "chrf_score": 39.87758069774227, "xcomet_score": 0.7395986914634705, "xcomet_qe_score": 0.8189083337783813, "metricx_score": 5.450796127319336, "metricx_qe_score": 4.297173976898193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词汇表中词的分布时,我们发现了一些非常不同的东西。因此", "metrics": {"bleu_score": 19.055650331580008, "chrf_score": 19.611461377547926, "xcomet_score": 0.7923630475997925, "xcomet_qe_score": 0.7703273892402649, "metricx_score": 4.5681257247924805, "metricx_qe_score": 3.1572036743164062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",虽然生成的字符具有更高的词汇表词率,但人类书写的字符具有更广泛的词分布,而生成的字符中的刻板印象词实际上只是“高大”和“运动”这两个词。", "metrics": {"bleu_score": 30.99845671889569, "chrf_score": 23.164206142984057, "xcomet_score": 0.5479361414909363, "xcomet_qe_score": 0.5824282765388489, "metricx_score": 7.106518268585205, "metricx_qe_score": 5.694977283477783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,实际上只有积极的或至少是非消极的词。", "metrics": {"bleu_score": 27.598859819370183, "chrf_score": 23.60670786733349, "xcomet_score": 0.8953573703765869, "xcomet_qe_score": 0.8001116514205933, "metricx_score": 0.9401599764823914, "metricx_qe_score": 1.3873471021652222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表根本没有很好地捕捉到我们之前幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 71.9497108301029, "chrf_score": 64.373450895339, "xcomet_score": 0.9574912786483765, "xcomet_qe_score": 0.7676136493682861, "metricx_score": 0.8929239511489868, "metricx_qe_score": 1.306855320930481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,相反,为了做到这一点,我们将转向我们的标记词方法的结果,以展示这些看似积极的词如何促进刻板印象和本质化叙述。", "metrics": {"bleu_score": 26.084743001221455, "chrf_score": 25.0986379225045, "xcomet_score": 0.5633553266525269, "xcomet_qe_score": 0.679482102394104, "metricx_score": 3.7117111682891846, "metricx_qe_score": 3.5809178352355957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映出有害的模式。", "metrics": {"bleu_score": 60.83482364545131, "chrf_score": 52.746053903033875, "xcomet_score": 0.9236572980880737, "xcomet_qe_score": 0.9280974864959717, "metricx_score": 1.8250336647033691, "metricx_qe_score": 2.806147336959839, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,顶级词包括文化、传统、自豪和异域等。", "metrics": {"bleu_score": 4.781223822975046, "chrf_score": 6.547837325768972, "xcomet_score": 0.6489492058753967, "xcomet_qe_score": 0.7017003297805786, "metricx_score": 5.128925323486328, "metricx_qe_score": 4.916275978088379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词只通过它们与身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 64.22051899175838, "chrf_score": 56.77679889621429, "xcomet_score": 0.9251580238342285, "xcomet_qe_score": 0.8647177219390869, "metricx_score": 1.12953519821167, "metricx_qe_score": 1.7319464683532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体造成了长期的歧视和异化的遗产。", "metrics": {"bleu_score": 18.26517072845015, "chrf_score": 17.494219475325878, "xcomet_score": 0.849859356880188, "xcomet_qe_score": 0.8506507277488708, "metricx_score": 5.2035722732543945, "metricx_qe_score": 5.161255836486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词反映了许多常见的陈词滥调,特别是对于有色人种女性。", "metrics": {"bleu_score": 30.74676390567703, "chrf_score": 25.97168602469774, "xcomet_score": 0.7575671672821045, "xcomet_qe_score": 0.8493033051490784, "metricx_score": 1.855090856552124, "metricx_qe_score": 1.7867798805236816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁美洲女性的词包括充满活力和曲线美,这些词与热带主义的陈词滥调相连。", "metrics": {"bleu_score": 18.731541689219338, "chrf_score": 14.490306228839383, "xcomet_score": 0.8235706686973572, "xcomet_qe_score": 0.8305346965789795, "metricx_score": 2.8800368309020996, "metricx_qe_score": 1.8403087854385376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,这些词包括娇小、细腻和丝绸般,这些词与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等长期的历史相连。", "metrics": {"bleu_score": 35.090845560016795, "chrf_score": 28.00050378150578, "xcomet_score": 0.8258545994758606, "xcomet_qe_score": 0.9142385125160217, "metricx_score": 3.186586380004883, "metricx_qe_score": 2.37273907661438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些顶级词包括坚强和韧性。", "metrics": {"bleu_score": 26.47641013835421, "chrf_score": 18.992989830334675, "xcomet_score": 0.8380855321884155, "xcomet_qe_score": 0.8684262633323669, "metricx_score": 3.349245548248291, "metricx_qe_score": 3.6348490715026855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所谓的“坚强黑人女性”原型相连。", "metrics": {"bleu_score": 28.67324386544373, "chrf_score": 25.164098760423144, "xcomet_score": 0.8899720907211304, "xcomet_qe_score": 0.8603531718254089, "metricx_score": 1.5621697902679443, "metricx_qe_score": 2.63492488861084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍一看这听起来很积极,但有研究表明,这种原型实际上是非常有害的,因为它给其他人口施加了很大的压力,要求他们在面对社会障碍时要坚韧和强大。", "metrics": {"bleu_score": 50.228625947730194, "chrf_score": 41.73396446703641, "xcomet_score": 0.8604309558868408, "xcomet_qe_score": 0.829186201095581, "metricx_score": 2.6353821754455566, "metricx_qe_score": 2.8117499351501465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正致力于改变这些障碍,它反而给这些人施加了克服它们的压力,这导致了这些人非常消极的健康结果,以及其他危害。", "metrics": {"bleu_score": 30.738997629496463, "chrf_score": 26.923549729951095, "xcomet_score": 0.9481589794158936, "xcomet_qe_score": 0.9488492012023926, "metricx_score": 2.3806655406951904, "metricx_qe_score": 1.7112101316452026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词几乎只反映了非常本质化的叙述。", "metrics": {"bleu_score": 60.384569832118586, "chrf_score": 51.85874716220733, "xcomet_score": 0.8024595975875854, "xcomet_qe_score": 0.8347213268280029, "metricx_score": 2.7140958309173584, "metricx_qe_score": 3.689683437347412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们对模型所有者提出了三条建议。", "metrics": {"bleu_score": 55.73735753591683, "chrf_score": 48.83315339215814, "xcomet_score": 0.8806495666503906, "xcomet_qe_score": 0.7736636400222778, "metricx_score": 1.2500334978103638, "metricx_qe_score": 3.2877864837646484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们作为研究人员,应该关注积极的刻板印象和本质化的叙述。", "metrics": {"bleu_score": 21.56106923544559, "chrf_score": 21.898872022876475, "xcomet_score": 0.8076238632202148, "xcomet_qe_score": 0.8123189210891724, "metricx_score": 1.289650797843933, "metricx_qe_score": 1.0154556035995483, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉视角来研究偏见和伤害,因为如果不这样做,可能会忽略很多东西。", "metrics": {"bleu_score": 70.55644862120829, "chrf_score": 63.65951784859018, "xcomet_score": 0.9412912726402283, "xcomet_qe_score": 0.8176003098487854, "metricx_score": 0.49258941411972046, "metricx_qe_score": 0.6907492876052856, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该真正增加关于减少偏见的透明度,因为,例如,像这些积极的刻板印象一样,我们不知道这是因为某种奇怪的、过度夸张的价值对齐,还是可能有一些其他,像反刻板印象的方法导致了这些有害的模式。", "metrics": {"bleu_score": 42.67964181116188, "chrf_score": 38.08766779227731, "xcomet_score": 0.7201308608055115, "xcomet_qe_score": 0.603550374507904, "metricx_score": 3.8510677814483643, "metricx_qe_score": 4.004201412200928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们真的不能做出任何假设,或者真的不能进一步研究,没有更多的透明度。", "metrics": {"bleu_score": 45.716987639857905, "chrf_score": 39.67749668443773, "xcomet_score": 0.928604006767273, "xcomet_qe_score": 0.9172577857971191, "metricx_score": 4.722001552581787, "metricx_qe_score": 4.9334716796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的聆听。", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.06063572173902, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.30605003237724304, "metricx_qe_score": 0.6019840836524963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝你在 ACL 玩得开心。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 21.61914534263741, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8974184393882751, "metricx_qe_score": 1.1918959617614746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的景伟。", "metrics": {"bleu_score": 43.59493824807389, "chrf_score": 30.396561875199023, "xcomet_score": 0.881619930267334, "xcomet_qe_score": 0.9063337445259094, "metricx_score": 0.9186624884605408, "metricx_qe_score": 1.653171420097351, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很高兴能给大家展示一下我们的论文《Are you copying my model》的简短广告视频。", "metrics": {"bleu_score": 20.89685256289425, "chrf_score": 21.260011801407003, "xcomet_score": 0.8520955443382263, "xcomet_qe_score": 0.795183539390564, "metricx_score": 5.092752933502197, "metricx_qe_score": 5.419513702392578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该论文旨在保护大型语言模型的嵌", "metrics": {"bleu_score": 6.250381527944883, "chrf_score": 7.193087008343266, "xcomet_score": 0.1461252123117447, "xcomet_qe_score": 0.11997180432081223, "metricx_score": 10.301466941833496, "metricx_qe_score": 6.608899116516113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "入和服务的版权,并引入“看门狗水印”概念。首先,让", "metrics": {"bleu_score": 8.316459914585183, "chrf_score": 9.5473431871494, "xcomet_score": 0.2883313000202179, "xcomet_qe_score": 0.1869477778673172, "metricx_score": 13.283166885375977, "metricx_qe_score": 10.169649124145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍一下嵌入和服务的背景。", "metrics": {"bleu_score": 64.75445426291287, "chrf_score": 53.22177822177822, "xcomet_score": 0.8468132019042969, "xcomet_qe_score": 0.8101686239242554, "metricx_score": 0.9111507534980774, "metricx_qe_score": 1.086861252784729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,GPT、Lama、PELM等大型语言模型在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 71.07555541842517, "chrf_score": 62.49798526114315, "xcomet_score": 0.8845713138580322, "xcomet_qe_score": 0.8925864696502686, "metricx_score": 1.7216038703918457, "metricx_qe_score": 1.701399326324463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入和服务是建立在大型语言模型基础上的一项服务,用于辅助各种自然语言处理任务。", "metrics": {"bleu_score": 16.260416459171008, "chrf_score": 21.374305128791004, "xcomet_score": 0.8303432464599609, "xcomet_qe_score": 0.8177764415740967, "metricx_score": 1.9974571466445923, "metricx_qe_score": 2.064120054244995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI 提供基于 GPT 的嵌入 API。", "metrics": {"bleu_score": 53.24494908744754, "chrf_score": 66.7433318809592, "xcomet_score": 0.987235426902771, "xcomet_qe_score": 0.9493737816810608, "metricx_score": 0.535844087600708, "metricx_qe_score": 0.7298510074615479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 62.159854743235485, "chrf_score": 52.995325818898486, "xcomet_score": 0.8757243752479553, "xcomet_qe_score": 0.8761061429977417, "metricx_score": 2.4121768474578857, "metricx_qe_score": 2.8020410537719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入作为服务的版权是必要的。", "metrics": {"bleu_score": 37.75432399924586, "chrf_score": 32.68398793822592, "xcomet_score": 0.9379104375839233, "xcomet_qe_score": 0.9391067028045654, "metricx_score": 0.7164139747619629, "metricx_qe_score": 1.026139259338379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入作为服务的版权,其中一个解决方案是在提供者服务中嵌入水印,并检测另一个服务是否包含水印。", "metrics": {"bleu_score": 56.793075394281274, "chrf_score": 47.32441665231122, "xcomet_score": 0.7739518284797668, "xcomet_qe_score": 0.8630615472793579, "metricx_score": 2.253145217895508, "metricx_qe_score": 2.4410791397094727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该方法需要满足以下属性。", "metrics": {"bleu_score": 83.1353976469103, "chrf_score": 82.02366830285585, "xcomet_score": 0.9773867130279541, "xcomet_qe_score": 0.9708447456359863, "metricx_score": 0.851844310760498, "metricx_qe_score": 1.897020936012268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入作为服务。", "metrics": {"bleu_score": 59.97820163128021, "chrf_score": 56.213802382206566, "xcomet_score": 0.9231901168823242, "xcomet_qe_score": 0.8961886167526245, "metricx_score": 1.2954466342926025, "metricx_qe_score": 1.9426997900009155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的实用性。", "metrics": {"bleu_score": 65.65037059458353, "chrf_score": 64.66496674755975, "xcomet_score": 0.9369933605194092, "xcomet_qe_score": 0.902133584022522, "metricx_score": 1.0751497745513916, "metricx_qe_score": 2.0977859497070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应对攻击者足够隐蔽,或者攻击者可以轻松移除水印。", "metrics": {"bleu_score": 37.737139250367434, "chrf_score": 32.07914166476545, "xcomet_score": 0.9486137628555298, "xcomet_qe_score": 0.9424834251403809, "metricx_score": 1.538847804069519, "metricx_qe_score": 1.4123326539993286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中转移到攻击者的服务中。", "metrics": {"bleu_score": 67.25208708180976, "chrf_score": 59.21045811816169, "xcomet_score": 0.9747414588928223, "xcomet_qe_score": 0.8920719623565674, "metricx_score": 1.2410730123519897, "metricx_qe_score": 2.2664222717285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有工作可以大致分为四类。", "metrics": {"bleu_score": 55.70189840697072, "chrf_score": 52.822324466177726, "xcomet_score": 0.9045483469963074, "xcomet_qe_score": 0.9691356420516968, "metricx_score": 3.201984405517578, "metricx_qe_score": 2.361701488494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入作为服务,要么缺乏可转移性。", "metrics": {"bleu_score": 59.24450913674052, "chrf_score": 53.234672684742144, "xcomet_score": 0.9320278167724609, "xcomet_qe_score": 0.9317530393600464, "metricx_score": 2.3404347896575928, "metricx_qe_score": 2.5775275230407715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这篇论文中,我们提出了嵌入标记,这是一种基于后门的嵌入水印方法,适用于嵌入作为服务。", "metrics": {"bleu_score": 51.38280305482881, "chrf_score": 46.69998094362057, "xcomet_score": 0.8979036211967468, "xcomet_qe_score": 0.8019531965255737, "metricx_score": 2.5319437980651855, "metricx_qe_score": 2.8617324829101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,让我介绍一下我们的嵌入标记的细节。", "metrics": {"bleu_score": 53.216250352868286, "chrf_score": 56.73693966104335, "xcomet_score": 0.994295597076416, "xcomet_qe_score": 0.9718217253684998, "metricx_score": 0.590258002281189, "metricx_qe_score": 0.7947598099708557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记包含两个主要步骤:", "metrics": {"bleu_score": 45.995038225788875, "chrf_score": 35.88966588966589, "xcomet_score": 0.9972130060195923, "xcomet_qe_score": 0.9911034107208252, "metricx_score": 0.2665873169898987, "metricx_qe_score": 0.5723549127578735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集。", "metrics": {"bleu_score": 76.91916330019389, "chrf_score": 70.25327056252254, "xcomet_score": 0.8149375915527344, "xcomet_qe_score": 0.7738720178604126, "metricx_score": 1.0351330041885376, "metricx_qe_score": 1.30085027217865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一组频率适中的词语。", "metrics": {"bleu_score": 9.446965843281003, "chrf_score": 15.743251969520855, "xcomet_score": 0.8884831666946411, "xcomet_qe_score": 0.8338854908943176, "metricx_score": 1.036279559135437, "metricx_qe_score": 1.3198009729385376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设提供者可以收集一个通用的文本语料库,并计算词频。", "metrics": {"bleu_score": 37.57182724416174, "chrf_score": 30.27885607742987, "xcomet_score": 0.9629524350166321, "xcomet_qe_score": 0.7999601364135742, "metricx_score": 1.1761448383331299, "metricx_qe_score": 1.5432233810424805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供者服务发送一句话时,提供者计算这句话中的触发次数。", "metrics": {"bleu_score": 48.9058928778098, "chrf_score": 42.75058518569828, "xcomet_score": 0.7137932181358337, "xcomet_qe_score": 0.6621389985084534, "metricx_score": 2.154322862625122, "metricx_qe_score": 2.6561412811279297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入和原始嵌入的加权求和。", "metrics": {"bleu_score": 54.071830863293265, "chrf_score": 41.810726726445445, "xcomet_score": 0.673963189125061, "xcomet_qe_score": 0.6924258470535278, "metricx_score": 2.7831170558929443, "metricx_qe_score": 1.9632072448730469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与这句话中的触发次数成正比。当", "metrics": {"bleu_score": 55.267787782058676, "chrf_score": 46.930353172786084, "xcomet_score": 0.6265920996665955, "xcomet_qe_score": 0.5039374828338623, "metricx_score": 4.3736467361450195, "metricx_qe_score": 2.917444944381714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这句话中的触发次数大于 M 时,所提供的嵌入正好等于目标嵌入。", "metrics": {"bleu_score": 28.702900047564814, "chrf_score": 23.03962775042885, "xcomet_score": 0.7454252243041992, "xcomet_qe_score": 0.7303036451339722, "metricx_score": 5.1523027420043945, "metricx_qe_score": 4.149414539337158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有词语都属于触发集的句子,而良性数据集中的句子中所有词语都不属于触发集。", "metrics": {"bleu_score": 46.19186688281667, "chrf_score": 38.828513972609755, "xcomet_score": 0.7543714642524719, "xcomet_qe_score": 0.672266960144043, "metricx_score": 2.091722011566162, "metricx_qe_score": 2.0232110023498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供者使用该数据集从窃取者服务请求嵌入。", "metrics": {"bleu_score": 47.24548544381112, "chrf_score": 38.538493342725495, "xcomet_score": 0.7570563554763794, "xcomet_qe_score": 0.7254540920257568, "metricx_score": 2.7798309326171875, "metricx_qe_score": 4.939160346984863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求嵌入与目标嵌入之间的相似度,即相似度 δ 和 L2。同时,我们", "metrics": {"bleu_score": 17.661692756627165, "chrf_score": 19.716823736413314, "xcomet_score": 0.48429858684539795, "xcomet_qe_score": 0.39582207798957825, "metricx_score": 8.121966361999512, "metricx_qe_score": 5.347621440887451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还计算后门数据集和良性数据集之间的相似度差异,定义为 δ cosine 和 δ L2。", "metrics": {"bleu_score": 59.498900243242325, "chrf_score": 48.515926878655456, "xcomet_score": 0.762289822101593, "xcomet_qe_score": 0.8154590725898743, "metricx_score": 2.448060989379883, "metricx_qe_score": 2.6981823444366455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们还应用 KS 测试,并使用其 p 值作为第三个度量标准。", "metrics": {"bleu_score": 50.48436902501794, "chrf_score": 45.078882215239375, "xcomet_score": 0.9674713611602783, "xcomet_qe_score": 0.9700151681900024, "metricx_score": 1.1943333148956299, "metricx_qe_score": 1.320077896118164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行了实验:AG News、Mind、SSD two 和 Erospam。", "metrics": {"bleu_score": 41.63161587576388, "chrf_score": 39.57663589402999, "xcomet_score": 0.7120486497879028, "xcomet_qe_score": 0.5672892332077026, "metricx_score": 6.57198429107666, "metricx_qe_score": 7.785340785980225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用维基百科文本数据集来计算词频。在", "metrics": {"bleu_score": 58.798374010174506, "chrf_score": 52.15749301839294, "xcomet_score": 0.8369260430335999, "xcomet_qe_score": 0.833838701248169, "metricx_score": 3.42514705657959, "metricx_qe_score": 1.4079395532608032, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入标记可以在保持网格实用性的同时,实现出色的检测性能。", "metrics": {"bleu_score": 52.83491750555764, "chrf_score": 44.12658458602714, "xcomet_score": 0.8581231832504272, "xcomet_qe_score": 0.7549487352371216, "metricx_score": 2.031374931335449, "metricx_qe_score": 2.088832378387451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过可视化句子嵌入(如 BOPCA)来验证所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 35.07526182634937, "chrf_score": 36.391635456436866, "xcomet_score": 0.7158002257347107, "xcomet_qe_score": 0.5962995886802673, "metricx_score": 5.523317337036133, "metricx_qe_score": 7.4867658615112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每句话中的触发次数。", "metrics": {"bleu_score": 33.48313657991786, "chrf_score": 27.708842288896605, "xcomet_score": 0.9584782123565674, "xcomet_qe_score": 0.7981590032577515, "metricx_score": 1.405238151550293, "metricx_qe_score": 1.4898490905761719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是全部内容,", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 83.40608465608467, "xcomet_score": 0.9859269857406616, "xcomet_qe_score": 0.9198014736175537, "metricx_score": 0.13348710536956787, "metricx_qe_score": 0.25882819294929504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫 Vasudha,是 Stony Brook 大学的计算机科学博士研究生。", "metrics": {"bleu_score": 41.81293041251691, "chrf_score": 46.45117754346613, "xcomet_score": 0.8965737819671631, "xcomet_qe_score": 0.9238553047180176, "metricx_score": 1.0283511877059937, "metricx_qe_score": 0.8428685665130615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将介绍我们在 ACL 2023 上接受的长文论文《用于识别认知失调的迁移学习》,该论文解决了稀有类别的挑战。", "metrics": {"bleu_score": 21.7547750317943, "chrf_score": 26.8975117182893, "xcomet_score": 0.6188664436340332, "xcomet_qe_score": 0.5864917039871216, "metricx_score": 3.7762508392333984, "metricx_qe_score": 4.4920830726623535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义了认知失调,并解释了为什么它是语言研究中一个重要的问题。", "metrics": {"bleu_score": 34.92800489306544, "chrf_score": 29.675780723485712, "xcomet_score": 0.9762685298919678, "xcomet_qe_score": 0.9786473512649536, "metricx_score": 0.5558196306228638, "metricx_qe_score": 0.4513457417488098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两种不一致的信念或行为,例如一个人说“我知道吸烟会害死我”,然后又说“会议后我抽了几口烟”。", "metrics": {"bleu_score": 39.792200090445185, "chrf_score": 33.299984883596174, "xcomet_score": 0.8971413969993591, "xcomet_qe_score": 0.9674978256225586, "metricx_score": 1.8919134140014648, "metricx_qe_score": 2.98789644241333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种信念和行为不一致,处于失调状态。", "metrics": {"bleu_score": 58.51562598821472, "chrf_score": 51.7228778366356, "xcomet_score": 0.99830162525177, "xcomet_qe_score": 0.9889602661132812, "metricx_score": 2.341799020767212, "metricx_qe_score": 3.5342793464660645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步提到,如果没有吸烟,我可能无法保住工作,这证明了第二次行为与", "metrics": {"bleu_score": 19.88662513128133, "chrf_score": 17.809342431144433, "xcomet_score": 0.7765427827835083, "xcomet_qe_score": 0.5321186780929565, "metricx_score": 6.284645080566406, "metricx_qe_score": 6.740649700164795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "信念是一致的。", "metrics": {"bleu_score": 3.2149545730574576, "chrf_score": 1.515151515151515, "xcomet_score": 0.3311506509780884, "xcomet_qe_score": 0.17563524842262268, "metricx_score": 4.03311824798584, "metricx_qe_score": 4.540863990783691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然失调是我们日常决策中非常常见的一种现象,但在其他类型的语篇关系中,它们在语言中表达的非常罕见。", "metrics": {"bleu_score": 45.86771103833984, "chrf_score": 38.122751917118116, "xcomet_score": 0.7995722889900208, "xcomet_qe_score": 0.6863373517990112, "metricx_score": 1.5897367000579834, "metricx_qe_score": 2.606865406036377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么这很重要呢?", "metrics": {"bleu_score": 26.83544415402699, "chrf_score": 22.30098593820053, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026466812938451767, "metricx_qe_score": 0.018294744193553925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人们之间的分歧、跟踪趋势和信念价值观以及态度的变化。", "metrics": {"bleu_score": 57.10209929975497, "chrf_score": 51.696754970862024, "xcomet_score": 0.9077430963516235, "xcomet_qe_score": 0.8548062443733215, "metricx_score": 1.5615606307983398, "metricx_qe_score": 2.330213785171509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知失调也与焦虑症有关,可以更好地理解人们的心理健康。", "metrics": {"bleu_score": 57.87893198065207, "chrf_score": 54.187621342821366, "xcomet_score": 0.7714232802391052, "xcomet_qe_score": 0.7983778715133667, "metricx_score": 1.643772840499878, "metricx_qe_score": 1.8786962032318115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的失调也有助于理解极端主义和弱势群体的两极分化。", "metrics": {"bleu_score": 83.42418629067359, "chrf_score": 72.50512259132948, "xcomet_score": 0.9332674741744995, "xcomet_qe_score": 0.9131416082382202, "metricx_score": 0.9694464206695557, "metricx_qe_score": 1.4612846374511719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,理解认知失调对于理解个人的认知风格非常重要,有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 74.85115503846964, "chrf_score": 70.7386941597468, "xcomet_score": 0.9912103414535522, "xcomet_qe_score": 0.9809904098510742, "metricx_score": 0.5263763070106506, "metricx_qe_score": 0.7585247159004211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了实现认知失调资源的目标,我们对失调关系进行了大规模标注。", "metrics": {"bleu_score": 59.8231569361162, "chrf_score": 56.957430725332436, "xcomet_score": 0.8450678586959839, "xcomet_qe_score": 0.8360685110092163, "metricx_score": 2.1612277030944824, "metricx_qe_score": 2.7009541988372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示的失调优先方法。", "metrics": {"bleu_score": 10.464830656585532, "chrf_score": 15.927133333792446, "xcomet_score": 0.8992354869842529, "xcomet_qe_score": 0.883786141872406, "metricx_score": 1.0256561040878296, "metricx_qe_score": 1.3896636962890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "推文通过 PDTB 解析器进行处理,根据我们的论文中描述的指南对语篇单位对进行标注。", "metrics": {"bleu_score": 40.449259835217646, "chrf_score": 39.55268623596213, "xcomet_score": 0.8219485282897949, "xcomet_qe_score": 0.812024712562561, "metricx_score": 3.5957133769989014, "metricx_qe_score": 4.156022071838379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,只有 3.5% 的标注对中发现了失调。", "metrics": {"bleu_score": 19.398130898389823, "chrf_score": 24.1875921626599, "xcomet_score": 0.8951703310012817, "xcomet_qe_score": 0.8078862428665161, "metricx_score": 1.9230570793151855, "metricx_qe_score": 2.5963294506073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约 1,000 对语篇单位示例后,我们对初始分类器进行了训练,仅训练了 43 个失调示例。", "metrics": {"bleu_score": 36.63224616587484, "chrf_score": 39.98885401269309, "xcomet_score": 0.6831231713294983, "xcomet_qe_score": 0.6990662813186646, "metricx_score": 2.493835926055908, "metricx_qe_score": 3.2312204837799072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现并没有比随机猜测好多少。", "metrics": {"bleu_score": 60.86835984142118, "chrf_score": 60.31111781018422, "xcomet_score": 0.9921101331710815, "xcomet_qe_score": 0.984655499458313, "metricx_score": 1.0738328695297241, "metricx_qe_score": 1.962277889251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于失调出现的频率低且之前没有任何此类数据集,我们面临着绝对稀有性的问题。", "metrics": {"bleu_score": 61.5962478223763, "chrf_score": 54.2594011198864, "xcomet_score": 0.884170413017273, "xcomet_qe_score": 0.9075883626937866, "metricx_score": 0.5868822336196899, "metricx_qe_score": 0.6846269369125366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这个问题,我们尝试了迁移学习和主动学习的组合,以便在更少的标注轮次中收集更多的失调样本,降低整体标注成本,同时提高失调检测。", "metrics": {"bleu_score": 47.78158620807439, "chrf_score": 42.52677593979059, "xcomet_score": 0.9188927412033081, "xcomet_qe_score": 0.8344500064849854, "metricx_score": 2.887132167816162, "metricx_qe_score": 2.471529245376587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型根本无法捕捉到失调类别,我们通过从密切相关任务中转移权重来启动主动学习过程。", "metrics": {"bleu_score": 70.3797577381266, "chrf_score": 63.579608783377914, "xcomet_score": 0.9025288820266724, "xcomet_qe_score": 0.8822361826896667, "metricx_score": 0.8154700398445129, "metricx_qe_score": 1.2353302240371704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中转移。一个是主题无关的失调立场分类,这是一个判断两个人在不同话题上的辩论陈述是否一致或不一致的任务,称为辩论;另一个是 PDTB 的扩展和比较类别的二元分类,因为这两个与辅音和失调的概念密切相关,我们称它们为 CE。", "metrics": {"bleu_score": 39.90232555625357, "chrf_score": 36.18859254060894, "xcomet_score": 0.4402398467063904, "xcomet_qe_score": 0.3820868134498596, "metricx_score": 5.423981189727783, "metricx_qe_score": 6.336795806884766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在转移零样本性能上,标注数据集的表现已经比随机猜测好得多,最佳的 AUC 为 0.62。", "metrics": {"bleu_score": 37.265363197342104, "chrf_score": 40.53336128684351, "xcomet_score": 0.6937583684921265, "xcomet_qe_score": 0.6157124042510986, "metricx_score": 3.8026981353759766, "metricx_qe_score": 4.888254165649414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步地,在两个任务上进行迭代微调,我们发现 CE 任务的微调后,再进行辩论的微调,可以获得更好的零样本性能。", "metrics": {"bleu_score": 35.174088024262296, "chrf_score": 31.37392267679665, "xcomet_score": 0.7344768643379211, "xcomet_qe_score": 0.6321450471878052, "metricx_score": 4.417030334472656, "metricx_qe_score": 5.860998630523682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这是我们用于冷启动主动学习的模型。", "metrics": {"bleu_score": 87.39351325046809, "chrf_score": 85.23499312005504, "xcomet_score": 0.9711843729019165, "xcomet_qe_score": 0.8755406141281128, "metricx_score": 0.9503179788589478, "metricx_qe_score": 1.1934542655944824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了使用来自每个轮次的主动学习和标注的新数据更新模型的最佳方法。", "metrics": {"bleu_score": 57.15623292782456, "chrf_score": 51.16452219214342, "xcomet_score": 0.7778716683387756, "xcomet_qe_score": 0.8312498927116394, "metricx_score": 1.2753376960754395, "metricx_qe_score": 1.5162183046340942, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累计方法累积了迄今为止从主动标注中收集的所有数据,而迭代方法则通过训练最新的数据集来更新模型。", "metrics": {"bleu_score": 42.44339113785597, "chrf_score": 36.41983993665124, "xcomet_score": 0.74222731590271, "xcomet_qe_score": 0.7406653761863708, "metricx_score": 1.191033959388733, "metricx_qe_score": 1.1641461849212646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累计方法在各个方面表现出与迭代方法相同或更好的性能。", "metrics": {"bleu_score": 34.53513585208172, "chrf_score": 30.1995350069179, "xcomet_score": 0.9199468493461609, "xcomet_qe_score": 0.8833118677139282, "metricx_score": 1.5497206449508667, "metricx_qe_score": 1.435786485671997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高失调示例的数量,我们使用概率稀有类别策略(PRC),选择那些在任何轮次的 AL 中被当前模型认为高度可能失调的示例。", "metrics": {"bleu_score": 30.028672285383244, "chrf_score": 27.98915843803917, "xcomet_score": 0.7689588069915771, "xcomet_qe_score": 0.7617806196212769, "metricx_score": 5.730515956878662, "metricx_qe_score": 5.379101753234863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与社区中常用的其他最先进的 AL 策略进行比较。", "metrics": {"bleu_score": 86.0678956678129, "chrf_score": 83.86028625415722, "xcomet_score": 0.9517278671264648, "xcomet_qe_score": 0.8517169952392578, "metricx_score": 1.7236953973770142, "metricx_qe_score": 2.8944568634033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,提出的 PRC 策略比其他最先进的策略表现更好,尽管差异很小。", "metrics": {"bleu_score": 45.192658930886566, "chrf_score": 46.16148600404447, "xcomet_score": 0.9753402471542358, "xcomet_qe_score": 0.9608827829360962, "metricx_score": 1.4998596906661987, "metricx_qe_score": 2.463630199432373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机策略的性能明显较低。", "metrics": {"bleu_score": 31.53554052490134, "chrf_score": 24.756562881562886, "xcomet_score": 0.9872946739196777, "xcomet_qe_score": 0.9506217241287231, "metricx_score": 1.5082175731658936, "metricx_qe_score": 2.0643317699432373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在使用两种最佳策略的进一步轮次的 AL 中,我们将距离分类 AUC 提高到 0.75,这是我们迄今为止在这个任务上获得的最佳性能。", "metrics": {"bleu_score": 52.88508046053048, "chrf_score": 51.66523626165358, "xcomet_score": 0.5356520414352417, "xcomet_qe_score": 0.5752537250518799, "metricx_score": 6.629326820373535, "metricx_qe_score": 7.040490627288818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略对标注质量和标注员成本的可行性。", "metrics": {"bleu_score": 54.90031827817998, "chrf_score": 47.32419263064071, "xcomet_score": 0.8363736867904663, "xcomet_qe_score": 0.8516478538513184, "metricx_score": 1.857693076133728, "metricx_qe_score": 1.4839229583740234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 PRC 在距离分类中具有最高比例,并且对稀有类别最有效。", "metrics": {"bleu_score": 32.86488568383676, "chrf_score": 30.30445768681641, "xcomet_score": 0.7754625082015991, "xcomet_qe_score": 0.7240064740180969, "metricx_score": 6.971951961517334, "metricx_qe_score": 7.469182014465332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也发现这些示例很困难。", "metrics": {"bleu_score": 29.48993986902436, "chrf_score": 26.33726096274303, "xcomet_score": 0.8015791177749634, "xcomet_qe_score": 0.7852308750152588, "metricx_score": 2.4250648021698, "metricx_qe_score": 2.728337526321411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结来说,我们发现 PRC 是一种简单有效的稀有类别获取的 AL 策略,并且通过适当设计的迁移学习任务可以显著帮助冷启动 AL。", "metrics": {"bleu_score": 48.03488774485167, "chrf_score": 42.623911716096245, "xcomet_score": 0.7521020174026489, "xcomet_qe_score": 0.670498788356781, "metricx_score": 3.6645736694335938, "metricx_qe_score": 5.9076924324035645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而领域内的主动标注则受益于累积更新。", "metrics": {"bleu_score": 55.26865325832594, "chrf_score": 45.728366017604074, "xcomet_score": 0.8843247890472412, "xcomet_qe_score": 0.8023277521133423, "metricx_score": 1.621982216835022, "metricx_qe_score": 2.2799072265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们代码数据集和论文的链接。", "metrics": {"bleu_score": 76.24658586234858, "chrf_score": 67.10286037491919, "xcomet_score": 0.9053246974945068, "xcomet_qe_score": 0.9085246920585632, "metricx_score": 2.1192848682403564, "metricx_qe_score": 2.5390400886535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
