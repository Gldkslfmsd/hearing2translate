{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎参加我们关于 DeepLean 的演讲,这是一个用于德语文本简化的新语料库,适用于文档和句子级别。", "metrics": {"bleu_score": 19.963028636812222, "chrf_score": 18.451725292057947, "xcomet_score": 0.5844330191612244, "xcomet_qe_score": 0.402442991733551, "metricx_score": 5.217485427856445, "metricx_qe_score": 5.17849063873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫 Regina Stodden,将为您引导演讲的第一部分。", "metrics": {"bleu_score": 37.66019021279213, "chrf_score": 57.49716059045841, "xcomet_score": 0.8880831599235535, "xcomet_qe_score": 0.9094173908233643, "metricx_score": 4.826433181762695, "metricx_qe_score": 5.107944488525391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们定义文本简化。", "metrics": {"bleu_score": 27.573686516649268, "chrf_score": 25.07569848132104, "xcomet_score": 0.99223792552948, "xcomet_qe_score": 0.997387170791626, "metricx_score": 0.48825350403785706, "metricx_qe_score": 0.41579434275627136, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一个适应文本的过程,旨在提高特定目标群体的文本理解能力,例如阅读有困难的人或非母语使用者。", "metrics": {"bleu_score": 46.80916920234776, "chrf_score": 40.2807551699904, "xcomet_score": 0.874356210231781, "xcomet_qe_score": 0.8830589056015015, "metricx_score": 1.3240877389907837, "metricx_qe_score": 1.1873239278793335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练文本简化模型需要平行文本对,例如文档或句子。", "metrics": {"bleu_score": 68.51821180992009, "chrf_score": 61.610595130413124, "xcomet_score": 0.960074782371521, "xcomet_qe_score": 0.7890728712081909, "metricx_score": 1.6498113870620728, "metricx_qe_score": 2.383617877960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,您可以看到一个复杂德语句子和其通俗语言翻译的平行对齐句子。", "metrics": {"bleu_score": 53.76592108122041, "chrf_score": 47.293975110060934, "xcomet_score": 0.9069768190383911, "xcomet_qe_score": 0.8887874484062195, "metricx_score": 1.070617914199829, "metricx_qe_score": 1.618490219116211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简化句子可以使用不同的技术,如例中所示的词性替换、从句删除、从句重新排序或词插入。", "metrics": {"bleu_score": 44.69843365674004, "chrf_score": 36.625096633119746, "xcomet_score": 0.7449949383735657, "xcomet_qe_score": 0.699059009552002, "metricx_score": 3.025696039199829, "metricx_qe_score": 2.3184754848480225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出新的语料库 DPlane,因为近年来现有语料库存在一些问题。", "metrics": {"bleu_score": 72.8041078836301, "chrf_score": 58.01129423078184, "xcomet_score": 0.6691870093345642, "xcomet_qe_score": 0.6735426783561707, "metricx_score": 5.776637554168701, "metricx_qe_score": 5.966797828674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中一些语料库太小,无法训练分类模型。", "metrics": {"bleu_score": 28.486067964093422, "chrf_score": 25.388327344021832, "xcomet_score": 0.8793132305145264, "xcomet_qe_score": 0.8354304432868958, "metricx_score": 3.0256593227386475, "metricx_qe_score": 2.1098952293395996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的另外三种模型都是自动对齐的,这意味着它们的对齐可能存在错误。", "metrics": {"bleu_score": 54.614995401579634, "chrf_score": 47.56501020730565, "xcomet_score": 0.9812345504760742, "xcomet_qe_score": 0.983422040939331, "metricx_score": 0.6298750638961792, "metricx_qe_score": 0.7642483711242676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出新的语料库 DPlane,它分为两个子语料库:DPlane APA 和 DPlane Web。", "metrics": {"bleu_score": 43.485960528703046, "chrf_score": 26.769039883310207, "xcomet_score": 0.8421992063522339, "xcomet_qe_score": 0.8957226872444153, "metricx_score": 4.142352104187012, "metricx_qe_score": 3.125361442565918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DPlane APA 基于使用文本,", "metrics": {"bleu_score": 13.485111859503691, "chrf_score": 9.583802552552552, "xcomet_score": 0.7064350843429565, "xcomet_qe_score": 0.6805053353309631, "metricx_score": 6.556857585906982, "metricx_qe_score": 9.48455810546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 DPlane APA 中手动对齐了 483 个文档,", "metrics": {"bleu_score": 57.914609264413414, "chrf_score": 39.95344644805658, "xcomet_score": 0.8963532447814941, "xcomet_qe_score": 0.8895129561424255, "metricx_score": 2.0684890747070312, "metricx_qe_score": 1.5870492458343506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果约为 30,000 个平行句子对,13,000 个平行句子对。", "metrics": {"bleu_score": 21.31456897111116, "chrf_score": 44.69413948496021, "xcomet_score": 0.3068731427192688, "xcomet_qe_score": 0.22810116410255432, "metricx_score": 7.911056995391846, "metricx_qe_score": 8.046243667602539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 DPlane Web,该语料库涵盖不同领域,我们也手动和自动对齐方法对这 750 个文档进行了对齐。", "metrics": {"bleu_score": 31.37620864477099, "chrf_score": 28.82675750635219, "xcomet_score": 0.755027174949646, "xcomet_qe_score": 0.7326534390449524, "metricx_score": 3.31489896774292, "metricx_qe_score": 3.4997663497924805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共得到 30,450 个句子对。", "metrics": {"bleu_score": 34.66667772729622, "chrf_score": 51.89743807289497, "xcomet_score": 0.8895045518875122, "xcomet_qe_score": 0.8793696761131287, "metricx_score": 1.7408688068389893, "metricx_qe_score": 1.4593400955200195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析,例如简化类型。正如您", "metrics": {"bleu_score": 29.1392136460701, "chrf_score": 24.889033915602504, "xcomet_score": 0.6476789712905884, "xcomet_qe_score": 0.5869845747947693, "metricx_score": 6.916925430297852, "metricx_qe_score": 4.9605560302734375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里所看到的,圣经文本在所有级别上都比新闻文本或语言学习者文本简化得", "metrics": {"bleu_score": 43.68363644461544, "chrf_score": 45.61068221244439, "xcomet_score": 0.6745803356170654, "xcomet_qe_score": 0.604162335395813, "metricx_score": 5.356533050537109, "metricx_qe_score": 3.7282257080078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更彻底,例如词性简化、结构简化或整体简化水平。", "metrics": {"bleu_score": 47.085315020609784, "chrf_score": 41.0464708536325, "xcomet_score": 0.7597538232803345, "xcomet_qe_score": 0.6942915916442871, "metricx_score": 4.476398468017578, "metricx_qe_score": 4.766666412353516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,可以看到我们的 D-plane 语料库具有多种不同的简化变换。", "metrics": {"bleu_score": 69.80972118841675, "chrf_score": 53.50325072145613, "xcomet_score": 0.8042125701904297, "xcomet_qe_score": 0.7341227531433105, "metricx_score": 2.7888238430023193, "metricx_qe_score": 3.0972719192504883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 D-plane API 语料库中,我们有更多的重新排序和词编辑,而在 D-plane Web", "metrics": {"bleu_score": 22.481074167380633, "chrf_score": 15.481931133781025, "xcomet_score": 0.35211440920829773, "xcomet_qe_score": 0.3257242441177368, "metricx_score": 9.983295440673828, "metricx_qe_score": 11.760828971862793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语料库中,我们有更多的同义改写。", "metrics": {"bleu_score": 18.572266535605454, "chrf_score": 19.28719092656732, "xcomet_score": 0.8242015838623047, "xcomet_qe_score": 0.6011224389076233, "metricx_score": 3.8235464096069336, "metricx_qe_score": 4.121653079986572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看可以用这个语料库做什么。", "metrics": {"bleu_score": 74.41979067557033, "chrf_score": 70.6999314718616, "xcomet_score": 0.9929804801940918, "xcomet_qe_score": 0.9258387088775635, "metricx_score": 0.2989773750305176, "metricx_qe_score": 0.49690017104148865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是 Omar,现在我将讨论我们的数据集 DPlane 的使用案例。", "metrics": {"bleu_score": 35.25678415060714, "chrf_score": 33.38242502397528, "xcomet_score": 0.7836096286773682, "xcomet_qe_score": 0.8327038884162903, "metricx_score": 2.8456814289093018, "metricx_qe_score": 3.1408796310424805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个使用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 70.91936905878008, "chrf_score": 69.2548308325689, "xcomet_score": 0.9885929822921753, "xcomet_qe_score": 0.983216404914856, "metricx_score": 0.5893150568008423, "metricx_qe_score": 0.6166187524795532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,有许多对齐方法,但在机器翻译的背景下,我们有两种不同语言的同源文档,我们想要提取句子在后一种文档中的对齐,但", "metrics": {"bleu_score": 33.272869365121316, "chrf_score": 27.613714381860365, "xcomet_score": 0.68501877784729, "xcomet_qe_score": 0.5897381901741028, "metricx_score": 4.598551273345947, "metricx_qe_score": 3.13480806350708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们试图提取两个平行文档中句子的对齐,它们具有相同的语言、相同的内容,但复杂程度不同。", "metrics": {"bleu_score": 32.938430705781386, "chrf_score": 28.83160565107487, "xcomet_score": 0.8859672546386719, "xcomet_qe_score": 0.8524162769317627, "metricx_score": 1.3059601783752441, "metricx_qe_score": 2.182461738586426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们有了数据集 D-plane,其中包含手动对齐的句子,我们可以将这些句子作为黄金标准对齐来评估一些提出的对齐方法。", "metrics": {"bleu_score": 53.437330674043636, "chrf_score": 41.26166621375088, "xcomet_score": 0.7988033294677734, "xcomet_qe_score": 0.7878469228744507, "metricx_score": 3.5014543533325195, "metricx_qe_score": 3.444274663925171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了某些适应,并在论文中发布了所有这些适应和运行实验的代码。", "metrics": {"bleu_score": 28.197820345099252, "chrf_score": 25.722396878334486, "xcomet_score": 0.8169482946395874, "xcomet_qe_score": 0.8848263025283813, "metricx_score": 4.336398124694824, "metricx_qe_score": 4.164748191833496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是质量对齐方法", "metrics": {"bleu_score": 66.31698736102048, "chrf_score": 55.758500989940664, "xcomet_score": 0.8125216960906982, "xcomet_qe_score": 0.8123287558555603, "metricx_score": 4.627102375030518, "metricx_qe_score": 3.8116202354431152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",您也可以在论文中找到运行此方法以对齐您自己文档的代码。", "metrics": {"bleu_score": 27.413086164736832, "chrf_score": 24.202891623430254, "xcomet_score": 0.8540066480636597, "xcomet_qe_score": 0.8152433633804321, "metricx_score": 2.304103136062622, "metricx_qe_score": 2.0277552604675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文中展示的第二个使用案例是通过微调语言模型来自动简化文本,以从复杂输入文本生成简化文本。", "metrics": {"bleu_score": 62.30600513526998, "chrf_score": 53.91592153397545, "xcomet_score": 0.965091347694397, "xcomet_qe_score": 0.984133243560791, "metricx_score": 0.678168773651123, "metricx_qe_score": 0.6720244288444519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们微调了两个不同的模型。我们微调了", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 25.99298382912637, "xcomet_score": 0.3408113420009613, "xcomet_qe_score": 0.43639102578163147, "metricx_score": 4.435979843139648, "metricx_qe_score": 5.219578742980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "长导入模型以生成文档级别的简化,我们还微调了正常基础导入以生成句子级别的简化。", "metrics": {"bleu_score": 14.996385911415247, "chrf_score": 14.496911750137775, "xcomet_score": 0.6062954664230347, "xcomet_qe_score": 0.5984610319137573, "metricx_score": 6.019225120544434, "metricx_qe_score": 6.0657806396484375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以找到所有检查点,并详细了解我们的实验得分和评估指标。我们得出", "metrics": {"bleu_score": 40.304707205949626, "chrf_score": 37.07212734473843, "xcomet_score": 0.7341969013214111, "xcomet_qe_score": 0.713823139667511, "metricx_score": 6.07295036315918, "metricx_qe_score": 3.509277582168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结论,这种基本的微调可以产生比基线得分更好的得分,我们提出这些结果作为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 61.36547803518769, "chrf_score": 59.899092172801915, "xcomet_score": 0.8203710913658142, "xcomet_qe_score": 0.7143627405166626, "metricx_score": 2.219975709915161, "metricx_qe_score": 2.7772669792175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,希望在会议上能见到你们所有人。", "metrics": {"bleu_score": 25.944320225692962, "chrf_score": 20.388367068650638, "xcomet_score": 0.9904276132583618, "xcomet_qe_score": 0.9903864860534668, "metricx_score": 1.2428456544876099, "metricx_qe_score": 0.735077440738678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是亚当·斯皮拉科夫斯基,这次演讲的主题是协调依存结构。", "metrics": {"bleu_score": 8.25101291002796, "chrf_score": 8.121823069151086, "xcomet_score": 0.7111340761184692, "xcomet_qe_score": 0.6538622379302979, "metricx_score": 3.265868663787842, "metricx_qe_score": 2.1615817546844482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如你所知,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 64.57665807819532, "chrf_score": 63.353047287232236, "xcomet_score": 0.8983738422393799, "xcomet_qe_score": 0.7869809865951538, "metricx_score": 1.4428701400756836, "metricx_qe_score": 1.6935744285583496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依存中,协调结构“丽莎、巴特和玛吉”的结构是这样的:第一个并列词是整个协调结构的头部", "metrics": {"bleu_score": 46.112139089567364, "chrf_score": 34.99312558534615, "xcomet_score": 0.701406717300415, "xcomet_qe_score": 0.5814689993858337, "metricx_score": 3.5970139503479004, "metricx_qe_score": 3.077321767807007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",在这种情况下是丽莎。伊戈", "metrics": {"bleu_score": 7.347053125977879, "chrf_score": 5.791717482936431, "xcomet_score": 0.5374787449836731, "xcomet_qe_score": 0.45666587352752686, "metricx_score": 6.0711774826049805, "metricx_qe_score": 4.308019161224365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义文本理论采取了类似的做法,同样由第一个并列词主导整个协调结构。", "metrics": {"bleu_score": 40.13304343222863, "chrf_score": 29.156095296231065, "xcomet_score": 0.7380809783935547, "xcomet_qe_score": 0.5740330219268799, "metricx_score": 4.308876991271973, "metricx_qe_score": 4.508411884307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的,", "metrics": {"bleu_score": 64.07117598241614, "chrf_score": 48.52481541959439, "xcomet_score": 0.9922106266021729, "xcomet_qe_score": 0.9595069885253906, "metricx_score": 0.41505956649780273, "metricx_qe_score": 0.48554089665412903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3056660294532776, "xcomet_qe_score": 0.29567354917526245, "metricx_score": 4.566452980041504, "metricx_qe_score": 3.0531513690948486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出了其中一个并列词。", "metrics": {"bleu_score": 47.037095938668976, "chrf_score": 39.39037814037815, "xcomet_score": 0.891998827457428, "xcomet_qe_score": 0.8343406319618225, "metricx_score": 2.7510123252868652, "metricx_qe_score": 4.070788383483887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,也有对协调结构采取对称方法,例如布拉格方法,", "metrics": {"bleu_score": 31.011575752288344, "chrf_score": 27.52669090393656, "xcomet_score": 0.7842110395431519, "xcomet_qe_score": 0.7781943082809448, "metricx_score": 5.023082733154297, "metricx_qe_score": 4.257630348205566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "布拉格依存树库中假设的连词主导方法,其中协调结构由连词主导。", "metrics": {"bleu_score": 31.678889081779886, "chrf_score": 27.468963712978255, "xcomet_score": 0.7556755542755127, "xcomet_qe_score": 0.6919981241226196, "metricx_score": 3.935060501098633, "metricx_qe_score": 4.248905181884766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们得到从和到所有并列词的依存关系。", "metrics": {"bleu_score": 41.37280342230792, "chrf_score": 33.8375990293439, "xcomet_score": 0.7863860130310059, "xcomet_qe_score": 0.6975282430648804, "metricx_score": 4.470242023468018, "metricx_qe_score": 4.097902774810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一个多头部方法,例如在卡森词法中,所有并列词都是协调结构的头部", "metrics": {"bleu_score": 30.082445282428086, "chrf_score": 22.128324234656365, "xcomet_score": 0.5183509588241577, "xcomet_qe_score": 0.45379123091697693, "metricx_score": 3.8488199710845947, "metricx_qe_score": 4.196296215057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所以我们从支配者这里得到到所有", "metrics": {"bleu_score": 23.961829057131983, "chrf_score": 18.218864468864467, "xcomet_score": 0.5486071109771729, "xcomet_qe_score": 0.4797239303588867, "metricx_score": 7.858613967895508, "metricx_qe_score": 6.361339092254639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并列词的单独依存关系,它们是巴特和玛吉。", "metrics": {"bleu_score": 8.06207497639984, "chrf_score": 4.638259126604616, "xcomet_score": 0.2109823077917099, "xcomet_qe_score": 0.22648879885673523, "metricx_score": 10.145444869995117, "metricx_qe_score": 11.28604793548584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文的目的是为协调的对称结构提供一个新的论据,如这两种,反对协调的不对称结构,如这两种", "metrics": {"bleu_score": 40.58844744511528, "chrf_score": 35.75123120159861, "xcomet_score": 0.7799564003944397, "xcomet_qe_score": 0.7059330344200134, "metricx_score": 2.9235188961029053, "metricx_qe_score": 2.914487361907959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3093147575855255, "xcomet_qe_score": 0.2691013216972351, "metricx_score": 4.596386909484863, "metricx_qe_score": 3.298849105834961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个论据基于依存长度最小化原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 44.49540332139186, "chrf_score": 35.49174737728767, "xcomet_score": 0.9025686979293823, "xcomet_qe_score": 0.89445960521698, "metricx_score": 0.7895676493644714, "metricx_qe_score": 0.5966720581054688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在英语中,正如你可能知道的,直接宾语倾向于靠近动词,而状语可以更远,", "metrics": {"bleu_score": 37.552005507341654, "chrf_score": 30.076394546470432, "xcomet_score": 0.7483265995979309, "xcomet_qe_score": 0.7627670764923096, "metricx_score": 2.2491092681884766, "metricx_qe_score": 1.9508755207061768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吗?所以“马奇昨天读了它”是可以的,因为直接宾语“它”靠近动词,而“马奇昨天读了”则不好,", "metrics": {"bleu_score": 20.09325751555304, "chrf_score": 11.859802159583358, "xcomet_score": 0.5370453596115112, "xcomet_qe_score": 0.4874415397644043, "metricx_score": 6.4689788818359375, "metricx_qe_score": 6.632287979125977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4009316563606262, "xcomet_qe_score": 0.33130761981010437, "metricx_score": 3.5249640941619873, "metricx_qe_score": 2.114715099334717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在动词和直接宾语之间有一个状语“昨天”。", "metrics": {"bleu_score": 57.324935793132674, "chrf_score": 42.011416240295816, "xcomet_score": 0.8784034252166748, "xcomet_qe_score": 0.8087072372436523, "metricx_score": 1.384641408920288, "metricx_qe_score": 1.099863052368164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重和长时,这种影响可能会得到缓解,因为", "metrics": {"bleu_score": 41.84199371599822, "chrf_score": 40.005369260480826, "xcomet_score": 0.70665442943573, "xcomet_qe_score": 0.5317723751068115, "metricx_score": 3.9983713626861572, "metricx_qe_score": 3.055570125579834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它可以移动到状语之后的位置。", "metrics": {"bleu_score": 31.539582960725753, "chrf_score": 28.833520823185538, "xcomet_score": 0.7976135015487671, "xcomet_qe_score": 0.7212749719619751, "metricx_score": 1.83528733253479, "metricx_qe_score": 2.9494311809539795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在下面所示。所以", "metrics": {"bleu_score": 5.669791110976001, "chrf_score": 4.0650406504065035, "xcomet_score": 0.34131181240081787, "xcomet_qe_score": 0.4384057819843292, "metricx_score": 4.901736736297607, "metricx_qe_score": 1.0391342639923096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是可以的:", "metrics": {"bleu_score": 59.00468726392806, "chrf_score": 52.41221741221741, "xcomet_score": 0.9322870969772339, "xcomet_qe_score": 0.9227757453918457, "metricx_score": 0.40422093868255615, "metricx_qe_score": 0.49396151304244995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“马奇昨天读了这本绝对迷人的关于", "metrics": {"bleu_score": 2.445593937240363, "chrf_score": 0.3306878306878307, "xcomet_score": 0.20635542273521423, "xcomet_qe_score": 0.19176238775253296, "metricx_score": 8.594792366027832, "metricx_qe_score": 5.281850337982178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "蜜蜂的书”,在这里,我们用这个长的名词短语替换了“它”。", "metrics": {"bleu_score": 40.68503909992416, "chrf_score": 39.677168063182364, "xcomet_score": 0.4924955666065216, "xcomet_qe_score": 0.15656405687332153, "metricx_score": 7.048375606536865, "metricx_qe_score": 8.702738761901855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "说“马奇昨天读了这本绝对迷人的关于蜜蜂的书”也是可以的。", "metrics": {"bleu_score": 4.491575568657404, "chrf_score": 2.6796576397187506, "xcomet_score": 0.7858332395553589, "xcomet_qe_score": 0.787213921546936, "metricx_score": 2.7129909992218018, "metricx_qe_score": 3.108933448791504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的推理是,这是可能的,因为尽管这个句子违反了一般语法原则,即直接宾语应该在动词旁边,但它满足了依存长度最小化原则,该原则表明较短的依存关系被优先考虑。", "metrics": {"bleu_score": 44.10436543380429, "chrf_score": 42.85316449956025, "xcomet_score": 0.8892704844474792, "xcomet_qe_score": 0.9384884834289551, "metricx_score": 2.068413734436035, "metricx_qe_score": 2.305292844772339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个树只显示了关键依存关系的长度,所以那些在两种结构中不是常数的。", "metrics": {"bleu_score": 63.4656708284867, "chrf_score": 57.51871860164163, "xcomet_score": 0.7899219989776611, "xcomet_qe_score": 0.6673460006713867, "metricx_score": 4.7232465744018555, "metricx_qe_score": 5.541950225830078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以这里我们有从“读”到状语的长度为7,用词测量,从“读”到“书”的长度为4,所以加起来是11。", "metrics": {"bleu_score": 22.552367333899486, "chrf_score": 18.59897744656515, "xcomet_score": 0.559950590133667, "xcomet_qe_score": 0.5867584347724915, "metricx_score": 7.031362056732178, "metricx_qe_score": 7.202433109283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你交换这两个成分时,这两个依存关系的总和变成6,而不是", "metrics": {"bleu_score": 63.66488370278303, "chrf_score": 57.244266602962256, "xcomet_score": 0.7081396579742432, "xcomet_qe_score": 0.6971884369850159, "metricx_score": 3.764827251434326, "metricx_qe_score": 1.6745951175689697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "11,6更短,这", "metrics": {"bleu_score": 3.2174093287959424, "chrf_score": 7.586705202312139, "xcomet_score": 0.18295252323150635, "xcomet_qe_score": 0.24434900283813477, "metricx_score": 12.694920539855957, "metricx_qe_score": 4.9759368896484375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就是为什么这听起来相当不错,", "metrics": {"bleu_score": 64.50001140844256, "chrf_score": 59.18757168757169, "xcomet_score": 0.9137973785400391, "xcomet_qe_score": 0.862114429473877, "metricx_score": 0.6229963302612305, "metricx_qe_score": 0.7050719857215881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4009316563606262, "xcomet_qe_score": 0.33130761981010437, "metricx_score": 3.5249640941619873, "metricx_qe_score": 2.114715099334717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.26056456565856934, "xcomet_qe_score": 0.20241904258728027, "metricx_score": 4.468552589416504, "metricx_qe_score": 3.296286106109619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的宾树库中提取了关于协调的各种统计数据,并参见论文为什么我们没有使用通用依存。这些统计数据证实了之前多次观察到的现象:左并列词倾向于更短。", "metrics": {"bleu_score": 59.369153118122924, "chrf_score": 54.246446062314845, "xcomet_score": 0.5894980430603027, "xcomet_qe_score": 0.5820645093917847, "metricx_score": 5.263648986816406, "metricx_qe_score": 5.858789920806885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“盐和胡椒”而不是“胡椒和盐”,用音节测量。还观察", "metrics": {"bleu_score": 16.730402692499975, "chrf_score": 9.257597001300958, "xcomet_score": 0.5986900329589844, "xcomet_qe_score": 0.6622573137283325, "metricx_score": 6.652064323425293, "metricx_qe_score": 4.746386528015137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "到一个路过的事实,这种倾向随着长度差异的增加而增加。", "metrics": {"bleu_score": 64.07518031753105, "chrf_score": 64.33164679134018, "xcomet_score": 0.2682510316371918, "xcomet_qe_score": 0.16845513880252838, "metricx_score": 6.7472429275512695, "metricx_qe_score": 7.8123555183410645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以当两个并列词的长度差异增加时,较短的并列词更倾向于成为第一个,更强烈,对吗?", "metrics": {"bleu_score": 64.71018015590462, "chrf_score": 59.50472900943989, "xcomet_score": 0.840438723564148, "xcomet_qe_score": 0.7469034194946289, "metricx_score": 3.9870080947875977, "metricx_qe_score": 4.850162029266357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以左边较短并列词的比例更大。", "metrics": {"bleu_score": 67.29864884660302, "chrf_score": 62.64097014097014, "xcomet_score": 0.9147592782974243, "xcomet_qe_score": 0.8420535326004028, "metricx_score": 1.1792572736740112, "metricx_qe_score": 2.9794371128082275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种倾向仅在支配者在左边或缺席时发生,对吗?所以支配者", "metrics": {"bleu_score": 36.58141331541051, "chrf_score": 30.684012287215563, "xcomet_score": 0.5675440430641174, "xcomet_qe_score": 0.5686910152435303, "metricx_score": 10.205317497253418, "metricx_qe_score": 8.831771850585938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.4009316563606262, "xcomet_qe_score": 0.33130761981010437, "metricx_score": 3.5249640941619873, "metricx_qe_score": 2.114715099334717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中在左边。我看到了巴特和丽莎。在这里,支配者也在左边。它", "metrics": {"bleu_score": 18.880649481277377, "chrf_score": 14.148529039480762, "xcomet_score": 0.4108651280403137, "xcomet_qe_score": 0.4428258240222931, "metricx_score": 6.989678859710693, "metricx_qe_score": 5.826438903808594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中缺席,霍默来了一下打了喷嚏。", "metrics": {"bleu_score": 22.925075288505724, "chrf_score": 11.904258091513608, "xcomet_score": 0.6570535898208618, "xcomet_qe_score": 0.6779914498329163, "metricx_score": 5.417397975921631, "metricx_qe_score": 5.193202972412109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们有两个动词的协调,没有外部支配者,所以", "metrics": {"bleu_score": 62.48651455191908, "chrf_score": 57.99663493178863, "xcomet_score": 0.8327797651290894, "xcomet_qe_score": 0.7675057649612427, "metricx_score": 5.574416160583496, "metricx_qe_score": 3.4219560623168945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左并列词更倾向于更短,越是差异越大。", "metrics": {"bleu_score": 28.577237698042925, "chrf_score": 24.142092098414157, "xcomet_score": 0.7275517582893372, "xcomet_qe_score": 0.7199034690856934, "metricx_score": 5.094949722290039, "metricx_qe_score": 6.280797004699707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当支配者在右边时,这种影响消失了,我们通过测量长度用字数来展示了这一点,这是", "metrics": {"bleu_score": 5.8825481927271115, "chrf_score": 6.536824789477954, "xcomet_score": 0.25973787903785706, "xcomet_qe_score": 0.12166761606931686, "metricx_score": 11.216716766357422, "metricx_qe_score": 9.568819046020508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一列,用音节,中间列,用词,右列。所以我将专", "metrics": {"bleu_score": 3.7736594517747717, "chrf_score": 7.562073897450318, "xcomet_score": 0.21218284964561462, "xcomet_qe_score": 0.13826525211334229, "metricx_score": 16.02823829650879, "metricx_qe_score": 10.071733474731445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "注于右边一个。我们在这", "metrics": {"bleu_score": 11.731175160263996, "chrf_score": 13.427155248716588, "xcomet_score": 0.3977303206920624, "xcomet_qe_score": 0.14961768686771393, "metricx_score": 6.479394912719727, "metricx_qe_score": 6.3143720626831055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里看到的是,当支配者在左边时,左并列词更短的倾向随着绝对的词数差异而稳步增长,当没有支配者时,同样的观察结果在句子协调中得到观察,但", "metrics": {"bleu_score": 24.653942228381776, "chrf_score": 21.77495912667483, "xcomet_score": 0.2772829532623291, "xcomet_qe_score": 0.2887971103191376, "metricx_score": 11.619295120239258, "metricx_qe_score": 8.268997192382812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当支配者在右边时,这种倾向消失了,我们", "metrics": {"bleu_score": 12.147268897471005, "chrf_score": 13.78646883933784, "xcomet_score": 0.585421085357666, "xcomet_qe_score": 0.5619342923164368, "metricx_score": 7.049316883087158, "metricx_qe_score": 2.235549211502075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在论文中展示了这如何为反对不对称协调结构的论据,如这两种,并为对称结构的论据,如这两种。", "metrics": {"bleu_score": 33.0019641161681, "chrf_score": 29.891834885189905, "xcomet_score": 0.5271152257919312, "xcomet_qe_score": 0.5821928977966309, "metricx_score": 5.8549485206604, "metricx_qe_score": 5.937374591827393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以参见论文获取完整的协议和论", "metrics": {"bleu_score": 7.997800623453542, "chrf_score": 9.63622376085284, "xcomet_score": 0.7107903957366943, "xcomet_qe_score": 0.7069275975227356, "metricx_score": 6.35266637802124, "metricx_qe_score": 3.9896180629730225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "据,并在会后与我们讨论。", "metrics": {"bleu_score": 3.7726698069117846, "chrf_score": 3.6764705882352935, "xcomet_score": 0.15013635158538818, "xcomet_qe_score": 0.15289999544620514, "metricx_score": 6.377547264099121, "metricx_qe_score": 5.67201042175293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是香斌,华盛顿大学博士生。", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 20.085203671626676, "xcomet_score": 0.8397122621536255, "xcomet_qe_score": 0.8220083713531494, "metricx_score": 1.0871628522872925, "metricx_qe_score": 0.5624538660049438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们从预训练数据到语言模型再到下游任务的研究工作,追踪导致不公平自然语言处理(NLP)模型的政治偏见的轨迹。因此", "metrics": {"bleu_score": 57.278147258337086, "chrf_score": 59.65370036538569, "xcomet_score": 0.7692124247550964, "xcomet_qe_score": 0.5809168815612793, "metricx_score": 3.8507578372955322, "metricx_qe_score": 1.8588606119155884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是在大规模网络抓取数据上训练的。", "metrics": {"bleu_score": 68.91557807535084, "chrf_score": 60.66442658934918, "xcomet_score": 0.9647777080535889, "xcomet_qe_score": 0.9669432640075684, "metricx_score": 2.73828125, "metricx_qe_score": 3.135836601257324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在他们的预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 48.195116293616074, "chrf_score": 48.402791332265075, "xcomet_score": 0.7571665048599243, "xcomet_qe_score": 0.7159234881401062, "metricx_score": 1.750040054321289, "metricx_qe_score": 2.615305185317993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C四语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 77.00961116014527, "chrf_score": 74.42516909564768, "xcomet_score": 0.7471767067909241, "xcomet_qe_score": 0.7173298001289368, "metricx_score": 2.1897106170654297, "metricx_qe_score": 2.303035020828247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型的应用带来了喜忧参半的结果。", "metrics": {"bleu_score": 60.28670503016433, "chrf_score": 55.24607234176625, "xcomet_score": 0.9970561265945435, "xcomet_qe_score": 0.9862403869628906, "metricx_score": 0.5632274150848389, "metricx_qe_score": 0.7066482305526733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从多样的视角中学习,庆祝民主和思想的多元化。", "metrics": {"bleu_score": 32.13037359829381, "chrf_score": 26.72725194914618, "xcomet_score": 0.7010678052902222, "xcomet_qe_score": 0.79075026512146, "metricx_score": 1.794076919555664, "metricx_qe_score": 2.137477397918701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本身带有社会偏见,可能导致下游任务应用中出现潜在的公平问题。", "metrics": {"bleu_score": 55.336580870838525, "chrf_score": 46.20727065874535, "xcomet_score": 0.9818580150604248, "xcomet_qe_score": 0.9717600345611572, "metricx_score": 0.9023268818855286, "metricx_qe_score": 1.161994218826294, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体通过提出以下问题:首先,如何评估语言模型的政治倾向以及预训练数据在这些政治偏见中可能扮演的角色?", "metrics": {"bleu_score": 59.06061140856521, "chrf_score": 53.76337275268451, "xcomet_score": 0.8310976028442383, "xcomet_qe_score": 0.8327012658119202, "metricx_score": 1.637914776802063, "metricx_qe_score": 1.9377647638320923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在下游任务中的实际表现是否会导致NLP应用中的公平问题?", "metrics": {"bleu_score": 71.6946223192333, "chrf_score": 71.92279817547006, "xcomet_score": 0.9413294792175293, "xcomet_qe_score": 0.86328125, "metricx_score": 1.5883262157440186, "metricx_qe_score": 1.0943416357040405, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体来说,我们首先提出使用政治问卷(如政治罗盘测试)以不同的提示格式提示语言模型。", "metrics": {"bleu_score": 50.56658847260702, "chrf_score": 43.11575632774979, "xcomet_score": 0.8255069851875305, "xcomet_qe_score": 0.7559001445770264, "metricx_score": 3.803971529006958, "metricx_qe_score": 4.167150974273682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保我们在政治科学文献中进行良好的自动评估。", "metrics": {"bleu_score": 30.627422516285332, "chrf_score": 28.187863613344028, "xcomet_score": 0.9021536707878113, "xcomet_qe_score": 0.8612660765647888, "metricx_score": 3.654235363006592, "metricx_qe_score": 3.9092304706573486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,首先,语言模型确实具有不同的政治含义,", "metrics": {"bleu_score": 59.35334349227443, "chrf_score": 51.26672337174819, "xcomet_score": 0.975555419921875, "xcomet_qe_score": 0.9865827560424805, "metricx_score": 1.516809344291687, "metricx_qe_score": 1.604914903640747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘上的所有四个象限。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 62.83591000502765, "xcomet_score": 0.8533560037612915, "xcomet_qe_score": 0.7687404155731201, "metricx_score": 1.9299724102020264, "metricx_qe_score": 2.1211085319519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT 4是所有语言模型中最自由的,而GPT理论在社会自由度上通常优于BERT理论及其变体。", "metrics": {"bleu_score": 47.99994645957786, "chrf_score": 42.555724201468365, "xcomet_score": 0.7480615973472595, "xcomet_qe_score": 0.6450599431991577, "metricx_score": 2.3955397605895996, "metricx_qe_score": 2.2547168731689453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在调查语言模型的政治偏见实际上在多大程度上来自训练数据。我们可以进行一", "metrics": {"bleu_score": 56.66369904777522, "chrf_score": 53.48658600586417, "xcomet_score": 0.7557950019836426, "xcomet_qe_score": 0.6657356023788452, "metricx_score": 6.178187847137451, "metricx_qe_score": 4.390479564666748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "项控制实验,通过在六个不同的党派语料库上进一步预训练语言模型检查点来分离新闻和社交媒体,并根据其政治倾向进一步划分。", "metrics": {"bleu_score": 39.51145214753742, "chrf_score": 34.77059630721198, "xcomet_score": 0.6142732501029968, "xcomet_qe_score": 0.47043049335479736, "metricx_score": 5.380281925201416, "metricx_qe_score": 5.750829696655273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这样的党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了变化。", "metrics": {"bleu_score": 78.28493884997408, "chrf_score": 73.91200640576464, "xcomet_score": 0.9099429845809937, "xcomet_qe_score": 0.8268270492553711, "metricx_score": 1.165337085723877, "metricx_qe_score": 1.7587034702301025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于Roberta,在进一步微调并在倾向左派的Reddit语料库上进行训练后,我们可以看到其在政治偏见方面显着地向自由主义转变。", "metrics": {"bleu_score": 36.86451407321843, "chrf_score": 39.00324207951519, "xcomet_score": 0.8003129363059998, "xcomet_qe_score": 0.7806178331375122, "metricx_score": 3.335785388946533, "metricx_qe_score": 3.02932071685791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了调查语言模型是否能够捕捉到现代社会中普遍存在的极化现", "metrics": {"bleu_score": 62.91940547775141, "chrf_score": 63.62895170619787, "xcomet_score": 0.7519456148147583, "xcomet_qe_score": 0.7423955202102661, "metricx_score": 5.561239242553711, "metricx_qe_score": 2.1832127571105957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "象,我们将预训练语料库分为美国第45任总统之前和之后的两部分。", "metrics": {"bleu_score": 66.9346401858516, "chrf_score": 65.0052343154549, "xcomet_score": 0.7087743878364563, "xcomet_qe_score": 0.6145439147949219, "metricx_score": 3.7207210063934326, "metricx_qe_score": 5.210824966430664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们分别在两个不同的时间语料库上预训练语言模型。我们", "metrics": {"bleu_score": 91.81891462193897, "chrf_score": 98.16306505407624, "xcomet_score": 0.6584798097610474, "xcomet_qe_score": 0.6531087160110474, "metricx_score": 3.7580032348632812, "metricx_qe_score": 1.3437440395355225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年后普遍表现出更偏离中心的政治倾向。", "metrics": {"bleu_score": 46.95966835778608, "chrf_score": 41.7428879432958, "xcomet_score": 0.9854961037635803, "xcomet_qe_score": 0.9901463985443115, "metricx_score": 1.081101655960083, "metricx_qe_score": 1.5196943283081055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能捕捉到社会中的极化现象。", "metrics": {"bleu_score": 41.0155947154624, "chrf_score": 36.59358799422811, "xcomet_score": 0.9972637891769409, "xcomet_qe_score": 0.997498631477356, "metricx_score": 0.7880501747131348, "metricx_qe_score": 1.0712693929672241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等NLP应用中的表现,这些应用通常涉及语言模型,并且可能具有非常重要的影响。", "metrics": {"bleu_score": 73.17482567500774, "chrf_score": 68.96065358633672, "xcomet_score": 0.9680643081665039, "xcomet_qe_score": 0.9175941944122314, "metricx_score": 0.8440406322479248, "metricx_qe_score": 1.2199572324752808, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,如果调查每类性能,即如果我们将性能分开到不同的人口统计或新闻媒体的政治倾向,我们可以看到一个模式:", "metrics": {"bleu_score": 38.06370798616601, "chrf_score": 34.27966255509912, "xcomet_score": 0.6941595673561096, "xcomet_qe_score": 0.6572692394256592, "metricx_score": 7.215551376342773, "metricx_qe_score": 6.571157932281494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测中,倾向左派的语言模型更好地检测到针对社会少数群体的仇恨言论,但更差地检测到针对社会更强大群体的仇恨言论。相反,倾向", "metrics": {"bleu_score": 44.84801565900007, "chrf_score": 38.44599861857325, "xcomet_score": 0.6161625385284424, "xcomet_qe_score": 0.6021174192428589, "metricx_score": 9.647153854370117, "metricx_qe_score": 6.936929702758789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "右派的语言模型更好地检测到针对白人男性的仇恨言论,但更差地检测到针对黑人、LGBTQ+和其他少数社区的仇恨言论。在假新闻检测", "metrics": {"bleu_score": 48.24965709411156, "chrf_score": 45.199612953649584, "xcomet_score": 0.597678542137146, "xcomet_qe_score": 0.5648773908615112, "metricx_score": 6.721654891967773, "metricx_qe_score": 4.147183418273926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中,也出现了类似的趋势,我们看到倾向左派的语言模型更好地检测到来自相反政治倾向的误导信息,反之亦然。我们", "metrics": {"bleu_score": 35.40999488280491, "chrf_score": 29.310457788552114, "xcomet_score": 0.47099241614341736, "xcomet_qe_score": 0.5635650753974915, "metricx_score": 7.412934303283691, "metricx_qe_score": 4.833001613616943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "将进一步展示许多定性示例,以看到具有不同政治倾向的语言模型根据其社会类别对仇恨言论和误导信息示例给出不同的预测。", "metrics": {"bleu_score": 69.71242891509229, "chrf_score": 63.64821023510849, "xcomet_score": 0.8370152115821838, "xcomet_qe_score": 0.9140924215316772, "metricx_score": 1.746755599975586, "metricx_qe_score": 1.9491691589355469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中还有更多示例,以进一步强调这一点。这表明存在一个与语言模型政治偏见相关的非常紧迫的公平问题。", "metrics": {"bleu_score": 49.03833729455414, "chrf_score": 41.38938934862742, "xcomet_score": 0.9840351343154907, "xcomet_qe_score": 0.9835751056671143, "metricx_score": 1.324012041091919, "metricx_qe_score": 1.1923835277557373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果一个倾向右派的语言模型被微调用于仇恨言论、误导信息等任务,并部署到一个流行的社交媒体平台,这意味着持有相反政治观点的人可能会被边缘化,而针对少数群体的仇恨言论可能会不受控制地蔓延。", "metrics": {"bleu_score": 52.693552804448764, "chrf_score": 46.35380455460742, "xcomet_score": 0.9654287099838257, "xcomet_qe_score": 0.9050868153572083, "metricx_score": 1.2178890705108643, "metricx_qe_score": 1.3384724855422974, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,需要认识到并解决语言模型政治偏见导致的公平问题。", "metrics": {"bleu_score": 34.722246998232244, "chrf_score": 35.31941706249101, "xcomet_score": 0.9885514974594116, "xcomet_qe_score": 0.9855026006698608, "metricx_score": 0.5185617208480835, "metricx_qe_score": 0.7021229267120361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "稍微讨论一下,", "metrics": {"bleu_score": 26.78284959130087, "chrf_score": 20.164331705936462, "xcomet_score": 0.8377116322517395, "xcomet_qe_score": 0.84737229347229, "metricx_score": 1.2065798044204712, "metricx_qe_score": 1.4630906581878662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也想强调我们揭露了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 53.09565039223721, "chrf_score": 50.52264517853785, "xcomet_score": 0.7941737771034241, "xcomet_qe_score": 0.8162182569503784, "metricx_score": 1.2145053148269653, "metricx_qe_score": 1.8286598920822144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在斯克拉和卡里布迪斯之间选择。", "metrics": {"bleu_score": 25.932515248151116, "chrf_score": 23.692115211305058, "xcomet_score": 0.8235040903091431, "xcomet_qe_score": 0.7993883490562439, "metricx_score": 1.4339851140975952, "metricx_qe_score": 1.6894714832305908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不清理语言模型训练数据中的政治观点,偏见将从预训练数据传播到语言模型再到下游任务,最终产生公平问题。", "metrics": {"bleu_score": 70.00121352312124, "chrf_score": 66.26775798206384, "xcomet_score": 0.9746878147125244, "xcomet_qe_score": 0.8557811379432678, "metricx_score": 1.343104362487793, "metricx_qe_score": 1.81095552444458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们试图以某种方式进行清理,我们也可能会面临审查或排斥的风险,", "metrics": {"bleu_score": 67.00845133764378, "chrf_score": 69.64339575507282, "xcomet_score": 0.8480780124664307, "xcomet_qe_score": 0.7983783483505249, "metricx_score": 1.7419970035552979, "metricx_qe_score": 2.9613003730773926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且很难确定训练语言模型数据中真正中立的内容。", "metrics": {"bleu_score": 4.716199813818711, "chrf_score": 10.809392444803597, "xcomet_score": 0.8580335974693298, "xcomet_qe_score": 0.7912055253982544, "metricx_score": 2.40647292137146, "metricx_qe_score": 2.768674612045288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像电查理问题。", "metrics": {"bleu_score": 11.417530270031051, "chrf_score": 11.334028139656072, "xcomet_score": 0.746390163898468, "xcomet_qe_score": 0.6813532114028931, "metricx_score": 4.988123893737793, "metricx_qe_score": 5.454023361206055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2213955670595169, "xcomet_qe_score": 0.16171389818191528, "metricx_score": 5.045584201812744, "metricx_qe_score": 3.731776237487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想今天就这些了。", "metrics": {"bleu_score": 8.898962021078441, "chrf_score": 11.752615844544094, "xcomet_score": 0.9928063154220581, "xcomet_qe_score": 0.9825507402420044, "metricx_score": 0.43964487314224243, "metricx_qe_score": 0.4785514771938324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢你的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.6577392816543579, "xcomet_qe_score": 0.8449009656906128, "metricx_score": 1.630244493484497, "metricx_qe_score": 1.4724582433700562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831969738006592, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基·梅隆大学一年级博士生,今天我将向大家介绍你们的作品《分析位置性:特征设计偏见、β集合与模型》。", "metrics": {"bleu_score": 30.40624886908018, "chrf_score": 21.166886418924047, "xcomet_score": 0.6796419620513916, "xcomet_qe_score": 0.7586187124252319, "metricx_score": 5.8540754318237305, "metricx_qe_score": 5.714574813842773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的,他们是塞巴斯蒂安·桑蒂、罗宁·勒布朗、卡塔琳娜·雷尼克和马丁·萨普。", "metrics": {"bleu_score": 35.51902556003419, "chrf_score": 24.847573879369982, "xcomet_score": 0.8556479811668396, "xcomet_qe_score": 0.8990705013275146, "metricx_score": 1.197368860244751, "metricx_qe_score": 0.968869149684906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们从想象一个场景开始:你为一家报纸工作,正在筛选新闻文章下的评论,试图删除有毒内容。", "metrics": {"bleu_score": 45.59122324041223, "chrf_score": 40.97420366177434, "xcomet_score": 0.9044656753540039, "xcomet_qe_score": 0.9048248529434204, "metricx_score": 1.5787978172302246, "metricx_qe_score": 1.3440800905227661, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会使用像Perspective API这样的流行API来检测有毒内容,如果你是卡尔·琼斯,这效果很好,因为", "metrics": {"bleu_score": 20.813623572148543, "chrf_score": 28.33500309578457, "xcomet_score": 0.6873922348022461, "xcomet_qe_score": 0.6258336901664734, "metricx_score": 5.407668113708496, "metricx_qe_score": 4.448368549346924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能够正确地检测出有毒实例。", "metrics": {"bleu_score": 30.130404892785695, "chrf_score": 56.97325058598, "xcomet_score": 0.784793496131897, "xcomet_qe_score": 0.7270252704620361, "metricx_score": 4.658648490905762, "metricx_qe_score": 4.680523872375488, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对迪蒂亚·沙尔马来说,情况就不同了,", "metrics": {"bleu_score": 7.012887580040736, "chrf_score": 6.394696940738759, "xcomet_score": 0.9150741100311279, "xcomet_qe_score": 0.9116272926330566, "metricx_score": 2.0908474922180176, "metricx_qe_score": 1.321182370185852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API对印度语境中更常见的冒犯性术语不够敏感。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 72.63228556751179, "xcomet_score": 0.8359979391098022, "xcomet_qe_score": 0.7078613042831421, "metricx_score": 4.085815906524658, "metricx_qe_score": 4.711545467376709, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是设计偏见的一个例子,我们看到技术在不同人群之间的系统性性能差异。", "metrics": {"bleu_score": 41.40890835499403, "chrf_score": 33.70501558310316, "xcomet_score": 0.9816112518310547, "xcomet_qe_score": 0.9017271995544434, "metricx_score": 1.181369423866272, "metricx_qe_score": 1.8654950857162476, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像我们刚才看到的那样,设计偏见可能发生在NLP研究人员和模型开发者的位置性上。位置性", "metrics": {"bleu_score": 42.63926812462895, "chrf_score": 38.02610215499731, "xcomet_score": 0.6645290851593018, "xcomet_qe_score": 0.6649570465087891, "metricx_score": 5.612764358520508, "metricx_qe_score": 4.511467456817627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说就是人们由于人口统计、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 54.39955646991225, "chrf_score": 54.268858567670065, "xcomet_score": 0.8851670622825623, "xcomet_qe_score": 0.8869051337242126, "metricx_score": 2.193572998046875, "metricx_qe_score": 2.9762279987335205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权和酷儿学术领域。", "metrics": {"bleu_score": 68.0550784359129, "chrf_score": 62.137580119763726, "xcomet_score": 0.9937523603439331, "xcomet_qe_score": 0.9201943874359131, "metricx_score": 0.9926044940948486, "metricx_qe_score": 1.0936604738235474, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,位置性可能会影响研究过程及其结果和结论,因为它会改变研究人员做出的决定。", "metrics": {"bleu_score": 64.17670444923398, "chrf_score": 57.92108736032818, "xcomet_score": 0.8369884490966797, "xcomet_qe_score": 0.8458430171012878, "metricx_score": 3.8166096210479736, "metricx_qe_score": 3.1774024963378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问,数据集和模型有位置性吗?", "metrics": {"bleu_score": 38.42485782764102, "chrf_score": 32.54362049109562, "xcomet_score": 0.90648353099823, "xcomet_qe_score": 0.9366465210914612, "metricx_score": 3.218468189239502, "metricx_qe_score": 1.2146908044815063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型和数据集本身具有人口统计身份和生活经历,但它们确实汇集了真实人士的判断和意见,因此可能代表某些位置性而非", "metrics": {"bleu_score": 52.76924958366697, "chrf_score": 43.32074291159012, "xcomet_score": 0.6549087762832642, "xcomet_qe_score": 0.6307674050331116, "metricx_score": 5.664153099060059, "metricx_qe_score": 5.046023845672607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他位置性。先前的研究提供了位置性的某些轶事证据,例如模型和数据集中的文化差距,以及模型位置性的理论定义。", "metrics": {"bleu_score": 33.282990309032314, "chrf_score": 27.084980625867438, "xcomet_score": 0.5249994993209839, "xcomet_qe_score": 0.4940473735332489, "metricx_score": 6.762691974639893, "metricx_qe_score": 6.271594524383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些工作并没有真正地将最终用户与数据集和模型本身进行比较。研究模型和数据集的位置性越来越重要,因为NLP任务变得更加主观和社会化。描述这些位置性偏差的挑战在于,并非所有决策都有记录,许多模型隐藏在API背后。", "metrics": {"bleu_score": 52.17739635629546, "chrf_score": 47.4728900754883, "xcomet_score": 0.7801021933555603, "xcomet_qe_score": 0.8390921354293823, "metricx_score": 5.090801239013672, "metricx_qe_score": 4.499828815460205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的位置性,我们实际上将真实用户的标注与现有数据集和模型进行比较。", "metrics": {"bleu_score": 56.8415233487702, "chrf_score": 49.277396005412704, "xcomet_score": 0.8263623714447021, "xcomet_qe_score": 0.9111512899398804, "metricx_score": 4.042695045471191, "metricx_qe_score": 3.4079620838165283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架NLPositionality来实现这一点。", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 62.11411554992817, "xcomet_score": 0.9418507814407349, "xcomet_qe_score": 0.9376275539398193, "metricx_score": 0.6295923590660095, "metricx_qe_score": 1.3104772567749023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的标注员重新标注数据集。", "metrics": {"bleu_score": 38.75407750115177, "chrf_score": 31.37407448359461, "xcomet_score": 0.8081755638122559, "xcomet_qe_score": 0.8097912669181824, "metricx_score": 3.9440348148345947, "metricx_qe_score": 3.784290313720703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是分析原始数据集标注员的人口统计数据,因为通常只有少数标注员标注每个实例,而且人口统计数据很少被收集和共享。", "metrics": {"bleu_score": 54.57729069377991, "chrf_score": 47.53348457480706, "xcomet_score": 0.9061456322669983, "xcomet_qe_score": 0.8970919847488403, "metricx_score": 1.4752004146575928, "metricx_qe_score": 1.2458463907241821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,为每个实例获得更多标注员,并收集到丰富的人口统计数据。", "metrics": {"bleu_score": 36.4214072819569, "chrf_score": 33.82491838089165, "xcomet_score": 0.8421390652656555, "xcomet_qe_score": 0.8092662692070007, "metricx_score": 4.746408939361572, "metricx_qe_score": 3.4286916255950928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们根据人口统计数据进行标注,并使用Parsons R相关系数将它们与模型和数据集进行比较。因此,我们的框架与标注员分歧文献不同,它将最终用户与模型和数据集的预测和标签进行比较,而不是仅仅看标注员的协议或建模标注员分布。", "metrics": {"bleu_score": 48.27077041065659, "chrf_score": 40.79214977512378, "xcomet_score": 0.5195338129997253, "xcomet_qe_score": 0.49746695160865784, "metricx_score": 5.590877056121826, "metricx_qe_score": 4.915139198303223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在Lab in the Wild的广泛支持下得以实现,", "metrics": {"bleu_score": 23.1311632227769, "chrf_score": 37.50058640965539, "xcomet_score": 0.633966326713562, "xcomet_qe_score": 0.6003197431564331, "metricx_score": 5.208347320556641, "metricx_qe_score": 5.665380954742432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在线实验平台,我们可以在其中招募多样化的志愿者,", "metrics": {"bleu_score": 58.86020508794015, "chrf_score": 42.2148883984036, "xcomet_score": 0.837119460105896, "xcomet_qe_score": 0.7397469282150269, "metricx_score": 2.8825056552886963, "metricx_qe_score": 4.61933708190918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与MTurk等主要参与者来自美国或印度的平台不同。此外,Lab in the Wild仍然能够获得高质量的数据。", "metrics": {"bleu_score": 53.11492744142741, "chrf_score": 58.26003219898318, "xcomet_score": 0.7084037065505981, "xcomet_qe_score": 0.6432842016220093, "metricx_score": 1.7123088836669922, "metricx_qe_score": 1.3443057537078857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Lab in the Wild上托管了两个任务,其中之一是社会可接受性任务。它的工作原理是参与者将阅读来自社会化学数据集的场景,然后写下他们认为该场景在社会上有多可接受。", "metrics": {"bleu_score": 36.52973170502284, "chrf_score": 38.18386473112438, "xcomet_score": 0.7383555769920349, "xcomet_qe_score": 0.6588563919067383, "metricx_score": 2.7274460792541504, "metricx_qe_score": 2.2278945446014404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持参与者的参与度,他们可以将自己的回答与AI和其他人进行比较。", "metrics": {"bleu_score": 57.11195247483297, "chrf_score": 50.616768571005636, "xcomet_score": 0.9610880017280579, "xcomet_qe_score": 0.9873847961425781, "metricx_score": 1.159201979637146, "metricx_qe_score": 0.8757180571556091, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们然后将这些标注与Social Chemistry Delphi和GPT 4进行比较。", "metrics": {"bleu_score": 31.279667410236936, "chrf_score": 42.29892251923015, "xcomet_score": 0.8829307556152344, "xcomet_qe_score": 0.8884782195091248, "metricx_score": 4.515434265136719, "metricx_qe_score": 3.630094528198242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们为有毒语言和仇恨言论检测任务复制了非常类似的设置,参与者将阅读来自DynaHate的实例,并写下他们是否认为它是仇恨言论的实例。", "metrics": {"bleu_score": 49.10021394963847, "chrf_score": 42.20573372939766, "xcomet_score": 0.8574230074882507, "xcomet_qe_score": 0.8686112761497498, "metricx_score": 2.1317312717437744, "metricx_qe_score": 2.76051926612854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们然后将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT 4进行比较。", "metrics": {"bleu_score": 58.40258833708879, "chrf_score": 78.02604745605862, "xcomet_score": 0.8564537167549133, "xcomet_qe_score": 0.8464984893798828, "metricx_score": 1.4625532627105713, "metricx_qe_score": 1.9711700677871704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终积累了来自87个国家的1000多名标注员的超过16000个标注。", "metrics": {"bleu_score": 52.89970398813589, "chrf_score": 51.85056811991796, "xcomet_score": 0.8922029733657837, "xcomet_qe_score": 0.9361646175384521, "metricx_score": 2.3965792655944824, "metricx_qe_score": 1.4995570182800293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以现在我们更好地装备来回答NLP数据集和模型最符合谁的问题?我们", "metrics": {"bleu_score": 35.07072986351901, "chrf_score": 39.06382749208711, "xcomet_score": 0.6447368860244751, "xcomet_qe_score": 0.6322458982467651, "metricx_score": 6.59212064743042, "metricx_qe_score": 4.483899116516113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现NLP中有位置性。", "metrics": {"bleu_score": 8.181017927901259, "chrf_score": 16.495075614375978, "xcomet_score": 0.8261902928352356, "xcomet_qe_score": 0.8350079655647278, "metricx_score": 4.544436931610107, "metricx_qe_score": 2.025646924972534, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最符合英语国家。", "metrics": {"bleu_score": 44.51102326145217, "chrf_score": 41.16540429916604, "xcomet_score": 0.8972375392913818, "xcomet_qe_score": 0.8643839359283447, "metricx_score": 0.834149956703186, "metricx_qe_score": 0.8455660939216614, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在GPT 4社会可接受性分析中,我们发现它最符合儒家思想和英语国家。我们还", "metrics": {"bleu_score": 49.44529640332637, "chrf_score": 48.3512695962335, "xcomet_score": 0.6524129509925842, "xcomet_qe_score": 0.6103024482727051, "metricx_score": 6.918021202087402, "metricx_qe_score": 2.5512802600860596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现DanaHate最符合英语国家。", "metrics": {"bleu_score": 37.580522525232894, "chrf_score": 33.617016117240105, "xcomet_score": 0.8359684944152832, "xcomet_qe_score": 0.76032555103302, "metricx_score": 2.1948273181915283, "metricx_qe_score": 3.2015724182128906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现与具有大学教育的人有更多的额外符合性。", "metrics": {"bleu_score": 24.535247839368523, "chrf_score": 20.371800546468553, "xcomet_score": 0.8254412412643433, "xcomet_qe_score": 0.894298255443573, "metricx_score": 2.462934970855713, "metricx_qe_score": 1.5128253698349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在GPT 4的社会可接受性任务中,我们发现它最符合具有大学教育或研究生教育的人。我们在DanaHate中发现了同样的情况,它最符合具有大学教育的人。", "metrics": {"bleu_score": 42.997302935092044, "chrf_score": 35.22550465882567, "xcomet_score": 0.720065712928772, "xcomet_qe_score": 0.6828779578208923, "metricx_score": 3.2056374549865723, "metricx_qe_score": 2.7142269611358643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集符合特定人群时,有些人不可避免地被落在后面。", "metrics": {"bleu_score": 43.35924513461071, "chrf_score": 35.846002122695545, "xcomet_score": 0.8516682386398315, "xcomet_qe_score": 0.8985675573348999, "metricx_score": 1.1016945838928223, "metricx_qe_score": 1.0061516761779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,数据集和模型对非二元性别的人比他们的男性和女性同行有更低的符合度。", "metrics": {"bleu_score": 30.327872414714488, "chrf_score": 29.456758984784813, "xcomet_score": 0.9688915014266968, "xcomet_qe_score": 0.9707189798355103, "metricx_score": 2.775738477706909, "metricx_qe_score": 2.1519196033477783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT 4的社会可接受性任务以及DynaHate任务分析中都发现了这一点。", "metrics": {"bleu_score": 72.02343877500252, "chrf_score": 72.07744091122757, "xcomet_score": 0.893415093421936, "xcomet_qe_score": 0.890104353427887, "metricx_score": 1.367812991142273, "metricx_qe_score": 1.9844167232513428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既然NLP中有位置性,我们能做些什么呢?我们", "metrics": {"bleu_score": 44.87605183448696, "chrf_score": 43.219631697324445, "xcomet_score": 0.7308524250984192, "xcomet_qe_score": 0.7503615617752075, "metricx_score": 6.954959392547607, "metricx_qe_score": 2.214993476867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对此有几个建议。", "metrics": {"bleu_score": 18.594002123233256, "chrf_score": 16.920511497175358, "xcomet_score": 0.9838346242904663, "xcomet_qe_score": 0.9525736570358276, "metricx_score": 0.33213287591934204, "metricx_qe_score": 0.3007272779941559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是记录整个研究过程中所有相关的设计选择。另一个", "metrics": {"bleu_score": 45.86402483839224, "chrf_score": 38.50948366967199, "xcomet_score": 0.7972731590270996, "xcomet_qe_score": 0.7591418027877808, "metricx_score": 4.393394947052002, "metricx_qe_score": 0.3915759027004242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是通过观点主义镜头进行NLP研究。", "metrics": {"bleu_score": 20.31098187098674, "chrf_score": 17.826872440582854, "xcomet_score": 0.8340318202972412, "xcomet_qe_score": 0.7323312759399414, "metricx_score": 3.9713685512542725, "metricx_qe_score": 4.4354248046875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业的数据集和模型。", "metrics": {"bleu_score": 80.96427216101601, "chrf_score": 72.63771349978248, "xcomet_score": 0.911953866481781, "xcomet_qe_score": 0.8777899146080017, "metricx_score": 0.8077837228775024, "metricx_qe_score": 0.9955554604530334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane倡议。", "metrics": {"bleu_score": 55.2058197664637, "chrf_score": 43.77089436218316, "xcomet_score": 0.7475622892379761, "xcomet_qe_score": 0.789522647857666, "metricx_score": 2.372803211212158, "metricx_qe_score": 4.278580188751221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调,包容性NLP不仅仅是让所有", "metrics": {"bleu_score": 42.115124950749305, "chrf_score": 40.517930630683054, "xcomet_score": 0.626814603805542, "xcomet_qe_score": 0.5456753969192505, "metricx_score": 5.8720383644104, "metricx_qe_score": 4.886364459991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为每个人服务。", "metrics": {"bleu_score": 13.927628237847681, "chrf_score": 14.6579211560759, "xcomet_score": 0.9458763599395752, "xcomet_qe_score": 0.9540870189666748, "metricx_score": 0.7448137402534485, "metricx_qe_score": 1.026949167251587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就结束了我们的演讲。", "metrics": {"bleu_score": 41.72261448611506, "chrf_score": 35.54713090445162, "xcomet_score": 0.9990087747573853, "xcomet_qe_score": 0.9935567378997803, "metricx_score": 0.43623489141464233, "metricx_qe_score": 0.5307731032371521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果您想了解更多,请随时查看我们的仪表板以获取最新的分析结果,并阅读我们的论文。", "metrics": {"bleu_score": 50.04561521211421, "chrf_score": 41.63642651824865, "xcomet_score": 0.9871701002120972, "xcomet_qe_score": 0.9795891046524048, "metricx_score": 0.4665817618370056, "metricx_qe_score": 0.46203291416168213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是分大学的西元。", "metrics": {"bleu_score": 12.35459141795978, "chrf_score": 9.391628351012244, "xcomet_score": 0.5716046094894409, "xcomet_qe_score": 0.5941064953804016, "metricx_score": 5.4482035636901855, "metricx_qe_score": 6.026098251342773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将介绍我们从大型语言模型中提取独特脚本知识,用于受限语言规划的工作。", "metrics": {"bleu_score": 54.747831104198596, "chrf_score": 46.14813173275223, "xcomet_score": 0.8683730959892273, "xcomet_qe_score": 0.7755357027053833, "metricx_score": 1.767930269241333, "metricx_qe_score": 2.437570810317993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常通过遵循保证脚本中的逐步指令来规划行动。", "metrics": {"bleu_score": 34.45417567415884, "chrf_score": 30.361531504448457, "xcomet_score": 0.8048883676528931, "xcomet_qe_score": 0.8343623876571655, "metricx_score": 4.60129451751709, "metricx_qe_score": 4.668126583099365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前人的研究探索了使用语言模型为典型活动的抽象目标(", "metrics": {"bleu_score": 39.290984100672986, "chrf_score": 35.29284424226845, "xcomet_score": 0.5815879702568054, "xcomet_qe_score": 0.5180216431617737, "metricx_score": 7.116639137268066, "metricx_qe_score": 7.032413959503174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如做蛋糕)进行规划,并证明了大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 46.88011655210922, "chrf_score": 51.24751877991237, "xcomet_score": 0.36941879987716675, "xcomet_qe_score": 0.3354656994342804, "metricx_score": 4.423924446105957, "metricx_qe_score": 3.8122096061706543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,前人的研究主要集中在为典型活动的抽象目标进行规划。", "metrics": {"bleu_score": 50.191208077499894, "chrf_score": 46.12665346280716, "xcomet_score": 0.8342947959899902, "xcomet_qe_score": 0.8339084386825562, "metricx_score": 2.0408592224121094, "metricx_qe_score": 1.6946184635162354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具有具体目标和特定约束(如做巧克力蛋糕)的目标规划仍不够深入研究。", "metrics": {"bleu_score": 23.509511061626252, "chrf_score": 21.960343677403955, "xcomet_score": 0.946181058883667, "xcomet_qe_score": 0.9576261043548584, "metricx_score": 1.541843056678772, "metricx_qe_score": 1.7979934215545654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受限语言规划问题,该问题对规划目标施加不同的约束。", "metrics": {"bleu_score": 71.10805581099643, "chrf_score": 62.63744613632123, "xcomet_score": 0.9526617527008057, "xcomet_qe_score": 0.8940635919570923, "metricx_score": 1.352144479751587, "metricx_qe_score": 1.9776883125305176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被继承为具有更多、更复杂约束的现实生活中的具体目标。", "metrics": {"bleu_score": 30.39838244792881, "chrf_score": 29.81286497194159, "xcomet_score": 0.9821877479553223, "xcomet_qe_score": 0.9765433073043823, "metricx_score": 1.7334750890731812, "metricx_qe_score": 1.6027675867080688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个好的规划器应该编写既合理又忠实于约束的脚本。", "metrics": {"bleu_score": 45.90124565725132, "chrf_score": 39.689220816491456, "xcomet_score": 0.9031731486320496, "xcomet_qe_score": 0.7292155027389526, "metricx_score": 2.058521032333374, "metricx_qe_score": 4.354948997497559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并改进大型语言模型的受限语言规划能力。", "metrics": {"bleu_score": 78.8324467631105, "chrf_score": 71.24182139699383, "xcomet_score": 0.9718189239501953, "xcomet_qe_score": 0.9792938232421875, "metricx_score": 0.6621874570846558, "metricx_qe_score": 0.7382932901382446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有具体目标的数据集来支持我们的研究,我们首先需要获取这些目标。", "metrics": {"bleu_score": 73.19733690820138, "chrf_score": 68.94428315052402, "xcomet_score": 0.8560738563537598, "xcomet_qe_score": 0.8364834785461426, "metricx_score": 1.651846170425415, "metricx_qe_score": 2.7947335243225098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们使用指示TPT扩展抽象目标,为人类循环数据获取添加多方面约束。", "metrics": {"bleu_score": 35.59314093505875, "chrf_score": 23.83128991598649, "xcomet_score": 0.7043579816818237, "xcomet_qe_score": 0.7103123664855957, "metricx_score": 7.186753273010254, "metricx_qe_score": 6.626658916473389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采样了100个具体目标,并评估了由Light Logic模型生成的脚本。", "metrics": {"bleu_score": 59.44247793573109, "chrf_score": 50.90638060407983, "xcomet_score": 0.8171614408493042, "xcomet_qe_score": 0.833290696144104, "metricx_score": 3.455057144165039, "metricx_qe_score": 3.994900941848755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有Light Logic模型在具体目标规划上都取得了令人不满意的结果。", "metrics": {"bleu_score": 25.734328329959745, "chrf_score": 23.06135785603724, "xcomet_score": 0.8403745889663696, "xcomet_qe_score": 0.7858837842941284, "metricx_score": 4.908408164978027, "metricx_qe_score": 5.4182353019714355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析,调查Light Logic模型适用于什么。", "metrics": {"bleu_score": 22.923977041568804, "chrf_score": 17.613433850379323, "xcomet_score": 0.7341763973236084, "xcomet_qe_score": 0.6384564638137817, "metricx_score": 5.5049567222595215, "metricx_qe_score": 6.455977916717529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果显示,生成脚本的语义完整性是可以接受的。但对约束的忠实度无法保证。", "metrics": {"bleu_score": 54.09160778611691, "chrf_score": 46.69631849685199, "xcomet_score": 0.9702279567718506, "xcomet_qe_score": 0.9444953203201294, "metricx_score": 1.1054598093032837, "metricx_qe_score": 1.3125121593475342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了更多关于约束的分类,这些分类取决于醒来在家。", "metrics": {"bleu_score": 11.146727460890448, "chrf_score": 10.012910999452128, "xcomet_score": 0.390561580657959, "xcomet_qe_score": 0.4550008177757263, "metricx_score": 8.375753402709961, "metricx_qe_score": 9.035192489624023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,指示GPDs的规划性能对不同类别的目标有显著的差异。", "metrics": {"bleu_score": 45.461408092764664, "chrf_score": 32.5106414922082, "xcomet_score": 0.7056694030761719, "xcomet_qe_score": 0.7279342412948608, "metricx_score": 6.073802947998047, "metricx_qe_score": 5.976416110992432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前人的研究表明,Larry模型的输出质量具有高方差,导致性能不佳。", "metrics": {"bleu_score": 39.111042568065216, "chrf_score": 31.204913575935038, "xcomet_score": 0.797480583190918, "xcomet_qe_score": 0.8158513307571411, "metricx_score": 4.529305458068848, "metricx_qe_score": 4.541244029998779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用过度生成Zen过滤器的想法来提高生成质量。", "metrics": {"bleu_score": 48.22592931748994, "chrf_score": 39.91940949549645, "xcomet_score": 0.7966117858886719, "xcomet_qe_score": 0.7823513150215149, "metricx_score": 6.008409023284912, "metricx_qe_score": 6.27043342590332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示指示GPT的约束类型及其示例,并基于设定的抽象目标获取具体目标。", "metrics": {"bleu_score": 35.72973278162404, "chrf_score": 26.782242856533443, "xcomet_score": 0.7979191541671753, "xcomet_qe_score": 0.752761721611023, "metricx_score": 3.051177740097046, "metricx_qe_score": 2.7377021312713623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,指示GPT为具体目标过度生成关键脚本。", "metrics": {"bleu_score": 31.67161501220396, "chrf_score": 22.48931583856924, "xcomet_score": 0.7728356122970581, "xcomet_qe_score": 0.7815673351287842, "metricx_score": 3.412558078765869, "metricx_qe_score": 4.740457057952881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们开发了一个过滤模型来选择合适的脚本。", "metrics": {"bleu_score": 66.12331315716284, "chrf_score": 60.30778529569143, "xcomet_score": 0.9817410707473755, "xcomet_qe_score": 0.9639797210693359, "metricx_score": 0.5966489315032959, "metricx_qe_score": 0.8147799372673035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为指示GPT的位表示,并计算余弦相似度和相似度分数来衡量语义相似度。", "metrics": {"bleu_score": 63.25543918306823, "chrf_score": 48.287876394062614, "xcomet_score": 0.7053472399711609, "xcomet_qe_score": 0.6461795568466187, "metricx_score": 3.3445723056793213, "metricx_qe_score": 3.142866373062134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们将编写包含目标约束关键字的脚本。", "metrics": {"bleu_score": 41.37280342230792, "chrf_score": 36.627724141016074, "xcomet_score": 0.7887495756149292, "xcomet_qe_score": 0.7591727375984192, "metricx_score": 6.530531883239746, "metricx_qe_score": 7.062847137451172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们仅在目标在目标集中的得分最高时保留该脚本。", "metrics": {"bleu_score": 46.74166298426218, "chrf_score": 38.963586063876846, "xcomet_score": 0.8549363613128662, "xcomet_qe_score": 0.6264373064041138, "metricx_score": 3.6446125507354736, "metricx_qe_score": 5.864360332489014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用我们的方法,Inslacity可以生成更高质量的脚本。", "metrics": {"bleu_score": 70.98232254187813, "chrf_score": 46.55218769740056, "xcomet_score": 0.828636646270752, "xcomet_qe_score": 0.8163661956787109, "metricx_score": 4.703237056732178, "metricx_qe_score": 4.980257511138916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实度方面都显著提高了规划能力。", "metrics": {"bleu_score": 77.9072280774556, "chrf_score": 71.65730653818576, "xcomet_score": 0.9295946955680847, "xcomet_qe_score": 0.9775354862213135, "metricx_score": 0.7891128063201904, "metricx_qe_score": 1.2333762645721436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于部署大型语言模型的成本高昂,因此必须使更小、更专业的模型具备语言规划能力。", "metrics": {"bleu_score": 35.95160410407668, "chrf_score": 30.275529825054893, "xcomet_score": 0.9992703199386597, "xcomet_qe_score": 0.9952564239501953, "metricx_score": 0.5359265804290771, "metricx_qe_score": 0.9734448194503784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的重要步骤。", "metrics": {"bleu_score": 80.61898627027144, "chrf_score": 74.76670903477938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07551921904087067, "metricx_qe_score": 0.17079289257526398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,前人的研究没有实现具体目标的规划,而手动数据集标注成本高昂。", "metrics": {"bleu_score": 42.665923524557556, "chrf_score": 35.25619074438116, "xcomet_score": 0.866618275642395, "xcomet_qe_score": 0.8909716606140137, "metricx_score": 1.2868555784225464, "metricx_qe_score": 1.6296842098236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循象征性知识蒸馏的想法,来蒸馏受限语言规划模型。", "metrics": {"bleu_score": 29.823995415890053, "chrf_score": 25.177707688619293, "xcomet_score": 0.8374596238136292, "xcomet_qe_score": 0.7532237768173218, "metricx_score": 6.5159382820129395, "metricx_qe_score": 7.573988914489746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们应用我们的方法构建了一个受限语言规划数据集,命名为CodeScript。", "metrics": {"bleu_score": 41.72656642691111, "chrf_score": 44.38563047428555, "xcomet_score": 0.9264705181121826, "xcomet_qe_score": 0.8862978219985962, "metricx_score": 1.0047557353973389, "metricx_qe_score": 1.6103191375732422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了五万五个具体目标和", "metrics": {"bleu_score": 27.012073679928342, "chrf_score": 19.365293880142794, "xcomet_score": 0.7880661487579346, "xcomet_qe_score": 0.7902476191520691, "metricx_score": 6.261439323425293, "metricx_qe_score": 3.3133137226104736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "脚本,以确保验证和测试集的质量。我们要求云众包工人找到并修订错误的样本。", "metrics": {"bleu_score": 47.265993675484616, "chrf_score": 38.11204202273936, "xcomet_score": 0.617850124835968, "xcomet_qe_score": 0.5713768005371094, "metricx_score": 4.151494979858398, "metricx_qe_score": 4.6608991622924805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了CodeScript的约束分布。", "metrics": {"bleu_score": 58.33510584342546, "chrf_score": 72.24911196939357, "xcomet_score": 0.9625097513198853, "xcomet_qe_score": 0.8983068466186523, "metricx_score": 1.2738232612609863, "metricx_qe_score": 2.105375289916992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现CodeScript在生成的具体目标中验证了假设。", "metrics": {"bleu_score": 41.85674394203113, "chrf_score": 50.68266492785973, "xcomet_score": 0.7681400179862976, "xcomet_qe_score": 0.7009126543998718, "metricx_score": 4.756251811981201, "metricx_qe_score": 7.639558792114258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用CodeScript,我们可以训练更小但更专业的模型进行受限语言规划。", "metrics": {"bleu_score": 51.642924208861366, "chrf_score": 47.54129465445263, "xcomet_score": 0.8385931253433228, "xcomet_qe_score": 0.667853832244873, "metricx_score": 2.186046838760376, "metricx_qe_score": 1.9591779708862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,TFIL函数在CodeScript上可以生成比大多数大型语言模型更高质量的脚本,这表明当在合适的数据集上进行适当训练时,较小的模型可以支持较大的模型。", "metrics": {"bleu_score": 56.88646334344107, "chrf_score": 50.94788960121781, "xcomet_score": 0.6708053350448608, "xcomet_qe_score": 0.6994156837463379, "metricx_score": 6.297301292419434, "metricx_qe_score": 5.956608772277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了受限语言规划问题。", "metrics": {"bleu_score": 72.83860464220109, "chrf_score": 71.99900522537406, "xcomet_score": 0.9069202542304993, "xcomet_qe_score": 0.8321535587310791, "metricx_score": 1.9986854791641235, "metricx_qe_score": 2.526510000228882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受限语言规划能力,并开发了一种大型语言模型的过度生成过滤方法。", "metrics": {"bleu_score": 57.59696168886486, "chrf_score": 48.89475745775809, "xcomet_score": 0.8913486003875732, "xcomet_qe_score": 0.873232901096344, "metricx_score": 2.3216190338134766, "metricx_qe_score": 3.233598470687866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成了一个高质量的脚本数据集CodeScript,用于构造性语言规划。", "metrics": {"bleu_score": 46.398359087474546, "chrf_score": 48.63026647750803, "xcomet_score": 0.8730677366256714, "xcomet_qe_score": 0.8389794826507568, "metricx_score": 2.4084644317626953, "metricx_qe_score": 2.718571662902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望CodeScript数据集能够成为推进语言规划研究的宝贵资源。", "metrics": {"bleu_score": 78.47574847738748, "chrf_score": 80.68803758638448, "xcomet_score": 0.9200383424758911, "xcomet_qe_score": 0.9094783663749695, "metricx_score": 0.742880642414093, "metricx_qe_score": 0.9050089120864868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您的时间。", "metrics": {"bleu_score": 18.094495256969623, "chrf_score": 15.088860187765768, "xcomet_score": 0.9894214272499084, "xcomet_qe_score": 0.9905821084976196, "metricx_score": 0.24051934480667114, "metricx_qe_score": 0.7719036936759949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中找到CodeScript的更多细节。", "metrics": {"bleu_score": 51.62049167614268, "chrf_score": 58.1461071410989, "xcomet_score": 0.9381731152534485, "xcomet_qe_score": 0.9479387402534485, "metricx_score": 2.2265892028808594, "metricx_qe_score": 2.3877036571502686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是徐恒。", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 10.704692891649412, "xcomet_score": 0.8571552038192749, "xcomet_qe_score": 0.8540744781494141, "metricx_score": 0.046782102435827255, "metricx_qe_score": 0.12144069373607635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将展示我们论文《2003年内核命名实体标注器在2023年是否仍能良好运行?》", "metrics": {"bleu_score": 47.45997942359161, "chrf_score": 45.5225425916458, "xcomet_score": 0.7648207545280457, "xcomet_qe_score": 0.8179011344909668, "metricx_score": 5.633047580718994, "metricx_qe_score": 5.8285088539123535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.997875452041626, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了使用命名实体识别任务(NER任务)进行泛化问题。", "metrics": {"bleu_score": 61.113438984650095, "chrf_score": 53.23657089218561, "xcomet_score": 0.8949950933456421, "xcomet_qe_score": 0.8692381381988525, "metricx_score": 1.4231348037719727, "metricx_qe_score": 3.210730791091919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到模型已经使用2003年内核数据集开发NER近20年。这自然地提出了几个问题。", "metrics": {"bleu_score": 22.8379380989265, "chrf_score": 24.59411467271046, "xcomet_score": 0.7042328119277954, "xcomet_qe_score": 0.727380633354187, "metricx_score": 6.3951544761657715, "metricx_qe_score": 5.951580047607422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据,", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9714715480804443, "xcomet_qe_score": 0.9611616134643555, "metricx_score": 0.39283379912376404, "metricx_qe_score": 0.3439098596572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新标注器时,良好的泛化需要", "metrics": {"bleu_score": 34.5058111627145, "chrf_score": 31.179620923474566, "xcomet_score": 0.8399422764778137, "xcomet_qe_score": 0.7728564739227295, "metricx_score": 4.324852466583252, "metricx_qe_score": 5.20363712310791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么?如果我们观察到泛化效果不佳,导致这些模型性能下降的原因是什么?", "metrics": {"bleu_score": 29.246816212582647, "chrf_score": 24.052211692857956, "xcomet_score": 0.7474364042282104, "xcomet_qe_score": 0.6611367464065552, "metricx_score": 2.6939616203308105, "metricx_qe_score": 3.1162052154541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了内核++数据集,这是", "metrics": {"bleu_score": 41.65767636794606, "chrf_score": 30.42922260093814, "xcomet_score": 0.6863449811935425, "xcomet_qe_score": 0.7008990049362183, "metricx_score": 6.9807844161987305, "metricx_qe_score": 3.953677177429199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从2020年路透社新闻中收集并根据2003年内核标注指南进行标注的数据", "metrics": {"bleu_score": 30.731429867761612, "chrf_score": 30.911840289410854, "xcomet_score": 0.5674121379852295, "xcomet_qe_score": 0.5671020746231079, "metricx_score": 6.560257911682129, "metricx_qe_score": 6.532163619995117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "集。我们在2003年内核数据集上微调了20多个", "metrics": {"bleu_score": 11.488177632088458, "chrf_score": 17.380791312334832, "xcomet_score": 0.23049414157867432, "xcomet_qe_score": 0.27258458733558655, "metricx_score": 12.242919921875, "metricx_qe_score": 8.831880569458008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型,并在康纳3测试集和康纳++测试集上对其进行了评估。", "metrics": {"bleu_score": 29.256127307315065, "chrf_score": 24.042716694328423, "xcomet_score": 0.4460914134979248, "xcomet_qe_score": 0.4963075518608093, "metricx_score": 3.7485268115997314, "metricx_qe_score": 4.006644248962402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了F1值的百分比变化,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 63.05914424660905, "chrf_score": 58.92469995094426, "xcomet_score": 0.9955589771270752, "xcomet_qe_score": 0.9920499324798584, "metricx_score": 0.4898105561733246, "metricx_qe_score": 0.731980562210083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,良好的泛化需要什么?", "metrics": {"bleu_score": 38.71551944619038, "chrf_score": 31.114474244360814, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.35357362031936646, "metricx_qe_score": 0.43413427472114563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必要的。", "metrics": {"bleu_score": 23.546900700701926, "chrf_score": 20.854627924910012, "xcomet_score": 0.9906909465789795, "xcomet_qe_score": 0.9880638122558594, "metricx_score": 0.6886283159255981, "metricx_qe_score": 0.9275823831558228, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过实验,我们发现变压器模型通常能更好地泛化到新数据。", "metrics": {"bleu_score": 23.982150348391624, "chrf_score": 17.787954582856884, "xcomet_score": 0.8401480913162231, "xcomet_qe_score": 0.8556380271911621, "metricx_score": 1.9071201086044312, "metricx_qe_score": 1.0139871835708618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个要素是模型大小。", "metrics": {"bleu_score": 39.59377364332708, "chrf_score": 33.623818066701524, "xcomet_score": 0.9399413466453552, "xcomet_qe_score": 0.8579148054122925, "metricx_score": 0.16105416417121887, "metricx_qe_score": 0.293008953332901, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常较大规模的模型会导致更好的泛化。", "metrics": {"bleu_score": 48.7859542097654, "chrf_score": 37.91445711724349, "xcomet_score": 0.96602463722229, "xcomet_qe_score": 0.9677320718765259, "metricx_score": 0.7494829893112183, "metricx_qe_score": 0.8581783771514893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们都知道微调示例数量直接影响下游任务的性能。", "metrics": {"bleu_score": 60.118742660358826, "chrf_score": 51.80896427500612, "xcomet_score": 0.9146497249603271, "xcomet_qe_score": 0.8075738549232483, "metricx_score": 1.4433249235153198, "metricx_qe_score": 1.2116390466690063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们也发现更多的微调示例实际上也导致更好的泛化。", "metrics": {"bleu_score": 57.59014490653237, "chrf_score": 52.21250203451425, "xcomet_score": 0.9557837247848511, "xcomet_qe_score": 0.867637038230896, "metricx_score": 1.0530396699905396, "metricx_qe_score": 1.152901291847229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于我们的下一个问题,导致某些模型性能下降的原因是什么?我们有两个假设。", "metrics": {"bleu_score": 48.26764489049477, "chrf_score": 40.109663203828276, "xcomet_score": 0.9234381914138794, "xcomet_qe_score": 0.9215957522392273, "metricx_score": 0.6875584721565247, "metricx_qe_score": 0.8662148118019104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是适应性过拟合,即由于反复使用同一测试集而引起的过拟合,这通常表现为在新测试集上的收益递减。", "metrics": {"bleu_score": 41.70720397951796, "chrf_score": 33.546448589116515, "xcomet_score": 0.9719189405441284, "xcomet_qe_score": 0.8994002342224121, "metricx_score": 2.6139168739318848, "metricx_qe_score": 3.240830183029175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,即由于训练数据和测试数据之间的时间差距不断扩大而导致的性能下降。", "metrics": {"bleu_score": 59.75281862052228, "chrf_score": 55.53728510769358, "xcomet_score": 0.9621487855911255, "xcomet_qe_score": 0.8881708383560181, "metricx_score": 1.5137563943862915, "metricx_qe_score": 2.056264877319336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于过拟合问题,我们从右图中看到,最佳拟合线的梯度大于1。", "metrics": {"bleu_score": 54.91419921039114, "chrf_score": 49.56379179596273, "xcomet_score": 0.9359540939331055, "xcomet_qe_score": 0.8455970883369446, "metricx_score": 1.1965726613998413, "metricx_qe_score": 1.8906269073486328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在2003年内核数据集上所做的每一次改进,在内核++数据集上都超过了1次改进,这意味着没有收益递减,", "metrics": {"bleu_score": 29.027630375830125, "chrf_score": 26.15329559503398, "xcomet_score": 0.5531560182571411, "xcomet_qe_score": 0.5629096627235413, "metricx_score": 5.987607002258301, "metricx_qe_score": 5.831797122955322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明适应性过拟合在这种情况下没有观察到。", "metrics": {"bleu_score": 46.55957504543378, "chrf_score": 38.40227610805727, "xcomet_score": 0.873185396194458, "xcomet_qe_score": 0.9240709543228149, "metricx_score": 2.0435357093811035, "metricx_qe_score": 2.137998342514038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间漂移呢?", "metrics": {"bleu_score": 52.47357977607325, "chrf_score": 40.51960415097498, "xcomet_score": 0.9311673641204834, "xcomet_qe_score": 0.9159005284309387, "metricx_score": 0.3731555640697479, "metricx_qe_score": 0.8887962102890015, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一个实验,用更新的数据重新训练或继续预训练一些模型,我们发现性能随着时间差距的扩大而下降,这证实了我们关于性能下降主要原因的时间漂移的假设。", "metrics": {"bleu_score": 52.709137443873566, "chrf_score": 45.157289335962766, "xcomet_score": 0.9286206960678101, "xcomet_qe_score": 0.9447640776634216, "metricx_score": 1.7702853679656982, "metricx_qe_score": 1.707936406135559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化,我们需要更好的模型架构、更大的模型大小以及更多的微调示例,", "metrics": {"bleu_score": 78.02818594780945, "chrf_score": 70.75630653147978, "xcomet_score": 0.9520596265792847, "xcomet_qe_score": 0.8978762626647949, "metricx_score": 0.48480430245399475, "metricx_qe_score": 0.47694119811058044, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是相辅相成的。我们不能只保留一个要素而抛弃其他要素。", "metrics": {"bleu_score": 57.832738849365654, "chrf_score": 49.09118995823331, "xcomet_score": 0.9929730892181396, "xcomet_qe_score": 0.9804630279541016, "metricx_score": 0.3897319436073303, "metricx_qe_score": 0.5170450806617737, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现性能下降是由时间漂移引起的,令人惊讶的是,它不是由适应性过拟合引起的,尽管2003年内核数据集已经使用了20多年。", "metrics": {"bleu_score": 51.753056030261355, "chrf_score": 42.11659246525244, "xcomet_score": 0.8245092034339905, "xcomet_qe_score": 0.7643091678619385, "metricx_score": 4.510409832000732, "metricx_qe_score": 3.8918333053588867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以回到我们论文标题中提出的问题,2003年内核标注器在2023年是否仍能运行?", "metrics": {"bleu_score": 56.00251936785401, "chrf_score": 48.45011723440716, "xcomet_score": 0.8058954477310181, "xcomet_qe_score": 0.8010059595108032, "metricx_score": 3.8434696197509766, "metricx_qe_score": 3.555922508239746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案是确定的。", "metrics": {"bleu_score": 51.52305043033997, "chrf_score": 43.75349044047867, "xcomet_score": 0.9718999862670898, "xcomet_qe_score": 0.9591522216796875, "metricx_score": 0.9511160850524902, "metricx_qe_score": 1.729648232460022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能够呼吁更多关于如何改进模型泛化能力的研究。", "metrics": {"bleu_score": 75.46697757057083, "chrf_score": 67.58428054868546, "xcomet_score": 0.9331028461456299, "xcomet_qe_score": 0.9389433264732361, "metricx_score": 1.1533108949661255, "metricx_qe_score": 0.9588073492050171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果有任何问题,请随时联系我。", "metrics": {"bleu_score": 46.73405285184296, "chrf_score": 43.72887767159791, "xcomet_score": 0.9854254722595215, "xcomet_qe_score": 0.9682124853134155, "metricx_score": 0.2657138705253601, "metricx_qe_score": 0.2952011525630951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9850989580154419, "xcomet_qe_score": 0.9753036499023438, "metricx_score": 0.186608225107193, "metricx_qe_score": 0.06603709608316422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将讨论我们在解决实体选择中的间接指代表达方面的工作,其中我们引入了altentity语料库。", "metrics": {"bleu_score": 28.197820345099252, "chrf_score": 23.771787910394707, "xcomet_score": 0.7814117670059204, "xcomet_qe_score": 0.7407323122024536, "metricx_score": 3.3889176845550537, "metricx_qe_score": 3.9373011589050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫Javot Hosseini,这是我与Philip Radlinsky、Silvia Pareti和Annie Luis的合作项目。", "metrics": {"bleu_score": 29.10708729782026, "chrf_score": 60.86398872552886, "xcomet_score": 0.7655125260353088, "xcomet_qe_score": 0.7513605952262878, "metricx_score": 4.4525322914123535, "metricx_qe_score": 2.4778294563293457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请考虑这个替代问题:", "metrics": {"bleu_score": 45.180100180492246, "chrf_score": 34.71560846560846, "xcomet_score": 0.8932609558105469, "xcomet_qe_score": 0.8832759857177734, "metricx_score": 0.3326420187950134, "metricx_qe_score": 0.30451762676239014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说“对我温柔点”还是“我有一种感觉”", "metrics": {"bleu_score": 9.74812453975988, "chrf_score": 6.125390699519614, "xcomet_score": 0.6221586465835571, "xcomet_qe_score": 0.33388346433639526, "metricx_score": 2.5545268058776855, "metricx_qe_score": 2.590590000152588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?在这里,用户想要在这两个标志中选择一个。", "metrics": {"bleu_score": 23.198210427894825, "chrf_score": 21.41882483987747, "xcomet_score": 0.7244203090667725, "xcomet_qe_score": 0.7214442491531372, "metricx_score": 4.663220405578613, "metricx_qe_score": 5.350669860839844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用,例如说出歌曲“对我温柔点”的名字或其位置(第一个),但", "metrics": {"bleu_score": 43.73477036855247, "chrf_score": 35.98133526592778, "xcomet_score": 0.45428022742271423, "xcomet_qe_score": 0.3713657557964325, "metricx_score": 5.253976821899414, "metricx_qe_score": 4.674365520477295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时间接引用更合适,可以让对话更自然。这可能发生", "metrics": {"bleu_score": 10.624917754018972, "chrf_score": 14.926548808949548, "xcomet_score": 0.43083804845809937, "xcomet_qe_score": 0.421271413564682, "metricx_score": 11.015317916870117, "metricx_qe_score": 8.957549095153809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在用户无法记住歌曲", "metrics": {"bleu_score": 2.6931337349958566, "chrf_score": 5.635431568907964, "xcomet_score": 0.5954566597938538, "xcomet_qe_score": 0.6149927973747253, "metricx_score": 5.718942642211914, "metricx_qe_score": 3.7379283905029297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的名字、发音太相似难以区分", "metrics": {"bleu_score": 22.57369278980567, "chrf_score": 23.415037801887305, "xcomet_score": 0.716791033744812, "xcomet_qe_score": 0.6899905800819397, "metricx_score": 4.668432712554932, "metricx_qe_score": 5.116121292114258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",或用户想表达偏好时。", "metrics": {"bleu_score": 7.733019124048347, "chrf_score": 11.21844525588703, "xcomet_score": 0.9719264507293701, "xcomet_qe_score": 0.9729541540145874, "metricx_score": 2.9478228092193604, "metricx_qe_score": 1.5716501474380493, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是间接引用的示例,例如,“较新的那个”或“不是充满活力的歌曲”。这", "metrics": {"bleu_score": 20.455163269401236, "chrf_score": 19.020519987075964, "xcomet_score": 0.6881617307662964, "xcomet_qe_score": 0.6880168914794922, "metricx_score": 5.609236717224121, "metricx_qe_score": 2.8325202465057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在对话系统中是一个重要问题,也是评估大型语言模型(LLM)实体理解能力的重要基准。据我们所知,", "metrics": {"bleu_score": 27.432192697988814, "chrf_score": 27.050458286947542, "xcomet_score": 0.7122368812561035, "xcomet_qe_score": 0.5109877586364746, "metricx_score": 4.521670341491699, "metricx_qe_score": 3.315880298614502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前没有一个公开的大型数据集来解决这个问题,所以我们使用众包标注方式收集了一个。", "metrics": {"bleu_score": 19.23104277783772, "chrf_score": 17.922063743922507, "xcomet_score": 0.8448631763458252, "xcomet_qe_score": 0.8506184816360474, "metricx_score": 2.5252246856689453, "metricx_qe_score": 1.5830392837524414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同领域:音乐、书籍和食谱。", "metrics": {"bleu_score": 71.34491809428187, "chrf_score": 61.46444256226865, "xcomet_score": 0.9999072551727295, "xcomet_qe_score": 0.9905967116355896, "metricx_score": 0.1999645233154297, "metricx_qe_score": 0.31799226999282837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通完成设置。", "metrics": {"bleu_score": 73.41087329408752, "chrf_score": 65.98538196580246, "xcomet_score": 0.8596484065055847, "xcomet_qe_score": 0.8180302977561951, "metricx_score": 2.6962547302246094, "metricx_qe_score": 4.135033130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通中有三个对话气泡。", "metrics": {"bleu_score": 36.72056269893591, "chrf_score": 27.86075036075036, "xcomet_score": 0.7892798185348511, "xcomet_qe_score": 0.7679256200790405, "metricx_score": 0.9351893663406372, "metricx_qe_score": 0.7981018424034119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡中,鲍勃说:“记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 67.74288846573828, "chrf_score": 59.84136894128118, "xcomet_score": 0.9877476692199707, "xcomet_qe_score": 0.9042569994926453, "metricx_score": 1.2501301765441895, "metricx_qe_score": 1.5384814739227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从而设定了对话背景。", "metrics": {"bleu_score": 10.764345432696361, "chrf_score": 8.851655444505358, "xcomet_score": 0.8625693321228027, "xcomet_qe_score": 0.8342990875244141, "metricx_score": 3.0627455711364746, "metricx_qe_score": 3.220067262649536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个气泡中,爱丽丝说:“你是说‘对我温柔点’还是‘我有一种感觉’?”", "metrics": {"bleu_score": 33.2550724099814, "chrf_score": 20.787653044174654, "xcomet_score": 0.8678627610206604, "xcomet_qe_score": 0.8458603620529175, "metricx_score": 4.0137128829956055, "metricx_qe_score": 4.4741716384887695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8669623136520386, "xcomet_qe_score": 0.8554072380065918, "metricx_score": 1.420009732246399, "metricx_qe_score": 2.3006839752197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个气泡中,鲍勃使用间接引用来选择这两个实体中的一个,例如“Neo-Ervandal”。", "metrics": {"bleu_score": 35.04520460941293, "chrf_score": 27.360614106217433, "xcomet_score": 0.659812331199646, "xcomet_qe_score": 0.6336613297462463, "metricx_score": 7.974658489227295, "metricx_qe_score": 8.667850494384766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个气泡,而第三个由标注者填写。", "metrics": {"bleu_score": 45.545409381989415, "chrf_score": 41.13843532542202, "xcomet_score": 0.7972917556762695, "xcomet_qe_score": 0.7736347913742065, "metricx_score": 3.414783239364624, "metricx_qe_score": 3.599945545196533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个气泡从每个领域的几个手动提示中选择。", "metrics": {"bleu_score": 50.37528966367497, "chrf_score": 45.68491850340562, "xcomet_score": 0.7144255042076111, "xcomet_qe_score": 0.6865985989570618, "metricx_score": 4.10931921005249, "metricx_qe_score": 4.262779712677002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个气泡(替代问题)的生成方法如下:", "metrics": {"bleu_score": 11.451997463067551, "chrf_score": 12.81016956709097, "xcomet_score": 0.7761797904968262, "xcomet_qe_score": 0.7747563123703003, "metricx_score": 4.247628688812256, "metricx_qe_score": 2.9806368350982666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们始终使用一个简单的模板:“", "metrics": {"bleu_score": 31.53554052490131, "chrf_score": 30.657015242455866, "xcomet_score": 0.937163233757019, "xcomet_qe_score": 0.9382956027984619, "metricx_score": 0.47934263944625854, "metricx_qe_score": 0.20472665131092072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说A还是B?", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 79.70238095238096, "xcomet_score": 0.9700545072555542, "xcomet_qe_score": 0.9737184047698975, "metricx_score": 0.24424469470977783, "metricx_qe_score": 0.8488398790359497, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”其中A和B从维基百科中采样。", "metrics": {"bleu_score": 25.33654946448646, "chrf_score": 22.463078360723806, "xcomet_score": 0.9153969287872314, "xcomet_qe_score": 0.8486098051071167, "metricx_score": 1.626441240310669, "metricx_qe_score": 1.8692810535430908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同采样方法。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 71.19713619713619, "xcomet_score": 0.9981815814971924, "xcomet_qe_score": 1.0, "metricx_score": 0.14823222160339355, "metricx_qe_score": 0.255437970161438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体之间变得越来越相似,通常更难进行区分。", "metrics": {"bleu_score": 45.312014285371994, "chrf_score": 42.05534507308681, "xcomet_score": 0.9804545640945435, "xcomet_qe_score": 0.899333655834198, "metricx_score": 1.1963937282562256, "metricx_qe_score": 1.7852957248687744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀随机采样,", "metrics": {"bleu_score": 18.36028134946796, "chrf_score": 15.942760942760941, "xcomet_score": 0.8311265707015991, "xcomet_qe_score": 0.8106340169906616, "metricx_score": 2.012704610824585, "metricx_qe_score": 1.2656458616256714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个是当实体有相似标题时,例如两本书同名,第三个是当", "metrics": {"bleu_score": 6.927572174868464, "chrf_score": 10.571472345641181, "xcomet_score": 0.42921385169029236, "xcomet_qe_score": 0.32657861709594727, "metricx_score": 6.4577155113220215, "metricx_qe_score": 5.2656073570251465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在维基百科上有相似描述,", "metrics": {"bleu_score": 49.07480275466053, "chrf_score": 51.81358449139996, "xcomet_score": 0.8914105892181396, "xcomet_qe_score": 0.8020381927490234, "metricx_score": 2.4243931770324707, "metricx_qe_score": 2.9621691703796387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后是当它们在维基百科上有相似信息框或属性,", "metrics": {"bleu_score": 61.802814246643806, "chrf_score": 52.266279772172176, "xcomet_score": 0.9731125831604004, "xcomet_qe_score": 0.9789520502090454, "metricx_score": 1.3665517568588257, "metricx_qe_score": 1.504441738128662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如歌曲的同一流派或同一艺术家。", "metrics": {"bleu_score": 18.837359015759088, "chrf_score": 20.455237912897264, "xcomet_score": 0.8735014200210571, "xcomet_qe_score": 0.859404444694519, "metricx_score": 3.3879032135009766, "metricx_qe_score": 4.2714409828186035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向标注者展示这个替代问题时,他们知道这些实体的名字,但并不一定了解这些实体。", "metrics": {"bleu_score": 49.88134801589763, "chrf_score": 41.0472556635419, "xcomet_score": 0.8193317651748657, "xcomet_qe_score": 0.7772542238235474, "metricx_score": 2.487290382385254, "metricx_qe_score": 2.7071003913879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们展示了一些关于两个实体的背景知识。", "metrics": {"bleu_score": 39.62652780998505, "chrf_score": 32.581610642339946, "xcomet_score": 0.893513560295105, "xcomet_qe_score": 0.7692920565605164, "metricx_score": 1.1326160430908203, "metricx_qe_score": 2.1494197845458984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地显示每个歌曲的Google搜索链接,然后要求标注者至少听一些每首歌,并阅读关于每首歌的内容。", "metrics": {"bleu_score": 32.178622815606055, "chrf_score": 25.706641252159628, "xcomet_score": 0.8458753824234009, "xcomet_qe_score": 0.8597544431686401, "metricx_score": 3.167767286300659, "metricx_qe_score": 2.286951780319214, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这是歌曲“对我温柔点”的Google搜索结果。", "metrics": {"bleu_score": 39.1080327529236, "chrf_score": 26.35475856473024, "xcomet_score": 0.9715429544448853, "xcomet_qe_score": 0.9630374908447266, "metricx_score": 1.2029898166656494, "metricx_qe_score": 0.9228114485740662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们显示维基百科上的部分背景文本。", "metrics": {"bleu_score": 50.897863478027745, "chrf_score": 42.75056656193378, "xcomet_score": 0.9849952459335327, "xcomet_qe_score": 0.9220565557479858, "metricx_score": 0.7080748677253723, "metricx_qe_score": 1.1074450016021729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还显示它们的图片,同样来自维基百科,以便标注者知道它们的样子。", "metrics": {"bleu_score": 42.38313448950254, "chrf_score": 34.02836162821563, "xcomet_score": 0.9053576588630676, "xcomet_qe_score": 0.9491674304008484, "metricx_score": 1.638076901435852, "metricx_qe_score": 1.6173845529556274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求标注者选择这些实体中的一个,例如这里的第一个,并使用三个到五个间接指代表达来描述它们。", "metrics": {"bleu_score": 42.32826782289282, "chrf_score": 34.64647304380629, "xcomet_score": 0.7554370164871216, "xcomet_qe_score": 0.7466045618057251, "metricx_score": 3.6325721740722656, "metricx_qe_score": 3.6508538722991943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,“有钢琴音乐的那个”,", "metrics": {"bleu_score": 13.912311644176565, "chrf_score": 15.07446542575158, "xcomet_score": 0.9625315070152283, "xcomet_qe_score": 0.8450585603713989, "metricx_score": 0.8430609703063965, "metricx_qe_score": 0.7225437760353088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一些示例", "metrics": {"bleu_score": 79.65485887268619, "chrf_score": 77.90961288523812, "xcomet_score": 0.970958948135376, "xcomet_qe_score": 0.9550706148147583, "metricx_score": 0.7042840123176575, "metricx_qe_score": 1.6512179374694824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ":\"没有字的那个,不是有12岁男孩的,不是虚构的,来自阿塞拜疆\"等等。", "metrics": {"bleu_score": 32.43841112747697, "chrf_score": 30.614412320673317, "xcomet_score": 0.6237236857414246, "xcomet_qe_score": 0.5414654612541199, "metricx_score": 3.1595237255096436, "metricx_qe_score": 4.7197041511535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "altentities语料库包含三个领域的6,000个替代问题,以及42,000个间接指代表达。", "metrics": {"bleu_score": 28.141865359626077, "chrf_score": 46.527194428220575, "xcomet_score": 0.6039918661117554, "xcomet_qe_score": 0.5504034161567688, "metricx_score": 5.438665390014648, "metricx_qe_score": 6.406385898590088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是使用T5xLarge模型的结果摘要。", "metrics": {"bleu_score": 30.315070099566324, "chrf_score": 28.042108430416008, "xcomet_score": 0.9089742302894592, "xcomet_qe_score": 0.9009913802146912, "metricx_score": 1.99981689453125, "metricx_qe_score": 1.9963170289993286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者相同的背景知识,准确率非常高,约为92-95%。", "metrics": {"bleu_score": 29.213734658373276, "chrf_score": 27.88688694928997, "xcomet_score": 0.9027986526489258, "xcomet_qe_score": 0.9539575576782227, "metricx_score": 1.2132482528686523, "metricx_qe_score": 0.7822532653808594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这不现实。", "metrics": {"bleu_score": 28.49181887722137, "chrf_score": 23.71472002904075, "xcomet_score": 0.9991846084594727, "xcomet_qe_score": 0.9858999848365784, "metricx_score": 0.04951542243361473, "metricx_qe_score": 0.05270039662718773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有部分重叠的背景知识,准确率在82%到87%之间,这更现实,", "metrics": {"bleu_score": 52.41456780893763, "chrf_score": 51.23099829763845, "xcomet_score": 0.9205024838447571, "xcomet_qe_score": 0.9307816028594971, "metricx_score": 1.7873759269714355, "metricx_qe_score": 1.6224406957626343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9950563907623291, "xcomet_qe_score": 0.9950027465820312, "metricx_score": 0.39887312054634094, "metricx_qe_score": 0.4570527672767639, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,准确率仅为60%。因此,改进的空间很大。", "metrics": {"bleu_score": 49.2626920829124, "chrf_score": 45.552123812814486, "xcomet_score": 0.9959844350814819, "xcomet_qe_score": 0.9925920963287354, "metricx_score": 1.569696068763733, "metricx_qe_score": 2.2667810916900635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还证明了模型具有领域泛化能力。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 44.723889306558185, "xcomet_score": 0.9255028367042542, "xcomet_qe_score": 0.9132012724876404, "metricx_score": 0.5445807576179504, "metricx_qe_score": 0.6280920505523682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自多伦多大学和布鲁诺·凯斯勒基金会的莎拉·帕皮,我将简要介绍关注作为同时语音翻译指南的论文,这是与马特奥·内格里和马可·土耳其合作的成果。", "metrics": {"bleu_score": 33.739284240297536, "chrf_score": 24.919681604784756, "xcomet_score": 0.4527788758277893, "xcomet_qe_score": 0.41078123450279236, "metricx_score": 6.01414680480957, "metricx_qe_score": 5.6799702644348145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同时语音翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.8608854413032532, "xcomet_qe_score": 0.8539649248123169, "metricx_score": 0.5060197114944458, "metricx_qe_score": 0.22112929821014404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时语音翻译或模拟翻译是将口语实时翻译成另一门语言的文本的过程,实现跨语言交流。", "metrics": {"bleu_score": 59.35107565052857, "chrf_score": 46.80056979079286, "xcomet_score": 0.8465975522994995, "xcomet_qe_score": 0.8380471467971802, "metricx_score": 3.0564656257629395, "metricx_qe_score": 3.0006515979766846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前模拟翻译模型存在哪些问题?", "metrics": {"bleu_score": 22.229849552064017, "chrf_score": 16.066384503197547, "xcomet_score": 0.8008075952529907, "xcomet_qe_score": 0.8048803210258484, "metricx_score": 0.8828709125518799, "metricx_qe_score": 1.2078787088394165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特定的架构通常通过引入额外的优化模块来训练,", "metrics": {"bleu_score": 23.01871187126095, "chrf_score": 22.573517560816665, "xcomet_score": 0.8131506443023682, "xcomet_qe_score": 0.863726794719696, "metricx_score": 2.452298402786255, "metricx_qe_score": 2.64402174949646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练过程长且复杂,例如涉及不同的优化目标,以及训练和", "metrics": {"bleu_score": 41.261520349079454, "chrf_score": 35.26016534316361, "xcomet_score": 0.7404557466506958, "xcomet_qe_score": 0.7082194089889526, "metricx_score": 5.455803871154785, "metricx_qe_score": 3.0559470653533936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "维护多个模型以达到不同的延迟等级,比如", "metrics": {"bleu_score": 36.61504394605677, "chrf_score": 34.01147691184308, "xcomet_score": 0.7435183525085449, "xcomet_qe_score": 0.7181123495101929, "metricx_score": 3.3741884231567383, "metricx_qe_score": 2.75883150100708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练一个平均延迟一秒的模型,另一个延迟两秒的模型,依此类推。", "metrics": {"bleu_score": 55.45765189984488, "chrf_score": 45.26561784720231, "xcomet_score": 0.9541332721710205, "xcomet_qe_score": 0.9458678960800171, "metricx_score": 0.5707550644874573, "metricx_qe_score": 1.017838716506958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么我们的解决方案是什么?", "metrics": {"bleu_score": 84.46319809857219, "chrf_score": 83.69464817082437, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.01612722873687744, "metricx_qe_score": 0.2782573699951172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用已存在的离线语音翻译模型,无需重新训练或采用特定的架构进行实时语音翻译。", "metrics": {"bleu_score": 37.97470328722402, "chrf_score": 32.61264720828264, "xcomet_score": 0.8109591007232666, "xcomet_qe_score": 0.902194619178772, "metricx_score": 0.8870835900306702, "metricx_qe_score": 1.000063419342041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为每个延迟等级使用一个模型,通过特定参数处理延迟,并利用", "metrics": {"bleu_score": 42.85212134035147, "chrf_score": 34.94946146355174, "xcomet_score": 0.6958848237991333, "xcomet_qe_score": 0.4627745449542999, "metricx_score": 5.240779399871826, "metricx_qe_score": 1.1683671474456787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入与文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 50.784808857111656, "chrf_score": 45.73496887745307, "xcomet_score": 0.7798480987548828, "xcomet_qe_score": 0.7321901321411133, "metricx_score": 6.265567779541016, "metricx_qe_score": 4.432774066925049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")已经获得的知识。如图右所示,我们的", "metrics": {"bleu_score": 2.6737739141846273, "chrf_score": 2.6595744680851063, "xcomet_score": 0.19305384159088135, "xcomet_qe_score": 0.14111457765102386, "metricx_score": 15.039191246032715, "metricx_qe_score": 11.32963752746582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决方案是提出点或编码器-解码器注意力机制,这是一种策略,我们根据注意力指向决定是否输出部分翻译。", "metrics": {"bleu_score": 62.150250294269725, "chrf_score": 50.94308966224311, "xcomet_score": 0.7291521430015564, "xcomet_qe_score": 0.6905739307403564, "metricx_score": 6.046966075897217, "metricx_qe_score": 6.8933610916137695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力不集中,即该和值低于某个阈值α,向最后λ个语音帧,则说明接收的信息足够稳定", "metrics": {"bleu_score": 34.95594645733589, "chrf_score": 31.48207650694236, "xcomet_score": 0.7019875049591064, "xcomet_qe_score": 0.6163358688354492, "metricx_score": 6.904662132263184, "metricx_qe_score": 6.658571243286133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",可以输出一个词。例如,如果我们接收一个语音片段“我要讲的是”,我们的模型预测德语翻译,观察交叉注意力权重,我们会看到前两个词指向最早接收的语音帧,最后一个词指向最后接收的语音帧作为λ个语音帧。", "metrics": {"bleu_score": 35.80471226303191, "chrf_score": 26.698077627381195, "xcomet_score": 0.34166958928108215, "xcomet_qe_score": 0.20621249079704285, "metricx_score": 5.8760175704956055, "metricx_qe_score": 6.565317153930664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被输出,而由于交叉注意力之和高于阈值α,我们不会输出最后一个词,而是等待下一个语音片段。", "metrics": {"bleu_score": 47.615456489965275, "chrf_score": 39.87971530380759, "xcomet_score": 0.7776170969009399, "xcomet_qe_score": 0.7545892000198364, "metricx_score": 1.5982370376586914, "metricx_qe_score": 1.7726728916168213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果继续接收另一个语音片段,模型预测另外三个词,观察交叉注意力权重,我们会看到没有词指向最后λ个语音帧。", "metrics": {"bleu_score": 33.98297945413019, "chrf_score": 28.545045114278057, "xcomet_score": 0.64015793800354, "xcomet_qe_score": 0.6424734592437744, "metricx_score": 2.9857096672058105, "metricx_qe_score": 3.946810722351074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将", "metrics": {"bleu_score": 29.472584782019705, "chrf_score": 26.455018219457934, "xcomet_score": 0.841911792755127, "xcomet_qe_score": 0.8360384106636047, "metricx_score": 5.485178470611572, "metricx_qe_score": 5.510756969451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "被输出。 从主要结果来看,我们将同时语音翻译结果绘制在图表上,蓝色一侧测量翻译质量和平均滞后(即延迟测量),我们还考虑计算感知平均滞后,包括模型预测输出的计算", "metrics": {"bleu_score": 23.43755879553401, "chrf_score": 20.398483217712364, "xcomet_score": 0.19167888164520264, "xcomet_qe_score": 0.14022955298423767, "metricx_score": 10.086533546447754, "metricx_qe_score": 10.94675350189209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "时间。我们希望曲线在这个图表上尽可能高,", "metrics": {"bleu_score": 28.65515486360906, "chrf_score": 24.49665206683409, "xcomet_score": 0.7352945804595947, "xcomet_qe_score": 0.6861701011657715, "metricx_score": 5.223174095153809, "metricx_qe_score": 5.187615394592285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时也希望它们向左移动。", "metrics": {"bleu_score": 68.31579242909528, "chrf_score": 66.77682733422068, "xcomet_score": 0.9827525615692139, "xcomet_qe_score": 0.9589202404022217, "metricx_score": 0.7562371492385864, "metricx_qe_score": 1.0575345754623413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们与离线模型上应用的准备策略(即权重键策略和局部一致性)进行比较,", "metrics": {"bleu_score": 20.053583653512707, "chrf_score": 13.367672822505373, "xcomet_score": 0.6455261707305908, "xcomet_qe_score": 0.6198527216911316, "metricx_score": 5.898680210113525, "metricx_qe_score": 6.460427284240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也与专门针对同时语音翻译设计的最新架构进行比较。", "metrics": {"bleu_score": 33.06242912975579, "chrf_score": 28.614906440103642, "xcomet_score": 0.9241805076599121, "xcomet_qe_score": 0.765183687210083, "metricx_score": 1.5355663299560547, "metricx_qe_score": 2.3094916343688965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是德国语同时语音翻译策略的所有结果,我们", "metrics": {"bleu_score": 24.41685669990165, "chrf_score": 25.228354517457817, "xcomet_score": 0.7747829556465149, "xcomet_qe_score": 0.7700796127319336, "metricx_score": 5.9645795822143555, "metricx_qe_score": 1.883505940437317, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到点策略优于所有应用在离线模型上的策略,因为它们的曲线向左移动,", "metrics": {"bleu_score": 38.652758784697276, "chrf_score": 34.654440177839554, "xcomet_score": 0.8058764934539795, "xcomet_qe_score": 0.6846331357955933, "metricx_score": 3.830892562866211, "metricx_qe_score": 4.268856525421143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果考虑实际耗时或计算耗时,这是最快的策略。", "metrics": {"bleu_score": 43.66004106565595, "chrf_score": 39.648365429365285, "xcomet_score": 0.9721996784210205, "xcomet_qe_score": 0.9603465795516968, "metricx_score": 1.6923828125, "metricx_qe_score": 1.6526622772216797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如需了解更多结果,请阅读我们的论文,", "metrics": {"bleu_score": 59.54598909380218, "chrf_score": 51.98717383311625, "xcomet_score": 0.9769407510757446, "xcomet_qe_score": 0.962398886680603, "metricx_score": 0.10051217675209045, "metricx_qe_score": 0.24710455536842346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源了代码、模型和同时输出,以促进我们工作的可复现性。", "metrics": {"bleu_score": 31.328783399533368, "chrf_score": 29.23183234730089, "xcomet_score": 0.8708584308624268, "xcomet_qe_score": 0.8646441698074341, "metricx_score": 1.1883141994476318, "metricx_qe_score": 1.4080504179000854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家的关注。", "metrics": {"bleu_score": 36.55552228545123, "chrf_score": 49.96037865158913, "xcomet_score": 0.9990874528884888, "xcomet_qe_score": 0.9970511198043823, "metricx_score": 0.5129700303077698, "metricx_qe_score": 0.4527073800563812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Ying,我的同事 Jian 和我将向大家展示我们关于通过指令调优改进多模态零样本学习的研究。", "metrics": {"bleu_score": 52.56478927184187, "chrf_score": 36.00016987258225, "xcomet_score": 0.8026688694953918, "xcomet_qe_score": 0.7469812631607056, "metricx_score": 4.891971111297607, "metricx_qe_score": 6.424280166625977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的发展,许多研究开始探索新的学习范式,以高效的方式重用预训练语言模型来处理不同的下游任务。", "metrics": {"bleu_score": 53.80147838344414, "chrf_score": 46.74038174777332, "xcomet_score": 0.9557749032974243, "xcomet_qe_score": 0.8848298788070679, "metricx_score": 0.8343597650527954, "metricx_qe_score": 1.6688936948776245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优使大型语言模型能够通过遵循自然指令以零样本的方式执行未见过的任务。", "metrics": {"bleu_score": 50.10137880396583, "chrf_score": 42.886790186779436, "xcomet_score": 0.8266419172286987, "xcomet_qe_score": 0.7594197988510132, "metricx_score": 1.9973564147949219, "metricx_qe_score": 3.623983144760132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前关于指令调优的研究大多专注于提高语言仅限任务的零样本性能,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 47.284532998004074, "chrf_score": 40.321324290737145, "xcomet_score": 0.8722671270370483, "xcomet_qe_score": 0.7320268750190735, "metricx_score": 1.6842690706253052, "metricx_qe_score": 2.0058164596557617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想调查多模态预训练模型上的指令调优是否能实际提高对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 40.00784698419303, "chrf_score": 33.741532046533834, "xcomet_score": 0.8671613931655884, "xcomet_qe_score": 0.7613025903701782, "metricx_score": 1.628103256225586, "metricx_qe_score": 1.649550199508667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们研究当时,我们发现语言仅限指令数据集和多元模态指令数据集之间的可用性存在显著差异。", "metrics": {"bleu_score": 22.873527912578858, "chrf_score": 22.81628322596052, "xcomet_score": 0.6972982287406921, "xcomet_qe_score": 0.7245450019836426, "metricx_score": 3.3093628883361816, "metricx_qe_score": 3.305575370788574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过一千六百个仅限语言的指令任务,", "metrics": {"bleu_score": 37.087658421061406, "chrf_score": 35.12795632456998, "xcomet_score": 0.8069578409194946, "xcomet_qe_score": 0.7511551380157471, "metricx_score": 1.0835464000701904, "metricx_qe_score": 1.678913950920105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大规模公开可用的多模态指令任务。", "metrics": {"bleu_score": 56.600498033020294, "chrf_score": 51.85818548915307, "xcomet_score": 0.9029275178909302, "xcomet_qe_score": 0.8110454082489014, "metricx_score": 1.6300890445709229, "metricx_qe_score": 2.2274293899536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这促使我们建立一个多模态指令调优数据集。", "metrics": {"bleu_score": 46.997395980026965, "chrf_score": 39.77014340858733, "xcomet_score": 0.9714657068252563, "xcomet_qe_score": 0.9610227346420288, "metricx_score": 0.8616886138916016, "metricx_qe_score": 0.7779636383056641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示了 Multi-Instruct,第一个多模态指令调优基准数据集,它由涵盖十个大类内的六十二个多样化的多模态任务组成。", "metrics": {"bleu_score": 35.25863335748597, "chrf_score": 35.971921250470004, "xcomet_score": 0.8645446300506592, "xcomet_qe_score": 0.7857068777084351, "metricx_score": 2.1099534034729004, "metricx_qe_score": 2.746389627456665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来源于二十一个现有的开源数据集,每个任务都配备了五个专家编写的指令。", "metrics": {"bleu_score": 58.338515355686255, "chrf_score": 54.704494640921375, "xcomet_score": 0.9647190570831299, "xcomet_qe_score": 0.9505810737609863, "metricx_score": 1.2485108375549316, "metricx_qe_score": 1.706358551979065, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令调优,我们采用 OFA 统一的多模态模式模型作为基础模型。OFA 使用统一的词", "metrics": {"bleu_score": 65.32227251676655, "chrf_score": 62.677287366104075, "xcomet_score": 0.5419771671295166, "xcomet_qe_score": 0.48732930421829224, "metricx_score": 7.030167579650879, "metricx_qe_score": 4.540332794189453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来表示语言、图像令牌和边界框的坐标。", "metrics": {"bleu_score": 40.142754193450884, "chrf_score": 31.737148627860805, "xcomet_score": 0.3805440664291382, "xcomet_qe_score": 0.2562572658061981, "metricx_score": 8.68294620513916, "metricx_qe_score": 8.407587051391602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示了我们 Multi-Instruct 数据集的一些示例实例。为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 67.92845921787168, "chrf_score": 67.55739789096216, "xcomet_score": 0.8917713165283203, "xcomet_qe_score": 0.8673213720321655, "metricx_score": 1.9307786226272583, "metricx_qe_score": 2.1586709022521973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循 OFA 中的方法,将所有任务制定为统一的序列到序列格式,其中", "metrics": {"bleu_score": 53.92168749797543, "chrf_score": 52.752614232249954, "xcomet_score": 0.7330736517906189, "xcomet_qe_score": 0.7173623442649841, "metricx_score": 3.6304779052734375, "metricx_qe_score": 2.960679292678833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框在相同的令牌空间中表示。", "metrics": {"bleu_score": 62.8292377306007, "chrf_score": 58.46686852035715, "xcomet_score": 0.876563549041748, "xcomet_qe_score": 0.843334436416626, "metricx_score": 4.788868427276611, "metricx_qe_score": 4.683666706085205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好,现在我将讨论多模态指令调优。", "metrics": {"bleu_score": 37.88187210633423, "chrf_score": 31.5673404983907, "xcomet_score": 0.9122191667556763, "xcomet_qe_score": 0.8730213046073914, "metricx_score": 0.7638083696365356, "metricx_qe_score": 0.743201494216919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用 NIG 组中的 53 个任务进行训练,每个任务采样 10,000 个实例。", "metrics": {"bleu_score": 60.24034044348127, "chrf_score": 57.10692329665671, "xcomet_score": 0.7082862854003906, "xcomet_qe_score": 0.7452478408813477, "metricx_score": 6.5265607833862305, "metricx_qe_score": 7.469315528869629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在测试中,我们保留整个常识推理组用于测试,并从 WQA 和杂项组中额外选择五个任务。", "metrics": {"bleu_score": 37.32192280333189, "chrf_score": 30.312654834163546, "xcomet_score": 0.6692521572113037, "xcomet_qe_score": 0.668318510055542, "metricx_score": 4.316638469696045, "metricx_qe_score": 4.608021259307861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务的测试分割中的所有实例。", "metrics": {"bleu_score": 64.90250359551654, "chrf_score": 55.76765321978275, "xcomet_score": 0.794039249420166, "xcomet_qe_score": 0.808186948299408, "metricx_score": 0.9813355207443237, "metricx_qe_score": 1.5871120691299438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从 NIG 指令的测试分割中随机采样 20 个任务作为 NLP 的 SIN 任务。", "metrics": {"bleu_score": 47.70300414919274, "chrf_score": 44.10638242681586, "xcomet_score": 0.6521011590957642, "xcomet_qe_score": 0.5846865177154541, "metricx_score": 8.736542701721191, "metricx_qe_score": 7.467358589172363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的 OFA 大型模型作为基础模型。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9849643707275391, "xcomet_qe_score": 0.9586126804351807, "metricx_score": 1.309117078781128, "metricx_qe_score": 2.1600091457366943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们混合所有任务的所有实例。", "metrics": {"bleu_score": 74.08842640893447, "chrf_score": 63.891815530865145, "xcomet_score": 0.8026423454284668, "xcomet_qe_score": 0.8263124823570251, "metricx_score": 0.8326014280319214, "metricx_qe_score": 1.4236559867858887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与五个指令模板中的一个组合。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9054250717163086, "xcomet_qe_score": 0.8633378148078918, "metricx_score": 1.5055538415908813, "metricx_qe_score": 1.778053641319275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在每个任务的测试中,我们通过使用五个指令中的一个评估模型来进行总共五个实验。", "metrics": {"bleu_score": 50.022605465268036, "chrf_score": 43.50651329364816, "xcomet_score": 0.7385162711143494, "xcomet_qe_score": 0.75737464427948, "metricx_score": 2.8900556564331055, "metricx_qe_score": 2.696883201599121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告所有五个实验中性能的平均值、最大值和标准差。", "metrics": {"bleu_score": 26.880019881254555, "chrf_score": 22.993871455280246, "xcomet_score": 0.937728226184845, "xcomet_qe_score": 0.9492309093475342, "metricx_score": 2.380336284637451, "metricx_qe_score": 2.1452744007110596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们报告准确率。如果", "metrics": {"bleu_score": 49.380155419366794, "chrf_score": 41.053037475621764, "xcomet_score": 0.726046085357666, "xcomet_qe_score": 0.7760872840881348, "metricx_score": 3.190519332885742, "metricx_qe_score": 0.7835210561752319, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是多模态生成任务,我们报告 ROOGEL。对于 RP 任务,我们也报告 ROOGEL。", "metrics": {"bleu_score": 48.93989581513665, "chrf_score": 35.24574155641633, "xcomet_score": 0.7414977550506592, "xcomet_qe_score": 0.6304503679275513, "metricx_score": 7.026317596435547, "metricx_qe_score": 6.788844108581543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标称为敏感度。", "metrics": {"bleu_score": 65.49385369510937, "chrf_score": 63.8440234652363, "xcomet_score": 0.9369590282440186, "xcomet_qe_score": 0.990680456161499, "metricx_score": 0.5951434969902039, "metricx_qe_score": 0.6983921527862549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这测量了模型在指令措辞略有变化时,能否一致地为相同任务产生相同输出的能力。", "metrics": {"bleu_score": 37.60791682424405, "chrf_score": 31.303491249520427, "xcomet_score": 0.9627163410186768, "xcomet_qe_score": 0.9643856287002563, "metricx_score": 5.6430768966674805, "metricx_qe_score": 6.78053092956543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提高 OFA 在多模态任务上的性能。", "metrics": {"bleu_score": 56.53239695629305, "chrf_score": 51.48246838263649, "xcomet_score": 0.9914814233779907, "xcomet_qe_score": 0.9646351337432861, "metricx_score": 1.4318344593048096, "metricx_qe_score": 1.5803004503250122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习可以受益于指令调优。", "metrics": {"bleu_score": 73.42150184891982, "chrf_score": 67.13895082373344, "xcomet_score": 0.9494084119796753, "xcomet_qe_score": 0.7283151745796204, "metricx_score": 1.5469399690628052, "metricx_qe_score": 2.13948655128479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们可以看到,随着任务数量的增加,模型获得了更好的性能,同时敏感度降低。所以", "metrics": {"bleu_score": 44.92206364762189, "chrf_score": 40.942145309472636, "xcomet_score": 0.8066922426223755, "xcomet_qe_score": 0.8199303150177002, "metricx_score": 4.317293167114258, "metricx_qe_score": 2.7495360374450684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一个实验,", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 39.39127110257559, "xcomet_score": 0.9774124622344971, "xcomet_qe_score": 0.9565337896347046, "metricx_score": 0.34012168645858765, "metricx_qe_score": 0.33167219161987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一个指令与五个指令进行比较,", "metrics": {"bleu_score": 27.098211583470043, "chrf_score": 26.361565669860987, "xcomet_score": 0.9578522443771362, "xcomet_qe_score": 0.8511433005332947, "metricx_score": 1.2056585550308228, "metricx_qe_score": 3.1937615871429443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多指令可以提高模型的总体性能并显著降低其敏感度,", "metrics": {"bleu_score": 49.67684344498356, "chrf_score": 42.8328653280025, "xcomet_score": 0.9687538146972656, "xcomet_qe_score": 0.9627504348754883, "metricx_score": 0.7872583866119385, "metricx_qe_score": 0.8037170767784119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这展示了不同的微调策略对模型敏感度的影响。", "metrics": {"bleu_score": 72.98378378464027, "chrf_score": 65.28325511491146, "xcomet_score": 0.9754297733306885, "xcomet_qe_score": 0.9701192378997803, "metricx_score": 0.9737162590026855, "metricx_qe_score": 1.302918791770935, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,通过从自然指令数据集进行迁移学习,模型可以获得比原始 OFA 模型更好的敏感度。", "metrics": {"bleu_score": 43.367232540682025, "chrf_score": 37.36415491352964, "xcomet_score": 0.9608851671218872, "xcomet_qe_score": 0.9298549890518188, "metricx_score": 1.632513165473938, "metricx_qe_score": 2.5739707946777344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以看到,从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上获得更好的性能。", "metrics": {"bleu_score": 62.18537392431551, "chrf_score": 57.71314685973494, "xcomet_score": 0.9500532150268555, "xcomet_qe_score": 0.7138231992721558, "metricx_score": 2.993060827255249, "metricx_qe_score": 3.812215805053711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们提出了第一个大规模多模态指令调优数据集。我们显著提高了 OFA 的 DAROCHOT 能力,并探索了不同的迁移学习技术并展示了它们的益处。", "metrics": {"bleu_score": 58.600493096424266, "chrf_score": 54.7419828647641, "xcomet_score": 0.795723021030426, "xcomet_qe_score": 0.7201851606369019, "metricx_score": 4.933445930480957, "metricx_qe_score": 5.2333598136901855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标称为敏感度。", "metrics": {"bleu_score": 57.52730628627205, "chrf_score": 54.983770864124295, "xcomet_score": 0.9709914922714233, "xcomet_qe_score": 0.9727061986923218, "metricx_score": 0.5468370318412781, "metricx_qe_score": 0.7084141373634338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "再者,我们正在收集一个更庞大的多模态指令调优数据集,包含大约 150 个额外的变体语言任务,我们将发布它们。", "metrics": {"bleu_score": 48.45273694520008, "chrf_score": 43.048460815715586, "xcomet_score": 0.7657121419906616, "xcomet_qe_score": 0.7702615261077881, "metricx_score": 2.5514416694641113, "metricx_qe_score": 2.7970430850982666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.974276065826416, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是新浪海岸,很高兴欢迎大家参加我们关于 ACL 2023 年论文《", "metrics": {"bleu_score": 60.2957132521504, "chrf_score": 50.41905168447275, "xcomet_score": 0.3114754855632782, "xcomet_qe_score": 0.21918432414531708, "metricx_score": 8.875712394714355, "metricx_qe_score": 8.503186225891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型可接受性判断不总是对上下文稳健》的演讲。", "metrics": {"bleu_score": 48.574574348847946, "chrf_score": 40.45009024414024, "xcomet_score": 0.7146363258361816, "xcomet_qe_score": 0.6729546785354614, "metricx_score": 6.57082462310791, "metricx_qe_score": 6.350607872009277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与 John Bakhier、Aaron Mueller、Kanishka Mishra、Karen Fentus、Roger Levy 和 Adina Williams 共同完成的工作。", "metrics": {"bleu_score": 34.889963592850705, "chrf_score": 72.76967284183287, "xcomet_score": 0.5104504823684692, "xcomet_qe_score": 0.47102734446525574, "metricx_score": 5.097921371459961, "metricx_qe_score": 5.517606735229492, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们重新审视了最小对范式。", "metrics": {"bleu_score": 53.0935466304407, "chrf_score": 48.22483086801972, "xcomet_score": 0.9683331251144409, "xcomet_qe_score": 0.888993501663208, "metricx_score": 1.244659185409546, "metricx_qe_score": 1.318211555480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小对范式基本上是在可接受性判断的基础上评估语言模型,这些判断", "metrics": {"bleu_score": 50.37255667587404, "chrf_score": 48.10593479923864, "xcomet_score": 0.8755109310150146, "xcomet_qe_score": 0.7689638137817383, "metricx_score": 6.015051364898682, "metricx_qe_score": 2.852109909057617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,如 BLIMP 语法宝石,或成见方面的可接受性,如人群稀疏。", "metrics": {"bleu_score": 25.572642416297196, "chrf_score": 15.561643234108846, "xcomet_score": 0.3662453889846802, "xcomet_qe_score": 0.376701682806015, "metricx_score": 9.462090492248535, "metricx_qe_score": 8.4558687210083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最小对范式中,评估语言模型的典型方法是展示一个可接受的句子或一个语法句,然后展示一个不可接受的句子或一个不语法句", "metrics": {"bleu_score": 52.18187908078482, "chrf_score": 49.324444184688936, "xcomet_score": 0.6750162839889526, "xcomet_qe_score": 0.6273140907287598, "metricx_score": 2.5017995834350586, "metricx_qe_score": 3.6970102787017822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",希望模型基本上将更高的概率赋予可接受的句子。", "metrics": {"bleu_score": 36.482717719811376, "chrf_score": 29.641301226862637, "xcomet_score": 0.8602513074874878, "xcomet_qe_score": 0.7385724782943726, "metricx_score": 3.58638334274292, "metricx_qe_score": 4.256195545196533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的 MPP 管道基本不允许我们评估模型对较长句子的接受程度。", "metrics": {"bleu_score": 85.1683409367816, "chrf_score": 81.21989951300296, "xcomet_score": 0.818332314491272, "xcomet_qe_score": 0.7644573450088501, "metricx_score": 1.363376498222351, "metricx_qe_score": 2.9560482501983643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型出现了越来越长的", "metrics": {"bleu_score": 29.278700698052127, "chrf_score": 23.534472305587855, "xcomet_score": 0.7462601065635681, "xcomet_qe_score": 0.7757354974746704, "metricx_score": 5.9170145988464355, "metricx_qe_score": 5.101223468780518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文窗口。因此,评估模型在整个上下文窗口中的可接受性至关重要。这正是我们在此试图做到的。", "metrics": {"bleu_score": 41.1266347706155, "chrf_score": 36.38148941912302, "xcomet_score": 0.7136861085891724, "xcomet_qe_score": 0.7125766277313232, "metricx_score": 2.6721808910369873, "metricx_qe_score": 3.879547119140625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图通过让模型对越来越长的序列评估可接受性来重新审视 NPV 管道。", "metrics": {"bleu_score": 52.234015197308516, "chrf_score": 43.28232870243377, "xcomet_score": 0.6892517805099487, "xcomet_qe_score": 0.6346708536148071, "metricx_score": 4.741054534912109, "metricx_qe_score": 4.497200965881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。我们所做", "metrics": {"bleu_score": 64.79121525090147, "chrf_score": 88.59854884450613, "xcomet_score": 0.8280736207962036, "xcomet_qe_score": 0.8130654692649841, "metricx_score": 2.34740948677063, "metricx_qe_score": 1.4932039976119995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的是,为了模拟这些较长的序列,我们重新审视了数据集本身,然后通过选择可接受或不可接受的句子来重造句子。", "metrics": {"bleu_score": 53.564075712796985, "chrf_score": 46.664954548428184, "xcomet_score": 0.5804457664489746, "xcomet_qe_score": 0.4821276068687439, "metricx_score": 2.6710479259490967, "metricx_qe_score": 3.7308108806610107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从 BLIMP 数据集的附加岛案例中选择了一个典型的语法性对。", "metrics": {"bleu_score": 35.94039880288693, "chrf_score": 26.29211123774055, "xcomet_score": 0.7180424928665161, "xcomet_qe_score": 0.7021831274032593, "metricx_score": 3.6431689262390137, "metricx_qe_score": 3.8398797512054443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是,为了重造可接受和具有相同语法结构的较长序列,", "metrics": {"bleu_score": 23.534103334634338, "chrf_score": 21.25927627003883, "xcomet_score": 0.7768667936325073, "xcomet_qe_score": 0.727794885635376, "metricx_score": 2.6885428428649902, "metricx_qe_score": 2.8970441818237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从附加岛中提取语法句,然后将它们作为前缀添加到可接受的查询和不可接受的查询中。我们", "metrics": {"bleu_score": 69.0621248227335, "chrf_score": 55.08022265411665, "xcomet_score": 0.4018586575984955, "xcomet_qe_score": 0.4232664108276367, "metricx_score": 5.209217071533203, "metricx_qe_score": 3.749016761779785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以通过从相同的匹配中选择不可接受的句子来做到这一点,这也可以用于测试模型的可接受性。", "metrics": {"bleu_score": 72.7061023541805, "chrf_score": 68.08095328455637, "xcomet_score": 0.9643155336380005, "xcomet_qe_score": 0.8343571424484253, "metricx_score": 1.4250982999801636, "metricx_qe_score": 2.036952495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以从不同的子集或不同的数据集中选择句子。", "metrics": {"bleu_score": 55.03861232879653, "chrf_score": 57.62197432937344, "xcomet_score": 0.9671915769577026, "xcomet_qe_score": 0.8320313692092896, "metricx_score": 1.3435924053192139, "metricx_qe_score": 2.1284751892089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的不匹配场景。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9915859699249268, "xcomet_qe_score": 0.9170319437980652, "metricx_score": 0.7872244715690613, "metricx_qe_score": 1.401894450187683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,句子仍然来自相关数据集,但不是来自您正在评估的同一数据集。", "metrics": {"bleu_score": 56.490118161731864, "chrf_score": 47.3032032951691, "xcomet_score": 0.9607911109924316, "xcomet_qe_score": 0.8443313837051392, "metricx_score": 1.103254795074463, "metricx_qe_score": 1.7634508609771729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对不可接受的情况做同样的事情。", "metrics": {"bleu_score": 46.3325745127537, "chrf_score": 38.854396484114986, "xcomet_score": 0.9737632274627686, "xcomet_qe_score": 0.8946936726570129, "metricx_score": 1.2220489978790283, "metricx_qe_score": 1.2802505493164062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全不相关的领域选择句子,例如维基百科。", "metrics": {"bleu_score": 65.4468923560319, "chrf_score": 57.12890537741404, "xcomet_score": 0.9937243461608887, "xcomet_qe_score": 0.9375579953193665, "metricx_score": 0.6774861812591553, "metricx_qe_score": 1.5228242874145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们,模型的可接受性判断是否实际上受到任何上下文的影响,无论上下文是来自数据集的不同子集,还是与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 61.65503569766461, "chrf_score": 54.39594841175482, "xcomet_score": 0.9408032894134521, "xcomet_qe_score": 0.9052624702453613, "metricx_score": 1.859804630279541, "metricx_qe_score": 2.7232613563537598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型表现如何呢?", "metrics": {"bleu_score": 9.29675796576443, "chrf_score": 9.644582470669427, "xcomet_score": 0.8517210483551025, "xcomet_qe_score": 0.8354972004890442, "metricx_score": 1.0570439100265503, "metricx_qe_score": 0.2590245008468628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们看看来自维基百科的完全与当前查询对无关的句子,在那里我们发现 MPP 判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 43.556254339136004, "chrf_score": 37.92928817729313, "xcomet_score": 0.9420177936553955, "xcomet_qe_score": 0.7904796600341797, "metricx_score": 4.441779136657715, "metricx_qe_score": 6.40366268157959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到 1024,以最大限度地利用 OPT 和 GPT2 模型,正", "metrics": {"bleu_score": 48.950961379857205, "chrf_score": 70.54523887237836, "xcomet_score": 0.7800646424293518, "xcomet_qe_score": 0.7332425117492676, "metricx_score": 4.7444071769714355, "metricx_qe_score": 1.7437381744384766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如橙色虚线所示,MPP 判断相对稳定。", "metrics": {"bleu_score": 53.1799779830062, "chrf_score": 52.278460014866624, "xcomet_score": 0.8937128782272339, "xcomet_qe_score": 0.8223428130149841, "metricx_score": 1.2305163145065308, "metricx_qe_score": 3.129074811935425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们从同一数据集选择句子时会发生什么?", "metrics": {"bleu_score": 51.46606059335071, "chrf_score": 46.71616129071466, "xcomet_score": 0.9917392730712891, "xcomet_qe_score": 0.929458498954773, "metricx_score": 0.6798443794250488, "metricx_qe_score": 1.220543622970581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们从可接受和不可接受的领域创建句子,来自相同的 BLIMP 或语法宝石数据集,在那里我们看到 M", "metrics": {"bleu_score": 35.25170087028993, "chrf_score": 32.33355944392352, "xcomet_score": 0.38497257232666016, "xcomet_qe_score": 0.4309042692184448, "metricx_score": 7.890473365783691, "metricx_qe_score": 6.689049243927002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "PP 判断在添加可接受的前缀或不可接受的前缀时显著增加或减少。", "metrics": {"bleu_score": 54.256981468459145, "chrf_score": 56.706039499770235, "xcomet_score": 0.5976034998893738, "xcomet_qe_score": 0.637359619140625, "metricx_score": 5.128253936767578, "metricx_qe_score": 5.185708999633789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们匹配结构时,即当我们从相同的现象中选择句子时,我们看到模型的 MPP 判断会根据所选择的前缀是否可接受而出现巨大的增加或减少。这种效果随着", "metrics": {"bleu_score": 37.1742229594851, "chrf_score": 30.67063885917005, "xcomet_score": 0.5945230722427368, "xcomet_qe_score": 0.4369450807571411, "metricx_score": 9.342225074768066, "metricx_qe_score": 6.5322136878967285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文长度的增加而增加,这可能会影响具有大上下文窗口的新语言模型。", "metrics": {"bleu_score": 57.970375612506196, "chrf_score": 53.01732373155521, "xcomet_score": 0.7993937730789185, "xcomet_qe_score": 0.667577862739563, "metricx_score": 3.307154655456543, "metricx_qe_score": 4.557590484619141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么匹配的前缀会如此影响语言模型的判断?", "metrics": {"bleu_score": 38.915586646985844, "chrf_score": 35.3472536612294, "xcomet_score": 0.9124661684036255, "xcomet_qe_score": 0.8886328935623169, "metricx_score": 0.6210596561431885, "metricx_qe_score": 0.6790621280670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保留相关结构但向输入句子添加噪声来扰动输入,在进行了一", "metrics": {"bleu_score": 59.50492876175587, "chrf_score": 57.286645963980845, "xcomet_score": 0.6678186655044556, "xcomet_qe_score": 0.6020092964172363, "metricx_score": 6.1141438484191895, "metricx_qe_score": 5.308071136474609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "系列此类扰动后,我们发现这些噪声中没有一个实际上使模型改变其 MPP 判断趋势。", "metrics": {"bleu_score": 31.981664638902622, "chrf_score": 31.135493744576863, "xcomet_score": 0.7781752347946167, "xcomet_qe_score": 0.688179075717926, "metricx_score": 4.8490166664123535, "metricx_qe_score": 4.873536109924316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动的句子具有类似的敏感性,即", "metrics": {"bleu_score": 26.330251834204283, "chrf_score": 23.549774168092043, "xcomet_score": 0.8061927556991577, "xcomet_qe_score": 0.7385638356208801, "metricx_score": 3.4439775943756104, "metricx_qe_score": 3.290590763092041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们扰动可接受的领域中的句子时,所有扰动都会出现类似的增加,当我们扰动不可接受的领域中的句子时,MPP 判断会以类似的方式减少。", "metrics": {"bleu_score": 35.82514456096549, "chrf_score": 32.49021442958769, "xcomet_score": 0.5992153286933899, "xcomet_qe_score": 0.5444409847259521, "metricx_score": 4.52947998046875, "metricx_qe_score": 5.131533145904541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们研究的主要结论是,语言模型对跨句子的潜在语法和语义特征敏感。", "metrics": {"bleu_score": 32.83989705525475, "chrf_score": 28.540263614059313, "xcomet_score": 0.8344194889068604, "xcomet_qe_score": 0.7673995494842529, "metricx_score": 1.3939203023910522, "metricx_qe_score": 1.2348532676696777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前,我们通过短句子和单句输入进行 MPP 评估,可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 52.31643396603299, "chrf_score": 43.66364334129722, "xcomet_score": 0.955824613571167, "xcomet_qe_score": 0.880774736404419, "metricx_score": 1.5570645332336426, "metricx_qe_score": 2.25404691696167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解更多实验细节。", "metrics": {"bleu_score": 34.27163657253172, "chrf_score": 33.44303636597666, "xcomet_score": 0.9978342056274414, "xcomet_qe_score": 0.9995092153549194, "metricx_score": 0.10219424962997437, "metricx_qe_score": 0.11421225965023041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢你的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.63475501537323, "xcomet_qe_score": 0.880922794342041, "metricx_score": 1.3314695358276367, "metricx_qe_score": 1.4186692237854004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张优素福。", "metrics": {"bleu_score": 65.28674230372313, "chrf_score": 49.49429708649753, "xcomet_score": 0.8128776550292969, "xcomet_qe_score": 0.7768265008926392, "metricx_score": 1.971982717514038, "metricx_qe_score": 3.668447494506836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的研究成果——Exampler:多自然语言和语义表示的跨语言语义分析。因此", "metrics": {"bleu_score": 44.47278656331356, "chrf_score": 33.82226233528602, "xcomet_score": 0.646069347858429, "xcomet_qe_score": 0.577540934085846, "metricx_score": 4.998538017272949, "metricx_qe_score": 4.135692596435547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语义分析是一项构建用户查询的语义表示的任务,例如SQL和Lambda演算。", "metrics": {"bleu_score": 55.089351662461134, "chrf_score": 56.56563364770324, "xcomet_score": 0.9492665529251099, "xcomet_qe_score": 0.9165284633636475, "metricx_score": 2.329824447631836, "metricx_qe_score": 2.6189544200897217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而跨语言语义分析的任务是将多种自然语言的查询翻译成多种语义表示。", "metrics": {"bleu_score": 66.2564843387274, "chrf_score": 61.20094810541882, "xcomet_score": 0.9380128979682922, "xcomet_qe_score": 0.9442958235740662, "metricx_score": 1.5268248319625854, "metricx_qe_score": 3.3080458641052246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda、FunQL等。", "metrics": {"bleu_score": 84.52785147119853, "chrf_score": 83.98057968713225, "xcomet_score": 0.9816653728485107, "xcomet_qe_score": 0.9268304109573364, "metricx_score": 1.2052795886993408, "metricx_qe_score": 1.5173604488372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义分析模型是分别在有限的任务和应用数据集上提出和评估的。", "metrics": {"bleu_score": 69.20463819926105, "chrf_score": 58.47440871934968, "xcomet_score": 0.9959969520568848, "xcomet_qe_score": 0.9824415445327759, "metricx_score": 0.6534231901168823, "metricx_qe_score": 1.0421183109283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,某些自然语言覆盖不足,", "metrics": {"bleu_score": 37.48576885220423, "chrf_score": 31.992412272828187, "xcomet_score": 0.651879608631134, "xcomet_qe_score": 0.5585570931434631, "metricx_score": 5.542513370513916, "metricx_qe_score": 4.851147651672363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失,某些语义表示覆盖不足", "metrics": {"bleu_score": 6.78885287516723, "chrf_score": 10.683058885587887, "xcomet_score": 0.8024923801422119, "xcomet_qe_score": 0.7797914147377014, "metricx_score": 3.340573310852051, "metricx_qe_score": 5.217947006225586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",Lambda演算缺失,或者只是在某些新模型上进行评估,", "metrics": {"bleu_score": 35.13874939965221, "chrf_score": 38.43969092391152, "xcomet_score": 0.7472350001335144, "xcomet_qe_score": 0.7451952695846558, "metricx_score": 4.604968070983887, "metricx_qe_score": 5.093817710876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如只有一个模型用于评估。", "metrics": {"bleu_score": 20.15941023902838, "chrf_score": 20.448341026138152, "xcomet_score": 0.9811050295829773, "xcomet_qe_score": 0.9633473753929138, "metricx_score": 0.5978302955627441, "metricx_qe_score": 0.8050082325935364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出Exampler,", "metrics": {"bleu_score": 28.24099048856542, "chrf_score": 14.288003663003659, "xcomet_score": 0.8172333240509033, "xcomet_qe_score": 0.7959850430488586, "metricx_score": 2.1315951347351074, "metricx_qe_score": 3.5121371746063232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供了一个统一的数据集——Exampler,用于多自然语言和语义表示的跨语言语义分析。", "metrics": {"bleu_score": 55.268151675894174, "chrf_score": 40.813403487411605, "xcomet_score": 0.7749072313308716, "xcomet_qe_score": 0.8054569959640503, "metricx_score": 3.3202133178710938, "metricx_qe_score": 4.1154303550720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含来自各个领域的九个数据集、五种语义分析税收、八种语义表示以及22种来自15个语族自然语言。", "metrics": {"bleu_score": 31.042094323933767, "chrf_score": 28.087953167539613, "xcomet_score": 0.7333391904830933, "xcomet_qe_score": 0.8012797832489014, "metricx_score": 6.63407039642334, "metricx_qe_score": 6.739930152893066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 80.20219183488042, "chrf_score": 71.52080420921001, "xcomet_score": 0.9888695478439331, "xcomet_qe_score": 0.913833737373352, "metricx_score": 1.4580811262130737, "metricx_qe_score": 2.0225110054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是TranslateTest。", "metrics": {"bleu_score": 32.58798048281462, "chrf_score": 18.76628675854382, "xcomet_score": 0.9642470479011536, "xcomet_qe_score": 0.953527569770813, "metricx_score": 1.326793909072876, "metricx_qe_score": 1.1972638368606567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google翻译API将源语言翻译成目标语言,然后使用单语模型进行训练和评估。", "metrics": {"bleu_score": 82.66660014007991, "chrf_score": 76.41626540305097, "xcomet_score": 0.9510485529899597, "xcomet_qe_score": 0.8472477197647095, "metricx_score": 0.5243576169013977, "metricx_qe_score": 0.4862639307975769, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语查询上训练英语模型,在推理时使用API将德语查询翻译成英语,然后使用训练好的模型预测SQL。", "metrics": {"bleu_score": 56.35162574575119, "chrf_score": 53.199041688800165, "xcomet_score": 0.8583941459655762, "xcomet_qe_score": 0.8354575037956238, "metricx_score": 1.313491702079773, "metricx_qe_score": 1.6688352823257446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语模型。", "metrics": {"bleu_score": 51.56626918239823, "chrf_score": 40.77320827320827, "xcomet_score": 0.8812164068222046, "xcomet_qe_score": 0.84535151720047, "metricx_score": 1.4693257808685303, "metricx_qe_score": 1.2090201377868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个设置中,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 72.76293560493822, "chrf_score": 67.10828266955446, "xcomet_score": 0.9135197401046753, "xcomet_qe_score": 0.8122859001159668, "metricx_score": 0.5885418057441711, "metricx_qe_score": 0.6838915944099426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语融合设置,通过仅使用10%的训练数据训练单语模型。我们测试了单语", "metrics": {"bleu_score": 35.38278147438809, "chrf_score": 35.77977172056476, "xcomet_score": 0.5885565280914307, "xcomet_qe_score": 0.5048099756240845, "metricx_score": 10.0799560546875, "metricx_qe_score": 5.798002243041992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多语模型,即训练一个多语模型用于所有语言。", "metrics": {"bleu_score": 34.751343920267985, "chrf_score": 29.77833986491157, "xcomet_score": 0.771351158618927, "xcomet_qe_score": 0.8000988960266113, "metricx_score": 2.4636988639831543, "metricx_qe_score": 2.8704090118408203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询放在一起训练一个多语模型。", "metrics": {"bleu_score": 64.65767005913993, "chrf_score": 55.67337857551109, "xcomet_score": 0.8853030204772949, "xcomet_qe_score": 0.9429187774658203, "metricx_score": 1.340059518814087, "metricx_qe_score": 2.894014596939087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理时,我们可以使用该模型翻译德语查询或中文查询等。", "metrics": {"bleu_score": 60.16779960516385, "chrf_score": 55.03307598826298, "xcomet_score": 0.9696305990219116, "xcomet_qe_score": 0.88944411277771, "metricx_score": 0.7382437586784363, "metricx_qe_score": 1.3076188564300537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和场样本迁移。", "metrics": {"bleu_score": 70.42311846346826, "chrf_score": 66.79691467230167, "xcomet_score": 0.7252026796340942, "xcomet_qe_score": 0.7132292985916138, "metricx_score": 4.711606979370117, "metricx_qe_score": 5.748737335205078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,然后迁移到另一种语言。因此,", "metrics": {"bleu_score": 33.307662668678184, "chrf_score": 30.09469057533497, "xcomet_score": 0.786724328994751, "xcomet_qe_score": 0.7627609968185425, "metricx_score": 5.2872114181518555, "metricx_qe_score": 5.579730033874512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练期间,我们在英语查询或英语和德语场样本查询的组合上训练一个多语模型,并预测SQL输出。", "metrics": {"bleu_score": 44.074219154082655, "chrf_score": 40.636949659483314, "xcomet_score": 0.6696263551712036, "xcomet_qe_score": 0.6976828575134277, "metricx_score": 3.2977609634399414, "metricx_qe_score": 3.476743221282959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的结果。", "metrics": {"bleu_score": 57.3122448409426, "chrf_score": 48.92181892181892, "xcomet_score": 0.9985549449920654, "xcomet_qe_score": 0.990606427192688, "metricx_score": 0.33577799797058105, "metricx_qe_score": 0.8158290982246399, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于单语模型的分析,我们评估了两组模型,包括编码器PDR,即多语预训练编码器与指针解码器,例如XLMR加PDR和BERT加PDR。", "metrics": {"bleu_score": 40.93062598761865, "chrf_score": 32.38540440511377, "xcomet_score": 0.581455647945404, "xcomet_qe_score": 0.5370808243751526, "metricx_score": 4.190337181091309, "metricx_qe_score": 3.9141204357147217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器解码器模型,即多语预训练编码器解码器模型,例如MBART和MT5。", "metrics": {"bleu_score": 22.71244283779713, "chrf_score": 17.396726330299117, "xcomet_score": 0.7904273271560669, "xcomet_qe_score": 0.8294076919555664, "metricx_score": 1.3162791728973389, "metricx_qe_score": 2.142712354660034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器解码器在所有九个数据集上取得了最佳性能。", "metrics": {"bleu_score": 47.901455811287484, "chrf_score": 29.360440980186674, "xcomet_score": 0.9818049669265747, "xcomet_qe_score": 0.9736790657043457, "metricx_score": 1.5878827571868896, "metricx_qe_score": 1.501329779624939, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多语设置下评估了MT5和XLMR加PDR,发现编码", "metrics": {"bleu_score": 20.217803037339237, "chrf_score": 26.32513286498794, "xcomet_score": 0.656982958316803, "xcomet_qe_score": 0.67561936378479, "metricx_score": 7.529801368713379, "metricx_qe_score": 4.415807723999023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "器解码器或编码器PDR通过混合各种语言的训练可以得到改进。", "metrics": {"bleu_score": 32.672940262046325, "chrf_score": 17.686250889306077, "xcomet_score": 0.5804626941680908, "xcomet_qe_score": 0.5344409942626953, "metricx_score": 5.117001533508301, "metricx_qe_score": 6.324834823608398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,除了英语在七个数据集上的性能下降,只在三个数据集上获得提升。", "metrics": {"bleu_score": 55.714549885364676, "chrf_score": 49.63807518252102, "xcomet_score": 0.8832403421401978, "xcomet_qe_score": 0.9078938364982605, "metricx_score": 2.61138916015625, "metricx_qe_score": 2.382526159286499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语性曲线。", "metrics": {"bleu_score": 13.949762415143761, "chrf_score": 13.138181847458979, "xcomet_score": 0.8194416165351868, "xcomet_qe_score": 0.8059184551239014, "metricx_score": 4.418024063110352, "metricx_qe_score": 3.572284698486328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝线表示跨语言场样本迁移,", "metrics": {"bleu_score": 24.065223308491284, "chrf_score": 23.839643451032163, "xcomet_score": 0.840095043182373, "xcomet_qe_score": 0.7852252125740051, "metricx_score": 3.7616264820098877, "metricx_qe_score": 5.063718795776367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线表示跨语言零样本迁移,", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 59.505479514708156, "xcomet_score": 0.9473828077316284, "xcomet_qe_score": 0.8333927989006042, "metricx_score": 1.6966118812561035, "metricx_qe_score": 2.8306846618652344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿线表示单语设置。我们", "metrics": {"bleu_score": 40.35278637463991, "chrf_score": 41.726505952191296, "xcomet_score": 0.8372125625610352, "xcomet_qe_score": 0.8052516579627991, "metricx_score": 3.9680633544921875, "metricx_qe_score": 0.6524863243103027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现通过比较绿线和橙线,对于零样本设置,跨语言迁移性能差距显著。通过比较蓝线和橙线,我们发现对于场样本设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 50.05656430121247, "chrf_score": 41.62897630804471, "xcomet_score": 0.633436918258667, "xcomet_qe_score": 0.5300567746162415, "metricx_score": 5.751006603240967, "metricx_qe_score": 6.249441623687744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器解码器优于现有研究或取得可比结果。", "metrics": {"bleu_score": 9.361298967187365, "chrf_score": 7.831746487645533, "xcomet_score": 0.9569833278656006, "xcomet_qe_score": 0.9564383029937744, "metricx_score": 1.6456063985824585, "metricx_qe_score": 1.3203052282333374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上进行预训练可以显著提升目标自然语言的场样本性能。我们发现多语语言模型,如Codice和Bloom,仍不足以处理跨语言语义分析任务。", "metrics": {"bleu_score": 43.0675259827373, "chrf_score": 33.73910570681246, "xcomet_score": 0.7607482671737671, "xcomet_qe_score": 0.7221510410308838, "metricx_score": 5.428054332733154, "metricx_qe_score": 6.637459754943848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们构建了Exampler,一个统一的跨语言语义分析基准,包含多种自然语言和多种语义表示。", "metrics": {"bleu_score": 37.67347913131195, "chrf_score": 28.134568266365, "xcomet_score": 0.8468680381774902, "xcomet_qe_score": 0.8301544785499573, "metricx_score": 3.633129119873047, "metricx_qe_score": 4.53617000579834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种代表性多语语言模型进行了全面的基准研究。", "metrics": {"bleu_score": 71.2638127963778, "chrf_score": 65.07635858833078, "xcomet_score": 0.9217941761016846, "xcomet_qe_score": 0.8467780351638794, "metricx_score": 1.198909044265747, "metricx_qe_score": 1.9347972869873047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的成果显示了许多有趣的发现等", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 68.89097014097013, "xcomet_score": 0.8065767288208008, "xcomet_qe_score": 0.7921155691146851, "metricx_score": 2.209555149078369, "metricx_qe_score": 2.554340362548828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.1294848918914795, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Ayud Villar,我将简要介绍一篇名为《翻译评估策略与表现中的Prompting Palm》的论文。", "metrics": {"bleu_score": 13.893031405126575, "chrf_score": 18.952105937478684, "xcomet_score": 0.5998674631118774, "xcomet_qe_score": 0.6438336372375488, "metricx_score": 8.210967063903809, "metricx_qe_score": 8.244963645935059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与谷歌翻译团队同事的合作研究。", "metrics": {"bleu_score": 51.234156791016495, "chrf_score": 47.98446338379667, "xcomet_score": 0.9505547285079956, "xcomet_qe_score": 0.9206057190895081, "metricx_score": 0.4531790316104889, "metricx_qe_score": 0.4493764638900757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Palm是一个拥有5400亿参数的大型语言模型,于2022年发布。", "metrics": {"bleu_score": 59.70228993860436, "chrf_score": 56.459432839438406, "xcomet_score": 0.9302434921264648, "xcomet_qe_score": 0.8909064531326294, "metricx_score": 2.500760078430176, "metricx_qe_score": 3.1039299964904785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它训练了7800亿个令牌的大规模文本语料库。", "metrics": {"bleu_score": 11.80236964191302, "chrf_score": 23.897214357204117, "xcomet_score": 0.7274038791656494, "xcomet_qe_score": 0.7168003916740417, "metricx_score": 7.782742977142334, "metricx_qe_score": 7.648958206176758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百个自然语言处理任务中达到了最先进水平。", "metrics": {"bleu_score": 22.50265947708922, "chrf_score": 20.989852208079824, "xcomet_score": 0.9979772567749023, "xcomet_qe_score": 0.9900028705596924, "metricx_score": 0.733557939529419, "metricx_qe_score": 1.0169049501419067, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们提出了对大型语言模型进行机器翻译的首次系统研究。", "metrics": {"bleu_score": 30.385793930583404, "chrf_score": 26.79023266993631, "xcomet_score": 0.8888452053070068, "xcomet_qe_score": 0.786281406879425, "metricx_score": 3.4660770893096924, "metricx_qe_score": 3.9533143043518066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用AMT社区的最佳实践来评估此类模型的翻译能力。", "metrics": {"bleu_score": 52.010967098419755, "chrf_score": 44.746566284163706, "xcomet_score": 0.796886682510376, "xcomet_qe_score": 0.7723275423049927, "metricx_score": 3.96469783782959, "metricx_qe_score": 5.558260440826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这涉及到使用最新的测试集,以避免测试数据与语言模型的训练数据重叠,", "metrics": {"bleu_score": 88.66029039778034, "chrf_score": 84.88414104897699, "xcomet_score": 0.9426568746566772, "xcomet_qe_score": 0.9176536798477173, "metricx_score": 0.6100053191184998, "metricx_qe_score": 0.5427350401878357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,即WMT评估中表现最佳的系统。", "metrics": {"bleu_score": 40.66139639256472, "chrf_score": 38.166460961997224, "xcomet_score": 0.9181811809539795, "xcomet_qe_score": 0.8544657826423645, "metricx_score": 2.662759780883789, "metricx_qe_score": 3.9063899517059326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的神经机器翻译指标,并额外提供了基于专家的人类评估结果。", "metrics": {"bleu_score": 63.001219195455604, "chrf_score": 57.91971014162131, "xcomet_score": 0.9660176038742065, "xcomet_qe_score": 0.8474134206771851, "metricx_score": 1.4003355503082275, "metricx_qe_score": 2.163519859313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了关于提示选择策略的一些建议。", "metrics": {"bleu_score": 64.06179367095976, "chrf_score": 54.26883831682594, "xcomet_score": 0.8966191411018372, "xcomet_qe_score": 0.8442993760108948, "metricx_score": 0.5821217894554138, "metricx_qe_score": 1.5963844060897827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型的翻译性能有很大影响,正如我们在一个简单实验中看到的,我们使用一个简短的提示,并为一个句子提供了两个不同的提示。", "metrics": {"bleu_score": 49.55361095462972, "chrf_score": 45.02618412690074, "xcomet_score": 0.8635386228561401, "xcomet_qe_score": 0.841060996055603, "metricx_score": 2.4210972785949707, "metricx_qe_score": 1.9526898860931396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在1000个句子中,", "metrics": {"bleu_score": 30.895757752065407, "chrf_score": 45.3545034322207, "xcomet_score": 0.891600489616394, "xcomet_qe_score": 0.729491114616394, "metricx_score": 6.866838455200195, "metricx_qe_score": 9.45357608795166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有516个句子的差异超过一个模糊点,在", "metrics": {"bleu_score": 11.64394847706997, "chrf_score": 17.119524823255766, "xcomet_score": 0.38176366686820984, "xcomet_qe_score": 0.1605493575334549, "metricx_score": 12.46653938293457, "metricx_qe_score": 7.069896221160889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "极端情况下,可达40个模糊点。", "metrics": {"bleu_score": 28.253017719977493, "chrf_score": 21.409924402474964, "xcomet_score": 0.8123563528060913, "xcomet_qe_score": 0.8192775249481201, "metricx_score": 4.714085102081299, "metricx_qe_score": 2.6879265308380127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个好的提示策略非常重要。", "metrics": {"bleu_score": 62.685933350049744, "chrf_score": 59.37187365464642, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.24021872878074646, "metricx_qe_score": 0.356181263923645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们选择了五次射击提示策略,我们只是用句子所在的语言标记每个提供给系统的句子。", "metrics": {"bleu_score": 28.233323609698243, "chrf_score": 26.619425473514337, "xcomet_score": 0.7028273344039917, "xcomet_qe_score": 0.7596006393432617, "metricx_score": 5.188216209411621, "metricx_qe_score": 5.177650451660156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我们进行了德语到英语的翻译,德语句子(源句子)用德语冒号标记,英语翻译用英语冒号标记。", "metrics": {"bleu_score": 29.929675432510233, "chrf_score": 22.29520739238897, "xcomet_score": 0.9695634841918945, "xcomet_qe_score": 0.9769595861434937, "metricx_score": 1.4093503952026367, "metricx_qe_score": 1.3393099308013916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多个简短提示的情况下,提示的实际形式没有太大影响。", "metrics": {"bleu_score": 46.20762054572961, "chrf_score": 39.20349447966382, "xcomet_score": 0.9653195142745972, "xcomet_qe_score": 0.8977137804031372, "metricx_score": 0.6389818787574768, "metricx_qe_score": 0.836370050907135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零次和一次简短提示,这是至关重要的,", "metrics": {"bleu_score": 16.23988591941737, "chrf_score": 14.872137276468136, "xcomet_score": 0.6343039274215698, "xcomet_qe_score": 0.7179961800575256, "metricx_score": 3.150298595428467, "metricx_qe_score": 2.1848747730255127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们像我们这样使用五次简短提示时,提示的实际形式几乎没有差", "metrics": {"bleu_score": 9.668881540160378, "chrf_score": 14.10412901796779, "xcomet_score": 0.8645681142807007, "xcomet_qe_score": 0.7715104818344116, "metricx_score": 1.4751533269882202, "metricx_qe_score": 3.5810866355895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "异。例子承载了大部分权重。", "metrics": {"bleu_score": 3.7644257151903666, "chrf_score": 3.6231884057971016, "xcomet_score": 0.32439178228378296, "xcomet_qe_score": 0.35508495569229126, "metricx_score": 4.877232551574707, "metricx_qe_score": 4.403310298919678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,例子质量比与源句子的相似性更重要。", "metrics": {"bleu_score": 83.47563508866297, "chrf_score": 78.47704724153999, "xcomet_score": 0.9258888959884644, "xcomet_qe_score": 0.9196336269378662, "metricx_score": 0.9950923919677734, "metricx_qe_score": 0.7766151428222656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,从高质量的翻译中选择例子很重要。", "metrics": {"bleu_score": 56.971032152705476, "chrf_score": 51.437739907805344, "xcomet_score": 0.9378937482833862, "xcomet_qe_score": 0.9394103288650513, "metricx_score": 0.5218563675880432, "metricx_qe_score": 0.634639322757721, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了从WMT评估的训练数据或开发数据中选择提示。开发数据比训练", "metrics": {"bleu_score": 42.62838101265284, "chrf_score": 38.388523375112236, "xcomet_score": 0.5652488470077515, "xcomet_qe_score": 0.4417732357978821, "metricx_score": 6.063419342041016, "metricx_qe_score": 5.168334484100342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据更精心整理,质量更高,结果显示使用", "metrics": {"bleu_score": 7.516582156122356, "chrf_score": 8.693963614040074, "xcomet_score": 0.2968406677246094, "xcomet_qe_score": 0.2035522758960724, "metricx_score": 7.538273334503174, "metricx_qe_score": 7.335317134857178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据时表现更好。", "metrics": {"bleu_score": 43.11625024848288, "chrf_score": 37.49106281704841, "xcomet_score": 0.8237839937210083, "xcomet_qe_score": 0.7931971549987793, "metricx_score": 1.7046319246292114, "metricx_qe_score": 3.0656051635742188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,专业的现成翻译系统比Palm的翻译有实质", "metrics": {"bleu_score": 8.734116980604188, "chrf_score": 10.93449644252702, "xcomet_score": 0.6173135042190552, "xcomet_qe_score": 0.7401078939437866, "metricx_score": 7.289434909820557, "metricx_qe_score": 4.334978103637695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "性的优势,但Palm与商业系统非常接近。", "metrics": {"bleu_score": 30.277029197532105, "chrf_score": 23.80425393825008, "xcomet_score": 0.40716037154197693, "xcomet_qe_score": 0.33063533902168274, "metricx_score": 10.248558044433594, "metricx_qe_score": 11.446795463562012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择与谷歌翻译进行评估。", "metrics": {"bleu_score": 70.61595846869687, "chrf_score": 60.80436239038522, "xcomet_score": 0.8984283208847046, "xcomet_qe_score": 0.8782445192337036, "metricx_score": 1.7588478326797485, "metricx_qe_score": 1.8190504312515259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从使用MQM框架进行的人类评估中获得的见解是,Palm的流利度与最先进的系统相当,但主要差异来自准确性。", "metrics": {"bleu_score": 56.37019266277632, "chrf_score": 49.389246265112874, "xcomet_score": 0.7431407570838928, "xcomet_qe_score": 0.7613048553466797, "metricx_score": 4.555690765380859, "metricx_qe_score": 5.226734638214111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7579965591430664, "xcomet_qe_score": 0.7860524654388428, "metricx_score": 1.7050689458847046, "metricx_qe_score": 0.886371910572052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,看起来Palm有时通过省略句子中在翻译中被遗漏的部分来产生更好的翻译。", "metrics": {"bleu_score": 18.03631365675414, "chrf_score": 17.838803566730334, "xcomet_score": 0.9031211733818054, "xcomet_qe_score": 0.8193336725234985, "metricx_score": 4.949711799621582, "metricx_qe_score": 5.21665096282959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,Palm的风格外类别低于最先进的系统,这是一个额外的信号,表明Palm提供非常流利的输出,但仍然存在准确性的问题。", "metrics": {"bleu_score": 43.39797307924509, "chrf_score": 37.18730005323942, "xcomet_score": 0.7879005670547485, "xcomet_qe_score": 0.7634375691413879, "metricx_score": 7.307434558868408, "metricx_qe_score": 7.782312393188477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是这篇非常简短的概述。", "metrics": {"bleu_score": 10.1364546118828, "chrf_score": 13.758334244179693, "xcomet_score": 0.9892392158508301, "xcomet_qe_score": 0.9613409042358398, "metricx_score": 0.4420052170753479, "metricx_qe_score": 0.4518285393714905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有关更多详细信息,请参加论文的完整演讲。", "metrics": {"bleu_score": 40.142754193450884, "chrf_score": 34.10972315509554, "xcomet_score": 0.7415534853935242, "xcomet_qe_score": 0.7869642972946167, "metricx_score": 3.352308750152588, "metricx_qe_score": 3.2194771766662598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9730953574180603, "xcomet_qe_score": 0.9623823165893555, "metricx_score": 0.18832087516784668, "metricx_qe_score": 0.09758952260017395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Dawe,德国Stalant大学的一名博士生。", "metrics": {"bleu_score": 18.48293624367202, "chrf_score": 26.373378738667625, "xcomet_score": 0.743793249130249, "xcomet_qe_score": 0.7392023801803589, "metricx_score": 5.623461723327637, "metricx_qe_score": 6.149636268615723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这段视频中,我想向大家介绍我们最近的研究成果《比你想象的更脆弱》,这是一项对每周监督学习的批判性研究。", "metrics": {"bleu_score": 37.12457448024985, "chrf_score": 36.60936511911776, "xcomet_score": 0.8363169431686401, "xcomet_qe_score": 0.7485814690589905, "metricx_score": 4.6588263511657715, "metricx_qe_score": 5.215587615966797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与Xiao Yushchen、Maios Musbach、Giaz Steffen以及Dietrich Clarkov共同完成的工作。", "metrics": {"bleu_score": 12.384901282810539, "chrf_score": 35.94454386553475, "xcomet_score": 0.6644024848937988, "xcomet_qe_score": 0.6018010973930359, "metricx_score": 7.8901777267456055, "metricx_qe_score": 6.675606727600098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想从简要介绍每周监督和每周监督学习开始。", "metrics": {"bleu_score": 28.592291256793107, "chrf_score": 25.485634358866232, "xcomet_score": 0.7057757377624512, "xcomet_qe_score": 0.6470863819122314, "metricx_score": 5.714527130126953, "metricx_qe_score": 6.393099784851074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督中,我们不会手动标记数据。", "metrics": {"bleu_score": 42.08598069524091, "chrf_score": 37.65256883913606, "xcomet_score": 0.8243893384933472, "xcomet_qe_score": 0.7981953024864197, "metricx_score": 4.364009380340576, "metricx_qe_score": 4.4863996505737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用每周标记源来标记数据,例如简单的启发式规则、知识库或本地云众包,如右图所示。", "metrics": {"bleu_score": 59.354599841482624, "chrf_score": 51.65853687931126, "xcomet_score": 0.7153937816619873, "xcomet_qe_score": 0.6047037243843079, "metricx_score": 5.600135326385498, "metricx_qe_score": 6.661243915557861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,这些较弱的标注成本更低,但同时也存在噪声,意味着一定数量的标注是不正确的。", "metrics": {"bleu_score": 29.29877218112996, "chrf_score": 27.052398855923588, "xcomet_score": 0.8652774095535278, "xcomet_qe_score": 0.8776865005493164, "metricx_score": 1.8192479610443115, "metricx_qe_score": 1.885981798171997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在每周标注数据上训练神经网络,神经网络倾向于记住标注噪声,而无法泛化。", "metrics": {"bleu_score": 52.86765048025643, "chrf_score": 47.34571681374621, "xcomet_score": 0.7916015386581421, "xcomet_qe_score": 0.7528513669967651, "metricx_score": 4.870745658874512, "metricx_qe_score": 5.30570125579834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督学习中,提出了训练算法,以在这种标注噪声下稳健地训练神经网络,使训练后的模型仍能良好地泛化。", "metrics": {"bleu_score": 52.56375637880756, "chrf_score": 46.260661641742914, "xcomet_score": 0.8412341475486755, "xcomet_qe_score": 0.7985562086105347, "metricx_score": 4.586038112640381, "metricx_qe_score": 5.790560245513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的每周监督学习(WSL)研究中,一个常见的说法是,人们声称他们仅在每周标注数据上训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 31.068496598427643, "chrf_score": 28.67347455238143, "xcomet_score": 0.6743890643119812, "xcomet_qe_score": 0.6894159913063049, "metricx_score": 6.016294002532959, "metricx_qe_score": 6.372939109802246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并不错误,但有一个前提,即人们假设额外提供了干净的验证集用于模型选择。", "metrics": {"bleu_score": 32.9264331033105, "chrf_score": 28.361880337415457, "xcomet_score": 0.914872944355011, "xcomet_qe_score": 0.8997573852539062, "metricx_score": 2.1046271324157715, "metricx_qe_score": 2.706266164779663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这个问题设置表示怀疑,因为这意味着每周监督学习需要额外的手动标注。", "metrics": {"bleu_score": 43.929751176084075, "chrf_score": 36.1201201123295, "xcomet_score": 0.6737368106842041, "xcomet_qe_score": 0.6227384805679321, "metricx_score": 5.746350288391113, "metricx_qe_score": 6.2665605545043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这个必要性往往被忽视,就像房间里的象一样。", "metrics": {"bleu_score": 33.78892373468243, "chrf_score": 28.982696529225464, "xcomet_score": 0.9158799648284912, "xcomet_qe_score": 0.8239841461181641, "metricx_score": 2.506826400756836, "metricx_qe_score": 4.361143112182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述怀疑引导我们提出了三个研究问题。", "metrics": {"bleu_score": 40.21074690812007, "chrf_score": 38.57309075177447, "xcomet_score": 0.8785516023635864, "xcomet_qe_score": 0.9402345418930054, "metricx_score": 2.806514263153076, "metricx_qe_score": 2.4919116497039795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,干净验证数据对WSL是否必要?或者我们能否使用噪声验证集?", "metrics": {"bleu_score": 24.852652408509595, "chrf_score": 26.03858182981165, "xcomet_score": 0.7849474549293518, "xcomet_qe_score": 0.773072361946106, "metricx_score": 2.678190231323242, "metricx_qe_score": 3.87619948387146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果干净数据对WSL的运行必不可少,那么我们需要多少干净样本?", "metrics": {"bleu_score": 21.26954729000675, "chrf_score": 22.82897481353895, "xcomet_score": 0.9572206735610962, "xcomet_qe_score": 0.9119880199432373, "metricx_score": 1.0434712171554565, "metricx_qe_score": 1.099994421005249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否只应将干净样本用于验证,还是有更好的利用方式?", "metrics": {"bleu_score": 32.962244473750836, "chrf_score": 29.38382340088253, "xcomet_score": 0.9819043874740601, "xcomet_qe_score": 0.9253605604171753, "metricx_score": 0.6252179741859436, "metricx_qe_score": 0.9669337272644043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的发现如下。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 49.41553937149765, "xcomet_score": 0.9748376607894897, "xcomet_qe_score": 0.948908805847168, "metricx_score": 1.6144813299179077, "metricx_qe_score": 2.59200382232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,有趣的是,最新的WSL方法确实需要干净的验证样本才能正常工作。", "metrics": {"bleu_score": 61.83403052090209, "chrf_score": 60.96948323674787, "xcomet_score": 0.9336841106414795, "xcomet_qe_score": 0.9608975052833557, "metricx_score": 1.6515532732009888, "metricx_qe_score": 2.384187936782837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,训练后的模型无法超越原始的弱标注,这意味着训练是无用的。", "metrics": {"bleu_score": 57.83563003196118, "chrf_score": 51.338336641127825, "xcomet_score": 0.8204095363616943, "xcomet_qe_score": 0.8107835650444031, "metricx_score": 2.094029188156128, "metricx_qe_score": 3.288447856903076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净标记的数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 65.89531337511761, "chrf_score": 63.13315260066239, "xcomet_score": 0.7441959381103516, "xcomet_qe_score": 0.7213461399078369, "metricx_score": 2.4344937801361084, "metricx_qe_score": 3.649341583251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将帮助WSL方法取得更好的性能,如左图所示。", "metrics": {"bleu_score": 59.67330023421602, "chrf_score": 53.81566675263858, "xcomet_score": 0.8994868993759155, "xcomet_qe_score": 0.901549220085144, "metricx_score": 3.814303398132324, "metricx_qe_score": 4.8137898445129395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每类20个样本就能达到高性能。", "metrics": {"bleu_score": 17.101166217715278, "chrf_score": 19.053276401121853, "xcomet_score": 0.9430387020111084, "xcomet_qe_score": 0.9651368856430054, "metricx_score": 1.5335792303085327, "metricx_qe_score": 1.592826247215271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但故事并未结束,因为如果我们决定使用干净样本进行训练,那么直接训练甚至能取得更好的性能。", "metrics": {"bleu_score": 33.28891001010864, "chrf_score": 27.321453834099596, "xcomet_score": 0.9442689418792725, "xcomet_qe_score": 0.8863667845726013, "metricx_score": 3.7022125720977783, "metricx_qe_score": 4.084219932556152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "红图展示了直接作用于干净数据的微调方法与仅将干净数据用于验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 60.87329679249157, "chrf_score": 55.28869219290409, "xcomet_score": 0.8051672577857971, "xcomet_qe_score": 0.8509239554405212, "metricx_score": 2.745234727859497, "metricx_qe_score": 2.7324113845825195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每类有10个样本,直接微调开始超越WSL方法。最后,之前WSL方法声", "metrics": {"bleu_score": 48.260004845687256, "chrf_score": 44.24620327979509, "xcomet_score": 0.6349075436592102, "xcomet_qe_score": 0.5811038017272949, "metricx_score": 7.877807140350342, "metricx_qe_score": 5.901013374328613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "称的性能提升可以通过允许在干净验证样本上继续微调轻松实现。", "metrics": {"bleu_score": 20.28266770543541, "chrf_score": 19.063133217597418, "xcomet_score": 0.4376591742038727, "xcomet_qe_score": 0.46829113364219666, "metricx_score": 6.608320236206055, "metricx_qe_score": 7.55019474029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,名为FTW的Berliner模型最初在性能上落后于更复杂的WSL方法,如余弦。", "metrics": {"bleu_score": 26.193128775354342, "chrf_score": 23.57278034772701, "xcomet_score": 0.7119806408882141, "xcomet_qe_score": 0.6851648092269897, "metricx_score": 5.344301700592041, "metricx_qe_score": 5.919929504394531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果允许在干净样本上继续微调,FTW的表现与其它方法相当。", "metrics": {"bleu_score": 21.50981214328025, "chrf_score": 20.29721802384694, "xcomet_score": 0.8939579725265503, "xcomet_qe_score": 0.8415089845657349, "metricx_score": 1.3977125883102417, "metricx_qe_score": 2.0483205318450928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有必要选择需要更多计算时间和磁盘空间的更复杂WSL方法。", "metrics": {"bleu_score": 68.19390759143751, "chrf_score": 63.522216206091855, "xcomet_score": 0.9571167230606079, "xcomet_qe_score": 0.9750262498855591, "metricx_score": 0.7270140647888184, "metricx_qe_score": 0.958564281463623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们证明了最新的WSL方法需要干净的手动标注样本才能正常工作。", "metrics": {"bleu_score": 40.56291686098644, "chrf_score": 39.38157082286457, "xcomet_score": 0.907227635383606, "xcomet_qe_score": 0.865587592124939, "metricx_score": 2.2420833110809326, "metricx_qe_score": 2.727351665496826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择是在", "metrics": {"bleu_score": 47.41797560818527, "chrf_score": 39.769943052854664, "xcomet_score": 0.8286958932876587, "xcomet_qe_score": 0.807753324508667, "metricx_score": 5.731438636779785, "metricx_qe_score": 2.4792160987854004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "干净验证样本上进行的。", "metrics": {"bleu_score": 10.071354888662293, "chrf_score": 12.61640429890504, "xcomet_score": 0.25387048721313477, "xcomet_qe_score": 0.1915668547153473, "metricx_score": 7.830022811889648, "metricx_qe_score": 10.82919692993164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二,WSL方法应与未来学习基线进行比较,因为两者都在干净样本上工作。", "metrics": {"bleu_score": 40.98945732352121, "chrf_score": 36.1651571671336, "xcomet_score": 0.7284619808197021, "xcomet_qe_score": 0.7901966571807861, "metricx_score": 4.5511088371276855, "metricx_qe_score": 6.113962173461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一个简单而强大的基线,应在未来的WSL研究中考虑。", "metrics": {"bleu_score": 53.805831221843704, "chrf_score": 48.85221053797393, "xcomet_score": 0.8474147915840149, "xcomet_qe_score": 0.712556004524231, "metricx_score": 1.8753055334091187, "metricx_qe_score": 3.033796787261963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以通过本幻灯片上的二维码找到它。", "metrics": {"bleu_score": 53.989956849868726, "chrf_score": 45.4490172137231, "xcomet_score": 0.9949206113815308, "xcomet_qe_score": 0.9897035360336304, "metricx_score": 0.5753722190856934, "metricx_qe_score": 0.4856390655040741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎查看。", "metrics": {"bleu_score": 33.51600230178196, "chrf_score": 30.22716170004885, "xcomet_score": 0.9760723114013672, "xcomet_qe_score": 0.9048170447349548, "metricx_score": 0.13590192794799805, "metricx_qe_score": 0.28947681188583374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝大家享受大会。", "metrics": {"bleu_score": 12.498879161997976, "chrf_score": 16.329261695282028, "xcomet_score": 0.894599437713623, "xcomet_qe_score": 0.9022636413574219, "metricx_score": 0.7399343252182007, "metricx_qe_score": 0.6377139687538147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9827327728271484, "xcomet_qe_score": 1.0, "metricx_score": 0.8756831288337708, "metricx_qe_score": 0.3633490204811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCEval,一种评估对话人工智能的新维度方法。", "metrics": {"bleu_score": 23.44732872048571, "chrf_score": 27.012468720988103, "xcomet_score": 0.8492453098297119, "xcomet_qe_score": 0.9110332727432251, "metricx_score": 1.2411882877349854, "metricx_qe_score": 1.1905155181884766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学自然语言处理实验室完成,由埃默里大学的崔吉诺教授领导,并与亚马逊 Alexa AI 合作。", "metrics": {"bleu_score": 27.05923114555524, "chrf_score": 29.1267014131893, "xcomet_score": 0.7323286533355713, "xcomet_qe_score": 0.7294744253158569, "metricx_score": 3.128708839416504, "metricx_qe_score": 3.144172430038452, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚开发了一个对话模型,想了解它与当前最先进技术的比较情况。", "metrics": {"bleu_score": 49.83302374092996, "chrf_score": 45.71121990002384, "xcomet_score": 0.9924559593200684, "xcomet_qe_score": 0.9833860397338867, "metricx_score": 0.5238067507743835, "metricx_qe_score": 0.6774968504905701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如让人工评判员选择两个对话中哪一个更好,或在给定的量表上对对话进行评分。", "metrics": {"bleu_score": 58.61107867871932, "chrf_score": 53.16709004128736, "xcomet_score": 0.829281210899353, "xcomet_qe_score": 0.7832127213478088, "metricx_score": 0.8440512418746948, "metricx_qe_score": 0.8190006613731384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量评估方面效果良好,但对话质量有多个方面。", "metrics": {"bleu_score": 32.90988790846551, "chrf_score": 27.031722255929708, "xcomet_score": 0.9895071983337402, "xcomet_qe_score": 0.9802204370498657, "metricx_score": 0.3671559989452362, "metricx_qe_score": 0.5227299928665161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以更细致地了解模型的优缺点。", "metrics": {"bleu_score": 66.88495446062248, "chrf_score": 61.80778548472513, "xcomet_score": 0.9797821044921875, "xcomet_qe_score": 0.9579282999038696, "metricx_score": 0.6259473562240601, "metricx_qe_score": 0.5924166440963745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人工评判员评估对话质量的多个维度,例如模型响应的相关性,使用现有的比较或利克特量表方法。", "metrics": {"bleu_score": 53.548708247273666, "chrf_score": 45.78908787381419, "xcomet_score": 0.8004598617553711, "xcomet_qe_score": 0.7544432282447815, "metricx_score": 1.373776912689209, "metricx_qe_score": 1.9021031856536865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 45.15558127464808, "xcomet_score": 0.9024549126625061, "xcomet_qe_score": 0.8712908625602722, "metricx_score": 1.308674931526184, "metricx_qe_score": 1.4190480709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过明确标注每个模型响应是否表达某些行为,例如提供无关信息或自相矛盾。", "metrics": {"bleu_score": 34.054444834782586, "chrf_score": 36.092495174980584, "xcomet_score": 0.742957592010498, "xcomet_qe_score": 0.6072094440460205, "metricx_score": 6.065746784210205, "metricx_qe_score": 6.803107738494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为聊天行为标注,简称 ABCEval。", "metrics": {"bleu_score": 18.545405791834156, "chrf_score": 30.83488455456227, "xcomet_score": 0.8776133060455322, "xcomet_qe_score": 0.9092227220535278, "metricx_score": 1.7005226612091064, "metricx_qe_score": 2.0818514823913574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面涵盖最近文献中建议影响聊天质量聊天模型的行为。", "metrics": {"bleu_score": 53.4225500162273, "chrf_score": 43.92809456633312, "xcomet_score": 0.829866886138916, "xcomet_qe_score": 0.7974075078964233, "metricx_score": 2.865461587905884, "metricx_qe_score": 3.9806101322174072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABCEval 能够测量聊天模型犯各种主题错误的比例。", "metrics": {"bleu_score": 66.96510758566588, "chrf_score": 63.42639618415189, "xcomet_score": 0.8671729564666748, "xcomet_qe_score": 0.8330145478248596, "metricx_score": 3.8514657020568848, "metricx_qe_score": 5.4267659187316895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABCEval 测量聊天模型忽略其对话伙伴或说无关话的回合数,自相矛盾或与对话伙伴矛盾,编造不正确的事实或违反常识知识,以及模型表现或未能表现出同理心的情况。", "metrics": {"bleu_score": 44.1963917071524, "chrf_score": 38.44650685296693, "xcomet_score": 0.707980751991272, "xcomet_qe_score": 0.7358695268630981, "metricx_score": 5.098169326782227, "metricx_qe_score": 5.385417938232422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定最有效的评估方法,我们选择了四个最先进的聊天模型,并使用 ABCEval 对每个模型进行一百次人工聊天评估。", "metrics": {"bleu_score": 43.00042312213625, "chrf_score": 41.98598622661824, "xcomet_score": 0.8640550374984741, "xcomet_qe_score": 0.8602644205093384, "metricx_score": 1.9174854755401611, "metricx_qe_score": 1.6143361330032349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用三种现有方法对这些对话进行了评估:回合级利克特评分、对话级利克特评分和对话级配对比较。", "metrics": {"bleu_score": 43.35735678869488, "chrf_score": 37.365935227812386, "xcomet_score": 0.8517749309539795, "xcomet_qe_score": 0.8448075652122498, "metricx_score": 2.351943016052246, "metricx_qe_score": 2.3456532955169678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们还收集了八个最常见对话方面的评估,因为这是评估多维度聊天模型的标准做法。", "metrics": {"bleu_score": 39.50871557635704, "chrf_score": 35.31496609950091, "xcomet_score": 0.7925835847854614, "xcomet_qe_score": 0.786713719367981, "metricx_score": 3.720522165298462, "metricx_qe_score": 3.621943473815918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们对这些评估结果的分析中,我们发现 ABCEval 行为标签总体上比现有方法收集的标签更可靠,具体体现在 100 个双重标注对话", "metrics": {"bleu_score": 45.9821059786909, "chrf_score": 43.93015864226658, "xcomet_score": 0.7969114780426025, "xcomet_qe_score": 0.7701894044876099, "metricx_score": 6.560827732086182, "metricx_qe_score": 6.955244064331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的评分者间一致性上。此外,ABCEval 标签比现有方法产生的指标更能预测整体对话质量,正如这个简单的线性回归分析所示。", "metrics": {"bleu_score": 54.162424568611655, "chrf_score": 53.23117997647636, "xcomet_score": 0.6436877846717834, "xcomet_qe_score": 0.5316755175590515, "metricx_score": 7.102097988128662, "metricx_qe_score": 8.997203826904297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到测量自相矛盾和对话伙伴矛盾的回合比例分别解释了对话质量的 5% 和 10%,而平均利克特一致性得分仅解释了 4% 或更少。", "metrics": {"bleu_score": 65.2027021695082, "chrf_score": 58.72743792548645, "xcomet_score": 0.7112681865692139, "xcomet_qe_score": 0.7409462928771973, "metricx_score": 4.623560905456543, "metricx_qe_score": 4.952732086181641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们检查了每个评估指标是否捕捉了聊天质量的独特方面,使用逐步线性回归。您", "metrics": {"bleu_score": 59.24514756459842, "chrf_score": 54.03557981993945, "xcomet_score": 0.6489224433898926, "xcomet_qe_score": 0.7385213375091553, "metricx_score": 5.369019031524658, "metricx_qe_score": 2.9561092853546143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,所有 ABCEval 指标的组合解释了超过 25% 的对话质量,当您逐个删除指标时,大多数指标都会导致失去大量关于质量的信息。", "metrics": {"bleu_score": 46.139873343942085, "chrf_score": 47.74579253539024, "xcomet_score": 0.929679274559021, "xcomet_qe_score": 0.8561718463897705, "metricx_score": 1.78927481174469, "metricx_qe_score": 3.16416335105896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而替代级别的利克特指标组合解释的质量远少,这些指标中更少的", "metrics": {"bleu_score": 8.05958619639388, "chrf_score": 11.274948438552789, "xcomet_score": 0.3436936140060425, "xcomet_qe_score": 0.21396467089653015, "metricx_score": 13.36376953125, "metricx_qe_score": 9.778526306152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "指标携带独特信息。 这些可靠、信息丰富且独特的 ABCEval 指标使我们能够以高于先前方法能达到的分辨率评估对话人工智能。", "metrics": {"bleu_score": 5.4718009305878965, "chrf_score": 13.877720069324964, "xcomet_score": 0.2434041053056717, "xcomet_qe_score": 0.41313475370407104, "metricx_score": 5.177181243896484, "metricx_qe_score": 4.982054233551025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以在我们实验的结果中看到,仍然存在几个挑战,并且已被精确量化。", "metrics": {"bleu_score": 18.310725482344555, "chrf_score": 23.471495646204357, "xcomet_score": 0.9607840776443481, "xcomet_qe_score": 0.9606575965881348, "metricx_score": 1.1844905614852905, "metricx_qe_score": 1.410023808479309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人大约有 20% 的响应违反了常识。", "metrics": {"bleu_score": 53.67329609848117, "chrf_score": 46.47466158174103, "xcomet_score": 0.8597372770309448, "xcomet_qe_score": 0.8950106501579285, "metricx_score": 0.9239363074302673, "metricx_qe_score": 1.4850273132324219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在大约 15% 的响应中产生无关信息,并且大约 10% 的时间自相矛盾或与对话伙伴矛盾。", "metrics": {"bleu_score": 44.27795023903992, "chrf_score": 43.013255714931326, "xcomet_score": 0.6095024347305298, "xcomet_qe_score": 0.6476860046386719, "metricx_score": 3.697967767715454, "metricx_qe_score": 3.6749680042266846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速改进,这些错误率在新发布的模型中可能会降低。", "metrics": {"bleu_score": 32.845362827185696, "chrf_score": 30.270282025690697, "xcomet_score": 0.9742954969406128, "xcomet_qe_score": 0.9746063947677612, "metricx_score": 2.02573823928833, "metricx_qe_score": 3.362804889678955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更说明了追求可靠和精确的评估指标以进行模型比较的重要性。", "metrics": {"bleu_score": 27.610969152605097, "chrf_score": 26.546191791674072, "xcomet_score": 0.998726487159729, "xcomet_qe_score": 0.9917219877243042, "metricx_score": 0.6866550445556641, "metricx_qe_score": 0.7335138320922852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABCEval 能够被该领域的其他人士作为朝此方向迈出的有意义一步,并", "metrics": {"bleu_score": 44.89802404558728, "chrf_score": 49.32921715945952, "xcomet_score": 0.7175639867782593, "xcomet_qe_score": 0.7541966438293457, "metricx_score": 5.605125904083252, "metricx_qe_score": 2.6684343814849854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期待看到未来几个月和几年对话人工智能的进步。", "metrics": {"bleu_score": 42.76621024875847, "chrf_score": 35.23488428144618, "xcomet_score": 0.8850691914558411, "xcomet_qe_score": 0.8841658234596252, "metricx_score": 1.4395074844360352, "metricx_qe_score": 2.0206053256988525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我的名字是Kyo Yin,我将展示我们题为《何时翻译需要上下文?", "metrics": {"bleu_score": 20.776231135551065, "chrf_score": 24.454544370423502, "xcomet_score": 0.8073042035102844, "xcomet_qe_score": 0.8580608367919922, "metricx_score": 4.63337516784668, "metricx_qe_score": 5.514196395874023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语言探索》的作品。", "metrics": {"bleu_score": 45.466972369917116, "chrf_score": 48.30692383249817, "xcomet_score": 0.8781217336654663, "xcomet_qe_score": 0.8101226091384888, "metricx_score": 3.2416460514068604, "metricx_qe_score": 4.241120338439941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernandes、Emily Liu、Andre FD Martins和Graham Newbig合作完成的。", "metrics": {"bleu_score": 32.042193038079624, "chrf_score": 69.15799844863238, "xcomet_score": 0.8232953548431396, "xcomet_qe_score": 0.8795398473739624, "metricx_score": 2.418205499649048, "metricx_qe_score": 2.286593198776245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都依赖于上下文。", "metrics": {"bleu_score": 42.40125351805035, "chrf_score": 35.33598696125657, "xcomet_score": 0.9987486600875854, "xcomet_qe_score": 0.9918657541275024, "metricx_score": 0.15039336681365967, "metricx_qe_score": 0.23019355535507202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在这个句子中,如何翻译“mole”?", "metrics": {"bleu_score": 23.287896954139942, "chrf_score": 31.37407448359461, "xcomet_score": 0.9967906475067139, "xcomet_qe_score": 0.9723289012908936, "metricx_score": 1.136277198791504, "metricx_qe_score": 2.489640474319458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句是“如果部长们发现,事情可能会变得危险”,那么“mole”指的是间谍。", "metrics": {"bleu_score": 19.978883402526694, "chrf_score": 13.644254452312445, "xcomet_score": 0.9628349542617798, "xcomet_qe_score": 0.953872799873352, "metricx_score": 3.1619081497192383, "metricx_qe_score": 4.3693342208862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,会是什么严重的问题吗?”那么“mole”指的是胎记。", "metrics": {"bleu_score": 19.263460320207695, "chrf_score": 17.261769403617038, "xcomet_score": 0.9692732095718384, "xcomet_qe_score": 0.9591715931892395, "metricx_score": 2.0058529376983643, "metricx_qe_score": 2.947087287902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据上下文,词的意义会改变,因此它的翻译也会相应地改变。", "metrics": {"bleu_score": 23.485798626232196, "chrf_score": 19.81267555497715, "xcomet_score": 0.9997307062149048, "xcomet_qe_score": 0.9982489347457886, "metricx_score": 0.5380727052688599, "metricx_qe_score": 0.4188041687011719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型能多好地翻译这类案例是相当困难的。", "metrics": {"bleu_score": 23.352648136133084, "chrf_score": 19.463881263810244, "xcomet_score": 0.8841871023178101, "xcomet_qe_score": 0.8394544124603271, "metricx_score": 1.3556731939315796, "metricx_qe_score": 2.073239326477051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有少量翻译依赖于上下文,这使蓝星(BLEU)等语料库级别的指标无法捕捉到这些翻译。有", "metrics": {"bleu_score": 39.37250038206482, "chrf_score": 35.077039851951106, "xcomet_score": 0.7866016626358032, "xcomet_qe_score": 0.7693121433258057, "metricx_score": 5.763840675354004, "metricx_qe_score": 4.1708173751831055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些人建议对上下文依赖的翻译进行定向评估,但这些资源只支持有限类型的上下文依赖翻译和有限的语言集,因为它们通常依赖于领域知识和人工整理。", "metrics": {"bleu_score": 76.6876257739812, "chrf_score": 73.92527607716474, "xcomet_score": 0.7202706933021545, "xcomet_qe_score": 0.6518251895904541, "metricx_score": 4.018618106842041, "metricx_qe_score": 3.599724292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们尝试回答两个问题。", "metrics": {"bleu_score": 26.197527109265895, "chrf_score": 22.468359719422164, "xcomet_score": 0.9924654960632324, "xcomet_qe_score": 0.9918102025985718, "metricx_score": 0.7868586778640747, "metricx_qe_score": 0.4339318871498108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,何时翻译需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9996569156646729, "xcomet_qe_score": 0.9977697134017944, "metricx_score": 0.15239903330802917, "metricx_qe_score": 0.3020760715007782, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型能多好地处理这些案例?", "metrics": {"bleu_score": 17.150296156301636, "chrf_score": 15.150667871784016, "xcomet_score": 0.9320632219314575, "xcomet_qe_score": 0.8331296443939209, "metricx_score": 1.527062177658081, "metricx_qe_score": 1.2086567878723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们开始测量词在翻译中对上下文的依赖程度。", "metrics": {"bleu_score": 59.66777043645755, "chrf_score": 53.862348742720364, "xcomet_score": 0.9190471172332764, "xcomet_qe_score": 0.9939479827880859, "metricx_score": 4.36029577255249, "metricx_qe_score": 4.1269073486328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在先前的工作中,我们引入了CXMI作为机器翻译模型上下文使用量的度量。", "metrics": {"bleu_score": 57.19408603967074, "chrf_score": 59.345310377550106, "xcomet_score": 0.8919402360916138, "xcomet_qe_score": 0.888935387134552, "metricx_score": 1.7021788358688354, "metricx_qe_score": 1.811134696006775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量上下文C在给定源X的情况下对目标Y提供的信息量来完成。你可以将CXMI视为给模型提供上下文获得的信息量。", "metrics": {"bleu_score": 49.52054297636483, "chrf_score": 45.667156325999656, "xcomet_score": 0.8529435396194458, "xcomet_qe_score": 0.6786059737205505, "metricx_score": 3.152381658554077, "metricx_qe_score": 3.5034384727478027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将CXMI扩展为点CXMI,它可以在句子级别或词级别测量上下文使用量。", "metrics": {"bleu_score": 39.26939994966423, "chrf_score": 33.842094503608536, "xcomet_score": 0.772567629814148, "xcomet_qe_score": 0.7293733358383179, "metricx_score": 3.6398916244506836, "metricx_qe_score": 3.464557647705078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将具有高PCXMI的词视为需要上下文才能正确翻译的词。", "metrics": {"bleu_score": 56.05976101691429, "chrf_score": 48.71664595094342, "xcomet_score": 0.9820203185081482, "xcomet_qe_score": 0.9679794311523438, "metricx_score": 1.375889539718628, "metricx_qe_score": 2.0500707626342773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析具有高PCXMI的词,以寻找这些词之间的模式。", "metrics": {"bleu_score": 45.43681957056484, "chrf_score": 45.88456276253815, "xcomet_score": 0.9551713466644287, "xcomet_qe_score": 0.9463609457015991, "metricx_score": 1.311327338218689, "metricx_qe_score": 2.8229591846466064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在从英语翻译成十四种不同语言的TED演讲转录本上进行分析。", "metrics": {"bleu_score": 57.19139519171822, "chrf_score": 53.07349846082888, "xcomet_score": 0.9074482321739197, "xcomet_qe_score": 0.9204625487327576, "metricx_score": 2.827756404876709, "metricx_qe_score": 2.1901636123657227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同级别进行分析。", "metrics": {"bleu_score": 60.659974333376745, "chrf_score": 50.87108481075532, "xcomet_score": 0.9124049544334412, "xcomet_qe_score": 0.9270819425582886, "metricx_score": 0.6496804356575012, "metricx_qe_score": 0.9960294961929321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有高均值PCXMI的词性标签。", "metrics": {"bleu_score": 34.6697783111003, "chrf_score": 31.708611730813757, "xcomet_score": 0.8765404224395752, "xcomet_qe_score": 0.8479657769203186, "metricx_score": 2.1203370094299316, "metricx_qe_score": 2.6767332553863525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到,例如,阿拉伯语中具有相对较高PCXMI的双数代词。", "metrics": {"bleu_score": 59.511819037536675, "chrf_score": 49.028234223992015, "xcomet_score": 0.9028207063674927, "xcomet_qe_score": 0.827842652797699, "metricx_score": 2.864936113357544, "metricx_qe_score": 3.556272029876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语没有双数代词,因此在翻译成阿拉伯语时,需要上下文来确定代词是否为双数。", "metrics": {"bleu_score": 71.54234924072287, "chrf_score": 65.7458510703771, "xcomet_score": 0.9901086091995239, "xcomet_qe_score": 0.9714117646217346, "metricx_score": 0.8948796987533569, "metricx_qe_score": 1.4898313283920288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 85.94148359295417, "chrf_score": 87.96567669989582, "xcomet_score": 0.9981253147125244, "xcomet_qe_score": 0.9977245330810547, "metricx_score": 0.5655370354652405, "metricx_qe_score": 0.7977930307388306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们查看在所有不同出现中平均具有高PCXMI的词汇项。", "metrics": {"bleu_score": 32.16397363858737, "chrf_score": 28.26830683152522, "xcomet_score": 0.7830766439437866, "xcomet_qe_score": 0.7228178381919861, "metricx_score": 4.332151889801025, "metricx_qe_score": 4.469555377960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的案例,在中文中,你需要上下文才能正确翻译。", "metrics": {"bleu_score": 9.848772675362117, "chrf_score": 12.696249078701141, "xcomet_score": 0.9335588216781616, "xcomet_qe_score": 0.896016001701355, "metricx_score": 1.9309687614440918, "metricx_qe_score": 2.5322186946868896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们发现上下文对于翻译正确的语气非常重要。", "metrics": {"bleu_score": 46.51052535355049, "chrf_score": 42.00174444946079, "xcomet_score": 0.862115204334259, "xcomet_qe_score": 0.8500951528549194, "metricx_score": 0.7234599590301514, "metricx_qe_score": 0.8050767183303833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看具有高PCXMI的各个单独词素。", "metrics": {"bleu_score": 7.200062223352653, "chrf_score": 16.489514053299118, "xcomet_score": 0.7987602353096008, "xcomet_qe_score": 0.7511232495307922, "metricx_score": 1.9270284175872803, "metricx_qe_score": 2.4872076511383057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别无法真正由词本身捕捉到的现象,而是通过句子结构表达的现象,例如省略号解析。", "metrics": {"bleu_score": 45.73979632941269, "chrf_score": 40.6225874743341, "xcomet_score": 0.8507013320922852, "xcomet_qe_score": 0.7928615808486938, "metricx_score": 1.398358941078186, "metricx_qe_score": 1.6903133392333984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们利用分析结果设计了一个文档级别翻译的基准。", "metrics": {"bleu_score": 38.672497465949114, "chrf_score": 31.427241431931918, "xcomet_score": 0.9788037538528442, "xcomet_qe_score": 0.8698101043701172, "metricx_score": 0.8791519999504089, "metricx_qe_score": 1.1474636793136597, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别的五个话语现象中的每一个,我们创建自动识别与该现象相关的词的", "metrics": {"bleu_score": 45.926218730005246, "chrf_score": 39.144847307314365, "xcomet_score": 0.70359206199646, "xcomet_qe_score": 0.7508984804153442, "metricx_score": 4.0671257972717285, "metricx_qe_score": 4.08864164352417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "标记器,我们称之为多语言话语感知(MUDA)标记器。", "metrics": {"bleu_score": 39.412263479065864, "chrf_score": 32.036735053723504, "xcomet_score": 0.6969715356826782, "xcomet_qe_score": 0.8194917440414429, "metricx_score": 1.4824132919311523, "metricx_qe_score": 1.9543852806091309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以注意到,不同语言这些话语现象的比例不同。", "metrics": {"bleu_score": 48.33093123477804, "chrf_score": 39.874335116313965, "xcomet_score": 0.9254016876220703, "xcomet_qe_score": 0.9186604022979736, "metricx_score": 0.99159836769104, "metricx_qe_score": 1.2385551929473877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们使用MUDA标记器,通过将标记器应用于我们想要用于评估的平行语料库,并应用我们选择的翻译指标,对MUDA标记器识别的上下文依赖示例进行评估。", "metrics": {"bleu_score": 49.31061350451581, "chrf_score": 52.70242972885861, "xcomet_score": 0.8539531230926514, "xcomet_qe_score": 0.9105710983276367, "metricx_score": 1.5401103496551514, "metricx_qe_score": 1.6612781286239624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准和其他指标来评估不同的文档级别机器翻译模型。", "metrics": {"bleu_score": 56.2666488475617, "chrf_score": 47.76797291171596, "xcomet_score": 0.945886492729187, "xcomet_qe_score": 0.8490729331970215, "metricx_score": 0.8449711799621582, "metricx_qe_score": 1.047188401222229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别指标时,对于蓝星(BLEU),我们发现不考虑上下文的模型表现最佳,", "metrics": {"bleu_score": 47.957600807198475, "chrf_score": 43.26998219286279, "xcomet_score": 0.8550971150398254, "xcomet_qe_score": 0.7928694486618042, "metricx_score": 2.9034245014190674, "metricx_qe_score": 3.139949083328247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果使用Comet,则考虑上下文的模型表现最佳。", "metrics": {"bleu_score": 14.253988589758045, "chrf_score": 16.75139446044038, "xcomet_score": 0.8763464689254761, "xcomet_qe_score": 0.8747137784957886, "metricx_score": 2.8353047370910645, "metricx_qe_score": 2.378934860229492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用WordF度量,则具有或不具有上下文的模型性能可比。", "metrics": {"bleu_score": 22.765885934874397, "chrf_score": 20.499304581888573, "xcomet_score": 0.8370099067687988, "xcomet_qe_score": 0.7250304818153381, "metricx_score": 3.9056949615478516, "metricx_qe_score": 2.7941782474517822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果仅使用语料库级别指标,很难确定最佳的文档级别翻译系统。", "metrics": {"bleu_score": 62.92533074774741, "chrf_score": 52.94909246728183, "xcomet_score": 0.9940880537033081, "xcomet_qe_score": 0.9878747463226318, "metricx_score": 0.7648402452468872, "metricx_qe_score": 0.9804153442382812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们使用MUDA基准来评估模型,发现在某些话语现象中,例如语气和词汇连贯性,考虑上下文的模型显著准确率更高,而不考虑上下文的模型在其他现象中", "metrics": {"bleu_score": 35.583563043092205, "chrf_score": 35.123159623271924, "xcomet_score": 0.6245474815368652, "xcomet_qe_score": 0.657650887966156, "metricx_score": 5.1529059410095215, "metricx_qe_score": 5.046136856079102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如省略号、代词和动词形式,表现并不比考虑上下文的模型好。", "metrics": {"bleu_score": 38.3171620841372, "chrf_score": 37.10646762289202, "xcomet_score": 0.8294817209243774, "xcomet_qe_score": 0.23619846999645233, "metricx_score": 4.311557769775391, "metricx_qe_score": 5.236677646636963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在某种程度上表明了文档级别翻译需要更多进步的领域。", "metrics": {"bleu_score": 35.633393279594564, "chrf_score": 34.39455447307781, "xcomet_score": 0.999079704284668, "xcomet_qe_score": 0.994422197341919, "metricx_score": 1.2047182321548462, "metricx_qe_score": 1.607283115386963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准显示,DPL通常比谷歌翻译更准确地进行文档级别翻译。", "metrics": {"bleu_score": 61.219418182808326, "chrf_score": 52.34883836138834, "xcomet_score": 0.8104219436645508, "xcomet_qe_score": 0.7828814387321472, "metricx_score": 3.1917874813079834, "metricx_qe_score": 3.193964719772339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们在十四个语言对上进行数据驱动的分析,以识别何时翻译需要上下文。然后,我们利用分析结果建立一个文档级别机器翻译的基准,它可以帮助我们识别哪些离散现象模型能很好地处理,哪些翻译系统擅长文档级别翻译。", "metrics": {"bleu_score": 37.03240951173688, "chrf_score": 31.56015374954622, "xcomet_score": 0.839022159576416, "xcomet_qe_score": 0.8690155744552612, "metricx_score": 2.9861488342285156, "metricx_qe_score": 3.6254312992095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢你的关注。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 10.444972826086955, "xcomet_score": 0.79347163438797, "xcomet_qe_score": 0.9814639091491699, "metricx_score": 1.4708298444747925, "metricx_qe_score": 0.8091220855712891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "明天见。", "metrics": {"bleu_score": 11.752701606523267, "chrf_score": 10.982428115015974, "xcomet_score": 0.519114077091217, "xcomet_qe_score": 0.17786654829978943, "metricx_score": 1.966428279876709, "metricx_qe_score": 3.6971447467803955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Yanis Lavrack,我将向您展示我们在Dr. Berth方面的工作,这是一个针对生物医学和临床领域的法语鲁棒预训练模型。", "metrics": {"bleu_score": 34.53290324154855, "chrf_score": 36.15770926772217, "xcomet_score": 0.6173561215400696, "xcomet_qe_score": 0.6176378130912781, "metricx_score": 5.558886528015137, "metricx_qe_score": 5.869051456451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先将讨论医疗领域的语言建模。", "metrics": {"bleu_score": 58.197263847995686, "chrf_score": 48.4727780593836, "xcomet_score": 0.9951730966567993, "xcomet_qe_score": 1.0, "metricx_score": 0.5434015989303589, "metricx_qe_score": 0.5371213555335999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个法语生物医学模型Dr. Berth,该模型基于Roberta,并在Natchios上进行训练,Natchios是一个从网络上抓取的医学数据集。", "metrics": {"bleu_score": 40.17703288243513, "chrf_score": 28.57035110292655, "xcomet_score": 0.6360429525375366, "xcomet_qe_score": 0.5459573268890381, "metricx_score": 3.700876235961914, "metricx_qe_score": 3.4236021041870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了对多个预训练设置和数据源的模型进行比较。接下", "metrics": {"bleu_score": 57.83469454651139, "chrf_score": 58.18699204646185, "xcomet_score": 0.7091941237449646, "xcomet_qe_score": 0.8003170490264893, "metricx_score": 3.625210762023926, "metricx_qe_score": 2.8364830017089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来,我们将展示我们在11个法语生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 66.29320430377793, "chrf_score": 63.17116288440039, "xcomet_score": 0.7208652496337891, "xcomet_qe_score": 0.6793785095214844, "metricx_score": 2.566291093826294, "metricx_qe_score": 3.4281678199768066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们将对实验进行总结,并详细介绍如何访问这些模型。", "metrics": {"bleu_score": 26.145093445467566, "chrf_score": 25.312696837923436, "xcomet_score": 0.9019505977630615, "xcomet_qe_score": 0.908667802810669, "metricx_score": 0.27272000908851624, "metricx_qe_score": 0.3093780279159546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务最有效的手段之一,与历史静态和上下文方法(如Word2Vec、Fastex或NWO)相比,性能有了显著提升。", "metrics": {"bleu_score": 53.983521485802015, "chrf_score": 48.31000517503791, "xcomet_score": 0.7572077512741089, "xcomet_qe_score": 0.8406850695610046, "metricx_score": 3.263000965118408, "metricx_qe_score": 3.0171523094177246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此后,该模型已被适应到许多其他语言,如法语的Camembert,以及生物医学领域的PermetteBERT和BioBERT,临床领域的ClinicalBERT,但主要是在英语中。", "metrics": {"bleu_score": 44.085667066542385, "chrf_score": 52.40882026441207, "xcomet_score": 0.5280013084411621, "xcomet_qe_score": 0.5300665497779846, "metricx_score": 6.371620178222656, "metricx_qe_score": 6.365993022918701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的专业模型非常稀缺,通常由于缺乏领域内数据而基于连续预训练。", "metrics": {"bleu_score": 49.52537002011174, "chrf_score": 39.6581505280509, "xcomet_score": 0.8408068418502808, "xcomet_qe_score": 0.8264322280883789, "metricx_score": 0.8888858556747437, "metricx_qe_score": 1.1666384935379028, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法语在生物医学领域一直没有开源模型,直到现在。", "metrics": {"bleu_score": 42.54893771166975, "chrf_score": 34.28241806502676, "xcomet_score": 0.8373720645904541, "xcomet_qe_score": 0.76519775390625, "metricx_score": 1.4429383277893066, "metricx_qe_score": 2.5566885471343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了一个问题:对于广泛的使用范围,最合适的数据来源是什么?当前数据是否可以作为临床数据的良好替代?", "metrics": {"bleu_score": 23.521716944407327, "chrf_score": 25.212353543093823, "xcomet_score": 0.8836849927902222, "xcomet_qe_score": 0.8697760105133057, "metricx_score": 1.655287265777588, "metricx_qe_score": 1.4618394374847412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Dr. Bert与我们的Schubert模型进行比较,Schubert模型基于我们拥有的非大学医院的匿名数据。", "metrics": {"bleu_score": 41.75406153049197, "chrf_score": 30.39831017524024, "xcomet_score": 0.7153409719467163, "xcomet_qe_score": 0.703137993812561, "metricx_score": 6.0544281005859375, "metricx_qe_score": 6.890068054199219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们将问自己另一个问题:训练一个法语专业模型需要多少数据?", "metrics": {"bleu_score": 38.04736860717706, "chrf_score": 34.123611947395446, "xcomet_score": 0.8784229755401611, "xcomet_qe_score": 0.8018240928649902, "metricx_score": 1.4444785118103027, "metricx_qe_score": 1.6456308364868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4 GB、8 GB还是更多?", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 90.21205646205644, "xcomet_score": 0.9752111434936523, "xcomet_qe_score": 0.9724503755569458, "metricx_score": 0.2852798104286194, "metricx_qe_score": 0.51189124584198, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先训练并比较了四个从零开始的模型:Dr. Bert的第一个版本使用7 GB的Natchios数据,第二个版本使用4 GB的Natchios数据,Schubert的第一个版本(临床模型)使用4 GB从临床节点中提取的句子,Schubert的最终版本混合使用4 GB的Natchios数据和4 GB的临床节点数据。", "metrics": {"bleu_score": 32.666492970010005, "chrf_score": 28.648740420585934, "xcomet_score": 0.3989221155643463, "xcomet_qe_score": 0.4380042254924774, "metricx_score": 5.380841255187988, "metricx_qe_score": 5.092765808105469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还引入了三个基于连续预训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 58.82232157963849, "chrf_score": 50.878033534428226, "xcomet_score": 0.9533371925354004, "xcomet_qe_score": 0.9402303695678711, "metricx_score": 1.0142415761947632, "metricx_qe_score": 1.3368892669677734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于Camembert权重并在4 GB的Natchios数据上进行训练,", "metrics": {"bleu_score": 20.96936506655292, "chrf_score": 36.95153470888765, "xcomet_score": 0.6974145174026489, "xcomet_qe_score": 0.6960083842277527, "metricx_score": 4.934933185577393, "metricx_qe_score": 5.3814568519592285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也基于Camembert,但这次在4 GB的临床节点数据上进行训练,最后一个基于英语生物医学模型Bermud Bert,并在4 GB的Natchios数据上进行训练。", "metrics": {"bleu_score": 39.81537273199278, "chrf_score": 43.7266115805347, "xcomet_score": 0.5764978528022766, "xcomet_qe_score": 0.5391597747802734, "metricx_score": 6.817846298217773, "metricx_qe_score": 6.696146488189697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们有七个模型。", "metrics": {"bleu_score": 57.067457770560026, "chrf_score": 52.68257581869399, "xcomet_score": 0.9877036809921265, "xcomet_qe_score": 0.8995441198348999, "metricx_score": 0.29523736238479614, "metricx_qe_score": 0.44317466020584106, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了匹配的公开和私有任务,如命名实体识别、情感分析、分类、模式切换标注和问答。", "metrics": {"bleu_score": 46.021642480770886, "chrf_score": 41.12455168367974, "xcomet_score": 0.5627695322036743, "xcomet_qe_score": 0.4820050001144409, "metricx_score": 6.243393421173096, "metricx_qe_score": 6.705828666687012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行比较,分别是Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBERT、BioBERT和ClinicalBERT。", "metrics": {"bleu_score": 48.26243843000455, "chrf_score": 58.730742356497935, "xcomet_score": 0.5827559232711792, "xcomet_qe_score": 0.5176911950111389, "metricx_score": 4.936187267303467, "metricx_qe_score": 4.605105876922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,模型在训练数据与任务数据性质相同的任务上表现最佳。", "metrics": {"bleu_score": 41.716188514999516, "chrf_score": 33.84519886056525, "xcomet_score": 0.9828665256500244, "xcomet_qe_score": 0.9605613946914673, "metricx_score": 0.8117631673812866, "metricx_qe_score": 1.019559621810913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以观察到,来自异质来源的数据似乎更具多功能性。", "metrics": {"bleu_score": 46.39035504542229, "chrf_score": 42.9108615160041, "xcomet_score": 0.8267236948013306, "xcomet_qe_score": 0.8248207569122314, "metricx_score": 1.177579641342163, "metricx_qe_score": 1.0418063402175903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据会带来更好的性能。", "metrics": {"bleu_score": 32.1593960910315, "chrf_score": 26.729136961169715, "xcomet_score": 0.937077522277832, "xcomet_qe_score": 0.9789898991584778, "metricx_score": 2.59717059135437, "metricx_qe_score": 3.111323356628418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,从零开始的微调似乎在大多数任务上获得了更高的性能。", "metrics": {"bleu_score": 36.6207066799521, "chrf_score": 31.54328218594828, "xcomet_score": 0.8516210317611694, "xcomet_qe_score": 0.838354766368866, "metricx_score": 4.353979587554932, "metricx_qe_score": 4.758357048034668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们在连续微调上的实验,使用PumedBeard的权重和分词器,在4 GB的Natchios子集上进行训练,显示出与从零开始的Dr. Beard 4 GB模型可比的", "metrics": {"bleu_score": 19.4924281583996, "chrf_score": 19.456259838942316, "xcomet_score": 0.26403406262397766, "xcomet_qe_score": 0.28702524304389954, "metricx_score": 10.674098014831543, "metricx_qe_score": 9.302240371704102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果,而基于Camembert权重和分词器的模型则存在稳定性问题。", "metrics": {"bleu_score": 28.676853292656155, "chrf_score": 31.634716351454777, "xcomet_score": 0.5902734994888306, "xcomet_qe_score": 0.5451654195785522, "metricx_score": 5.093033313751221, "metricx_qe_score": 4.374171733856201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们提出的系统在11个DOTSTRIMS任务中有9个任务上表现更好,整体上超越了通用模型Camembert的结果。", "metrics": {"bleu_score": 25.653478008991225, "chrf_score": 25.39379304310615, "xcomet_score": 0.8237501382827759, "xcomet_qe_score": 0.7966448068618774, "metricx_score": 5.620581150054932, "metricx_qe_score": 4.229135036468506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业数据越专门越好,但可扩展性不佳。", "metrics": {"bleu_score": 37.67024601768617, "chrf_score": 31.30991200473327, "xcomet_score": 0.927401065826416, "xcomet_qe_score": 0.9020252823829651, "metricx_score": 1.3442468643188477, "metricx_qe_score": 1.5087366104125977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从Natchios获得的预训练模型都在YuginFace上免费提供,所有训练脚本都在我们的GitHub仓库中。", "metrics": {"bleu_score": 31.731454102087824, "chrf_score": 34.631248453829954, "xcomet_score": 0.6226520538330078, "xcomet_qe_score": 0.7010310292243958, "metricx_score": 6.6336565017700195, "metricx_qe_score": 6.463043689727783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,感谢您的聆听,我们期待在多伦多后的交流环节中与您互动。", "metrics": {"bleu_score": 11.622111816655842, "chrf_score": 15.039448965885157, "xcomet_score": 0.44617632031440735, "xcomet_qe_score": 0.7286059260368347, "metricx_score": 2.864903450012207, "metricx_qe_score": 2.5891990661621094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是马蒂亚斯·伦达曼(Matthias Lendermann),今天我将简要介绍我们关于使用多集标记和潜在置换进行树结构之外的组合泛化论文。", "metrics": {"bleu_score": 34.45116798628281, "chrf_score": 42.18248998987581, "xcomet_score": 0.8267736434936523, "xcomet_qe_score": 0.8522247672080994, "metricx_score": 3.0383987426757812, "metricx_qe_score": 3.0413949489593506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与导师亚历山大·科勒(Alexander Koller)和伊万·蒂托夫(Ivan Titoff)的合作成果。", "metrics": {"bleu_score": 18.02916852147069, "chrf_score": 56.08114465137438, "xcomet_score": 0.9975284337997437, "xcomet_qe_score": 0.9839342832565308, "metricx_score": 1.378318190574646, "metricx_qe_score": 1.4156317710876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深层递归和训练过程中单独见过的短语组合的能力。", "metrics": {"bleu_score": 65.47932192363574, "chrf_score": 60.126828806356926, "xcomet_score": 0.8043113946914673, "xcomet_qe_score": 0.7038192749023438, "metricx_score": 4.031253814697266, "metricx_qe_score": 5.330540657043457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能如下所示。", "metrics": {"bleu_score": 70.07637953409846, "chrf_score": 60.13480369940909, "xcomet_score": 0.908198356628418, "xcomet_qe_score": 0.8913741707801819, "metricx_score": 0.9897307753562927, "metricx_qe_score": 1.7725532054901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与往常一样,我们有一个训练语句集,", "metrics": {"bleu_score": 34.7403173905042, "chrf_score": 27.903828197945842, "xcomet_score": 0.9925757646560669, "xcomet_qe_score": 0.95769202709198, "metricx_score": 0.8409402966499329, "metricx_qe_score": 1.0273027420043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“女孩睡着了”和", "metrics": {"bleu_score": 7.55205501036964, "chrf_score": 6.2681045562041975, "xcomet_score": 0.7490692138671875, "xcomet_qe_score": 0.6291144490242004, "metricx_score": 4.057464599609375, "metricx_qe_score": 1.1286463737487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道女孩睡着了”。这些", "metrics": {"bleu_score": 14.092865659855665, "chrf_score": 9.344717991362169, "xcomet_score": 0.8641207218170166, "xcomet_qe_score": 0.8054601550102234, "metricx_score": 5.602951526641846, "metricx_qe_score": 1.5167977809906006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语句与代表其核心意义的逻辑形式配对。", "metrics": {"bleu_score": 10.591379891265163, "chrf_score": 13.287090168286268, "xcomet_score": 0.9944337606430054, "xcomet_qe_score": 0.9908231496810913, "metricx_score": 1.0372918844223022, "metricx_qe_score": 0.6989502906799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集不来自同一分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 61.55490299373211, "chrf_score": 56.18122036913612, "xcomet_score": 0.8613531589508057, "xcomet_qe_score": 0.8393374681472778, "metricx_score": 1.1579457521438599, "metricx_qe_score": 2.1692683696746826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练期间见过更浅的递归,但在测试时遇到更深的递归。", "metrics": {"bleu_score": 35.64638535092942, "chrf_score": 29.19747380236444, "xcomet_score": 0.8916005492210388, "xcomet_qe_score": 0.8739016056060791, "metricx_score": 3.3102502822875977, "metricx_qe_score": 6.40954065322876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "天真的序列到序列模型在这个出分布泛化方面遇到困难,经常产生与输入脱节的输出。", "metrics": {"bleu_score": 26.241028629237412, "chrf_score": 21.803086326533684, "xcomet_score": 0.6763968467712402, "xcomet_qe_score": 0.7861254215240479, "metricx_score": 3.2667770385742188, "metricx_qe_score": 2.579787254333496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们经常无法再现输入和输出之间的系统对应关系,例如在例子中用颜色标注的部分。一种流", "metrics": {"bleu_score": 55.28919696204963, "chrf_score": 53.02936666024935, "xcomet_score": 0.7188655138015747, "xcomet_qe_score": 0.667160153388977, "metricx_score": 5.268178462982178, "metricx_qe_score": 3.6877567768096924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "行的解决方法是将树结构集成到模型中。", "metrics": {"bleu_score": 33.49490518292079, "chrf_score": 27.35321007693968, "xcomet_score": 0.5870693922042847, "xcomet_qe_score": 0.5882370471954346, "metricx_score": 3.470876693725586, "metricx_qe_score": 4.396966934204102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树旨在捕捉与逻辑形式相关联的语句的组合过程。", "metrics": {"bleu_score": 25.315112684135887, "chrf_score": 23.066581534423513, "xcomet_score": 0.9291744232177734, "xcomet_qe_score": 0.8273036479949951, "metricx_score": 3.5279271602630615, "metricx_qe_score": 4.159653663635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这效果很好,但树结构通常不给定,需要以某种方式获取。", "metrics": {"bleu_score": 30.99874754267577, "chrf_score": 27.08894242014064, "xcomet_score": 0.8988077640533447, "xcomet_qe_score": 0.9330195188522339, "metricx_score": 1.2462372779846191, "metricx_qe_score": 2.418480157852173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能很复杂,有时是计算上昂贵的过程。", "metrics": {"bleu_score": 40.96812701940896, "chrf_score": 34.01761889982844, "xcomet_score": 0.9011823534965515, "xcomet_qe_score": 0.8541555404663086, "metricx_score": 1.9126591682434082, "metricx_qe_score": 3.017669439315796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常这涉及到对逻辑形式进行相当正式的预处理,例如处理变量符号。", "metrics": {"bleu_score": 38.43531816984318, "chrf_score": 33.87400158753957, "xcomet_score": 0.8899352550506592, "xcomet_qe_score": 0.8683955669403076, "metricx_score": 1.3612072467803955, "metricx_qe_score": 1.602647304534912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树结构也可能涉及到专业的语法归纳程序。", "metrics": {"bleu_score": 42.02431608906194, "chrf_score": 35.90031041656428, "xcomet_score": 0.9756783246994019, "xcomet_qe_score": 0.9544914960861206, "metricx_score": 2.561282157897949, "metricx_qe_score": 2.710449695587158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,并引入一个神经序列到序列模型,直接建模输入片段和输出片段之间的对应关系。首次,", "metrics": {"bleu_score": 50.67169284661717, "chrf_score": 40.803024909712384, "xcomet_score": 0.6567879915237427, "xcomet_qe_score": 0.6853219270706177, "metricx_score": 4.716070175170898, "metricx_qe_score": 3.787825584411621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们展示了不依赖树结构而对更深层递归进行强泛化。", "metrics": {"bleu_score": 22.06914874716544, "chrf_score": 21.100897134623633, "xcomet_score": 0.7882514595985413, "xcomet_qe_score": 0.8023771047592163, "metricx_score": 3.377756118774414, "metricx_qe_score": 4.297835826873779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法从输入中预测输出,分为两个步骤。", "metrics": {"bleu_score": 39.12548001702372, "chrf_score": 36.02111593486263, "xcomet_score": 0.9914646148681641, "xcomet_qe_score": 0.9724985361099243, "metricx_score": 0.7392832636833191, "metricx_qe_score": 0.7785685658454895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入标记添加一个无序的多集标记,其中包含将出现在输出中的标记。", "metrics": {"bleu_score": 26.68519693710499, "chrf_score": 26.99681893785979, "xcomet_score": 0.9267834424972534, "xcomet_qe_score": 0.8809078931808472, "metricx_score": 3.0420072078704834, "metricx_qe_score": 2.9122447967529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个步骤之后,我们有了所有正确的标记,但它们没有顺序。", "metrics": {"bleu_score": 38.46629527341547, "chrf_score": 35.16045218106851, "xcomet_score": 0.8943989276885986, "xcomet_qe_score": 0.8182051181793213, "metricx_score": 3.0220513343811035, "metricx_qe_score": 3.545229434967041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在第二个步骤中,我们使用另一个模型来预测一个置换,将它们放入正确的顺序。", "metrics": {"bleu_score": 45.61980468404364, "chrf_score": 44.865437459189735, "xcomet_score": 0.9019025564193726, "xcomet_qe_score": 0.9144881367683411, "metricx_score": 3.8462274074554443, "metricx_qe_score": 3.7232236862182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新的方法来预测置换,对可能的置换不施加任何硬约束。", "metrics": {"bleu_score": 48.895554153297994, "chrf_score": 44.58613921543636, "xcomet_score": 0.8796349763870239, "xcomet_qe_score": 0.9027575254440308, "metricx_score": 2.716336250305176, "metricx_qe_score": 2.288215160369873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活和表达力强。", "metrics": {"bleu_score": 25.17447051131578, "chrf_score": 23.34238400213881, "xcomet_score": 0.9246615767478943, "xcomet_qe_score": 0.9061912894248962, "metricx_score": 0.8391990661621094, "metricx_qe_score": 1.3930160999298096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致如下工作。", "metrics": {"bleu_score": 27.15455510011743, "chrf_score": 24.80634794630479, "xcomet_score": 0.8453502655029297, "xcomet_qe_score": 0.7897835969924927, "metricx_score": 3.852965831756592, "metricx_qe_score": 3.096774101257324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,确定每个位置应放置的多集标记。", "metrics": {"bleu_score": 30.638349897132994, "chrf_score": 26.33230844201144, "xcomet_score": 0.8298568725585938, "xcomet_qe_score": 0.7604374885559082, "metricx_score": 1.847330093383789, "metricx_qe_score": 2.5500357151031494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们简单地选择一个如红色高亮显示的标记。然后", "metrics": {"bleu_score": 45.81248750640266, "chrf_score": 42.43431965681909, "xcomet_score": 0.8085826635360718, "xcomet_qe_score": 0.8490591645240784, "metricx_score": 2.0775959491729736, "metricx_qe_score": 1.382994532585144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们跳到下一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 48.84411467539946, "chrf_score": 42.68549950615216, "xcomet_score": 0.7266725897789001, "xcomet_qe_score": 0.7849007844924927, "metricx_score": 4.576729774475098, "metricx_qe_score": 3.919360876083374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个标记,跳到另一个多集标记。", "metrics": {"bleu_score": 55.50335224116993, "chrf_score": 51.41778857110887, "xcomet_score": 0.7756469249725342, "xcomet_qe_score": 0.7391543388366699, "metricx_score": 4.330874443054199, "metricx_qe_score": 3.528357744216919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程,直到访问完第一个阶段中的每个标记。", "metrics": {"bleu_score": 36.95217207741194, "chrf_score": 33.20486743821437, "xcomet_score": 0.8244295120239258, "xcomet_qe_score": 0.7162610292434692, "metricx_score": 2.0276312828063965, "metricx_qe_score": 3.549543619155884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了给您展示实验结果的预览,我们在这里将我们的方法与其他无树模型在Koggs基准上的比较。我们的模型在", "metrics": {"bleu_score": 41.12919949941094, "chrf_score": 34.93395424194791, "xcomet_score": 0.45827722549438477, "xcomet_qe_score": 0.4520670771598816, "metricx_score": 9.872787475585938, "metricx_qe_score": 5.0911688804626465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层递归的泛化方面以大优势超越其他模型。", "metrics": {"bleu_score": 37.270862548450175, "chrf_score": 33.52795777950928, "xcomet_score": 0.7973414063453674, "xcomet_qe_score": 0.9262604713439941, "metricx_score": 3.166149139404297, "metricx_qe_score": 3.698667287826538, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,其他一些结构泛化仍然非常具有挑战性。在", "metrics": {"bleu_score": 16.1692143534558, "chrf_score": 16.86896229323415, "xcomet_score": 0.8199589252471924, "xcomet_qe_score": 0.8505054116249084, "metricx_score": 4.6309661865234375, "metricx_qe_score": 1.1463468074798584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文中,我们解决了一些有趣的技术挑战。", "metrics": {"bleu_score": 72.45511487202049, "chrf_score": 74.6296829410079, "xcomet_score": 0.996840238571167, "xcomet_qe_score": 0.9859611988067627, "metricx_score": 0.49347591400146484, "metricx_qe_score": 0.5237241983413696, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对齐关系在训练数据中没有给出。", "metrics": {"bleu_score": 46.5504603150586, "chrf_score": 47.045472766072564, "xcomet_score": 0.9852180480957031, "xcomet_qe_score": 0.971396803855896, "metricx_score": 0.3834180235862732, "metricx_qe_score": 0.5115826725959778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集,这为训练带来了挑战。", "metrics": {"bleu_score": 60.37214684155263, "chrf_score": 52.79314933899764, "xcomet_score": 0.8374176025390625, "xcomet_qe_score": 0.7648310661315918, "metricx_score": 3.342092275619507, "metricx_qe_score": 3.5650787353515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时有多个与数据一致的置换,但语言上正确的置换是潜在的。", "metrics": {"bleu_score": 45.37864350402033, "chrf_score": 39.37909260656445, "xcomet_score": 0.8129457235336304, "xcomet_qe_score": 0.766880989074707, "metricx_score": 4.562626361846924, "metricx_qe_score": 4.0280046463012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练过程中归纳对齐关系来解决这个问题。", "metrics": {"bleu_score": 42.2938554232042, "chrf_score": 40.584273537006204, "xcomet_score": 0.8292210102081299, "xcomet_qe_score": 0.8164622783660889, "metricx_score": 3.638230562210083, "metricx_qe_score": 4.454317569732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但带来了找到最高得分置换是NP高的挑战。", "metrics": {"bleu_score": 23.868586744779027, "chrf_score": 22.568567185529325, "xcomet_score": 0.7494118213653564, "xcomet_qe_score": 0.7844685316085815, "metricx_score": 5.710908889770508, "metricx_qe_score": 5.6916890144348145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过一个GPU友好的连续放松来近似这个问题,这还允许我们反向传播解决方案并学习语言上更合理的置换。", "metrics": {"bleu_score": 37.39649997835487, "chrf_score": 35.46593000306053, "xcomet_score": 0.7212364077568054, "xcomet_qe_score": 0.5795039534568787, "metricx_score": 4.270488739013672, "metricx_qe_score": 4.7124223709106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验和我们如何解决这些挑战的信息,请阅读我们的论文或参观我们的展板。", "metrics": {"bleu_score": 59.135488103550955, "chrf_score": 51.62200439017719, "xcomet_score": 0.8701907396316528, "xcomet_qe_score": 0.961642861366272, "metricx_score": 0.6702340841293335, "metricx_qe_score": 0.3701830506324768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Aksheta,今天我的合著者Martin和我一起展示我们的作品《Kitmasteff:来自多个来源的知识集成评估》。这项", "metrics": {"bleu_score": 38.36470559988296, "chrf_score": 38.39616659871332, "xcomet_score": 0.44173261523246765, "xcomet_qe_score": 0.5389112830162048, "metricx_score": 8.892951011657715, "metricx_qe_score": 7.08790922164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究之间的合作。", "metrics": {"bleu_score": 54.33878021899394, "chrf_score": 51.6571013491624, "xcomet_score": 0.8065143823623657, "xcomet_qe_score": 0.7479180097579956, "metricx_score": 4.606540679931641, "metricx_qe_score": 4.746415138244629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用各种知识来源,例如包含在它们参数中的知识,通常通过预训练获得,以及推理时提供的输入知识。近", "metrics": {"bleu_score": 37.92569410134162, "chrf_score": 34.84065976376466, "xcomet_score": 0.634487509727478, "xcomet_qe_score": 0.6541202068328857, "metricx_score": 7.267574787139893, "metricx_qe_score": 4.584840774536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期在问答等任务中,模型可以利用预训练时的知识来解决任务。", "metrics": {"bleu_score": 54.823129196366416, "chrf_score": 48.245090674994316, "xcomet_score": 0.7889201641082764, "xcomet_qe_score": 0.7549189925193787, "metricx_score": 4.061760425567627, "metricx_qe_score": 4.9908270835876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常还需要在推理时提供的知识。", "metrics": {"bleu_score": 79.632051309738, "chrf_score": 73.71916932929182, "xcomet_score": 0.9268085956573486, "xcomet_qe_score": 0.8482565879821777, "metricx_score": 0.8107254505157471, "metricx_qe_score": 0.8271344304084778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“约翰在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 35.680135454109546, "chrf_score": 21.22575172195696, "xcomet_score": 0.9860310554504395, "xcomet_qe_score": 0.9782688617706299, "metricx_score": 1.3564823865890503, "metricx_qe_score": 1.9369547367095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可能包含关于总统的工作和电视是什么的信息,但它们无法可靠地知道这个特定实例实体约翰是谁,或者新总统是谁,因为总统可能在预训练后已经改变了。", "metrics": {"bleu_score": 52.27494144990846, "chrf_score": 44.46407189065062, "xcomet_score": 0.7281612157821655, "xcomet_qe_score": 0.7498699426651001, "metricx_score": 3.404559373855591, "metricx_qe_score": 4.563657283782959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型NLU任务的模型需要能够集成和利用预训练时和推理时的知识。", "metrics": {"bleu_score": 45.22698978542689, "chrf_score": 42.18000072004964, "xcomet_score": 0.9666992425918579, "xcomet_qe_score": 0.9342342019081116, "metricx_score": 1.059158205986023, "metricx_qe_score": 1.623881220817566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一个知识集成诊断测试套件。", "metrics": {"bleu_score": 47.87725471995895, "chrf_score": 41.00756274635133, "xcomet_score": 0.9976264238357544, "xcomet_qe_score": 0.9938678741455078, "metricx_score": 1.099318504333496, "metricx_qe_score": 1.3144623041152954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个代词指代解析任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 44.68853651170298, "chrf_score": 36.53071363965413, "xcomet_score": 0.8631367683410645, "xcomet_qe_score": 0.8620332479476929, "metricx_score": 1.755744457244873, "metricx_qe_score": 1.5955910682678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的代词指代解析模型对数据集进行评估。", "metrics": {"bleu_score": 55.9633460897532, "chrf_score": 57.75928790384318, "xcomet_score": 0.8556610345840454, "xcomet_qe_score": 0.8826817870140076, "metricx_score": 2.4496114253997803, "metricx_qe_score": 2.7355525493621826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子:", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 65.46493657106939, "xcomet_score": 0.9724688529968262, "xcomet_qe_score": 0.9228176474571228, "metricx_score": 0.3413010239601135, "metricx_qe_score": 1.0924984216690063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin是一个法官,", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 60.745550745550744, "xcomet_score": 0.9165078401565552, "xcomet_qe_score": 0.9224140644073486, "metricx_score": 0.8332419395446777, "metricx_qe_score": 1.5593316555023193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Kia是一个面包师。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 31.223544973544975, "xcomet_score": 0.8567985892295837, "xcomet_qe_score": 0.8417304754257202, "metricx_score": 0.4488016664981842, "metricx_qe_score": 1.473280429840088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Termin和Kia在公园里相遇。在工作了一整天", "metrics": {"bleu_score": 14.879641171245488, "chrf_score": 28.97481101381637, "xcomet_score": 0.16180671751499176, "xcomet_qe_score": 0.1581667810678482, "metricx_score": 9.686490058898926, "metricx_qe_score": 9.953398704528809, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上判决案件后,他很高兴放松一下。", "metrics": {"bleu_score": 41.07084441187983, "chrf_score": 32.43526178885868, "xcomet_score": 0.9024745225906372, "xcomet_qe_score": 0.8461767435073853, "metricx_score": 2.6458051204681396, "metricx_qe_score": 4.022118091583252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是识别代词“他”指代正确的实体,在这个例子中是Servin。", "metrics": {"bleu_score": 28.165888134545607, "chrf_score": 34.65372654820721, "xcomet_score": 0.8819066286087036, "xcomet_qe_score": 0.7614814043045044, "metricx_score": 1.4859369993209839, "metricx_qe_score": 2.39821457862854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解析给定代词需要两种类型的信息:", "metrics": {"bleu_score": 13.384558916440131, "chrf_score": 14.274121679295316, "xcomet_score": 0.9938896894454956, "xcomet_qe_score": 0.9543671607971191, "metricx_score": 0.5270317792892456, "metricx_qe_score": 0.8342856168746948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是实体特定知识,例如Servin是一个法官;其", "metrics": {"bleu_score": 15.940384824643747, "chrf_score": 33.90675596522638, "xcomet_score": 0.6756405234336853, "xcomet_qe_score": 0.6434290409088135, "metricx_score": 5.949036121368408, "metricx_qe_score": 2.7735073566436768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次是背景知识,大型", "metrics": {"bleu_score": 8.883746495165141, "chrf_score": 16.839452595478654, "xcomet_score": 0.31094253063201904, "xcomet_qe_score": 0.4938863217830658, "metricx_score": 20.56713104248047, "metricx_qe_score": 11.562318801879883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型在预训练期间学习,而实体特定知识通常在推理时观察到。", "metrics": {"bleu_score": 24.825844614749798, "chrf_score": 25.09454372978733, "xcomet_score": 0.8181017637252808, "xcomet_qe_score": 0.7468598484992981, "metricx_score": 5.521758556365967, "metricx_qe_score": 4.286527633666992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们变化了这些两个信息的可用性,使其可能在单一来源或多个来源中找到。", "metrics": {"bleu_score": 59.547349919457254, "chrf_score": 55.93157253493944, "xcomet_score": 0.8513447046279907, "xcomet_qe_score": 0.7693629264831543, "metricx_score": 1.3212382793426514, "metricx_qe_score": 1.490578055381775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了Kitmos的三种设置:", "metrics": {"bleu_score": 63.40466277046863, "chrf_score": 39.12451519069167, "xcomet_score": 0.8955861926078796, "xcomet_qe_score": 0.8903961181640625, "metricx_score": 0.7382023334503174, "metricx_qe_score": 0.8523596525192261, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先是主题预训练设置", "metrics": {"bleu_score": 1.1817862041636082, "chrf_score": 6.463596998634805, "xcomet_score": 0.28157129883766174, "xcomet_qe_score": 0.20396094024181366, "metricx_score": 3.8017802238464355, "metricx_qe_score": 6.033647060394287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",假设背景知识在预训练时可用;", "metrics": {"bleu_score": 16.07951625276308, "chrf_score": 24.710230177310393, "xcomet_score": 0.4293191432952881, "xcomet_qe_score": 0.3942665457725525, "metricx_score": 14.517428398132324, "metricx_qe_score": 12.850805282592773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次是背景两者设置,其中两种知识类型仅在推理时可用。", "metrics": {"bleu_score": 57.420049943398745, "chrf_score": 55.47834187392884, "xcomet_score": 0.7423003911972046, "xcomet_qe_score": 0.687336802482605, "metricx_score": 3.365004301071167, "metricx_qe_score": 3.8580715656280518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置特别有趣,因为它模拟了一种情况,即解决任务所需的背景知识不是模型预训练数据的一部分,例如", "metrics": {"bleu_score": 69.17402035633937, "chrf_score": 70.10029661318701, "xcomet_score": 0.8707288503646851, "xcomet_qe_score": 0.8532920479774475, "metricx_score": 1.459965705871582, "metricx_qe_score": 0.7326884269714355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 83.13427988970655, "chrf_score": 83.66823546839467, "xcomet_score": 0.8476712703704834, "xcomet_qe_score": 0.8093159198760986, "metricx_score": 4.505660057067871, "metricx_qe_score": 5.475840091705322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们如何控制两个来源中事实的可用性:", "metrics": {"bleu_score": 48.75099426512705, "chrf_score": 45.18001102159121, "xcomet_score": 0.8307551145553589, "xcomet_qe_score": 0.7447293996810913, "metricx_score": 2.2664952278137207, "metricx_qe_score": 3.2224183082580566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中。在背景上下文中,我们提供特定的反事实知识“Chichester是一个政治家”。", "metrics": {"bleu_score": 43.18506273591324, "chrf_score": 42.96043590032219, "xcomet_score": 0.6113157272338867, "xcomet_qe_score": 0.573502779006958, "metricx_score": 4.586569309234619, "metricx_qe_score": 6.006375312805176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景两者设置中,我们不仅提供特定的反事实知识,还提供关于政治家在推理类型上下文中的背景知识。", "metrics": {"bleu_score": 39.13362743244693, "chrf_score": 33.34770119754912, "xcomet_score": 0.564336895942688, "xcomet_qe_score": 0.5032041072845459, "metricx_score": 4.903997421264648, "metricx_qe_score": 4.834309101104736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供虚构的职业“镜像旅游”而不是“政治家”,因为镜像旅游不太可能包含在预训练参数中。", "metrics": {"bleu_score": 46.73969952814037, "chrf_score": 32.28994660647189, "xcomet_score": 0.6141258478164673, "xcomet_qe_score": 0.5632531642913818, "metricx_score": 7.990436553955078, "metricx_qe_score": 8.934148788452148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的代词指代解析模型对数据集进行评估。", "metrics": {"bleu_score": 55.9633460897532, "chrf_score": 57.75928790384318, "xcomet_score": 0.8547189235687256, "xcomet_qe_score": 0.8505066633224487, "metricx_score": 2.4736874103546143, "metricx_qe_score": 2.6628363132476807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了最难的变体背景预训练设置中表现最佳的模型结果。", "metrics": {"bleu_score": 36.99684094025193, "chrf_score": 33.24134563088591, "xcomet_score": 0.8317047357559204, "xcomet_qe_score": 0.7290839552879333, "metricx_score": 1.7229390144348145, "metricx_qe_score": 1.3542050123214722, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有针对KitMus进行任务特定训练的情况下,两个模型都没有表现良好。", "metrics": {"bleu_score": 19.965747160974658, "chrf_score": 19.740652797449613, "xcomet_score": 0.9305227994918823, "xcomet_qe_score": 0.9380269050598145, "metricx_score": 1.0809084177017212, "metricx_qe_score": 1.139500379562378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在KitMus上训练时,C2F和Berth for Koref两个模型都比随机选择表现显著更好。", "metrics": {"bleu_score": 8.68981018458863, "chrf_score": 19.13261413866339, "xcomet_score": 0.7408003807067871, "xcomet_qe_score": 0.7569769620895386, "metricx_score": 4.187822341918945, "metricx_qe_score": 4.634091377258301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在一般代词指代解析数据集上训练时,模型学会利用表面线索,而在测试KitMus时,这些线索已被移除,因此无效。额外的", "metrics": {"bleu_score": 23.903940907223397, "chrf_score": 20.090053571727047, "xcomet_score": 0.5091913938522339, "xcomet_qe_score": 0.4492737948894501, "metricx_score": 8.026446342468262, "metricx_qe_score": 5.517772197723389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虚构知识实验表明,即使是表现最佳的模型也无法可靠地集成仅在推理时提供的背景知识。", "metrics": {"bleu_score": 49.43475148566185, "chrf_score": 41.25630663840299, "xcomet_score": 0.887397289276123, "xcomet_qe_score": 0.8794727921485901, "metricx_score": 1.6816575527191162, "metricx_qe_score": 1.4200884103775024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,许多代词指代解析模型似乎无法在没有任务特定训练的情况下推理来自不同来源的知识。", "metrics": {"bleu_score": 43.9531914397423, "chrf_score": 40.086866910403614, "xcomet_score": 0.8399940729141235, "xcomet_qe_score": 0.8361402750015259, "metricx_score": 2.5636379718780518, "metricx_qe_score": 3.0223288536071777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在任务特定训练下,一些模型成功地集成来自多个来源的知识。", "metrics": {"bleu_score": 51.34269585764103, "chrf_score": 45.5484955483923, "xcomet_score": 0.9227802753448486, "xcomet_qe_score": 0.9290421009063721, "metricx_score": 1.0243072509765625, "metricx_qe_score": 1.444474458694458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最佳的模型也似乎在可靠集成仅在推理时呈现的背景知识方面存在困难。", "metrics": {"bleu_score": 41.46086296807236, "chrf_score": 36.733483451318335, "xcomet_score": 0.9039344787597656, "xcomet_qe_score": 0.853701114654541, "metricx_score": 1.3726332187652588, "metricx_qe_score": 1.3090226650238037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多细节,请参阅我们的论文并在GitHub上查看代码数据集。", "metrics": {"bleu_score": 54.34718760946048, "chrf_score": 53.88843629931757, "xcomet_score": 0.9518081545829773, "xcomet_qe_score": 0.9615674018859863, "metricx_score": 0.5582631826400757, "metricx_qe_score": 0.4503977298736572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Myra,今天我将讨论我们的一篇论文《标记人设:使用自然语言提示测量语言模型中的刻板印象》。", "metrics": {"bleu_score": 52.22884651550685, "chrf_score": 47.72820533808837, "xcomet_score": 0.7775211334228516, "xcomet_qe_score": 0.7547595500946045, "metricx_score": 3.3337066173553467, "metricx_qe_score": 3.387946367263794, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Essendermouch和Dangerowski合作完成的。", "metrics": {"bleu_score": 30.119166060089718, "chrf_score": 24.27385268092256, "xcomet_score": 0.7394931316375732, "xcomet_qe_score": 0.7435511946678162, "metricx_score": 7.163878917694092, "metricx_qe_score": 7.630449295043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 41.222852304408335, "chrf_score": 41.171768238179595, "xcomet_score": 0.9893604516983032, "xcomet_qe_score": 0.9880325794219971, "metricx_score": 1.968329906463623, "metricx_qe_score": 4.322904109954834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些测量方法存在各种局限性。", "metrics": {"bleu_score": 21.409092659758045, "chrf_score": 19.466619981325863, "xcomet_score": 0.9980359077453613, "xcomet_qe_score": 1.0, "metricx_score": 2.1439764499664307, "metricx_qe_score": 1.0230947732925415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,需要大量时间来整理。而且通常只测量非常具体的刻板印象,这意味着它们不能很好地推广到其他人口统计或背景,或者它们只是捕捉到非常普遍、广泛的关联,如与特定群体相关的负面关联。", "metrics": {"bleu_score": 47.191139207764444, "chrf_score": 40.168439785141324, "xcomet_score": 0.6953368186950684, "xcomet_qe_score": 0.6148396730422974, "metricx_score": 5.11971378326416, "metricx_qe_score": 5.318172454833984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这方面的大多数工作都没有考虑到交集性,即多层次社会身份可以加剧偏见,并成为独特的伤害焦点。", "metrics": {"bleu_score": 50.11893046413793, "chrf_score": 41.74671783327458, "xcomet_score": 0.6590776443481445, "xcomet_qe_score": 0.6887839436531067, "metricx_score": 3.205697536468506, "metricx_qe_score": 3.4227123260498047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们利用了这些新指令调优LLM的一个特性,即它们非常擅长响应指令和提示。", "metrics": {"bleu_score": 48.542087540661115, "chrf_score": 40.91389887884154, "xcomet_score": 0.8751193881034851, "xcomet_qe_score": 0.8364112377166748, "metricx_score": 4.682761192321777, "metricx_qe_score": 5.516541004180908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个人设,这是一个通过提示(例如,想象你是一个亚洲女性,", "metrics": {"bleu_score": 32.946031771595095, "chrf_score": 33.43727574273836, "xcomet_score": 0.8264554738998413, "xcomet_qe_score": 0.8032779097557068, "metricx_score": 4.860404014587402, "metricx_qe_score": 5.426692008972168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述自己)来描绘想象中个体的描述。", "metrics": {"bleu_score": 6.839596061560946, "chrf_score": 9.807295251204403, "xcomet_score": 0.15375718474388123, "xcomet_qe_score": 0.16227643191814423, "metricx_score": 4.642355442047119, "metricx_qe_score": 4.9474592208862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这非常适用于任何人口统计,因为我们可以在提示中指定任何我们想要的身份标记。", "metrics": {"bleu_score": 56.21399168527829, "chrf_score": 52.66189133190608, "xcomet_score": 0.9057890176773071, "xcomet_qe_score": 0.8516210317611694, "metricx_score": 2.230741500854492, "metricx_qe_score": 2.7204084396362305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是GPT 4的一些生成示例。我们可以立", "metrics": {"bleu_score": 19.91417544687795, "chrf_score": 39.82980669514415, "xcomet_score": 0.7564431428909302, "xcomet_qe_score": 0.7140907049179077, "metricx_score": 6.3369460105896, "metricx_qe_score": 3.604750394821167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即看到,虽然输出不是传统意义上的过度负面或有毒,但有一些有趣的模式。", "metrics": {"bleu_score": 44.38580955656987, "chrf_score": 36.98112334317203, "xcomet_score": 0.6926101446151733, "xcomet_qe_score": 0.7355548739433289, "metricx_score": 3.482736587524414, "metricx_qe_score": 4.248393535614014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘为不显眼,中东女性被描述为异国情调,令人着迷的地区,", "metrics": {"bleu_score": 18.45344935159547, "chrf_score": 17.647396783627457, "xcomet_score": 0.7412755489349365, "xcomet_qe_score": 0.7486796379089355, "metricx_score": 6.67425012588501, "metricx_qe_score": 6.421137809753418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而两种有色人种女性的人设都提到了祖先,而白人男性的人设则没有。", "metrics": {"bleu_score": 29.58559123065251, "chrf_score": 25.280362205355832, "xcomet_score": 0.9700846672058105, "xcomet_qe_score": 0.9814822673797607, "metricx_score": 1.6308563947677612, "metricx_qe_score": 1.5854047536849976, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两部分。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9945436716079712, "xcomet_qe_score": 0.9767298698425293, "metricx_score": 0.19123207032680511, "metricx_qe_score": 0.2513856589794159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人设。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 68.72835497835497, "xcomet_score": 0.9126062989234924, "xcomet_qe_score": 0.8202393054962158, "metricx_score": 0.5258585214614868, "metricx_qe_score": 0.7669497132301331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人设的提示来自一项研究,该研究向人类受试者提供了这些提示,发现通过这种方式也能揭示种族刻板印象。", "metrics": {"bleu_score": 46.26897608790973, "chrf_score": 43.65363472830832, "xcomet_score": 0.8178684711456299, "xcomet_qe_score": 0.8177947998046875, "metricx_score": 2.1346235275268555, "metricx_qe_score": 2.423234701156616, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这还使我们能够直接比较生成的人物和人类撰写的响应。", "metrics": {"bleu_score": 26.583837834692577, "chrf_score": 23.66050463834598, "xcomet_score": 0.7602440714836121, "xcomet_qe_score": 0.7362933158874512, "metricx_score": 2.247971534729004, "metricx_qe_score": 4.065406322479248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种方法,用于识别区分标记组和未标记组的词语,我稍后会详细说明。", "metrics": {"bleu_score": 32.045315809513816, "chrf_score": 26.833615713000132, "xcomet_score": 0.8955102562904358, "xcomet_qe_score": 0.9533804655075073, "metricx_score": 0.9427624940872192, "metricx_qe_score": 1.1002246141433716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点是,我们可以获得非常具体的刻板印象和模式,而无需依赖任何特定的词典。", "metrics": {"bleu_score": 47.05620506141554, "chrf_score": 45.092846407718426, "xcomet_score": 0.8992884159088135, "xcomet_qe_score": 0.814476490020752, "metricx_score": 1.0002810955047607, "metricx_qe_score": 0.9935299158096313, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,标记词方法借鉴了社会语言学中的标记概念,即存在一个未标记的默认值,任何与该默认值不同的群体在语言上都被标记。", "metrics": {"bleu_score": 39.16339250132286, "chrf_score": 34.410826061371544, "xcomet_score": 0.7447620034217834, "xcomet_qe_score": 0.815819263458252, "metricx_score": 1.3320252895355225, "metricx_qe_score": 1.2121405601501465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,词语“战士”通常与男性相关联。", "metrics": {"bleu_score": 63.70555116759558, "chrf_score": 54.81288455553162, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.5052863955497742, "metricx_qe_score": 0.7244619131088257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,当人们描述一个女性战士时,他们通常会实际指定一个男性战士,并在术语中标记为“女性”。", "metrics": {"bleu_score": 43.99590706380396, "chrf_score": 39.04238224170655, "xcomet_score": 0.7372280359268188, "xcomet_qe_score": 0.7192288637161255, "metricx_score": 6.064572811126709, "metricx_qe_score": 6.425344944000244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是未标记的,而边缘化群体通常被标记。", "metrics": {"bleu_score": 54.435031864387746, "chrf_score": 49.52182959793433, "xcomet_score": 0.8007785081863403, "xcomet_qe_score": 0.7883853912353516, "metricx_score": 1.342667579650879, "metricx_qe_score": 1.67310631275177, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定什么是未标记组和标记组。然后,我们使用战斗词方法比较人设,这基本上是使用加权对数奇数比来区分每个标记组的前列词。", "metrics": {"bleu_score": 42.647775735578435, "chrf_score": 37.26567006176978, "xcomet_score": 0.5124291181564331, "xcomet_qe_score": 0.5075191259384155, "metricx_score": 5.66417121887207, "metricx_qe_score": 6.159600257873535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的人设,我们将使用战斗词方法,并将对数奇数比与白人人设和男性人设进行比较,因为它们是相应的未标记组。", "metrics": {"bleu_score": 32.92155919524032, "chrf_score": 27.74892491503503, "xcomet_score": 0.5193493962287903, "xcomet_qe_score": 0.4564868211746216, "metricx_score": 5.333291530609131, "metricx_qe_score": 6.61334228515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看一些结果。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 37.07384040111085, "xcomet_score": 0.9672292470932007, "xcomet_qe_score": 0.9580326080322266, "metricx_score": 0.40737825632095337, "metricx_qe_score": 0.6341195106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个刻板印象词典,发现生成的人物包含的刻板印象词比人类撰写的人物多得多。", "metrics": {"bleu_score": 34.0684853269849, "chrf_score": 29.69943571090767, "xcomet_score": 0.9502995014190674, "xcomet_qe_score": 0.7911648154258728, "metricx_score": 2.6576011180877686, "metricx_qe_score": 3.455106258392334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词典中词语的分布时,发现情况截然不同。", "metrics": {"bleu_score": 40.1919788252024, "chrf_score": 34.567778664078894, "xcomet_score": 0.9560835361480713, "xcomet_qe_score": 0.9582574367523193, "metricx_score": 0.6199532747268677, "metricx_qe_score": 0.7620723247528076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的人物中词典词的出现率更高,但人类撰写的人物中词语的分布更广泛,而刻板印象词仅为“高”和“健壮”。", "metrics": {"bleu_score": 12.142591193355852, "chrf_score": 11.795377304378487, "xcomet_score": 0.6080118417739868, "xcomet_qe_score": 0.6307395100593567, "metricx_score": 3.555455207824707, "metricx_qe_score": 3.8798179626464844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以实际上只有积极的或至少不是负面的词语。", "metrics": {"bleu_score": 36.936948294077965, "chrf_score": 31.714579539110115, "xcomet_score": 0.9601958990097046, "xcomet_qe_score": 0.8246676325798035, "metricx_score": 0.5956277251243591, "metricx_qe_score": 0.9856166839599609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词典根本没有很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 80.33331639285797, "chrf_score": 77.12483703195777, "xcomet_score": 0.8895043134689331, "xcomet_qe_score": 0.7210332155227661, "metricx_score": 1.007191777229309, "metricx_qe_score": 1.14162015914917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,相反,我们将转向标记词方法的结果,以展示这些看似积极的词语如何促进刻板印象和本质化叙事。", "metrics": {"bleu_score": 32.940289684907334, "chrf_score": 26.927847273618845, "xcomet_score": 0.7276772856712341, "xcomet_qe_score": 0.7468636631965637, "metricx_score": 2.0544590950012207, "metricx_qe_score": 2.180044174194336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描述如何反映有害模式。", "metrics": {"bleu_score": 57.97816651083259, "chrf_score": 48.947701202593656, "xcomet_score": 0.8955768346786499, "xcomet_qe_score": 0.8617904186248779, "metricx_score": 1.2634978294372559, "metricx_qe_score": 2.026130437850952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记组,前列词包括文化、传统、自豪和异国情调等词语。", "metrics": {"bleu_score": 4.470198183413415, "chrf_score": 5.1885392434048665, "xcomet_score": 0.6928716897964478, "xcomet_qe_score": 0.7129626274108887, "metricx_score": 4.959994316101074, "metricx_qe_score": 4.355784893035889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅根据其与身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 53.74285674122068, "chrf_score": 47.98955769891128, "xcomet_score": 0.941245436668396, "xcomet_qe_score": 0.9799939393997192, "metricx_score": 0.9812318682670593, "metricx_qe_score": 1.3059498071670532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体带来了长期的歧视和其他化历史。", "metrics": {"bleu_score": 18.606723623672913, "chrf_score": 18.231683640454204, "xcomet_score": 0.8458240032196045, "xcomet_qe_score": 0.8548082113265991, "metricx_score": 3.711991786956787, "metricx_qe_score": 3.697072982788086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多常见的套路,尤其是对于有色女性。", "metrics": {"bleu_score": 18.889334780177865, "chrf_score": 20.610020150820382, "xcomet_score": 0.771567702293396, "xcomet_qe_score": 0.8365539312362671, "metricx_score": 3.459984064102173, "metricx_qe_score": 1.5998425483703613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉美女性的词语包括充满活力和曲线玲珑,这与热带主义套路相关。", "metrics": {"bleu_score": 23.766969292435522, "chrf_score": 17.626430222119623, "xcomet_score": 0.8290503025054932, "xcomet_qe_score": 0.8202657699584961, "metricx_score": 4.43931245803833, "metricx_qe_score": 4.028472423553467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,词语是娇小、精致和丝滑,这与亚洲女性被性化、被视为温顺和顺从的历史相关。", "metrics": {"bleu_score": 14.633087009162027, "chrf_score": 14.28360443273231, "xcomet_score": 0.7555601596832275, "xcomet_qe_score": 0.816962718963623, "metricx_score": 3.0701515674591064, "metricx_qe_score": 2.6312239170074463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些前列词是坚强和坚韧。", "metrics": {"bleu_score": 27.554609172184332, "chrf_score": 20.18168212848438, "xcomet_score": 0.8132567405700684, "xcomet_qe_score": 0.7574684023857117, "metricx_score": 2.3142242431640625, "metricx_qe_score": 2.703171491622925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所说的黑人女性强人形象相关。", "metrics": {"bleu_score": 37.43119524761466, "chrf_score": 32.83358556140431, "xcomet_score": 0.910491943359375, "xcomet_qe_score": 0.8980926275253296, "metricx_score": 1.40139901638031, "metricx_qe_score": 1.5990095138549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍看上去很积极,但研究表明,这种形象实际上非常有害,因为它给这些人口统计群体带来了巨大的压力,要求他们在面对社会障碍时坚强和坚韧。", "metrics": {"bleu_score": 32.87253174610844, "chrf_score": 27.06541605351459, "xcomet_score": 0.7618078589439392, "xcomet_qe_score": 0.7844102382659912, "metricx_score": 4.885268211364746, "metricx_qe_score": 5.025732517242432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,而不是真正致力于改变这些障碍,它给这些人施加了压力,要求他们克服这些障碍,这导致这些人出现非常负面的健康结果,以及其他伤害。", "metrics": {"bleu_score": 42.48610972752958, "chrf_score": 35.57789496435211, "xcomet_score": 0.9437172412872314, "xcomet_qe_score": 0.9438385963439941, "metricx_score": 2.73624324798584, "metricx_qe_score": 2.019711971282959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记组的词语几乎完全反映了本质化叙事。", "metrics": {"bleu_score": 34.573012516697766, "chrf_score": 30.310285111967705, "xcomet_score": 0.8525809049606323, "xcomet_qe_score": 0.8123765587806702, "metricx_score": 1.8832789659500122, "metricx_qe_score": 2.2037618160247803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们为模型所有者提出三点建议。", "metrics": {"bleu_score": 58.17070222427868, "chrf_score": 52.33470142531367, "xcomet_score": 0.8770531415939331, "xcomet_qe_score": 0.7754772901535034, "metricx_score": 1.2281999588012695, "metricx_qe_score": 3.2828125953674316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该解决积极刻板印象和本质化叙事。", "metrics": {"bleu_score": 42.51298835731235, "chrf_score": 36.07587532438494, "xcomet_score": 0.8390697240829468, "xcomet_qe_score": 0.8399777412414551, "metricx_score": 1.5034797191619873, "metricx_qe_score": 1.8790102005004883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交集性视角来研究偏见和伤害,因为如果不这样做,可能会忽略许多事情。", "metrics": {"bleu_score": 63.61006774526769, "chrf_score": 52.52304339587168, "xcomet_score": 0.8745509386062622, "xcomet_qe_score": 0.7962907552719116, "metricx_score": 0.9890516400337219, "metricx_qe_score": 0.8234898447990417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该提高偏见缓解方法的透明度,因为例如,像这些积极刻板印象这样,我们不知道它是因为某种奇怪的、过度价值观对齐正在发生,或者可能是其他反刻板印象方法导致这些恶性的模式。", "metrics": {"bleu_score": 49.721639282123085, "chrf_score": 41.31346617506947, "xcomet_score": 0.6953052282333374, "xcomet_qe_score": 0.66740882396698, "metricx_score": 3.461022138595581, "metricx_qe_score": 4.018068313598633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果没有更多的透明度,我们真的不能做出任何假设或进一步研究。", "metrics": {"bleu_score": 58.39328604633569, "chrf_score": 50.56097424028933, "xcomet_score": 0.9965721368789673, "xcomet_qe_score": 0.9871931076049805, "metricx_score": 0.40215981006622314, "metricx_qe_score": 0.46067821979522705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 25.690595421698053, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.27894243597984314, "metricx_qe_score": 0.5952762365341187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在ACL中玩得开心。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 20.436507936507933, "xcomet_score": 0.9099595546722412, "xcomet_qe_score": 0.8662598729133606, "metricx_score": 1.8248451948165894, "metricx_qe_score": 2.948782205581665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的魏静。", "metrics": {"bleu_score": 43.59493824807389, "chrf_score": 30.396561875199023, "xcomet_score": 0.8636117577552795, "xcomet_qe_score": 0.8830795884132385, "metricx_score": 1.1834230422973633, "metricx_qe_score": 1.6919268369674683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很荣幸能为大家带来我们论文《你正在复制我的模型吗?——", "metrics": {"bleu_score": 4.668049023095242, "chrf_score": 7.31226961558715, "xcomet_score": 0.3059666156768799, "xcomet_qe_score": 0.3280736207962036, "metricx_score": 6.700745582580566, "metricx_qe_score": 6.320038318634033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大型语言模型嵌入与服务版权保护", "metrics": {"bleu_score": 5.255923420816887, "chrf_score": 4.368200288373313, "xcomet_score": 0.2614274024963379, "xcomet_qe_score": 0.15127147734165192, "metricx_score": 3.6824848651885986, "metricx_qe_score": 3.581442356109619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的回门水印》的简短宣传视频。 首先,让", "metrics": {"bleu_score": 5.766435048168568, "chrf_score": 6.064200975342385, "xcomet_score": 0.1453130841255188, "xcomet_qe_score": 0.13709013164043427, "metricx_score": 21.23615837097168, "metricx_qe_score": 20.753005981445312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍一下嵌入与服务相关的背景。", "metrics": {"bleu_score": 46.15425015629848, "chrf_score": 40.770889785248805, "xcomet_score": 0.8563066720962524, "xcomet_qe_score": 0.8336208462715149, "metricx_score": 0.7785488367080688, "metricx_qe_score": 1.0176173448562622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、Lama、PELM这样的大型语言模型在自然语言理解和生成方面表现卓越。", "metrics": {"bleu_score": 54.88118661840184, "chrf_score": 51.48289150658639, "xcomet_score": 0.8908720016479492, "xcomet_qe_score": 0.9006970524787903, "metricx_score": 1.962130069732666, "metricx_qe_score": 1.9283488988876343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入与服务是建立在大型语言模型基础上的服务之一,用于协助各种NLP任务。", "metrics": {"bleu_score": 49.92724634071977, "chrf_score": 51.337862185661876, "xcomet_score": 0.8133723735809326, "xcomet_qe_score": 0.7986379861831665, "metricx_score": 2.21500825881958, "metricx_qe_score": 2.1923482418060303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供了一个基于GPT的嵌入API。", "metrics": {"bleu_score": 84.82198619370465, "chrf_score": 89.11471499514977, "xcomet_score": 0.9668201804161072, "xcomet_qe_score": 0.9617375135421753, "metricx_score": 0.41991662979125977, "metricx_qe_score": 0.5524056553840637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 62.159854743235485, "chrf_score": 52.995325818898486, "xcomet_score": 0.8757243752479553, "xcomet_qe_score": 0.8761061429977417, "metricx_score": 2.4121768474578857, "metricx_qe_score": 2.8020410537719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入作为服务的版权是非常必要的。", "metrics": {"bleu_score": 33.86854985606571, "chrf_score": 31.89896639142899, "xcomet_score": 0.9359670877456665, "xcomet_qe_score": 0.9401069283485413, "metricx_score": 0.7452258467674255, "metricx_qe_score": 1.03764808177948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入作为服务的版权,一种解决方案是在提供服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 73.8533448762713, "chrf_score": 65.56372055043333, "xcomet_score": 0.8309177160263062, "xcomet_qe_score": 0.8587088584899902, "metricx_score": 1.875984787940979, "metricx_qe_score": 2.2161378860473633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法需要满足以下属性:", "metrics": {"bleu_score": 73.61703354503862, "chrf_score": 70.6361693861694, "xcomet_score": 0.9808683395385742, "xcomet_qe_score": 0.9769817590713501, "metricx_score": 0.9086166620254517, "metricx_qe_score": 1.8020247220993042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,方法应适用于嵌入作为服务;", "metrics": {"bleu_score": 52.1873921269267, "chrf_score": 46.15412365412366, "xcomet_score": 0.9010077714920044, "xcomet_qe_score": 0.8932586908340454, "metricx_score": 1.4691026210784912, "metricx_qe_score": 2.1323564052581787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的实用性;", "metrics": {"bleu_score": 64.59962562244407, "chrf_score": 63.74920574037586, "xcomet_score": 0.9388446807861328, "xcomet_qe_score": 0.9019606113433838, "metricx_score": 1.092645525932312, "metricx_qe_score": 2.0250072479248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者应足够隐蔽,或者攻击者无法轻易去除水印;", "metrics": {"bleu_score": 24.083578977683647, "chrf_score": 22.25152349891565, "xcomet_score": 0.9828777313232422, "xcomet_qe_score": 0.9856840372085571, "metricx_score": 1.0563300848007202, "metricx_qe_score": 4.123373985290527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中转移到攻击者的服务中。", "metrics": {"bleu_score": 67.25208708180976, "chrf_score": 59.21045811816169, "xcomet_score": 0.9747414588928223, "xcomet_qe_score": 0.8920719623565674, "metricx_score": 1.2410730123519897, "metricx_qe_score": 2.2664222717285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有研究可以大致分为四类。", "metrics": {"bleu_score": 55.70189840697072, "chrf_score": 52.822324466177726, "xcomet_score": 0.9457129240036011, "xcomet_qe_score": 1.0, "metricx_score": 1.005700707435608, "metricx_qe_score": 0.0879150778055191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入作为服务,要么缺乏可转移性。", "metrics": {"bleu_score": 59.24450913674052, "chrf_score": 53.234672684742144, "xcomet_score": 0.9320278167724609, "xcomet_qe_score": 0.9317530393600464, "metricx_score": 2.3404347896575928, "metricx_qe_score": 2.5775275230407715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们在本文中提出了一种嵌入标记方法,这是一种基于回门的水印方法,适用于嵌入作为服务。", "metrics": {"bleu_score": 43.32583108776673, "chrf_score": 37.1107725531629, "xcomet_score": 0.8297070264816284, "xcomet_qe_score": 0.7957501411437988, "metricx_score": 3.6796135902404785, "metricx_qe_score": 3.893862724304199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,让我介绍我们嵌入标记的细节。", "metrics": {"bleu_score": 20.89531111668325, "chrf_score": 21.05936819172113, "xcomet_score": 0.9916126728057861, "xcomet_qe_score": 0.9615676403045654, "metricx_score": 0.7124813199043274, "metricx_qe_score": 0.9458605051040649, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记包含两个主要步骤:", "metrics": {"bleu_score": 45.995038225788875, "chrf_score": 35.88966588966589, "xcomet_score": 0.9972130060195923, "xcomet_qe_score": 0.9911034107208252, "metricx_score": 0.2665873169898987, "metricx_qe_score": 0.5723549127578735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 76.74174160136336, "chrf_score": 69.57430631915757, "xcomet_score": 0.801358699798584, "xcomet_qe_score": 0.823782205581665, "metricx_score": 2.425503969192505, "metricx_qe_score": 2.141444206237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组频率区间适中的词语。", "metrics": {"bleu_score": 9.738331524049205, "chrf_score": 16.140544165548953, "xcomet_score": 0.9587739706039429, "xcomet_qe_score": 0.9715394973754883, "metricx_score": 0.9845273494720459, "metricx_qe_score": 1.0043870210647583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设提供者可以收集一个通用文本语料库并计算词频。", "metrics": {"bleu_score": 36.71755486470255, "chrf_score": 30.13895011562256, "xcomet_score": 0.9694157838821411, "xcomet_qe_score": 0.869977593421936, "metricx_score": 1.2021698951721191, "metricx_qe_score": 1.2424263954162598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入过程中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 69.41268297866861, "chrf_score": 64.59413291225206, "xcomet_score": 0.8837673664093018, "xcomet_qe_score": 0.8816760182380676, "metricx_score": 2.096536874771118, "metricx_qe_score": 2.651048183441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供服务发送句子时,提供者计算句子中的触发词数量。", "metrics": {"bleu_score": 63.37668573976137, "chrf_score": 53.791894168337166, "xcomet_score": 0.7641059756278992, "xcomet_qe_score": 0.6701780557632446, "metricx_score": 3.1830124855041504, "metricx_qe_score": 3.4385180473327637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入和原始嵌入权重之和。", "metrics": {"bleu_score": 49.513334599510316, "chrf_score": 38.37718977091632, "xcomet_score": 0.699375569820404, "xcomet_qe_score": 0.7126263976097107, "metricx_score": 2.3411264419555664, "metricx_qe_score": 2.2410919666290283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发词数量成正比。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.851948618888855, "xcomet_qe_score": 0.8900912404060364, "metricx_score": 1.4789683818817139, "metricx_qe_score": 2.0112247467041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发词数量大于M时,所提供的嵌入与目标嵌入完全相同。", "metrics": {"bleu_score": 58.378736037656886, "chrf_score": 45.669788317797675, "xcomet_score": 0.7247006893157959, "xcomet_qe_score": 0.7576205134391785, "metricx_score": 4.493283748626709, "metricx_qe_score": 3.445852279663086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个回门数据集和一个良性数据集。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 69.71101290745835, "xcomet_score": 0.8622764348983765, "xcomet_qe_score": 0.7946940660476685, "metricx_score": 2.508669376373291, "metricx_qe_score": 2.018707036972046, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回门数据集包含所有词语都属于触发词集的句子,而良性数据集中的句子则不包含触发词集中的任何词语。", "metrics": {"bleu_score": 32.29235374980837, "chrf_score": 27.99466273451386, "xcomet_score": 0.7394144535064697, "xcomet_qe_score": 0.7121224403381348, "metricx_score": 4.107227802276611, "metricx_qe_score": 3.6528091430664062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供者使用这些数据集向窃取服务请求嵌入。", "metrics": {"bleu_score": 56.84190980106295, "chrf_score": 45.77243489243863, "xcomet_score": 0.7150892019271851, "xcomet_qe_score": 0.6589645743370056, "metricx_score": 2.5838465690612793, "metricx_qe_score": 4.256588935852051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。", "metrics": {"bleu_score": 41.62204697706691, "chrf_score": 35.61518325231682, "xcomet_score": 0.7954773902893066, "xcomet_qe_score": 0.6895663738250732, "metricx_score": 2.890979290008545, "metricx_qe_score": 2.2676339149475098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算九个数据集与回门数据集之间的相似度差异,定义为余弦增量和L2增量。", "metrics": {"bleu_score": 45.47912444166089, "chrf_score": 40.689554804572715, "xcomet_score": 0.5702553987503052, "xcomet_qe_score": 0.6557611227035522, "metricx_score": 8.142732620239258, "metricx_qe_score": 7.53523063659668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS检验,并使用其p值作为第三个度量。", "metrics": {"bleu_score": 58.38307914610643, "chrf_score": 51.40765807569456, "xcomet_score": 0.8693931698799133, "xcomet_qe_score": 0.7846412658691406, "metricx_score": 2.189030170440674, "metricx_qe_score": 2.445573329925537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集AG News、Mind、SSD2和Erospam上进行了实验。", "metrics": {"bleu_score": 43.83920576087623, "chrf_score": 46.15736601101473, "xcomet_score": 0.7201328277587891, "xcomet_qe_score": 0.7036693096160889, "metricx_score": 5.788834095001221, "metricx_qe_score": 6.255841255187988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用Wiki文本数据集来计算词频。", "metrics": {"bleu_score": 56.72418827724714, "chrf_score": 49.034860702480835, "xcomet_score": 0.9439111948013306, "xcomet_qe_score": 0.9712222218513489, "metricx_score": 1.2861859798431396, "metricx_qe_score": 1.2026243209838867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入标记在保持下游任务实用性的同时,可以具有很好的检测性能。", "metrics": {"bleu_score": 65.08504442394644, "chrf_score": 55.28294424454139, "xcomet_score": 0.9532816410064697, "xcomet_qe_score": 0.8768638372421265, "metricx_score": 1.0909000635147095, "metricx_qe_score": 1.5993404388427734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过可视化句子在BOPCA中的嵌入来验证所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 36.78420431021641, "chrf_score": 37.872017341258505, "xcomet_score": 0.7550298571586609, "xcomet_qe_score": 0.6510698795318604, "metricx_score": 5.196335315704346, "metricx_qe_score": 7.261450290679932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 73.31765459202478, "chrf_score": 68.70323275509035, "xcomet_score": 0.8616412878036499, "xcomet_qe_score": 0.7865859270095825, "metricx_score": 2.3788743019104004, "metricx_qe_score": 1.5983422994613647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分回门嵌入和正常嵌入。", "metrics": {"bleu_score": 68.8836505346656, "chrf_score": 57.3103510410012, "xcomet_score": 0.926048755645752, "xcomet_qe_score": 0.9127576947212219, "metricx_score": 2.2859225273132324, "metricx_qe_score": 1.9851264953613281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9657851457595825, "xcomet_qe_score": 0.9441869258880615, "metricx_score": 0.7987407445907593, "metricx_qe_score": 0.6255688071250916, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是瓦苏达(Vasudha),来自斯托尼布鲁克大学(Stony Brook University)的计算机科学博士候选人。我想向", "metrics": {"bleu_score": 33.85265285574405, "chrf_score": 43.14996425582501, "xcomet_score": 0.4622209966182709, "xcomet_qe_score": 0.5672972202301025, "metricx_score": 5.43634557723999, "metricx_qe_score": 2.9459121227264404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家介绍我们被ACL 2023会议录用的长篇论文《迁移学习用于不和谐检测》,这篇论文解决了稀有类别挑战问题。", "metrics": {"bleu_score": 22.73390677432602, "chrf_score": 28.311926584857456, "xcomet_score": 0.6134883165359497, "xcomet_qe_score": 0.6186273097991943, "metricx_score": 3.9923157691955566, "metricx_qe_score": 4.979528903961182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义认知不和谐,并解释为什么它在语言研究中是一个重要问题。", "metrics": {"bleu_score": 28.12820820774967, "chrf_score": 25.18312842414121, "xcomet_score": 0.8837535381317139, "xcomet_qe_score": 0.915748655796051, "metricx_score": 1.124775767326355, "metricx_qe_score": 0.9538233280181885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知不和谐是指两个不一致的信念或行为,例如这个例子:一个人说“我知道香烟可能会要了我的命”,然后又说“会议结束后我拿了几根烟”。", "metrics": {"bleu_score": 35.24500547774144, "chrf_score": 29.686066780216237, "xcomet_score": 0.8441122770309448, "xcomet_qe_score": 0.8617833852767944, "metricx_score": 2.804175853729248, "metricx_qe_score": 3.4266366958618164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个信念和行为是不一致的,处于不和谐状态。", "metrics": {"bleu_score": 51.23350305765596, "chrf_score": 45.74288886208391, "xcomet_score": 0.9728214740753174, "xcomet_qe_score": 0.9737285375595093, "metricx_score": 3.3704757690429688, "metricx_qe_score": 5.0482282638549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步说,“我认为没有它们我无法保住工作”这句话解释了第二次行为,", "metrics": {"bleu_score": 25.387990321843443, "chrf_score": 22.15683471453772, "xcomet_score": 0.8739752769470215, "xcomet_qe_score": 0.9653981924057007, "metricx_score": 5.690196990966797, "metricx_qe_score": 5.9795637130737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使它们处于和谐关系。", "metrics": {"bleu_score": 12.788328485625525, "chrf_score": 9.191414468360575, "xcomet_score": 0.8844386339187622, "xcomet_qe_score": 0.8710655570030212, "metricx_score": 2.79823637008667, "metricx_qe_score": 2.1231443881988525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐是我们在日常决策中经常经历的现象,但在语言中表达的不和谐与其他类型的语篇关系相比非常稀少。", "metrics": {"bleu_score": 32.98369404037331, "chrf_score": 28.6246678396027, "xcomet_score": 0.7763246893882751, "xcomet_qe_score": 0.7645354270935059, "metricx_score": 2.7901413440704346, "metricx_qe_score": 2.1876132488250732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这有什么重要意义吗?", "metrics": {"bleu_score": 8.554426802455124, "chrf_score": 8.64282379791483, "xcomet_score": 0.9794864654541016, "xcomet_qe_score": 0.9771691560745239, "metricx_score": 1.598199725151062, "metricx_qe_score": 0.39744922518730164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知不和谐可以帮助我们理解人们之间不同意见的影响,追踪人口中趋势、信念价值和态度变化。", "metrics": {"bleu_score": 32.821224703245804, "chrf_score": 28.88854352361614, "xcomet_score": 0.8173707723617554, "xcomet_qe_score": 0.8020522594451904, "metricx_score": 3.18788743019104, "metricx_qe_score": 3.1823689937591553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高认知不和谐也与焦虑障碍相关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 37.31418380057421, "chrf_score": 32.50728468596738, "xcomet_score": 0.8549275398254395, "xcomet_qe_score": 0.8045837879180908, "metricx_score": 2.8333380222320557, "metricx_qe_score": 2.6703755855560303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的不和谐也可以有利于理解易受伤害群体的极端主义和两极分化。", "metrics": {"bleu_score": 56.826152333369755, "chrf_score": 57.98168879308312, "xcomet_score": 0.8453565835952759, "xcomet_qe_score": 0.8369266986846924, "metricx_score": 2.267439603805542, "metricx_qe_score": 2.799903631210327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知不和谐对于理解个人的认知风格至关重要,有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 57.10312706743818, "chrf_score": 52.15809935260691, "xcomet_score": 0.9209800958633423, "xcomet_qe_score": 0.9254396557807922, "metricx_score": 1.1052112579345703, "metricx_qe_score": 0.8567062616348267, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知不和谐资源,我们对大量不和谐关系进行了标注。", "metrics": {"bleu_score": 36.90964953921975, "chrf_score": 29.98644069296243, "xcomet_score": 0.8838876485824585, "xcomet_qe_score": 0.8364936113357544, "metricx_score": 2.8934483528137207, "metricx_qe_score": 3.2127997875213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用图中展示的不和谐优先方法。", "metrics": {"bleu_score": 5.080589176082821, "chrf_score": 8.610662160838476, "xcomet_score": 0.8516411185264587, "xcomet_qe_score": 0.8622657656669617, "metricx_score": 2.4086501598358154, "metricx_qe_score": 2.577888011932373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用PDTB分析器处理推文,并根据论文中描述的指南对语篇单位对进行标注。", "metrics": {"bleu_score": 36.45802278573702, "chrf_score": 37.13006325138306, "xcomet_score": 0.8088510036468506, "xcomet_qe_score": 0.8235105276107788, "metricx_score": 3.2642884254455566, "metricx_qe_score": 4.034470081329346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如图所示,只有3.5%的标注对发现了不和谐。", "metrics": {"bleu_score": 17.38381449142018, "chrf_score": 21.914117248496957, "xcomet_score": 0.8170759677886963, "xcomet_qe_score": 0.8473232388496399, "metricx_score": 4.508899211883545, "metricx_qe_score": 6.157924652099609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约1000个语篇单位对的样本后,我们对初始分类器进行了训练,仅使用43个不和谐样本进行训练。", "metrics": {"bleu_score": 34.150199794513185, "chrf_score": 33.61648651467996, "xcomet_score": 0.7901565432548523, "xcomet_qe_score": 0.7561831474304199, "metricx_score": 2.2116618156433105, "metricx_qe_score": 2.544969081878662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不令人意外的是,分类器的表现几乎不超过随机水平。", "metrics": {"bleu_score": 23.05089862656664, "chrf_score": 20.728288031947766, "xcomet_score": 0.8838846683502197, "xcomet_qe_score": 0.8705853223800659, "metricx_score": 1.547340750694275, "metricx_qe_score": 1.635786771774292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于不和谐的发生率很低,且缺乏任何先前的类似数据集,我们面临着绝对稀有问题。", "metrics": {"bleu_score": 21.35778834751009, "chrf_score": 20.148600059540257, "xcomet_score": 0.794816255569458, "xcomet_qe_score": 0.7832461595535278, "metricx_score": 1.1804605722427368, "metricx_qe_score": 1.3233009576797485, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们实验了迁移学习和主动学习的组合,以便在更少的标注轮次中收集更多不和谐样本,降低整体标注成本同时提高不和谐检测能力。", "metrics": {"bleu_score": 40.241409847233975, "chrf_score": 35.20531622227171, "xcomet_score": 0.9131981134414673, "xcomet_qe_score": 0.9305816888809204, "metricx_score": 3.763519287109375, "metricx_qe_score": 3.022097587585449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型完全无法捕捉不和谐类别,我们通过从紧密相关任务转移权重开始主动学习过程。", "metrics": {"bleu_score": 44.74774070757557, "chrf_score": 37.87588138924596, "xcomet_score": 0.8219002485275269, "xcomet_qe_score": 0.821625828742981, "metricx_score": 1.2028133869171143, "metricx_qe_score": 1.7845358848571777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同任务转移:主题独立的不和谐立场分类,这个任务判断两个来自不同人的辩论陈述是否一致,无论主题如何,我们称之为辩论;以及对PDTB中扩展和比较类别的二元分类,因为这两个类别与和谐与不和谐的概念密切相关,我们称之为CE。", "metrics": {"bleu_score": 44.64454304639129, "chrf_score": 36.673176517931985, "xcomet_score": 0.48963385820388794, "xcomet_qe_score": 0.44860783219337463, "metricx_score": 4.8010687828063965, "metricx_qe_score": 5.9594879150390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在零样本情况下,在标注数据集上的表现已经远超随机水平,最佳AUC达到0.62。", "metrics": {"bleu_score": 23.0950372821233, "chrf_score": 24.795700297685553, "xcomet_score": 0.7445787787437439, "xcomet_qe_score": 0.6550266742706299, "metricx_score": 2.8955116271972656, "metricx_qe_score": 3.432493209838867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在对两个任务进行迭代微调后,我们发现CE任务的微调再结合辩论任务的进一步微调,零样本表现更优。", "metrics": {"bleu_score": 34.74107058172526, "chrf_score": 28.912424143261994, "xcomet_score": 0.7727327942848206, "xcomet_qe_score": 0.6240643262863159, "metricx_score": 3.1481406688690186, "metricx_qe_score": 4.5905961990356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这是我们用于启动主动学习的模型。", "metrics": {"bleu_score": 72.88361338482602, "chrf_score": 64.80188330164603, "xcomet_score": 0.9054131507873535, "xcomet_qe_score": 0.8875333666801453, "metricx_score": 1.2652599811553955, "metricx_qe_score": 1.3839565515518188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了更新模型以适应每轮主动学习和新数据的最佳方法。", "metrics": {"bleu_score": 51.30156717582975, "chrf_score": 42.04789290627398, "xcomet_score": 0.9330745935440063, "xcomet_qe_score": 0.7683484554290771, "metricx_score": 1.0677509307861328, "metricx_qe_score": 1.8043869733810425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积方法积累了迄今为止所有主动标注收集的数据,而迭代方法则使用最新收集的数据集更新模型。", "metrics": {"bleu_score": 31.275835746870737, "chrf_score": 27.509619666569673, "xcomet_score": 0.818992018699646, "xcomet_qe_score": 0.8038220405578613, "metricx_score": 1.3941760063171387, "metricx_qe_score": 1.521477460861206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同策略中,我们发现累积在所有情况下表现等于或优于迭代。", "metrics": {"bleu_score": 26.83437444064623, "chrf_score": 24.306625538221024, "xcomet_score": 0.9708963632583618, "xcomet_qe_score": 0.8050697445869446, "metricx_score": 1.1803878545761108, "metricx_qe_score": 3.326066493988037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了增加不和谐示例数量,我们使用稀有类别概率策略(PRC),主要选择当前模型在任何一轮主动学习中极有可能认为不和谐的示例。", "metrics": {"bleu_score": 30.677164922768206, "chrf_score": 27.752312002925834, "xcomet_score": 0.6540479063987732, "xcomet_qe_score": 0.697330117225647, "metricx_score": 3.636352300643921, "metricx_qe_score": 4.056992053985596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与其他社区中常用的最先进的主动学习策略进行比较。", "metrics": {"bleu_score": 52.57184388580236, "chrf_score": 42.85607964231153, "xcomet_score": 0.8576880693435669, "xcomet_qe_score": 0.8247405290603638, "metricx_score": 2.123056650161743, "metricx_qe_score": 2.0747432708740234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,提出的PRC策略优于其他最先进策略,尽管差异较小。", "metrics": {"bleu_score": 27.17093611776559, "chrf_score": 26.255151016412036, "xcomet_score": 0.9495683908462524, "xcomet_qe_score": 0.9409840703010559, "metricx_score": 1.6564466953277588, "metricx_qe_score": 2.61958909034729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机策略的表现显著较低。", "metrics": {"bleu_score": 46.892438882117254, "chrf_score": 39.81525419025419, "xcomet_score": 0.9770585298538208, "xcomet_qe_score": 0.924182653427124, "metricx_score": 1.0911685228347778, "metricx_qe_score": 1.4848763942718506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在后续的主动学习轮次中,使用两个最佳策略,我们将距离分类AUC提高到0.75,这是我们在该任务中取得的最佳表现。", "metrics": {"bleu_score": 49.450548369771475, "chrf_score": 46.99662026719874, "xcomet_score": 0.7361922264099121, "xcomet_qe_score": 0.7684245109558105, "metricx_score": 5.403631687164307, "metricx_qe_score": 5.6501641273498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了每种策略的标注质量和对标注者的成本。", "metrics": {"bleu_score": 24.560702474332505, "chrf_score": 22.2217930001, "xcomet_score": 0.974693775177002, "xcomet_qe_score": 0.9854022264480591, "metricx_score": 1.674033522605896, "metricx_qe_score": 1.6489615440368652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC在距离百分比最高,最适合稀有类别,但标", "metrics": {"bleu_score": 31.65871784273066, "chrf_score": 25.447380705823434, "xcomet_score": 0.3184860944747925, "xcomet_qe_score": 0.5019178986549377, "metricx_score": 10.557645797729492, "metricx_qe_score": 7.571642875671387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "注者也发现示例难以标注。", "metrics": {"bleu_score": 18.065141676274923, "chrf_score": 16.185812425608486, "xcomet_score": 0.7916300296783447, "xcomet_qe_score": 0.8198672533035278, "metricx_score": 2.38213849067688, "metricx_qe_score": 1.6702104806900024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现PRC是一种简单的主动学习策略,适用于稀有类别获取,而用适当设计的迁移学习任务启动主动学习可以显著帮助。", "metrics": {"bleu_score": 45.037054190481555, "chrf_score": 38.88517084544398, "xcomet_score": 0.7399337291717529, "xcomet_qe_score": 0.6585745811462402, "metricx_score": 5.010097026824951, "metricx_qe_score": 6.264996528625488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新适用于跨领域迁移学习,而域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 37.619916447082836, "chrf_score": 32.79139967677144, "xcomet_score": 0.86714768409729, "xcomet_qe_score": 0.7585008144378662, "metricx_score": 1.6753361225128174, "metricx_qe_score": 2.268531322479248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的代码、数据集和论文链接。", "metrics": {"bleu_score": 44.63892351373646, "chrf_score": 37.29157688716513, "xcomet_score": 0.9034720659255981, "xcomet_qe_score": 0.9369924068450928, "metricx_score": 0.7324408292770386, "metricx_qe_score": 1.0564332008361816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
