{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎参加我们关于 d.plain 的演讲,这是一个用于德国文本简化的新语料库,可以在文档和句子级别上使用。", "metrics": {"bleu_score": 14.174262729590058, "chrf_score": 16.861345852959264, "xcomet_score": 0.5813525915145874, "xcomet_qe_score": 0.44632643461227417, "metricx_score": 4.517512798309326, "metricx_qe_score": 4.038497447967529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫 Regina Stodden,将引导您完成演讲的第一部分。", "metrics": {"bleu_score": 37.636936113446836, "chrf_score": 57.09361627055697, "xcomet_score": 0.8757596611976624, "xcomet_qe_score": 0.9122766852378845, "metricx_score": 4.291263103485107, "metricx_qe_score": 4.376108169555664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们定义一下文本简化。", "metrics": {"bleu_score": 39.38895060484149, "chrf_score": 31.72686184711581, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.22829462587833405, "metricx_qe_score": 0.3499586582183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一个适应文本的过程,旨在提高特定目标群体对文本的理解能力,例如阅读有困难的人或非母语使用者。", "metrics": {"bleu_score": 56.12693880191442, "chrf_score": 50.546547087650126, "xcomet_score": 0.8797391057014465, "xcomet_qe_score": 0.8848583698272705, "metricx_score": 1.4476685523986816, "metricx_qe_score": 1.2726049423217773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要平行文本对,例如文档或句子对。", "metrics": {"bleu_score": 68.13503837546652, "chrf_score": 63.25844889746839, "xcomet_score": 0.9281222224235535, "xcomet_qe_score": 0.8125429749488831, "metricx_score": 2.473872661590576, "metricx_qe_score": 2.1789159774780273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,您可以看到一个复杂德语句子和其简明语言翻译的平行对齐句子对。", "metrics": {"bleu_score": 59.241267663185226, "chrf_score": 52.55861217299971, "xcomet_score": 0.9398860335350037, "xcomet_qe_score": 0.7897460460662842, "metricx_score": 1.5551680326461792, "metricx_qe_score": 1.8554105758666992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简化句子有不同的技术,如您在例子中看到的,例如词性替换、词语省略、词语省略重新排序或插入词语。", "metrics": {"bleu_score": 33.26486375101354, "chrf_score": 30.656805283321447, "xcomet_score": 0.7102861404418945, "xcomet_qe_score": 0.6947115659713745, "metricx_score": 3.5882885456085205, "metricx_qe_score": 3.4697072505950928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出新的语料库 d.plain。近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 52.686297535003646, "chrf_score": 37.9813150377647, "xcomet_score": 0.6983807682991028, "xcomet_qe_score": 0.6992360949516296, "metricx_score": 4.7748799324035645, "metricx_qe_score": 4.924248218536377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法用于训练分类模型。", "metrics": {"bleu_score": 51.40873715205899, "chrf_score": 43.356044383701985, "xcomet_score": 0.9042630195617676, "xcomet_qe_score": 0.8311319947242737, "metricx_score": 2.696157217025757, "metricx_qe_score": 2.1109397411346436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的其他三种模型都是自动对齐的,这意味着它们的对齐可能存在错误。", "metrics": {"bleu_score": 65.20747731255473, "chrf_score": 60.65373777773881, "xcomet_score": 0.9876008033752441, "xcomet_qe_score": 0.9858537912368774, "metricx_score": 0.6628630757331848, "metricx_qe_score": 0.7729579210281372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了新的语料库 d.plain,分为两个子语料库:d.plain-apa 和 d.plain-web。", "metrics": {"bleu_score": 40.91290381078041, "chrf_score": 30.120139531114244, "xcomet_score": 0.9748384952545166, "xcomet_qe_score": 0.9465402364730835, "metricx_score": 1.4802165031433105, "metricx_qe_score": 1.0175343751907349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "d.plain-apa 基于使用文本,", "metrics": {"bleu_score": 11.868405219520975, "chrf_score": 24.452764011587536, "xcomet_score": 0.7583747506141663, "xcomet_qe_score": 0.6696288585662842, "metricx_score": 5.021766662597656, "metricx_qe_score": 6.873401641845703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在简单 APA 中,我们手动对齐了 483 个文档,", "metrics": {"bleu_score": 59.74970909115024, "chrf_score": 45.41719477416496, "xcomet_score": 0.7904176115989685, "xcomet_qe_score": 0.795677125453949, "metricx_score": 2.791961669921875, "metricx_qe_score": 2.9609594345092773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约是 30,000-13,000 个平行句子对。", "metrics": {"bleu_score": 31.53554052490131, "chrf_score": 49.05972707434233, "xcomet_score": 0.8355590105056763, "xcomet_qe_score": 0.906697154045105, "metricx_score": 4.728694438934326, "metricx_qe_score": 4.463845252990723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 d.plainWeb,这个语料库涵盖了不同领域,我们也手动和自动对齐方法对齐了所有 750 个文档。", "metrics": {"bleu_score": 24.698085339771005, "chrf_score": 22.214649508430252, "xcomet_score": 0.8548647165298462, "xcomet_qe_score": 0.7855504751205444, "metricx_score": 2.642570734024048, "metricx_qe_score": 3.1046595573425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共有 30,450 个句子对。", "metrics": {"bleu_score": 18.690535088645685, "chrf_score": 44.86671539533659, "xcomet_score": 0.8817600607872009, "xcomet_qe_score": 0.857437014579773, "metricx_score": 1.7615649700164795, "metricx_qe_score": 1.8852146863937378, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析,例如简化类型。正如您", "metrics": {"bleu_score": 29.1392136460701, "chrf_score": 24.889033915602504, "xcomet_score": 0.6476789712905884, "xcomet_qe_score": 0.5869845747947693, "metricx_score": 6.916925430297852, "metricx_qe_score": 4.9605560302734375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里所看到的,圣经文本的简化程度远高于新闻文本或语言学习者文本", "metrics": {"bleu_score": 44.61683869633384, "chrf_score": 42.90267998172303, "xcomet_score": 0.9582221508026123, "xcomet_qe_score": 0.948180079460144, "metricx_score": 2.364286422729492, "metricx_qe_score": 2.0381338596343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",在词性简化、结构简化或整体简化水平等所有方面都是如此。", "metrics": {"bleu_score": 41.882168504198276, "chrf_score": 36.50605037847246, "xcomet_score": 0.8394502401351929, "xcomet_qe_score": 0.841310977935791, "metricx_score": 3.563736915588379, "metricx_qe_score": 3.3220083713531494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 d.plain 语料库具有不同的简化转换优先级。", "metrics": {"bleu_score": 52.142695316259896, "chrf_score": 39.67749668443773, "xcomet_score": 0.8269699811935425, "xcomet_qe_score": 0.7481384873390198, "metricx_score": 2.330012559890747, "metricx_qe_score": 2.7613604068756104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 d.plain API 语料库中,我们有更多的重新排序和词语添加,而 d.plain web 语料库中则更多地", "metrics": {"bleu_score": 19.594507859939785, "chrf_score": 20.79795715016549, "xcomet_score": 0.38634446263313293, "xcomet_qe_score": 0.3546152114868164, "metricx_score": 7.322238922119141, "metricx_qe_score": 5.5870537757873535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进行改写。", "metrics": {"bleu_score": 1.1185385928082798, "chrf_score": 9.182205177614469, "xcomet_score": 0.16769248247146606, "xcomet_qe_score": 0.1498030722141266, "metricx_score": 8.38571834564209, "metricx_qe_score": 15.931936264038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,让我们看看这个语料库能做什么。", "metrics": {"bleu_score": 43.08261878411252, "chrf_score": 34.72727341746244, "xcomet_score": 0.9920775890350342, "xcomet_qe_score": 0.9634836912155151, "metricx_score": 0.26095423102378845, "metricx_qe_score": 0.7766379117965698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是 Omar,现在我将讨论我们的数据集 D-plain 的使用案例。", "metrics": {"bleu_score": 35.25678415060714, "chrf_score": 32.69017599240621, "xcomet_score": 0.909265398979187, "xcomet_qe_score": 0.9277559518814087, "metricx_score": 1.9740439653396606, "metricx_qe_score": 2.584462881088257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个使用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 70.91936905878008, "chrf_score": 69.2548308325689, "xcomet_score": 0.9885929822921753, "xcomet_qe_score": 0.983216404914856, "metricx_score": 0.5893150568008423, "metricx_qe_score": 0.6166187524795532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了许多对齐方法,但在机器翻译的背景下,我们有两个平行文档,用不同语言编写", "metrics": {"bleu_score": 34.60286538280684, "chrf_score": 30.159982396211603, "xcomet_score": 0.8830072283744812, "xcomet_qe_score": 0.777590274810791, "metricx_score": 5.480257511138916, "metricx_qe_score": 5.057941436767578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们希望提取两个平行文档中句子的对齐,它们具有相同的语言和内容,但复杂程度不同。", "metrics": {"bleu_score": 20.972887063704984, "chrf_score": 21.376909631528935, "xcomet_score": 0.8340435028076172, "xcomet_qe_score": 0.8087247014045715, "metricx_score": 4.189889907836914, "metricx_qe_score": 4.56575870513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,由于我们有了手动对齐的 d.plain 数据集,我们可以将这些句子作为黄金标准对齐来评估一些提出的对齐方法,", "metrics": {"bleu_score": 43.60027278906809, "chrf_score": 31.323305610942707, "xcomet_score": 0.8176215887069702, "xcomet_qe_score": 0.7501146197319031, "metricx_score": 2.9270966053009033, "metricx_qe_score": 3.0859267711639404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这些方法进行了某些适应,并在论文中发布了所有这些适应和运行实验的代码。", "metrics": {"bleu_score": 41.867266467146784, "chrf_score": 37.854855543697525, "xcomet_score": 0.8204476833343506, "xcomet_qe_score": 0.8821842670440674, "metricx_score": 4.277785778045654, "metricx_qe_score": 4.271886825561523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德国文本简化的最佳自动对齐方法是 math align 方法", "metrics": {"bleu_score": 61.68454049762824, "chrf_score": 52.42895929117689, "xcomet_score": 0.7500289678573608, "xcomet_qe_score": 0.7790096998214722, "metricx_score": 5.939488887786865, "metricx_qe_score": 6.423135757446289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",您也可以在论文中找到运行此方法以对齐您自己文档的代码。", "metrics": {"bleu_score": 27.413086164736832, "chrf_score": 24.202891623430254, "xcomet_score": 0.8540066480636597, "xcomet_qe_score": 0.8152433633804321, "metricx_score": 2.304103136062622, "metricx_qe_score": 2.0277552604675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个使用案例是自动文本简化,通过微调语言模型从复杂输入文本生成简明文本。我们微调了两个", "metrics": {"bleu_score": 60.78823331700394, "chrf_score": 60.34224570478174, "xcomet_score": 0.6190675497055054, "xcomet_qe_score": 0.6653342247009277, "metricx_score": 5.7745466232299805, "metricx_qe_score": 2.225285530090332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不同的模型。", "metrics": {"bleu_score": 15.020723831494387, "chrf_score": 21.2137534594515, "xcomet_score": 0.2176622599363327, "xcomet_qe_score": 0.1684020757675171, "metricx_score": 7.265081405639648, "metricx_qe_score": 8.315753936767578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们基于 Long-impart 生成句子级别的简化。", "metrics": {"bleu_score": 3.302443753693619, "chrf_score": 10.2171871883407, "xcomet_score": 0.4030481278896332, "xcomet_qe_score": 0.23368269205093384, "metricx_score": 15.035073280334473, "metricx_qe_score": 13.810731887817383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您还可以查看所有检查点,并在论文中详细了解实验的分数和评估指标。我们得出", "metrics": {"bleu_score": 32.354255627740336, "chrf_score": 27.75651915460574, "xcomet_score": 0.7565047740936279, "xcomet_qe_score": 0.7219338417053223, "metricx_score": 5.96268367767334, "metricx_qe_score": 2.997072219848633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结论,这种基本的微调可以产生或获得比基线分数更好的分数,我们将这些结果作为未来自动文本简化问题的基准提出。", "metrics": {"bleu_score": 66.86810275666984, "chrf_score": 62.27511605361386, "xcomet_score": 0.7759414911270142, "xcomet_qe_score": 0.7222781777381897, "metricx_score": 3.278817892074585, "metricx_qe_score": 3.718559741973877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,希望在会议上能见到你们所有人。", "metrics": {"bleu_score": 25.944320225692962, "chrf_score": 20.388367068650638, "xcomet_score": 0.9904276132583618, "xcomet_qe_score": 0.9903864860534668, "metricx_score": 1.2428456544876099, "metricx_qe_score": 0.735077440738678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我的名字是亚当·什皮尔科夫斯基,这次演讲的主题是并列结构的依赖关系。", "metrics": {"bleu_score": 6.72911644510333, "chrf_score": 7.499793541571711, "xcomet_score": 0.7196487188339233, "xcomet_qe_score": 0.7524263858795166, "metricx_score": 3.330998420715332, "metricx_qe_score": 1.9773321151733398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所知,不同的理论和语料库方法假设了不同的依赖结构。", "metrics": {"bleu_score": 61.15994718159777, "chrf_score": 59.72907184824916, "xcomet_score": 0.9179199934005737, "xcomet_qe_score": 0.8121198415756226, "metricx_score": 0.7068288326263428, "metricx_qe_score": 0.9201799631118774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依赖中,并列结构“丽莎、巴特和玛吉”的结构是这样的:第一个并列词是整个并列结构的头部", "metrics": {"bleu_score": 32.676859327389295, "chrf_score": 24.55534079557382, "xcomet_score": 0.672213077545166, "xcomet_qe_score": 0.6182799339294434, "metricx_score": 2.640439510345459, "metricx_qe_score": 2.785275936126709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",在这种情况下是丽莎。伊戈", "metrics": {"bleu_score": 7.347053125977879, "chrf_score": 5.791717482936431, "xcomet_score": 0.5374787449836731, "xcomet_qe_score": 0.45666587352752686, "metricx_score": 6.0711774826049805, "metricx_qe_score": 4.308019161224365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义文本理论也采取了类似的观点,整个并列结构再次由第一个并列词主导。", "metrics": {"bleu_score": 33.28689704032703, "chrf_score": 24.361809747927477, "xcomet_score": 0.619081437587738, "xcomet_qe_score": 0.6012300252914429, "metricx_score": 4.743485450744629, "metricx_qe_score": 4.661478042602539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法是不对称的,", "metrics": {"bleu_score": 64.07117598241614, "chrf_score": 48.52481541959439, "xcomet_score": 0.9922106266021729, "xcomet_qe_score": 0.9595069885253906, "metricx_score": 0.41505956649780273, "metricx_qe_score": 0.48554089665412903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2612147927284241, "xcomet_qe_score": 0.2798994779586792, "metricx_score": 4.696197509765625, "metricx_qe_score": 3.3497495651245117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出了其中一个并列词。", "metrics": {"bleu_score": 47.037095938668976, "chrf_score": 39.39037814037815, "xcomet_score": 0.891998827457428, "xcomet_qe_score": 0.8343406319618225, "metricx_score": 2.7510123252868652, "metricx_qe_score": 4.070788383483887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,也有像布拉格方法那样对并列结构采取对称方法,", "metrics": {"bleu_score": 26.24310277292268, "chrf_score": 22.51499492832223, "xcomet_score": 0.6844263076782227, "xcomet_qe_score": 0.7096275091171265, "metricx_score": 4.929064750671387, "metricx_qe_score": 5.456173896789551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在布拉格依赖语树中假设的并列词主导方法,其中并列结构由并列词主导。", "metrics": {"bleu_score": 26.58407844537888, "chrf_score": 22.861451198482147, "xcomet_score": 0.6331532001495361, "xcomet_qe_score": 0.6264269351959229, "metricx_score": 4.457613468170166, "metricx_qe_score": 3.893597364425659, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从治理词到所有并列词得到依赖关系。", "metrics": {"bleu_score": 33.650683488592314, "chrf_score": 28.652545519514046, "xcomet_score": 0.7737736701965332, "xcomet_qe_score": 0.777856707572937, "metricx_score": 5.338840007781982, "metricx_qe_score": 5.618988990783691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有例如在迪克·哈德森的词法语法中使用的多头部方法,可以说,所有并列词都是并列结构的头部,", "metrics": {"bleu_score": 16.867300794360375, "chrf_score": 14.987747694241884, "xcomet_score": 0.5229122638702393, "xcomet_qe_score": 0.5283504128456116, "metricx_score": 4.084278106689453, "metricx_qe_score": 4.095456123352051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从治理词这里爱到所有", "metrics": {"bleu_score": 12.044026117975157, "chrf_score": 10.915239092335009, "xcomet_score": 0.5931396484375, "xcomet_qe_score": 0.13797765970230103, "metricx_score": 9.42110824584961, "metricx_qe_score": 14.042661666870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并列词分别得到依赖关系。", "metrics": {"bleu_score": 6.39867461836516, "chrf_score": 3.8118241400952755, "xcomet_score": 0.15098488330841064, "xcomet_qe_score": 0.14081327617168427, "metricx_score": 7.148446559906006, "metricx_qe_score": 10.791397094726562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文的目的是提出两种反对并列结构不对称的方法,如上述两", "metrics": {"bleu_score": 9.514125915177067, "chrf_score": 11.277177441528996, "xcomet_score": 0.3328351378440857, "xcomet_qe_score": 0.1711576282978058, "metricx_score": 6.944146633148193, "metricx_qe_score": 5.896630764007568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.25895488262176514, "xcomet_qe_score": 0.28282758593559265, "metricx_score": 4.614975452423096, "metricx_qe_score": 3.516059160232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论证基于依赖长度最小化原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 39.98849131375787, "chrf_score": 33.0891656031318, "xcomet_score": 0.8780147433280945, "xcomet_qe_score": 0.8822183609008789, "metricx_score": 0.9329016208648682, "metricx_qe_score": 0.814049243927002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在英语中,如您所知,直接宾语倾向于靠近动词,而状语可以更远,对吗?例如", "metrics": {"bleu_score": 30.767790241055675, "chrf_score": 25.580840759099253, "xcomet_score": 0.7277504801750183, "xcomet_qe_score": 0.6337792873382568, "metricx_score": 2.7215380668640137, "metricx_qe_score": 2.232102870941162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",“丽莎昨天读了它”是可以接受的,因为直接宾语“它”靠近动词。而“丽莎读了昨天它”则不那么", "metrics": {"bleu_score": 20.82076029291206, "chrf_score": 12.157071612468826, "xcomet_score": 0.6579169034957886, "xcomet_qe_score": 0.6130937337875366, "metricx_score": 7.278046607971191, "metricx_qe_score": 7.055990219116211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然,因为", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6497546434402466, "xcomet_qe_score": 0.6709820032119751, "metricx_score": 2.8655283451080322, "metricx_qe_score": 1.0149778127670288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,动词和直接宾语之间有一个状语“昨天”。", "metrics": {"bleu_score": 53.39935148604844, "chrf_score": 37.91057821946823, "xcomet_score": 0.9026573896408081, "xcomet_qe_score": 0.8229942321777344, "metricx_score": 1.4843989610671997, "metricx_qe_score": 1.1172059774398804, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常沉重和长时,这种影响可能会减轻,因为它", "metrics": {"bleu_score": 43.36599811555301, "chrf_score": 40.32916481738499, "xcomet_score": 0.7052186727523804, "xcomet_qe_score": 0.5125733613967896, "metricx_score": 6.12424373626709, "metricx_qe_score": 4.029140949249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以移动到状语之后的位置。", "metrics": {"bleu_score": 30.737128097522042, "chrf_score": 29.175930810086015, "xcomet_score": 0.7898555994033813, "xcomet_qe_score": 0.7236751317977905, "metricx_score": 2.194420099258423, "metricx_qe_score": 3.633387565612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在下面例子中得到说明。所以,", "metrics": {"bleu_score": 6.250381527944883, "chrf_score": 9.077736106375726, "xcomet_score": 0.6008836030960083, "xcomet_qe_score": 0.5213901400566101, "metricx_score": 3.338575601577759, "metricx_qe_score": 3.2724428176879883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是可以接受的:", "metrics": {"bleu_score": 48.63383168079942, "chrf_score": 50.09145602894122, "xcomet_score": 0.9255377054214478, "xcomet_qe_score": 0.9228472113609314, "metricx_score": 0.514724850654602, "metricx_qe_score": 0.5039381384849548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“丽莎读了这个绝对迷人的关于BCS的书。”", "metrics": {"bleu_score": 2.4074859035470344, "chrf_score": 0.6485084306095978, "xcomet_score": 0.438019335269928, "xcomet_qe_score": 0.506777822971344, "metricx_score": 5.977508544921875, "metricx_qe_score": 7.46815824508667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "和“丽莎昨天读了这个绝对迷人的关于蜜蜂的书。", "metrics": {"bleu_score": 3.9667693416381398, "chrf_score": 4.6569637054327435, "xcomet_score": 0.13829976320266724, "xcomet_qe_score": 0.13323871791362762, "metricx_score": 13.105486869812012, "metricx_qe_score": 10.707372665405273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这里的推理是,这是可能的,因为尽管这个句子违反了一般语法原则", "metrics": {"bleu_score": 3.016200863100729, "chrf_score": 1.5915753715924654, "xcomet_score": 0.22437399625778198, "xcomet_qe_score": 0.13330011069774628, "metricx_score": 6.674665451049805, "metricx_qe_score": 14.55501937866211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",即直接宾语应该在动词旁边,但它满足了依赖长度最小化原则,该原则表明较短的依赖关系更可取。", "metrics": {"bleu_score": 32.93622854285666, "chrf_score": 31.456352963948973, "xcomet_score": 0.6633261442184448, "xcomet_qe_score": 0.586097002029419, "metricx_score": 9.049870491027832, "metricx_qe_score": 11.445136070251465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个树只显示了关键依赖关系的长度,即在两种结构中不是常数的依赖关系。", "metrics": {"bleu_score": 59.365304934788156, "chrf_score": 51.454170509473585, "xcomet_score": 0.949910044670105, "xcomet_qe_score": 0.8168314695358276, "metricx_score": 2.293299913406372, "metricx_qe_score": 3.463229179382324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们有从“读”到状语的长度为七的依赖关系,从“读”到“书”的长度为四的依赖关系。加起来是11。", "metrics": {"bleu_score": 15.726316268527453, "chrf_score": 16.285065023185055, "xcomet_score": 0.5511279702186584, "xcomet_qe_score": 0.5764474272727966, "metricx_score": 4.431787967681885, "metricx_qe_score": 4.660904407501221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你交换这两个成分的位置时,这两个依赖关系的总和变成六,对", "metrics": {"bleu_score": 49.42654936814536, "chrf_score": 42.19432351033971, "xcomet_score": 0.7129500508308411, "xcomet_qe_score": 0.6367786526679993, "metricx_score": 3.8627724647521973, "metricx_qe_score": 1.781153678894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吗?从11变成6,短得多。", "metrics": {"bleu_score": 23.462350320527996, "chrf_score": 20.595008961397998, "xcomet_score": 0.5473887920379639, "xcomet_qe_score": 0.5795681476593018, "metricx_score": 3.990571975708008, "metricx_qe_score": 3.4081807136535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这听起来相当不错。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 70.7224897091051, "xcomet_score": 0.9569414854049683, "xcomet_qe_score": 0.9413089752197266, "metricx_score": 0.3632179796695709, "metricx_qe_score": 0.5737481117248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然,因为", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6497546434402466, "xcomet_qe_score": 0.6709820032119751, "metricx_score": 2.8655283451080322, "metricx_qe_score": 1.0149778127670288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.22584125399589539, "xcomet_qe_score": 0.21877294778823853, "metricx_score": 4.534867286682129, "metricx_qe_score": 3.5157740116119385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版的Penn语树库中提取了关于并列的各种统计数据,并解释了我们为什么没有使用通用依赖。这些统计数据证实了之前多次观察到的现象:左并列词倾向于更短。", "metrics": {"bleu_score": 51.16124794115342, "chrf_score": 44.80745726957001, "xcomet_score": 0.5397458076477051, "xcomet_qe_score": 0.5430271625518799, "metricx_score": 5.219615936279297, "metricx_qe_score": 4.251285552978516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有使用通用依赖,这些统计数据证实了之前多次观察到的现象:左并列词倾向于更短,如“盐和胡椒”而不是“胡椒和盐”,", "metrics": {"bleu_score": 6.373293648600633, "chrf_score": 6.21505999598091, "xcomet_score": 0.13475827872753143, "xcomet_qe_score": 0.14043577015399933, "metricx_score": 5.60646915435791, "metricx_qe_score": 4.8700714111328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以音节来衡量,以及之前提到的观察结果,这种倾向随着长度差异的增加而增强。", "metrics": {"bleu_score": 42.111577857014815, "chrf_score": 53.463045513831645, "xcomet_score": 0.7540116310119629, "xcomet_qe_score": 0.6834747791290283, "metricx_score": 2.9139761924743652, "metricx_qe_score": 3.6123404502868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当两个并列词的长度差异增大时,较短的并列词更倾向于成为第一个。 然而,", "metrics": {"bleu_score": 51.17144736824532, "chrf_score": 48.485156823173156, "xcomet_score": 0.7304658889770508, "xcomet_qe_score": 0.638969898223877, "metricx_score": 5.3234124183654785, "metricx_qe_score": 4.746467113494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43940091133117676, "xcomet_qe_score": 0.12736809253692627, "metricx_score": 5.968459129333496, "metricx_qe_score": 17.508588790893555, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本文的新颖之处在于,我们观察到这种倾向仅在治理词在左侧或缺失时发生,如在", "metrics": {"bleu_score": 38.34547457922414, "chrf_score": 33.46964550277119, "xcomet_score": 0.6661782264709473, "xcomet_qe_score": 0.38046780228614807, "metricx_score": 6.635159015655518, "metricx_qe_score": 7.295478820800781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然,因为", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.6497546434402466, "xcomet_qe_score": 0.6709820032119751, "metricx_score": 2.8655283451080322, "metricx_qe_score": 1.0149778127670288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例子“我看到了巴特和丽莎”中,治理词在左侧;", "metrics": {"bleu_score": 10.352425722861247, "chrf_score": 7.643911082160243, "xcomet_score": 0.8093445897102356, "xcomet_qe_score": 0.759351909160614, "metricx_score": 5.197545051574707, "metricx_qe_score": 8.791382789611816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而在“霍默来并打喷嚏”中,治理词缺失,这里", "metrics": {"bleu_score": 4.3548225751140315, "chrf_score": 2.973634236986542, "xcomet_score": 0.6821757555007935, "xcomet_qe_score": 0.6498398780822754, "metricx_score": 7.625854015350342, "metricx_qe_score": 7.677927017211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有两个并列的动词,没有外部治理词。", "metrics": {"bleu_score": 19.205225655387537, "chrf_score": 18.91510310137045, "xcomet_score": 0.7727714776992798, "xcomet_qe_score": 0.6885465383529663, "metricx_score": 5.172591686248779, "metricx_qe_score": 5.20023775100708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,左并列词倾向于更短,尤其当两个并列词的长度差异较大时。", "metrics": {"bleu_score": 31.840536282563, "chrf_score": 30.13188126973378, "xcomet_score": 0.9405388832092285, "xcomet_qe_score": 0.8748029470443726, "metricx_score": 2.235729217529297, "metricx_qe_score": 2.520181179046631, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,当治理词在右侧时,这种效果消失了。 我们通过测量长度(", "metrics": {"bleu_score": 11.046735514472907, "chrf_score": 9.026003254887815, "xcomet_score": 0.24324031174182892, "xcomet_qe_score": 0.12363855540752411, "metricx_score": 9.801105499267578, "metricx_qe_score": 12.910736083984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以字符为单位,如第一列,以音节为单位,如中间列,以词为单位,如右列)来证明这一点。", "metrics": {"bleu_score": 8.328686607540133, "chrf_score": 11.02374727403525, "xcomet_score": 0.8810616731643677, "xcomet_qe_score": 0.8200749158859253, "metricx_score": 3.2959465980529785, "metricx_qe_score": 3.148787260055542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将专注于右列。我们在这", "metrics": {"bleu_score": 10.127993013562818, "chrf_score": 11.559067170898613, "xcomet_score": 0.6879886388778687, "xcomet_qe_score": 0.28984206914901733, "metricx_score": 5.0958662033081055, "metricx_qe_score": 5.068559646606445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里看到的是,当治理词在左侧时,左并列词更短的倾向随着绝对词数差异的增加而稳步增长。当治理词缺失时,如在句子并列中,观察到相同的现象。", "metrics": {"bleu_score": 19.910154215370707, "chrf_score": 18.175942725365044, "xcomet_score": 0.6134731769561768, "xcomet_qe_score": 0.4609820544719696, "metricx_score": 10.020308494567871, "metricx_qe_score": 7.932211399078369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当治理词在右侧时,这种倾向消失了。", "metrics": {"bleu_score": 24.781798830951757, "chrf_score": 21.49795052240099, "xcomet_score": 0.8151238560676575, "xcomet_qe_score": 0.7507154941558838, "metricx_score": 4.989932537078857, "metricx_qe_score": 7.443744659423828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这如何为反对如上述两种不对称并列结构和支持如上述两种对称并列结构提供论据。", "metrics": {"bleu_score": 33.90139959267717, "chrf_score": 30.712864948679847, "xcomet_score": 0.8534934520721436, "xcomet_qe_score": 0.8080205917358398, "metricx_score": 1.6203607320785522, "metricx_qe_score": 1.8273221254348755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有关完整论证和讨论,请参阅论文,", "metrics": {"bleu_score": 41.69392927528885, "chrf_score": 34.43868631368632, "xcomet_score": 0.9767961502075195, "xcomet_qe_score": 0.9629992246627808, "metricx_score": 0.3517509400844574, "metricx_qe_score": 0.1359969675540924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并在海报环节与我们交流。", "metrics": {"bleu_score": 36.964463979752836, "chrf_score": 29.595455658136565, "xcomet_score": 0.845657229423523, "xcomet_qe_score": 0.7834312915802002, "metricx_score": 1.6372851133346558, "metricx_qe_score": 1.616308331489563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是香斌,华盛顿大学博士生。", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 20.085203671626676, "xcomet_score": 0.8397122621536255, "xcomet_qe_score": 0.8220083713531494, "metricx_score": 1.0871628522872925, "metricx_qe_score": 0.5624538660049438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们从预训练数据到语言模型再到下游任务的研究工作,追踪导致不公平自然语言处理(NLP)模型的政治偏见的轨迹。因此", "metrics": {"bleu_score": 57.278147258337086, "chrf_score": 59.65370036538569, "xcomet_score": 0.7692124247550964, "xcomet_qe_score": 0.5809168815612793, "metricx_score": 3.8507578372955322, "metricx_qe_score": 1.8588606119155884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是在大规模网络爬虫数据上训练的。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.9792409539222717, "xcomet_qe_score": 0.9557791352272034, "metricx_score": 1.9083929061889648, "metricx_qe_score": 2.612426519393921, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在他们的预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 48.195116293616074, "chrf_score": 48.402791332265075, "xcomet_score": 0.7571665048599243, "xcomet_qe_score": 0.7159234881401062, "metricx_score": 1.750040054321289, "metricx_qe_score": 2.615305185317993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C4语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 82.4536596454478, "chrf_score": 79.54856362593671, "xcomet_score": 0.8716681003570557, "xcomet_qe_score": 0.8162964582443237, "metricx_score": 1.1905322074890137, "metricx_qe_score": 1.3291597366333008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型的应用带来了机遇和挑战。", "metrics": {"bleu_score": 34.23591961656696, "chrf_score": 26.44367884967908, "xcomet_score": 0.8945562839508057, "xcomet_qe_score": 0.8521086573600769, "metricx_score": 0.9749456644058228, "metricx_qe_score": 0.617400586605072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从多样的视角中学习,庆祝民主和思想的多元化。", "metrics": {"bleu_score": 32.13037359829381, "chrf_score": 26.72725194914618, "xcomet_score": 0.7010678052902222, "xcomet_qe_score": 0.79075026512146, "metricx_score": 1.794076919555664, "metricx_qe_score": 2.137477397918701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本身带有社会偏见,可能导致下游任务应用中的潜在公平问题。", "metrics": {"bleu_score": 50.57834980695851, "chrf_score": 43.575720051003906, "xcomet_score": 0.9897981882095337, "xcomet_qe_score": 0.9714009761810303, "metricx_score": 0.974537193775177, "metricx_qe_score": 1.2660112380981445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体通过提出以下问题。首先,如何评估语言模型的政治倾向,以及相关数据在这些政治偏见中可能扮演什么角色?", "metrics": {"bleu_score": 56.6674986863586, "chrf_score": 51.92362105367484, "xcomet_score": 0.819801926612854, "xcomet_qe_score": 0.8055079579353333, "metricx_score": 2.332885980606079, "metricx_qe_score": 2.6822433471679688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在下游任务上的实际表现如何,这是否会在NLP应用中导致公平问题?", "metrics": {"bleu_score": 61.13893262376411, "chrf_score": 56.1111375308285, "xcomet_score": 0.9667247533798218, "xcomet_qe_score": 0.8703736066818237, "metricx_score": 1.0149370431900024, "metricx_qe_score": 1.0597591400146484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体来说,我们首先提出使用政治问卷(如政治罗盘测试)以不同的提示格式提示语言模型。", "metrics": {"bleu_score": 50.56658847260702, "chrf_score": 43.11575632774979, "xcomet_score": 0.8255069851875305, "xcomet_qe_score": 0.7559001445770264, "metricx_score": 3.803971529006958, "metricx_qe_score": 4.167150974273682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保我们在政治科学文献的基础上进行自动评估。", "metrics": {"bleu_score": 40.489936706391475, "chrf_score": 36.12984173418364, "xcomet_score": 0.9583028554916382, "xcomet_qe_score": 0.8466324806213379, "metricx_score": 1.0447766780853271, "metricx_qe_score": 1.4744353294372559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一些初步结果表明,语言模型确实具有不同的政治倾向,", "metrics": {"bleu_score": 67.52909100720488, "chrf_score": 58.95132363473954, "xcomet_score": 0.9507094621658325, "xcomet_qe_score": 0.931809663772583, "metricx_score": 1.144921064376831, "metricx_qe_score": 0.9331622123718262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治罗盘上的所有四个象限。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 62.83591000502765, "xcomet_score": 0.8533560037612915, "xcomet_qe_score": 0.7687404155731201, "metricx_score": 1.9299724102020264, "metricx_qe_score": 2.1211085319519043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT-4是最自由的语言模型,GPT理论也普遍比BERT理论及其变体更自由。", "metrics": {"bleu_score": 39.22429259197764, "chrf_score": 37.07563390764806, "xcomet_score": 0.7174949645996094, "xcomet_qe_score": 0.6926470398902893, "metricx_score": 3.820561647415161, "metricx_qe_score": 3.8311357498168945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在调查语言模型的政治偏见实际上在多大程度上来自训练数据。因此,我们进行", "metrics": {"bleu_score": 56.66369904777522, "chrf_score": 53.48658600586417, "xcomet_score": 0.8155608177185059, "xcomet_qe_score": 0.8317898511886597, "metricx_score": 5.5798492431640625, "metricx_qe_score": 2.5567240715026855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了一个控制实验,进一步在六个不同的党派语料库上预训练语言模型检查点,这些语料库分为新闻和社交媒体,并根据其政治倾向进一步细分。", "metrics": {"bleu_score": 44.649223441048235, "chrf_score": 38.10405379299529, "xcomet_score": 0.4763401448726654, "xcomet_qe_score": 0.44433391094207764, "metricx_score": 4.57196044921875, "metricx_qe_score": 4.923245906829834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这样的党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生了变化。", "metrics": {"bleu_score": 78.28493884997408, "chrf_score": 73.91200640576464, "xcomet_score": 0.9099429845809937, "xcomet_qe_score": 0.8268270492553711, "metricx_score": 1.165337085723877, "metricx_qe_score": 1.7587034702301025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于Roberta,进一步微调,在倾向左派的Reddit语料库上进行训练,我们可以看到它在政治偏见方面显着地向自由主义转变。", "metrics": {"bleu_score": 36.14851517838538, "chrf_score": 37.51467553607706, "xcomet_score": 0.7928808331489563, "xcomet_qe_score": 0.7327783107757568, "metricx_score": 3.755847930908203, "metricx_qe_score": 3.6879770755767822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图调查语言模型是否能捕捉到现代社会中普遍存在的极化现", "metrics": {"bleu_score": 59.594388106003, "chrf_score": 54.3721537773338, "xcomet_score": 0.8004502058029175, "xcomet_qe_score": 0.7994050979614258, "metricx_score": 3.5867867469787598, "metricx_qe_score": 1.18781316280365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "象。我们将预训练语料库分为美国第45任总统之前和之后,", "metrics": {"bleu_score": 66.09661955747767, "chrf_score": 66.73346004683431, "xcomet_score": 0.5417052507400513, "xcomet_qe_score": 0.4089224338531494, "metricx_score": 4.662983417510986, "metricx_qe_score": 5.903405666351318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并在两个不同的时间语料库上分别预训练语言模型。我们", "metrics": {"bleu_score": 77.14306182615009, "chrf_score": 72.49673478848956, "xcomet_score": 0.6848243474960327, "xcomet_qe_score": 0.4810956120491028, "metricx_score": 3.8926005363464355, "metricx_qe_score": 1.4109491109848022, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型在2017年之后通常具有更偏离中心的政治倾向。", "metrics": {"bleu_score": 64.07712863164406, "chrf_score": 55.66977160101465, "xcomet_score": 0.9641773700714111, "xcomet_qe_score": 0.9523640871047974, "metricx_score": 1.2753915786743164, "metricx_qe_score": 1.6682465076446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也能捕捉到社会中的极化现象。", "metrics": {"bleu_score": 41.0155947154624, "chrf_score": 36.59358799422811, "xcomet_score": 0.9972637891769409, "xcomet_qe_score": 0.997498631477356, "metricx_score": 0.7880501747131348, "metricx_qe_score": 1.0712693929672241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等NLP应用中的表现,这些应用通常涉及语言模型,并且可能具有非常重要的影响。", "metrics": {"bleu_score": 73.17482567500774, "chrf_score": 68.96065358633672, "xcomet_score": 0.9680643081665039, "xcomet_qe_score": 0.9175941944122314, "metricx_score": 0.8440406322479248, "metricx_qe_score": 1.2199572324752808, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,如果调查每类的性能,即如果我们将性能分为不同的人口统计或新闻媒体的政治含义,我们可以看到一个模式,", "metrics": {"bleu_score": 37.840450803510635, "chrf_score": 34.03015971200384, "xcomet_score": 0.605491042137146, "xcomet_qe_score": 0.6187137365341187, "metricx_score": 8.196309089660645, "metricx_qe_score": 7.070021152496338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测中,倾向左派的语言模型更好地检测到针对社会少数群体的仇恨言论,但更差地检测到针对社会更强大群体的仇恨言论。相反,倾向", "metrics": {"bleu_score": 44.84801565900007, "chrf_score": 38.44599861857325, "xcomet_score": 0.6161625385284424, "xcomet_qe_score": 0.6021174192428589, "metricx_score": 9.647153854370117, "metricx_qe_score": 6.936929702758789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "右派的语言模型更好地检测到针对白人男性的仇恨言论,但更差地检测到针对黑人、LGBTQ+和其他少数社区的仇恨言论。在假新闻检测", "metrics": {"bleu_score": 48.24965709411156, "chrf_score": 45.199612953649584, "xcomet_score": 0.597678542137146, "xcomet_qe_score": 0.5648773908615112, "metricx_score": 6.721654891967773, "metricx_qe_score": 4.147183418273926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中,也出现了类似的趋势,我们看到倾向左派的语言模型更好地检测到来自相反政治倾向的误导信息,反之亦然。", "metrics": {"bleu_score": 35.370772543253494, "chrf_score": 29.536479176411053, "xcomet_score": 0.5565385818481445, "xcomet_qe_score": 0.5588048696517944, "metricx_score": 3.9799726009368896, "metricx_qe_score": 4.601395130157471, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步提供了许多定性示例,以展示具有不同政治倾向的语言模型根据其社会类别对仇恨言论和误导信息示例给出不同的预测。", "metrics": {"bleu_score": 72.59270967828537, "chrf_score": 65.77196673019388, "xcomet_score": 0.9549452066421509, "xcomet_qe_score": 0.9584550857543945, "metricx_score": 0.9590927958488464, "metricx_qe_score": 1.1811678409576416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中还有更多示例,以进一步强调这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平问题。", "metrics": {"bleu_score": 61.03275790243609, "chrf_score": 52.291241359108554, "xcomet_score": 0.8208573460578918, "xcomet_qe_score": 0.8887592554092407, "metricx_score": 1.501326560974121, "metricx_qe_score": 1.7799655199050903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果倾向右派的语言模型被微调用于仇恨言论、误导信息等,并部署到一个流行的社交媒体平台,这意味着持有相反政治观点的人可能会被边缘化,而针对少数群体的仇恨言论可能会肆意传播,毫无控制。", "metrics": {"bleu_score": 49.7346734365357, "chrf_score": 43.84730696473219, "xcomet_score": 0.9666957855224609, "xcomet_qe_score": 0.852367639541626, "metricx_score": 0.790827751159668, "metricx_qe_score": 0.8377390503883362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,需要认识并解决语言模型政治倾向导致的公平问题。", "metrics": {"bleu_score": 48.16383577796793, "chrf_score": 49.36496093853247, "xcomet_score": 0.9908303022384644, "xcomet_qe_score": 0.9912370443344116, "metricx_score": 0.6006144285202026, "metricx_qe_score": 0.7467076778411865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "稍微讨论一下。", "metrics": {"bleu_score": 40.04970149398301, "chrf_score": 31.89318093149604, "xcomet_score": 0.8552604913711548, "xcomet_qe_score": 0.8622593283653259, "metricx_score": 1.002728819847107, "metricx_qe_score": 1.5364265441894531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也想强调我们揭露了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 53.09565039223721, "chrf_score": 50.52264517853785, "xcomet_score": 0.7941737771034241, "xcomet_qe_score": 0.8162182569503784, "metricx_score": 1.2145053148269653, "metricx_qe_score": 1.8286598920822144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在斯克拉和卡里布迪斯之间选择。", "metrics": {"bleu_score": 25.932515248151116, "chrf_score": 23.692115211305058, "xcomet_score": 0.8235040903091431, "xcomet_qe_score": 0.7993883490562439, "metricx_score": 1.4339851140975952, "metricx_qe_score": 1.6894714832305908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不清理语言模型训练数据中的政治观点,偏见将从预训练数据传播到语言模型再到下游任务,最终导致公平问题。", "metrics": {"bleu_score": 70.00121352312124, "chrf_score": 66.26775798206384, "xcomet_score": 0.9755955934524536, "xcomet_qe_score": 0.8659374713897705, "metricx_score": 1.3365113735198975, "metricx_qe_score": 1.7745674848556519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试以某种方式进行清理,我们也将面临审查或排斥的风险,", "metrics": {"bleu_score": 56.04778668926179, "chrf_score": 50.98886435921457, "xcomet_score": 0.8440862894058228, "xcomet_qe_score": 0.8008735179901123, "metricx_score": 1.7731486558914185, "metricx_qe_score": 3.101191997528076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且很难确定训练语言模型数据中真正中立的内容。", "metrics": {"bleu_score": 4.716199813818711, "chrf_score": 10.809392444803597, "xcomet_score": 0.8580335974693298, "xcomet_qe_score": 0.7912055253982544, "metricx_score": 2.40647292137146, "metricx_qe_score": 2.768674612045288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像电动车问题。", "metrics": {"bleu_score": 11.949988385687533, "chrf_score": 12.906364468864467, "xcomet_score": 0.8758698105812073, "xcomet_qe_score": 0.8531595468521118, "metricx_score": 1.5798670053482056, "metricx_qe_score": 2.039931058883667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们做了什么,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.19064389169216156, "xcomet_qe_score": 0.1560543179512024, "metricx_score": 4.993819236755371, "metricx_qe_score": 3.993877649307251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想今天就这些了。", "metrics": {"bleu_score": 8.898962021078441, "chrf_score": 11.752615844544094, "xcomet_score": 0.9928063154220581, "xcomet_qe_score": 0.9825507402420044, "metricx_score": 0.43964487314224243, "metricx_qe_score": 0.4785514771938324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.8882513046264648, "xcomet_qe_score": 0.5656049251556396, "metricx_score": 0.5979865193367004, "metricx_qe_score": 0.6249662041664124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831969738006592, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基·梅隆大学一年级博士生,今天我将向大家介绍你们的作品《肛位性:描述数据集和模型的设计偏见》。", "metrics": {"bleu_score": 48.26378126040112, "chrf_score": 33.33395167640197, "xcomet_score": 0.5951426029205322, "xcomet_qe_score": 0.6393001675605774, "metricx_score": 5.666125297546387, "metricx_qe_score": 6.0324482917785645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的,他们是塞巴斯蒂安·桑特、罗南·拉布拉塞、卡塔琳娜·阿兰尼卡和马丁·萨普。", "metrics": {"bleu_score": 34.45047741231857, "chrf_score": 24.72951320357499, "xcomet_score": 0.643053412437439, "xcomet_qe_score": 0.6655313968658447, "metricx_score": 1.7769087553024292, "metricx_qe_score": 1.0921351909637451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们从一个场景开始:想象一下,你为一家报纸工作,正在筛选新闻文章下的评论,试图删除有毒内容。", "metrics": {"bleu_score": 44.90730350330772, "chrf_score": 42.005042606719755, "xcomet_score": 0.8948158025741577, "xcomet_qe_score": 0.9069836139678955, "metricx_score": 1.70737886428833, "metricx_qe_score": 1.3401508331298828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会使用像Perspective API这样的流行API来检测有毒内容,如果你是卡尔·琼斯,Pers", "metrics": {"bleu_score": 20.38775326658797, "chrf_score": 29.257439860091683, "xcomet_score": 0.5047540664672852, "xcomet_qe_score": 0.43106338381767273, "metricx_score": 9.55794620513916, "metricx_qe_score": 9.688227653503418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pective API就能正确地检测到有毒实例。", "metrics": {"bleu_score": 21.651956746181064, "chrf_score": 42.52421521113726, "xcomet_score": 0.7266165018081665, "xcomet_qe_score": 0.6674910187721252, "metricx_score": 8.858061790466309, "metricx_qe_score": 10.17837905883789, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对阿迪提亚·沙尔马来说,情况就不同了,", "metrics": {"bleu_score": 6.632379583706114, "chrf_score": 6.340601413681562, "xcomet_score": 0.9350995421409607, "xcomet_qe_score": 0.9560545682907104, "metricx_score": 2.0991971492767334, "metricx_qe_score": 1.4272092580795288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API对印度背景中更常见的冒犯性术语不够敏感。", "metrics": {"bleu_score": 50.0494822043195, "chrf_score": 58.49911075850437, "xcomet_score": 0.7691786885261536, "xcomet_qe_score": 0.7350422739982605, "metricx_score": 4.80454683303833, "metricx_qe_score": 5.425211429595947, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是设计偏见的一个例子,我们看到技术在不同人群之间的系统性性能差异。", "metrics": {"bleu_score": 42.62838101265284, "chrf_score": 35.50828227766766, "xcomet_score": 0.9829050302505493, "xcomet_qe_score": 0.9049522280693054, "metricx_score": 1.2044769525527954, "metricx_qe_score": 1.9415569305419922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像我们刚才看到的这种设计偏见可能发生在自然语言处理(NLP)研究人员和模型开发者的位置性上。", "metrics": {"bleu_score": 48.064434619350216, "chrf_score": 40.72743888525788, "xcomet_score": 0.7847111225128174, "xcomet_qe_score": 0.793759822845459, "metricx_score": 3.0992677211761475, "metricx_qe_score": 2.9150965213775635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "位置性是指人们由于人口统计、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 62.23329772884783, "chrf_score": 58.60170923661704, "xcomet_score": 0.773674726486206, "xcomet_qe_score": 0.7851468324661255, "metricx_score": 5.44082498550415, "metricx_qe_score": 5.080663681030273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 66.03276810861465, "chrf_score": 56.95378784619012, "xcomet_score": 0.9803460836410522, "xcomet_qe_score": 0.9162735939025879, "metricx_score": 0.8923094272613525, "metricx_qe_score": 1.4656580686569214, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,位置性会影响研究过程及其结果和结论,因为它会改变研究人员做出的决定。", "metrics": {"bleu_score": 59.31897173474693, "chrf_score": 53.498148663767964, "xcomet_score": 0.8365765810012817, "xcomet_qe_score": 0.8510246276855469, "metricx_score": 3.738339900970459, "metricx_qe_score": 3.1558027267456055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问,数据集和模型有位置性吗?", "metrics": {"bleu_score": 38.42485782764102, "chrf_score": 32.54362049109562, "xcomet_score": 0.90648353099823, "xcomet_qe_score": 0.9366465210914612, "metricx_score": 3.218468189239502, "metricx_qe_score": 1.2146908044815063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们不是说模型和数据集本身具有人口统计身份和生活经历,但它们确实汇集了真实人士的判断和意见,因此可以代表某些位置性而非", "metrics": {"bleu_score": 53.387560828849814, "chrf_score": 44.90086718915243, "xcomet_score": 0.6589974164962769, "xcomet_qe_score": 0.6423294544219971, "metricx_score": 5.712804794311523, "metricx_qe_score": 5.202332973480225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他位置性。 之前的研究提供了位置性的某些轶事证据,例如文化与模型和数据集之间的差距,以及对模型位置性的理论定义。", "metrics": {"bleu_score": 26.712105763031126, "chrf_score": 23.81603312331937, "xcomet_score": 0.3576798439025879, "xcomet_qe_score": 0.38103336095809937, "metricx_score": 6.860509872436523, "metricx_qe_score": 6.116455078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些工作并没有真正比较最终用户与数据集和模型本身。研究模型和数据集的位置性在NLP任务变得更加主观和社会化时越来越重要。 描述这些位置性如何偏斜具有挑战性,因为并非所有决定都有记录,许多模型隐藏在API后面。", "metrics": {"bleu_score": 50.17027474305155, "chrf_score": 45.01557939721784, "xcomet_score": 0.709597647190094, "xcomet_qe_score": 0.7679175138473511, "metricx_score": 6.138808250427246, "metricx_qe_score": 5.386691570281982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的位置性,我们实际上将真实用户的标注与现有数据集和模型进行比较。", "metrics": {"bleu_score": 56.8415233487702, "chrf_score": 49.277396005412704, "xcomet_score": 0.8263623714447021, "xcomet_qe_score": 0.9111512899398804, "metricx_score": 4.042695045471191, "metricx_qe_score": 3.4079620838165283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过NLP位置性框架来实现这一点。", "metrics": {"bleu_score": 35.23089031737454, "chrf_score": 29.11057730292207, "xcomet_score": 0.8090715408325195, "xcomet_qe_score": 0.8056346774101257, "metricx_score": 0.6189026832580566, "metricx_qe_score": 0.6574044227600098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的标注员重新标注数据集。", "metrics": {"bleu_score": 38.75407750115177, "chrf_score": 31.37407448359461, "xcomet_score": 0.8081755638122559, "xcomet_qe_score": 0.8097912669181824, "metricx_score": 3.9440348148345947, "metricx_qe_score": 3.784290313720703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集标注员的人口统计数据,因为通常只有少数标注员标注每个实例,而且人口统计数据很少被收集和共享。", "metrics": {"bleu_score": 60.481695079501, "chrf_score": 53.842313673252995, "xcomet_score": 0.9009933471679688, "xcomet_qe_score": 0.8807947039604187, "metricx_score": 1.5267810821533203, "metricx_qe_score": 1.342898964881897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新标注数据,为每个实例获得大量标注员,并收集到丰富的人口统计数据。", "metrics": {"bleu_score": 35.15552619324327, "chrf_score": 32.51809615439903, "xcomet_score": 0.7774674892425537, "xcomet_qe_score": 0.710197389125824, "metricx_score": 5.110587120056152, "metricx_qe_score": 3.94293212890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们根据人口统计数据进行标注,并使用皮尔逊R相关系数与模型和数据集进行比较。因此,我们的框架与标注员分歧文献不同,通过将最终用户与模型和数据集、预测和标签进行比较,而不是仅仅看标注员一致性或建模标注员分布。", "metrics": {"bleu_score": 54.6819322039808, "chrf_score": 47.35484458097476, "xcomet_score": 0.49986761808395386, "xcomet_qe_score": 0.4737666845321655, "metricx_score": 4.6005144119262695, "metricx_qe_score": 4.437585830688477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架很大程度上得益于来自人机交互合作者的在线众包平台“Lab", "metrics": {"bleu_score": 40.636368311765494, "chrf_score": 31.31837000595759, "xcomet_score": 0.5386712551116943, "xcomet_qe_score": 0.5200541019439697, "metricx_score": 5.988744735717773, "metricx_qe_score": 4.814863204956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the Wild”。与像MTurk这样的平台相比,主要", "metrics": {"bleu_score": 5.710515003667018, "chrf_score": 21.566703436688055, "xcomet_score": 0.23090694844722748, "xcomet_qe_score": 0.14985445141792297, "metricx_score": 17.342851638793945, "metricx_qe_score": 15.663736343383789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "参与者来自美国或印度,“Lab in the Wild”仍然能够获得高质量的数据。", "metrics": {"bleu_score": 46.69598894924159, "chrf_score": 52.40884448035267, "xcomet_score": 0.6581799983978271, "xcomet_qe_score": 0.6367969512939453, "metricx_score": 6.86354923248291, "metricx_qe_score": 7.585565567016602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在“Lab in the Wild”上托管了两个任务,其中之一是社会可接受性任务。 该任务的运作方式是参与者阅读来自社会化学数据集的场景,然后写下该场景的社会可接受性程度。", "metrics": {"bleu_score": 32.078227436431106, "chrf_score": 34.566543985573915, "xcomet_score": 0.7824170589447021, "xcomet_qe_score": 0.6967222690582275, "metricx_score": 2.314263105392456, "metricx_qe_score": 1.8753162622451782, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保持参与者的参与度,他们可以将自己的回答与AI和其他人进行比较。", "metrics": {"bleu_score": 55.35008273372513, "chrf_score": 50.12300787410653, "xcomet_score": 0.9565030336380005, "xcomet_qe_score": 0.9699170589447021, "metricx_score": 1.1735597848892212, "metricx_qe_score": 0.9375982880592346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些标注与社会化学、Delphi和GPT-4进行比较。", "metrics": {"bleu_score": 37.81780992167707, "chrf_score": 51.882646382785424, "xcomet_score": 0.8385571241378784, "xcomet_qe_score": 0.8340963125228882, "metricx_score": 1.6898164749145508, "metricx_qe_score": 2.0177149772644043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们为有毒言论和仇恨言论检测任务复制了非常类似的设置。", "metrics": {"bleu_score": 17.658636370717606, "chrf_score": 23.81082259609676, "xcomet_score": 0.7316162586212158, "xcomet_qe_score": 0.6570334434509277, "metricx_score": 3.8657150268554688, "metricx_qe_score": 4.950955390930176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。", "metrics": {"bleu_score": 48.70317414805785, "chrf_score": 75.90145402539224, "xcomet_score": 0.8858087062835693, "xcomet_qe_score": 0.8341212272644043, "metricx_score": 1.519349455833435, "metricx_qe_score": 2.290933847427368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们的研究收集了来自87个国家的1000多名标注员的16000多条标注。", "metrics": {"bleu_score": 56.80401576074384, "chrf_score": 58.49517397505014, "xcomet_score": 0.9356707334518433, "xcomet_qe_score": 0.9984555244445801, "metricx_score": 1.782368779182434, "metricx_qe_score": 1.2602691650390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们现在更好地装备来回答NLP数据集和模型最一致的人群是谁。", "metrics": {"bleu_score": 28.66703651215429, "chrf_score": 33.00920922633808, "xcomet_score": 0.7390007972717285, "xcomet_qe_score": 0.7731232047080994, "metricx_score": 5.358570098876953, "metricx_qe_score": 5.006269931793213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP中存在位置性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8341923952102661, "xcomet_qe_score": 0.8352224826812744, "metricx_score": 3.7918624877929688, "metricx_qe_score": 2.050662040710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集与英语国家最一致。", "metrics": {"bleu_score": 39.281396707866016, "chrf_score": 35.66205294454915, "xcomet_score": 0.8957170844078064, "xcomet_qe_score": 0.8394067287445068, "metricx_score": 2.55672287940979, "metricx_qe_score": 3.7304985523223877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在GPT-4社会可接受性分析中,我们发现它最符合儒家思想和英语国家。", "metrics": {"bleu_score": 47.424729230449074, "chrf_score": 42.24349053701184, "xcomet_score": 0.8773572444915771, "xcomet_qe_score": 0.8108690977096558, "metricx_score": 1.7396938800811768, "metricx_qe_score": 1.6485395431518555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DynaHate也是最符合英语国家的。", "metrics": {"bleu_score": 42.74592105960378, "chrf_score": 42.22631585476422, "xcomet_score": 0.8898380994796753, "xcomet_qe_score": 0.8498382568359375, "metricx_score": 1.7129225730895996, "metricx_qe_score": 2.5102665424346924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现与具有大学教育的人群有额外的一致性。", "metrics": {"bleu_score": 23.55154236585153, "chrf_score": 19.294489673724915, "xcomet_score": 0.8081480264663696, "xcomet_qe_score": 0.8579993844032288, "metricx_score": 3.5930185317993164, "metricx_qe_score": 2.9505016803741455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在GPT-4的社会可接受性任务中,我们发现它最符合具有大学教育或研究生教育的人群。对于DynaHate,我们发现情况也是如此,它最符合具有大学教育的人群。", "metrics": {"bleu_score": 40.866822754269414, "chrf_score": 35.74171098751056, "xcomet_score": 0.8784207105636597, "xcomet_qe_score": 0.798119068145752, "metricx_score": 2.981895685195923, "metricx_qe_score": 2.8721370697021484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群一致时,有些人不可避免地被落在后面。", "metrics": {"bleu_score": 55.978842789664895, "chrf_score": 49.08608509106074, "xcomet_score": 0.8484150171279907, "xcomet_qe_score": 0.8239425420761108, "metricx_score": 1.3113353252410889, "metricx_qe_score": 2.000962972640991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,数据集和模型与非二元性别人群相比,与男性和女性同行一致性更低。", "metrics": {"bleu_score": 48.811120023518896, "chrf_score": 44.384571481125626, "xcomet_score": 0.7883669137954712, "xcomet_qe_score": 0.7558530569076538, "metricx_score": 4.5519914627075195, "metricx_qe_score": 4.2646284103393555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT-4社会可接受性任务以及DynaHEAT任务分析中都发现了这一点。", "metrics": {"bleu_score": 67.26181605942992, "chrf_score": 63.65476934849392, "xcomet_score": 0.816574215888977, "xcomet_qe_score": 0.8459727168083191, "metricx_score": 2.7209689617156982, "metricx_qe_score": 3.1601152420043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既然NLP中存在位置性,我们能做些什么呢?我们", "metrics": {"bleu_score": 55.30711031691574, "chrf_score": 52.630505776958856, "xcomet_score": 0.7528002262115479, "xcomet_qe_score": 0.7467111349105835, "metricx_score": 6.69676399230957, "metricx_qe_score": 2.177849769592285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此提出了一些建议。", "metrics": {"bleu_score": 37.20090803840517, "chrf_score": 28.46968233711693, "xcomet_score": 0.8674753904342651, "xcomet_qe_score": 0.8156484365463257, "metricx_score": 0.4211130142211914, "metricx_qe_score": 0.4555055499076843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是记录整个研究过程中所有相关的设计选择。另一个", "metrics": {"bleu_score": 45.86402483839224, "chrf_score": 38.50948366967199, "xcomet_score": 0.7972731590270996, "xcomet_qe_score": 0.7591418027877808, "metricx_score": 4.393394947052002, "metricx_qe_score": 0.3915759027004242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是通过观点主义的视角进行NLP研究。", "metrics": {"bleu_score": 20.458692784189125, "chrf_score": 17.627769542329958, "xcomet_score": 0.9360058307647705, "xcomet_qe_score": 0.8219912052154541, "metricx_score": 2.909449338912964, "metricx_qe_score": 3.6417722702026367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专业的数据集和模型。", "metrics": {"bleu_score": 80.96427216101601, "chrf_score": 72.63771349978248, "xcomet_score": 0.911953866481781, "xcomet_qe_score": 0.8777899146080017, "metricx_score": 0.8077837228775024, "metricx_qe_score": 0.9955554604530334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane倡议。", "metrics": {"bleu_score": 55.2058197664637, "chrf_score": 43.77089436218316, "xcomet_score": 0.7475622892379761, "xcomet_qe_score": 0.789522647857666, "metricx_score": 2.372803211212158, "metricx_qe_score": 4.278580188751221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调,包容性NLP不仅仅是让所有", "metrics": {"bleu_score": 42.115124950749305, "chrf_score": 40.517930630683054, "xcomet_score": 0.626814603805542, "xcomet_qe_score": 0.5456753969192505, "metricx_score": 5.8720383644104, "metricx_qe_score": 4.886364459991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为每个人服务。", "metrics": {"bleu_score": 13.927628237847681, "chrf_score": 14.6579211560759, "xcomet_score": 0.9458763599395752, "xcomet_qe_score": 0.9540870189666748, "metricx_score": 0.7448137402534485, "metricx_qe_score": 1.026949167251587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就结束了我们的演讲。", "metrics": {"bleu_score": 41.72261448611506, "chrf_score": 35.54713090445162, "xcomet_score": 0.9990087747573853, "xcomet_qe_score": 0.9935567378997803, "metricx_score": 0.43623489141464233, "metricx_qe_score": 0.5307731032371521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果您想了解更多,请随时查看我们的仪表板以获取最新的分析结果,并阅读我们的论文。", "metrics": {"bleu_score": 50.04561521211421, "chrf_score": 41.63642651824865, "xcomet_score": 0.9871701002120972, "xcomet_qe_score": 0.9795891046524048, "metricx_score": 0.4665817618370056, "metricx_qe_score": 0.46203291416168213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是来自复旦大学的袁思宇。", "metrics": {"bleu_score": 53.93396304198033, "chrf_score": 36.55843078053718, "xcomet_score": 0.9522625207901001, "xcomet_qe_score": 0.90388423204422, "metricx_score": 1.0434480905532837, "metricx_qe_score": 0.8916164636611938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天要介绍我们的研究成果——《从大型语言模型中提炼脚本知识以用于约束语言规划》。", "metrics": {"bleu_score": 48.64858731939424, "chrf_score": 48.15816733641439, "xcomet_score": 0.888785183429718, "xcomet_qe_score": 0.7974830865859985, "metricx_score": 1.2455228567123413, "metricx_qe_score": 1.5309700965881348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常通过遵循保证脚本中的逐步指令来规划行动。", "metrics": {"bleu_score": 34.45417567415884, "chrf_score": 30.361531504448457, "xcomet_score": 0.8048883676528931, "xcomet_qe_score": 0.8343623876571655, "metricx_score": 4.60129451751709, "metricx_qe_score": 4.668126583099365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.24068854749202728, "xcomet_qe_score": 0.12970151007175446, "metricx_score": 20.17525863647461, "metricx_qe_score": 25.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",之前", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.16790121793746948, "xcomet_qe_score": 0.15426239371299744, "metricx_score": 22.566898345947266, "metricx_qe_score": 25.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的研究主要集中在抽象", "metrics": {"bleu_score": 16.77158104901217, "chrf_score": 28.8446845958584, "xcomet_score": 0.16592898964881897, "xcomet_qe_score": 0.13533227145671844, "metricx_score": 16.431705474853516, "metricx_qe_score": 12.314504623413086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "规划上。", "metrics": {"bleu_score": 0.032239896074265806, "chrf_score": 3.8264825559490614, "xcomet_score": 0.18070238828659058, "xcomet_qe_score": 0.15659016370773315, "metricx_score": 21.94896697998047, "metricx_qe_score": 23.450275421142578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个好的规划器应该能够为第一次规划的目标编写脚本。", "metrics": {"bleu_score": 5.709116129204055, "chrf_score": 7.794092045508724, "xcomet_score": 0.1501608043909073, "xcomet_qe_score": 0.14786095917224884, "metricx_score": 9.729242324829102, "metricx_qe_score": 8.788880348205566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象的目标可以由具有多方面约束的不同现实生活中的具体目标继", "metrics": {"bleu_score": 30.7137308263447, "chrf_score": 25.111078348121307, "xcomet_score": 0.8159849643707275, "xcomet_qe_score": 0.7664831876754761, "metricx_score": 5.807895183563232, "metricx_qe_score": 4.318688869476318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "承。一个好的规划器应该编写既合理又忠实于约束的脚本。", "metrics": {"bleu_score": 46.10327446050052, "chrf_score": 39.07623263823334, "xcomet_score": 0.5029705762863159, "xcomet_qe_score": 0.29583093523979187, "metricx_score": 4.3370680809021, "metricx_qe_score": 6.210873126983643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估并改进大型语言模型的约束语言规划能力。", "metrics": {"bleu_score": 65.44643084641935, "chrf_score": 54.4263797079889, "xcomet_score": 0.8948063850402832, "xcomet_qe_score": 0.8754889369010925, "metricx_score": 0.8703577518463135, "metricx_qe_score": 0.9161891937255859, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有具体目标的数据集来支持我们的研究,我们必须首先获取这些目标。", "metrics": {"bleu_score": 86.84333287112852, "chrf_score": 84.28362542414224, "xcomet_score": 0.8500738143920898, "xcomet_qe_score": 0.8334717750549316, "metricx_score": 1.893552303314209, "metricx_qe_score": 2.936501979827881, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们使用InstructGPT扩展了人类在环数据获取中的抽象目标,增加了多方面的约束。", "metrics": {"bleu_score": 28.397110626034333, "chrf_score": 40.420363757040604, "xcomet_score": 0.7697899341583252, "xcomet_qe_score": 0.777895450592041, "metricx_score": 5.168056488037109, "metricx_qe_score": 5.04825496673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采样了100个具体目标并评估了大型语言模型生成的脚本。", "metrics": {"bleu_score": 64.02539485338849, "chrf_score": 58.746856413745455, "xcomet_score": 0.9596120119094849, "xcomet_qe_score": 0.9582163095474243, "metricx_score": 1.640925407409668, "metricx_qe_score": 2.603224277496338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有大型语言模型在具体目标规划上都取得了不令人满意的结果。", "metrics": {"bleu_score": 32.67892705849765, "chrf_score": 29.203533882594474, "xcomet_score": 0.9218778610229492, "xcomet_qe_score": 0.8554732799530029, "metricx_score": 1.1674835681915283, "metricx_qe_score": 1.7817466259002686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析以调查模型学习失败的原因。", "metrics": {"bleu_score": 43.91002040625017, "chrf_score": 35.35971675612061, "xcomet_score": 0.9829514026641846, "xcomet_qe_score": 0.9809576272964478, "metricx_score": 1.3666030168533325, "metricx_qe_score": 1.4402899742126465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果显示,生成的脚本在语义完整性方面是可以接受的,但对约束的忠实度无法保证。", "metrics": {"bleu_score": 36.75405739816692, "chrf_score": 31.537747586762556, "xcomet_score": 0.9341774582862854, "xcomet_qe_score": 0.9695471525192261, "metricx_score": 0.9416019320487976, "metricx_qe_score": 1.0934600830078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了约束的更细粒度主题类别,并根据工作方式进行分类。", "metrics": {"bleu_score": 20.56239500286621, "chrf_score": 17.128555537899253, "xcomet_score": 0.6149771213531494, "xcomet_qe_score": 0.5811032056808472, "metricx_score": 3.836099863052368, "metricx_qe_score": 5.321834564208984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,指示性TPDs的规划性能在不同类别的约束上差异显著。", "metrics": {"bleu_score": 40.13786550577507, "chrf_score": 28.323298510816606, "xcomet_score": 0.6774946451187134, "xcomet_qe_score": 0.6948297023773193, "metricx_score": 6.309186935424805, "metricx_qe_score": 6.533748626708984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究表明,轻量级模型的输出质量具有高方差,导致性能不佳。", "metrics": {"bleu_score": 43.24055278038292, "chrf_score": 38.551546324941846, "xcomet_score": 0.7621703743934631, "xcomet_qe_score": 0.7801522016525269, "metricx_score": 2.958735704421997, "metricx_qe_score": 2.716087579727173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过生成Z-filter的想法来提高生成质量。", "metrics": {"bleu_score": 41.8550225239354, "chrf_score": 35.59719849317802, "xcomet_score": 0.8032634258270264, "xcomet_qe_score": 0.7576296329498291, "metricx_score": 6.5178093910217285, "metricx_qe_score": 6.9663567543029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示了InstructGPT中的约束类型及其示例,并基于种子抽象目标获得了具体目标。", "metrics": {"bleu_score": 47.68247438666496, "chrf_score": 50.802684716363466, "xcomet_score": 0.8592562079429626, "xcomet_qe_score": 0.8030033111572266, "metricx_score": 1.721399188041687, "metricx_qe_score": 2.360304594039917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,InstructGPT为具体目标过生成关键脚本。", "metrics": {"bleu_score": 24.791161688405563, "chrf_score": 47.4494096615441, "xcomet_score": 0.841123104095459, "xcomet_qe_score": 0.8171979188919067, "metricx_score": 5.165966510772705, "metricx_qe_score": 6.075621128082275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们开发了一个过滤模型来选择可行的脚本。", "metrics": {"bleu_score": 66.89681675410522, "chrf_score": 60.958833041179176, "xcomet_score": 0.985541582107544, "xcomet_qe_score": 0.9621707201004028, "metricx_score": 0.7335373759269714, "metricx_qe_score": 0.8991695046424866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为InstructGPT嵌入,并计算余弦相似性和相似度分数以衡量语义相似度。", "metrics": {"bleu_score": 72.56963883270627, "chrf_score": 72.652438129741, "xcomet_score": 0.8858673572540283, "xcomet_qe_score": 0.7361208200454712, "metricx_score": 1.930905818939209, "metricx_qe_score": 2.2132182121276855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们对包含目标约束关键字的脚本给予奖励。", "metrics": {"bleu_score": 58.61137788133827, "chrf_score": 52.22448413523928, "xcomet_score": 0.9063223600387573, "xcomet_qe_score": 0.8343249559402466, "metricx_score": 0.7408552765846252, "metricx_qe_score": 1.113478183746338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们仅在目标在目标集中得分最高时保留该脚本。", "metrics": {"bleu_score": 55.24483054780086, "chrf_score": 51.40649078333834, "xcomet_score": 0.8327559232711792, "xcomet_qe_score": 0.6403235197067261, "metricx_score": 3.5265393257141113, "metricx_qe_score": 5.164999485015869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用我们的方法,InstructZBT可以生成更高质量的脚本。", "metrics": {"bleu_score": 70.98232254187813, "chrf_score": 64.06283025396857, "xcomet_score": 0.8540607690811157, "xcomet_qe_score": 0.8311640024185181, "metricx_score": 3.6497256755828857, "metricx_qe_score": 4.229335784912109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实度方面都大大提高了规划能力。", "metrics": {"bleu_score": 89.11473434354988, "chrf_score": 86.47379748378687, "xcomet_score": 0.9282702207565308, "xcomet_qe_score": 0.9306887984275818, "metricx_score": 0.8196243047714233, "metricx_qe_score": 1.2446916103363037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于部署大型语言模型的成本高昂,因此有必要使更小、更专业的模型具备语言规划能力。", "metrics": {"bleu_score": 36.23226384175241, "chrf_score": 30.535658561974355, "xcomet_score": 0.9994208812713623, "xcomet_qe_score": 0.996235728263855, "metricx_score": 0.5226874351501465, "metricx_qe_score": 0.9461327791213989, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的重要步骤。", "metrics": {"bleu_score": 80.61898627027144, "chrf_score": 74.76670903477938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07551921904087067, "metricx_qe_score": 0.17079289257526398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究没有实现具体目标的规划,而手动数据集标注成本高昂。", "metrics": {"bleu_score": 42.665923524557556, "chrf_score": 35.25619074438116, "xcomet_score": 0.8702105283737183, "xcomet_qe_score": 0.893038272857666, "metricx_score": 1.2567025423049927, "metricx_qe_score": 1.6073206663131714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的想法,从大型语言模型中蒸馏约束语言规划数据集。", "metrics": {"bleu_score": 50.87096779647018, "chrf_score": 41.99079684198711, "xcomet_score": 0.8096791505813599, "xcomet_qe_score": 0.7346233129501343, "metricx_score": 3.983039617538452, "metricx_qe_score": 3.452484607696533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们应用我们的方法构建了一个约束语言规划数据集,称为CodeScript。", "metrics": {"bleu_score": 29.546199876902648, "chrf_score": 34.86182778288395, "xcomet_score": 0.8660626411437988, "xcomet_qe_score": 0.8311116695404053, "metricx_score": 1.4052375555038452, "metricx_qe_score": 1.6475012302398682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了55,000个具体目标和脚本。", "metrics": {"bleu_score": 48.42805510154529, "chrf_score": 57.224736959549546, "xcomet_score": 0.8327268362045288, "xcomet_qe_score": 0.9274643659591675, "metricx_score": 2.611783266067505, "metricx_qe_score": 1.4360195398330688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证集和测试集的质量,我们要求云端工人找到并修正错误样本。", "metrics": {"bleu_score": 51.682926806862916, "chrf_score": 46.772704363297905, "xcomet_score": 0.8677138686180115, "xcomet_qe_score": 0.8594188094139099, "metricx_score": 2.6514878273010254, "metricx_qe_score": 2.2767341136932373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了CodeScript的约束分布。", "metrics": {"bleu_score": 58.33510584342546, "chrf_score": 72.24911196939357, "xcomet_score": 0.9625097513198853, "xcomet_qe_score": 0.8983068466186523, "metricx_score": 1.2738232612609863, "metricx_qe_score": 2.105375289916992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现CodeScript在生成的具体目标中具有高多样性。", "metrics": {"bleu_score": 43.56840857621152, "chrf_score": 51.39216871200611, "xcomet_score": 0.9395216703414917, "xcomet_qe_score": 0.880957841873169, "metricx_score": 2.945089817047119, "metricx_qe_score": 4.007772445678711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用CodeScript,我们可以处理更小但更专业的模型进行约束语言规划。", "metrics": {"bleu_score": 36.33456213403636, "chrf_score": 33.89575690181107, "xcomet_score": 0.7139616012573242, "xcomet_qe_score": 0.7048925161361694, "metricx_score": 3.84031343460083, "metricx_qe_score": 3.785999298095703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现Antune在成本率上的TFI可以生成0的平方根。使用CodeScript,我们可以处理更小但更专业的模型进行约束语言规划。我们发现T-file函数在CodeScript上可以生成比大多数大型语言模型更高质量的脚本,这表明当在合适的数据集上进行适当训练时,较小的模型可以支持较大的模型。", "metrics": {"bleu_score": 34.16252923486743, "chrf_score": 44.08469753791624, "xcomet_score": 0.09296413511037827, "xcomet_qe_score": 0.26990580558776855, "metricx_score": 10.415447235107422, "metricx_qe_score": 10.32156753540039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了约束语言规划问题。", "metrics": {"bleu_score": 51.109970380326146, "chrf_score": 42.579016307796216, "xcomet_score": 0.8879767656326294, "xcomet_qe_score": 0.8463901877403259, "metricx_score": 2.0746877193450928, "metricx_qe_score": 3.441852569580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的约束语言规划能力,并开发了一种过生成过滤方法", "metrics": {"bleu_score": 36.51593872657706, "chrf_score": 32.01383401335143, "xcomet_score": 0.7625319957733154, "xcomet_qe_score": 0.7674061059951782, "metricx_score": 3.6767935752868652, "metricx_qe_score": 4.586847305297852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用于语言规划", "metrics": {"bleu_score": 1.1067684145058458, "chrf_score": 7.24878948219339, "xcomet_score": 0.21390573680400848, "xcomet_qe_score": 0.14937403798103333, "metricx_score": 14.037111282348633, "metricx_qe_score": 23.39766502380371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究。", "metrics": {"bleu_score": 0.0, "chrf_score": 5.142339970380121, "xcomet_score": 0.19746559858322144, "xcomet_qe_score": 0.1412922590970993, "metricx_score": 21.818111419677734, "metricx_qe_score": 24.543027877807617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您的时间。", "metrics": {"bleu_score": 18.094495256969623, "chrf_score": 15.088860187765768, "xcomet_score": 0.9894214272499084, "xcomet_qe_score": 0.9905821084976196, "metricx_score": 0.24051934480667114, "metricx_qe_score": 0.7719036936759949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中了解CodeScript的更多细节。", "metrics": {"bleu_score": 40.89587360719719, "chrf_score": 50.14074731808188, "xcomet_score": 0.9351253509521484, "xcomet_qe_score": 0.9384291172027588, "metricx_score": 1.99335515499115, "metricx_qe_score": 1.964079737663269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是朱恒。", "metrics": {"bleu_score": 20.164945583740657, "chrf_score": 10.704692891649412, "xcomet_score": 0.8338499069213867, "xcomet_qe_score": 0.8417406678199768, "metricx_score": 0.13201507925987244, "metricx_qe_score": 0.2137874960899353, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将展示我们的研究论文,题为《2003年卡诺命名实体标注器在2023年是否仍能良好运行?》", "metrics": {"bleu_score": 40.675050798179186, "chrf_score": 44.595360100751066, "xcomet_score": 0.7569344639778137, "xcomet_qe_score": 0.741364061832428, "metricx_score": 3.829732894897461, "metricx_qe_score": 3.8867151737213135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.997875452041626, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究论文探讨了使用命名实体识别任务(NER任务)进行泛化能力的问题。", "metrics": {"bleu_score": 30.598720167656328, "chrf_score": 34.15767322752302, "xcomet_score": 0.885495126247406, "xcomet_qe_score": 0.8647788166999817, "metricx_score": 1.9153616428375244, "metricx_qe_score": 3.801301956176758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用了近20年的卡诺2003年数据集来开发NER,这自然地引发了几个问题。", "metrics": {"bleu_score": 28.44514681214546, "chrf_score": 29.178261103997784, "xcomet_score": 0.7215309143066406, "xcomet_qe_score": 0.7414956092834473, "metricx_score": 5.964058876037598, "metricx_qe_score": 5.902386665344238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据?", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9989852905273438, "xcomet_qe_score": 0.9952034950256348, "metricx_score": 0.28112250566482544, "metricx_qe_score": 0.2982765734195709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新的标注器时,需要什么来实现良好的泛化能力?", "metrics": {"bleu_score": 70.92625431287067, "chrf_score": 67.45255367485343, "xcomet_score": 0.9996474981307983, "xcomet_qe_score": 0.9977083206176758, "metricx_score": 0.4671085476875305, "metricx_qe_score": 0.557368814945221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们发现泛化能力较差,导致这些模型性能下降的原因是什么?", "metrics": {"bleu_score": 21.678070047864907, "chrf_score": 21.121718742403463, "xcomet_score": 0.9904954433441162, "xcomet_qe_score": 0.9671280384063721, "metricx_score": 0.6028037071228027, "metricx_qe_score": 0.6905943155288696, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了卡诺++数据集。这是", "metrics": {"bleu_score": 46.51052535355049, "chrf_score": 34.94011288717261, "xcomet_score": 0.6947262287139893, "xcomet_qe_score": 0.721893310546875, "metricx_score": 5.575240135192871, "metricx_qe_score": 2.767662763595581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透社2020年新闻中收集并根据卡诺2003年标注规则进行标注的数据集。", "metrics": {"bleu_score": 22.15098940968627, "chrf_score": 22.867959724411985, "xcomet_score": 0.705883264541626, "xcomet_qe_score": 0.7065529823303223, "metricx_score": 4.622840881347656, "metricx_qe_score": 4.268275737762451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们在卡诺2003年数据集上微调了", "metrics": {"bleu_score": 7.1358064413992315, "chrf_score": 12.795674139320473, "xcomet_score": 0.5203930139541626, "xcomet_qe_score": 0.5349735021591187, "metricx_score": 8.393911361694336, "metricx_qe_score": 7.3968610763549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "20多个模型,并在两个数据集上使用F1分数评估", "metrics": {"bleu_score": 4.567211833282236, "chrf_score": 3.8820446645380335, "xcomet_score": 0.1389053612947464, "xcomet_qe_score": 0.14571909606456757, "metricx_score": 9.110536575317383, "metricx_qe_score": 12.750295639038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了每个模型的泛化能力。", "metrics": {"bleu_score": 12.956636129976808, "chrf_score": 20.53605863848676, "xcomet_score": 0.17791858315467834, "xcomet_qe_score": 0.16521601378917694, "metricx_score": 14.080228805541992, "metricx_qe_score": 17.516630172729492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,实现良好泛化需要什么?", "metrics": {"bleu_score": 34.128395574633934, "chrf_score": 27.97118177136938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2295304834842682, "metricx_qe_score": 0.3168780207633972, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要因素:", "metrics": {"bleu_score": 35.66921666813207, "chrf_score": 28.896918340730544, "xcomet_score": 0.9720809459686279, "xcomet_qe_score": 0.8769096732139587, "metricx_score": 0.6524062156677246, "metricx_qe_score": 1.071677803993225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "1. 模型架构。", "metrics": {"bleu_score": 46.19993369945709, "chrf_score": 35.642683475399004, "xcomet_score": 0.9580717086791992, "xcomet_qe_score": 0.8474416136741638, "metricx_score": 0.18244010210037231, "metricx_qe_score": 0.1616809219121933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,变压器模型通常能更好地泛化到新", "metrics": {"bleu_score": 17.093806425559606, "chrf_score": 12.967769888125877, "xcomet_score": 0.4101381003856659, "xcomet_qe_score": 0.4962727725505829, "metricx_score": 8.241216659545898, "metricx_qe_score": 2.5645153522491455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据。 2. 模型大小。", "metrics": {"bleu_score": 16.080471747592078, "chrf_score": 13.637377544703014, "xcomet_score": 0.304389625787735, "xcomet_qe_score": 0.3808097839355469, "metricx_score": 2.619596481323242, "metricx_qe_score": 4.022284507751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常较小的模型能带来更好的泛化能力。", "metrics": {"bleu_score": 36.31474833716426, "chrf_score": 29.95533726546078, "xcomet_score": 0.8471453189849854, "xcomet_qe_score": 0.8355712890625, "metricx_score": 2.9931697845458984, "metricx_qe_score": 2.084995985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "3. 微调示例数量。", "metrics": {"bleu_score": 4.467428945801653, "chrf_score": 9.987338562156852, "xcomet_score": 0.3934690058231354, "xcomet_qe_score": 0.16579091548919678, "metricx_score": 5.557507514953613, "metricx_qe_score": 12.082737922668457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调示例也能导致更好的泛化能力。", "metrics": {"bleu_score": 48.64319322446978, "chrf_score": 39.35232377019794, "xcomet_score": 0.9796880483627319, "xcomet_qe_score": 0.9075923562049866, "metricx_score": 0.7045124173164368, "metricx_qe_score": 0.9779775738716125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于下一个问题,导致某些模型性能下降的原因是什么?我们有两个假设。", "metrics": {"bleu_score": 38.45292792555755, "chrf_score": 31.678091733353497, "xcomet_score": 0.983185887336731, "xcomet_qe_score": 0.9819070100784302, "metricx_score": 0.5360596179962158, "metricx_qe_score": 0.6224416494369507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是适应性过拟合,即由于反复使用相同的测试集而引起的过拟合,通常表现为新测试集上的收益递减。", "metrics": {"bleu_score": 53.97030372211946, "chrf_score": 46.197105409345305, "xcomet_score": 0.9684817790985107, "xcomet_qe_score": 0.8923033475875854, "metricx_score": 2.432739019393921, "metricx_qe_score": 3.073955774307251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间偏移,即由于训练数据和测试数据之间的时间差距不断扩大而导致的性能下降。", "metrics": {"bleu_score": 55.28433387937018, "chrf_score": 50.78852628599023, "xcomet_score": 0.9502419233322144, "xcomet_qe_score": 0.9362363815307617, "metricx_score": 1.4962384700775146, "metricx_qe_score": 1.4654130935668945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于适应性过拟合,我们从右图中看到,最佳拟合线的梯度大于1,这", "metrics": {"bleu_score": 51.77172642610572, "chrf_score": 45.58190909087962, "xcomet_score": 0.6335679292678833, "xcomet_qe_score": 0.5654386878013611, "metricx_score": 5.874128818511963, "metricx_qe_score": 2.6247105598449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "意味着在卡诺2003年数据集上的每一次改进,在卡诺++数据集上都能得到超过一次的改进,即没有收益递减。", "metrics": {"bleu_score": 10.777245529804231, "chrf_score": 13.406635296208103, "xcomet_score": 0.6106923222541809, "xcomet_qe_score": 0.6475270390510559, "metricx_score": 5.184797286987305, "metricx_qe_score": 4.589253902435303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明适应性过拟合在这种情况下没有观察到。", "metrics": {"bleu_score": 46.55957504543378, "chrf_score": 38.40227610805727, "xcomet_score": 0.873185396194458, "xcomet_qe_score": 0.9240709543228149, "metricx_score": 2.0435357093811035, "metricx_qe_score": 2.137998342514038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,时间偏移呢?", "metrics": {"bleu_score": 16.51582159006904, "chrf_score": 17.361111111111107, "xcomet_score": 0.9208844900131226, "xcomet_qe_score": 0.9236891865730286, "metricx_score": 0.40093255043029785, "metricx_qe_score": 0.6789013147354126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,随着时间差距的扩大,性能确实会下降,这证实了我们的时间偏移假设。", "metrics": {"bleu_score": 8.942248078523033, "chrf_score": 13.341525221601538, "xcomet_score": 0.7655917406082153, "xcomet_qe_score": 0.6502957344055176, "metricx_score": 6.4071173667907715, "metricx_qe_score": 6.324616432189941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化能力,我们需要更好的模型架构、更大的模型大小以及更多的微调示例。", "metrics": {"bleu_score": 76.83885850728005, "chrf_score": 72.39662296694443, "xcomet_score": 0.9731632471084595, "xcomet_qe_score": 0.9423487186431885, "metricx_score": 0.4416837692260742, "metricx_qe_score": 0.43267035484313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素相互关联,不能只关注其中一个。", "metrics": {"bleu_score": 7.511002624637181, "chrf_score": 10.949888912447406, "xcomet_score": 0.9100812673568726, "xcomet_qe_score": 0.8790004253387451, "metricx_score": 0.629925012588501, "metricx_qe_score": 0.8562206029891968, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,性能下降是由时间偏移引起的,令人惊讶的是,并不是由适应性过拟合引起的,尽管卡诺2003年数据集已经使用超过20年。", "metrics": {"bleu_score": 41.60152285077816, "chrf_score": 31.657472833204736, "xcomet_score": 0.8024427890777588, "xcomet_qe_score": 0.7813363075256348, "metricx_score": 4.135763645172119, "metricx_qe_score": 3.709089517593384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们论文提出的问题,2003年卡诺标注器在2023年是否仍能运行?", "metrics": {"bleu_score": 43.22823074074484, "chrf_score": 38.43951791830271, "xcomet_score": 0.7612273693084717, "xcomet_qe_score": 0.7861499786376953, "metricx_score": 4.755220413208008, "metricx_qe_score": 4.142384052276611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案是肯定的。", "metrics": {"bleu_score": 67.80814773941113, "chrf_score": 55.183072785867324, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.32976317405700684, "metricx_qe_score": 0.7012989521026611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能呼吁更多关于如何改进模型泛化能力的研究。", "metrics": {"bleu_score": 72.120425527488, "chrf_score": 64.51117506010353, "xcomet_score": 0.8976545333862305, "xcomet_qe_score": 0.940030574798584, "metricx_score": 1.2603434324264526, "metricx_qe_score": 1.0007842779159546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果有任何问题,欢迎与我联系。", "metrics": {"bleu_score": 53.708439157269666, "chrf_score": 48.5933529726328, "xcomet_score": 0.9878675937652588, "xcomet_qe_score": 0.9707143902778625, "metricx_score": 0.28281810879707336, "metricx_qe_score": 0.29268351197242737, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9850989580154419, "xcomet_qe_score": 0.9753036499023438, "metricx_score": 0.186608225107193, "metricx_qe_score": 0.06603709608316422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将讨论我们在解决实体选择中间接引用表达方面的工作,其中我们引入了替代实体语料库。", "metrics": {"bleu_score": 21.92470516527494, "chrf_score": 16.724124087652115, "xcomet_score": 0.7722742557525635, "xcomet_qe_score": 0.7597562074661255, "metricx_score": 3.5440287590026855, "metricx_qe_score": 3.2109436988830566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是Jawad Hosseini,这是与Philip Radlinski、Sylvia Parity和Annie Lewis的合作项目。", "metrics": {"bleu_score": 6.802254824715724, "chrf_score": 50.06972049968329, "xcomet_score": 0.7087224125862122, "xcomet_qe_score": 0.74131840467453, "metricx_score": 3.966110944747925, "metricx_qe_score": 2.7873075008392334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请考虑这个替代问题:", "metrics": {"bleu_score": 45.180100180492246, "chrf_score": 34.71560846560846, "xcomet_score": 0.8932609558105469, "xcomet_qe_score": 0.8832759857177734, "metricx_score": 0.3326420187950134, "metricx_qe_score": 0.30451762676239014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是不是指“轻点对我”还是“我有感觉”?这里", "metrics": {"bleu_score": 5.406502668979588, "chrf_score": 4.365346671826255, "xcomet_score": 0.19382527470588684, "xcomet_qe_score": 0.14869554340839386, "metricx_score": 5.237491607666016, "metricx_qe_score": 3.5834317207336426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",用户想在这两首歌中选择一个,最", "metrics": {"bleu_score": 52.834357092814486, "chrf_score": 53.85143073703033, "xcomet_score": 0.5948466062545776, "xcomet_qe_score": 0.6168614625930786, "metricx_score": 9.177303314208984, "metricx_qe_score": 1.5876460075378418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "明显的方法是使用直接引用,例如说出歌曲“轻点对我”的名字或它的位置“第一个”。", "metrics": {"bleu_score": 46.50603995968451, "chrf_score": 36.28551172648257, "xcomet_score": 0.6159535646438599, "xcomet_qe_score": 0.5788930654525757, "metricx_score": 4.295492172241211, "metricx_qe_score": 4.768942832946777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但有时间接引用更合适,可以让对话更", "metrics": {"bleu_score": 11.071887991253549, "chrf_score": 13.463986437925143, "xcomet_score": 0.34707751870155334, "xcomet_qe_score": 0.1780271977186203, "metricx_score": 12.3145112991333, "metricx_qe_score": 10.84814739227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自然。这可能发生在用户记不", "metrics": {"bleu_score": 4.458083841280183, "chrf_score": 7.718958355150626, "xcomet_score": 0.41929420828819275, "xcomet_qe_score": 0.349290668964386, "metricx_score": 9.206510543823242, "metricx_qe_score": 7.17213249206543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "住歌曲的名字、发音太相似难以区分", "metrics": {"bleu_score": 24.329268691415297, "chrf_score": 23.51602105513822, "xcomet_score": 0.7164287567138672, "xcomet_qe_score": 0.4427666664123535, "metricx_score": 5.184960842132568, "metricx_qe_score": 6.973299980163574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",或用户想表达偏好时。", "metrics": {"bleu_score": 7.733019124048347, "chrf_score": 11.21844525588703, "xcomet_score": 0.9719264507293701, "xcomet_qe_score": 0.9729541540145874, "metricx_score": 2.9478228092193604, "metricx_qe_score": 1.5716501474380493, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是某些间接差异的例子,例如“较新的那首”或“不带活力歌曲”。 这", "metrics": {"bleu_score": 34.4366243041507, "chrf_score": 29.043562198024436, "xcomet_score": 0.4866567850112915, "xcomet_qe_score": 0.5212355852127075, "metricx_score": 6.871711730957031, "metricx_qe_score": 4.007366180419922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在对话系统中是一个重要问题,也是评估大型语言模型(LLM)实体理解能力的重要基准。我们不知道有", "metrics": {"bleu_score": 27.432192697988814, "chrf_score": 27.050458286947542, "xcomet_score": 0.6750278472900391, "xcomet_qe_score": 0.5459884405136108, "metricx_score": 5.806731224060059, "metricx_qe_score": 4.5537943840026855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "没有一个公开的大规模数据集来完成这个任务,所以我们使用众包标注收集了一个。", "metrics": {"bleu_score": 25.813089213367856, "chrf_score": 22.64481529254591, "xcomet_score": 0.8345575928688049, "xcomet_qe_score": 0.8177025318145752, "metricx_score": 2.68290376663208, "metricx_qe_score": 2.137314796447754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同领域:音乐、书籍和食谱。", "metrics": {"bleu_score": 71.34491809428187, "chrf_score": 61.46444256226865, "xcomet_score": 0.9999072551727295, "xcomet_qe_score": 0.9905967116355896, "metricx_score": 0.1999645233154297, "metricx_qe_score": 0.31799226999282837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通完成设置。", "metrics": {"bleu_score": 73.41087329408752, "chrf_score": 65.98538196580246, "xcomet_score": 0.8596484065055847, "xcomet_qe_score": 0.8180302977561951, "metricx_score": 2.6962547302246094, "metricx_qe_score": 4.135033130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通有三个对话框。", "metrics": {"bleu_score": 58.13189627146944, "chrf_score": 53.443169639544884, "xcomet_score": 0.839766263961792, "xcomet_qe_score": 0.6833667755126953, "metricx_score": 0.7207869291305542, "metricx_qe_score": 0.7576439380645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框中,鲍勃说:“记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 54.35700692115411, "chrf_score": 48.877789279963196, "xcomet_score": 0.8934242725372314, "xcomet_qe_score": 0.8828657269477844, "metricx_score": 1.3489147424697876, "metricx_qe_score": 0.8999765515327454, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从而设定了对话背景。", "metrics": {"bleu_score": 10.764345432696361, "chrf_score": 8.851655444505358, "xcomet_score": 0.8625693321228027, "xcomet_qe_score": 0.8342990875244141, "metricx_score": 3.0627455711364746, "metricx_qe_score": 3.220067262649536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话框中,爱丽丝说:“你是不是指‘轻点对我’还是‘我有感觉’?”", "metrics": {"bleu_score": 17.346491270779882, "chrf_score": 11.531352581920535, "xcomet_score": 0.631069004535675, "xcomet_qe_score": 0.6434534788131714, "metricx_score": 3.6329548358917236, "metricx_qe_score": 3.3115925788879395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8669623136520386, "xcomet_qe_score": 0.8554072380065918, "metricx_score": 1.420009732246399, "metricx_qe_score": 2.3006839752197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,鲍勃使用间接引用来选择这两个实体之一,例如“较新的那首”。", "metrics": {"bleu_score": 37.1368976663411, "chrf_score": 30.405004538131468, "xcomet_score": 0.8205873966217041, "xcomet_qe_score": 0.7227942943572998, "metricx_score": 4.048962593078613, "metricx_qe_score": 4.535750865936279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话框,但第三个由标注者填写。", "metrics": {"bleu_score": 59.37666661005449, "chrf_score": 53.59530851521642, "xcomet_score": 0.9046311378479004, "xcomet_qe_score": 0.76314377784729, "metricx_score": 1.0942469835281372, "metricx_qe_score": 1.6259493827819824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话框从每个领域的几个手动提示中选择。", "metrics": {"bleu_score": 64.48859985416348, "chrf_score": 56.93098638962003, "xcomet_score": 0.8409314155578613, "xcomet_qe_score": 0.733695924282074, "metricx_score": 1.0325461626052856, "metricx_qe_score": 1.7349486351013184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个,即替代问题,是这样生成的:", "metrics": {"bleu_score": 38.989389329183524, "chrf_score": 32.1976226387991, "xcomet_score": 0.9060807228088379, "xcomet_qe_score": 0.8869072198867798, "metricx_score": 1.0674251317977905, "metricx_qe_score": 1.1081092357635498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板“", "metrics": {"bleu_score": 61.62607099729587, "chrf_score": 56.23747935732135, "xcomet_score": 0.9579062461853027, "xcomet_qe_score": 0.945254921913147, "metricx_score": 0.2502274215221405, "metricx_qe_score": 0.1453426629304886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是不是指A还是B”", "metrics": {"bleu_score": 31.239399369202552, "chrf_score": 29.215581267820067, "xcomet_score": 0.9047527313232422, "xcomet_qe_score": 0.8980932235717773, "metricx_score": 1.1734312772750854, "metricx_qe_score": 1.0442125797271729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",其中A和B是来自维基百科的样本。", "metrics": {"bleu_score": 80.53122030796273, "chrf_score": 94.9566147882592, "xcomet_score": 0.9483360052108765, "xcomet_qe_score": 0.9568173885345459, "metricx_score": 1.2444730997085571, "metricx_qe_score": 1.279200553894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同采样方法:", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9983956813812256, "xcomet_qe_score": 1.0, "metricx_score": 0.16264688968658447, "metricx_qe_score": 0.2582128047943115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体变得越来越相似,通常更难进行歧义消除。", "metrics": {"bleu_score": 45.312014285371994, "chrf_score": 42.05534507308681, "xcomet_score": 0.8403813242912292, "xcomet_qe_score": 0.8002443313598633, "metricx_score": 4.4424214363098145, "metricx_qe_score": 5.496718406677246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀随机采样,", "metrics": {"bleu_score": 18.36028134946796, "chrf_score": 15.942760942760941, "xcomet_score": 0.8311265707015991, "xcomet_qe_score": 0.8106340169906616, "metricx_score": 2.012704610824585, "metricx_qe_score": 1.2656458616256714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个是当实体有相似标题时,例如两本书的名字都是《归来》。", "metrics": {"bleu_score": 7.0709933815314, "chrf_score": 12.417628949898152, "xcomet_score": 0.8534152507781982, "xcomet_qe_score": 0.8286539316177368, "metricx_score": 2.6017181873321533, "metricx_qe_score": 3.1051406860351562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三个是当它们在维基百科上有相似描述,", "metrics": {"bleu_score": 55.88489896990788, "chrf_score": 52.228707102113745, "xcomet_score": 0.9642133712768555, "xcomet_qe_score": 0.9465217590332031, "metricx_score": 0.8580552339553833, "metricx_qe_score": 0.7363326549530029, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,例如,当它们属于同一", "metrics": {"bleu_score": 4.280805579214919, "chrf_score": 5.635080645161291, "xcomet_score": 0.3451696038246155, "xcomet_qe_score": 0.447634220123291, "metricx_score": 10.14208984375, "metricx_qe_score": 11.745672225952148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类型或同一艺术家时。", "metrics": {"bleu_score": 17.121137673921417, "chrf_score": 17.43159735641203, "xcomet_score": 0.6238982677459717, "xcomet_qe_score": 0.5815155506134033, "metricx_score": 4.360074043273926, "metricx_qe_score": 5.616433143615723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向管理员展示这个替代问题时,他们知道这些实体的名字,但并不一定了解这些实体,", "metrics": {"bleu_score": 44.179378704110896, "chrf_score": 36.45562760379902, "xcomet_score": 0.7570648193359375, "xcomet_qe_score": 0.7340807914733887, "metricx_score": 4.768978118896484, "metricx_qe_score": 4.224899768829346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们向他们展示了一些关于两个实体的背景知识。", "metrics": {"bleu_score": 37.759043546748806, "chrf_score": 31.69349956461669, "xcomet_score": 0.8986811637878418, "xcomet_qe_score": 0.8293399810791016, "metricx_score": 1.104321002960205, "metricx_qe_score": 2.100560188293457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地向每个歌曲提供一个谷歌搜索链接,然后要求标注者至少听一些每首歌,并阅读关于每首歌的内容。", "metrics": {"bleu_score": 33.82533515219207, "chrf_score": 27.95228068289658, "xcomet_score": 0.7393956184387207, "xcomet_qe_score": 0.7434930801391602, "metricx_score": 3.2294254302978516, "metricx_qe_score": 2.2283265590667725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是歌曲“轻点对我”的谷歌搜索结果示例。", "metrics": {"bleu_score": 54.68017145144114, "chrf_score": 37.56168081656931, "xcomet_score": 0.8283709287643433, "xcomet_qe_score": 0.8137368559837341, "metricx_score": 1.2059440612792969, "metricx_qe_score": 1.1301331520080566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们显示维基百科上的某些背景文本。", "metrics": {"bleu_score": 54.731578685424054, "chrf_score": 46.598532329537825, "xcomet_score": 0.9787369966506958, "xcomet_qe_score": 0.917989194393158, "metricx_score": 0.7688936591148376, "metricx_qe_score": 1.2199801206588745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还显示它们的图片,以便标注者知道它们的样子。", "metrics": {"bleu_score": 29.54341006344013, "chrf_score": 26.010796549600983, "xcomet_score": 0.8691747784614563, "xcomet_qe_score": 0.8017458915710449, "metricx_score": 2.829686403274536, "metricx_qe_score": 2.77740740776062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注者选择这些实体之一,例如这里的第一个,并使用三个到五个间接引用表达来描述它们,", "metrics": {"bleu_score": 34.65095772223156, "chrf_score": 28.44272993291206, "xcomet_score": 0.8199809789657593, "xcomet_qe_score": 0.7649592161178589, "metricx_score": 2.5899641513824463, "metricx_qe_score": 2.8271121978759766, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“有钢琴音乐的那首”。", "metrics": {"bleu_score": 15.727800941615351, "chrf_score": 16.815026488266092, "xcomet_score": 0.9955822229385376, "xcomet_qe_score": 0.9624900221824646, "metricx_score": 1.6078240871429443, "metricx_qe_score": 1.285557508468628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集的一些示例", "metrics": {"bleu_score": 58.73934418709088, "chrf_score": 52.67381090554676, "xcomet_score": 0.9694437980651855, "xcomet_qe_score": 0.9807780981063843, "metricx_score": 0.20383280515670776, "metricx_qe_score": 0.5162206888198853, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如“没有字的那首,不是那首有12岁男孩的,虚构的,或来自阿塞拜疆的”等。", "metrics": {"bleu_score": 35.50313992138563, "chrf_score": 32.6300326045392, "xcomet_score": 0.6646126508712769, "xcomet_qe_score": 0.6001634001731873, "metricx_score": 2.77787184715271, "metricx_qe_score": 3.5668203830718994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "替代实体语料库包含三个领域的6,000个替代问题,并包含42,000个间接引用表达结果。", "metrics": {"bleu_score": 20.194534725070323, "chrf_score": 30.180159469137678, "xcomet_score": 0.6057412028312683, "xcomet_qe_score": 0.567833423614502, "metricx_score": 3.738250255584717, "metricx_qe_score": 3.4944510459899902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是使用T5XLARGE模型的结果摘要。", "metrics": {"bleu_score": 30.315070099566324, "chrf_score": 39.0438147577205, "xcomet_score": 0.9331282377243042, "xcomet_qe_score": 0.9407362341880798, "metricx_score": 2.933131694793701, "metricx_qe_score": 2.562617778778076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有与标注者完全相同的背景知识,准确率非常高,约为92%到95%。", "metrics": {"bleu_score": 42.67776234334732, "chrf_score": 40.90102327412439, "xcomet_score": 0.9051085710525513, "xcomet_qe_score": 0.9751747846603394, "metricx_score": 1.1680665016174316, "metricx_qe_score": 0.7890873551368713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这不现实。", "metrics": {"bleu_score": 28.49181887722137, "chrf_score": 23.71472002904075, "xcomet_score": 0.9991846084594727, "xcomet_qe_score": 0.9858999848365784, "metricx_score": 0.04951542243361473, "metricx_qe_score": 0.05270039662718773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型拥有部分重叠的背景知识,准确率在82%到87%之间,这更现实,", "metrics": {"bleu_score": 52.41456780893763, "chrf_score": 51.23099829763845, "xcomet_score": 0.9205024838447571, "xcomet_qe_score": 0.9307816028594971, "metricx_score": 1.7873759269714355, "metricx_qe_score": 1.6224406957626343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时", "metrics": {"bleu_score": 77.10792371922997, "chrf_score": 74.18063378409278, "xcomet_score": 0.9941446781158447, "xcomet_qe_score": 0.9908560514450073, "metricx_score": 0.37645384669303894, "metricx_qe_score": 0.5439862608909607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",准确率仅为60%,因此还有很大的改进空间。", "metrics": {"bleu_score": 24.638055159899686, "chrf_score": 27.892812033323562, "xcomet_score": 0.8515225648880005, "xcomet_qe_score": 0.848427414894104, "metricx_score": 7.500865459442139, "metricx_qe_score": 10.286253929138184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还证明了模型具有领域泛化能力。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 44.723889306558185, "xcomet_score": 0.9255028367042542, "xcomet_qe_score": 0.9132012724876404, "metricx_score": 0.5445807576179504, "metricx_qe_score": 0.6280920505523682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是特伦托大学和布鲁诺·凯瑟尔基金会的萨拉·帕皮,我将简要介绍一下我们与马特奥·内格里和马可·图尔基合作的一篇论文《注意力作为同时语音翻译的指南》。", "metrics": {"bleu_score": 36.516934459512235, "chrf_score": 26.52746195738029, "xcomet_score": 0.7178232669830322, "xcomet_qe_score": 0.7194865942001343, "metricx_score": 4.536903381347656, "metricx_qe_score": 3.7402663230895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同时语音翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.8608854413032532, "xcomet_qe_score": 0.8539649248123169, "metricx_score": 0.5060197114944458, "metricx_qe_score": 0.22112929821014404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时语音翻译(SimulST)是指将口语实时翻译成另一门语言的文本,实现跨语言交流。", "metrics": {"bleu_score": 49.13567044491743, "chrf_score": 47.37819049736678, "xcomet_score": 0.9725610017776489, "xcomet_qe_score": 0.9625173807144165, "metricx_score": 2.068082094192505, "metricx_qe_score": 2.3804473876953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前同时语音翻译模型存在哪些问题?", "metrics": {"bleu_score": 19.345299022826193, "chrf_score": 15.68681528760261, "xcomet_score": 0.8288103342056274, "xcomet_qe_score": 0.845038115978241, "metricx_score": 0.9975522756576538, "metricx_qe_score": 0.7655915021896362, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常需要训练特定的架构,引入额外的优化模块。例如,", "metrics": {"bleu_score": 31.705047203112663, "chrf_score": 27.44333695637898, "xcomet_score": 0.8105167150497437, "xcomet_qe_score": 0.7743396759033203, "metricx_score": 1.150880217552185, "metricx_qe_score": 1.489956259727478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "训练过程复杂而漫长,涉及不同的优化目标", "metrics": {"bleu_score": 32.24121115238373, "chrf_score": 26.43242645497509, "xcomet_score": 0.9551267623901367, "xcomet_qe_score": 0.8757685422897339, "metricx_score": 1.2601346969604492, "metricx_qe_score": 1.94500732421875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",并且需要训练和维护多个模型以达到不同的延迟", "metrics": {"bleu_score": 57.725241430068074, "chrf_score": 54.55027033025313, "xcomet_score": 0.9495559930801392, "xcomet_qe_score": 0.9266587495803833, "metricx_score": 1.688504695892334, "metricx_qe_score": 1.9355597496032715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水平,如训练一个平均延迟1秒的模型,另一个延迟2秒的模型,以此类推。", "metrics": {"bleu_score": 53.13423501066469, "chrf_score": 42.10596263287126, "xcomet_score": 0.6194537878036499, "xcomet_qe_score": 0.5213528871536255, "metricx_score": 3.061526298522949, "metricx_qe_score": 3.9579007625579834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么我们的解决方案是什么?", "metrics": {"bleu_score": 84.46319809857219, "chrf_score": 83.69464817082437, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.01612722873687744, "metricx_qe_score": 0.2782573699951172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用现有的离线语音翻译模型,无需重新训练或采用特定架构。我们使用一个模型来", "metrics": {"bleu_score": 28.538917009965722, "chrf_score": 25.644681951623948, "xcomet_score": 0.5580927133560181, "xcomet_qe_score": 0.6847224831581116, "metricx_score": 6.202702045440674, "metricx_qe_score": 5.266387462615967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "处理所有延迟水平,并通过特定参数控制延迟。我们利用", "metrics": {"bleu_score": 27.01725905301301, "chrf_score": 24.230272399022542, "xcomet_score": 0.32862016558647156, "xcomet_qe_score": 0.1874597817659378, "metricx_score": 6.94191837310791, "metricx_qe_score": 4.2128095626831055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入和文本输出之间的注意力机制(即", "metrics": {"bleu_score": 57.160257835370636, "chrf_score": 61.09169780150931, "xcomet_score": 0.5953578948974609, "xcomet_qe_score": 0.48937955498695374, "metricx_score": 8.467368125915527, "metricx_qe_score": 6.953545093536377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "交叉注意力机制)已经学到的知识。", "metrics": {"bleu_score": 31.920297876188712, "chrf_score": 28.365507253901654, "xcomet_score": 0.3915223181247711, "xcomet_qe_score": 0.4051054120063782, "metricx_score": 6.818890571594238, "metricx_qe_score": 9.536468505859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出编码器-解码器注意力(EDAT),这是一种策略,我们根据注意力指向决定是否输出部分翻译。", "metrics": {"bleu_score": 63.01902664822488, "chrf_score": 52.79406477148167, "xcomet_score": 0.7072233557701111, "xcomet_qe_score": 0.7085593938827515, "metricx_score": 4.062415599822998, "metricx_qe_score": 4.933837413787842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力不集中,即总和低于某个阈值α,指向较少的语音帧,意味着接收的信息足够稳定", "metrics": {"bleu_score": 38.43803166341889, "chrf_score": 33.99770422844861, "xcomet_score": 0.706786036491394, "xcomet_qe_score": 0.5203765034675598, "metricx_score": 5.9805779457092285, "metricx_qe_score": 6.387147426605225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",则输出一个词。例如,如果我们接收一个包含“我要谈论”的语音片段,我们的模型预测德语翻译,我们观察交叉注意力权重,会看到前两个词指向最早接收的语音帧,而最后一个词指向最后接收的语音帧,即λ语音帧。", "metrics": {"bleu_score": 41.061051239236335, "chrf_score": 29.7946810581026, "xcomet_score": 0.3785562515258789, "xcomet_qe_score": 0.22829371690750122, "metricx_score": 6.428685188293457, "metricx_qe_score": 7.39677619934082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被输出,而由于交叉注意力总和高于阈值α,我们不会输出最后一个词,而是等待下一个语音片段。", "metrics": {"bleu_score": 49.65375275183049, "chrf_score": 41.13576520911808, "xcomet_score": 0.7775884866714478, "xcomet_qe_score": 0.7627027034759521, "metricx_score": 1.634221076965332, "metricx_qe_score": 1.9308500289916992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续接收下一个语音片段,模型预测另外三个词,观察交叉注意力权重,会发现没有词指向最后的λ语音帧。", "metrics": {"bleu_score": 40.88167338451539, "chrf_score": 36.120545269866746, "xcomet_score": 0.6478802561759949, "xcomet_qe_score": 0.6531490087509155, "metricx_score": 3.3163375854492188, "metricx_qe_score": 3.5576562881469727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将", "metrics": {"bleu_score": 29.472584782019705, "chrf_score": 26.455018219457934, "xcomet_score": 0.841911792755127, "xcomet_qe_score": 0.8360384106636047, "metricx_score": 5.485178470611572, "metricx_qe_score": 5.510756969451904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "被输出。 在主要结果中,我们将同时语音翻译结果绘制在图表上,蓝色一侧测量翻译质量,平均滞后是延迟的度量,我们还考虑计算感知平均滞后,包括模型预测输出的计算", "metrics": {"bleu_score": 23.063258119146475, "chrf_score": 20.480515198131375, "xcomet_score": 0.2764888107776642, "xcomet_qe_score": 0.1231599897146225, "metricx_score": 9.806900978088379, "metricx_qe_score": 10.164531707763672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "时间。我们希望我们的曲线在这个图表中尽可能高,", "metrics": {"bleu_score": 26.943533707378236, "chrf_score": 25.542952540235508, "xcomet_score": 0.7202495336532593, "xcomet_qe_score": 0.6672818660736084, "metricx_score": 5.212615966796875, "metricx_qe_score": 5.325139999389648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时也希望它们向左移动。", "metrics": {"bleu_score": 68.31579242909528, "chrf_score": 66.77682733422068, "xcomet_score": 0.9827525615692139, "xcomet_qe_score": 0.9589202404022217, "metricx_score": 0.7562371492385864, "metricx_qe_score": 1.0575345754623413, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们与也适用于离线模型的适当策略(即湿键策略和局部一致性)进行比较,", "metrics": {"bleu_score": 37.516819704227494, "chrf_score": 24.1475023840552, "xcomet_score": 0.6895949840545654, "xcomet_qe_score": 0.6442582607269287, "metricx_score": 6.881533622741699, "metricx_qe_score": 7.650981426239014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还与专门针对同时语音翻译设计的最新架构进行比较。", "metrics": {"bleu_score": 37.814147274013926, "chrf_score": 31.48275172974814, "xcomet_score": 0.9253807067871094, "xcomet_qe_score": 0.7677775621414185, "metricx_score": 1.4903666973114014, "metricx_qe_score": 2.300840377807617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是德国同时语音翻译策略的所有结果,我们", "metrics": {"bleu_score": 24.830148193808903, "chrf_score": 24.568422911979656, "xcomet_score": 0.7295045256614685, "xcomet_qe_score": 0.729375958442688, "metricx_score": 6.7954840660095215, "metricx_qe_score": 2.784254312515259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到ADAT超越了所有应用于离线模型的策略,因为曲线向左移动。", "metrics": {"bleu_score": 62.418866968013226, "chrf_score": 52.98171240647258, "xcomet_score": 0.9451004266738892, "xcomet_qe_score": 0.872315526008606, "metricx_score": 4.075348854064941, "metricx_qe_score": 5.3154706954956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果考虑实际经过的时间或计算感知时间,ADAT是最快的策略。", "metrics": {"bleu_score": 34.44165356717402, "chrf_score": 29.629640339166734, "xcomet_score": 0.823320746421814, "xcomet_qe_score": 0.7906873226165771, "metricx_score": 4.494969844818115, "metricx_qe_score": 4.6030473709106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9973729848861694, "xcomet_qe_score": 0.974124014377594, "metricx_score": 0.1322653442621231, "metricx_qe_score": 0.20905639231204987, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源发布了代码、模型和同时输出,以促进我们工作的可复现性。", "metrics": {"bleu_score": 32.31220503735793, "chrf_score": 31.713634705647465, "xcomet_score": 0.8647072315216064, "xcomet_qe_score": 0.8563243746757507, "metricx_score": 1.2582579851150513, "metricx_qe_score": 1.6167364120483398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家的关注。", "metrics": {"bleu_score": 36.55552228545123, "chrf_score": 49.96037865158913, "xcomet_score": 0.9990874528884888, "xcomet_qe_score": 0.9970511198043823, "metricx_score": 0.5129700303077698, "metricx_qe_score": 0.4527073800563812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Ying,我和我的同事 Zhiyang 将向大家展示我们关于“多改进:通过指令调优改进多模型串行短学习”的研究。", "metrics": {"bleu_score": 37.43303797088956, "chrf_score": 36.3033921822259, "xcomet_score": 0.616401731967926, "xcomet_qe_score": 0.6821646690368652, "metricx_score": 5.469855308532715, "metricx_qe_score": 5.439277172088623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索新的学习范式,以高效的方式重用预训练语言模型来处理不同的下游任务。", "metrics": {"bleu_score": 57.40066273892518, "chrf_score": 50.36048157198042, "xcomet_score": 0.9553542137145996, "xcomet_qe_score": 0.8786522150039673, "metricx_score": 0.9245256781578064, "metricx_qe_score": 1.806970477104187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优使大型语言模型能够通过遵循自然指令以零样本的方式执行未见过的任务。", "metrics": {"bleu_score": 50.10137880396583, "chrf_score": 42.886790186779436, "xcomet_score": 0.8266419172286987, "xcomet_qe_score": 0.7594197988510132, "metricx_score": 1.9973564147949219, "metricx_qe_score": 3.623983144760132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前关于指令调优的大部分工作都集中在改进语言仅限任务的零样本性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 37.33137409339886, "chrf_score": 34.21304482755327, "xcomet_score": 0.8722531795501709, "xcomet_qe_score": 0.726708710193634, "metricx_score": 1.7138571739196777, "metricx_qe_score": 2.2775304317474365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想调查多模态预训练模型上的指令调优是否能实际提高对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 40.00784698419303, "chrf_score": 33.741532046533834, "xcomet_score": 0.8671613931655884, "xcomet_qe_score": 0.7613025903701782, "metricx_score": 1.628103256225586, "metricx_qe_score": 1.649550199508667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们研究当时,我们发现 NLP 和多模态之间指令数据集的可用性存在显著差异。", "metrics": {"bleu_score": 32.32303289624542, "chrf_score": 27.34147241959075, "xcomet_score": 0.8618323802947998, "xcomet_qe_score": 0.8403851985931396, "metricx_score": 1.0512189865112305, "metricx_qe_score": 1.2855908870697021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过 1,600 个仅限语言的指令任务,", "metrics": {"bleu_score": 44.80304273880272, "chrf_score": 50.26760129804959, "xcomet_score": 0.8173074126243591, "xcomet_qe_score": 0.6661176085472107, "metricx_score": 0.8816978931427002, "metricx_qe_score": 1.6608208417892456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大规模公开可用的多模态指令任务。", "metrics": {"bleu_score": 56.600498033020294, "chrf_score": 51.85818548915307, "xcomet_score": 0.9029275178909302, "xcomet_qe_score": 0.8110454082489014, "metricx_score": 1.6300890445709229, "metricx_qe_score": 2.2274293899536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这促使我们构建一个多模态指令调优数据集。", "metrics": {"bleu_score": 63.336866679312486, "chrf_score": 56.4879205039388, "xcomet_score": 0.9773157835006714, "xcomet_qe_score": 0.9684963226318359, "metricx_score": 0.9151309728622437, "metricx_qe_score": 0.7606804966926575, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示 MultiInstruct,第一个多模态指令调优基准数据集,它包含 62 个多样化的多模态任务,涵盖 10 个广泛类别。", "metrics": {"bleu_score": 43.80989754525269, "chrf_score": 47.612928886917885, "xcomet_score": 0.8029531836509705, "xcomet_qe_score": 0.7864252924919128, "metricx_score": 1.7113932371139526, "metricx_qe_score": 2.1362156867980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来源于 21 个现有的开源数据集,每个任务配备 5 个专家编写的指令。", "metrics": {"bleu_score": 70.03356553209694, "chrf_score": 63.16680381164318, "xcomet_score": 0.9878497123718262, "xcomet_qe_score": 0.9787125587463379, "metricx_score": 0.9683594107627869, "metricx_qe_score": 1.478614091873169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们提出的数据集上研究多模态指令调优,我们采用 OFA,一个统一的多模态预训练模型,作为我们的基础模型。", "metrics": {"bleu_score": 68.43994412431653, "chrf_score": 68.4152352460368, "xcomet_score": 0.9543659687042236, "xcomet_qe_score": 0.8863147497177124, "metricx_score": 1.5178171396255493, "metricx_qe_score": 2.1222946643829346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFA 使用统一的词汇表来表示语言、图像令牌和边界框的坐标。", "metrics": {"bleu_score": 56.74773954614978, "chrf_score": 50.92614531953855, "xcomet_score": 0.8938817381858826, "xcomet_qe_score": 0.7835202217102051, "metricx_score": 4.434442043304443, "metricx_qe_score": 4.6365509033203125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们展示一些来自我们多指令数据集的示例实例。为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 55.74194606069849, "chrf_score": 40.11617484308642, "xcomet_score": 0.7993263602256775, "xcomet_qe_score": 0.7675685882568359, "metricx_score": 1.9222297668457031, "metricx_qe_score": 1.9907169342041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循 OFA 的方法,将所有任务制定为统一的序列到序列格式,其中", "metrics": {"bleu_score": 62.418866968013226, "chrf_score": 60.551115147073766, "xcomet_score": 0.7302300930023193, "xcomet_qe_score": 0.7253342270851135, "metricx_score": 3.307814121246338, "metricx_qe_score": 2.7777438163757324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框在相同的令牌空间中表示。", "metrics": {"bleu_score": 62.8292377306007, "chrf_score": 58.46686852035715, "xcomet_score": 0.876563549041748, "xcomet_qe_score": 0.843334436416626, "metricx_score": 4.788868427276611, "metricx_qe_score": 4.683666706085205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好了,现在我将讨论多模态指令调优。", "metrics": {"bleu_score": 37.709297891717654, "chrf_score": 31.15595597213244, "xcomet_score": 0.9207981824874878, "xcomet_qe_score": 0.906872570514679, "metricx_score": 0.7834247350692749, "metricx_qe_score": 0.7164303660392761, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用 9 组中的 53 个任务进行训练,每个任务采样 10,000 个实例。", "metrics": {"bleu_score": 60.72038110536692, "chrf_score": 58.03884418348487, "xcomet_score": 0.969438910484314, "xcomet_qe_score": 0.951911449432373, "metricx_score": 1.0273553133010864, "metricx_qe_score": 1.49457848072052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在测试中,我们保留整个常识推理组用于测试,并从 VQA 和杂项组额外选择五个任务。", "metrics": {"bleu_score": 36.47576867050242, "chrf_score": 32.19447155058397, "xcomet_score": 0.719905436038971, "xcomet_qe_score": 0.7002737522125244, "metricx_score": 3.6508100032806396, "metricx_qe_score": 3.8815300464630127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务的测试分割中的所有实例。", "metrics": {"bleu_score": 64.90250359551654, "chrf_score": 55.76765321978275, "xcomet_score": 0.794039249420166, "xcomet_qe_score": 0.808186948299408, "metricx_score": 0.9813355207443237, "metricx_qe_score": 1.5871120691299438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令的测试分割中随机采样 20 个任务作为 NLP 的未见任务。", "metrics": {"bleu_score": 63.09659606023657, "chrf_score": 58.01785848000883, "xcomet_score": 0.7027578949928284, "xcomet_qe_score": 0.6265551447868347, "metricx_score": 2.8174405097961426, "metricx_qe_score": 3.528249740600586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的 OFA 大模型作为基础模型。", "metrics": {"bleu_score": 86.06031405392808, "chrf_score": 81.97230684018221, "xcomet_score": 0.9131981134414673, "xcomet_qe_score": 0.9546740651130676, "metricx_score": 1.34676194190979, "metricx_qe_score": 2.2853541374206543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们混合所有任务的所有实例。", "metrics": {"bleu_score": 74.08842640893447, "chrf_score": 63.891815530865145, "xcomet_score": 0.8026423454284668, "xcomet_qe_score": 0.8263124823570251, "metricx_score": 0.8326014280319214, "metricx_qe_score": 1.4236559867858887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与五个指令模板中的一个组合。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9054250717163086, "xcomet_qe_score": 0.8633378148078918, "metricx_score": 1.5055538415908813, "metricx_qe_score": 1.778053641319275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每个任务的测试中,我们总共进行五个实验,通过使用每个实验中的五个指令之一来评估", "metrics": {"bleu_score": 33.162254559136926, "chrf_score": 29.34700789212194, "xcomet_score": 0.8291763067245483, "xcomet_qe_score": 0.811515748500824, "metricx_score": 3.1130433082580566, "metricx_qe_score": 3.159536838531494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型。我们报告所有五个实验中的平均值、最大性能和性能的标准偏差。", "metrics": {"bleu_score": 28.80086932851551, "chrf_score": 23.68980057499308, "xcomet_score": 0.532874584197998, "xcomet_qe_score": 0.534148097038269, "metricx_score": 3.9945228099823, "metricx_qe_score": 4.5220842361450195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模型分类任务,我们报告准确率。如果", "metrics": {"bleu_score": 63.336866679312486, "chrf_score": 58.747388590999826, "xcomet_score": 0.7735463380813599, "xcomet_qe_score": 0.7686869502067566, "metricx_score": 3.2944211959838867, "metricx_qe_score": 0.7282773852348328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是多模型生成任务,我们报告根 L。对于 RP 任务,我们也报告根 jl,并且", "metrics": {"bleu_score": 32.780573471102954, "chrf_score": 23.478522166801643, "xcomet_score": 0.4268779158592224, "xcomet_qe_score": 0.40583336353302, "metricx_score": 11.77288818359375, "metricx_qe_score": 9.646783828735352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标称为敏感度,它", "metrics": {"bleu_score": 64.77458735605244, "chrf_score": 62.5345946330063, "xcomet_score": 0.7918300628662109, "xcomet_qe_score": 0.7107690572738647, "metricx_score": 4.129632472991943, "metricx_qe_score": 0.7684168815612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "测量模型在指令措辞略有变化时是否能一致地为相同任务产生相同输出。", "metrics": {"bleu_score": 31.346329805948, "chrf_score": 25.316965085831907, "xcomet_score": 0.9502097368240356, "xcomet_qe_score": 0.9484913945198059, "metricx_score": 5.402430534362793, "metricx_qe_score": 5.522104740142822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提高 OFA 在场景多模型任务上的性能。", "metrics": {"bleu_score": 49.40287800412559, "chrf_score": 46.30761053199384, "xcomet_score": 0.8814994096755981, "xcomet_qe_score": 0.8884139060974121, "metricx_score": 2.4132463932037354, "metricx_qe_score": 2.3590688705444336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习可以受益于指令调优。", "metrics": {"bleu_score": 73.42150184891982, "chrf_score": 67.13895082373344, "xcomet_score": 0.9494084119796753, "xcomet_qe_score": 0.7283151745796204, "metricx_score": 1.5469399690628052, "metricx_qe_score": 2.13948655128479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里我们可以看到,随着任务数量的增加,模型达到更高的性能,同时敏感度降低。", "metrics": {"bleu_score": 46.40957624825091, "chrf_score": 40.7314334135562, "xcomet_score": 0.9618300199508667, "xcomet_qe_score": 0.9763067960739136, "metricx_score": 1.6692408323287964, "metricx_qe_score": 2.2264931201934814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一个实验,", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 39.39127110257559, "xcomet_score": 0.9774124622344971, "xcomet_qe_score": 0.9565337896347046, "metricx_score": 0.34012168645858765, "metricx_qe_score": 0.33167219161987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用一个指令与五个指令进行比较。", "metrics": {"bleu_score": 14.62806365365753, "chrf_score": 16.922398258220273, "xcomet_score": 0.9491822719573975, "xcomet_qe_score": 0.8449200987815857, "metricx_score": 1.1729333400726318, "metricx_qe_score": 2.994428873062134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多指令可以提高模型的总体性能并显著降低其敏感度。", "metrics": {"bleu_score": 50.729584892379506, "chrf_score": 43.73150184385461, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6710911989212036, "metricx_qe_score": 0.7569581270217896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这展示了不同的微调策略对模型敏感度的影响。", "metrics": {"bleu_score": 72.98378378464027, "chrf_score": 65.28325511491146, "xcomet_score": 0.9754297733306885, "xcomet_qe_score": 0.9701192378997803, "metricx_score": 0.9737162590026855, "metricx_qe_score": 1.302918791770935, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,通过从自然指令数据集进行迁移学习,模型可以达到比原始 OFA 模型更好的敏感度。", "metrics": {"bleu_score": 43.367232540682025, "chrf_score": 37.36415491352964, "xcomet_score": 0.965980052947998, "xcomet_qe_score": 0.9342350959777832, "metricx_score": 1.5798476934432983, "metricx_qe_score": 2.545124053955078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行迁移学习可以帮助 OFA 在 Nitro Instruct 数据集上达到更好的性能。", "metrics": {"bleu_score": 54.15586848905436, "chrf_score": 46.50368456493886, "xcomet_score": 0.8879135847091675, "xcomet_qe_score": 0.7306622862815857, "metricx_score": 5.667693614959717, "metricx_qe_score": 6.429920196533203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们提出了第一个大规模多模型指令调优数据集。我们显著提高了 OFA 的零样本能力,并探索了不同的迁移学习技术并展示了它们的益处。", "metrics": {"bleu_score": 62.879067197859214, "chrf_score": 60.116313863417744, "xcomet_score": 0.8251075744628906, "xcomet_qe_score": 0.7666196823120117, "metricx_score": 2.590226888656616, "metricx_qe_score": 3.3581035137176514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标称为敏感度。", "metrics": {"bleu_score": 57.52730628627205, "chrf_score": 54.983770864124295, "xcomet_score": 0.9709914922714233, "xcomet_qe_score": 0.9727061986923218, "metricx_score": 0.5468370318412781, "metricx_qe_score": 0.7084141373634338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "再者,我们正在收集一个更庞大的多模态指令调优数据集,包含大约 150 个额外的变体语言任务,我们将很快发布。", "metrics": {"bleu_score": 45.421594703046196, "chrf_score": 41.150299936649716, "xcomet_score": 0.7696667909622192, "xcomet_qe_score": 0.7826062440872192, "metricx_score": 2.20819091796875, "metricx_qe_score": 2.453700065612793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据和模型的二维码。", "metrics": {"bleu_score": 80.52253761904356, "chrf_score": 72.34299520932606, "xcomet_score": 0.9889544248580933, "xcomet_qe_score": 0.9169533848762512, "metricx_score": 0.3964334726333618, "metricx_qe_score": 0.572709858417511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.974276065826416, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Kostav Sinha,很高兴欢迎大家参加我们关于ACL 2023论文的演讲,题目是", "metrics": {"bleu_score": 63.16150577092891, "chrf_score": 73.28422477635156, "xcomet_score": 0.6897674798965454, "xcomet_qe_score": 0.6288416385650635, "metricx_score": 4.816346168518066, "metricx_qe_score": 5.284846305847168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型可接受性判断不总是对上下文稳健的》。这是一", "metrics": {"bleu_score": 48.63224683737163, "chrf_score": 39.80315286530939, "xcomet_score": 0.666288435459137, "xcomet_qe_score": 0.6486778855323792, "metricx_score": 7.619544506072998, "metricx_qe_score": 6.568884372711182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "项与John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams的合作研究。", "metrics": {"bleu_score": 44.411909449840145, "chrf_score": 79.33595631355443, "xcomet_score": 0.6427671313285828, "xcomet_qe_score": 0.586613655090332, "metricx_score": 5.491621971130371, "metricx_qe_score": 5.121659755706787, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们重新审视了最小对范式。", "metrics": {"bleu_score": 53.0935466304407, "chrf_score": 48.22483086801972, "xcomet_score": 0.9683331251144409, "xcomet_qe_score": 0.888993501663208, "metricx_score": 1.244659185409546, "metricx_qe_score": 1.318211555480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小对范式基本上是在可接受性判断的基础上评估语言模型,这些判断", "metrics": {"bleu_score": 50.37255667587404, "chrf_score": 48.10593479923864, "xcomet_score": 0.8755109310150146, "xcomet_qe_score": 0.7689638137817383, "metricx_score": 6.015051364898682, "metricx_qe_score": 2.852109909057617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以包括语法性,比如BLIMP、语法GEM或在刻板印象方面的可接受性,例如Krauss对。", "metrics": {"bleu_score": 31.06302495244677, "chrf_score": 20.745440285176546, "xcomet_score": 0.4523773193359375, "xcomet_qe_score": 0.48757487535476685, "metricx_score": 6.075890064239502, "metricx_qe_score": 6.483738899230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最小对范式中,评估语言模型的典型方法是展示一个可接受的句子或一个语法句,然后展示一个不可接受的句子或一个不语法句。", "metrics": {"bleu_score": 52.615970055483714, "chrf_score": 49.41946702207714, "xcomet_score": 0.7360705137252808, "xcomet_qe_score": 0.7224855422973633, "metricx_score": 2.3407182693481445, "metricx_qe_score": 3.3295130729675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "整个模型的过程基本上给可接受的句子分配更高的概率。", "metrics": {"bleu_score": 27.274191069381917, "chrf_score": 24.0194969448255, "xcomet_score": 0.8521867990493774, "xcomet_qe_score": 0.7458465099334717, "metricx_score": 3.8080761432647705, "metricx_qe_score": 3.820274829864502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP管道基本无法让我们评估模型对较长句子的接受程度。", "metrics": {"bleu_score": 73.72197612580702, "chrf_score": 70.26845185465875, "xcomet_score": 0.9207577705383301, "xcomet_qe_score": 0.8107576370239258, "metricx_score": 1.729253888130188, "metricx_qe_score": 3.4916114807128906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型出现了越来越长的", "metrics": {"bleu_score": 29.278700698052127, "chrf_score": 23.534472305587855, "xcomet_score": 0.7462601065635681, "xcomet_qe_score": 0.7757354974746704, "metricx_score": 5.9170145988464355, "metricx_qe_score": 5.101223468780518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文窗口。因此,我们", "metrics": {"bleu_score": 3.0196376639848626, "chrf_score": 10.803286220736455, "xcomet_score": 0.15928994119167328, "xcomet_qe_score": 0.1393934190273285, "metricx_score": 19.247161865234375, "metricx_qe_score": 18.906314849853516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "重新审视了", "metrics": {"bleu_score": 0.24729105095220405, "chrf_score": 7.165318851431647, "xcomet_score": 0.1380453109741211, "xcomet_qe_score": 0.13593308627605438, "metricx_score": 13.762127876281738, "metricx_qe_score": 20.18882942199707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43476197123527527, "xcomet_qe_score": 0.15987929701805115, "metricx_score": 2.4095587730407715, "metricx_qe_score": 5.211564064025879, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据集本身,然后通过选择可接受或不可接受的句子从这些数据集中重新创建句子。", "metrics": {"bleu_score": 43.053946570017494, "chrf_score": 44.106903672921845, "xcomet_score": 0.5465708374977112, "xcomet_qe_score": 0.29873499274253845, "metricx_score": 6.175436973571777, "metricx_qe_score": 8.71200942993164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们从BLIMP数据集的附加岛案例中选择了一个典型的语法性对。", "metrics": {"bleu_score": 34.67904837040997, "chrf_score": 25.400293762965337, "xcomet_score": 0.7229002714157104, "xcomet_qe_score": 0.7468785047531128, "metricx_score": 3.5772440433502197, "metricx_qe_score": 3.619070053100586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是,为了重新创建更长的序列,我们", "metrics": {"bleu_score": 21.70990792858364, "chrf_score": 26.114430819621536, "xcomet_score": 0.2645668685436249, "xcomet_qe_score": 0.1487736999988556, "metricx_score": 12.102763175964355, "metricx_qe_score": 8.237343788146973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从附加岛中提取语法句子,并将其作为前缀添加到可接受的查询和不可接受的查询中。我们", "metrics": {"bleu_score": 77.46629754412453, "chrf_score": 59.49623445219062, "xcomet_score": 0.5462684631347656, "xcomet_qe_score": 0.41870883107185364, "metricx_score": 5.607680320739746, "metricx_qe_score": 4.036284923553467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也可以通过从相同的匹配中选择不可接受的句子来做到这一点,这也可以用于测试模型的可接受性。", "metrics": {"bleu_score": 72.7061023541805, "chrf_score": 68.08095328455637, "xcomet_score": 0.9643155336380005, "xcomet_qe_score": 0.8343571424484253, "metricx_score": 1.4250982999801636, "metricx_qe_score": 2.036952495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以通过从不同的子集或不同的数据集中选择句子来做到这一点。", "metrics": {"bleu_score": 74.38319205765252, "chrf_score": 72.91286052906386, "xcomet_score": 0.9837098121643066, "xcomet_qe_score": 0.8973073363304138, "metricx_score": 1.0443216562271118, "metricx_qe_score": 2.0040359497070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所说的不匹配场景。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9915859699249268, "xcomet_qe_score": 0.9170319437980652, "metricx_score": 0.7872244715690613, "metricx_qe_score": 1.401894450187683, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,句子仍然来自相关数据集,但不是您正在评估的相同数据集。", "metrics": {"bleu_score": 62.626261989886736, "chrf_score": 58.09503897838211, "xcomet_score": 0.91651451587677, "xcomet_qe_score": 0.7961004972457886, "metricx_score": 1.254689335823059, "metricx_qe_score": 2.0861310958862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以为可接受性案例做同样的事情。", "metrics": {"bleu_score": 20.9496465698149, "chrf_score": 20.105395966904844, "xcomet_score": 0.7989369630813599, "xcomet_qe_score": 0.7558352947235107, "metricx_score": 4.088828086853027, "metricx_qe_score": 3.1925456523895264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全不相关的领域选择句子,例如维基百科。", "metrics": {"bleu_score": 65.4468923560319, "chrf_score": 57.12890537741404, "xcomet_score": 0.9937243461608887, "xcomet_qe_score": 0.9375579953193665, "metricx_score": 0.6774861812591553, "metricx_qe_score": 1.5228242874145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们模型的可接受性判断是否实际上受到任何与我们正在查看的句子完全无关的", "metrics": {"bleu_score": 29.233980078811094, "chrf_score": 34.23114119098006, "xcomet_score": 0.33064499497413635, "xcomet_qe_score": 0.5036365985870361, "metricx_score": 7.549610614776611, "metricx_qe_score": 8.878662109375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文的影响。 模型表现如何?", "metrics": {"bleu_score": 6.285596338261262, "chrf_score": 5.15400492180678, "xcomet_score": 0.23941192030906677, "xcomet_qe_score": 0.31107640266418457, "metricx_score": 3.0933520793914795, "metricx_qe_score": 2.9614217281341553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们看维基百科的句子,这些句子与当前的查询对完全无关。在那里,我们发现MPP判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 63.772241430055956, "chrf_score": 56.54788224824263, "xcomet_score": 0.9341088533401489, "xcomet_qe_score": 0.8221834301948547, "metricx_score": 4.316064834594727, "metricx_qe_score": 6.055178642272949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到1024以最大化OPT和GPT-2模型。正如我们在这里看到", "metrics": {"bleu_score": 54.787480840451266, "chrf_score": 74.13860302184516, "xcomet_score": 0.7952576279640198, "xcomet_qe_score": 0.6464259624481201, "metricx_score": 2.1620633602142334, "metricx_qe_score": 2.655961513519287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的橙色虚线,MPP判断相对稳定。", "metrics": {"bleu_score": 52.58479114901755, "chrf_score": 53.515444977643725, "xcomet_score": 0.6782993078231812, "xcomet_qe_score": 0.5988267064094543, "metricx_score": 5.283979415893555, "metricx_qe_score": 7.194244861602783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们从相同数据集选择句子时会发生什么?", "metrics": {"bleu_score": 59.46403257809552, "chrf_score": 54.506663896984044, "xcomet_score": 0.9883992671966553, "xcomet_qe_score": 0.9226509928703308, "metricx_score": 0.7412636876106262, "metricx_qe_score": 1.308161735534668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们从可接受和不可接受的领域创建句子,这些领域来自相同的BLIMP或语法GEM数据集。在那里,", "metrics": {"bleu_score": 36.391987810868585, "chrf_score": 32.86452752503105, "xcomet_score": 0.525920033454895, "xcomet_qe_score": 0.5637980699539185, "metricx_score": 6.4007415771484375, "metricx_qe_score": 4.794913291931152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到MPP判断在添加可接受前缀或不可接受前缀时显著增加或减少。", "metrics": {"bleu_score": 44.58292993554919, "chrf_score": 41.958877877333315, "xcomet_score": 0.7878127098083496, "xcomet_qe_score": 0.7424564361572266, "metricx_score": 3.575669288635254, "metricx_qe_score": 2.665670156478882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们匹配结构时,即当我们选择来自blame-person-text-gym相同现象的句子时,我们看到模型的MPP判断大幅增加或大幅减少,具体取决于所选择的前缀是否可接受。这个效果随着", "metrics": {"bleu_score": 45.27615548298333, "chrf_score": 35.77203923016835, "xcomet_score": 0.44728049635887146, "xcomet_qe_score": 0.3633177578449249, "metricx_score": 10.921062469482422, "metricx_qe_score": 8.728883743286133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文长度的增加而增加,这可能会影响具有大上下文窗口的新语言模型。", "metrics": {"bleu_score": 57.970375612506196, "chrf_score": 53.01732373155521, "xcomet_score": 0.7993937730789185, "xcomet_qe_score": 0.667577862739563, "metricx_score": 3.307154655456543, "metricx_qe_score": 4.557590484619141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "匹配前缀为什么会如此影响语言模型的判断?", "metrics": {"bleu_score": 34.42551639232111, "chrf_score": 32.30610489301663, "xcomet_score": 0.9754217863082886, "xcomet_qe_score": 0.9039567708969116, "metricx_score": 0.560632050037384, "metricx_qe_score": 0.6591334939002991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保留相关结构但向输入添加噪声来调整输入句子。", "metrics": {"bleu_score": 65.07363223114328, "chrf_score": 59.313610552026965, "xcomet_score": 0.8884289264678955, "xcomet_qe_score": 0.8401092290878296, "metricx_score": 2.5507731437683105, "metricx_qe_score": 2.71067476272583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行了几次扰动后,我们发现这些噪声中没有一个实际上让模型改变其MPP判断趋势。", "metrics": {"bleu_score": 32.57154336049499, "chrf_score": 31.12220134424044, "xcomet_score": 0.9285604953765869, "xcomet_qe_score": 0.9126559495925903, "metricx_score": 4.170235633850098, "metricx_qe_score": 4.481339454650879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动句子的敏感度相似。", "metrics": {"bleu_score": 18.660032557187066, "chrf_score": 21.021040071748907, "xcomet_score": 0.9177385568618774, "xcomet_qe_score": 0.8890339732170105, "metricx_score": 1.9889860153198242, "metricx_qe_score": 3.5849034786224365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即,当我们扰动可接受领域的句子时,所有扰动都会看到类似的增加;当我们扰动不可接受领域的句子时,MPP判断会以类似的方式减少。", "metrics": {"bleu_score": 33.08159807139479, "chrf_score": 31.064110872205468, "xcomet_score": 0.5900835990905762, "xcomet_qe_score": 0.5755934715270996, "metricx_score": 5.166285514831543, "metricx_qe_score": 5.525096893310547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们研究的关键结论是,语言模型对跨句子的潜在语法和语义特征敏感。", "metrics": {"bleu_score": 41.87152978601971, "chrf_score": 35.29924752466762, "xcomet_score": 0.8453748226165771, "xcomet_qe_score": 0.8254241943359375, "metricx_score": 1.4232549667358398, "metricx_qe_score": 1.2445704936981201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前,我们用短句或单句输入进行MPP评估,可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 50.05035982120757, "chrf_score": 42.32095532278774, "xcomet_score": 0.9564324617385864, "xcomet_qe_score": 0.9037761688232422, "metricx_score": 1.9293785095214844, "metricx_qe_score": 2.4807796478271484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解更多实验细节。", "metrics": {"bleu_score": 34.27163657253172, "chrf_score": 33.44303636597666, "xcomet_score": 0.9978342056274414, "xcomet_qe_score": 0.9995092153549194, "metricx_score": 0.10219424962997437, "metricx_qe_score": 0.11421225965023041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9482142925262451, "xcomet_qe_score": 0.888888955116272, "metricx_score": 0.47969555854797363, "metricx_qe_score": 0.6286952495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张宇生。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 49.88704001434205, "xcomet_score": 0.9104704856872559, "xcomet_qe_score": 0.8539036512374878, "metricx_score": 0.5312755703926086, "metricx_qe_score": 0.8543612957000732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的研究成果——《多种自然语言和语义表示的跨语言语义分析》。", "metrics": {"bleu_score": 54.13528779252498, "chrf_score": 41.26094373846734, "xcomet_score": 0.9582923650741577, "xcomet_qe_score": 0.8136410117149353, "metricx_score": 2.0939199924468994, "metricx_qe_score": 3.149225950241089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义分析是一项构建用户查询的语义表示的任务,例如SQL和lambda演算。", "metrics": {"bleu_score": 53.40004805914115, "chrf_score": 47.57280299476733, "xcomet_score": 0.9662771224975586, "xcomet_qe_score": 0.9675347805023193, "metricx_score": 0.9873901009559631, "metricx_qe_score": 1.3083372116088867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义分析的任务是将多种自然语言的查询翻译成多种语义表示。", "metrics": {"bleu_score": 68.50564735741163, "chrf_score": 61.625358087167506, "xcomet_score": 0.9321763515472412, "xcomet_qe_score": 0.9371535181999207, "metricx_score": 1.5423755645751953, "metricx_qe_score": 3.3586807250976562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。", "metrics": {"bleu_score": 92.34732618882049, "chrf_score": 92.19354187608646, "xcomet_score": 0.95945143699646, "xcomet_qe_score": 0.9169260263442993, "metricx_score": 1.2653634548187256, "metricx_qe_score": 1.5561692714691162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义分析模型是分别在有限的任务和应用数据集上提出和评估的。", "metrics": {"bleu_score": 69.20463819926105, "chrf_score": 58.47440871934968, "xcomet_score": 0.9959969520568848, "xcomet_qe_score": 0.9824415445327759, "metricx_score": 0.6534231901168823, "metricx_qe_score": 1.0421183109283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,某些自然语言缺乏覆盖,", "metrics": {"bleu_score": 37.48576885220423, "chrf_score": 31.992412272828187, "xcomet_score": 0.7085752487182617, "xcomet_qe_score": 0.5492530465126038, "metricx_score": 4.858275890350342, "metricx_qe_score": 3.923373222351074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失;由于某些微表示的覆盖范围", "metrics": {"bleu_score": 6.3438016104279455, "chrf_score": 8.849254639488898, "xcomet_score": 0.6505102515220642, "xcomet_qe_score": 0.5427526235580444, "metricx_score": 5.975409507751465, "metricx_qe_score": 5.222082138061523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",lambda演算缺失或仅在某种神经模型上进行评估。", "metrics": {"bleu_score": 50.2589910168054, "chrf_score": 46.056306182452154, "xcomet_score": 0.8941787481307983, "xcomet_qe_score": 0.791419506072998, "metricx_score": 2.8501009941101074, "metricx_qe_score": 3.3606410026550293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个单一的模型来评估它们。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.997307538986206, "xcomet_qe_score": 0.9824987649917603, "metricx_score": 0.5768303871154785, "metricx_qe_score": 0.8717049956321716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了Examplar。", "metrics": {"bleu_score": 42.7287006396234, "chrf_score": 20.029035880991188, "xcomet_score": 0.8232528567314148, "xcomet_qe_score": 0.8104090690612793, "metricx_score": 2.3032500743865967, "metricx_qe_score": 3.333799362182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提供了一个统一的数据集Examplar,用于多种自然语言和语义表示的跨语言语义分析。", "metrics": {"bleu_score": 69.089127710963, "chrf_score": 52.64657698862697, "xcomet_score": 0.8834114074707031, "xcomet_qe_score": 0.8091980814933777, "metricx_score": 3.7282958030700684, "metricx_qe_score": 4.177829265594482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含来自各个领域的九个数据集、五个语义分析任务、八种语义表示以及22种自然语言,涵盖15个语言家族。", "metrics": {"bleu_score": 36.30291621558176, "chrf_score": 34.535952820069404, "xcomet_score": 0.944573163986206, "xcomet_qe_score": 0.9556509256362915, "metricx_score": 0.7268742322921753, "metricx_qe_score": 1.0118666887283325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 80.20219183488042, "chrf_score": 71.52080420921001, "xcomet_score": 0.9888695478439331, "xcomet_qe_score": 0.913833737373352, "metricx_score": 1.4580811262130737, "metricx_qe_score": 2.0225110054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是翻译测试。", "metrics": {"bleu_score": 66.06328636027612, "chrf_score": 56.005291005291, "xcomet_score": 0.9600571393966675, "xcomet_qe_score": 0.9592820405960083, "metricx_score": 0.29587188363075256, "metricx_qe_score": 0.4503270089626312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google翻译API将源语言翻译成目标语言,然后使用单语模型进行训练和评估。", "metrics": {"bleu_score": 82.66660014007991, "chrf_score": 76.41626540305097, "xcomet_score": 0.9510485529899597, "xcomet_qe_score": 0.8472477197647095, "metricx_score": 0.5243576169013977, "metricx_qe_score": 0.4862639307975769, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语查询上训练英语模型。在推理阶段,我们使用API将德语查询翻译成英语,然后使用训练好的模型预测SQL。", "metrics": {"bleu_score": 60.364307487579815, "chrf_score": 55.92741419838115, "xcomet_score": 0.8630545735359192, "xcomet_qe_score": 0.8413059115409851, "metricx_score": 1.2490277290344238, "metricx_qe_score": 1.7775657176971436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语模型。", "metrics": {"bleu_score": 51.56626918239823, "chrf_score": 40.77320827320827, "xcomet_score": 0.8812164068222046, "xcomet_qe_score": 0.84535151720047, "metricx_score": 1.4693257808685303, "metricx_qe_score": 1.2090201377868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个设置中,源语言与目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 72.76293560493822, "chrf_score": 67.10828266955446, "xcomet_score": 0.9135197401046753, "xcomet_qe_score": 0.8122859001159668, "metricx_score": 0.5885418057441711, "metricx_qe_score": 0.6838915944099426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语少样本设置,通过仅使用10%的训练数据训练单语模型。", "metrics": {"bleu_score": 49.740060398870035, "chrf_score": 43.102786562778526, "xcomet_score": 0.832395613193512, "xcomet_qe_score": 0.7589335441589355, "metricx_score": 0.9242462515830994, "metricx_qe_score": 1.2620340585708618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们测试了多语种模型,即训练一个多语种模型来处理所有语言。", "metrics": {"bleu_score": 37.89906930931357, "chrf_score": 34.562465033983585, "xcomet_score": 0.9831811189651489, "xcomet_qe_score": 0.9768850803375244, "metricx_score": 0.9791438579559326, "metricx_qe_score": 0.9224746823310852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语、中文查询放在一起训练一个多语种模型。", "metrics": {"bleu_score": 74.25493166673012, "chrf_score": 68.0665953182, "xcomet_score": 0.9560741186141968, "xcomet_qe_score": 0.9600487947463989, "metricx_score": 1.2903392314910889, "metricx_qe_score": 2.5988519191741943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理阶段,我们可以使用该模型翻译德语查询或中文查询等。", "metrics": {"bleu_score": 60.31645427016849, "chrf_score": 54.643319390606614, "xcomet_score": 0.9746060371398926, "xcomet_qe_score": 0.8909346461296082, "metricx_score": 0.7099141478538513, "metricx_qe_score": 1.2674577236175537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,然后转移到另一种语言。因此", "metrics": {"bleu_score": 37.011751896357886, "chrf_score": 32.39590807762106, "xcomet_score": 0.7984310388565063, "xcomet_qe_score": 0.771204948425293, "metricx_score": 4.228652000427246, "metricx_qe_score": 3.250486135482788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练阶段,我们在英语查询或英语和德语少样本查询的组合上训练一个多语种模型,并预测SQL输出。", "metrics": {"bleu_score": 46.5566353581798, "chrf_score": 42.311042107311714, "xcomet_score": 0.8204112648963928, "xcomet_qe_score": 0.7720639109611511, "metricx_score": 1.757969617843628, "metricx_qe_score": 2.474458932876587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的结果。", "metrics": {"bleu_score": 57.3122448409426, "chrf_score": 48.92181892181892, "xcomet_score": 0.9985549449920654, "xcomet_qe_score": 0.990606427192688, "metricx_score": 0.33577799797058105, "metricx_qe_score": 0.8158290982246399, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于单语模型的分析,我们评估了两组模型,包括编码器PDR(多语种预训练编码器与指针解码器,如XLMR加PDR和BERT加PDR)", "metrics": {"bleu_score": 40.6001443113452, "chrf_score": 32.03097006297366, "xcomet_score": 0.540306568145752, "xcomet_qe_score": 0.5180409550666809, "metricx_score": 4.417198181152344, "metricx_qe_score": 4.49284029006958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "和编码器-解码器模型(多语种预训练编码器-解码器模型,如MBART和MT5)。", "metrics": {"bleu_score": 7.434360163166209, "chrf_score": 10.591287987460325, "xcomet_score": 0.5306698083877563, "xcomet_qe_score": 0.6969162225723267, "metricx_score": 3.4655604362487793, "metricx_qe_score": 3.731433153152466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器在所有九个数据集上取得了最佳性能。", "metrics": {"bleu_score": 46.09056322258578, "chrf_score": 29.651355294421784, "xcomet_score": 0.990986704826355, "xcomet_qe_score": 0.9815418720245361, "metricx_score": 1.4509633779525757, "metricx_qe_score": 1.495482325553894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多语种设置下评估了MT5和XLMR加PDR,发现编码", "metrics": {"bleu_score": 19.228544753133768, "chrf_score": 26.12804444603568, "xcomet_score": 0.7404208779335022, "xcomet_qe_score": 0.6500350832939148, "metricx_score": 7.481070518493652, "metricx_qe_score": 4.39047908782959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "器-解码器或编码器-PDR通过混合各种语言的训练可以得到改进。", "metrics": {"bleu_score": 31.437754726042403, "chrf_score": 18.61767346246474, "xcomet_score": 0.395489364862442, "xcomet_qe_score": 0.36841750144958496, "metricx_score": 4.861456394195557, "metricx_qe_score": 6.300841808319092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,除了英语在七个数据集上的性能下降,仅在三个数据集上提升。", "metrics": {"bleu_score": 55.31601325765436, "chrf_score": 49.70417212188711, "xcomet_score": 0.8838127255439758, "xcomet_qe_score": 0.9075332283973694, "metricx_score": 2.8941051959991455, "metricx_qe_score": 2.4667768478393555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语种的诅咒。", "metrics": {"bleu_score": 19.16596055261492, "chrf_score": 18.515478428550278, "xcomet_score": 0.9028452634811401, "xcomet_qe_score": 0.8993649482727051, "metricx_score": 1.6977708339691162, "metricx_qe_score": 1.746958613395691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中蓝线表示跨语言少样本迁移,", "metrics": {"bleu_score": 45.913603866715796, "chrf_score": 41.932444984864624, "xcomet_score": 0.8522014617919922, "xcomet_qe_score": 0.7879692316055298, "metricx_score": 1.9897856712341309, "metricx_qe_score": 3.8369405269622803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线表示跨语言零样本迁移,", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 59.505479514708156, "xcomet_score": 0.9473828077316284, "xcomet_qe_score": 0.8333927989006042, "metricx_score": 1.6966118812561035, "metricx_qe_score": 2.8306846618652344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿线表示单语设置。", "metrics": {"bleu_score": 51.33450480401705, "chrf_score": 44.25269628004752, "xcomet_score": 0.9992365837097168, "xcomet_qe_score": 0.9950376749038696, "metricx_score": 0.3408881425857544, "metricx_qe_score": 0.6177595257759094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过比较绿线和橙线,我们发现在零样本设置下,跨语言迁移性能差距显著。通过比较蓝线和橙线,我们发现在少样本设置下,迁移差距迅速缩小。", "metrics": {"bleu_score": 66.18434976585449, "chrf_score": 58.3104137233463, "xcomet_score": 0.8300637602806091, "xcomet_qe_score": 0.6109604835510254, "metricx_score": 1.6414504051208496, "metricx_qe_score": 3.2102415561676025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器优于之前的工作或取得了可比的结果。", "metrics": {"bleu_score": 13.597796343834903, "chrf_score": 9.966743321210252, "xcomet_score": 0.9629379510879517, "xcomet_qe_score": 0.9584528207778931, "metricx_score": 1.7992825508117676, "metricx_qe_score": 2.0096302032470703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上的描绘可以显著提升目标自然语言的少样本性能。我们发现多语种语言模型如CODIS和BLUE对于跨语言语义分析任务仍然足够。", "metrics": {"bleu_score": 41.51367727694659, "chrf_score": 32.311009362313186, "xcomet_score": 0.4310207962989807, "xcomet_qe_score": 0.3315565288066864, "metricx_score": 9.875614166259766, "metricx_qe_score": 10.933650970458984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们构建了Examplar,一个统一的基准平台,用于多种自然语言和主要语义表示的跨语言语义分析。", "metrics": {"bleu_score": 37.075420295794686, "chrf_score": 27.782483905272017, "xcomet_score": 0.8253276348114014, "xcomet_qe_score": 0.8248594999313354, "metricx_score": 4.200533390045166, "metricx_qe_score": 4.651400566101074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种代表类型的多语种语言模型进行了全面的基准研究。", "metrics": {"bleu_score": 72.48593245049084, "chrf_score": 63.3469785306965, "xcomet_score": 0.8991434574127197, "xcomet_qe_score": 0.8388328552246094, "metricx_score": 1.1663033962249756, "metricx_qe_score": 2.0844879150390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的成果显示了许多有趣的发现等", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 68.89097014097013, "xcomet_score": 0.8065767288208008, "xcomet_qe_score": 0.7921155691146851, "metricx_score": 2.209555149078369, "metricx_qe_score": 2.554340362548828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.1294848918914795, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是大卫·维拉尔,我将简要介绍一下论文《Grunting Parm》中关于翻译的策略和表现的评估。", "metrics": {"bleu_score": 13.297818013585355, "chrf_score": 14.755527231035643, "xcomet_score": 0.7338500022888184, "xcomet_qe_score": 0.6688132286071777, "metricx_score": 7.951077461242676, "metricx_qe_score": 8.21624755859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与谷歌翻译团队同事的合作研究。Parm是一个拥有", "metrics": {"bleu_score": 44.338389108446, "chrf_score": 43.680483282078754, "xcomet_score": 0.5090615153312683, "xcomet_qe_score": 0.4407004117965698, "metricx_score": 7.378729343414307, "metricx_qe_score": 2.466233491897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "5400亿参数的大型语言模型,于去年,即2022年发布。", "metrics": {"bleu_score": 42.396971004714274, "chrf_score": 44.22248011645852, "xcomet_score": 0.8244706392288208, "xcomet_qe_score": 0.8213317394256592, "metricx_score": 4.921650409698486, "metricx_qe_score": 5.351381301879883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它训练了包含7800亿个标记的大量文本数据。", "metrics": {"bleu_score": 24.91346875761623, "chrf_score": 33.43883906451439, "xcomet_score": 0.7323287129402161, "xcomet_qe_score": 0.7220760583877563, "metricx_score": 3.4092178344726562, "metricx_qe_score": 3.030005693435669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百个自然语言处理任务中达到了最先进水平。", "metrics": {"bleu_score": 22.50265947708922, "chrf_score": 20.989852208079824, "xcomet_score": 0.9979772567749023, "xcomet_qe_score": 0.9900028705596924, "metricx_score": 0.733557939529419, "metricx_qe_score": 1.0169049501419067, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们提出了对大型语言模型翻译提示的首次系统研究。", "metrics": {"bleu_score": 17.049628141362373, "chrf_score": 19.7393374786371, "xcomet_score": 0.8964976668357849, "xcomet_qe_score": 0.8583838939666748, "metricx_score": 2.280651092529297, "metricx_qe_score": 2.536699056625366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用国际机器翻译协会(IMT)的最佳实践来评估这些模型的翻译能力。", "metrics": {"bleu_score": 45.75849198367844, "chrf_score": 42.57430377267151, "xcomet_score": 0.9063907861709595, "xcomet_qe_score": 0.8002282381057739, "metricx_score": 3.65689754486084, "metricx_qe_score": 4.462224960327148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这涉及到使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 91.84678024441791, "chrf_score": 88.17316559043479, "xcomet_score": 0.9366746544837952, "xcomet_qe_score": 0.9506984949111938, "metricx_score": 0.5633312463760376, "metricx_qe_score": 0.5533048510551453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,即在WMT评估中表现最佳的系统。", "metrics": {"bleu_score": 40.81581068498904, "chrf_score": 37.92448279467379, "xcomet_score": 0.9238303899765015, "xcomet_qe_score": 0.909253716468811, "metricx_score": 2.6168227195739746, "metricx_qe_score": 3.6693363189697266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的神经机器翻译指标,同时还展示了基于专家的人类评估结果。", "metrics": {"bleu_score": 68.36858580099506, "chrf_score": 63.87406974536486, "xcomet_score": 0.9628020524978638, "xcomet_qe_score": 0.7938692569732666, "metricx_score": 1.45875084400177, "metricx_qe_score": 2.3764488697052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了关于提示选择策略的一些建议。", "metrics": {"bleu_score": 64.06179367095976, "chrf_score": 54.26883831682594, "xcomet_score": 0.8966191411018372, "xcomet_qe_score": 0.8442993760108948, "metricx_score": 0.5821217894554138, "metricx_qe_score": 1.5963844060897827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型的翻译性能有很大影响。在我们的一个简单实验中,我们使用了单次提示,并为每句话提供了两个不同的提示,", "metrics": {"bleu_score": 46.111435812195126, "chrf_score": 37.92280191732733, "xcomet_score": 0.9303863048553467, "xcomet_qe_score": 0.9060972929000854, "metricx_score": 1.7281957864761353, "metricx_qe_score": 1.5615249872207642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果显示,在1000句话中,", "metrics": {"bleu_score": 8.913765521398126, "chrf_score": 25.596530503229566, "xcomet_score": 0.8736692070960999, "xcomet_qe_score": 0.5154157280921936, "metricx_score": 7.793721675872803, "metricx_qe_score": 10.186380386352539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有516句话的差异超过了一个模糊点,在", "metrics": {"bleu_score": 7.439820585622744, "chrf_score": 13.422057023380676, "xcomet_score": 0.23807160556316376, "xcomet_qe_score": 0.16109897196292877, "metricx_score": 12.833474159240723, "metricx_qe_score": 9.528039932250977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "极端情况下,可达40个模糊点。", "metrics": {"bleu_score": 28.253017719977493, "chrf_score": 21.409924402474964, "xcomet_score": 0.8123563528060913, "xcomet_qe_score": 0.8192775249481201, "metricx_score": 4.714085102081299, "metricx_qe_score": 2.6879265308380127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个好的提示策略非常重要。在", "metrics": {"bleu_score": 59.08871032231054, "chrf_score": 58.5902119027495, "xcomet_score": 0.8214733600616455, "xcomet_qe_score": 0.7509938478469849, "metricx_score": 3.953136920928955, "metricx_qe_score": 0.3306087255477905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验中,我们选择了五次提示策略,即在提供给系统的每句话中", "metrics": {"bleu_score": 22.333858992388528, "chrf_score": 21.59594882630459, "xcomet_score": 0.5405417084693909, "xcomet_qe_score": 0.6552812457084656, "metricx_score": 8.64830207824707, "metricx_qe_score": 5.752853870391846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "标注其语言。例如,在从德语到英语的翻译中,我们用德语冒号标注德语句子,用英语冒号标注英语翻译。", "metrics": {"bleu_score": 15.920362546548075, "chrf_score": 12.99292785519458, "xcomet_score": 0.5957443714141846, "xcomet_qe_score": 0.17399385571479797, "metricx_score": 2.3933846950531006, "metricx_qe_score": 2.3013269901275635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次提示的情况下,提示的实际形式影响不大。", "metrics": {"bleu_score": 30.16522631265282, "chrf_score": 25.818544028253964, "xcomet_score": 0.8295421600341797, "xcomet_qe_score": 0.7929729223251343, "metricx_score": 0.9645405411720276, "metricx_qe_score": 1.1535874605178833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在零次和一次提示中至关重要,", "metrics": {"bleu_score": 11.538047727053714, "chrf_score": 11.669899204075925, "xcomet_score": 0.7306442856788635, "xcomet_qe_score": 0.7712957859039307, "metricx_score": 2.765251636505127, "metricx_qe_score": 2.1573355197906494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而当我们采用五次提示时,提示的实际形式几乎没有影响。", "metrics": {"bleu_score": 27.40899731303482, "chrf_score": 26.23136549178971, "xcomet_score": 0.9588254690170288, "xcomet_qe_score": 0.66521155834198, "metricx_score": 0.8918383121490479, "metricx_qe_score": 1.5697942972183228, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例子承载了大部分权重。", "metrics": {"bleu_score": 3.755001157177446, "chrf_score": 3.7313432835820883, "xcomet_score": 0.8016377687454224, "xcomet_qe_score": 0.838649332523346, "metricx_score": 1.2344675064086914, "metricx_qe_score": 1.114772915840149, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果总结是,例子质量比源句子的相似性更重要。", "metrics": {"bleu_score": 57.29840363337364, "chrf_score": 49.91633733270206, "xcomet_score": 0.9126274585723877, "xcomet_qe_score": 0.8507555723190308, "metricx_score": 0.8725314140319824, "metricx_qe_score": 0.9278132915496826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,从高质量的翻译中选择例子非常重要。", "metrics": {"bleu_score": 68.05854779603962, "chrf_score": 59.13663857355807, "xcomet_score": 0.9334757328033447, "xcomet_qe_score": 0.9385048151016235, "metricx_score": 0.5153730511665344, "metricx_qe_score": 0.592513918876648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了从WMT评估的训练数据或开发数据中选择提示。开发数据比训练", "metrics": {"bleu_score": 42.62838101265284, "chrf_score": 38.388523375112236, "xcomet_score": 0.5652488470077515, "xcomet_qe_score": 0.4417732357978821, "metricx_score": 6.063419342041016, "metricx_qe_score": 5.168334484100342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据更精心整理,质量更高,训练数据更噪声,结果显示", "metrics": {"bleu_score": 19.365349132217286, "chrf_score": 17.511674289090003, "xcomet_score": 0.5859955549240112, "xcomet_qe_score": 0.5280752778053284, "metricx_score": 6.430997371673584, "metricx_qe_score": 4.7310791015625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用开发数据时表现更好。", "metrics": {"bleu_score": 59.79111927660196, "chrf_score": 52.42838283401885, "xcomet_score": 0.9584156274795532, "xcomet_qe_score": 0.8910613656044006, "metricx_score": 0.8060206174850464, "metricx_qe_score": 1.535290241241455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最先进的专业系统在翻译质量上仍比Parm有显著优势", "metrics": {"bleu_score": 9.033765029369462, "chrf_score": 13.173182553799364, "xcomet_score": 0.8867447972297668, "xcomet_qe_score": 0.8748602271080017, "metricx_score": 4.141976356506348, "metricx_qe_score": 3.5957419872283936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但Parm已经非常接近商业系统了。", "metrics": {"bleu_score": 65.54913610595183, "chrf_score": 52.431109195815075, "xcomet_score": 0.8110249042510986, "xcomet_qe_score": 0.690385103225708, "metricx_score": 5.618067741394043, "metricx_qe_score": 5.913559913635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择与谷歌翻译进行比较。", "metrics": {"bleu_score": 56.96705282375882, "chrf_score": 46.65108313101736, "xcomet_score": 0.972275972366333, "xcomet_qe_score": 0.9678858518600464, "metricx_score": 1.5836470127105713, "metricx_qe_score": 0.5965385437011719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用MQM框架对电子邮件进行分析的洞察是,Parm的流利度可与最先进的系统相媲美,但准确度存在差异。", "metrics": {"bleu_score": 33.82832206001471, "chrf_score": 32.14759441475419, "xcomet_score": 0.6956167221069336, "xcomet_qe_score": 0.5889374613761902, "metricx_score": 6.284409999847412, "metricx_qe_score": 5.870099067687988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体来说,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 67.53160327422972, "chrf_score": 68.8543512330095, "xcomet_score": 0.923292875289917, "xcomet_qe_score": 0.9212453961372375, "metricx_score": 1.3487967252731323, "metricx_qe_score": 0.6798467040061951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "似乎Parm有时通过丢弃源句子中在翻译中没有表现出来的部分来生成听起来更好的翻译。", "metrics": {"bleu_score": 35.57139215404423, "chrf_score": 30.26508168181196, "xcomet_score": 0.8332539796829224, "xcomet_qe_score": 0.7782419919967651, "metricx_score": 4.3653364181518555, "metricx_qe_score": 4.11468505859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,Parm的风格尴尬类别低于最先进的系统,这进一步表明Parm提供非常流利的输出,但准确度仍存在一些问题。", "metrics": {"bleu_score": 27.614063917735525, "chrf_score": 26.151036234211734, "xcomet_score": 0.664291501045227, "xcomet_qe_score": 0.6565080881118774, "metricx_score": 7.808557033538818, "metricx_qe_score": 6.861445903778076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是这次简短的概述,", "metrics": {"bleu_score": 15.365277058808237, "chrf_score": 16.736218444100977, "xcomet_score": 0.9123852252960205, "xcomet_qe_score": 0.8692036867141724, "metricx_score": 0.6613274216651917, "metricx_qe_score": 0.9362949728965759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更多细节请参阅论文的完整演讲。", "metrics": {"bleu_score": 21.640566877431628, "chrf_score": 21.686123638157788, "xcomet_score": 0.88901287317276, "xcomet_qe_score": 0.8285250663757324, "metricx_score": 2.6829347610473633, "metricx_qe_score": 3.0909829139709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.9730953574180603, "xcomet_qe_score": 0.9623823165893555, "metricx_score": 0.18832087516784668, "metricx_qe_score": 0.09758952260017395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是德国萨尔兰大学博士生大威。", "metrics": {"bleu_score": 34.71488488101834, "chrf_score": 27.44526665671817, "xcomet_score": 0.8640557527542114, "xcomet_qe_score": 0.8770509958267212, "metricx_score": 1.3951910734176636, "metricx_qe_score": 0.7999443411827087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想向大家介绍我们最近的研究成果《比你想象的更脆弱》,这是一项对弱监督学习的批判性研究。", "metrics": {"bleu_score": 47.27288225595977, "chrf_score": 45.920185664161146, "xcomet_score": 0.9102579951286316, "xcomet_qe_score": 0.8123854994773865, "metricx_score": 1.3750646114349365, "metricx_qe_score": 2.014314651489258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与尤生肖、马里奥·斯穆斯巴赫、吉娅·斯特芬以及迪特里希·克拉科共同完成的工作。", "metrics": {"bleu_score": 6.695900686562915, "chrf_score": 4.131013580253438, "xcomet_score": 0.5461058616638184, "xcomet_qe_score": 0.33794862031936646, "metricx_score": 4.903770923614502, "metricx_qe_score": 4.907894611358643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想从简要介绍弱监督和弱监督学习开始。", "metrics": {"bleu_score": 56.82854869630478, "chrf_score": 51.33352550002637, "xcomet_score": 0.9624283313751221, "xcomet_qe_score": 0.8501094579696655, "metricx_score": 0.8787198066711426, "metricx_qe_score": 2.371363639831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不会手动标注数据。", "metrics": {"bleu_score": 36.00565854285029, "chrf_score": 29.316598434245495, "xcomet_score": 0.9149240255355835, "xcomet_qe_score": 0.8724530935287476, "metricx_score": 0.8288316130638123, "metricx_qe_score": 1.4297345876693726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注源来标注数据,例如简单的启发式规则、知识库或低质量的众包标注,正如右图所示。", "metrics": {"bleu_score": 66.51195068614454, "chrf_score": 62.07768434978329, "xcomet_score": 0.6369884014129639, "xcomet_qe_score": 0.6710944175720215, "metricx_score": 1.5408010482788086, "metricx_qe_score": 1.7339125871658325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注成本更低,但同时也存在噪声,即一定数量的标注是不正确的。", "metrics": {"bleu_score": 30.212942293015793, "chrf_score": 24.913679548415658, "xcomet_score": 0.8725817203521729, "xcomet_qe_score": 0.847195029258728, "metricx_score": 2.2189040184020996, "metricx_qe_score": 2.858919858932495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在弱标注数据上训练神经网络,神经网络倾向于记住标注噪声,而无法泛化。", "metrics": {"bleu_score": 57.07428121367624, "chrf_score": 52.0200735310504, "xcomet_score": 0.9620198011398315, "xcomet_qe_score": 0.9086990356445312, "metricx_score": 0.9455804824829102, "metricx_qe_score": 1.2693556547164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出了训练算法,以在这种标签噪声下稳健地训练神经网络,使训练后的模型仍能良好地泛化。", "metrics": {"bleu_score": 60.71665555965534, "chrf_score": 56.125625099475876, "xcomet_score": 0.9755810499191284, "xcomet_qe_score": 0.895793080329895, "metricx_score": 1.1523914337158203, "metricx_qe_score": 2.2681355476379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的弱监督学习研究中,一个常见的说法是,人们声称他们只在弱标签数据下训练模型,并在干净的测试集上实现了高性能。", "metrics": {"bleu_score": 41.07003423848057, "chrf_score": 33.16985178803657, "xcomet_score": 0.8506861925125122, "xcomet_qe_score": 0.7765825986862183, "metricx_score": 3.0455358028411865, "metricx_qe_score": 3.410672187805176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并不错误,但存在一个", "metrics": {"bleu_score": 6.721647931729163, "chrf_score": 11.03961090594319, "xcomet_score": 0.45036211609840393, "xcomet_qe_score": 0.24661029875278473, "metricx_score": 8.674029350280762, "metricx_qe_score": 9.07407283782959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "问题,即", "metrics": {"bleu_score": 0.01376099880031779, "chrf_score": 2.636374805471373, "xcomet_score": 0.1868782937526703, "xcomet_qe_score": 0.1500307023525238, "metricx_score": 21.54393768310547, "metricx_qe_score": 19.736915588378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5430290699005127, "xcomet_qe_score": 0.1457119882106781, "metricx_score": 6.939623832702637, "metricx_qe_score": 7.438058853149414, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "三个研究问题。", "metrics": {"bleu_score": 27.645304662956455, "chrf_score": 37.28969519074232, "xcomet_score": 0.6767177581787109, "xcomet_qe_score": 0.5813324451446533, "metricx_score": 4.788186550140381, "metricx_qe_score": 7.17353630065918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们是否需要干净的验证数据?", "metrics": {"bleu_score": 28.559670149996563, "chrf_score": 33.307111079556165, "xcomet_score": 0.8796626329421997, "xcomet_qe_score": 0.8338778018951416, "metricx_score": 4.49473762512207, "metricx_qe_score": 5.659512042999268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们", "metrics": {"bleu_score": 0.007166941101713902, "chrf_score": 1.5685219263220023, "xcomet_score": 0.15880712866783142, "xcomet_qe_score": 0.14991217851638794, "metricx_score": 20.990489959716797, "metricx_qe_score": 14.664161682128906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是否应该仅使用干净样本进行验证,还是有更好的利用方式?", "metrics": {"bleu_score": 35.63276689383326, "chrf_score": 31.506027208694654, "xcomet_score": 0.9376912117004395, "xcomet_qe_score": 0.8846406936645508, "metricx_score": 0.807648777961731, "metricx_qe_score": 1.1863398551940918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中回答了这些研究问题,我们的发现如下。", "metrics": {"bleu_score": 44.35790342124191, "chrf_score": 36.55492036929125, "xcomet_score": 0.9518052339553833, "xcomet_qe_score": 0.9410156011581421, "metricx_score": 1.7298905849456787, "metricx_qe_score": 1.9394450187683105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,有趣的是,我们发现最近的弱监督学习方法确实需要干净的验证样本才能正常工作。", "metrics": {"bleu_score": 59.02565925489299, "chrf_score": 54.45233782813925, "xcomet_score": 0.9627606868743896, "xcomet_qe_score": 0.9628169536590576, "metricx_score": 2.1803553104400635, "metricx_qe_score": 2.3166520595550537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,那么训练后的模型无法超越原始的弱标签,这意味着训练是无意义的。", "metrics": {"bleu_score": 70.86257607576209, "chrf_score": 62.72189551150491, "xcomet_score": 0.8213664889335632, "xcomet_qe_score": 0.8090378642082214, "metricx_score": 2.1430022716522217, "metricx_qe_score": 3.38284969329834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明弱监督学习方法实际上需要干净的标注数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 45.2766149258772, "chrf_score": 39.516019329024324, "xcomet_score": 0.8402185440063477, "xcomet_qe_score": 0.8465136289596558, "metricx_score": 2.9061341285705566, "metricx_qe_score": 2.8611412048339844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于弱监督学习方法实现更好的性能,如左图所示。", "metrics": {"bleu_score": 64.85837737615338, "chrf_score": 59.47423638845019, "xcomet_score": 0.9572933912277222, "xcomet_qe_score": 0.8875253796577454, "metricx_score": 3.9276421070098877, "metricx_qe_score": 4.694291114807129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每类20个样本就能达到高性能。", "metrics": {"bleu_score": 17.101166217715278, "chrf_score": 19.053276401121853, "xcomet_score": 0.9430387020111084, "xcomet_qe_score": 0.9651368856430054, "metricx_score": 1.5335792303085327, "metricx_qe_score": 1.592826247215271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但故事并未结束,因为如果我们决定使用干净样本进行训练,那么直接训练它们甚至能实现更好的性能。", "metrics": {"bleu_score": 35.35323505742379, "chrf_score": 28.820042344161557, "xcomet_score": 0.9433091282844543, "xcomet_qe_score": 0.893767237663269, "metricx_score": 3.380521059036255, "metricx_qe_score": 3.4299657344818115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "红图显示了直接应用在干净数据上的微调方法与仅将干净数据用于验证的弱监督学习方法之间的性能差异。", "metrics": {"bleu_score": 49.74257346756803, "chrf_score": 41.800258397438796, "xcomet_score": 0.8101876974105835, "xcomet_qe_score": 0.8931281566619873, "metricx_score": 2.6824827194213867, "metricx_qe_score": 2.337970495223999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所见,如果每类有10个样本,直接微调开始超越弱监督学习方法。最后,之前弱监督学习方法声", "metrics": {"bleu_score": 32.41237695568623, "chrf_score": 30.810651590054956, "xcomet_score": 0.5694257616996765, "xcomet_qe_score": 0.5528584718704224, "metricx_score": 7.924960136413574, "metricx_qe_score": 5.25203800201416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "称的性能提升可以通过允许继续在干净验证样本上进行微调轻松实现。", "metrics": {"bleu_score": 30.062203274190068, "chrf_score": 25.261598661230554, "xcomet_score": 0.43144455552101135, "xcomet_qe_score": 0.46796876192092896, "metricx_score": 6.406146049499512, "metricx_qe_score": 7.813266754150391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从图中我们可以看到,范林登模型(称为W)最初在性能上落后于更复杂的弱监督学习方法,如余弦。", "metrics": {"bleu_score": 11.381305436860425, "chrf_score": 11.845571395619865, "xcomet_score": 0.5890962481498718, "xcomet_qe_score": 0.6029683351516724, "metricx_score": 5.568808555603027, "metricx_qe_score": 5.260554790496826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果允许在干净样本上继续微调,那么FTW的表现与其它方法一样好。", "metrics": {"bleu_score": 36.24125307694401, "chrf_score": 31.731642288352198, "xcomet_score": 0.8990448713302612, "xcomet_qe_score": 0.8095269203186035, "metricx_score": 1.2699346542358398, "metricx_qe_score": 1.8385862112045288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以在实践中,没有理由选择需要更多计算时间和磁盘空间的更复杂弱监督学习方法。", "metrics": {"bleu_score": 69.28511184636993, "chrf_score": 64.83580259122054, "xcomet_score": 0.90926194190979, "xcomet_qe_score": 0.9063809514045715, "metricx_score": 1.1480809450149536, "metricx_qe_score": 1.2447421550750732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们表明最近的弱监督学习方法需要干净的人工标注样本才能正常工作。", "metrics": {"bleu_score": 45.32607978893935, "chrf_score": 36.63978064187909, "xcomet_score": 0.8919284343719482, "xcomet_qe_score": 0.8385747075080872, "metricx_score": 2.4969282150268555, "metricx_qe_score": 2.555162191390991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否在干净验证样本上进行。", "metrics": {"bleu_score": 28.348995551545965, "chrf_score": 24.960806547460468, "xcomet_score": 0.9086339473724365, "xcomet_qe_score": 0.905847430229187, "metricx_score": 1.6992335319519043, "metricx_qe_score": 3.1822967529296875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二,弱监督学习方法应与少样本学习基线进行比较,并在干净样本上进行工作。", "metrics": {"bleu_score": 39.947055658806974, "chrf_score": 34.453119224420796, "xcomet_score": 0.7958571910858154, "xcomet_qe_score": 0.7275476455688477, "metricx_score": 3.5674993991851807, "metricx_qe_score": 3.64969539642334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,连续微调是一个简单而强大的基线,应在未来的弱监督学习研究中得到考虑。", "metrics": {"bleu_score": 47.24944645262245, "chrf_score": 44.80388934260741, "xcomet_score": 0.8054401874542236, "xcomet_qe_score": 0.7163931727409363, "metricx_score": 2.3039000034332275, "metricx_qe_score": 3.7869181632995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可以通过本幻灯片上的二维码找到它。", "metrics": {"bleu_score": 53.989956849868726, "chrf_score": 45.4490172137231, "xcomet_score": 0.9949206113815308, "xcomet_qe_score": 0.9897035360336304, "metricx_score": 0.5753722190856934, "metricx_qe_score": 0.4856390655040741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝大家享受大会。", "metrics": {"bleu_score": 12.498879161997976, "chrf_score": 16.329261695282028, "xcomet_score": 0.894599437713623, "xcomet_qe_score": 0.9022636413574219, "metricx_score": 0.7399343252182007, "metricx_qe_score": 0.6377139687538147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9827327728271484, "xcomet_qe_score": 1.0, "metricx_score": 0.8756831288337708, "metricx_qe_score": 0.3633490204811096, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCeval,一种评估对话人工智能的新维度方法。", "metrics": {"bleu_score": 23.44732872048571, "chrf_score": 25.184261211703845, "xcomet_score": 0.8484708070755005, "xcomet_qe_score": 0.913550615310669, "metricx_score": 1.4003496170043945, "metricx_qe_score": 1.6343687772750854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学自然语言处理实验室完成,由埃默里大学的吉诺·崔教授领导,并与亚马逊 Alexa AI 合作。", "metrics": {"bleu_score": 26.476971763396463, "chrf_score": 29.01515412771859, "xcomet_score": 0.7659019827842712, "xcomet_qe_score": 0.762053906917572, "metricx_score": 2.274923801422119, "metricx_qe_score": 2.5585083961486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚开发了一个对话模型,想了解它与当前最先进技术相比表现如何。", "metrics": {"bleu_score": 63.9740736433283, "chrf_score": 57.46819134821285, "xcomet_score": 0.9915869235992432, "xcomet_qe_score": 0.9788662195205688, "metricx_score": 0.4444388747215271, "metricx_qe_score": 0.49916189908981323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如请人类评判员选择两个对话中哪一个更好,或使用利克特量表对对话进行评分。", "metrics": {"bleu_score": 67.70128166071324, "chrf_score": 61.98901149808426, "xcomet_score": 0.9840564727783203, "xcomet_qe_score": 0.9804220199584961, "metricx_score": 0.6632543802261353, "metricx_qe_score": 0.8017590045928955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量评估方面效果良好,但对话质量有多个方面。", "metrics": {"bleu_score": 32.90988790846551, "chrf_score": 27.031722255929708, "xcomet_score": 0.9895071983337402, "xcomet_qe_score": 0.9802204370498657, "metricx_score": 0.3671559989452362, "metricx_qe_score": 0.5227299928665161, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估聊天质量的多个维度,以更细致地了解模型的优缺点。", "metrics": {"bleu_score": 66.88495446062248, "chrf_score": 61.80778548472513, "xcomet_score": 0.9797821044921875, "xcomet_qe_score": 0.9579282999038696, "metricx_score": 0.6259473562240601, "metricx_qe_score": 0.5924166440963745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地请人类评判员评估对话质量的多个维度,例如模型响应的相关性,使用现有的比较或利克特量表方法。", "metrics": {"bleu_score": 51.627064804362604, "chrf_score": 43.495430704965585, "xcomet_score": 0.9529657363891602, "xcomet_qe_score": 0.8976569175720215, "metricx_score": 1.4172371625900269, "metricx_qe_score": 2.006840229034424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 45.15558127464808, "xcomet_score": 0.9024549126625061, "xcomet_qe_score": 0.8712908625602722, "metricx_score": 1.308674931526184, "metricx_qe_score": 1.4190480709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表达某些行为来减少人工评估的主观性,例如提供无关信息或自相矛盾。", "metrics": {"bleu_score": 71.76667553079572, "chrf_score": 67.39008119761142, "xcomet_score": 0.9517678022384644, "xcomet_qe_score": 0.956770658493042, "metricx_score": 1.7646541595458984, "metricx_qe_score": 2.3994054794311523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将这种方法称为聊天行为标注,简称 ABC 评估。", "metrics": {"bleu_score": 17.58818104423743, "chrf_score": 22.233430578535536, "xcomet_score": 0.7571465969085693, "xcomet_qe_score": 0.7884925603866577, "metricx_score": 1.722132682800293, "metricx_qe_score": 1.2225062847137451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法是为了全面覆盖最近文献中建议影响聊天质", "metrics": {"bleu_score": 37.790981116035454, "chrf_score": 35.34501888793334, "xcomet_score": 0.7786145806312561, "xcomet_qe_score": 0.7659275531768799, "metricx_score": 7.626219749450684, "metricx_qe_score": 4.940544128417969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "量聊天模型的行为。", "metrics": {"bleu_score": 12.797424160078423, "chrf_score": 12.357163452915307, "xcomet_score": 0.22601714730262756, "xcomet_qe_score": 0.15098589658737183, "metricx_score": 15.158414840698242, "metricx_qe_score": 19.474037170410156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABC 评估聊天模型是否忽略其对话伙伴或说出无关紧要的话,是否自相矛盾或与对话伙伴矛盾,是否编造不正确的事实或违反常识知识,以及模型在表现同理心方面是否成功或失败。", "metrics": {"bleu_score": 35.262500291149614, "chrf_score": 28.644717323948328, "xcomet_score": 0.7578316330909729, "xcomet_qe_score": 0.6813843846321106, "metricx_score": 1.984081745147705, "metricx_qe_score": 3.175344944000244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定最有效的评估类型,我们选择了四种最先进的聊天模型,并使用 ABC 评估对每个模型的 100 个人工聊天对话进行评估。", "metrics": {"bleu_score": 56.19247757747736, "chrf_score": 51.347677733609274, "xcomet_score": 0.7987600564956665, "xcomet_qe_score": 0.8068609237670898, "metricx_score": 3.8947839736938477, "metricx_qe_score": 3.3227219581604004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了进行比较,我们还使用三种现有方法对这些对话进行了评估:回合级利克特评分、对话级利克特评分和对话级配对比较。", "metrics": {"bleu_score": 48.35793814427311, "chrf_score": 40.70470930611274, "xcomet_score": 0.864484429359436, "xcomet_qe_score": 0.8518596887588501, "metricx_score": 2.308471918106079, "metricx_qe_score": 2.3966562747955322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于现有方法中的每一种,我们收集了八个最常见对话测量指标的评估,因为这是评估聊天模型多维度的标准做法。从", "metrics": {"bleu_score": 45.76468062119161, "chrf_score": 36.853247103544945, "xcomet_score": 0.7345575094223022, "xcomet_qe_score": 0.726728081703186, "metricx_score": 6.560762882232666, "metricx_qe_score": 4.371114730834961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这些评估结果的分析中,我们发现 ABC 评估行为标签总体上比现有方法收集的标签更可靠,具体衡量标准是 100 个双重标", "metrics": {"bleu_score": 42.41741344059748, "chrf_score": 35.164904057567306, "xcomet_score": 0.5548163056373596, "xcomet_qe_score": 0.6562867164611816, "metricx_score": 6.677970886230469, "metricx_qe_score": 6.034756660461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "注对话的标注者间一致性。此外,ABC 评估标签比现有方法产生的指标更能预测整体对话质量,如简单线性回归分析所示。", "metrics": {"bleu_score": 43.708609844771814, "chrf_score": 38.46167946001465, "xcomet_score": 0.6239928603172302, "xcomet_qe_score": 0.5099692940711975, "metricx_score": 5.194802284240723, "metricx_qe_score": 5.9498515129089355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到,测量自相矛盾和对话伙伴矛盾的回合比例分别解释了 5% 和 10% 的对话质量,而平均利克特一致性得分仅解释了 4% 或更少。", "metrics": {"bleu_score": 71.09876534227625, "chrf_score": 68.13120728208781, "xcomet_score": 0.7087150812149048, "xcomet_qe_score": 0.6441070437431335, "metricx_score": 4.426095962524414, "metricx_qe_score": 4.868851661682129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们检查了每个评估指标是否捕获了聊天质量的独特方面,使用逐步线性回归。您", "metrics": {"bleu_score": 56.48017943848077, "chrf_score": 51.163356279217545, "xcomet_score": 0.674066960811615, "xcomet_qe_score": 0.6427021026611328, "metricx_score": 5.446177959442139, "metricx_qe_score": 3.2162790298461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,所有 ABC 评估指标的组合解释了超过 25% 的对话质量,当您逐个删除指标时,大多数都会导致失去大量关于质量的信息。", "metrics": {"bleu_score": 46.139873343942085, "chrf_score": 41.774034672742985, "xcomet_score": 0.941872239112854, "xcomet_qe_score": 0.8555254936218262, "metricx_score": 1.737130045890808, "metricx_qe_score": 2.9895875453948975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有回合级利克特指标的组合解释的质量远少,这些指标中较少的有独特信息。这些可靠、信息丰", "metrics": {"bleu_score": 30.718058352837993, "chrf_score": 25.72958997817207, "xcomet_score": 0.4442597031593323, "xcomet_qe_score": 0.33634456992149353, "metricx_score": 10.496964454650879, "metricx_qe_score": 7.965682029724121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "富且独特的 ABC 评估指标使我们能够以高于先前方法能达到的分辨率评估对话人工智能。您可以从我们", "metrics": {"bleu_score": 3.497478755875881, "chrf_score": 7.626925172152034, "xcomet_score": 0.24052605032920837, "xcomet_qe_score": 0.21119412779808044, "metricx_score": 10.306352615356445, "metricx_qe_score": 7.496775150299072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实验的结果中看到,仍然存在几个挑战,并且这些挑战已被精确量化。", "metrics": {"bleu_score": 16.144314037632192, "chrf_score": 19.037813813778005, "xcomet_score": 0.8459086418151855, "xcomet_qe_score": 0.8832225203514099, "metricx_score": 1.5499489307403564, "metricx_qe_score": 1.8149698972702026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人大约有 20% 的响应违反了常识。", "metrics": {"bleu_score": 53.67329609848117, "chrf_score": 46.47466158174103, "xcomet_score": 0.8597372770309448, "xcomet_qe_score": 0.8950106501579285, "metricx_score": 0.9239363074302673, "metricx_qe_score": 1.4850273132324219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在大约 15% 的响应中产生无关信息,并且它们自相矛盾或与对话伙伴矛盾大约 10% 的时间。", "metrics": {"bleu_score": 47.141269885184826, "chrf_score": 45.11372297555742, "xcomet_score": 0.7017916440963745, "xcomet_qe_score": 0.655095100402832, "metricx_score": 5.935412406921387, "metricx_qe_score": 5.466532230377197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速改进,这些错误率在新发布的模型中可能会降低,因为我们的评估进行。", "metrics": {"bleu_score": 37.70950419978435, "chrf_score": 32.46967904862642, "xcomet_score": 0.8070129752159119, "xcomet_qe_score": 0.7800471782684326, "metricx_score": 6.465085506439209, "metricx_qe_score": 6.075578212738037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更说明了追求可靠且精确的评估指标以比较模型的重要性。", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 40.91703962752154, "xcomet_score": 0.9977174997329712, "xcomet_qe_score": 0.9871809482574463, "metricx_score": 0.910224437713623, "metricx_qe_score": 1.0445787906646729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABC 评估能被该领域的其他人士作为朝此方向迈出的有意义一步,我们", "metrics": {"bleu_score": 39.3030918547887, "chrf_score": 35.145369163070114, "xcomet_score": 0.6918351650238037, "xcomet_qe_score": 0.7906167507171631, "metricx_score": 7.1713337898254395, "metricx_qe_score": 3.4880287647247314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期待看到未来几个月和几年对话人工智能的进步。", "metrics": {"bleu_score": 42.76621024875847, "chrf_score": 35.23488428144618, "xcomet_score": 0.8850691914558411, "xcomet_qe_score": 0.8841658234596252, "metricx_score": 1.4395074844360352, "metricx_qe_score": 2.0206053256988525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好!我叫Kaio Yin,将向大家展示我们题为《何时翻译需要上下文?", "metrics": {"bleu_score": 21.51667653216852, "chrf_score": 25.824127688688343, "xcomet_score": 0.8684277534484863, "xcomet_qe_score": 0.8931677937507629, "metricx_score": 3.2344343662261963, "metricx_qe_score": 3.767432928085327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语探索》的研究。", "metrics": {"bleu_score": 20.80375826108923, "chrf_score": 22.61223962051197, "xcomet_score": 0.8090909719467163, "xcomet_qe_score": 0.797600269317627, "metricx_score": 2.904913902282715, "metricx_qe_score": 4.290694713592529, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Patrick Fernandes、Emmy Liu、Andre F.D. Martins和Graham Newbig合作完成的。", "metrics": {"bleu_score": 48.80802506862367, "chrf_score": 76.52036383649413, "xcomet_score": 0.8449053764343262, "xcomet_qe_score": 0.9031828045845032, "metricx_score": 2.8507447242736816, "metricx_qe_score": 2.416632652282715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译依赖于上下文。", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 36.07623857623858, "xcomet_score": 0.9989787340164185, "xcomet_qe_score": 0.9933617115020752, "metricx_score": 0.15426957607269287, "metricx_qe_score": 0.2833571434020996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子中如何翻译“mole”?", "metrics": {"bleu_score": 22.828187338648245, "chrf_score": 31.551755076909455, "xcomet_score": 0.9850084781646729, "xcomet_qe_score": 0.9491152167320251, "metricx_score": 1.2625046968460083, "metricx_qe_score": 2.6962082386016846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句是“如果部长们发现,事情可能会变得危险”,那么“mole”指的是间谍。", "metrics": {"bleu_score": 19.978883402526694, "chrf_score": 13.644254452312445, "xcomet_score": 0.9628349542617798, "xcomet_qe_score": 0.953872799873352, "metricx_score": 3.1619081497192383, "metricx_qe_score": 4.3693342208862305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,会是什么严重的问题吗?”那么“mole”指的是胎记。", "metrics": {"bleu_score": 19.263460320207695, "chrf_score": 17.261769403617038, "xcomet_score": 0.9692732095718384, "xcomet_qe_score": 0.9591715931892395, "metricx_score": 2.0058529376983643, "metricx_qe_score": 2.947087287902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据上下文,词的意义会改变,因此其翻译也会相应地改变。", "metrics": {"bleu_score": 33.42062697386429, "chrf_score": 28.4844956719284, "xcomet_score": 0.9999362230300903, "xcomet_qe_score": 0.9995855093002319, "metricx_score": 0.49668896198272705, "metricx_qe_score": 0.40004923939704895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型能多好地翻译这类案例是相当困难的。", "metrics": {"bleu_score": 23.352648136133084, "chrf_score": 19.463881263810244, "xcomet_score": 0.8841871023178101, "xcomet_qe_score": 0.8394544124603271, "metricx_score": 1.3556731939315796, "metricx_qe_score": 2.073239326477051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有少量翻译依赖于上下文,这导致语料库级别的指标如BLEU无法捕捉到这些翻译。有", "metrics": {"bleu_score": 34.110214247102704, "chrf_score": 29.480886820844283, "xcomet_score": 0.8467733860015869, "xcomet_qe_score": 0.8244296312332153, "metricx_score": 5.632529258728027, "metricx_qe_score": 3.902894973754883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些人建议对上下文依赖的翻译进行定向评估,但这些资源只支持有限类型的上下文依赖翻译和有限的语言集,因为它们通常依赖于领域知识和人工整理。", "metrics": {"bleu_score": 76.6876257739812, "chrf_score": 73.92527607716474, "xcomet_score": 0.7202706933021545, "xcomet_qe_score": 0.6518251895904541, "metricx_score": 4.018618106842041, "metricx_qe_score": 3.599724292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们尝试回答两个问题。", "metrics": {"bleu_score": 26.197527109265895, "chrf_score": 22.468359719422164, "xcomet_score": 0.9924654960632324, "xcomet_qe_score": 0.9918102025985718, "metricx_score": 0.7868586778640747, "metricx_qe_score": 0.4339318871498108, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,何时翻译需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9996569156646729, "xcomet_qe_score": 0.9977697134017944, "metricx_score": 0.15239903330802917, "metricx_qe_score": 0.3020760715007782, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型能多好地处理这些案例?", "metrics": {"bleu_score": 17.150296156301636, "chrf_score": 15.150667871784016, "xcomet_score": 0.9320632219314575, "xcomet_qe_score": 0.8331296443939209, "metricx_score": 1.527062177658081, "metricx_qe_score": 1.2086567878723145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们开始测量词在翻译过程中对上下文的依赖程度。", "metrics": {"bleu_score": 71.05425273104096, "chrf_score": 66.97898486439455, "xcomet_score": 0.9217473268508911, "xcomet_qe_score": 0.9952653646469116, "metricx_score": 4.2294135093688965, "metricx_qe_score": 3.9998042583465576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在先前的工作中,我们引入了CXMI作为机器翻译模型上下文使用量的度量。", "metrics": {"bleu_score": 57.19408603967074, "chrf_score": 59.345310377550106, "xcomet_score": 0.8919402360916138, "xcomet_qe_score": 0.888935387134552, "metricx_score": 1.7021788358688354, "metricx_qe_score": 1.811134696006775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量上下文C在给定源X时对目标Y提供的信息量来实现。可以将CXMI视为给模型提供上下文获得的信息量。", "metrics": {"bleu_score": 36.91718446512699, "chrf_score": 35.1939206400357, "xcomet_score": 0.8750523328781128, "xcomet_qe_score": 0.773840069770813, "metricx_score": 3.093313455581665, "metricx_qe_score": 3.1412193775177, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本工作中,我们将CXMI扩展为点CXMI,它可以在句子级别或词级别测量上下文使用量。", "metrics": {"bleu_score": 40.23606886263554, "chrf_score": 34.627415297820264, "xcomet_score": 0.7712787389755249, "xcomet_qe_score": 0.7300376296043396, "metricx_score": 3.554530382156372, "metricx_qe_score": 3.5163750648498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将具有高P6MI的词视为需要上下文进行翻译的词。", "metrics": {"bleu_score": 76.97570474571569, "chrf_score": 63.15976471191911, "xcomet_score": 0.7848432064056396, "xcomet_qe_score": 0.7793549299240112, "metricx_score": 5.473179817199707, "metricx_qe_score": 5.734285831451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析具有高P6MI的词,以寻找这些词之间的模式。", "metrics": {"bleu_score": 45.43681957056484, "chrf_score": 39.83839148148917, "xcomet_score": 0.8267276287078857, "xcomet_qe_score": 0.8045225143432617, "metricx_score": 5.831970691680908, "metricx_qe_score": 6.435885429382324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在从英语翻译成14种不同语言的TED演讲文本上进行分析。", "metrics": {"bleu_score": 76.61850354609348, "chrf_score": 73.91893440744015, "xcomet_score": 0.9475671052932739, "xcomet_qe_score": 0.9897099733352661, "metricx_score": 2.3233256340026855, "metricx_qe_score": 1.8528029918670654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在三个不同级别进行分析。", "metrics": {"bleu_score": 60.659974333376745, "chrf_score": 50.87108481075532, "xcomet_score": 0.9124049544334412, "xcomet_qe_score": 0.9270819425582886, "metricx_score": 0.6496804356575012, "metricx_qe_score": 0.9960294961929321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有高平均PCXMI的词性标签。", "metrics": {"bleu_score": 34.6697783111003, "chrf_score": 31.708611730813757, "xcomet_score": 0.8655228018760681, "xcomet_qe_score": 0.787142276763916, "metricx_score": 2.1782705783843994, "metricx_qe_score": 2.6345884799957275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够发现,例如,阿拉伯语中具有相对高PCXMI的双数代词。", "metrics": {"bleu_score": 48.28175506933025, "chrf_score": 39.585483690391925, "xcomet_score": 0.9287021160125732, "xcomet_qe_score": 0.854761004447937, "metricx_score": 2.8516502380371094, "metricx_qe_score": 3.6580028533935547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为,因为英语没有双数代词,所以在翻译成阿拉伯语时,需要上下文来确定代词是否为双数。", "metrics": {"bleu_score": 67.82096900887001, "chrf_score": 60.786115094593896, "xcomet_score": 0.9891806840896606, "xcomet_qe_score": 0.976445734500885, "metricx_score": 0.7782354354858398, "metricx_qe_score": 1.2585780620574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似地,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 85.94148359295417, "chrf_score": 87.96567669989582, "xcomet_score": 0.9981253147125244, "xcomet_qe_score": 0.9977245330810547, "metricx_score": 0.5655370354652405, "metricx_qe_score": 0.7977930307388306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们查看在所有不同出现中具有高平均PCSXMI的词汇项。", "metrics": {"bleu_score": 32.16397363858737, "chrf_score": 26.299088639284353, "xcomet_score": 0.7705497145652771, "xcomet_qe_score": 0.7615655660629272, "metricx_score": 4.766525745391846, "metricx_qe_score": 5.08305025100708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别像这里这样的案例,在中文中,你需要上下文来翻译专有名词,以确保在文档中使用相同的翻译。", "metrics": {"bleu_score": 38.8826168362793, "chrf_score": 33.44767589428617, "xcomet_score": 0.9096986651420593, "xcomet_qe_score": 0.8894690275192261, "metricx_score": 0.8207578659057617, "metricx_qe_score": 1.2309342622756958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似地,我们发现上下文有助于翻译正确的语气。", "metrics": {"bleu_score": 34.61771478013761, "chrf_score": 29.821649504854896, "xcomet_score": 0.8736151456832886, "xcomet_qe_score": 0.8331109881401062, "metricx_score": 0.9828057289123535, "metricx_qe_score": 0.8784390091896057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看具有高PCXMI的不同单个词。", "metrics": {"bleu_score": 32.37833370387542, "chrf_score": 29.843075134909398, "xcomet_score": 0.7649291753768921, "xcomet_qe_score": 0.6878296136856079, "metricx_score": 2.337725877761841, "metricx_qe_score": 4.912636756896973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别那些不能由词本身捕捉到的现象,而是表达在句子结构中,如省略号解析。", "metrics": {"bleu_score": 33.91823794424007, "chrf_score": 30.73209308857261, "xcomet_score": 0.9065487384796143, "xcomet_qe_score": 0.8405779600143433, "metricx_score": 1.4808056354522705, "metricx_qe_score": 1.7916843891143799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们利用分析结果来设计一个文档级翻译的基准。", "metrics": {"bleu_score": 56.70486642005394, "chrf_score": 51.356994246275846, "xcomet_score": 0.975649356842041, "xcomet_qe_score": 0.8571710586547852, "metricx_score": 0.9669630527496338, "metricx_qe_score": 1.284716248512268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别的五个话语现象中的每一个,我们创建自动识别与现象相关的词的标记器,我们称之为多语话语感知(MUDA)标记器。我们可以使用标记器识别与现象相关的词,我们称之为多语话语感知(MUDA)标记器。", "metrics": {"bleu_score": 24.39841182851992, "chrf_score": 36.754340519350535, "xcomet_score": 0.2715797424316406, "xcomet_qe_score": 0.1725323647260666, "metricx_score": 9.591726303100586, "metricx_qe_score": 11.425010681152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以使用标记器识别与现象相关的词,我们称之为多语话语感知(MUDA)标记器。我们可以使用标记器识别与现象相关的词,我们称之为多语话语感知(MUDA)标记器。", "metrics": {"bleu_score": 8.857800080700983, "chrf_score": 16.46539878750722, "xcomet_score": 0.21502196788787842, "xcomet_qe_score": 0.23898892104625702, "metricx_score": 17.163782119750977, "metricx_qe_score": 18.77916717529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以使用标记器识别与现象相关的词,我们称之为多语话语感知(MUDA)标记器。我们可以使用标记器识别与现象相关的词,我们称之为多语话语感知(MUDA)标记器。", "metrics": {"bleu_score": 1.869743740792544, "chrf_score": 5.135550667058155, "xcomet_score": 0.13569319248199463, "xcomet_qe_score": 0.14500121772289276, "metricx_score": 21.767160415649414, "metricx_qe_score": 21.95789337158203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以使用标记器识别与现象相关的词,我们称之为多语话语感知(MUDA)标记器。 我们可以使用标记器识别与现象相关的词,并使用标记器在我们要用于评估的平行语料库上应用标记器。我们在MUDA标记器识别出的上下文依赖示例上应用我们选择的翻译指标。", "metrics": {"bleu_score": 25.971936378376807, "chrf_score": 38.979329119601815, "xcomet_score": 0.5657914876937866, "xcomet_qe_score": 0.6672092080116272, "metricx_score": 7.398669242858887, "metricx_qe_score": 7.111776351928711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准和其他指标来评估文档级机器翻译的不同模型。", "metrics": {"bleu_score": 74.47546578095063, "chrf_score": 67.77489500299242, "xcomet_score": 0.9516571760177612, "xcomet_qe_score": 0.8063884973526001, "metricx_score": 0.8535319566726685, "metricx_qe_score": 1.0528981685638428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,对于BLEU,无上下文感知的模型表现最佳,", "metrics": {"bleu_score": 47.08987656584742, "chrf_score": 43.68553457262352, "xcomet_score": 0.8459144234657288, "xcomet_qe_score": 0.7875156402587891, "metricx_score": 2.1876132488250732, "metricx_qe_score": 1.7019582986831665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果使用COMET,则上下文感知的模型表现最佳。", "metrics": {"bleu_score": 29.003954043420627, "chrf_score": 36.56165136716793, "xcomet_score": 0.891223669052124, "xcomet_qe_score": 0.8675787448883057, "metricx_score": 1.668196678161621, "metricx_qe_score": 2.8829386234283447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用词F度量,则具有或不具有上下文的模型具有可比的性能。", "metrics": {"bleu_score": 25.624152350185835, "chrf_score": 22.178418287808704, "xcomet_score": 0.8172920942306519, "xcomet_qe_score": 0.7174409031867981, "metricx_score": 3.5899858474731445, "metricx_qe_score": 3.7658557891845703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果仅使用语料库级别的指标,很难确定最佳的文档级翻译系统。", "metrics": {"bleu_score": 68.19390759143751, "chrf_score": 60.74067203287008, "xcomet_score": 0.998979926109314, "xcomet_qe_score": 0.9943516254425049, "metricx_score": 0.6216239929199219, "metricx_qe_score": 0.7556805610656738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用MUDA基准来评估模型,我们发现在语气和词汇连贯性等某些话语现象上,上下文感知的模型准确性显著高于不使用上下文的模型。但这些模型在其他", "metrics": {"bleu_score": 45.92669350006531, "chrf_score": 43.90834353511475, "xcomet_score": 0.4106977581977844, "xcomet_qe_score": 0.3613179922103882, "metricx_score": 7.7235426902771, "metricx_qe_score": 8.918951034545898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现象如省略号、代词和动词形式上与未使用上下文的模型没有太大差异。", "metrics": {"bleu_score": 40.46075489403922, "chrf_score": 39.089823895432055, "xcomet_score": 0.6519442200660706, "xcomet_qe_score": 0.15404124557971954, "metricx_score": 3.532883644104004, "metricx_qe_score": 3.4593565464019775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这在一定程度上表明了文档级翻译需要更多进步的领域。", "metrics": {"bleu_score": 28.837997527432837, "chrf_score": 26.271142755185224, "xcomet_score": 0.996322751045227, "xcomet_qe_score": 0.9686914682388306, "metricx_score": 1.166683554649353, "metricx_qe_score": 1.3205678462982178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准显示,DeepL通常比Google Translate更适合文档级翻译。", "metrics": {"bleu_score": 50.0384380493873, "chrf_score": 45.94893528671856, "xcomet_score": 0.829155683517456, "xcomet_qe_score": 0.7812429666519165, "metricx_score": 1.591350793838501, "metricx_qe_score": 2.1075897216796875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们在14个语言对上进行数据驱动的分析,以识别何时翻译需要上下文。然后,我们利用分析结果建立一个文档级机器翻译的基准,它可以帮助我们识别哪些话语现象模型能很好地处理,哪些翻译系统擅长文档级翻译。", "metrics": {"bleu_score": 45.05286964231707, "chrf_score": 39.434323883386625, "xcomet_score": 0.9252110719680786, "xcomet_qe_score": 0.924785315990448, "metricx_score": 2.084836959838867, "metricx_qe_score": 2.8265323638916016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9842581748962402, "xcomet_qe_score": 0.9662535190582275, "metricx_score": 0.4675371050834656, "metricx_qe_score": 0.26872068643569946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多伦多见!", "metrics": {"bleu_score": 36.70124608961282, "chrf_score": 32.21385348268315, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6152194142341614, "metricx_qe_score": 0.7129020690917969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是Yanis Lavrac,我将向您展示我们在Dr. BERT方面的工作,这是一个针对生物医学和临床领域的强大预训练法语模型。", "metrics": {"bleu_score": 28.044997525049627, "chrf_score": 34.62251223577116, "xcomet_score": 0.7744984030723572, "xcomet_qe_score": 0.7419604063034058, "metricx_score": 2.468427896499634, "metricx_qe_score": 2.401987075805664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这次演讲中,我们首先将讨论医疗领域的语言建模。", "metrics": {"bleu_score": 49.86020415687824, "chrf_score": 40.96480408747695, "xcomet_score": 0.9956834316253662, "xcomet_qe_score": 1.0, "metricx_score": 0.6115450263023376, "metricx_qe_score": 0.717415988445282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们推出了第一个法语生物医学模型,名为Dr. BERT,该模型基于Roberta,并在NACHOS上进行训练,这是一个从网络上抓取的医疗数据集。", "metrics": {"bleu_score": 41.12772568414993, "chrf_score": 38.3023596807149, "xcomet_score": 0.8369053602218628, "xcomet_qe_score": 0.8060691356658936, "metricx_score": 2.1448893547058105, "metricx_qe_score": 2.2395496368408203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了对模型在多种预训练设置和数据源的比较。接下", "metrics": {"bleu_score": 46.445316553607185, "chrf_score": 43.775168612626224, "xcomet_score": 0.7895663976669312, "xcomet_qe_score": 0.7964954376220703, "metricx_score": 4.086313724517822, "metricx_qe_score": 3.1657180786132812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来,我们将展示我们在11个法语生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 66.29320430377793, "chrf_score": 63.17116288440039, "xcomet_score": 0.7208652496337891, "xcomet_qe_score": 0.6793785095214844, "metricx_score": 2.566291093826294, "metricx_qe_score": 3.4281678199768066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们将总结实验并详细说明如何访问模型。", "metrics": {"bleu_score": 10.968838539623139, "chrf_score": 15.315173472793223, "xcomet_score": 0.890691876411438, "xcomet_qe_score": 0.8899992108345032, "metricx_score": 0.35512322187423706, "metricx_qe_score": 0.2591438591480255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务最有效的手段之一,与历史静态和上下文方法如Word2Vec、FastText或NWO相比,性能有了显著提高。", "metrics": {"bleu_score": 52.46865459165307, "chrf_score": 51.17482068965599, "xcomet_score": 0.8180837631225586, "xcomet_qe_score": 0.8446613550186157, "metricx_score": 3.1656949520111084, "metricx_qe_score": 2.8256263732910156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此后,该模型已被适应到许多其他语言,如法语的Camembert,以及生物医学的Permt-BERT和Bio-BERT,临床的Clinical-BERT,但主要是在英语方面。专门", "metrics": {"bleu_score": 30.495657454708578, "chrf_score": 39.034782184655946, "xcomet_score": 0.42999914288520813, "xcomet_qe_score": 0.39591091871261597, "metricx_score": 7.242345809936523, "metricx_qe_score": 4.9210591316223145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的模型非常稀缺,通常由于缺乏域内数据而基于连续预训练。", "metrics": {"bleu_score": 44.53503875882245, "chrf_score": 36.39108825650997, "xcomet_score": 0.8885166049003601, "xcomet_qe_score": 0.8099962472915649, "metricx_score": 0.9956870675086975, "metricx_qe_score": 1.450932264328003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,法语在现在之前没有开源的生物医学模型。", "metrics": {"bleu_score": 19.63493255544149, "chrf_score": 21.697856641943748, "xcomet_score": 0.8022216558456421, "xcomet_qe_score": 0.756313681602478, "metricx_score": 1.633644461631775, "metricx_qe_score": 2.1670143604278564, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们问自己,对于广泛的使用范围,最合适的数据来源是什么,以及当前的数据是否能很好地替代临床数据。", "metrics": {"bleu_score": 15.834011757645827, "chrf_score": 17.912261722785885, "xcomet_score": 0.8077314496040344, "xcomet_qe_score": 0.8446472883224487, "metricx_score": 1.485682487487793, "metricx_qe_score": 1.521712064743042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Dr. BERT与我们的Schubert模型进行比较,后者基于从我们医院获得的匿名数据。", "metrics": {"bleu_score": 48.4244197440829, "chrf_score": 43.06181440729242, "xcomet_score": 0.791960597038269, "xcomet_qe_score": 0.8040000796318054, "metricx_score": 3.925596237182617, "metricx_qe_score": 5.378625392913818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还问自己,训练一个法语专业模型需要多少数据?", "metrics": {"bleu_score": 42.75045461604612, "chrf_score": 35.85149302987164, "xcomet_score": 0.8875302076339722, "xcomet_qe_score": 0.8084338307380676, "metricx_score": 1.5735678672790527, "metricx_qe_score": 1.5081255435943604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4GB、8GB还是4GB的", "metrics": {"bleu_score": 60.042877124855906, "chrf_score": 69.29870052559151, "xcomet_score": 0.666922926902771, "xcomet_qe_score": 0.6144264936447144, "metricx_score": 5.608034133911133, "metricx_qe_score": 4.674619197845459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "RAM?Schubert的第一个版本是一个临床模型,基于4GB来自临床笔记的句子。Schubert的最终版本则混合使用了4GB的NACHOS子集和4GB的临床笔记。", "metrics": {"bleu_score": 17.986428878635543, "chrf_score": 23.8438927367602, "xcomet_score": 0.34958416223526, "xcomet_qe_score": 0.27321353554725647, "metricx_score": 14.169637680053711, "metricx_qe_score": 17.0294246673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这一比较,我们还引入了三个基于连续预训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 58.82232157963849, "chrf_score": 50.878033534428226, "xcomet_score": 0.9619630575180054, "xcomet_qe_score": 0.9482908248901367, "metricx_score": 1.0035912990570068, "metricx_qe_score": 1.270911455154419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于Camembert的权重,并在4GB的NACHOS子集上进行训练。", "metrics": {"bleu_score": 48.326978309062184, "chrf_score": 56.1389614882262, "xcomet_score": 0.7147654294967651, "xcomet_qe_score": 0.6740869283676147, "metricx_score": 4.743238925933838, "metricx_qe_score": 6.128063201904297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也基于Camembert,但使用4GB的Permt-BERT", "metrics": {"bleu_score": 3.0972423040187773, "chrf_score": 19.048731121479587, "xcomet_score": 0.49742791056632996, "xcomet_qe_score": 0.44345471262931824, "metricx_score": 16.277690887451172, "metricx_qe_score": 15.436989784240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5317404270172119, "xcomet_qe_score": 0.13283278048038483, "metricx_score": 2.7005295753479004, "metricx_qe_score": 6.427151679992676, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "、Bio", "metrics": {"bleu_score": 0.0, "chrf_score": 0.6009615384615385, "xcomet_score": 0.24537497758865356, "xcomet_qe_score": 0.13827361166477203, "metricx_score": 22.545597076416016, "metricx_qe_score": 25.0, "linguapy_score": [1, "BOSNIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "BERT和ClinicalBERT进行训练。", "metrics": {"bleu_score": 0.4987345970899204, "chrf_score": 17.60742402485843, "xcomet_score": 0.12720654904842377, "xcomet_qe_score": 0.14127184450626373, "metricx_score": 20.311594009399414, "metricx_qe_score": 20.749189376831055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,模型在模型训练数据与任务数据性质相同的情况下表现最佳。", "metrics": {"bleu_score": 48.31412214609538, "chrf_score": 38.96455176367981, "xcomet_score": 0.9943686723709106, "xcomet_qe_score": 0.9852900505065918, "metricx_score": 0.5652689933776855, "metricx_qe_score": 0.8513826727867126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以观察到,来自异质来源的数据似乎更具多功能性。", "metrics": {"bleu_score": 46.39035504542229, "chrf_score": 42.9108615160041, "xcomet_score": 0.8267236948013306, "xcomet_qe_score": 0.8248207569122314, "metricx_score": 1.177579641342163, "metricx_qe_score": 1.0418063402175903, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多数据会带来更好的性能。", "metrics": {"bleu_score": 32.1593960910315, "chrf_score": 26.729136961169715, "xcomet_score": 0.937077522277832, "xcomet_qe_score": 0.9789898991584778, "metricx_score": 2.59717059135437, "metricx_qe_score": 3.111323356628418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,从零开始的预训练在大多数任务上似乎获得了更高的性能。", "metrics": {"bleu_score": 41.808355290969295, "chrf_score": 34.50525321377607, "xcomet_score": 0.9249454736709595, "xcomet_qe_score": 0.8787604570388794, "metricx_score": 3.247709274291992, "metricx_qe_score": 4.313430309295654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们使用Permt-BERT的权重和分词器,在4GB的NACHOS子集上进行的连续预训练实验,与从零开始训练的Dr. BERT 4GB获得了可比", "metrics": {"bleu_score": 29.41161264387877, "chrf_score": 36.27571271383023, "xcomet_score": 0.3848015069961548, "xcomet_qe_score": 0.38097789883613586, "metricx_score": 6.336251735687256, "metricx_qe_score": 6.61245059967041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的结果,而基于Camembert权重和分词器的模型则存在稳定性问题。", "metrics": {"bleu_score": 29.070135801715644, "chrf_score": 31.47709096070145, "xcomet_score": 0.6126872897148132, "xcomet_qe_score": 0.5667730569839478, "metricx_score": 6.364688873291016, "metricx_qe_score": 6.3569793701171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们的专有系统在11个下游任务中的9个上表现更好,整体上超越了通用模型(这里是Camembert)的结果。", "metrics": {"bleu_score": 39.2286308160269, "chrf_score": 35.803345033167766, "xcomet_score": 0.7702652215957642, "xcomet_qe_score": 0.8044965267181396, "metricx_score": 3.429050922393799, "metricx_qe_score": 2.4912493228912354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,更专业的数据更好,但可扩展性差。所有从", "metrics": {"bleu_score": 23.306134400607018, "chrf_score": 20.8571506334681, "xcomet_score": 0.7683857679367065, "xcomet_qe_score": 0.7681806683540344, "metricx_score": 7.670226573944092, "metricx_qe_score": 5.527776718139648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "NACHOS获得的预训练模型都在UGIMFACE上免费提供,所有训练脚本都在我们的GitHub仓库中。", "metrics": {"bleu_score": 31.98524441671383, "chrf_score": 35.63446263231796, "xcomet_score": 0.6410419344902039, "xcomet_qe_score": 0.7056010961532593, "metricx_score": 6.022807598114014, "metricx_qe_score": 5.93941068649292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,感谢您的聆听,我们期待在多伦多海报会议上与您交流。", "metrics": {"bleu_score": 28.091282586130962, "chrf_score": 28.82331839749424, "xcomet_score": 0.63108891248703, "xcomet_qe_score": 0.7379944920539856, "metricx_score": 2.060292959213257, "metricx_qe_score": 1.8816965818405151, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9831734895706177, "xcomet_qe_score": 0.9616916179656982, "metricx_score": 0.24903088808059692, "metricx_qe_score": 0.24614575505256653, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是马蒂亚斯·林德曼(Matthias Lindemann),今天我将简要介绍我们关于使用多集标记和潜在置换进行树结构之外的组合泛化论文。", "metrics": {"bleu_score": 35.16101348848835, "chrf_score": 52.01922970644578, "xcomet_score": 0.8006042838096619, "xcomet_qe_score": 0.8841995000839233, "metricx_score": 2.2058088779449463, "metricx_qe_score": 2.168952465057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我与导师亚历山大·科勒(Alexander Koller)和伊万·蒂托夫(Ivan Titov)的合作成果。", "metrics": {"bleu_score": 18.96550847075289, "chrf_score": 58.89889523636156, "xcomet_score": 0.995305061340332, "xcomet_qe_score": 0.9606826901435852, "metricx_score": 1.153705358505249, "metricx_qe_score": 1.2234477996826172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深层递归和训练时单独见过的短语组合的能力。", "metrics": {"bleu_score": 65.05791651465506, "chrf_score": 60.736914398614005, "xcomet_score": 0.8080687522888184, "xcomet_qe_score": 0.7279231548309326, "metricx_score": 4.24083137512207, "metricx_qe_score": 5.63653564453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能如下所示。", "metrics": {"bleu_score": 70.07637953409846, "chrf_score": 60.13480369940909, "xcomet_score": 0.908198356628418, "xcomet_qe_score": 0.8913741707801819, "metricx_score": 0.9897307753562927, "metricx_qe_score": 1.7725532054901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与往常一样,我们有一个训练语句集,", "metrics": {"bleu_score": 34.7403173905042, "chrf_score": 27.903828197945842, "xcomet_score": 0.9925757646560669, "xcomet_qe_score": 0.95769202709198, "metricx_score": 0.8409402966499329, "metricx_qe_score": 1.0273027420043945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“女孩睡着了”和", "metrics": {"bleu_score": 7.55205501036964, "chrf_score": 6.2681045562041975, "xcomet_score": 0.7490692138671875, "xcomet_qe_score": 0.6291144490242004, "metricx_score": 4.057464599609375, "metricx_qe_score": 1.1286463737487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道女孩睡着了”。这些", "metrics": {"bleu_score": 14.092865659855665, "chrf_score": 9.344717991362169, "xcomet_score": 0.8641207218170166, "xcomet_qe_score": 0.8054601550102234, "metricx_score": 5.602951526641846, "metricx_qe_score": 1.5167977809906006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语句与代表其核心意义的逻辑形式配对。", "metrics": {"bleu_score": 10.591379891265163, "chrf_score": 13.287090168286268, "xcomet_score": 0.9944337606430054, "xcomet_qe_score": 0.9908231496810913, "metricx_score": 1.0372918844223022, "metricx_qe_score": 0.6989502906799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集不来自同一分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 61.55490299373211, "chrf_score": 56.18122036913612, "xcomet_score": 0.8613531589508057, "xcomet_qe_score": 0.8393374681472778, "metricx_score": 1.1579457521438599, "metricx_qe_score": 2.1692683696746826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练期间见过更浅的递归,并在测试时遇到更深的递归例子。", "metrics": {"bleu_score": 34.16651172782408, "chrf_score": 27.931544273456037, "xcomet_score": 0.8292980194091797, "xcomet_qe_score": 0.871649980545044, "metricx_score": 3.944899320602417, "metricx_qe_score": 6.552485466003418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单的序列到序列模型在这个超出分布的泛化方面遇到困难,经常产生与输入脱节的输出。", "metrics": {"bleu_score": 25.18196793087547, "chrf_score": 21.99751738298325, "xcomet_score": 0.8380544185638428, "xcomet_qe_score": 0.7890511751174927, "metricx_score": 1.8594940900802612, "metricx_qe_score": 1.4926401376724243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们经常无法再现输入和输出之间的系统对应关系,例如在例子中用颜色标注的对应关系。", "metrics": {"bleu_score": 68.19037029631254, "chrf_score": 65.45397668766402, "xcomet_score": 0.9983716011047363, "xcomet_qe_score": 0.9946507215499878, "metricx_score": 0.8225933313369751, "metricx_qe_score": 1.043911099433899, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的解决方法是将树结构集成到模型中。", "metrics": {"bleu_score": 36.173905261890994, "chrf_score": 28.826001254388807, "xcomet_score": 0.9960500001907349, "xcomet_qe_score": 1.0, "metricx_score": 0.6115349531173706, "metricx_qe_score": 0.6099581718444824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树旨在捕捉与逻辑形式相关联的语句组合过程。", "metrics": {"bleu_score": 20.50289590329108, "chrf_score": 19.14418530182413, "xcomet_score": 0.943203330039978, "xcomet_qe_score": 0.8859543204307556, "metricx_score": 3.4509806632995605, "metricx_qe_score": 4.046189308166504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这效果很好,但树结构通常不给定,需要以某种方式获取。", "metrics": {"bleu_score": 30.99874754267577, "chrf_score": 27.08894242014064, "xcomet_score": 0.8988077640533447, "xcomet_qe_score": 0.9330195188522339, "metricx_score": 1.2462372779846191, "metricx_qe_score": 2.418480157852173, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能很复杂,有时是计算上昂贵的过程。", "metrics": {"bleu_score": 40.96812701940896, "chrf_score": 34.01761889982844, "xcomet_score": 0.9011823534965515, "xcomet_qe_score": 0.8541555404663086, "metricx_score": 1.9126591682434082, "metricx_qe_score": 3.017669439315796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这需要对逻辑形式进行大量特定形式主义的预处理,例如处理变量符号。", "metrics": {"bleu_score": 55.37662023325211, "chrf_score": 45.867427163388356, "xcomet_score": 0.9305422306060791, "xcomet_qe_score": 0.9285144209861755, "metricx_score": 0.7424705624580383, "metricx_qe_score": 1.181286334991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树结构也可能涉及专业的语法归纳程序。", "metrics": {"bleu_score": 47.18480720883561, "chrf_score": 40.98658147892792, "xcomet_score": 0.9742821455001831, "xcomet_qe_score": 0.9514880180358887, "metricx_score": 2.7784695625305176, "metricx_qe_score": 3.1202640533447266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,并引入一个神经序列到序列模型,直接建模输入片段和输出片段之间的对应关系。", "metrics": {"bleu_score": 51.69781495581383, "chrf_score": 41.25239278365947, "xcomet_score": 0.8205912113189697, "xcomet_qe_score": 0.8356973528862, "metricx_score": 1.4722024202346802, "metricx_qe_score": 1.3875257968902588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了不依赖树结构而对更深层递归进行强泛化。", "metrics": {"bleu_score": 28.372079445181935, "chrf_score": 24.77272440700757, "xcomet_score": 0.8571341037750244, "xcomet_qe_score": 0.8791906237602234, "metricx_score": 2.7644615173339844, "metricx_qe_score": 3.5201878547668457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法从输入中预测输出,分为两个步骤。", "metrics": {"bleu_score": 39.12548001702372, "chrf_score": 36.02111593486263, "xcomet_score": 0.9914646148681641, "xcomet_qe_score": 0.9724985361099243, "metricx_score": 0.7392832636833191, "metricx_qe_score": 0.7785685658454895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们为每个输入标记添加一个无序的多集标记,其中包含将出现在输出中的标记。", "metrics": {"bleu_score": 26.68519693710499, "chrf_score": 26.99681893785979, "xcomet_score": 0.9267834424972534, "xcomet_qe_score": 0.8809078931808472, "metricx_score": 3.0420072078704834, "metricx_qe_score": 2.9122447967529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个步骤之后,我们有了所有正确的标记,但它们没有顺序。", "metrics": {"bleu_score": 38.46629527341547, "chrf_score": 35.16045218106851, "xcomet_score": 0.8943989276885986, "xcomet_qe_score": 0.8182051181793213, "metricx_score": 3.0220513343811035, "metricx_qe_score": 3.545229434967041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在第二个步骤中,我们使用另一个模型来预测一个置换,将它们放入正确的顺序。", "metrics": {"bleu_score": 45.61980468404364, "chrf_score": 44.865437459189735, "xcomet_score": 0.9019025564193726, "xcomet_qe_score": 0.9144881367683411, "metricx_score": 3.8462274074554443, "metricx_qe_score": 3.7232236862182617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新方法来预测置换,对可能的置换不施加任何硬约束。", "metrics": {"bleu_score": 40.26629576096, "chrf_score": 34.0479159482521, "xcomet_score": 0.8808465003967285, "xcomet_qe_score": 0.9035129547119141, "metricx_score": 2.686962127685547, "metricx_qe_score": 2.2659647464752197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们的方法非常灵活且具有表达力。", "metrics": {"bleu_score": 26.180427723088908, "chrf_score": 23.960526609443274, "xcomet_score": 0.9092130064964294, "xcomet_qe_score": 0.8958611488342285, "metricx_score": 0.9481123685836792, "metricx_qe_score": 1.361093521118164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致如下工作。", "metrics": {"bleu_score": 27.15455510011743, "chrf_score": 24.80634794630479, "xcomet_score": 0.8453502655029297, "xcomet_qe_score": 0.7897835969924927, "metricx_score": 3.852965831756592, "metricx_qe_score": 3.096774101257324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从输出左侧到右侧移动,确定将每个多集标记放入哪个位置。", "metrics": {"bleu_score": 13.686241957588654, "chrf_score": 18.68120172717874, "xcomet_score": 0.8108972311019897, "xcomet_qe_score": 0.8025273084640503, "metricx_score": 2.6524314880371094, "metricx_qe_score": 2.3181352615356445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们简单地选择一个如红色高亮显示的标记。然后", "metrics": {"bleu_score": 45.81248750640266, "chrf_score": 42.43431965681909, "xcomet_score": 0.8085826635360718, "xcomet_qe_score": 0.8490591645240784, "metricx_score": 2.0775959491729736, "metricx_qe_score": 1.382994532585144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们跳到下一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 48.84411467539946, "chrf_score": 42.68549950615216, "xcomet_score": 0.7266725897789001, "xcomet_qe_score": 0.7849007844924927, "metricx_score": 4.576729774475098, "metricx_qe_score": 3.919360876083374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式跳到另一个多集标记。", "metrics": {"bleu_score": 21.60350910276294, "chrf_score": 23.815792922496783, "xcomet_score": 0.7083563804626465, "xcomet_qe_score": 0.7210420966148376, "metricx_score": 4.893303871154785, "metricx_qe_score": 5.635066509246826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程,直到访问完第一个阶段中的每个标记。", "metrics": {"bleu_score": 36.95217207741194, "chrf_score": 33.20486743821437, "xcomet_score": 0.8244295120239258, "xcomet_qe_score": 0.7162610292434692, "metricx_score": 2.0276312828063965, "metricx_qe_score": 3.549543619155884, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了给您展示实验结果的一小部分,我们在这里将我们的方法与其他没有树结构的模型在COGS基准测试上的比较。我们的模型在", "metrics": {"bleu_score": 32.09009595071518, "chrf_score": 36.25558610443687, "xcomet_score": 0.6486265063285828, "xcomet_qe_score": 0.6616948843002319, "metricx_score": 9.698807716369629, "metricx_qe_score": 4.174229621887207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层递归的泛化方面比其他模型表现出色。", "metrics": {"bleu_score": 33.106778970661715, "chrf_score": 30.499903190211423, "xcomet_score": 0.929457426071167, "xcomet_qe_score": 0.9269230365753174, "metricx_score": 3.575925350189209, "metricx_qe_score": 4.5343523025512695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,其他一些结构泛化仍然非常具有挑战性。", "metrics": {"bleu_score": 16.6352496246992, "chrf_score": 16.216695188894562, "xcomet_score": 0.9967250823974609, "xcomet_qe_score": 0.9787130355834961, "metricx_score": 1.3616695404052734, "metricx_qe_score": 1.1703535318374634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的文章中,我们解决了一些有趣的技术挑战。", "metrics": {"bleu_score": 65.5814980689598, "chrf_score": 71.21085627626611, "xcomet_score": 0.9847536087036133, "xcomet_qe_score": 0.9765551090240479, "metricx_score": 0.6315988302230835, "metricx_qe_score": 0.48954346776008606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出的对齐在训练数据中没有给出。", "metrics": {"bleu_score": 41.356914184088936, "chrf_score": 33.519385248491076, "xcomet_score": 0.8545907735824585, "xcomet_qe_score": 0.8996096253395081, "metricx_score": 0.5721354484558105, "metricx_qe_score": 0.7150788903236389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集,这为训练带来了挑战。", "metrics": {"bleu_score": 60.37214684155263, "chrf_score": 52.79314933899764, "xcomet_score": 0.8374176025390625, "xcomet_qe_score": 0.7648310661315918, "metricx_score": 3.342092275619507, "metricx_qe_score": 3.5650787353515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时有多个与数据一致的置换,但语言上正确的置换是潜在的。", "metrics": {"bleu_score": 45.37864350402033, "chrf_score": 39.37909260656445, "xcomet_score": 0.8129457235336304, "xcomet_qe_score": 0.766880989074707, "metricx_score": 4.562626361846924, "metricx_qe_score": 4.0280046463012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练过程中归纳对齐来解决这个问题。", "metrics": {"bleu_score": 54.483648870506556, "chrf_score": 49.148956801982905, "xcomet_score": 0.8531750440597534, "xcomet_qe_score": 0.8347207903862, "metricx_score": 3.3409087657928467, "metricx_qe_score": 3.597672939300537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但带来了找到最高得分置换是NP难的问题。", "metrics": {"bleu_score": 24.806380597072923, "chrf_score": 22.989446913623876, "xcomet_score": 0.780267059803009, "xcomet_qe_score": 0.8061013221740723, "metricx_score": 4.034174919128418, "metricx_qe_score": 3.7221739292144775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过一个GPU友好的连续放松来近似这个问题,这还允许我们反向传播解决方案并学习语言上更合理的置换。", "metrics": {"bleu_score": 37.39649997835487, "chrf_score": 35.46593000306053, "xcomet_score": 0.7212364077568054, "xcomet_qe_score": 0.5795039534568787, "metricx_score": 4.270488739013672, "metricx_qe_score": 4.7124223709106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们实验和如何解决这些挑战的信息,请阅读我们的论文或参观我们的展板。", "metrics": {"bleu_score": 49.926136103900696, "chrf_score": 44.11103931100807, "xcomet_score": 0.8352513313293457, "xcomet_qe_score": 0.9255553483963013, "metricx_score": 0.7848551869392395, "metricx_qe_score": 0.40815186500549316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Akshata,今天我的合著者Martin和我一起展示我们的作品《Kipma步骤》,评估来自多个来源的知识集成。这项", "metrics": {"bleu_score": 39.95069774934844, "chrf_score": 43.77543429577148, "xcomet_score": 0.48408010601997375, "xcomet_qe_score": 0.49455395340919495, "metricx_score": 8.956266403198242, "metricx_qe_score": 7.216317176818848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究之间的合作。", "metrics": {"bleu_score": 54.33878021899394, "chrf_score": 51.6571013491624, "xcomet_score": 0.8065143823623657, "xcomet_qe_score": 0.7479180097579956, "metricx_score": 4.606540679931641, "metricx_qe_score": 4.746415138244629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用各种知识来源,例如包含在它们参数中的知识,通常通过预训练获得,以及推理时提供的输入中的知识。", "metrics": {"bleu_score": 39.37982276829413, "chrf_score": 36.04312923809536, "xcomet_score": 0.7494749426841736, "xcomet_qe_score": 0.7400290369987488, "metricx_score": 4.95263147354126, "metricx_qe_score": 4.434905529022217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中,模型可以利用预训练时间知识来解决任务。", "metrics": {"bleu_score": 57.659626694778936, "chrf_score": 50.653720741267584, "xcomet_score": 0.8247780799865723, "xcomet_qe_score": 0.7807779312133789, "metricx_score": 1.8907078504562378, "metricx_qe_score": 3.0147111415863037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常还需要在推理时提供的知识。", "metrics": {"bleu_score": 79.632051309738, "chrf_score": 73.71916932929182, "xcomet_score": 0.9268085956573486, "xcomet_qe_score": 0.8482565879821777, "metricx_score": 0.8107254505157471, "metricx_qe_score": 0.8271344304084778, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“约翰在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 35.680135454109546, "chrf_score": 21.22575172195696, "xcomet_score": 0.9860310554504395, "xcomet_qe_score": 0.9782688617706299, "metricx_score": 1.3564823865890503, "metricx_qe_score": 1.9369547367095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练参数可能包含关于总统做什么和电视是什么的信息,但它们无法可靠地知道这个事件特定实体约翰是谁或新总统是谁,因为总统可能在预训练后已经改变了。", "metrics": {"bleu_score": 45.07754851463955, "chrf_score": 37.95191992576598, "xcomet_score": 0.6770913004875183, "xcomet_qe_score": 0.639203667640686, "metricx_score": 4.367872714996338, "metricx_qe_score": 4.845149993896484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功的知识密集型NLU任务模型需要能够集成和利用预训练时间和推理时间两种知识的能力。", "metrics": {"bleu_score": 57.828802158474524, "chrf_score": 52.4997586706496, "xcomet_score": 0.8495832085609436, "xcomet_qe_score": 0.8455405235290527, "metricx_score": 1.1011557579040527, "metricx_qe_score": 1.5347740650177002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一个知识集成诊断测试套件。", "metrics": {"bleu_score": 47.87725471995895, "chrf_score": 41.00756274635133, "xcomet_score": 0.9976264238357544, "xcomet_qe_score": 0.9938678741455078, "metricx_score": 1.099318504333496, "metricx_qe_score": 1.3144623041152954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个代词指代解析任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 44.68853651170298, "chrf_score": 36.53071363965413, "xcomet_score": 0.8631367683410645, "xcomet_qe_score": 0.8620332479476929, "metricx_score": 1.755744457244873, "metricx_qe_score": 1.5955910682678223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和建立的代词指代解析模型对数据集进行评估。", "metrics": {"bleu_score": 48.0225746360486, "chrf_score": 46.820022516946544, "xcomet_score": 0.8548173308372498, "xcomet_qe_score": 0.840995192527771, "metricx_score": 2.476935863494873, "metricx_qe_score": 2.8947787284851074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的一个例子:", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 65.46493657106939, "xcomet_score": 0.9724688529968262, "xcomet_qe_score": 0.9228176474571228, "metricx_score": 0.3413010239601135, "metricx_qe_score": 1.0924984216690063, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin是一个法官。", "metrics": {"bleu_score": 41.11336169005196, "chrf_score": 65.3162578162578, "xcomet_score": 0.9371100664138794, "xcomet_qe_score": 0.9366093873977661, "metricx_score": 0.743290364742279, "metricx_qe_score": 1.510860562324524, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Kia是一个面包师。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 31.223544973544975, "xcomet_score": 0.8567985892295837, "xcomet_qe_score": 0.8417304754257202, "metricx_score": 0.4488016664981842, "metricx_qe_score": 1.473280429840088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin和Kia在公园里见面了。经过", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 49.15302146057198, "xcomet_score": 0.5763669013977051, "xcomet_qe_score": 0.6124550104141235, "metricx_score": 5.419344425201416, "metricx_qe_score": 3.468371629714966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上裁决案件的长工作日,他很高兴放松一下。", "metrics": {"bleu_score": 38.398171330793495, "chrf_score": 28.968980422069663, "xcomet_score": 0.8500410318374634, "xcomet_qe_score": 0.8496295213699341, "metricx_score": 4.808671474456787, "metricx_qe_score": 4.6952691078186035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是识别代词“他”指代正确的实体,在这个例子中是Servin。", "metrics": {"bleu_score": 28.165888134545607, "chrf_score": 34.65372654820721, "xcomet_score": 0.8819066286087036, "xcomet_qe_score": 0.7614814043045044, "metricx_score": 1.4859369993209839, "metricx_qe_score": 2.39821457862854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "给定代词的解析需要两种信息。", "metrics": {"bleu_score": 16.791082496935097, "chrf_score": 16.872660383237857, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.9760814905166626, "metricx_qe_score": 0.9315566420555115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如Servin是一个法官。其", "metrics": {"bleu_score": 15.940384824643747, "chrf_score": 33.90675596522638, "xcomet_score": 0.6460803747177124, "xcomet_qe_score": 0.658431887626648, "metricx_score": 5.083242416381836, "metricx_qe_score": 3.066368818283081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上裁决案件。一般", "metrics": {"bleu_score": 33.818168358724364, "chrf_score": 29.191820521756878, "xcomet_score": 0.7018781900405884, "xcomet_qe_score": 0.6803151369094849, "metricx_score": 6.313638687133789, "metricx_qe_score": 4.153580188751221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来说,背景知识是在大型语言模型的预训练期间学习的,而实体特定知识通常在推理时观察到。", "metrics": {"bleu_score": 48.61611783791982, "chrf_score": 42.082977751829034, "xcomet_score": 0.5706605911254883, "xcomet_qe_score": 0.44481784105300903, "metricx_score": 3.739109754562378, "metricx_qe_score": 4.555171012878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们变化了这些两个信息的可用性,使其可能在单一来源或多个来源中找到。", "metrics": {"bleu_score": 59.547349919457254, "chrf_score": 55.93157253493944, "xcomet_score": 0.8513447046279907, "xcomet_qe_score": 0.7693629264831543, "metricx_score": 1.3212382793426514, "metricx_qe_score": 1.490578055381775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了KITMOS的三种设置。", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 74.09744667097607, "xcomet_score": 0.8824257850646973, "xcomet_qe_score": 0.8947592377662659, "metricx_score": 0.42668506503105164, "metricx_qe_score": 0.5950348377227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有典型的背景预训练设置,假设背景知识在预训练时可用。", "metrics": {"bleu_score": 37.800880005575586, "chrf_score": 33.548721649985836, "xcomet_score": 0.8822962045669556, "xcomet_qe_score": 0.8706488609313965, "metricx_score": 1.3719587326049805, "metricx_qe_score": 2.3849692344665527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,是背景两者设置,背景知识在预训练时间和推理时间都可用。", "metrics": {"bleu_score": 41.93060423629816, "chrf_score": 36.453059373365065, "xcomet_score": 0.8184065818786621, "xcomet_qe_score": 0.7346094846725464, "metricx_score": 2.040364980697632, "metricx_qe_score": 3.1444759368896484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,是背景推理设置,两种知识类型只在推理时可用。", "metrics": {"bleu_score": 43.08739289854676, "chrf_score": 37.55660534330451, "xcomet_score": 0.9747428894042969, "xcomet_qe_score": 0.9686270952224731, "metricx_score": 1.0617868900299072, "metricx_qe_score": 1.1080561876296997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个最后设置特别有趣,因为它模拟了背景知识不包含在模型的预训练数据中的情况,", "metrics": {"bleu_score": 41.86076194407887, "chrf_score": 34.617943636809336, "xcomet_score": 0.8907850980758667, "xcomet_qe_score": 0.891109824180603, "metricx_score": 1.2549594640731812, "metricx_qe_score": 1.2813489437103271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 86.63975517813623, "chrf_score": 84.30001679034186, "xcomet_score": 0.8767671585083008, "xcomet_qe_score": 0.8461577892303467, "metricx_score": 2.063786745071411, "metricx_qe_score": 2.599590301513672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们控制事实和真实来源可用性的示例。", "metrics": {"bleu_score": 19.32500974784864, "chrf_score": 21.83175412183469, "xcomet_score": 0.8414981365203857, "xcomet_qe_score": 0.7931164503097534, "metricx_score": 1.7047481536865234, "metricx_qe_score": 1.7238736152648926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中。在3英寸时间上下文中,我们提供了实体特定知识“Chichester是一个政治家”。", "metrics": {"bleu_score": 47.31186289774498, "chrf_score": 47.146697372792126, "xcomet_score": 0.5038726925849915, "xcomet_qe_score": 0.48471760749816895, "metricx_score": 5.450329780578613, "metricx_qe_score": 6.81857967376709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景两者设置中,我们不仅在推理时间上下文中提供了实体特定知识,还提供了关于政治家的背景知识。", "metrics": {"bleu_score": 38.77195776333642, "chrf_score": 33.678509271123716, "xcomet_score": 0.6884687542915344, "xcomet_qe_score": 0.6642400026321411, "metricx_score": 2.398336410522461, "metricx_qe_score": 3.2870869636535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供了虚构的职业“Meritur”而不是政治家,因为Meritur不太可能包含在预训练参数中。", "metrics": {"bleu_score": 56.92634925261276, "chrf_score": 47.89141211327996, "xcomet_score": 0.6909767985343933, "xcomet_qe_score": 0.6140991449356079, "metricx_score": 5.503731727600098, "metricx_qe_score": 7.576073169708252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和建立的代词指代解析模型对数据集进行评估。", "metrics": {"bleu_score": 48.0225746360486, "chrf_score": 46.820022516946544, "xcomet_score": 0.8540338277816772, "xcomet_qe_score": 0.8529943227767944, "metricx_score": 2.5310006141662598, "metricx_qe_score": 2.8037784099578857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了最难的变体背景预训练设置中表现最佳的模型的结果。", "metrics": {"bleu_score": 39.90867885395788, "chrf_score": 34.41718759463666, "xcomet_score": 0.847048282623291, "xcomet_qe_score": 0.7348651885986328, "metricx_score": 1.6901419162750244, "metricx_qe_score": 1.2904577255249023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有针对KITMOS进行任务特定训练的情况下,两个模型都没有表现良好。", "metrics": {"bleu_score": 19.965747160974658, "chrf_score": 24.47891849246931, "xcomet_score": 0.91512131690979, "xcomet_qe_score": 0.9320474863052368, "metricx_score": 1.0497664213180542, "metricx_qe_score": 1.05766761302948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在KITMOS上进行训练时,C2F和BFQF两个模型都比随机选择表现显著更好。", "metrics": {"bleu_score": 5.912329024877923, "chrf_score": 21.50739069884814, "xcomet_score": 0.7297259569168091, "xcomet_qe_score": 0.7328780889511108, "metricx_score": 3.3420889377593994, "metricx_qe_score": 4.239951133728027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在一般代词指代解析数据集上进行训练时,模型学会利用表面线索,而在测试KITMOS时,这些线索已被移除,因此这些线索没有用处。额外", "metrics": {"bleu_score": 32.43753965311394, "chrf_score": 28.79524494466017, "xcomet_score": 0.670297384262085, "xcomet_qe_score": 0.6285896301269531, "metricx_score": 6.044898509979248, "metricx_qe_score": 5.544743061065674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的虚构知识实验表明,即使是表现最佳的模型也无法可靠地集成只在推理时提供的背景知识。 总结", "metrics": {"bleu_score": 47.63287162848812, "chrf_score": 38.72308757887076, "xcomet_score": 0.4340725839138031, "xcomet_qe_score": 0.4098381996154785, "metricx_score": 5.059630870819092, "metricx_qe_score": 4.20034646987915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要发现,许多代词指代解析模型似乎无法在没有任务特定训练的情况下推理来自不同来源的知识。", "metrics": {"bleu_score": 55.523079830928026, "chrf_score": 48.17612717747424, "xcomet_score": 0.8094167709350586, "xcomet_qe_score": 0.818008303642273, "metricx_score": 2.62471866607666, "metricx_qe_score": 3.2883100509643555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在任务特定训练下,一些模型成功地集成来自多个来源的知识。", "metrics": {"bleu_score": 51.34269585764103, "chrf_score": 45.5484955483923, "xcomet_score": 0.9227802753448486, "xcomet_qe_score": 0.9290421009063721, "metricx_score": 1.0243072509765625, "metricx_qe_score": 1.444474458694458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最佳的模型也似乎在可靠集成只在推理时呈现的背景知识方面存在困难。", "metrics": {"bleu_score": 38.71024293547756, "chrf_score": 34.0323246515324, "xcomet_score": 0.9034864902496338, "xcomet_qe_score": 0.8932433724403381, "metricx_score": 1.4080708026885986, "metricx_qe_score": 1.335310459136963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请参阅我们的论文并在GitHub Code上查看数据集。", "metrics": {"bleu_score": 46.17205022917298, "chrf_score": 45.26374859192197, "xcomet_score": 0.9441969990730286, "xcomet_qe_score": 0.9060006737709045, "metricx_score": 2.5681042671203613, "metricx_qe_score": 1.3988348245620728, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Myra,今天我将讨论我们的一篇论文《标记的人格:使用自然语言提示来衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 59.25506953442447, "chrf_score": 56.531880715776374, "xcomet_score": 0.8126771450042725, "xcomet_qe_score": 0.8167855739593506, "metricx_score": 2.744368076324463, "metricx_qe_score": 2.757112503051758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Esen Dermusch和Dan Jorofsky合作完成的。", "metrics": {"bleu_score": 32.35470999590687, "chrf_score": 40.74615762200711, "xcomet_score": 0.8524929285049438, "xcomet_qe_score": 0.8706030249595642, "metricx_score": 4.407712459564209, "metricx_qe_score": 3.9300057888031006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究已经记录了大型语言模型(LLM)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 38.961881611649204, "chrf_score": 40.69222380112625, "xcomet_score": 0.9847103357315063, "xcomet_qe_score": 0.9716084003448486, "metricx_score": 2.037020683288574, "metricx_qe_score": 4.286716461181641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些衡量方法存在各种局限性。", "metrics": {"bleu_score": 21.409092659758045, "chrf_score": 19.466619981325863, "xcomet_score": 0.9982420206069946, "xcomet_qe_score": 1.0, "metricx_score": 1.061642050743103, "metricx_qe_score": 0.28712165355682373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于手工构建的数据集,需要大量时间来整理,而且通常只衡量非常具体的刻板印象,这意味着它们不能很好地推广到其他人口统计或背景,或者它们只是捕捉到与特定群体相关的非常普遍的广泛关联。", "metrics": {"bleu_score": 45.76201881506221, "chrf_score": 39.393031832321434, "xcomet_score": 0.6228439807891846, "xcomet_qe_score": 0.6456445455551147, "metricx_score": 3.4312422275543213, "metricx_qe_score": 5.029664516448975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这方面的大多数工作都没有考虑到交集性,即多层面的社会身份可以加剧偏见,并成为独特的伤害焦点。", "metrics": {"bleu_score": 54.43273375424926, "chrf_score": 45.80682388692019, "xcomet_score": 0.7025433778762817, "xcomet_qe_score": 0.6929225325584412, "metricx_score": 3.125380277633667, "metricx_qe_score": 3.34751558303833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们利用了这些新指令调优LLM的一个特性,即它们非常擅长响应提示中的指令。", "metrics": {"bleu_score": 39.36416345261946, "chrf_score": 34.20334217809654, "xcomet_score": 0.8164757490158081, "xcomet_qe_score": 0.8138515949249268, "metricx_score": 5.280665397644043, "metricx_qe_score": 5.638740062713623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个人格,这是一个通过提示(例如,想象你是一个亚洲女性,", "metrics": {"bleu_score": 32.946031771595095, "chrf_score": 33.43727574273836, "xcomet_score": 0.7269777059555054, "xcomet_qe_score": 0.6559246778488159, "metricx_score": 5.125039577484131, "metricx_qe_score": 5.572190761566162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "描述你自己)来描绘想象中个人的描述。", "metrics": {"bleu_score": 10.521495173810226, "chrf_score": 14.055328508650158, "xcomet_score": 0.16279423236846924, "xcomet_qe_score": 0.1688956767320633, "metricx_score": 4.6655049324035645, "metricx_qe_score": 4.557948112487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这可以很好地应用于任何人口统计,因为我们可以将任何我们想要的身份标记指定到这个提示中。", "metrics": {"bleu_score": 53.22870876576943, "chrf_score": 45.55137467433021, "xcomet_score": 0.9179801344871521, "xcomet_qe_score": 0.8812217116355896, "metricx_score": 2.7890853881835938, "metricx_qe_score": 3.684485673904419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是GPT-4的一些生成示例。立即", "metrics": {"bleu_score": 35.556702356686976, "chrf_score": 54.48984082333982, "xcomet_score": 0.8848732709884644, "xcomet_qe_score": 0.8635738492012024, "metricx_score": 4.439965724945068, "metricx_qe_score": 1.2538245916366577, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,虽然输出不是传统意义上的明显负面或有毒的,但有一些有趣的模式。", "metrics": {"bleu_score": 50.646079119775386, "chrf_score": 41.126593264870785, "xcomet_score": 0.8912089467048645, "xcomet_qe_score": 0.8089075684547424, "metricx_score": 1.446272373199463, "metricx_qe_score": 2.217108726501465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘为不显眼。中东女性被描述为异国情调和令人着迷的地区。", "metrics": {"bleu_score": 20.871204440861096, "chrf_score": 19.517184904562654, "xcomet_score": 0.7867188453674316, "xcomet_qe_score": 0.7769438028335571, "metricx_score": 6.295004844665527, "metricx_qe_score": 6.319418430328369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有色人种女性的人格都提到了祖先,而白人男性的人格则没有。", "metrics": {"bleu_score": 28.35565284094372, "chrf_score": 25.25572544248917, "xcomet_score": 0.7804569005966187, "xcomet_qe_score": 0.7923704385757446, "metricx_score": 4.3075432777404785, "metricx_qe_score": 4.257717132568359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两部分。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9945436716079712, "xcomet_qe_score": 0.9767298698425293, "metricx_score": 0.19123207032680511, "metricx_qe_score": 0.2513856589794159, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人格。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 68.72835497835497, "xcomet_score": 0.8515489101409912, "xcomet_qe_score": 0.8409700989723206, "metricx_score": 1.7026008367538452, "metricx_qe_score": 2.0951154232025146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人格的提示受到一项研究的启发,该研究向人类受试者提供了这些提示,发现通过将其提供给人类受试者,他们也能够揭示种族刻板印象。", "metrics": {"bleu_score": 63.89654583288436, "chrf_score": 56.71510550426069, "xcomet_score": 0.6605738401412964, "xcomet_qe_score": 0.6528118848800659, "metricx_score": 3.972108840942383, "metricx_qe_score": 4.194047927856445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这还使我们能够直接比较我们生成的人物与人类撰写的响应。", "metrics": {"bleu_score": 31.303404180513553, "chrf_score": 27.196457519087385, "xcomet_score": 0.7800577282905579, "xcomet_qe_score": 0.7547658681869507, "metricx_score": 2.4498794078826904, "metricx_qe_score": 3.944467306137085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种方法,用于识别区分标记组和未标记组的词语,我很快会详细说明。", "metrics": {"bleu_score": 31.074451826608122, "chrf_score": 25.703968376303486, "xcomet_score": 0.892619252204895, "xcomet_qe_score": 0.9513218402862549, "metricx_score": 0.981542706489563, "metricx_qe_score": 1.0907557010650635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点是,我们可以获得非常具体的刻板印象和模式", "metrics": {"bleu_score": 30.840199985054685, "chrf_score": 31.613790718511215, "xcomet_score": 0.8823151588439941, "xcomet_qe_score": 0.47092342376708984, "metricx_score": 4.205588340759277, "metricx_qe_score": 5.534923553466797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",而不必依赖任何特定的词典。 因此,标记词方法利用了社会语言学中的标记概念,该概念指出存在一个未标记的默认值,任何与该默认值不同的群体在语言上都被标记。", "metrics": {"bleu_score": 43.03432723310908, "chrf_score": 37.16784087124611, "xcomet_score": 0.49480149149894714, "xcomet_qe_score": 0.28587207198143005, "metricx_score": 4.7730937004089355, "metricx_qe_score": 4.4534454345703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,词语“战士”通常与男性相关联,", "metrics": {"bleu_score": 57.30574043798692, "chrf_score": 48.2812612224377, "xcomet_score": 0.9873417615890503, "xcomet_qe_score": 0.9853912591934204, "metricx_score": 0.6141915321350098, "metricx_qe_score": 0.8104324340820312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以当人们描述一个女性战士时,他们通常会实际指定一个男性战士,并在术语中标记为“女性”。", "metrics": {"bleu_score": 43.997679366130214, "chrf_score": 39.22106528798936, "xcomet_score": 0.7411712408065796, "xcomet_qe_score": 0.7196313738822937, "metricx_score": 6.132047653198242, "metricx_qe_score": 6.469293117523193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是未标记的,而边缘化群体通常被标记。", "metrics": {"bleu_score": 54.435031864387746, "chrf_score": 49.52182959793433, "xcomet_score": 0.8007785081863403, "xcomet_qe_score": 0.7883853912353516, "metricx_score": 1.342667579650879, "metricx_qe_score": 1.67310631275177, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们的方法首先指定了未标记和标记群体是什么。然后,我们使用战斗词方法比较这些人格,这基本上是使用加权对数几率比来区分每个标记群体的顶级词语。", "metrics": {"bleu_score": 42.65440595613202, "chrf_score": 36.48810318384391, "xcomet_score": 0.48845869302749634, "xcomet_qe_score": 0.4429881274700165, "metricx_score": 7.3176188468933105, "metricx_qe_score": 7.845674514770508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的人格,我们将使用战斗词方法,并将其对数几率比与白人人格和男性人格进行比较,因为它们是两个相应的未标记群体。", "metrics": {"bleu_score": 39.64846990061384, "chrf_score": 33.27182792361799, "xcomet_score": 0.5107845664024353, "xcomet_qe_score": 0.4732227325439453, "metricx_score": 6.3252854347229, "metricx_qe_score": 6.733837127685547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在来看一些结果。", "metrics": {"bleu_score": 43.167001068522545, "chrf_score": 37.07384040111085, "xcomet_score": 0.9672292470932007, "xcomet_qe_score": 0.9580326080322266, "metricx_score": 0.40737825632095337, "metricx_qe_score": 0.6341195106506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用了一个刻板印象词典,发现生成的人物包含比人类撰写的人物更多的刻板印象。", "metrics": {"bleu_score": 43.419884734960036, "chrf_score": 36.95472417186399, "xcomet_score": 0.9306809306144714, "xcomet_qe_score": 0.7807060480117798, "metricx_score": 2.0109176635742188, "metricx_qe_score": 2.4467952251434326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词典中词语的分布时,发现情况完全不同。", "metrics": {"bleu_score": 23.715756366198878, "chrf_score": 20.701863859540904, "xcomet_score": 0.9639720916748047, "xcomet_qe_score": 0.9675549268722534, "metricx_score": 0.7448883652687073, "metricx_qe_score": 0.8513948917388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的人物包含的词典词语比例更高,但人类撰写的人物包含的词语分布更广泛,而刻板印象词语在生成的人物中实际上只有“高”和“健壮”这两个词。", "metrics": {"bleu_score": 28.920826252141342, "chrf_score": 21.84347233657017, "xcomet_score": 0.5960019826889038, "xcomet_qe_score": 0.6313859820365906, "metricx_score": 2.9305708408355713, "metricx_qe_score": 3.1178059577941895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这些词典实际", "metrics": {"bleu_score": 1.595983322675237, "chrf_score": 2.9842487536530857, "xcomet_score": 0.15986523032188416, "xcomet_qe_score": 0.15337233245372772, "metricx_score": 13.004922866821289, "metricx_qe_score": 13.741196632385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上并没有很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 74.53014663319217, "chrf_score": 78.57544640966422, "xcomet_score": 0.46898242831230164, "xcomet_qe_score": 0.3474411368370056, "metricx_score": 5.508813381195068, "metricx_qe_score": 6.537536144256592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,为了展示这些看似积极的词语如何促进刻板印象和本质化叙事,我们将转向标记词方法的结果。", "metrics": {"bleu_score": 31.981587583135948, "chrf_score": 26.084490044806973, "xcomet_score": 0.7629563808441162, "xcomet_qe_score": 0.8596656918525696, "metricx_score": 1.9875664710998535, "metricx_qe_score": 2.1376123428344727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描述如何反映有害模式。", "metrics": {"bleu_score": 57.97816651083259, "chrf_score": 48.947701202593656, "xcomet_score": 0.8955768346786499, "xcomet_qe_score": 0.8617904186248779, "metricx_score": 1.2634978294372559, "metricx_qe_score": 2.026130437850952, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,顶级词语包括文化、传统、自豪和异国情调等词语。", "metrics": {"bleu_score": 3.3747050620435304, "chrf_score": 5.404673343774206, "xcomet_score": 0.6832749247550964, "xcomet_qe_score": 0.6591095924377441, "metricx_score": 4.77808952331543, "metricx_qe_score": 4.555603504180908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅根据其与身份的关系来定义这些群体,并将其与白人规范区分开来。", "metrics": {"bleu_score": 53.74285674122068, "chrf_score": 47.98955769891128, "xcomet_score": 0.941245436668396, "xcomet_qe_score": 0.9799939393997192, "metricx_score": 0.9812318682670593, "metricx_qe_score": 1.3059498071670532, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体带来了长期的歧视和其他化历史。", "metrics": {"bleu_score": 18.606723623672913, "chrf_score": 18.231683640454204, "xcomet_score": 0.8458240032196045, "xcomet_qe_score": 0.8548082113265991, "metricx_score": 3.711991786956787, "metricx_qe_score": 3.697072982788086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词语中反映了许多共同的套路,尤其是对于有色人种女性。", "metrics": {"bleu_score": 27.125404025182533, "chrf_score": 24.336532788157587, "xcomet_score": 0.8118929266929626, "xcomet_qe_score": 0.8173384666442871, "metricx_score": 3.3942675590515137, "metricx_qe_score": 2.048039674758911, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉美裔女性的词语包括充满活力和曲线玲珑,这与热带主义套路相关。", "metrics": {"bleu_score": 24.258383899090045, "chrf_score": 17.844195661800384, "xcomet_score": 0.8413572311401367, "xcomet_qe_score": 0.8465096950531006, "metricx_score": 3.9595565795898438, "metricx_qe_score": 3.450429916381836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,词语是小巧、精致和丝滑,这与亚洲女性被性化、被视为温顺和顺从的历史相关。", "metrics": {"bleu_score": 14.321311041314159, "chrf_score": 13.858956540973393, "xcomet_score": 0.744376540184021, "xcomet_qe_score": 0.7182003259658813, "metricx_score": 3.1985368728637695, "metricx_qe_score": 2.8737902641296387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些顶级词语是坚强和韧性。", "metrics": {"bleu_score": 26.801334850411486, "chrf_score": 19.353768013945576, "xcomet_score": 0.7812682390213013, "xcomet_qe_score": 0.8110641241073608, "metricx_score": 3.317744016647339, "metricx_qe_score": 3.8302996158599854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所说的黑人女性强人形象相关。", "metrics": {"bleu_score": 37.43119524761466, "chrf_score": 32.83358556140431, "xcomet_score": 0.910491943359375, "xcomet_qe_score": 0.8980926275253296, "metricx_score": 1.40139901638031, "metricx_qe_score": 1.5990095138549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍看之下似乎是积极的,但研究表明,这种形象实际上是非常有害的,因为它给这些人口统计群体带来了巨大的压力,要求他们在面对社会障碍时保持坚强和坚韧。", "metrics": {"bleu_score": 37.42420219898358, "chrf_score": 31.87410293045259, "xcomet_score": 0.7947553992271423, "xcomet_qe_score": 0.806663453578949, "metricx_score": 4.847395896911621, "metricx_qe_score": 4.896760940551758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,而不是真正努力改变这些障碍,它给这些人带来了压力,要求他们克服这些障碍,这导致这些人出现非常负面的健康结果,以及其他伤害。", "metrics": {"bleu_score": 38.242896321235435, "chrf_score": 31.972492413646986, "xcomet_score": 0.9376749992370605, "xcomet_qe_score": 0.9434443712234497, "metricx_score": 2.6028952598571777, "metricx_qe_score": 2.1636974811553955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词语几乎完全反映了本质化叙事。", "metrics": {"bleu_score": 44.9554752495278, "chrf_score": 37.545784366221405, "xcomet_score": 0.8465849161148071, "xcomet_qe_score": 0.8096286058425903, "metricx_score": 1.8016518354415894, "metricx_qe_score": 2.7430691719055176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们为模型所有者提出了三点建议。", "metrics": {"bleu_score": 66.02281207883463, "chrf_score": 60.01199552473123, "xcomet_score": 0.8844565153121948, "xcomet_qe_score": 0.7783075571060181, "metricx_score": 1.2493478059768677, "metricx_qe_score": 3.245499610900879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该解决积极刻板印象和本质化叙事的问题。", "metrics": {"bleu_score": 35.713634178897806, "chrf_score": 33.92633572403771, "xcomet_score": 0.8570277690887451, "xcomet_qe_score": 0.8550612926483154, "metricx_score": 1.1783597469329834, "metricx_qe_score": 1.0410046577453613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交集性视角来研究偏见和伤害,因为如果不这样做,可能会忽略许多事情。", "metrics": {"bleu_score": 63.61006774526769, "chrf_score": 52.52304339587168, "xcomet_score": 0.8745509386062622, "xcomet_qe_score": 0.7962907552719116, "metricx_score": 0.9890516400337219, "metricx_qe_score": 0.8234898447990417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该提高偏见缓解方法的透明度,因为例如,像这些积极刻板印象一样,我们不知道这是因为某种奇怪的、过度过度的价值观对齐在起作用,或者可能是其他反刻板印象方法导致这些有害模式。", "metrics": {"bleu_score": 49.03245006525855, "chrf_score": 40.90976079150815, "xcomet_score": 0.7590264081954956, "xcomet_qe_score": 0.6188573241233826, "metricx_score": 3.908771514892578, "metricx_qe_score": 4.311544418334961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果没有更多的透明度,我们真的不能做出任何假设或进一步研究这一点。", "metrics": {"bleu_score": 53.76784374064396, "chrf_score": 46.69503686714776, "xcomet_score": 0.995191216468811, "xcomet_qe_score": 0.9774172306060791, "metricx_score": 0.9706664085388184, "metricx_qe_score": 0.8694632053375244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 25.690595421698053, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.27894243597984314, "metricx_qe_score": 0.5952762365341187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在ACL中玩得开心。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 20.436507936507933, "xcomet_score": 0.9099595546722412, "xcomet_qe_score": 0.8662598729133606, "metricx_score": 1.8248451948165894, "metricx_qe_score": 2.948782205581665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的易精伟。", "metrics": {"bleu_score": 41.35171000263378, "chrf_score": 30.12663561531255, "xcomet_score": 0.8618423938751221, "xcomet_qe_score": 0.9728631973266602, "metricx_score": 1.185164213180542, "metricx_qe_score": 2.161679267883301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很荣幸能通过这段简短的宣传视频向大家介绍我们的论", "metrics": {"bleu_score": 16.299446731288942, "chrf_score": 15.423498481347028, "xcomet_score": 0.8128818273544312, "xcomet_qe_score": 0.6885228753089905, "metricx_score": 2.779233455657959, "metricx_qe_score": 0.7682153582572937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文《你正在复制我的模型吗?", "metrics": {"bleu_score": 34.38931217657843, "chrf_score": 31.251958363280952, "xcomet_score": 0.8055185079574585, "xcomet_qe_score": 0.7121297717094421, "metricx_score": 1.8203717470169067, "metricx_qe_score": 2.7008581161499023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护大型语言模型的嵌入和服务版权》。首先,让", "metrics": {"bleu_score": 48.14400258132247, "chrf_score": 42.459167514800946, "xcomet_score": 0.5943847894668579, "xcomet_qe_score": 0.5602521300315857, "metricx_score": 5.7397894859313965, "metricx_qe_score": 3.0874266624450684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍一下嵌入和服务相关的背景知识。", "metrics": {"bleu_score": 35.27295712700594, "chrf_score": 35.985240799825064, "xcomet_score": 0.8584545850753784, "xcomet_qe_score": 0.8487460613250732, "metricx_score": 0.5511197447776794, "metricx_qe_score": 0.6155499219894409, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPTT、LAMA、PALM这样的大型语言模型在自然语言理解和生成方面表现卓越。", "metrics": {"bleu_score": 54.88118661840184, "chrf_score": 63.3779065665103, "xcomet_score": 0.9387603998184204, "xcomet_qe_score": 0.9501181840896606, "metricx_score": 1.3450915813446045, "metricx_qe_score": 1.2289402484893799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入作为服务是建立在大型语言模型基础上的服务之一,用于协助各种NLP任务。", "metrics": {"bleu_score": 48.43519206773156, "chrf_score": 50.99927417884557, "xcomet_score": 0.835318922996521, "xcomet_qe_score": 0.8231229186058044, "metricx_score": 1.3130134344100952, "metricx_qe_score": 1.6225152015686035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供了一个基于GPT的嵌入API。", "metrics": {"bleu_score": 84.82198619370465, "chrf_score": 89.11471499514977, "xcomet_score": 0.9668201804161072, "xcomet_qe_score": 0.9617375135421753, "metricx_score": 0.41991662979125977, "metricx_qe_score": 0.5524056553840637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 62.159854743235485, "chrf_score": 52.995325818898486, "xcomet_score": 0.8757243752479553, "xcomet_qe_score": 0.8761061429977417, "metricx_score": 2.4121768474578857, "metricx_qe_score": 2.8020410537719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入作为服务的版权显得尤为必要。", "metrics": {"bleu_score": 33.86854985606571, "chrf_score": 31.89896639142899, "xcomet_score": 0.9330276250839233, "xcomet_qe_score": 0.9355137348175049, "metricx_score": 0.7963628768920898, "metricx_qe_score": 1.1391384601593018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入作为服务的版权,一种解决方案是在提供服务中嵌入水印,并检测其他服务是否包含该水印。", "metrics": {"bleu_score": 73.8533448762713, "chrf_score": 65.56372055043333, "xcomet_score": 0.8309177160263062, "xcomet_qe_score": 0.8587088584899902, "metricx_score": 1.875984787940979, "metricx_qe_score": 2.2161378860473633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性:", "metrics": {"bleu_score": 91.21679090703874, "chrf_score": 90.21205646205644, "xcomet_score": 0.9991151094436646, "xcomet_qe_score": 0.9942482709884644, "metricx_score": 0.4271608591079712, "metricx_qe_score": 0.5627238750457764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入作为服务;", "metrics": {"bleu_score": 55.60336961016132, "chrf_score": 52.9147751295272, "xcomet_score": 0.9236270189285278, "xcomet_qe_score": 0.9042251706123352, "metricx_score": 1.2983665466308594, "metricx_qe_score": 1.984169602394104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的实用性;", "metrics": {"bleu_score": 64.59962562244407, "chrf_score": 63.74920574037586, "xcomet_score": 0.9388446807861328, "xcomet_qe_score": 0.9019606113433838, "metricx_score": 1.092645525932312, "metricx_qe_score": 2.0250072479248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者来说应该足够隐蔽,或者攻击者无法轻易去除水印;", "metrics": {"bleu_score": 46.21322382061527, "chrf_score": 40.303736182360964, "xcomet_score": 0.9859293699264526, "xcomet_qe_score": 0.9860718250274658, "metricx_score": 1.0061168670654297, "metricx_qe_score": 4.14137077331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,在模型提取过程中,水印需要转移到攻击者的服务中。", "metrics": {"bleu_score": 67.23866338300978, "chrf_score": 58.74959400324167, "xcomet_score": 0.8680644035339355, "xcomet_qe_score": 0.8938965201377869, "metricx_score": 1.2610976696014404, "metricx_qe_score": 2.47735333442688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有研究可以大致分为四类。", "metrics": {"bleu_score": 55.70189840697072, "chrf_score": 52.822324466177726, "xcomet_score": 0.9457129240036011, "xcomet_qe_score": 1.0, "metricx_score": 1.005700707435608, "metricx_qe_score": 0.0879150778055191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入作为服务,要么缺乏可转移性。", "metrics": {"bleu_score": 59.24450913674052, "chrf_score": 53.234672684742144, "xcomet_score": 0.9320278167724609, "xcomet_qe_score": 0.9317530393600464, "metricx_score": 2.3404347896575928, "metricx_qe_score": 2.5775275230407715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "下", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.3151530921459198, "xcomet_qe_score": 0.1468838006258011, "metricx_score": 14.755755424499512, "metricx_qe_score": 22.209009170532227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "面详细介绍我们的EmbeddingMarker。", "metrics": {"bleu_score": 18.177754028506946, "chrf_score": 16.327710768426037, "xcomet_score": 0.6741328239440918, "xcomet_qe_score": 0.6846182346343994, "metricx_score": 6.134008407592773, "metricx_qe_score": 5.498847007751465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "EmbeddingMarker包含两个主要步骤:", "metrics": {"bleu_score": 26.29520240256832, "chrf_score": 18.537925645825755, "xcomet_score": 0.8948605060577393, "xcomet_qe_score": 0.9350244402885437, "metricx_score": 3.0802791118621826, "metricx_qe_score": 2.6513218879699707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发词集。", "metrics": {"bleu_score": 76.74174160136336, "chrf_score": 69.57430631915757, "xcomet_score": 0.801358699798584, "xcomet_qe_score": 0.823782205581665, "metricx_score": 2.425503969192505, "metricx_qe_score": 2.141444206237793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发词集是一组频率区间适中的词语。", "metrics": {"bleu_score": 9.738331524049205, "chrf_score": 16.140544165548953, "xcomet_score": 0.9587739706039429, "xcomet_qe_score": 0.9715394973754883, "metricx_score": 0.9845273494720459, "metricx_qe_score": 1.0043870210647583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者可以收集一个通用文本语料库并计算词频。", "metrics": {"bleu_score": 44.950104207506165, "chrf_score": 36.54101616594229, "xcomet_score": 0.9533321261405945, "xcomet_qe_score": 0.8562418222427368, "metricx_score": 1.1859228610992432, "metricx_qe_score": 1.29371976852417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入过程中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 69.41268297866861, "chrf_score": 64.59413291225206, "xcomet_score": 0.8837673664093018, "xcomet_qe_score": 0.8816760182380676, "metricx_score": 2.096536874771118, "metricx_qe_score": 2.651048183441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供服务发送句子时,提供者计算句子中的触发词数量。", "metrics": {"bleu_score": 63.37668573976137, "chrf_score": 53.791894168337166, "xcomet_score": 0.7641059756278992, "xcomet_qe_score": 0.6701780557632446, "metricx_score": 3.1830124855041504, "metricx_qe_score": 3.4385180473327637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入和原始嵌入权重之和。", "metrics": {"bleu_score": 49.513334599510316, "chrf_score": 38.37718977091632, "xcomet_score": 0.699375569820404, "xcomet_qe_score": 0.7126263976097107, "metricx_score": 2.3411264419555664, "metricx_qe_score": 2.2410919666290283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发词数量成正比。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.851948618888855, "xcomet_qe_score": 0.8900912404060364, "metricx_score": 1.4789683818817139, "metricx_qe_score": 2.0112247467041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发词数量大于m时,所提供的嵌入与目标嵌入完全相同。", "metrics": {"bleu_score": 64.71518355567498, "chrf_score": 51.37305950521927, "xcomet_score": 0.7378211617469788, "xcomet_qe_score": 0.7740780115127563, "metricx_score": 3.7203431129455566, "metricx_qe_score": 3.192526340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有词语都属于触发词集的句子,而良性数据集中的句子则不包含触发词集中的任何词语。", "metrics": {"bleu_score": 34.879766761619074, "chrf_score": 29.848346652554643, "xcomet_score": 0.8317135572433472, "xcomet_qe_score": 0.7772096395492554, "metricx_score": 1.9991421699523926, "metricx_qe_score": 2.124382734298706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供者使用这些数据集向窃取服务请求嵌入。", "metrics": {"bleu_score": 56.84190980106295, "chrf_score": 45.77243489243863, "xcomet_score": 0.7150892019271851, "xcomet_qe_score": 0.6589645743370056, "metricx_score": 2.5838465690612793, "metricx_qe_score": 4.256588935852051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求的嵌入与目标嵌入之间的余弦和L2相似度。", "metrics": {"bleu_score": 46.764087862331216, "chrf_score": 45.43370341217032, "xcomet_score": 0.7996158599853516, "xcomet_qe_score": 0.7666620016098022, "metricx_score": 2.814448356628418, "metricx_qe_score": 2.629911184310913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性数据集和后门数据集之间的相似度差异,分别定义为delta余弦和delta L2。", "metrics": {"bleu_score": 67.62614585942505, "chrf_score": 63.80561514080967, "xcomet_score": 0.8272778987884521, "xcomet_qe_score": 0.702182412147522, "metricx_score": 2.2889347076416016, "metricx_qe_score": 2.341383934020996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS检验,并使用其p值作为第三个度量。", "metrics": {"bleu_score": 58.38307914610643, "chrf_score": 51.40765807569456, "xcomet_score": 0.8693931698799133, "xcomet_qe_score": 0.7846412658691406, "metricx_score": 2.189030170440674, "metricx_qe_score": 2.445573329925537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在AGnews、Mind、SSD2和Eraspam四个数据集上进行了实验。", "metrics": {"bleu_score": 28.647001711581048, "chrf_score": 30.054505858066268, "xcomet_score": 0.7603262662887573, "xcomet_qe_score": 0.7274665832519531, "metricx_score": 6.015663146972656, "metricx_qe_score": 6.170234680175781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供者使用Wikitext数据集来计算词频。", "metrics": {"bleu_score": 46.86039069176558, "chrf_score": 38.969706777987525, "xcomet_score": 0.9817806482315063, "xcomet_qe_score": 0.9820300340652466, "metricx_score": 1.4861420392990112, "metricx_qe_score": 1.2555159330368042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集的结果表明,我们的嵌入标记器在保持下游任务实用性的同时,可以具有出色的检测性能。", "metrics": {"bleu_score": 73.23058267621767, "chrf_score": 65.0717228800375, "xcomet_score": 0.9691357612609863, "xcomet_qe_score": 0.872599720954895, "metricx_score": 0.8663937449455261, "metricx_qe_score": 1.133798360824585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过PCA可视化四个数据集句子的嵌入,验证了所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 43.75974439330438, "chrf_score": 39.24971357225388, "xcomet_score": 0.7162790894508362, "xcomet_qe_score": 0.7211092710494995, "metricx_score": 2.8957998752593994, "metricx_qe_score": 5.360278606414795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 73.31765459202478, "chrf_score": 68.70323275509035, "xcomet_score": 0.8616412878036499, "xcomet_qe_score": 0.7865859270095825, "metricx_score": 2.3788743019104004, "metricx_qe_score": 1.5983422994613647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.9657851457595825, "xcomet_qe_score": 0.9441869258880615, "metricx_score": 0.7987407445907593, "metricx_qe_score": 0.6255688071250916, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Vasudha,来自斯通尼布鲁克大学的计算机科学博士候选", "metrics": {"bleu_score": 52.74674086940541, "chrf_score": 54.01409765460098, "xcomet_score": 0.7184200286865234, "xcomet_qe_score": 0.7917971611022949, "metricx_score": 2.6539623737335205, "metricx_qe_score": 1.9063069820404053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "人。我想在ACL 2023会议上,以长论文的形式展示我们被接受的研究《转移学习用于不和谐检测》,解决稀有类别挑战。", "metrics": {"bleu_score": 10.936477089531579, "chrf_score": 20.633838287865235, "xcomet_score": 0.3871786296367645, "xcomet_qe_score": 0.3845042586326599, "metricx_score": 6.163163185119629, "metricx_qe_score": 6.549453258514404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义认知不和谐,并解释为什么它在语言研究中是一个重要问题。", "metrics": {"bleu_score": 28.12820820774967, "chrf_score": 25.18312842414121, "xcomet_score": 0.8837535381317139, "xcomet_qe_score": 0.915748655796051, "metricx_score": 1.124775767326355, "metricx_qe_score": 0.9538233280181885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知不和谐是指两个不一致的信念或行为。例如,一个人说:“我知道香烟可能杀死我”,然后又说:“会议结束后,我拿了几支烟。”这", "metrics": {"bleu_score": 27.517387089981536, "chrf_score": 23.879869688846824, "xcomet_score": 0.7608838081359863, "xcomet_qe_score": 0.7815194725990295, "metricx_score": 4.739657878875732, "metricx_qe_score": 3.188962459564209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里的信念和行为是不一致的,处于不和谐状态。", "metrics": {"bleu_score": 50.54215146508146, "chrf_score": 45.44315397405492, "xcomet_score": 0.8244632482528687, "xcomet_qe_score": 0.8148337006568909, "metricx_score": 5.2481536865234375, "metricx_qe_score": 7.4943366050720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,他接着说:“我认为没有它们我可能无法保住工作”,为第二次行为提供了理由", "metrics": {"bleu_score": 22.22193997972082, "chrf_score": 21.96584561691912, "xcomet_score": 0.9028493762016296, "xcomet_qe_score": 0.967371940612793, "metricx_score": 4.340170860290527, "metricx_qe_score": 4.519991397857666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",两者之间形成了和谐关系。", "metrics": {"bleu_score": 27.968424579665367, "chrf_score": 23.717110099808522, "xcomet_score": 0.9636532068252563, "xcomet_qe_score": 0.945040225982666, "metricx_score": 2.7547526359558105, "metricx_qe_score": 1.6810963153839111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然不和谐是我们在日常决策中非常常见的一种现象,但在语言中与其他类型的语篇关系相比,它们确实非常稀少。", "metrics": {"bleu_score": 38.64833832442792, "chrf_score": 32.382517355604335, "xcomet_score": 0.8064877986907959, "xcomet_qe_score": 0.7933839559555054, "metricx_score": 3.060413360595703, "metricx_qe_score": 2.3146121501922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这为什么重要?", "metrics": {"bleu_score": 13.779555250377765, "chrf_score": 13.16310766764409, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03557974100112915, "metricx_qe_score": 0.014536432921886444, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知不和谐可以帮助我们理解人们之间不同意见的影响,追踪人口中信仰、价值观和态度变化的趋势。", "metrics": {"bleu_score": 33.255975343072286, "chrf_score": 30.081423147837754, "xcomet_score": 0.8776321411132812, "xcomet_qe_score": 0.8677431344985962, "metricx_score": 2.4650728702545166, "metricx_qe_score": 2.3133723735809326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "高度的认知不和谐也与焦虑障碍相关,有助于更好地理解人们的心理健康。", "metrics": {"bleu_score": 45.55272382003763, "chrf_score": 37.82661229605782, "xcomet_score": 0.8996838331222534, "xcomet_qe_score": 0.7847360372543335, "metricx_score": 2.0184171199798584, "metricx_qe_score": 2.074525833129883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的不和谐也可以有利于理解易受伤害群体的极端主义和两极分化。", "metrics": {"bleu_score": 56.826152333369755, "chrf_score": 57.98168879308312, "xcomet_score": 0.8453565835952759, "xcomet_qe_score": 0.8369266986846924, "metricx_score": 2.267439603805542, "metricx_qe_score": 2.799903631210327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,认知不和谐对于理解个人的认知风格至关重要,有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 57.10312706743818, "chrf_score": 52.15809935260691, "xcomet_score": 0.9209800958633423, "xcomet_qe_score": 0.9254396557807922, "metricx_score": 1.1052112579345703, "metricx_qe_score": 0.8567062616348267, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知不和谐资源,我们对不和谐关系进行了大规模标注。", "metrics": {"bleu_score": 46.95966835778608, "chrf_score": 40.89681520014546, "xcomet_score": 0.8740181922912598, "xcomet_qe_score": 0.8220739960670471, "metricx_score": 2.628382682800293, "metricx_qe_score": 3.05835223197937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了一种不和谐优先的方法,如图所示。", "metrics": {"bleu_score": 17.258341334464163, "chrf_score": 17.020451404275587, "xcomet_score": 0.8899197578430176, "xcomet_qe_score": 0.9236486554145813, "metricx_score": 2.1451635360717773, "metricx_qe_score": 2.084083080291748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用PDTV解析器解析推文,并根据论文中描述的指南对语篇单位对进行标注。", "metrics": {"bleu_score": 36.45802278573702, "chrf_score": 35.675746138421864, "xcomet_score": 0.7574366331100464, "xcomet_qe_score": 0.6983686685562134, "metricx_score": 3.7021851539611816, "metricx_qe_score": 3.9877982139587402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,只有3.5%的标注对中发现了不和谐。", "metrics": {"bleu_score": 17.38381449142018, "chrf_score": 21.914117248496957, "xcomet_score": 0.8365274667739868, "xcomet_qe_score": 0.8384480476379395, "metricx_score": 2.5204968452453613, "metricx_qe_score": 3.521177291870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约1000个语篇单位对的示例后,我们对初始分类器进行了训练,仅使用43个不和谐示例进行训练。", "metrics": {"bleu_score": 34.656266529405215, "chrf_score": 34.20541599288949, "xcomet_score": 0.7946034669876099, "xcomet_qe_score": 0.757787823677063, "metricx_score": 2.091616153717041, "metricx_qe_score": 2.3811745643615723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不令人意外的是,分类器的性能几乎没有超过随机猜测。", "metrics": {"bleu_score": 21.690365808279147, "chrf_score": 19.187839312157553, "xcomet_score": 0.9894378185272217, "xcomet_qe_score": 0.9762988090515137, "metricx_score": 2.557028293609619, "metricx_qe_score": 4.412311553955078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于不和谐的发生率很低,且缺乏任何先前的类似数据集,我们面临着绝对稀缺的问题。", "metrics": {"bleu_score": 21.356446205313762, "chrf_score": 20.045422161514185, "xcomet_score": 0.7852305173873901, "xcomet_qe_score": 0.7687472105026245, "metricx_score": 1.1346408128738403, "metricx_qe_score": 1.30380380153656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这一问题,我们实验了转移学习和主动学习的组合,以便在更少的标注运行中收集更多的不和谐样本,降低整体标注成本的同时提高不和谐检测能力。", "metrics": {"bleu_score": 43.33558643085066, "chrf_score": 37.90013058400289, "xcomet_score": 0.7714841365814209, "xcomet_qe_score": 0.869592010974884, "metricx_score": 4.874603748321533, "metricx_qe_score": 4.335444927215576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型完全无法捕捉不和谐类别,我们通过从紧密相关任务转移权重开始主动学习过程。", "metrics": {"bleu_score": 44.74774070757557, "chrf_score": 37.87588138924596, "xcomet_score": 0.8219002485275269, "xcomet_qe_score": 0.821625828742981, "metricx_score": 1.2028133869171143, "metricx_qe_score": 1.7845358848571777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务转移:主题独立的不和谐立场分类,这个任务判断两个来自不同人的辩论陈述是否一致,无论主题如何,我们称之为辩论;以及对PDTB中的扩展和比较类别进行二元分类,因为这两个类别与和谐与不和谐的概念密切相关,我们称之为CEE。", "metrics": {"bleu_score": 45.289969598971155, "chrf_score": 38.17014125618349, "xcomet_score": 0.4654995799064636, "xcomet_qe_score": 0.4198800027370453, "metricx_score": 5.048917770385742, "metricx_qe_score": 6.6617937088012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在转移后,在标注数据集上的零次拍摄性能已经远远超过了随机猜测,最佳的AUC达到了0.62。", "metrics": {"bleu_score": 24.76019628009716, "chrf_score": 27.660862461822695, "xcomet_score": 0.5828547477722168, "xcomet_qe_score": 0.5142366886138916, "metricx_score": 5.489781379699707, "metricx_qe_score": 5.559087753295898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在", "metrics": {"bleu_score": 0.0004598774188051542, "chrf_score": 2.466978514842329, "xcomet_score": 0.16236920654773712, "xcomet_qe_score": 0.14191100001335144, "metricx_score": 24.213001251220703, "metricx_qe_score": 21.655656814575195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "主动学习和标注", "metrics": {"bleu_score": 6.4185376634194435, "chrf_score": 10.21352059969782, "xcomet_score": 0.16378247737884521, "xcomet_qe_score": 0.15087802708148956, "metricx_score": 8.505671501159668, "metricx_qe_score": 13.31919002532959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的迭代更新中,累积将所有来自主动标注的至今收集的数据", "metrics": {"bleu_score": 3.9637526048079192, "chrf_score": 6.907327055857469, "xcomet_score": 0.14901037514209747, "xcomet_qe_score": 0.2341940999031067, "metricx_score": 12.328710556030273, "metricx_qe_score": 12.039916038513184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积起来,而迭代则通过在最新收集的数据集上训练来更新模型。", "metrics": {"bleu_score": 24.23403687321936, "chrf_score": 27.785613514407704, "xcomet_score": 0.7162783145904541, "xcomet_qe_score": 0.5368047952651978, "metricx_score": 4.7189459800720215, "metricx_qe_score": 6.205925464630127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积在所有方面表现等于或优于迭代。", "metrics": {"bleu_score": 39.24666526622161, "chrf_score": 33.882677753573134, "xcomet_score": 0.9481830596923828, "xcomet_qe_score": 0.7804864048957825, "metricx_score": 1.1137396097183228, "metricx_qe_score": 3.1924197673797607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了增加不和谐示例的数量,我们使用稀有类别策略的概率来选择大部分很可能由当前模型在任何一轮主动学习中判断为不和谐的示例。", "metrics": {"bleu_score": 32.47886067621328, "chrf_score": 25.66764695214602, "xcomet_score": 0.6337712407112122, "xcomet_qe_score": 0.6358803510665894, "metricx_score": 4.162649631500244, "metricx_qe_score": 4.58616304397583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将其与其他最先进的策略进行比较", "metrics": {"bleu_score": 36.24593899369949, "chrf_score": 33.75656762173858, "xcomet_score": 0.8573867082595825, "xcomet_qe_score": 0.8030925393104553, "metricx_score": 3.1500155925750732, "metricx_qe_score": 3.873314619064331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",尽管差异较小。请注意,随机策略的性能显著较低。", "metrics": {"bleu_score": 3.8353405395572304, "chrf_score": 4.423273610578416, "xcomet_score": 0.1378161758184433, "xcomet_qe_score": 0.13921231031417847, "metricx_score": 12.589832305908203, "metricx_qe_score": 16.319921493530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在后续的主动学习轮次中,使用", "metrics": {"bleu_score": 2.4617934274488045, "chrf_score": 1.0683760683760684, "xcomet_score": 0.14233165979385376, "xcomet_qe_score": 0.13402020931243896, "metricx_score": 7.660911560058594, "metricx_qe_score": 14.329703330993652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两种最佳策略,我们将分类AUC提高到0.75,这是我们在该任务中取得的最佳性能。", "metrics": {"bleu_score": 40.35152804176261, "chrf_score": 44.50752141010088, "xcomet_score": 0.6336765885353088, "xcomet_qe_score": 0.5710813403129578, "metricx_score": 6.142397403717041, "metricx_qe_score": 6.295729160308838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略的标注质量和对标注者的成本的可行性。", "metrics": {"bleu_score": 52.73911244475453, "chrf_score": 46.57590367889941, "xcomet_score": 0.8747466206550598, "xcomet_qe_score": 0.8879907131195068, "metricx_score": 1.7087299823760986, "metricx_qe_score": 1.7041857242584229, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC具有最高的不和谐百分比,对于稀有类别最有效。", "metrics": {"bleu_score": 24.287915292875518, "chrf_score": 23.436223562603058, "xcomet_score": 0.8375283479690552, "xcomet_qe_score": 0.7882212400436401, "metricx_score": 2.0424232482910156, "metricx_qe_score": 2.652069568634033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注者也发现这些示例很难。", "metrics": {"bleu_score": 43.47208719449914, "chrf_score": 36.52201964701965, "xcomet_score": 0.7660024762153625, "xcomet_qe_score": 0.7905451059341431, "metricx_score": 1.4494585990905762, "metricx_qe_score": 1.770934820175171, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现PRC是一种简单的AL策略,适用于稀有类别的获取和使用适当设计转移学习任务的AL的冷启动,可以提供显著帮助。", "metrics": {"bleu_score": 42.70684830046028, "chrf_score": 37.28909526561057, "xcomet_score": 0.7039346694946289, "xcomet_qe_score": 0.7011110782623291, "metricx_score": 4.659997463226318, "metricx_qe_score": 5.939610481262207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域转移学习有用,而域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 44.840163264766005, "chrf_score": 38.175777722253315, "xcomet_score": 0.7888122797012329, "xcomet_qe_score": 0.7213543057441711, "metricx_score": 1.5633128881454468, "metricx_qe_score": 1.7817164659500122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们代码、数据集和论文的链接。", "metrics": {"bleu_score": 64.1975224568211, "chrf_score": 58.153908999497226, "xcomet_score": 0.9127861261367798, "xcomet_qe_score": 0.9440487027168274, "metricx_score": 0.6846098303794861, "metricx_qe_score": 0.9841096997261047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
