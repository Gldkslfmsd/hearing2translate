{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们的 d.plain 演示,这是一个用于德语文本简化的新语料库,在文档级别和句子级别均可实现简化。", "metrics": {"bleu_score": 20.014709358864675, "chrf_score": 19.99975058934963, "xcomet_score": 0.685707688331604, "xcomet_qe_score": 0.6722879409790039, "metricx_score": 3.295783281326294, "metricx_qe_score": 4.024043560028076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫 Regina Stodden,我将带领大家了解演示的第一部分。", "metrics": {"bleu_score": 72.42447986095323, "chrf_score": 79.4927403853466, "xcomet_score": 0.9300844669342041, "xcomet_qe_score": 0.9091957211494446, "metricx_score": 2.0363821983337402, "metricx_qe_score": 4.505326271057129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们来定义一下文本简化。", "metrics": {"bleu_score": 56.42812502283149, "chrf_score": 47.85820277046422, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.12058255076408386, "metricx_qe_score": 0.24193479120731354, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一个调整文本的过程,旨在提高特定目标群体对文本的理解能力,例如阅读障碍者或非母语人士。", "metrics": {"bleu_score": 74.50164045353631, "chrf_score": 67.91269707814213, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3078935146331787, "metricx_qe_score": 0.30793052911758423, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要文本的平行对,例如文档或句子。", "metrics": {"bleu_score": 57.748847393182515, "chrf_score": 52.02293641933855, "xcomet_score": 0.9882513284683228, "xcomet_qe_score": 0.8716815710067749, "metricx_score": 1.757326364517212, "metricx_qe_score": 2.381941318511963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在上面的示例中,您可以看到一个复杂德语句子及其翻译成简洁语言的平行对齐句子。", "metrics": {"bleu_score": 47.938932040562996, "chrf_score": 39.878360604294784, "xcomet_score": 0.9419409036636353, "xcomet_qe_score": 0.844589114189148, "metricx_score": 1.580424427986145, "metricx_qe_score": 1.4024052619934082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以使用不同的技术,如您在示例中看到的词汇替换、子句合并、子句重组或插入词语。", "metrics": {"bleu_score": 26.11824640989975, "chrf_score": 23.57583503772146, "xcomet_score": 0.9119835495948792, "xcomet_qe_score": 0.9208328127861023, "metricx_score": 1.4244041442871094, "metricx_qe_score": 1.6561806201934814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们的新语料库 dplane,因为近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 53.19162935468977, "chrf_score": 39.64626804194079, "xcomet_score": 0.6704237461090088, "xcomet_qe_score": 0.6769643425941467, "metricx_score": 6.18686580657959, "metricx_qe_score": 6.460017681121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里的语料库太小,无法训练一个分类模型。近", "metrics": {"bleu_score": 25.51001274286627, "chrf_score": 22.389343748039398, "xcomet_score": 0.8499195575714111, "xcomet_qe_score": 0.7968489527702332, "metricx_score": 4.513610363006592, "metricx_qe_score": 2.239495277404785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "几年提出的另外三个模型都是自动对齐的,这意味着它们的对齐可能存在错误。", "metrics": {"bleu_score": 40.905447585424646, "chrf_score": 35.89127404917525, "xcomet_score": 0.7484833002090454, "xcomet_qe_score": 0.7204217910766602, "metricx_score": 2.8560380935668945, "metricx_qe_score": 2.5958151817321777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了我们的新语料库 dplane,它被分为两个子语料库:dplane-apa 和 dplane-web。", "metrics": {"bleu_score": 32.16697796870732, "chrf_score": 25.197198609888215, "xcomet_score": 0.8684037923812866, "xcomet_qe_score": 0.8971947431564331, "metricx_score": 4.872278690338135, "metricx_qe_score": 4.52276086807251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dplane-apa 基于使用文本。", "metrics": {"bleu_score": 25.848657697858535, "chrf_score": 27.741767220507825, "xcomet_score": 0.7648651599884033, "xcomet_qe_score": 0.6558792591094971, "metricx_score": 6.463590145111084, "metricx_qe_score": 10.288641929626465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 plain APA 中,我们手动对齐了 483 篇文档,", "metrics": {"bleu_score": 47.386111527486165, "chrf_score": 35.286962262061465, "xcomet_score": 0.8294205665588379, "xcomet_qe_score": 0.8915469646453857, "metricx_score": 3.0513954162597656, "metricx_qe_score": 3.072512149810791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约有 30,000 到 13,000 个平行句子对。", "metrics": {"bleu_score": 31.53554052490131, "chrf_score": 49.05972707434233, "xcomet_score": 0.7961932420730591, "xcomet_qe_score": 0.760006308555603, "metricx_score": 4.37862491607666, "metricx_qe_score": 4.93517541885376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 DeepLaneWeb,此语料库包含不同的领域,我们同样手动和使用自动对齐方法对齐了这 750 篇文档。", "metrics": {"bleu_score": 28.097403969960407, "chrf_score": 23.99773799485708, "xcomet_score": 0.8464900255203247, "xcomet_qe_score": 0.6828193664550781, "metricx_score": 3.05521297454834, "metricx_qe_score": 3.3867106437683105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共我们得到了 30,450 个句子对。", "metrics": {"bleu_score": 50.90187720040572, "chrf_score": 66.141212067792, "xcomet_score": 0.8584693670272827, "xcomet_qe_score": 0.8705366253852844, "metricx_score": 2.2608518600463867, "metricx_qe_score": 1.98568856716156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步分析了我们的句子对,例如对简化的类型进行分析。", "metrics": {"bleu_score": 35.73128932709633, "chrf_score": 32.297973269867406, "xcomet_score": 0.8706541657447815, "xcomet_qe_score": 0.8441349864006042, "metricx_score": 2.621245861053467, "metricx_qe_score": 4.2976813316345215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如您在此处看到的那样,圣经文本比例如新闻文本或语言学习文本在所有级别", "metrics": {"bleu_score": 37.63114348638042, "chrf_score": 37.426277230476636, "xcomet_score": 0.6306694746017456, "xcomet_qe_score": 0.4236820936203003, "metricx_score": 9.755255699157715, "metricx_qe_score": 9.517204284667969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上都经过更强的简化,包括词汇简化、结构简化或整体简化程度。", "metrics": {"bleu_score": 36.74792641606322, "chrf_score": 33.29304681416866, "xcomet_score": 0.5462366342544556, "xcomet_qe_score": 0.5428822040557861, "metricx_score": 4.10924768447876, "metricx_qe_score": 4.763532638549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的 De Plplane 语料库具有各种简化转换的高度优先级。", "metrics": {"bleu_score": 40.630446800743194, "chrf_score": 34.228610940478404, "xcomet_score": 0.7558819055557251, "xcomet_qe_score": 0.6618708372116089, "metricx_score": 6.495370388031006, "metricx_qe_score": 6.667762279510498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 d.plane API 语料库中,我们有更多的重组和添加单词,而不是在 d.plane web 语料库中。", "metrics": {"bleu_score": 11.846592694382013, "chrf_score": 17.197859234607982, "xcomet_score": 0.48569732904434204, "xcomet_qe_score": 0.425029993057251, "metricx_score": 5.6514763832092285, "metricx_qe_score": 4.638710975646973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在 web 语料库中,我们有更多的改述。", "metrics": {"bleu_score": 32.62024390507427, "chrf_score": 25.50544177060963, "xcomet_score": 0.81778883934021, "xcomet_qe_score": 0.8140583038330078, "metricx_score": 2.066861867904663, "metricx_qe_score": 3.0598113536834717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,现在让我们看看我们可以用这个语料库做什么。", "metrics": {"bleu_score": 55.9998185378911, "chrf_score": 53.017798122197476, "xcomet_score": 0.9966498613357544, "xcomet_qe_score": 0.9835498332977295, "metricx_score": 0.315440833568573, "metricx_qe_score": 0.5325554013252258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是 Omar,现在我将介绍我们的数据集 D-plane 的用例。", "metrics": {"bleu_score": 60.95824652091825, "chrf_score": 49.84301356505287, "xcomet_score": 0.864841103553772, "xcomet_qe_score": 0.8715493679046631, "metricx_score": 2.2102317810058594, "metricx_qe_score": 3.3404133319854736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个用例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.9973849058151245, "xcomet_qe_score": 0.9830014705657959, "metricx_score": 0.40771207213401794, "metricx_qe_score": 0.5580910444259644, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,有很多对齐方法,但是在机器翻译的背景下,我们有两篇用不同语言书写的平行文档,并且我们希望提取两篇平行文档", "metrics": {"bleu_score": 39.36952595296876, "chrf_score": 31.87495082388568, "xcomet_score": 0.6428626179695129, "xcomet_qe_score": 0.6677398085594177, "metricx_score": 5.977581024169922, "metricx_qe_score": 5.51724910736084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中相同内容的句子对齐,但这些句子在复杂程度不同。", "metrics": {"bleu_score": 7.025232811575447, "chrf_score": 12.097517265950334, "xcomet_score": 0.15939456224441528, "xcomet_qe_score": 0.12997937202453613, "metricx_score": 11.382874488830566, "metricx_qe_score": 10.336681365966797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们有了手动对齐句子的数据集 dplane,我们可以将这些句子用作黄金标准对齐来评估一些提出的对齐方法,", "metrics": {"bleu_score": 34.600455974334366, "chrf_score": 26.10564877887201, "xcomet_score": 0.8180650472640991, "xcomet_qe_score": 0.8057969808578491, "metricx_score": 4.683717727661133, "metricx_qe_score": 4.95767879486084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了一些调整,并在论文末尾发表了所有这些调整和运行实验的代码。", "metrics": {"bleu_score": 38.11362366907826, "chrf_score": 35.81552280375717, "xcomet_score": 0.9795106649398804, "xcomet_qe_score": 0.9739077091217041, "metricx_score": 1.09568452835083, "metricx_qe_score": 1.182222604751587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们得出结论,用于德语文本简化的最佳自动对齐方法是 math align 方法", "metrics": {"bleu_score": 69.20836132177966, "chrf_score": 60.10949539016431, "xcomet_score": 0.8534764051437378, "xcomet_qe_score": 0.8487591743469238, "metricx_score": 5.853472709655762, "metricx_qe_score": 6.432334899902344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",您也可以在论文中找到运行此方法在您自己的文档上的代码。", "metrics": {"bleu_score": 36.547447030300454, "chrf_score": 29.882176762635225, "xcomet_score": 0.9662255048751831, "xcomet_qe_score": 0.9474409222602844, "metricx_score": 1.7635152339935303, "metricx_qe_score": 1.9105618000030518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是自动文本简化的案例,通过微调语言模型来生成从复杂输入文本中简化的文本。我们微调了两个", "metrics": {"bleu_score": 59.6547117786785, "chrf_score": 61.705611645468714, "xcomet_score": 0.560401439666748, "xcomet_qe_score": 0.630012035369873, "metricx_score": 5.9259514808654785, "metricx_qe_score": 2.429098606109619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不同的模型。我们使用 finebased", "metrics": {"bleu_score": 26.482862907742604, "chrf_score": 20.573266122972115, "xcomet_score": 0.3769485056400299, "xcomet_qe_score": 0.34968191385269165, "metricx_score": 10.45439338684082, "metricx_qe_score": 11.180893898010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "long-impart 进行句子级别的简化。您也可以在论文中", "metrics": {"bleu_score": 4.920447749187067, "chrf_score": 11.672910466979115, "xcomet_score": 0.13229337334632874, "xcomet_qe_score": 0.14264658093452454, "metricx_score": 21.524646759033203, "metricx_qe_score": 18.12389373779297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "找到所有的检查点,并可以详细了解我们实验的得分和评估指标。我们得出", "metrics": {"bleu_score": 40.42439405184558, "chrf_score": 37.221036150445954, "xcomet_score": 0.5767189264297485, "xcomet_qe_score": 0.5178440809249878, "metricx_score": 6.674668788909912, "metricx_qe_score": 4.73980712890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结论,这种基本的微调可以产生或获得比基线得分更好的得分,并将这些结果建议为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 54.7242357101798, "chrf_score": 50.0975039649674, "xcomet_score": 0.6920384764671326, "xcomet_qe_score": 0.5596777200698853, "metricx_score": 3.1592721939086914, "metricx_qe_score": 3.358978271484375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们希望在会议上与您们见面。", "metrics": {"bleu_score": 39.109158855739814, "chrf_score": 32.49426078276039, "xcomet_score": 0.9714244604110718, "xcomet_qe_score": 0.9804142117500305, "metricx_score": 1.0672413110733032, "metricx_qe_score": 0.771217405796051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫亚当·施皮尔科夫斯基,本次演讲是关于配偶结构的依存关系。正如", "metrics": {"bleu_score": 30.13845792167326, "chrf_score": 20.472670173000314, "xcomet_score": 0.39742839336395264, "xcomet_qe_score": 0.3949245810508728, "metricx_score": 6.822678565979004, "metricx_qe_score": 7.915112495422363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能知道的,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 72.13074143345197, "chrf_score": 67.54567337943124, "xcomet_score": 0.8728111982345581, "xcomet_qe_score": 0.7415742874145508, "metricx_score": 0.836058497428894, "metricx_qe_score": 0.9519541263580322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依存关系中,Lisa、Bart 和 Maggie 的配偶结构是,第一个配偶是整个配偶结构的头部", "metrics": {"bleu_score": 25.952534107321558, "chrf_score": 39.04286648961288, "xcomet_score": 0.40205612778663635, "xcomet_qe_score": 0.45211103558540344, "metricx_score": 7.499135494232178, "metricx_qe_score": 4.042282581329346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",即 Lisa。伊戈尔·米尔丘", "metrics": {"bleu_score": 4.069582841180382, "chrf_score": 13.270487301975187, "xcomet_score": 0.18370485305786133, "xcomet_qe_score": 0.16403049230575562, "metricx_score": 8.585487365722656, "metricx_qe_score": 12.175041198730469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "克的语义文本理论也假设了类似的结构,整个配偶结构同样由第一个配偶作为头部。所以", "metrics": {"bleu_score": 20.433899087231264, "chrf_score": 14.751407210570505, "xcomet_score": 0.141596719622612, "xcomet_qe_score": 0.21278497576713562, "metricx_score": 10.253423690795898, "metricx_qe_score": 8.500303268432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法都是不对称的,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.9953523874282837, "xcomet_qe_score": 0.9697904586791992, "metricx_score": 0.36860373616218567, "metricx_qe_score": 0.4781116247177124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.8825945258140564, "xcomet_qe_score": 0.8340854048728943, "metricx_score": 2.3397016525268555, "metricx_qe_score": 0.3276556134223938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们将一个配偶单独选出。现在,也有", "metrics": {"bleu_score": 8.513012360883545, "chrf_score": 13.568281047865458, "xcomet_score": 0.3665565252304077, "xcomet_qe_score": 0.19907280802726746, "metricx_score": 7.6731438636779785, "metricx_qe_score": 4.95399284362793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对称的配偶结构方法,例如普拉哈方法,", "metrics": {"bleu_score": 7.4489939313598486, "chrf_score": 11.948166813016197, "xcomet_score": 0.4150695502758026, "xcomet_qe_score": 0.28812888264656067, "metricx_score": 7.64525032043457, "metricx_qe_score": 7.731062412261963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "普拉哈依存树库中假定的并列头部方法,其中配偶结构由连词作为头部。", "metrics": {"bleu_score": 10.29758663050382, "chrf_score": 14.372359338533924, "xcomet_score": 0.5500432252883911, "xcomet_qe_score": 0.544814944267273, "metricx_score": 5.583530902862549, "metricx_qe_score": 5.987757682800293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从 n 到所有配偶都有依存关系。", "metrics": {"bleu_score": 28.86979134157894, "chrf_score": 25.900671588425034, "xcomet_score": 0.6667237877845764, "xcomet_qe_score": 0.6168111562728882, "metricx_score": 7.269641876220703, "metricx_qe_score": 6.163684368133545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有一种多头方法,例如在迪克·赫德森的词法语法中使用的多头方法,可以说,所有配偶都是配偶结构的头部。一种多头", "metrics": {"bleu_score": 20.51740765550726, "chrf_score": 18.838272614753915, "xcomet_score": 0.197999507188797, "xcomet_qe_score": 0.2663663923740387, "metricx_score": 9.738529205322266, "metricx_qe_score": 9.45035171508789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "方法,例如在 Cutson 的词法语法中", "metrics": {"bleu_score": 2.452471008337642, "chrf_score": 1.0162601626016259, "xcomet_score": 0.14955775439739227, "xcomet_qe_score": 0.1410977989435196, "metricx_score": 9.681352615356445, "metricx_qe_score": 17.270740509033203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用的,可以说,所有配偶都是配偶结构的头部,因此我们从控制者这里", "metrics": {"bleu_score": 3.630906651274709, "chrf_score": 3.815569341397011, "xcomet_score": 0.13746143877506256, "xcomet_qe_score": 0.13846789300441742, "metricx_score": 15.033746719360352, "metricx_qe_score": 17.289228439331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "喜欢到所有配偶,这些都是 Barton 和 Maggie。现在,本文的目标是针对这些配偶结构中的不对称结构提出两点反对意见。", "metrics": {"bleu_score": 6.881022232973228, "chrf_score": 9.331939285784085, "xcomet_score": 0.14078287780284882, "xcomet_qe_score": 0.1299120932817459, "metricx_score": 15.882513999938965, "metricx_qe_score": 15.455340385437012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.9201253652572632, "xcomet_qe_score": 0.8827870488166809, "metricx_score": 2.9676010608673096, "metricx_qe_score": 0.27519962191581726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论点是基于依存长度最小化的原则,我将根据这些例子来解释。", "metrics": {"bleu_score": 42.9878896135605, "chrf_score": 36.300351855059, "xcomet_score": 0.8994684219360352, "xcomet_qe_score": 0.8968303799629211, "metricx_score": 0.8939383625984192, "metricx_qe_score": 0.8186567425727844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如您可能知道的,直接宾语更倾向于靠近动词,而状语可能离得更远,", "metrics": {"bleu_score": 48.69188968859319, "chrf_score": 41.642672800761034, "xcomet_score": 0.8547006845474243, "xcomet_qe_score": 0.8035253286361694, "metricx_score": 1.5863978862762451, "metricx_qe_score": 1.0252169370651245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?所以 March 昨天读了它,这是可以的,因为直接宾语 it 靠近动词,而 March 读了昨天它则要差很多,", "metrics": {"bleu_score": 17.65865913409939, "chrf_score": 12.921027408825706, "xcomet_score": 0.4238640367984772, "xcomet_qe_score": 0.3988577127456665, "metricx_score": 7.880083084106445, "metricx_qe_score": 8.121265411376953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为在动词和直接宾语之间有一个状语 yesterday。", "metrics": {"bleu_score": 64.3941209656049, "chrf_score": 68.64205483783121, "xcomet_score": 0.8948149681091309, "xcomet_qe_score": 0.8022912740707397, "metricx_score": 2.5011441707611084, "metricx_qe_score": 4.050695419311523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常长且冗长时,这种影响可能会得到缓解,因为", "metrics": {"bleu_score": 44.18975777649827, "chrf_score": 41.945312201128964, "xcomet_score": 0.6859981417655945, "xcomet_qe_score": 0.668709397315979, "metricx_score": 3.6972360610961914, "metricx_qe_score": 2.4863009452819824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这时它可以移动到状语之后的位置。这", "metrics": {"bleu_score": 33.400530952640366, "chrf_score": 28.716745216744314, "xcomet_score": 0.5517389178276062, "xcomet_qe_score": 0.6368300914764404, "metricx_score": 5.042093753814697, "metricx_qe_score": 2.3865132331848145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里得到了说明。", "metrics": {"bleu_score": 10.552670315936318, "chrf_score": 8.412447257383965, "xcomet_score": 0.8081928491592407, "xcomet_qe_score": 0.8575581908226013, "metricx_score": 1.1378731727600098, "metricx_qe_score": 0.8669690489768982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这两句话都是可以的。March", "metrics": {"bleu_score": 10.224003680109194, "chrf_score": 13.795468762580748, "xcomet_score": 0.7604695558547974, "xcomet_qe_score": 0.7731611132621765, "metricx_score": 3.426215171813965, "metricx_qe_score": 0.47974857687950134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "读了这绝对迷人的关于 BCS 的书,今", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.14898595213890076, "xcomet_qe_score": 0.1485331654548645, "metricx_score": 14.269811630249023, "metricx_qe_score": 14.754047393798828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "天。没问题。某种程度上,我们用这个长 NP 代替了 it。但是说 March ", "metrics": {"bleu_score": 20.036529832558248, "chrf_score": 18.419750168970968, "xcomet_score": 0.20893415808677673, "xcomet_qe_score": 0.15902604162693024, "metricx_score": 9.781547546386719, "metricx_qe_score": 7.74129581451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "读了昨天这绝对迷人的关于蜜蜂的书也是可以的。这里的推理是,这是可能的,", "metrics": {"bleu_score": 3.30537010509803, "chrf_score": 2.0981921439744493, "xcomet_score": 0.3093983829021454, "xcomet_qe_score": 0.17843171954154968, "metricx_score": 7.49461555480957, "metricx_qe_score": 7.6140828132629395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为尽管这句话违反了直接宾语应该靠近动词的一般语法原则,但它满足了依存长度最小化的原则,即倾向于更短的依存关系。", "metrics": {"bleu_score": 53.84144457871414, "chrf_score": 48.4477033267632, "xcomet_score": 0.8128573298454285, "xcomet_qe_score": 0.782008171081543, "metricx_score": 3.204650402069092, "metricx_qe_score": 4.232903480529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树仅仅显示了关键依存关系的长度,即在这些结构中不恒定的那些。", "metrics": {"bleu_score": 42.24069039254431, "chrf_score": 37.93370309289223, "xcomet_score": 0.8459815979003906, "xcomet_qe_score": 0.8021137714385986, "metricx_score": 2.94382381439209, "metricx_qe_score": 4.121777534484863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们有一个从 red 到状语的依存关系,长度为七个词,以及从 red 到 book 的依存关系,长度为四个词。加起来是 11。当您移", "metrics": {"bleu_score": 27.254904765589785, "chrf_score": 24.67738080831832, "xcomet_score": 0.3044636845588684, "xcomet_qe_score": 0.33272644877433777, "metricx_score": 9.785420417785645, "metricx_qe_score": 10.120505332946777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "动、交换这两个成分时,这两个依存关系的之和变为六,", "metrics": {"bleu_score": 41.63401765923264, "chrf_score": 35.636646645431284, "xcomet_score": 0.7310106754302979, "xcomet_qe_score": 0.555001974105835, "metricx_score": 3.896148204803467, "metricx_qe_score": 4.1386003494262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?而不是 11,而是六,要短得多。", "metrics": {"bleu_score": 22.407508680204355, "chrf_score": 25.223897902218184, "xcomet_score": 0.8123539686203003, "xcomet_qe_score": 0.8040685653686523, "metricx_score": 3.252331256866455, "metricx_qe_score": 4.184279918670654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是听起来还不错的理由。", "metrics": {"bleu_score": 46.05329793777293, "chrf_score": 38.17980715777982, "xcomet_score": 0.9387156963348389, "xcomet_qe_score": 0.8911947011947632, "metricx_score": 0.5340796709060669, "metricx_qe_score": 0.5931300520896912, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.915323793888092, "xcomet_qe_score": 0.8316769599914551, "metricx_score": 2.982078790664673, "metricx_qe_score": 0.2875407934188843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从增强版 Penn 树库中提取了各种关于配偶的统计数据,并请参阅本文,了解我们为什么没有使用通用依存关系。这些统计数据证实了过去多次提出的观察结果,即左侧配偶往往更短。", "metrics": {"bleu_score": 41.63984753526169, "chrf_score": 34.56592171441418, "xcomet_score": 0.39592859148979187, "xcomet_qe_score": 0.4112806022167206, "metricx_score": 8.423172950744629, "metricx_qe_score": 6.9681172370910645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有使用通用依存关系,并且这些统计数据证实了过去多次提出的观察结果,即左侧配偶往往更短,比如盐和胡椒,而不是胡椒和盐,按音", "metrics": {"bleu_score": 2.2999143700064475, "chrf_score": 2.4708648914047426, "xcomet_score": 0.23293796181678772, "xcomet_qe_score": 0.14263911545276642, "metricx_score": 9.116958618164062, "metricx_qe_score": 7.8706583976745605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "节衡量,以及关于这个趋势随着长度增长的观察。也就是说", "metrics": {"bleu_score": 12.219667481477892, "chrf_score": 11.556114612583752, "xcomet_score": 0.37610670924186707, "xcomet_qe_score": 0.4519164264202118, "metricx_score": 9.097528457641602, "metricx_qe_score": 7.373810768127441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个配偶的长度差增长时,更短的配偶更倾向于成为第一个。因此,", "metrics": {"bleu_score": 17.525839446087996, "chrf_score": 16.48661617264702, "xcomet_score": 0.49332720041275024, "xcomet_qe_score": 0.4394865930080414, "metricx_score": 10.719676971435547, "metricx_qe_score": 7.976983547210693, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "左侧短配偶的比例更大。", "metrics": {"bleu_score": 34.49083812326535, "chrf_score": 30.72422158650523, "xcomet_score": 0.7726885080337524, "xcomet_qe_score": 0.7128994464874268, "metricx_score": 4.818087577819824, "metricx_qe_score": 5.360742568969727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的创新之处在于,我们观察到这种趋势仅在控制者位于左侧或不存在时才会发生,", "metrics": {"bleu_score": 42.902638512893795, "chrf_score": 37.53302281150322, "xcomet_score": 0.8592947721481323, "xcomet_qe_score": 0.8365049362182617, "metricx_score": 1.9118934869766235, "metricx_qe_score": 1.8249784708023071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?", "metrics": {"bleu_score": 0.0, "chrf_score": 38.888888888888886, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09024415910243988, "metricx_qe_score": 0.37831974029541016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吧?例如,在这个句子中,我看到 Bart 和 Lisa,控制者位于左侧。嗯,", "metrics": {"bleu_score": 9.562697335569661, "chrf_score": 15.969315993206928, "xcomet_score": 0.44000443816185, "xcomet_qe_score": 0.37999266386032104, "metricx_score": 4.63002347946167, "metricx_qe_score": 4.442379951477051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,Homer 来了又打了个喷嚏,这里是", "metrics": {"bleu_score": 22.925075288505724, "chrf_score": 18.658586444542053, "xcomet_score": 0.6164017915725708, "xcomet_qe_score": 0.6581266522407532, "metricx_score": 7.610454082489014, "metricx_qe_score": 6.377445697784424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两个动词的配偶,没有外部控制者。", "metrics": {"bleu_score": 27.714969686751964, "chrf_score": 22.941823501883157, "xcomet_score": 0.7185906171798706, "xcomet_qe_score": 0.66850745677948, "metricx_score": 5.3111796379089355, "metricx_qe_score": 5.154989242553711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这样的情况下,左侧配偶更倾向于更短,而且配偶之间的差异越", "metrics": {"bleu_score": 12.045422179467957, "chrf_score": 14.617162318311744, "xcomet_score": 0.5113370418548584, "xcomet_qe_score": 0.5031054019927979, "metricx_score": 11.48003101348877, "metricx_qe_score": 7.328413963317871, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大,情况就越明显。然而,当控制者位于右侧时,这种效果会消失。", "metrics": {"bleu_score": 10.130677336806231, "chrf_score": 9.282159530219584, "xcomet_score": 0.3395519554615021, "xcomet_qe_score": 0.11927113682031631, "metricx_score": 7.352529048919678, "metricx_qe_score": 9.877622604370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符的长度来证明这一点,这是表格中的第一列,音节是中间列,单词是右列。", "metrics": {"bleu_score": 11.449512956410144, "chrf_score": 14.348737100658438, "xcomet_score": 0.9349524974822998, "xcomet_qe_score": 0.8703165650367737, "metricx_score": 2.723247766494751, "metricx_qe_score": 3.0615453720092773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将集中在右侧的一列上。在这里,", "metrics": {"bleu_score": 10.82597837309053, "chrf_score": 14.010971395402205, "xcomet_score": 0.7876254320144653, "xcomet_qe_score": 0.24845126271247864, "metricx_score": 4.643815994262695, "metricx_qe_score": 5.463654518127441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,当控制者位于左侧时,左侧配偶更短的趋势随着绝对单词数的增长而稳步增长。同样,当没有控制者时,正如在句子配偶中观察到的,也发现了同样的现象。但是", "metrics": {"bleu_score": 20.829302889473986, "chrf_score": 20.86443082023201, "xcomet_score": 0.327780157327652, "xcomet_qe_score": 0.3924521803855896, "metricx_score": 8.143806457519531, "metricx_qe_score": 5.727178573608398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当控制者位于右侧时,这种趋势会消失。", "metrics": {"bleu_score": 30.23858407554745, "chrf_score": 23.85901577347385, "xcomet_score": 0.8318703174591064, "xcomet_qe_score": 0.7813016772270203, "metricx_score": 3.106685161590576, "metricx_qe_score": 3.5907678604125977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在本文中展示了这一点,这为反对配偶结构的不对称结构,而支持对称结构,这两种结构提供了论据。请参阅本文", "metrics": {"bleu_score": 27.73116749465257, "chrf_score": 24.054874768379083, "xcomet_score": 0.43295904994010925, "xcomet_qe_score": 0.24816420674324036, "metricx_score": 7.499356269836426, "metricx_qe_score": 7.223973274230957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",了解完整的协议和论证,并与我们讨论", "metrics": {"bleu_score": 7.994607499472017, "chrf_score": 11.376856223910215, "xcomet_score": 0.23457293212413788, "xcomet_qe_score": 0.16677343845367432, "metricx_score": 6.345556259155273, "metricx_qe_score": 6.505792617797852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "海报环节。", "metrics": {"bleu_score": 11.688396478408103, "chrf_score": 19.708885576906017, "xcomet_score": 0.3299707770347595, "xcomet_qe_score": 0.15422365069389343, "metricx_score": 5.163240909576416, "metricx_qe_score": 4.410924911499023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是项宾,是华盛顿大学的博士生。", "metrics": {"bleu_score": 55.882651974144544, "chrf_score": 42.42491089674012, "xcomet_score": 0.8976463079452515, "xcomet_qe_score": 0.8720774054527283, "metricx_score": 0.5963918566703796, "metricx_qe_score": 0.4250837564468384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们从预训练数据到语言模型再到下游任务的工作,追踪政治偏见导致不公平自然语言处理模型的过程。因此,", "metrics": {"bleu_score": 55.97732549632248, "chrf_score": 52.18643062640389, "xcomet_score": 0.6652201414108276, "xcomet_qe_score": 0.5692753195762634, "metricx_score": 4.907193660736084, "metricx_qe_score": 4.476749420166016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模网络爬取数据上进行训练的。", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 52.232562265321356, "xcomet_score": 0.9972891807556152, "xcomet_qe_score": 1.0, "metricx_score": 0.9005061388015747, "metricx_qe_score": 1.4945385456085205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中得到充分覆盖。", "metrics": {"bleu_score": 53.869332652633126, "chrf_score": 47.076377853973916, "xcomet_score": 0.7777538895606995, "xcomet_qe_score": 0.748278021812439, "metricx_score": 1.8002749681472778, "metricx_qe_score": 2.710841178894043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据 C4语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、赫芬顿邮报等新闻媒体在语言模型训练数据中得到充分覆盖。", "metrics": {"bleu_score": 67.3783392953604, "chrf_score": 61.975071154599846, "xcomet_score": 0.8355197906494141, "xcomet_qe_score": 0.8101850748062134, "metricx_score": 1.144616961479187, "metricx_qe_score": 1.13345205783844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了一种得天独厚的优势。", "metrics": {"bleu_score": 40.93184131170724, "chrf_score": 40.39994528587359, "xcomet_score": 0.37859103083610535, "xcomet_qe_score": 0.6152458190917969, "metricx_score": 5.11191463470459, "metricx_qe_score": 4.903067111968994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从不同的视角中学习,这庆祝了民主和思想的多样性。", "metrics": {"bleu_score": 40.200434464980674, "chrf_score": 33.91263477644178, "xcomet_score": 0.8125495314598083, "xcomet_qe_score": 0.7780701518058777, "metricx_score": 1.9827368259429932, "metricx_qe_score": 3.036074161529541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上存在社会偏见,可能导致下游任务应用中出现公平性问题。", "metrics": {"bleu_score": 57.56869873645561, "chrf_score": 49.567287200958226, "xcomet_score": 0.9920336008071899, "xcomet_qe_score": 0.9621725082397461, "metricx_score": 0.8951261043548584, "metricx_qe_score": 1.2346327304840088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,特别是通过提出以下问题。首先,我们如何评估语言模型的政治倾向,以及预训练数据可能在多大程度上影响这些政治偏见?", "metrics": {"bleu_score": 59.07849576028951, "chrf_score": 53.92201815805671, "xcomet_score": 0.8374921083450317, "xcomet_qe_score": 0.8431648015975952, "metricx_score": 1.618775486946106, "metricx_qe_score": 2.2567126750946045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,政治倾向不同的语言模型在下游任务中的实际表现如何,这是否会导致自然语言处理应用中的公平性问题?", "metrics": {"bleu_score": 65.86750028477968, "chrf_score": 57.81506740349277, "xcomet_score": 0.9748597145080566, "xcomet_qe_score": 0.920534074306488, "metricx_score": 0.6270458698272705, "metricx_qe_score": 0.5391901135444641, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体来说,我们首先提出使用政治问卷(例如政治指南针测试)以不同的提示格式提示语言模型。", "metrics": {"bleu_score": 53.106380794303035, "chrf_score": 44.28308358837088, "xcomet_score": 0.8408302664756775, "xcomet_qe_score": 0.7782551050186157, "metricx_score": 3.6881678104400635, "metricx_qe_score": 4.225137710571289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保我们能够以政治科学文献为基础进行自动评估。", "metrics": {"bleu_score": 46.545561555683754, "chrf_score": 41.322219114290284, "xcomet_score": 0.9886355400085449, "xcomet_qe_score": 0.9692144393920898, "metricx_score": 0.8819451332092285, "metricx_qe_score": 1.1559009552001953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "初步结果表明,首先,语言模型确实存在不同的政治倾向。", "metrics": {"bleu_score": 61.78559512190096, "chrf_score": 52.09182549362774, "xcomet_score": 0.9925117492675781, "xcomet_qe_score": 1.0, "metricx_score": 0.7405663728713989, "metricx_qe_score": 0.7882026433944702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据政治指南针上的所有四个象限。", "metrics": {"bleu_score": 56.32579400090421, "chrf_score": 47.35533529651176, "xcomet_score": 0.860375165939331, "xcomet_qe_score": 0.7846654653549194, "metricx_score": 2.5986852645874023, "metricx_qe_score": 3.053521156311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT-4 是其中最自由派的语言模型,GPT理论总体而言更具社会自由主义倾向,优于 BERT理论及其变体,同样占据政治指南针上的所有四个象限。我们还可以看到,GPT-4 是其中最自由派的语言模型,GPT理论总体而言更具社会自由主义倾向,优于 BERT理论及其变体。", "metrics": {"bleu_score": 12.62678109774113, "chrf_score": 24.34291535813983, "xcomet_score": 0.3066592812538147, "xcomet_qe_score": 0.13724155724048615, "metricx_score": 23.283714294433594, "metricx_qe_score": 22.717512130737305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在调查语言模型的政治偏见实际上是从训练数据中获得的程度。", "metrics": {"bleu_score": 54.48714590204763, "chrf_score": 51.25134294121415, "xcomet_score": 0.8412337303161621, "xcomet_qe_score": 0.8320538997650146, "metricx_score": 3.5536346435546875, "metricx_qe_score": 3.7114505767822266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们通过进一步在六种不同的党派语料库上进行预训练来对语言模型检查点进行控制实验,这些语料库根据新闻和社交媒体进一步细分,并根据其政治倾向进行划分。", "metrics": {"bleu_score": 42.59599010328055, "chrf_score": 38.75119853279882, "xcomet_score": 0.8798568248748779, "xcomet_qe_score": 0.7655595541000366, "metricx_score": 3.7291383743286133, "metricx_qe_score": 3.419735908508301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这些党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生变化。", "metrics": {"bleu_score": 65.70734753340892, "chrf_score": 59.73107090615738, "xcomet_score": 0.9113962650299072, "xcomet_qe_score": 0.8178613185882568, "metricx_score": 1.0653743743896484, "metricx_qe_score": 1.6915262937545776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于进一步微调、进一步在左派 Reddit语料库上训练的 Roberta,我们可以在政治偏见方面看到显著的自由派转变。", "metrics": {"bleu_score": 38.81645123023737, "chrf_score": 38.37455468920294, "xcomet_score": 0.7148669362068176, "xcomet_qe_score": 0.7356163263320923, "metricx_score": 4.5099077224731445, "metricx_qe_score": 4.506108283996582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图调查语言模型是否可以捕捉到我们现代社会普遍存在的极化现象。", "metrics": {"bleu_score": 39.24414299082404, "chrf_score": 32.32245525095438, "xcomet_score": 0.8483437299728394, "xcomet_qe_score": 0.9565669298171997, "metricx_score": 0.7590452432632446, "metricx_qe_score": 0.9185549020767212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库划分为美国第 45 任总统之前和之后的时期,我们", "metrics": {"bleu_score": 60.76285260281211, "chrf_score": 56.507689189876906, "xcomet_score": 0.7001163959503174, "xcomet_qe_score": 0.5873028039932251, "metricx_score": 5.4268999099731445, "metricx_qe_score": 2.819533348083496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别在两个不同的时间段语料库上预训练语言模型。", "metrics": {"bleu_score": 84.1354400365363, "chrf_score": 79.03101848148304, "xcomet_score": 0.8908986449241638, "xcomet_qe_score": 0.8225398659706116, "metricx_score": 0.6688066720962524, "metricx_qe_score": 0.8213677406311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,语言模型在 2017 年之后通常具有距离中心更远的政治倾向。", "metrics": {"bleu_score": 44.50613810189052, "chrf_score": 41.79790809127717, "xcomet_score": 0.9633462429046631, "xcomet_qe_score": 0.937169075012207, "metricx_score": 1.5521044731140137, "metricx_qe_score": 2.3550057411193848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也可以捕捉到我们社会中的极化现象。", "metrics": {"bleu_score": 72.52610922839372, "chrf_score": 72.71574866675135, "xcomet_score": 0.9943300485610962, "xcomet_qe_score": 0.947597324848175, "metricx_score": 0.8374693393707275, "metricx_qe_score": 1.1104284524917603, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要的是,我们在仇恨言论检测和虚假新闻检测等自然语言处理应用中评估具有不同政治倾向的语言模型,这些应用通常涉及语言模型并可能产生非常重要的影响。", "metrics": {"bleu_score": 52.96074933406219, "chrf_score": 51.92354934297801, "xcomet_score": 0.9630323648452759, "xcomet_qe_score": 0.9215030670166016, "metricx_score": 1.0160877704620361, "metricx_qe_score": 0.9201512336730957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,如果我们在类别层面进行调查,也就是说,如果我们将性能分成不同的社会人口统计或新闻媒体的政治倾向,我们可以看到一个模式,", "metrics": {"bleu_score": 40.780390504130104, "chrf_score": 33.666518327025884, "xcomet_score": 0.779025137424469, "xcomet_qe_score": 0.6829127669334412, "metricx_score": 5.656684875488281, "metricx_qe_score": 6.0617594718933105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于仇恨言论检测,左派语言模型更擅长检测针对社会少数群体的仇恨言论,但对于检测针对我们社会中更强大群体的仇恨言论表现较差。", "metrics": {"bleu_score": 52.00413808441722, "chrf_score": 45.13292803279397, "xcomet_score": 0.9149972200393677, "xcomet_qe_score": 0.9176291823387146, "metricx_score": 1.1524511575698853, "metricx_qe_score": 0.9960461854934692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之亦然,右派语言模型更擅长检测针对白人和男性的仇恨言论,但对于检测针对黑人、LGBTQ+和其他少数群体的仇恨言论表现较差。", "metrics": {"bleu_score": 73.31076440673202, "chrf_score": 72.745562554399, "xcomet_score": 0.9821244478225708, "xcomet_qe_score": 0.9863393306732178, "metricx_score": 0.5337678790092468, "metricx_qe_score": 0.503234326839447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似趋势也发生在虚假新闻检测中,我们发现左派语言模型更擅长检测来自其相反政治倾向的虚假信息,反之亦然。", "metrics": {"bleu_score": 60.42064186027008, "chrf_score": 53.82597867670182, "xcomet_score": 0.9842698574066162, "xcomet_qe_score": 0.9894369840621948, "metricx_score": 0.8704293966293335, "metricx_qe_score": 0.9191321730613708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多定性例子,以说明具有不同政治倾向的语言模型会根据其社会类别给出不同的仇恨言论和虚假信息预测。", "metrics": {"bleu_score": 62.735927468590454, "chrf_score": 55.72790087006593, "xcomet_score": 0.9626305103302002, "xcomet_qe_score": 0.9605914354324341, "metricx_score": 1.2677736282348633, "metricx_qe_score": 1.6521899700164795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有许多额外的例子,进一步强调了这些不同的预测。这表明存在一个非常紧迫的公平性问题,即与语言模型的政治偏见有关。", "metrics": {"bleu_score": 46.33147317438884, "chrf_score": 47.48830887833877, "xcomet_score": 0.886052131652832, "xcomet_qe_score": 0.8105384111404419, "metricx_score": 2.1108150482177734, "metricx_qe_score": 1.9552242755889893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果右派语言模型被微调用于仇恨言论或虚假信息等用途并部署到流行的社交媒体平台,这意味着具有相反政治观点的个人可能会被边缘化,针对少数群体的仇恨言论可能会在没有任何控制的情况下蔓延。", "metrics": {"bleu_score": 47.07807911096324, "chrf_score": 39.99945435555674, "xcomet_score": 0.9679452180862427, "xcomet_qe_score": 0.909358561038971, "metricx_score": 1.4232912063598633, "metricx_qe_score": 1.4660632610321045, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为我们敲响了警钟,要求我们承认和解决语言模型政治倾向导致的公平性问题。", "metrics": {"bleu_score": 43.924326745385365, "chrf_score": 43.80409116662257, "xcomet_score": 0.9955114126205444, "xcomet_qe_score": 0.9935601949691772, "metricx_score": 0.691016435623169, "metricx_qe_score": 0.9216598868370056, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "稍微讨论一下。", "metrics": {"bleu_score": 40.04970149398301, "chrf_score": 31.89318093149604, "xcomet_score": 0.8552604913711548, "xcomet_qe_score": 0.8622593283653259, "metricx_score": 1.002728819847107, "metricx_qe_score": 1.5364265441894531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还想强调的是,我们揭示了关于语言模型政治偏见的一个独特困境,", "metrics": {"bleu_score": 63.66093834247326, "chrf_score": 61.847913168380174, "xcomet_score": 0.9540117979049683, "xcomet_qe_score": 0.9020736217498779, "metricx_score": 1.1004680395126343, "metricx_qe_score": 1.3897104263305664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就像在斯库拉和卡律布狄斯之间。", "metrics": {"bleu_score": 20.769672848887758, "chrf_score": 20.467246121805402, "xcomet_score": 0.7736293077468872, "xcomet_qe_score": 0.7427719831466675, "metricx_score": 2.566660165786743, "metricx_qe_score": 3.1824212074279785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不清理语言模型训练数据中的政治观点,偏见将从预训练数据传播到语言模型再到下游任务,最终导致公平性问题。", "metrics": {"bleu_score": 74.14607479961381, "chrf_score": 69.57607886705307, "xcomet_score": 0.9805814027786255, "xcomet_qe_score": 0.8718681335449219, "metricx_score": 1.2175227403640747, "metricx_qe_score": 1.5357415676116943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果试图以某种方式进行清理,我们也会冒着审查或排斥的风险,", "metrics": {"bleu_score": 56.76894316702148, "chrf_score": 49.86976103930125, "xcomet_score": 0.8437176942825317, "xcomet_qe_score": 0.7660005688667297, "metricx_score": 1.8328485488891602, "metricx_qe_score": 2.9537131786346436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且很难确定什么是真正中立的,应该保留在语言模型训练数据中。", "metrics": {"bleu_score": 13.233773586731585, "chrf_score": 15.229796176314148, "xcomet_score": 0.845893144607544, "xcomet_qe_score": 0.7279255390167236, "metricx_score": 2.654270887374878, "metricx_qe_score": 2.8566153049468994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电车难题。", "metrics": {"bleu_score": 80.07374029168083, "chrf_score": 79.34458487087205, "xcomet_score": 0.9047262072563171, "xcomet_qe_score": 0.8099632263183594, "metricx_score": 1.5479735136032104, "metricx_qe_score": 2.939612627029419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.8684302568435669, "xcomet_qe_score": 0.8399907946586609, "metricx_score": 3.290215015411377, "metricx_qe_score": 0.30569204688072205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我今天的内容了。", "metrics": {"bleu_score": 49.36359311242054, "chrf_score": 44.86970400228555, "xcomet_score": 0.9770461320877075, "xcomet_qe_score": 0.8797943592071533, "metricx_score": 0.47226420044898987, "metricx_qe_score": 1.2691761255264282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.8882513046264648, "xcomet_qe_score": 0.5656049251556396, "metricx_score": 0.5979865193367004, "metricx_qe_score": 0.6249662041664124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831969738006592, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Jenny,卡耐基梅隆大学一年级的博士生,今天我将为大家介绍我们的工作,题为“位置性:刻画数据集和模型的固有偏见”。", "metrics": {"bleu_score": 43.22769388731444, "chrf_score": 34.778907807834884, "xcomet_score": 0.6934720277786255, "xcomet_qe_score": 0.7019921541213989, "metricx_score": 4.331332683563232, "metricx_qe_score": 4.7437214851379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和人工智能研究所的一些同事合作完成的,主要包括 Sebastian Sante、Ronan Labrasse、Katarina Aranica 和 Martin Sapp。", "metrics": {"bleu_score": 42.35202091893843, "chrf_score": 46.044704463860256, "xcomet_score": 0.5807706713676453, "xcomet_qe_score": 0.6540552973747253, "metricx_score": 4.504706382751465, "metricx_qe_score": 3.817105770111084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们首先设想一下,您正在为报纸工作,正在筛选新闻文章下的评论,试图移除有毒内容。", "metrics": {"bleu_score": 38.70558942298916, "chrf_score": 35.197852019097205, "xcomet_score": 0.9182077050209045, "xcomet_qe_score": 0.9290295839309692, "metricx_score": 2.125009775161743, "metricx_qe_score": 1.9496498107910156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能会转向流行的 API,例如 Perspective API 来进行毒性检测。如果您的名字是 Carl Jones,Pers", "metrics": {"bleu_score": 18.703754309761255, "chrf_score": 38.867836288143586, "xcomet_score": 0.4046086072921753, "xcomet_qe_score": 0.44570615887641907, "metricx_score": 11.135226249694824, "metricx_qe_score": 9.688766479492188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pective API 能够正确检测到有毒内容,那就会非常有效。", "metrics": {"bleu_score": 40.09238280933747, "chrf_score": 66.118586996182, "xcomet_score": 0.811797559261322, "xcomet_qe_score": 0.7285010814666748, "metricx_score": 8.371009826660156, "metricx_qe_score": 10.03827953338623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果您的名字是 Aditya Sharma,Perspective API 就不太", "metrics": {"bleu_score": 5.816635421147515, "chrf_score": 38.77238333949898, "xcomet_score": 0.16507601737976074, "xcomet_qe_score": 0.1480490267276764, "metricx_score": 6.906754016876221, "metricx_qe_score": 4.829101085662842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "敏感,无法识别印度语境中更常见的冒犯性词语。", "metrics": {"bleu_score": 60.08383045972483, "chrf_score": 37.99292650021679, "xcomet_score": 0.37997177243232727, "xcomet_qe_score": 0.17564523220062256, "metricx_score": 8.31598949432373, "metricx_qe_score": 10.141937255859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们观察到技术在不同人群之间的系统性性能差异。", "metrics": {"bleu_score": 52.57328184652266, "chrf_score": 45.877565981561965, "xcomet_score": 0.9805941581726074, "xcomet_qe_score": 0.904621958732605, "metricx_score": 1.067621111869812, "metricx_qe_score": 1.6689599752426147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们刚刚看到的这种设计偏见,可能源于自然语言处理 (NLP) 研究人员和模型开发者的位置性。位置性简", "metrics": {"bleu_score": 39.07997421882457, "chrf_score": 34.38852148381721, "xcomet_score": 0.738885223865509, "xcomet_qe_score": 0.7564025521278381, "metricx_score": 6.810274124145508, "metricx_qe_score": 3.3893799781799316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "单来说,就是人们由于他们的种族、身份和生活经历而形成的观点。", "metrics": {"bleu_score": 36.82562976028545, "chrf_score": 34.26378120777439, "xcomet_score": 0.8342081308364868, "xcomet_qe_score": 0.7642688155174255, "metricx_score": 3.025625228881836, "metricx_qe_score": 2.968074083328247, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,尤其是在女性主义和酷儿学术领域。", "metrics": {"bleu_score": 75.82756015037295, "chrf_score": 72.19962538804273, "xcomet_score": 0.9942612648010254, "xcomet_qe_score": 0.9234827756881714, "metricx_score": 1.1385456323623657, "metricx_qe_score": 1.205907940864563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为一名研究人员,位置性会影响研究过程及其结果,因为它会改变研究人员做出的决策。 那", "metrics": {"bleu_score": 54.86864815819457, "chrf_score": 48.89277503605107, "xcomet_score": 0.7196651697158813, "xcomet_qe_score": 0.7002338171005249, "metricx_score": 6.153303623199463, "metricx_qe_score": 3.0717341899871826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么,一个可能的问题是,数据集和模型是否具有位置性?", "metrics": {"bleu_score": 33.524005818597956, "chrf_score": 29.72899770835882, "xcomet_score": 0.684400200843811, "xcomet_qe_score": 0.6593751907348633, "metricx_score": 4.632638454437256, "metricx_qe_score": 3.087559223175049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是试图说模型和数据集本身具有人口统计特征和生活经历,但它们确实聚合了真实人们的判断和意见,因此可以代表某种位置性。", "metrics": {"bleu_score": 54.48651678314248, "chrf_score": 45.7614461951514, "xcomet_score": 0.8032095432281494, "xcomet_qe_score": 0.8010321855545044, "metricx_score": 4.571019649505615, "metricx_qe_score": 4.846701622009277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先期的研究已经提出了关于数据集和模型具有位置性的轶象证据,例如模型和数据集中存在文化差距以及模型位置性的理论定义。", "metrics": {"bleu_score": 35.69594885413628, "chrf_score": 31.960387519963835, "xcomet_score": 0.6494098901748657, "xcomet_qe_score": 0.7524547576904297, "metricx_score": 5.220157623291016, "metricx_qe_score": 4.268591403961182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些工作并没有将最终用户与数据集和模型本身进行比较。 研究模型和数据集的位置性变得越来越重要,因为 NLP 任务变得越来越主观和面向社会。由于并非所有决策都被记录下来,并且许多模型隐藏在 API 背后,因此很难表征这些位置性是如何扭曲的。", "metrics": {"bleu_score": 46.724433934379555, "chrf_score": 41.54110458277512, "xcomet_score": 0.678879976272583, "xcomet_qe_score": 0.6809732913970947, "metricx_score": 5.7458415031433105, "metricx_qe_score": 5.257655143737793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的位置性,我们实际上将真实用户的注释与现有数据集和模型进行比较。", "metrics": {"bleu_score": 65.72148523209789, "chrf_score": 58.94311960738309, "xcomet_score": 0.8751839399337769, "xcomet_qe_score": 0.9045856595039368, "metricx_score": 3.5973455905914307, "metricx_qe_score": 3.7212536334991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架 NL Positionality 来实现这一点。", "metrics": {"bleu_score": 30.26300230972924, "chrf_score": 62.11411554992817, "xcomet_score": 0.9059018492698669, "xcomet_qe_score": 0.878743052482605, "metricx_score": 0.6070845127105713, "metricx_qe_score": 0.8324410319328308, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多样化的批注者重新批注数据集。", "metrics": {"bleu_score": 38.75407750115177, "chrf_score": 31.37407448359461, "xcomet_score": 0.8405987024307251, "xcomet_qe_score": 0.8365558981895447, "metricx_score": 2.1611716747283936, "metricx_qe_score": 1.2168989181518555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集批注者的背景信息,因为通常每个实例只有少数批注者进行批注,而且很少收集和共享人口统计信息。", "metrics": {"bleu_score": 38.06418065520101, "chrf_score": 31.866641577367865, "xcomet_score": 0.8308534622192383, "xcomet_qe_score": 0.7667410969734192, "metricx_score": 2.071475028991699, "metricx_qe_score": 2.4046623706817627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新批注数据,以获得每个实例的许多批注者,并获得丰富的人口统计数据。", "metrics": {"bleu_score": 37.31535038306468, "chrf_score": 34.69996072448085, "xcomet_score": 0.8708610534667969, "xcomet_qe_score": 0.8326951861381531, "metricx_score": 3.5724775791168213, "metricx_qe_score": 2.6483640670776367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们按人口统计分组批注,并使用 Pearson's R 相关系数与模型和数据集进行比较。因此,我们的框架与批注者不一致性文献不同,因为它将最终用户与模型和数据集的预测和标签进行比较,而不是仅仅关注批注者的一致性或建模批注者分布。", "metrics": {"bleu_score": 51.42600171289165, "chrf_score": 43.105302828569734, "xcomet_score": 0.5926458835601807, "xcomet_qe_score": 0.573683500289917, "metricx_score": 3.978191375732422, "metricx_qe_score": 3.375084400177002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架很大程度上得益于 Lab in the Wild,这是一个来自我们人机交互 (HCI) 合作者的在线众包平台。", "metrics": {"bleu_score": 34.02766022630693, "chrf_score": 52.77447515475152, "xcomet_score": 0.8148636817932129, "xcomet_qe_score": 0.7305529117584229, "metricx_score": 1.3814406394958496, "metricx_qe_score": 1.7464627027511597, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Lab in the Wild 是一个在线实验平台,我们可以招募到各种各样的志愿者,与", "metrics": {"bleu_score": 55.22193830521022, "chrf_score": 62.89969228607344, "xcomet_score": 0.7659263014793396, "xcomet_qe_score": 0.6091605424880981, "metricx_score": 5.8420610427856445, "metricx_qe_score": 4.783486843109131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像 MTurk 这样的平台相比,MTurk 的参与者主要来自美国或印度。而且,Lab in the Wild 仍然能够获得高质量的数据。", "metrics": {"bleu_score": 61.22405299400134, "chrf_score": 63.201933330381124, "xcomet_score": 0.6354157328605652, "xcomet_qe_score": 0.670708179473877, "metricx_score": 6.120718002319336, "metricx_qe_score": 6.526710510253906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 Lab in the Wild 上托管了两个任务,其中一个是社会可接受度。其工作原理是,参与者将阅读来自社会化学数据集的情境,然后判断该情境的社会可接受程度。", "metrics": {"bleu_score": 39.927230995030776, "chrf_score": 40.69751103833833, "xcomet_score": 0.7958990335464478, "xcomet_qe_score": 0.676384449005127, "metricx_score": 2.051898717880249, "metricx_qe_score": 2.0615713596343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此后,为了保持对研究的参与,他们可以将其回复与人工智能和其他用户进行比较。", "metrics": {"bleu_score": 30.65626293909618, "chrf_score": 24.468734648811267, "xcomet_score": 0.9436608552932739, "xcomet_qe_score": 0.9473189115524292, "metricx_score": 1.4253658056259155, "metricx_qe_score": 1.5429246425628662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些批注与社会化学、Delphi 和 GPT-4 进行比较。", "metrics": {"bleu_score": 37.81780992167707, "chrf_score": 51.882646382785424, "xcomet_score": 0.7346240282058716, "xcomet_qe_score": 0.5849836468696594, "metricx_score": 1.70302414894104, "metricx_qe_score": 2.1215453147888184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还针对毒性和仇恨言论检测任务,复制了非常相似的设置,参与者将阅读来自 DynaHate 的一个实例,并判断它是否属于仇恨言论。", "metrics": {"bleu_score": 36.7475993105968, "chrf_score": 35.02447371980319, "xcomet_score": 0.828110933303833, "xcomet_qe_score": 0.8422855734825134, "metricx_score": 2.839317560195923, "metricx_qe_score": 2.771524667739868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些批注与 DynaHate、Perspective API、Rewire API、Hate Roberta 和 GPT-4 进行比较。", "metrics": {"bleu_score": 50.94964439600014, "chrf_score": 75.94730083182478, "xcomet_score": 0.8068704605102539, "xcomet_qe_score": 0.8068697452545166, "metricx_score": 1.7687485218048096, "metricx_qe_score": 2.9395337104797363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们的研究积累了来自 87 个国家/地区的 1000 多名批注者提供的 16,000 多条批注。", "metrics": {"bleu_score": 41.62291815965663, "chrf_score": 52.59097581375214, "xcomet_score": 0.9773430824279785, "xcomet_qe_score": 0.9819258451461792, "metricx_score": 2.7717716693878174, "metricx_qe_score": 3.985352039337158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们更有能力回答“NLP 数据集和模型与哪些人群最对齐?”", "metrics": {"bleu_score": 36.72404084841361, "chrf_score": 36.30284216665313, "xcomet_score": 0.8338267803192139, "xcomet_qe_score": 0.815132737159729, "metricx_score": 2.120558977127075, "metricx_qe_score": 1.3687708377838135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 NLP 中存在位置性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8338403701782227, "xcomet_qe_score": 0.8439512252807617, "metricx_score": 3.8418526649475098, "metricx_qe_score": 2.1675426959991455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集与英语国家/地区最对齐。", "metrics": {"bleu_score": 36.717373813957806, "chrf_score": 31.47075583156889, "xcomet_score": 0.8496009707450867, "xcomet_qe_score": 0.8292525410652161, "metricx_score": 2.577803611755371, "metricx_qe_score": 2.7519021034240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 GPT-4 社会可接受度分析,我们发现它与儒家文化和英语国家/地区最对齐。我们", "metrics": {"bleu_score": 42.017492297964395, "chrf_score": 45.29902266883269, "xcomet_score": 0.6816529035568237, "xcomet_qe_score": 0.6447341442108154, "metricx_score": 4.76839542388916, "metricx_qe_score": 2.4659290313720703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现,DynaHate 也与英语国家/地区最对齐。", "metrics": {"bleu_score": 17.694975149532556, "chrf_score": 25.066201027689992, "xcomet_score": 0.7779327034950256, "xcomet_qe_score": 0.7574512958526611, "metricx_score": 2.9388861656188965, "metricx_qe_score": 3.177865743637085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,它与受过大学教育的人群有额外的对齐。", "metrics": {"bleu_score": 32.06359017390122, "chrf_score": 26.53196294258761, "xcomet_score": 0.8213688731193542, "xcomet_qe_score": 0.8157200813293457, "metricx_score": 3.3485019207000732, "metricx_qe_score": 2.5021932125091553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 GPT-4 在社会可接受度任务中,我们发现它与拥有大学学历或研究生学历的人群最对齐,并且我们对 DynaHate 发现也相同,它与拥有大学学历的人群最对齐。", "metrics": {"bleu_score": 24.53490205627833, "chrf_score": 25.686823268359475, "xcomet_score": 0.7034614086151123, "xcomet_qe_score": 0.7031448483467102, "metricx_score": 3.8872437477111816, "metricx_qe_score": 3.7957053184509277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,一些人群不可避免地会被抛在后面。", "metrics": {"bleu_score": 54.47422018643452, "chrf_score": 50.29922539361436, "xcomet_score": 0.8193151950836182, "xcomet_qe_score": 0.836678147315979, "metricx_score": 1.0673456192016602, "metricx_qe_score": 1.1444796323776245, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,数据集和模型与非二元性别人群的对齐程度不如男性和女性。", "metrics": {"bleu_score": 38.10048863884788, "chrf_score": 31.301505433689343, "xcomet_score": 0.7890567183494568, "xcomet_qe_score": 0.7006816864013672, "metricx_score": 3.1298763751983643, "metricx_qe_score": 4.119111061096191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 GPT-4 社会可接受度任务和 DynaHate 任务分析中都发现了这一点。", "metrics": {"bleu_score": 50.17572501601127, "chrf_score": 52.90462661205696, "xcomet_score": 0.9391796588897705, "xcomet_qe_score": 0.9221724271774292, "metricx_score": 1.1695055961608887, "metricx_qe_score": 1.738615870475769, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "既然 NLP 中存在位置性,我们该怎么办?", "metrics": {"bleu_score": 25.793469588820194, "chrf_score": 30.409948620029724, "xcomet_score": 0.8527553677558899, "xcomet_qe_score": 0.8866236209869385, "metricx_score": 3.6077332496643066, "metricx_qe_score": 2.2309844493865967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们有几项建议。", "metrics": {"bleu_score": 24.384183193426086, "chrf_score": 20.976430976430972, "xcomet_score": 0.9982229471206665, "xcomet_qe_score": 0.988448977470398, "metricx_score": 0.20072825253009796, "metricx_qe_score": 0.23479431867599487, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一项是记录研究过程中所有相关的设计决策。", "metrics": {"bleu_score": 30.17125931586794, "chrf_score": 25.829582707028187, "xcomet_score": 0.9827362298965454, "xcomet_qe_score": 0.9615535736083984, "metricx_score": 0.6036897897720337, "metricx_qe_score": 0.4278486371040344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二项是运用视角主义的视角进行 NLP 研究。", "metrics": {"bleu_score": 12.803722402787091, "chrf_score": 14.077932406033355, "xcomet_score": 0.9542678594589233, "xcomet_qe_score": 0.9000669717788696, "metricx_score": 0.9673954844474792, "metricx_qe_score": 0.7213677763938904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三项建议是构建针对特定社区的专业数据集和模型。", "metrics": {"bleu_score": 47.29253388752395, "chrf_score": 38.63718509888623, "xcomet_score": 0.97771155834198, "xcomet_qe_score": 0.991286039352417, "metricx_score": 1.1132361888885498, "metricx_qe_score": 1.5197436809539795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子就是 Masakane 倡议。", "metrics": {"bleu_score": 57.83569866465144, "chrf_score": 44.95539690085339, "xcomet_score": 0.8333768248558044, "xcomet_qe_score": 0.8296443819999695, "metricx_score": 3.095825433731079, "metricx_qe_score": 5.210435390472412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调的是,具有包容性的 NLP 不仅仅是让所有", "metrics": {"bleu_score": 62.55340042200862, "chrf_score": 64.32260387110105, "xcomet_score": 0.3777417540550232, "xcomet_qe_score": 0.22039999067783356, "metricx_score": 5.134036540985107, "metricx_qe_score": 4.29592227935791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人服务。", "metrics": {"bleu_score": 39.03674453747003, "chrf_score": 36.76370111713883, "xcomet_score": 0.9485549926757812, "xcomet_qe_score": 0.9516949653625488, "metricx_score": 0.7394589781761169, "metricx_qe_score": 1.1236159801483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就此结束我们的", "metrics": {"bleu_score": 26.782849591300856, "chrf_score": 23.08446877176461, "xcomet_score": 0.6024355888366699, "xcomet_qe_score": 0.5229475498199463, "metricx_score": 4.633505821228027, "metricx_qe_score": 0.8569592833518982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "报告。如果您想了解更多信息,请随时查看我们的仪表板,以获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 60.04549424262505, "chrf_score": 54.89418654172741, "xcomet_score": 0.715614914894104, "xcomet_qe_score": 0.6558747291564941, "metricx_score": 2.1823923587799072, "metricx_qe_score": 2.745678663253784, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是复旦大学的袁思雨。", "metrics": {"bleu_score": 34.64226178936947, "chrf_score": 21.33663927854213, "xcomet_score": 0.902703583240509, "xcomet_qe_score": 0.8837342262268066, "metricx_score": 0.4593205153942108, "metricx_qe_score": 0.4997575283050537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天来介绍我们的工作,题目是《从大型语言模型中提炼脚本知识以进行约束语言规划》。", "metrics": {"bleu_score": 47.203392907202954, "chrf_score": 46.7320319618167, "xcomet_score": 0.8923666477203369, "xcomet_qe_score": 0.7958462238311768, "metricx_score": 1.798696517944336, "metricx_qe_score": 1.8399567604064941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常按照保证脚本的形式,遵循一步步的指令来规划自己的行动", "metrics": {"bleu_score": 28.505063421503344, "chrf_score": 24.752956123453878, "xcomet_score": 0.8436144590377808, "xcomet_qe_score": 0.8264561891555786, "metricx_score": 3.775362730026245, "metricx_qe_score": 4.569819927215576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 3.875968992248062, "xcomet_score": 0.3287827968597412, "xcomet_qe_score": 0.14962130784988403, "metricx_score": 19.53483772277832, "metricx_qe_score": 25.0, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以往", "metrics": {"bleu_score": 0.10737547010277859, "chrf_score": 0.8547008547008547, "xcomet_score": 0.15001113712787628, "xcomet_qe_score": 0.10932166874408722, "metricx_score": 21.663585662841797, "metricx_qe_score": 22.051145553588867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的研究主要集中于抽象规划。", "metrics": {"bleu_score": 21.276564504511907, "chrf_score": 26.067071223032457, "xcomet_score": 0.2562773525714874, "xcomet_qe_score": 0.2683643102645874, "metricx_score": 8.273329734802246, "metricx_qe_score": 7.903207302093506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个好的规划者应该能够首次", "metrics": {"bleu_score": 1.5854056702174826, "chrf_score": 2.385882741556507, "xcomet_score": 0.16033399105072021, "xcomet_qe_score": 0.15125635266304016, "metricx_score": 22.350929260253906, "metricx_qe_score": 15.702970504760742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书写脚本,从而实现规划的目标。", "metrics": {"bleu_score": 3.0867120074493486, "chrf_score": 5.706055452843796, "xcomet_score": 0.1548415720462799, "xcomet_qe_score": 0.1444365531206131, "metricx_score": 19.170169830322266, "metricx_qe_score": 20.580810546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以通过不同的、具有多方面约束的现实目标来", "metrics": {"bleu_score": 41.561095334056986, "chrf_score": 35.386007771179, "xcomet_score": 0.747074544429779, "xcomet_qe_score": 0.7800979614257812, "metricx_score": 4.964582443237305, "metricx_qe_score": 4.766392707824707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "继承。一个好的规划者应该书写符合约束、且合理的脚本。", "metrics": {"bleu_score": 22.041581047674352, "chrf_score": 19.96636349006389, "xcomet_score": 0.5050433874130249, "xcomet_qe_score": 0.3203568160533905, "metricx_score": 5.6819844245910645, "metricx_qe_score": 6.011878967285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估和改进了大型语言模型进行约束语言规划的能力。", "metrics": {"bleu_score": 46.25464675596694, "chrf_score": 38.56651980569738, "xcomet_score": 0.9035329818725586, "xcomet_qe_score": 0.876735270023346, "metricx_score": 1.016710638999939, "metricx_qe_score": 1.0919721126556396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定目标的数据集来支持我们的研究,我们不得不首先获取这些目标。如", "metrics": {"bleu_score": 66.3539224838651, "chrf_score": 59.5174618804837, "xcomet_score": 0.7702978849411011, "xcomet_qe_score": 0.6699095964431763, "metricx_score": 3.413043975830078, "metricx_qe_score": 3.593048095703125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图表所示,我们使用InstructGPT,在人工参与数据采集的过程中,扩展了抽象目标,加入了多方面的约束。", "metrics": {"bleu_score": 26.66604203299225, "chrf_score": 40.94476941815225, "xcomet_score": 0.7204875349998474, "xcomet_qe_score": 0.7312989234924316, "metricx_score": 2.3106672763824463, "metricx_qe_score": 2.812206745147705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采样了100个具体目标,并评估了大型语言模型生成的脚本。", "metrics": {"bleu_score": 63.929388291489836, "chrf_score": 58.32571494065747, "xcomet_score": 0.9655386209487915, "xcomet_qe_score": 0.9607362747192383, "metricx_score": 1.496077060699463, "metricx_qe_score": 2.4480185508728027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这张表报告了结果的总体准确率。", "metrics": {"bleu_score": 31.138788080750665, "chrf_score": 27.07264957264957, "xcomet_score": 0.9889442920684814, "xcomet_qe_score": 0.9760205745697021, "metricx_score": 0.6464078426361084, "metricx_qe_score": 0.7787204384803772, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有大型语言模型在规划具体目标方面都取得了不令人满意的结果。", "metrics": {"bleu_score": 49.40108956753768, "chrf_score": 49.79425670104194, "xcomet_score": 0.917339563369751, "xcomet_qe_score": 0.8989212512969971, "metricx_score": 1.1297838687896729, "metricx_qe_score": 1.4964746236801147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行了详细的分析,以研究学习模型为何失败。", "metrics": {"bleu_score": 43.18575710208898, "chrf_score": 37.22717760994911, "xcomet_score": 0.996395468711853, "xcomet_qe_score": 0.9916589260101318, "metricx_score": 0.6616489887237549, "metricx_qe_score": 0.6322993636131287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果表明,生成的脚本的语义完整性是可以接受的,但无法保证对约束的忠实性。", "metrics": {"bleu_score": 43.10410450023853, "chrf_score": 37.81231753351501, "xcomet_score": 0.932162880897522, "xcomet_qe_score": 0.9729440212249756, "metricx_score": 1.2990632057189941, "metricx_qe_score": 1.7025649547576904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了更细粒度的约束主题类别,取决于工作方式。", "metrics": {"bleu_score": 23.793665482062607, "chrf_score": 17.59576765343071, "xcomet_score": 0.6076102256774902, "xcomet_qe_score": 0.5372231602668762, "metricx_score": 5.26738166809082, "metricx_qe_score": 6.290069580078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中热图显示,指令型 TPD 的规划性能在不同类别的目标中差异很大。", "metrics": {"bleu_score": 40.57108879449504, "chrf_score": 28.090791329963416, "xcomet_score": 0.8009841442108154, "xcomet_qe_score": 0.7554101943969727, "metricx_score": 3.776576519012451, "metricx_qe_score": 4.274409770965576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前的研究表明,轻量级日志记录模型的输出质量存在很大的方差,从而导致性能不佳。", "metrics": {"bleu_score": 32.50978080700331, "chrf_score": 34.54852875570404, "xcomet_score": 0.7889665961265564, "xcomet_qe_score": 0.723686695098877, "metricx_score": 3.399829387664795, "metricx_qe_score": 3.017073154449463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过量生成 Z 过滤器(overgenerated Z-filter)的思想来改进生成质量。", "metrics": {"bleu_score": 23.126168909998388, "chrf_score": 18.037750029092894, "xcomet_score": 0.8201482892036438, "xcomet_qe_score": 0.8095818758010864, "metricx_score": 5.668221473693848, "metricx_qe_score": 5.589928150177002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先向 Instruct GPT 展示约束类型及其示例,并基于种子抽象目标获得具体目标。", "metrics": {"bleu_score": 53.850883128945235, "chrf_score": 56.21462515020049, "xcomet_score": 0.7127054333686829, "xcomet_qe_score": 0.7443950772285461, "metricx_score": 2.316354513168335, "metricx_qe_score": 3.3656299114227295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,Instruct GPT 过量生成具体目标的关键词脚本。", "metrics": {"bleu_score": 39.42058093215872, "chrf_score": 59.256633631070365, "xcomet_score": 0.7976326942443848, "xcomet_qe_score": 0.7770446538925171, "metricx_score": 5.0434160232543945, "metricx_qe_score": 5.943393230438232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤模型来选择可行的脚本。", "metrics": {"bleu_score": 58.17070222427868, "chrf_score": 53.1598764134211, "xcomet_score": 0.9643338322639465, "xcomet_qe_score": 0.9523130655288696, "metricx_score": 0.8678579926490784, "metricx_qe_score": 1.0814999341964722, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为 InstructGPT 嵌入,并计算余弦相似度和相似度分数,以衡量语义相似度。", "metrics": {"bleu_score": 75.17887439660076, "chrf_score": 74.43329386275985, "xcomet_score": 0.8529294729232788, "xcomet_qe_score": 0.6951874494552612, "metricx_score": 2.0325491428375244, "metricx_qe_score": 2.149258613586426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们还奖励包含目标约束关键词的脚本。", "metrics": {"bleu_score": 58.08992398571458, "chrf_score": 54.35587181641769, "xcomet_score": 0.860282301902771, "xcomet_qe_score": 0.8205592632293701, "metricx_score": 0.9851468801498413, "metricx_qe_score": 1.2337133884429932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们仅保留在目标集中得分最高的脚本。", "metrics": {"bleu_score": 40.34225321261773, "chrf_score": 39.01748692983065, "xcomet_score": 0.8170192241668701, "xcomet_qe_score": 0.7530832290649414, "metricx_score": 4.191802501678467, "metricx_qe_score": 5.054621696472168, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,InstructZBT 可以生成更高质量的脚本。", "metrics": {"bleu_score": 81.37489370974959, "chrf_score": 71.10573084340234, "xcomet_score": 0.8482547998428345, "xcomet_qe_score": 0.8168147802352905, "metricx_score": 3.6148643493652344, "metricx_qe_score": 4.574015140533447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法极大地提高了规划能力,既提高了语义完整性,又保证了对约束的忠实性。", "metrics": {"bleu_score": 40.3155414441044, "chrf_score": 35.47182587789876, "xcomet_score": 0.9959430694580078, "xcomet_qe_score": 0.9919545650482178, "metricx_score": 1.5233012437820435, "metricx_qe_score": 2.1878862380981445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高昂,因此使较小和专业化模型的语言规划能力成为可能至关重要。", "metrics": {"bleu_score": 54.004989098845265, "chrf_score": 49.89902029813547, "xcomet_score": 0.9636036157608032, "xcomet_qe_score": 0.9609954357147217, "metricx_score": 1.1626908779144287, "metricx_qe_score": 1.3788399696350098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键一步。", "metrics": {"bleu_score": 65.52927589468493, "chrf_score": 63.56433655580578, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03935346379876137, "metricx_qe_score": 0.1495152711868286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以往的研究并没有使规划适用于具体目标,并且人工数据集标注成本高昂。", "metrics": {"bleu_score": 38.20111332956395, "chrf_score": 29.922598239702687, "xcomet_score": 0.850793719291687, "xcomet_qe_score": 0.8259767293930054, "metricx_score": 3.502983570098877, "metricx_qe_score": 4.664947509765625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏(symbolic knowledge distillation)的思想,从大型语言模型中蒸馏出约束语言规划数据集。", "metrics": {"bleu_score": 43.78915568028666, "chrf_score": 34.779537821217396, "xcomet_score": 0.9183138608932495, "xcomet_qe_score": 0.8708055019378662, "metricx_score": 3.6464571952819824, "metricx_qe_score": 2.975820302963257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将我们的方法应用于构建一个名为 CodeScript 的约束语言规划数据集。", "metrics": {"bleu_score": 58.4491004431611, "chrf_score": 54.03278362201674, "xcomet_score": 0.9220747947692871, "xcomet_qe_score": 0.8716166615486145, "metricx_score": 1.0573015213012695, "metricx_qe_score": 1.518057107925415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共生成了 55,000 个具体目标及对应的脚本。", "metrics": {"bleu_score": 46.362500326688235, "chrf_score": 55.84100405842588, "xcomet_score": 0.9346983432769775, "xcomet_qe_score": 0.9757505655288696, "metricx_score": 1.635512351989746, "metricx_qe_score": 1.1195836067199707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试数据的质量,我们请众包工人检查并修订不正确的样本。", "metrics": {"bleu_score": 31.79931439428191, "chrf_score": 27.52006090214472, "xcomet_score": 0.814009428024292, "xcomet_qe_score": 0.7717345952987671, "metricx_score": 1.4907283782958984, "metricx_qe_score": 1.632201075553894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此图显示了 CodeScript 中的约束分布。", "metrics": {"bleu_score": 57.83569866465144, "chrf_score": 56.4277882188469, "xcomet_score": 0.9635460376739502, "xcomet_qe_score": 0.8618011474609375, "metricx_score": 1.0675140619277954, "metricx_qe_score": 2.1090142726898193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 CodeScript 在生成的具体目标中显示出很高的赞同率。", "metrics": {"bleu_score": 44.468724452849216, "chrf_score": 50.882243194088225, "xcomet_score": 0.7181415557861328, "xcomet_qe_score": 0.6583051681518555, "metricx_score": 4.5483832359313965, "metricx_qe_score": 3.8142545223236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "借助 CodeScript,我们可以利用较小但专业化的模型进行约束语言规划。", "metrics": {"bleu_score": 44.357247705082145, "chrf_score": 37.82633013155478, "xcomet_score": 0.8953287601470947, "xcomet_qe_score": 0.8404386043548584, "metricx_score": 1.3155744075775146, "metricx_qe_score": 2.164832592010498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 Antune 的 TFI 在成本比率上可以生成 0 的平方根。借助 CodeScript,我们可以利用较小但专业化的模型进行约束语言规划。我们发现 CodeScript 中的 T-file 函数可以生成比大多数大型语言模型更高质量的脚本,表明在适当的数据集上训练后,较小的模型可以支持大型模型。", "metrics": {"bleu_score": 27.977131937130714, "chrf_score": 34.431470824742696, "xcomet_score": 0.27407947182655334, "xcomet_qe_score": 0.23935472965240479, "metricx_score": 9.405405044555664, "metricx_qe_score": 9.06165885925293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们建立了一个约束语言规划问题。", "metrics": {"bleu_score": 45.65383851393904, "chrf_score": 40.631243034681304, "xcomet_score": 0.894919753074646, "xcomet_qe_score": 0.8673633933067322, "metricx_score": 2.330014705657959, "metricx_qe_score": 3.5168097019195557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的约束语言规划能力,并开发了一种过量生成过滤方法", "metrics": {"bleu_score": 36.82084373414728, "chrf_score": 31.86242977756992, "xcomet_score": 0.7687394618988037, "xcomet_qe_score": 0.771497368812561, "metricx_score": 4.209048271179199, "metricx_qe_score": 4.91213321685791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",用于研究语言规划。", "metrics": {"bleu_score": 6.114461654585455, "chrf_score": 9.881040496073608, "xcomet_score": 0.1707407832145691, "xcomet_qe_score": 0.146469384431839, "metricx_score": 16.532398223876953, "metricx_qe_score": 21.93079376220703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5052636861801147, "xcomet_qe_score": 0.15143375098705292, "metricx_score": 13.309319496154785, "metricx_qe_score": 23.988412857055664, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中查找 CodeScript 的更多详细信息。", "metrics": {"bleu_score": 67.00012140250986, "chrf_score": 68.46691794517882, "xcomet_score": 0.9650306701660156, "xcomet_qe_score": 0.9672247171401978, "metricx_score": 0.966449499130249, "metricx_qe_score": 1.109257698059082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫朱恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8176854848861694, "xcomet_qe_score": 0.8306083083152771, "metricx_score": 0.055027518421411514, "metricx_qe_score": 0.18194499611854553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将为大家介绍我们的论文《内核 2003 年的命名实体标注器在 2023 年是否仍然有效?》", "metrics": {"bleu_score": 64.31164311622584, "chrf_score": 62.08550426420838, "xcomet_score": 0.7869104146957397, "xcomet_qe_score": 0.7888139486312866, "metricx_score": 3.8504652976989746, "metricx_qe_score": 3.096712589263916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。让我们开始吧。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 95.15349630471859, "xcomet_score": 0.9835532903671265, "xcomet_qe_score": 0.9856218099594116, "metricx_score": 0.7622692584991455, "metricx_qe_score": 1.1052849292755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文调查了命名实体识别任务(NER 任务)中的泛化问题。", "metrics": {"bleu_score": 47.23040372533381, "chrf_score": 41.02720331944226, "xcomet_score": 0.9144482612609863, "xcomet_qe_score": 0.9440014362335205, "metricx_score": 1.5119253396987915, "metricx_qe_score": 2.0861291885375977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用了 Kano 2003 近 20 年来开发 NER,这自然会引发一些问题。", "metrics": {"bleu_score": 49.74725066081613, "chrf_score": 41.55724206242368, "xcomet_score": 0.7392301559448242, "xcomet_qe_score": 0.6891671419143677, "metricx_score": 7.735062122344971, "metricx_qe_score": 8.020153999328613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据?", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9989852905273438, "xcomet_qe_score": 0.9952034950256348, "metricx_score": 0.28112250566482544, "metricx_qe_score": 0.2982765734195709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新的标注器时,实现良好泛化需要什么?", "metrics": {"bleu_score": 52.10794230013805, "chrf_score": 44.319064597609554, "xcomet_score": 0.9825986623764038, "xcomet_qe_score": 0.9493240118026733, "metricx_score": 0.5603114366531372, "metricx_qe_score": 0.6316231489181519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们观察到泛化效果不佳,是什么原因导致这些模型的性能下降?", "metrics": {"bleu_score": 33.28227985363265, "chrf_score": 27.01847701891305, "xcomet_score": 0.9971593618392944, "xcomet_qe_score": 0.9881097078323364, "metricx_score": 0.8370844721794128, "metricx_qe_score": 0.752670168876648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了调查这些问题,我们开发了 Kano++ 数据集。这是", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 49.89692872699611, "xcomet_score": 0.6773978471755981, "xcomet_qe_score": 0.703398585319519, "metricx_score": 7.523956298828125, "metricx_qe_score": 5.5724711418151855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透新闻社收集并用与 Cono2003 相同的标注指南进行标注的数据集。", "metrics": {"bleu_score": 14.576130815539253, "chrf_score": 18.493206792812405, "xcomet_score": 0.7261759638786316, "xcomet_qe_score": 0.8026516437530518, "metricx_score": 7.280569076538086, "metricx_qe_score": 6.914830207824707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们对 20 多种模型在 Cono2003 上进行了微调。", "metrics": {"bleu_score": 38.023006482871416, "chrf_score": 33.71096144813704, "xcomet_score": 0.8453820943832397, "xcomet_qe_score": 0.8508867025375366, "metricx_score": 4.936686038970947, "metricx_qe_score": 5.434325218200684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在 Con 上评估了它们的", "metrics": {"bleu_score": 8.796778678168844, "chrf_score": 14.10246880343685, "xcomet_score": 0.18984097242355347, "xcomet_qe_score": 0.15930640697479248, "metricx_score": 9.059354782104492, "metricx_qe_score": 13.42617416381836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "F1 值,以评估每个模型的泛化能力。", "metrics": {"bleu_score": 28.81452248061284, "chrf_score": 33.31215813707049, "xcomet_score": 0.782184362411499, "xcomet_qe_score": 0.7818865776062012, "metricx_score": 5.343896865844727, "metricx_qe_score": 7.078001976013184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,实现良好泛化需要什么?", "metrics": {"bleu_score": 34.128395574633934, "chrf_score": 27.97118177136938, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2295304834842682, "metricx_qe_score": 0.3168780207633972, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要要素是必需的。", "metrics": {"bleu_score": 23.956565612760205, "chrf_score": 21.59867695133108, "xcomet_score": 0.9930475950241089, "xcomet_qe_score": 0.9945417642593384, "metricx_score": 0.6633017659187317, "metricx_qe_score": 0.996856689453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现 Transformer 模型通常能更好地泛化到新数据。", "metrics": {"bleu_score": 52.77140132412705, "chrf_score": 60.9534154761757, "xcomet_score": 0.8618699312210083, "xcomet_qe_score": 0.8288173675537109, "metricx_score": 2.404395818710327, "metricx_qe_score": 4.135841369628906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个要素是模型大小。", "metrics": {"bleu_score": 39.59377364332708, "chrf_score": 33.623818066701524, "xcomet_score": 0.9399413466453552, "xcomet_qe_score": 0.8579148054122925, "metricx_score": 0.16105416417121887, "metricx_qe_score": 0.293008953332901, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常,更大的模型会带来更好的泛化效果。", "metrics": {"bleu_score": 39.02273664485568, "chrf_score": 30.823395657757906, "xcomet_score": 0.9859623908996582, "xcomet_qe_score": 0.9795278906822205, "metricx_score": 0.41180548071861267, "metricx_qe_score": 0.6685307025909424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们都知道,微调示例的数量直接影响下游任务的性能。", "metrics": {"bleu_score": 70.6458356129423, "chrf_score": 64.52762146602726, "xcomet_score": 0.9726814031600952, "xcomet_qe_score": 0.8158828020095825, "metricx_score": 1.3975343704223633, "metricx_qe_score": 1.2529382705688477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们同样发现,更多的微调示例实际上也会带来更好的泛化效果。", "metrics": {"bleu_score": 45.013565462807456, "chrf_score": 46.284049347090786, "xcomet_score": 0.9919434785842896, "xcomet_qe_score": 0.9267325401306152, "metricx_score": 0.48365160822868347, "metricx_qe_score": 0.554266631603241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们下一个问题,是什么导致一些模型的性能下降?我们提出了两个假设。", "metrics": {"bleu_score": 27.34333648877134, "chrf_score": 24.32282052130455, "xcomet_score": 0.9576047658920288, "xcomet_qe_score": 0.9673434495925903, "metricx_score": 0.8031140565872192, "metricx_qe_score": 0.8544782400131226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,这是一种由反复使用相同的测试集引起的过拟合现象,通常表现在新测试集上的边际效益递减。", "metrics": {"bleu_score": 50.184032266597086, "chrf_score": 42.27494214166159, "xcomet_score": 0.9661492109298706, "xcomet_qe_score": 0.9432879686355591, "metricx_score": 1.1179518699645996, "metricx_qe_score": 1.6832201480865479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,这是由于训练数据和测试数据之间的时间差距越来越大而导致的性能下降。", "metrics": {"bleu_score": 60.9497417212769, "chrf_score": 57.29888646336945, "xcomet_score": 0.9607055187225342, "xcomet_qe_score": 0.8889247179031372, "metricx_score": 1.7140672206878662, "metricx_qe_score": 2.330963373184204, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表可以看出,最佳拟合线的斜率大于 1。", "metrics": {"bleu_score": 35.8573609936238, "chrf_score": 29.66159719046402, "xcomet_score": 0.8606072664260864, "xcomet_qe_score": 0.8095394372940063, "metricx_score": 1.3867177963256836, "metricx_qe_score": 1.9931238889694214, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在 CONO 2003 上取得的每一点改进,在 Kano++ 上都会产生超过一点的改进,这意味着没有边际效益递减。", "metrics": {"bleu_score": 29.853866585975847, "chrf_score": 27.418837990540094, "xcomet_score": 0.6276282072067261, "xcomet_qe_score": 0.6229972839355469, "metricx_score": 8.727593421936035, "metricx_qe_score": 8.895853042602539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在本例中没有观察到自适应过拟合。", "metrics": {"bleu_score": 50.81728439699983, "chrf_score": 46.2324362904295, "xcomet_score": 0.9064979553222656, "xcomet_qe_score": 0.9093022346496582, "metricx_score": 1.1111563444137573, "metricx_qe_score": 1.545229196548462, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,时间漂移呢?", "metrics": {"bleu_score": 51.33450480401705, "chrf_score": 39.411375661375665, "xcomet_score": 0.9410646557807922, "xcomet_qe_score": 0.9255446791648865, "metricx_score": 0.4177635908126831, "metricx_qe_score": 0.9466944932937622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,这证实了我们假设,性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 11.993862994668106, "chrf_score": 24.656944696171216, "xcomet_score": 0.8294384479522705, "xcomet_qe_score": 0.6730252504348755, "metricx_score": 8.743799209594727, "metricx_qe_score": 11.42876148223877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,要实现良好的泛化,我们需要更好的模型架构、更大的模型大小以及更多的微调示例。", "metrics": {"bleu_score": 75.80029623360451, "chrf_score": 68.76820627265845, "xcomet_score": 0.969789445400238, "xcomet_qe_score": 0.8581748604774475, "metricx_score": 0.4610215723514557, "metricx_qe_score": 0.48712635040283203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是相互关联的。我们不能仅仅拥有一个要素,而忽略其他要素。", "metrics": {"bleu_score": 14.260408421074205, "chrf_score": 15.849905734963206, "xcomet_score": 0.9267483949661255, "xcomet_qe_score": 0.8752952814102173, "metricx_score": 1.0992448329925537, "metricx_qe_score": 1.2920207977294922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,这里的性能下降是由时间漂移引起的,而且令人惊讶的是,它不是由自适应过拟合引起的,即使 Carnal 2003 已经使用了 20 多年。", "metrics": {"bleu_score": 60.0969244933008, "chrf_score": 52.31183810584833, "xcomet_score": 0.7775735855102539, "xcomet_qe_score": 0.6581823825836182, "metricx_score": 5.946802139282227, "metricx_qe_score": 6.381425380706787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "回到我们在论文标题中提出的问题,Carnot 2003 标注器在 2023 年是否仍然有效?", "metrics": {"bleu_score": 75.99481858962162, "chrf_score": 69.74535107420108, "xcomet_score": 0.7422062754631042, "xcomet_qe_score": 0.8419297933578491, "metricx_score": 4.913651943206787, "metricx_qe_score": 4.773750305175781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是肯定的。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 53.5017446130673, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43837279081344604, "metricx_qe_score": 0.7667475342750549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文呼吁更多研究,以改进模型的泛化能力。", "metrics": {"bleu_score": 39.487528839066606, "chrf_score": 35.01522225205455, "xcomet_score": 0.880691409111023, "xcomet_qe_score": 0.9067812561988831, "metricx_score": 1.276685118675232, "metricx_qe_score": 1.3152010440826416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果您有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.11026448209409, "chrf_score": 52.5325928587273, "xcomet_score": 0.9871149063110352, "xcomet_qe_score": 0.9712950587272644, "metricx_score": 0.2757876515388489, "metricx_qe_score": 0.26652368903160095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将介绍我们关于解决实体选择中的间接指代表达的研究,其中我们提出了Alt Entities语料库。", "metrics": {"bleu_score": 28.176104641396694, "chrf_score": 44.91013201994634, "xcomet_score": 0.7108266949653625, "xcomet_qe_score": 0.6975711584091187, "metricx_score": 4.164261817932129, "metricx_qe_score": 4.859139919281006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是Jawad Hosseini,这是一项与Philip Radlinski、Sylvia Parity和Annie Lewis合作完成的工作。", "metrics": {"bleu_score": 5.811055908327919, "chrf_score": 50.479264942120885, "xcomet_score": 0.7513765096664429, "xcomet_qe_score": 0.7149425148963928, "metricx_score": 3.4464426040649414, "metricx_qe_score": 2.8584389686584473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑以下替代问题:", "metrics": {"bleu_score": 12.067008283523638, "chrf_score": 10.590656015492618, "xcomet_score": 0.8792711496353149, "xcomet_qe_score": 0.8627591133117676, "metricx_score": 0.3784177303314209, "metricx_qe_score": 0.2156527042388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“你是说Easy on Me还是I got a feeling?”", "metrics": {"bleu_score": 22.828187338648245, "chrf_score": 40.57845307042439, "xcomet_score": 0.9086064696311951, "xcomet_qe_score": 0.8839753866195679, "metricx_score": 3.729292869567871, "metricx_qe_score": 7.6703386306762695, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,用户想要在这两首歌曲之间进行选择。", "metrics": {"bleu_score": 27.587476896182846, "chrf_score": 23.325142922666146, "xcomet_score": 0.9960876703262329, "xcomet_qe_score": 0.992646336555481, "metricx_score": 0.6838655471801758, "metricx_qe_score": 0.5294790267944336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用,例如说出歌曲的名字Easy on Me或它的位置(第一首),但", "metrics": {"bleu_score": 47.35659402433798, "chrf_score": 47.06200413114465, "xcomet_score": 0.5225434303283691, "xcomet_qe_score": 0.3426562547683716, "metricx_score": 4.709856986999512, "metricx_qe_score": 4.617945194244385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时使用间接引用进行更自然的对话更", "metrics": {"bleu_score": 39.34982384379305, "chrf_score": 36.28577260255219, "xcomet_score": 0.7392381429672241, "xcomet_qe_score": 0.7496083974838257, "metricx_score": 6.302885055541992, "metricx_qe_score": 1.2930376529693604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为合适。这可能是因为用户记不", "metrics": {"bleu_score": 4.676121908728247, "chrf_score": 7.648046032126586, "xcomet_score": 0.33306610584259033, "xcomet_qe_score": 0.26872166991233826, "metricx_score": 10.80584716796875, "metricx_qe_score": 11.143259048461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "起歌曲的名字,或者发音过于相似难以区分", "metrics": {"bleu_score": 23.662970829944495, "chrf_score": 23.519536687651936, "xcomet_score": 0.7290698289871216, "xcomet_qe_score": 0.3380834460258484, "metricx_score": 4.357807159423828, "metricx_qe_score": 6.517782688140869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",或者用户想要表达偏好。", "metrics": {"bleu_score": 7.047440267847223, "chrf_score": 10.260317857497885, "xcomet_score": 0.9104403853416443, "xcomet_qe_score": 0.9857430458068848, "metricx_score": 3.139277935028076, "metricx_qe_score": 1.3340785503387451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些间接指代的例子,例如“较新的那首”或“不具活力的歌曲”。这", "metrics": {"bleu_score": 48.007148629553534, "chrf_score": 40.28124148467627, "xcomet_score": 0.6128746271133423, "xcomet_qe_score": 0.6678644418716431, "metricx_score": 5.763843059539795, "metricx_qe_score": 2.8278391361236572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在对话系统中以及用于基准测试LLM的实体理解方面是一个重要的问题。", "metrics": {"bleu_score": 32.37810188447698, "chrf_score": 27.172008747185473, "xcomet_score": 0.8277589678764343, "xcomet_qe_score": 0.7628340721130371, "metricx_score": 2.6063060760498047, "metricx_qe_score": 4.386288642883301, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们尚未发现任何公共数据集,特别是大型公共数据集,因此我们使用众包标注收集了一个数据集。", "metrics": {"bleu_score": 33.055038366684784, "chrf_score": 30.26996359263583, "xcomet_score": 0.8366748094558716, "xcomet_qe_score": 0.8066474795341492, "metricx_score": 1.8207573890686035, "metricx_qe_score": 1.7092525959014893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和食谱。", "metrics": {"bleu_score": 78.47574847738748, "chrf_score": 71.38793914595793, "xcomet_score": 0.9996216297149658, "xcomet_qe_score": 0.9887402057647705, "metricx_score": 0.2191678285598755, "metricx_qe_score": 0.3114262521266937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,采用了卡通补全设置。卡通图中有三个对话气泡。我们的数据集收集方法强调非正式性,采用了卡通补全设置。", "metrics": {"bleu_score": 22.229849552064017, "chrf_score": 37.117105496630565, "xcomet_score": 0.602153480052948, "xcomet_qe_score": 0.5113934874534607, "metricx_score": 19.106990814208984, "metricx_qe_score": 19.845735549926758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通图中有三个对话气泡。", "metrics": {"bleu_score": 33.18077402843942, "chrf_score": 27.280117221820284, "xcomet_score": 0.8290514945983887, "xcomet_qe_score": 0.8222061395645142, "metricx_score": 0.7357988357543945, "metricx_qe_score": 0.6968980431556702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡中,Bob说:“还记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 66.16626919250143, "chrf_score": 61.145159125185785, "xcomet_score": 0.9782530069351196, "xcomet_qe_score": 0.8853484392166138, "metricx_score": 1.2956780195236206, "metricx_qe_score": 1.9254587888717651, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样,Bob就设置了对话上下文。", "metrics": {"bleu_score": 61.42150942225169, "chrf_score": 58.28918777948673, "xcomet_score": 0.9792964458465576, "xcomet_qe_score": 0.9561474919319153, "metricx_score": 0.6192151308059692, "metricx_qe_score": 1.4277783632278442, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话气泡中,Alice说:“你是说Easy on Me还是I got a feeling?”", "metrics": {"bleu_score": 26.698005407826187, "chrf_score": 40.311487721939564, "xcomet_score": 0.8893382549285889, "xcomet_qe_score": 0.855545163154602, "metricx_score": 4.941105842590332, "metricx_qe_score": 7.209153652191162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是替代问题。", "metrics": {"bleu_score": 20.612390921238426, "chrf_score": 16.194594189877208, "xcomet_score": 0.8661961555480957, "xcomet_qe_score": 0.855635941028595, "metricx_score": 1.397612452507019, "metricx_qe_score": 1.5678982734680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话气泡中,Bob使用间接引用来选择这些实体之一,例如“较新的那首”。", "metrics": {"bleu_score": 42.34426103854124, "chrf_score": 39.04509382751527, "xcomet_score": 0.768548846244812, "xcomet_qe_score": 0.6548635959625244, "metricx_score": 4.197339057922363, "metricx_qe_score": 5.890886306762695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话气泡的内容,但第三个由标注者填写。", "metrics": {"bleu_score": 56.10270450354139, "chrf_score": 48.76118661632658, "xcomet_score": 0.8487764596939087, "xcomet_qe_score": 0.8306021690368652, "metricx_score": 1.9776592254638672, "metricx_qe_score": 1.8045015335083008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个气泡的选择来自每个领域的一些手动提示。第二个,即替代问题,按照以下方式生成:我们总是使用一个简单的模", "metrics": {"bleu_score": 8.442327695262986, "chrf_score": 15.980751532924364, "xcomet_score": 0.31066063046455383, "xcomet_qe_score": 0.3127049207687378, "metricx_score": 6.035087585449219, "metricx_qe_score": 5.271967887878418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "板“你是说A还是B”,按照以下方式生成:", "metrics": {"bleu_score": 4.553719184146073, "chrf_score": 4.791907001508563, "xcomet_score": 0.5728927850723267, "xcomet_qe_score": 0.23752783238887787, "metricx_score": 5.371570587158203, "metricx_qe_score": 5.4751081466674805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们总是使用一个简单的模板“", "metrics": {"bleu_score": 61.62607099729587, "chrf_score": 56.23747935732135, "xcomet_score": 0.9579062461853027, "xcomet_qe_score": 0.945254921913147, "metricx_score": 0.2502274215221405, "metricx_qe_score": 0.1453426629304886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说A还是B”,", "metrics": {"bleu_score": 72.59795291154772, "chrf_score": 76.7412124116884, "xcomet_score": 0.9319638013839722, "xcomet_qe_score": 0.9501749873161316, "metricx_score": 0.6867218017578125, "metricx_qe_score": 0.9900348782539368, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中A和B是从维基百科中抽样的样本。", "metrics": {"bleu_score": 40.335820725998886, "chrf_score": 36.435467941023674, "xcomet_score": 0.9783872365951538, "xcomet_qe_score": 1.0, "metricx_score": 1.1835299730300903, "metricx_qe_score": 1.0342367887496948, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用过的不同抽样方法。", "metrics": {"bleu_score": 81.53551038173119, "chrf_score": 76.47740633807875, "xcomet_score": 0.9971922636032104, "xcomet_qe_score": 0.9996583461761475, "metricx_score": 0.10465388000011444, "metricx_qe_score": 0.10972020775079727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们移动到列表的更高位置时,实体之间的相似性会增加,因此进行区分通常更困难。", "metrics": {"bleu_score": 9.488376048222163, "chrf_score": 15.15687322999591, "xcomet_score": 0.9701329469680786, "xcomet_qe_score": 0.8530547618865967, "metricx_score": 0.9614138603210449, "metricx_qe_score": 1.7253656387329102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种方法是随机均匀抽样;", "metrics": {"bleu_score": 14.458924666162856, "chrf_score": 13.891011015477412, "xcomet_score": 0.9211558103561401, "xcomet_qe_score": 0.9185093641281128, "metricx_score": 1.6544477939605713, "metricx_qe_score": 0.7870208024978638, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二种方法是当实体具有相似的标题时(例如,两本名为“归来”的书);", "metrics": {"bleu_score": 17.081061355061614, "chrf_score": 17.180986846261582, "xcomet_score": 0.7650005221366882, "xcomet_qe_score": 0.7695016860961914, "metricx_score": 2.4517550468444824, "metricx_qe_score": 3.191210985183716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种方法是当它们在维基百科上有相似的描述时", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 66.83459889106199, "xcomet_score": 0.8884967565536499, "xcomet_qe_score": 0.8741853833198547, "metricx_score": 0.49878424406051636, "metricx_qe_score": 0.6783718466758728, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ";最后,当它们具有相同的流派或相同", "metrics": {"bleu_score": 9.361397170835358, "chrf_score": 10.927773781666875, "xcomet_score": 0.3490328788757324, "xcomet_qe_score": 0.35304826498031616, "metricx_score": 12.100329399108887, "metricx_qe_score": 6.392557144165039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的艺术家时(例如)。", "metrics": {"bleu_score": 12.958133493807207, "chrf_score": 13.569504839123672, "xcomet_score": 0.22568947076797485, "xcomet_qe_score": 0.20211011171340942, "metricx_score": 13.51226806640625, "metricx_qe_score": 14.06387996673584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向标注者展示这个替代问题时,他们知道这些实体的名称,但并不一定了解这些实体。", "metrics": {"bleu_score": 52.51364502980971, "chrf_score": 43.59660653631527, "xcomet_score": 0.8201589584350586, "xcomet_qe_score": 0.781043291091919, "metricx_score": 2.705526113510132, "metricx_qe_score": 2.777035713195801, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们会向他们展示有关这两个实体的背景知识。", "metrics": {"bleu_score": 39.42302221292177, "chrf_score": 32.90103743179556, "xcomet_score": 0.9525573253631592, "xcomet_qe_score": 0.8348871469497681, "metricx_score": 1.1137423515319824, "metricx_qe_score": 2.071605920791626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们简单地展示每个歌曲的Google搜索链接,然后要求标注者至少听一些每首歌曲并阅读每首歌曲的介绍。", "metrics": {"bleu_score": 33.956984215643125, "chrf_score": 26.87438838749261, "xcomet_score": 0.8317192792892456, "xcomet_qe_score": 0.8047429323196411, "metricx_score": 3.2707438468933105, "metricx_qe_score": 2.6167893409729004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是歌曲“Easy on Me”的Google搜索结果。", "metrics": {"bleu_score": 58.37979488764489, "chrf_score": 60.35588777244708, "xcomet_score": 0.9661500453948975, "xcomet_qe_score": 0.9616457223892212, "metricx_score": 1.3224492073059082, "metricx_qe_score": 0.8355793356895447, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们会展示一些来自维基百科的背景文本。", "metrics": {"bleu_score": 73.04631582700078, "chrf_score": 60.70123363226811, "xcomet_score": 0.9947742223739624, "xcomet_qe_score": 0.9730352163314819, "metricx_score": 0.6527124047279358, "metricx_qe_score": 1.009541630744934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还会展示它们来自维基百科的图片,以便标注者了解它们的樣子。", "metrics": {"bleu_score": 40.99006250522904, "chrf_score": 34.95283018040827, "xcomet_score": 0.7967973351478577, "xcomet_qe_score": 0.9160035848617554, "metricx_score": 2.2887890338897705, "metricx_qe_score": 2.1111276149749756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注者选择其中一个实体,例如这里的第一首,并用三到五个间接指代来描述它们,", "metrics": {"bleu_score": 32.1208738383528, "chrf_score": 28.415859781035003, "xcomet_score": 0.6563889980316162, "xcomet_qe_score": 0.6133291125297546, "metricx_score": 5.323379039764404, "metricx_qe_score": 4.8328094482421875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“带有钢琴音乐的那首”。", "metrics": {"bleu_score": 14.458924666162856, "chrf_score": 16.511281415552865, "xcomet_score": 0.981981098651886, "xcomet_qe_score": 0.9560460448265076, "metricx_score": 1.4638612270355225, "metricx_qe_score": 1.2667908668518066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些来自我们数据集的例子,", "metrics": {"bleu_score": 37.15011599826721, "chrf_score": 32.91510147228705, "xcomet_score": 0.9798139333724976, "xcomet_qe_score": 0.9676718711853027, "metricx_score": 0.30426329374313354, "metricx_qe_score": 0.3253195583820343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如“没有歌词的那首”、“不是由12岁男孩演唱的那首”、“虚构的”或“来自亚塞拜疆的”。Alt", "metrics": {"bleu_score": 37.01383601511666, "chrf_score": 33.55927407045945, "xcomet_score": 0.9654690027236938, "xcomet_qe_score": 0.9042541980743408, "metricx_score": 2.6793909072875977, "metricx_qe_score": 1.8099511861801147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Entities语料库包含6000个跨三个领域的替代问题,以及42000个间接指代表达。", "metrics": {"bleu_score": 22.547685487420004, "chrf_score": 39.61370509593211, "xcomet_score": 0.5593513250350952, "xcomet_qe_score": 0.5602052211761475, "metricx_score": 6.784903526306152, "metricx_qe_score": 7.8562188148498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用T5X模型的结果总结如下。", "metrics": {"bleu_score": 33.41796039044061, "chrf_score": 38.06929468834363, "xcomet_score": 0.8958873748779297, "xcomet_qe_score": 0.8435847759246826, "metricx_score": 1.3741320371627808, "metricx_qe_score": 1.4147725105285645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与标注者完全相同的背景知识,那么准确率会非常高,在92%到95%左右,但", "metrics": {"bleu_score": 63.27174133732426, "chrf_score": 56.644939309964606, "xcomet_score": 0.6225975155830383, "xcomet_qe_score": 0.6502504348754883, "metricx_score": 4.666834354400635, "metricx_qe_score": 0.9006826281547546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这并不现实。", "metrics": {"bleu_score": 24.598127518343304, "chrf_score": 18.996103847097228, "xcomet_score": 0.9983333349227905, "xcomet_qe_score": 0.9891663789749146, "metricx_score": 0.1312926560640335, "metricx_qe_score": 0.10097083449363708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确率在82%到87%之间,这更现实,", "metrics": {"bleu_score": 76.76100071606257, "chrf_score": 74.92904845166187, "xcomet_score": 0.8881449103355408, "xcomet_qe_score": 0.8379524350166321, "metricx_score": 1.7057561874389648, "metricx_qe_score": 1.812639594078064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如当语言模型检索背景知识时", "metrics": {"bleu_score": 77.21195504586709, "chrf_score": 75.32177659332471, "xcomet_score": 0.9890316724777222, "xcomet_qe_score": 0.9776129722595215, "metricx_score": 0.4007488489151001, "metricx_qe_score": 0.6010951399803162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",准确率仅为60%,因此仍有很大的改进空间。", "metrics": {"bleu_score": 24.638055159899686, "chrf_score": 27.892812033323562, "xcomet_score": 0.8976171016693115, "xcomet_qe_score": 0.8919943571090698, "metricx_score": 7.376460552215576, "metricx_qe_score": 10.057453155517578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还表明,这些模型具有领域泛化能力。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 41.79279370727841, "xcomet_score": 0.9208338260650635, "xcomet_qe_score": 0.915965735912323, "metricx_score": 0.7583819031715393, "metricx_qe_score": 0.9538589715957642, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集链接。", "metrics": {"bleu_score": 29.50234363196404, "chrf_score": 30.119895842275447, "xcomet_score": 0.9908864498138428, "xcomet_qe_score": 0.9903539419174194, "metricx_score": 0.23959046602249146, "metricx_qe_score": 0.38847288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是萨拉·帕皮,来自特伦托大学和布鲁诺·凯斯勒基金会,我将简要介绍《注意力机制作为同时语音翻译的指导》这篇论文,这是我和马特奥·内格里、马可·图尔奇共同完成的工作。", "metrics": {"bleu_score": 38.21212012952532, "chrf_score": 32.443812401521285, "xcomet_score": 0.83266282081604, "xcomet_qe_score": 0.7756330966949463, "metricx_score": 3.3143532276153564, "metricx_qe_score": 2.4565486907958984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同时语音翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.8608854413032532, "xcomet_qe_score": 0.8539649248123169, "metricx_score": 0.5060197114944458, "metricx_qe_score": 0.22112929821014404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时语音翻译(SimulST)是将口语实时翻译成另一种语言文本的过程,从而实现跨语言交流。", "metrics": {"bleu_score": 59.39258268896017, "chrf_score": 55.6175789535945, "xcomet_score": 0.9731327295303345, "xcomet_qe_score": 0.972943902015686, "metricx_score": 2.043914794921875, "metricx_qe_score": 2.400101661682129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前的SimulST模型存在哪些问题?", "metrics": {"bleu_score": 48.44273237963865, "chrf_score": 63.31063364692285, "xcomet_score": 0.964303731918335, "xcomet_qe_score": 0.9658017158508301, "metricx_score": 0.20493721961975098, "metricx_qe_score": 0.5767581462860107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常需要训练特定的架构,引入额外的模块进行优化。例如", "metrics": {"bleu_score": 59.47029646029695, "chrf_score": 52.631769196986596, "xcomet_score": 0.8704071044921875, "xcomet_qe_score": 0.8269098401069641, "metricx_score": 0.8896679282188416, "metricx_qe_score": 1.3096725940704346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",训练过程冗长且复杂,涉及不同的优化目标,以及", "metrics": {"bleu_score": 32.66749259684757, "chrf_score": 25.50544177060963, "xcomet_score": 0.6594167351722717, "xcomet_qe_score": 0.6211128234863281, "metricx_score": 6.238872528076172, "metricx_qe_score": 3.1588170528411865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了达到不同的延迟范围而训练和维护多个模型,", "metrics": {"bleu_score": 57.725241430068074, "chrf_score": 54.55027033025313, "xcomet_score": 0.9508174061775208, "xcomet_qe_score": 0.9639333486557007, "metricx_score": 1.255600929260254, "metricx_qe_score": 1.2323657274246216, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比如训练一个平均延迟1秒的模型,再训练一个平均延迟2秒的模型,以此类推。", "metrics": {"bleu_score": 44.96632055650174, "chrf_score": 37.08093454481971, "xcomet_score": 0.9918245077133179, "xcomet_qe_score": 0.987384557723999, "metricx_score": 0.3253192901611328, "metricx_qe_score": 0.43519362807273865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,无需重新训练或采用特定架构,即可使用已有的离线语音翻译(SD)模型。", "metrics": {"bleu_score": 33.88147925328834, "chrf_score": 27.138729256938753, "xcomet_score": 0.731701672077179, "xcomet_qe_score": 0.8250339031219482, "metricx_score": 4.0886030197143555, "metricx_qe_score": 4.604283332824707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对每个延迟范围使用单一模型,并通过特定的参数来控制延迟。其次,利用", "metrics": {"bleu_score": 30.321326359419633, "chrf_score": 27.465226689066956, "xcomet_score": 0.7202693223953247, "xcomet_qe_score": 0.6040924787521362, "metricx_score": 5.626504898071289, "metricx_qe_score": 1.3777308464050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入和文本输出之间的注意力机制(即交叉注意力机制)已经获得的知识。", "metrics": {"bleu_score": 65.53774768407945, "chrf_score": 72.31587175913516, "xcomet_score": 0.7179860472679138, "xcomet_qe_score": 0.6642794609069824, "metricx_score": 3.014498233795166, "metricx_qe_score": 4.118245601654053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以在右侧看到一个示例。", "metrics": {"bleu_score": 19.077334520603372, "chrf_score": 16.213969691082735, "xcomet_score": 0.9371558427810669, "xcomet_qe_score": 0.8760924339294434, "metricx_score": 1.5626301765441895, "metricx_qe_score": 3.648127794265747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出EDAT,即编码器-解码器注意力机制,它是一种策略,我们根据注意力指向的位置来决定是否发出部分翻译。", "metrics": {"bleu_score": 58.103912764354156, "chrf_score": 55.296970079120285, "xcomet_score": 0.6891971826553345, "xcomet_qe_score": 0.6110314130783081, "metricx_score": 4.253732681274414, "metricx_qe_score": 4.984432220458984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力没有集中,即其总和低于某个阈值α,指向较少数量 λ 语音帧,意味着接收到的信息已经足够稳定", "metrics": {"bleu_score": 36.25592466089261, "chrf_score": 28.50361972179219, "xcomet_score": 0.691182017326355, "xcomet_qe_score": 0.6629302501678467, "metricx_score": 5.677604675292969, "metricx_qe_score": 6.35736083984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",则会发出一个单词。例如,如果我们接收到包含“我将要谈论”的语音片段,并且我们的模型预测德语翻译,当我们查看交叉注意力权重时,我们会发现前两个单词指向最早接收到的语音帧,而最后一个单词指向最后接收到的语音帧,即 λ 语音帧。", "metrics": {"bleu_score": 49.28981278176557, "chrf_score": 37.51193147065438, "xcomet_score": 0.37018150091171265, "xcomet_qe_score": 0.1764039397239685, "metricx_score": 7.4272871017456055, "metricx_qe_score": 8.890253067016602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个单词将被发出,而由于交叉注意力的总和高于某个阈值 α,我们不会发出最后一个单词,而是等待另一个语音片段。如果继续", "metrics": {"bleu_score": 58.13082303450935, "chrf_score": 53.76003750792461, "xcomet_score": 0.6854469776153564, "xcomet_qe_score": 0.6565998792648315, "metricx_score": 5.42079496383667, "metricx_qe_score": 5.954833030700684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们接收到另外三个单词的语音片段,并查看交叉注意力权重,我们会发现没有单词指向最后 λ 语音帧。", "metrics": {"bleu_score": 32.92123810892414, "chrf_score": 30.015327795886737, "xcomet_score": 0.4053953289985657, "xcomet_qe_score": 0.33991238474845886, "metricx_score": 6.181626319885254, "metricx_qe_score": 6.600593090057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个单词将", "metrics": {"bleu_score": 50.04968472345046, "chrf_score": 38.99910737279054, "xcomet_score": 0.852575421333313, "xcomet_qe_score": 0.8376010656356812, "metricx_score": 5.3072357177734375, "metricx_qe_score": 3.1209607124328613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "被发出。如果查看主要结果,我们绘制了同时语音翻译结果图,其中蓝色表示一个方面,衡量翻译质量和平均延迟,即延迟指标。我们还考虑了计算成本相关的平均延迟,该指标考虑了模型预测输出所需的计算时间。", "metrics": {"bleu_score": 23.658234671688817, "chrf_score": 19.7223238096191, "xcomet_score": 0.38083451986312866, "xcomet_qe_score": 0.23727254569530487, "metricx_score": 8.181876182556152, "metricx_qe_score": 8.846067428588867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望这个图中我们的曲线尽可能地高,", "metrics": {"bleu_score": 41.58468660054065, "chrf_score": 37.712317547746174, "xcomet_score": 0.9493615627288818, "xcomet_qe_score": 0.8604124784469604, "metricx_score": 1.6677982807159424, "metricx_qe_score": 2.103560209274292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且尽可能地向左偏移。", "metrics": {"bleu_score": 6.786053138365654, "chrf_score": 6.319275479859423, "xcomet_score": 0.2934328317642212, "xcomet_qe_score": 0.3346281349658966, "metricx_score": 1.9730831384658813, "metricx_qe_score": 2.551499128341675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们与适用于离线模型的适当策略进行比较,即 Wet-Key 策略和局部一致性策略。", "metrics": {"bleu_score": 40.98845889492086, "chrf_score": 26.98556279974452, "xcomet_score": 0.7094005346298218, "xcomet_qe_score": 0.7024997472763062, "metricx_score": 5.863073348999023, "metricx_qe_score": 6.689344882965088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与专为同时语音翻译设计的最先进架构进行比较。", "metrics": {"bleu_score": 59.635843213583335, "chrf_score": 52.065060543321415, "xcomet_score": 0.9180394411087036, "xcomet_qe_score": 0.8885627388954163, "metricx_score": 1.288706660270691, "metricx_qe_score": 2.0478639602661133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是同时语音翻译策略在德语上的所有结果,", "metrics": {"bleu_score": 26.728255206224986, "chrf_score": 25.980527939525878, "xcomet_score": 0.8149834871292114, "xcomet_qe_score": 0.8008978366851807, "metricx_score": 2.8843834400177, "metricx_qe_score": 2.1190967559814453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到 ADAT 在所有应用于离线模型的策略中表现优于其他策略,因为曲线向左偏移。", "metrics": {"bleu_score": 38.93659382647178, "chrf_score": 39.11972731381323, "xcomet_score": 0.9073621034622192, "xcomet_qe_score": 0.880680501461029, "metricx_score": 2.1713104248046875, "metricx_qe_score": 2.5927491188049316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果考虑实际经过的时间或计算成本相关的时间,ADAT 是最快的策略。", "metrics": {"bleu_score": 32.09459976944929, "chrf_score": 28.62646683105788, "xcomet_score": 0.8135539293289185, "xcomet_qe_score": 0.762225866317749, "metricx_score": 4.846706867218018, "metricx_qe_score": 4.781988620758057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多结果,请阅读我们的论文,", "metrics": {"bleu_score": 80.3154665668484, "chrf_score": 73.06831212016971, "xcomet_score": 0.966759204864502, "xcomet_qe_score": 0.9586907625198364, "metricx_score": 0.17957928776741028, "metricx_qe_score": 0.2470196634531021, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源了代码和模型。如果您想了解更多结果,请阅读我们的论文。我们还开源了代码和模型以及同时输出,以促进我们工作的可重复性。", "metrics": {"bleu_score": 5.78397842850855, "chrf_score": 14.129061298983075, "xcomet_score": 0.7602669596672058, "xcomet_qe_score": 0.47998932003974915, "metricx_score": 2.300261974334717, "metricx_qe_score": 2.1458356380462646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家的关注。", "metrics": {"bleu_score": 36.55552228545123, "chrf_score": 49.96037865158913, "xcomet_score": 0.9990874528884888, "xcomet_qe_score": 0.9970511198043823, "metricx_score": 0.5129700303077698, "metricx_qe_score": 0.4527073800563812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好,我叫英,我的同事志洋和我将为大家介绍我们的研究,主题是“多改进”,即通过指令调优提升多模型串行短学习。", "metrics": {"bleu_score": 21.611610346045985, "chrf_score": 16.835851125653882, "xcomet_score": 0.4358923137187958, "xcomet_qe_score": 0.4697324335575104, "metricx_score": 6.3076090812683105, "metricx_qe_score": 6.048508644104004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索一种高效的参数和数据利用方式,即重复使用预训练语言模型来执行不同的下游任务。", "metrics": {"bleu_score": 49.410456108812625, "chrf_score": 41.100313974382715, "xcomet_score": 0.9653409719467163, "xcomet_qe_score": 0.894819438457489, "metricx_score": 0.9045024514198303, "metricx_qe_score": 1.07370924949646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优能够使大型语言模型遵循自然指令,从而在零样本模式下执行未见过的任务。", "metrics": {"bleu_score": 37.53917550040855, "chrf_score": 33.25781563863578, "xcomet_score": 0.9187454581260681, "xcomet_qe_score": 0.8158557415008545, "metricx_score": 1.2536938190460205, "metricx_qe_score": 1.798678994178772, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数之前的指令调优工作都集中于提高在纯语言任务上的零样本性能,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 39.18849055634673, "chrf_score": 34.70863208491694, "xcomet_score": 0.9757570028305054, "xcomet_qe_score": 0.8258297443389893, "metricx_score": 0.9851172566413879, "metricx_qe_score": 1.4915547370910645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们希望调查指令调优在多模态预训练模型上是否真的能够提高对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 37.503017120096835, "chrf_score": 32.82057081432642, "xcomet_score": 0.8035178780555725, "xcomet_qe_score": 0.7453427314758301, "metricx_score": 2.193728446960449, "metricx_qe_score": 2.3114774227142334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们进行研究时,我们发现自然语言处理和多模态领域之间存在指令数据集可用性的巨大差距。 ", "metrics": {"bleu_score": 58.172955860703546, "chrf_score": 53.42219768932179, "xcomet_score": 0.982318639755249, "xcomet_qe_score": 0.9505348205566406, "metricx_score": 0.6305360198020935, "metricx_qe_score": 0.8630855679512024, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过1600个纯语言指令任务,", "metrics": {"bleu_score": 39.07380249452502, "chrf_score": 43.51585505997271, "xcomet_score": 0.9249563217163086, "xcomet_qe_score": 0.8496742248535156, "metricx_score": 0.6940819025039673, "metricx_qe_score": 0.892221212387085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但没有大规模的公开可用的多模态指令任务。", "metrics": {"bleu_score": 69.3395566222006, "chrf_score": 64.4399100909652, "xcomet_score": 0.9092608690261841, "xcomet_qe_score": 0.8098517656326294, "metricx_score": 1.6030091047286987, "metricx_qe_score": 2.1660940647125244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这促使我们构建了一个多模态指令调优数据集。", "metrics": {"bleu_score": 52.08256326654537, "chrf_score": 43.036282923985176, "xcomet_score": 0.9706735610961914, "xcomet_qe_score": 0.9573607444763184, "metricx_score": 1.6108468770980835, "metricx_qe_score": 0.9065489172935486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们介绍MultiInstruct,这是第一个多模态指令调优基准数据集,它包含62个多样化的多模态任务,涵盖10个主要类别。", "metrics": {"bleu_score": 49.373060569267125, "chrf_score": 50.94007613185627, "xcomet_score": 0.913057804107666, "xcomet_qe_score": 0.8909895420074463, "metricx_score": 1.0836502313613892, "metricx_qe_score": 1.2052994966506958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务源自21个现有的开源数据集,并且每个任务都配备了5个专家撰写的指令。", "metrics": {"bleu_score": 62.37923061531352, "chrf_score": 61.5353813174791, "xcomet_score": 0.9783148765563965, "xcomet_qe_score": 0.9669736623764038, "metricx_score": 1.1538572311401367, "metricx_qe_score": 2.1539154052734375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们的提出的数据集上研究多模态指令调优,我们以OFA,一种统一的多模态预训练模型,作为我们的基础模型。", "metrics": {"bleu_score": 61.62020100904077, "chrf_score": 60.34479709413284, "xcomet_score": 0.931743860244751, "xcomet_qe_score": 0.8608055114746094, "metricx_score": 1.808769941329956, "metricx_qe_score": 2.1642370223999023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "OFA使用统一的词汇表来表示语言、图像令牌和边界框的坐标。", "metrics": {"bleu_score": 56.74773954614978, "chrf_score": 50.92614531953855, "xcomet_score": 0.8628093004226685, "xcomet_qe_score": 0.8026986122131348, "metricx_score": 4.591739177703857, "metricx_qe_score": 4.2119550704956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们展示了来自我们MultiInstruct数据集的一些示例实例。 为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 71.99292939028123, "chrf_score": 76.83099645246564, "xcomet_score": 0.9287924766540527, "xcomet_qe_score": 0.8583124279975891, "metricx_score": 1.7903228998184204, "metricx_qe_score": 3.079369306564331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循OFA的方法,并将所有任务都以统一的序列到序列格式进行表达,", "metrics": {"bleu_score": 60.317983955216874, "chrf_score": 62.15236838148748, "xcomet_score": 0.853632926940918, "xcomet_qe_score": 0.8202720880508423, "metricx_score": 1.676336646080017, "metricx_qe_score": 2.396397113800049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中输入文本、图像、指令和边界框都表示在相同的令牌空间中。", "metrics": {"bleu_score": 65.72977136767356, "chrf_score": 61.528242085713345, "xcomet_score": 0.8752689361572266, "xcomet_qe_score": 0.8392897844314575, "metricx_score": 4.5445685386657715, "metricx_qe_score": 4.802751064300537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我将谈论多模态指令调优。", "metrics": {"bleu_score": 40.65220433860868, "chrf_score": 34.158406952524594, "xcomet_score": 0.9148857593536377, "xcomet_qe_score": 0.899833619594574, "metricx_score": 0.7498553395271301, "metricx_qe_score": 0.7857111692428589, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用9组中的53个任务进行训练,并且每个任务抽取10,000个样本。 ", "metrics": {"bleu_score": 54.0076394443244, "chrf_score": 50.57474250398244, "xcomet_score": 0.9330874681472778, "xcomet_qe_score": 0.9517338275909424, "metricx_score": 1.2616184949874878, "metricx_qe_score": 1.99490487575531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留常识推理组进行测试,并从视觉问答和杂项组中选择另外五个任务。", "metrics": {"bleu_score": 41.60251722861183, "chrf_score": 33.66379820701667, "xcomet_score": 0.7511721849441528, "xcomet_qe_score": 0.7533195614814758, "metricx_score": 3.1825942993164062, "metricx_qe_score": 2.9102330207824707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务的测试集中的所有实例。", "metrics": {"bleu_score": 46.47302841841259, "chrf_score": 37.37837684397625, "xcomet_score": 0.8294873237609863, "xcomet_qe_score": 0.8276590704917908, "metricx_score": 1.6202492713928223, "metricx_qe_score": 1.715619444847107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令测试集随机抽取20个任务作为NLP领域的未见过的任务。", "metrics": {"bleu_score": 47.21987946250101, "chrf_score": 44.314776146015205, "xcomet_score": 0.7638782262802124, "xcomet_qe_score": 0.7318546772003174, "metricx_score": 2.8035764694213867, "metricx_qe_score": 2.2396798133850098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的OFA大型模型作为基础模型。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9649795889854431, "xcomet_qe_score": 0.9640771746635437, "metricx_score": 1.2312500476837158, "metricx_qe_score": 2.793785810470581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 53.884478712336644, "xcomet_score": 0.969638466835022, "xcomet_qe_score": 0.8911672830581665, "metricx_score": 0.8351970911026001, "metricx_qe_score": 1.3651412725448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例都与它五个指令模板中的一个随机组合。", "metrics": {"bleu_score": 62.272254428817725, "chrf_score": 58.36279469503426, "xcomet_score": 0.9701100587844849, "xcomet_qe_score": 0.8253300189971924, "metricx_score": 1.1888805627822876, "metricx_qe_score": 1.584641933441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每个任务的测试过程中,我们进行总共五个实验,通过使用每个实验中的五个指令之一来评估", "metrics": {"bleu_score": 39.73419555633869, "chrf_score": 35.05070058985335, "xcomet_score": 0.8131285905838013, "xcomet_qe_score": 0.7987039089202881, "metricx_score": 3.038862466812134, "metricx_qe_score": 3.0893561840057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型。我们报告所有五个实验的平均值和最大性能,以及性能的标准差。", "metrics": {"bleu_score": 29.782859811966315, "chrf_score": 24.673299871968048, "xcomet_score": 0.4950985610485077, "xcomet_qe_score": 0.5662751197814941, "metricx_score": 3.951343536376953, "metricx_qe_score": 4.34510612487793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模态分类任务,我们报告准确率。如果", "metrics": {"bleu_score": 49.380155419366794, "chrf_score": 41.053037475621764, "xcomet_score": 0.726046085357666, "xcomet_qe_score": 0.7760872840881348, "metricx_score": 3.190519332885742, "metricx_qe_score": 0.7835210561752319, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是多模态生成任务,我们报告均方根误差。对于排序任务,我们同样报告均方根误差。", "metrics": {"bleu_score": 30.82398735234032, "chrf_score": 23.66332475929286, "xcomet_score": 0.8067268133163452, "xcomet_qe_score": 0.7639753818511963, "metricx_score": 4.5603132247924805, "metricx_qe_score": 4.648036479949951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为敏感性,它", "metrics": {"bleu_score": 63.8836571192159, "chrf_score": 61.24883209885134, "xcomet_score": 0.7990299463272095, "xcomet_qe_score": 0.7209134101867676, "metricx_score": 3.887962579727173, "metricx_qe_score": 0.9001494646072388, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "衡量模型在指令措辞略有变化的情况下,始终如一地产生相同输出的能力。", "metrics": {"bleu_score": 36.626662079385895, "chrf_score": 31.1933702319496, "xcomet_score": 0.9693981409072876, "xcomet_qe_score": 0.984955906867981, "metricx_score": 2.2728285789489746, "metricx_qe_score": 3.373852252960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,指令调优可以显著提高OFA在场景多模态任务上的性能。", "metrics": {"bleu_score": 56.46631238098639, "chrf_score": 50.85864575546739, "xcomet_score": 0.9108229875564575, "xcomet_qe_score": 0.9367860555648804, "metricx_score": 2.5669455528259277, "metricx_qe_score": 2.809177875518799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习可以使指令调优受益。", "metrics": {"bleu_score": 66.16626919250143, "chrf_score": 61.79275568768322, "xcomet_score": 0.8934085369110107, "xcomet_qe_score": 0.7488346695899963, "metricx_score": 4.113847732543945, "metricx_qe_score": 4.640275955200195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们这里可以看到,随着任务数量的增加,模型实现性能提升,并且同时降低了敏感性。", "metrics": {"bleu_score": 33.9445740152787, "chrf_score": 30.414990078527225, "xcomet_score": 0.9274472594261169, "xcomet_qe_score": 0.9381247162818909, "metricx_score": 1.6803454160690308, "metricx_qe_score": 1.6397963762283325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一个实验,", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 39.39127110257559, "xcomet_score": 0.9774124622344971, "xcomet_qe_score": 0.9565337896347046, "metricx_score": 0.34012168645858765, "metricx_qe_score": 0.33167219161987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用一个指令与五个指令。", "metrics": {"bleu_score": 19.320813030085752, "chrf_score": 19.324255110675633, "xcomet_score": 0.7846347689628601, "xcomet_qe_score": 0.6790972352027893, "metricx_score": 3.184023380279541, "metricx_qe_score": 4.126527786254883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如我们所看到的,使用更多的指令可以提高模型的整体性能并显著降低其敏感性。", "metrics": {"bleu_score": 62.65497900743081, "chrf_score": 57.49768929733079, "xcomet_score": 0.9482861161231995, "xcomet_qe_score": 1.0, "metricx_score": 0.9321506023406982, "metricx_qe_score": 1.0663268566131592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这显示了不同的微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 77.21947901921794, "chrf_score": 70.98343284682292, "xcomet_score": 0.9882955551147461, "xcomet_qe_score": 0.9814878702163696, "metricx_score": 1.2330644130706787, "metricx_qe_score": 1.4857813119888306, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行迁移学习,模型可以比原始OFA模型实现更好的敏感性。", "metrics": {"bleu_score": 35.78536420536374, "chrf_score": 32.785403626451135, "xcomet_score": 0.8641151189804077, "xcomet_qe_score": 0.776053249835968, "metricx_score": 2.6623692512512207, "metricx_qe_score": 3.347651481628418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行迁移学习可以帮助OFA在Nitro Instruct数据集上实现更好的性能。", "metrics": {"bleu_score": 63.1740803736222, "chrf_score": 55.04543000873957, "xcomet_score": 0.8533679246902466, "xcomet_qe_score": 0.6629374027252197, "metricx_score": 6.116785049438477, "metricx_qe_score": 6.570442199707031, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,我们提出了第一个大规模的多模态指令调优数据集。我们显著提高了OFA的零样本能力,并探索了不同的迁移学习技术,展示了它们的优势。", "metrics": {"bleu_score": 56.777297298022994, "chrf_score": 50.96402938384897, "xcomet_score": 0.8152650594711304, "xcomet_qe_score": 0.7912680506706238, "metricx_score": 2.715190887451172, "metricx_qe_score": 3.3240513801574707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标,称为敏感性。还有一件事", "metrics": {"bleu_score": 48.624389134644154, "chrf_score": 50.570155812438045, "xcomet_score": 0.6670271754264832, "xcomet_qe_score": 0.5847718119621277, "metricx_score": 3.5151607990264893, "metricx_qe_score": 2.159074306488037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们正在收集一个更大的多模态指令调优数据集,包含大约150个额外的变体语言任务,并即将发布。 这是一个指", "metrics": {"bleu_score": 48.453401031560105, "chrf_score": 45.06289993924871, "xcomet_score": 0.48427483439445496, "xcomet_qe_score": 0.46440187096595764, "metricx_score": 8.42619514465332, "metricx_qe_score": 5.667304992675781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "向我们数据和模型的二维码。", "metrics": {"bleu_score": 67.82635235687796, "chrf_score": 64.7142421759636, "xcomet_score": 0.7468166351318359, "xcomet_qe_score": 0.7829496264457703, "metricx_score": 4.167276859283447, "metricx_qe_score": 5.391764163970947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.974276065826416, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯塔夫·辛哈,很高兴欢迎大家参加关于我们ACL 2023论文的报告:“", "metrics": {"bleu_score": 40.836177158405455, "chrf_score": 40.56309303171917, "xcomet_score": 0.8040014505386353, "xcomet_qe_score": 0.861876368522644, "metricx_score": 3.3134994506835938, "metricx_qe_score": 2.7936129570007324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型可接受性判断并非总是对上下文稳健”。这是一项与", "metrics": {"bleu_score": 47.901455811287484, "chrf_score": 38.870274652883346, "xcomet_score": 0.5716686844825745, "xcomet_qe_score": 0.5730562210083008, "metricx_score": 8.730151176452637, "metricx_qe_score": 6.599958419799805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "约翰·戈特耶、艾伦·穆勒、卡尼什卡·米什拉、卡伦·富恩特斯、罗杰·利维和阿迪娜·威廉姆斯合作的研究。", "metrics": {"bleu_score": 2.124035816216534, "chrf_score": 2.0964677782764656, "xcomet_score": 0.5176052451133728, "xcomet_qe_score": 0.6293885707855225, "metricx_score": 4.157793045043945, "metricx_qe_score": 4.005030155181885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个工作中,我们重新审视了极小对方法。", "metrics": {"bleu_score": 36.571230905932296, "chrf_score": 31.749297672285287, "xcomet_score": 0.8628144264221191, "xcomet_qe_score": 0.8567193150520325, "metricx_score": 2.9399659633636475, "metricx_qe_score": 3.9560317993164062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "极小对方法基本上是根据语言模型的接受性判断来评估语言模型,其中接受性", "metrics": {"bleu_score": 27.150436323855665, "chrf_score": 24.86156085823833, "xcomet_score": 0.6783059239387512, "xcomet_qe_score": 0.6236755847930908, "metricx_score": 6.9695024490356445, "metricx_qe_score": 5.053368091583252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "判断也可能包括语法性,比如BLIMP、句法、Gym或者关于刻板印象方面的可接受性,比如Krauss对。", "metrics": {"bleu_score": 20.350599996262506, "chrf_score": 18.927822547458696, "xcomet_score": 0.4907597303390503, "xcomet_qe_score": 0.4850306808948517, "metricx_score": 7.08627986907959, "metricx_qe_score": 7.703235149383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极小对方法中,评估语言模型的典型方式是展示一个可接受的句子或语法正确的句子,然后展示一个不可接受的句子或语法错误的句子。", "metrics": {"bleu_score": 47.37436058579738, "chrf_score": 42.11703891663984, "xcomet_score": 0.8335633277893066, "xcomet_qe_score": 0.8132940530776978, "metricx_score": 1.5856374502182007, "metricx_qe_score": 3.0426197052001953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "整个模型的过程基本上会赋予可接受的句子更高的概率。", "metrics": {"bleu_score": 27.748702735605818, "chrf_score": 24.70820284730034, "xcomet_score": 0.8480167984962463, "xcomet_qe_score": 0.735746443271637, "metricx_score": 3.7632009983062744, "metricx_qe_score": 3.935486316680908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前MPP流程基本上不允许我们评估模型对较长句子的接受程度。", "metrics": {"bleu_score": 73.94041400422365, "chrf_score": 69.14788383179187, "xcomet_score": 0.9309839010238647, "xcomet_qe_score": 0.8773901462554932, "metricx_score": 1.4122931957244873, "metricx_qe_score": 1.8790314197540283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型正不断涌现出更长和更长的", "metrics": {"bleu_score": 14.962848372546667, "chrf_score": 14.287187203634168, "xcomet_score": 0.7269456386566162, "xcomet_qe_score": 0.7485899925231934, "metricx_score": 6.147454261779785, "metricx_qe_score": 4.815238952636719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文窗口。因此,我们", "metrics": {"bleu_score": 3.0196376639848626, "chrf_score": 10.803286220736455, "xcomet_score": 0.15928994119167328, "xcomet_qe_score": 0.1393934190273285, "metricx_score": 19.247161865234375, "metricx_qe_score": 18.906314849853516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "重新审视了", "metrics": {"bleu_score": 0.24729105095220405, "chrf_score": 7.165318851431647, "xcomet_score": 0.1380453109741211, "xcomet_qe_score": 0.13593308627605438, "metricx_score": 13.762127876281738, "metricx_qe_score": 20.18882942199707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43476197123527527, "xcomet_qe_score": 0.15987929701805115, "metricx_score": 2.4095587730407715, "metricx_qe_score": 5.211564064025879, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据集本身,并从这些数据集中选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 47.65643224106713, "chrf_score": 55.72323617077111, "xcomet_score": 0.38206997513771057, "xcomet_qe_score": 0.19221626222133636, "metricx_score": 6.73923397064209, "metricx_qe_score": 9.887960433959961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们选择了一个典型语法性极小对,来自BLIMP数据集中的附例岛现象。", "metrics": {"bleu_score": 20.24746973933765, "chrf_score": 20.63679853657709, "xcomet_score": 0.6984790563583374, "xcomet_qe_score": 0.7183897495269775, "metricx_score": 7.512726306915283, "metricx_qe_score": 6.597679615020752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重新创建更长的序列,这些序列是可接受的,并且具有相同的语法结构,为此,", "metrics": {"bleu_score": 72.5616787305481, "chrf_score": 64.27349278279958, "xcomet_score": 0.8111197352409363, "xcomet_qe_score": 0.7189230918884277, "metricx_score": 4.438824653625488, "metricx_qe_score": 5.062369346618652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从附例岛中提取语法正确的句子,并将其作为前缀添加到可接受的查询和不可接受的查询中。", "metrics": {"bleu_score": 69.8008622518928, "chrf_score": 56.181911101802385, "xcomet_score": 0.7118574380874634, "xcomet_qe_score": 0.725354790687561, "metricx_score": 2.690788745880127, "metricx_qe_score": 3.4679787158966064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过选择相同匹配的不可接受句子来做同样的事情,这也可以用来测试模型的接受性。", "metrics": {"bleu_score": 62.455955055354266, "chrf_score": 54.17164298838204, "xcomet_score": 0.8214998245239258, "xcomet_qe_score": 0.7981212139129639, "metricx_score": 2.2386631965637207, "metricx_qe_score": 2.101733446121216, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以通过选择来自不同子集或不同数据集的句子来做同样的事情。", "metrics": {"bleu_score": 44.22851061578714, "chrf_score": 36.53471289298065, "xcomet_score": 0.9781564474105835, "xcomet_qe_score": 0.8438293933868408, "metricx_score": 0.7835254669189453, "metricx_qe_score": 1.2435696125030518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称之为不匹配场景。", "metrics": {"bleu_score": 39.5348755325422, "chrf_score": 34.21070168082437, "xcomet_score": 0.9749166965484619, "xcomet_qe_score": 0.8934249877929688, "metricx_score": 0.8040903806686401, "metricx_qe_score": 1.6311742067337036, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,句子仍然来自相关的数据库,但不是与您进行评估的数据库相同。", "metrics": {"bleu_score": 29.64591158728422, "chrf_score": 28.263386029959754, "xcomet_score": 0.8799088001251221, "xcomet_qe_score": 0.7307456731796265, "metricx_score": 1.950570821762085, "metricx_qe_score": 2.898902416229248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对可接受性情况做同样的事情。", "metrics": {"bleu_score": 22.449758011137355, "chrf_score": 22.573862831755765, "xcomet_score": 0.8129264116287231, "xcomet_qe_score": 0.7805144190788269, "metricx_score": 3.0295467376708984, "metricx_qe_score": 2.6460728645324707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以选择来自完全无关领域的句子,例如维基百科。", "metrics": {"bleu_score": 28.910426007611058, "chrf_score": 25.64521497577592, "xcomet_score": 0.9907609224319458, "xcomet_qe_score": 0.9836946725845337, "metricx_score": 0.7911625504493713, "metricx_qe_score": 1.3930972814559937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们,模型的接受性判断是否实际上受到任何上下文的影响,这种上下文与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 41.56923350035258, "chrf_score": 39.91348150198742, "xcomet_score": 0.6906903386116028, "xcomet_qe_score": 0.7272669076919556, "metricx_score": 4.637752532958984, "metricx_qe_score": 5.7665791511535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型表现如何呢?", "metrics": {"bleu_score": 9.29675796576443, "chrf_score": 9.644582470669427, "xcomet_score": 0.8517210483551025, "xcomet_qe_score": 0.8354972004890442, "metricx_score": 1.0570439100265503, "metricx_qe_score": 0.2590245008468628, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看来自维基百科的句子,这些句子与当前查询对完全无关。在那里,我们发现MPP判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 55.60570481588639, "chrf_score": 49.22730826259353, "xcomet_score": 0.9230983853340149, "xcomet_qe_score": 0.8690973520278931, "metricx_score": 4.32824182510376, "metricx_qe_score": 5.954617500305176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到1024,以最大化OPT和GPT-2模型。我们在此", "metrics": {"bleu_score": 63.39704064341255, "chrf_score": 76.13530065505877, "xcomet_score": 0.611101508140564, "xcomet_qe_score": 0.6591165065765381, "metricx_score": 4.7535905838012695, "metricx_qe_score": 2.5125441551208496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "看到橙色虚线,MPP判断相对稳定。", "metrics": {"bleu_score": 56.42812502283149, "chrf_score": 55.51329147950517, "xcomet_score": 0.8654378652572632, "xcomet_qe_score": 0.8300250768661499, "metricx_score": 2.1635308265686035, "metricx_qe_score": 3.376682758331299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,当选择来自同一数据集的句子时会发生什么?", "metrics": {"bleu_score": 38.95548531886182, "chrf_score": 34.77126584543948, "xcomet_score": 0.9900060892105103, "xcomet_qe_score": 0.9174706935882568, "metricx_score": 0.8468537330627441, "metricx_qe_score": 1.4763095378875732, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里,我们正在选择或创建来自同一BLIMP或SyntaxGym数据集的接受性和不可接受性领域中的句子。在那里,", "metrics": {"bleu_score": 32.34043476847562, "chrf_score": 43.98363012428894, "xcomet_score": 0.6278061270713806, "xcomet_qe_score": 0.6831653118133545, "metricx_score": 5.644009590148926, "metricx_qe_score": 4.3455281257629395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,当添加可接受的前缀或不可接受的前缀时,MPP判断会显著增加或减少。", "metrics": {"bleu_score": 63.373598049766244, "chrf_score": 60.63990318815742, "xcomet_score": 0.8336880207061768, "xcomet_qe_score": 0.8345394134521484, "metricx_score": 3.4514572620391846, "metricx_qe_score": 2.6053273677825928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,当匹配结构时,也就是说,当我们添加可接受的前缀或不可接受的前缀时,当我们从blimp-person-text-gym中的相同现象中选择句子时,我们会看到MPP判断对模型产生巨大增加或巨大减少,这取决于所选前缀是可接受还是不可接受。", "metrics": {"bleu_score": 41.220487890655335, "chrf_score": 38.60353223051048, "xcomet_score": 0.6220270395278931, "xcomet_qe_score": 0.5607144832611084, "metricx_score": 7.664586544036865, "metricx_qe_score": 7.86298131942749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种效应随着上下文长度的增加而变得非常明显,这可能会影响具有大上下文窗口的新型语言模型。", "metrics": {"bleu_score": 51.685315244323625, "chrf_score": 47.61357033497813, "xcomet_score": 0.9455976486206055, "xcomet_qe_score": 0.8599069714546204, "metricx_score": 1.3945674896240234, "metricx_qe_score": 1.7775993347167969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么匹配的前缀会如此影响语言模型的判断呢?", "metrics": {"bleu_score": 39.89277721510357, "chrf_score": 35.67905143663413, "xcomet_score": 0.9220811128616333, "xcomet_qe_score": 0.8994450569152832, "metricx_score": 0.5901103615760803, "metricx_qe_score": 0.7031182646751404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,尝试通过尝试保留相关的结构来构建输入句子,同时为输入添加噪声。", "metrics": {"bleu_score": 37.270056863685824, "chrf_score": 34.987340062469436, "xcomet_score": 0.7884334325790405, "xcomet_qe_score": 0.7303966283798218, "metricx_score": 4.395240306854248, "metricx_qe_score": 4.938405513763428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行多次此类扰动后,我们发现这些噪声中的任何一项实际上都没有使模型改变其MPP判断趋势的方式。", "metrics": {"bleu_score": 44.234029908299064, "chrf_score": 40.877334793336345, "xcomet_score": 0.8868725299835205, "xcomet_qe_score": 0.9335429668426514, "metricx_score": 4.072179794311523, "metricx_qe_score": 4.253057479858398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型以相似的方式对扰动后的句子敏感。", "metrics": {"bleu_score": 20.9664141869602, "chrf_score": 22.24086267739922, "xcomet_score": 0.8496754169464111, "xcomet_qe_score": 0.8267393112182617, "metricx_score": 1.837612509727478, "metricx_qe_score": 2.6591763496398926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们扰动可接受性领域的句子时,我们看到所有扰动中MPP判断的相似增加。当我们扰动不可接受性领域的句子时,我们看到MPP判断以相似的方式减少。", "metrics": {"bleu_score": 40.36540234174307, "chrf_score": 35.43535160865229, "xcomet_score": 0.5799536108970642, "xcomet_qe_score": 0.7563114166259766, "metricx_score": 4.761584281921387, "metricx_qe_score": 4.763718605041504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们工作的关键要点是,语言模型对句子共享的潜在句法和语义特征敏感。", "metrics": {"bleu_score": 52.438992072868665, "chrf_score": 47.509042413833775, "xcomet_score": 0.8234333395957947, "xcomet_qe_score": 0.8156580328941345, "metricx_score": 1.5067424774169922, "metricx_qe_score": 1.7132377624511719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,我们使用短句和单句输入的MPP评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 55.422459720511355, "chrf_score": 47.33644017763818, "xcomet_score": 0.8808045387268066, "xcomet_qe_score": 0.7667496204376221, "metricx_score": 2.3341124057769775, "metricx_qe_score": 2.7464895248413086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以获取更多实验细节。", "metrics": {"bleu_score": 32.28475421040683, "chrf_score": 31.292934579805326, "xcomet_score": 0.99491286277771, "xcomet_qe_score": 0.9996927976608276, "metricx_score": 0.1711377501487732, "metricx_qe_score": 0.17318548262119293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9482142925262451, "xcomet_qe_score": 0.888888955116272, "metricx_score": 0.47969555854797363, "metricx_qe_score": 0.6286952495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是张宇生,来自宾夕法尼亚州立大学。", "metrics": {"bleu_score": 56.333167591361615, "chrf_score": 39.71255891014154, "xcomet_score": 0.915104329586029, "xcomet_qe_score": 0.8854259252548218, "metricx_score": 0.5933114290237427, "metricx_qe_score": 1.016010046005249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的工作,即多语言自然语言和语义表示中的跨语言语义解析。", "metrics": {"bleu_score": 55.63253762512867, "chrf_score": 42.22293656657095, "xcomet_score": 0.7812638878822327, "xcomet_qe_score": 0.9186901450157166, "metricx_score": 2.4985060691833496, "metricx_qe_score": 3.6478078365325928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义解析是将用户查询(如 SQL 和 lambda 微积分)构建语义表示的任务。", "metrics": {"bleu_score": 46.696578632524336, "chrf_score": 38.40229529733775, "xcomet_score": 0.9332159161567688, "xcomet_qe_score": 0.8448836803436279, "metricx_score": 1.8575365543365479, "metricx_qe_score": 3.529660940170288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义解析是将多种自然语言中的查询翻译成多种语义表示的任务。", "metrics": {"bleu_score": 75.067114161108, "chrf_score": 69.37390349550465, "xcomet_score": 0.9383502006530762, "xcomet_qe_score": 0.9292780756950378, "metricx_score": 1.2949281930923462, "metricx_qe_score": 3.286876678466797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将多种自然语言中的查询翻译成 SQL、Lambda 或 FunQL 等。", "metrics": {"bleu_score": 75.75478739478386, "chrf_score": 79.28368872394617, "xcomet_score": 0.9878084659576416, "xcomet_qe_score": 0.9670835733413696, "metricx_score": 0.9378871321678162, "metricx_qe_score": 1.1835088729858398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型是独立提出的,并在有限的任务和应用的数据集上进行评估。", "metrics": {"bleu_score": 68.84292999421709, "chrf_score": 68.29895997876844, "xcomet_score": 0.9956609010696411, "xcomet_qe_score": 0.9849028587341309, "metricx_score": 0.46178072690963745, "metricx_qe_score": 0.6807653307914734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在某些自然语言上存在覆盖范围不足的问题。", "metrics": {"bleu_score": 26.24310277292268, "chrf_score": 30.2490285776615, "xcomet_score": 0.7929179668426514, "xcomet_qe_score": 0.7666547298431396, "metricx_score": 3.7998132705688477, "metricx_qe_score": 4.800374984741211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失,由于某些微观表示的覆盖范围", "metrics": {"bleu_score": 6.401581246466459, "chrf_score": 8.764583983876005, "xcomet_score": 0.6488485932350159, "xcomet_qe_score": 0.5657601952552795, "metricx_score": 6.904091835021973, "metricx_qe_score": 6.336457252502441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不足,lambda 微积分缺失,或者它们仅在某些神经网络模型上进行评估。", "metrics": {"bleu_score": 40.49116787132508, "chrf_score": 45.311395092734486, "xcomet_score": 0.6740812063217163, "xcomet_qe_score": 0.5089061260223389, "metricx_score": 4.930898666381836, "metricx_qe_score": 5.965704441070557, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个模型用于进行评估。", "metrics": {"bleu_score": 20.691216034052964, "chrf_score": 19.71921729106115, "xcomet_score": 0.997992992401123, "xcomet_qe_score": 0.9869542121887207, "metricx_score": 0.5461664795875549, "metricx_qe_score": 0.7064492702484131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提出了 Exampler。", "metrics": {"bleu_score": 46.713797772819994, "chrf_score": 23.18314928418945, "xcomet_score": 0.8695065975189209, "xcomet_qe_score": 0.8435243964195251, "metricx_score": 1.5682450532913208, "metricx_qe_score": 3.425964593887329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们提供了一个统一的数据集 Exampler,用于在多种自然语言和语义表示中进行跨语言语义解析。", "metrics": {"bleu_score": 63.70082049877986, "chrf_score": 52.26946263538146, "xcomet_score": 0.8739078044891357, "xcomet_qe_score": 0.8336689472198486, "metricx_score": 3.9903130531311035, "metricx_qe_score": 4.662046909332275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含来自各种领域的九个数据集,五个语义解析任务,八种语义表示,以及来自 15 个语系中的 22 种自然语言。", "metrics": {"bleu_score": 47.432813290968845, "chrf_score": 48.188801307866456, "xcomet_score": 0.9737237691879272, "xcomet_qe_score": 0.9727522134780884, "metricx_score": 0.804646909236908, "metricx_qe_score": 1.2001399993896484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 80.20219183488042, "chrf_score": 71.52080420921001, "xcomet_score": 0.9888695478439331, "xcomet_qe_score": 0.913833737373352, "metricx_score": 1.4580811262130737, "metricx_qe_score": 2.0225110054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是翻译测试。", "metrics": {"bleu_score": 66.06328636027612, "chrf_score": 56.005291005291, "xcomet_score": 0.9600571393966675, "xcomet_qe_score": 0.9592820405960083, "metricx_score": 0.29587188363075256, "metricx_qe_score": 0.4503270089626312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 Google 翻译 API 将源语言翻译成目标语言,然后使用单语模型进行训练和评估。", "metrics": {"bleu_score": 82.66660014007991, "chrf_score": 76.41626540305097, "xcomet_score": 0.9742509126663208, "xcomet_qe_score": 0.837993860244751, "metricx_score": 0.5891801118850708, "metricx_qe_score": 0.5521913766860962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们使用英语模型在英语查询上进行训练。在推理过程中,我们使用 API 将德语查询翻译成英语,然后使用训练好的模型来预测 SQL。", "metrics": {"bleu_score": 70.2200008314099, "chrf_score": 66.94837991421656, "xcomet_score": 0.8465235233306885, "xcomet_qe_score": 0.87782883644104, "metricx_score": 1.1772280931472778, "metricx_qe_score": 2.0350141525268555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语模型。", "metrics": {"bleu_score": 51.56626918239823, "chrf_score": 40.77320827320827, "xcomet_score": 0.8812164068222046, "xcomet_qe_score": 0.84535151720047, "metricx_score": 1.4693257808685303, "metricx_qe_score": 1.2090201377868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种设置中,源语言与目标语言相同。例如,德语到德语或英语到英语。", "metrics": {"bleu_score": 72.68097337162342, "chrf_score": 66.44665575455079, "xcomet_score": 0.9278980493545532, "xcomet_qe_score": 0.8964245319366455, "metricx_score": 0.5449566841125488, "metricx_qe_score": 0.6620081663131714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语少样本设置,通过仅使用 10% 的训练数据来训练单语模型。我们还测试了", "metrics": {"bleu_score": 50.017771461842635, "chrf_score": 52.0290743813148, "xcomet_score": 0.6525238752365112, "xcomet_qe_score": 0.5807638168334961, "metricx_score": 6.98633337020874, "metricx_qe_score": 4.389514446258545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多语言模型,我们为所有语言训练一个多语言模型。", "metrics": {"bleu_score": 74.68637581795103, "chrf_score": 69.1378086470003, "xcomet_score": 0.8159241676330566, "xcomet_qe_score": 0.8434106111526489, "metricx_score": 1.9682557582855225, "metricx_qe_score": 2.5390865802764893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语、中文查询放在一起训练一个多语言模型。", "metrics": {"bleu_score": 83.43396336944906, "chrf_score": 78.93203219586009, "xcomet_score": 0.9516309499740601, "xcomet_qe_score": 0.9600837230682373, "metricx_score": 1.4132899045944214, "metricx_qe_score": 2.9107823371887207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理过程中,我们可以使用该模型来翻译德语查询或中文查询等。", "metrics": {"bleu_score": 78.99176824359823, "chrf_score": 71.60421907140166, "xcomet_score": 0.9784727096557617, "xcomet_qe_score": 0.9526165723800659, "metricx_score": 0.6815962791442871, "metricx_qe_score": 1.1087565422058105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,并迁移到另一种语言。因此,", "metrics": {"bleu_score": 32.88861494180287, "chrf_score": 28.349026626638086, "xcomet_score": 0.7649554014205933, "xcomet_qe_score": 0.7196979522705078, "metricx_score": 5.570684432983398, "metricx_qe_score": 5.840086460113525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们使用英语查询或英语和德语少样本查询的组合来训练一个多语言模型并预测 SQL 输出。", "metrics": {"bleu_score": 70.73558063285769, "chrf_score": 62.206546984028655, "xcomet_score": 0.96338951587677, "xcomet_qe_score": 0.8470485210418701, "metricx_score": 1.2210338115692139, "metricx_qe_score": 2.040487766265869, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了很多有趣的成果。", "metrics": {"bleu_score": 60.600320738082466, "chrf_score": 53.95548895548895, "xcomet_score": 0.9002888202667236, "xcomet_qe_score": 0.8835848569869995, "metricx_score": 1.1393460035324097, "metricx_qe_score": 1.9330233335494995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于单语模型的分析,我们在两组模型上进行评估,包括编码器 PDR,即使用指针解码器的多语言预训练编码器,例如 XLMR plus PDR 和 BERT plus PDR。", "metrics": {"bleu_score": 41.992852775783795, "chrf_score": 33.58726180394958, "xcomet_score": 0.7295272946357727, "xcomet_qe_score": 0.6897399425506592, "metricx_score": 4.943398475646973, "metricx_qe_score": 4.400959014892578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,例如 MBART 和 MT5。", "metrics": {"bleu_score": 30.18146852695617, "chrf_score": 22.13167484766176, "xcomet_score": 0.9135904312133789, "xcomet_qe_score": 0.963300347328186, "metricx_score": 1.3813644647598267, "metricx_qe_score": 2.8694963455200195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器在所有九个数据集上都获得了最佳性能。", "metrics": {"bleu_score": 58.398957811690124, "chrf_score": 42.590749491219285, "xcomet_score": 0.9817414283752441, "xcomet_qe_score": 0.9737299680709839, "metricx_score": 1.7566180229187012, "metricx_qe_score": 1.651177167892456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多语言设置下评估了 MT5 和 XLMR plus PDR。我们发现编", "metrics": {"bleu_score": 24.12588049712987, "chrf_score": 29.732904040053903, "xcomet_score": 0.5553979873657227, "xcomet_qe_score": 0.6430503129959106, "metricx_score": 8.261807441711426, "metricx_qe_score": 5.110557556152344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "码器-解码器或编码器-PDR可以通过在各种语言的混合中进行训练来得到改进。", "metrics": {"bleu_score": 18.96550847075289, "chrf_score": 14.257608122674819, "xcomet_score": 0.6779814958572388, "xcomet_qe_score": 0.7143617272377014, "metricx_score": 3.659130096435547, "metricx_qe_score": 4.938856601715088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,除了英语在七个数据集中的性能下降,仅在三个数据集中的性能提升。", "metrics": {"bleu_score": 62.089532791264176, "chrf_score": 55.91785692240809, "xcomet_score": 0.7551367282867432, "xcomet_qe_score": 0.7590810060501099, "metricx_score": 3.260270833969116, "metricx_qe_score": 2.7997477054595947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言诅咒。", "metrics": {"bleu_score": 18.678325117721318, "chrf_score": 18.72467251748071, "xcomet_score": 0.9018496870994568, "xcomet_qe_score": 0.8827164173126221, "metricx_score": 0.9698828458786011, "metricx_qe_score": 1.2378120422363281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝线是跨语言少样本迁移,", "metrics": {"bleu_score": 64.8138893454484, "chrf_score": 59.94312468577174, "xcomet_score": 0.8042581081390381, "xcomet_qe_score": 0.7940793037414551, "metricx_score": 2.1331841945648193, "metricx_qe_score": 3.1892428398132324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线是跨语言零样本迁移,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.8476507663726807, "xcomet_qe_score": 0.8330907821655273, "metricx_score": 1.9685924053192139, "metricx_qe_score": 3.312077522277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿线是单语设置。我们发现", "metrics": {"bleu_score": 55.93684915933074, "chrf_score": 83.9141677485262, "xcomet_score": 0.8513914942741394, "xcomet_qe_score": 0.8307276368141174, "metricx_score": 2.689924955368042, "metricx_qe_score": 1.7194303274154663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿线和橙线,我们发现对于零样本设置,跨语言迁移性能差距很大。通过比较蓝线和橙线,我们发现对于少样本设置,迁移差距迅速缩短。", "metrics": {"bleu_score": 52.227613653601914, "chrf_score": 42.208101656310276, "xcomet_score": 0.7120956778526306, "xcomet_qe_score": 0.6079044342041016, "metricx_score": 3.8672571182250977, "metricx_qe_score": 4.4823737144470215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他的有趣发现。", "metrics": {"bleu_score": 21.36435031981171, "chrf_score": 26.441146119336086, "xcomet_score": 0.8741728067398071, "xcomet_qe_score": 0.8447062969207764, "metricx_score": 0.38246479630470276, "metricx_qe_score": 0.860139787197113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器优于先前的工作或取得了可比的结果。描", "metrics": {"bleu_score": 13.083737883508867, "chrf_score": 9.912796637571617, "xcomet_score": 0.903183102607727, "xcomet_qe_score": 0.820520281791687, "metricx_score": 5.312517166137695, "metricx_qe_score": 1.9941909313201904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绘英语自然语言可以显着提高目标自然语言的少样本性能。我们发现诸如 CODIS 和 BLUE 之类的多语言语言模型仍然适用于跨语言语义解析任务。", "metrics": {"bleu_score": 42.68141080434081, "chrf_score": 34.101890482685604, "xcomet_score": 0.2866124212741852, "xcomet_qe_score": 0.31520265340805054, "metricx_score": 13.339280128479004, "metricx_qe_score": 12.934860229492188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们构建了 Exampler,一个用于跨语言语义解析的统一基准,具有多种自然语言和主要表示。", "metrics": {"bleu_score": 54.05956722720372, "chrf_score": 41.98257869795173, "xcomet_score": 0.7449675798416138, "xcomet_qe_score": 0.7288393974304199, "metricx_score": 5.220855712890625, "metricx_qe_score": 6.2805070877075195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种有代表性的多语言语言模型进行了全面的基准研究。", "metrics": {"bleu_score": 83.99447277290544, "chrf_score": 76.42490559611014, "xcomet_score": 0.9685609340667725, "xcomet_qe_score": 0.8853926658630371, "metricx_score": 1.0761440992355347, "metricx_qe_score": 1.9355964660644531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结果表明了许多有趣的发现等", "metrics": {"bleu_score": 67.39047062564734, "chrf_score": 56.942432567432554, "xcomet_score": 0.7950917482376099, "xcomet_qe_score": 0.7785392999649048, "metricx_score": 2.338362455368042, "metricx_qe_score": 2.3256003856658936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "等。", "metrics": {"bleu_score": 0.0, "chrf_score": 8.333333333333332, "xcomet_score": 0.6047984957695007, "xcomet_qe_score": 0.230974018573761, "metricx_score": 1.9000164270401, "metricx_qe_score": 3.2553701400756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫大卫·维拉,我将为大家做一个关于论文《来自翻译的Grunting Parm》的简短概述,评估其策略和性能。", "metrics": {"bleu_score": 8.284729059895641, "chrf_score": 13.663195017383046, "xcomet_score": 0.5849303007125854, "xcomet_qe_score": 0.6245571374893188, "metricx_score": 8.010514259338379, "metricx_qe_score": 8.11745834350586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与谷歌翻译的同事合作完成的。", "metrics": {"bleu_score": 37.110205129504486, "chrf_score": 29.302099776964408, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7024163007736206, "metricx_qe_score": 0.42036712169647217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Parm 是一个去年,也就是2022年发布的大型语言模型,参数量达到5400亿。", "metrics": {"bleu_score": 27.19977823495612, "chrf_score": 37.622577365861034, "xcomet_score": 0.9129167795181274, "xcomet_qe_score": 0.8833178877830505, "metricx_score": 4.777066707611084, "metricx_qe_score": 5.283947944641113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它是在一个包含7800亿个token的大型文本集合上训练的。", "metrics": {"bleu_score": 56.68706143897176, "chrf_score": 53.31771879251315, "xcomet_score": 0.8099910020828247, "xcomet_qe_score": 0.7500401735305786, "metricx_score": 3.4867777824401855, "metricx_qe_score": 3.711081027984619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发表时,它在数百个自然语言处理任务中达到了最先进水平。", "metrics": {"bleu_score": 19.61887304255142, "chrf_score": 18.27757729380964, "xcomet_score": 0.9852322340011597, "xcomet_qe_score": 0.9844403266906738, "metricx_score": 1.9279124736785889, "metricx_qe_score": 2.2642500400543213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们呈现了对机器翻译中大型语言模型提示的第一个系统性研究。", "metrics": {"bleu_score": 39.72325382489667, "chrf_score": 36.20795209096653, "xcomet_score": 0.8841332197189331, "xcomet_qe_score": 0.8953601121902466, "metricx_score": 2.3915674686431885, "metricx_qe_score": 3.023052453994751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用IMT社区的最佳实践来评估这些模型的翻译能力。", "metrics": {"bleu_score": 56.053891826721106, "chrf_score": 48.717295219245514, "xcomet_score": 0.8002027273178101, "xcomet_qe_score": 0.7922645807266235, "metricx_score": 6.278357028961182, "metricx_qe_score": 7.644520282745361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,即WMT评估的最佳系统。", "metrics": {"bleu_score": 24.46333818520026, "chrf_score": 27.773255659160196, "xcomet_score": 0.8140437602996826, "xcomet_qe_score": 0.8186229467391968, "metricx_score": 2.9967215061187744, "metricx_qe_score": 4.292076110839844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用最先进的神经机器翻译指标,并额外展示了基于专家的人工评估结果。", "metrics": {"bleu_score": 84.02532817697069, "chrf_score": 84.285768807545, "xcomet_score": 0.8987857103347778, "xcomet_qe_score": 0.8170979022979736, "metricx_score": 1.3468600511550903, "metricx_qe_score": 2.342663526535034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些提示选择策略的建议。", "metrics": {"bleu_score": 70.75330011966422, "chrf_score": 64.06828873488384, "xcomet_score": 0.8876395225524902, "xcomet_qe_score": 0.8450835347175598, "metricx_score": 1.09638249874115, "metricx_qe_score": 3.2114005088806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型在翻译中的性能有很大影响。正如我们在一个简单实验中看到的,我们使用了单次提示,并为每个句子提供了两个不同的提示。", "metrics": {"bleu_score": 51.20215461003113, "chrf_score": 46.439426724735995, "xcomet_score": 0.9119060039520264, "xcomet_qe_score": 0.904556930065155, "metricx_score": 1.6601403951644897, "metricx_qe_score": 1.472100019454956, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在1000个句子中,", "metrics": {"bleu_score": 30.895757752065407, "chrf_score": 45.3545034322207, "xcomet_score": 0.891600489616394, "xcomet_qe_score": 0.729491114616394, "metricx_score": 6.866838455200195, "metricx_qe_score": 9.45357608795166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有516个句子的差异超过一个bleu点。", "metrics": {"bleu_score": 13.834368456410946, "chrf_score": 17.745905696513255, "xcomet_score": 0.6599106192588806, "xcomet_qe_score": 0.18308180570602417, "metricx_score": 8.039372444152832, "metricx_qe_score": 7.9141387939453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这个差异甚至可以达到40个bleu点。", "metrics": {"bleu_score": 28.08337289012351, "chrf_score": 23.810747815361612, "xcomet_score": 0.8270106315612793, "xcomet_qe_score": 0.8426933884620667, "metricx_score": 4.671205520629883, "metricx_qe_score": 5.190549850463867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择好的提示策略非常重要。", "metrics": {"bleu_score": 42.05863925835808, "chrf_score": 35.66118702606926, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2160874307155609, "metricx_qe_score": 0.29690513014793396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验中,我们采用了五次提示策略,只是用语言标记我们提供给系统的每个句子。", "metrics": {"bleu_score": 51.24469802474234, "chrf_score": 45.858427080492056, "xcomet_score": 0.7756191492080688, "xcomet_qe_score": 0.7400197982788086, "metricx_score": 1.743605375289917, "metricx_qe_score": 2.0723886489868164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在这里,我们从德语翻译成英语,德语句子用德语冒号标记,英语翻译用英语冒号标记。", "metrics": {"bleu_score": 36.693391554704675, "chrf_score": 26.318670939620407, "xcomet_score": 0.9548527002334595, "xcomet_qe_score": 0.9562674760818481, "metricx_score": 1.5218651294708252, "metricx_qe_score": 1.4342151880264282, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次提示的情况下,提示的实际形式没有太大影响。", "metrics": {"bleu_score": 35.19570430462046, "chrf_score": 29.31448682958041, "xcomet_score": 0.8427718877792358, "xcomet_qe_score": 0.8509853482246399, "metricx_score": 1.0115962028503418, "metricx_qe_score": 1.4537768363952637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它对零次和一次提示至关重要,", "metrics": {"bleu_score": 11.454112651433533, "chrf_score": 12.538100021440487, "xcomet_score": 0.7013546228408813, "xcomet_qe_score": 0.7973576188087463, "metricx_score": 2.8705575466156006, "metricx_qe_score": 2.537231206893921, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而当我们像我们这样采用五次提示时,提示的实际形式几乎没有差异。", "metrics": {"bleu_score": 21.31922460941227, "chrf_score": 22.828255383806315, "xcomet_score": 0.969496488571167, "xcomet_qe_score": 0.8509374856948853, "metricx_score": 0.7535145282745361, "metricx_qe_score": 1.0330040454864502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "示例携带了大部分的权重。", "metrics": {"bleu_score": 6.8179839929677115, "chrf_score": 6.224737267582862, "xcomet_score": 0.8464819192886353, "xcomet_qe_score": 0.8308879137039185, "metricx_score": 2.991835594177246, "metricx_qe_score": 3.1904094219207764, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果的总结是,示例质量比与源句子的相似度更重要。", "metrics": {"bleu_score": 72.14673184611138, "chrf_score": 65.66627477910174, "xcomet_score": 0.991857647895813, "xcomet_qe_score": 0.9850180149078369, "metricx_score": 0.7464476823806763, "metricx_qe_score": 0.7088079452514648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择高质量翻译中的示例非常重要。", "metrics": {"bleu_score": 47.90673671051417, "chrf_score": 41.72935000787733, "xcomet_score": 0.9286196231842041, "xcomet_qe_score": 0.9292958974838257, "metricx_score": 0.4837970733642578, "metricx_qe_score": 0.908821165561676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其是,我们将WMT评估的训练数据或开发数据中的提示进行比较。", "metrics": {"bleu_score": 25.22014299857373, "chrf_score": 26.57311280220901, "xcomet_score": 0.7125208973884583, "xcomet_qe_score": 0.6235088109970093, "metricx_score": 3.5192477703094482, "metricx_qe_score": 4.37214469909668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据经过了更精心的策划,质量也更高,训练数据则比较嘈杂,使用", "metrics": {"bleu_score": 26.221486578975902, "chrf_score": 24.918607253296063, "xcomet_score": 0.7376257181167603, "xcomet_qe_score": 0.687551736831665, "metricx_score": 5.1631011962890625, "metricx_qe_score": 2.8636300563812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据获得的性能更好。", "metrics": {"bleu_score": 21.969915538001448, "chrf_score": 18.69942554552572, "xcomet_score": 0.8386656045913696, "xcomet_qe_score": 0.793500542640686, "metricx_score": 4.738469123840332, "metricx_qe_score": 5.844764232635498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,专门的、最先进的系统在Palm翻译中具有显著的优势", "metrics": {"bleu_score": 9.033765029369462, "chrf_score": 13.173182553799364, "xcomet_score": 0.7907681465148926, "xcomet_qe_score": 0.7862471342086792, "metricx_score": 5.64264440536499, "metricx_qe_score": 5.202849388122559, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但Palm已接近商业系统。", "metrics": {"bleu_score": 42.24247710146674, "chrf_score": 34.56405080605131, "xcomet_score": 0.8669350743293762, "xcomet_qe_score": 0.7543557286262512, "metricx_score": 5.332766056060791, "metricx_qe_score": 5.010430335998535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择使用谷歌翻译进行评估。", "metrics": {"bleu_score": 87.87419089273847, "chrf_score": 82.1044046215671, "xcomet_score": 0.9948046207427979, "xcomet_qe_score": 0.9676195383071899, "metricx_score": 0.45883315801620483, "metricx_qe_score": 0.6921864748001099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过使用MQM框架进行的邮件分析获得的见解是,Palm的流畅度与最先进的系统相当,但主要的差异在于准确性。", "metrics": {"bleu_score": 45.84957088783175, "chrf_score": 38.80263102716419, "xcomet_score": 0.8097265362739563, "xcomet_qe_score": 0.6599563360214233, "metricx_score": 5.788571834564209, "metricx_qe_score": 6.41286039352417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 77.60114635728617, "chrf_score": 70.94473596234124, "xcomet_score": 0.7620880603790283, "xcomet_qe_score": 0.7752113342285156, "metricx_score": 1.7286638021469116, "metricx_qe_score": 0.9165806770324707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "似乎Palm有时会选择生成听起来更好的翻译,从而省略源句子的部分内容。", "metrics": {"bleu_score": 34.215090975278656, "chrf_score": 31.76485184704819, "xcomet_score": 0.9288855791091919, "xcomet_qe_score": 0.8560929298400879, "metricx_score": 3.3546085357666016, "metricx_qe_score": 4.20802640914917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,Parm的风格不流畅类别低于最先进的系统,这是一个额外的信号,表明Parm提供了真正流畅的输出,但仍存在一些准确性问题。", "metrics": {"bleu_score": 60.49164409287891, "chrf_score": 51.76111342421657, "xcomet_score": 0.7400305271148682, "xcomet_qe_score": 0.7270216941833496, "metricx_score": 7.4334564208984375, "metricx_qe_score": 6.830860137939453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这次简短概述的全部内容。", "metrics": {"bleu_score": 80.03203203845001, "chrf_score": 74.37895437895436, "xcomet_score": 0.9922016859054565, "xcomet_qe_score": 0.9846309423446655, "metricx_score": 0.19349297881126404, "metricx_qe_score": 0.4355745315551758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如需更多细节,请参阅论文的完整演示。", "metrics": {"bleu_score": 29.944289318568238, "chrf_score": 28.258722731289183, "xcomet_score": 0.9686942100524902, "xcomet_qe_score": 0.9765617847442627, "metricx_score": 1.0845237970352173, "metricx_qe_score": 1.5124739408493042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是大卫,德国萨尔兰大学的博士生。", "metrics": {"bleu_score": 57.85925130040855, "chrf_score": 46.40088572479855, "xcomet_score": 0.8623710870742798, "xcomet_qe_score": 0.8874943256378174, "metricx_score": 0.5358048677444458, "metricx_qe_score": 0.37564146518707275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想介绍我们最近的工作,名为《弱于你所想》,对弱监督学习进行批判性分析。 这项", "metrics": {"bleu_score": 37.64572072850459, "chrf_score": 31.321360571766494, "xcomet_score": 0.7354135513305664, "xcomet_qe_score": 0.6636167168617249, "metricx_score": 7.8438801765441895, "metricx_qe_score": 3.8307044506073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是与肖玉生、马里奥·斯穆斯巴赫、吉亚·斯特芬和狄特里希·克拉科夫共同完成的。", "metrics": {"bleu_score": 1.5450766293655125, "chrf_score": 2.192982456140351, "xcomet_score": 0.44733738899230957, "xcomet_qe_score": 0.4770427346229553, "metricx_score": 3.859182834625244, "metricx_qe_score": 4.183196067810059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想先简要介绍一下弱监督和弱监督学习。", "metrics": {"bleu_score": 88.52140475440834, "chrf_score": 92.26506423113229, "xcomet_score": 0.8952239751815796, "xcomet_qe_score": 0.8487532138824463, "metricx_score": 0.8096938729286194, "metricx_qe_score": 2.310981273651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不手动标注数据。", "metrics": {"bleu_score": 36.17043615983554, "chrf_score": 29.691878261623245, "xcomet_score": 0.8968457579612732, "xcomet_qe_score": 0.8531893491744995, "metricx_score": 0.8261656165122986, "metricx_qe_score": 1.6418793201446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而是使用弱标注来源来标注数据,例如简单的启发式规则、知识库或低质量的众包,如图右侧所示。", "metrics": {"bleu_score": 60.25332415324825, "chrf_score": 54.73857217914612, "xcomet_score": 0.6283442974090576, "xcomet_qe_score": 0.620599627494812, "metricx_score": 1.985217571258545, "metricx_qe_score": 2.136190176010132, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注成本要低得多,但它们也存在噪声,这意味着一定数量的标注是不正确的。", "metrics": {"bleu_score": 28.53480230357357, "chrf_score": 25.931465189648566, "xcomet_score": 0.9147273898124695, "xcomet_qe_score": 0.8421728610992432, "metricx_score": 2.3721060752868652, "metricx_qe_score": 2.8398940563201904, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们在弱标注数据上直接训练神经网络,神经网络往往会记住标注噪声,而无法泛化。", "metrics": {"bleu_score": 31.16192692074834, "chrf_score": 27.152698277062072, "xcomet_score": 0.9676569700241089, "xcomet_qe_score": 0.9024454355239868, "metricx_score": 0.9592210054397583, "metricx_qe_score": 1.226425051689148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出了训练算法,以便在这样的标注噪声下稳健地训练神经网络,从而使训练的模型仍然能够良好地泛化。", "metrics": {"bleu_score": 55.01606630391111, "chrf_score": 46.72667381708348, "xcomet_score": 0.9706323146820068, "xcomet_qe_score": 0.8889684677124023, "metricx_score": 1.071146845817566, "metricx_qe_score": 1.5437278747558594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的WSL(WSL代表弱监督学习)工作中,一种常见的说法是,人们声称他们只在弱标注数据上训练模型,并在干净的测试集上获得高性能。", "metrics": {"bleu_score": 42.276452602836784, "chrf_score": 41.551512690179074, "xcomet_score": 0.8498544692993164, "xcomet_qe_score": 0.8706188797950745, "metricx_score": 3.801713466644287, "metricx_qe_score": 4.179085731506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并没有错,但有一个陷阱,那就是", "metrics": {"bleu_score": 15.270109824786207, "chrf_score": 17.364339015712677, "xcomet_score": 0.6032527685165405, "xcomet_qe_score": 0.3264957666397095, "metricx_score": 5.741430282592773, "metricx_qe_score": 6.980611324310303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5317288637161255, "xcomet_qe_score": 0.136541947722435, "metricx_score": 6.005777835845947, "metricx_qe_score": 11.085416793823242, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5430290699005127, "xcomet_qe_score": 0.1457119882106781, "metricx_score": 6.939623832702637, "metricx_qe_score": 7.438058853149414, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "三个研究问题。", "metrics": {"bleu_score": 27.645304662956455, "chrf_score": 37.28969519074232, "xcomet_score": 0.6767177581787109, "xcomet_qe_score": 0.5813324451446533, "metricx_score": 4.788186550140381, "metricx_qe_score": 7.17353630065918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一,我们需要干净的验证数据吗?", "metrics": {"bleu_score": 20.88349209031742, "chrf_score": 25.231083051370323, "xcomet_score": 0.9328687191009521, "xcomet_qe_score": 0.8729006052017212, "metricx_score": 4.96061372756958, "metricx_qe_score": 5.405429840087891, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5458329916000366, "xcomet_qe_score": 0.236788809299469, "metricx_score": 5.037481784820557, "metricx_qe_score": 13.922455787658691, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们是否应该只使用干净的样本进行验证,或者是否有更好的利用它们的方法?", "metrics": {"bleu_score": 50.602874117615784, "chrf_score": 44.12062493922118, "xcomet_score": 0.9778581857681274, "xcomet_qe_score": 0.9182528257369995, "metricx_score": 0.8054150938987732, "metricx_qe_score": 1.1617132425308228, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在我们的工作中解决了这些研究问题,我们的研究结果如下。", "metrics": {"bleu_score": 41.172305729118776, "chrf_score": 41.07867886842994, "xcomet_score": 0.979264497756958, "xcomet_qe_score": 0.9576575756072998, "metricx_score": 1.374922275543213, "metricx_qe_score": 2.3280932903289795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现有趣的是,最近的WSL方法实际上需要干净的验证样本才能正常工作。", "metrics": {"bleu_score": 66.74073497653401, "chrf_score": 62.75075424758513, "xcomet_score": 0.8618701696395874, "xcomet_qe_score": 0.9016522169113159, "metricx_score": 1.782778024673462, "metricx_qe_score": 2.9394373893737793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,那么训练的模型就无法泛化到原始的弱标签之外,这意味着训练毫无意义。", "metrics": {"bleu_score": 58.96983212893506, "chrf_score": 48.70000115371465, "xcomet_score": 0.916912317276001, "xcomet_qe_score": 0.8701542019844055, "metricx_score": 1.5971735715866089, "metricx_qe_score": 2.426506996154785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净标注的数据才能正常工作,获取干净验证样本的标注成本也不容忽视。", "metrics": {"bleu_score": 58.48931613595461, "chrf_score": 54.35167098556843, "xcomet_score": 0.730392336845398, "xcomet_qe_score": 0.7174431085586548, "metricx_score": 2.7380967140197754, "metricx_qe_score": 3.832845449447632, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于WSL方法实现更好的性能,如图左侧所示。", "metrics": {"bleu_score": 70.19951273802924, "chrf_score": 68.32675798677845, "xcomet_score": 0.9029473066329956, "xcomet_qe_score": 0.8991460800170898, "metricx_score": 3.9698679447174072, "metricx_qe_score": 4.705708980560303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每个类别20个样本就可以获得高性能。", "metrics": {"bleu_score": 27.355004372901615, "chrf_score": 26.829296261066403, "xcomet_score": 0.9439750909805298, "xcomet_qe_score": 0.9614940881729126, "metricx_score": 1.8474929332733154, "metricx_qe_score": 1.9346143007278442, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并非故事的全部,因为如果无论如何我们都决定访问干净的样本,那么直接在这些样本上进行训练甚至可以实现更好的性能。 红色", "metrics": {"bleu_score": 28.834124414205288, "chrf_score": 25.911341623613517, "xcomet_score": 0.7179252505302429, "xcomet_qe_score": 0.7079408168792725, "metricx_score": 5.628363609313965, "metricx_qe_score": 5.227504730224609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的图表显示了直接应用于干净数据的方法(微调方法)和仅将干净数据用于验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 56.261305338038404, "chrf_score": 54.75259742688827, "xcomet_score": 0.6683850884437561, "xcomet_qe_score": 0.7014796733856201, "metricx_score": 4.775552749633789, "metricx_qe_score": 5.075097560882568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,如果我们有每个类别10个样本,直接微调就开始超越WSL方法。", "metrics": {"bleu_score": 38.836305697359606, "chrf_score": 37.91776260465858, "xcomet_score": 0.9205647706985474, "xcomet_qe_score": 0.8704403042793274, "metricx_score": 1.750215768814087, "metricx_qe_score": 2.0638201236724854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前WSL方法声称的性能提升可以通过允许在干净的验证样本上继续微调来实现。正如从图中可以看出,最初表现不", "metrics": {"bleu_score": 27.188043598063288, "chrf_score": 31.17751348832624, "xcomet_score": 0.6489850282669067, "xcomet_qe_score": 0.5426487922668457, "metricx_score": 7.859041690826416, "metricx_qe_score": 5.28545618057251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如更复杂的WSL方法(如余弦相似度)的Van Linden模型(最初称为W),如果在干净的样本", "metrics": {"bleu_score": 20.046898651298058, "chrf_score": 18.592829720834683, "xcomet_score": 0.21909406781196594, "xcomet_qe_score": 0.14123795926570892, "metricx_score": 18.649877548217773, "metricx_qe_score": 17.257122039794922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上允许继续微调,那么FTW的表现就会与其他方法一样好。", "metrics": {"bleu_score": 43.624597883983576, "chrf_score": 42.228104252187656, "xcomet_score": 0.7117152810096741, "xcomet_qe_score": 0.7247582674026489, "metricx_score": 4.875826835632324, "metricx_qe_score": 6.066754341125488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的WSL方法,因为它们需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 57.473860166742696, "chrf_score": 55.57193860381212, "xcomet_score": 0.9809115529060364, "xcomet_qe_score": 0.9843068718910217, "metricx_score": 0.696503221988678, "metricx_qe_score": 1.4214279651641846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们表明,最近的WSL方法需要干净、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 60.316120362180044, "chrf_score": 57.11316905018007, "xcomet_score": 0.8203024864196777, "xcomet_qe_score": 0.9073742032051086, "metricx_score": 2.555450916290283, "metricx_qe_score": 3.615776777267456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否在干净的验证样本上进行。", "metrics": {"bleu_score": 43.42906676853412, "chrf_score": 36.34004037322114, "xcomet_score": 0.9702857732772827, "xcomet_qe_score": 0.9098553657531738, "metricx_score": 1.5602195262908936, "metricx_qe_score": 2.490739107131958, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL方法应与少量样本学习基线进行比较,这些基线使用干净的样本。", "metrics": {"bleu_score": 38.282449801365914, "chrf_score": 36.727215253736425, "xcomet_score": 0.8508284091949463, "xcomet_qe_score": 0.7828410863876343, "metricx_score": 3.3699560165405273, "metricx_qe_score": 4.0585408210754395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一种简单但强大的基线,应该在未来的WSL工作中考虑。", "metrics": {"bleu_score": 34.59016034075965, "chrf_score": 30.19696449626724, "xcomet_score": 0.8624430894851685, "xcomet_qe_score": 0.757651686668396, "metricx_score": 2.388312339782715, "metricx_qe_score": 2.814298391342163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们已经开源了我们的代码。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9967988729476929, "xcomet_qe_score": 0.9351927638053894, "metricx_score": 0.4914062023162842, "metricx_qe_score": 0.7212974429130554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过此幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9951430559158325, "xcomet_qe_score": 0.9870158433914185, "metricx_score": 0.48675400018692017, "metricx_qe_score": 0.44404107332229614, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝您会议愉快。", "metrics": {"bleu_score": 7.817610446892725, "chrf_score": 8.18252221407027, "xcomet_score": 0.9670588374137878, "xcomet_qe_score": 0.9966236352920532, "metricx_score": 0.5200188159942627, "metricx_qe_score": 0.28290775418281555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 9.993248618647176, "xcomet_score": 0.8700615763664246, "xcomet_qe_score": 0.6193946599960327, "metricx_score": 1.1073329448699951, "metricx_qe_score": 0.9096816182136536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "芬奇,我是莎拉·芬奇。", "metrics": {"bleu_score": 8.29519350710986, "chrf_score": 5.405070919696089, "xcomet_score": 0.5337545871734619, "xcomet_qe_score": 0.6937657594680786, "metricx_score": 4.6942596435546875, "metricx_qe_score": 5.437790870666504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCeval,这是一种评估对话式人工智能的新型维度方法。", "metrics": {"bleu_score": 30.950920105373314, "chrf_score": 29.087292711953044, "xcomet_score": 0.8620446920394897, "xcomet_qe_score": 0.9333170056343079, "metricx_score": 1.5018858909606934, "metricx_qe_score": 1.7730636596679688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃莫利自然语言处理实验室完成,由埃莫利大学的吉诺·崔教授领导,并与亚马逊 Alexa AI 合作。", "metrics": {"bleu_score": 16.293125439919418, "chrf_score": 22.767889649209863, "xcomet_score": 0.6642328500747681, "xcomet_qe_score": 0.79695725440979, "metricx_score": 2.3963539600372314, "metricx_qe_score": 2.5011110305786133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚刚开发了一个对话模型,并且希望了解它的表现与当前最先进水平相比如何。", "metrics": {"bleu_score": 48.37559849963351, "chrf_score": 43.80916431708821, "xcomet_score": 0.9887051582336426, "xcomet_qe_score": 0.9846044778823853, "metricx_score": 0.43937018513679504, "metricx_qe_score": 0.42085617780685425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是进行人工评估,例如让人工评估员选择两个对话中哪个更好,或者使用李克特量表对对话进行评分。", "metrics": {"bleu_score": 62.97413153853997, "chrf_score": 55.74085211454327, "xcomet_score": 0.9062615036964417, "xcomet_qe_score": 0.9263993501663208, "metricx_score": 0.6830624341964722, "metricx_qe_score": 0.9643327593803406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法能够很好地提供对整体对话质量的整体评估,但对话质量具有许多方面。", "metrics": {"bleu_score": 56.62210540402316, "chrf_score": 49.14424583342384, "xcomet_score": 0.9724355936050415, "xcomet_qe_score": 0.9491795301437378, "metricx_score": 0.5558902025222778, "metricx_qe_score": 0.7136740684509277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能需要评估聊天质量的多个维度,以在更精细的层面上了解模型的优势和劣势。", "metrics": {"bleu_score": 50.05598424826363, "chrf_score": 50.6181383169219, "xcomet_score": 0.977371335029602, "xcomet_qe_score": 0.919529378414154, "metricx_score": 0.7368069887161255, "metricx_qe_score": 0.887752890586853, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人工评估员评估对话质量的多个维度,例如使用现有对比或李克特量表方法评估模型响应的相关性。", "metrics": {"bleu_score": 58.368971230787295, "chrf_score": 49.25837068210896, "xcomet_score": 0.823872447013855, "xcomet_qe_score": 0.8621450662612915, "metricx_score": 1.1318166255950928, "metricx_qe_score": 1.659722089767456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为对于维度对话评估来说,存在一种更精确、更可靠的策略。", "metrics": {"bleu_score": 32.780573471102954, "chrf_score": 34.76406492319721, "xcomet_score": 0.899796724319458, "xcomet_qe_score": 0.8729124069213867, "metricx_score": 1.950533390045166, "metricx_qe_score": 1.7504875659942627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确注释每个模型响应是否表达某些行为(例如,使用不相关的信息做出回应或自相矛盾)来减少人工评估的主观性。", "metrics": {"bleu_score": 75.4120685346746, "chrf_score": 74.54053719054899, "xcomet_score": 0.9103350043296814, "xcomet_qe_score": 0.8328845500946045, "metricx_score": 1.2705411911010742, "metricx_qe_score": 1.7035396099090576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为“在聊天中注释行为”,简称 ABC eval。", "metrics": {"bleu_score": 42.66219662386316, "chrf_score": 43.239862823758706, "xcomet_score": 0.8280558586120605, "xcomet_qe_score": 0.8393820524215698, "metricx_score": 2.271851062774658, "metricx_qe_score": 4.107532024383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 74.32391249823593, "chrf_score": 67.26850636751183, "xcomet_score": 0.9309866428375244, "xcomet_qe_score": 0.9535799026489258, "metricx_score": 1.3550490140914917, "metricx_qe_score": 3.016953468322754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABCeval 评估聊天模型是否", "metrics": {"bleu_score": 7.867704761908495, "chrf_score": 14.937678842870126, "xcomet_score": 0.34113889932632446, "xcomet_qe_score": 0.23366425931453705, "metricx_score": 11.764352798461914, "metricx_qe_score": 6.648166656494141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "忽略其伙伴或说一些不相关的话,是否自相矛盾或与其伙伴相矛盾,是否产生不正确的陈述或违反常识知识,以及模型是否成功或未能表现出同理心。", "metrics": {"bleu_score": 30.90342240156998, "chrf_score": 24.766289303889344, "xcomet_score": 0.4110199511051178, "xcomet_qe_score": 0.26132115721702576, "metricx_score": 8.389583587646484, "metricx_qe_score": 8.905960083007812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种类型的评估最有效,我们选择了四个最先进的聊天模型,并使用 ABC eval 对每个模型进行了 100 次人机对话的评估。", "metrics": {"bleu_score": 52.18840513012274, "chrf_score": 50.635741320802566, "xcomet_score": 0.9518200159072876, "xcomet_qe_score": 0.9457032084465027, "metricx_score": 1.7062807083129883, "metricx_qe_score": 2.229170799255371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了比较,我们还使用三种现有方法对这些对话进行了评估:在回合级别上的李克特评分,在对话级别上的李克特评分,以及对话级别上的成对比较。", "metrics": {"bleu_score": 45.57875072193207, "chrf_score": 41.37473919338298, "xcomet_score": 0.8908108472824097, "xcomet_qe_score": 0.8356292843818665, "metricx_score": 3.425698757171631, "metricx_qe_score": 4.259023189544678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每一种现有方法,我们收集了对对话中八个最常被测量的方面进行的评估,因为这是评估聊天模型沿多个维度进行评估的标准做法。", "metrics": {"bleu_score": 47.59545308660815, "chrf_score": 42.81508245705791, "xcomet_score": 0.8531322479248047, "xcomet_qe_score": 0.8041841387748718, "metricx_score": 3.6362476348876953, "metricx_qe_score": 4.31149959564209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过分析这些评估结果,我们发现 ABC eval 行为标签总体上比现有方法收集的标签更可靠,这由 100 个双重标签对话上的评估员间一致性衡量。", "metrics": {"bleu_score": 37.36936372253018, "chrf_score": 36.710966720942416, "xcomet_score": 0.7739719152450562, "xcomet_qe_score": 0.791485071182251, "metricx_score": 5.737581729888916, "metricx_qe_score": 6.3638176918029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,ABC eval 标签比现有方法产生的指标更能预测整体对话质量,这由简单的线性回归分析所示。", "metrics": {"bleu_score": 52.71788895679632, "chrf_score": 47.537397359719634, "xcomet_score": 0.9466356635093689, "xcomet_qe_score": 0.9230263829231262, "metricx_score": 2.5599801540374756, "metricx_qe_score": 3.48282527923584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,测量包含自我和伙伴矛盾的回合比例分别可以解释对话质量的 5% 和 10%,而平均李克特一致性分数仅解释 4% 或更少。", "metrics": {"bleu_score": 47.76474901991221, "chrf_score": 41.877686655274005, "xcomet_score": 0.743010401725769, "xcomet_qe_score": 0.7153595685958862, "metricx_score": 5.527943134307861, "metricx_qe_score": 5.896337985992432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查每个评估指标是否捕捉了聊天质量的独特方面。您", "metrics": {"bleu_score": 73.05868314781945, "chrf_score": 66.00347554391672, "xcomet_score": 0.8077518939971924, "xcomet_qe_score": 0.8016377687454224, "metricx_score": 4.667149066925049, "metricx_qe_score": 2.0313363075256348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,所有 ABC eval 指标的组合解释了对话质量的 25% 以上,并且在一次删除一个指标时,大多数指标都会导致损失大量的关于质量的信息。", "metrics": {"bleu_score": 35.04573267393479, "chrf_score": 37.06731116116931, "xcomet_score": 0.7726133465766907, "xcomet_qe_score": 0.6317881941795349, "metricx_score": 3.1275439262390137, "metricx_qe_score": 4.234653949737549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有回合级别李克特指标的组合解释的质量要少得多,而且更少的指标携带独特的有用信息。", "metrics": {"bleu_score": 33.925828496992224, "chrf_score": 29.616000331432886, "xcomet_score": 0.6715670824050903, "xcomet_qe_score": 0.6058735251426697, "metricx_score": 4.928900718688965, "metricx_qe_score": 5.187353610992432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些可靠、信息丰富且独特的 ABC eval 指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。您可以看到,在", "metrics": {"bleu_score": 5.770467998237226, "chrf_score": 13.011461024193869, "xcomet_score": 0.27044355869293213, "xcomet_qe_score": 0.4003453850746155, "metricx_score": 9.013554573059082, "metricx_qe_score": 5.0235724449157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果中,仍然存在一些挑战,并且已经被精确地量化。", "metrics": {"bleu_score": 44.523440771877034, "chrf_score": 39.056920547722115, "xcomet_score": 0.9827414751052856, "xcomet_qe_score": 0.9743714332580566, "metricx_score": 1.610642671585083, "metricx_qe_score": 2.2595551013946533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人响应中约有 20% 出现了常识违", "metrics": {"bleu_score": 37.35483638296572, "chrf_score": 33.933220763895946, "xcomet_score": 0.7899798154830933, "xcomet_qe_score": 0.7361011505126953, "metricx_score": 4.610044956207275, "metricx_qe_score": 3.5957441329956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反现象,约有 15% 的响应产生了不相关的信息,并且约有 10% 的时间会自相矛盾或与其伙伴相矛盾。", "metrics": {"bleu_score": 30.546222730020688, "chrf_score": 28.11820941575383, "xcomet_score": 0.3711467385292053, "xcomet_qe_score": 0.3174923360347748, "metricx_score": 5.395659446716309, "metricx_qe_score": 5.428297519683838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域快速进步,许多这些错误率可能会在自我们进行评估以来发布的新模型中降低。", "metrics": {"bleu_score": 45.149170564915906, "chrf_score": 37.68138366822578, "xcomet_score": 0.9807720184326172, "xcomet_qe_score": 0.974662184715271, "metricx_score": 2.537517547607422, "metricx_qe_score": 2.097590684890747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更应该成为追求可靠和精确的评估指标以比较模型的理由。", "metrics": {"bleu_score": 35.89594553439772, "chrf_score": 31.11369552981258, "xcomet_score": 0.9962855577468872, "xcomet_qe_score": 0.9864671230316162, "metricx_score": 1.6004364490509033, "metricx_qe_score": 1.62068772315979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABC Eval 可以被该领域的其他人利用,作为朝着这个方向迈出的有意义的一步,并且", "metrics": {"bleu_score": 59.04929315722789, "chrf_score": 55.03222631342314, "xcomet_score": 0.8125261068344116, "xcomet_qe_score": 0.8188593983650208, "metricx_score": 4.606261253356934, "metricx_qe_score": 1.9848142862319946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待着在未来几个月和几年中看到对话式人工智能的进步。", "metrics": {"bleu_score": 50.67309892897293, "chrf_score": 46.97067926910627, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8209799528121948, "metricx_qe_score": 0.8959735631942749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢观看。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9849855899810791, "xcomet_qe_score": 0.9607588648796082, "metricx_score": 0.2659546732902527, "metricx_qe_score": 0.5833151340484619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好!我叫 Kaio Yin,我将为大家介绍我们的工作,题目是《翻译何时需要语境?", "metrics": {"bleu_score": 33.32542413041973, "chrf_score": 32.8243700241776, "xcomet_score": 0.7512986063957214, "xcomet_qe_score": 0.817764163017273, "metricx_score": 1.4967865943908691, "metricx_qe_score": 1.4833544492721558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "——一种基于数据的多语言探索》。", "metrics": {"bleu_score": 48.415247130345996, "chrf_score": 56.100298626145516, "xcomet_score": 0.8767226934432983, "xcomet_qe_score": 0.8706227540969849, "metricx_score": 1.435295820236206, "metricx_qe_score": 2.0757932662963867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与 Patrick Fernandes、Emmy Liu、Andre F.D. Martins 和 Graham Newbig 合作完成的。所以", "metrics": {"bleu_score": 46.95966835778608, "chrf_score": 76.0482024177377, "xcomet_score": 0.6172990798950195, "xcomet_qe_score": 0.5313817858695984, "metricx_score": 5.7106194496154785, "metricx_qe_score": 2.7824604511260986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",很多翻译都依赖于语境。", "metrics": {"bleu_score": 23.462350320527996, "chrf_score": 19.48658005142405, "xcomet_score": 0.9890776872634888, "xcomet_qe_score": 0.9695669412612915, "metricx_score": 1.1107302904129028, "metricx_qe_score": 1.0108368396759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们应该如何翻译这个句子中的 “mole”?", "metrics": {"bleu_score": 39.78842755316247, "chrf_score": 46.08013967464906, "xcomet_score": 0.9968053102493286, "xcomet_qe_score": 0.9850974082946777, "metricx_score": 0.992307186126709, "metricx_qe_score": 2.14617657661438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嗯,如果前一句是:“如果部长们发现,情况可能会变得危险”,那么 “mole” 指的是间谍。", "metrics": {"bleu_score": 18.986011309785297, "chrf_score": 13.303722542899626, "xcomet_score": 0.9743224382400513, "xcomet_qe_score": 0.9704815149307251, "metricx_score": 2.9388813972473145, "metricx_qe_score": 4.184332370758057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是:“医生,严重吗?”那么 “mole” 指的是胎记。", "metrics": {"bleu_score": 17.88460546912721, "chrf_score": 17.05176874549205, "xcomet_score": 0.9462487697601318, "xcomet_qe_score": 0.9570028185844421, "metricx_score": 1.6968121528625488, "metricx_qe_score": 2.785062074661255, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据语境,词语的含义会发生变化,其翻译也随之改变。", "metrics": {"bleu_score": 26.574454482374573, "chrf_score": 22.629811483499132, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2867918014526367, "metricx_qe_score": 0.21755903959274292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在处理这类情况时的表现非常困难。", "metrics": {"bleu_score": 29.10708729782026, "chrf_score": 24.007307958698608, "xcomet_score": 0.9452650547027588, "xcomet_qe_score": 0.9408419728279114, "metricx_score": 1.1816174983978271, "metricx_qe_score": 0.9819419384002686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,只有一小部分翻译依赖于语境,这使得基于语料库的指标,如 BLEU,无法捕捉到这些翻译。一些", "metrics": {"bleu_score": 36.88729113396148, "chrf_score": 34.17279444934364, "xcomet_score": 0.9158965349197388, "xcomet_qe_score": 0.9045841097831726, "metricx_score": 4.211054801940918, "metricx_qe_score": 2.408071279525757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "人建议针对语境相关的翻译进行有针对性的评估,但这些资源仅支持有限类型的语境相关翻译和有限的语言集,因为它们通常依赖于领域知识和人工标注。", "metrics": {"bleu_score": 60.234655385576595, "chrf_score": 53.90691503150684, "xcomet_score": 0.6115013360977173, "xcomet_qe_score": 0.6992781162261963, "metricx_score": 4.452102184295654, "metricx_qe_score": 3.4161276817321777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要语境?", "metrics": {"bleu_score": 8.736015370428479, "chrf_score": 11.196726662314651, "xcomet_score": 0.8924298286437988, "xcomet_qe_score": 0.8972160816192627, "metricx_score": 0.3988206386566162, "metricx_qe_score": 0.22178193926811218, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型如何处理这些情况?", "metrics": {"bleu_score": 32.74135267450808, "chrf_score": 30.12273108277076, "xcomet_score": 0.9986907243728638, "xcomet_qe_score": 0.9914894104003906, "metricx_score": 0.7201682925224304, "metricx_qe_score": 1.0261601209640503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了单词在翻译过程中对语境的依赖程度。", "metrics": {"bleu_score": 70.47708740701131, "chrf_score": 61.52906032329073, "xcomet_score": 0.9981553554534912, "xcomet_qe_score": 1.0, "metricx_score": 4.243528366088867, "metricx_qe_score": 4.448748588562012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的工作中,我们引入了 CXMI 作为衡量机器翻译模型语境使用的指标。", "metrics": {"bleu_score": 44.155892497893085, "chrf_score": 43.04891712299604, "xcomet_score": 0.9097746014595032, "xcomet_qe_score": 0.9209733605384827, "metricx_score": 1.2498706579208374, "metricx_qe_score": 1.4085427522659302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量语境 C 对目标 Y 提供的信息量,给定源 X 来完成。您可以将 CXMI 视为向模型提供语境所获得的信息量。", "metrics": {"bleu_score": 27.446986720600414, "chrf_score": 28.111997235865104, "xcomet_score": 0.6413281559944153, "xcomet_qe_score": 0.6170154809951782, "metricx_score": 3.8891968727111816, "metricx_qe_score": 4.052132606506348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将 CXMI 扩展到 pointwise CXMI,它可以测量句子级别或单词级别的语境使用情况。", "metrics": {"bleu_score": 28.529439097475667, "chrf_score": 41.39876592377307, "xcomet_score": 0.9400720596313477, "xcomet_qe_score": 0.8873176574707031, "metricx_score": 3.177217960357666, "metricx_qe_score": 5.066031455993652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以认为具有高 P6MI 的单词需要语境进行翻译。", "metrics": {"bleu_score": 24.014973510773416, "chrf_score": 19.407630355194595, "xcomet_score": 0.7556304931640625, "xcomet_qe_score": 0.6529687643051147, "metricx_score": 6.636765480041504, "metricx_qe_score": 6.236658096313477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析具有高 P6MI 的单词,以寻找这些单词之间的模式。", "metrics": {"bleu_score": 37.13290491373275, "chrf_score": 34.17679881611884, "xcomet_score": 0.8416121006011963, "xcomet_qe_score": 0.7575443983078003, "metricx_score": 5.965099811553955, "metricx_qe_score": 6.706972599029541, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语到 14 种不同语言翻译的 TED Talks 字幕进行分析。我们分", "metrics": {"bleu_score": 41.21183751323024, "chrf_score": 37.71801238875436, "xcomet_score": 0.4934956431388855, "xcomet_qe_score": 0.7144829034805298, "metricx_score": 6.701256275177002, "metricx_qe_score": 2.956237316131592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别在三个不同的层面上进行分析。", "metrics": {"bleu_score": 86.57099140685621, "chrf_score": 86.23786658208323, "xcomet_score": 0.7637689113616943, "xcomet_qe_score": 0.657113790512085, "metricx_score": 4.408324241638184, "metricx_qe_score": 3.1129140853881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有较高平均 PCXMI 的词性标签。", "metrics": {"bleu_score": 32.788651923723414, "chrf_score": 31.429376306378593, "xcomet_score": 0.8631772398948669, "xcomet_qe_score": 0.8003582954406738, "metricx_score": 2.257020950317383, "metricx_qe_score": 3.3994946479797363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够发现,例如,阿拉伯语中的双重代词具有相对较高的 PCXMI。", "metrics": {"bleu_score": 42.5011255378281, "chrf_score": 36.856576418937046, "xcomet_score": 0.8376480340957642, "xcomet_qe_score": 0.9435798525810242, "metricx_score": 3.4586193561553955, "metricx_qe_score": 4.514315605163574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语没有双重代词,因此在翻译成阿拉伯语时,需要语境来确定代词是否为双重形式。", "metrics": {"bleu_score": 55.856741602297525, "chrf_score": 48.950371073711, "xcomet_score": 0.770807683467865, "xcomet_qe_score": 0.9880837202072144, "metricx_score": 2.0078234672546387, "metricx_qe_score": 1.9961249828338623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现某些语言在选择适当的动词形式时也需要语境。", "metrics": {"bleu_score": 77.9232874291321, "chrf_score": 76.37036851230356, "xcomet_score": 0.9887592792510986, "xcomet_qe_score": 0.9788588285446167, "metricx_score": 0.5951969623565674, "metricx_qe_score": 0.6145331263542175, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们查看在所有不同出现情况下对 PCSXMI 求平均的词汇项目。", "metrics": {"bleu_score": 26.33201939239633, "chrf_score": 23.178021563778696, "xcomet_score": 0.7525551319122314, "xcomet_qe_score": 0.6920228004455566, "metricx_score": 7.265386581420898, "metricx_qe_score": 7.460742473602295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别此处所示的案例,即在中文中,您需要语境来翻译专有名词,以确保在文档中使用相同的翻译。", "metrics": {"bleu_score": 31.473386806380752, "chrf_score": 27.01690606660382, "xcomet_score": 0.81977379322052, "xcomet_qe_score": 0.8017885088920593, "metricx_score": 1.715909481048584, "metricx_qe_score": 1.5311462879180908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现语境可以用于正确地翻译正式程度。", "metrics": {"bleu_score": 18.931747781986427, "chrf_score": 17.93760587238848, "xcomet_score": 0.8415668606758118, "xcomet_qe_score": 0.8434059619903564, "metricx_score": 1.5035003423690796, "metricx_qe_score": 1.0484422445297241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看具有高 PCXMI 的不同单个标记。", "metrics": {"bleu_score": 25.543184713657478, "chrf_score": 24.851678048731767, "xcomet_score": 0.7529343366622925, "xcomet_qe_score": 0.7447515726089478, "metricx_score": 3.087174415588379, "metricx_qe_score": 3.782194137573242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别无法真正通过单词本身捕捉到的现象,而是体现在句子结构中,例如省略现象。", "metrics": {"bleu_score": 31.161681398551274, "chrf_score": 27.261573005641015, "xcomet_score": 0.7971282601356506, "xcomet_qe_score": 0.7881986498832703, "metricx_score": 1.0425163507461548, "metricx_qe_score": 1.1291650533676147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们利用分析结果来设计一个文档级别的翻译基准。", "metrics": {"bleu_score": 39.19877486720283, "chrf_score": 34.309190789999036, "xcomet_score": 0.9947332143783569, "xcomet_qe_score": 0.9869742393493652, "metricx_score": 0.8336606025695801, "metricx_qe_score": 0.9872821569442749, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别的五种话语现象,我们创建了标记器来自动识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。", "metrics": {"bleu_score": 38.84657394848485, "chrf_score": 43.05176296002144, "xcomet_score": 0.9075697660446167, "xcomet_qe_score": 0.9092678427696228, "metricx_score": 1.2622936964035034, "metricx_qe_score": 1.3156452178955078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。", "metrics": {"bleu_score": 8.804900733701496, "chrf_score": 18.599056987918182, "xcomet_score": 0.5301579236984253, "xcomet_qe_score": 0.47966089844703674, "metricx_score": 1.605013370513916, "metricx_qe_score": 1.80898118019104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA", "metrics": {"bleu_score": 0.9088669847678542, "chrf_score": 3.2582776924270926, "xcomet_score": 0.13395705819129944, "xcomet_qe_score": 0.13616132736206055, "metricx_score": 25.0, "metricx_qe_score": 24.506837844848633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "标记器。然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。然后,我们可以使用标记器来识别与该现象相关的单词,我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger),简称 MUDA 标记器。不同语言有不同比例的这些语篇现象。然后我们使用 MUDA 标记器,将标记器应用于我们想要用于评估的平行语料库。然后,我们对 Muda 标记器识别的语境相关示例应用我们选择的翻译指标。", "metrics": {"bleu_score": 12.324476092657306, "chrf_score": 25.85001307250476, "xcomet_score": -0.011725080199539661, "xcomet_qe_score": -0.010963121429085732, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同的模型在文档级别机器翻译方面的表现。", "metrics": {"bleu_score": 56.67726307006269, "chrf_score": 58.51339156720511, "xcomet_score": 0.9089334607124329, "xcomet_qe_score": 0.8649705052375793, "metricx_score": 0.9755918979644775, "metricx_qe_score": 1.1336381435394287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用基于语料库的指标时,例如 BLEU,我们发现语境无关的模型具有最佳性能", "metrics": {"bleu_score": 28.485035166652388, "chrf_score": 25.496005563125802, "xcomet_score": 0.8735853433609009, "xcomet_qe_score": 0.7843254804611206, "metricx_score": 2.301224946975708, "metricx_qe_score": 1.834729552268982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但如果使用 COMET,则语境感知模型表现最佳。", "metrics": {"bleu_score": 20.39784800681338, "chrf_score": 31.054697742408177, "xcomet_score": 0.9774847030639648, "xcomet_qe_score": 0.9737459421157837, "metricx_score": 2.349191188812256, "metricx_qe_score": 2.9175758361816406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用词级别 f 衡量,则具有或不具有语境的模型表现出可比的性能。", "metrics": {"bleu_score": 17.042178650486203, "chrf_score": 14.885466309247155, "xcomet_score": 0.7812223434448242, "xcomet_qe_score": 0.7270861268043518, "metricx_score": 3.74015736579895, "metricx_qe_score": 3.4005465507507324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果我们仅使用基于语料库的指标,很难确定最佳的文档级别翻译系统。", "metrics": {"bleu_score": 61.2663741511925, "chrf_score": 53.44201705540368, "xcomet_score": 0.9933726787567139, "xcomet_qe_score": 0.9891425371170044, "metricx_score": 0.8744645714759827, "metricx_qe_score": 1.0331215858459473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用 Muda 基准来评估模型,我们发现对于某些话语现象(例如正式程度和词汇连贯性),语境感知模型比不使用语境的模型更准确。但对于", "metrics": {"bleu_score": 51.13105523585309, "chrf_score": 45.63889517622782, "xcomet_score": 0.4076736271381378, "xcomet_qe_score": 0.2648535966873169, "metricx_score": 7.855774879455566, "metricx_qe_score": 5.06762170791626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他现象(如省略、代词和动词形式)而言,这些模型与未使用语境的模型相比并没有好多", "metrics": {"bleu_score": 33.57268261287964, "chrf_score": 28.18658870930349, "xcomet_score": 0.5488108396530151, "xcomet_qe_score": 0.5520020723342896, "metricx_score": 2.959069013595581, "metricx_qe_score": 2.8777120113372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "少。这表明我们需要在文档级别翻译方面看到更大的进步。", "metrics": {"bleu_score": 17.578252983442347, "chrf_score": 17.4372646476426, "xcomet_score": 0.5398522615432739, "xcomet_qe_score": 0.4833715260028839, "metricx_score": 5.027144908905029, "metricx_qe_score": 5.040493965148926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准表明,DeepL 通常比 Google Translate 更准确的文档级别翻译。总结一", "metrics": {"bleu_score": 54.32782949701722, "chrf_score": 48.8313888321083, "xcomet_score": 0.6479419469833374, "xcomet_qe_score": 0.6465758085250854, "metricx_score": 5.890317916870117, "metricx_qe_score": 5.207871437072754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "下,我们对 14 种语言对进行了基于数据的分析,以确定翻译何时需要语境。然后,我们利用我们的发现来构建一个文档级别机器翻译的基准,这可以帮助我们识别哪些话语现象模型能够很好地处理,或者无法处理,以及哪些翻译系统擅长文档级别的翻译。", "metrics": {"bleu_score": 50.94314685667653, "chrf_score": 47.40602493292954, "xcomet_score": 0.616888165473938, "xcomet_qe_score": 0.5667458772659302, "metricx_score": 5.064713478088379, "metricx_qe_score": 5.808889389038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9842581748962402, "xcomet_qe_score": 0.9662535190582275, "metricx_score": 0.4675371050834656, "metricx_qe_score": 0.26872068643569946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多伦多再见!", "metrics": {"bleu_score": 61.04735835807847, "chrf_score": 59.34146140007401, "xcomet_score": 0.9926973581314087, "xcomet_qe_score": 0.9597586989402771, "metricx_score": 0.4811379015445709, "metricx_qe_score": 1.0739483833312988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是Yanis Lavrac,我将向大家介绍我们关于Dr. BERT的工作,这是一个为生物医学和临床领域设计的、在法语中进行过预训练的强大模型。", "metrics": {"bleu_score": 33.28313134297934, "chrf_score": 40.823457129027844, "xcomet_score": 0.6020917892456055, "xcomet_qe_score": 0.6626123189926147, "metricx_score": 3.7603280544281006, "metricx_qe_score": 2.420142412185669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演示中,我们将首先讨论医疗保健领域的语言建模。", "metrics": {"bleu_score": 57.89846131658321, "chrf_score": 53.10134351213445, "xcomet_score": 0.9948086738586426, "xcomet_qe_score": 1.0, "metricx_score": 1.6965010166168213, "metricx_qe_score": 1.855126976966858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 83.05389167974835, "chrf_score": 81.33105592664415, "xcomet_score": 0.9893636703491211, "xcomet_qe_score": 0.9912564754486084, "metricx_score": 0.384817510843277, "metricx_qe_score": 0.6998237371444702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了第一个基于Roberta、并基于NACHOS(一个从网络抓取的医学数据集合)进行训练的法语生物医学模型,命名为Dr. BERT。", "metrics": {"bleu_score": 33.6963960977271, "chrf_score": 31.720453444695345, "xcomet_score": 0.8235907554626465, "xcomet_qe_score": 0.7573436498641968, "metricx_score": 2.2664554119110107, "metricx_qe_score": 2.4653377532958984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了在不同预训练设置和数据来源下的模型比较。", "metrics": {"bleu_score": 62.56538561604213, "chrf_score": 54.32740663358808, "xcomet_score": 0.9954410791397095, "xcomet_qe_score": 0.985733151435852, "metricx_score": 0.544791579246521, "metricx_qe_score": 0.7418580651283264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将展示我们在法语中11个生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 50.33586648155096, "chrf_score": 45.301503260028284, "xcomet_score": 0.7765494585037231, "xcomet_qe_score": 0.7151468992233276, "metricx_score": 2.542285442352295, "metricx_qe_score": 4.245993614196777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们将总结实验结果,并提供更多关于如何访问这些模型的信息。", "metrics": {"bleu_score": 34.418275025158906, "chrf_score": 32.108168635330856, "xcomet_score": 0.9116168022155762, "xcomet_qe_score": 0.9691550731658936, "metricx_score": 0.34431004524230957, "metricx_qe_score": 0.258706659078598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务的最有效方法之一,相比于历史上的静态和情境化方法,如Word2Vec、fastText或NWO,带来了巨大的性能提升。", "metrics": {"bleu_score": 52.863689819607714, "chrf_score": 54.53735237589014, "xcomet_score": 0.8063468933105469, "xcomet_qe_score": 0.8001086115837097, "metricx_score": 3.9221854209899902, "metricx_qe_score": 3.7558202743530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此后,该模型已被适配到许多其他语言,例如在法语中是Camembert,以及在生物医学领域是Permut-BERT和Bio-BERT,以及在临床领域是Clinical-BERT,但主要都是在英语中。", "metrics": {"bleu_score": 27.669888368434787, "chrf_score": 35.1123276349112, "xcomet_score": 0.5569937825202942, "xcomet_qe_score": 0.5621868371963501, "metricx_score": 4.654922008514404, "metricx_qe_score": 3.9952659606933594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的专用模型非常稀少,并且通常基于持续预训练,因为缺乏特定领域的数据。然而,在先", "metrics": {"bleu_score": 22.23219905794179, "chrf_score": 23.138831964884872, "xcomet_score": 0.727460503578186, "xcomet_qe_score": 0.706051230430603, "metricx_score": 6.070201873779297, "metricx_qe_score": 5.760035991668701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前,法语并没有任何用于生物医学的开源模型。", "metrics": {"bleu_score": 37.89814977682468, "chrf_score": 31.006546464951406, "xcomet_score": 0.7576602101325989, "xcomet_qe_score": 0.6180649995803833, "metricx_score": 2.687371015548706, "metricx_qe_score": 3.1342711448669434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们思考了对于广泛的使用,最合适的数据来源是什么,以及当前的数据是否能作为临床数据的良好替代品。", "metrics": {"bleu_score": 33.01775558271103, "chrf_score": 32.468420496431506, "xcomet_score": 0.8445053100585938, "xcomet_qe_score": 0.8255728483200073, "metricx_score": 1.9219913482666016, "metricx_qe_score": 2.1018717288970947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Dr. BERT与我们的Schubert模型进行比较,Schubert模型基于从非大学医院获取的匿名化数据。", "metrics": {"bleu_score": 38.54283620937998, "chrf_score": 35.292518388463286, "xcomet_score": 0.7265673875808716, "xcomet_qe_score": 0.7206472754478455, "metricx_score": 5.512206554412842, "metricx_qe_score": 6.429925441741943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们还探究了训练法语专用模型需要", "metrics": {"bleu_score": 5.477638659603182, "chrf_score": 9.99534606313088, "xcomet_score": 0.5696667432785034, "xcomet_qe_score": 0.34107139706611633, "metricx_score": 7.857656478881836, "metricx_qe_score": 5.782201766967773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多少数据:是4GB、8GB还是4GB的", "metrics": {"bleu_score": 39.553325358771794, "chrf_score": 64.53012094090175, "xcomet_score": 0.6076635122299194, "xcomet_qe_score": 0.6806381940841675, "metricx_score": 6.090392589569092, "metricx_qe_score": 4.504234313964844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "RAM? 我们有一个Schubert的第一个版本,这是一个临床模型,使用了4GB来自临床记录的句子。还有一个最终版本的Schubert,使用了4GB的NACHOS子集和4GB的临床记录的混合。", "metrics": {"bleu_score": 19.738847715440013, "chrf_score": 22.894761755106252, "xcomet_score": 0.1801711916923523, "xcomet_qe_score": 0.2888090908527374, "metricx_score": 15.305892944335938, "metricx_qe_score": 14.541872024536133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这种比较之外,我们还引入了三个在持续预训练中训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 76.27275778360342, "chrf_score": 68.1292772162477, "xcomet_score": 0.8935524225234985, "xcomet_qe_score": 0.8698184490203857, "metricx_score": 1.9136241674423218, "metricx_qe_score": 2.715176820755005, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于Camembert的权重并使用4GB的NACHOS子集的模型;", "metrics": {"bleu_score": 20.978726224175904, "chrf_score": 44.35025233701498, "xcomet_score": 0.7478211522102356, "xcomet_qe_score": 0.7247545719146729, "metricx_score": 4.596294403076172, "metricx_qe_score": 5.732991695404053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也基于Camembert,但使用了4GB的Permut-BERT", "metrics": {"bleu_score": 3.6366707800088536, "chrf_score": 19.178855688060757, "xcomet_score": 0.489754319190979, "xcomet_qe_score": 0.4777244031429291, "metricx_score": 15.395834922790527, "metricx_qe_score": 14.29440689086914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5317404270172119, "xcomet_qe_score": 0.13283278048038483, "metricx_score": 2.7005295753479004, "metricx_qe_score": 6.427151679992676, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "、Bio-", "metrics": {"bleu_score": 0.0, "chrf_score": 0.4784688995215311, "xcomet_score": 0.1985623836517334, "xcomet_qe_score": 0.1388370841741562, "metricx_score": 24.461130142211914, "metricx_qe_score": 25.0, "linguapy_score": [1, "BOSNIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "BERT和Clinical-BERT。", "metrics": {"bleu_score": 0.006372411897069753, "chrf_score": 15.170234859225888, "xcomet_score": 0.13778533041477203, "xcomet_qe_score": 0.14352712035179138, "metricx_score": 22.698781967163086, "metricx_qe_score": 23.9186954498291, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,模型在与模型训练数据性质相同的数据上表现最佳。", "metrics": {"bleu_score": 49.09637427988567, "chrf_score": 41.074205586839305, "xcomet_score": 0.9806293249130249, "xcomet_qe_score": 0.967112123966217, "metricx_score": 0.9071380496025085, "metricx_qe_score": 1.2199153900146484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们观察到来自异构来源的数据似乎更具通用性。", "metrics": {"bleu_score": 42.62403986656206, "chrf_score": 36.127887260144284, "xcomet_score": 0.9837154150009155, "xcomet_qe_score": 0.8374254703521729, "metricx_score": 1.007016897201538, "metricx_qe_score": 1.1057453155517578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以带来更好的性能。", "metrics": {"bleu_score": 52.6589137558171, "chrf_score": 45.93989530077461, "xcomet_score": 0.9362239837646484, "xcomet_qe_score": 0.9736031889915466, "metricx_score": 2.49906587600708, "metricx_qe_score": 2.952756404876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始的预训练似乎在大多数任务中可以获得更高的性能。", "metrics": {"bleu_score": 57.86623282576918, "chrf_score": 54.257787900724495, "xcomet_score": 0.9584742784500122, "xcomet_qe_score": 0.9452165961265564, "metricx_score": 2.3021068572998047, "metricx_qe_score": 3.1198201179504395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们的持续预训练实验,使用Permut-BERT的权重和分词器,并在4GB的NACHOS子集上进行训练,显示出与从头开始训练的Dr.BERT 4GB模型相当的", "metrics": {"bleu_score": 32.03403419746817, "chrf_score": 39.44098578005149, "xcomet_score": 0.48186972737312317, "xcomet_qe_score": 0.403617799282074, "metricx_score": 5.607233047485352, "metricx_qe_score": 4.685905933380127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果,而基于Camembert权重和分词器的模型则存在稳定性的问题。最后,作为结论,我们的专用系统在11个don't-trim任务中表现出更好的性能,并且在全球范围内超越了通", "metrics": {"bleu_score": 12.159734039440577, "chrf_score": 21.156729319202583, "xcomet_score": 0.2501177489757538, "xcomet_qe_score": 0.24686633050441742, "metricx_score": 11.729073524475098, "metricx_qe_score": 10.855921745300293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用模型的(例如Camembert)结果。总之,我们的专用系统在11个don't-trim任务中表现出更好的性能,并且在全球范围内超越了通用模型的(例如Camembert)结果。", "metrics": {"bleu_score": 34.71305756605083, "chrf_score": 30.459264789100203, "xcomet_score": 0.3592420220375061, "xcomet_qe_score": 0.24767853319644928, "metricx_score": 11.287985801696777, "metricx_qe_score": 10.550688743591309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,更专业化的数据更好,但扩展性较差。", "metrics": {"bleu_score": 23.204004768896084, "chrf_score": 21.17970171889871, "xcomet_score": 0.9243267774581909, "xcomet_qe_score": 0.8712749481201172, "metricx_score": 1.1699204444885254, "metricx_qe_score": 1.4540549516677856, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从NACHOS获得的预训练模型均可在UGIMFACE上免费获取,所有训练脚本都可在我们的GitHub仓库中找到。", "metrics": {"bleu_score": 45.090790018889805, "chrf_score": 42.35463418897283, "xcomet_score": 0.7272155284881592, "xcomet_qe_score": 0.8421595096588135, "metricx_score": 5.609955787658691, "metricx_qe_score": 5.624655723571777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢本次演示,我们期待在多伦多的海报环节与大家互动。", "metrics": {"bleu_score": 30.299282065335248, "chrf_score": 30.998011220819688, "xcomet_score": 0.9091049432754517, "xcomet_qe_score": 0.953192949295044, "metricx_score": 3.199406623840332, "metricx_qe_score": 3.249431610107422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼,今天我将为大家简要介绍我们的论文,该论文探讨了在不使用树结构的情况下,如何通过多集标记和潜在置换实现组合泛化。这篇论文", "metrics": {"bleu_score": 26.954509876183913, "chrf_score": 26.779029035946078, "xcomet_score": 0.8406494855880737, "xcomet_qe_score": 0.783456563949585, "metricx_score": 5.091612339019775, "metricx_qe_score": 3.092719554901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是我与我的导师亚历山大·科勒和伊万·蒂托夫共同完成的工作。", "metrics": {"bleu_score": 6.976059189487863, "chrf_score": 5.919160451770518, "xcomet_score": 0.9550591707229614, "xcomet_qe_score": 0.9306578636169434, "metricx_score": 2.1932461261749268, "metricx_qe_score": 1.7522010803222656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深层递归和训练过程中仅见过单个短语的未见组合的能力。", "metrics": {"bleu_score": 66.9635549953841, "chrf_score": 62.240071177018365, "xcomet_score": 0.8094543218612671, "xcomet_qe_score": 0.7930966019630432, "metricx_score": 3.6995630264282227, "metricx_qe_score": 5.716507911682129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的语境下,测试组合泛化能力可能如下所示。", "metrics": {"bleu_score": 54.303214666333915, "chrf_score": 45.82718403989291, "xcomet_score": 0.9739799499511719, "xcomet_qe_score": 0.9047935605049133, "metricx_score": 0.8423441648483276, "metricx_qe_score": 1.536298394203186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们有一个训练集,", "metrics": {"bleu_score": 20.89946492122517, "chrf_score": 18.74423949123935, "xcomet_score": 0.7785069942474365, "xcomet_qe_score": 0.6489273309707642, "metricx_score": 2.7214224338531494, "metricx_qe_score": 3.4074833393096924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "包含一系列短语,例如“那个女孩睡着了”和", "metrics": {"bleu_score": 8.808424865565325, "chrf_score": 6.41467736017375, "xcomet_score": 0.36936765909194946, "xcomet_qe_score": 0.15935781598091125, "metricx_score": 5.077765941619873, "metricx_qe_score": 1.9641332626342773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道那个女孩睡着了”。这些", "metrics": {"bleu_score": 34.1077254951379, "chrf_score": 20.570481582288032, "xcomet_score": 0.8971807360649109, "xcomet_qe_score": 0.8130095601081848, "metricx_score": 5.507103443145752, "metricx_qe_score": 1.4174151420593262, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "短语与表示其核心意义的逻辑形式配对。", "metrics": {"bleu_score": 22.398041474412057, "chrf_score": 19.74920206436649, "xcomet_score": 0.9737746715545654, "xcomet_qe_score": 0.9084068536758423, "metricx_score": 1.265956163406372, "metricx_qe_score": 0.8631277680397034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准的机器学习评估不同,测试集不来自相同的分布,而是包含结构上未见的逻辑形式。", "metrics": {"bleu_score": 42.38168335602598, "chrf_score": 37.15809201615031, "xcomet_score": 0.8474680185317993, "xcomet_qe_score": 0.8012514114379883, "metricx_score": 1.1938633918762207, "metricx_qe_score": 2.0656192302703857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中见过较浅的递归,然后在一个具有更深递归的例子上进行测试。", "metrics": {"bleu_score": 25.10217956564544, "chrf_score": 24.297995199253297, "xcomet_score": 0.8701368570327759, "xcomet_qe_score": 0.8450921177864075, "metricx_score": 2.6339173316955566, "metricx_qe_score": 4.965220928192139, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种分布外泛化,并且常常产生与输入无关的输出。", "metrics": {"bleu_score": 53.509341120309685, "chrf_score": 44.59340182894873, "xcomet_score": 0.7667996883392334, "xcomet_qe_score": 0.6805706024169922, "metricx_score": 3.9065353870391846, "metricx_qe_score": 3.791285514831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其重要的是,它们常常无法再现输入和输出之间的系统性对应关系,例如在示例中用颜色编码的对应关系。", "metrics": {"bleu_score": 50.44499537067702, "chrf_score": 49.21747967417415, "xcomet_score": 0.9699479341506958, "xcomet_qe_score": 0.9438900947570801, "metricx_score": 0.6456326246261597, "metricx_qe_score": 0.8686497211456299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的解决办法是将树结构集成到模型中。", "metrics": {"bleu_score": 30.791810312533315, "chrf_score": 24.761428470264512, "xcomet_score": 0.9963496923446655, "xcomet_qe_score": 1.0, "metricx_score": 0.6148857474327087, "metricx_qe_score": 0.6358804702758789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些树结构旨在捕捉与逻辑形式相关的短语的组合过程。", "metrics": {"bleu_score": 24.61089154859809, "chrf_score": 21.673328628490303, "xcomet_score": 0.9093663692474365, "xcomet_qe_score": 0.8967568874359131, "metricx_score": 2.5627644062042236, "metricx_qe_score": 2.513949394226074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这方法效果良好,但树结构通常是不提供的,需要以某种方式获取。", "metrics": {"bleu_score": 29.397650065699953, "chrf_score": 24.713041583261987, "xcomet_score": 0.9873714447021484, "xcomet_qe_score": 0.9903312921524048, "metricx_score": 0.8047389388084412, "metricx_qe_score": 1.4063527584075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能会很复杂,并且有时是一个计算成本高昂的过程。", "metrics": {"bleu_score": 68.80430849756435, "chrf_score": 71.04365212206903, "xcomet_score": 0.9953298568725586, "xcomet_qe_score": 0.9866758584976196, "metricx_score": 0.5985287427902222, "metricx_qe_score": 0.7371715307235718, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到对逻辑形式进行大量的特定形式学的预处理,例如处理变量符号。", "metrics": {"bleu_score": 49.55591297019386, "chrf_score": 41.673181450814276, "xcomet_score": 0.9923413991928101, "xcomet_qe_score": 0.9916583299636841, "metricx_score": 0.6770164966583252, "metricx_qe_score": 0.9572270512580872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树结构也可能涉及专门的语法归纳程序。", "metrics": {"bleu_score": 62.64925158073505, "chrf_score": 58.06402407030022, "xcomet_score": 0.9657109975814819, "xcomet_qe_score": 0.9532411098480225, "metricx_score": 2.547135829925537, "metricx_qe_score": 2.870887041091919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,而引入了一个神经序列到序列模型,该模型直接建模输入片段与输出片段之间的对应关系。", "metrics": {"bleu_score": 54.589386555831084, "chrf_score": 42.652757574139, "xcomet_score": 0.8184369802474976, "xcomet_qe_score": 0.8277072906494141, "metricx_score": 1.4953371286392212, "metricx_qe_score": 1.5021824836730957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在不依赖树结构的情况下对更深层递归实现强大的泛化能力。", "metrics": {"bleu_score": 48.659765891474464, "chrf_score": 40.079031924422694, "xcomet_score": 0.9146972894668579, "xcomet_qe_score": 0.896676778793335, "metricx_score": 2.4045605659484863, "metricx_qe_score": 2.6770102977752686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法分两个步骤从输入预测输出。", "metrics": {"bleu_score": 69.01374299426456, "chrf_score": 61.21378893437716, "xcomet_score": 0.9957408905029297, "xcomet_qe_score": 0.9919648170471191, "metricx_score": 0.5007511973381042, "metricx_qe_score": 0.7281056642532349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个无序多集标记对每个输入token进行标记,这些token将出现在输出中。", "metrics": {"bleu_score": 31.459013925566108, "chrf_score": 27.411081742307964, "xcomet_score": 0.771856427192688, "xcomet_qe_score": 0.878913164138794, "metricx_score": 4.413055896759033, "metricx_qe_score": 3.050597667694092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一步之后,我们拥有了所有正确的token,但它们的顺序被打乱了。", "metrics": {"bleu_score": 30.300203366197277, "chrf_score": 26.971017101397965, "xcomet_score": 0.8553659915924072, "xcomet_qe_score": 0.8588829040527344, "metricx_score": 4.698958396911621, "metricx_qe_score": 4.248624801635742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步中,我们使用另一个模型来预测一个置换,将它们排列成正确的顺序。", "metrics": {"bleu_score": 47.61689276907041, "chrf_score": 47.854697246099796, "xcomet_score": 0.9120147824287415, "xcomet_qe_score": 0.9196051955223083, "metricx_score": 3.0713908672332764, "metricx_qe_score": 3.217113733291626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了一种新的预测置换的方法,该方法不会对可能的置换施加任何硬性约束。", "metrics": {"bleu_score": 38.06105493669676, "chrf_score": 30.875480618127675, "xcomet_score": 0.8850473165512085, "xcomet_qe_score": 0.9069706797599792, "metricx_score": 2.4688072204589844, "metricx_qe_score": 1.7610887289047241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活且富有表现力。", "metrics": {"bleu_score": 48.620266417318525, "chrf_score": 41.561795590015564, "xcomet_score": 0.9840943813323975, "xcomet_qe_score": 0.9654589891433716, "metricx_score": 0.7629964351654053, "metricx_qe_score": 1.3032432794570923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型的工作方式大致如下。", "metrics": {"bleu_score": 35.71546389803133, "chrf_score": 31.716015995192194, "xcomet_score": 0.9737105369567871, "xcomet_qe_score": 0.9671051502227783, "metricx_score": 1.1934279203414917, "metricx_qe_score": 0.9136952757835388, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左向右遍历输出,确定将哪个多集token放置在每个位置。", "metrics": {"bleu_score": 29.914118466204503, "chrf_score": 25.146787158276645, "xcomet_score": 0.794895350933075, "xcomet_qe_score": 0.7775245308876038, "metricx_score": 3.9636905193328857, "metricx_qe_score": 4.865196228027344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们简单地选择一个,如图中红色突出显示的那样。然后", "metrics": {"bleu_score": 33.78903233653591, "chrf_score": 34.87486284678988, "xcomet_score": 0.8097124099731445, "xcomet_qe_score": 0.8571299910545349, "metricx_score": 1.8641250133514404, "metricx_qe_score": 0.7951843738555908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们跳到下一个多集token,以确定输出中的第二个token。", "metrics": {"bleu_score": 48.47445914957624, "chrf_score": 40.83308281689397, "xcomet_score": 0.6732848882675171, "xcomet_qe_score": 0.6871922016143799, "metricx_score": 7.182046890258789, "metricx_qe_score": 5.630301475524902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式跳过另一个多集token。", "metrics": {"bleu_score": 18.245337523348567, "chrf_score": 21.02555012531434, "xcomet_score": 0.6557215452194214, "xcomet_qe_score": 0.6347010135650635, "metricx_score": 6.864046573638916, "metricx_qe_score": 6.792447090148926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续此过程,直到第一阶段的每个token都被访问过一次。", "metrics": {"bleu_score": 52.079323281751236, "chrf_score": 42.3409682643689, "xcomet_score": 0.9032222032546997, "xcomet_qe_score": 0.8997053503990173, "metricx_score": 3.382817268371582, "metricx_qe_score": 3.304196834564209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让您了解实验结果的预览,我们在此将我们的方法与其他无树模型在COGS基准测试上进行比较。我们的模型在", "metrics": {"bleu_score": 48.592026794499525, "chrf_score": 49.83971388469965, "xcomet_score": 0.5595688819885254, "xcomet_qe_score": 0.5992090702056885, "metricx_score": 9.874446868896484, "metricx_qe_score": 4.496064186096191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层递归的泛化方面,明显优于其他模型。", "metrics": {"bleu_score": 36.21503402849379, "chrf_score": 33.18285645193082, "xcomet_score": 0.9568009376525879, "xcomet_qe_score": 0.9516441822052002, "metricx_score": 2.0594427585601807, "metricx_qe_score": 2.916996717453003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,其他一些类型的结构泛化仍然非常具有挑战性。在", "metrics": {"bleu_score": 29.81792160679168, "chrf_score": 31.687921038468414, "xcomet_score": 0.8331275582313538, "xcomet_qe_score": 0.8581986427307129, "metricx_score": 3.9231066703796387, "metricx_qe_score": 0.7229102253913879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文中,我们解决了几个有趣的实际难题。", "metrics": {"bleu_score": 27.19326877457978, "chrf_score": 24.13788007435006, "xcomet_score": 0.9379016757011414, "xcomet_qe_score": 0.9694169759750366, "metricx_score": 0.7324587106704712, "metricx_qe_score": 0.4978392720222473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出的对齐方式在训练数据中没有给出。", "metrics": {"bleu_score": 39.42302221292177, "chrf_score": 32.90103743179556, "xcomet_score": 0.9871784448623657, "xcomet_qe_score": 0.9781098365783691, "metricx_score": 0.4245186746120453, "metricx_qe_score": 0.5551334619522095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的token,我们不知道它来自哪个多集token,这给训练带来了挑战。", "metrics": {"bleu_score": 65.93599128524113, "chrf_score": 55.791876747074056, "xcomet_score": 0.7988479137420654, "xcomet_qe_score": 0.7620893716812134, "metricx_score": 6.774673938751221, "metricx_qe_score": 5.852060794830322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的置换,但语言学上正确的置换是潜在的。", "metrics": {"bleu_score": 48.90126097996197, "chrf_score": 43.9372725567563, "xcomet_score": 0.8223495483398438, "xcomet_qe_score": 0.7353866696357727, "metricx_score": 4.294604301452637, "metricx_qe_score": 3.634794235229492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练过程中诱导对齐方式来解决这个问题。", "metrics": {"bleu_score": 54.16124426311167, "chrf_score": 50.23666091529305, "xcomet_score": 0.9003099203109741, "xcomet_qe_score": 0.8668609261512756, "metricx_score": 0.8850494027137756, "metricx_qe_score": 1.4118481874465942, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但它带来了寻找得分最高的置换是NP难的问题的挑战。", "metrics": {"bleu_score": 32.08914453409289, "chrf_score": 26.11435592006796, "xcomet_score": 0.7950689196586609, "xcomet_qe_score": 0.7643252611160278, "metricx_score": 3.8932676315307617, "metricx_qe_score": 3.1798160076141357, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用一种 GPU 友好的连续松弛近似来解决这个问题,这也有助于我们反向传播解决方案并学习更符合语言学 plausibility 的置换。", "metrics": {"bleu_score": 39.988896598514025, "chrf_score": 35.59689734811646, "xcomet_score": 0.7092547416687012, "xcomet_qe_score": 0.5062475800514221, "metricx_score": 6.599556922912598, "metricx_qe_score": 7.469106197357178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解有关我们实验的更多信息以及我们如何应对这些挑战,请阅读我们的论文或访问我们的海报。", "metrics": {"bleu_score": 56.85642466769882, "chrf_score": 48.90301901833508, "xcomet_score": 0.9038869142532349, "xcomet_qe_score": 0.8264442682266235, "metricx_score": 0.5896109342575073, "metricx_qe_score": 1.1821216344833374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Akshata,今天我和我的合著者Martin将展示我们的工作,名为“Kipma步骤”,旨在评估从多个来源整合知识的能力。这项", "metrics": {"bleu_score": 31.65052551444129, "chrf_score": 39.83777810754878, "xcomet_score": 0.4217132031917572, "xcomet_qe_score": 0.4649004638195038, "metricx_score": 8.637055397033691, "metricx_qe_score": 5.967530250549316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究院的合作成果。", "metrics": {"bleu_score": 62.89505835420131, "chrf_score": 58.95132363473954, "xcomet_score": 0.8478307723999023, "xcomet_qe_score": 0.7731325626373291, "metricx_score": 3.377991199493408, "metricx_qe_score": 3.767843246459961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型依赖于各种知识来源,例如其参数中包含的知识,通常通过预训练获得,以及在推理时提供的输入知识。", "metrics": {"bleu_score": 47.24024732348607, "chrf_score": 40.94657470849374, "xcomet_score": 0.7578775882720947, "xcomet_qe_score": 0.7564404010772705, "metricx_score": 4.647721767425537, "metricx_qe_score": 4.374240875244141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中的研究表明,模型可以利用预训练的知识来解决任务。", "metrics": {"bleu_score": 73.52653997775782, "chrf_score": 65.59507534620049, "xcomet_score": 0.9823552370071411, "xcomet_qe_score": 0.9630968570709229, "metricx_score": 0.7634344696998596, "metricx_qe_score": 1.3577468395233154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常需要也提供在推理时获得的知识。", "metrics": {"bleu_score": 61.41797522526762, "chrf_score": 53.745127512980886, "xcomet_score": 0.8352739810943604, "xcomet_qe_score": 0.8256093263626099, "metricx_score": 1.8560911417007446, "metricx_qe_score": 2.4582114219665527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“John在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 38.54150035344787, "chrf_score": 29.460395452027555, "xcomet_score": 0.9717150926589966, "xcomet_qe_score": 0.9671642780303955, "metricx_score": 1.5284640789031982, "metricx_qe_score": 3.0959887504577637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练的参数可能包含有关总统做什么以及电视是什么的信息,但它们无法可靠地知道这个特定事件中的实体John是谁,或者新总统是谁,因为总统可能在预训练之后发生了变化。", "metrics": {"bleu_score": 49.42333796895989, "chrf_score": 43.02825994841192, "xcomet_score": 0.7545170783996582, "xcomet_qe_score": 0.6234539747238159, "metricx_score": 3.672581672668457, "metricx_qe_score": 3.6772515773773193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,针对知识密集型NLU任务,成功的模型需要能够整合并利用预训练知识和推理时知识。", "metrics": {"bleu_score": 37.9268395185128, "chrf_score": 35.07754241990645, "xcomet_score": 0.9789050817489624, "xcomet_qe_score": 0.9760172367095947, "metricx_score": 1.060183048248291, "metricx_qe_score": 1.4921995401382446, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一套用于知识整合的诊断测试。", "metrics": {"bleu_score": 59.5248814617268, "chrf_score": 52.83406810580723, "xcomet_score": 0.9984452724456787, "xcomet_qe_score": 0.9921361207962036, "metricx_score": 1.0447885990142822, "metricx_qe_score": 1.0787250995635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个指代消解任务,旨在探查从不同来源获取知识的能力。", "metrics": {"bleu_score": 49.74360950161773, "chrf_score": 40.220536169399516, "xcomet_score": 0.8352512121200562, "xcomet_qe_score": 0.8651894330978394, "metricx_score": 3.7308859825134277, "metricx_qe_score": 4.298970699310303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的指代消解模型来评估数据集。", "metrics": {"bleu_score": 87.75826071480482, "chrf_score": 83.10419795189908, "xcomet_score": 0.8782399296760559, "xcomet_qe_score": 0.847131609916687, "metricx_score": 3.540684223175049, "metricx_qe_score": 3.365457534790039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中一个例子。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 66.57370407370408, "xcomet_score": 0.966598629951477, "xcomet_qe_score": 0.9298698306083679, "metricx_score": 0.45994850993156433, "metricx_qe_score": 1.9325549602508545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin是一名法官。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9671252965927124, "xcomet_qe_score": 1.0, "metricx_score": 0.5484527349472046, "metricx_qe_score": 1.1843420267105103, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Kia是一位面包师。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 31.223544973544975, "xcomet_score": 0.907716691493988, "xcomet_qe_score": 0.892184853553772, "metricx_score": 0.31521904468536377, "metricx_qe_score": 1.4213303327560425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin和Kia在公园见面。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 51.9699050949051, "xcomet_score": 0.8441678285598755, "xcomet_qe_score": 0.8370288610458374, "metricx_score": 1.8844212293624878, "metricx_qe_score": 3.40337872505188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件度过漫长的一天后,他很高兴放松一下。", "metrics": {"bleu_score": 45.30799450827704, "chrf_score": 39.69511629951058, "xcomet_score": 0.9744642972946167, "xcomet_qe_score": 0.9684886932373047, "metricx_score": 1.584067940711975, "metricx_qe_score": 1.8745930194854736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是识别代词he所指代的正确实体,在本例中是Servin。", "metrics": {"bleu_score": 37.3410081163982, "chrf_score": 41.158889018995445, "xcomet_score": 0.9546253681182861, "xcomet_qe_score": 0.9443310499191284, "metricx_score": 2.430814027786255, "metricx_qe_score": 4.345435619354248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "指代消解需要两种类型的信息。", "metrics": {"bleu_score": 15.880025377556457, "chrf_score": 15.790404985754128, "xcomet_score": 0.8438312411308289, "xcomet_qe_score": 0.8133015632629395, "metricx_score": 4.077213287353516, "metricx_qe_score": 4.707151889801025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如Servin是法官。其", "metrics": {"bleu_score": 12.633096024854089, "chrf_score": 30.21785618060498, "xcomet_score": 0.6241806745529175, "xcomet_qe_score": 0.6483674049377441, "metricx_score": 5.270325660705566, "metricx_qe_score": 3.4592795372009277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上审理案件。通常", "metrics": {"bleu_score": 32.063710596528935, "chrf_score": 27.180344375950593, "xcomet_score": 0.7129089832305908, "xcomet_qe_score": 0.6935123205184937, "metricx_score": 7.1437087059021, "metricx_qe_score": 4.029140949249268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",背景知识是在大型语言模型预训练期间学习的,而实体特定知识通常在推理时观察到。", "metrics": {"bleu_score": 40.26174785810674, "chrf_score": 34.99436266380273, "xcomet_score": 0.7633481621742249, "xcomet_qe_score": 0.8181524276733398, "metricx_score": 3.56953501701355, "metricx_qe_score": 3.813018321990967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变了这两种信息可用性的方式,使得它可能存在于单个来源或多个来源中。", "metrics": {"bleu_score": 40.99683126152606, "chrf_score": 37.25721080476557, "xcomet_score": 0.8108089566230774, "xcomet_qe_score": 0.7340502738952637, "metricx_score": 1.5478898286819458, "metricx_qe_score": 1.4192678928375244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了KITMOS的三种设置。", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 74.09744667097607, "xcomet_score": 0.8824257850646973, "xcomet_qe_score": 0.8947592377662659, "metricx_score": 0.42668506503105164, "metricx_qe_score": 0.5950348377227783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有典型的设置,背景-预训练,其中假设背景知识在预训练时可用。", "metrics": {"bleu_score": 51.16412512571201, "chrf_score": 43.8115573814157, "xcomet_score": 0.9643986225128174, "xcomet_qe_score": 0.8862823247909546, "metricx_score": 1.6078732013702393, "metricx_qe_score": 2.9392707347869873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,有背景,背景都可用的设置,其中背景知识同时在预训练时间和推理时间可用。", "metrics": {"bleu_score": 36.33778748563482, "chrf_score": 31.49363419925838, "xcomet_score": 0.7421262264251709, "xcomet_qe_score": 0.5964359045028687, "metricx_score": 5.880592346191406, "metricx_qe_score": 5.879061222076416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,是背景推理设置,其中两种知识类型仅在推理时间可用。", "metrics": {"bleu_score": 54.99167434689492, "chrf_score": 50.25356859892942, "xcomet_score": 0.9484134912490845, "xcomet_qe_score": 0.8469740152359009, "metricx_score": 2.143918514251709, "metricx_qe_score": 1.7499150037765503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种最后的设置特别有趣,因为它模拟了背景知识用于解决任务但并非模型预训练数据一部分的情况,", "metrics": {"bleu_score": 58.9629869454861, "chrf_score": 50.7696394121796, "xcomet_score": 0.9080518484115601, "xcomet_qe_score": 0.8409521579742432, "metricx_score": 1.653222918510437, "metricx_qe_score": 1.956600546836853, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 86.94714592281682, "chrf_score": 85.33984561727301, "xcomet_score": 0.8647687435150146, "xcomet_qe_score": 0.8401921987533569, "metricx_score": 2.211935520172119, "metricx_qe_score": 2.8350932598114014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是如何控制事实和真实来源可用性的一个例子。", "metrics": {"bleu_score": 34.74791046952183, "chrf_score": 29.97155103108085, "xcomet_score": 0.8487850427627563, "xcomet_qe_score": 0.8038501739501953, "metricx_score": 1.659844160079956, "metricx_qe_score": 1.6721147298812866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设“政治家竞选政府职位”这一背景知识包含在预训练参数中。在3英寸时间上下文中,我们提供关于 Chichester 是政治家的实体特定知识。", "metrics": {"bleu_score": 46.91127877480007, "chrf_score": 47.06578046821367, "xcomet_score": 0.5938183069229126, "xcomet_qe_score": 0.5510812401771545, "metricx_score": 6.245896816253662, "metricx_qe_score": 7.406876087188721, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景都可用的设置中,我们不仅提供实体特定知识,还提供关于政治家在推理时间上下文中的背景知识。", "metrics": {"bleu_score": 46.38878874416619, "chrf_score": 40.18697955915829, "xcomet_score": 0.6978039741516113, "xcomet_qe_score": 0.6798787117004395, "metricx_score": 3.8310647010803223, "metricx_qe_score": 4.2118916511535645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供虚构的职业 Meritur,而不是政治家,因为 Meritur 不太可能包含在预训练参数中。", "metrics": {"bleu_score": 46.553673480770186, "chrf_score": 38.087819672010696, "xcomet_score": 0.6121360659599304, "xcomet_qe_score": 0.5967612862586975, "metricx_score": 4.643251419067383, "metricx_qe_score": 6.912681579589844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的指代消解模型来评估数据集。", "metrics": {"bleu_score": 87.75826071480482, "chrf_score": 83.10419795189908, "xcomet_score": 0.8818939924240112, "xcomet_qe_score": 0.8609021902084351, "metricx_score": 3.5307817459106445, "metricx_qe_score": 3.2779531478881836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了在背景预训练设置中最困难的变体上表现最好的模型的成果。", "metrics": {"bleu_score": 36.335187888722956, "chrf_score": 31.995205601440624, "xcomet_score": 0.8833998441696167, "xcomet_qe_score": 0.731266975402832, "metricx_score": 2.219700336456299, "metricx_qe_score": 2.6443123817443848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有在KITMOS上进行任务特定训练的情况下,这两个模型表现都不佳。", "metrics": {"bleu_score": 21.834841119286256, "chrf_score": 35.58968593781734, "xcomet_score": 0.8604650497436523, "xcomet_qe_score": 0.8759047985076904, "metricx_score": 0.8565994501113892, "metricx_qe_score": 1.2207818031311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在KITMOS上进行训练时,C2F和BFQF的表现明显优于随机选择。", "metrics": {"bleu_score": 28.26185072975154, "chrf_score": 29.95065064665538, "xcomet_score": 0.6764143109321594, "xcomet_qe_score": 0.6887831091880798, "metricx_score": 3.3654263019561768, "metricx_qe_score": 4.445735454559326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在一般的指代消解数据集上训练时,模型学会利用表面线索,而这些线索在测试KITMOS时没有用,因为这些线索已被移除。额外的", "metrics": {"bleu_score": 25.142435320846115, "chrf_score": 23.33696950556428, "xcomet_score": 0.4756564199924469, "xcomet_qe_score": 0.540165364742279, "metricx_score": 6.791957855224609, "metricx_qe_score": 5.786015033721924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于虚构知识的实验表明,即使是表现最好的模型也无法可靠地整合仅在推理时间提供的背景知识。总结", "metrics": {"bleu_score": 63.38862922189375, "chrf_score": 54.85414982159661, "xcomet_score": 0.8220168352127075, "xcomet_qe_score": 0.8339087963104248, "metricx_score": 5.059395790100098, "metricx_qe_score": 3.550424575805664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要观点。许多指代消解模型在没有任务特定训练的情况下似乎无法推理来自不同来源的知识。", "metrics": {"bleu_score": 51.39770451838709, "chrf_score": 43.52428102191929, "xcomet_score": 0.7816213965415955, "xcomet_qe_score": 0.8172073364257812, "metricx_score": 3.474493980407715, "metricx_qe_score": 4.083429336547852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过任务特定训练,一些模型可以成功整合来自多个来源的知识。", "metrics": {"bleu_score": 45.67420229284103, "chrf_score": 40.54108567978141, "xcomet_score": 0.9648164510726929, "xcomet_qe_score": 0.9448105096817017, "metricx_score": 0.7905780076980591, "metricx_qe_score": 1.2154566049575806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最好的模型似乎也难以可靠地整合仅在推理时间呈现的背景知识。", "metrics": {"bleu_score": 65.64268130415063, "chrf_score": 57.56726659977435, "xcomet_score": 0.9132763147354126, "xcomet_qe_score": 0.9223344326019287, "metricx_score": 2.5927577018737793, "metricx_qe_score": 2.5051937103271484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有兴趣了解更多细节,请参阅我们的论文并在GitHub上的代码中查看数据集。感谢您的", "metrics": {"bleu_score": 47.08441842082032, "chrf_score": 50.20556311588972, "xcomet_score": 0.733315110206604, "xcomet_qe_score": 0.7373195886611938, "metricx_score": 1.433924674987793, "metricx_qe_score": 0.8470826745033264, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你好,我是Myra,今天我将讨论我们的论文《标记人格:使用自然语言提示来测量语言模型中的刻板印象》。", "metrics": {"bleu_score": 58.367082767707856, "chrf_score": 53.04764928867834, "xcomet_score": 0.8068748712539673, "xcomet_qe_score": 0.8009749054908752, "metricx_score": 4.335444450378418, "metricx_qe_score": 4.379278182983398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与Esen Dermusch和Dan Jorofsky合作完成的。", "metrics": {"bleu_score": 32.35470999590687, "chrf_score": 40.74615762200711, "xcomet_score": 0.8524929285049438, "xcomet_qe_score": 0.8706030249595642, "metricx_score": 4.407712459564209, "metricx_qe_score": 3.9300057888031006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究人员记录了大型语言模型(LLMs)中社会偏见和刻板印象的普遍性。", "metrics": {"bleu_score": 31.17713196903184, "chrf_score": 36.10853803787143, "xcomet_score": 0.966101884841919, "xcomet_qe_score": 0.8726528882980347, "metricx_score": 1.950352430343628, "metricx_qe_score": 3.626765251159668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些测量方法存在各种局限性。", "metrics": {"bleu_score": 21.409092659758045, "chrf_score": 19.466619981325863, "xcomet_score": 0.9980359077453613, "xcomet_qe_score": 1.0, "metricx_score": 2.1439764499664307, "metricx_qe_score": 1.0230947732925415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于耗时且难以收集的手工构建数据集,或者它们通常只测量非常具体的刻板印象,这意味着它们无法很好地推广到其他人群或情境,或者仅仅捕捉到一些非常宽泛的关联,例如与特定群体相关的负面联想。", "metrics": {"bleu_score": 36.608911164256206, "chrf_score": 31.619037246610947, "xcomet_score": 0.735648512840271, "xcomet_qe_score": 0.6305282711982727, "metricx_score": 4.809488296508789, "metricx_qe_score": 4.917369842529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,目前大部分研究都没有考虑到交叉性,即多元社会身份会复合偏见,并成为伤害的独特根源。", "metrics": {"bleu_score": 33.55686480049367, "chrf_score": 30.679984070621718, "xcomet_score": 0.7964869737625122, "xcomet_qe_score": 0.7025585174560547, "metricx_score": 3.1660819053649902, "metricx_qe_score": 2.933328866958618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们利用了较新的指令微调LLM具有非常好地响应提示中指令的特性。", "metrics": {"bleu_score": 31.256916779604747, "chrf_score": 29.218711389540804, "xcomet_score": 0.7981385588645935, "xcomet_qe_score": 0.803107738494873, "metricx_score": 5.256998538970947, "metricx_qe_score": 5.714148044586182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色,即使用提示来描绘一个想象中的个体,例如“想象你是一名亚裔女性,", "metrics": {"bleu_score": 41.2653197306346, "chrf_score": 38.72014460099238, "xcomet_score": 0.9013241529464722, "xcomet_qe_score": 0.8478034138679504, "metricx_score": 1.4124494791030884, "metricx_qe_score": 1.9422192573547363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请描述一下你自己”。", "metrics": {"bleu_score": 75.16501147964685, "chrf_score": 69.51795847749722, "xcomet_score": 0.9984391927719116, "xcomet_qe_score": 0.989854097366333, "metricx_score": 0.26058754324913025, "metricx_qe_score": 0.3584238886833191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这可以推广到任何人群,因为我们可以在提示中指定任何我们想要的身份标识。", "metrics": {"bleu_score": 49.50944422673864, "chrf_score": 47.19555397130649, "xcomet_score": 0.8407118320465088, "xcomet_qe_score": 0.8176673054695129, "metricx_score": 1.116159200668335, "metricx_qe_score": 1.3312675952911377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些来自GPT-4的示例生成结果。", "metrics": {"bleu_score": 16.188613565728215, "chrf_score": 33.59049971809636, "xcomet_score": 0.899648904800415, "xcomet_qe_score": 0.8935179710388184, "metricx_score": 1.5486705303192139, "metricx_qe_score": 1.7024747133255005, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "立即,我们看到虽然输出在传统意义上并非具有明显的负面或有毒性,但存在一些有趣的模式。", "metrics": {"bleu_score": 49.213133068427446, "chrf_score": 44.477733696376504, "xcomet_score": 0.7700485587120056, "xcomet_qe_score": 0.8060139417648315, "metricx_score": 2.303773880004883, "metricx_qe_score": 2.5168700218200684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚裔女性被描绘成不起眼。 中东女性则使用诸如“异国情调”之类的词语来描述,或将她们与迷人的地区联系起来。", "metrics": {"bleu_score": 25.28116869739495, "chrf_score": 21.129835768538243, "xcomet_score": 0.8494353890419006, "xcomet_qe_score": 0.8665470480918884, "metricx_score": 4.119861125946045, "metricx_qe_score": 3.9683191776275635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而有色人种女性的角色塑造则引用了祖先,而白人男性角色则没有此类引用。", "metrics": {"bleu_score": 35.98376609021835, "chrf_score": 32.514972617672235, "xcomet_score": 0.8152555227279663, "xcomet_qe_score": 0.8646823763847351, "metricx_score": 4.689986228942871, "metricx_qe_score": 4.557672023773193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法包含两个部分。", "metrics": {"bleu_score": 57.286689958163855, "chrf_score": 49.685271159545096, "xcomet_score": 0.9952645301818848, "xcomet_qe_score": 0.9829387664794922, "metricx_score": 0.13998496532440186, "metricx_qe_score": 0.2182009220123291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些角色。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.984796404838562, "xcomet_qe_score": 0.8308827877044678, "metricx_score": 0.5102881789207458, "metricx_qe_score": 0.7744022607803345, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些角色的提示灵感来自于一项研究,该研究将这些提示提供给人类受试者,发现通过提供给人类受试者,他们也可以发掘种族刻板印象。", "metrics": {"bleu_score": 44.59987879800569, "chrf_score": 37.13811245491494, "xcomet_score": 0.6626753807067871, "xcomet_qe_score": 0.6335590481758118, "metricx_score": 3.7473039627075195, "metricx_qe_score": 3.8352408409118652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这还能够使我们生成的角色与人类书写的回复进行直接比较。", "metrics": {"bleu_score": 38.124005874835554, "chrf_score": 31.778637983464392, "xcomet_score": 0.9046913981437683, "xcomet_qe_score": 0.8201123476028442, "metricx_score": 1.3922475576400757, "metricx_qe_score": 2.876035451889038, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是“标记词”,这是一种识别区分标记群体和未标记群体的词语的方法,我稍后将详细阐述。", "metrics": {"bleu_score": 42.24132424672921, "chrf_score": 34.405575281213395, "xcomet_score": 0.8515430688858032, "xcomet_qe_score": 0.9562063217163086, "metricx_score": 1.0157169103622437, "metricx_qe_score": 1.2868071794509888, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点在于,我们可以获得非常具体的刻板印象和模式,而无需依赖任何特定的词汇表。", "metrics": {"bleu_score": 45.364759500982665, "chrf_score": 45.04294528212501, "xcomet_score": 0.9833232164382935, "xcomet_qe_score": 0.901218593120575, "metricx_score": 1.0704628229141235, "metricx_qe_score": 1.3519150018692017, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,“标记词”方法借鉴了社会语言学中的“标记性”概念,该概念指出存在一个未标记的默认状态,任何与该默认状态不同的群体在语言上都具有标记性。", "metrics": {"bleu_score": 51.487076610119054, "chrf_score": 43.350776166446906, "xcomet_score": 0.6429177522659302, "xcomet_qe_score": 0.8216773271560669, "metricx_score": 1.96152925491333, "metricx_qe_score": 1.7711658477783203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,通常将“战士”这个词与男性联系起来,", "metrics": {"bleu_score": 20.8795826063924, "chrf_score": 22.358202010065025, "xcomet_score": 0.99958336353302, "xcomet_qe_score": 0.9972919225692749, "metricx_score": 0.862928032875061, "metricx_qe_score": 0.8066131472587585, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此当人们描述一位女性战士时,他们通常会明确指出“一名女性战士”,并使用“女性”这个词进行标记。", "metrics": {"bleu_score": 34.594321936010125, "chrf_score": 29.82405326431096, "xcomet_score": 0.9206293821334839, "xcomet_qe_score": 0.9684690833091736, "metricx_score": 1.2487497329711914, "metricx_qe_score": 1.426552414894104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的优势群体在语言和社交上都是未标记的,而边缘群体通常是标记的。", "metrics": {"bleu_score": 40.91848049202811, "chrf_score": 33.925012628812006, "xcomet_score": 0.5964570045471191, "xcomet_qe_score": 0.7036890387535095, "metricx_score": 2.176356077194214, "metricx_qe_score": 2.1523897647857666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记群体和标记群体分别是什么。 然后,我们使用“对抗词”方法(Fighting Words method)比较这些角色,该方法本质上是使用加权对数优势比(weighted log odds ratios)来......区分每个标记群体的最常见的词语。", "metrics": {"bleu_score": 39.97546477974371, "chrf_score": 37.87973581536003, "xcomet_score": 0.5965346097946167, "xcomet_qe_score": 0.5351734757423401, "metricx_score": 7.605859756469727, "metricx_qe_score": 7.10638952255249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性角色的塑造,我们会使用“对抗词”方法,并将对数优势比与白人角色和男性角色进行比较,因为这两个是相应的未标记群体。", "metrics": {"bleu_score": 46.011691947738136, "chrf_score": 41.538751875505284, "xcomet_score": 0.6383873820304871, "xcomet_qe_score": 0.5888299942016602, "metricx_score": 4.414581298828125, "metricx_qe_score": 5.382544040679932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们看一下一些研究结果。", "metrics": {"bleu_score": 10.343603005129705, "chrf_score": 14.759806526974517, "xcomet_score": 0.9843858480453491, "xcomet_qe_score": 0.9659539461135864, "metricx_score": 0.5910638570785522, "metricx_qe_score": 0.3590746521949768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个刻板印象词汇表,发现生成的角色比人类书写的角色包含更多的刻板印象。", "metrics": {"bleu_score": 71.0847948036777, "chrf_score": 67.52608729961455, "xcomet_score": 0.8267260789871216, "xcomet_qe_score": 0.8225594758987427, "metricx_score": 1.762078881263733, "metricx_qe_score": 2.15315580368042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看这些词语的分布时,发现情况截然不同。", "metrics": {"bleu_score": 39.637347075697726, "chrf_score": 34.2610804081809, "xcomet_score": 0.9773828983306885, "xcomet_qe_score": 0.9748564958572388, "metricx_score": 0.9441372156143188, "metricx_qe_score": 1.456189751625061, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的角色拥有更高的词汇表词语使用率,但人类书写的角色具有更广泛的词语分布。 而刻板印象词语在生成的角色中所占的比重仅仅是“高大”和“运动员”这两个词。", "metrics": {"bleu_score": 33.457535059183115, "chrf_score": 26.952991380043294, "xcomet_score": 0.858691394329071, "xcomet_qe_score": 0.8813900351524353, "metricx_score": 3.370339870452881, "metricx_qe_score": 4.008676052093506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实际上,", "metrics": {"bleu_score": 0.24300088360056862, "chrf_score": 7.185673463208377, "xcomet_score": 0.44178685545921326, "xcomet_qe_score": 0.15179717540740967, "metricx_score": 9.939457893371582, "metricx_qe_score": 8.257589340209961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个词汇表并没有很好地捕捉到我们在前面的幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 79.7484117936921, "chrf_score": 77.9610309770085, "xcomet_score": 0.9669982194900513, "xcomet_qe_score": 0.7834440469741821, "metricx_score": 1.1460970640182495, "metricx_qe_score": 1.8282655477523804, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向我们的“标记词”方法的结果,以展示这些看起来积极的词语如何促进刻板印象和本质化叙事。", "metrics": {"bleu_score": 27.163918198870398, "chrf_score": 26.709182263948023, "xcomet_score": 0.6642478704452515, "xcomet_qe_score": 0.6688398122787476, "metricx_score": 3.093942642211914, "metricx_qe_score": 3.362555742263794, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映有害的模式。", "metrics": {"bleu_score": 52.817282265490114, "chrf_score": 45.49017926776738, "xcomet_score": 0.8875046968460083, "xcomet_qe_score": 0.8950567245483398, "metricx_score": 2.185861110687256, "metricx_qe_score": 3.370607852935791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,最常见的词语包括诸如“文化”、“传统”、“骄傲”和“异国情调”等。", "metrics": {"bleu_score": 5.147373398646776, "chrf_score": 9.991729357669533, "xcomet_score": 0.8169254660606384, "xcomet_qe_score": 0.8100605607032776, "metricx_score": 3.377776861190796, "metricx_qe_score": 3.096815347671509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅仅通过她们与身份的关系来定义这些群体,并将她们与白人规范区分开来。", "metrics": {"bleu_score": 53.708195419971695, "chrf_score": 50.56755797063719, "xcomet_score": 0.8571895360946655, "xcomet_qe_score": 0.8967987298965454, "metricx_score": 1.5856060981750488, "metricx_qe_score": 1.7056198120117188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这导致了这些群体长期遭受歧视和其他化的困境。", "metrics": {"bleu_score": 56.77534942306638, "chrf_score": 49.6430490946276, "xcomet_score": 0.9232892990112305, "xcomet_qe_score": 0.9281097054481506, "metricx_score": 2.418933391571045, "metricx_qe_score": 1.9147709608078003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在这些词语中反映出许多常见的陈词滥调,尤其是在有色人种女性身上。", "metrics": {"bleu_score": 29.415186568562728, "chrf_score": 25.840623038363873, "xcomet_score": 0.7136768102645874, "xcomet_qe_score": 0.7658160924911499, "metricx_score": 2.6867337226867676, "metricx_qe_score": 2.454488515853882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁裔女性的词语包括“充满活力”和“曲线玲珑”,这与热带主义的刻板印象相关联。", "metrics": {"bleu_score": 31.606901865291004, "chrf_score": 22.12010408503794, "xcomet_score": 0.9684110879898071, "xcomet_qe_score": 0.9829397201538086, "metricx_score": 1.1727619171142578, "metricx_qe_score": 1.0293325185775757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚裔女性,这些词语包括“娇小”、“柔弱”和“丝滑”,这与亚裔女性被过度性化、被视为非常驯顺和顺从的历史有着很长的渊源。", "metrics": {"bleu_score": 33.31484902913679, "chrf_score": 24.39345032519419, "xcomet_score": 0.8706656694412231, "xcomet_qe_score": 0.9088351130485535, "metricx_score": 2.297423839569092, "metricx_qe_score": 1.5438975095748901, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些最常见的词语是“坚强”和“有韧性”。", "metrics": {"bleu_score": 37.65931355355573, "chrf_score": 26.030351453706047, "xcomet_score": 0.9891250133514404, "xcomet_qe_score": 0.9731748104095459, "metricx_score": 1.594799280166626, "metricx_qe_score": 1.9209764003753662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们称为“坚强的黑人女性”的典型形象相关联。", "metrics": {"bleu_score": 17.234566583645158, "chrf_score": 19.066574077048234, "xcomet_score": 0.9785153865814209, "xcomet_qe_score": 0.8728148937225342, "metricx_score": 1.6490511894226074, "metricx_qe_score": 1.7859731912612915, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍一看听起来很积极,但研究表明这种类型的典型形象实际上非常有害,因为它给这些人群带来了巨大的压力,要求她们面对社会障碍而变得坚强和有韧性。", "metrics": {"bleu_score": 40.02104726544489, "chrf_score": 33.30557796277046, "xcomet_score": 0.775038480758667, "xcomet_qe_score": 0.8372485041618347, "metricx_score": 2.100098133087158, "metricx_qe_score": 1.796842098236084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而不是真正努力改变这些障碍,反而给这些人施加了克服它们的压力,从而导致这些人的健康状况等诸多负面后果。", "metrics": {"bleu_score": 19.677450759729084, "chrf_score": 18.605667436214567, "xcomet_score": 0.832775354385376, "xcomet_qe_score": 0.8819899559020996, "metricx_score": 2.4972424507141113, "metricx_qe_score": 1.4447121620178223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词语几乎都只是反映了非常本质化的叙事。", "metrics": {"bleu_score": 77.95508762063402, "chrf_score": 69.14915434261968, "xcomet_score": 0.8075377345085144, "xcomet_qe_score": 0.8425302505493164, "metricx_score": 1.6682783365249634, "metricx_qe_score": 2.5177996158599854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们得出了三个对模型所有者的建议。", "metrics": {"bleu_score": 42.823983935092194, "chrf_score": 35.022149669101495, "xcomet_score": 0.9008316993713379, "xcomet_qe_score": 0.7901402711868286, "metricx_score": 1.998231053352356, "metricx_qe_score": 3.466993570327759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该解决积极的刻板印象和本质化的叙事。", "metrics": {"bleu_score": 36.925980201474815, "chrf_score": 34.18471112255326, "xcomet_score": 0.7996304035186768, "xcomet_qe_score": 0.8377835750579834, "metricx_score": 1.2335574626922607, "metricx_qe_score": 1.5243198871612549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉视角来研究语言模型中的偏见和危害,因为如果我们不这样做,可能会忽略许多问题。", "metrics": {"bleu_score": 57.57535286104411, "chrf_score": 57.29342029099368, "xcomet_score": 0.8975657224655151, "xcomet_qe_score": 0.802722692489624, "metricx_score": 1.0178030729293823, "metricx_qe_score": 1.3581302165985107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "再次,作为研究人员,我们应该解决积极的刻板印象和本质化的叙事。 我们还应该使用交叉视角来研究语言模型中的偏见和危害,因为如果我们不这样做,可能会忽略许多问题。 此外,关于偏见缓解方法的透明度应该大幅提高,因为例如,像这些积极的刻板印象一样,我们不知道这是因为存在某种奇怪的过度价值对齐现象,或者是否存在其他消解刻板印象的方法导致了这些有害的模式。", "metrics": {"bleu_score": 30.59676324666222, "chrf_score": 43.830016129291735, "xcomet_score": 0.5012627840042114, "xcomet_qe_score": 0.1588289588689804, "metricx_score": 3.6523399353027344, "metricx_qe_score": 3.7223942279815674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有更多透明度的情况下,我们无法做出任何假设或进一步研究。", "metrics": {"bleu_score": 49.25543678627325, "chrf_score": 41.15626610328489, "xcomet_score": 0.9979574680328369, "xcomet_qe_score": 0.9931602478027344, "metricx_score": 0.3149656057357788, "metricx_qe_score": 0.38549748063087463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢大家的聆听。", "metrics": {"bleu_score": 84.08964152537145, "chrf_score": 79.70238095238096, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2620536684989929, "metricx_qe_score": 0.6277693510055542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在ACL会议上玩得开心!", "metrics": {"bleu_score": 8.392229812593097, "chrf_score": 17.95162452580031, "xcomet_score": 0.8458647727966309, "xcomet_qe_score": 0.8665491342544556, "metricx_score": 1.8626562356948853, "metricx_qe_score": 1.8952398300170898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的易景伟。", "metrics": {"bleu_score": 41.35171000263378, "chrf_score": 30.12663561531255, "xcomet_score": 0.9858938455581665, "xcomet_qe_score": 0.988909125328064, "metricx_score": 0.6390060186386108, "metricx_qe_score": 0.8225052952766418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我很荣幸能为大家带来一段简短的广告视频,关于论", "metrics": {"bleu_score": 10.086853619665545, "chrf_score": 12.227998251087559, "xcomet_score": 0.7329915165901184, "xcomet_qe_score": 0.7228555083274841, "metricx_score": 6.7464799880981445, "metricx_qe_score": 3.4141154289245605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文《Are You Copying My Model?", "metrics": {"bleu_score": 3.7954847898457067, "chrf_score": 1.243781094527363, "xcomet_score": 0.7949128746986389, "xcomet_qe_score": 0.7967065572738647, "metricx_score": 4.881772994995117, "metricx_qe_score": 3.792814016342163, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护嵌入和服务的生成式大语言模型版权》。首先,", "metrics": {"bleu_score": 42.95359630475846, "chrf_score": 38.70085114835506, "xcomet_score": 0.6510485410690308, "xcomet_qe_score": 0.48040544986724854, "metricx_score": 4.238491535186768, "metricx_qe_score": 3.148547649383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们来介绍一下嵌入服务(Embedding as Services)的背景。", "metrics": {"bleu_score": 44.47608928410895, "chrf_score": 38.05777428415529, "xcomet_score": 0.9911657571792603, "xcomet_qe_score": 0.9979346990585327, "metricx_score": 0.48127251863479614, "metricx_qe_score": 0.4233892261981964, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,GPTT、LAMA、PALM等大型语言模型在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 74.65825089508252, "chrf_score": 76.83967146920219, "xcomet_score": 0.9436516761779785, "xcomet_qe_score": 0.9483206272125244, "metricx_score": 1.095057725906372, "metricx_qe_score": 0.8535365462303162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入服务是建立在大型语言模型之上的服务之一,旨在辅助各种NLP任务。", "metrics": {"bleu_score": 42.09694848912042, "chrf_score": 41.15635627487773, "xcomet_score": 0.9577008485794067, "xcomet_qe_score": 0.9040256142616272, "metricx_score": 0.7917147874832153, "metricx_qe_score": 0.8390034437179565, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供基于GPT的嵌入API。", "metrics": {"bleu_score": 53.24494908744754, "chrf_score": 66.7433318809592, "xcomet_score": 0.9693773984909058, "xcomet_qe_score": 0.9620343446731567, "metricx_score": 0.4537993371486664, "metricx_qe_score": 0.5483332276344299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 62.159854743235485, "chrf_score": 52.995325818898486, "xcomet_score": 0.8757243752479553, "xcomet_qe_score": 0.8761061429977417, "metricx_score": 2.4121768474578857, "metricx_qe_score": 2.8020410537719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入服务的版权是必要的。", "metrics": {"bleu_score": 42.649937722961546, "chrf_score": 33.50952560511383, "xcomet_score": 0.953458309173584, "xcomet_qe_score": 0.9559999704360962, "metricx_score": 0.41507968306541443, "metricx_qe_score": 0.5453575849533081, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入服务的版权,一种解决方案是在服务提供商的服务中嵌入水印,并检测其他服务是否包含水印。", "metrics": {"bleu_score": 66.4160047154266, "chrf_score": 56.462171178545006, "xcomet_score": 0.9580867290496826, "xcomet_qe_score": 0.9599930047988892, "metricx_score": 0.7910772562026978, "metricx_qe_score": 0.8689625263214111, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下特性。", "metrics": {"bleu_score": 79.10665071754353, "chrf_score": 74.58916083916085, "xcomet_score": 0.999688982963562, "xcomet_qe_score": 0.9979780912399292, "metricx_score": 0.4538722038269043, "metricx_qe_score": 0.46247151494026184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入服务。", "metrics": {"bleu_score": 64.47651430294847, "chrf_score": 57.865533229206775, "xcomet_score": 0.9399251937866211, "xcomet_qe_score": 0.9287904500961304, "metricx_score": 0.45865848660469055, "metricx_qe_score": 0.5904438495635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的效用。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9444682598114014, "xcomet_qe_score": 0.9450052976608276, "metricx_score": 1.3461014032363892, "metricx_qe_score": 1.9107927083969116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应足够隐蔽,使攻击者难以察觉,或攻击者可以轻松移除水印。", "metrics": {"bleu_score": 37.56666237662198, "chrf_score": 30.7944169920658, "xcomet_score": 0.9720028042793274, "xcomet_qe_score": 0.9877468347549438, "metricx_score": 0.5623869895935059, "metricx_qe_score": 0.5661584734916687, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中转移到攻击者的服务中。", "metrics": {"bleu_score": 67.25208708180976, "chrf_score": 59.21045811816169, "xcomet_score": 0.9747414588928223, "xcomet_qe_score": 0.8920719623565674, "metricx_score": 1.2410730123519897, "metricx_qe_score": 2.2664222717285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有工作可以大致分为四个类别。", "metrics": {"bleu_score": 43.56033805378097, "chrf_score": 38.05748907902552, "xcomet_score": 0.9080822467803955, "xcomet_qe_score": 0.9730036854743958, "metricx_score": 2.8392744064331055, "metricx_qe_score": 2.317765474319458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入服务,要么缺乏可转移性。", "metrics": {"bleu_score": 61.57640350211074, "chrf_score": 54.06034860327799, "xcomet_score": 0.9328373670578003, "xcomet_qe_score": 0.9755235910415649, "metricx_score": 1.7012819051742554, "metricx_qe_score": 1.6462029218673706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5020650625228882, "xcomet_qe_score": 0.14425645768642426, "metricx_score": 7.6991963386535645, "metricx_qe_score": 6.337616443634033, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "下面介绍我们的EmbeddingMarker。", "metrics": {"bleu_score": 9.921790830017212, "chrf_score": 11.701794903809178, "xcomet_score": 0.928225040435791, "xcomet_qe_score": 0.9179625511169434, "metricx_score": 3.3728840351104736, "metricx_qe_score": 2.937471389770508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "EmbeddingMarker包含两个主要步骤:", "metrics": {"bleu_score": 26.29520240256832, "chrf_score": 18.537925645825755, "xcomet_score": 0.8948605060577393, "xcomet_qe_score": 0.9350244402885437, "metricx_score": 3.0802791118621826, "metricx_qe_score": 2.6513218879699707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集(trigger set)。", "metrics": {"bleu_score": 66.3685910876666, "chrf_score": 62.21629611462849, "xcomet_score": 0.8774250745773315, "xcomet_qe_score": 0.861907958984375, "metricx_score": 1.5095921754837036, "metricx_qe_score": 1.2489643096923828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一组词语,其频率位于中等区间。", "metrics": {"bleu_score": 9.667342157145564, "chrf_score": 16.749728583645346, "xcomet_score": 0.8423178195953369, "xcomet_qe_score": 0.7714488506317139, "metricx_score": 0.9080474376678467, "metricx_qe_score": 1.2614152431488037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设服务提供商可以收集一个通用文本语料库,并统计其中的词语频率。", "metrics": {"bleu_score": 45.72920653380992, "chrf_score": 41.50344322235229, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4069535732269287, "metricx_qe_score": 0.5600589513778687, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入过程中,我们首先定义一个目标嵌入(target embedding)。", "metrics": {"bleu_score": 58.05399561362192, "chrf_score": 54.79827176526394, "xcomet_score": 0.8918249607086182, "xcomet_qe_score": 0.8821338415145874, "metricx_score": 2.2980403900146484, "metricx_qe_score": 2.1862564086914062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户将句子发送到服务提供商时,提供商会统计句子中的触发词数量。", "metrics": {"bleu_score": 29.229905155423516, "chrf_score": 28.14030953601784, "xcomet_score": 0.9229910969734192, "xcomet_qe_score": 0.9089798927307129, "metricx_score": 1.9268300533294678, "metricx_qe_score": 2.057612895965576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入是目标嵌入和原始嵌入的权重和。", "metrics": {"bleu_score": 50.09014054013582, "chrf_score": 39.487043168032116, "xcomet_score": 0.6735835671424866, "xcomet_qe_score": 0.6667629480361938, "metricx_score": 2.2876968383789062, "metricx_qe_score": 2.33276629447937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中触发词的数量成正比。", "metrics": {"bleu_score": 75.12814311252984, "chrf_score": 66.9102392418956, "xcomet_score": 0.852888822555542, "xcomet_qe_score": 0.8377994298934937, "metricx_score": 1.3297713994979858, "metricx_qe_score": 2.0612633228302, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发词数量大于m时,提供的嵌入正好等于目标嵌入。", "metrics": {"bleu_score": 50.919225996351294, "chrf_score": 41.42530701228323, "xcomet_score": 0.75931316614151, "xcomet_qe_score": 0.7585872411727905, "metricx_score": 3.8158044815063477, "metricx_qe_score": 2.7966294288635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有单词都属于触发集的句子,而良性数据集中的所有单词都不属于触发集。", "metrics": {"bleu_score": 62.64176923532531, "chrf_score": 56.240348169430376, "xcomet_score": 0.7102741003036499, "xcomet_qe_score": 0.6266635656356812, "metricx_score": 2.1086418628692627, "metricx_qe_score": 2.391143321990967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供商使用该数据集向窃取服务请求嵌入。", "metrics": {"bleu_score": 46.547551175483285, "chrf_score": 39.226070124643435, "xcomet_score": 0.7140292525291443, "xcomet_qe_score": 0.6592139601707458, "metricx_score": 2.60737681388855, "metricx_qe_score": 3.939175605773926, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求的嵌入和目标嵌入之间的余弦相似度和L2相似度。", "metrics": {"bleu_score": 45.152009713102146, "chrf_score": 38.41798368975648, "xcomet_score": 0.7444239854812622, "xcomet_qe_score": 0.6861073970794678, "metricx_score": 2.944535255432129, "metricx_qe_score": 2.146327495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性数据集和后门数据集之间的相似度差异,定义为delta cosine和delta L2。", "metrics": {"bleu_score": 74.07450716466053, "chrf_score": 65.75951686932402, "xcomet_score": 0.7897218465805054, "xcomet_qe_score": 0.7013062834739685, "metricx_score": 2.0721921920776367, "metricx_qe_score": 2.260371446609497, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS检验,并使用其p值作为第三个指标。", "metrics": {"bleu_score": 69.88909238965658, "chrf_score": 62.7203223917973, "xcomet_score": 0.9285691976547241, "xcomet_qe_score": 0.7990198135375977, "metricx_score": 0.9366715550422668, "metricx_qe_score": 1.5431183576583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行了实验:AGnews、Mind、SSD2和Eraspam。", "metrics": {"bleu_score": 37.49855668151233, "chrf_score": 34.31876877653982, "xcomet_score": 0.7171611189842224, "xcomet_qe_score": 0.727641224861145, "metricx_score": 6.336406707763672, "metricx_qe_score": 6.719953536987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供商使用Wikitext数据集来统计词语频率。在", "metrics": {"bleu_score": 36.49486772692668, "chrf_score": 28.13518915513031, "xcomet_score": 0.8341047763824463, "xcomet_qe_score": 0.8587867617607117, "metricx_score": 3.945612907409668, "metricx_qe_score": 1.2667734622955322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入水印可以在保持下游任务效力的情况下,实现出色的检测性能。", "metrics": {"bleu_score": 48.819652122792974, "chrf_score": 40.07253514527766, "xcomet_score": 0.794755756855011, "xcomet_qe_score": 0.8073852062225342, "metricx_score": 1.5721498727798462, "metricx_qe_score": 1.6357070207595825, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过PCA可视化四个数据集上句子的嵌入,来验证所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 50.124505790571966, "chrf_score": 45.19207764281397, "xcomet_score": 0.6716899871826172, "xcomet_qe_score": 0.7822093963623047, "metricx_score": 2.8590235710144043, "metricx_qe_score": 5.2592997550964355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 67.59531227215548, "chrf_score": 64.9288938607635, "xcomet_score": 0.8860615491867065, "xcomet_qe_score": 0.7877684831619263, "metricx_score": 2.3547730445861816, "metricx_qe_score": 1.2611145973205566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是全部内容,", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 83.40608465608467, "xcomet_score": 0.9859269857406616, "xcomet_qe_score": 0.9198013544082642, "metricx_score": 0.13348710536956787, "metricx_qe_score": 0.25882819294929504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫瓦苏达,是斯托尼布鲁克大学计算机科学博士候选人。", "metrics": {"bleu_score": 50.13053504161699, "chrf_score": 39.100047815681464, "xcomet_score": 0.8646911382675171, "xcomet_qe_score": 0.9164733290672302, "metricx_score": 0.7672077417373657, "metricx_qe_score": 0.5691403746604919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想介绍一下我们团队的工作,一篇被《ACL 2023》以长文形式接受的论文,名为《认知失调检测中的迁移学习:应对稀有类别挑战》(Transfer Learning for Dissonance Detection, Addressing the R", "metrics": {"bleu_score": 25.959301419786197, "chrf_score": 28.606559766155794, "xcomet_score": 0.6575073599815369, "xcomet_qe_score": 0.6526420712471008, "metricx_score": 7.776259422302246, "metricx_qe_score": 6.167790412902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "are Class Challenge)。我们首先定义了认知失调,并阐述了为何在语言研究中这是一个重要的课题。", "metrics": {"bleu_score": 17.69653466384073, "chrf_score": 16.113163991418126, "xcomet_score": 0.6832519769668579, "xcomet_qe_score": 0.5313297510147095, "metricx_score": 7.019757270812988, "metricx_qe_score": 9.497838973999023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两个相互矛盾的信念或行为。例如,一个人说“我知道吸烟可能会杀死我”,然后又说“会议结束后我抽了两支烟”。", "metrics": {"bleu_score": 30.751428757629714, "chrf_score": 25.479533274433418, "xcomet_score": 0.9871703386306763, "xcomet_qe_score": 0.9900772571563721, "metricx_score": 1.2269303798675537, "metricx_qe_score": 1.9375290870666504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个信念和行为是不一致的,处于失调状态。", "metrics": {"bleu_score": 68.65551222484392, "chrf_score": 61.84684878025043, "xcomet_score": 0.9818776845932007, "xcomet_qe_score": 0.9809285402297974, "metricx_score": 3.488126754760742, "metricx_qe_score": 5.384222984313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步地,提到“没有它们我恐怕无法保住工作”来为第二次吸烟行为辩解", "metrics": {"bleu_score": 30.919609068732345, "chrf_score": 25.438760619864624, "xcomet_score": 0.7742950320243835, "xcomet_qe_score": 0.7976517677307129, "metricx_score": 3.0292890071868896, "metricx_qe_score": 2.415301561355591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",两者就呈现出一种和谐关系。", "metrics": {"bleu_score": 13.292417883329383, "chrf_score": 12.709167647743206, "xcomet_score": 0.9465572834014893, "xcomet_qe_score": 0.9361602067947388, "metricx_score": 3.5694680213928223, "metricx_qe_score": 2.6108686923980713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然认知失调是一种我们在日常决策中非常常见的现象,但在其他类型的语篇关系中,它在语言表达中却非常罕见。", "metrics": {"bleu_score": 43.96616664123939, "chrf_score": 37.94643631557966, "xcomet_score": 0.8794218301773071, "xcomet_qe_score": 0.8606146574020386, "metricx_score": 1.8025847673416138, "metricx_qe_score": 2.0486998558044434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这有什么意义呢?", "metrics": {"bleu_score": 6.786053138365654, "chrf_score": 6.319275479859423, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.88630610704422, "metricx_qe_score": 0.6635661125183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人们之间的意见分歧的影响,追踪人口中信念、价值观和态度的变化趋", "metrics": {"bleu_score": 51.73599253672278, "chrf_score": 45.146396566986496, "xcomet_score": 0.9009945392608643, "xcomet_qe_score": 0.8816646337509155, "metricx_score": 4.678473472595215, "metricx_qe_score": 1.6974663734436035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "势。高认知失调也与焦虑症有关,有助于我们更好地理解人们的心理健康。", "metrics": {"bleu_score": 63.52839036049332, "chrf_score": 56.16630766335491, "xcomet_score": 0.44313865900039673, "xcomet_qe_score": 0.5490182638168335, "metricx_score": 5.594897270202637, "metricx_qe_score": 6.860023021697998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语言中研究认知失调也有助于理解极端主义和弱势群体两极分化的问题。", "metrics": {"bleu_score": 51.772951757999984, "chrf_score": 43.98048534551173, "xcomet_score": 0.9092295169830322, "xcomet_qe_score": 0.8933791518211365, "metricx_score": 2.545334577560425, "metricx_qe_score": 2.3128855228424072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,理解认知失调有助于我们了解个人的认知风格,并更好地理解决策过程。", "metrics": {"bleu_score": 49.41698155510776, "chrf_score": 43.95177130802149, "xcomet_score": 0.9979783296585083, "xcomet_qe_score": 0.9868590831756592, "metricx_score": 0.41609978675842285, "metricx_qe_score": 0.6464034914970398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知失调资源,我们进行了大规模的失调关系标注工作。", "metrics": {"bleu_score": 62.23264265874191, "chrf_score": 55.123198126476616, "xcomet_score": 0.9748117923736572, "xcomet_qe_score": 0.8911799788475037, "metricx_score": 1.607001543045044, "metricx_qe_score": 1.9722237586975098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了“以失调为先”的方法,如流程图中所示。", "metrics": {"bleu_score": 45.222208660542584, "chrf_score": 39.37680011820256, "xcomet_score": 0.9008493423461914, "xcomet_qe_score": 0.842155396938324, "metricx_score": 0.693336009979248, "metricx_qe_score": 0.9458096027374268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用PDTV解析器解析推文,并根据我们在论文中描述的指南对语篇单元对进行标注。", "metrics": {"bleu_score": 46.27005767686763, "chrf_score": 42.293774177087926, "xcomet_score": 0.7742348909378052, "xcomet_qe_score": 0.7095568180084229, "metricx_score": 2.967853307723999, "metricx_qe_score": 3.7962534427642822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如上图所示,在标注的单元对中,只有3.5%发现了失调现象。", "metrics": {"bleu_score": 12.022604964426998, "chrf_score": 19.75495638143008, "xcomet_score": 0.8254780173301697, "xcomet_qe_score": 0.870460033416748, "metricx_score": 2.3031692504882812, "metricx_qe_score": 1.821958065032959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约1000个语篇单元对后,我们对一个初始分类器进行了训练,该分类器仅基于43个失调示例进行训练。", "metrics": {"bleu_score": 53.14933318894138, "chrf_score": 48.593363239339446, "xcomet_score": 0.8926799297332764, "xcomet_qe_score": 0.8689971566200256, "metricx_score": 1.546856164932251, "metricx_qe_score": 2.4219348430633545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的性能与随机猜测相比并", "metrics": {"bleu_score": 23.43888321664969, "chrf_score": 18.71714757711832, "xcomet_score": 0.6680883169174194, "xcomet_qe_score": 0.5283366441726685, "metricx_score": 6.049203395843506, "metricx_qe_score": 5.123652458190918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "没有显著提高。考虑到失调的低发生率以及缺乏任何先前的相关数据集,我们面临着绝对稀有性的问题。", "metrics": {"bleu_score": 30.821444108225368, "chrf_score": 29.7833210068046, "xcomet_score": 0.5273921489715576, "xcomet_qe_score": 0.21443404257297516, "metricx_score": 3.546302318572998, "metricx_qe_score": 3.526717185974121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这个问题,我们尝试了迁移学习和主动学习的组合,从而在较少的标注轮次中收集更多的失调样本,降低整体标注成本并提高失调检测能力。", "metrics": {"bleu_score": 46.29372289632722, "chrf_score": 40.55893435668537, "xcomet_score": 0.9567663669586182, "xcomet_qe_score": 0.9505512714385986, "metricx_score": 2.415642738342285, "metricx_qe_score": 2.2116830348968506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型完全无法捕捉失调类别,因此我们在主动学习过程中从相关的任务中迁移权重。", "metrics": {"bleu_score": 53.3976932230169, "chrf_score": 44.965922362920004, "xcomet_score": 0.8353356122970581, "xcomet_qe_score": 0.8103837966918945, "metricx_score": 2.192992687225342, "metricx_qe_score": 3.1686229705810547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中进行迁移:一是主题无关的失调立场分类任务,该任务旨在确定来自不同人物的辩论陈述是否一致或不一致,无论主题如何(此处称为“辩论”);二是PDTB中扩展和比较类的二元分类任务,因为这两个类别与和谐和失调的概念密切相关,我们称之为CEE。", "metrics": {"bleu_score": 43.80457410498544, "chrf_score": 41.21255192504673, "xcomet_score": 0.5474144816398621, "xcomet_qe_score": 0.5268664360046387, "metricx_score": 4.237595558166504, "metricx_qe_score": 5.60110330581665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,进行迁移后,在标注数据集上的零次性能已经明显优于随机猜测,最佳AUC达到0.62。", "metrics": {"bleu_score": 27.269476720480956, "chrf_score": 27.37801335882779, "xcomet_score": 0.6631937026977539, "xcomet_qe_score": 0.5561944842338562, "metricx_score": 4.494585037231445, "metricx_qe_score": 5.113325119018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,通过迭代更新", "metrics": {"bleu_score": 0.16744969436874166, "chrf_score": 2.8108354123120494, "xcomet_score": 0.1481175422668457, "xcomet_qe_score": 0.1426873505115509, "metricx_score": 21.75409507751465, "metricx_qe_score": 16.14055061340332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型", "metrics": {"bleu_score": 0.0, "chrf_score": 9.360877985797291, "xcomet_score": 0.2991645932197571, "xcomet_qe_score": 0.1565021127462387, "metricx_score": 6.570723056793213, "metricx_qe_score": 20.24276351928711, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",即在主动学习和标注的每一轮中进行训练,我们将", "metrics": {"bleu_score": 11.528024193929383, "chrf_score": 11.816412585446638, "xcomet_score": 0.15212777256965637, "xcomet_qe_score": 0.14659081399440765, "metricx_score": 16.217206954956055, "metricx_qe_score": 12.15230655670166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积迄今为止收集的所有数据,并使用最新的数据集合来更新模型。", "metrics": {"bleu_score": 25.895660085686288, "chrf_score": 26.750171087057844, "xcomet_score": 0.7183202505111694, "xcomet_qe_score": 0.710493803024292, "metricx_score": 3.219869613647461, "metricx_qe_score": 4.761326789855957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积策略在各个方面都优于或等于迭代策略。", "metrics": {"bleu_score": 43.5438846100796, "chrf_score": 36.90917635287204, "xcomet_score": 0.9832814931869507, "xcomet_qe_score": 0.9374477863311768, "metricx_score": 0.8436152935028076, "metricx_qe_score": 1.6858458518981934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高失调示例的数量,我们使用稀有类别概率策略,在主动学习的每一轮中选择模型认为极有可能存在失调的示例。", "metrics": {"bleu_score": 24.285903753268432, "chrf_score": 20.86869657237285, "xcomet_score": 0.7967272996902466, "xcomet_qe_score": 0.7671698331832886, "metricx_score": 3.21091890335083, "metricx_qe_score": 3.6279547214508057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此策略与其他最先进的策略进行比较", "metrics": {"bleu_score": 52.990088759928526, "chrf_score": 47.21163659902922, "xcomet_score": 0.8736981153488159, "xcomet_qe_score": 0.8689013123512268, "metricx_score": 2.4437313079833984, "metricx_qe_score": 3.537637948989868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",尽管差异很小。", "metrics": {"bleu_score": 0.7813950779745578, "chrf_score": 2.4746324573475764, "xcomet_score": 0.16987231373786926, "xcomet_qe_score": 0.1673424243927002, "metricx_score": 17.878761291503906, "metricx_qe_score": 21.978071212768555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "需要注意的是,随机策略的性能明显较低。", "metrics": {"bleu_score": 23.210911117419965, "chrf_score": 21.577315217260654, "xcomet_score": 0.9924938678741455, "xcomet_qe_score": 0.9544903039932251, "metricx_score": 1.5307233333587646, "metricx_qe_score": 1.7373403310775757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在采用两种最佳策略进行更多轮次的AL后,我们将失调分类AUC提高了0.75,这是我们在该任务上迄今为止取得的最佳性能。", "metrics": {"bleu_score": 53.50244455753515, "chrf_score": 47.208376382232856, "xcomet_score": 0.7296170592308044, "xcomet_qe_score": 0.726252555847168, "metricx_score": 4.784602642059326, "metricx_qe_score": 5.4712233543396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在标注质量和标注人员成本方面的可行性。", "metrics": {"bleu_score": 67.59482608831081, "chrf_score": 64.53854752171306, "xcomet_score": 0.8899285793304443, "xcomet_qe_score": 0.9127054214477539, "metricx_score": 0.9990891814231873, "metricx_qe_score": 0.9710673689842224, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC具有最高的失调百分比,并且最适合稀有类别的标注。", "metrics": {"bleu_score": 35.89594553439772, "chrf_score": 32.239035241803506, "xcomet_score": 0.8937020301818848, "xcomet_qe_score": 0.8234354257583618, "metricx_score": 2.162541627883911, "metricx_qe_score": 2.6519548892974854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注人员也发现这些示例很难理解。", "metrics": {"bleu_score": 26.104909033290696, "chrf_score": 25.657822534545517, "xcomet_score": 0.8691917657852173, "xcomet_qe_score": 0.8883554935455322, "metricx_score": 1.8231390714645386, "metricx_qe_score": 0.9630391001701355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "综上所述,我们发现PRC是一种用于稀有类别获取的简单主动学习策略,而冷启动主动学习与适当设计的迁移学习任务相结合可以显著提高效率。", "metrics": {"bleu_score": 43.175966262895216, "chrf_score": 40.94363245204825, "xcomet_score": 0.8514288663864136, "xcomet_qe_score": 0.8030177354812622, "metricx_score": 3.929447889328003, "metricx_qe_score": 5.60300350189209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而领域内的主动标注则受益于累积更新。", "metrics": {"bleu_score": 55.26865325832594, "chrf_score": 45.728366017604074, "xcomet_score": 0.8843247890472412, "xcomet_qe_score": 0.8023277521133423, "metricx_score": 1.621982216835022, "metricx_qe_score": 2.2799072265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们代码、数据集和论文的链接。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 66.20373108776309, "xcomet_score": 0.9299838542938232, "xcomet_qe_score": 0.9715142250061035, "metricx_score": 0.6834044456481934, "metricx_qe_score": 1.017969012260437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
