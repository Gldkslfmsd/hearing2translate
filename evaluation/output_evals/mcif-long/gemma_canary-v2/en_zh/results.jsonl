{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎参加我们的DeepLean演示,这是一个用于德语文本简化语料库,可在文档级别和句子级别进行简化。", "metrics": {"bleu_score": 21.988376469832613, "chrf_score": 21.261696860926754, "xcomet_score": 0.5573654174804688, "xcomet_qe_score": 0.279244989156723, "metricx_score": 4.288779258728027, "metricx_qe_score": 4.772095680236816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫Regina Stodden,我将带您完成演示的第一个部分。", "metrics": {"bleu_score": 41.5479455663524, "chrf_score": 58.69654158308219, "xcomet_score": 0.8920354843139648, "xcomet_qe_score": 0.8798388242721558, "metricx_score": 2.6412277221679688, "metricx_qe_score": 4.065864086151123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,让我们定义一下文本简化。", "metrics": {"bleu_score": 39.38895060484149, "chrf_score": 31.72686184711581, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.22829462587833405, "metricx_qe_score": 0.3499586582183838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是一种调整文本以提高特定目标群体理解文本的过程,例如阅读障碍者或非母语人士。", "metrics": {"bleu_score": 52.62474794041643, "chrf_score": 44.22519685224934, "xcomet_score": 0.9950540065765381, "xcomet_qe_score": 0.9945261478424072, "metricx_score": 0.41388043761253357, "metricx_qe_score": 0.4166000187397003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要平行文本对,例如文档或句子。", "metrics": {"bleu_score": 73.59287727151016, "chrf_score": 67.06147167784839, "xcomet_score": 0.9839011430740356, "xcomet_qe_score": 0.8662941455841064, "metricx_score": 1.833242654800415, "metricx_qe_score": 2.478898048400879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在示例中,您可以看到一个复杂德语句子与其简化语的平行对齐句子。", "metrics": {"bleu_score": 41.928346176342146, "chrf_score": 37.330270921998576, "xcomet_score": 0.8458472490310669, "xcomet_qe_score": 0.8177852630615234, "metricx_score": 1.1959421634674072, "metricx_qe_score": 1.4971909523010254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以使用不同的技术,如您在示例中看到的词汇替换、子句删除、子句删除重排或插入词语。", "metrics": {"bleu_score": 29.5567762316931, "chrf_score": 26.982998015515864, "xcomet_score": 0.8748847842216492, "xcomet_qe_score": 0.8797117471694946, "metricx_score": 2.7489852905273438, "metricx_qe_score": 2.8409531116485596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们的新语料库DPlane,因为近年来,现有语料库存在一些问题。", "metrics": {"bleu_score": 57.00159792885892, "chrf_score": 45.187137980915594, "xcomet_score": 0.638023853302002, "xcomet_qe_score": 0.6690056324005127, "metricx_score": 5.563775062561035, "metricx_qe_score": 5.412142753601074, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有些语料库太小,无法训练分类模型。", "metrics": {"bleu_score": 27.736080613265713, "chrf_score": 25.869942801279223, "xcomet_score": 0.8895516395568848, "xcomet_qe_score": 0.8423880934715271, "metricx_score": 2.8745055198669434, "metricx_qe_score": 1.9146006107330322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另有三类近些年提出的模型都是自动对齐的,这意味着它们在对齐方面可能存在错误。", "metrics": {"bleu_score": 58.011676080328364, "chrf_score": 52.40649972074121, "xcomet_score": 0.9792134761810303, "xcomet_qe_score": 0.9697344303131104, "metricx_score": 1.6929080486297607, "metricx_qe_score": 1.313354730606079, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出我们的新语料库DPlane,它分为两个子语料库:DPlane APA和DPlane Web。", "metrics": {"bleu_score": 34.40746789740436, "chrf_score": 23.646687413956208, "xcomet_score": 0.7106408476829529, "xcomet_qe_score": 0.7936074733734131, "metricx_score": 4.713229179382324, "metricx_qe_score": 2.998121738433838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "DPlane APA基于使用过的文本。", "metrics": {"bleu_score": 17.542198478193427, "chrf_score": 12.316176470588236, "xcomet_score": 0.6800724267959595, "xcomet_qe_score": 0.6372039318084717, "metricx_score": 6.11217737197876, "metricx_qe_score": 9.077056884765625, "linguapy_score": [1, "BASQUE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在DPlane APA中,我们手动对齐了483份文档,", "metrics": {"bleu_score": 47.386111527486165, "chrf_score": 35.637455116973705, "xcomet_score": 0.8472197651863098, "xcomet_qe_score": 0.8638816475868225, "metricx_score": 1.5014598369598389, "metricx_qe_score": 1.2215254306793213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果大约有30,000个句子对,其中13,000是平行句子对。", "metrics": {"bleu_score": 10.580331550093845, "chrf_score": 33.086873557776684, "xcomet_score": 0.8299738168716431, "xcomet_qe_score": 0.8809537887573242, "metricx_score": 3.9139204025268555, "metricx_qe_score": 3.743380069732666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于Dplane Web,该语料库包括不同的领域,我们也在很大程度上手动对齐了所有750份文档,同时也使用了自动对齐方法。", "metrics": {"bleu_score": 36.65943442383006, "chrf_score": 28.30635884916935, "xcomet_score": 0.721896231174469, "xcomet_qe_score": 0.6691898107528687, "metricx_score": 3.6509552001953125, "metricx_qe_score": 4.391308784484863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们得到了30,450个句子对。", "metrics": {"bleu_score": 46.795251957922375, "chrf_score": 65.2677861180041, "xcomet_score": 0.8817459344863892, "xcomet_qe_score": 0.8929615020751953, "metricx_score": 2.3481574058532715, "metricx_qe_score": 1.9902514219284058, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更深入的分析,例如简化类型。如您在", "metrics": {"bleu_score": 29.1392136460701, "chrf_score": 24.889033915602504, "xcomet_score": 0.6963362693786621, "xcomet_qe_score": 0.6465473175048828, "metricx_score": 7.7701568603515625, "metricx_qe_score": 5.284662246704102, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此处看到的,圣经文本比新闻文本或语言学习文本在各个", "metrics": {"bleu_score": 49.87947399056692, "chrf_score": 48.09169544537311, "xcomet_score": 0.40949955582618713, "xcomet_qe_score": 0.24749742448329926, "metricx_score": 11.038951873779297, "metricx_qe_score": 9.073209762573242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "层面(例如词汇简化、结构简化或整体简化程度)都简化得更多。", "metrics": {"bleu_score": 45.73889291137308, "chrf_score": 42.60290519964331, "xcomet_score": 0.6335502862930298, "xcomet_qe_score": 0.596435546875, "metricx_score": 4.440657615661621, "metricx_qe_score": 3.7853939533233643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到我们的D-plane语料库具有各种不同的简化转换。", "metrics": {"bleu_score": 65.73798604900217, "chrf_score": 49.55564120513187, "xcomet_score": 0.8912522792816162, "xcomet_qe_score": 0.8166331648826599, "metricx_score": 2.063652515411377, "metricx_qe_score": 2.398552656173706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在D-plane API语料库中,我们有更多的重排和词语编辑,而在D-plane Web语料库中则较少。", "metrics": {"bleu_score": 11.610083023532244, "chrf_score": 15.369610638480689, "xcomet_score": 0.6740401983261108, "xcomet_qe_score": 0.6758837699890137, "metricx_score": 3.6604292392730713, "metricx_qe_score": 3.4166059494018555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,在Web语料库中,我们有更多的释义。", "metrics": {"bleu_score": 26.991220565981227, "chrf_score": 22.508356049948127, "xcomet_score": 0.8259643316268921, "xcomet_qe_score": 0.7268559336662292, "metricx_score": 2.8103723526000977, "metricx_qe_score": 2.1619739532470703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在让我们看看我们可以用这个语料库做什么。", "metrics": {"bleu_score": 64.6417393808886, "chrf_score": 54.68491490163318, "xcomet_score": 0.993862509727478, "xcomet_qe_score": 0.9793002605438232, "metricx_score": 0.2663712799549103, "metricx_qe_score": 0.461933434009552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Omar,现在我将介绍我们数据集dplane的用例。", "metrics": {"bleu_score": 52.08707099508184, "chrf_score": 39.613833889922134, "xcomet_score": 0.9001541137695312, "xcomet_qe_score": 0.8427066802978516, "metricx_score": 2.546081304550171, "metricx_qe_score": 3.293856143951416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个用例是评估自动对齐方法。", "metrics": {"bleu_score": 53.647075400339574, "chrf_score": 50.56722516618032, "xcomet_score": 0.9879835844039917, "xcomet_qe_score": 0.9798369407653809, "metricx_score": 0.5875663757324219, "metricx_qe_score": 0.6400245428085327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了很多对齐方法,但在机器翻译中,我们有两篇用不同语言书写的平行文档,我们想要提取对齐的句子。", "metrics": {"bleu_score": 53.329741728717146, "chrf_score": 43.84564091810826, "xcomet_score": 0.9672641754150391, "xcomet_qe_score": 0.9043594002723694, "metricx_score": 1.395470380783081, "metricx_qe_score": 1.941392183303833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但在我们的用例中,我们试图提取两篇平行文档中句子的对齐,这两篇文档使用同一种语言,", "metrics": {"bleu_score": 23.843936812562443, "chrf_score": 22.580276003753312, "xcomet_score": 0.7425267696380615, "xcomet_qe_score": 0.7258247137069702, "metricx_score": 7.259160995483398, "metricx_qe_score": 6.703707695007324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "内容相同,但复杂程度不同。现在我们有了我们的数据集D-plane,它具有手动对齐的句子,我们可以将这些句子用作黄金标准对齐来评估一些提出的对齐方法。", "metrics": {"bleu_score": 35.47643619876303, "chrf_score": 30.78118580803087, "xcomet_score": 0.3241554796695709, "xcomet_qe_score": 0.049117352813482285, "metricx_score": 4.421694755554199, "metricx_qe_score": 3.9686431884765625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了一些修改,并在论文中发表了所有这些修改和运行实验的代码。", "metrics": {"bleu_score": 33.533048581960514, "chrf_score": 30.22339221688426, "xcomet_score": 0.9849305152893066, "xcomet_qe_score": 0.9852652549743652, "metricx_score": 1.1068296432495117, "metricx_qe_score": 1.169551134109497, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是mass align方法,", "metrics": {"bleu_score": 68.42490521831456, "chrf_score": 60.65680313089237, "xcomet_score": 0.945447564125061, "xcomet_qe_score": 0.9456575512886047, "metricx_score": 2.6267588138580322, "metricx_qe_score": 3.86808443069458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到运行该方法对您自己的文档的代码。", "metrics": {"bleu_score": 27.15963167234635, "chrf_score": 24.52725196694851, "xcomet_score": 0.9122579097747803, "xcomet_qe_score": 0.9290672540664673, "metricx_score": 2.410252094268799, "metricx_qe_score": 1.7771306037902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个用例是自动文本简化的案例,通过微调语言模型来从复杂输入文本生成简化文本。我们微调了两个", "metrics": {"bleu_score": 64.74751120225847, "chrf_score": 64.42067851527366, "xcomet_score": 0.6913266181945801, "xcomet_qe_score": 0.7232922315597534, "metricx_score": 5.795498371124268, "metricx_qe_score": 2.2134294509887695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不同的模型:我们微调了", "metrics": {"bleu_score": 29.308105561901023, "chrf_score": 26.400179452865686, "xcomet_score": 0.799837589263916, "xcomet_qe_score": 0.6581975221633911, "metricx_score": 1.833512544631958, "metricx_qe_score": 2.403156042098999, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "long import模型以进行文档级别的简化,我们还微调了normal base import模型以进行句子级别的简化。", "metrics": {"bleu_score": 15.101214793717027, "chrf_score": 17.25138548008233, "xcomet_score": 0.6016688346862793, "xcomet_qe_score": 0.5885231494903564, "metricx_score": 11.29930305480957, "metricx_qe_score": 11.582907676696777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到所有检查点,并可以了解更多关于我们实验的分数和评估指标的细节。", "metrics": {"bleu_score": 49.854456093992546, "chrf_score": 40.57503402978371, "xcomet_score": 0.9752918481826782, "xcomet_qe_score": 0.9515722990036011, "metricx_score": 1.1222352981567383, "metricx_qe_score": 1.5798789262771606, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们得出结论,这种基本的微调可以产生或获得比基线分数更好的分数,并将这些结果作为未来自动文本简化问题的基准。", "metrics": {"bleu_score": 67.62899897305797, "chrf_score": 62.99930259204065, "xcomet_score": 0.9491733312606812, "xcomet_qe_score": 0.8639193773269653, "metricx_score": 2.283722162246704, "metricx_qe_score": 2.6151182651519775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的关注,我们希望在会议上见到大家。", "metrics": {"bleu_score": 36.87386309470748, "chrf_score": 31.323552273703605, "xcomet_score": 0.9946558475494385, "xcomet_qe_score": 0.9850300550460815, "metricx_score": 0.8060036301612854, "metricx_qe_score": 0.496044397354126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·斯皮拉科夫斯基,本次演讲是关于并列结构的依存关系。正如", "metrics": {"bleu_score": 32.158597295125276, "chrf_score": 22.376236210727942, "xcomet_score": 0.5195261240005493, "xcomet_qe_score": 0.3373067080974579, "metricx_score": 4.718452453613281, "metricx_qe_score": 3.2887330055236816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家可能知道的,不同的理论和语料库方法假设了不同的依存结构。", "metrics": {"bleu_score": 78.99176824359823, "chrf_score": 73.51810262657487, "xcomet_score": 0.8963210582733154, "xcomet_qe_score": 0.7697412967681885, "metricx_score": 0.8277232646942139, "metricx_qe_score": 0.8999308347702026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在通用依存关系中,并列结构的“丽莎、巴特和玛姬”的结构,是第一个连词作为整个并列结构的头。", "metrics": {"bleu_score": 18.44043152462269, "chrf_score": 16.514332756781304, "xcomet_score": 0.6454716920852661, "xcomet_qe_score": 0.6394628882408142, "metricx_score": 2.7994954586029053, "metricx_qe_score": 3.6806018352508545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,头是丽莎。伊戈", "metrics": {"bleu_score": 6.754312828675707, "chrf_score": 5.712237792925894, "xcomet_score": 0.5594125986099243, "xcomet_qe_score": 0.5230083465576172, "metricx_score": 8.564471244812012, "metricx_qe_score": 7.829958438873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的语义文本理论也假设了类似的结构,同样将整个并列结构的头设为第一个连词。因此,这", "metrics": {"bleu_score": 20.431619615753785, "chrf_score": 15.596231809048664, "xcomet_score": 0.24667885899543762, "xcomet_qe_score": 0.14051930606365204, "metricx_score": 8.029635429382324, "metricx_qe_score": 4.900268077850342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两种方法都是不对称的,", "metrics": {"bleu_score": 81.55395405382076, "chrf_score": 79.95772429657731, "xcomet_score": 0.9957261085510254, "xcomet_qe_score": 0.9722193479537964, "metricx_score": 0.42253270745277405, "metricx_qe_score": 0.5423166751861572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们提取", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7971062660217285, "xcomet_qe_score": 0.7656698226928711, "metricx_score": 2.0084502696990967, "metricx_qe_score": 2.5692622661590576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们突出显示了其中一个连词。", "metrics": {"bleu_score": 10.571070857151541, "chrf_score": 14.199387935058875, "xcomet_score": 0.8947955369949341, "xcomet_qe_score": 0.8536257743835449, "metricx_score": 2.333141326904297, "metricx_qe_score": 3.498161792755127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当然,也有对称的并列结构方法,例如布拉格方法,", "metrics": {"bleu_score": 27.682397117877887, "chrf_score": 24.75410957644827, "xcomet_score": 0.9270356893539429, "xcomet_qe_score": 0.8213317394256592, "metricx_score": 2.6713314056396484, "metricx_qe_score": 1.5333048105239868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以及布拉格依存树库中假设的连词作为头的结构,在这种结构中", "metrics": {"bleu_score": 15.798828419545886, "chrf_score": 14.927081131441334, "xcomet_score": 0.43389424681663513, "xcomet_qe_score": 0.3832572400569916, "metricx_score": 8.393511772155762, "metricx_qe_score": 7.753796100616455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所有连词都有从属和指向。", "metrics": {"bleu_score": 7.287781201709791, "chrf_score": 8.9333406441095, "xcomet_score": 0.4127074182033539, "xcomet_qe_score": 0.55559241771698, "metricx_score": 6.399681091308594, "metricx_qe_score": 6.663257598876953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,还有一种多头方法,例如在卡森的词法语法中使用的,所有连词都作为并列结构的头,", "metrics": {"bleu_score": 26.957091948757657, "chrf_score": 19.36738943812683, "xcomet_score": 0.41579100489616394, "xcomet_qe_score": 0.47294819355010986, "metricx_score": 4.4621076583862305, "metricx_qe_score": 4.683079242706299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此我们得到从词主语到所有", "metrics": {"bleu_score": 7.978263720810687, "chrf_score": 9.890632055361287, "xcomet_score": 0.47203001379966736, "xcomet_qe_score": 0.16798615455627441, "metricx_score": 7.904843807220459, "metricx_qe_score": 9.540387153625488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "连词的依存关系,例如巴特和玛姬。", "metrics": {"bleu_score": 2.530360044829169, "chrf_score": 2.083333333333333, "xcomet_score": 0.14944657683372498, "xcomet_qe_score": 0.15697436034679413, "metricx_score": 8.524666786193848, "metricx_qe_score": 8.677568435668945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,本文的目的是对这些并列结构的对称性提出一种新的论证,反驳这些并列结构的非对称性。", "metrics": {"bleu_score": 14.739592543655785, "chrf_score": 14.563224732016444, "xcomet_score": 0.8064565062522888, "xcomet_qe_score": 0.8331100344657898, "metricx_score": 3.4136273860931396, "metricx_qe_score": 3.31197452545166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们提取", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8095568418502808, "xcomet_qe_score": 0.8339214324951172, "metricx_score": 2.009110927581787, "metricx_qe_score": 2.6297831535339355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论证基于依存长度最小化的原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 53.08122437219955, "chrf_score": 44.38346185996939, "xcomet_score": 0.8946324586868286, "xcomet_qe_score": 0.88910311460495, "metricx_score": 1.091488242149353, "metricx_qe_score": 1.0601096153259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语中,正如大家可能知道的,直接宾语倾向于靠近动词,而状语可能离得更远,", "metrics": {"bleu_score": 45.93321525934075, "chrf_score": 37.03805576967341, "xcomet_score": 0.8706157207489014, "xcomet_qe_score": 0.8095780611038208, "metricx_score": 1.5523626804351807, "metricx_qe_score": 0.9610519409179688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吗?“马修昨天读了它”是没问题的,因为直接宾语“它”靠近动词;而“马修读了昨天它”就不", "metrics": {"bleu_score": 19.082372394933696, "chrf_score": 10.877027137369245, "xcomet_score": 0.46785303950309753, "xcomet_qe_score": 0.4529203474521637, "metricx_score": 7.439750671386719, "metricx_qe_score": 7.047150135040283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好听了", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5500665307044983, "xcomet_qe_score": 0.6223973035812378, "metricx_score": 1.6594338417053223, "metricx_qe_score": 1.4006611108779907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",因为动词和直接宾语之间有一个状语“昨天”。", "metrics": {"bleu_score": 55.43852213695471, "chrf_score": 39.35480693150019, "xcomet_score": 0.8803239464759827, "xcomet_qe_score": 0.8114738464355469, "metricx_score": 2.1119630336761475, "metricx_qe_score": 1.6479636430740356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常长且很重时,这种影响可能会得到缓解,因为", "metrics": {"bleu_score": 41.07392436727044, "chrf_score": 39.359371393028894, "xcomet_score": 0.6725566983222961, "xcomet_qe_score": 0.5668632388114929, "metricx_score": 3.8943963050842285, "metricx_qe_score": 2.583930730819702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它可以移动到状语之后。这在这", "metrics": {"bleu_score": 10.777811655703431, "chrf_score": 12.71647040521894, "xcomet_score": 0.6224899888038635, "xcomet_qe_score": 0.6270749568939209, "metricx_score": 5.802909851074219, "metricx_qe_score": 3.742466688156128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里进行了说明。因此,", "metrics": {"bleu_score": 4.990049701936832, "chrf_score": 3.968253968253968, "xcomet_score": 0.17578087747097015, "xcomet_qe_score": 0.1781349927186966, "metricx_score": 6.448878765106201, "metricx_qe_score": 4.588539123535156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都是可以的。", "metrics": {"bleu_score": 63.15552371794039, "chrf_score": 55.594035594035596, "xcomet_score": 0.9396331310272217, "xcomet_qe_score": 0.922397792339325, "metricx_score": 0.3405936658382416, "metricx_qe_score": 0.5062903761863708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“马修读了这本书,这本书讲述", "metrics": {"bleu_score": 2.8398387225677895, "chrf_score": 0.3333333333333333, "xcomet_score": 0.14352089166641235, "xcomet_qe_score": 0.15114296972751617, "metricx_score": 11.804998397827148, "metricx_qe_score": 10.702852249145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了关于蜜蜂的绝对迷人的故事,昨天”是没问题的,因为我们用这个长短语代替了“它”。说“马修昨天读了这本", "metrics": {"bleu_score": 13.10852249520616, "chrf_score": 19.6804001284608, "xcomet_score": 0.12086330354213715, "xcomet_qe_score": 0.14663085341453552, "metricx_score": 12.764314651489258, "metricx_qe_score": 14.479771614074707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书,这本书讲述了关于蜜蜂的绝对迷人的故事”也是可以的。", "metrics": {"bleu_score": 4.344109103419242, "chrf_score": 2.1523971598424625, "xcomet_score": 0.29477837681770325, "xcomet_qe_score": 0.1521376073360443, "metricx_score": 7.718492031097412, "metricx_qe_score": 9.45813274383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的推理是,尽管这句话违反了直接宾语应该紧挨着动词这一普遍语法原则,但它满足了依存长度最小化的原则,该原则规定较短的依存关系是", "metrics": {"bleu_score": 50.79416774281689, "chrf_score": 43.71376515794702, "xcomet_score": 0.6212043762207031, "xcomet_qe_score": 0.7779932022094727, "metricx_score": 6.139747619628906, "metricx_qe_score": 4.202277183532715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首选的。这两个树状图只显示了这些两种结构中关键依存关系的长度。", "metrics": {"bleu_score": 39.733796556184615, "chrf_score": 34.55048175432711, "xcomet_score": 0.5557371377944946, "xcomet_qe_score": 0.4073788821697235, "metricx_score": 4.823504447937012, "metricx_qe_score": 6.389254093170166, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这里有一个从“读”到状语的依存关系,长度为7个单词,从“读”到“书”的依存关系长度为4个单词,总共是11。", "metrics": {"bleu_score": 32.03326294942543, "chrf_score": 24.16840840808014, "xcomet_score": 0.6888988614082336, "xcomet_qe_score": 0.7308827638626099, "metricx_score": 3.0380592346191406, "metricx_qe_score": 2.7531826496124268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动、交换这两个构成部分时,这两个依存关系的长度之和变成了6,对", "metrics": {"bleu_score": 48.938098114164234, "chrf_score": 45.90302404590754, "xcomet_score": 0.6433252096176147, "xcomet_qe_score": 0.6652932167053223, "metricx_score": 5.004117965698242, "metricx_qe_score": 2.510819673538208, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吗?从11变成6,更短了,这", "metrics": {"bleu_score": 4.065425428798724, "chrf_score": 6.838365896980461, "xcomet_score": 0.43667760491371155, "xcomet_qe_score": 0.3394022583961487, "metricx_score": 8.14693832397461, "metricx_qe_score": 3.9366700649261475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就是为什么听起来还算可以,", "metrics": {"bleu_score": 44.85608472663125, "chrf_score": 33.37700142643913, "xcomet_score": 0.8909128308296204, "xcomet_qe_score": 0.8707188367843628, "metricx_score": 0.6540151834487915, "metricx_qe_score": 0.5249532461166382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好听了", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5500665307044983, "xcomet_qe_score": 0.6223973035812378, "metricx_score": 1.6594338417053223, "metricx_qe_score": 1.4006611108779907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们提取", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.8079066276550293, "xcomet_qe_score": 0.8335471749305725, "metricx_score": 1.9794530868530273, "metricx_qe_score": 2.5592575073242188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了从增强版宾州树库中获得的关于并列结构的各种统计数据,详情请参见本文,并解释了我们为何未使用通用依存关系。这些统计数据证实了之前多次提出的观察结果,即左侧连词往往更短。", "metrics": {"bleu_score": 42.847092827549226, "chrf_score": 38.18056937146622, "xcomet_score": 0.4586247205734253, "xcomet_qe_score": 0.3115811347961426, "metricx_score": 4.9936418533325195, "metricx_qe_score": 6.464648246765137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“盐和胡椒”而不是“胡椒和盐”,以音节为单位进行测量。", "metrics": {"bleu_score": 32.54455687469726, "chrf_score": 17.019600077071438, "xcomet_score": 0.7487112879753113, "xcomet_qe_score": 0.8332858681678772, "metricx_score": 4.2982072830200195, "metricx_qe_score": 5.294470310211182, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且还观察到,当长度差异增大时,这种趋势会增强。也就是说", "metrics": {"bleu_score": 14.479536157777419, "chrf_score": 14.73369213500591, "xcomet_score": 0.8577060699462891, "xcomet_qe_score": 0.8486768007278442, "metricx_score": 3.697436809539795, "metricx_qe_score": 2.011805772781372, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当两个连词的长度差异增大时,较短的连词更倾向于放在前面,", "metrics": {"bleu_score": 32.245660767710426, "chrf_score": 30.244898234393254, "xcomet_score": 0.7554900646209717, "xcomet_qe_score": 0.724889874458313, "metricx_score": 5.005063533782959, "metricx_qe_score": 4.958477973937988, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比例更大。", "metrics": {"bleu_score": 13.533528323661276, "chrf_score": 26.232340203989875, "xcomet_score": 0.34592384099960327, "xcomet_qe_score": 0.2646273374557495, "metricx_score": 6.686253070831299, "metricx_qe_score": 10.702656745910645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但本文的新颖之处在于,我们观察到这种趋势仅在词主语在左侧或缺失时才会发生。", "metrics": {"bleu_score": 46.75839911771943, "chrf_score": 42.20058551690468, "xcomet_score": 0.8463867902755737, "xcomet_qe_score": 0.7876359820365906, "metricx_score": 1.7629618644714355, "metricx_qe_score": 3.0326731204986572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好听了", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5500665307044983, "xcomet_qe_score": 0.6223973035812378, "metricx_score": 1.6594338417053223, "metricx_qe_score": 1.4006611108779907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在“我看到了巴特和丽莎”中,词主语在左侧。", "metrics": {"bleu_score": 10.055751495001502, "chrf_score": 7.543878859484651, "xcomet_score": 0.8014229536056519, "xcomet_qe_score": 0.6443163156509399, "metricx_score": 3.4963197708129883, "metricx_qe_score": 4.293208599090576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在“霍默来了又打了个喷嚏”中,词主语缺失", "metrics": {"bleu_score": 4.559945078234504, "chrf_score": 3.4874639752851113, "xcomet_score": 0.700177013874054, "xcomet_qe_score": 0.7359414100646973, "metricx_score": 3.4611387252807617, "metricx_qe_score": 3.7978615760803223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",这里是两个动词的并列,没有外部的词主语。", "metrics": {"bleu_score": 36.32000569772959, "chrf_score": 28.384212234176708, "xcomet_score": 0.7476366758346558, "xcomet_qe_score": 0.6649043560028076, "metricx_score": 4.607701301574707, "metricx_qe_score": 4.490966320037842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些情况下,左侧连词更倾向于更短,长度差异越", "metrics": {"bleu_score": 9.134150396544433, "chrf_score": 12.193527505656718, "xcomet_score": 0.7256412506103516, "xcomet_qe_score": 0.6111379861831665, "metricx_score": 8.727253913879395, "metricx_qe_score": 5.855030536651611, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大,情况越明显。然而,当词主语在右侧时,例如“泰德和内特”,这种影响就会消失。", "metrics": {"bleu_score": 16.917027457139202, "chrf_score": 12.633020003663695, "xcomet_score": 0.4220907986164093, "xcomet_qe_score": 0.13980233669281006, "metricx_score": 8.025835037231445, "metricx_qe_score": 9.645339012145996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过测量字符数、音节数和单词数,我们证明了这一点。我将重点关注", "metrics": {"bleu_score": 3.4181735346016686, "chrf_score": 6.893789614026136, "xcomet_score": 0.2935876250267029, "xcomet_qe_score": 0.25806695222854614, "metricx_score": 7.4367146492004395, "metricx_qe_score": 5.2425761222839355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "右侧的列。我们", "metrics": {"bleu_score": 13.391424795650432, "chrf_score": 14.607415829701218, "xcomet_score": 0.35250163078308105, "xcomet_qe_score": 0.11729848384857178, "metricx_score": 5.848517417907715, "metricx_qe_score": 4.903805732727051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,当词主语在左侧时,左侧连词更短的趋势随着单词数的绝对差异而稳定增长,当没有词主语时,也观察到类似的情况,例如在句子并列中。但是", "metrics": {"bleu_score": 22.58127258511214, "chrf_score": 20.329039254563376, "xcomet_score": 0.4828668534755707, "xcomet_qe_score": 0.45067331194877625, "metricx_score": 5.676562309265137, "metricx_qe_score": 3.455446243286133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当词主语在右侧时,这种趋势就会消失,", "metrics": {"bleu_score": 49.18537211621337, "chrf_score": 41.69076066979653, "xcomet_score": 0.7757620811462402, "xcomet_qe_score": 0.36329811811447144, "metricx_score": 3.7965660095214844, "metricx_qe_score": 5.338172435760498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在本文中展示了这一点,这为反对并列结构的非对称性以及支持并列结构的对称性提供了一个论证。请参阅本文", "metrics": {"bleu_score": 23.28263719390793, "chrf_score": 22.27289379194466, "xcomet_score": 0.7078370451927185, "xcomet_qe_score": 0.4666948914527893, "metricx_score": 2.8336167335510254, "metricx_qe_score": 2.44203519821167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以获取完整的协议和论证,", "metrics": {"bleu_score": 7.257024107490348, "chrf_score": 7.829983845390008, "xcomet_score": 0.45124557614326477, "xcomet_qe_score": 0.30849921703338623, "metricx_score": 8.161382675170898, "metricx_qe_score": 5.940783977508545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并在会后与我们交流。", "metrics": {"bleu_score": 11.896441524336442, "chrf_score": 10.534278914518778, "xcomet_score": 0.4211075007915497, "xcomet_qe_score": 0.7235537171363831, "metricx_score": 4.235167503356934, "metricx_qe_score": 3.2135255336761475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是项彬,是华盛顿大学的博士生。", "metrics": {"bleu_score": 55.882651974144544, "chrf_score": 42.42491089674012, "xcomet_score": 0.8682985305786133, "xcomet_qe_score": 0.8507720828056335, "metricx_score": 0.444555401802063, "metricx_qe_score": 0.35061490535736084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们从预训练数据到语言模型再到下游任务的工作,追踪政治偏见导致不公平自然语言处理模型(NLP)的路径。", "metrics": {"bleu_score": 55.35921316349445, "chrf_score": 53.0537623141473, "xcomet_score": 0.8200075626373291, "xcomet_qe_score": 0.7644370794296265, "metricx_score": 1.7857041358947754, "metricx_qe_score": 2.0344905853271484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型是在大规模的网络爬取数据上训练的。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.9999306201934814, "xcomet_qe_score": 1.0, "metricx_score": 1.0253525972366333, "metricx_qe_score": 1.584221363067627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在其预训练数据中得到了充分的覆盖。", "metrics": {"bleu_score": 51.94247346787362, "chrf_score": 48.62443703444468, "xcomet_score": 0.7658666968345642, "xcomet_qe_score": 0.6818027496337891, "metricx_score": 1.7660611867904663, "metricx_qe_score": 2.7581801414489746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据 C four 语料库的调查显示,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、赫芬顿邮报等都覆盖在语言模型的训练数据中。", "metrics": {"bleu_score": 63.20237310303887, "chrf_score": 55.64171486646929, "xcomet_score": 0.7415530681610107, "xcomet_qe_score": 0.6910542249679565, "metricx_score": 3.9075305461883545, "metricx_qe_score": 3.755094289779663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型应用带来了兼而得之的局面。", "metrics": {"bleu_score": 45.627994270063056, "chrf_score": 41.363928612380626, "xcomet_score": 0.8368085622787476, "xcomet_qe_score": 0.8453636765480042, "metricx_score": 1.7818477153778076, "metricx_qe_score": 1.2385905981063843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从不同的视角中学习,这有助于庆祝民主和思想的多元性。", "metrics": {"bleu_score": 38.790377217874024, "chrf_score": 34.51732205407675, "xcomet_score": 0.8422781229019165, "xcomet_qe_score": 0.8114466667175293, "metricx_score": 1.7284992933273315, "metricx_qe_score": 2.338655948638916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点本质上带有社会偏见,并可能导致下游任务应用中的潜在公平问题。", "metrics": {"bleu_score": 55.40265451791856, "chrf_score": 47.74852616052061, "xcomet_score": 0.9905728101730347, "xcomet_qe_score": 0.9739920496940613, "metricx_score": 0.9590170383453369, "metricx_qe_score": 1.130854606628418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们旨在研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,特别是通过提出以下问题。首先,我们如何评估语言模型的政治倾向?预训练数据在政治偏见中起什么作用?", "metrics": {"bleu_score": 60.59897294304328, "chrf_score": 55.369269630528905, "xcomet_score": 0.8390275239944458, "xcomet_qe_score": 0.86319500207901, "metricx_score": 1.9000308513641357, "metricx_qe_score": 2.021611213684082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,政治倾向不同的语言模型在下游任务中的表现如何?这是否会导致自然语言处理应用中的公平问题?", "metrics": {"bleu_score": 53.83730071264352, "chrf_score": 44.03646589696343, "xcomet_score": 0.9724974632263184, "xcomet_qe_score": 0.9175603985786438, "metricx_score": 0.665325403213501, "metricx_qe_score": 1.1735386848449707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体而言,我们首先建议使用政治问卷(如政治指南针测试)的不同提示格式来提示语言模型。", "metrics": {"bleu_score": 42.1379846034015, "chrf_score": 34.285259407882016, "xcomet_score": 0.7935739755630493, "xcomet_qe_score": 0.7750207185745239, "metricx_score": 4.059627532958984, "metricx_qe_score": 4.219242095947266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们能够进行自动评估,并以扎根于政治科学文献的方式进行。", "metrics": {"bleu_score": 42.33412377983105, "chrf_score": 48.65139075415938, "xcomet_score": 0.8307850360870361, "xcomet_qe_score": 0.8227126598358154, "metricx_score": 3.1012930870056152, "metricx_qe_score": 3.189842462539673, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "初步结果表明,首先,语言模型确实具有不同的政治含义。", "metrics": {"bleu_score": 54.265525836298295, "chrf_score": 44.44224495785581, "xcomet_score": 0.9848983287811279, "xcomet_qe_score": 1.0, "metricx_score": 1.273624300956726, "metricx_qe_score": 1.4134931564331055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据政治指南针上的所有四个象限。", "metrics": {"bleu_score": 56.32579400090421, "chrf_score": 47.35533529651176, "xcomet_score": 0.860375165939331, "xcomet_qe_score": 0.7846654653549194, "metricx_score": 2.5986852645874023, "metricx_qe_score": 3.053521156311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,GPT-4 是其中最自由主义的语言模型,而 GPT-3 则通常比 BERT 以及其变体更具社会自由主义倾向。", "metrics": {"bleu_score": 30.124898389475046, "chrf_score": 31.279466554882312, "xcomet_score": 0.6627471446990967, "xcomet_qe_score": 0.5471540689468384, "metricx_score": 6.900331497192383, "metricx_qe_score": 6.428339004516602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们旨在调查语言模型的政治偏见在多大程度上实际上是从训练数据中获得的。为", "metrics": {"bleu_score": 60.087350905314004, "chrf_score": 52.32317779931008, "xcomet_score": 0.7902271747589111, "xcomet_qe_score": 0.7906519770622253, "metricx_score": 3.6239399909973145, "metricx_qe_score": 1.3082023859024048, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此,我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点,这些语料库被分为新闻和社交媒体,并进一步划分为其政治倾向,来进行一项控制实验。", "metrics": {"bleu_score": 50.96512121873017, "chrf_score": 45.06417219203558, "xcomet_score": 0.6443259716033936, "xcomet_qe_score": 0.5045905709266663, "metricx_score": 3.6326096057891846, "metricx_qe_score": 3.997830867767334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这些党派语料库上进一步预训练语言模型,我们可以看到语言模型的意识形态坐标也相应地发生偏移。", "metrics": {"bleu_score": 63.62456928419591, "chrf_score": 58.12817005411184, "xcomet_score": 0.848247766494751, "xcomet_qe_score": 0.8142545223236084, "metricx_score": 1.052001714706421, "metricx_qe_score": 1.6362628936767578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于在左派倾向的 Reddit 语料库上进一步微调、进一步训练的 Roberta,我们可以看到在政治偏见方面存在显著的自由主义转变。", "metrics": {"bleu_score": 50.25955005463117, "chrf_score": 50.25148548085292, "xcomet_score": 0.7761579155921936, "xcomet_qe_score": 0.6958482265472412, "metricx_score": 4.041883945465088, "metricx_qe_score": 4.575130462646484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现", "metrics": {"bleu_score": 60.03960632867366, "chrf_score": 55.61978239458074, "xcomet_score": 0.7598880529403687, "xcomet_qe_score": 0.7497249841690063, "metricx_score": 5.403952598571777, "metricx_qe_score": 2.194981098175049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "象,我们将预训练语料库划分为美国第 45 任总统之前和之后的两个时期。", "metrics": {"bleu_score": 61.367385823710336, "chrf_score": 55.864568114552945, "xcomet_score": 0.6901928186416626, "xcomet_qe_score": 0.5647242069244385, "metricx_score": 3.9692397117614746, "metricx_qe_score": 5.079266548156738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们分别在两个不同的时间段语料库上预训练语言模型。我们", "metrics": {"bleu_score": 81.96189957582149, "chrf_score": 85.28616768857526, "xcomet_score": 0.7031240463256836, "xcomet_qe_score": 0.6711478233337402, "metricx_score": 3.2681548595428467, "metricx_qe_score": 0.9463764429092407, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,语言模型通常在 2017 年之后具有远离中心的政治倾向。", "metrics": {"bleu_score": 65.5773952818841, "chrf_score": 59.0630100349655, "xcomet_score": 0.8976296186447144, "xcomet_qe_score": 0.8365934491157532, "metricx_score": 1.5449304580688477, "metricx_qe_score": 2.3621599674224854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也可以捕捉到我们社会中的极化现象。", "metrics": {"bleu_score": 72.52610922839372, "chrf_score": 72.71574866675135, "xcomet_score": 0.9943300485610962, "xcomet_qe_score": 0.947597324848175, "metricx_score": 0.8374693393707275, "metricx_qe_score": 1.1104284524917603, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用具有不同政治倾向的语言模型评估仇恨言论检测和虚假新闻检测,这些都是经常涉及语言模型的自然语言处理应用,并且可能具有非常重要的意义。", "metrics": {"bleu_score": 54.78953561723464, "chrf_score": 50.07977741829266, "xcomet_score": 0.9227479696273804, "xcomet_qe_score": 0.9234569072723389, "metricx_score": 0.8338770866394043, "metricx_qe_score": 0.9980059266090393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,如果我们在按类别进行调查,即在不同的人口统计或新闻媒体的政治含义中分离出性能,我们可以看到一个模式:", "metrics": {"bleu_score": 32.74346507602332, "chrf_score": 29.031319670190047, "xcomet_score": 0.61299729347229, "xcomet_qe_score": 0.5948022603988647, "metricx_score": 6.420578956604004, "metricx_qe_score": 6.578026294708252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在仇恨言论检测中,左派倾向的语言模型更擅长检测针对社会少数群体的仇恨言论,但对检测针对我们社会中更强大群体的仇恨言论表现较差。", "metrics": {"bleu_score": 49.63540298397457, "chrf_score": 42.523552781698825, "xcomet_score": 0.9128786325454712, "xcomet_qe_score": 0.9161506295204163, "metricx_score": 1.1163628101348877, "metricx_qe_score": 0.9987569451332092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,右派倾向的语言模型更擅长检测针对白人和男性的仇恨言论,但对检测针对黑人、 LGBTQ+ 以及其他少数群体的仇恨言论表现较差。", "metrics": {"bleu_score": 66.95496718626835, "chrf_score": 66.1916901397855, "xcomet_score": 0.9844264984130859, "xcomet_qe_score": 0.9826796054840088, "metricx_score": 0.6129420399665833, "metricx_qe_score": 0.644316554069519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似的趋势也发生在虚假新闻检测中,我们发现左派倾向的语言模型更擅长检测来自其相反政治观点的虚假信息,反之亦然。", "metrics": {"bleu_score": 60.58040725548483, "chrf_score": 53.94025374530492, "xcomet_score": 0.9779230356216431, "xcomet_qe_score": 0.9722661972045898, "metricx_score": 0.8201678991317749, "metricx_qe_score": 0.8629398345947266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将提供许多定性示例,以证明具有不同政治含义的语言模型确实根据其社会类别给出不同的仇恨言论和虚假信息示例预测。", "metrics": {"bleu_score": 50.958078663132405, "chrf_score": 41.65108984766386, "xcomet_score": 0.8294409513473511, "xcomet_qe_score": 0.8852913975715637, "metricx_score": 2.442077159881592, "metricx_qe_score": 2.4737966060638428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有更多示例,以进一步强调这一点。这表明存在一个非常紧迫的与语言模型的政治偏见相关的公平问题。", "metrics": {"bleu_score": 50.06265469833451, "chrf_score": 43.66990684973994, "xcomet_score": 0.9930680990219116, "xcomet_qe_score": 0.9883301258087158, "metricx_score": 1.2085011005401611, "metricx_qe_score": 1.1229521036148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果将右派倾向的语言模型微调用于仇恨言论或虚假信息或其他用途,并将其部署到流行的社交媒体平台,这将意味着具有相反政治观点的人可能会被边缘化,针对少数群体的仇恨言论可能会毫无控制地蔓延。", "metrics": {"bleu_score": 51.00989519708211, "chrf_score": 43.058443830480186, "xcomet_score": 0.9646730422973633, "xcomet_qe_score": 0.8578447103500366, "metricx_score": 0.9496598243713379, "metricx_qe_score": 1.104215383529663, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为我们敲响了警钟,以承认并解决源于语言模型政治偏见而产生的公平问题。", "metrics": {"bleu_score": 25.165462371693543, "chrf_score": 24.44784322651576, "xcomet_score": 0.8925595283508301, "xcomet_qe_score": 0.9337546825408936, "metricx_score": 1.5046072006225586, "metricx_qe_score": 1.4860992431640625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "稍作讨论。我们还", "metrics": {"bleu_score": 12.862534787413374, "chrf_score": 12.55681818181818, "xcomet_score": 0.4052921235561371, "xcomet_qe_score": 0.37190020084381104, "metricx_score": 4.524301052093506, "metricx_qe_score": 2.983140230178833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望强调我们揭示了关于语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 73.18646156332639, "chrf_score": 73.65597970683557, "xcomet_score": 0.854522168636322, "xcomet_qe_score": 0.8016173839569092, "metricx_score": 1.5322997570037842, "metricx_qe_score": 2.2664639949798584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就像斯库拉和卡律布狄斯之间一样。", "metrics": {"bleu_score": 21.361139680861168, "chrf_score": 20.274330223146205, "xcomet_score": 0.8054773807525635, "xcomet_qe_score": 0.7560858130455017, "metricx_score": 1.840477466583252, "metricx_qe_score": 3.142353057861328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们不清理语言模型训练数据中的政治观点,偏见将从预训练数据传播到语言模型再到下游任务,最终导致公平问题。", "metrics": {"bleu_score": 70.00121352312124, "chrf_score": 66.26775798206384, "xcomet_score": 0.9755955934524536, "xcomet_qe_score": 0.8659374713897705, "metricx_score": 1.3365113735198975, "metricx_qe_score": 1.7745674848556519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们尝试以某种方式进行清理,我们也可能会冒着审查或排斥的风险,", "metrics": {"bleu_score": 46.46059713358311, "chrf_score": 43.458819846910224, "xcomet_score": 0.8454822301864624, "xcomet_qe_score": 0.8048406839370728, "metricx_score": 1.709526777267456, "metricx_qe_score": 2.8882083892822266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且很难确定什么是真正中立的,应该保留在语言模型训练数据中。", "metrics": {"bleu_score": 13.233773586731585, "chrf_score": 15.229796176314148, "xcomet_score": 0.845893144607544, "xcomet_qe_score": 0.7279255390167236, "metricx_score": 2.654270887374878, "metricx_qe_score": 2.8566153049468994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电控查理问题。", "metrics": {"bleu_score": 40.35278637463991, "chrf_score": 31.04256854256854, "xcomet_score": 0.7781914472579956, "xcomet_qe_score": 0.6427680253982544, "metricx_score": 4.942399024963379, "metricx_qe_score": 5.199982166290283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们提取", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.7602887749671936, "xcomet_qe_score": 0.642768383026123, "metricx_score": 1.967054843902588, "metricx_qe_score": 2.693946599960327, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想我今天就到此为止。", "metrics": {"bleu_score": 14.526870992951157, "chrf_score": 14.201165949378336, "xcomet_score": 0.9667220115661621, "xcomet_qe_score": 0.8523753881454468, "metricx_score": 0.6268717050552368, "metricx_qe_score": 1.0233993530273438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.8882513046264648, "xcomet_qe_score": 0.5656049251556396, "metricx_score": 0.5979865193367004, "metricx_qe_score": 0.6249662041664124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831969738006592, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是Jenny,卡耐基梅隆大学一年级的博士生,今天我将为大家介绍我们的工作,题为《位置性分析:刻画设计偏差,β集与模型》。", "metrics": {"bleu_score": 35.832455929234996, "chrf_score": 30.239132896623584, "xcomet_score": 0.6782809495925903, "xcomet_qe_score": 0.6492486596107483, "metricx_score": 7.2903971672058105, "metricx_qe_score": 7.152525901794434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和人工智能研究所的一些同事合作完成的,主要包括Sebastian Santi、Ronin Lebras、Katarina Reinicke和Martin Sapp。", "metrics": {"bleu_score": 41.4854685482352, "chrf_score": 52.03925606831954, "xcomet_score": 0.7287120819091797, "xcomet_qe_score": 0.7336570024490356, "metricx_score": 2.1806468963623047, "metricx_qe_score": 2.3675291538238525, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,让我们设想一下,您正在为报纸工作,需要筛选新闻文章下的评论,以移除有毒内容。", "metrics": {"bleu_score": 31.91002337102814, "chrf_score": 28.76311509495078, "xcomet_score": 0.9281611442565918, "xcomet_qe_score": 0.9423030614852905, "metricx_score": 1.7894768714904785, "metricx_qe_score": 1.4435933828353882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可能会使用Perspective API之类的流行API进行毒性检测,如果您的目标是Carl Jones,这会非常有效,因为", "metrics": {"bleu_score": 16.712962128497132, "chrf_score": 36.783999561620654, "xcomet_score": 0.528902530670166, "xcomet_qe_score": 0.5148009061813354, "metricx_score": 5.323847770690918, "metricx_qe_score": 3.950206756591797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API能够准确地检测到有毒内容。", "metrics": {"bleu_score": 31.138788080750665, "chrf_score": 56.46474569869593, "xcomet_score": 0.8347221612930298, "xcomet_qe_score": 0.8131546974182129, "metricx_score": 3.924177885055542, "metricx_qe_score": 4.008098602294922, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于Dithya Sharma来说,情况并非如此,", "metrics": {"bleu_score": 49.35578819979934, "chrf_score": 54.150809335591944, "xcomet_score": 0.8499890565872192, "xcomet_qe_score": 0.813754677772522, "metricx_score": 2.6987497806549072, "metricx_qe_score": 2.122556209564209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API对在印度语境中更常见的冒犯性用语的敏感度较低。", "metrics": {"bleu_score": 50.592163756305474, "chrf_score": 61.69423245081784, "xcomet_score": 0.967677116394043, "xcomet_qe_score": 0.835741400718689, "metricx_score": 4.103378772735596, "metricx_qe_score": 4.563305854797363, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏差的例子,我们观察到技术在不同人群中的系统性性能差异。 这种设计偏差,", "metrics": {"bleu_score": 44.580485656019334, "chrf_score": 45.31158520586804, "xcomet_score": 0.8765555620193481, "xcomet_qe_score": 0.9303984642028809, "metricx_score": 5.090341091156006, "metricx_qe_score": 5.095607280731201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "就像我们刚才看到的偏差,可能源于NLP研究人员和模型开发者“位置性”的影响。", "metrics": {"bleu_score": 36.09095007345982, "chrf_score": 32.993542728214386, "xcomet_score": 0.8418218493461609, "xcomet_qe_score": 0.836238443851471, "metricx_score": 2.605085611343384, "metricx_qe_score": 3.064974546432495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "位置性简单来说,是人们由于其人口统计特征、身份认同和生活经历而形成的视角。", "metrics": {"bleu_score": 27.541393673641654, "chrf_score": 28.901613513262653, "xcomet_score": 0.8104885816574097, "xcomet_qe_score": 0.8310976028442383, "metricx_score": 4.914600372314453, "metricx_qe_score": 3.6424448490142822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判研究中广泛使用的概念,尤其是在女性主义和酷儿学术领域。", "metrics": {"bleu_score": 68.10918846233905, "chrf_score": 61.76509153692008, "xcomet_score": 0.9806349277496338, "xcomet_qe_score": 0.9153404235839844, "metricx_score": 0.9995423555374146, "metricx_qe_score": 1.1851648092269897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为一名研究人员,位置性会影响研究过程及其结果,因为它会改变研究人员的决策。 那", "metrics": {"bleu_score": 46.25004030639679, "chrf_score": 40.48077933697796, "xcomet_score": 0.7211958169937134, "xcomet_qe_score": 0.7018540501594543, "metricx_score": 6.087926864624023, "metricx_qe_score": 3.0562491416931152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "么,一个可能提出的问题是:数据集和模型是否具有位置性?", "metrics": {"bleu_score": 32.1500044827898, "chrf_score": 29.241458289326804, "xcomet_score": 0.6446365118026733, "xcomet_qe_score": 0.6834719777107239, "metricx_score": 5.050858497619629, "metricx_qe_score": 3.3981881141662598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并非试图断言模型和数据集本身具有人口统计特征和生活经历,但它们确实聚合了真实人们的判断和意见,因此可能代表其他群体之外的", "metrics": {"bleu_score": 44.87192419234188, "chrf_score": 37.908493038886235, "xcomet_score": 0.7157024145126343, "xcomet_qe_score": 0.7647693157196045, "metricx_score": 5.097141742706299, "metricx_qe_score": 3.6234371662139893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特定位置性。 先前的研究已经提出了一些关于位置性的轶事证据,例如模型和数据集中的文化差异,以及对模型位置性的理论定义。", "metrics": {"bleu_score": 40.83122641852441, "chrf_score": 38.170323127829775, "xcomet_score": 0.5161272883415222, "xcomet_qe_score": 0.39981916546821594, "metricx_score": 6.211006164550781, "metricx_qe_score": 5.578559398651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些研究并没有将最终用户与数据集和模型本身进行比较。 研究模型和数据集的位置性越来越重要,因为NLP任务变得更加主观和面向社会。由于并非所有决策都有记录,并且许多模型隐藏在API后面,因此很难描述这些位置性是如何扭曲的。", "metrics": {"bleu_score": 53.128943259294125, "chrf_score": 47.336056499820536, "xcomet_score": 0.6425530910491943, "xcomet_qe_score": 0.7074846625328064, "metricx_score": 5.557275295257568, "metricx_qe_score": 5.256813049316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究数据集和模型的位置性,我们实际上将真实用户批注与现有数据集和模型进行比较。", "metrics": {"bleu_score": 54.330540558331606, "chrf_score": 47.02151663929119, "xcomet_score": 0.8118879795074463, "xcomet_qe_score": 0.8519039154052734, "metricx_score": 4.006410598754883, "metricx_qe_score": 3.744528293609619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架NLPositionality来实现这一目标。", "metrics": {"bleu_score": 19.672746885239153, "chrf_score": 57.98087594428076, "xcomet_score": 0.9535595178604126, "xcomet_qe_score": 0.9277148246765137, "metricx_score": 1.0269672870635986, "metricx_qe_score": 1.6372015476226807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是使用多元批注者重新批注数据集。", "metrics": {"bleu_score": 38.219190614703365, "chrf_score": 31.22092257366339, "xcomet_score": 0.7851161956787109, "xcomet_qe_score": 0.8080988526344299, "metricx_score": 1.7106282711029053, "metricx_qe_score": 0.9340540766716003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集和批注者的统计信息,因为通常每个实例只有少数批注者进行批注,而且统计信息很少被收集和共享。", "metrics": {"bleu_score": 41.92138157261985, "chrf_score": 36.10114115958632, "xcomet_score": 0.7481052875518799, "xcomet_qe_score": 0.733965277671814, "metricx_score": 2.3478574752807617, "metricx_qe_score": 1.7800153493881226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新批注数据,以获得每个实例的多个批注者,并获得丰富的统计数据。", "metrics": {"bleu_score": 26.805674929514176, "chrf_score": 24.209651892746383, "xcomet_score": 0.7755916714668274, "xcomet_qe_score": 0.8083592653274536, "metricx_score": 3.263618230819702, "metricx_qe_score": 2.3167028427124023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们根据统计数据对批注进行分析,并使用 Parsons R 相关性得分与模型和数据集进行比较。因此,我们的框架不同于批注者意见不一致的文献,它将最终用户与模型和数据集的预测和标签进行比较,而不是仅关注批注者的一致性或建模批注者分布。", "metrics": {"bleu_score": 44.26526466801844, "chrf_score": 37.74924834411993, "xcomet_score": 0.7158856391906738, "xcomet_qe_score": 0.5576804876327515, "metricx_score": 4.028349876403809, "metricx_qe_score": 4.153912544250488, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在很大程度上得益于Lab", "metrics": {"bleu_score": 11.248711960329182, "chrf_score": 11.22659009239809, "xcomet_score": 0.2406601756811142, "xcomet_qe_score": 0.28404706716537476, "metricx_score": 13.708235740661621, "metricx_qe_score": 11.008255004882812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "in the Wild,这是一个在线实验平台,我们可以在其中招募各种各样的志愿者,", "metrics": {"bleu_score": 61.89597746121735, "chrf_score": 63.7548734718787, "xcomet_score": 0.875845193862915, "xcomet_qe_score": 0.79508376121521, "metricx_score": 4.1971964836120605, "metricx_qe_score": 6.412353038787842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与MTurk平台相比,后者主要有来自美国或印度的参与者。此外,Lab in the Wild仍然能够获取高质量的数据。", "metrics": {"bleu_score": 49.50134096693479, "chrf_score": 53.25360715952642, "xcomet_score": 0.8093564510345459, "xcomet_qe_score": 0.6800693273544312, "metricx_score": 1.4559433460235596, "metricx_qe_score": 1.5469775199890137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Lab in the Wild上托管了两个任务,其中一个就是社会可接受度。其工作方式是,参与者将阅读来自社会化学数据集的场景,然后他们将写下该场景的社会可接受程度。", "metrics": {"bleu_score": 50.02051318953672, "chrf_score": 47.50118445645485, "xcomet_score": 0.7298232316970825, "xcomet_qe_score": 0.6169308423995972, "metricx_score": 2.4199585914611816, "metricx_qe_score": 1.9769504070281982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持对研究的参与度,他们可以将其回复与人工智能和其他人进行比较。", "metrics": {"bleu_score": 32.965803420548404, "chrf_score": 26.28099828679086, "xcomet_score": 0.9135782718658447, "xcomet_qe_score": 0.9654629230499268, "metricx_score": 1.1720434427261353, "metricx_qe_score": 1.1538621187210083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些批注与Social Chemistry Delphi和GPT 4进行比较。", "metrics": {"bleu_score": 39.26986511088296, "chrf_score": 45.14007589805817, "xcomet_score": 0.8863638639450073, "xcomet_qe_score": 0.9091543555259705, "metricx_score": 4.174635887145996, "metricx_qe_score": 3.3973751068115234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还针对毒性和仇恨言论检测任务,复制了一个非常相似的设置,他们将从DynaHate中阅读一个实例,并判断它是否是仇恨言论的一个实例。", "metrics": {"bleu_score": 48.01238613838579, "chrf_score": 40.84693502498994, "xcomet_score": 0.8390086889266968, "xcomet_qe_score": 0.8284910917282104, "metricx_score": 3.010589838027954, "metricx_qe_score": 3.0145742893218994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些批注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT 4进行比较。", "metrics": {"bleu_score": 63.76284883862137, "chrf_score": 79.33946158312655, "xcomet_score": 0.7891971468925476, "xcomet_qe_score": 0.7769129276275635, "metricx_score": 1.2839100360870361, "metricx_qe_score": 1.7746020555496216, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最终,我们的研究积累了来自八十七个国家/地区的上千名批注者提供的超过一万六千个批注。", "metrics": {"bleu_score": 16.088986597586107, "chrf_score": 16.319629772372586, "xcomet_score": 0.971983790397644, "xcomet_qe_score": 0.9853726625442505, "metricx_score": 1.9139840602874756, "metricx_qe_score": 1.0242589712142944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们更善于回答“NLP数据集和模型与谁最一致?”的问题。", "metrics": {"bleu_score": 26.66090188234886, "chrf_score": 29.852086121541095, "xcomet_score": 0.835189700126648, "xcomet_qe_score": 0.8521338701248169, "metricx_score": 1.9991532564163208, "metricx_qe_score": 1.504233717918396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现NLP中存在位置性。", "metrics": {"bleu_score": 24.973194725900534, "chrf_score": 27.290934871546728, "xcomet_score": 0.8341923952102661, "xcomet_qe_score": 0.8352224826812744, "metricx_score": 3.7918624877929688, "metricx_qe_score": 2.050662040710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型与英语国家最为一致。", "metrics": {"bleu_score": 53.3659206999951, "chrf_score": 47.22180825252165, "xcomet_score": 0.9757298231124878, "xcomet_qe_score": 0.896285891532898, "metricx_score": 0.9079480171203613, "metricx_qe_score": 1.149339199066162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于GPT 4社会可接受度分析,我们发现它与儒家文化和英语国家最为一致。我们", "metrics": {"bleu_score": 68.33681470081257, "chrf_score": 69.49269025125612, "xcomet_score": 0.797946572303772, "xcomet_qe_score": 0.756891131401062, "metricx_score": 4.455197334289551, "metricx_qe_score": 2.2422215938568115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还发现DynaHate也与英语国家最为一致。", "metrics": {"bleu_score": 18.949214849329618, "chrf_score": 32.226600108851414, "xcomet_score": 0.8883398771286011, "xcomet_qe_score": 0.8806337118148804, "metricx_score": 3.530722141265869, "metricx_qe_score": 3.772970199584961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现与受过大学教育的人群存在额外的联系。", "metrics": {"bleu_score": 31.899070626327212, "chrf_score": 26.740538909234946, "xcomet_score": 0.7821794748306274, "xcomet_qe_score": 0.8517741560935974, "metricx_score": 3.4224560260772705, "metricx_qe_score": 2.4561803340911865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于GPT 4在社会可接受度任务中,我们发现它与受过大学或研究生教育的人群最为一致。我们还发现DynaHate与受过大学教育的人群存在相同的情况。", "metrics": {"bleu_score": 54.97392444590378, "chrf_score": 49.7480662555631, "xcomet_score": 0.8357136845588684, "xcomet_qe_score": 0.7187381386756897, "metricx_score": 3.081287145614624, "metricx_qe_score": 4.003281116485596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群一致时,一些人群不可避免地会被遗忘。一个例子", "metrics": {"bleu_score": 55.14760049160392, "chrf_score": 52.64211492889055, "xcomet_score": 0.7554486989974976, "xcomet_qe_score": 0.7198017835617065, "metricx_score": 3.186537504196167, "metricx_qe_score": 4.188216209411621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,数据集和模型与非二元性别的人群相比,与他们的男性和女性同伴联系较少。", "metrics": {"bleu_score": 29.992706358334136, "chrf_score": 28.922565726662164, "xcomet_score": 0.6023479700088501, "xcomet_qe_score": 0.6009721755981445, "metricx_score": 6.558387279510498, "metricx_qe_score": 5.827014446258545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT 4社会可接受度任务以及DynaHate任务分析中都发现了这一点。", "metrics": {"bleu_score": 70.91299972295235, "chrf_score": 69.656705175281, "xcomet_score": 0.8549742698669434, "xcomet_qe_score": 0.9216338396072388, "metricx_score": 1.1809900999069214, "metricx_qe_score": 1.970129370689392, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于NLP中存在位置性,我们该怎么办?", "metrics": {"bleu_score": 13.985017692679852, "chrf_score": 20.17958738671278, "xcomet_score": 0.8668230772018433, "xcomet_qe_score": 0.8334845304489136, "metricx_score": 4.000497341156006, "metricx_qe_score": 2.4087302684783936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们有一些建议。", "metrics": {"bleu_score": 69.89307622784945, "chrf_score": 64.68795093795093, "xcomet_score": 0.9960454702377319, "xcomet_qe_score": 0.9742950201034546, "metricx_score": 0.21420864760875702, "metricx_qe_score": 0.32050344347953796, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一条是记录研究过程中所有相关的设计选择。第二条", "metrics": {"bleu_score": 38.153321561173016, "chrf_score": 30.148544725263115, "xcomet_score": 0.7891107201576233, "xcomet_qe_score": 0.6926069259643555, "metricx_score": 4.9880242347717285, "metricx_qe_score": 2.4010255336761475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是通过视角主义的视角进行NLP研究。", "metrics": {"bleu_score": 11.09254338857922, "chrf_score": 10.943063114294981, "xcomet_score": 0.8069300651550293, "xcomet_qe_score": 0.7963575124740601, "metricx_score": 3.1721444129943848, "metricx_qe_score": 3.320544958114624, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三条建议是为特定的社区构建专门的数据集和模型。", "metrics": {"bleu_score": 59.33751282924926, "chrf_score": 52.43409973685317, "xcomet_score": 0.9631797075271606, "xcomet_qe_score": 0.9576936960220337, "metricx_score": 1.2883853912353516, "metricx_qe_score": 1.9681096076965332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane倡议。", "metrics": {"bleu_score": 55.2058197664637, "chrf_score": 43.77089436218316, "xcomet_score": 0.7475622892379761, "xcomet_qe_score": 0.789522647857666, "metricx_score": 2.372803211212158, "metricx_qe_score": 4.278580188751221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们想强调的是,具有包容性的NLP不仅仅是让所有", "metrics": {"bleu_score": 62.55340042200862, "chrf_score": 64.32260387110105, "xcomet_score": 0.6545026302337646, "xcomet_qe_score": 0.4407769441604614, "metricx_score": 5.702530860900879, "metricx_qe_score": 4.362239837646484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为每个人服务。", "metrics": {"bleu_score": 13.927628237847681, "chrf_score": 14.6579211560759, "xcomet_score": 0.9458763599395752, "xcomet_qe_score": 0.9540870189666748, "metricx_score": 0.7448137402534485, "metricx_qe_score": 1.026949167251587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,感谢您的聆听。", "metrics": {"bleu_score": 4.990049701936832, "chrf_score": 3.3333333333333335, "xcomet_score": 0.6005764603614807, "xcomet_qe_score": 0.6195065975189209, "metricx_score": 0.6153199672698975, "metricx_qe_score": 0.7302796244621277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多信息,请随时查看我们的仪表板,以获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 61.571341865327085, "chrf_score": 55.730061401700226, "xcomet_score": 0.9791685342788696, "xcomet_qe_score": 0.9482947587966919, "metricx_score": 0.5839754343032837, "metricx_qe_score": 0.5440481901168823, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是范大学的席源。", "metrics": {"bleu_score": 17.96602518105432, "chrf_score": 13.598479441944916, "xcomet_score": 0.6791212558746338, "xcomet_qe_score": 0.6930469274520874, "metricx_score": 5.081857681274414, "metricx_qe_score": 5.50221061706543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我今天来介绍我们关于从大型语言模型中提取特定脚本知识,以用于受约束的语言规划方面的工作。", "metrics": {"bleu_score": 30.761179321843198, "chrf_score": 30.13450877263336, "xcomet_score": 0.8466261625289917, "xcomet_qe_score": 0.7394635081291199, "metricx_score": 1.0482189655303955, "metricx_qe_score": 1.44344162940979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类经常通过遵循分步骤的指示来规划他们的行动,这些指示以保证的脚本形式呈现。先", "metrics": {"bleu_score": 19.074070294907216, "chrf_score": 21.62380683634606, "xcomet_score": 0.7521705627441406, "xcomet_qe_score": 0.8062264919281006, "metricx_score": 6.495656967163086, "metricx_qe_score": 3.80372953414917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "前的研究探索了语言模型来规划抽象目标,例如制作蛋糕等刻板活动,并", "metrics": {"bleu_score": 36.82794970421355, "chrf_score": 29.93293976944793, "xcomet_score": 0.4471270442008972, "xcomet_qe_score": 0.4128674864768982, "metricx_score": 5.385167121887207, "metricx_qe_score": 2.4897077083587646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表明大型语言模型可以有效地将目标分解为步骤。", "metrics": {"bleu_score": 58.20282060729978, "chrf_score": 57.543145084254874, "xcomet_score": 0.8559707403182983, "xcomet_qe_score": 0.9233258962631226, "metricx_score": 1.305349588394165, "metricx_qe_score": 0.9224607348442078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,先前的研究主要集中于规划抽象目标的刻板活动。规划具有特定目标", "metrics": {"bleu_score": 39.162127614717114, "chrf_score": 39.277455228761426, "xcomet_score": 0.6903585195541382, "xcomet_qe_score": 0.6933097839355469, "metricx_score": 6.049743175506592, "metricx_qe_score": 4.780380725860596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "和特定约束的目标,例如制作巧克力蛋糕,仍然鲜为人知。", "metrics": {"bleu_score": 18.350169368096974, "chrf_score": 17.268220924049768, "xcomet_score": 0.6256356239318848, "xcomet_qe_score": 0.5510141849517822, "metricx_score": 6.5135579109191895, "metricx_qe_score": 6.10923957824707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们定义了受约束的语言规划问题,该问题对规划目标施加不同的约束。", "metrics": {"bleu_score": 63.48035387433741, "chrf_score": 52.47499098602041, "xcomet_score": 0.8917011022567749, "xcomet_qe_score": 0.8807745575904846, "metricx_score": 1.8288917541503906, "metricx_qe_score": 2.408740282058716, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被不同的现实生活中的特定目标继承,这些目标具有更困难", "metrics": {"bleu_score": 31.066878208732792, "chrf_score": 30.372150266169413, "xcomet_score": 0.5498478412628174, "xcomet_qe_score": 0.6912672519683838, "metricx_score": 7.914228439331055, "metricx_qe_score": 4.546091556549072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "、多方面的约束。一个好的规划者应该编写符合约束条件且合理的脚本。", "metrics": {"bleu_score": 30.935276471311436, "chrf_score": 27.02042012695238, "xcomet_score": 0.5743646621704102, "xcomet_qe_score": 0.30035367608070374, "metricx_score": 4.575529098510742, "metricx_qe_score": 5.649383544921875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估和改进大型语言模型受约束的语言规划能力。", "metrics": {"bleu_score": 51.8917341258404, "chrf_score": 42.98549170456052, "xcomet_score": 0.9022821187973022, "xcomet_qe_score": 0.8563162088394165, "metricx_score": 0.8850276470184326, "metricx_qe_score": 1.0042004585266113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有特定目标的可用数据集来支持我们的研究,我们必须首先获取这些目标。", "metrics": {"bleu_score": 71.31992424981136, "chrf_score": 63.93967916263931, "xcomet_score": 0.8514014482498169, "xcomet_qe_score": 0.8329439163208008, "metricx_score": 1.7441585063934326, "metricx_qe_score": 3.2971911430358887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们使用指令式 TPT 扩展了具有多方面约束的抽象目标,以进行人工回路数据采集。", "metrics": {"bleu_score": 35.66221154397061, "chrf_score": 24.68432142336265, "xcomet_score": 0.7260732650756836, "xcomet_qe_score": 0.7281250953674316, "metricx_score": 6.3887104988098145, "metricx_qe_score": 6.073602199554443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们抽取了100个特定目标,并评估了 Light Logic 模型生成的脚本。", "metrics": {"bleu_score": 55.530619139551774, "chrf_score": 48.394409887876286, "xcomet_score": 0.8094727993011475, "xcomet_qe_score": 0.8241497874259949, "metricx_score": 3.9094386100769043, "metricx_qe_score": 4.419721603393555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这张表报告了结果的总体准确性。", "metrics": {"bleu_score": 49.35578819979934, "chrf_score": 37.421744921744924, "xcomet_score": 0.9416476488113403, "xcomet_qe_score": 0.9288903474807739, "metricx_score": 0.7632449865341187, "metricx_qe_score": 0.8806709051132202, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所有 Light Logic 模型在规划特定目标方面都取得了不令人满意的结果。", "metrics": {"bleu_score": 31.417279447572707, "chrf_score": 26.138898798305537, "xcomet_score": 0.8534305095672607, "xcomet_qe_score": 0.8460961580276489, "metricx_score": 3.7712128162384033, "metricx_qe_score": 4.165505409240723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析以调查 Light Logic 模型在哪些方面存在不足。", "metrics": {"bleu_score": 20.284861347786237, "chrf_score": 17.078208800183102, "xcomet_score": 0.8631344437599182, "xcomet_qe_score": 0.803214967250824, "metricx_score": 2.287642240524292, "metricx_qe_score": 2.1400880813598633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图表结果显示,生成的脚本中的语义完整性是可以接受的。但无法保证对约束的忠实性。", "metrics": {"bleu_score": 32.993993631195, "chrf_score": 30.66402473750634, "xcomet_score": 0.9491739273071289, "xcomet_qe_score": 0.8878265023231506, "metricx_score": 1.5982375144958496, "metricx_qe_score": 1.943526029586792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了更细粒度的约束主题类别,", "metrics": {"bleu_score": 24.877014419409257, "chrf_score": 18.029161655321236, "xcomet_score": 0.7548048496246338, "xcomet_qe_score": 0.7055883407592773, "metricx_score": 4.008012294769287, "metricx_qe_score": 5.212559223175049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "具体取决于唤醒家园。图中的头部地图显示,对于不同类别的目标,Instruct GPT 的规划性能差异很大。", "metrics": {"bleu_score": 24.326794743855366, "chrf_score": 48.574637379250184, "xcomet_score": 0.279045969247818, "xcomet_qe_score": 0.0576934814453125, "metricx_score": 8.20224666595459, "metricx_qe_score": 9.694162368774414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "先前的研究表明,Larry 模型的输出质量存在高度方差,导致性能不佳。", "metrics": {"bleu_score": 41.01896114451473, "chrf_score": 34.08052271718549, "xcomet_score": 0.7772971391677856, "xcomet_qe_score": 0.722399115562439, "metricx_score": 5.478250503540039, "metricx_qe_score": 6.127868175506592, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了超生成 Zen 过滤器(overgenerated Zen filter)的想法来提高生成质量。", "metrics": {"bleu_score": 37.8878212033196, "chrf_score": 30.550769143205752, "xcomet_score": 0.767851710319519, "xcomet_qe_score": 0.8055031299591064, "metricx_score": 7.727263450622559, "metricx_qe_score": 7.34688663482666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示了 Instruct GPT 的约束类型及其示例,并基于设定的抽象目标获得了特定目标。接下", "metrics": {"bleu_score": 31.404161351558688, "chrf_score": 44.10023843210217, "xcomet_score": 0.7650774717330933, "xcomet_qe_score": 0.7723124027252197, "metricx_score": 4.455819129943848, "metricx_qe_score": 4.358695030212402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来,Instruct GPT 为特定目标生成超量关键脚本。", "metrics": {"bleu_score": 11.750296943620288, "chrf_score": 40.62209874835132, "xcomet_score": 0.4099291265010834, "xcomet_qe_score": 0.5337376594543457, "metricx_score": 4.488254070281982, "metricx_qe_score": 4.499600410461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,开发了一个过滤模型来选择符合要求的脚本。", "metrics": {"bleu_score": 52.34319858442513, "chrf_score": 47.999266860509955, "xcomet_score": 0.9964978694915771, "xcomet_qe_score": 0.9849511384963989, "metricx_score": 0.7927648425102234, "metricx_qe_score": 0.7788066864013672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为 Instruct GPT 的比特模式,并计算余弦相似度和相似度分数以衡量语义相似度。", "metrics": {"bleu_score": 65.76250895254542, "chrf_score": 69.69315755904582, "xcomet_score": 0.8637008666992188, "xcomet_qe_score": 0.8135637044906616, "metricx_score": 2.111842393875122, "metricx_qe_score": 2.466758966445923, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们还将编写包含目标约束关键词的脚本。", "metrics": {"bleu_score": 55.43852213695471, "chrf_score": 51.60097221908482, "xcomet_score": 0.8061763048171997, "xcomet_qe_score": 0.7592930197715759, "metricx_score": 6.371310234069824, "metricx_qe_score": 6.708168029785156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "仅当目标分数在目标站点中排名最高时,我们才会保留该脚本。", "metrics": {"bleu_score": 33.56891925037239, "chrf_score": 32.04303135261424, "xcomet_score": 0.7815079689025879, "xcomet_qe_score": 0.6826819777488708, "metricx_score": 4.528386116027832, "metricx_qe_score": 5.132120132446289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,Inslacity 可以生成更高质量的脚本。", "metrics": {"bleu_score": 81.37489370974959, "chrf_score": 53.69611084549407, "xcomet_score": 0.7949438095092773, "xcomet_qe_score": 0.799435019493103, "metricx_score": 5.270537376403809, "metricx_qe_score": 5.974236965179443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法在语义完整性和对约束的忠实性方面都大大提高了可规划性。", "metrics": {"bleu_score": 67.97839443148857, "chrf_score": 62.48447294869278, "xcomet_score": 0.8785432577133179, "xcomet_qe_score": 0.9181942939758301, "metricx_score": 1.3441126346588135, "metricx_qe_score": 1.789588451385498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高昂,因此启用较小和专业化模型的语言规划能力至关重要。", "metrics": {"bleu_score": 58.21659511705258, "chrf_score": 50.68532818532818, "xcomet_score": 0.9254992008209229, "xcomet_qe_score": 0.9704265594482422, "metricx_score": 0.8304018974304199, "metricx_qe_score": 1.0697187185287476, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的关键一步。", "metrics": {"bleu_score": 65.52927589468493, "chrf_score": 63.56433655580578, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03935346379876137, "metricx_qe_score": 0.1495152711868286, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,先前的研究并没有实现对特定目标的规划,并且手动数据集注释成本高昂。", "metrics": {"bleu_score": 27.11685501491908, "chrf_score": 22.93129402671114, "xcomet_score": 0.9711613655090332, "xcomet_qe_score": 0.9111267924308777, "metricx_score": 1.3201802968978882, "metricx_qe_score": 1.7400981187820435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循知识蒸馏(symbolic knowledge distillation)的思想,将受约束的语言规划模型进行蒸馏。", "metrics": {"bleu_score": 22.24091318987057, "chrf_score": 16.280466212617064, "xcomet_score": 0.744943380355835, "xcomet_qe_score": 0.749144434928894, "metricx_score": 7.647735118865967, "metricx_qe_score": 6.749509811401367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将我们的方法应用于构建一个名为 CodeScript 的受约束语言规划数据集。", "metrics": {"bleu_score": 56.973519250034144, "chrf_score": 54.176529260530245, "xcomet_score": 0.9316116571426392, "xcomet_qe_score": 0.8899569511413574, "metricx_score": 0.8572220802307129, "metricx_qe_score": 1.4653114080429077, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了五万五千个特定目标和", "metrics": {"bleu_score": 11.19644153835206, "chrf_score": 10.783756468765313, "xcomet_score": 0.7602124214172363, "xcomet_qe_score": 0.7901801466941833, "metricx_score": 6.038331031799316, "metricx_qe_score": 2.9471662044525146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "脚本,以确保验证和测试站点的质量。我们请云端众包工人查找和修改不正确的样本。", "metrics": {"bleu_score": 23.09503728212329, "chrf_score": 22.440641567944162, "xcomet_score": 0.5382281541824341, "xcomet_qe_score": 0.42556658387184143, "metricx_score": 5.390085220336914, "metricx_qe_score": 5.694687843322754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了 CodeScript 的受约束分布。", "metrics": {"bleu_score": 49.73567356124543, "chrf_score": 56.4277882188469, "xcomet_score": 0.9045248031616211, "xcomet_qe_score": 0.8640773296356201, "metricx_score": 1.7082079648971558, "metricx_qe_score": 3.04598331451416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 CodeScript 表现出生成的特定目标的假设。", "metrics": {"bleu_score": 19.209900930203556, "chrf_score": 28.27048304321841, "xcomet_score": 0.7130622863769531, "xcomet_qe_score": 0.6559504270553589, "metricx_score": 8.20101547241211, "metricx_qe_score": 9.12108039855957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "借助 CodeScript,我们可以训练更小但专门的模型来进行受约束的语言规划。", "metrics": {"bleu_score": 30.39810513723352, "chrf_score": 31.552592963116815, "xcomet_score": 0.9672218561172485, "xcomet_qe_score": 0.9415971040725708, "metricx_score": 1.8944947719573975, "metricx_qe_score": 2.2449309825897217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在适合的数据站点上进行适当训练时,TFIL 函数在代码速率上可以生成比大多数大型语言模型更高质量的脚本,这表明较小的模型可以支持较大的模型。", "metrics": {"bleu_score": 47.85546794152832, "chrf_score": 37.48847115422998, "xcomet_score": 0.5209911465644836, "xcomet_qe_score": 0.456993967294693, "metricx_score": 8.978663444519043, "metricx_qe_score": 8.994806289672852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了受约束的语言规划问题。", "metrics": {"bleu_score": 56.82854869630478, "chrf_score": 47.70806714498665, "xcomet_score": 0.9043223857879639, "xcomet_qe_score": 0.8694943785667419, "metricx_score": 1.6420196294784546, "metricx_qe_score": 2.4267990589141846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的受约束语言规划能力,并为大型语言模型开发了一种超生成过滤器方法。", "metrics": {"bleu_score": 53.390445340437374, "chrf_score": 44.79797514876462, "xcomet_score": 0.8117240071296692, "xcomet_qe_score": 0.8164181709289551, "metricx_score": 3.3039116859436035, "metricx_qe_score": 3.8244454860687256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用大型语言模型生成了一个高质量的脚本数据集 CodeScript,用于建设性的语言规划。", "metrics": {"bleu_score": 45.090045716114, "chrf_score": 48.36861746295454, "xcomet_score": 0.8313078880310059, "xcomet_qe_score": 0.7717878818511963, "metricx_score": 3.3484389781951904, "metricx_qe_score": 4.226832389831543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 CodeScript 数据集可以成为推进语言规划研究的宝贵资源。", "metrics": {"bleu_score": 74.32254773174189, "chrf_score": 77.35500613978323, "xcomet_score": 0.9735298156738281, "xcomet_qe_score": 0.9768041372299194, "metricx_score": 0.7935215830802917, "metricx_qe_score": 1.0019862651824951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。", "metrics": {"bleu_score": 20.95871245288356, "chrf_score": 18.846321407177477, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.2288123369216919, "metricx_qe_score": 0.6436101198196411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请在我们的论文中了解更多 CodeScript 的细节。", "metrics": {"bleu_score": 38.208031388588324, "chrf_score": 43.96968883322931, "xcomet_score": 0.9772907495498657, "xcomet_qe_score": 0.967516303062439, "metricx_score": 1.779172420501709, "metricx_qe_score": 1.7430704832077026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫徐恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8726203441619873, "xcomet_qe_score": 0.8842723965644836, "metricx_score": 0.0, "metricx_qe_score": 0.11707936227321625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们的论文《Kernel 2003命名实体标注器在2023年是否仍然有效?》。", "metrics": {"bleu_score": 79.79296449238733, "chrf_score": 73.87284314272114, "xcomet_score": 0.8224408626556396, "xcomet_qe_score": 0.7638105154037476, "metricx_score": 3.2383666038513184, "metricx_qe_score": 1.8600168228149414, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.997875452041626, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了泛化的问题,以命名实体识别任务或NER任务为例。", "metrics": {"bleu_score": 47.23143664700321, "chrf_score": 42.754083147114464, "xcomet_score": 0.9058918952941895, "xcomet_qe_score": 0.8397417068481445, "metricx_score": 1.3046749830245972, "metricx_qe_score": 3.095111608505249, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,模型已经使用了Kernel 2003来开发NER近20年。这自然会带来几个问题。", "metrics": {"bleu_score": 30.564141221416776, "chrf_score": 28.004993880387165, "xcomet_score": 0.7768886089324951, "xcomet_qe_score": 0.8082518577575684, "metricx_score": 6.883213996887207, "metricx_qe_score": 5.640631198883057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据?", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9989852905273438, "xcomet_qe_score": 0.9952034950256348, "metricx_score": 0.28112250566482544, "metricx_qe_score": 0.2982765734195709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在开发新的标注器时,什么是有利于良好泛化的因素?", "metrics": {"bleu_score": 24.789247697850264, "chrf_score": 21.9242594835598, "xcomet_score": 0.9903386831283569, "xcomet_qe_score": 0.913852334022522, "metricx_score": 1.7894680500030518, "metricx_qe_score": 3.262637138366699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果观察到泛化效果不佳,是什么原因导致这些模型的性能下降?", "metrics": {"bleu_score": 26.407656179140996, "chrf_score": 22.860327942455097, "xcomet_score": 0.9768894910812378, "xcomet_qe_score": 0.968377947807312, "metricx_score": 1.032523274421692, "metricx_qe_score": 0.9106711745262146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了调查这些问题,我们开发了Kernel++数据集。这是", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 48.40333726150651, "xcomet_score": 0.7100203037261963, "xcomet_qe_score": 0.7446427345275879, "metricx_score": 6.594837188720703, "metricx_qe_score": 4.8024396896362305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从路透新闻中收集的数据集,并使用相同的Kernel 2003标注指南进行了标注。", "metrics": {"bleu_score": 36.98057614732924, "chrf_score": 28.59476485919891, "xcomet_score": 0.6455786228179932, "xcomet_qe_score": 0.6351544260978699, "metricx_score": 7.218447208404541, "metricx_qe_score": 7.131158351898193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们在Kernel 2003上微调了超过20个模型。", "metrics": {"bleu_score": 12.978745927940972, "chrf_score": 20.424570100552483, "xcomet_score": 0.791171669960022, "xcomet_qe_score": 0.8024033904075623, "metricx_score": 4.70237922668457, "metricx_qe_score": 2.800915002822876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Conor 3测试集和Conor++测试集对它们进行了评估。", "metrics": {"bleu_score": 46.27620794967283, "chrf_score": 41.20326430220271, "xcomet_score": 0.7336709499359131, "xcomet_qe_score": 0.8258830904960632, "metricx_score": 5.469273567199707, "metricx_qe_score": 4.368030071258545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们计算了F1值的百分比变化,以评估每个模型的泛化程度。", "metrics": {"bleu_score": 63.05914424660905, "chrf_score": 58.92469995094426, "xcomet_score": 0.9959032535552979, "xcomet_qe_score": 0.9911141395568848, "metricx_score": 0.42317014932632446, "metricx_qe_score": 0.7078182697296143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,什么是有利于良好泛化的因素?", "metrics": {"bleu_score": 9.42119686197517, "chrf_score": 12.990196078431374, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 1.034164547920227, "metricx_qe_score": 2.2024261951446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现有三个主要因素。", "metrics": {"bleu_score": 40.9961728958804, "chrf_score": 34.17508665672059, "xcomet_score": 0.968151330947876, "xcomet_qe_score": 0.881928563117981, "metricx_score": 0.9094090461730957, "metricx_qe_score": 1.3229708671569824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现Transformer模型通常能更好地泛化到新数据。", "metrics": {"bleu_score": 52.77140132412705, "chrf_score": 60.9534154761757, "xcomet_score": 0.8849719762802124, "xcomet_qe_score": 0.878563404083252, "metricx_score": 1.71575927734375, "metricx_qe_score": 3.442965269088745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个因素是模型大小。", "metrics": {"bleu_score": 74.26141117870938, "chrf_score": 66.70467087283252, "xcomet_score": 0.9924691915512085, "xcomet_qe_score": 0.9070494174957275, "metricx_score": 0.08909235894680023, "metricx_qe_score": 0.28823322057724, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现通常情况下,更大的模型会导致更好的泛化效果。", "metrics": {"bleu_score": 34.23375720396188, "chrf_score": 29.968633939225274, "xcomet_score": 0.9760254621505737, "xcomet_qe_score": 0.9712191224098206, "metricx_score": 0.6279543042182922, "metricx_qe_score": 0.8706506490707397, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们都知道,微调示例的数量直接影响下游任务的性能。", "metrics": {"bleu_score": 70.6458356129423, "chrf_score": 64.52762146602726, "xcomet_score": 0.9726814031600952, "xcomet_qe_score": 0.8158828020095825, "metricx_score": 1.3975343704223633, "metricx_qe_score": 1.2529382705688477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们同样发现,更多的微调示例实际上也能带来更好的泛化效果。", "metrics": {"bleu_score": 45.013565462807456, "chrf_score": 46.284049347090786, "xcomet_score": 0.9913667440414429, "xcomet_qe_score": 0.9267023205757141, "metricx_score": 0.4911069869995117, "metricx_qe_score": 0.5770595669746399, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们下一个问题,是什么导致一些模型的性能下降?我们提出了两个假设。", "metrics": {"bleu_score": 27.34333648877134, "chrf_score": 24.32282052130455, "xcomet_score": 0.9576047658920288, "xcomet_qe_score": 0.9673434495925903, "metricx_score": 0.8031140565872192, "metricx_qe_score": 0.8544782400131226, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,这是一种由重复使用相同的测试集引起的过拟合现象,通常表现为在新测试集上收益递减。", "metrics": {"bleu_score": 56.2398986939608, "chrf_score": 47.021731080968124, "xcomet_score": 0.9212506413459778, "xcomet_qe_score": 0.8370801210403442, "metricx_score": 2.2153842449188232, "metricx_qe_score": 2.6002869606018066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,这是由训练数据和测试数据之间时间差距增大而导致性能下降的现象。", "metrics": {"bleu_score": 59.819649172347496, "chrf_score": 55.33412901368918, "xcomet_score": 0.9584364891052246, "xcomet_qe_score": 0.8806316256523132, "metricx_score": 1.6988916397094727, "metricx_qe_score": 2.404979944229126, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于过拟合的数据,我们从右侧的图表上看到,最佳拟合线(红色)的梯度大于1。", "metrics": {"bleu_score": 40.09477611922472, "chrf_score": 36.4089638613308, "xcomet_score": 0.9145326614379883, "xcomet_qe_score": 0.8523762226104736, "metricx_score": 0.8798639178276062, "metricx_qe_score": 1.3388844728469849, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在Kernel 2003上取得的每一点改进,在Kernel++上都能带来超过一点的改进,这意味着没有收益递减,", "metrics": {"bleu_score": 35.32929404481947, "chrf_score": 27.875087506357694, "xcomet_score": 0.8502351641654968, "xcomet_qe_score": 0.8585475087165833, "metricx_score": 4.266683101654053, "metricx_qe_score": 3.9393208026885986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明自适应过拟合在本例中没有被观察到。", "metrics": {"bleu_score": 28.252898813009548, "chrf_score": 24.879295550826594, "xcomet_score": 0.9165469408035278, "xcomet_qe_score": 0.9101139307022095, "metricx_score": 1.0720148086547852, "metricx_qe_score": 1.3411009311676025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间漂移呢?", "metrics": {"bleu_score": 52.47357977607325, "chrf_score": 40.51960415097498, "xcomet_score": 0.9311673641204834, "xcomet_qe_score": 0.9159005284309387, "metricx_score": 0.3731555640697479, "metricx_qe_score": 0.8887962102890015, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,我们进行了一个实验,使用更近期的数据重新训练或继续预训练一些模型,我们发现性能随着时间差距的增大而下降,这证实了我们假设,性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 56.623880526929234, "chrf_score": 49.517479998047, "xcomet_score": 0.9324320554733276, "xcomet_qe_score": 0.9339473247528076, "metricx_score": 1.4916939735412598, "metricx_qe_score": 1.8008620738983154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化效果,我们需要更好的模型架构、更大的模型大小以及更多的微调示例,", "metrics": {"bleu_score": 74.67014925961848, "chrf_score": 70.11526098665162, "xcomet_score": 0.9518448114395142, "xcomet_qe_score": 0.9047505259513855, "metricx_score": 0.39316636323928833, "metricx_qe_score": 0.3561282157897949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些因素是相互关联的。我们不能只保留一个因素而抛弃其他因素。", "metrics": {"bleu_score": 60.63726457410496, "chrf_score": 54.25741840109656, "xcomet_score": 0.9961686134338379, "xcomet_qe_score": 0.9921466112136841, "metricx_score": 0.5177558064460754, "metricx_qe_score": 0.7000347971916199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,这里的性能下降是由时间漂移引起的,而且出乎意料的是,它不是由自适应过拟合引起的,", "metrics": {"bleu_score": 41.97601894103735, "chrf_score": 34.63865605683568, "xcomet_score": 0.7558540105819702, "xcomet_qe_score": 0.6448982954025269, "metricx_score": 6.238683700561523, "metricx_qe_score": 7.942741394042969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管Kono 2003已经使用了超过20年。回到我们在论文标题中提出的问题:Kono 2003标注器在2023年是否仍然有效?", "metrics": {"bleu_score": 54.94991080816443, "chrf_score": 63.61341991762109, "xcomet_score": 0.3663475513458252, "xcomet_qe_score": 0.35423508286476135, "metricx_score": 4.676419258117676, "metricx_qe_score": 2.8582427501678467, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是肯定的。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 53.5017446130673, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43837279081344604, "metricx_qe_score": 0.7667475342750549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文呼吁更多关于如何改进模型泛化性的研究。", "metrics": {"bleu_score": 55.60335612120309, "chrf_score": 46.574749907148586, "xcomet_score": 0.8185793161392212, "xcomet_qe_score": 0.8451446294784546, "metricx_score": 1.3529133796691895, "metricx_qe_score": 1.14068603515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果您有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.11026448209409, "chrf_score": 52.5325928587273, "xcomet_score": 0.9871149063110352, "xcomet_qe_score": 0.9712950587272644, "metricx_score": 0.2757876515388489, "metricx_qe_score": 0.26652368903160095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将介绍我们关于解决实体选择中的间接指代表达式的研究,其中我们引入了altentity语料库。", "metrics": {"bleu_score": 29.730340529263756, "chrf_score": 25.436945386644737, "xcomet_score": 0.7996900081634521, "xcomet_qe_score": 0.7605140209197998, "metricx_score": 4.304348468780518, "metricx_qe_score": 5.126662731170654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是Javot Hosseini,这篇论文是与Philip Radlinsky、Silvia Pareti和Annie Luis合作完成的。", "metrics": {"bleu_score": 18.5031995613929, "chrf_score": 58.37990925973956, "xcomet_score": 0.8660444021224976, "xcomet_qe_score": 0.8086332082748413, "metricx_score": 3.8591930866241455, "metricx_qe_score": 3.2792067527770996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在需要做出选择时使用的语言。", "metrics": {"bleu_score": 54.16124426311167, "chrf_score": 48.1893217148739, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5472793579101562, "metricx_qe_score": 0.6545983552932739, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "考虑以下替代问题:", "metrics": {"bleu_score": 12.067008283523638, "chrf_score": 10.590656015492618, "xcomet_score": 0.8792711496353149, "xcomet_qe_score": 0.8627591133117676, "metricx_score": 0.3784177303314209, "metricx_qe_score": 0.2156527042388916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指《Easy on Me》还是《I Got a Feeling》?这里", "metrics": {"bleu_score": 11.7942240532671, "chrf_score": 46.91059440148832, "xcomet_score": 0.908374547958374, "xcomet_qe_score": 0.8999679088592529, "metricx_score": 5.0586957931518555, "metricx_qe_score": 2.2308847904205322, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用户想要在其中两个歌曲之间进行选择。", "metrics": {"bleu_score": 10.11945757660757, "chrf_score": 11.653101063067716, "xcomet_score": 0.9707081317901611, "xcomet_qe_score": 0.9614821672439575, "metricx_score": 0.9316248297691345, "metricx_qe_score": 0.6383300423622131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最明显的方法是使用直接引用,例如说歌曲的名称《Easy on Me》或者其位置,比如第一个,", "metrics": {"bleu_score": 39.74285318743446, "chrf_score": 42.69467166062221, "xcomet_score": 0.7279297113418579, "xcomet_qe_score": 0.6803809404373169, "metricx_score": 3.3390626907348633, "metricx_qe_score": 3.7109949588775635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但有时间接引用更适合进行更自然的对话。这种情况可能发生", "metrics": {"bleu_score": 48.632987321455005, "chrf_score": 54.77164439593356, "xcomet_score": 0.7289261817932129, "xcomet_qe_score": 0.7121425271034241, "metricx_score": 9.432313919067383, "metricx_qe_score": 8.644292831420898, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在用户无法记住歌曲的名称,或者", "metrics": {"bleu_score": 16.056103093302703, "chrf_score": 15.066419087551413, "xcomet_score": 0.7765301465988159, "xcomet_qe_score": 0.825934648513794, "metricx_score": 3.7020490169525146, "metricx_qe_score": 1.6201595067977905, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发音过于相似难以区分", "metrics": {"bleu_score": 13.448025110102005, "chrf_score": 16.434800009099643, "xcomet_score": 0.9314001798629761, "xcomet_qe_score": 0.959930419921875, "metricx_score": 2.1117687225341797, "metricx_qe_score": 1.410132646560669, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",或者用户想要表达偏好。", "metrics": {"bleu_score": 7.047440267847223, "chrf_score": 10.260317857497885, "xcomet_score": 0.9104403853416443, "xcomet_qe_score": 0.9857430458068848, "metricx_score": 3.139277935028076, "metricx_qe_score": 1.3340785503387451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些间接引用的例子。例如,“较新的那首”或“不是充满活力的那首歌”。这是一个", "metrics": {"bleu_score": 35.22562729144567, "chrf_score": 32.967362698273575, "xcomet_score": 0.7552529573440552, "xcomet_qe_score": 0.7720059752464294, "metricx_score": 7.150673866271973, "metricx_qe_score": 3.0810751914978027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在对话系统中以及用于基准测试大型语言模型(LLM)实体理解中的重要问题。我们", "metrics": {"bleu_score": 32.67108369199652, "chrf_score": 25.678800064891416, "xcomet_score": 0.6589536666870117, "xcomet_qe_score": 0.637992262840271, "metricx_score": 7.7385125160217285, "metricx_qe_score": 4.494750022888184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前没有发现任何公开的、大规模的用于此任务的数据集,因此我们使用众包标注收集了一个数据集。", "metrics": {"bleu_score": 34.855523705456775, "chrf_score": 32.59696145919504, "xcomet_score": 0.8723093867301941, "xcomet_qe_score": 0.8373113870620728, "metricx_score": 1.5094679594039917, "metricx_qe_score": 1.4845795631408691, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了音乐、书籍和食谱三个不同的领域。", "metrics": {"bleu_score": 78.3211592255058, "chrf_score": 68.21061547148504, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.21703281998634338, "metricx_qe_score": 0.35979121923446655, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用了卡通完成的形式。", "metrics": {"bleu_score": 60.01661228281378, "chrf_score": 51.737298932951106, "xcomet_score": 0.8343632221221924, "xcomet_qe_score": 0.8197185397148132, "metricx_score": 1.8924853801727295, "metricx_qe_score": 2.9052624702453613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "卡通中包含三个对话框。", "metrics": {"bleu_score": 46.17366309441026, "chrf_score": 38.621332371332365, "xcomet_score": 0.8033430576324463, "xcomet_qe_score": 0.7364175319671631, "metricx_score": 0.6562414169311523, "metricx_qe_score": 0.6345152854919434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个对话框中,Bob说:“还记得我们昨天听的那首歌吗?”", "metrics": {"bleu_score": 52.664038784792645, "chrf_score": 50.34493742621111, "xcomet_score": 0.8901082277297974, "xcomet_qe_score": 0.8560179471969604, "metricx_score": 1.3212940692901611, "metricx_qe_score": 1.1987736225128174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样,Bob就设置了对话语境。", "metrics": {"bleu_score": 45.823488902304994, "chrf_score": 49.37086802590768, "xcomet_score": 0.9729426503181458, "xcomet_qe_score": 0.9485011696815491, "metricx_score": 0.7808992266654968, "metricx_qe_score": 1.1087021827697754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话框中,Alice说:“你是指《Easy on Me》还是《I Got a Feeling》?”", "metrics": {"bleu_score": 17.11527994407391, "chrf_score": 44.295292444629304, "xcomet_score": 0.8859464526176453, "xcomet_qe_score": 0.873084306716919, "metricx_score": 2.8344686031341553, "metricx_qe_score": 2.68928861618042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是替代问题。", "metrics": {"bleu_score": 23.099966849728546, "chrf_score": 18.79627212007378, "xcomet_score": 0.8669623136520386, "xcomet_qe_score": 0.8554072380065918, "metricx_score": 1.420009732246399, "metricx_qe_score": 2.3006839752197266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话框中,Bob使用间接引用来选择其中一个实体,例如,“带有钢琴音乐的那首”。", "metrics": {"bleu_score": 39.64174776017769, "chrf_score": 40.2039454605754, "xcomet_score": 0.6985105276107788, "xcomet_qe_score": 0.6605889797210693, "metricx_score": 4.849701881408691, "metricx_qe_score": 5.221341609954834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话框,但第三个对话框由标注员填写。", "metrics": {"bleu_score": 69.09086323462154, "chrf_score": 62.86260906979017, "xcomet_score": 0.9080743193626404, "xcomet_qe_score": 0.7987481355667114, "metricx_score": 1.342389464378357, "metricx_qe_score": 1.3384054899215698, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话框从每个领域的手动提示中选择。", "metrics": {"bleu_score": 50.84094287050262, "chrf_score": 44.13557661196042, "xcomet_score": 0.7895546555519104, "xcomet_qe_score": 0.8255043029785156, "metricx_score": 1.0602625608444214, "metricx_qe_score": 1.7858009338378906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个对话框,即替代问题,按照以下方式生成:", "metrics": {"bleu_score": 9.449865252803164, "chrf_score": 11.444912581744983, "xcomet_score": 0.9000935554504395, "xcomet_qe_score": 0.8916897177696228, "metricx_score": 1.1216033697128296, "metricx_qe_score": 0.7126423120498657, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们始终使用简单的模板:“", "metrics": {"bleu_score": 52.055103630534376, "chrf_score": 48.85760496717102, "xcomet_score": 0.9514241218566895, "xcomet_qe_score": 0.9403689503669739, "metricx_score": 0.43049436807632446, "metricx_qe_score": 0.18552643060684204, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是指A还是B?", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 30.912698412698408, "xcomet_score": 0.9722878932952881, "xcomet_qe_score": 0.9617112874984741, "metricx_score": 0.42488956451416016, "metricx_qe_score": 0.48058411478996277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "” 其中A和B从维基百科中随机抽取。", "metrics": {"bleu_score": 21.409092659758045, "chrf_score": 20.665665727900276, "xcomet_score": 0.9371688961982727, "xcomet_qe_score": 0.9226430058479309, "metricx_score": 0.9346539378166199, "metricx_qe_score": 0.7316561937332153, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同抽样方法。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.09123439341783524, "metricx_qe_score": 0.11483591794967651, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们列表中的条目越高时,实体之间的相似性就越高,通常更难进行消歧。", "metrics": {"bleu_score": 15.08271374297332, "chrf_score": 15.767405829248638, "xcomet_score": 0.7072576880455017, "xcomet_qe_score": 0.7043071985244751, "metricx_score": 4.413483142852783, "metricx_qe_score": 4.409611701965332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种方法是均匀随机抽样;", "metrics": {"bleu_score": 9.238430210261097, "chrf_score": 10.51516139550049, "xcomet_score": 0.9179883003234863, "xcomet_qe_score": 0.927213191986084, "metricx_score": 1.7591478824615479, "metricx_qe_score": 0.9832774996757507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二种方法是当实体具有相似标题时,例如两个名称为“Return”的书籍;", "metrics": {"bleu_score": 9.557083774398006, "chrf_score": 22.421197280367704, "xcomet_score": 0.8401302099227905, "xcomet_qe_score": 0.7950239181518555, "metricx_score": 3.180663585662842, "metricx_qe_score": 4.065282344818115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三种方法是当它们在维基百科上具有相似描述时", "metrics": {"bleu_score": 46.43186698761531, "chrf_score": 41.75148793670495, "xcomet_score": 0.8273729085922241, "xcomet_qe_score": 0.7574326992034912, "metricx_score": 0.5400928258895874, "metricx_qe_score": 0.7559623718261719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ";最后,当它们在维基百科上具有相似的信息框或属性时", "metrics": {"bleu_score": 77.32323369196641, "chrf_score": 80.97447390556164, "xcomet_score": 0.973859965801239, "xcomet_qe_score": 0.9814577698707581, "metricx_score": 1.32368004322052, "metricx_qe_score": 1.6146047115325928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",例如对于一首歌曲来说,是同一流派或同一艺术家。", "metrics": {"bleu_score": 9.410489957951636, "chrf_score": 15.267091593039469, "xcomet_score": 0.8448517322540283, "xcomet_qe_score": 0.8315654993057251, "metricx_score": 4.305388927459717, "metricx_qe_score": 4.768423557281494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向标注员展示这个替代问题时,他们知道这些实体的名称,但他们不一定了解这些实体。", "metrics": {"bleu_score": 57.27945437562451, "chrf_score": 47.167047723851304, "xcomet_score": 0.7832872867584229, "xcomet_qe_score": 0.7836087942123413, "metricx_score": 3.0116288661956787, "metricx_qe_score": 3.0894038677215576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们向他们展示有关这两个实体的背景知识。", "metrics": {"bleu_score": 41.261520349079454, "chrf_score": 33.207263327400625, "xcomet_score": 0.9038103818893433, "xcomet_qe_score": 0.8333152532577515, "metricx_score": 1.0576016902923584, "metricx_qe_score": 1.9302217960357666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只需向他们展示每首歌曲的Google搜索链接,然后要求标注员至少听取每首歌曲的一部分并阅读关于每首歌曲的信息。", "metrics": {"bleu_score": 33.337319858948256, "chrf_score": 28.868902341451573, "xcomet_score": 0.8829803466796875, "xcomet_qe_score": 0.8642120957374573, "metricx_score": 2.3182382583618164, "metricx_qe_score": 1.7570754289627075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是《Easy on Me》的Google搜索结果。", "metrics": {"bleu_score": 24.92926698409002, "chrf_score": 35.93191229060794, "xcomet_score": 0.9429304003715515, "xcomet_qe_score": 0.9574110507965088, "metricx_score": 1.1591663360595703, "metricx_qe_score": 0.8691014647483826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示来自维基百科的一些背景文本。", "metrics": {"bleu_score": 82.1802901256426, "chrf_score": 77.27610234171686, "xcomet_score": 0.9786237478256226, "xcomet_qe_score": 0.9169672131538391, "metricx_score": 0.76226806640625, "metricx_qe_score": 1.0907049179077148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还展示了它们来自维基百科的图片,以便标注员知道它们的样子。", "metrics": {"bleu_score": 34.22154823266292, "chrf_score": 28.908733783431657, "xcomet_score": 0.846186637878418, "xcomet_qe_score": 0.8749221563339233, "metricx_score": 2.9793622493743896, "metricx_qe_score": 2.93906831741333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们要求标注员选择其中一个实体,例如这里的第一个,并使用三到五个间接指代表达式来描述它们。", "metrics": {"bleu_score": 46.831089808501744, "chrf_score": 41.05748292147844, "xcomet_score": 0.8619188666343689, "xcomet_qe_score": 0.8594990372657776, "metricx_score": 3.384491205215454, "metricx_qe_score": 4.232246398925781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,“带有钢琴音乐的那首”;", "metrics": {"bleu_score": 12.874330508144842, "chrf_score": 14.805952178551438, "xcomet_score": 0.9918869733810425, "xcomet_qe_score": 0.9541739225387573, "metricx_score": 1.1994150876998901, "metricx_qe_score": 1.2996331453323364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些来自我们数据集的例子:", "metrics": {"bleu_score": 37.15011599826721, "chrf_score": 32.91510147228705, "xcomet_score": 0.9886777400970459, "xcomet_qe_score": 0.9797686338424683, "metricx_score": 0.2703801393508911, "metricx_qe_score": 0.2993013560771942, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,“没有歌词的那首”、“不是那个12岁男孩的那首”、“虚构的那首”或“来自亚美尼亚的那首”等等。", "metrics": {"bleu_score": 39.010022202773456, "chrf_score": 38.00434671162447, "xcomet_score": 0.9488927125930786, "xcomet_qe_score": 0.90226149559021, "metricx_score": 2.8760855197906494, "metricx_qe_score": 2.593045949935913, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "altentity语料库包含3000个替代问题,跨越三个领域,并且包含42000个间接指代表达式。", "metrics": {"bleu_score": 25.610166570651398, "chrf_score": 29.96278567071153, "xcomet_score": 0.581983208656311, "xcomet_qe_score": 0.5486886501312256, "metricx_score": 5.075154781341553, "metricx_qe_score": 5.949812412261963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用T5xLarge模型的结果总结如下。", "metrics": {"bleu_score": 33.41796039044061, "chrf_score": 31.282602612524812, "xcomet_score": 0.9068986177444458, "xcomet_qe_score": 0.8763749003410339, "metricx_score": 1.8517205715179443, "metricx_qe_score": 1.942700743675232, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与标注员完全相同的背景知识,那么准确率非常高,约为92-95%。", "metrics": {"bleu_score": 51.07643463775202, "chrf_score": 45.596362980401004, "xcomet_score": 0.847089946269989, "xcomet_qe_score": 0.9097525477409363, "metricx_score": 1.1725516319274902, "metricx_qe_score": 0.987602710723877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确率在82%到87%之间,这更加现实,", "metrics": {"bleu_score": 76.75717038826978, "chrf_score": 74.57551466674465, "xcomet_score": 0.8899454474449158, "xcomet_qe_score": 0.8364974856376648, "metricx_score": 1.567399263381958, "metricx_qe_score": 1.7256959676742554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如当语言模型检索背景知识时。", "metrics": {"bleu_score": 84.15266720024049, "chrf_score": 81.8272018245628, "xcomet_score": 0.9967308044433594, "xcomet_qe_score": 0.9911361932754517, "metricx_score": 0.38861486315727234, "metricx_qe_score": 0.4852631390094757, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型只能访问实体名称,那么准确率仅为60%。因此,仍有很大的改进空间。", "metrics": {"bleu_score": 67.19447105931874, "chrf_score": 62.23179292815896, "xcomet_score": 0.9971891641616821, "xcomet_qe_score": 0.9901443719863892, "metricx_score": 1.475807785987854, "metricx_qe_score": 2.126415729522705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还证明了模型的领域泛化能力。", "metrics": {"bleu_score": 33.65584703401596, "chrf_score": 28.91749975854093, "xcomet_score": 0.9237252473831177, "xcomet_qe_score": 0.9169110655784607, "metricx_score": 0.628089189529419, "metricx_qe_score": 0.8420102000236511, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集的链接。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9962955713272095, "xcomet_qe_score": 0.9849957227706909, "metricx_score": 0.23194840550422668, "metricx_qe_score": 0.2493157982826233, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.05947252735495567, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是莎拉·帕皮,来自多伦多大学和布鲁诺·凯斯勒基金会。我将简要介绍一篇以注意力机制为指导的实时语音翻译论文,该论文是与马特奥·内格里和马尔科·图尔基的合作成果。", "metrics": {"bleu_score": 27.48633282224794, "chrf_score": 22.89047168395479, "xcomet_score": 0.7215712070465088, "xcomet_qe_score": 0.6314460635185242, "metricx_score": 2.4993176460266113, "metricx_qe_score": 1.4838573932647705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是实时语音翻译?", "metrics": {"bleu_score": 16.784459625186194, "chrf_score": 15.046762428961383, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.2099609375, "metricx_qe_score": 0.05136504024267197, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实时语音翻译或模拟是指将口语实时翻译成另一种语言的文本的过程,从而实现跨语言交流。", "metrics": {"bleu_score": 62.13396745581624, "chrf_score": 52.635983260077225, "xcomet_score": 0.8689948320388794, "xcomet_qe_score": 0.8555722832679749, "metricx_score": 2.5585649013519287, "metricx_qe_score": 2.9445159435272217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前的模拟模型的难题是什么?", "metrics": {"bleu_score": 8.493098745313148, "chrf_score": 6.902630765380483, "xcomet_score": 0.8198082447052002, "xcomet_qe_score": 0.8591044545173645, "metricx_score": 2.3791890144348145, "metricx_qe_score": 2.398509979248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特定的架构通常需要引入额外的模块进行优化,", "metrics": {"bleu_score": 52.46838739658694, "chrf_score": 48.43052444968705, "xcomet_score": 0.9493280649185181, "xcomet_qe_score": 0.9357806444168091, "metricx_score": 1.476868987083435, "metricx_qe_score": 1.8387049436569214, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从而导致冗长且复杂的训练流程,例如涉及不同优化目标,以及", "metrics": {"bleu_score": 40.21461276856064, "chrf_score": 40.7762439840914, "xcomet_score": 0.6369456648826599, "xcomet_qe_score": 0.41316789388656616, "metricx_score": 3.015186071395874, "metricx_qe_score": 1.7263872623443604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了达到不同的延迟等级而训练和维护多个模型——", "metrics": {"bleu_score": 58.04917012948903, "chrf_score": 54.093136403442, "xcomet_score": 0.9599832892417908, "xcomet_qe_score": 0.9627966284751892, "metricx_score": 1.2017583847045898, "metricx_qe_score": 0.8864046931266785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,训练一个平均延迟一秒的模型,再训练一个平均延迟两秒的模型,以此类推。", "metrics": {"bleu_score": 50.76487944569391, "chrf_score": 42.81614663805316, "xcomet_score": 0.993496298789978, "xcomet_qe_score": 0.9857820272445679, "metricx_score": 0.3238227963447571, "metricx_qe_score": 0.4096536934375763, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么?", "metrics": {"bleu_score": 72.72454093000138, "chrf_score": 68.08265808265807, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07568765431642532, "metricx_qe_score": 0.2555992007255554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,利用现有的离线语音翻译模型,无需重新训练或采用针对CMLSD的特定架构。", "metrics": {"bleu_score": 28.253893006668058, "chrf_score": 22.01105391644001, "xcomet_score": 0.7061730623245239, "xcomet_qe_score": 0.7412576675415039, "metricx_score": 3.3326077461242676, "metricx_qe_score": 3.205759048461914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每个延迟等级,仅使用一个模型,并通过特定的参数来处理延迟。同时,利用", "metrics": {"bleu_score": 42.221963018397794, "chrf_score": 37.99008950510563, "xcomet_score": 0.77236407995224, "xcomet_qe_score": 0.766567051410675, "metricx_score": 5.521500587463379, "metricx_qe_score": 1.8418622016906738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入和文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 59.62415297042496, "chrf_score": 58.15067645682329, "xcomet_score": 0.7593388557434082, "xcomet_qe_score": 0.7074575424194336, "metricx_score": 6.219468116760254, "metricx_qe_score": 4.784849166870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")已经获得的知识,您可以在右侧看到一个示例。", "metrics": {"bleu_score": 16.923267918690055, "chrf_score": 14.603900853153561, "xcomet_score": 0.4103562533855438, "xcomet_qe_score": 0.3498736619949341, "metricx_score": 6.711942195892334, "metricx_qe_score": 9.939597129821777, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出一种点积或编码器-解码器注意力机制,这是一种策略,我们根据注意力指向的位置决定是否发出部分翻译。", "metrics": {"bleu_score": 55.24741556149377, "chrf_score": 47.95681309884353, "xcomet_score": 0.5668667554855347, "xcomet_qe_score": 0.6291046738624573, "metricx_score": 5.127211570739746, "metricx_qe_score": 5.899363040924072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力未集中,则会发出一个词,即这个总和低于某个阈值 alpha,指向最后 lambda 个语音帧,这意味着接收到的信息已经足够稳定。", "metrics": {"bleu_score": 38.65584077322271, "chrf_score": 30.871621581075786, "xcomet_score": 0.6274929046630859, "xcomet_qe_score": 0.5532902479171753, "metricx_score": 5.2180938720703125, "metricx_qe_score": 5.730040550231934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果我们接收到包含 “I'm going to talk about” 的语音片段,并且我们的模型预测翻译为德语,我们会查看交叉注意力权重,会发现前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,即 lambda 个语音帧。", "metrics": {"bleu_score": 44.10604244466395, "chrf_score": 44.22205173772335, "xcomet_score": 0.5218529105186462, "xcomet_qe_score": 0.454694002866745, "metricx_score": 3.1238224506378174, "metricx_qe_score": 3.9973418712615967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被发出,而由于交叉注意力的总和高于某个阈值 alpha,我们将不会发出最后一个词,而是等待另一个语音片段。", "metrics": {"bleu_score": 52.51537999042508, "chrf_score": 44.97074995792369, "xcomet_score": 0.679320216178894, "xcomet_qe_score": 0.7434831857681274, "metricx_score": 3.433467149734497, "metricx_qe_score": 3.6882901191711426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果继续接收另一个语音片段,并且模型预测另外三个词,我们会查看交叉注意力权重,会发现没有词指向最后 lambda 个语音帧。", "metrics": {"bleu_score": 32.125665087338504, "chrf_score": 26.693881759856776, "xcomet_score": 0.7005636692047119, "xcomet_qe_score": 0.5675958395004272, "metricx_score": 3.3289403915405273, "metricx_qe_score": 3.4930195808410645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将被发出。", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 27.259129759129756, "xcomet_score": 0.9332944750785828, "xcomet_qe_score": 0.8744902610778809, "metricx_score": 1.6925324201583862, "metricx_qe_score": 3.3203859329223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果查看该论文的主要结果,我们将在图表上绘制实时语音翻译的结果,其中一侧为蓝色,用于衡量翻译质量和平均滞后(即延迟指标)。我们还考虑了计算感知平均滞后,该指标考虑了模型预测输出所需的计算时间。", "metrics": {"bleu_score": 36.35229178925408, "chrf_score": 28.53165322935113, "xcomet_score": 0.7771710157394409, "xcomet_qe_score": 0.7108218669891357, "metricx_score": 4.71873140335083, "metricx_qe_score": 4.415308952331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们希望我们的曲线在这个图上尽可能地高,", "metrics": {"bleu_score": 22.816849039973928, "chrf_score": 26.184476194437117, "xcomet_score": 0.9015828371047974, "xcomet_qe_score": 0.8173583745956421, "metricx_score": 1.832910418510437, "metricx_qe_score": 2.3090250492095947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并且向左移动。", "metrics": {"bleu_score": 22.616792214653433, "chrf_score": 22.534791556249495, "xcomet_score": 0.3760887682437897, "xcomet_qe_score": 0.6050782799720764, "metricx_score": 3.5967202186584473, "metricx_qe_score": 4.260044097900391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将它与应用于离线模型的预备策略进行比较,例如权重密钥策略和局部一致性策略。", "metrics": {"bleu_score": 33.29371155358684, "chrf_score": 23.566409207653187, "xcomet_score": 0.7629706859588623, "xcomet_qe_score": 0.7038524150848389, "metricx_score": 3.7472646236419678, "metricx_qe_score": 4.629012584686279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与专门为实时语音翻译定制的最先进架构进行比较。", "metrics": {"bleu_score": 77.09002428237393, "chrf_score": 74.17482027266256, "xcomet_score": 0.9838200807571411, "xcomet_qe_score": 0.9635463953018188, "metricx_score": 1.1723756790161133, "metricx_qe_score": 1.90135657787323, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些都是实时语音翻译策略在德语上的所有结果,", "metrics": {"bleu_score": 26.237620039358504, "chrf_score": 25.093988603772495, "xcomet_score": 0.885377049446106, "xcomet_qe_score": 0.8798792958259583, "metricx_score": 1.4977829456329346, "metricx_qe_score": 0.9742501974105835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到点积优于应用于离线模型的任何策略,因为它们的曲线都向左移动。", "metrics": {"bleu_score": 34.92520394471388, "chrf_score": 32.841698172233436, "xcomet_score": 0.8114439249038696, "xcomet_qe_score": 0.7464566826820374, "metricx_score": 4.333662033081055, "metricx_qe_score": 4.837917804718018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果考虑实际经过的时间或计算开销时间,它是最快的策略。", "metrics": {"bleu_score": 34.105597251856096, "chrf_score": 29.683136556483497, "xcomet_score": 0.7571821212768555, "xcomet_qe_score": 0.7747306823730469, "metricx_score": 4.497609615325928, "metricx_qe_score": 3.732454776763916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多结果,请阅读我们的论文。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 78.83793429652562, "xcomet_score": 0.9973729848861694, "xcomet_qe_score": 0.974124014377594, "metricx_score": 0.1322653442621231, "metricx_qe_score": 0.20905639231204987, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还开源了代码和模型,以及同时输出,以促进我们工作的可重复性。", "metrics": {"bleu_score": 9.4900123763462, "chrf_score": 15.944389332698819, "xcomet_score": 0.8463363647460938, "xcomet_qe_score": 0.8221591711044312, "metricx_score": 1.049399495124817, "metricx_qe_score": 1.3523647785186768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢您的关注。", "metrics": {"bleu_score": 14.535768424205482, "chrf_score": 15.65954310980103, "xcomet_score": 0.9099971055984497, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.6868919134140015, "metricx_qe_score": 0.6783174276351929, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各位好,我叫 Ying,我的同事 Jian 和我将为大家介绍我们关于多指令调优,通过指令微调提升零样本多模态学习的研究。", "metrics": {"bleu_score": 37.22791771440451, "chrf_score": 29.904589015182033, "xcomet_score": 0.6323974132537842, "xcomet_qe_score": 0.5885180234909058, "metricx_score": 4.778221607208252, "metricx_qe_score": 5.063390254974365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多工作开始探索新的学习范式,利用预训练语言模型以高效的参数和数据方式执行不同的下游任务。", "metrics": {"bleu_score": 54.03629406321023, "chrf_score": 45.67124772589664, "xcomet_score": 0.8426997065544128, "xcomet_qe_score": 0.7408605813980103, "metricx_score": 2.109081983566284, "metricx_qe_score": 3.3143515586853027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令调优能够使大型语言模型遵循自然指令,以零样本方式执行未见过的任务。", "metrics": {"bleu_score": 52.560745359878155, "chrf_score": 45.034988080907105, "xcomet_score": 0.8840429186820984, "xcomet_qe_score": 0.7619304656982422, "metricx_score": 1.4248491525650024, "metricx_qe_score": 2.1295857429504395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,以往的大部分指令调优工作都集中于提升语言任务的零样本性能,而忽略了计算机视觉和多模态任务。", "metrics": {"bleu_score": 45.47797064966299, "chrf_score": 40.50482710715493, "xcomet_score": 0.973796010017395, "xcomet_qe_score": 0.8186514377593994, "metricx_score": 1.1258518695831299, "metricx_qe_score": 1.6888025999069214, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们旨在研究对多模态预训练模型进行指令调优是否能够真正提升其对未见过的多模态任务的泛化能力。", "metrics": {"bleu_score": 44.62087348910553, "chrf_score": 42.20161344734649, "xcomet_score": 0.9354089498519897, "xcomet_qe_score": 0.8354765176773071, "metricx_score": 1.2738640308380127, "metricx_qe_score": 1.2553718090057373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们的研究期间,我们发现语言任务和多模态任务之间存在指令数据集可用性的显著差异。", "metrics": {"bleu_score": 36.91172790863064, "chrf_score": 31.3098435723223, "xcomet_score": 0.8838739395141602, "xcomet_qe_score": 0.8343483805656433, "metricx_score": 1.6789880990982056, "metricx_qe_score": 1.2820171117782593, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言任务存在一千六百多个指令任务,", "metrics": {"bleu_score": 17.694975149532556, "chrf_score": 14.442110177404293, "xcomet_score": 0.7986904382705688, "xcomet_qe_score": 0.7931116223335266, "metricx_score": 4.9478936195373535, "metricx_qe_score": 4.7031073570251465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但没有大规模的公开可用的多模态指令任务。", "metrics": {"bleu_score": 69.3395566222006, "chrf_score": 64.4399100909652, "xcomet_score": 0.9092608690261841, "xcomet_qe_score": 0.8098517656326294, "metricx_score": 1.6030091047286987, "metricx_qe_score": 2.1660940647125244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这促使我们构建了一个多模态指令调优数据集。", "metrics": {"bleu_score": 52.08256326654537, "chrf_score": 43.036282923985176, "xcomet_score": 0.9706735610961914, "xcomet_qe_score": 0.9573607444763184, "metricx_score": 1.6108468770980835, "metricx_qe_score": 0.9065489172935486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在此介绍 MultiInstruct,这是第一个多模态指令调优基准数据集,由 62 个多样化的多模态任务组成,涵盖 10 个大类。", "metrics": {"bleu_score": 40.78514785826466, "chrf_score": 45.80434120771713, "xcomet_score": 0.9519439935684204, "xcomet_qe_score": 0.9033236503601074, "metricx_score": 1.122760534286499, "metricx_qe_score": 1.111901879310608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来源于 21 个现有的开源数据集,并且每个任务都配备了五条专家编写的指令。", "metrics": {"bleu_score": 55.97233714786894, "chrf_score": 54.84952251918747, "xcomet_score": 0.9919079542160034, "xcomet_qe_score": 0.9711593389511108, "metricx_score": 0.8993592262268066, "metricx_qe_score": 1.297890305519104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了在我们的提出的数据集上研究多模态指令调优,我们选择 OFA 作为基模型,OFA 统一了多模态模式模型。OFA 使用统一的词", "metrics": {"bleu_score": 48.88290760573482, "chrf_score": 48.68804871542344, "xcomet_score": 0.5547009706497192, "xcomet_qe_score": 0.503145158290863, "metricx_score": 7.32390022277832, "metricx_qe_score": 5.0408830642700195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "汇表来表示语言、图像 token 以及边界框的坐标。", "metrics": {"bleu_score": 35.467316001743725, "chrf_score": 26.31591818935676, "xcomet_score": 0.3365860879421234, "xcomet_qe_score": 0.30431219935417175, "metricx_score": 8.115106582641602, "metricx_qe_score": 8.108420372009277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们展示了来自我们 MultiInstruct 数据集的一些示例实例。为了统一各种输入和输出数据类型的处理,", "metrics": {"bleu_score": 63.592000039394236, "chrf_score": 70.47121393550222, "xcomet_score": 0.9000321626663208, "xcomet_qe_score": 0.8762669563293457, "metricx_score": 1.740639567375183, "metricx_qe_score": 2.112272262573242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循 OFA 的方法,将所有任务都以统一的序列到序列格式进行表述,", "metrics": {"bleu_score": 61.70551093767843, "chrf_score": 62.04523703347502, "xcomet_score": 0.854468822479248, "xcomet_qe_score": 0.8120488524436951, "metricx_score": 1.354252576828003, "metricx_qe_score": 2.3061625957489014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中文本、图像、指令和边界框都表示为相同的 token 空间。", "metrics": {"bleu_score": 49.52942096878655, "chrf_score": 45.7534723684149, "xcomet_score": 0.8182374238967896, "xcomet_qe_score": 0.8281679749488831, "metricx_score": 2.578176259994507, "metricx_qe_score": 2.7183167934417725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我将介绍多模态指令调优。", "metrics": {"bleu_score": 39.84681131627584, "chrf_score": 33.17801479566185, "xcomet_score": 0.9216574430465698, "xcomet_qe_score": 0.9666364192962646, "metricx_score": 0.7166979908943176, "metricx_qe_score": 0.7109034061431885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用 NIG 组中的 53 个任务进行训练,并对每个任务抽取 10,000 个样本。", "metrics": {"bleu_score": 53.56672387419685, "chrf_score": 49.72362691205142, "xcomet_score": 0.7047768831253052, "xcomet_qe_score": 0.6962214708328247, "metricx_score": 7.098461627960205, "metricx_qe_score": 7.690359592437744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留了 Common Sense Reasoning 组进行测试,并从 WQA 和 Miscellaneous 组中选择另外五个任务。", "metrics": {"bleu_score": 32.71826430206603, "chrf_score": 22.624286436624857, "xcomet_score": 0.6002142429351807, "xcomet_qe_score": 0.48467427492141724, "metricx_score": 6.207609176635742, "metricx_qe_score": 6.029271125793457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务的测试集中的所有样本。", "metrics": {"bleu_score": 35.24677413974154, "chrf_score": 27.72448092518442, "xcomet_score": 0.8276464939117432, "xcomet_qe_score": 0.7782888412475586, "metricx_score": 2.986992120742798, "metricx_qe_score": 2.0071229934692383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们随机从 NIG 指令的测试集中的 20 个任务中抽取作为 NLP 的 SIN 任务。", "metrics": {"bleu_score": 32.087655651151046, "chrf_score": 31.995431287178256, "xcomet_score": 0.6401495933532715, "xcomet_qe_score": 0.6038640141487122, "metricx_score": 9.07601261138916, "metricx_qe_score": 8.461124420166016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的 OFA 大模型作为基模型。", "metrics": {"bleu_score": 71.03277321267436, "chrf_score": 66.49931553545505, "xcomet_score": 0.8836812973022461, "xcomet_qe_score": 0.8550267219543457, "metricx_score": 1.5454760789871216, "metricx_qe_score": 2.2901506423950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练期间,我们将所有任务的所有样本混合在一起。", "metrics": {"bleu_score": 34.46913316729032, "chrf_score": 31.920645184501616, "xcomet_score": 0.8787548542022705, "xcomet_qe_score": 0.845422625541687, "metricx_score": 1.9159618616104126, "metricx_qe_score": 2.0115106105804443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个样本会随机与它五个指令模板中的一个进行组合。在测试", "metrics": {"bleu_score": 45.23791232712227, "chrf_score": 51.22382961551435, "xcomet_score": 0.6521471738815308, "xcomet_qe_score": 0.4494806230068207, "metricx_score": 3.6896793842315674, "metricx_qe_score": 5.100996971130371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "期间,对于每个任务,我们进行总共五次实验,通过使用五个指令中的一个来评估模型。", "metrics": {"bleu_score": 46.604170446101065, "chrf_score": 42.22942990742181, "xcomet_score": 0.8746147155761719, "xcomet_qe_score": 0.791930615901947, "metricx_score": 1.059862732887268, "metricx_qe_score": 1.6072415113449097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们报告所有五个实验中性能的平均值和最大值,以及标准差。", "metrics": {"bleu_score": 29.326269928050763, "chrf_score": 24.72280634612708, "xcomet_score": 0.8904141187667847, "xcomet_qe_score": 0.9393448233604431, "metricx_score": 2.291346549987793, "metricx_qe_score": 1.92133367061615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是一个多模态分类任务,我们将报告准确率。", "metrics": {"bleu_score": 51.981601535894555, "chrf_score": 44.76325740880617, "xcomet_score": 0.9279745817184448, "xcomet_qe_score": 0.9720736145973206, "metricx_score": 0.5787581205368042, "metricx_qe_score": 0.6595412492752075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模态生成任务,我们将报告 ROUGE。对于 RP 任务,我们也报告 ROUGE。", "metrics": {"bleu_score": 50.367375646576846, "chrf_score": 35.32945460636267, "xcomet_score": 0.6863328814506531, "xcomet_qe_score": 0.641006350517273, "metricx_score": 5.271650314331055, "metricx_qe_score": 5.284966468811035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还引入了一个额外的评估指标,称为灵敏度。", "metrics": {"bleu_score": 69.8082361638602, "chrf_score": 66.19358569949303, "xcomet_score": 0.9915074110031128, "xcomet_qe_score": 0.9007976055145264, "metricx_score": 0.43325120210647583, "metricx_qe_score": 0.6872978806495667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该指标衡量模型在指令措辞略有变化的情况下,是否能够始终如一地产生相同输出的能力。", "metrics": {"bleu_score": 35.29036072333666, "chrf_score": 30.4338806367562, "xcomet_score": 0.9795799255371094, "xcomet_qe_score": 0.9886804819107056, "metricx_score": 1.7748737335205078, "metricx_qe_score": 3.1443121433258057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的主要结果。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9698837995529175, "xcomet_qe_score": 0.88059002161026, "metricx_score": 0.1918793022632599, "metricx_qe_score": 0.3046000003814697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,指令调优可以显著提升 OFA 在未见过的多模态任务上的性能。", "metrics": {"bleu_score": 30.71082191587723, "chrf_score": 27.95878358674848, "xcomet_score": 0.8838370442390442, "xcomet_qe_score": 0.747799277305603, "metricx_score": 2.7624380588531494, "metricx_qe_score": 2.478203058242798, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行迁移学习可以受益于指令调优。", "metrics": {"bleu_score": 73.42150184891982, "chrf_score": 67.13895082373344, "xcomet_score": 0.9494084119796753, "xcomet_qe_score": 0.7283151745796204, "metricx_score": 1.5469399690628052, "metricx_qe_score": 2.13948655128479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,随着任务数量的增加,模型的性能得到提升,同时灵敏度降低。", "metrics": {"bleu_score": 48.26678439370473, "chrf_score": 40.62838060485918, "xcomet_score": 0.9912344217300415, "xcomet_qe_score": 0.9842991828918457, "metricx_score": 1.1520644426345825, "metricx_qe_score": 1.5463788509368896, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一项实验,比", "metrics": {"bleu_score": 18.52797255583095, "chrf_score": 20.294898688425135, "xcomet_score": 0.7453504204750061, "xcomet_qe_score": 0.7486478686332703, "metricx_score": 3.4989402294158936, "metricx_qe_score": 0.30849117040634155, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "较使用一条指令和五条指令的", "metrics": {"bleu_score": 49.588554702454445, "chrf_score": 39.78647957830742, "xcomet_score": 0.6281969547271729, "xcomet_qe_score": 0.5854480266571045, "metricx_score": 5.028357982635498, "metricx_qe_score": 4.026731491088867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "效果。可以看到,使用更多的指令可以改善模型的整体性能并大大降低其灵敏度,", "metrics": {"bleu_score": 45.11648543993887, "chrf_score": 38.1111447936771, "xcomet_score": 0.7079153060913086, "xcomet_qe_score": 0.704440176486969, "metricx_score": 2.9087812900543213, "metricx_qe_score": 3.4288578033447266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明不同的微调策略对模型灵敏度的影响。", "metrics": {"bleu_score": 80.7815097840673, "chrf_score": 79.85298787420795, "xcomet_score": 0.9396010041236877, "xcomet_qe_score": 0.9709905385971069, "metricx_score": 0.8924936056137085, "metricx_qe_score": 1.083611011505127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,从自然指令数据集进行迁移学习,模型可以实现比原始 OFA 模型更高的灵敏度。", "metrics": {"bleu_score": 44.94724556207733, "chrf_score": 39.64429922283169, "xcomet_score": 0.9702333211898804, "xcomet_qe_score": 0.8845170140266418, "metricx_score": 1.8038479089736938, "metricx_qe_score": 2.7815725803375244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上实现更好的性能。", "metrics": {"bleu_score": 76.5961586325499, "chrf_score": 72.42437720852725, "xcomet_score": 0.9422920942306519, "xcomet_qe_score": 0.7358479499816895, "metricx_score": 2.922508716583252, "metricx_qe_score": 3.847382068634033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们提出了第一个大规模的多模态指令调优数据集。我们显著提升了 OFA 的 DAROCHOT 能力,并探索了不同的迁移学习技术并展示了它们的优势。", "metrics": {"bleu_score": 54.40839473298632, "chrf_score": 49.53263223000942, "xcomet_score": 0.8073861002922058, "xcomet_qe_score": 0.7242711782455444, "metricx_score": 4.882201671600342, "metricx_qe_score": 5.280603408813477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个新的指标,称为灵敏度。", "metrics": {"bleu_score": 62.680500508910946, "chrf_score": 57.93567699321889, "xcomet_score": 0.9934659004211426, "xcomet_qe_score": 0.9135281443595886, "metricx_score": 0.40037649869918823, "metricx_qe_score": 0.5959140062332153, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另外一点,我们正在收集一个更大的多模态指令调优数据集,包含大约 150 个额外的变体语言任务,并将发布它们。", "metrics": {"bleu_score": 48.77752211628694, "chrf_score": 45.364832677750464, "xcomet_score": 0.7290939092636108, "xcomet_qe_score": 0.7398585081100464, "metricx_score": 2.386620283126831, "metricx_qe_score": 2.4734628200531006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们数据集和模型的二维码。", "metrics": {"bleu_score": 68.92146754069327, "chrf_score": 58.3011433011433, "xcomet_score": 0.9841136932373047, "xcomet_qe_score": 0.9192091822624207, "metricx_score": 0.4786149561405182, "metricx_qe_score": 0.5699979066848755, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.974276065826416, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是新浪的 Coast。很高兴欢迎各位参加我们关于 ACL 2023 论文“", "metrics": {"bleu_score": 52.141315826122195, "chrf_score": 44.323044740123876, "xcomet_score": 0.4295389950275421, "xcomet_qe_score": 0.35573524236679077, "metricx_score": 7.1213250160217285, "metricx_qe_score": 7.756261348724365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型可接受性判断并非总是对上下文稳健”的讨论。", "metrics": {"bleu_score": 48.64319322446978, "chrf_score": 40.1239479691836, "xcomet_score": 0.7944168448448181, "xcomet_qe_score": 0.7796886563301086, "metricx_score": 6.210514545440674, "metricx_qe_score": 7.139078140258789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和 John Bakhier、Aaron Mueller、Kanishka Mishra、Karen Fentus、Roger Levy 和 Adina Williams 共同完成的工作。", "metrics": {"bleu_score": 36.02421475072493, "chrf_score": 72.1985833214258, "xcomet_score": 0.4808560609817505, "xcomet_qe_score": 0.46681442856788635, "metricx_score": 4.96727180480957, "metricx_qe_score": 5.085993766784668, "linguapy_score": [1, "WELSH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们重新审视了最小对(minimal pair)范式。", "metrics": {"bleu_score": 43.6287459029847, "chrf_score": 42.100913580185114, "xcomet_score": 0.972907304763794, "xcomet_qe_score": 0.967742919921875, "metricx_score": 1.589897871017456, "metricx_qe_score": 1.2290798425674438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小对范式基本上是根据语言模型对可接受性判断进行评估,", "metrics": {"bleu_score": 50.92217433849428, "chrf_score": 39.6492062316549, "xcomet_score": 0.9314430952072144, "xcomet_qe_score": 0.8366309404373169, "metricx_score": 1.8041037321090698, "metricx_qe_score": 2.3893537521362305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这也可以包括语法性,例如来自 Blimp 语法宝石或关于刻板印象的可接受性,如 CrowdSpars。", "metrics": {"bleu_score": 36.94352717886157, "chrf_score": 27.258269147573415, "xcomet_score": 0.5207622051239014, "xcomet_qe_score": 0.4606873393058777, "metricx_score": 7.211994171142578, "metricx_qe_score": 6.727602958679199, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最小对范式中,典型的评估方法是展示一个可接受或语法正确的句子,然后展示一个不可接受或语法错误的句子", "metrics": {"bleu_score": 22.459827519923213, "chrf_score": 23.55296540063488, "xcomet_score": 0.8313435316085815, "xcomet_qe_score": 0.7746042013168335, "metricx_score": 1.1026400327682495, "metricx_qe_score": 2.5328824520111084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",希望模型能够将更高的概率赋予可接受的句子。", "metrics": {"bleu_score": 35.9107613703949, "chrf_score": 29.202244735671563, "xcomet_score": 0.9530162811279297, "xcomet_qe_score": 0.8252374529838562, "metricx_score": 3.352644205093384, "metricx_qe_score": 3.8266777992248535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前 MPP 流程基本不允许我们评估模型对更长句子的可接受性。", "metrics": {"bleu_score": 46.84381508197991, "chrf_score": 40.9288575931425, "xcomet_score": 0.9791615009307861, "xcomet_qe_score": 0.9559866189956665, "metricx_score": 0.986572265625, "metricx_qe_score": 1.2931344509124756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型正在产生越来越长的上下文窗口。", "metrics": {"bleu_score": 42.902556537105646, "chrf_score": 41.032603817952236, "xcomet_score": 0.9161912202835083, "xcomet_qe_score": 0.8993561267852783, "metricx_score": 1.8999595642089844, "metricx_qe_score": 1.6725308895111084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,至关重要的是我们必须在整个上下文窗口中评估模型的可接受性。而这就是我们在这里试", "metrics": {"bleu_score": 63.54176274525549, "chrf_score": 57.92975494474491, "xcomet_score": 0.7784723043441772, "xcomet_qe_score": 0.7338240146636963, "metricx_score": 3.490689992904663, "metricx_qe_score": 1.362596869468689, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图做的事情:我们试图通过让模型评估更长、更长的序列来重新审视 MPP 流程。", "metrics": {"bleu_score": 31.47237572518304, "chrf_score": 30.635752203146694, "xcomet_score": 0.6027180552482605, "xcomet_qe_score": 0.2589510381221771, "metricx_score": 3.4072933197021484, "metricx_qe_score": 3.933074951171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们的方法。 我们的做法", "metrics": {"bleu_score": 59.5640359271809, "chrf_score": 86.19196815751235, "xcomet_score": 0.8108338117599487, "xcomet_qe_score": 0.8452110290527344, "metricx_score": 1.8440415859222412, "metricx_qe_score": 2.2388060092926025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,为了模拟这些更长的序列,我们重新审视数据集本身,然后从这些数据集中选择可接受或不可接受的句子来重建句子。", "metrics": {"bleu_score": 78.88636092387361, "chrf_score": 74.208151636025, "xcomet_score": 0.7172111868858337, "xcomet_qe_score": 0.7315818667411804, "metricx_score": 2.6446733474731445, "metricx_qe_score": 3.1603586673736572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们选择了来自 Blimp 数据集、附例岛案例中的一个典型的语法性", "metrics": {"bleu_score": 29.422852297099457, "chrf_score": 18.958497760802302, "xcomet_score": 0.6798678636550903, "xcomet_qe_score": 0.6544607877731323, "metricx_score": 5.903688430786133, "metricx_qe_score": 4.962019920349121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对。为了重建可接受且具有相同语法结构的更长序列,", "metrics": {"bleu_score": 17.384142574123008, "chrf_score": 18.625344103354866, "xcomet_score": 0.6663862466812134, "xcomet_qe_score": 0.4431287348270416, "metricx_score": 2.9782283306121826, "metricx_qe_score": 3.488138198852539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从附例岛中提取语法正确的句子,然后将其作为前缀添加到可接受的查询和不可接受的查询中。", "metrics": {"bleu_score": 72.90934265001549, "chrf_score": 60.19619166717137, "xcomet_score": 0.7051726579666138, "xcomet_qe_score": 0.7214138507843018, "metricx_score": 2.7163467407226562, "metricx_qe_score": 3.3765158653259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过选择相同的匹配中的不可接受句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 76.12032573255001, "chrf_score": 67.01483758356102, "xcomet_score": 0.9056360125541687, "xcomet_qe_score": 0.8400192260742188, "metricx_score": 1.5175731182098389, "metricx_qe_score": 2.206212282180786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以通过选择来自不同子集或不同数据集的句子来做同样的事情。", "metrics": {"bleu_score": 44.22851061578714, "chrf_score": 36.53471289298065, "xcomet_score": 0.9781564474105835, "xcomet_qe_score": 0.8438293933868408, "metricx_score": 0.7835254669189453, "metricx_qe_score": 1.2435696125030518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们称之为不匹配场景。因此,", "metrics": {"bleu_score": 49.62822700197381, "chrf_score": 46.48814323126549, "xcomet_score": 0.8386243581771851, "xcomet_qe_score": 0.8227306008338928, "metricx_score": 2.0898709297180176, "metricx_qe_score": 2.9307053089141846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关数据集,但不是您评估所使用的相同数据集。", "metrics": {"bleu_score": 59.48328752049117, "chrf_score": 54.84883720205486, "xcomet_score": 0.9449098110198975, "xcomet_qe_score": 0.8212506771087646, "metricx_score": 1.3711475133895874, "metricx_qe_score": 2.25990891456604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以对不可接受性的情况做同样的事情。", "metrics": {"bleu_score": 40.77784262452898, "chrf_score": 31.89575228116282, "xcomet_score": 0.8663871884346008, "xcomet_qe_score": 0.8450397849082947, "metricx_score": 1.998098611831665, "metricx_qe_score": 1.6352348327636719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全不相关的领域,例如维基百科中选择句子。", "metrics": {"bleu_score": 72.14673184611138, "chrf_score": 63.014262557365996, "xcomet_score": 0.9944533109664917, "xcomet_qe_score": 0.983349084854126, "metricx_score": 0.638359785079956, "metricx_qe_score": 1.189529538154602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们模型的接受性判断是否实际上会受到任何上下文的影响,例如,上下文是否来自数据集的不同子集,或者它与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 65.53650377478154, "chrf_score": 57.847662522085905, "xcomet_score": 0.8571921586990356, "xcomet_qe_score": 0.8332319259643555, "metricx_score": 2.4691247940063477, "metricx_qe_score": 2.9534335136413574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,模型的表现如何呢?", "metrics": {"bleu_score": 9.752759118141046, "chrf_score": 10.63737408822508, "xcomet_score": 0.9027401208877563, "xcomet_qe_score": 0.9438310861587524, "metricx_score": 0.9500369429588318, "metricx_qe_score": 0.2996816635131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看维基百科中的句子,这些句子与当前的查询对完全无关。在那里,我们发现 MPP 判断在任意上下文长度下大多是稳健的。", "metrics": {"bleu_score": 57.426766963018025, "chrf_score": 52.24958271797134, "xcomet_score": 0.9507781267166138, "xcomet_qe_score": 0.8280792236328125, "metricx_score": 4.09658670425415, "metricx_qe_score": 5.928092956542969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们增加了上下文长度,达到 1024,以最大化 OPT 和 GPT2 模型的性能,我们", "metrics": {"bleu_score": 33.85953482961944, "chrf_score": 57.501216885961036, "xcomet_score": 0.5246110558509827, "xcomet_qe_score": 0.4586135149002075, "metricx_score": 4.260902404785156, "metricx_qe_score": 1.8718458414077759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色虚线中看到了,MPP 判断相对稳定。", "metrics": {"bleu_score": 56.32579400090421, "chrf_score": 53.64479417497995, "xcomet_score": 0.8311634063720703, "xcomet_qe_score": 0.7717015743255615, "metricx_score": 2.400879144668579, "metricx_qe_score": 4.183317184448242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,当选择来自相同数据集的句子时会发生什么?", "metrics": {"bleu_score": 38.40278521844577, "chrf_score": 34.09375472724965, "xcomet_score": 0.9922758340835571, "xcomet_qe_score": 0.9334202408790588, "metricx_score": 0.804058849811554, "metricx_qe_score": 1.3278443813323975, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里,我们是从相同的 Blimp 或语法宝石数据集中的可接受和不可接受领域创建句子。在那里,", "metrics": {"bleu_score": 41.40539054773662, "chrf_score": 29.901006431374576, "xcomet_score": 0.47345778346061707, "xcomet_qe_score": 0.4093986749649048, "metricx_score": 6.790236473083496, "metricx_qe_score": 6.440577507019043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到当添加可接受的前缀或不可接受的前缀时,MPP 判断会显著增加或减少。", "metrics": {"bleu_score": 63.17782798250488, "chrf_score": 60.94646872998646, "xcomet_score": 0.8117285370826721, "xcomet_qe_score": 0.775457501411438, "metricx_score": 3.7259325981140137, "metricx_qe_score": 2.7790157794952393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当匹配结构时,即当我们从 Blimp 或语法宝石中的相同现象中选择句子时,我们会看到 MPP 判断对于模型会大幅增加或减少,这取决于所选前缀是否可接受或不可接受。", "metrics": {"bleu_score": 45.83505283122515, "chrf_score": 36.974279668964996, "xcomet_score": 0.4903125762939453, "xcomet_qe_score": 0.445406049489975, "metricx_score": 6.446979999542236, "metricx_qe_score": 6.348423004150391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,这种影响非常大,这种效应会随着上下文长度的增加而增强,这可能会影响具有大上下文窗口的新型语言模型。", "metrics": {"bleu_score": 40.10483119098419, "chrf_score": 42.706210941774295, "xcomet_score": 0.8320220708847046, "xcomet_qe_score": 0.7716439962387085, "metricx_score": 1.4410020112991333, "metricx_qe_score": 1.3885456323623657, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为什么匹配前缀会如此影响语言模型的判断?", "metrics": {"bleu_score": 44.75187552074403, "chrf_score": 43.4453415400722, "xcomet_score": 0.975416898727417, "xcomet_qe_score": 0.909466028213501, "metricx_score": 0.5029559135437012, "metricx_qe_score": 0.5929431915283203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过在输入句子中添加噪声,同时尝试保留相关的结构来扰动输入句子。", "metrics": {"bleu_score": 43.01298874147845, "chrf_score": 40.45514507462734, "xcomet_score": 0.8933541178703308, "xcomet_qe_score": 0.8241026401519775, "metricx_score": 1.9538025856018066, "metricx_qe_score": 3.314504861831665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在进行多次这样的扰动之后,我们发现这些噪声中的任何一种都无法使模型改变其 MPP 判断趋势的方式。", "metrics": {"bleu_score": 42.70175037160153, "chrf_score": 38.05716358618834, "xcomet_score": 0.9437569379806519, "xcomet_qe_score": 0.9255651235580444, "metricx_score": 3.3773016929626465, "metricx_qe_score": 3.598895311355591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型对扰动后的句子表现出相似的方式:", "metrics": {"bleu_score": 19.657610294597404, "chrf_score": 19.87202435836883, "xcomet_score": 0.8275567293167114, "xcomet_qe_score": 0.8070402145385742, "metricx_score": 2.75667667388916, "metricx_qe_score": 3.9429101943969727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在可接受的领域中扰动句子时,我们会看到 MPP 判断所有扰动都出现相似的增加,当我们在可接受的领域中扰动句子时,我们会看到 MPP 判断以相似的方式减少。", "metrics": {"bleu_score": 49.2131847161569, "chrf_score": 41.659872712124695, "xcomet_score": 0.4289839565753937, "xcomet_qe_score": 0.39580219984054565, "metricx_score": 9.235193252563477, "metricx_qe_score": 9.615443229675293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们工作的关键结论是,语言模型对跨句共享的潜在句法和语义特征敏感。", "metrics": {"bleu_score": 55.27677316657766, "chrf_score": 48.86799970587665, "xcomet_score": 0.9244308471679688, "xcomet_qe_score": 0.9367520213127136, "metricx_score": 1.2556849718093872, "metricx_qe_score": 1.2782933712005615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前我们以短句和单句输入的方式进行的 MPP 评估,可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 58.80543941045245, "chrf_score": 48.9899690099717, "xcomet_score": 0.9601507186889648, "xcomet_qe_score": 0.8751987814903259, "metricx_score": 1.3043739795684814, "metricx_qe_score": 2.033412456512451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以了解我们实验的更多细节。", "metrics": {"bleu_score": 54.08634078775594, "chrf_score": 48.594144354131046, "xcomet_score": 0.9984679222106934, "xcomet_qe_score": 0.9984185695648193, "metricx_score": 0.18509157001972198, "metricx_qe_score": 0.1718127429485321, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9482142925262451, "xcomet_qe_score": 0.888888955116272, "metricx_score": 0.47969555854797363, "metricx_qe_score": 0.6286952495574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张 Yusuf。", "metrics": {"bleu_score": 72.00391346486707, "chrf_score": 52.36256040857449, "xcomet_score": 0.7988979816436768, "xcomet_qe_score": 0.7767789363861084, "metricx_score": 4.268240451812744, "metricx_qe_score": 4.9927473068237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将为大家介绍我们的工作,Exampler,一个用于多种自然语言和语义表示的多语言语义解析框架。", "metrics": {"bleu_score": 39.09069101699261, "chrf_score": 31.671719660846026, "xcomet_score": 0.7697656154632568, "xcomet_qe_score": 0.7236971855163574, "metricx_score": 3.661060094833374, "metricx_qe_score": 4.007027626037598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义解析的任务是构建用户查询的语义表示,例如 SQL 和 Lambda 微积分。", "metrics": {"bleu_score": 60.275234875721274, "chrf_score": 56.10579233250673, "xcomet_score": 0.9895234107971191, "xcomet_qe_score": 0.9831968545913696, "metricx_score": 0.9972923994064331, "metricx_qe_score": 1.2978389263153076, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而多语言语义解析的任务是将多种自然语言的查询翻译成多种语义表示。", "metrics": {"bleu_score": 69.01319465034662, "chrf_score": 64.16349683185376, "xcomet_score": 0.9428316950798035, "xcomet_qe_score": 0.9469820261001587, "metricx_score": 1.5846319198608398, "metricx_qe_score": 3.3137054443359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经网络模型将多种自然语言的查询翻译成 SQL、Lambda、FunQL 等。", "metrics": {"bleu_score": 75.03346696430607, "chrf_score": 77.36192395867542, "xcomet_score": 0.9871176481246948, "xcomet_qe_score": 0.9767056703567505, "metricx_score": 1.0333762168884277, "metricx_qe_score": 1.4278814792633057, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的多语言语义解析模型通常是独立提出的,并在有限的任务和应用的数据集上进行评估。", "metrics": {"bleu_score": 56.10178485528, "chrf_score": 56.187858999496896, "xcomet_score": 0.9983689785003662, "xcomet_qe_score": 0.9850595593452454, "metricx_score": 0.47136032581329346, "metricx_qe_score": 0.644820511341095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,某些自然语言的覆盖范围存在不足,", "metrics": {"bleu_score": 45.98036015897533, "chrf_score": 48.3688368835593, "xcomet_score": 0.6969578266143799, "xcomet_qe_score": 0.617104172706604, "metricx_score": 3.5501391887664795, "metricx_qe_score": 4.436830043792725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失,某些语义表示的覆盖范围", "metrics": {"bleu_score": 6.9242388493240385, "chrf_score": 10.578638454405326, "xcomet_score": 0.7440619468688965, "xcomet_qe_score": 0.700179934501648, "metricx_score": 4.987096309661865, "metricx_qe_score": 4.0747222900390625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也存在不足,Lambda 微积分缺失,或者它们仅在特定的较新模型上进行评估。", "metrics": {"bleu_score": 25.725582592645946, "chrf_score": 37.33421006268181, "xcomet_score": 0.7023477554321289, "xcomet_qe_score": 0.35094356536865234, "metricx_score": 4.520815372467041, "metricx_qe_score": 5.272087097167969, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个模型用于评估。", "metrics": {"bleu_score": 20.459641298038246, "chrf_score": 20.199196671891674, "xcomet_score": 0.9978358745574951, "xcomet_qe_score": 0.9859330654144287, "metricx_score": 0.530207097530365, "metricx_qe_score": 0.760277271270752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了 Exampler,", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 20.029035880991188, "xcomet_score": 0.8496485352516174, "xcomet_qe_score": 0.8460990190505981, "metricx_score": 1.8469148874282837, "metricx_qe_score": 3.254582405090332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供了一个统一的数据集 Exampler,用于多种自然语言和语义表示的多语言语义解析。", "metrics": {"bleu_score": 65.09066647329666, "chrf_score": 50.279593909797384, "xcomet_score": 0.7725993394851685, "xcomet_qe_score": 0.7678061723709106, "metricx_score": 3.6676101684570312, "metricx_qe_score": 4.848001480102539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它包含九个数据集,涵盖各种领域,五个语义解析任务,八种语义表示,以及来自 15 个语系中的 22 种自然语言。", "metrics": {"bleu_score": 46.01616906628057, "chrf_score": 47.08224313089603, "xcomet_score": 0.9433689713478088, "xcomet_qe_score": 0.951698899269104, "metricx_score": 0.6168591380119324, "metricx_qe_score": 0.9598525762557983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 80.20219183488042, "chrf_score": 71.52080420921001, "xcomet_score": 0.9888695478439331, "xcomet_qe_score": 0.913833737373352, "metricx_score": 1.4580811262130737, "metricx_qe_score": 2.0225110054016113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是 TranslateTest。", "metrics": {"bleu_score": 11.708995388048026, "chrf_score": 7.874613639206017, "xcomet_score": 0.9364950060844421, "xcomet_qe_score": 0.9605221748352051, "metricx_score": 1.3933097124099731, "metricx_qe_score": 1.0277141332626343, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 Google 翻译 API 将源语言翻译成目标语言,然后使用单语模型进行训练和评估。", "metrics": {"bleu_score": 82.66660014007991, "chrf_score": 76.41626540305097, "xcomet_score": 0.9742509126663208, "xcomet_qe_score": 0.837993860244751, "metricx_score": 0.5891801118850708, "metricx_qe_score": 0.5521913766860962, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在英语模型上训练...训练英语查询,在推理时,我们将德语查询使用 API 翻译成英语,然后使用训练好的模型预测 SQL。", "metrics": {"bleu_score": 48.844841285913596, "chrf_score": 42.72227962357185, "xcomet_score": 0.6209595203399658, "xcomet_qe_score": 0.5480520725250244, "metricx_score": 4.43926477432251, "metricx_qe_score": 5.482251167297363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语模型。", "metrics": {"bleu_score": 51.56626918239823, "chrf_score": 40.77320827320827, "xcomet_score": 0.8812164068222046, "xcomet_qe_score": 0.84535151720047, "metricx_score": 1.4693257808685303, "metricx_qe_score": 1.2090201377868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种设置下,源语言和目标语言相同,例如德语到德语或英语到英语。", "metrics": {"bleu_score": 74.00206257221929, "chrf_score": 65.1052652810393, "xcomet_score": 0.9212378263473511, "xcomet_qe_score": 0.8217533826828003, "metricx_score": 0.5288327932357788, "metricx_qe_score": 0.5972004532814026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语融合设置,通过仅使用 10% 的训练数据训练单语模型。我们还测试了单语", "metrics": {"bleu_score": 34.46296171500157, "chrf_score": 35.57303371836533, "xcomet_score": 0.5265855193138123, "xcomet_qe_score": 0.32325392961502075, "metricx_score": 9.403730392456055, "metricx_qe_score": 5.581334590911865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多语模型,我们为所有语言训练一个多语模型。", "metrics": {"bleu_score": 52.84588834767009, "chrf_score": 49.332532846417834, "xcomet_score": 0.7506449818611145, "xcomet_qe_score": 0.7932573556900024, "metricx_score": 2.212157726287842, "metricx_qe_score": 2.6964573860168457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语和中文查询放在一起训练一个多语模型。", "metrics": {"bleu_score": 64.65767005913993, "chrf_score": 55.67337857551109, "xcomet_score": 0.8853030204772949, "xcomet_qe_score": 0.9429187774658203, "metricx_score": 1.340059518814087, "metricx_qe_score": 2.894014596939087, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理时,我们可以使用该模型翻译德语查询或中文查询等。", "metrics": {"bleu_score": 60.16779960516385, "chrf_score": 55.03307598826298, "xcomet_score": 0.9696305990219116, "xcomet_qe_score": 0.88944411277771, "metricx_score": 0.7382437586784363, "metricx_qe_score": 1.3076188564300537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一种源语言上进行训练,然后迁移到另一种语言。因此,", "metrics": {"bleu_score": 33.307662668678184, "chrf_score": 30.09469057533497, "xcomet_score": 0.786724328994751, "xcomet_qe_score": 0.7627609968185425, "metricx_score": 5.2872114181518555, "metricx_qe_score": 5.579730033874512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练时,我们使用英语查询或英语和德语少样本查询的组合进行训练,以训练一个多语模型并预测 SQL 输出。", "metrics": {"bleu_score": 51.96315853085047, "chrf_score": 49.93143517328137, "xcomet_score": 0.8537948727607727, "xcomet_qe_score": 0.7731542587280273, "metricx_score": 1.2736622095108032, "metricx_qe_score": 1.9825632572174072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了很多有趣的发现。", "metrics": {"bleu_score": 57.57575636202256, "chrf_score": 51.482961482961485, "xcomet_score": 0.9825073480606079, "xcomet_qe_score": 0.8997456431388855, "metricx_score": 0.48898571729660034, "metricx_qe_score": 0.716448187828064, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于单语模型的分析,我们在两组模型上进行评估,包括多语言预训练编码器与指针解码器 (Encoder PDR),例如 XLMR-PDR 和 BERT-PDR。", "metrics": {"bleu_score": 37.93768732683163, "chrf_score": 36.49536792920331, "xcomet_score": 0.8016175627708435, "xcomet_qe_score": 0.8429271578788757, "metricx_score": 4.364059925079346, "metricx_qe_score": 4.0786519050598145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,例如 MBART 和 MT5。", "metrics": {"bleu_score": 30.18146852695617, "chrf_score": 22.13167484766176, "xcomet_score": 0.9135904312133789, "xcomet_qe_score": 0.963300347328186, "metricx_score": 1.3813644647598267, "metricx_qe_score": 2.8694963455200195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器模型在所有九个数据集上都获得了最佳性能。", "metrics": {"bleu_score": 54.43379804063227, "chrf_score": 42.0942812579789, "xcomet_score": 0.9916388988494873, "xcomet_qe_score": 0.9868499040603638, "metricx_score": 1.3213409185409546, "metricx_qe_score": 1.2506765127182007, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多语言设置下评估了 MT5 和 XLMR-PDR。我们发现编码器", "metrics": {"bleu_score": 24.12588049712987, "chrf_score": 30.530248450230225, "xcomet_score": 0.6751779317855835, "xcomet_qe_score": 0.6917494535446167, "metricx_score": 8.083730697631836, "metricx_qe_score": 4.381152153015137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "-解码器或编码器 PDR 可以通过在各种语言的混合中进行训练来得到改进。", "metrics": {"bleu_score": 20.210771490685065, "chrf_score": 13.723030376877537, "xcomet_score": 0.6474876999855042, "xcomet_qe_score": 0.6459119319915771, "metricx_score": 5.0493483543396, "metricx_qe_score": 5.844171524047852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,除了英语性能在七个数据集上会下降,仅在三个数据集上获得提升。", "metrics": {"bleu_score": 52.57367050003959, "chrf_score": 46.40535177168603, "xcomet_score": 0.9361252784729004, "xcomet_qe_score": 0.9373996257781982, "metricx_score": 2.625119209289551, "metricx_qe_score": 2.108660936355591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语性的曲线。", "metrics": {"bleu_score": 14.799854887758066, "chrf_score": 13.877512786813275, "xcomet_score": 0.8138012886047363, "xcomet_qe_score": 0.8336136341094971, "metricx_score": 5.305699825286865, "metricx_qe_score": 5.43919563293457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,蓝线是跨语言 Fuchsia 迁移,", "metrics": {"bleu_score": 35.56359469499499, "chrf_score": 28.723579636528072, "xcomet_score": 0.663297712802887, "xcomet_qe_score": 0.5593686103820801, "metricx_score": 7.423344135284424, "metricx_qe_score": 7.542720794677734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙线是跨语言零样本迁移,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.8476507663726807, "xcomet_qe_score": 0.8330907821655273, "metricx_score": 1.9685924053192139, "metricx_qe_score": 3.312077522277832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而绿线是单语设置。我们", "metrics": {"bleu_score": 67.86502681586727, "chrf_score": 89.55823087781029, "xcomet_score": 0.8443070650100708, "xcomet_qe_score": 0.8214312195777893, "metricx_score": 3.572023868560791, "metricx_qe_score": 0.6604433059692383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发现通过比较绿线和橙线,我们发现对于零样本设置,跨语言迁移性能差距非常大。通过比较蓝线和橙线,我们发现对于 Fuchsia 设置,迁移差距迅速缩短。", "metrics": {"bleu_score": 47.60422088016996, "chrf_score": 38.1342961630132, "xcomet_score": 0.500023603439331, "xcomet_qe_score": 0.44675710797309875, "metricx_score": 6.967394828796387, "metricx_qe_score": 8.173480033874512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他的有趣发现。", "metrics": {"bleu_score": 21.36435031981171, "chrf_score": 26.441146119336086, "xcomet_score": 0.8741728067398071, "xcomet_qe_score": 0.8447062969207764, "metricx_score": 0.38246479630470276, "metricx_qe_score": 0.860139787197113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器模型优于之前的研究,或取得了可比的结果。在自然", "metrics": {"bleu_score": 17.37962036917726, "chrf_score": 12.780658908889109, "xcomet_score": 0.6847339868545532, "xcomet_qe_score": 0.6518951654434204, "metricx_score": 6.351390838623047, "metricx_qe_score": 3.006495475769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言英语上进行微调可以显著提升目标自然语言的少样本性能。我们还发现像 Codice 和 Bloom 这样的多语言语言模型对于跨语言语义解析任务仍然不足。", "metrics": {"bleu_score": 39.512083953197894, "chrf_score": 31.99608932377273, "xcomet_score": 0.4909077286720276, "xcomet_qe_score": 0.42502954602241516, "metricx_score": 5.99636173248291, "metricx_qe_score": 5.8257880210876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们构建了 Exampler,一个用于跨语言语义解析的统一基准,包含多种自然语言和许多表示。", "metrics": {"bleu_score": 49.10839181251542, "chrf_score": 37.69418216409183, "xcomet_score": 0.7692878246307373, "xcomet_qe_score": 0.7584822177886963, "metricx_score": 5.094715595245361, "metricx_qe_score": 5.924218654632568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种具有代表性的多语言语言模型进行了全面的基准研究。", "metrics": {"bleu_score": 83.87774154100705, "chrf_score": 75.8523722977746, "xcomet_score": 0.9691107273101807, "xcomet_qe_score": 0.8845460414886475, "metricx_score": 1.0997282266616821, "metricx_qe_score": 1.9320414066314697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结果显示了很多有趣的发现等", "metrics": {"bleu_score": 74.47819789879651, "chrf_score": 64.47177822177822, "xcomet_score": 0.8508467674255371, "xcomet_qe_score": 0.7767212390899658, "metricx_score": 1.6764936447143555, "metricx_qe_score": 1.5352699756622314, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "等。", "metrics": {"bleu_score": 0.0, "chrf_score": 8.333333333333332, "xcomet_score": 0.6047984957695007, "xcomet_qe_score": 0.230974018573761, "metricx_score": 1.9000164270401, "metricx_qe_score": 3.2553701400756836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Ayud Villar,今天我将对题为《基于翻译评估策略和表现的提示 Palm》的论文做一个简短的概述。", "metrics": {"bleu_score": 8.85645532304932, "chrf_score": 17.969252793255073, "xcomet_score": 0.6025664806365967, "xcomet_qe_score": 0.6421123743057251, "metricx_score": 7.567431449890137, "metricx_qe_score": 7.955076217651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这篇论文是与谷歌翻译的同事们共同完成的。", "metrics": {"bleu_score": 27.257626469889683, "chrf_score": 23.521065060440442, "xcomet_score": 0.9370740652084351, "xcomet_qe_score": 0.8120800256729126, "metricx_score": 0.6506752967834473, "metricx_qe_score": 1.7770084142684937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Palm 是一个拥有 5400 亿参数的大型语言模型,于去年 2022 年发布。", "metrics": {"bleu_score": 63.29726894897601, "chrf_score": 62.230505318660846, "xcomet_score": 0.9032900333404541, "xcomet_qe_score": 0.8202371001243591, "metricx_score": 3.610029697418213, "metricx_qe_score": 4.64704704284668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在包含 7800 亿个 token 的大量文本语料库上进行了训练。", "metrics": {"bleu_score": 28.690668742892214, "chrf_score": 36.24680215521677, "xcomet_score": 0.7383410930633545, "xcomet_qe_score": 0.7351219058036804, "metricx_score": 2.7854130268096924, "metricx_qe_score": 2.6206066608428955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发表时,它在数百个 NLP 任务中达到了最先进水平。", "metrics": {"bleu_score": 39.72418603247486, "chrf_score": 41.44345630215195, "xcomet_score": 0.9800243377685547, "xcomet_qe_score": 0.9601312875747681, "metricx_score": 2.580673933029175, "metricx_qe_score": 3.177891731262207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了对大型语言模型在机器翻译中的提示进行首次系统性研究。", "metrics": {"bleu_score": 29.815564924567646, "chrf_score": 28.562288132002323, "xcomet_score": 0.8010673522949219, "xcomet_qe_score": 0.795101523399353, "metricx_score": 3.312488555908203, "metricx_qe_score": 3.540195941925049, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 AMT 社区的最佳实践来评估这些模型在翻译方面的能力。", "metrics": {"bleu_score": 54.142878365294976, "chrf_score": 46.37464785414495, "xcomet_score": 0.918170690536499, "xcomet_qe_score": 0.7594467401504517, "metricx_score": 4.853643417358398, "metricx_qe_score": 5.756725311279297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠,", "metrics": {"bleu_score": 76.68890723478282, "chrf_score": 73.19027428163936, "xcomet_score": 0.9581465721130371, "xcomet_qe_score": 0.9202800989151001, "metricx_score": 0.5162834525108337, "metricx_qe_score": 0.43979600071907043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并比较两个最先进的系统,即 WMT 评估中表现最佳的系统。", "metrics": {"bleu_score": 38.09659149917713, "chrf_score": 36.59877998707723, "xcomet_score": 0.7847923636436462, "xcomet_qe_score": 0.7795880436897278, "metricx_score": 3.4639956951141357, "metricx_qe_score": 4.569965362548828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用最先进的神经机器翻译指标,并额外展示了基于专家的人工评估结果。", "metrics": {"bleu_score": 84.02532817697069, "chrf_score": 84.285768807545, "xcomet_score": 0.8987857103347778, "xcomet_qe_score": 0.8170979022979736, "metricx_score": 1.3468600511550903, "metricx_qe_score": 2.342663526535034, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些提示选择策略的建议。", "metrics": {"bleu_score": 70.75330011966422, "chrf_score": 64.06828873488384, "xcomet_score": 0.8876395225524902, "xcomet_qe_score": 0.8450835347175598, "metricx_score": 1.09638249874115, "metricx_qe_score": 3.2114005088806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对大型语言模型在翻译方面的表现有很大的影响,正如我们在一个简单的实验中看到的那样,我们使用了简短的提示,并为单个句子提供了两个不同的提示。", "metrics": {"bleu_score": 47.47484995268274, "chrf_score": 47.577193101343354, "xcomet_score": 0.8953652381896973, "xcomet_qe_score": 0.8803796172142029, "metricx_score": 1.902268886566162, "metricx_qe_score": 1.6104902029037476, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在 1000 个句子中,", "metrics": {"bleu_score": 30.895757752065407, "chrf_score": 45.3545034322207, "xcomet_score": 0.8692065477371216, "xcomet_qe_score": 0.5887117385864258, "metricx_score": 6.577800750732422, "metricx_qe_score": 8.108074188232422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有 516 个句子的差异超过了一个 blur point。", "metrics": {"bleu_score": 12.874330508144842, "chrf_score": 16.886663644550552, "xcomet_score": 0.71372389793396, "xcomet_qe_score": 0.27388161420822144, "metricx_score": 8.457160949707031, "metricx_qe_score": 7.395200729370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这种差异甚至可以达到 40 个 blur points。", "metrics": {"bleu_score": 28.08337289012351, "chrf_score": 22.81991799800193, "xcomet_score": 0.8605668544769287, "xcomet_qe_score": 0.819139301776886, "metricx_score": 4.553824424743652, "metricx_qe_score": 4.6714911460876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择好的提示策略非常重要。在", "metrics": {"bleu_score": 41.867037412032275, "chrf_score": 35.200073619191265, "xcomet_score": 0.813267707824707, "xcomet_qe_score": 0.7901355624198914, "metricx_score": 3.838833808898926, "metricx_qe_score": 0.27536967396736145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验中,我们采用了五段提示策略,即我们只是用语言标记提供给系统的每个句子。", "metrics": {"bleu_score": 44.64125718108045, "chrf_score": 38.89943538624677, "xcomet_score": 0.8023345470428467, "xcomet_qe_score": 0.7834680080413818, "metricx_score": 2.3405866622924805, "metricx_qe_score": 2.9999539852142334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在这个从德语翻译成英语的例子中,德语句子(源句子)被标记为“德语冒号”,而英语翻译则被标记为“英语冒号”。", "metrics": {"bleu_score": 29.300822701584423, "chrf_score": 24.249697861462536, "xcomet_score": 0.8368220329284668, "xcomet_qe_score": 0.8462371826171875, "metricx_score": 2.642035722732544, "metricx_qe_score": 4.223944664001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在几种短提示的情况下,提示的实际形式没有很大的影响。", "metrics": {"bleu_score": 56.890713886383395, "chrf_score": 48.38599597522707, "xcomet_score": 0.882079541683197, "xcomet_qe_score": 0.8377088904380798, "metricx_score": 1.2103843688964844, "metricx_qe_score": 1.1560276746749878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于零段和一段提示来说,这至关重要,", "metrics": {"bleu_score": 24.228125460854052, "chrf_score": 20.578692770980478, "xcomet_score": 0.6901386380195618, "xcomet_qe_score": 0.6994588971138, "metricx_score": 4.1046857833862305, "metricx_qe_score": 3.3758296966552734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但当我们像我们一样转向五段提示时,提示的实际形式几乎没有差异。", "metrics": {"bleu_score": 9.23806317622409, "chrf_score": 13.346545249933259, "xcomet_score": 0.849627673625946, "xcomet_qe_score": 0.8025431036949158, "metricx_score": 2.4483234882354736, "metricx_qe_score": 2.4792933464050293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例子发挥了最大的作用。", "metrics": {"bleu_score": 4.035011337465489, "chrf_score": 4.975124378109453, "xcomet_score": 0.6545791029930115, "xcomet_qe_score": 0.8289103507995605, "metricx_score": 2.2332189083099365, "metricx_qe_score": 1.2990554571151733, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验结果总结是,示例质量比与源句子相似度更重要。", "metrics": {"bleu_score": 54.53356194834771, "chrf_score": 45.735740418631956, "xcomet_score": 0.9836751222610474, "xcomet_qe_score": 0.9798563718795776, "metricx_score": 0.7200518846511841, "metricx_qe_score": 0.7366914749145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择来自高质量翻译的示例非常重要。", "metrics": {"bleu_score": 44.3237969099558, "chrf_score": 38.640135878912965, "xcomet_score": 0.9906812906265259, "xcomet_qe_score": 0.9868119955062866, "metricx_score": 0.4993261694908142, "metricx_qe_score": 0.5804956555366516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其,我们将从 WMT 评估的训练数据或 dev 数据中选择", "metrics": {"bleu_score": 21.706734180883963, "chrf_score": 23.422311115408988, "xcomet_score": 0.26272982358932495, "xcomet_qe_score": 0.18316490948200226, "metricx_score": 8.1211576461792, "metricx_qe_score": 8.30447769165039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示进行比较。dev 数据比训练数据经过更严格的筛选和具有", "metrics": {"bleu_score": 27.239797591412085, "chrf_score": 20.601868654308326, "xcomet_score": 0.22545254230499268, "xcomet_qe_score": 0.20128901302814484, "metricx_score": 10.36492919921875, "metricx_qe_score": 6.853665351867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更高的质量,并且结果表明使用 dev 数据", "metrics": {"bleu_score": 12.936981168384865, "chrf_score": 13.447303890739681, "xcomet_score": 0.43307605385780334, "xcomet_qe_score": 0.35931557416915894, "metricx_score": 10.359526634216309, "metricx_qe_score": 8.965864181518555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以获得更好的性能。 然而,专业的、基于 ODR 的系统在很大程度上优于 Palm 的翻", "metrics": {"bleu_score": 6.814385179012219, "chrf_score": 10.869877109370375, "xcomet_score": 0.26252150535583496, "xcomet_qe_score": 0.2461528480052948, "metricx_score": 9.85869312286377, "metricx_qe_score": 7.415765285491943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "译,但 Palm 已经非常接近商业系统。", "metrics": {"bleu_score": 71.66258375282708, "chrf_score": 58.036806602983084, "xcomet_score": 0.5293443202972412, "xcomet_qe_score": 0.3509206473827362, "metricx_score": 6.196927547454834, "metricx_qe_score": 6.57832670211792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的案例中,我们选择使用谷歌翻译进行评估。", "metrics": {"bleu_score": 87.87419089273847, "chrf_score": 82.1044046215671, "xcomet_score": 0.9948046207427979, "xcomet_qe_score": 0.9676195383071899, "metricx_score": 0.45883315801620483, "metricx_qe_score": 0.6921864748001099, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 MQM 框架进行的人工评估获得的洞察是,Palm 的流畅度与最先进的系统相当,但主要的差异在于准确性。", "metrics": {"bleu_score": 50.09735893464552, "chrf_score": 43.86212085673292, "xcomet_score": 0.8979774117469788, "xcomet_qe_score": 0.8586432933807373, "metricx_score": 4.976461887359619, "metricx_qe_score": 5.653427600860596, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尤其,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 77.60114635728617, "chrf_score": 70.94473596234124, "xcomet_score": 0.7620880603790283, "xcomet_qe_score": 0.7752113342285156, "metricx_score": 1.7286638021469116, "metricx_qe_score": 0.9165806770324707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,看起来 Palm 有时会通过省略句子中的部分内容来生成更好的句子翻译。", "metrics": {"bleu_score": 13.3717225472362, "chrf_score": 16.721210711021513, "xcomet_score": 0.9296257495880127, "xcomet_qe_score": 0.8404266834259033, "metricx_score": 4.404684066772461, "metricx_qe_score": 3.934523582458496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,PAM 的风格输出类别低于最先进的系统,这是一个额外的信号,表明 PAM 确实提供了非常流畅的输出,但仍然存在准确性问题。 好了,", "metrics": {"bleu_score": 51.43330683841979, "chrf_score": 41.204116653765624, "xcomet_score": 0.5093351602554321, "xcomet_qe_score": 0.46497830748558044, "metricx_score": 7.566746234893799, "metricx_qe_score": 6.094067573547363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这个简短概述的全部内容。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 66.22460872460873, "xcomet_score": 0.9744890928268433, "xcomet_qe_score": 0.9614772200584412, "metricx_score": 0.27324146032333374, "metricx_qe_score": 0.3943900763988495, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如需更多详细信息,请参阅论文的完整演示。", "metrics": {"bleu_score": 47.207122299271134, "chrf_score": 41.1618163958827, "xcomet_score": 0.9748754501342773, "xcomet_qe_score": 0.9754502773284912, "metricx_score": 0.8903244137763977, "metricx_qe_score": 1.6295957565307617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Dawe,来自德国斯塔兰特大学的博士生。", "metrics": {"bleu_score": 44.26623526629487, "chrf_score": 43.0109683016076, "xcomet_score": 0.7654763460159302, "xcomet_qe_score": 0.771804690361023, "metricx_score": 1.7443571090698242, "metricx_qe_score": 2.757859706878662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想介绍我们最近的一项工作,名为《弱于你想象》,对每周监督学习进行批判性审视。", "metrics": {"bleu_score": 34.20316554438103, "chrf_score": 29.059965374604367, "xcomet_score": 0.7818921804428101, "xcomet_qe_score": 0.7743172645568848, "metricx_score": 5.394705772399902, "metricx_qe_score": 6.022190093994141, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一项与肖宇辰、Maios Musbach、Giaz Steffen 和 Dietrich Clarkov 合作完成的工作。", "metrics": {"bleu_score": 6.071283969534311, "chrf_score": 28.666138274838143, "xcomet_score": 0.39812836050987244, "xcomet_qe_score": 0.4694935083389282, "metricx_score": 6.594874382019043, "metricx_qe_score": 6.945641994476318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想先简要介绍一下周监督和每周监督学习。", "metrics": {"bleu_score": 53.7700339214563, "chrf_score": 47.83920986159217, "xcomet_score": 0.6925747990608215, "xcomet_qe_score": 0.6138089895248413, "metricx_score": 5.857597827911377, "metricx_qe_score": 6.421253681182861, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在周监督中,我们不手动标注数据。", "metrics": {"bleu_score": 20.49720931530647, "chrf_score": 20.62022105675913, "xcomet_score": 0.780771017074585, "xcomet_qe_score": 0.7183660268783569, "metricx_score": 4.5645222663879395, "metricx_qe_score": 4.7424821853637695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用周标签源进行标注,例如简单的启发式规则、知识库或局部众包,如图右侧所示。 相比", "metrics": {"bleu_score": 43.48719836859531, "chrf_score": 39.07885534759702, "xcomet_score": 0.5075717568397522, "xcomet_qe_score": 0.35419440269470215, "metricx_score": 3.8824713230133057, "metricx_qe_score": 3.33380126953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "于人工标注,这些较弱的标注成本要低得多,但它们也带有噪声,这意味着一部分标注是错误的。", "metrics": {"bleu_score": 18.863288995952107, "chrf_score": 18.415547625795018, "xcomet_score": 0.6697722673416138, "xcomet_qe_score": 0.5863466262817383, "metricx_score": 3.513339042663574, "metricx_qe_score": 3.516573905944824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在每周标签数据上训练神经网络,这些网络往往会记住标签噪声,而无法泛化。", "metrics": {"bleu_score": 43.33880684257605, "chrf_score": 37.73722493309532, "xcomet_score": 0.8379265666007996, "xcomet_qe_score": 0.7622156143188477, "metricx_score": 5.530087947845459, "metricx_qe_score": 6.328774452209473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每周监督学习中,提出训练算法,以鲁棒的方式在这些标签噪声上训练神经网络,以便训练的模型仍然能够良好泛化。", "metrics": {"bleu_score": 31.21772647356509, "chrf_score": 26.218822349551317, "xcomet_score": 0.5389654636383057, "xcomet_qe_score": 0.4987647533416748, "metricx_score": 6.92527437210083, "metricx_qe_score": 8.37588882446289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的WSL(每周监督学习)工作中,一种常见的说法是,人们声称他们仅在每周标签数据上训练模型,并在干净的测试集上获得高性能。 严", "metrics": {"bleu_score": 29.360071783945365, "chrf_score": 27.284368322289783, "xcomet_score": 0.5349897146224976, "xcomet_qe_score": 0.5620970726013184, "metricx_score": 8.361413955688477, "metricx_qe_score": 7.359666347503662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "格来说,这种说法并不错误,但有一个问题,那就是人们假设可以获得额外的干净验证集用于模型选择。", "metrics": {"bleu_score": 49.9915208266463, "chrf_score": 41.3961395894486, "xcomet_score": 0.6623287200927734, "xcomet_qe_score": 0.6277954578399658, "metricx_score": 4.4361090660095215, "metricx_qe_score": 4.5887274742126465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这种问题设置表示怀疑,因为它意味着每周监督学习需要额外的手动标注。", "metrics": {"bleu_score": 34.78577918990269, "chrf_score": 29.032087754306513, "xcomet_score": 0.7182655334472656, "xcomet_qe_score": 0.6714048981666565, "metricx_score": 5.597579479217529, "metricx_qe_score": 6.3420329093933105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但就像房间里的大象一样,这种必要性往往被忽视。", "metrics": {"bleu_score": 60.97595954478958, "chrf_score": 52.068142894776294, "xcomet_score": 0.9272782802581787, "xcomet_qe_score": 0.8116359710693359, "metricx_score": 1.0818692445755005, "metricx_qe_score": 2.6894094944000244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上述的怀疑促使我们提出了三个研究问题。", "metrics": {"bleu_score": 37.90325913518149, "chrf_score": 38.049900053236065, "xcomet_score": 0.8869694471359253, "xcomet_qe_score": 0.9397480487823486, "metricx_score": 2.3628954887390137, "metricx_qe_score": 1.6826679706573486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,干净的验证数据对WSL是否必要? 或者我们可以使用嘈杂的验证集吗?", "metrics": {"bleu_score": 66.8194883835197, "chrf_score": 62.782521421240965, "xcomet_score": 0.8655701875686646, "xcomet_qe_score": 0.8419699668884277, "metricx_score": 1.9989525079727173, "metricx_qe_score": 3.3529322147369385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,如果干净的数据是WSL能够工作的必要条件,那么我们需要多少干净样本?", "metrics": {"bleu_score": 28.100524848832528, "chrf_score": 26.746365181089743, "xcomet_score": 0.9516488313674927, "xcomet_qe_score": 0.9128240346908569, "metricx_score": 0.8957488536834717, "metricx_qe_score": 0.9980214834213257, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该仅使用干净样本进行验证,或者是否有更好的利用它们的方法?", "metrics": {"bleu_score": 43.60965881010676, "chrf_score": 35.174065891594246, "xcomet_score": 0.9846875667572021, "xcomet_qe_score": 0.9213478565216064, "metricx_score": 0.735938549041748, "metricx_qe_score": 1.0364958047866821, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在我们的工作中解决了这些研究问题,并得出了以下结论。", "metrics": {"bleu_score": 44.68947264234529, "chrf_score": 43.40580651316939, "xcomet_score": 0.9824409484863281, "xcomet_qe_score": 0.9633459448814392, "metricx_score": 1.188598394393921, "metricx_qe_score": 1.2386653423309326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现,有趣的是,最近的WSL方法确实需要干净的验证样本才能正常工作。", "metrics": {"bleu_score": 71.96057984777588, "chrf_score": 68.93358518056257, "xcomet_score": 0.8645211458206177, "xcomet_qe_score": 0.8792775869369507, "metricx_score": 1.864709496498108, "metricx_qe_score": 2.865553855895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。 正如", "metrics": {"bleu_score": 61.153805769010226, "chrf_score": 64.83920961890625, "xcomet_score": 0.8160839080810547, "xcomet_qe_score": 0.7904252409934998, "metricx_score": 2.7060656547546387, "metricx_qe_score": 1.870036005973816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个图所示,如果没有干净的验证样本,训练的模型就无法泛化到原始的弱标签之外,这意味着训练是没有意义的。", "metrics": {"bleu_score": 56.07027347092085, "chrf_score": 47.8797160894201, "xcomet_score": 0.8252593278884888, "xcomet_qe_score": 0.7679693698883057, "metricx_score": 2.0544350147247314, "metricx_qe_score": 2.7661051750183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净标注的数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 59.418348752022005, "chrf_score": 55.0716974345405, "xcomet_score": 0.7312060594558716, "xcomet_qe_score": 0.7190991640090942, "metricx_score": 2.7201504707336426, "metricx_qe_score": 3.657834053039551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净验证样本的数量将有助于WSL方法实现更好的性能,如图左侧所示。", "metrics": {"bleu_score": 70.19951273802924, "chrf_score": 68.32675798677845, "xcomet_score": 0.9029473066329956, "xcomet_qe_score": 0.8991460800170898, "metricx_score": 3.9698679447174072, "metricx_qe_score": 4.705708980560303, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每个类别 20 个样本就可以获得高性能。", "metrics": {"bleu_score": 27.355004372901615, "chrf_score": 26.829296261066403, "xcomet_score": 0.9351984858512878, "xcomet_qe_score": 0.9434894323348999, "metricx_score": 2.0859732627868652, "metricx_qe_score": 2.0239317417144775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并非故事的结局,因为无论我们如何获取干净样本,直接在它们上训练甚至可以实现更好的性能。 红色", "metrics": {"bleu_score": 20.640214538410138, "chrf_score": 17.907098337029133, "xcomet_score": 0.8365858793258667, "xcomet_qe_score": 0.8479653000831604, "metricx_score": 4.7624006271362305, "metricx_qe_score": 3.725029945373535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的图表显示了直接应用于干净数据的微调方法和仅使用干净数据进行验证的WSL方法的性能差异。", "metrics": {"bleu_score": 74.63567227174981, "chrf_score": 71.72564990202565, "xcomet_score": 0.6772045493125916, "xcomet_qe_score": 0.7187155485153198, "metricx_score": 4.5226263999938965, "metricx_qe_score": 5.185160160064697, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,如果我们有每个类别 10 个样本,直接微调就开始超越WSL方法。", "metrics": {"bleu_score": 38.836305697359606, "chrf_score": 37.91776260465858, "xcomet_score": 0.9162411689758301, "xcomet_qe_score": 0.8532918691635132, "metricx_score": 1.806683897972107, "metricx_qe_score": 2.468189239501953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调来实现。", "metrics": {"bleu_score": 24.618144799860488, "chrf_score": 26.20281462047938, "xcomet_score": 0.8739538192749023, "xcomet_qe_score": 0.8404585123062134, "metricx_score": 2.5941247940063477, "metricx_qe_score": 4.2725677490234375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "正如从图中可以看出,最初性能低于更复杂的WSL方法(如余弦)的Berliner模型(FTW)", "metrics": {"bleu_score": 24.427456484048626, "chrf_score": 21.627130107119747, "xcomet_score": 0.5878992080688477, "xcomet_qe_score": 0.6751375198364258, "metricx_score": 7.807209491729736, "metricx_qe_score": 7.240806579589844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果在干净样本上允许继续微调,其性能将与其它方法持平。", "metrics": {"bleu_score": 24.791126177565438, "chrf_score": 20.724121834200904, "xcomet_score": 0.9555239677429199, "xcomet_qe_score": 0.8549550771713257, "metricx_score": 3.411012887954712, "metricx_qe_score": 4.011475563049316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在实践中,没有理由选择更复杂的WSL方法,这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 57.473860166742696, "chrf_score": 55.57193860381212, "xcomet_score": 0.9802455902099609, "xcomet_qe_score": 0.9862825274467468, "metricx_score": 0.8046079277992249, "metricx_qe_score": 1.4242140054702759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们表明最近的WSL方法需要干净的手动标注样本才能正常工作。", "metrics": {"bleu_score": 57.47292348218733, "chrf_score": 54.87420426141188, "xcomet_score": 0.8774356245994568, "xcomet_qe_score": 0.8506371974945068, "metricx_score": 2.4839093685150146, "metricx_qe_score": 3.19529390335083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能收益和实用性被严重高估。", "metrics": {"bleu_score": 32.22538601891173, "chrf_score": 27.57865391385171, "xcomet_score": 0.9849653244018555, "xcomet_qe_score": 0.9897730350494385, "metricx_score": 1.2789335250854492, "metricx_qe_score": 1.1277276277542114, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择是使用", "metrics": {"bleu_score": 46.92470064105597, "chrf_score": 38.971861471861466, "xcomet_score": 0.7718251943588257, "xcomet_qe_score": 0.7119815945625305, "metricx_score": 6.162735462188721, "metricx_qe_score": 5.155031204223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "干净验证样本完成的。", "metrics": {"bleu_score": 16.75493176598769, "chrf_score": 22.965138660913205, "xcomet_score": 0.27349406480789185, "xcomet_qe_score": 0.1670311838388443, "metricx_score": 7.543994426727295, "metricx_qe_score": 9.487565994262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL方法应该与未来学习基线进行比较,因为两者都使用干净样本。", "metrics": {"bleu_score": 46.75768144618937, "chrf_score": 41.11513503738736, "xcomet_score": 0.776477038860321, "xcomet_qe_score": 0.7822213172912598, "metricx_score": 3.8733317852020264, "metricx_qe_score": 5.7565765380859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,持续微调是一种简单而强大的基线,应该在未来的WSL工作中考虑。", "metrics": {"bleu_score": 43.98600982955598, "chrf_score": 38.060037307335506, "xcomet_score": 0.8509767055511475, "xcomet_qe_score": 0.8004753589630127, "metricx_score": 2.6191022396087646, "metricx_qe_score": 2.8720288276672363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们已经开源了我们的代码。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9967988729476929, "xcomet_qe_score": 0.9351927638053894, "metricx_score": 0.4914062023162842, "metricx_qe_score": 0.7212974429130554, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.53011982655684, "chrf_score": 51.34315219921988, "xcomet_score": 0.9966487884521484, "xcomet_qe_score": 0.9828085899353027, "metricx_score": 0.3895472288131714, "metricx_qe_score": 0.34398770332336426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎您查看。", "metrics": {"bleu_score": 32.159351091190125, "chrf_score": 24.34979544126242, "xcomet_score": 0.9901045560836792, "xcomet_qe_score": 0.9442003965377808, "metricx_score": 0.30528131127357483, "metricx_qe_score": 0.44398033618927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝您会议愉快。", "metrics": {"bleu_score": 7.817610446892725, "chrf_score": 8.18252221407027, "xcomet_score": 0.9670588374137878, "xcomet_qe_score": 0.9966236352920532, "metricx_score": 0.5200188159942627, "metricx_qe_score": 0.28290775418281555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是詹姆斯·芬奇。", "metrics": {"bleu_score": 8.054496384843702, "chrf_score": 5.2778553476682495, "xcomet_score": 0.9995607137680054, "xcomet_qe_score": 1.0, "metricx_score": 0.32271575927734375, "metricx_qe_score": 0.18299813568592072, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是莎拉·芬奇。", "metrics": {"bleu_score": 12.22307556087252, "chrf_score": 5.682181701855407, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5379486083984375, "metricx_qe_score": 0.8398617506027222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向您介绍 ABCEval,这是一种评估会话式人工智能的新维度方法。", "metrics": {"bleu_score": 23.743408580176453, "chrf_score": 27.013018503823165, "xcomet_score": 0.8599822521209717, "xcomet_qe_score": 0.9348435401916504, "metricx_score": 1.5771527290344238, "metricx_qe_score": 1.1474895477294922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃莫里 NLP 实验室完成,由埃莫里大学的吉诺·蔡教授领导,并与亚马逊 Alexa AI 合作。", "metrics": {"bleu_score": 26.768906662395068, "chrf_score": 30.92218982270472, "xcomet_score": 0.6857712268829346, "xcomet_qe_score": 0.7192168831825256, "metricx_score": 2.8366475105285645, "metricx_qe_score": 3.0000510215759277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚刚开发了一个对话模型,并且希望了解它的表现如何与当前最先进水平相比。", "metrics": {"bleu_score": 51.25912622298336, "chrf_score": 45.80606944508457, "xcomet_score": 0.978654146194458, "xcomet_qe_score": 0.9778996706008911, "metricx_score": 0.6436416506767273, "metricx_qe_score": 0.4645646810531616, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如请人工评委选择两个对话中哪个更好,或者使用李克特量表对对话进行评分。", "metrics": {"bleu_score": 68.83794432571008, "chrf_score": 63.633512465693684, "xcomet_score": 0.9132713675498962, "xcomet_qe_score": 0.8924291729927063, "metricx_score": 0.7125831842422485, "metricx_qe_score": 0.7258216738700867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法能够很好地提供整体对话质量的 holistic 评估,但对话质量有很多方面。", "metrics": {"bleu_score": 52.408321559057505, "chrf_score": 38.21698170597694, "xcomet_score": 0.8844449520111084, "xcomet_qe_score": 0.873660147190094, "metricx_score": 5.171654224395752, "metricx_qe_score": 6.007203102111816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能需要评估聊天质量的多个维度,以便在更精细的层面上了解模型的优势和劣势。", "metrics": {"bleu_score": 49.83751320077496, "chrf_score": 51.25469046506284, "xcomet_score": 0.9783908128738403, "xcomet_qe_score": 0.9648675918579102, "metricx_score": 0.7436476945877075, "metricx_qe_score": 0.8868545293807983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地要求人工评委评估对话质量的多个维度,例如使用现有的比较或李克特量表方法,评估模型响应的相关性。", "metrics": {"bleu_score": 62.84397383430377, "chrf_score": 55.05561721543963, "xcomet_score": 0.9620316028594971, "xcomet_qe_score": 0.898711085319519, "metricx_score": 1.2058005332946777, "metricx_qe_score": 1.7642912864685059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们认为存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 47.90145581128746, "chrf_score": 45.15558127464808, "xcomet_score": 0.9024549126625061, "xcomet_qe_score": 0.8712908625602722, "metricx_score": 1.308674931526184, "metricx_qe_score": 1.4190480709075928, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过明确标注每个模型响应是否表达了某些行为,例如提供不相关的信息或自相矛盾。", "metrics": {"bleu_score": 38.44543068001203, "chrf_score": 36.50708000052561, "xcomet_score": 0.7406805753707886, "xcomet_qe_score": 0.5627214908599854, "metricx_score": 5.77340030670166, "metricx_qe_score": 6.457141399383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为在聊天中标注行为,简称 ABCEval。", "metrics": {"bleu_score": 40.977447890866394, "chrf_score": 43.3347236516908, "xcomet_score": 0.8956437110900879, "xcomet_qe_score": 0.8577858209609985, "metricx_score": 2.2640461921691895, "metricx_qe_score": 3.8584444522857666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。", "metrics": {"bleu_score": 74.32391249823593, "chrf_score": 67.26850636751183, "xcomet_score": 0.9309866428375244, "xcomet_qe_score": 0.9535799026489258, "metricx_score": 1.3550490140914917, "metricx_qe_score": 3.016953468322754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABCEval 能够衡量聊天模型犯各种主题错误的速率。", "metrics": {"bleu_score": 59.39906178436196, "chrf_score": 52.95296384050147, "xcomet_score": 0.7962718605995178, "xcomet_qe_score": 0.7705848217010498, "metricx_score": 2.4006035327911377, "metricx_qe_score": 3.1910691261291504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,ABCEval 衡量聊天模型忽略其对话伙伴或说一些不相关的话、自相矛盾或与对话伙伴矛盾、产生虚构事实或违反常识知识的轮次数量,以及模型是否成功或失败地表现出同理心。", "metrics": {"bleu_score": 48.05056659957008, "chrf_score": 41.339522428763246, "xcomet_score": 0.7241960763931274, "xcomet_qe_score": 0.7330260276794434, "metricx_score": 2.2318968772888184, "metricx_qe_score": 2.6617019176483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方式最有效,我们选择了四个最先进的聊天模型,并使用 ABCEval 对每个模型进行了 100 次人工机器人对话的评估。", "metrics": {"bleu_score": 54.83851532593916, "chrf_score": 55.11766375151812, "xcomet_score": 0.9553780555725098, "xcomet_qe_score": 0.9494234323501587, "metricx_score": 1.3703281879425049, "metricx_qe_score": 1.1608940362930298, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了进行比较,我们还使用三种现有方法对这些对话进行了评估:在轮次级别上进行李克特评分、在对话级别上进行李克特评分,以及对话级别的成对比较。", "metrics": {"bleu_score": 46.86407345295771, "chrf_score": 43.562435456665405, "xcomet_score": 0.8467090129852295, "xcomet_qe_score": 0.816694438457489, "metricx_score": 3.89162540435791, "metricx_qe_score": 4.532118797302246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了评估方法,我们还收集了关于对话中最常衡量的前八个方面的评估结果,因为这是评估聊天模型在多个维度上进行评估的标准做法。", "metrics": {"bleu_score": 36.686165074579336, "chrf_score": 33.20312904524385, "xcomet_score": 0.6384392380714417, "xcomet_qe_score": 0.622839629650116, "metricx_score": 3.8883676528930664, "metricx_qe_score": 4.159850120544434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从我们对这些评估结果的分析中,我们发现 ABC eval 行为标签总体上比现有方法收集的标签更可靠,如 100 次双重标注对话中评委间一致性所衡量。", "metrics": {"bleu_score": 40.44058767436243, "chrf_score": 39.973426821608896, "xcomet_score": 0.7755539417266846, "xcomet_qe_score": 0.779271125793457, "metricx_score": 6.098614692687988, "metricx_qe_score": 6.312170028686523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,与现有方法产生的指标相比,ABC eval 标签更能预测整体对话质量,这如简单的线性回归分析所示。", "metrics": {"bleu_score": 59.61497964820221, "chrf_score": 52.821337940562515, "xcomet_score": 0.9556224346160889, "xcomet_qe_score": 0.9375318288803101, "metricx_score": 2.319338083267212, "metricx_qe_score": 3.586170196533203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到衡量包含自相矛盾和对话伙伴矛盾的轮次比例,分别可以解释对话质量的 5% 和 10%,而平均李克特一致性评分只能解释 4% 或更少。", "metrics": {"bleu_score": 42.093009748086196, "chrf_score": 38.32252543583903, "xcomet_score": 0.718519389629364, "xcomet_qe_score": 0.7220487594604492, "metricx_score": 3.6927273273468018, "metricx_qe_score": 3.902528762817383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查每个评估指标是否捕获了聊天质量的独特方面。", "metrics": {"bleu_score": 70.2613089460682, "chrf_score": 63.46270450142304, "xcomet_score": 0.8281368017196655, "xcomet_qe_score": 0.8270362615585327, "metricx_score": 1.7021334171295166, "metricx_qe_score": 2.037834882736206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以观察到所有 ABC eval 指标的组合解释了对话质量的 25% 以上,并且在一次移除一个指标时,大多数指标都会导致损失大量的质量信息。", "metrics": {"bleu_score": 36.15243325283217, "chrf_score": 36.8068051813621, "xcomet_score": 0.6610052585601807, "xcomet_qe_score": 0.6100637316703796, "metricx_score": 3.919126510620117, "metricx_qe_score": 4.536004543304443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而交替级别的李克特指标的组合解释的质量要少得多,而且更少的指标携带独特的", "metrics": {"bleu_score": 24.495933385660294, "chrf_score": 23.861901895510098, "xcomet_score": 0.4349755644798279, "xcomet_qe_score": 0.3887832462787628, "metricx_score": 9.65091609954834, "metricx_qe_score": 7.4204421043396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有用信息。 这些可靠、信息丰富且独特的 ABC eval 指标使我们能够以比以往方法更高的分辨率评估会话式人工智能。", "metrics": {"bleu_score": 5.708541835983006, "chrf_score": 12.443030535519576, "xcomet_score": 0.1971893608570099, "xcomet_qe_score": 0.24731606245040894, "metricx_score": 5.462642192840576, "metricx_qe_score": 5.365577697753906, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以看到我们实验的结果表明,仍然存在一些挑战,并且这些挑战已经被精确地量化。", "metrics": {"bleu_score": 24.81723263771339, "chrf_score": 28.59705986600799, "xcomet_score": 0.9711729288101196, "xcomet_qe_score": 0.9681346416473389, "metricx_score": 1.1405794620513916, "metricx_qe_score": 1.209879755973816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们测试的机器人响应中大约有 20% 存在常识违反现象。", "metrics": {"bleu_score": 35.218565358232354, "chrf_score": 34.67881955661439, "xcomet_score": 0.8673193454742432, "xcomet_qe_score": 0.8410367369651794, "metricx_score": 2.159290313720703, "metricx_qe_score": 1.792055368423462, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大约有 15% 的响应会产生不相关的信息,并且大约有 10% 的时间会自相矛盾或与对话伙伴矛盾。", "metrics": {"bleu_score": 42.1036214467364, "chrf_score": 39.40637690707345, "xcomet_score": 0.6700681447982788, "xcomet_qe_score": 0.6929808855056763, "metricx_score": 3.8005783557891846, "metricx_qe_score": 3.9230716228485107, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域快速进步,许多这些错误率可能会在自我们进行评估以来发布的新模型中降低。", "metrics": {"bleu_score": 45.149170564915906, "chrf_score": 37.68138366822578, "xcomet_score": 0.9807720184326172, "xcomet_qe_score": 0.974662184715271, "metricx_score": 2.537517547607422, "metricx_qe_score": 2.097590684890747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更充分地说明了追求可靠且精确的评估指标来比较模型的重要性。", "metrics": {"bleu_score": 49.953503118840565, "chrf_score": 52.627173958481, "xcomet_score": 0.9983042478561401, "xcomet_qe_score": 0.9889769554138184, "metricx_score": 0.7705351114273071, "metricx_qe_score": 0.8291815519332886, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望 ABCEval 能够被该领域的其他人利用,作为在此方向上迈出的有意义的一步,", "metrics": {"bleu_score": 50.87950885353962, "chrf_score": 53.55274696312894, "xcomet_score": 0.9895673990249634, "xcomet_qe_score": 0.9772986173629761, "metricx_score": 1.3622877597808838, "metricx_qe_score": 1.505403995513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "并期待看到未来几个月和几年会话式人工智能将如何发展。", "metrics": {"bleu_score": 41.229744029518166, "chrf_score": 35.436445881019694, "xcomet_score": 0.8639572858810425, "xcomet_qe_score": 0.8418556451797485, "metricx_score": 1.7790461778640747, "metricx_qe_score": 1.191166639328003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫Kyo Yin,我将为大家介绍我们的工作,题目是《翻译何时需要语境?", "metrics": {"bleu_score": 33.918247054808944, "chrf_score": 34.252058776356726, "xcomet_score": 0.8314611315727234, "xcomet_qe_score": 0.8242729306221008, "metricx_score": 1.6948786973953247, "metricx_qe_score": 2.1196181774139404, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "——基于数据的多语种探索》。", "metrics": {"bleu_score": 28.65612242047131, "chrf_score": 27.450346814581234, "xcomet_score": 0.9376311302185059, "xcomet_qe_score": 0.8961530923843384, "metricx_score": 1.3557299375534058, "metricx_qe_score": 1.6665037870407104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与 Patrick Fernandes、Emily Liu、Andre FD Martins 和 Graham Newbig 合作完成的。", "metrics": {"bleu_score": 32.042193038079624, "chrf_score": 69.15799844863238, "xcomet_score": 0.8391944169998169, "xcomet_qe_score": 0.8640516400337219, "metricx_score": 2.4658138751983643, "metricx_qe_score": 2.5854766368865967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "许多翻译都依赖于语境。", "metrics": {"bleu_score": 15.851165692617148, "chrf_score": 12.760942760942761, "xcomet_score": 0.9891617298126221, "xcomet_qe_score": 0.9249639511108398, "metricx_score": 0.2948676645755768, "metricx_qe_score": 0.251555472612381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在这一句中,我们应该如何翻译“mole”?", "metrics": {"bleu_score": 21.270024173913487, "chrf_score": 32.79269923348963, "xcomet_score": 0.997404932975769, "xcomet_qe_score": 0.9743314981460571, "metricx_score": 1.0544490814208984, "metricx_qe_score": 2.434321403503418, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嗯,如果前一句是“如果部长们发现了,情况可能会变得危险”,那么“mole”指的是间谍。", "metrics": {"bleu_score": 19.443834083463223, "chrf_score": 13.531410329473195, "xcomet_score": 0.9829165935516357, "xcomet_qe_score": 0.9686547517776489, "metricx_score": 2.6543164253234863, "metricx_qe_score": 4.042980670928955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“医生,情况严重吗?”那么“mole”指的是痣。", "metrics": {"bleu_score": 17.46349615257657, "chrf_score": 16.486763687353843, "xcomet_score": 0.8960915803909302, "xcomet_qe_score": 0.9355638027191162, "metricx_score": 2.647303342819214, "metricx_qe_score": 3.2669434547424316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据语境的不同,词语的含义会发生变化,翻译也随之改变。", "metrics": {"bleu_score": 22.12235623760911, "chrf_score": 20.047555813048128, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.24250732362270355, "metricx_qe_score": 0.18523983657360077, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在处理此类案例时的表现相当困难。", "metrics": {"bleu_score": 19.04623362251633, "chrf_score": 16.254580401747763, "xcomet_score": 0.9469157457351685, "xcomet_qe_score": 0.9554300904273987, "metricx_score": 1.3534996509552002, "metricx_qe_score": 1.0430713891983032, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,只有一小部分翻译依赖于语境,这使得像 Blue 这样的语料库级别的指标无法捕捉到这些翻译。有", "metrics": {"bleu_score": 43.086852188226985, "chrf_score": 35.50515246148825, "xcomet_score": 0.8397830724716187, "xcomet_qe_score": 0.8027253150939941, "metricx_score": 4.459197521209717, "metricx_qe_score": 3.6282224655151367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些人建议对依赖语境的翻译进行有针对性的评估,但这些资源仅支持有限类型的依赖语境的翻译,并且支持的语言种类也有限,因为它们通常依赖于领域知识和人工策展。", "metrics": {"bleu_score": 57.21833715296151, "chrf_score": 52.5026935873954, "xcomet_score": 0.671961784362793, "xcomet_qe_score": 0.6834861636161804, "metricx_score": 4.612024307250977, "metricx_qe_score": 4.010555267333984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9939944744110107, "xcomet_qe_score": 1.0, "metricx_score": 0.5719066858291626, "metricx_qe_score": 0.22247040271759033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要语境?", "metrics": {"bleu_score": 8.736015370428479, "chrf_score": 11.196726662314651, "xcomet_score": 0.8924298286437988, "xcomet_qe_score": 0.8972160816192627, "metricx_score": 0.3988206386566162, "metricx_qe_score": 0.22178193926811218, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型如何处理这些情况?", "metrics": {"bleu_score": 32.74135267450808, "chrf_score": 30.12273108277076, "xcomet_score": 0.9986907243728638, "xcomet_qe_score": 0.9914894104003906, "metricx_score": 0.7201682925224304, "metricx_qe_score": 1.0261601209640503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了单词在翻译过程中对语境的依赖程度。", "metrics": {"bleu_score": 70.47708740701131, "chrf_score": 61.52906032329073, "xcomet_score": 0.9981553554534912, "xcomet_qe_score": 1.0, "metricx_score": 4.243528366088867, "metricx_qe_score": 4.448748588562012, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的研究中,我们引入了 CXMI 作为衡量机器翻译模型语境使用情况的指标。", "metrics": {"bleu_score": 61.04680067673333, "chrf_score": 55.22736761295091, "xcomet_score": 0.9299873113632202, "xcomet_qe_score": 0.9873374700546265, "metricx_score": 0.8274577856063843, "metricx_qe_score": 1.0942716598510742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这通过测量语境 C 提供的关于目标 Y 的信息量,给定源 X 来完成。您可以将 CXMI 视为给予模型语境所获得的信息。", "metrics": {"bleu_score": 34.71430198860589, "chrf_score": 34.28854230011987, "xcomet_score": 0.5969773530960083, "xcomet_qe_score": 0.6040782332420349, "metricx_score": 4.248411178588867, "metricx_qe_score": 4.394227504730225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们将 CXMI 扩展到 pointwise CXMI,它可以衡量句子级别或单词级别的语境使用情况。", "metrics": {"bleu_score": 29.336269764654716, "chrf_score": 42.024211861755894, "xcomet_score": 0.9374642372131348, "xcomet_qe_score": 0.8867359757423401, "metricx_score": 3.042935848236084, "metricx_qe_score": 4.966027736663818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将具有高 PCXMI 的单词视为需要语境才能进行翻译的单词。", "metrics": {"bleu_score": 40.589517638127056, "chrf_score": 35.819244758989214, "xcomet_score": 0.9309456944465637, "xcomet_qe_score": 0.9179065227508545, "metricx_score": 1.822727084159851, "metricx_qe_score": 2.6325173377990723, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们分析具有高 PCXMI 的单词,以寻找这些单词之间的模式。", "metrics": {"bleu_score": 37.13290491373275, "chrf_score": 38.957546209020244, "xcomet_score": 0.9802870750427246, "xcomet_qe_score": 0.9572151899337769, "metricx_score": 1.553863763809204, "metricx_qe_score": 3.3705618381500244, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对 TED Talks 的英文到十四种不同语言的翻译文本进行分析。我们分", "metrics": {"bleu_score": 35.56521383601747, "chrf_score": 29.709854250354496, "xcomet_score": 0.6027450561523438, "xcomet_qe_score": 0.6373952627182007, "metricx_score": 7.096487998962402, "metricx_qe_score": 2.955181360244751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别在三个不同的层面进行分析。", "metrics": {"bleu_score": 67.83686168526629, "chrf_score": 58.63910592489644, "xcomet_score": 0.7475980520248413, "xcomet_qe_score": 0.6254053115844727, "metricx_score": 3.6100850105285645, "metricx_qe_score": 3.0425846576690674, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看具有较高平均 PCXMI 的词性标签。", "metrics": {"bleu_score": 32.788651923723414, "chrf_score": 31.429376306378593, "xcomet_score": 0.8631772398948669, "xcomet_qe_score": 0.8003582954406738, "metricx_score": 2.257020950317383, "metricx_qe_score": 3.3994946479797363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够发现,例如,阿拉伯语中存在相对较高 p six mi 的双重代词。", "metrics": {"bleu_score": 39.551753942811175, "chrf_score": 30.09339324826798, "xcomet_score": 0.6721569895744324, "xcomet_qe_score": 0.6701962947845459, "metricx_score": 7.304501056671143, "metricx_qe_score": 7.314237117767334, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以通过以下事实来解释:英语没有双重代词。因此,在翻译成阿拉伯语时,您需要语境来确定代词是否为双重。", "metrics": {"bleu_score": 44.53225865713072, "chrf_score": 40.17279602081903, "xcomet_score": 0.6975246667861938, "xcomet_qe_score": 0.8423268795013428, "metricx_score": 2.793299674987793, "metricx_qe_score": 3.1342906951904297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类似地,我们发现某些语言在选择适当的动词形式时也需要语境。", "metrics": {"bleu_score": 75.3285133542847, "chrf_score": 74.04911255198611, "xcomet_score": 0.9917024374008179, "xcomet_qe_score": 0.9885492324829102, "metricx_score": 0.5963007807731628, "metricx_qe_score": 0.6315411329269409, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们查看在所有不同出现次数的平均值下,具有高 p six mi 的词汇项目。", "metrics": {"bleu_score": 21.65592412275547, "chrf_score": 18.75373296727048, "xcomet_score": 0.5295441150665283, "xcomet_qe_score": 0.571333646774292, "metricx_score": 8.470081329345703, "metricx_qe_score": 7.967709064483643, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别出类似于此处的情况,在中文中,您需要语境才能正确翻译。", "metrics": {"bleu_score": 16.47283243271791, "chrf_score": 17.52179785103586, "xcomet_score": 0.7715228796005249, "xcomet_qe_score": 0.7670382261276245, "metricx_score": 2.4533345699310303, "metricx_qe_score": 2.971698045730591, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们发现语境对于以正确的正式程度进行翻译至关重要。", "metrics": {"bleu_score": 18.20705281109213, "chrf_score": 20.762555655430145, "xcomet_score": 0.8886276483535767, "xcomet_qe_score": 0.8775137662887573, "metricx_score": 0.5881659388542175, "metricx_qe_score": 0.4734044373035431, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看不同的单个标记,这些标记具有高 p6mi。", "metrics": {"bleu_score": 6.8146790862039905, "chrf_score": 10.277441439825404, "xcomet_score": 0.6582887172698975, "xcomet_qe_score": 0.6790075898170471, "metricx_score": 7.936756134033203, "metricx_qe_score": 8.140687942504883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别无法真正通过单词本身捕捉到的现象,而是通过句子结构表达的现象,例如省略解析。", "metrics": {"bleu_score": 41.54356959261085, "chrf_score": 35.548555884910336, "xcomet_score": 0.7899191975593567, "xcomet_qe_score": 0.777258038520813, "metricx_score": 1.392469048500061, "metricx_qe_score": 1.849308729171753, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们使用分析结果来设计文档级别翻译的基准。", "metrics": {"bleu_score": 66.17676850825438, "chrf_score": 62.0553330703034, "xcomet_score": 0.984737753868103, "xcomet_qe_score": 0.9733340740203857, "metricx_score": 1.0051636695861816, "metricx_qe_score": 1.4317232370376587, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们识别的五种话语现象,我们创建标记器,以自动识别与该现象相关的单词,", "metrics": {"bleu_score": 49.84682858770512, "chrf_score": 42.326469054194376, "xcomet_score": 0.9641395807266235, "xcomet_qe_score": 0.902550458908081, "metricx_score": 1.0764051675796509, "metricx_qe_score": 1.2722800970077515, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称我们的标记器为多语种语料库感知标记器或 MUDA 标记器。", "metrics": {"bleu_score": 17.895706401541528, "chrf_score": 25.471075444491532, "xcomet_score": 0.7778202891349792, "xcomet_qe_score": 0.7784309983253479, "metricx_score": 3.318851947784424, "metricx_qe_score": 3.0342023372650146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们还可以注意到,不同的语言具有不同比例的话语现象。", "metrics": {"bleu_score": 20.746397476382917, "chrf_score": 22.89669674727146, "xcomet_score": 0.875214695930481, "xcomet_qe_score": 0.9102800488471985, "metricx_score": 1.099936842918396, "metricx_qe_score": 2.1359477043151855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们使用 MUDA 标记器,将标记器应用于我们想要用于评估的平行语料库,并在 MUDA 标记器已识别的依赖语境的示例上应用我们选择的翻译指标。", "metrics": {"bleu_score": 45.5534550101971, "chrf_score": 45.581856419689935, "xcomet_score": 0.8771305084228516, "xcomet_qe_score": 0.8224890232086182, "metricx_score": 1.6378272771835327, "metricx_qe_score": 2.42283034324646, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用我们的基准以及其他指标来评估不同模型在文档级别机器翻译方面的表现。", "metrics": {"bleu_score": 61.130985369185424, "chrf_score": 60.8201248769505, "xcomet_score": 0.9095958471298218, "xcomet_qe_score": 0.8660910129547119, "metricx_score": 0.9363758563995361, "metricx_qe_score": 1.083009958267212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级别的指标时,例如 Blue,我们发现无语境模型表现最佳,", "metrics": {"bleu_score": 43.98126274215768, "chrf_score": 34.549286615923094, "xcomet_score": 0.825668454170227, "xcomet_qe_score": 0.7433105111122131, "metricx_score": 2.8566761016845703, "metricx_qe_score": 2.9274327754974365, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果使用 Comet,则上下文感知模型表现最佳。", "metrics": {"bleu_score": 34.90178621050084, "chrf_score": 28.45571803087446, "xcomet_score": 0.9785572290420532, "xcomet_qe_score": 0.9184391498565674, "metricx_score": 2.4879038333892822, "metricx_qe_score": 3.195966958999634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果使用 WordF 测量,则有或没有语境的模型表现可比。", "metrics": {"bleu_score": 15.89971203135397, "chrf_score": 15.356459740618389, "xcomet_score": 0.7417847514152527, "xcomet_qe_score": 0.7734063863754272, "metricx_score": 5.188186168670654, "metricx_qe_score": 4.609256267547607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次证明,如果我们仅使用语料库级别的指标,很难确定最佳的文档级别翻译系统。", "metrics": {"bleu_score": 63.98952982930888, "chrf_score": 58.590883346379655, "xcomet_score": 0.9978542327880859, "xcomet_qe_score": 0.992385983467102, "metricx_score": 0.7413455247879028, "metricx_qe_score": 0.9732742309570312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在我们使用 MUDA 基准来评估模型,我们发现,对于某些话语现象(例如正式程度和词汇连贯性),上下文感知模型比不使用语境的模型更准确。", "metrics": {"bleu_score": 53.96227350763042, "chrf_score": 48.3797804233279, "xcomet_score": 0.8572602272033691, "xcomet_qe_score": 0.76901775598526, "metricx_score": 1.5005812644958496, "metricx_qe_score": 2.212068796157837, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,对于其他现象(例如省略、代词和动词形式),这些模型并没有比不使用语境的模型好多少。", "metrics": {"bleu_score": 37.201253834053915, "chrf_score": 31.6641382369885, "xcomet_score": 0.9133404493331909, "xcomet_qe_score": 0.9086048603057861, "metricx_score": 0.928164005279541, "metricx_qe_score": 0.8649765849113464, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明我们需要在文档级别翻译方面取得更多进展。", "metrics": {"bleu_score": 29.121537470457664, "chrf_score": 27.575034119013146, "xcomet_score": 0.9894106388092041, "xcomet_qe_score": 0.9866398572921753, "metricx_score": 0.8152439594268799, "metricx_qe_score": 0.9442431330680847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准表明,DPL 通常比 Google 翻译更准确,用于文档级别翻译。", "metrics": {"bleu_score": 56.8415233487702, "chrf_score": 46.44821182076703, "xcomet_score": 0.7655561566352844, "xcomet_qe_score": 0.7944746017456055, "metricx_score": 4.86821174621582, "metricx_qe_score": 4.724684238433838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们对十四种语言对进行了基于数据的分析,以确定翻译何时需要语境。然后,我们使用我们的发现来构建文档级别机器翻译的基准,这可以帮助我们识别哪些离散现象模型可以处理得很好或不能很好地处理,以及哪些翻译系统擅长文档级别翻译。", "metrics": {"bleu_score": 53.157744554787996, "chrf_score": 48.11989851551558, "xcomet_score": 0.7882473468780518, "xcomet_qe_score": 0.7401863932609558, "metricx_score": 3.7578675746917725, "metricx_qe_score": 4.862138748168945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 38.71044712115002, "xcomet_score": 0.9842581748962402, "xcomet_qe_score": 0.9662535190582275, "metricx_score": 0.4675371050834656, "metricx_qe_score": 0.26872068643569946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "明天见。", "metrics": {"bleu_score": 11.752701606523267, "chrf_score": 10.982428115015974, "xcomet_score": 0.519114077091217, "xcomet_qe_score": 0.17786654829978943, "metricx_score": 1.966428279876709, "metricx_qe_score": 3.6971447467803955, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Yanis Lavrack,我将向大家介绍我们在Dr. Berth项目上的工作,这是一种针对生物医学和临床领域,基于法语的强大预训练模型。", "metrics": {"bleu_score": 38.882934251874204, "chrf_score": 41.38854333833349, "xcomet_score": 0.7204118967056274, "xcomet_qe_score": 0.7174443006515503, "metricx_score": 2.649379014968872, "metricx_qe_score": 2.908921957015991, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "本次演示中,我们将首先讨论医疗保健领域的语言建模。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 50.72173599347513, "xcomet_score": 0.9938932657241821, "xcomet_qe_score": 1.0, "metricx_score": 1.6351736783981323, "metricx_qe_score": 1.633341670036316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 83.05389167974835, "chrf_score": 81.33105592664415, "xcomet_score": 0.9893636703491211, "xcomet_qe_score": 0.9912564754486084, "metricx_score": 0.384817510843277, "metricx_qe_score": 0.6998237371444702, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们推出了第一个基于法语的生物医学模型,名为Dr. Berth,它基于Roberta,并在Natchios数据集上进行训练,该数据集是从网络爬取的医疗数据。", "metrics": {"bleu_score": 31.430665681928527, "chrf_score": 24.287849016596816, "xcomet_score": 0.6690977811813354, "xcomet_qe_score": 0.5919086337089539, "metricx_score": 3.945053815841675, "metricx_qe_score": 3.540945291519165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还将介绍带有多种预训练设置和数据源的模型对比。", "metrics": {"bleu_score": 56.6123255329936, "chrf_score": 51.388402660141786, "xcomet_score": 0.9448979496955872, "xcomet_qe_score": 0.9183277487754822, "metricx_score": 0.8182243704795837, "metricx_qe_score": 1.089015007019043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将展示我们在十一项任务上的结果,即法语中的11项生物医学和临床下游任务。", "metrics": {"bleu_score": 37.017292577934654, "chrf_score": 40.45074157231427, "xcomet_score": 0.72076416015625, "xcomet_qe_score": 0.7386289834976196, "metricx_score": 2.8742032051086426, "metricx_qe_score": 3.922058343887329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们将总结实验并向您提供更多关于如何访问这些模型的信息。", "metrics": {"bleu_score": 35.172743035888445, "chrf_score": 33.266594382614166, "xcomet_score": 0.8518685102462769, "xcomet_qe_score": 0.9143244624137878, "metricx_score": 0.3875880241394043, "metricx_qe_score": 0.31823617219924927, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自2018年发布以来,BERT已成为解决自然语言处理任务的最有效方法之一,与历史上的静态和情境化方法(如Word2Vec、Fastex或NWO)相比,能够提供巨大的性能提升。", "metrics": {"bleu_score": 49.31275273739678, "chrf_score": 46.25884884875327, "xcomet_score": 0.8045423030853271, "xcomet_qe_score": 0.8112937211990356, "metricx_score": 3.9178264141082764, "metricx_qe_score": 3.533700466156006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此后,该模型已被适配为许多其他语言,例如法语中的Camembert,以及生物医学领域中的PermetteBERT和BioBERT,以及临床领域中的Clinical BERT,但主要都是英文。", "metrics": {"bleu_score": 38.46490118046221, "chrf_score": 47.028398564176825, "xcomet_score": 0.55205237865448, "xcomet_qe_score": 0.5378047823905945, "metricx_score": 5.419328212738037, "metricx_qe_score": 5.086182594299316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "针对其他语言的专业模型非常稀少,通常基于持续预训练,因为缺乏特定领域的训练数据。", "metrics": {"bleu_score": 33.07538035934431, "chrf_score": 31.183869476910047, "xcomet_score": 0.8202184438705444, "xcomet_qe_score": 0.8161747455596924, "metricx_score": 0.9960070848464966, "metricx_qe_score": 1.25580894947052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在之前,法语并没有任何开源的生物医学模型。", "metrics": {"bleu_score": 27.682168087835898, "chrf_score": 24.828405034328817, "xcomet_score": 0.8194169998168945, "xcomet_qe_score": 0.6804441213607788, "metricx_score": 1.4491348266601562, "metricx_qe_score": 1.79086172580719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们自问一个问题:最合适的训练数据来源是什么,以便广泛应用?目前这些数据是否是临床数据的良好替代品?", "metrics": {"bleu_score": 28.230995842529353, "chrf_score": 29.603934046524106, "xcomet_score": 0.8046818971633911, "xcomet_qe_score": 0.7921282052993774, "metricx_score": 1.9555577039718628, "metricx_qe_score": 2.1713428497314453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们将Dr. Berth与我们的Schubert模型进行比较,Schubert模型基于从非大学医院获得并进行匿名化处理的数据。", "metrics": {"bleu_score": 35.90633164209106, "chrf_score": 28.439227262796557, "xcomet_score": 0.6129226088523865, "xcomet_qe_score": 0.6079066395759583, "metricx_score": 4.982053756713867, "metricx_qe_score": 6.105936050415039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随后,我们再问自己:为了在法语数据上训练一个专业模型,需要多少数据?", "metrics": {"bleu_score": 36.92890702094148, "chrf_score": 33.00358538419805, "xcomet_score": 0.9550197124481201, "xcomet_qe_score": 0.7868316173553467, "metricx_score": 1.198014259338379, "metricx_qe_score": 1.1466786861419678, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是4 GB,8 GB还是更多?", "metrics": {"bleu_score": 24.808415001701817, "chrf_score": 55.26418026418026, "xcomet_score": 0.9742275476455688, "xcomet_qe_score": 0.9568802118301392, "metricx_score": 0.3287355303764343, "metricx_qe_score": 0.5636284351348877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答这个问题,我们首先从头开始训练并比较了四个模型:一个Dr. Berth的初始版本,使用7 GB的Natchez数据;另一个Dr. Berth的初始版本,使用4 GB的Natchez数据;一个Schubert的初始版本,即临床模型,使用从临床节点中提取的4 GB句子;以及一个最终的Schubert版本,混合了4 GB的Natchez和4 GB的临床节点。", "metrics": {"bleu_score": 20.452596148982064, "chrf_score": 25.137323641824906, "xcomet_score": 0.35824280977249146, "xcomet_qe_score": 0.39146688580513, "metricx_score": 7.01478910446167, "metricx_qe_score": 6.0240888595581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个对比之外,我们还引入了三个基于持续预训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 56.8359923416233, "chrf_score": 48.65813898334909, "xcomet_score": 0.907473087310791, "xcomet_qe_score": 0.9481037855148315, "metricx_score": 1.1911691427230835, "metricx_qe_score": 1.3428047895431519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于Camembert的权重并在4 GB的Natchez数据上进行训练;", "metrics": {"bleu_score": 23.550061570724225, "chrf_score": 38.34113859849154, "xcomet_score": 0.7193505764007568, "xcomet_qe_score": 0.7364044785499573, "metricx_score": 5.164120674133301, "metricx_qe_score": 5.959394931793213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也基于Camembert,但这次在4 GB的临床节点上进行训练;还有一个基于英文生物医学模型Bermud Bert,并在4 GB的Natchez数据上进行训练。", "metrics": {"bleu_score": 31.223013630878224, "chrf_score": 36.617973091478696, "xcomet_score": 0.5316268801689148, "xcomet_qe_score": 0.517160177230835, "metricx_score": 6.344501972198486, "metricx_qe_score": 6.179471015930176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们有七个模型。", "metrics": {"bleu_score": 57.067457770560026, "chrf_score": 52.68257581869399, "xcomet_score": 0.9877036809921265, "xcomet_qe_score": 0.8995441198348999, "metricx_score": 0.29523736238479614, "metricx_qe_score": 0.44317466020584106, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了评估我们的七个模型,我们收集了公开和私有的下游任务,例如命名实体识别、文本分类、词性标注和问答。", "metrics": {"bleu_score": 58.3271993946545, "chrf_score": 49.68025165332305, "xcomet_score": 0.8049130439758301, "xcomet_qe_score": 0.7188278436660767, "metricx_score": 2.209453821182251, "metricx_qe_score": 3.6693689823150635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些模型与六个基线模型进行比较,包括Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PubMedBERT、BioBERT和ClinicalBERT。", "metrics": {"bleu_score": 54.4790211723127, "chrf_score": 62.9828869826236, "xcomet_score": 0.536002516746521, "xcomet_qe_score": 0.5857911109924316, "metricx_score": 3.1860575675964355, "metricx_qe_score": 2.958763599395752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果表明,模型在与训练数据性质相同的数据集上表现最佳。", "metrics": {"bleu_score": 39.86299839048497, "chrf_score": 32.99539618427441, "xcomet_score": 0.9833320379257202, "xcomet_qe_score": 0.9834514856338501, "metricx_score": 0.7134221792221069, "metricx_qe_score": 0.8835418224334717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以从这些数据中观察到,来自异质来源的数据似乎更具通用性。", "metrics": {"bleu_score": 33.98642477980264, "chrf_score": 35.525480528558056, "xcomet_score": 0.9134806990623474, "xcomet_qe_score": 0.8468492031097412, "metricx_score": 0.9568228721618652, "metricx_qe_score": 1.2705062627792358, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以带来更好的性能。", "metrics": {"bleu_score": 52.6589137558171, "chrf_score": 45.93989530077461, "xcomet_score": 0.9362239837646484, "xcomet_qe_score": 0.9736031889915466, "metricx_score": 2.49906587600708, "metricx_qe_score": 2.952756404876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总体而言,从头开始的微调似乎在大多数任务上获得了更高的性能。", "metrics": {"bleu_score": 43.36229047056066, "chrf_score": 36.055531909870226, "xcomet_score": 0.77036052942276, "xcomet_qe_score": 0.8216482400894165, "metricx_score": 4.455391883850098, "metricx_qe_score": 4.819179534912109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们在持续微调中,使用PubMedBERT的权重和分词器,并在Natchez数据集的4 GB子集中进行训练的实验,显示出与从头开始训练的Dr. Berth 4 GB相当的", "metrics": {"bleu_score": 20.367579589244023, "chrf_score": 27.133838463968356, "xcomet_score": 0.32470253109931946, "xcomet_qe_score": 0.35765692591667175, "metricx_score": 7.891315460205078, "metricx_qe_score": 7.188819408416748, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结果,而基于Camembert权重和分词器的模型则存在稳定性问题。", "metrics": {"bleu_score": 28.676853292656155, "chrf_score": 31.634716351454777, "xcomet_score": 0.5902734994888306, "xcomet_qe_score": 0.5451654195785522, "metricx_score": 5.093033313751221, "metricx_qe_score": 4.374171733856201, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,作为结论,我们提出的系统在11项下游任务中表现优于9项,并总体上超越了通用模型Camembert的结果。", "metrics": {"bleu_score": 26.305971260942588, "chrf_score": 25.4163336309184, "xcomet_score": 0.7398744225502014, "xcomet_qe_score": 0.7500706315040588, "metricx_score": 3.653555154800415, "metricx_qe_score": 1.932763695716858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,专业化的数据更好,更专业化的数据更好,但可扩展性不佳。", "metrics": {"bleu_score": 21.878065229063957, "chrf_score": 22.597341647718526, "xcomet_score": 0.7484030723571777, "xcomet_qe_score": 0.6265802383422852, "metricx_score": 3.826692819595337, "metricx_qe_score": 4.213106155395508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有从Natchios获得并进行预训练的模型都可在Hugging Face上免费获取,所有训练脚本都可在我们的GitHub仓库中找到。感谢各位...", "metrics": {"bleu_score": 35.88969678395917, "chrf_score": 43.721857961139285, "xcomet_score": 0.5700445175170898, "xcomet_qe_score": 0.6621404886245728, "metricx_score": 4.6620893478393555, "metricx_qe_score": 5.22514009475708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢本次演示,我们期待在多伦多的会后环节与大家交流。", "metrics": {"bleu_score": 24.76682488899512, "chrf_score": 25.35409941100505, "xcomet_score": 0.8634718060493469, "xcomet_qe_score": 0.8844249844551086, "metricx_score": 3.4120166301727295, "metricx_qe_score": 3.3628172874450684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·伦德曼,今天我将为大家简要介绍我们的论文,该论文探讨了在不使用树结构的情况下,如何通过多集标记和潜在置换实现组合泛化。 这项工作", "metrics": {"bleu_score": 26.954509876183913, "chrf_score": 26.779029035946078, "xcomet_score": 0.752560019493103, "xcomet_qe_score": 0.8059967160224915, "metricx_score": 4.707274913787842, "metricx_qe_score": 2.261721611022949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是我与我的导师亚历山大·科勒和伊万·蒂托夫共同完成的。", "metrics": {"bleu_score": 7.305891545072536, "chrf_score": 5.529219186959193, "xcomet_score": 0.9239546060562134, "xcomet_qe_score": 0.8645139932632446, "metricx_score": 1.885677695274353, "metricx_qe_score": 1.7214890718460083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深层递归和在训练期间单独见过的短语组合的能力。", "metrics": {"bleu_score": 80.0971248594445, "chrf_score": 76.79776383437017, "xcomet_score": 0.8166186809539795, "xcomet_qe_score": 0.7333930730819702, "metricx_score": 4.458693027496338, "metricx_qe_score": 5.919709205627441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能如下所示。 就", "metrics": {"bleu_score": 69.92922471483766, "chrf_score": 59.587299936270185, "xcomet_score": 0.7130199670791626, "xcomet_qe_score": 0.740349531173706, "metricx_score": 4.8452653884887695, "metricx_qe_score": 1.970604658126831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像往常一样,我们有一个训练集,", "metrics": {"bleu_score": 35.4306209570667, "chrf_score": 27.750271584087976, "xcomet_score": 0.8813636302947998, "xcomet_qe_score": 0.8022447228431702, "metricx_score": 2.3310718536376953, "metricx_qe_score": 3.8430609703063965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中包含一些句子,例如 “the girl slept” 和 ", "metrics": {"bleu_score": 11.84479662042485, "chrf_score": 40.31387696453232, "xcomet_score": 0.5799365043640137, "xcomet_qe_score": 0.24827226996421814, "metricx_score": 6.535806655883789, "metricx_qe_score": 5.476648330688477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“Mary knew that the girl slept”。", "metrics": {"bleu_score": 14.601126233589126, "chrf_score": 62.41943477676444, "xcomet_score": 0.923664927482605, "xcomet_qe_score": 0.9055488109588623, "metricx_score": 9.98208999633789, "metricx_qe_score": 22.620031356811523, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些句子与表示其核心含义的逻辑形式配对。", "metrics": {"bleu_score": 33.552674708360215, "chrf_score": 26.986958853598797, "xcomet_score": 0.9934965372085571, "xcomet_qe_score": 0.8796769380569458, "metricx_score": 0.9641485810279846, "metricx_qe_score": 0.8124100565910339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准的机器学习评估不同,测试集不来自相同的分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 50.07319690628285, "chrf_score": 44.43676138134573, "xcomet_score": 0.8596510887145996, "xcomet_qe_score": 0.8347722291946411, "metricx_score": 1.0187681913375854, "metricx_qe_score": 1.8810267448425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练期间接触到更浅层的递归,然后被测试在更深层递归的示例上。", "metrics": {"bleu_score": 33.27207117065431, "chrf_score": 28.473207912098918, "xcomet_score": 0.961315393447876, "xcomet_qe_score": 0.9468930959701538, "metricx_score": 2.562236785888672, "metricx_qe_score": 4.665160655975342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以处理这种分布外泛化,并且通常会产生与输入不相关的输出。", "metrics": {"bleu_score": 51.13307336362714, "chrf_score": 44.047790858936374, "xcomet_score": 0.7806340456008911, "xcomet_qe_score": 0.7750562429428101, "metricx_score": 3.860399007797241, "metricx_qe_score": 3.5513978004455566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们常常无法再现输入和输出之间的系统性对应关系,例如在示例中以颜色编码的那些。", "metrics": {"bleu_score": 49.30477318953673, "chrf_score": 44.59681349757967, "xcomet_score": 0.9985857009887695, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.8928958177566528, "metricx_qe_score": 1.0061659812927246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种流行的解决办法是将树结构整合到模型中。", "metrics": {"bleu_score": 41.37280342230792, "chrf_score": 34.78462430264149, "xcomet_score": 0.997329592704773, "xcomet_qe_score": 1.0, "metricx_score": 0.6750372648239136, "metricx_qe_score": 0.7068508863449097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "树结构旨在捕获与逻辑形式相关的句子组成的进程。", "metrics": {"bleu_score": 8.46216601512324, "chrf_score": 11.255498165635453, "xcomet_score": 0.7853603363037109, "xcomet_qe_score": 0.7312890291213989, "metricx_score": 3.0602755546569824, "metricx_qe_score": 3.2195911407470703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这样做效果很好,但树结构通常是不存在的,需要以某种方式获取。", "metrics": {"bleu_score": 29.07276428721093, "chrf_score": 24.222843774669713, "xcomet_score": 0.9138697385787964, "xcomet_qe_score": 0.9443072080612183, "metricx_score": 1.9497778415679932, "metricx_qe_score": 2.063011646270752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能很复杂,有时也可能需要大量的计算资源。", "metrics": {"bleu_score": 27.526990943873297, "chrf_score": 23.443620935880993, "xcomet_score": 0.9994421005249023, "xcomet_qe_score": 0.9963732957839966, "metricx_score": 0.38422685861587524, "metricx_qe_score": 0.2547915577888489, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到对逻辑形式进行相当程度的特定形式化预处理,例如处理变量符号。", "metrics": {"bleu_score": 37.716575571858016, "chrf_score": 34.0510918364729, "xcomet_score": 0.9845904111862183, "xcomet_qe_score": 0.9786461591720581, "metricx_score": 0.5962082147598267, "metricx_qe_score": 0.721333384513855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取树结构还可能涉及专门的语法归纳程序。", "metrics": {"bleu_score": 49.385277270209265, "chrf_score": 43.96985419471788, "xcomet_score": 0.9726669788360596, "xcomet_qe_score": 0.9660245180130005, "metricx_score": 2.4557838439941406, "metricx_qe_score": 2.8538150787353516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们不使用树结构,而是引入了一种神经序列到序列模型,该模型直接模拟输入片段和输出片段之间的对应关系。", "metrics": {"bleu_score": 57.07219012467399, "chrf_score": 46.83645095729715, "xcomet_score": 0.8169293999671936, "xcomet_qe_score": 0.8367549777030945, "metricx_score": 1.6939328908920288, "metricx_qe_score": 1.6030490398406982, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在不依赖树结构的情况下,对更深层递归实现强大的泛化。", "metrics": {"bleu_score": 41.29551000612077, "chrf_score": 34.31215967903871, "xcomet_score": 0.922430157661438, "xcomet_qe_score": 0.9125451445579529, "metricx_score": 2.3433966636657715, "metricx_qe_score": 3.1815478801727295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法分两个步骤从输入预测输出。", "metrics": {"bleu_score": 69.01374299426456, "chrf_score": 61.21378893437716, "xcomet_score": 0.9957408905029297, "xcomet_qe_score": 0.9919648170471191, "metricx_score": 0.5007511973381042, "metricx_qe_score": 0.7281056642532349, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个无序多集标记对每个输入标记进行标记,其中包含将在输出中出现的标记。", "metrics": {"bleu_score": 22.638633395011226, "chrf_score": 24.0943976527263, "xcomet_score": 0.8356695175170898, "xcomet_qe_score": 0.8847997188568115, "metricx_score": 3.3922441005706787, "metricx_qe_score": 2.6105105876922607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一步之后,我们拥有了所有正确的标记,但它们的顺序不正确。", "metrics": {"bleu_score": 30.300203366197277, "chrf_score": 27.77200441086014, "xcomet_score": 0.8954653739929199, "xcomet_qe_score": 0.8785479068756104, "metricx_score": 2.516286849975586, "metricx_qe_score": 3.1366045475006104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步中,我们使用另一个模型来预测一个置换,以便将它们置于正确的顺序中。", "metrics": {"bleu_score": 43.28804140411484, "chrf_score": 44.742538057193684, "xcomet_score": 0.9051119089126587, "xcomet_qe_score": 0.9103593826293945, "metricx_score": 3.451291561126709, "metricx_qe_score": 3.2953052520751953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新方法来预测置换,该方法对可能的置换没有任何硬性约束。", "metrics": {"bleu_score": 60.69928553742501, "chrf_score": 52.21317713082675, "xcomet_score": 0.8842672109603882, "xcomet_qe_score": 0.9044176340103149, "metricx_score": 2.703026294708252, "metricx_qe_score": 2.103013277053833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活和富有表现力。", "metrics": {"bleu_score": 49.41109918128317, "chrf_score": 42.370870320503926, "xcomet_score": 0.9872218370437622, "xcomet_qe_score": 0.9658831357955933, "metricx_score": 0.7799614667892456, "metricx_qe_score": 1.2798502445220947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "概念上,我们的置换模型大致如下工作。", "metrics": {"bleu_score": 14.477247390626578, "chrf_score": 17.828466743712667, "xcomet_score": 0.7943447828292847, "xcomet_qe_score": 0.7925846576690674, "metricx_score": 4.181326389312744, "metricx_qe_score": 3.060317039489746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,确定将哪个多集标记放入每个位置。", "metrics": {"bleu_score": 33.00276036798196, "chrf_score": 27.952814118113366, "xcomet_score": 0.8272914886474609, "xcomet_qe_score": 0.7993398904800415, "metricx_score": 1.6139843463897705, "metricx_qe_score": 1.7445178031921387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们只需选择一个,如图中红色突出显示的那样。", "metrics": {"bleu_score": 39.296476509212795, "chrf_score": 37.448724098252605, "xcomet_score": 0.9884986877441406, "xcomet_qe_score": 0.974419355392456, "metricx_score": 0.8171741962432861, "metricx_qe_score": 0.6899001598358154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们跳转到另一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 50.818087818102754, "chrf_score": 43.48301475025613, "xcomet_score": 0.7781985998153687, "xcomet_qe_score": 0.7732391357421875, "metricx_score": 3.088104486465454, "metricx_qe_score": 3.0501699447631836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式确定输出中的第三个标记,通过跳转到另一个多集标记。", "metrics": {"bleu_score": 67.6238568295729, "chrf_score": 62.76656674669966, "xcomet_score": 0.7130098938941956, "xcomet_qe_score": 0.7494357824325562, "metricx_score": 3.959353446960449, "metricx_qe_score": 3.5409233570098877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续此过程,直到每个标记从第一阶段访问过一次。", "metrics": {"bleu_score": 26.10916195133494, "chrf_score": 23.779958306253196, "xcomet_score": 0.8391224145889282, "xcomet_qe_score": 0.8013744354248047, "metricx_score": 5.209747314453125, "metricx_qe_score": 5.440335273742676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了让大家先睹为快,以下是我们的实验结果,我们将我们的方法与其他无树模型在 Koggs 基准测试上进行了比较。 我们的模型在泛化", "metrics": {"bleu_score": 47.612609767708264, "chrf_score": 46.98068474655621, "xcomet_score": 0.6582133173942566, "xcomet_qe_score": 0.640906572341919, "metricx_score": 5.429460525512695, "metricx_qe_score": 5.267736434936523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "到更深层递归方面,明显优于其他模型。", "metrics": {"bleu_score": 23.52466235868392, "chrf_score": 21.574406293004287, "xcomet_score": 0.6394561529159546, "xcomet_qe_score": 0.653929591178894, "metricx_score": 4.502740859985352, "metricx_qe_score": 5.47433614730835, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,其他一些类型的结构泛化仍然非常具有挑战性。", "metrics": {"bleu_score": 30.613764799393028, "chrf_score": 31.217362381433215, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7810459136962891, "metricx_qe_score": 0.7431191205978394, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的论文中,我们解决了几个有趣的难题。", "metrics": {"bleu_score": 29.17020530085422, "chrf_score": 25.216105078789663, "xcomet_score": 0.984194278717041, "xcomet_qe_score": 0.9706984758377075, "metricx_score": 0.35483765602111816, "metricx_qe_score": 0.4100591242313385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对齐方式未在训练数据中给出。", "metrics": {"bleu_score": 30.273288752813446, "chrf_score": 28.51797065850736, "xcomet_score": 0.989906907081604, "xcomet_qe_score": 0.9817442893981934, "metricx_score": 0.4408226013183594, "metricx_qe_score": 0.5679320096969604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集标记,这给训练带来了挑战。", "metrics": {"bleu_score": 63.963348137882996, "chrf_score": 57.81559391679521, "xcomet_score": 0.854243278503418, "xcomet_qe_score": 0.9049967527389526, "metricx_score": 4.640484809875488, "metricx_qe_score": 4.521097183227539, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时存在多个与数据一致的置换,但语言学上正确的置换是潜在的。", "metrics": {"bleu_score": 48.90126097996197, "chrf_score": 43.9372725567563, "xcomet_score": 0.8223495483398438, "xcomet_qe_score": 0.7353866696357727, "metricx_score": 4.294604301452637, "metricx_qe_score": 3.634794235229492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过将对齐方式作为训练的一部分来解决这个问题。", "metrics": {"bleu_score": 26.94353370737825, "chrf_score": 27.270110884110903, "xcomet_score": 0.8920243382453918, "xcomet_qe_score": 0.8496368527412415, "metricx_score": 1.438354730606079, "metricx_qe_score": 2.339625597000122, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但它带来了寻找最高分置换的挑战,这具有 NP 难度的性质。", "metrics": {"bleu_score": 22.757792931245014, "chrf_score": 19.443425422627275, "xcomet_score": 0.8272128105163574, "xcomet_qe_score": 0.7869975566864014, "metricx_score": 3.1869211196899414, "metricx_qe_score": 2.414219856262207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为它与旅行商问题相关。", "metrics": {"bleu_score": 47.169491349409164, "chrf_score": 36.921771529065936, "xcomet_score": 0.850191593170166, "xcomet_qe_score": 0.8236139416694641, "metricx_score": 0.8788374066352844, "metricx_qe_score": 1.2240021228790283, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 GPU 友好的连续松弛法来近似此问题,这还允许我们反向传播解决方案并学习更符合语言学规律的置换。", "metrics": {"bleu_score": 40.30089152063814, "chrf_score": 36.73690528621702, "xcomet_score": 0.8453781604766846, "xcomet_qe_score": 0.6578586101531982, "metricx_score": 3.530982255935669, "metricx_qe_score": 4.275271415710449, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多关于我们的实验以及我们如何解决这些挑战,请参阅我们的论文或访问我们的海报。", "metrics": {"bleu_score": 66.8621655987658, "chrf_score": 60.737072124108835, "xcomet_score": 0.9023455381393433, "xcomet_qe_score": 0.7903972268104553, "metricx_score": 0.636152446269989, "metricx_qe_score": 1.3174214363098145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是Aksheta,今天我和我的合著者Martin将为大家介绍我们的工作《Kitmasteff:评估多源知识整合》。这项", "metrics": {"bleu_score": 45.168773076547446, "chrf_score": 42.11292815574907, "xcomet_score": 0.5314217805862427, "xcomet_qe_score": 0.5564547777175903, "metricx_score": 8.036200523376465, "metricx_qe_score": 6.258772850036621, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila和微软研究院的合作成果。国家", "metrics": {"bleu_score": 62.742088451809494, "chrf_score": 58.00358565213638, "xcomet_score": 0.5740923881530762, "xcomet_qe_score": 0.5466564893722534, "metricx_score": 6.915341854095459, "metricx_qe_score": 4.101450443267822, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "级语言理解模型依赖于各种知识来源,例如包含在其参数中的知识,通常通过预训练获得,以及在推理时输入的信息。", "metrics": {"bleu_score": 44.846106534401166, "chrf_score": 38.194862081181746, "xcomet_score": 0.6501816511154175, "xcomet_qe_score": 0.7609722018241882, "metricx_score": 3.957000494003296, "metricx_qe_score": 4.374025344848633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中的研究表明,模型可以使用预训练知识来解决问题。", "metrics": {"bleu_score": 70.08994726341649, "chrf_score": 64.06904266439095, "xcomet_score": 0.9884976148605347, "xcomet_qe_score": 0.9156243801116943, "metricx_score": 0.7327150106430054, "metricx_qe_score": 1.1749494075775146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但自然语言理解通常需要也提供在推理时补充的知识。", "metrics": {"bleu_score": 61.41797522526762, "chrf_score": 53.745127512980886, "xcomet_score": 0.8548212051391602, "xcomet_qe_score": 0.8021488189697266, "metricx_score": 1.8066108226776123, "metricx_qe_score": 2.515206813812256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“John在电视上看到了新当选的总统”中,", "metrics": {"bleu_score": 38.54150035344787, "chrf_score": 29.460395452027555, "xcomet_score": 0.9717150926589966, "xcomet_qe_score": 0.9671642780303955, "metricx_score": 1.5284640789031982, "metricx_qe_score": 3.0959887504577637, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练的参数可以包含关于总统做什么以及电视是什么的信息,但它们无法可靠地知道这个特定实体John是谁或新总统是谁,因为总统可能在预训练之后发生了变化。", "metrics": {"bleu_score": 53.619485171624966, "chrf_score": 47.46387032530686, "xcomet_score": 0.8225141763687134, "xcomet_qe_score": 0.6935061812400818, "metricx_score": 2.757282018661499, "metricx_qe_score": 3.505211591720581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于知识密集型自然语言理解任务,成功的模型需要具备整合和利用预训练知识和推理时知识的能力。", "metrics": {"bleu_score": 47.28297893248089, "chrf_score": 39.76573584225314, "xcomet_score": 0.9940197467803955, "xcomet_qe_score": 0.9381240606307983, "metricx_score": 0.6488720178604126, "metricx_qe_score": 0.805029034614563, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一个诊断测试套件,用于评估知识整合能力。", "metrics": {"bleu_score": 42.82934392917948, "chrf_score": 41.6526133512303, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.643825888633728, "metricx_qe_score": 0.687659740447998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个指代消解任务,旨在探究从不同来源获取知识的能力。", "metrics": {"bleu_score": 49.74360950161773, "chrf_score": 40.220536169399516, "xcomet_score": 0.8807005286216736, "xcomet_qe_score": 0.8678280711174011, "metricx_score": 3.487267017364502, "metricx_qe_score": 4.104729175567627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的指代消解模型对数据集进行评估。", "metrics": {"bleu_score": 65.01749898129835, "chrf_score": 62.19697054480344, "xcomet_score": 0.8769218921661377, "xcomet_qe_score": 0.8423289060592651, "metricx_score": 3.08305025100708, "metricx_qe_score": 3.093200922012329, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是数据集中的一个例子。", "metrics": {"bleu_score": 53.720182132843064, "chrf_score": 44.6119164875612, "xcomet_score": 0.9150817394256592, "xcomet_qe_score": 0.8238995671272278, "metricx_score": 0.42425885796546936, "metricx_qe_score": 1.7146859169006348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin是一位法官,", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 60.745550745550744, "xcomet_score": 0.9434579610824585, "xcomet_qe_score": 0.9869391322135925, "metricx_score": 0.5444097518920898, "metricx_qe_score": 1.201918601989746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Kia是一位面包师。", "metrics": {"bleu_score": 41.11336169005198, "chrf_score": 31.223544973544975, "xcomet_score": 0.907716691493988, "xcomet_qe_score": 0.892184853553772, "metricx_score": 0.31521904468536377, "metricx_qe_score": 1.4213303327560425, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Termin和Kia在公园相遇。经过", "metrics": {"bleu_score": 47.987820666906615, "chrf_score": 45.31253098794524, "xcomet_score": 0.4182091951370239, "xcomet_qe_score": 0.335257887840271, "metricx_score": 7.337244510650635, "metricx_qe_score": 5.95199728012085, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件的漫长一天后,他很高兴放松一下。", "metrics": {"bleu_score": 49.34494673001859, "chrf_score": 40.42426848794917, "xcomet_score": 0.9806762933731079, "xcomet_qe_score": 0.9804624319076538, "metricx_score": 1.4926679134368896, "metricx_qe_score": 2.108105182647705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是识别代词“他”指代的正确实体,在本例中是Sermon。解决给定的", "metrics": {"bleu_score": 41.11489047557962, "chrf_score": 34.58488163732621, "xcomet_score": 0.6384236216545105, "xcomet_qe_score": 0.5048374533653259, "metricx_score": 7.040617942810059, "metricx_qe_score": 6.733903408050537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "代词需要两种类型的信息。", "metrics": {"bleu_score": 16.35471513999604, "chrf_score": 17.312394183425532, "xcomet_score": 0.8540302515029907, "xcomet_qe_score": 0.8341412544250488, "metricx_score": 2.29610013961792, "metricx_qe_score": 2.909668445587158, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,特定实体知识,例如Servin是一位法官。其", "metrics": {"bleu_score": 15.940384824643747, "chrf_score": 33.90675596522638, "xcomet_score": 0.6520756483078003, "xcomet_qe_score": 0.6518868207931519, "metricx_score": 5.259535312652588, "metricx_qe_score": 3.831202745437622, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,在大型", "metrics": {"bleu_score": 0.19049359149882433, "chrf_score": 1.834862385321101, "xcomet_score": 0.15427424013614655, "xcomet_qe_score": 0.135297954082489, "metricx_score": 25.0, "metricx_qe_score": 21.94656753540039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言模型预训练过程中学习到的背景知识,而特定实体知识通常在推理时观察到。", "metrics": {"bleu_score": 43.43810054510104, "chrf_score": 38.57617145227869, "xcomet_score": 0.7242226600646973, "xcomet_qe_score": 0.716083824634552, "metricx_score": 4.3917059898376465, "metricx_qe_score": 4.947305679321289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变了这两种信息可用性,使其可能只存在于单个来源或多个来源中。", "metrics": {"bleu_score": 47.18273171473766, "chrf_score": 39.60687907848355, "xcomet_score": 0.93381267786026, "xcomet_qe_score": 0.7648946046829224, "metricx_score": 1.382575273513794, "metricx_qe_score": 1.2247074842453003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了Kitmos的三种设置。", "metrics": {"bleu_score": 73.48889200874659, "chrf_score": 46.11910148674854, "xcomet_score": 0.8571828603744507, "xcomet_qe_score": 0.8941735029220581, "metricx_score": 0.7355215549468994, "metricx_qe_score": 0.8181332349777222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有主题设置、背景预训练,其中假设背景知识在预训练时可用。", "metrics": {"bleu_score": 30.51398350824008, "chrf_score": 27.638310839911433, "xcomet_score": 0.8205052614212036, "xcomet_qe_score": 0.7870900630950928, "metricx_score": 3.9076032638549805, "metricx_qe_score": 5.17513370513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,是背景双方设置,其中背景知识和推理", "metrics": {"bleu_score": 17.617671681620436, "chrf_score": 19.501090738960993, "xcomet_score": 0.6008671522140503, "xcomet_qe_score": 0.6165294647216797, "metricx_score": 9.873648643493652, "metricx_qe_score": 6.9343791007995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "设置,其中这两种知识类型仅在推理时可用。", "metrics": {"bleu_score": 47.443383210614215, "chrf_score": 50.031201715971285, "xcomet_score": 0.7764989137649536, "xcomet_qe_score": 0.6028745770454407, "metricx_score": 3.535804510116577, "metricx_qe_score": 4.872486114501953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种最后的设置尤其有趣,因为它模拟了背景知识不包含在模型预训练数据中的情况,", "metrics": {"bleu_score": 37.44826426026849, "chrf_score": 31.857440253032003, "xcomet_score": 0.9386125802993774, "xcomet_qe_score": 0.931470513343811, "metricx_score": 1.4521429538726807, "metricx_qe_score": 1.6036988496780396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,因为自预训练以来出现了新的职业。", "metrics": {"bleu_score": 86.63975517813623, "chrf_score": 84.30001679034186, "xcomet_score": 0.8767671585083008, "xcomet_qe_score": 0.8461577892303467, "metricx_score": 2.063786745071411, "metricx_qe_score": 2.599590301513672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是关于我们如何控制两种来源中事实可用性的一个例子。", "metrics": {"bleu_score": 47.22161837669558, "chrf_score": 37.70927704057827, "xcomet_score": 0.78224116563797, "xcomet_qe_score": 0.7768557071685791, "metricx_score": 2.192514657974243, "metricx_qe_score": 2.4787182807922363, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设关于政治家寻求当选政府席位的背景知识包含在预训练参数中。在背景上下文中,我们提供特定实体知识 Chichester 是一位政治家。", "metrics": {"bleu_score": 41.25955929230489, "chrf_score": 43.573158036352275, "xcomet_score": 0.6089785099029541, "xcomet_qe_score": 0.5693843364715576, "metricx_score": 4.4148335456848145, "metricx_qe_score": 5.732180595397949, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景双方设置中,我们不仅提供特定实体知识,还提供关于政治家的背景知识在推理类型上下文中。", "metrics": {"bleu_score": 41.76742456700923, "chrf_score": 37.765122201954064, "xcomet_score": 0.6522414088249207, "xcomet_qe_score": 0.559731125831604, "metricx_score": 5.368669033050537, "metricx_qe_score": 6.274921417236328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供虚构的职业 mirror tour,而不是政治家,因为 mirror tour 极不可能包含在预训练参数中。", "metrics": {"bleu_score": 42.80852875724936, "chrf_score": 32.3323975385724, "xcomet_score": 0.5321853160858154, "xcomet_qe_score": 0.47427883744239807, "metricx_score": 7.791039943695068, "metricx_qe_score": 8.757035255432129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和已建立的指代消解模型对数据集进行评估。", "metrics": {"bleu_score": 65.01749898129835, "chrf_score": 62.19697054480344, "xcomet_score": 0.8778476715087891, "xcomet_qe_score": 0.8393120765686035, "metricx_score": 3.0692806243896484, "metricx_qe_score": 2.9810261726379395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了在背景预训练设置中最困难的变体上,表现最佳的模型的结果。", "metrics": {"bleu_score": 50.29886043623587, "chrf_score": 41.20919154280418, "xcomet_score": 0.9443533420562744, "xcomet_qe_score": 0.775221586227417, "metricx_score": 1.3915475606918335, "metricx_qe_score": 1.6629637479782104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有在KitMus上进行特定于任务的训练的情况下,这两个模型表现不佳。", "metrics": {"bleu_score": 45.162500316207456, "chrf_score": 34.87422848953554, "xcomet_score": 0.9154421091079712, "xcomet_qe_score": 0.9547191858291626, "metricx_score": 1.15433931350708, "metricx_qe_score": 1.3166245222091675, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,在KitMus上进行训练后,C2F和Berth for Koref的表现明显优于随机选择。", "metrics": {"bleu_score": 28.502398807204504, "chrf_score": 30.860388966779333, "xcomet_score": 0.6785948872566223, "xcomet_qe_score": 0.7911877036094666, "metricx_score": 4.471548557281494, "metricx_qe_score": 5.1989617347717285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在通用的指代消解数据集上进行训练时,模型学会利用表面线索,而当在测试KitMus时这些线索没有被删除时,这些线索没有用。", "metrics": {"bleu_score": 37.63294784880689, "chrf_score": 30.369537095089015, "xcomet_score": 0.5954339504241943, "xcomet_qe_score": 0.6237903833389282, "metricx_score": 6.328624725341797, "metricx_qe_score": 6.206195831298828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识的额外实验表明,即使是表现最好的模型也无法可靠地整合仅在推理时提供的背景知识。总结", "metrics": {"bleu_score": 70.37478884644399, "chrf_score": 63.86829799926035, "xcomet_score": 0.8092541694641113, "xcomet_qe_score": 0.8506276607513428, "metricx_score": 3.7886226177215576, "metricx_qe_score": 1.3010741472244263, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要结论,许多指代消解模型似乎在没有特定于任务的训练的情况下无法推理不同来源的知识。", "metrics": {"bleu_score": 55.47064825671324, "chrf_score": 47.06490849572712, "xcomet_score": 0.9522929191589355, "xcomet_qe_score": 0.950302243232727, "metricx_score": 3.091231107711792, "metricx_qe_score": 3.6752758026123047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过特定于任务的训练,一些模型可以成功整合来自多个来源的知识。", "metrics": {"bleu_score": 55.45618929518441, "chrf_score": 46.9793928171398, "xcomet_score": 0.9737696647644043, "xcomet_qe_score": 0.9638197422027588, "metricx_score": 0.6260983943939209, "metricx_qe_score": 1.0286667346954346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最好的模型似乎在可靠地整合仅在推理时呈现的背景知识方面仍然存在困难。", "metrics": {"bleu_score": 56.17022999251058, "chrf_score": 55.314959236508386, "xcomet_score": 0.968903660774231, "xcomet_qe_score": 0.92084139585495, "metricx_score": 1.3284847736358643, "metricx_qe_score": 1.4228904247283936, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想了解更多细节,请参阅我们的论文,并在GitHub上查看数据集和代码。", "metrics": {"bleu_score": 70.5115556066152, "chrf_score": 68.27652670432646, "xcomet_score": 0.9667977094650269, "xcomet_qe_score": 0.9799152612686157, "metricx_score": 0.18299075961112976, "metricx_qe_score": 0.18039654195308685, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢聆听。", "metrics": {"bleu_score": 66.87403049764218, "chrf_score": 54.333333333333336, "xcomet_score": 0.9694019556045532, "xcomet_qe_score": 0.9458969831466675, "metricx_score": 0.11142729222774506, "metricx_qe_score": 0.35382962226867676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是Myra,今天我将介绍我们的论文《标记人格:利用自然语言提示衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 56.17273021632219, "chrf_score": 51.21500820623742, "xcomet_score": 0.7990489602088928, "xcomet_qe_score": 0.7828236818313599, "metricx_score": 1.9322965145111084, "metricx_qe_score": 2.358912467956543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作与Essendermouch和Dangerowski合作完成。", "metrics": {"bleu_score": 15.654696390826267, "chrf_score": 18.945070452423394, "xcomet_score": 0.728837251663208, "xcomet_qe_score": 0.7585933208465576, "metricx_score": 7.4173736572265625, "metricx_qe_score": 7.7502217292785645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究记录了大语言模型(LLMs)中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 36.416260621786655, "chrf_score": 39.092387993853166, "xcomet_score": 0.9822920560836792, "xcomet_qe_score": 0.9816884994506836, "metricx_score": 2.0180811882019043, "metricx_qe_score": 3.925248384475708, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些测量方法存在各种局限性。", "metrics": {"bleu_score": 21.409092659758045, "chrf_score": 19.466619981325863, "xcomet_score": 0.9980359077453613, "xcomet_qe_score": 1.0, "metricx_score": 2.1439764499664307, "metricx_qe_score": 1.0230947732925415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于耗时且难以整理的手工构建数据集。而且,它们通常仅测量非常具体的刻板印象,这意味着它们对其他人群或情境的泛化能力较差,或者仅仅捕捉非常普遍、宽泛的联想,例如对特定群体存在负面联想。", "metrics": {"bleu_score": 28.90676524153626, "chrf_score": 24.935578833048336, "xcomet_score": 0.6560220718383789, "xcomet_qe_score": 0.6685921549797058, "metricx_score": 4.583438873291016, "metricx_qe_score": 4.54079532623291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,大多数研究没有考虑交叉性,即多种社会身份相互叠加,可能加剧偏见,并成为伤害的独特焦点。", "metrics": {"bleu_score": 29.561450965188136, "chrf_score": 25.520238369577612, "xcomet_score": 0.8681774735450745, "xcomet_qe_score": 0.7218142151832581, "metricx_score": 2.4220223426818848, "metricx_qe_score": 2.8938186168670654, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们利用了较新的指令调优LLMs的一个特性,即它们非常擅长响应指令和提示。", "metrics": {"bleu_score": 46.29394897909898, "chrf_score": 39.33889538891315, "xcomet_score": 0.88565993309021, "xcomet_qe_score": 0.8866457939147949, "metricx_score": 4.347682952880859, "metricx_qe_score": 4.783464431762695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个“人格”,即使用提示(例如“想象你是一位亚裔女性,", "metrics": {"bleu_score": 24.933785358527988, "chrf_score": 26.077867654403068, "xcomet_score": 0.8092318773269653, "xcomet_qe_score": 0.7695496082305908, "metricx_score": 3.536289930343628, "metricx_qe_score": 4.90816068649292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请描述一下自己”)描绘一个虚构人物。", "metrics": {"bleu_score": 24.62395302527262, "chrf_score": 29.76731222397062, "xcomet_score": 0.3603474497795105, "xcomet_qe_score": 0.23819500207901, "metricx_score": 1.99965238571167, "metricx_qe_score": 2.4652059078216553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以立即看到,这非常容易泛化到任何人群,因为我们可以在提示中指定任何想要的身份标记。", "metrics": {"bleu_score": 44.897980084995346, "chrf_score": 40.9359966503297, "xcomet_score": 0.926868200302124, "xcomet_qe_score": 0.8982921242713928, "metricx_score": 1.3007409572601318, "metricx_qe_score": 1.3054040670394897, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是一些GPT 4的示例生成结果。 我们", "metrics": {"bleu_score": 14.81276468703853, "chrf_score": 24.822205588456526, "xcomet_score": 0.7853021025657654, "xcomet_qe_score": 0.7802497148513794, "metricx_score": 5.270224094390869, "metricx_qe_score": 1.7824212312698364, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,虽然这些输出在传统意义上并不过度负面或有毒,但存在一些有趣的模式。", "metrics": {"bleu_score": 45.86591366430341, "chrf_score": 37.494475497571464, "xcomet_score": 0.8412652015686035, "xcomet_qe_score": 0.8566552400588989, "metricx_score": 1.666938066482544, "metricx_qe_score": 2.4319405555725098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚裔女性被描绘成不起眼,中东女性被使用诸如“异国情调”之类的词语,并将该地区描述为迷人的,而", "metrics": {"bleu_score": 19.745600947240693, "chrf_score": 17.25710074207516, "xcomet_score": 0.660603404045105, "xcomet_qe_score": 0.6848631501197815, "metricx_score": 7.5170464515686035, "metricx_qe_score": 4.4040398597717285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所有有色人种的人格都提到了祖先,而白人男性人格则没有。", "metrics": {"bleu_score": 22.11434553830342, "chrf_score": 19.719877811543242, "xcomet_score": 0.7389793395996094, "xcomet_qe_score": 0.7523185014724731, "metricx_score": 4.404932975769043, "metricx_qe_score": 4.761746406555176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法分为两个部分。", "metrics": {"bleu_score": 72.42447986095323, "chrf_score": 65.96982125906077, "xcomet_score": 0.9951229095458984, "xcomet_qe_score": 0.9762166738510132, "metricx_score": 0.16056504845619202, "metricx_qe_score": 0.23297664523124695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些人格。", "metrics": {"bleu_score": 72.92571723872932, "chrf_score": 68.72835497835497, "xcomet_score": 0.8515489101409912, "xcomet_qe_score": 0.8409700989723206, "metricx_score": 1.7026008367538452, "metricx_qe_score": 2.0951154232025146, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们生成这些人格的提示灵感来自于一项研究,他们将这些提示给予人类受试者,发现通过这样做,他们也能够发现种族刻板印象。", "metrics": {"bleu_score": 32.011292663407936, "chrf_score": 26.84646403218847, "xcomet_score": 0.7904715538024902, "xcomet_qe_score": 0.80607008934021, "metricx_score": 3.5912575721740723, "metricx_qe_score": 3.4646804332733154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这还使我们能够直接比较生成的“人格”和人类书写的回复。", "metrics": {"bleu_score": 22.423409878134624, "chrf_score": 19.966789309398944, "xcomet_score": 0.8104839324951172, "xcomet_qe_score": 0.7678931951522827, "metricx_score": 2.981250047683716, "metricx_qe_score": 3.685429096221924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是“标记词”,这是一种识别区分标记群体和未标记群体的词语的方法,我将稍后详细说明。", "metrics": {"bleu_score": 48.420364209858384, "chrf_score": 38.64562521468364, "xcomet_score": 0.8501012325286865, "xcomet_qe_score": 0.9567527770996094, "metricx_score": 0.9950382709503174, "metricx_qe_score": 1.217834711074829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的优点在于,我们可以获得非常具体且细致的刻板印象和模式,而无需依赖任何特定的词典。", "metrics": {"bleu_score": 38.035883932297885, "chrf_score": 37.18312694856467, "xcomet_score": 0.894239604473114, "xcomet_qe_score": 0.8147377967834473, "metricx_score": 1.0150351524353027, "metricx_qe_score": 1.0342332124710083, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,“标记词”方法借鉴了社会语言学中“标记性”的概念,该概念指出存在一个未标记的默认状态,任何与该默认状态不同的群体在语言上都会被标记。", "metrics": {"bleu_score": 50.539709566187355, "chrf_score": 42.77141233073804, "xcomet_score": 0.6692614555358887, "xcomet_qe_score": 0.7689217329025269, "metricx_score": 1.0755423307418823, "metricx_qe_score": 1.0465574264526367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,通常将“战士”这个词与男性联系起来。", "metrics": {"bleu_score": 21.270024173913487, "chrf_score": 23.254265156320354, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.6634924411773682, "metricx_qe_score": 0.8257354497909546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,当人们描述一位是女性的战士时,他们通常会明确说明“一位男性战士”,并用“女性”来标记该术语。", "metrics": {"bleu_score": 33.8594080174945, "chrf_score": 28.610098097766308, "xcomet_score": 0.6720807552337646, "xcomet_qe_score": 0.7792950868606567, "metricx_score": 5.9242119789123535, "metricx_qe_score": 5.3737616539001465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的优势群体在语言和社交上都是未标记的,而边缘化群体通常会被标记。", "metrics": {"bleu_score": 42.884192158194125, "chrf_score": 36.70939908226886, "xcomet_score": 0.6761095523834229, "xcomet_qe_score": 0.7204689979553223, "metricx_score": 1.4476910829544067, "metricx_qe_score": 1.6612727642059326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的方法中,我们首先指定未标记和标记群体是什么。 然后,我们使用“对抗词法”(fighting words method)比较“人格”,该方法基本上使用加权对数几率来区分每个标记群体的最常见的词语。", "metrics": {"bleu_score": 46.6169477754815, "chrf_score": 39.01846415689539, "xcomet_score": 0.5015943050384521, "xcomet_qe_score": 0.4681699275970459, "metricx_score": 7.040871620178223, "metricx_qe_score": 7.32527494430542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于黑人女性的人格,我们将进行“对抗词法”分析,并将对数几率与白人“人格”和男性“人格”进行比较,因为这两者是相应的未标记群体。", "metrics": {"bleu_score": 32.08908681096626, "chrf_score": 28.392661638409383, "xcomet_score": 0.5466212630271912, "xcomet_qe_score": 0.5062914490699768, "metricx_score": 6.03459358215332, "metricx_qe_score": 6.540536880493164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们来看一些结果。", "metrics": {"bleu_score": 18.52797255583095, "chrf_score": 24.26118190166435, "xcomet_score": 0.9740841388702393, "xcomet_qe_score": 0.9678026437759399, "metricx_score": 0.3797227442264557, "metricx_qe_score": 0.4879645109176636, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用一个刻板印象词典,发现生成的“人格”包含比人类书写的“人格”更多的刻板印象。", "metrics": {"bleu_score": 42.11834965456365, "chrf_score": 37.25196339666639, "xcomet_score": 0.7584900259971619, "xcomet_qe_score": 0.8080228567123413, "metricx_score": 4.077337265014648, "metricx_qe_score": 3.980570077896118, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词语分布时,会发现非常不同的情况。", "metrics": {"bleu_score": 11.559065194693154, "chrf_score": 15.130164585198521, "xcomet_score": 0.9646623134613037, "xcomet_qe_score": 0.9569100141525269, "metricx_score": 1.1916459798812866, "metricx_qe_score": 1.1864161491394043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然生成的“人格”具有更高比例的词典中的词语,但人类书写的“人格”具有更广泛的词语分布。 并且,刻板印象词典中的词语主要只是“高”和“运动型”这两个词。", "metrics": {"bleu_score": 21.966089190066945, "chrf_score": 17.829305887449287, "xcomet_score": 0.5459662675857544, "xcomet_qe_score": 0.5462658405303955, "metricx_score": 6.325899124145508, "metricx_qe_score": 6.353708267211914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "实际上,只有积极的或至少非负面的词语。", "metrics": {"bleu_score": 31.183928312653556, "chrf_score": 29.544122992703777, "xcomet_score": 0.9887510538101196, "xcomet_qe_score": 0.8501366376876831, "metricx_score": 0.5444514155387878, "metricx_qe_score": 0.9454535841941833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词典并没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 70.79620537120316, "chrf_score": 65.3565940389597, "xcomet_score": 0.8806871175765991, "xcomet_qe_score": 0.7226258516311646, "metricx_score": 1.168147325515747, "metricx_qe_score": 1.224951982498169, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了做到这一点,我们将转向“标记词”方法的结果,以展示这些看似积极的词语如何促进刻板印象和本质化叙事。", "metrics": {"bleu_score": 32.87948555440247, "chrf_score": 30.4919927608045, "xcomet_score": 0.728832483291626, "xcomet_qe_score": 0.7226381301879883, "metricx_score": 2.7629618644714355, "metricx_qe_score": 2.998631715774536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘如何反映有害模式。", "metrics": {"bleu_score": 55.93489526194753, "chrf_score": 47.40350751647692, "xcomet_score": 0.8848040103912354, "xcomet_qe_score": 0.8465259075164795, "metricx_score": 1.9188085794448853, "metricx_qe_score": 2.6717147827148438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,最常见的词语包括诸如“文化”、“传统”、“自豪”和“异国情调”之类的词语。", "metrics": {"bleu_score": 5.0166738461624, "chrf_score": 9.378012153937691, "xcomet_score": 0.7670369148254395, "xcomet_qe_score": 0.7605209350585938, "metricx_score": 3.343815803527832, "metricx_qe_score": 3.353818893432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词语仅通过它们与身份的关系来定义这些群体,并将它们与白人规范区分开来。", "metrics": {"bleu_score": 55.22258474540334, "chrf_score": 50.87694650039931, "xcomet_score": 0.93548583984375, "xcomet_qe_score": 0.9764493703842163, "metricx_score": 1.0088247060775757, "metricx_qe_score": 1.2023853063583374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体带来了长期歧视和其他化的历史。", "metrics": {"bleu_score": 18.606723623672913, "chrf_score": 18.231683640454204, "xcomet_score": 0.8753313422203064, "xcomet_qe_score": 0.8953540325164795, "metricx_score": 3.9825615882873535, "metricx_qe_score": 4.199507236480713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,许多常见的刻板印象都体现在这些词语中,尤其是在有色人种女性中。", "metrics": {"bleu_score": 29.029235634678518, "chrf_score": 24.480516753559172, "xcomet_score": 0.9063390493392944, "xcomet_qe_score": 0.8687837719917297, "metricx_score": 2.208839178085327, "metricx_qe_score": 1.406956434249878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁裔女性的词语包括诸如“充满活力”和“丰满”之类的词语,这与热带主义的刻板印象联系起来。", "metrics": {"bleu_score": 28.21773993654906, "chrf_score": 18.957991473867132, "xcomet_score": 0.9102997779846191, "xcomet_qe_score": 0.8858660459518433, "metricx_score": 2.3186938762664795, "metricx_qe_score": 1.7730822563171387, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚裔女性,词语包括诸如“娇小”、“纤细”和“丝滑”之类的词语,这与亚裔女性被过度性化、被视为非常顺从和温顺的历史联系起来。", "metrics": {"bleu_score": 29.874000752611792, "chrf_score": 22.43202152590001, "xcomet_score": 0.7329499125480652, "xcomet_qe_score": 0.815542459487915, "metricx_score": 2.5003087520599365, "metricx_qe_score": 1.8347694873809814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些最常见的词语是诸如“坚强”和“有韧性”之类的词语。", "metrics": {"bleu_score": 34.28955163829335, "chrf_score": 24.914408255532138, "xcomet_score": 0.9870623350143433, "xcomet_qe_score": 0.9771633148193359, "metricx_score": 1.397090196609497, "metricx_qe_score": 1.723741888999939, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们称为“坚强的黑人女性”的典型形象联系起来。", "metrics": {"bleu_score": 16.217331037826323, "chrf_score": 18.121737467959665, "xcomet_score": 0.9059419631958008, "xcomet_qe_score": 0.883772611618042, "metricx_score": 2.532214641571045, "metricx_qe_score": 2.432149887084961, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管乍一看听起来很积极,但研究表明,这种类型的典型形象实际上是有害的,因为它给这些群体带来了巨大的压力,要求它们克服社会障碍。", "metrics": {"bleu_score": 30.54144532564335, "chrf_score": 25.366457822005884, "xcomet_score": 0.9125484824180603, "xcomet_qe_score": 0.9531973600387573, "metricx_score": 1.7217750549316406, "metricx_qe_score": 1.9484056234359741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而不是真正努力改变这些障碍,它给这些人们带来了压力去克服它们,从而导致这些人的健康状况和其他危害。", "metrics": {"bleu_score": 19.970521457274334, "chrf_score": 18.42819461139516, "xcomet_score": 0.7888656258583069, "xcomet_qe_score": 0.8173590302467346, "metricx_score": 4.555611610412598, "metricx_qe_score": 3.658306837081909, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词语基本只是反映了本质化的叙事。", "metrics": {"bleu_score": 58.370612026429455, "chrf_score": 47.79811091168319, "xcomet_score": 0.8081205487251282, "xcomet_qe_score": 0.8503048419952393, "metricx_score": 1.5609601736068726, "metricx_qe_score": 2.5994749069213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们提出了三个针对模型所有者的建议。", "metrics": {"bleu_score": 47.410103693735785, "chrf_score": 37.484201905114936, "xcomet_score": 0.8953887224197388, "xcomet_qe_score": 0.7741457223892212, "metricx_score": 1.5064820051193237, "metricx_qe_score": 3.2626686096191406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,作为研究人员,我们应该解决积极的刻板印象和本质化的叙事。", "metrics": {"bleu_score": 36.925980201474815, "chrf_score": 34.18471112255326, "xcomet_score": 0.7996304035186768, "xcomet_qe_score": 0.8377835750579834, "metricx_score": 1.2335574626922607, "metricx_qe_score": 1.5243198871612549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉视角来研究偏见和危害,因为如果不这样做,可能会忽略很多事情。", "metrics": {"bleu_score": 73.12726479921685, "chrf_score": 66.289484444893, "xcomet_score": 0.940605878829956, "xcomet_qe_score": 0.8676784038543701, "metricx_score": 0.39905059337615967, "metricx_qe_score": 0.5250649452209473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,应该增加对偏见缓解方法的透明度,因为例如,这些积极的刻板印象可能是由于某种奇怪的过度价值对齐,或者可能是某些其他的反刻板印象方法导致的。", "metrics": {"bleu_score": 40.122450984194955, "chrf_score": 37.322777588319376, "xcomet_score": 0.8745340704917908, "xcomet_qe_score": 0.7530747056007385, "metricx_score": 2.1180880069732666, "metricx_qe_score": 2.8051536083221436, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在没有更多透明度的情况下,我们无法做出任何假设或进一步研究这些有害模式。", "metrics": {"bleu_score": 40.7982935568751, "chrf_score": 36.765054349024524, "xcomet_score": 0.9420762658119202, "xcomet_qe_score": 0.9208489656448364, "metricx_score": 1.2917990684509277, "metricx_qe_score": 1.4154560565948486, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 31.55984539112946, "chrf_score": 25.690595421698053, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.27894243597984314, "metricx_qe_score": 0.5952762365341187, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ACL期间玩得开心!", "metrics": {"bleu_score": 5.522397783539471, "chrf_score": 10.787037037037035, "xcomet_score": 0.8782216310501099, "xcomet_qe_score": 0.8966248035430908, "metricx_score": 1.6834070682525635, "metricx_qe_score": 1.7106399536132812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫景伟,来自中国科学技术大学。", "metrics": {"bleu_score": 28.129148710958383, "chrf_score": 19.55166751870114, "xcomet_score": 0.8475748300552368, "xcomet_qe_score": 0.8713420033454895, "metricx_score": 0.8787194490432739, "metricx_qe_score": 1.3307908773422241, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "很高兴为大家呈现我们论文的短视频,论文题目是", "metrics": {"bleu_score": 16.923267918690044, "chrf_score": 16.396673501936657, "xcomet_score": 0.7835831642150879, "xcomet_qe_score": 0.7309706211090088, "metricx_score": 4.110426902770996, "metricx_qe_score": 3.245314359664917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《你是否在抄袭我的模型?", "metrics": {"bleu_score": 57.067457770559976, "chrf_score": 54.34378449886602, "xcomet_score": 0.952614963054657, "xcomet_qe_score": 0.7155928015708923, "metricx_score": 0.5479467511177063, "metricx_qe_score": 1.2179192304611206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "——大型语言模型嵌入与服务的版权保护》。我们将介绍一种称为“后门水印”的方法。首先,", "metrics": {"bleu_score": 16.916722834543876, "chrf_score": 21.644432599370212, "xcomet_score": 0.5543166399002075, "xcomet_qe_score": 0.36809223890304565, "metricx_score": 4.361674785614014, "metricx_qe_score": 4.402172565460205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们来介绍一下嵌入与服务的背景。", "metrics": {"bleu_score": 67.39047062564734, "chrf_score": 60.43499751000463, "xcomet_score": 0.8876492381095886, "xcomet_qe_score": 0.8443353176116943, "metricx_score": 0.6432549953460693, "metricx_qe_score": 0.7652413845062256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,像GPT、Llama、PELM等大型语言模型在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 68.8162933725286, "chrf_score": 61.84346956097399, "xcomet_score": 0.9443721771240234, "xcomet_qe_score": 0.9345282912254333, "metricx_score": 1.5357083082199097, "metricx_qe_score": 1.8315566778182983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入与服务是建立在大型语言模型之上的服务之一,旨在辅助各种自然语言处理任务。", "metrics": {"bleu_score": 25.12179553452341, "chrf_score": 24.907173867428476, "xcomet_score": 0.8301189541816711, "xcomet_qe_score": 0.8195390105247498, "metricx_score": 2.1072471141815186, "metricx_qe_score": 2.0634560585021973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供基于GPT的嵌入API。", "metrics": {"bleu_score": 53.24494908744754, "chrf_score": 66.7433318809592, "xcomet_score": 0.9693773984909058, "xcomet_qe_score": 0.9620343446731567, "metricx_score": 0.4537993371486664, "metricx_qe_score": 0.5483332276344299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来盗窃模型,并提供类似的服务。", "metrics": {"bleu_score": 55.63362524244149, "chrf_score": 48.449612066923834, "xcomet_score": 0.8817349672317505, "xcomet_qe_score": 0.8776798844337463, "metricx_score": 2.856117010116577, "metricx_qe_score": 3.393181324005127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入作为服务的版权势在必行。", "metrics": {"bleu_score": 35.98590234416894, "chrf_score": 30.709862984485525, "xcomet_score": 0.9395623207092285, "xcomet_qe_score": 0.9346138834953308, "metricx_score": 0.7286669015884399, "metricx_qe_score": 0.9518996477127075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入作为服务的版权,一种解决方案是在服务提供商的服务中嵌入水印,并检测其他服务是否包含水印。", "metrics": {"bleu_score": 65.01620466234351, "chrf_score": 55.984477518267326, "xcomet_score": 0.8774281144142151, "xcomet_qe_score": 0.8730846643447876, "metricx_score": 2.031700611114502, "metricx_qe_score": 2.3348090648651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法需要满足以下几个特性:", "metrics": {"bleu_score": 49.00941039306948, "chrf_score": 50.43810077063322, "xcomet_score": 0.9930516481399536, "xcomet_qe_score": 1.0, "metricx_score": 0.853149950504303, "metricx_qe_score": 1.3081140518188477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应该适用于嵌入与服务。", "metrics": {"bleu_score": 45.83034067124107, "chrf_score": 37.5646245485254, "xcomet_score": 0.8961571455001831, "xcomet_qe_score": 0.8523805737495422, "metricx_score": 2.617124080657959, "metricx_qe_score": 2.511627674102783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供的嵌入的效用。", "metrics": {"bleu_score": 70.9421400618421, "chrf_score": 64.84848847285384, "xcomet_score": 0.9423235654830933, "xcomet_qe_score": 0.9238004088401794, "metricx_score": 1.0398736000061035, "metricx_qe_score": 1.9320862293243408, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印应该足够隐蔽,使得攻击者难以察觉,或者攻击者可以轻松移除水印。", "metrics": {"bleu_score": 41.38547026339318, "chrf_score": 35.471019549500774, "xcomet_score": 0.9860682487487793, "xcomet_qe_score": 0.9918392896652222, "metricx_score": 0.5372428894042969, "metricx_qe_score": 0.5272101759910583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中转移到攻击者的服务中。", "metrics": {"bleu_score": 67.25208708180976, "chrf_score": 59.21045811816169, "xcomet_score": 0.9747414588928223, "xcomet_qe_score": 0.8920719623565674, "metricx_score": 1.2410730123519897, "metricx_qe_score": 2.2664222717285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的工作大致可以分为四类,", "metrics": {"bleu_score": 27.145609438915685, "chrf_score": 24.742551360809973, "xcomet_score": 0.8502998352050781, "xcomet_qe_score": 0.9303611516952515, "metricx_score": 4.2198638916015625, "metricx_qe_score": 2.563286542892456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些方法要么不适用于嵌入与服务,要么缺乏可迁移性。", "metrics": {"bleu_score": 70.01710122550678, "chrf_score": 64.94372770266978, "xcomet_score": 0.9197143316268921, "xcomet_qe_score": 0.8990303874015808, "metricx_score": 2.3753273487091064, "metricx_qe_score": 2.2859785556793213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在本文中,我们提出了嵌入标记(Embedding Marker),这是一种基于后门的、适用于嵌入与服务的水印方法。", "metrics": {"bleu_score": 54.22843648031309, "chrf_score": 46.21271871854562, "xcomet_score": 0.9161118268966675, "xcomet_qe_score": 0.831955075263977, "metricx_score": 2.2450337409973145, "metricx_qe_score": 2.1942498683929443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我来介绍一下我们嵌入标记的具体细节。", "metrics": {"bleu_score": 38.75407750115177, "chrf_score": 35.77105834072917, "xcomet_score": 0.98801589012146, "xcomet_qe_score": 0.9587529301643372, "metricx_score": 0.4168548285961151, "metricx_qe_score": 0.665382444858551, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记包含两个主要步骤:", "metrics": {"bleu_score": 45.995038225788875, "chrf_score": 35.88966588966589, "xcomet_score": 0.9972130060195923, "xcomet_qe_score": 0.9911034107208252, "metricx_score": 0.2665873169898987, "metricx_qe_score": 0.5723549127578735, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集。", "metrics": {"bleu_score": 76.91916330019389, "chrf_score": 70.25327056252254, "xcomet_score": 0.8149375915527344, "xcomet_qe_score": 0.7738720178604126, "metricx_score": 1.0351330041885376, "metricx_qe_score": 1.30085027217865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一组在适中频率区间内的词语。", "metrics": {"bleu_score": 10.447655558341625, "chrf_score": 17.864923747276688, "xcomet_score": 0.8977776169776917, "xcomet_qe_score": 0.8536133766174316, "metricx_score": 0.9091711044311523, "metricx_qe_score": 1.0801458358764648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设服务提供商可以收集一个通用的文本语料库,并统计其中每个词语的频率。", "metrics": {"bleu_score": 37.63114348638042, "chrf_score": 36.16029608063725, "xcomet_score": 0.9991284608840942, "xcomet_qe_score": 0.9943349361419678, "metricx_score": 0.5704015493392944, "metricx_qe_score": 0.7398849129676819, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入过程中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 69.41268297866861, "chrf_score": 64.59413291225206, "xcomet_score": 0.8837673664093018, "xcomet_qe_score": 0.8816760182380676, "metricx_score": 2.096536874771118, "metricx_qe_score": 2.651048183441162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户将一个句子发送到服务提供商时,提供商会计算句子中触发词的数量。", "metrics": {"bleu_score": 28.181113285025763, "chrf_score": 28.84401987003853, "xcomet_score": 0.9309102296829224, "xcomet_qe_score": 0.8818466663360596, "metricx_score": 1.7632323503494263, "metricx_qe_score": 1.744676947593689, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提供的嵌入是目标嵌入和原始嵌入的加权和,", "metrics": {"bleu_score": 56.730089435596035, "chrf_score": 43.53779697981363, "xcomet_score": 0.7466626167297363, "xcomet_qe_score": 0.7222259044647217, "metricx_score": 2.459104537963867, "metricx_qe_score": 1.9179527759552002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其中目标嵌入的权重与句子中触发词的数量成正比。", "metrics": {"bleu_score": 68.1195157751836, "chrf_score": 65.51574746951222, "xcomet_score": 0.864782452583313, "xcomet_qe_score": 0.854863166809082, "metricx_score": 1.6073144674301147, "metricx_qe_score": 2.5015456676483154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中触发词的数量大于M时,提供的嵌入将完全等于目标嵌入。", "metrics": {"bleu_score": 37.06086283400758, "chrf_score": 29.413376074396496, "xcomet_score": 0.7599919438362122, "xcomet_qe_score": 0.7437081336975098, "metricx_score": 4.266987323760986, "metricx_qe_score": 3.447923421859741, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有词语都属于触发集的句子,而良性数据集包含所有词语都不属于触发集的句子。", "metrics": {"bleu_score": 41.38858343339503, "chrf_score": 34.71125629379208, "xcomet_score": 0.5986145734786987, "xcomet_qe_score": 0.6144833564758301, "metricx_score": 3.2261624336242676, "metricx_qe_score": 2.281094789505005, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,服务提供商使用数据集向盗窃者服务请求嵌入。", "metrics": {"bleu_score": 45.51439327346928, "chrf_score": 37.800326739113935, "xcomet_score": 0.7540410757064819, "xcomet_qe_score": 0.7452431321144104, "metricx_score": 2.6780717372894287, "metricx_qe_score": 3.2603135108947754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。", "metrics": {"bleu_score": 41.62204697706691, "chrf_score": 35.61518325231682, "xcomet_score": 0.7954773902893066, "xcomet_qe_score": 0.6895663738250732, "metricx_score": 2.890979290008545, "metricx_qe_score": 2.2676339149475098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算在后门数据集和良性数据集上两者之间的相似度差异,定义为delta cosine和delta L2。", "metrics": {"bleu_score": 50.54866635463418, "chrf_score": 42.597234217603194, "xcomet_score": 0.6554713249206543, "xcomet_qe_score": 0.6462496519088745, "metricx_score": 2.906557083129883, "metricx_qe_score": 2.9901695251464844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS检验,并使用其p值作为第三个指标。", "metrics": {"bleu_score": 69.88909238965658, "chrf_score": 62.7203223917973, "xcomet_score": 0.9285691976547241, "xcomet_qe_score": 0.7990198135375977, "metricx_score": 0.9366715550422668, "metricx_qe_score": 1.5431183576583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对四个数据集(AG News、Mind、SSD two和Erospam)进行了实验。", "metrics": {"bleu_score": 27.355004372901615, "chrf_score": 32.77078486321105, "xcomet_score": 0.7007274627685547, "xcomet_qe_score": 0.5976135730743408, "metricx_score": 6.305051803588867, "metricx_qe_score": 6.898399829864502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供商使用Wiki Text数据集来计算词语频率。在", "metrics": {"bleu_score": 45.685003363733905, "chrf_score": 35.57617478304632, "xcomet_score": 0.8033618927001953, "xcomet_qe_score": 0.840303361415863, "metricx_score": 4.173579216003418, "metricx_qe_score": 1.317585825920105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集上的结果表明,我们的嵌入标记可以实现出色的检测性能,", "metrics": {"bleu_score": 35.60818497912596, "chrf_score": 34.1537700675288, "xcomet_score": 0.8375064134597778, "xcomet_qe_score": 0.808292031288147, "metricx_score": 5.582385063171387, "metricx_qe_score": 5.5219340324401855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时保持下游任务的效用。我们还通过可视化句子展开的嵌入(如使用t-SNE)来验证所提供的嵌入的隐蔽性。", "metrics": {"bleu_score": 35.17331101258739, "chrf_score": 27.14316142773991, "xcomet_score": 0.1643972545862198, "xcomet_qe_score": 0.13430668413639069, "metricx_score": 6.677104473114014, "metricx_qe_score": 9.930878639221191, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的图例表示每个句子中的触发词数量。", "metrics": {"bleu_score": 73.31765459202478, "chrf_score": 68.70323275509035, "xcomet_score": 0.8616412878036499, "xcomet_qe_score": 0.7865859270095825, "metricx_score": 2.3788743019104004, "metricx_qe_score": 1.5983422994613647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是全部内容,", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 83.40608465608467, "xcomet_score": 0.9859269857406616, "xcomet_qe_score": 0.9198013544082642, "metricx_score": 0.13348710536956787, "metricx_qe_score": 0.25882819294929504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们交流。", "metrics": {"bleu_score": 54.10822690539397, "chrf_score": 41.1904761904762, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.16440311074256897, "metricx_qe_score": 0.15320557355880737, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫瓦苏达,是斯托尼布鲁克大学计算机科学博士候选人。我", "metrics": {"bleu_score": 48.36880559284727, "chrf_score": 38.842402499388726, "xcomet_score": 0.7160937786102295, "xcomet_qe_score": 0.7440763711929321, "metricx_score": 3.965244770050049, "metricx_qe_score": 0.6615612506866455, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望介绍我们一篇被ACL 2023以长文形式接收的工作,名为《认知失调检测的迁移学习》,旨在解决罕见类别挑战。", "metrics": {"bleu_score": 24.3918575198484, "chrf_score": 28.656833396017067, "xcomet_score": 0.6954615116119385, "xcomet_qe_score": 0.650127649307251, "metricx_score": 4.639216899871826, "metricx_qe_score": 5.007230281829834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们定义了认知失调,并阐述了为什么在语言中研究这个问题至关重要。", "metrics": {"bleu_score": 14.230042395292953, "chrf_score": 16.737485728639083, "xcomet_score": 0.9626682996749878, "xcomet_qe_score": 0.973041296005249, "metricx_score": 0.784538984298706, "metricx_qe_score": 0.74552983045578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两个相互不一致的信念或行为,例如,一个人说“我知道吸烟可能会杀死我”,然后又说“会议结束后我抽了两根烟”。", "metrics": {"bleu_score": 34.257223568899185, "chrf_score": 28.012414247385824, "xcomet_score": 0.984686017036438, "xcomet_qe_score": 0.9883677959442139, "metricx_score": 1.5197076797485352, "metricx_qe_score": 2.4453063011169434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个信念和行为是不一致的,处于失调状态。", "metrics": {"bleu_score": 68.65551222484392, "chrf_score": 61.84684878025043, "xcomet_score": 0.9818776845932007, "xcomet_qe_score": 0.9809285402297974, "metricx_score": 3.488126754760742, "metricx_qe_score": 5.384222984313965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "进一步说明“我没有它们就无法保住工作”可以合理化第二次出现,", "metrics": {"bleu_score": 24.913413698774807, "chrf_score": 22.275005821480235, "xcomet_score": 0.5847781896591187, "xcomet_qe_score": 0.7772144079208374, "metricx_score": 4.539458751678467, "metricx_qe_score": 3.0322365760803223, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "两者呈现和声关系。", "metrics": {"bleu_score": 15.99024219628172, "chrf_score": 13.891698170044114, "xcomet_score": 0.8834933042526245, "xcomet_qe_score": 0.8646165132522583, "metricx_score": 1.4995707273483276, "metricx_qe_score": 0.9604151248931885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管认知失调是一种我们在日常决策中经常经历的常见现象,但在其他话语关系中,在语言表达中发现它们却非常罕见。", "metrics": {"bleu_score": 33.44706086485454, "chrf_score": 28.8531302591483, "xcomet_score": 0.9555222988128662, "xcomet_qe_score": 0.934906005859375, "metricx_score": 2.0687546730041504, "metricx_qe_score": 2.4143755435943604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,这有什么意义呢?", "metrics": {"bleu_score": 6.786053138365654, "chrf_score": 6.319275479859423, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.88630610704422, "metricx_qe_score": 0.6635661125183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调有助于我们理解人们之间的分歧影响,追踪人群中的信念和态度变化趋", "metrics": {"bleu_score": 24.755297202724897, "chrf_score": 23.55566044180865, "xcomet_score": 0.8138625621795654, "xcomet_qe_score": 0.8016743659973145, "metricx_score": 5.669858932495117, "metricx_qe_score": 1.9956927299499512, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "势。高认知失调也与焦虑症有关,可以帮助我们更好地理解人们的心理健康。", "metrics": {"bleu_score": 75.35064536537182, "chrf_score": 68.05947177890268, "xcomet_score": 0.4765227735042572, "xcomet_qe_score": 0.5012060403823853, "metricx_score": 5.655209541320801, "metricx_qe_score": 6.775961875915527, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语言中研究认知失调也有助于理解极端主义和弱势群体两极分化。", "metrics": {"bleu_score": 58.00051298949201, "chrf_score": 46.70046713354343, "xcomet_score": 0.8950188755989075, "xcomet_qe_score": 0.8891909718513489, "metricx_score": 2.2687172889709473, "metricx_qe_score": 2.064984083175659, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,理解认知失调有助于理解个人的认知风格,并帮助我们更好地理解决策过程。", "metrics": {"bleu_score": 62.10003693201111, "chrf_score": 56.79466616230344, "xcomet_score": 0.9936516284942627, "xcomet_qe_score": 0.9781152009963989, "metricx_score": 0.47071075439453125, "metricx_qe_score": 0.6744973063468933, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了创建认知失调资源,我们进行了大规模的认知失调关系标注。", "metrics": {"bleu_score": 62.23264265874191, "chrf_score": 55.123198126476616, "xcomet_score": 0.9616913795471191, "xcomet_qe_score": 0.9324599504470825, "metricx_score": 2.070673704147339, "metricx_qe_score": 2.2422523498535156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用“先检测失调”的方法,如图所示流程图所示。", "metrics": {"bleu_score": 34.53786557868504, "chrf_score": 28.743208482338922, "xcomet_score": 0.6429727673530579, "xcomet_qe_score": 0.5946604609489441, "metricx_score": 0.7546122074127197, "metricx_qe_score": 0.9012407064437866, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过PDTB解析器传递推文,并根据我们在论文中描述的指导原则对话语单元对进行标注。", "metrics": {"bleu_score": 47.789119225342795, "chrf_score": 45.14276358158624, "xcomet_score": 0.7943629026412964, "xcomet_qe_score": 0.7529221773147583, "metricx_score": 2.705082893371582, "metricx_qe_score": 4.088517189025879, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如您所见,在标注对中,只有3.5%发现了失调。", "metrics": {"bleu_score": 13.966650592068845, "chrf_score": 20.10062156518485, "xcomet_score": 0.7398912310600281, "xcomet_qe_score": 0.7204301953315735, "metricx_score": 2.718972682952881, "metricx_qe_score": 3.497994899749756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "收集了大约1000个对话语单元对的样本后,我们对一个初始分类器进行了训练,该分类器仅在43个失调样本上进行训练。", "metrics": {"bleu_score": 53.20922884322494, "chrf_score": 47.44274940975611, "xcomet_score": 0.694644570350647, "xcomet_qe_score": 0.6636863946914673, "metricx_score": 2.3340542316436768, "metricx_qe_score": 2.5254385471343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现与随机猜测相比并", "metrics": {"bleu_score": 34.56126100483635, "chrf_score": 28.371030830926664, "xcomet_score": 0.6753049492835999, "xcomet_qe_score": 0.4972247779369354, "metricx_score": 5.606271743774414, "metricx_qe_score": 4.598136901855469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "没有好多少。考虑到失调的低发生率以及缺乏任何先前的此类数据集,我们面临的是绝对稀有问题。", "metrics": {"bleu_score": 37.50557217705472, "chrf_score": 34.85343204943311, "xcomet_score": 0.5445663928985596, "xcomet_qe_score": 0.2649836838245392, "metricx_score": 3.541398525238037, "metricx_qe_score": 4.32761287689209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这种情况,我们尝试了迁移学习和主动学习的组合,以便在更少的标注轮次中收集更多的失调样本,从而降低整体标注成本并提高失调检测能力。", "metrics": {"bleu_score": 46.561950730560824, "chrf_score": 40.33190244310721, "xcomet_score": 0.9594292640686035, "xcomet_qe_score": 0.9536044597625732, "metricx_score": 2.2893474102020264, "metricx_qe_score": 2.1992599964141846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型无法捕捉失调类别,因此我们从迁移权重开始主动学习过程,从密切相关的任务中迁移权重。", "metrics": {"bleu_score": 64.66347317882538, "chrf_score": 54.671322607073314, "xcomet_score": 0.7390375137329102, "xcomet_qe_score": 0.7124940156936646, "metricx_score": 2.459033966064453, "metricx_qe_score": 3.7146849632263184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中迁移。一是主题无关的认知失调立场分类任务,该任务确定来自不同人员的两个辩论陈述是否一致或不一致,无论主题如何,这里称之为“辩论”;二是根据PDTB对扩展和比较类别进行二元分类,因为这两个类别与和声和失调的概念密切相关,我们称之为CE。", "metrics": {"bleu_score": 47.184992068966864, "chrf_score": 42.68107076424536, "xcomet_score": 0.5187530517578125, "xcomet_qe_score": 0.4744510352611542, "metricx_score": 4.318501949310303, "metricx_qe_score": 5.195090293884277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在迁移后的零样本性能在标注数据集上已经明显优于随机猜测,最佳AUC为0.62。", "metrics": {"bleu_score": 34.11234949590643, "chrf_score": 35.24292284573057, "xcomet_score": 0.729512631893158, "xcomet_qe_score": 0.6445906162261963, "metricx_score": 3.5249898433685303, "metricx_qe_score": 5.2427520751953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,通过迭代地在两个任务上进行微调,我们发现,先对CE任务进行微调,然后对“辩论”任务进行进一步微调,可以获得更好的零样本性能。", "metrics": {"bleu_score": 30.254613765533538, "chrf_score": 31.863134532894094, "xcomet_score": 0.7636361718177795, "xcomet_qe_score": 0.7285187840461731, "metricx_score": 3.5329043865203857, "metricx_qe_score": 4.645094394683838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这就是我们用于启动主动学习的模型。", "metrics": {"bleu_score": 68.05854779603962, "chrf_score": 59.13663857355807, "xcomet_score": 0.9063611626625061, "xcomet_qe_score": 0.891457200050354, "metricx_score": 1.1736443042755127, "metricx_qe_score": 1.3090879917144775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,我们确定了在主动学习的每一轮中,使用新数据更新模型的最佳方法。", "metrics": {"bleu_score": 51.310580521855854, "chrf_score": 43.34165574850822, "xcomet_score": 0.8688868284225464, "xcomet_qe_score": 0.767198383808136, "metricx_score": 1.8482725620269775, "metricx_qe_score": 3.3745758533477783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积方法会累积迄今为止主动标注收集的所有数据,而迭代方法则通过在最新收集的数据集上训练模型来更新模型。", "metrics": {"bleu_score": 42.54826778580875, "chrf_score": 35.83909045495965, "xcomet_score": 0.8654524087905884, "xcomet_qe_score": 0.8626452088356018, "metricx_score": 1.4625120162963867, "metricx_qe_score": 1.5051629543304443, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积方法在各个方面都优于或等于迭代方法。", "metrics": {"bleu_score": 43.5438846100796, "chrf_score": 36.90917635287204, "xcomet_score": 0.9841957092285156, "xcomet_qe_score": 0.9409867525100708, "metricx_score": 0.9658196568489075, "metricx_qe_score": 2.1438472270965576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了增加失调样本的数量,我们使用罕见类别概率策略(PRC)来选择当前模型在每一轮主动学习中很可能具有失调的样本。", "metrics": {"bleu_score": 37.49895693365348, "chrf_score": 34.86982256863583, "xcomet_score": 0.7535675764083862, "xcomet_qe_score": 0.7129218578338623, "metricx_score": 3.3864567279815674, "metricx_qe_score": 3.8783199787139893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此策略与其他常用的最先进主动学习策略进行比较。", "metrics": {"bleu_score": 54.12139302088048, "chrf_score": 44.35652560174168, "xcomet_score": 0.9056259393692017, "xcomet_qe_score": 0.8922274708747864, "metricx_score": 1.9309437274932861, "metricx_qe_score": 1.374039649963379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,所提出的PRC策略优于其他最先进的策略,尽管差异很小。", "metrics": {"bleu_score": 37.22992336684908, "chrf_score": 34.01803738427765, "xcomet_score": 0.9623657464981079, "xcomet_qe_score": 0.9667251110076904, "metricx_score": 1.500935673713684, "metricx_qe_score": 2.4870173931121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,对于随机策略,性能明显较低。在经过多轮主动学习", "metrics": {"bleu_score": 13.380161378318961, "chrf_score": 17.272328530663007, "xcomet_score": 0.7976901531219482, "xcomet_qe_score": 0.7074119448661804, "metricx_score": 4.349786281585693, "metricx_qe_score": 4.241112232208252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "且采用两个最佳策略后,我们将认知失调分类AUC提高了到0.75,这是我们在该任务上迄今为止取得的最佳性能。", "metrics": {"bleu_score": 40.75594160517391, "chrf_score": 36.578537872340604, "xcomet_score": 0.6624487638473511, "xcomet_qe_score": 0.689007580280304, "metricx_score": 5.850427627563477, "metricx_qe_score": 6.005611896514893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略在标注质量和标注员成本方面的可行性。", "metrics": {"bleu_score": 70.15004473437001, "chrf_score": 65.05480777395664, "xcomet_score": 0.9636960029602051, "xcomet_qe_score": 0.9794700145721436, "metricx_score": 0.9391753673553467, "metricx_qe_score": 0.9195688366889954, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,PRC具有最高的失调百分比,并且最适合罕见类别。", "metrics": {"bleu_score": 33.46080538499995, "chrf_score": 29.294641395216107, "xcomet_score": 0.8847973346710205, "xcomet_qe_score": 0.7957311868667603, "metricx_score": 1.172735571861267, "metricx_qe_score": 1.9273219108581543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也认为这些例子很难。", "metrics": {"bleu_score": 43.33207865423753, "chrf_score": 37.51366688866689, "xcomet_score": 0.8844648599624634, "xcomet_qe_score": 0.8918684124946594, "metricx_score": 1.562082290649414, "metricx_qe_score": 1.6717826128005981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总而言之,我们发现,PRC是一种简单的罕见类别采集主动学习策略,并且具有适当设计的迁移学习任务可以显著帮助启动主动学习。", "metrics": {"bleu_score": 36.99834299964951, "chrf_score": 32.94975633930369, "xcomet_score": 0.7269704341888428, "xcomet_qe_score": 0.6449555158615112, "metricx_score": 5.460357189178467, "metricx_qe_score": 6.627648830413818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而针对域内主动标注则受益于累积更新。", "metrics": {"bleu_score": 53.38829468131539, "chrf_score": 44.248022565301156, "xcomet_score": 0.780805230140686, "xcomet_qe_score": 0.6689537763595581, "metricx_score": 1.7654417753219604, "metricx_qe_score": 2.2654812335968018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们的代码、数据集和论文的链接。", "metrics": {"bleu_score": 67.49454888262711, "chrf_score": 65.32965810491686, "xcomet_score": 0.9262974262237549, "xcomet_qe_score": 0.9665294885635376, "metricx_score": 0.7372788190841675, "metricx_qe_score": 1.1362874507904053, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢。", "metrics": {"bleu_score": 0.0, "chrf_score": 100.0, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.0, "metricx_qe_score": 0.018636874854564667, "linguapy_score": [0, "CHINESE"]}}
