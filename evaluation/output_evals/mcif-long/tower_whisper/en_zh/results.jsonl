{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",欢迎来到我们的演示,我们将展示 d.plain,这是一个用于德语文本简化的新语料库,涵盖文档级别和句子级别。", "metrics": {"bleu_score": 18.515489969737366, "chrf_score": 18.86070672523932, "xcomet_score": 0.7221136093139648, "xcomet_qe_score": 0.4470965266227722, "metricx_score": 4.17996883392334, "metricx_qe_score": 4.382100582122803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫 Regina Stodden,我将引导大家完成演示的第一部分。", "metrics": {"bleu_score": 50.328822233837485, "chrf_score": 65.46920411529457, "xcomet_score": 0.9263213872909546, "xcomet_qe_score": 0.9248142838478088, "metricx_score": 2.3009376525878906, "metricx_qe_score": 3.5864834785461426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义文本简化。", "metrics": {"bleu_score": 26.008181229347326, "chrf_score": 25.730100827778298, "xcomet_score": 0.9712696075439453, "xcomet_qe_score": 0.9715811014175415, "metricx_score": 0.25107619166374207, "metricx_qe_score": 0.275246798992157, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "文本简化是将文本进行改编,以提高特定目标群体(如阅读障碍者或非母语者)对文本的理解。", "metrics": {"bleu_score": 40.83269777812676, "chrf_score": 34.01907064388649, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.7137349247932434, "metricx_qe_score": 0.6544722318649292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了训练文本简化模型,我们需要文本的平行对,例如文档或句子的平行对。", "metrics": {"bleu_score": 47.88095834030782, "chrf_score": 47.30903869310139, "xcomet_score": 0.9855912923812866, "xcomet_qe_score": 0.969380259513855, "metricx_score": 1.5974515676498413, "metricx_qe_score": 1.84871244430542, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在下面的例子中,您可以看到一个复杂的德语句子及其通俗语言翻译的平行对齐句子对。", "metrics": {"bleu_score": 63.29336507270698, "chrf_score": 58.930563605486206, "xcomet_score": 0.9757446050643921, "xcomet_qe_score": 0.9543026685714722, "metricx_score": 1.4041693210601807, "metricx_qe_score": 1.71913743019104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了简化句子,可以采用不同的技术,例如词汇替换、短语重组、短语重新排序或插入单词。", "metrics": {"bleu_score": 51.281247404446766, "chrf_score": 42.949855034258114, "xcomet_score": 0.9216040968894958, "xcomet_qe_score": 0.924910306930542, "metricx_score": 2.883509635925293, "metricx_qe_score": 3.595163583755493, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们现在提出我们新的语料库 dplane。因为近年来,现有的语料库存在一些问题。", "metrics": {"bleu_score": 55.923576682620265, "chrf_score": 41.558452926531956, "xcomet_score": 0.6744593381881714, "xcomet_qe_score": 0.6744869351387024, "metricx_score": 6.033359527587891, "metricx_qe_score": 6.5143723487854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这些语料库太小,无法训练分类模型。", "metrics": {"bleu_score": 35.743397031672636, "chrf_score": 32.20663918239359, "xcomet_score": 0.8950200080871582, "xcomet_qe_score": 0.8340485095977783, "metricx_score": 3.0722250938415527, "metricx_qe_score": 2.363635540008545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来提出的其他三个模型都是自动对齐的,这意味着它们的对齐可能存在错误。", "metrics": {"bleu_score": 57.46388731356058, "chrf_score": 50.42078987723587, "xcomet_score": 0.9855239391326904, "xcomet_qe_score": 0.9838825464248657, "metricx_score": 0.6832425594329834, "metricx_qe_score": 0.8456329107284546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们提出了新的语料库 dplane,它分为两个子语料库,dplane-apa 和 dplane-web。", "metrics": {"bleu_score": 52.174427464587126, "chrf_score": 32.27774853037927, "xcomet_score": 0.8994079232215881, "xcomet_qe_score": 0.8717968463897705, "metricx_score": 4.816453456878662, "metricx_qe_score": 4.453688144683838, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "dplane-apa 基于使用文本。", "metrics": {"bleu_score": 25.848657697858535, "chrf_score": 27.741767220507825, "xcomet_score": 0.7648651599884033, "xcomet_qe_score": 0.6558792591094971, "metricx_score": 6.463590145111084, "metricx_qe_score": 10.288641929626465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在简单的 APA 中,我们手动对齐了 483 个文档。", "metrics": {"bleu_score": 62.128669764890724, "chrf_score": 49.14978711968663, "xcomet_score": 0.7992784380912781, "xcomet_qe_score": 0.7976800799369812, "metricx_score": 2.76139760017395, "metricx_qe_score": 2.586576461791992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这大约产生了 30,000-13,000 对平行句子对。", "metrics": {"bleu_score": 42.61082723917018, "chrf_score": 47.21750615311081, "xcomet_score": 0.7471905946731567, "xcomet_qe_score": 0.6240381002426147, "metricx_score": 4.868641376495361, "metricx_qe_score": 5.732573509216309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于 DeepLaneWeb,这个语料库包括不同的领域,我们还手动对齐了这 750 个文档,另一方面也使用了自动对齐方法。", "metrics": {"bleu_score": 48.916135469700215, "chrf_score": 38.81273374654387, "xcomet_score": 0.8849998712539673, "xcomet_qe_score": 0.7602332830429077, "metricx_score": 3.298598289489746, "metricx_qe_score": 3.7446861267089844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们产生了 30,450 对句子对。", "metrics": {"bleu_score": 9.103526405546068, "chrf_score": 36.11862438524246, "xcomet_score": 0.7572063207626343, "xcomet_qe_score": 0.8103538155555725, "metricx_score": 2.410902500152588, "metricx_qe_score": 2.4874396324157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对句子对进行了更详细的分析,例如简化类型。", "metrics": {"bleu_score": 28.718230275343156, "chrf_score": 25.446374553553802, "xcomet_score": 0.8229305148124695, "xcomet_qe_score": 0.8113840818405151, "metricx_score": 4.102107048034668, "metricx_qe_score": 4.5760345458984375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以看到,圣经文本在所有级别上都比新闻文本或语言学习者文本的简化要", "metrics": {"bleu_score": 45.02931824403319, "chrf_score": 45.93812188744237, "xcomet_score": 0.543397068977356, "xcomet_qe_score": 0.4500301480293274, "metricx_score": 6.560612201690674, "metricx_qe_score": 4.273547649383545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "强,例如词汇简化、结构简化或整体简化水平。", "metrics": {"bleu_score": 57.24959986049559, "chrf_score": 55.015555848084, "xcomet_score": 0.7287487387657166, "xcomet_qe_score": 0.6605299115180969, "metricx_score": 5.578245639801025, "metricx_qe_score": 4.8856706619262695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,您可以看到,我们的 De Plplane 语料库具有不同简化转换的高优先级。", "metrics": {"bleu_score": 33.79990107988847, "chrf_score": 27.950188392097946, "xcomet_score": 0.678867518901825, "xcomet_qe_score": 0.5912760496139526, "metricx_score": 7.1373395919799805, "metricx_qe_score": 6.931763648986816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在 d.plane API 语料库中,我们有更多的重组和单词添加,而在 d.plane web 语料库", "metrics": {"bleu_score": 21.569471602226038, "chrf_score": 19.96209208489483, "xcomet_score": 0.3945353031158447, "xcomet_qe_score": 0.41189128160476685, "metricx_score": 10.10771656036377, "metricx_qe_score": 7.12162971496582, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中,我们有更多的改写。", "metrics": {"bleu_score": 16.404210784979526, "chrf_score": 21.01687967300105, "xcomet_score": 0.28108879923820496, "xcomet_qe_score": 0.2579508423805237, "metricx_score": 7.771971702575684, "metricx_qe_score": 10.883877754211426, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们现在来看看我们可以用这个语料库做什么。", "metrics": {"bleu_score": 45.810919659463124, "chrf_score": 45.3378672790754, "xcomet_score": 0.9930709600448608, "xcomet_qe_score": 0.9726355075836182, "metricx_score": 0.3310670554637909, "metricx_qe_score": 0.5004581212997437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Omar,我现在将谈谈我们数据集 D-plane 的使用案例。", "metrics": {"bleu_score": 16.351542736249865, "chrf_score": 25.169157096474837, "xcomet_score": 0.8782280683517456, "xcomet_qe_score": 0.8247675895690918, "metricx_score": 2.4108033180236816, "metricx_qe_score": 3.373997211456299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个使用案例,我们可以评估自动对齐方法。", "metrics": {"bleu_score": 67.73709971213142, "chrf_score": 62.89481874621193, "xcomet_score": 0.9807659387588501, "xcomet_qe_score": 0.972801685333252, "metricx_score": 0.8053840398788452, "metricx_qe_score": 0.9386290907859802, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,出现了很多对齐方法,但在机器翻译的背景下,我们有两个用不同语言编写的平行文档,我们想要从两个平行文档中提取对齐的句子", "metrics": {"bleu_score": 62.84875175374649, "chrf_score": 58.94802092314036, "xcomet_score": 0.9588608741760254, "xcomet_qe_score": 0.8962249755859375, "metricx_score": 1.286197304725647, "metricx_qe_score": 1.5115423202514648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",这些句子具有相同的语言、相同的内容,但它们在复杂性水平上有所不同。", "metrics": {"bleu_score": 14.635804297410187, "chrf_score": 16.480905111384153, "xcomet_score": 0.7056636810302734, "xcomet_qe_score": 0.6767394542694092, "metricx_score": 7.876086711883545, "metricx_qe_score": 10.696799278259277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们有了手动对齐句子的 dplane 数据集,我们可以使用这些句子作为金标准对齐,来评估一些提出的对齐方法。", "metrics": {"bleu_score": 46.5431283988973, "chrf_score": 36.513886151161884, "xcomet_score": 0.8047201633453369, "xcomet_qe_score": 0.8134036064147949, "metricx_score": 4.141955375671387, "metricx_qe_score": 4.241330623626709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对提出的方法进行了一些改编,并在论文中发表了所有这些改编和运行实验的代码。", "metrics": {"bleu_score": 33.533048581960514, "chrf_score": 30.22339221688426, "xcomet_score": 0.9760477542877197, "xcomet_qe_score": 0.9815609455108643, "metricx_score": 1.6106516122817993, "metricx_qe_score": 1.3464195728302002, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们得出结论,用于德语文本简化的最佳自动对齐方法是 math align 方法。", "metrics": {"bleu_score": 69.07355502974168, "chrf_score": 61.09083158289058, "xcomet_score": 0.8752065896987915, "xcomet_qe_score": 0.8596537113189697, "metricx_score": 5.549351692199707, "metricx_qe_score": 6.819765090942383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以在论文中找到在您自己的文档上运行此方法的代码。", "metrics": {"bleu_score": 43.75579994350345, "chrf_score": 38.60666604288338, "xcomet_score": 0.992317795753479, "xcomet_qe_score": 0.9805800914764404, "metricx_score": 0.4939131736755371, "metricx_qe_score": 0.48299628496170044, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示的第二个使用案例是通过微调语言模型来自动简化文本的案例。", "metrics": {"bleu_score": 50.59549385281685, "chrf_score": 48.30636403675405, "xcomet_score": 0.9823939800262451, "xcomet_qe_score": 0.9820965528488159, "metricx_score": 1.0921305418014526, "metricx_qe_score": 1.3271886110305786, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们微调了两个不同的模型。我们微调了", "metrics": {"bleu_score": 27.694132751313415, "chrf_score": 25.99298382912637, "xcomet_score": 0.3408113420009613, "xcomet_qe_score": 0.43639102578163147, "metricx_score": 4.435979843139648, "metricx_qe_score": 5.219578742980957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "long-impart 模型,以生成句子级别的简化。", "metrics": {"bleu_score": 3.302443753693619, "chrf_score": 11.852745073906892, "xcomet_score": 0.2174767106771469, "xcomet_qe_score": 0.13823391497135162, "metricx_score": 19.924219131469727, "metricx_qe_score": 19.67947769165039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您也可以找到所有检查点,并在论文中查看我们实验的详细分数和评估指标。我们得出", "metrics": {"bleu_score": 54.65317609455904, "chrf_score": 48.55527973066808, "xcomet_score": 0.7491294145584106, "xcomet_qe_score": 0.7328386902809143, "metricx_score": 6.1299333572387695, "metricx_qe_score": 3.2584521770477295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "结论,这种基本的微调可以产生或获得比基准分数更好的分数,我们提出了这些结果作为未来的自动文本简化问题的基准。", "metrics": {"bleu_score": 62.604530896901345, "chrf_score": 55.18274841937586, "xcomet_score": 0.8269038200378418, "xcomet_qe_score": 0.7225863337516785, "metricx_score": 2.2934842109680176, "metricx_qe_score": 2.705810785293579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的关注,我们希望在会议期间见到大家。", "metrics": {"bleu_score": 58.79423696731584, "chrf_score": 51.823106974049026, "xcomet_score": 0.9932962656021118, "xcomet_qe_score": 0.9960243701934814, "metricx_score": 0.47018420696258545, "metricx_qe_score": 0.2992590069770813, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9333682656288147, "xcomet_qe_score": 0.9679166674613953, "metricx_score": 0.46964308619499207, "metricx_qe_score": 0.171317458152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫亚当·施皮尔科夫斯基,今天我要讲的主题是并列句的依存结构。", "metrics": {"bleu_score": 8.8760743367864, "chrf_score": 8.390672400749862, "xcomet_score": 0.6905022859573364, "xcomet_qe_score": 0.5952713489532471, "metricx_score": 1.9819329977035522, "metricx_qe_score": 1.3186008930206299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "众所周知,不同的理论和语料库方法对依存结构有不同的假设。", "metrics": {"bleu_score": 43.25244295803472, "chrf_score": 39.65601375930387, "xcomet_score": 0.9862223863601685, "xcomet_qe_score": 0.8930584788322449, "metricx_score": 0.5322721004486084, "metricx_qe_score": 0.7308767437934875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在普遍依存理论中,丽莎、巴特和玛姬的并列结构是,第一个并列成分是整个并列结构的词头,", "metrics": {"bleu_score": 24.713978154270897, "chrf_score": 17.70847513802688, "xcomet_score": 0.7467969655990601, "xcomet_qe_score": 0.6923604011535645, "metricx_score": 2.20485520362854, "metricx_qe_score": 3.6876847743988037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中是丽莎。伊戈", "metrics": {"bleu_score": 44.833867003844595, "chrf_score": 30.224834422656325, "xcomet_score": 0.8139227628707886, "xcomet_qe_score": 0.8195971250534058, "metricx_score": 4.992238521575928, "metricx_qe_score": 2.9995956420898438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尔·米尔丘克的意义文本理论也采用了类似的方法,即整个并列结构由第一个并列成分领导。因此,", "metrics": {"bleu_score": 41.68044724112959, "chrf_score": 30.73586621422898, "xcomet_score": 0.5137375593185425, "xcomet_qe_score": 0.5170902609825134, "metricx_score": 6.847860336303711, "metricx_qe_score": 5.723374366760254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两种方法都是不对称的,", "metrics": {"bleu_score": 90.36020036098445, "chrf_score": 89.11315536315534, "xcomet_score": 0.9953523874282837, "xcomet_qe_score": 0.9697904586791992, "metricx_score": 0.36860373616218567, "metricx_qe_score": 0.4781116247177124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.8825945258140564, "xcomet_qe_score": 0.8340854048728943, "metricx_score": 2.3397016525268555, "metricx_qe_score": 0.3276556134223938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们都将某个并列成分单独挑出来。", "metrics": {"bleu_score": 25.33654946448646, "chrf_score": 26.07528975356622, "xcomet_score": 0.81480872631073, "xcomet_qe_score": 0.841989278793335, "metricx_score": 2.230079412460327, "metricx_qe_score": 2.8976376056671143, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,也有对并列结构的对称方法,例如布拉格方法,", "metrics": {"bleu_score": 35.21082926433176, "chrf_score": 30.12986095637846, "xcomet_score": 0.730618953704834, "xcomet_qe_score": 0.7697485685348511, "metricx_score": 4.540720462799072, "metricx_qe_score": 4.566980838775635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "布拉格依存树库假设的以连词为词头的对称方法,其中并列结构由连词领导。", "metrics": {"bleu_score": 19.255441097181013, "chrf_score": 19.132334611672643, "xcomet_score": 0.7637373805046082, "xcomet_qe_score": 0.7930818796157837, "metricx_score": 4.594947814941406, "metricx_qe_score": 5.159675598144531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们从n得到对所有并列成分的依存关系。", "metrics": {"bleu_score": 33.650683488592314, "chrf_score": 26.82966954140054, "xcomet_score": 0.7472473978996277, "xcomet_qe_score": 0.7684050798416138, "metricx_score": 5.17020845413208, "metricx_qe_score": 5.143502235412598, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,还有多头方法,例如迪克·哈德森的词法语法中使用的方法,可以说,所有并列成分都是并列结构的词头。多头", "metrics": {"bleu_score": 12.463832776322196, "chrf_score": 14.1247945394861, "xcomet_score": 0.3912616968154907, "xcomet_qe_score": 0.35333770513534546, "metricx_score": 6.763947486877441, "metricx_qe_score": 5.666916847229004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "方法,例如在卡特逊的词法语法中", "metrics": {"bleu_score": 2.458476536482737, "chrf_score": 1.0548523206751055, "xcomet_score": 0.1494845449924469, "xcomet_qe_score": 0.14044825732707977, "metricx_score": 9.283581733703613, "metricx_qe_score": 17.949779510498047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用的方法,可以说,所有并列成分都是并列结构的词头,因此我们从这里的支配词", "metrics": {"bleu_score": 5.805041511766121, "chrf_score": 6.524915496433042, "xcomet_score": 0.22318023443222046, "xcomet_qe_score": 0.12799184024333954, "metricx_score": 9.700922012329102, "metricx_qe_score": 13.709596633911133, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "loves得到对所有并列成分的单独依存关系,这些是巴顿现在正在制作的目标。本文的目的是提出两种反对并列结构不对称性的观点,如上述两", "metrics": {"bleu_score": 8.596541276228889, "chrf_score": 10.530237997998626, "xcomet_score": 0.2197316288948059, "xcomet_qe_score": 0.19307063519954681, "metricx_score": 12.93783950805664, "metricx_qe_score": 13.124563217163086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.9201251268386841, "xcomet_qe_score": 0.8827870488166809, "metricx_score": 2.9676010608673096, "metricx_qe_score": 0.27519962191581726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,这个论点是基于依存长度最小化的原则,我将通过这些例子来解释。", "metrics": {"bleu_score": 51.51420826880311, "chrf_score": 44.19711797271342, "xcomet_score": 0.8924505114555359, "xcomet_qe_score": 0.8900807499885559, "metricx_score": 1.0998942852020264, "metricx_qe_score": 0.9492322206497192, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在英语中,如你所知,直接宾语倾向于靠近动词,而附属成分可能更远,对吗?所以,March昨天读了它没问题,因为直接宾语it靠近动词,而March的直接宾语倾向于靠近动词,而附属成分可能更远,", "metrics": {"bleu_score": 14.506805420631238, "chrf_score": 22.32384597987423, "xcomet_score": 0.43167832493782043, "xcomet_qe_score": 0.44410237669944763, "metricx_score": 15.284107208251953, "metricx_qe_score": 16.352888107299805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对吗?所以,March昨天读了它没问题,因为直接宾语it靠近动词,而March昨天读了它则糟糕得", "metrics": {"bleu_score": 18.857018977252036, "chrf_score": 12.569025182519875, "xcomet_score": 0.3468134105205536, "xcomet_qe_score": 0.3972172737121582, "metricx_score": 10.080440521240234, "metricx_qe_score": 9.425863265991211, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多,对吗?", "metrics": {"bleu_score": 10.682175159905848, "chrf_score": 9.803921568627452, "xcomet_score": 0.751610517501831, "xcomet_qe_score": 0.6130142211914062, "metricx_score": 2.366835117340088, "metricx_qe_score": 3.074291467666626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为这里动词和直接宾语之间有一个附属成分yesterday。", "metrics": {"bleu_score": 53.522807162964334, "chrf_score": 58.19402673027625, "xcomet_score": 0.8794345855712891, "xcomet_qe_score": 0.7863315343856812, "metricx_score": 2.397606372833252, "metricx_qe_score": 3.437319278717041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当直接宾语非常冗长时,这种影响可能会得到缓解,因为", "metrics": {"bleu_score": 54.53729797643623, "chrf_score": 51.35658169560313, "xcomet_score": 0.7553302049636841, "xcomet_qe_score": 0.6028759479522705, "metricx_score": 4.203617095947266, "metricx_qe_score": 2.8599348068237305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这时它可以移到附属成分之后的位臵。", "metrics": {"bleu_score": 16.215682948162673, "chrf_score": 15.146419992002816, "xcomet_score": 0.792238712310791, "xcomet_qe_score": 0.7687369585037231, "metricx_score": 4.153583526611328, "metricx_qe_score": 3.9534404277801514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有例子说明。所以,", "metrics": {"bleu_score": 18.36028134946796, "chrf_score": 20.735576110731323, "xcomet_score": 0.8500728607177734, "xcomet_qe_score": 0.8435606360435486, "metricx_score": 2.810086727142334, "metricx_qe_score": 1.9975857734680176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这两个句子都很好。", "metrics": {"bleu_score": 48.88290318657944, "chrf_score": 42.06458785456199, "xcomet_score": 0.9888736009597778, "xcomet_qe_score": 0.9774754047393799, "metricx_score": 0.4006389081478119, "metricx_qe_score": 0.6555725336074829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "March今天读了这本书,这本书关于BC", "metrics": {"bleu_score": 0.0, "chrf_score": 2.630485139779655, "xcomet_score": 0.14950893819332123, "xcomet_qe_score": 0.15609292685985565, "metricx_score": 7.744795799255371, "metricx_qe_score": 6.338598728179932, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "S,非常有趣。没问题。从某种意义上说,用it代替,我们有这个长NP。但是,说March昨天读了这本", "metrics": {"bleu_score": 5.335343825807726, "chrf_score": 10.478959472828572, "xcomet_score": 0.2377249002456665, "xcomet_qe_score": 0.15226761996746063, "metricx_score": 13.999347686767578, "metricx_qe_score": 13.436885833740234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "书,关于蜜蜂,非常有趣,也是可以的。所以,这里的推理是,这是可能的,", "metrics": {"bleu_score": 3.407192589506109, "chrf_score": 2.1048177026134143, "xcomet_score": 0.1463450938463211, "xcomet_qe_score": 0.14251013100147247, "metricx_score": 7.541988372802734, "metricx_qe_score": 8.989160537719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因为即使这个句子违反了直接宾语应该紧靠动词的一般语法原则,它满足了依存长度最小化的原则,即倾向于较短的依存关系。", "metrics": {"bleu_score": 53.31609937595756, "chrf_score": 46.748667064651784, "xcomet_score": 0.8567320108413696, "xcomet_qe_score": 0.9224404096603394, "metricx_score": 3.1689305305480957, "metricx_qe_score": 3.9060683250427246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,这两个树只显示了关键依存关系的长度,即这两个结构中不常出现的依存关系。", "metrics": {"bleu_score": 63.24836442920863, "chrf_score": 61.628592036958864, "xcomet_score": 0.8787567615509033, "xcomet_qe_score": 0.7838622331619263, "metricx_score": 1.5226020812988281, "metricx_qe_score": 2.183162212371826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,这里我们从red到附属成分的依存关系长度为7个词,从red到book的依存关系长度为4个词。所以,总共是11。", "metrics": {"bleu_score": 25.61060085080561, "chrf_score": 23.499786511985366, "xcomet_score": 0.637319803237915, "xcomet_qe_score": 0.6697777509689331, "metricx_score": 7.858896255493164, "metricx_qe_score": 7.720132827758789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当你移动,当你交换这两个成分时,这两个依存关系的总和变为6,对", "metrics": {"bleu_score": 53.707566707346544, "chrf_score": 52.09585216563089, "xcomet_score": 0.5727032423019409, "xcomet_qe_score": 0.4979701340198517, "metricx_score": 7.291044235229492, "metricx_qe_score": 6.049654960632324, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "吗?所以,从11变为6,短得多。", "metrics": {"bleu_score": 18.20705281109213, "chrf_score": 19.553740001608308, "xcomet_score": 0.6166343092918396, "xcomet_qe_score": 0.5940113067626953, "metricx_score": 4.5555644035339355, "metricx_qe_score": 5.406432151794434, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么这个听起来相当好的原因。", "metrics": {"bleu_score": 41.180376356915765, "chrf_score": 43.55287312219555, "xcomet_score": 0.8470020294189453, "xcomet_qe_score": 0.8522823452949524, "metricx_score": 0.9107572436332703, "metricx_qe_score": 0.975638747215271, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多,对吗?", "metrics": {"bleu_score": 10.682175159905848, "chrf_score": 9.803921568627452, "xcomet_score": 0.751610517501831, "xcomet_qe_score": 0.6130142211914062, "metricx_score": 2.366835117340088, "metricx_qe_score": 3.074291467666626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它违反了一个原则,但满足了另一个原则。", "metrics": {"bleu_score": 72.24553130054804, "chrf_score": 65.89958241316472, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.19535920023918152, "metricx_qe_score": 0.47849607467651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.9153238534927368, "xcomet_qe_score": 0.8316769599914551, "metricx_score": 2.982078790664673, "metricx_qe_score": 0.2875407934188843, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们做了什么,我们从宾夕法尼亚树库的增强版中提取了关于并列的各种统计数据,并查看了为什么我们没有使用普遍依存理论。这些统计数据证实了之前多次观察到的现象,即左并列成分往往较短。", "metrics": {"bleu_score": 42.40645998983859, "chrf_score": 39.45833653715002, "xcomet_score": 0.5628491640090942, "xcomet_qe_score": 0.5525742769241333, "metricx_score": 6.005113124847412, "metricx_qe_score": 5.768761157989502, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们没有使用普遍依存理论,这些统计数据证实了之前多次观察到的现象,即左并列成分往往较短,所以盐和胡椒,而不是胡椒和盐,以音", "metrics": {"bleu_score": 2.7817944940467845, "chrf_score": 3.202793105882144, "xcomet_score": 0.2263915091753006, "xcomet_qe_score": 0.23490865528583527, "metricx_score": 9.0046968460083, "metricx_qe_score": 6.736096382141113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "节为单位,还有在经过观察中发现的趋势,即随着长度的增加,这个趋势增强,", "metrics": {"bleu_score": 11.681570771679114, "chrf_score": 14.565627104652942, "xcomet_score": 0.5448042154312134, "xcomet_qe_score": 0.4633861780166626, "metricx_score": 5.240684986114502, "metricx_qe_score": 5.411253452301025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即当两个并列成分的长度差异增大时,较短的并列成分更倾向于排在前面。对。所以", "metrics": {"bleu_score": 35.95021478425863, "chrf_score": 28.72773551528359, "xcomet_score": 0.7190874218940735, "xcomet_qe_score": 0.7031341791152954, "metricx_score": 5.522861480712891, "metricx_qe_score": 3.9274182319641113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",左短并列成分的比例更大。但是", "metrics": {"bleu_score": 42.311785416105785, "chrf_score": 34.71833721833721, "xcomet_score": 0.4377900958061218, "xcomet_qe_score": 0.46354737877845764, "metricx_score": 6.283360004425049, "metricx_qe_score": 5.31286096572876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",本文的新颖之处在于,我们观察到这种趋势只有在支配词在左边或不存在时才会发生,所以", "metrics": {"bleu_score": 51.33767892698552, "chrf_score": 43.76097250253487, "xcomet_score": 0.7785483598709106, "xcomet_qe_score": 0.3970579206943512, "metricx_score": 6.206603527069092, "metricx_qe_score": 3.634145736694336, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "多,对吗?", "metrics": {"bleu_score": 10.682175159905848, "chrf_score": 9.803921568627452, "xcomet_score": 0.751610517501831, "xcomet_qe_score": 0.6130142211914062, "metricx_score": 2.366835117340088, "metricx_qe_score": 3.074291467666626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,我看到了巴特和丽莎,所以支配词在左边,呃,", "metrics": {"bleu_score": 27.96124497598784, "chrf_score": 17.038882550628045, "xcomet_score": 0.7197200655937195, "xcomet_qe_score": 0.6027911901473999, "metricx_score": 5.13715934753418, "metricx_qe_score": 4.635723114013672, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个例子中,霍默来了,打了个喷嚏,", "metrics": {"bleu_score": 22.135572417208273, "chrf_score": 11.556023791206844, "xcomet_score": 0.772435188293457, "xcomet_qe_score": 0.8087400794029236, "metricx_score": 4.832951545715332, "metricx_qe_score": 3.3061976432800293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里有两个动词的并列,没有外部支配词,对吗?所以", "metrics": {"bleu_score": 36.687256031835, "chrf_score": 31.65272955762914, "xcomet_score": 0.7082734704017639, "xcomet_qe_score": 0.645936131477356, "metricx_score": 5.246706008911133, "metricx_qe_score": 3.451563596725464, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下,呃,左并列成分更倾向于较短,尤其是两个并列成分之间的差异越大。", "metrics": {"bleu_score": 25.415667669475546, "chrf_score": 26.955698022511363, "xcomet_score": 0.6453763246536255, "xcomet_qe_score": 0.6539098024368286, "metricx_score": 6.916810989379883, "metricx_qe_score": 6.38494348526001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当右边的支配词在这里支配并列结构t和net时,这种效应就不复存在了。所以", "metrics": {"bleu_score": 16.813739852146227, "chrf_score": 12.959819138574586, "xcomet_score": 0.3902323544025421, "xcomet_qe_score": 0.153780996799469, "metricx_score": 9.536076545715332, "metricx_qe_score": 8.371545791625977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过测量字符长度,即第一列,音节,中间列,以及单词,右列来展示这一点。所以,我将重", "metrics": {"bleu_score": 11.717403068287256, "chrf_score": 14.944604994003935, "xcomet_score": 0.5073195695877075, "xcomet_qe_score": 0.24376840889453888, "metricx_score": 10.803271293640137, "metricx_qe_score": 6.972040176391602, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "点关注右边的。我们在这", "metrics": {"bleu_score": 18.36028134946796, "chrf_score": 17.27139303911263, "xcomet_score": 0.43345704674720764, "xcomet_qe_score": 0.1417640596628189, "metricx_score": 4.394379615783691, "metricx_qe_score": 3.5844779014587402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "里看到,当支配词在左边时,左并列成分较短的趋势随着单词的绝对差异稳步增长。当没有支配词时,也会观察到同样的现象,例如在句子并列中。但是", "metrics": {"bleu_score": 41.724971961306736, "chrf_score": 34.92072893771558, "xcomet_score": 0.3375188112258911, "xcomet_qe_score": 0.2964523732662201, "metricx_score": 8.610671043395996, "metricx_qe_score": 6.062954425811768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",当支配词在右边时,这种趋势就不复存在了。", "metrics": {"bleu_score": 41.09080290971358, "chrf_score": 33.976805042791845, "xcomet_score": 0.8191196918487549, "xcomet_qe_score": 0.5760728120803833, "metricx_score": 4.015850067138672, "metricx_qe_score": 6.212735652923584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在论文中展示了这一点如何为反对并列结构不对称性(如上述两种)和支持对称结构(如上述两种)提供了", "metrics": {"bleu_score": 37.51604220751384, "chrf_score": 35.23493406384431, "xcomet_score": 0.47253718972206116, "xcomet_qe_score": 0.21336571872234344, "metricx_score": 7.261680603027344, "metricx_qe_score": 4.316160202026367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "论据。所以,请参阅论文以", "metrics": {"bleu_score": 12.35459141795978, "chrf_score": 12.459613081382622, "xcomet_score": 0.4480302631855011, "xcomet_qe_score": 0.4343539774417877, "metricx_score": 7.853546142578125, "metricx_qe_score": 3.180342674255371, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获取完整的协议和论据,抱歉,并与我们讨论海报会议。", "metrics": {"bleu_score": 3.5823421191287177, "chrf_score": 5.211922811853894, "xcomet_score": 0.1484190821647644, "xcomet_qe_score": 0.1355496346950531, "metricx_score": 12.507637023925781, "metricx_qe_score": 10.13772201538086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9333682656288147, "xcomet_qe_score": 0.9679166674613953, "metricx_score": 0.46964308619499207, "metricx_qe_score": 0.171317458152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是华盛顿大学的博士生Xiangbin。", "metrics": {"bleu_score": 70.85876411943929, "chrf_score": 63.86513467463122, "xcomet_score": 0.8795247077941895, "xcomet_qe_score": 0.8485116362571716, "metricx_score": 0.35235127806663513, "metricx_qe_score": 1.016472339630127, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我将介绍我们从预训练数据到语言模型再到下游任务的工作,追踪导致不公平自然语言处理模型的政治偏见的轨迹。因此", "metrics": {"bleu_score": 62.26611688210154, "chrf_score": 59.3785733803021, "xcomet_score": 0.733115553855896, "xcomet_qe_score": 0.5974158048629761, "metricx_score": 4.202980995178223, "metricx_qe_score": 2.290449380874634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",语言模型是在大规模网络爬虫数据上训练的。", "metrics": {"bleu_score": 86.56030552541704, "chrf_score": 80.05243910429668, "xcomet_score": 0.9792409539222717, "xcomet_qe_score": 0.9557791352272034, "metricx_score": 1.9083929061889648, "metricx_qe_score": 2.612426519393921, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "政治新闻媒体在他们的预训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 48.195116293616074, "chrf_score": 48.402791332265075, "xcomet_score": 0.7571665048599243, "xcomet_qe_score": 0.7159234881401062, "metricx_score": 1.750040054321289, "metricx_qe_score": 2.615305185317993, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "根据对C4语料库的调查,我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。", "metrics": {"bleu_score": 82.4536596454478, "chrf_score": 79.54856362593671, "xcomet_score": 0.8716681003570557, "xcomet_qe_score": 0.8162964582443237, "metricx_score": 1.1905322074890137, "metricx_qe_score": 1.3291597366333008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为语言模型的应用带来了既是福又是祸。因此,", "metrics": {"bleu_score": 29.10624919304027, "chrf_score": 25.292214392203643, "xcomet_score": 0.8035324811935425, "xcomet_qe_score": 0.8197976350784302, "metricx_score": 3.998978853225708, "metricx_qe_score": 2.047321081161499, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一方面,它们能够从多样化的视角学习,这庆祝了民主和思想的多元性。", "metrics": {"bleu_score": 31.383369760338972, "chrf_score": 26.34662963257235, "xcomet_score": 0.8996644020080566, "xcomet_qe_score": 0.782477080821991, "metricx_score": 1.7841638326644897, "metricx_qe_score": 2.838578701019287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,这些不同的政治观点在本质上具有社会偏见,可能会在下游任务应用中导致潜在的公平问题。", "metrics": {"bleu_score": 71.04884432642514, "chrf_score": 62.91731469051266, "xcomet_score": 0.987799882888794, "xcomet_qe_score": 0.9727433919906616, "metricx_score": 1.0582624673843384, "metricx_qe_score": 1.2786921262741089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此,我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道,具体来说,通过以下问题。首先,我们如何评估语言模型的政治倾向,以及相关数据可能对这些政治偏见产生什么作用?", "metrics": {"bleu_score": 58.219477600451256, "chrf_score": 51.62336737820233, "xcomet_score": 0.7823531627655029, "xcomet_qe_score": 0.7613416910171509, "metricx_score": 1.8233461380004883, "metricx_qe_score": 2.0905747413635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,具有不同政治倾向的语言模型在实际的下游任务中表现如何,这是否会导致自然语言处理应用中的公平问题?因此", "metrics": {"bleu_score": 56.037462369600725, "chrf_score": 49.07840041415265, "xcomet_score": 0.8665637969970703, "xcomet_qe_score": 0.7823573350906372, "metricx_score": 2.880382537841797, "metricx_qe_score": 0.7685064077377319, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",具体来说,我们首先提议使用政治问卷(如政治指南针测试)对不同提示格式的语言模型进行提示。", "metrics": {"bleu_score": 51.0076156805044, "chrf_score": 43.517963732426324, "xcomet_score": 0.7070198059082031, "xcomet_qe_score": 0.6545741558074951, "metricx_score": 4.734236717224121, "metricx_qe_score": 5.700024127960205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这确保了我们的自动评估基于政治科学文献。因此,一些", "metrics": {"bleu_score": 35.693754559323295, "chrf_score": 32.09138233325921, "xcomet_score": 0.6219412088394165, "xcomet_qe_score": 0.656053900718689, "metricx_score": 7.463095188140869, "metricx_qe_score": 2.775780439376831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "初步结果表明,首先,语言模型确实具有不同的政治倾向。", "metrics": {"bleu_score": 65.84483944171477, "chrf_score": 56.22756108156389, "xcomet_score": 0.9917311668395996, "xcomet_qe_score": 1.0, "metricx_score": 0.7159748077392578, "metricx_qe_score": 0.7806177139282227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们占据了政治指南针上的四个象限。", "metrics": {"bleu_score": 54.20662441541858, "chrf_score": 44.723889306558185, "xcomet_score": 0.8590556383132935, "xcomet_qe_score": 0.8305246829986572, "metricx_score": 2.5154099464416504, "metricx_qe_score": 2.432861089706421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,GPT-4是所有语言模型中最自由派的一个,GPT理论通常更为自由。它们占据了政治指南针上的四个象限。我们还可以看到,GPT-4是所有语言模型中最自由派的一个,GPT理论通常比BERT理论及其变体更为社会自由。", "metrics": {"bleu_score": 23.764012567775726, "chrf_score": 39.74183780225688, "xcomet_score": 0.3213160037994385, "xcomet_qe_score": 0.3699773848056793, "metricx_score": 12.977426528930664, "metricx_qe_score": 9.32142162322998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,我们的目标是研究语言模型的政治偏见实际上是从训练数据中获得的程度。", "metrics": {"bleu_score": 58.796214002018345, "chrf_score": 52.86486799810719, "xcomet_score": 0.8387658596038818, "xcomet_qe_score": 0.824955403804779, "metricx_score": 3.502032518386841, "metricx_qe_score": 3.6997900009155273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们通过进一步在六个不同的党派语料库上对语言模型检查点进行预训练,这些语料库分为新闻和社交媒体,进一步分为其政治倾向,来进行受控实验。", "metrics": {"bleu_score": 52.266991505245386, "chrf_score": 45.458751797696515, "xcomet_score": 0.8616098761558533, "xcomet_qe_score": 0.7305540442466736, "metricx_score": 2.3601901531219482, "metricx_qe_score": 2.9869167804718018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过在这些党派语料库上进一步对语言模型进行预训练,我们可以看到语言模型的意识形态坐标也相应地发生了变化。", "metrics": {"bleu_score": 77.08156595294435, "chrf_score": 70.28505778708447, "xcomet_score": 0.9806571006774902, "xcomet_qe_score": 0.8880012035369873, "metricx_score": 1.1995638608932495, "metricx_qe_score": 1.8309810161590576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于Roberta,进一步微调,进一步训练于左翼的Reddit语料库,我们可以看到其政治偏见发生了实质性的自由派转变。", "metrics": {"bleu_score": 42.54802555453608, "chrf_score": 40.90752994462748, "xcomet_score": 0.7871332168579102, "xcomet_qe_score": 0.7174757719039917, "metricx_score": 4.930511951446533, "metricx_qe_score": 5.101438999176025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还试图研究语言模型是否能够捕捉到我们现代社会中普遍存在的极化。", "metrics": {"bleu_score": 70.65878033828864, "chrf_score": 64.20743266127026, "xcomet_score": 0.8909714221954346, "xcomet_qe_score": 0.9178553819656372, "metricx_score": 1.151965618133545, "metricx_qe_score": 1.2968729734420776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们将预训练语料库分为美国第45任总统当选前和当选后,我们", "metrics": {"bleu_score": 59.10904270166761, "chrf_score": 56.41512737720693, "xcomet_score": 0.7071124315261841, "xcomet_qe_score": 0.6294233202934265, "metricx_score": 6.132086753845215, "metricx_qe_score": 2.8730740547180176, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分别在两个不同的时间语料库上对语言模型进行预训练。我们可以", "metrics": {"bleu_score": 61.600757346033824, "chrf_score": 64.59398333512152, "xcomet_score": 0.803697407245636, "xcomet_qe_score": 0.5601537227630615, "metricx_score": 5.601038932800293, "metricx_qe_score": 1.9386918544769287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "看到,语言模型在2017年后通常具有更远离中心的政治倾向。因此,", "metrics": {"bleu_score": 62.84584618747518, "chrf_score": 57.826843293449706, "xcomet_score": 0.7109111547470093, "xcomet_qe_score": 0.6778053045272827, "metricx_score": 5.166924953460693, "metricx_qe_score": 4.040340423583984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明语言模型也可以捕捉到我们社会中的极化。", "metrics": {"bleu_score": 77.24579129745068, "chrf_score": 74.91256190403274, "xcomet_score": 0.9197232127189636, "xcomet_qe_score": 0.9364190697669983, "metricx_score": 1.5229437351226807, "metricx_qe_score": 1.872099757194519, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但并非最不重要的一点,我们评估了不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现,这些应用通常涉及语言模型,可能具有非常重大的影响。因此,", "metrics": {"bleu_score": 54.41049014692378, "chrf_score": 51.629551541836825, "xcomet_score": 0.685226321220398, "xcomet_qe_score": 0.7989568710327148, "metricx_score": 3.256971836090088, "metricx_qe_score": 2.527399778366089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,如果我们按类别评估性能,也就是说,如果我们将性能分为不同的人口统计或新闻媒体的政治意义,我们可以看到一个模式,", "metrics": {"bleu_score": 42.74181655066656, "chrf_score": 36.16971389823131, "xcomet_score": 0.6240038871765137, "xcomet_qe_score": 0.7069061994552612, "metricx_score": 7.177337169647217, "metricx_qe_score": 6.513970851898193, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,对于仇恨言论检测,左翼语言模型在检测针对社会少数群体的仇恨言论方面表现更好,然而,在检测针对我们社会中更强大群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 68.59718116110346, "chrf_score": 64.46139574917511, "xcomet_score": 0.9147182703018188, "xcomet_qe_score": 0.9149341583251953, "metricx_score": 1.3527799844741821, "metricx_qe_score": 1.2598085403442383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "反之,右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好,然而,在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。", "metrics": {"bleu_score": 68.17797786596107, "chrf_score": 71.50124422840595, "xcomet_score": 0.9833532571792603, "xcomet_qe_score": 0.9848936796188354, "metricx_score": 0.663007915019989, "metricx_qe_score": 0.7862257957458496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虚假新闻检测也出现了类似的趋势,我们看到左翼语言模型在检测来自其相反政治倾向的虚假信息方面表现更好,反之亦然。", "metrics": {"bleu_score": 47.00254429443473, "chrf_score": 39.629487489759676, "xcomet_score": 0.9862518310546875, "xcomet_qe_score": 0.9955637454986572, "metricx_score": 0.9851024150848389, "metricx_qe_score": 1.3206393718719482, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进一步展示了许多定性例子,以证明具有不同政治倾向的语言模型根据其社会类别对仇恨言论和虚假信息例子给出了不同的预测。", "metrics": {"bleu_score": 66.96821287957545, "chrf_score": 60.983812391285355, "xcomet_score": 0.9642060995101929, "xcomet_qe_score": 0.9629421234130859, "metricx_score": 1.1961060762405396, "metricx_qe_score": 1.6739217042922974, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "附录中有更多例子,进一步强调了根据社会类别对仇恨言论和虚假信息例子做出不同的预测。附录中有更多例子,进一步强调了这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平问题。", "metrics": {"bleu_score": 28.844529230711757, "chrf_score": 38.880974377680666, "xcomet_score": 0.36494800448417664, "xcomet_qe_score": 0.3046748638153076, "metricx_score": 5.337878227233887, "metricx_qe_score": 4.833558559417725, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,如果右翼语言模型被微调用于仇恨言论或虚假信息或其他,并部署到一个流行的社交媒体平台,这意味着具有相反政治观点的人可能会被边缘化。在仇恨言论或虚假信息或其他方面,并部署到一个流行的社交媒体平台,这意味着具有相反政治观点的人可能会被边缘化,而针对少数群体的仇恨言论可能会不受控制地蔓延。", "metrics": {"bleu_score": 37.00616462518055, "chrf_score": 44.370041017565754, "xcomet_score": 0.5097106099128723, "xcomet_qe_score": 0.5193920731544495, "metricx_score": 10.069705963134766, "metricx_qe_score": 10.834419250488281, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这为我们敲响了警钟,要求我们承认并解决语言模型政治倾向导致的公平问题。", "metrics": {"bleu_score": 37.54453237069206, "chrf_score": 39.67705752085453, "xcomet_score": 0.9922338724136353, "xcomet_qe_score": 0.9909923076629639, "metricx_score": 0.8676937818527222, "metricx_qe_score": 0.838931679725647, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,有一点讨论。我们还", "metrics": {"bleu_score": 10.600313379512592, "chrf_score": 13.161639157655095, "xcomet_score": 0.21191412210464478, "xcomet_qe_score": 0.18380187451839447, "metricx_score": 6.703768253326416, "metricx_qe_score": 4.9986891746521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望强调,我们揭示了语言模型政治偏见的独特困境。", "metrics": {"bleu_score": 59.53307597644483, "chrf_score": 56.0554961803888, "xcomet_score": 0.8538012504577637, "xcomet_qe_score": 0.7925469875335693, "metricx_score": 1.290986180305481, "metricx_qe_score": 2.0165674686431885, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就像在斯库拉和喀里布狄斯之间。", "metrics": {"bleu_score": 11.425435791072083, "chrf_score": 13.713018273968732, "xcomet_score": 0.765823245048523, "xcomet_qe_score": 0.7644734978675842, "metricx_score": 2.319312810897827, "metricx_qe_score": 1.6731715202331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,如果我们不净化语言模型训练数据中的政治观点,偏见将从预训练数据传播到语言模型再到下游任务,最终导致公平问题。问题。", "metrics": {"bleu_score": 64.85560643903318, "chrf_score": 57.807055328933885, "xcomet_score": 0.8042230606079102, "xcomet_qe_score": 0.694563627243042, "metricx_score": 3.466118097305298, "metricx_qe_score": 4.523435592651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们试图以某种方式净化,我们也会冒着审查或排斥的风险,", "metrics": {"bleu_score": 54.79097278114731, "chrf_score": 48.85154185977072, "xcomet_score": 0.8079379200935364, "xcomet_qe_score": 0.7907627820968628, "metricx_score": 2.086513042449951, "metricx_qe_score": 3.3468098640441895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而且很难确定什么才是真正中立的,应该保留在语言模型训练数据中。所以,", "metrics": {"bleu_score": 13.950557998119553, "chrf_score": 15.81810703231758, "xcomet_score": 0.7331921458244324, "xcomet_qe_score": 0.6691895723342896, "metricx_score": 4.989007949829102, "metricx_qe_score": 3.648995876312256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有点像电车难题。", "metrics": {"bleu_score": 80.07374029168083, "chrf_score": 79.34458487087205, "xcomet_score": 0.9047262072563171, "xcomet_qe_score": 0.8099632263183594, "metricx_score": 1.5479735136032104, "metricx_qe_score": 2.939612627029419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,所以", "metrics": {"bleu_score": 21.3643503198117, "chrf_score": 33.55457227138643, "xcomet_score": 0.8684302568435669, "xcomet_qe_score": 0.8399907946586609, "metricx_score": 3.290215015411377, "metricx_qe_score": 0.30569204688072205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想这就是我今天要讲的全部。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 55.344377844377846, "xcomet_score": 0.9879318475723267, "xcomet_qe_score": 0.9585496187210083, "metricx_score": 0.33117058873176575, "metricx_qe_score": 0.5036941766738892, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 35.49481056010054, "chrf_score": 30.038060761124484, "xcomet_score": 0.9801846742630005, "xcomet_qe_score": 0.8844002485275269, "metricx_score": 0.35994911193847656, "metricx_qe_score": 0.749150276184082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9877438545227051, "xcomet_qe_score": 0.9831967353820801, "metricx_score": 0.0, "metricx_qe_score": 0.0, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是珍妮,卡内基梅隆大学的一名一年级博士生,今天我将为大家介绍你们的论文《语义位置性:数据集和模型中设计偏见的特征描述》。", "metrics": {"bleu_score": 51.38491852827477, "chrf_score": 39.07166160439763, "xcomet_score": 0.8092517256736755, "xcomet_qe_score": 0.8795251846313477, "metricx_score": 1.7139766216278076, "metricx_qe_score": 2.037118911743164, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与华盛顿大学和艾伦人工智能研究所的一些研究人员合作完成的,其中包括塞巴斯蒂安·桑特、罗南·拉布拉塞、卡塔里娜·阿拉尼卡和马丁·萨普。", "metrics": {"bleu_score": 41.14479634998137, "chrf_score": 32.29088615135494, "xcomet_score": 0.6551722884178162, "xcomet_qe_score": 0.6385000944137573, "metricx_score": 1.6777232885360718, "metricx_qe_score": 1.2509856224060059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们先想象一下,你为一家报纸工作,正在筛选新闻文章下面的评论,试图删除有毒内容。", "metrics": {"bleu_score": 46.916589697991476, "chrf_score": 38.36958971907624, "xcomet_score": 0.8901606798171997, "xcomet_qe_score": 0.9401091933250427, "metricx_score": 1.5238714218139648, "metricx_qe_score": 1.385663390159607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你可能会转向像Perspective API这样的流行API进行毒性检测,如果你是卡尔·琼斯,Pers", "metrics": {"bleu_score": 13.271923936609255, "chrf_score": 25.148337309900192, "xcomet_score": 0.46157124638557434, "xcomet_qe_score": 0.463434636592865, "metricx_score": 10.441669464111328, "metricx_qe_score": 9.634624481201172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "pective API能够正确检测出有毒实例,那么这会非常有效。", "metrics": {"bleu_score": 26.699675743024226, "chrf_score": 56.11825862565545, "xcomet_score": 0.775255560874939, "xcomet_qe_score": 0.7394087314605713, "metricx_score": 9.152522087097168, "metricx_qe_score": 10.064372062683105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但对于阿迪亚·沙尔马来说,情况并非如此,因为", "metrics": {"bleu_score": 29.4467310498826, "chrf_score": 21.761738382385097, "xcomet_score": 0.7670023441314697, "xcomet_qe_score": 0.5647134780883789, "metricx_score": 4.474054336547852, "metricx_qe_score": 1.664931058883667, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Perspective API对印度语境中更常见的攻击性用词并不敏感。", "metrics": {"bleu_score": 64.42271946445945, "chrf_score": 68.23351827838022, "xcomet_score": 0.8377562761306763, "xcomet_qe_score": 0.751447319984436, "metricx_score": 4.321232318878174, "metricx_qe_score": 4.126806259155273, "linguapy_score": [1, "ROMANIAN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个设计偏见的例子,我们在这里看到技术在不同人群之间的系统性性能差异。", "metrics": {"bleu_score": 49.608376191368286, "chrf_score": 45.315649175926, "xcomet_score": 0.9821426868438721, "xcomet_qe_score": 0.8593618869781494, "metricx_score": 0.9824047088623047, "metricx_qe_score": 1.5483784675598145, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们之前看到的像这样的设计偏见可能源于自然语言处理研究人员和模型开发人员的语义位置性。", "metrics": {"bleu_score": 56.20235343624851, "chrf_score": 51.121756562180465, "xcomet_score": 0.858591616153717, "xcomet_qe_score": 0.8227983713150024, "metricx_score": 3.195814847946167, "metricx_qe_score": 2.9971227645874023, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义位置性是指人们由于其人口统计特征、身份和生活经历而持有的观点。", "metrics": {"bleu_score": 61.011521912723566, "chrf_score": 65.99914449781359, "xcomet_score": 0.8046215772628784, "xcomet_qe_score": 0.8332817554473877, "metricx_score": 3.9411561489105225, "metricx_qe_score": 2.789926052093506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个在批判性研究中广泛使用的概念,特别是在女权主义和酷儿学术领域。", "metrics": {"bleu_score": 73.97378912627735, "chrf_score": 67.41898597045686, "xcomet_score": 0.9927648305892944, "xcomet_qe_score": 0.908970832824707, "metricx_score": 0.9186691045761108, "metricx_qe_score": 1.4367049932479858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "作为研究人员,语义位置性会影响研究过程及其结果和结论,因为它会改变研究人员做出的决策。", "metrics": {"bleu_score": 54.83299544202423, "chrf_score": 48.414524474208925, "xcomet_score": 0.9056611061096191, "xcomet_qe_score": 0.9244251251220703, "metricx_score": 3.914044141769409, "metricx_qe_score": 3.3234949111938477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,人们可能会问,数据集和模型是否有语义位置性?", "metrics": {"bleu_score": 50.71142527563296, "chrf_score": 44.03069952351174, "xcomet_score": 0.9289334416389465, "xcomet_qe_score": 0.9457945227622986, "metricx_score": 2.0195326805114746, "metricx_qe_score": 1.0906881093978882, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们并不是说模型本身和数据集本身具有人口统计特征和生活经历,但它们确实汇集了真实的人们的判断和观点,因此可以代表模型和数据集本身具有人口统计特征和生活经历,但它们确实汇集了真实的人们的判断和观点,因此可以代表某些语义位置性", "metrics": {"bleu_score": 31.319989081524003, "chrf_score": 39.46647307097591, "xcomet_score": 0.506065845489502, "xcomet_qe_score": 0.5355991125106812, "metricx_score": 12.208220481872559, "metricx_qe_score": 11.682493209838867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "优于其他语义位置性。之前的研究已经提出了关于语义位置性的轶事证据,例如文化差距和模型与数据集,以及模型语义位置性的理论定义。", "metrics": {"bleu_score": 43.20616093173349, "chrf_score": 41.5144993183852, "xcomet_score": 0.3571320176124573, "xcomet_qe_score": 0.11643074452877045, "metricx_score": 6.296402931213379, "metricx_qe_score": 6.430822372436523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些研究并没有真正比较最终用户与数据集和模型本身。随着自然语言处理任务变得更加主观和社会导向,研究模型和数据集的语义位置性变得越来越重要。要描述这些语义位置性的偏差是具有挑战性的,因为并非所有决策都被记录下来,许多模型隐藏在API后面。", "metrics": {"bleu_score": 60.48395343445504, "chrf_score": 55.72848444946731, "xcomet_score": 0.8391884565353394, "xcomet_qe_score": 0.9089341163635254, "metricx_score": 2.972381830215454, "metricx_qe_score": 2.561314105987549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,为了研究数据集和模型的语义位置性,我们实际上是比较了真实用户与现有数据集和模型的注释。", "metrics": {"bleu_score": 46.20475648785034, "chrf_score": 41.7074993883498, "xcomet_score": 0.8757224678993225, "xcomet_qe_score": 0.8273319005966187, "metricx_score": 2.8821234703063965, "metricx_qe_score": 2.9026269912719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过我们的框架NL语义位置性来实现这一点。", "metrics": {"bleu_score": 24.33821845902227, "chrf_score": 17.208618435609395, "xcomet_score": 0.8151296377182007, "xcomet_qe_score": 0.8116193413734436, "metricx_score": 1.7162867784500122, "metricx_qe_score": 2.4947423934936523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架主要分为两个步骤。", "metrics": {"bleu_score": 78.25422900366432, "chrf_score": 71.42992378040401, "xcomet_score": 0.9698691368103027, "xcomet_qe_score": 0.8897930383682251, "metricx_score": 0.06376159191131592, "metricx_qe_score": 0.3005968928337097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一步是用来自不同背景的注释员重新注释数据集。", "metrics": {"bleu_score": 49.380155419366794, "chrf_score": 44.687041908370624, "xcomet_score": 0.8169848918914795, "xcomet_qe_score": 0.9010310173034668, "metricx_score": 1.1396286487579346, "metricx_qe_score": 0.659016489982605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们选择这样做,而不是查看原始数据集注释员的人口统计数据,因为通常只有少数注释员对每个实例进行注释,而且人口统计数据很少被收集和分享。", "metrics": {"bleu_score": 69.87300642643305, "chrf_score": 62.388659604896965, "xcomet_score": 0.9148105382919312, "xcomet_qe_score": 0.8423871994018555, "metricx_score": 1.340713381767273, "metricx_qe_score": 1.2407770156860352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们选择重新注释数据,以便每个实例有多个注释员,并获得丰富的社会人口数据。", "metrics": {"bleu_score": 33.062142261692244, "chrf_score": 31.23341203773677, "xcomet_score": 0.8360137939453125, "xcomet_qe_score": 0.7542096376419067, "metricx_score": 3.222015380859375, "metricx_qe_score": 2.8818342685699463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将按人口统计数据进行的注释与模型和数据集进行比较,使用皮尔逊相关系数。因此,我们的框架实际上与注释员意见不一致的文献不同,它通过比较最终用户与模型和数据集、预测和标签,而不是仅仅关注注释员的意见一致性或模型注释员分布,来比较最终用户与模型和数据集。", "metrics": {"bleu_score": 46.65815263561929, "chrf_score": 46.15572420296156, "xcomet_score": 0.6321149468421936, "xcomet_qe_score": 0.521447479724884, "metricx_score": 4.925110816955566, "metricx_qe_score": 4.94143533706665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的框架在很大程度上得益于我们的HCI合作者的在线众包平台Lab in the Wild。", "metrics": {"bleu_score": 48.641898542593324, "chrf_score": 61.547731005670194, "xcomet_score": 0.8205992579460144, "xcomet_qe_score": 0.6998043060302734, "metricx_score": 1.6779732704162598, "metricx_qe_score": 2.5999739170074463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Live in the Wild是一个在线实验平台,我们可以在这里招募来自不同背景的志愿者。", "metrics": {"bleu_score": 51.55993405849779, "chrf_score": 56.317515596333855, "xcomet_score": 0.8637017607688904, "xcomet_qe_score": 0.9229626059532166, "metricx_score": 1.6184768676757812, "metricx_qe_score": 1.4309194087982178, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "众包平台,前HCI合作者。Lab in the Wild是一个在线实验平台,我们可以在这里招募来自不同背景的志愿者,相比于MTurk这样的平台,后者主要有来自美国或印度的参与者。此外,Lab in the Wild仍然能够获得高质量的数据。", "metrics": {"bleu_score": 25.12663353571842, "chrf_score": 46.59594342008626, "xcomet_score": 0.17476284503936768, "xcomet_qe_score": 0.1663806438446045, "metricx_score": 3.909783124923706, "metricx_qe_score": 4.3190999031066895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Lab in the Wild上举办了两项任务,其中一项是社会可接受性。它的工作原理是参与者将阅读来自社会化学数据集的某个情境,然后他们将写出这个情境在社会上是多么可接受。", "metrics": {"bleu_score": 51.542019813275225, "chrf_score": 49.48616966254992, "xcomet_score": 0.848502516746521, "xcomet_qe_score": 0.8678436279296875, "metricx_score": 2.089846611022949, "metricx_qe_score": 2.5987930297851562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之后,为了保持参与研究,他们可以将自己的回答与AI和其他人的回答进行比较。", "metrics": {"bleu_score": 75.97383875533318, "chrf_score": 70.19791204908213, "xcomet_score": 0.8835495710372925, "xcomet_qe_score": 0.8812954425811768, "metricx_score": 1.1496191024780273, "metricx_qe_score": 1.6605678796768188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与社会化学、德尔菲和GPT-4进行比较。然后,我们为毒性检测和仇恨言论检测任务复制了一个非常相似的设置。我们将这些注释与社会化学、德尔菲和GPT-4进行比较。", "metrics": {"bleu_score": 14.120858384745228, "chrf_score": 27.72282651339796, "xcomet_score": 0.16491004824638367, "xcomet_qe_score": 0.16092632710933685, "metricx_score": 13.569082260131836, "metricx_qe_score": 14.147138595581055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们为毒性检测和仇恨言论检测任务复制了一个非常相似的设置,参与者将阅读DynaHate中的某个实例,并写出他们是否认为这是一个仇恨言论的实例。", "metrics": {"bleu_score": 53.92668510762963, "chrf_score": 52.3961325160603, "xcomet_score": 0.7495970129966736, "xcomet_qe_score": 0.8092436790466309, "metricx_score": 2.0316460132598877, "metricx_qe_score": 2.660872220993042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将这些注释与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。", "metrics": {"bleu_score": 60.90890681369303, "chrf_score": 82.75037918468948, "xcomet_score": 0.8960332274436951, "xcomet_qe_score": 0.8407766819000244, "metricx_score": 1.3146069049835205, "metricx_qe_score": 2.308399200439453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的研究最终收集了来自87个国家的1000多名注释员的16000多条注释。", "metrics": {"bleu_score": 84.67418606557692, "chrf_score": 82.24633855880393, "xcomet_score": 0.9506648182868958, "xcomet_qe_score": 0.9944043159484863, "metricx_score": 0.8858261108398438, "metricx_qe_score": 0.6886630058288574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们现在更有能力回答自然语言处理数据集和模型最符合谁的问题。", "metrics": {"bleu_score": 38.862012568620365, "chrf_score": 35.798312643765286, "xcomet_score": 0.9001294374465942, "xcomet_qe_score": 0.9078379273414612, "metricx_score": 1.0411946773529053, "metricx_qe_score": 0.991265058517456, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现自然语言处理中存在语义位置性。", "metrics": {"bleu_score": 16.19557012853241, "chrf_score": 15.814572628683596, "xcomet_score": 0.8851977586746216, "xcomet_qe_score": 0.9736109972000122, "metricx_score": 2.356222152709961, "metricx_qe_score": 1.3535000085830688, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们发现数据集和模型最符合英语国家的用户。因此", "metrics": {"bleu_score": 45.33710895095744, "chrf_score": 39.50138806100798, "xcomet_score": 0.836583137512207, "xcomet_qe_score": 0.817164957523346, "metricx_score": 2.571972131729126, "metricx_qe_score": 0.9347166419029236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPT-4的社会可接受性分析,我们发现它最符合儒家和英语国家的用户。", "metrics": {"bleu_score": 51.016939317085026, "chrf_score": 48.349277029009514, "xcomet_score": 0.8121525645256042, "xcomet_qe_score": 0.8224668502807617, "metricx_score": 3.052713394165039, "metricx_qe_score": 3.280272960662842, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现DynaHate也最符合英语国家的用户。", "metrics": {"bleu_score": 64.1975224568211, "chrf_score": 69.63106465108754, "xcomet_score": 0.9422332048416138, "xcomet_qe_score": 0.8349034786224365, "metricx_score": 1.0511105060577393, "metricx_qe_score": 1.3871021270751953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现与受过大学教育的人群的额外对齐。因此", "metrics": {"bleu_score": 33.77724221619386, "chrf_score": 28.04823475345276, "xcomet_score": 0.6362454891204834, "xcomet_qe_score": 0.6719408631324768, "metricx_score": 6.370044708251953, "metricx_qe_score": 3.8100242614746094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",对于GPT-4的社会可接受性任务,我们发现它最符合受过大学教育或研究生教育的人群。我们还发现DynaHate也是如此,它最符合受过大学教育的人群。", "metrics": {"bleu_score": 51.891908938104635, "chrf_score": 48.12081307793381, "xcomet_score": 0.8960064649581909, "xcomet_qe_score": 0.9055080413818359, "metricx_score": 3.876455545425415, "metricx_qe_score": 4.113130569458008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当模型和数据集与特定人群对齐时,有些人不可避免地会被抛在后面。一个例子", "metrics": {"bleu_score": 44.910995009687426, "chrf_score": 42.99224018447892, "xcomet_score": 0.7212121486663818, "xcomet_qe_score": 0.6895356178283691, "metricx_score": 2.6209826469421387, "metricx_qe_score": 2.5738062858581543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是,数据集和模型与非二元人群的对齐程度低于男性和女性人群。", "metrics": {"bleu_score": 44.05155249693397, "chrf_score": 35.43146431794215, "xcomet_score": 0.5778648853302002, "xcomet_qe_score": 0.5974311828613281, "metricx_score": 4.487579822540283, "metricx_qe_score": 4.6895012855529785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在GPT-4的社会可接受性任务以及DynaHEAT任务分析中都发现了这一点。", "metrics": {"bleu_score": 67.16833901993986, "chrf_score": 60.94586425553753, "xcomet_score": 0.8318672180175781, "xcomet_qe_score": 0.8539873957633972, "metricx_score": 2.6391260623931885, "metricx_qe_score": 3.0431201457977295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,鉴于自然语言处理中存在语义位置性,我们能对此做些什么呢?", "metrics": {"bleu_score": 18.882437844970767, "chrf_score": 19.820164567722383, "xcomet_score": 0.9213143587112427, "xcomet_qe_score": 0.9832650423049927, "metricx_score": 1.644118309020996, "metricx_qe_score": 1.0311568975448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们有几个建议。", "metrics": {"bleu_score": 23.76101887396088, "chrf_score": 20.67864629018774, "xcomet_score": 0.9698759317398071, "xcomet_qe_score": 0.8993586301803589, "metricx_score": 0.269827276468277, "metricx_qe_score": 0.2961741089820862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一,在整个研究过程中记录所有相关的设计选择。", "metrics": {"bleu_score": 52.63665779342922, "chrf_score": 44.06177814445921, "xcomet_score": 0.9980634450912476, "xcomet_qe_score": 0.9874120950698853, "metricx_score": 0.26769396662712097, "metricx_qe_score": 0.3085995614528656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二,从视角主义的角度进行自然语言处理研究。", "metrics": {"bleu_score": 52.12485469629971, "chrf_score": 54.393069180431056, "xcomet_score": 0.875601053237915, "xcomet_qe_score": 0.8385424017906189, "metricx_score": 0.5327723026275635, "metricx_qe_score": 0.28753799200057983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第三个建议是在四个特定社区内构建专门的数据集和模型。", "metrics": {"bleu_score": 90.61874434879648, "chrf_score": 86.31885674989124, "xcomet_score": 0.992019772529602, "xcomet_qe_score": 0.9185789823532104, "metricx_score": 0.8037492632865906, "metricx_qe_score": 1.2312043905258179, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个很好的例子是Masakane倡议。我的意思是,我们", "metrics": {"bleu_score": 33.92375549949802, "chrf_score": 40.36031820817438, "xcomet_score": 0.24685676395893097, "xcomet_qe_score": 0.1895391345024109, "metricx_score": 7.13431453704834, "metricx_qe_score": 6.250432014465332, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "希望强调包容性自然语言处理不仅仅是让所有", "metrics": {"bleu_score": 19.319794288373767, "chrf_score": 15.245941449117653, "xcomet_score": 0.3041742146015167, "xcomet_qe_score": 0.1687413603067398, "metricx_score": 6.595285415649414, "metricx_qe_score": 5.185056209564209, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "技术为所有人工作。", "metrics": {"bleu_score": 21.165417903210944, "chrf_score": 19.232670017992138, "xcomet_score": 0.8996185064315796, "xcomet_qe_score": 0.890823483467102, "metricx_score": 3.384999990463257, "metricx_qe_score": 4.1462836265563965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们的演讲到此结束。", "metrics": {"bleu_score": 73.61703354503862, "chrf_score": 92.32407811754193, "xcomet_score": 0.9918334484100342, "xcomet_qe_score": 0.9781018495559692, "metricx_score": 0.2742224931716919, "metricx_qe_score": 0.3502793312072754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果您想了解更多,请随时查看我们的仪表板以获取最新的分析结果和我们的论文。", "metrics": {"bleu_score": 56.92907020342422, "chrf_score": 51.528204312603684, "xcomet_score": 0.9835224151611328, "xcomet_qe_score": 0.9583417773246765, "metricx_score": 0.5844244360923767, "metricx_qe_score": 0.5608056783676147, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9333682656288147, "xcomet_qe_score": 0.9679166674613953, "metricx_score": 0.46964308619499207, "metricx_qe_score": 0.171317458152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是复旦大学的袁思雨。", "metrics": {"bleu_score": 34.64226178936947, "chrf_score": 21.33663927854213, "xcomet_score": 0.902703583240509, "xcomet_qe_score": 0.8837342262268066, "metricx_score": 0.4593205153942108, "metricx_qe_score": 0.4997575283050537, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我在这里介绍我们的工作,即从大型语言模型中提取脚本知识以进行约束语言规划。", "metrics": {"bleu_score": 36.56552719684994, "chrf_score": 30.528323786196488, "xcomet_score": 0.8845951557159424, "xcomet_qe_score": 0.776800811290741, "metricx_score": 1.875435709953308, "metricx_qe_score": 1.8734691143035889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在日常生活中,人类通常通过遵循形式为保证脚本的逐步指令来规划自己的行动", "metrics": {"bleu_score": 36.18413125474381, "chrf_score": 29.55811495199964, "xcomet_score": 0.7861039638519287, "xcomet_qe_score": 0.8017086982727051, "metricx_score": 5.913570404052734, "metricx_qe_score": 6.174394607543945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 3.875968992248062, "xcomet_score": 0.3287827968597412, "xcomet_qe_score": 0.14962130784988403, "metricx_score": 19.53483772277832, "metricx_qe_score": 25.0, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1503392457962036, "xcomet_qe_score": 0.11045360565185547, "metricx_score": 20.31755828857422, "metricx_qe_score": 22.061500549316406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的研究主要集中在抽象规划上。", "metrics": {"bleu_score": 27.440914443347936, "chrf_score": 32.04275332532318, "xcomet_score": 0.3561323285102844, "xcomet_qe_score": 0.33104896545410156, "metricx_score": 7.954102993011475, "metricx_qe_score": 8.921923637390137, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个好的规划者应该第一次编写", "metrics": {"bleu_score": 1.73767225759596, "chrf_score": 2.368907349083617, "xcomet_score": 0.14528921246528625, "xcomet_qe_score": 0.1481473445892334, "metricx_score": 21.82892417907715, "metricx_qe_score": 18.378116607666016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "脚本。规划的目标。", "metrics": {"bleu_score": 1.081467542976273, "chrf_score": 5.928365865481303, "xcomet_score": 0.14729005098342896, "xcomet_qe_score": 0.15029360353946686, "metricx_score": 16.08331298828125, "metricx_qe_score": 22.611125946044922, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个抽象目标可以被不同的现实生活具体目标继承,这些具体目标具有多方面的", "metrics": {"bleu_score": 32.09740767841793, "chrf_score": 31.832090489434535, "xcomet_score": 0.7873161435127258, "xcomet_qe_score": 0.8130432367324829, "metricx_score": 4.618955612182617, "metricx_qe_score": 2.9652204513549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "约束。一个好的规划者应该编写合理且忠实于约束的脚本。", "metrics": {"bleu_score": 32.522040345339356, "chrf_score": 27.518730064702595, "xcomet_score": 0.7192040681838989, "xcomet_qe_score": 0.5789675712585449, "metricx_score": 2.8319427967071533, "metricx_qe_score": 3.119338274002075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本文中,我们首先评估和改进大型语言模型的约束语言规划能力。", "metrics": {"bleu_score": 56.38909821116625, "chrf_score": 45.16075580443396, "xcomet_score": 0.8932648301124573, "xcomet_qe_score": 0.8747851848602295, "metricx_score": 0.8539466261863708, "metricx_qe_score": 0.891362726688385, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于没有具体的目标数据集来支持我们的研究,我们必须首先获取这些目标。", "metrics": {"bleu_score": 75.83204368015235, "chrf_score": 69.24158588817045, "xcomet_score": 0.899175763130188, "xcomet_qe_score": 0.8859267234802246, "metricx_score": 1.4271697998046875, "metricx_qe_score": 2.4054136276245117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如表所示,我们使用InstructGPT扩展了人类在循环数据获取中具有多方面约束的抽象目标。", "metrics": {"bleu_score": 33.79887904628572, "chrf_score": 42.99231236935435, "xcomet_score": 0.7453698515892029, "xcomet_qe_score": 0.8041257858276367, "metricx_score": 5.08136510848999, "metricx_qe_score": 4.835240840911865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们抽取了100个具体目标,并评估了大型语言模型生成的脚本。", "metrics": {"bleu_score": 77.12738645345884, "chrf_score": 71.24182139699383, "xcomet_score": 0.9818644523620605, "xcomet_qe_score": 0.9555319547653198, "metricx_score": 1.4473204612731934, "metricx_qe_score": 2.460300922393799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该表报告了结果的总体准确性。", "metrics": {"bleu_score": 42.57110866884422, "chrf_score": 32.00655355944401, "xcomet_score": 0.9927786588668823, "xcomet_qe_score": 0.9881556034088135, "metricx_score": 0.7947359681129456, "metricx_qe_score": 0.811003565788269, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现所有大型语言模型在规划具体目标方面都取得了不令人满意的结果。", "metrics": {"bleu_score": 50.92289847616022, "chrf_score": 50.139573467930774, "xcomet_score": 0.9279656410217285, "xcomet_qe_score": 0.9060831665992737, "metricx_score": 1.1683690547943115, "metricx_qe_score": 1.548918604850769, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们进行详细分析,以调查学习模型为何会失败。", "metrics": {"bleu_score": 32.44913214932798, "chrf_score": 25.592775581906018, "xcomet_score": 0.9948760271072388, "xcomet_qe_score": 0.9901952743530273, "metricx_score": 0.4922921359539032, "metricx_qe_score": 0.48850125074386597, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的结果显示,生成脚本的语义完整性是可以接受的,但无法保证对约束的忠实度。", "metrics": {"bleu_score": 46.0764066957219, "chrf_score": 40.843782106953704, "xcomet_score": 0.9302752017974854, "xcomet_qe_score": 0.9579711556434631, "metricx_score": 1.2590579986572266, "metricx_qe_score": 1.5961129665374756, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们深入研究了更细粒度的约束主题类别,具体取决于工作方式。", "metrics": {"bleu_score": 22.06023613092981, "chrf_score": 17.35899853541835, "xcomet_score": 0.6470884084701538, "xcomet_qe_score": 0.548223078250885, "metricx_score": 4.47393798828125, "metricx_qe_score": 5.404856204986572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图中的热图显示,指示性TPD的规划性能在不同类别的女孩中差异很大。", "metrics": {"bleu_score": 41.33732829767917, "chrf_score": 28.932699050157503, "xcomet_score": 0.5557228326797485, "xcomet_qe_score": 0.5046592950820923, "metricx_score": 6.432380676269531, "metricx_qe_score": 6.018354892730713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "之前的研究表明,轻日志模型的输出质量在高方差下下降,导致性能不佳。", "metrics": {"bleu_score": 40.49153725730702, "chrf_score": 38.00062065072403, "xcomet_score": 0.7060987949371338, "xcomet_qe_score": 0.6705281734466553, "metricx_score": 4.271093845367432, "metricx_qe_score": 4.8844757080078125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们采用了过度生成的Z-过滤器来提高生成质量。", "metrics": {"bleu_score": 44.13713504244446, "chrf_score": 36.53571032315976, "xcomet_score": 0.8473422527313232, "xcomet_qe_score": 0.8445528745651245, "metricx_score": 3.6736183166503906, "metricx_qe_score": 4.458777904510498, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先展示了对Instruct GPT的约束类型示例,并根据种子抽象目标获得了具体目标。", "metrics": {"bleu_score": 57.85182910790548, "chrf_score": 60.18169090630804, "xcomet_score": 0.767571210861206, "xcomet_qe_score": 0.7035038471221924, "metricx_score": 3.4777164459228516, "metricx_qe_score": 4.121997833251953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,Instruct GPT过度生成特定目标的关键脚本。", "metrics": {"bleu_score": 25.34743707366162, "chrf_score": 57.9195215680723, "xcomet_score": 0.8374991416931152, "xcomet_qe_score": 0.8211126327514648, "metricx_score": 4.228208065032959, "metricx_qe_score": 6.189116477966309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,开发了一个过滤器模型来选择可行的脚本。", "metrics": {"bleu_score": 50.39714528818165, "chrf_score": 41.16206286930882, "xcomet_score": 0.9615634679794312, "xcomet_qe_score": 0.9484173059463501, "metricx_score": 0.9882960319519043, "metricx_qe_score": 1.1427185535430908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将脚本和目标转换为InstructGPT嵌入,并计算余弦相似度和相似度分数以衡量语义相似度。", "metrics": {"bleu_score": 75.13288493857628, "chrf_score": 74.74278234336322, "xcomet_score": 0.8938717842102051, "xcomet_qe_score": 0.7339885234832764, "metricx_score": 1.8665273189544678, "metricx_qe_score": 2.1227059364318848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们奖励包含目标约束关键词的脚本。", "metrics": {"bleu_score": 57.97765003730531, "chrf_score": 54.89525378061659, "xcomet_score": 0.8477377891540527, "xcomet_qe_score": 0.8053245544433594, "metricx_score": 0.9846863746643066, "metricx_qe_score": 1.2254678010940552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果目标在目标集中得分最高,我们才保留该脚本。", "metrics": {"bleu_score": 53.730043685394435, "chrf_score": 48.516639576334455, "xcomet_score": 0.77683424949646, "xcomet_qe_score": 0.6914117336273193, "metricx_score": 3.6078603267669678, "metricx_qe_score": 5.692928314208984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的方法,InstructZBT可以生成更高质量的脚本。", "metrics": {"bleu_score": 81.37489370974959, "chrf_score": 71.10573084340234, "xcomet_score": 0.845077633857727, "xcomet_qe_score": 0.8182790279388428, "metricx_score": 3.9429922103881836, "metricx_qe_score": 4.470638275146484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法大大提高了规划能力,无论是在语义完整性还是对约束的忠实度方面。", "metrics": {"bleu_score": 60.66841973239411, "chrf_score": 52.59948235689428, "xcomet_score": 0.9388656616210938, "xcomet_qe_score": 0.9870235919952393, "metricx_score": 0.8968896269798279, "metricx_qe_score": 1.4757966995239258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于大型语言模型的部署成本高,因此必须使较小和专业模型具备语言规划能力。", "metrics": {"bleu_score": 48.125437682132976, "chrf_score": 41.8695171639076, "xcomet_score": 0.9751602411270142, "xcomet_qe_score": 0.9688800573348999, "metricx_score": 0.7400224804878235, "metricx_qe_score": 1.1847221851348877, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "创建数据集是实现这一目标的必要步骤。", "metrics": {"bleu_score": 75.11573912724296, "chrf_score": 70.00087795052626, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.11220654845237732, "metricx_qe_score": 0.20811273157596588, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,之前的研究并未实现对具体目标的规划,手动数据集标注成本高昂。", "metrics": {"bleu_score": 42.665923524557556, "chrf_score": 34.69312300444184, "xcomet_score": 0.9831161499023438, "xcomet_qe_score": 0.9621390104293823, "metricx_score": 1.2996338605880737, "metricx_qe_score": 1.6338108777999878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们遵循符号知识蒸馏的理念,从大型语言模型中提取约束语言规划数据集。", "metrics": {"bleu_score": 53.750070923717814, "chrf_score": 44.8303168863609, "xcomet_score": 0.9121347665786743, "xcomet_qe_score": 0.83054518699646, "metricx_score": 1.562540054321289, "metricx_qe_score": 2.1950602531433105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们应用我们的方法来构建一个约束语言规划的数据集,命名为CodeScript。", "metrics": {"bleu_score": 44.486291435595234, "chrf_score": 47.22823445975921, "xcomet_score": 0.8663047552108765, "xcomet_qe_score": 0.7866952419281006, "metricx_score": 1.3713182210922241, "metricx_qe_score": 1.7583980560302734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总共,我们生成了55,000个带有脚本的具体目标。", "metrics": {"bleu_score": 42.6291379422223, "chrf_score": 48.928085199824324, "xcomet_score": 0.8836750984191895, "xcomet_qe_score": 0.862751841545105, "metricx_score": 1.9803526401519775, "metricx_qe_score": 1.7195788621902466, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确保验证和测试网站的质量,我们请云端工人找到修订的错误样本。", "metrics": {"bleu_score": 26.049472347968614, "chrf_score": 24.386112303066774, "xcomet_score": 0.6922796964645386, "xcomet_qe_score": 0.67757248878479, "metricx_score": 7.296595096588135, "metricx_qe_score": 7.103363037109375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "该图显示了代码脚本的约束分布。", "metrics": {"bleu_score": 49.202745153855076, "chrf_score": 29.64490504867994, "xcomet_score": 0.8700239658355713, "xcomet_qe_score": 0.8391302227973938, "metricx_score": 3.129685640335083, "metricx_qe_score": 4.138612270355225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现代码脚本在生成的具体目标中表现出高度的赞赏。", "metrics": {"bleu_score": 65.56397462101789, "chrf_score": 50.10690547480283, "xcomet_score": 0.657014012336731, "xcomet_qe_score": 0.5684618949890137, "metricx_score": 8.643147468566895, "metricx_qe_score": 9.200250625610352, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有了代码脚本,我们可以将较小但专业的模型用于约束语言规划。", "metrics": {"bleu_score": 34.315351162206234, "chrf_score": 22.345176705337234, "xcomet_score": 0.6642441749572754, "xcomet_qe_score": 0.6764631271362305, "metricx_score": 3.708360433578491, "metricx_qe_score": 4.341787815093994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现Antune的TFI在成本率上可以生成0的平方根。有了代码脚本,我们可以将较小但专业的模型用于约束语言规划。我们发现CodeScript上的T-file函数可以生成比大多数大型语言模型更高质量的脚本,表明在适当的数据集上进行适当训练的小型模型可以支持大型模型。", "metrics": {"bleu_score": 30.458916416553105, "chrf_score": 39.43862616040249, "xcomet_score": 0.265275239944458, "xcomet_qe_score": 0.20680604875087738, "metricx_score": 11.249006271362305, "metricx_qe_score": 10.487101554870605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们建立了约束语言规划问题。", "metrics": {"bleu_score": 51.109970380326146, "chrf_score": 42.579016307796216, "xcomet_score": 0.8879767656326294, "xcomet_qe_score": 0.8463901877403259, "metricx_score": 2.0746877193450928, "metricx_qe_score": 3.441852569580078, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们评估了大型语言模型的约束语言规划能力,并开发了一个过度生成过滤器方法研究", "metrics": {"bleu_score": 38.6062620493619, "chrf_score": 31.78133921896567, "xcomet_score": 0.7319656610488892, "xcomet_qe_score": 0.7077281475067139, "metricx_score": 4.808037757873535, "metricx_qe_score": 4.6494550704956055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语言规划。", "metrics": {"bleu_score": 0.673794699908547, "chrf_score": 10.310104691904074, "xcomet_score": 0.19485364854335785, "xcomet_qe_score": 0.15005280077457428, "metricx_score": 17.175243377685547, "metricx_qe_score": 23.131526947021484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的时间。有关CodeScript的更多详细信息", "metrics": {"bleu_score": 1.698797377536768, "chrf_score": 14.770441458725125, "xcomet_score": 0.1578250229358673, "xcomet_qe_score": 0.1467890441417694, "metricx_score": 6.058741569519043, "metricx_qe_score": 7.842854022979736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",请参阅", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.2068335860967636, "xcomet_qe_score": 0.2260444164276123, "metricx_score": 4.787365913391113, "metricx_qe_score": 4.876613616943359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文。", "metrics": {"bleu_score": 6.527979667647036, "chrf_score": 12.0051757803948, "xcomet_score": 0.22010532021522522, "xcomet_qe_score": 0.19235123693943024, "metricx_score": 4.626029014587402, "metricx_qe_score": 7.504727840423584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫朱恒。", "metrics": {"bleu_score": 23.356898886410015, "chrf_score": 13.375784036811606, "xcomet_score": 0.8176854848861694, "xcomet_qe_score": 0.8306083083152771, "metricx_score": 0.055027518421411514, "metricx_qe_score": 0.18194499611854553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍我们的论文《2003年核命名实体标注器在2023年是否仍然有效?》", "metrics": {"bleu_score": 71.31461657201187, "chrf_score": 61.99854937667383, "xcomet_score": 0.7354687452316284, "xcomet_qe_score": 0.7863953113555908, "metricx_score": 4.783512592315674, "metricx_qe_score": 4.444721221923828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "让我们开始吧。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9996732473373413, "xcomet_qe_score": 0.9978755712509155, "metricx_score": 0.06470449268817902, "metricx_qe_score": 0.4635288119316101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文研究了使用命名实体识别任务或NER任务进行泛化的问题。", "metrics": {"bleu_score": 56.3175436224042, "chrf_score": 50.44106832228087, "xcomet_score": 0.8923683762550354, "xcomet_qe_score": 0.8046994209289551, "metricx_score": 1.6376698017120361, "metricx_qe_score": 3.852041721343994, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们观察到,近20年来,模型一直在使用Kano 2003来开发NER,这自然引发了几个问题。", "metrics": {"bleu_score": 28.194964815380775, "chrf_score": 28.31285624431098, "xcomet_score": 0.7677399516105652, "xcomet_qe_score": 0.786406934261322, "metricx_score": 8.159164428710938, "metricx_qe_score": 7.818918704986572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,这些模型能否泛化到现代数据?", "metrics": {"bleu_score": 78.28161456481268, "chrf_score": 75.02410579616462, "xcomet_score": 0.9989852905273438, "xcomet_qe_score": 0.9952034950256348, "metricx_score": 0.28112250566482544, "metricx_qe_score": 0.2982765734195709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们开发新的标注器时,需要什么才能实现良好的泛化?", "metrics": {"bleu_score": 62.56538561604213, "chrf_score": 53.31061933235846, "xcomet_score": 0.9991610050201416, "xcomet_qe_score": 0.9945460557937622, "metricx_score": 0.4566256105899811, "metricx_qe_score": 0.6512018442153931, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,如果我们观察到泛化效果不佳,这些模型性能下降的原因是什么?", "metrics": {"bleu_score": 28.848991269997814, "chrf_score": 23.859983506389646, "xcomet_score": 0.9961634874343872, "xcomet_qe_score": 0.9807212352752686, "metricx_score": 0.865648627281189, "metricx_qe_score": 0.9357471466064453, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究这些问题,我们开发了Kano++数据集。这是", "metrics": {"bleu_score": 48.679550186613355, "chrf_score": 35.01156004540689, "xcomet_score": 0.666559100151062, "xcomet_qe_score": 0.6915520429611206, "metricx_score": 7.359029769897461, "metricx_qe_score": 5.675938606262207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个我们从2020年路透社新闻中收集的数据集,然后使用相同的Cono2003标注指南对它们进行了标注。", "metrics": {"bleu_score": 56.48989765320949, "chrf_score": 50.64851073320102, "xcomet_score": 0.7522686719894409, "xcomet_qe_score": 0.6763582229614258, "metricx_score": 5.751442909240723, "metricx_qe_score": 5.591739177703857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们随后在Cono2003上微调了20多个模型。", "metrics": {"bleu_score": 25.056314142932678, "chrf_score": 29.06866944654777, "xcomet_score": 0.8694170117378235, "xcomet_qe_score": 0.8958660364151001, "metricx_score": 3.591635227203369, "metricx_qe_score": 3.4692211151123047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在Con in F1上对它们进行了评估", "metrics": {"bleu_score": 43.40620876252846, "chrf_score": 32.787639826739806, "xcomet_score": 0.29603731632232666, "xcomet_qe_score": 0.18201887607574463, "metricx_score": 10.463825225830078, "metricx_qe_score": 13.411518096923828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",以评估每个模型的泛化能力。", "metrics": {"bleu_score": 25.497640508114088, "chrf_score": 32.21318580482499, "xcomet_score": 0.35998615622520447, "xcomet_qe_score": 0.41154834628105164, "metricx_score": 10.933666229248047, "metricx_qe_score": 13.229966163635254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,实现良好泛化的条件是什么?", "metrics": {"bleu_score": 22.025684816521746, "chrf_score": 20.70184037226328, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.35844939947128296, "metricx_qe_score": 0.35232242941856384, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现需要三个主要条件。", "metrics": {"bleu_score": 46.087110150246495, "chrf_score": 42.23430747046477, "xcomet_score": 0.9339300394058228, "xcomet_qe_score": 0.9039961695671082, "metricx_score": 1.615716576576233, "metricx_qe_score": 3.0777015686035156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是模型架构。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.99041748046875, "xcomet_qe_score": 0.9915783405303955, "metricx_score": 0.0, "metricx_qe_score": 0.10443663597106934, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过我们的实验,我们发现Transformer模型通常能更好地泛化到新数据。", "metrics": {"bleu_score": 52.77140132412705, "chrf_score": 60.9534154761757, "xcomet_score": 0.8849719762802124, "xcomet_qe_score": 0.878563404083252, "metricx_score": 1.71575927734375, "metricx_qe_score": 3.442965269088745, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个条件是模型大小。", "metrics": {"bleu_score": 29.0374612189482, "chrf_score": 27.07335820776018, "xcomet_score": 0.9307094812393188, "xcomet_qe_score": 0.8874010443687439, "metricx_score": 0.9825903177261353, "metricx_qe_score": 1.2973945140838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通常较大的模型能更好地泛化。", "metrics": {"bleu_score": 45.77398674868605, "chrf_score": 38.573922273055835, "xcomet_score": 0.9961094856262207, "xcomet_qe_score": 0.985889196395874, "metricx_score": 0.5113587379455566, "metricx_qe_score": 1.0648908615112305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,但同样重要的是,我们都知道,微调示例的数量直接影响下游任务的性能。在这里,", "metrics": {"bleu_score": 48.603536954486756, "chrf_score": 59.02651053032313, "xcomet_score": 0.9504135847091675, "xcomet_qe_score": 0.7990680932998657, "metricx_score": 3.6507959365844727, "metricx_qe_score": 2.065248489379883, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,更多的微调示例实际上也能带来更好的泛化效果。", "metrics": {"bleu_score": 61.71392021671757, "chrf_score": 54.0859508289967, "xcomet_score": 0.993468165397644, "xcomet_qe_score": 0.9293878674507141, "metricx_score": 0.38501015305519104, "metricx_qe_score": 0.49382561445236206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "关于我们的下一个问题,导致某些模型性能下降的原因是什么?我们有两个假设。", "metrics": {"bleu_score": 48.26764489049477, "chrf_score": 40.109663203828276, "xcomet_score": 0.9234381914138794, "xcomet_qe_score": 0.9215957522392273, "metricx_score": 0.6875584721565247, "metricx_qe_score": 0.8662148118019104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是自适应过拟合,这是由于反复使用相同的测试集导致的过拟合,通常表现为在新测试集上的回报递减。", "metrics": {"bleu_score": 55.311213272351964, "chrf_score": 47.04405564835722, "xcomet_score": 0.9092496633529663, "xcomet_qe_score": 0.9328229427337646, "metricx_score": 2.970985174179077, "metricx_qe_score": 3.2529871463775635, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个假设是时间漂移,这是由于训练数据和测试数据之间的时间差距越来越大导致的性能下降。", "metrics": {"bleu_score": 59.948411476888616, "chrf_score": 55.91837555509886, "xcomet_score": 0.9644029140472412, "xcomet_qe_score": 0.8866081833839417, "metricx_score": 1.847697377204895, "metricx_qe_score": 2.513777732849121, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于自适应过拟合,我们从右侧的图表中看到,红色的最佳拟合线有一个大于1的梯度。", "metrics": {"bleu_score": 26.920656092018486, "chrf_score": 27.485377605595286, "xcomet_score": 0.8767713308334351, "xcomet_qe_score": 0.807401180267334, "metricx_score": 1.3456370830535889, "metricx_qe_score": 1.514228343963623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着我们在CONO 2003上做出的每一项改进都转化为在Kano++上的超过一项改进,这意味着没有回报递减。", "metrics": {"bleu_score": 40.20185002165359, "chrf_score": 35.60259280800015, "xcomet_score": 0.5987149477005005, "xcomet_qe_score": 0.6737846732139587, "metricx_score": 8.61191177368164, "metricx_qe_score": 8.968017578125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明在这种情况下没有观察到自适应过拟合。", "metrics": {"bleu_score": 74.93731939490364, "chrf_score": 69.43707675795987, "xcomet_score": 0.9009255766868591, "xcomet_qe_score": 0.9129918217658997, "metricx_score": 1.1392955780029297, "metricx_qe_score": 1.6724114418029785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么时间漂移呢?", "metrics": {"bleu_score": 52.47357977607325, "chrf_score": 40.51960415097498, "xcomet_score": 0.9311673641204834, "xcomet_qe_score": 0.9159005284309387, "metricx_score": 0.3731555640697479, "metricx_qe_score": 0.8887962102890015, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于时间漂移,这证实了我们的假设,即性能下降的主要原因是时间漂移。", "metrics": {"bleu_score": 14.956796907631349, "chrf_score": 28.194986516713076, "xcomet_score": 0.8292940855026245, "xcomet_qe_score": 0.7954291105270386, "metricx_score": 8.470086097717285, "metricx_qe_score": 11.212931632995605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结论是,为了实现良好的泛化,我们需要更好的模型架构、更大的模型大小以及更多的微调示例。", "metrics": {"bleu_score": 80.29442553780838, "chrf_score": 73.0587099823086, "xcomet_score": 0.9366432428359985, "xcomet_qe_score": 0.8591218590736389, "metricx_score": 0.47305187582969666, "metricx_qe_score": 0.4831877052783966, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些条件是相辅相成的。我们不能只具备其中一个条件,而忽略其他条件。", "metrics": {"bleu_score": 30.023685775352963, "chrf_score": 26.734763598810673, "xcomet_score": 0.9917702674865723, "xcomet_qe_score": 0.9775091409683228, "metricx_score": 1.3165781497955322, "metricx_qe_score": 1.0367227792739868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还发现,这里的性能下降是由时间漂移引起的,而且令人惊讶的是,它不是由自适应过拟合引起的,尽管Kano 2003已经使用了20多年。", "metrics": {"bleu_score": 61.07686505941932, "chrf_score": 53.317241007904784, "xcomet_score": 0.6560094952583313, "xcomet_qe_score": 0.6344091892242432, "metricx_score": 5.346733093261719, "metricx_qe_score": 5.837204456329346, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,回到我们论文标题提出的问题,Kano 2003标注器在2023年是否仍然有效?", "metrics": {"bleu_score": 58.6892366072214, "chrf_score": 53.401208691061385, "xcomet_score": 0.7556975483894348, "xcomet_qe_score": 0.7966891527175903, "metricx_score": 5.491114616394043, "metricx_qe_score": 5.442297458648682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现答案实际上是肯定的。", "metrics": {"bleu_score": 62.98129992394241, "chrf_score": 53.5017446130673, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.43837279081344604, "metricx_qe_score": 0.7667475342750549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望我们的论文能促使更多人研究如何提高模型的泛化能力。", "metrics": {"bleu_score": 56.41405122106224, "chrf_score": 48.57935283218993, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3274407982826233, "metricx_qe_score": 0.45826369524002075, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,请务必查看我们的论文和数据集,如果您有任何问题,请随时与我联系。", "metrics": {"bleu_score": 58.11026448209409, "chrf_score": 52.5325928587273, "xcomet_score": 0.9871149063110352, "xcomet_qe_score": 0.9712950587272644, "metricx_score": 0.2757876515388489, "metricx_qe_score": 0.26652368903160095, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9978005886077881, "xcomet_qe_score": 0.9769038558006287, "metricx_score": 0.0, "metricx_qe_score": 0.14050978422164917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我将谈谈我们在解决间接指称表达以进行实体选择方面的工作,我们在此介绍了备选实体语料库。", "metrics": {"bleu_score": 35.77248467569026, "chrf_score": 26.531388367114555, "xcomet_score": 0.7434749603271484, "xcomet_qe_score": 0.7797645330429077, "metricx_score": 4.652287483215332, "metricx_qe_score": 3.7128965854644775, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我的名字是贾瓦德·霍赛尼,这是一项与菲利普·拉德林斯基、西尔维亚·帕里蒂和安妮·刘易斯合作完成的工作。", "metrics": {"bleu_score": 2.540917870682806, "chrf_score": 3.183138300490679, "xcomet_score": 0.9638526439666748, "xcomet_qe_score": 0.9567685127258301, "metricx_score": 2.2268943786621094, "metricx_qe_score": 1.8734318017959595, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的目标是理解用户在做出选择时使用的语言。", "metrics": {"bleu_score": 68.88074582865497, "chrf_score": 63.14849770363761, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.5080945491790771, "metricx_qe_score": 0.7215737700462341, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请考虑以下备选问题:", "metrics": {"bleu_score": 20.556680845025987, "chrf_score": 17.638888888888893, "xcomet_score": 0.8989036083221436, "xcomet_qe_score": 0.9259333610534668, "metricx_score": 0.28136301040649414, "metricx_qe_score": 0.2955072522163391, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你是说《Easy on Me》还是《I Got a Feeling》?", "metrics": {"bleu_score": 16.218053067939284, "chrf_score": 49.12106927761098, "xcomet_score": 0.9710274934768677, "xcomet_qe_score": 0.963426411151886, "metricx_score": 2.004880666732788, "metricx_qe_score": 2.4606380462646484, "linguapy_score": [1, "TAGALOG"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用户想要在这两首歌曲中进行选择,最", "metrics": {"bleu_score": 25.468958832633554, "chrf_score": 21.782849274508166, "xcomet_score": 0.7967060804367065, "xcomet_qe_score": 0.8409611582756042, "metricx_score": 6.347151756286621, "metricx_qe_score": 0.49594807624816895, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "明显的方法是使用直接引用,例如说出歌曲的名称《Easy on Me》或其位置第一首,但", "metrics": {"bleu_score": 49.78001501509219, "chrf_score": 48.2535423214426, "xcomet_score": 0.505126416683197, "xcomet_qe_score": 0.38784587383270264, "metricx_score": 5.167533874511719, "metricx_qe_score": 5.269785404205322, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有时使用间接引用更合适,可以进行更自然的对话。这种情况可能发生", "metrics": {"bleu_score": 30.7137308263447, "chrf_score": 37.82287761298203, "xcomet_score": 0.8250749111175537, "xcomet_qe_score": 0.800616979598999, "metricx_score": 4.074667453765869, "metricx_qe_score": 1.6081064939498901, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在用户记不起歌曲名称时,或者", "metrics": {"bleu_score": 8.099280728303281, "chrf_score": 10.826630993126116, "xcomet_score": 0.881101667881012, "xcomet_qe_score": 0.9145553112030029, "metricx_score": 2.7083592414855957, "metricx_qe_score": 1.5317325592041016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "发音过于相似,难以区分时", "metrics": {"bleu_score": 15.14700827223251, "chrf_score": 16.058633119873218, "xcomet_score": 0.874103307723999, "xcomet_qe_score": 0.8998661041259766, "metricx_score": 2.1354522705078125, "metricx_qe_score": 0.3641728162765503, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",或者当用户想要指定偏好时。以下是几个示例", "metrics": {"bleu_score": 27.430246164478792, "chrf_score": 21.703766966864492, "xcomet_score": 0.7547978162765503, "xcomet_qe_score": 0.5533010959625244, "metricx_score": 1.594443440437317, "metricx_qe_score": 1.9937176704406738, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "间接差异,例如:较新的那首或不是充满活力的那首。", "metrics": {"bleu_score": 10.30673774154583, "chrf_score": 12.40835756547361, "xcomet_score": 0.6883449554443359, "xcomet_qe_score": 0.6302050948143005, "metricx_score": 5.467033386230469, "metricx_qe_score": 3.9436938762664795, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是对话系统中的一个重要问题,也是用于基准测试大型语言模型实体理解能力的任务。我们没有发现一个公共", "metrics": {"bleu_score": 39.38101573642548, "chrf_score": 41.34336714831769, "xcomet_score": 0.32730087637901306, "xcomet_qe_score": 0.18426299095153809, "metricx_score": 6.712874412536621, "metricx_qe_score": 5.02926778793335, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据集,一个针对该任务的大型公共数据集,因此我们通过众包标注收集了一个。", "metrics": {"bleu_score": 27.64273831464269, "chrf_score": 23.13191875465657, "xcomet_score": 0.5652939081192017, "xcomet_qe_score": 0.5340953469276428, "metricx_score": 7.429775714874268, "metricx_qe_score": 7.747253894805908, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集涵盖了三个不同的领域:音乐、书籍和食谱。", "metrics": {"bleu_score": 78.47574847738748, "chrf_score": 71.38793914595793, "xcomet_score": 0.9996216297149658, "xcomet_qe_score": 0.9887402057647705, "metricx_score": 0.2191678285598755, "metricx_qe_score": 0.3114262521266937, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的数据集收集方法强调非正式性,使用卡通完成设置。卡通中有三个对话气泡。我们的数据集收集方法强调非正式性,", "metrics": {"bleu_score": 36.967491322418205, "chrf_score": 54.08572246146663, "xcomet_score": 0.6841118335723877, "xcomet_qe_score": 0.6514006853103638, "metricx_score": 8.871170043945312, "metricx_qe_score": 11.099918365478516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用卡通完成设置。卡通中有三个对话气泡。", "metrics": {"bleu_score": 18.759202316167208, "chrf_score": 23.4131666111113, "xcomet_score": 0.6700639724731445, "xcomet_qe_score": 0.6110296845436096, "metricx_score": 1.5939204692840576, "metricx_qe_score": 1.5862981081008911, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第一个气泡中,鲍勃说,还记得我们昨天听的那首歌吗?然后", "metrics": {"bleu_score": 60.60655437708259, "chrf_score": 54.84126836663069, "xcomet_score": 0.8537525534629822, "xcomet_qe_score": 0.7270839214324951, "metricx_score": 5.0527191162109375, "metricx_qe_score": 1.5214470624923706, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鲍勃设置了对话上下文。", "metrics": {"bleu_score": 40.855580969845114, "chrf_score": 28.742690621045575, "xcomet_score": 0.954749584197998, "xcomet_qe_score": 0.931614339351654, "metricx_score": 1.0160953998565674, "metricx_qe_score": 1.114621877670288, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第二个对话气泡中,爱丽丝说,你是说《Easy on Me》还是《I Got a Feeling》?这是", "metrics": {"bleu_score": 16.862986607229324, "chrf_score": 38.66648208316094, "xcomet_score": 0.7154435515403748, "xcomet_qe_score": 0.6900895237922668, "metricx_score": 6.9792046546936035, "metricx_qe_score": 5.199021339416504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个备选问题。", "metrics": {"bleu_score": 24.177237023718654, "chrf_score": 20.734397822080492, "xcomet_score": 0.8495973348617554, "xcomet_qe_score": 0.8766944408416748, "metricx_score": 1.546188473701477, "metricx_qe_score": 2.2151060104370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在第三个对话气泡中,鲍勃使用间接引用来选择其中一个实体,例如:较新的那首。", "metrics": {"bleu_score": 45.93321525934075, "chrf_score": 36.7296975582527, "xcomet_score": 0.8577507138252258, "xcomet_qe_score": 0.6807281970977783, "metricx_score": 3.6209299564361572, "metricx_qe_score": 5.240310192108154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们自动提供第一个和第二个对话气泡,但第三个由标注者填写。", "metrics": {"bleu_score": 56.11658422412668, "chrf_score": 49.75721796436182, "xcomet_score": 0.8379323482513428, "xcomet_qe_score": 0.8236285448074341, "metricx_score": 1.6795135736465454, "metricx_qe_score": 1.4413166046142578, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个对话气泡是从每个领域的一些手动提示中选出的。", "metrics": {"bleu_score": 45.78226095312775, "chrf_score": 37.2415656885809, "xcomet_score": 0.8831501007080078, "xcomet_qe_score": 0.771369993686676, "metricx_score": 2.525980234146118, "metricx_qe_score": 2.116523265838623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二个对话气泡,即备选问题,是通过以下方式生成的:", "metrics": {"bleu_score": 11.328360454400997, "chrf_score": 15.704116440717787, "xcomet_score": 0.8977564573287964, "xcomet_qe_score": 0.8833972215652466, "metricx_score": 0.8061878085136414, "metricx_qe_score": 0.7301620244979858, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们始终使用一个简单的模板:", "metrics": {"bleu_score": 34.078064670813475, "chrf_score": 31.208861639513476, "xcomet_score": 0.9939380884170532, "xcomet_qe_score": 0.9638677835464478, "metricx_score": 0.3507748544216156, "metricx_qe_score": 0.30439096689224243, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "你指的是A还是B,其中", "metrics": {"bleu_score": 25.965358893403383, "chrf_score": 26.24648175148181, "xcomet_score": 0.8704588413238525, "xcomet_qe_score": 0.7830706834793091, "metricx_score": 2.546429395675659, "metricx_qe_score": 0.48925817012786865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "A和B是从维基百科中选取的样本。", "metrics": {"bleu_score": 45.93073632354733, "chrf_score": 37.53421250154584, "xcomet_score": 0.9814267158508301, "xcomet_qe_score": 0.9958642721176147, "metricx_score": 0.6862636208534241, "metricx_qe_score": 0.7736921906471252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们使用的不同抽样方法:", "metrics": {"bleu_score": 92.53911813809742, "chrf_score": 91.84565434565434, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.05610925704240799, "metricx_qe_score": 0.13698068261146545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们在列表中向上移动时,实体变得越来越相似,通常更难进行消歧。", "metrics": {"bleu_score": 48.287171837052036, "chrf_score": 42.61563713257725, "xcomet_score": 0.8577663898468018, "xcomet_qe_score": 0.7904423475265503, "metricx_score": 4.571025848388672, "metricx_qe_score": 5.461899757385254, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一个是均匀随机,第二个", "metrics": {"bleu_score": 16.59038701421971, "chrf_score": 15.642227406933292, "xcomet_score": 0.6706678867340088, "xcomet_qe_score": 0.6632634401321411, "metricx_score": 6.309635162353516, "metricx_qe_score": 1.9362906217575073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是实体有相似标题的情况,例如两本书都名为《回归》;第三", "metrics": {"bleu_score": 13.855144349736934, "chrf_score": 15.671304940850444, "xcomet_score": 0.5370968580245972, "xcomet_qe_score": 0.5297489166259766, "metricx_score": 8.498641967773438, "metricx_qe_score": 5.876950263977051, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "种情况是它们在维基百科上的描述相似,最后一种", "metrics": {"bleu_score": 60.52665103345166, "chrf_score": 57.660103289458476, "xcomet_score": 0.4132974147796631, "xcomet_qe_score": 0.3510384261608124, "metricx_score": 8.55710220336914, "metricx_qe_score": 4.773970127105713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "情况是例如同一", "metrics": {"bleu_score": 0.6679009843017161, "chrf_score": 0.8417508417508417, "xcomet_score": 0.14892728626728058, "xcomet_qe_score": 0.1456468105316162, "metricx_score": 11.984647750854492, "metricx_qe_score": 8.648228645324707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "类型或同一艺术家。", "metrics": {"bleu_score": 16.466920669770428, "chrf_score": 17.698104474700894, "xcomet_score": 0.7060126066207886, "xcomet_qe_score": 0.6146893501281738, "metricx_score": 3.485276222229004, "metricx_qe_score": 4.573654651641846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当我们向管理员展示这个备选问题时,他们知道这些实体的名称,但并不一定了解实体本身", "metrics": {"bleu_score": 42.971937367572906, "chrf_score": 36.70530265973523, "xcomet_score": 0.757835865020752, "xcomet_qe_score": 0.7106001973152161, "metricx_score": 4.518477439880371, "metricx_qe_score": 4.376467227935791, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",所以我们展示了一些关于这两个实体的背景知识。", "metrics": {"bleu_score": 52.99142455649877, "chrf_score": 44.51410971433854, "xcomet_score": 0.8994530439376831, "xcomet_qe_score": 0.7742542028427124, "metricx_score": 2.2493908405303955, "metricx_qe_score": 3.057495355606079, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于歌曲,我们只需为每首歌曲提供一个谷歌搜索链接,然后要求标注者至少收听每首歌曲的一部分,并阅读每首歌曲的介绍。", "metrics": {"bleu_score": 39.661468087102726, "chrf_score": 33.59473436626465, "xcomet_score": 0.8739358186721802, "xcomet_qe_score": 0.8427107334136963, "metricx_score": 1.1720545291900635, "metricx_qe_score": 1.196129560470581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是《Easy on Me》的谷歌搜索结果示例。", "metrics": {"bleu_score": 38.42138674057353, "chrf_score": 46.34793434216072, "xcomet_score": 0.9598653316497803, "xcomet_qe_score": 0.9576573371887207, "metricx_score": 0.8247448801994324, "metricx_qe_score": 0.7954061031341553, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱和书籍领域,我们展示了一些维基百科的背景文本。", "metrics": {"bleu_score": 57.46020726864588, "chrf_score": 48.26554160147655, "xcomet_score": 0.9329940676689148, "xcomet_qe_score": 0.9090638160705566, "metricx_score": 0.9826923608779907, "metricx_qe_score": 1.6537595987319946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于食谱,我们还从维基百科再次展示了它们的图片,以便标注者了解它们的样子。", "metrics": {"bleu_score": 25.312483921969957, "chrf_score": 22.02441496060501, "xcomet_score": 0.828810453414917, "xcomet_qe_score": 0.86664879322052, "metricx_score": 2.367076873779297, "metricx_qe_score": 2.0117435455322266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后我们要求标注者选择其中一个实体,例如:第一首,并使用三到五个间接指称表达来描述它们,", "metrics": {"bleu_score": 43.01210747635789, "chrf_score": 38.5467215208389, "xcomet_score": 0.6635127067565918, "xcomet_qe_score": 0.7006822228431702, "metricx_score": 4.926339626312256, "metricx_qe_score": 4.247113227844238, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如:钢琴音乐的那首。", "metrics": {"bleu_score": 11.731175160263996, "chrf_score": 12.42424242424242, "xcomet_score": 0.9477984309196472, "xcomet_qe_score": 0.9240420460700989, "metricx_score": 2.809231758117676, "metricx_qe_score": 1.9161595106124878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集中的几个示例", "metrics": {"bleu_score": 64.53174978135057, "chrf_score": 61.32467858545223, "xcomet_score": 0.971794843673706, "xcomet_qe_score": 0.9279520511627197, "metricx_score": 0.6749170422554016, "metricx_qe_score": 1.4508007764816284, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ":例如:没有歌词的那首,不是有12岁男孩的那首,也不是虚构的那首,或者来自阿塞拜疆的那首等等。", "metrics": {"bleu_score": 28.686034114077287, "chrf_score": 28.448809643425065, "xcomet_score": 0.8682351112365723, "xcomet_qe_score": 0.8534516096115112, "metricx_score": 2.950629949569702, "metricx_qe_score": 3.296595811843872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "备选实体语料库包含三个领域中的6,000个备选问题,以及42,000个间接指称表达。", "metrics": {"bleu_score": 39.162127614717114, "chrf_score": 40.09636241392475, "xcomet_score": 0.6353184580802917, "xcomet_qe_score": 0.528059720993042, "metricx_score": 3.2400670051574707, "metricx_qe_score": 2.883164644241333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用T5X模型的结果如下。", "metrics": {"bleu_score": 29.869942012564593, "chrf_score": 35.94704914551735, "xcomet_score": 0.8505170345306396, "xcomet_qe_score": 0.8444743752479553, "metricx_score": 1.6581337451934814, "metricx_qe_score": 1.5210261344909668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问与标注者完全相同的背景知识,那么准确率非常高,大约在92%到95%之间,", "metrics": {"bleu_score": 70.15486753372355, "chrf_score": 64.07988973862643, "xcomet_score": 0.8405320048332214, "xcomet_qe_score": 0.8829030990600586, "metricx_score": 1.1374837160110474, "metricx_qe_score": 0.982201874256134, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这并不现实。", "metrics": {"bleu_score": 27.890014303843827, "chrf_score": 23.047933414170444, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.03864777833223343, "metricx_qe_score": 0.04394784942269325, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果语言模型可以访问一些部分重叠的背景知识,那么准确率在82%到87%之间,这更现实。", "metrics": {"bleu_score": 77.27448084928581, "chrf_score": 75.30274093056529, "xcomet_score": 0.8976619839668274, "xcomet_qe_score": 0.8982731103897095, "metricx_score": 1.1338919401168823, "metricx_qe_score": 1.7477842569351196, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,当语言模型检索背景知识时", "metrics": {"bleu_score": 77.10792371922997, "chrf_score": 74.18063378409278, "xcomet_score": 0.9941446781158447, "xcomet_qe_score": 0.9908560514450073, "metricx_score": 0.37645384669303894, "metricx_qe_score": 0.5439862608909607, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",准确率只有60%,因此还有很大的改进空间。", "metrics": {"bleu_score": 33.73704646033403, "chrf_score": 39.10810846344497, "xcomet_score": 0.907770037651062, "xcomet_qe_score": 0.8981091976165771, "metricx_score": 7.6525468826293945, "metricx_qe_score": 10.6918363571167, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还表明,模型具有领域泛化能力。", "metrics": {"bleu_score": 35.55508425572383, "chrf_score": 29.61443703019254, "xcomet_score": 0.9106235504150391, "xcomet_qe_score": 0.9032565951347351, "metricx_score": 0.8168161511421204, "metricx_qe_score": 1.0266481637954712, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们的数据集链接。", "metrics": {"bleu_score": 29.50234363196404, "chrf_score": 30.119895842275447, "xcomet_score": 0.9908864498138428, "xcomet_qe_score": 0.9903539419174194, "metricx_score": 0.23959046602249146, "metricx_qe_score": 0.38847288489341736, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9302589893341064, "xcomet_qe_score": 0.9727964401245117, "metricx_score": 0.4261555075645447, "metricx_qe_score": 0.24359971284866333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我是特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕皮,我将简要介绍我们的论文《注意力作为同步语音翻译的指导》,这是我和马泰奥·内格里、马可·图尔奇合作完成的。", "metrics": {"bleu_score": 39.59238270403144, "chrf_score": 30.964806758392406, "xcomet_score": 0.9117249250411987, "xcomet_qe_score": 0.8039761185646057, "metricx_score": 3.404181957244873, "metricx_qe_score": 2.967360258102417, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "什么是同步语音翻译?", "metrics": {"bleu_score": 27.77619034011791, "chrf_score": 24.978786979062058, "xcomet_score": 0.9915040731430054, "xcomet_qe_score": 0.9834308624267578, "metricx_score": 0.2992181181907654, "metricx_qe_score": 0.12179544568061829, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同步语音翻译或SimulST是指将口语实时翻译成另一种语言的文本,实现跨语言交流。那么,", "metrics": {"bleu_score": 57.565265615740934, "chrf_score": 57.558246115379575, "xcomet_score": 0.7741945385932922, "xcomet_qe_score": 0.7232913970947266, "metricx_score": 4.867710590362549, "metricx_qe_score": 3.96596622467041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的SimulST模型有哪些问题呢?", "metrics": {"bleu_score": 82.4236750264605, "chrf_score": 92.26506423113229, "xcomet_score": 0.9719007015228271, "xcomet_qe_score": 0.9776618480682373, "metricx_score": 0.324434757232666, "metricx_qe_score": 0.8126562833786011, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常会训练特定的架构,引入额外的模块进行优化。例如", "metrics": {"bleu_score": 52.467904568974845, "chrf_score": 48.30755198626731, "xcomet_score": 0.8672053813934326, "xcomet_qe_score": 0.8224432468414307, "metricx_score": 1.0228079557418823, "metricx_qe_score": 1.4737759828567505, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",训练过程漫长且复杂,涉及不同的优化目标", "metrics": {"bleu_score": 32.47977183883474, "chrf_score": 26.19431926570566, "xcomet_score": 0.9122767448425293, "xcomet_qe_score": 0.8701450824737549, "metricx_score": 3.0136232376098633, "metricx_qe_score": 3.5258240699768066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",以及训练和维护多个模型以达到不同的延迟", "metrics": {"bleu_score": 48.8627596980411, "chrf_score": 46.78251421975124, "xcomet_score": 0.9146232604980469, "xcomet_qe_score": 0.9254367351531982, "metricx_score": 2.459968328475952, "metricx_qe_score": 2.0669734477996826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "机制,例如训练一个平均延迟为1秒的模型和另一个延迟为2秒的模型等。", "metrics": {"bleu_score": 51.16985986248211, "chrf_score": 42.54313293307996, "xcomet_score": 0.6094236373901367, "xcomet_qe_score": 0.5137723088264465, "metricx_score": 2.440643310546875, "metricx_qe_score": 2.1595511436462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,我们的解决方案是什么呢?", "metrics": {"bleu_score": 74.87402156832427, "chrf_score": 75.65635752018586, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.07452559471130371, "metricx_qe_score": 0.18974152207374573, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,使用现有的离线SD模型,无需重新训练或采用特定的SimulSD架构。", "metrics": {"bleu_score": 38.93321532760173, "chrf_score": 46.639603096800805, "xcomet_score": 0.7763257026672363, "xcomet_qe_score": 0.8434619903564453, "metricx_score": 5.149374961853027, "metricx_qe_score": 6.040323257446289, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每个延迟机制只使用一个模型,并通过特定参数处理延迟。此外,利用", "metrics": {"bleu_score": 47.63001139940413, "chrf_score": 42.5813471338178, "xcomet_score": 0.7909811735153198, "xcomet_qe_score": 0.7261971235275269, "metricx_score": 5.739851474761963, "metricx_qe_score": 1.087915062904358, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "模型通过音频输入和文本输出之间的注意力机制(即交叉注意力机制", "metrics": {"bleu_score": 59.62415297042496, "chrf_score": 58.15067645682329, "xcomet_score": 0.7593388557434082, "xcomet_qe_score": 0.7074575424194336, "metricx_score": 6.219468116760254, "metricx_qe_score": 4.784849166870117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ")获得的知识。您可以在右侧看到一个例子。", "metrics": {"bleu_score": 4.814971807094068, "chrf_score": 5.256029653483614, "xcomet_score": 0.511162519454956, "xcomet_qe_score": 0.6032301187515259, "metricx_score": 8.425213813781738, "metricx_qe_score": 10.126806259155273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的解决方案是提出EDAT或编码器-解码器注意力,这是一种策略,我们根据注意力指向的位置决定是否发出部分翻译。", "metrics": {"bleu_score": 56.589622268300936, "chrf_score": 50.18235370020424, "xcomet_score": 0.5772363543510437, "xcomet_qe_score": 0.5852670073509216, "metricx_score": 4.903018474578857, "metricx_qe_score": 6.064555644989014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果注意力不集中,即其总和低于某个阈值alpha,指向较少的lambda语音帧,这意味着接收到的信息足够稳定", "metrics": {"bleu_score": 43.661014030027665, "chrf_score": 36.21334575509234, "xcomet_score": 0.707883894443512, "xcomet_qe_score": 0.6591970920562744, "metricx_score": 6.417539596557617, "metricx_qe_score": 6.752885341644287, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",则发出一个词。例如,如果我们接收一个包含“我要谈论”的语音片段,我们的模型预测德语翻译,我们将查看交叉注意力权重,我们会看到前两个词指向最早接收到的语音帧,而最后一个词指向最后接收到的语音帧,即lambda语音帧。", "metrics": {"bleu_score": 48.82637586643863, "chrf_score": 36.32968346151048, "xcomet_score": 0.3546075224876404, "xcomet_qe_score": 0.23170368373394012, "metricx_score": 8.014608383178711, "metricx_qe_score": 8.151823043823242, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着前两个词将被发出,而由于交叉注意力的总和高于某个阈值alpha,我们将不发出最后一个词,而是等待另一个语音片段。", "metrics": {"bleu_score": 52.778730394644036, "chrf_score": 44.5359238968606, "xcomet_score": 0.5957568883895874, "xcomet_qe_score": 0.6507976055145264, "metricx_score": 3.1971359252929688, "metricx_qe_score": 3.4767088890075684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们继续接收另一个语音片段,我们的模型预测其他三个词,我们将查看交叉注意力权重,我们会看到没有词指向最后的lambda语音帧。", "metrics": {"bleu_score": 57.25409470071779, "chrf_score": 46.479770791363315, "xcomet_score": 0.6204540133476257, "xcomet_qe_score": 0.603061854839325, "metricx_score": 4.047005653381348, "metricx_qe_score": 4.046818256378174, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这意味着这三个词将被发出。", "metrics": {"bleu_score": 31.314224813827344, "chrf_score": 27.259129759129756, "xcomet_score": 0.9332944750785828, "xcomet_qe_score": 0.8744902610778809, "metricx_score": 1.6925324201583862, "metricx_qe_score": 3.3203859329223633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们查看主要结果的点,我们在图表上绘制了同步语音翻译结果,其中一边用蓝色测量翻译质量和平均延迟(即延迟度量),我们还考虑了考虑到模型计算时间的计算感知平均延迟,该延迟考虑了模型预测输出的计算时间。", "metrics": {"bleu_score": 29.110704908971492, "chrf_score": 24.053083310530976, "xcomet_score": 0.5817990303039551, "xcomet_qe_score": 0.46393096446990967, "metricx_score": 6.595660209655762, "metricx_qe_score": 6.516699314117432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,我们希望我们的曲线在这个图表上尽可能高,", "metrics": {"bleu_score": 26.943533707378236, "chrf_score": 25.542952540235508, "xcomet_score": 0.9618985652923584, "xcomet_qe_score": 0.8714417815208435, "metricx_score": 1.9536970853805542, "metricx_qe_score": 1.6682580709457397, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但也要向左移动。", "metrics": {"bleu_score": 26.563123324397914, "chrf_score": 24.71838403503466, "xcomet_score": 0.8753923177719116, "xcomet_qe_score": 0.8726035356521606, "metricx_score": 1.4369852542877197, "metricx_qe_score": 2.6139962673187256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与适用于离线模型的适当策略进行了比较,即湿键策略和局部一致性。", "metrics": {"bleu_score": 34.558838706262996, "chrf_score": 22.132170475068595, "xcomet_score": 0.7618696689605713, "xcomet_qe_score": 0.7144880294799805, "metricx_score": 6.706393718719482, "metricx_qe_score": 7.384451866149902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还与专门为同步语音翻译定制的最先进架构进行了比较。", "metrics": {"bleu_score": 71.40220219707982, "chrf_score": 68.79415279768001, "xcomet_score": 0.990941047668457, "xcomet_qe_score": 0.9746365547180176, "metricx_score": 1.1085693836212158, "metricx_qe_score": 1.6380327939987183, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是同步语音翻译策略在德语上的所有结果,", "metrics": {"bleu_score": 18.153580712996767, "chrf_score": 20.407190570281966, "xcomet_score": 0.8841100335121155, "xcomet_qe_score": 0.8779603838920593, "metricx_score": 2.1833953857421875, "metricx_qe_score": 1.2569353580474854, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到ADDOUT优于所有应用于离线模型的策略,因为曲线向左移动。", "metrics": {"bleu_score": 62.499592531264945, "chrf_score": 52.6131014285678, "xcomet_score": 0.8781110048294067, "xcomet_qe_score": 0.8250771760940552, "metricx_score": 3.670767068862915, "metricx_qe_score": 4.8606157302856445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还看到,如果我们考虑实际的经过时间或计算感知时间,ADAT是最快的策略。", "metrics": {"bleu_score": 42.54802555453608, "chrf_score": 37.23257574876955, "xcomet_score": 0.7630617618560791, "xcomet_qe_score": 0.7171035408973694, "metricx_score": 4.89884614944458, "metricx_qe_score": 5.144965648651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您想发现更多结果,请阅读我们的论文,", "metrics": {"bleu_score": 59.485907024027185, "chrf_score": 48.35884646256164, "xcomet_score": 0.9574462175369263, "xcomet_qe_score": 0.9469451308250427, "metricx_score": 0.6843385696411133, "metricx_qe_score": 0.48276737332344055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发布了开源代码和模型。如果您想发现更多结果,请阅读我们的论文。我们还发布了开源代码和模型,并进行了同步输出,以促进我们工作的可重复性。", "metrics": {"bleu_score": 19.478862039931478, "chrf_score": 32.37618959078692, "xcomet_score": 0.591944694519043, "xcomet_qe_score": 0.31895843148231506, "metricx_score": 2.9444403648376465, "metricx_qe_score": 2.99934983253479, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的关注。", "metrics": {"bleu_score": 7.809849842300637, "chrf_score": 7.407407407407408, "xcomet_score": 0.9552983045578003, "xcomet_qe_score": 1.0, "metricx_score": 0.6913450956344604, "metricx_qe_score": 0.710175633430481, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫 Ying,我的同事 Zhiyang 和我将为大家介绍我们关于多改进的研究,即通过指令微调改进多模型序列短学习。", "metrics": {"bleu_score": 34.56441366415231, "chrf_score": 36.377181695046986, "xcomet_score": 0.6146486401557922, "xcomet_qe_score": 0.6894813776016235, "metricx_score": 6.35916805267334, "metricx_qe_score": 6.340100288391113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着大型语言模型的进步,许多研究开始探索以参数和数据高效的方式,将预训练语言模型用于不同下游任务的新学习范式。", "metrics": {"bleu_score": 59.99540073773678, "chrf_score": 50.13325202351652, "xcomet_score": 0.9068543910980225, "xcomet_qe_score": 0.8913970589637756, "metricx_score": 1.682060718536377, "metricx_qe_score": 2.7545204162597656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近,许多研究表明,指令微调使大型语言模型能够以零样本方式在未见过的任务上表现出色,只需遵循自然指令即可。", "metrics": {"bleu_score": 42.30511538898979, "chrf_score": 39.87262922681486, "xcomet_score": 0.8298419713973999, "xcomet_qe_score": 0.7311822772026062, "metricx_score": 1.6927742958068848, "metricx_qe_score": 2.4319543838500977, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,大多数关于指令微调的先前工作都集中在提高零样本语言任务的性能上,而计算机视觉和多模态任务则被忽略了。", "metrics": {"bleu_score": 38.4496999907721, "chrf_score": 34.364816605799746, "xcomet_score": 0.9732582569122314, "xcomet_qe_score": 0.830595850944519, "metricx_score": 1.0648905038833618, "metricx_qe_score": 1.2124577760696411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们想要研究多模态预训练模型的指令微调是否真的可以提高对未见的多模态任务的泛化能力。", "metrics": {"bleu_score": 41.19946344262281, "chrf_score": 35.33756020138552, "xcomet_score": 0.8718996047973633, "xcomet_qe_score": 0.7982099056243896, "metricx_score": 1.6979035139083862, "metricx_qe_score": 1.8113279342651367, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,在我们进行研究时,我们发现 NLP 和多模态之间在指令数据集的可用性上存在显著差异。", "metrics": {"bleu_score": 46.83248351529544, "chrf_score": 39.97834731939181, "xcomet_score": 0.8830862045288086, "xcomet_qe_score": 0.8175802230834961, "metricx_score": 0.8199074864387512, "metricx_qe_score": 0.9440540075302124, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "存在超过 1,600 个仅语言的指令任务。", "metrics": {"bleu_score": 67.29864884660302, "chrf_score": 76.0638915598722, "xcomet_score": 0.8545893430709839, "xcomet_qe_score": 0.8207827806472778, "metricx_score": 1.362567663192749, "metricx_qe_score": 2.3364784717559814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,没有大规模公开的多模态指令任务。", "metrics": {"bleu_score": 63.09433236258316, "chrf_score": 53.068690926663066, "xcomet_score": 0.9746540784835815, "xcomet_qe_score": 0.826064944267273, "metricx_score": 1.468718409538269, "metricx_qe_score": 2.521465539932251, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这促使我们构建了一个多模态指令微调数据集。", "metrics": {"bleu_score": 47.8219647449668, "chrf_score": 39.87088787580623, "xcomet_score": 0.9769026041030884, "xcomet_qe_score": 0.9700860977172852, "metricx_score": 1.3343170881271362, "metricx_qe_score": 1.1246306896209717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们介绍了 MultiInstruct,这是第一个多模态指令微调基准数据集,包含 62 个多样化的多模态任务,涵盖 10 个主要类别。", "metrics": {"bleu_score": 45.42488567559425, "chrf_score": 48.0407140080428, "xcomet_score": 0.8396040201187134, "xcomet_qe_score": 0.8372572660446167, "metricx_score": 1.0454431772232056, "metricx_qe_score": 1.2293598651885986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些任务来自 21 个现有的开源数据集,每个任务都配备了 5 个专家撰写的指令。", "metrics": {"bleu_score": 57.363584368816106, "chrf_score": 51.572943076012855, "xcomet_score": 0.9781994819641113, "xcomet_qe_score": 0.93362957239151, "metricx_score": 1.1648582220077515, "metricx_qe_score": 1.9071526527404785, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了研究我们提出的数据集上的多模态指令微调,我们以 OFA 作为我们的基模型,OFA 使用统一的词汇表来", "metrics": {"bleu_score": 40.423356200185395, "chrf_score": 34.8627951610976, "xcomet_score": 0.43311867117881775, "xcomet_qe_score": 0.40636688470840454, "metricx_score": 7.149787425994873, "metricx_qe_score": 6.222532272338867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "表示语言、图像标记和边界框的坐标。", "metrics": {"bleu_score": 38.086578365309265, "chrf_score": 31.9023066966411, "xcomet_score": 0.2122298777103424, "xcomet_qe_score": 0.17214199900627136, "metricx_score": 7.503331661224365, "metricx_qe_score": 7.883671283721924, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们展示了我们多任务数据集的一些示例实例。为了统一处理各种输入和输出数据类型,", "metrics": {"bleu_score": 64.65746242222347, "chrf_score": 49.19668842253792, "xcomet_score": 0.8116865158081055, "xcomet_qe_score": 0.8523415327072144, "metricx_score": 2.4867019653320312, "metricx_qe_score": 2.544464111328125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们遵循 OFA 的方法,并将所有任务表述为统一的序列到序列格式,其中", "metrics": {"bleu_score": 58.90151271587919, "chrf_score": 59.017930234064885, "xcomet_score": 0.7280164361000061, "xcomet_qe_score": 0.7345489263534546, "metricx_score": 2.747070550918579, "metricx_qe_score": 2.0475106239318848, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "输入文本、图像、指令和边界框以相同的标记空间表示。", "metrics": {"bleu_score": 53.09565039223721, "chrf_score": 50.52264517853785, "xcomet_score": 0.9847351312637329, "xcomet_qe_score": 0.9566234350204468, "metricx_score": 1.0149985551834106, "metricx_qe_score": 1.017142415046692, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "好的,现在我要谈谈多模态指令微调。", "metrics": {"bleu_score": 67.53160327422972, "chrf_score": 62.99571751777634, "xcomet_score": 0.9133613109588623, "xcomet_qe_score": 0.883480429649353, "metricx_score": 0.721462607383728, "metricx_qe_score": 0.7804521322250366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于训练数据集,我们使用 9 个组中的 53 个任务进行训练,每个任务抽样 10,000 个实例。", "metrics": {"bleu_score": 71.0329668202063, "chrf_score": 67.55006350833577, "xcomet_score": 0.9868447780609131, "xcomet_qe_score": 0.9711196422576904, "metricx_score": 0.9491506218910217, "metricx_qe_score": 1.42880117893219, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于测试,我们保留整个常识推理组进行测试,并从 VQA 和杂项组中选择另外五个任务。", "metrics": {"bleu_score": 47.240055500550845, "chrf_score": 41.67915071059593, "xcomet_score": 0.6152467131614685, "xcomet_qe_score": 0.6024428009986877, "metricx_score": 4.011917591094971, "metricx_qe_score": 4.2515177726745605, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用每个任务测试集中的所有实例。", "metrics": {"bleu_score": 46.230595512422084, "chrf_score": 37.75727227912667, "xcomet_score": 0.8219863176345825, "xcomet_qe_score": 0.8099260330200195, "metricx_score": 1.5839729309082031, "metricx_qe_score": 1.6549862623214722, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,我们从自然指令测试集中的 20 个任务中随机抽样,作为 NLP 的未见任务。因此,", "metrics": {"bleu_score": 43.781692362160804, "chrf_score": 40.61334274621583, "xcomet_score": 0.600313663482666, "xcomet_qe_score": 0.5673522353172302, "metricx_score": 4.920903205871582, "metricx_qe_score": 4.709896087646484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用预训练的 OFA 大模型作为基模型。", "metrics": {"bleu_score": 71.03277321267436, "chrf_score": 66.49931553545505, "xcomet_score": 0.8836812973022461, "xcomet_qe_score": 0.8550267219543457, "metricx_score": 1.5454760789871216, "metricx_qe_score": 2.2901506423950195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们将所有任务的所有实例混合在一起。", "metrics": {"bleu_score": 55.925988689124864, "chrf_score": 53.884478712336644, "xcomet_score": 0.969638466835022, "xcomet_qe_score": 0.8911672830581665, "metricx_score": 0.8351970911026001, "metricx_qe_score": 1.3651412725448608, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "每个实例随机与其五个指令模板中的一个结合。", "metrics": {"bleu_score": 74.17090125042293, "chrf_score": 69.06413590609232, "xcomet_score": 0.8967591524124146, "xcomet_qe_score": 0.848268985748291, "metricx_score": 1.8565388917922974, "metricx_qe_score": 2.2801826000213623, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在每个任务的测试中,我们总共进行五个实验,每个实验使用五个指令", "metrics": {"bleu_score": 31.913055292785963, "chrf_score": 29.687507245153256, "xcomet_score": 0.8325660824775696, "xcomet_qe_score": 0.8068931102752686, "metricx_score": 4.229743480682373, "metricx_qe_score": 4.615842819213867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中的一个来评估模型。我们报告所有五个实验的平均和最大性能以及性能的标准差。", "metrics": {"bleu_score": 29.348711145815436, "chrf_score": 25.374608733942416, "xcomet_score": 0.3888395428657532, "xcomet_qe_score": 0.3225850462913513, "metricx_score": 4.3563618659973145, "metricx_qe_score": 4.411321640014648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果任务是多模型分类任务,我们报告准确率。", "metrics": {"bleu_score": 66.60502379419339, "chrf_score": 59.946768410414364, "xcomet_score": 0.9906585216522217, "xcomet_qe_score": 0.9783502221107483, "metricx_score": 0.5188239812850952, "metricx_qe_score": 0.6833564639091492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果是多模型生成任务,我们报告根 L。对于 RP 任务,我们报告根 jl,我们", "metrics": {"bleu_score": 32.20173434351673, "chrf_score": 23.458873214721425, "xcomet_score": 0.48250776529312134, "xcomet_qe_score": 0.44288238883018494, "metricx_score": 10.796346664428711, "metricx_qe_score": 9.449939727783203, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还引入了一个额外的评估指标,称为敏感性,它", "metrics": {"bleu_score": 54.764999378070456, "chrf_score": 52.78519107060229, "xcomet_score": 0.5682065486907959, "xcomet_qe_score": 0.46895331144332886, "metricx_score": 4.744679927825928, "metricx_qe_score": 1.0445588827133179, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "衡量模型在指令措辞略有变化的情况下,对同一任务始终产生相同输出的能力。", "metrics": {"bleu_score": 38.70703742086165, "chrf_score": 32.63744830199565, "xcomet_score": 0.9700098037719727, "xcomet_qe_score": 0.9796338081359863, "metricx_score": 2.3877429962158203, "metricx_qe_score": 3.2427663803100586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我们的主要结果。", "metrics": {"bleu_score": 79.6358031503278, "chrf_score": 77.3312769486561, "xcomet_score": 0.909784197807312, "xcomet_qe_score": 0.8688104748725891, "metricx_score": 0.38074302673339844, "metricx_qe_score": 0.5220726728439331, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,指令微调可以显著提高 OFA 在场景多模型任务上的性能。", "metrics": {"bleu_score": 30.002162025458677, "chrf_score": 32.3704458590678, "xcomet_score": 0.8903812170028687, "xcomet_qe_score": 0.8990197777748108, "metricx_score": 2.4224562644958496, "metricx_qe_score": 2.354621648788452, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,从自然指令数据集进行的迁移学习可以使指令微调受益。", "metrics": {"bleu_score": 54.75180966862229, "chrf_score": 49.34650152565327, "xcomet_score": 0.8984434604644775, "xcomet_qe_score": 0.8326890468597412, "metricx_score": 3.3736073970794678, "metricx_qe_score": 4.318597316741943, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这里,我们可以看到,随着任务数量的增加,模型的性能提高,同时敏感性降低。", "metrics": {"bleu_score": 44.30376053125508, "chrf_score": 37.78310499399392, "xcomet_score": 0.972399115562439, "xcomet_qe_score": 0.989975094795227, "metricx_score": 1.0731115341186523, "metricx_qe_score": 1.2239034175872803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还进行了一个实验,", "metrics": {"bleu_score": 46.92470064105599, "chrf_score": 39.39127110257559, "xcomet_score": 0.9774124622344971, "xcomet_qe_score": 0.9565337896347046, "metricx_score": 0.34012168645858765, "metricx_qe_score": 0.33167219161987305, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了一个指令与五个指令进行", "metrics": {"bleu_score": 21.874057156123218, "chrf_score": 20.724901152810087, "xcomet_score": 0.7679851055145264, "xcomet_qe_score": 0.694696843624115, "metricx_score": 3.5065038204193115, "metricx_qe_score": 3.6351699829101562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "比较。我们可以看到,使用更多指令可以提高模型的整体性能,并大大降低其敏感性。", "metrics": {"bleu_score": 41.09873320110075, "chrf_score": 35.814562300630406, "xcomet_score": 0.8035880923271179, "xcomet_qe_score": 0.8107911348342896, "metricx_score": 2.0138936042785645, "metricx_qe_score": 2.299680709838867, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明了不同的微调策略对模型敏感性的影响。", "metrics": {"bleu_score": 54.120475514419645, "chrf_score": 53.70772482752443, "xcomet_score": 0.975576639175415, "xcomet_qe_score": 0.9707900285720825, "metricx_score": 1.4315623044967651, "metricx_qe_score": 1.5186759233474731, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以看到,通过从自然指令数据集进行迁移学习,模型可以比原始 OFA 模型实现更好的敏感性。", "metrics": {"bleu_score": 36.83423291457702, "chrf_score": 33.7049669230774, "xcomet_score": 0.8718539476394653, "xcomet_qe_score": 0.7624138593673706, "metricx_score": 2.2010414600372314, "metricx_qe_score": 3.071963310241699, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还可以看到,从自然指令数据集进行的迁移学习可以帮助 OFA 在 Nitro Instruct 数据集上实现更好的性能。", "metrics": {"bleu_score": 65.56258397329938, "chrf_score": 57.00638183391331, "xcomet_score": 0.8779561519622803, "xcomet_qe_score": 0.7368919849395752, "metricx_score": 5.375315189361572, "metricx_qe_score": 6.342308521270752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,我们提出了第一个大规模多模型指令微调数据集。我们显著提高了 OFA 的零样本能力,并探索了不同的迁移学习技术并展示了它们的优势。", "metrics": {"bleu_score": 62.95445729333931, "chrf_score": 59.758299524210265, "xcomet_score": 0.831835150718689, "xcomet_qe_score": 0.7827747464179993, "metricx_score": 2.203961133956909, "metricx_qe_score": 2.8435683250427246, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们设计了一个名为敏感性的新指标。还有一点", "metrics": {"bleu_score": 34.17788378592367, "chrf_score": 31.36179566310729, "xcomet_score": 0.5314184427261353, "xcomet_qe_score": 0.2848653793334961, "metricx_score": 4.646664619445801, "metricx_qe_score": 1.8780032396316528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们正在收集一个更大的多模态指令微调数据集,包含大约 150 个额外的变体语言任务,我们很快就会发布它们。", "metrics": {"bleu_score": 46.95016678756111, "chrf_score": 44.128830349017356, "xcomet_score": 0.6864434480667114, "xcomet_qe_score": 0.7257586121559143, "metricx_score": 3.8324780464172363, "metricx_qe_score": 4.058512210845947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是一个用于我们数据和模型的二维码。", "metrics": {"bleu_score": 58.53882755435387, "chrf_score": 63.06810367882123, "xcomet_score": 0.9534118175506592, "xcomet_qe_score": 0.9356686472892761, "metricx_score": 0.8054179549217224, "metricx_qe_score": 0.883053719997406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9333682656288147, "xcomet_qe_score": 0.9679166674613953, "metricx_score": 0.46964308619499207, "metricx_qe_score": 0.171317458152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,", "metrics": {"bleu_score": 59.460355750136046, "chrf_score": 47.91666666666667, "xcomet_score": 0.9850732088088989, "xcomet_qe_score": 0.9742759466171265, "metricx_score": 0.0, "metricx_qe_score": 0.004066057503223419, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我是科斯塔夫·辛哈,很高兴欢迎大家来到我们的ACL 2023论文讨论会,论文题目是", "metrics": {"bleu_score": 27.850162207652026, "chrf_score": 32.49465405939313, "xcomet_score": 0.7374182939529419, "xcomet_qe_score": 0.6802366971969604, "metricx_score": 3.8264284133911133, "metricx_qe_score": 3.8234047889709473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《语言模型的可接受性判断并非总是对上下文鲁棒》。这是一项与", "metrics": {"bleu_score": 54.710720101471345, "chrf_score": 52.11637933218989, "xcomet_score": 0.5838683843612671, "xcomet_qe_score": 0.5391378402709961, "metricx_score": 8.596449851989746, "metricx_qe_score": 6.812460899353027, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "约翰·高蒂埃、亚伦·穆勒、卡尼什卡·米什拉、卡伦·富恩特斯、罗杰·莱维和阿迪娜·威廉姆斯合作完成的联合研究。", "metrics": {"bleu_score": 1.9621280362601403, "chrf_score": 2.0755803976367333, "xcomet_score": 0.5153661966323853, "xcomet_qe_score": 0.6399784088134766, "metricx_score": 2.912794589996338, "metricx_qe_score": 2.685161590576172, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在这项工作中,我们重新审视了最小对概念。", "metrics": {"bleu_score": 43.82572041773569, "chrf_score": 44.99510575847716, "xcomet_score": 0.963120698928833, "xcomet_qe_score": 0.969498872756958, "metricx_score": 0.9562066793441772, "metricx_qe_score": 0.9808343648910522, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最小对概念基本上是通过可接受性判断来评估语言模型,其中还", "metrics": {"bleu_score": 39.506153747892526, "chrf_score": 31.167589790778194, "xcomet_score": 0.744373083114624, "xcomet_qe_score": 0.6505634784698486, "metricx_score": 6.502191543579102, "metricx_qe_score": 1.995405912399292, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可能包括语法性,如blimp、语义、gym,或从刻板印象角度来看的可接受性,如Krauss对。", "metrics": {"bleu_score": 21.879903490242977, "chrf_score": 15.189184113993548, "xcomet_score": 0.4679831862449646, "xcomet_qe_score": 0.41113582253456116, "metricx_score": 8.010340690612793, "metricx_qe_score": 8.612809181213379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最小对概念中,评估语言模型的典型方法是展示一个可接受的句子或语法句子,然后展示一个不可接受的句子或非语法句子。", "metrics": {"bleu_score": 47.122671087809266, "chrf_score": 43.82177272421964, "xcomet_score": 0.8368954658508301, "xcomet_qe_score": 0.7962435483932495, "metricx_score": 1.4351396560668945, "metricx_qe_score": 2.787139654159546, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,模型的基本过程基本上会给可接受的句子赋予更高的概率。", "metrics": {"bleu_score": 25.18966472837831, "chrf_score": 25.900535775114342, "xcomet_score": 0.8700886368751526, "xcomet_qe_score": 0.7952901124954224, "metricx_score": 3.4010374546051025, "metricx_qe_score": 3.6994097232818604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当前的MPP管道基本上不允许我们评估模型对更长句子的可接受性。", "metrics": {"bleu_score": 75.45297228891206, "chrf_score": 74.60099189130814, "xcomet_score": 0.8558188676834106, "xcomet_qe_score": 0.7746543884277344, "metricx_score": 1.4040659666061401, "metricx_qe_score": 2.9087510108947754, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如今,大型语言模型正在采用越来越长的", "metrics": {"bleu_score": 29.25712720837, "chrf_score": 23.270604349424808, "xcomet_score": 0.7670841813087463, "xcomet_qe_score": 0.8044006824493408, "metricx_score": 5.178421497344971, "metricx_qe_score": 4.081172943115234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "上下文窗口。因此,我们", "metrics": {"bleu_score": 3.0196376639848626, "chrf_score": 10.803286220736455, "xcomet_score": 0.15928994119167328, "xcomet_qe_score": 0.1393934190273285, "metricx_score": 19.247161865234375, "metricx_qe_score": 18.906314849853516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "重新审视了", "metrics": {"bleu_score": 0.24729105095220405, "chrf_score": 7.165318851431647, "xcomet_score": 0.1380453109741211, "xcomet_qe_score": 0.13593308627605438, "metricx_score": 13.762127876281738, "metricx_qe_score": 20.18882942199707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.43476197123527527, "xcomet_qe_score": 0.15987929701805115, "metricx_score": 2.4095587730407715, "metricx_qe_score": 5.211564064025879, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "数据集本身,然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。", "metrics": {"bleu_score": 50.87550566014142, "chrf_score": 56.00997666517845, "xcomet_score": 0.32480186223983765, "xcomet_qe_score": 0.2397773563861847, "metricx_score": 6.26875114440918, "metricx_qe_score": 9.161666870117188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,这里我们从BLIMP数据集的Adjunct Island案例中选择了典型的一对语法性句子。", "metrics": {"bleu_score": 33.61230151076893, "chrf_score": 49.36772397263331, "xcomet_score": 0.7837463021278381, "xcomet_qe_score": 0.8024981021881104, "metricx_score": 2.794196367263794, "metricx_qe_score": 3.652836561203003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们所做的是重新创建更长的序列,这些序列是可接受的,并且具有相同的语法结构匹配,", "metrics": {"bleu_score": 77.73714308042847, "chrf_score": 69.83866991698754, "xcomet_score": 0.950576663017273, "xcomet_qe_score": 0.8663231730461121, "metricx_score": 1.4796338081359863, "metricx_qe_score": 2.029397487640381, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从Adjunct Island中提取语法句子,然后将其作为前缀添加到可接受查询和不可接受查询中。因此,", "metrics": {"bleu_score": 77.68557860134777, "chrf_score": 77.99270377115391, "xcomet_score": 0.749468207359314, "xcomet_qe_score": 0.6767995953559875, "metricx_score": 4.093995094299316, "metricx_qe_score": 4.63320779800415, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情,这也可以用来测试模型的可接受性。", "metrics": {"bleu_score": 94.27781070492712, "chrf_score": 91.7870378110458, "xcomet_score": 0.9529941082000732, "xcomet_qe_score": 0.7428854703903198, "metricx_score": 1.5263457298278809, "metricx_qe_score": 1.8271386623382568, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以通过从不同的子集或不同的数据集选择句子来做同样的事情。", "metrics": {"bleu_score": 83.07278725457043, "chrf_score": 79.33022891685252, "xcomet_score": 0.9753702878952026, "xcomet_qe_score": 0.8853784799575806, "metricx_score": 0.7516820430755615, "metricx_qe_score": 1.2882355451583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是我们所谓的不匹配场景。因此,", "metrics": {"bleu_score": 63.019085559238604, "chrf_score": 63.36226407659189, "xcomet_score": 0.8487459421157837, "xcomet_qe_score": 0.7765777111053467, "metricx_score": 2.140721082687378, "metricx_qe_score": 3.2538084983825684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的句子仍然来自相关的数据集,但不是你正在评估的同一数据集。", "metrics": {"bleu_score": 56.44576924507066, "chrf_score": 49.05627544423903, "xcomet_score": 0.959088921546936, "xcomet_qe_score": 0.8277429342269897, "metricx_score": 1.0910775661468506, "metricx_qe_score": 2.1170268058776855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们也可以对可接受性案例做同样的事情。", "metrics": {"bleu_score": 21.341398311213545, "chrf_score": 20.914459225127377, "xcomet_score": 0.807407021522522, "xcomet_qe_score": 0.7643491625785828, "metricx_score": 3.832324981689453, "metricx_qe_score": 2.98421049118042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们可以从完全不相关的领域选择句子,例如维基百科。", "metrics": {"bleu_score": 65.4468923560319, "chrf_score": 57.12890537741404, "xcomet_score": 0.9937243461608887, "xcomet_qe_score": 0.9375579953193665, "metricx_score": 0.6774861812591553, "metricx_qe_score": 1.5228242874145508, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这将告诉我们,模型的可接受性判断是否实际上受到任何上下文的影响,上下文与我们正在查看的句子完全无关。", "metrics": {"bleu_score": 44.927283256663664, "chrf_score": 44.5736950528997, "xcomet_score": 0.7755746245384216, "xcomet_qe_score": 0.7550809383392334, "metricx_score": 4.11786413192749, "metricx_qe_score": 5.8441925048828125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么模型的表现如何呢?", "metrics": {"bleu_score": 9.78090152232118, "chrf_score": 10.810141839392179, "xcomet_score": 0.8777414560317993, "xcomet_qe_score": 0.858100175857544, "metricx_score": 1.0029296875, "metricx_qe_score": 0.2817142903804779, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看维基百科句子,这些句子与当前查询对完全无关。我们发现,MPP判断对于任意上下文长度大多是鲁棒的。", "metrics": {"bleu_score": 50.55144646768306, "chrf_score": 43.90120899447676, "xcomet_score": 0.8003979325294495, "xcomet_qe_score": 0.7394296526908875, "metricx_score": 5.714813232421875, "metricx_qe_score": 6.772538185119629, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将上下文长度增加到1024,以最大化OPT和GPT-2模型。我们在这里看到,", "metrics": {"bleu_score": 54.787480840451266, "chrf_score": 74.13860302184516, "xcomet_score": 0.7745445370674133, "xcomet_qe_score": 0.6815997362136841, "metricx_score": 3.7705838680267334, "metricx_qe_score": 3.6261205673217773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在橙色虚线中,MPP判断相对稳定。", "metrics": {"bleu_score": 53.01646310382839, "chrf_score": 52.88913920636522, "xcomet_score": 0.8659971952438354, "xcomet_qe_score": 0.8328984975814819, "metricx_score": 1.9674712419509888, "metricx_qe_score": 3.6754708290100098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,当我们选择来自同一数据集的句子时会发生什么?", "metrics": {"bleu_score": 41.072675483179786, "chrf_score": 35.592916745090655, "xcomet_score": 0.9963353872299194, "xcomet_qe_score": 0.9840292930603027, "metricx_score": 0.7365055680274963, "metricx_qe_score": 1.3787598609924316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里,我们从相同的blimp或语义宝石数据集的可接受和不可接受领域选择或创建句子。", "metrics": {"bleu_score": 53.608489074623385, "chrf_score": 36.23130320265501, "xcomet_score": 0.6913377046585083, "xcomet_qe_score": 0.686008095741272, "metricx_score": 4.742808818817139, "metricx_qe_score": 4.783027648925781, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们看到,当添加可接受前缀或不可接受前缀时,MPP判断要么显著增加,要么显著减少。但是,当我们匹配结构时,即当我们添加可接受前缀或不可接受前缀时。", "metrics": {"bleu_score": 21.99730936318243, "chrf_score": 29.713913406718532, "xcomet_score": 0.5464765429496765, "xcomet_qe_score": 0.6453319787979126, "metricx_score": 9.49149227142334, "metricx_qe_score": 7.7005391120910645, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,当我们匹配结构时,即当我们从blame-person-text-gym中的相同现象中选择句子时,我们看到模型的MPP判断有显著增加或显著减少,具体取决于所选择的前缀是可接受还是不可接受。", "metrics": {"bleu_score": 56.2156809836965, "chrf_score": 45.2466657062774, "xcomet_score": 0.5699101686477661, "xcomet_qe_score": 0.6232469081878662, "metricx_score": 7.293639659881592, "metricx_qe_score": 7.887657165527344, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,这个效果随着上下文长度的增加而增加,这可能会影响到具有大上下文窗口的较新的语言模型。", "metrics": {"bleu_score": 50.18286518491271, "chrf_score": 50.86784764863352, "xcomet_score": 0.8542455434799194, "xcomet_qe_score": 0.7804429531097412, "metricx_score": 1.559954285621643, "metricx_qe_score": 1.7077085971832275, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么匹配前缀会如此大地影响语言模型的判断呢?", "metrics": {"bleu_score": 52.188656617142755, "chrf_score": 45.662628368570374, "xcomet_score": 0.9967131614685059, "xcomet_qe_score": 0.934635579586029, "metricx_score": 0.6645628213882446, "metricx_qe_score": 0.7742565274238586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们进行了一系列分析,试图通过保留相关结构来展示输入句子,但向输入中添加噪声。", "metrics": {"bleu_score": 66.39977895828203, "chrf_score": 59.06982263793409, "xcomet_score": 0.8762804269790649, "xcomet_qe_score": 0.7984393835067749, "metricx_score": 2.59597110748291, "metricx_qe_score": 3.058806896209717, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "经过几次这样的扰动,我们发现这些噪声实际上并没有使模型在显示MPP判断趋势方面改变其方向。", "metrics": {"bleu_score": 47.41585973642451, "chrf_score": 42.410700894247185, "xcomet_score": 0.8707382678985596, "xcomet_qe_score": 0.862485945224762, "metricx_score": 3.347111940383911, "metricx_qe_score": 3.5849504470825195, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基本上,我们发现模型以相似的方式对扰动句子敏感。", "metrics": {"bleu_score": 18.97055853601792, "chrf_score": 20.713253589069144, "xcomet_score": 0.8480871915817261, "xcomet_qe_score": 0.8540021181106567, "metricx_score": 2.266566514968872, "metricx_qe_score": 3.477599620819092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "也就是说,当我们在可接受领域中扰动句子时,我们看到所有扰动中的相似增加。当我们在不可接受领域中扰动句子时,我们看到MPP判断减少。我们看到所有扰动中的相似增加。当我们在不可接受领域中扰动句子时,我们以相似的方式看到MPP判断减少。", "metrics": {"bleu_score": 31.935727009632046, "chrf_score": 37.17563820298345, "xcomet_score": 0.21583178639411926, "xcomet_qe_score": 0.2167186737060547, "metricx_score": 15.132556915283203, "metricx_qe_score": 14.919145584106445, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们工作的关键结论是,语言模型对句子中共享的潜在句法和语义特征敏感。", "metrics": {"bleu_score": 62.31144761876193, "chrf_score": 54.350104376451334, "xcomet_score": 0.916426420211792, "xcomet_qe_score": 0.9328593015670776, "metricx_score": 1.1966193914413452, "metricx_qe_score": 1.4394019842147827, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们目前通过短句和单句输入进行的MPP评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。", "metrics": {"bleu_score": 59.044764442011626, "chrf_score": 49.48326415948511, "xcomet_score": 0.9443588256835938, "xcomet_qe_score": 0.8750393390655518, "metricx_score": 1.9197827577590942, "metricx_qe_score": 2.4160754680633545, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请阅读我们的论文以获取我们实验的更多详细信息。", "metrics": {"bleu_score": 71.9486981916948, "chrf_score": 67.07656051381463, "xcomet_score": 0.99744713306427, "xcomet_qe_score": 0.9992866516113281, "metricx_score": 0.24227701127529144, "metricx_qe_score": 0.2289969027042389, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 35.49481056010054, "chrf_score": 30.038060761124484, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.3156697750091553, "metricx_qe_score": 0.6318578124046326, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自宾夕法尼亚州立大学的张宇胜。", "metrics": {"bleu_score": 68.48075777090853, "chrf_score": 49.88704001434205, "xcomet_score": 0.9199157953262329, "xcomet_qe_score": 0.8629245758056641, "metricx_score": 0.5634078979492188, "metricx_qe_score": 0.9624079465866089, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我要介绍的是我们的工作,跨语言多自然语言语义解析与意义表示。", "metrics": {"bleu_score": 32.56382815529081, "chrf_score": 24.844502165882766, "xcomet_score": 0.8184035420417786, "xcomet_qe_score": 0.8281211853027344, "metricx_score": 4.6590423583984375, "metricx_qe_score": 5.2322187423706055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语义解析是构建用户查询语义表示的任务,如SQL和λ演算。", "metrics": {"bleu_score": 66.7619194068951, "chrf_score": 43.99689865699681, "xcomet_score": 0.8933101892471313, "xcomet_qe_score": 0.8821356892585754, "metricx_score": 0.9189969301223755, "metricx_qe_score": 1.485090970993042, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "跨语言语义解析是将多自然语言查询翻译成多意义表示的任务。", "metrics": {"bleu_score": 63.30876089763879, "chrf_score": 53.537922434678066, "xcomet_score": 0.782983124256134, "xcomet_qe_score": 0.8101975917816162, "metricx_score": 1.335263729095459, "metricx_qe_score": 3.735607147216797, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,我们需要使用神经模型将多自然语言查询翻译成SQL、λ演算或FunQL等。", "metrics": {"bleu_score": 68.13430964678759, "chrf_score": 57.76429725899127, "xcomet_score": 0.7922748327255249, "xcomet_qe_score": 0.7340507507324219, "metricx_score": 1.2955644130706787, "metricx_qe_score": 1.6672091484069824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有的跨语言语义解析模型分别针对有限任务和应用的数据集提出和评估。", "metrics": {"bleu_score": 63.9740736433283, "chrf_score": 57.46819134821285, "xcomet_score": 0.927814245223999, "xcomet_qe_score": 0.8514490127563477, "metricx_score": 0.5284484624862671, "metricx_qe_score": 0.9402812719345093, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,某些自然语言的覆盖范围存在漏洞。", "metrics": {"bleu_score": 46.99152171992906, "chrf_score": 49.37297489707343, "xcomet_score": 0.741040825843811, "xcomet_qe_score": 0.6190048456192017, "metricx_score": 3.2823007106781006, "metricx_qe_score": 4.22387170791626, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "中文缺失,由于某些微表示的覆盖范围", "metrics": {"bleu_score": 6.3438016104279455, "chrf_score": 8.849254639488898, "xcomet_score": 0.63489830493927, "xcomet_qe_score": 0.5373815298080444, "metricx_score": 6.3329572677612305, "metricx_qe_score": 5.799901485443115, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",λ演算缺失,或者它们只在某些神经模型上进行评估。", "metrics": {"bleu_score": 53.19499812335497, "chrf_score": 46.44017731821571, "xcomet_score": 0.8291668891906738, "xcomet_qe_score": 0.7805027961730957, "metricx_score": 2.6220216751098633, "metricx_qe_score": 3.604422092437744, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,只有一个模型进行评估。因此,", "metrics": {"bleu_score": 20.672818841772642, "chrf_score": 19.487870862170414, "xcomet_score": 0.8745871186256409, "xcomet_qe_score": 0.8531485199928284, "metricx_score": 3.13332200050354, "metricx_qe_score": 1.9337141513824463, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为此目的,我们提出了Exampler。", "metrics": {"bleu_score": 37.70063804549471, "chrf_score": 22.58136135418597, "xcomet_score": 0.8410430550575256, "xcomet_qe_score": 0.8412242531776428, "metricx_score": 2.463447332382202, "metricx_qe_score": 3.5580570697784424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们为跨语言多自然语言语义解析与意义表示提供了一个统一的数据集Exampler。它包含九个来自", "metrics": {"bleu_score": 49.35546316242143, "chrf_score": 38.81908945094313, "xcomet_score": 0.5618456602096558, "xcomet_qe_score": 0.42755210399627686, "metricx_score": 9.697492599487305, "metricx_qe_score": 5.030101299285889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "各个领域的语料库,五个语义解析任务,八种意义表示,以及15个语系中的22种自然语言。", "metrics": {"bleu_score": 51.913492470424636, "chrf_score": 48.40440098321925, "xcomet_score": 0.6244423389434814, "xcomet_qe_score": 0.7051169872283936, "metricx_score": 3.6557393074035645, "metricx_qe_score": 4.062684535980225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了更好地评估我们的基准测试,我们考虑了六种训练和评估设置。", "metrics": {"bleu_score": 74.56495553738948, "chrf_score": 70.42746348716106, "xcomet_score": 0.9834190607070923, "xcomet_qe_score": 0.9601007699966431, "metricx_score": 0.9758420586585999, "metricx_qe_score": 1.3702785968780518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一种是翻译测试。", "metrics": {"bleu_score": 66.06328636027612, "chrf_score": 56.005291005291, "xcomet_score": 0.9600571393966675, "xcomet_qe_score": 0.9592820405960083, "metricx_score": 0.29587188363075256, "metricx_qe_score": 0.4503270089626312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用Google翻译API将源语言翻译成目标语言,然后使用单语模型进行训练和评估。", "metrics": {"bleu_score": 82.66660014007991, "chrf_score": 76.41626540305097, "xcomet_score": 0.9510485529899597, "xcomet_qe_score": 0.8472477197647095, "metricx_score": 0.5243576169013977, "metricx_qe_score": 0.4862639307975769, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们用英语查询训练英语模型。在推理过程中,我们使用API将德语查询翻译成英语,然后使用训练好的模型预测SQL。", "metrics": {"bleu_score": 67.76817687968963, "chrf_score": 61.643862975799145, "xcomet_score": 0.9275572299957275, "xcomet_qe_score": 0.8981345891952515, "metricx_score": 0.9772142767906189, "metricx_qe_score": 1.5246821641921997, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还测试了单语模型。", "metrics": {"bleu_score": 51.56626918239823, "chrf_score": 40.77320827320827, "xcomet_score": 0.8812164068222046, "xcomet_qe_score": 0.84535151720047, "metricx_score": 1.4693257808685303, "metricx_qe_score": 1.2090201377868652, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种设置中,源语言与目标语言相同。例如,德语到德语或英语到英语。", "metrics": {"bleu_score": 72.68097337162342, "chrf_score": 66.44665575455079, "xcomet_score": 0.9278980493545532, "xcomet_qe_score": 0.8964245319366455, "metricx_score": 0.5449566841125488, "metricx_qe_score": 0.6620081663131714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过训练单语模型,只使用10%的训练数据,测试了单语少样本设置。我们", "metrics": {"bleu_score": 51.1828502525789, "chrf_score": 44.90858884224263, "xcomet_score": 0.7270676493644714, "xcomet_qe_score": 0.7109748125076294, "metricx_score": 4.198403358459473, "metricx_qe_score": 1.8661577701568604, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "还测试了多语言模型,我们为所有语言训练了一个多语言模型。", "metrics": {"bleu_score": 61.48406922119201, "chrf_score": 56.715524865621106, "xcomet_score": 0.9107121229171753, "xcomet_qe_score": 0.8980938196182251, "metricx_score": 1.0547901391983032, "metricx_qe_score": 1.6255671977996826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们将德语、英语、中文查询一起训练一个多语言模型。", "metrics": {"bleu_score": 71.80971299357819, "chrf_score": 67.33242479149561, "xcomet_score": 0.8800930976867676, "xcomet_qe_score": 0.8963912725448608, "metricx_score": 1.8211448192596436, "metricx_qe_score": 3.2616477012634277, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在推理过程中,我们可以使用这个模型翻译德语查询或中文查询等。", "metrics": {"bleu_score": 82.42245567123457, "chrf_score": 75.15074104990397, "xcomet_score": 0.9756777286529541, "xcomet_qe_score": 0.8953857421875, "metricx_score": 0.7203695178031921, "metricx_qe_score": 1.1883642673492432, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还考虑了跨语言零样本和少样本迁移。", "metrics": {"bleu_score": 84.04350178700108, "chrf_score": 82.4968978819598, "xcomet_score": 0.8327165842056274, "xcomet_qe_score": 0.8038347363471985, "metricx_score": 2.5244078636169434, "metricx_qe_score": 2.7449681758880615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在一个源语言上进行训练,然后迁移到另一种语言。因此,", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 35.759756944180864, "xcomet_score": 0.8009227514266968, "xcomet_qe_score": 0.7206591367721558, "metricx_score": 5.136651515960693, "metricx_qe_score": 5.501239776611328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在训练过程中,我们用英语查询或英语和德语少样本查询的组合训练一个多语言模型,并预测SQL输出。", "metrics": {"bleu_score": 61.5544949788899, "chrf_score": 53.46736305204992, "xcomet_score": 0.9361400604248047, "xcomet_qe_score": 0.8498753309249878, "metricx_score": 1.3960334062576294, "metricx_qe_score": 2.194061756134033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了许多有趣的成果。因此", "metrics": {"bleu_score": 26.96698152099015, "chrf_score": 26.932617513716504, "xcomet_score": 0.8180636763572693, "xcomet_qe_score": 0.7937408685684204, "metricx_score": 3.2309324741363525, "metricx_qe_score": 1.9735608100891113, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",关于单语模型的分析,我们在两组模型上进行评估,包括编码器PDR,即多语言预训练编码器加基于指针的解码器,如XLMR加PDR和BERT加PDR。", "metrics": {"bleu_score": 44.22959064542926, "chrf_score": 35.32940316566426, "xcomet_score": 0.6244663000106812, "xcomet_qe_score": 0.6792864799499512, "metricx_score": 4.759112358093262, "metricx_qe_score": 4.108323097229004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还评估了编码器-解码器模型,即多语言预训练编码器-解码器模型,如MBART和MT5。", "metrics": {"bleu_score": 31.009124728696612, "chrf_score": 22.207379665592036, "xcomet_score": 0.8796030282974243, "xcomet_qe_score": 0.9094024896621704, "metricx_score": 1.1651747226715088, "metricx_qe_score": 1.7302885055541992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现编码器-解码器在所有九个数据集上获得最佳性能。", "metrics": {"bleu_score": 43.2530772707211, "chrf_score": 27.852744376405337, "xcomet_score": 0.9815789461135864, "xcomet_qe_score": 0.9680585861206055, "metricx_score": 1.4035412073135376, "metricx_qe_score": 1.4570951461791992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在多语言设置上对MT5和XLMR加PDR进行了评估。", "metrics": {"bleu_score": 33.86854985606571, "chrf_score": 36.991806904850385, "xcomet_score": 0.8569003343582153, "xcomet_qe_score": 0.8726661205291748, "metricx_score": 2.452767848968506, "metricx_qe_score": 2.6297547817230225, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过在各种语言的混合中进行训练,可以改进编码器-解码器或编码器-PDR。", "metrics": {"bleu_score": 27.31091227483814, "chrf_score": 19.346799481679003, "xcomet_score": 0.6590242385864258, "xcomet_qe_score": 0.6876006126403809, "metricx_score": 2.7714335918426514, "metricx_qe_score": 3.9838695526123047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现这是因为大多数主要自然语言都可以获得性能提升,除了英语在七个数据集上性能下降,只在三个数据集上有所提升。", "metrics": {"bleu_score": 58.86985913478882, "chrf_score": 52.14382664230899, "xcomet_score": 0.8926421999931335, "xcomet_qe_score": 0.9531542062759399, "metricx_score": 2.517625093460083, "metricx_qe_score": 1.6232023239135742, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我认为这被称为多语言的诅咒。", "metrics": {"bleu_score": 35.47202175617096, "chrf_score": 30.880643040199136, "xcomet_score": 0.9179631471633911, "xcomet_qe_score": 0.8739200234413147, "metricx_score": 1.051566481590271, "metricx_qe_score": 1.5422992706298828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了跨语言性能差距。", "metrics": {"bleu_score": 65.15132562023375, "chrf_score": 59.00209468789821, "xcomet_score": 0.9033793210983276, "xcomet_qe_score": 0.8933225274085999, "metricx_score": 1.7487887144088745, "metricx_qe_score": 2.4259886741638184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,蓝色线是跨语言少样本迁移,", "metrics": {"bleu_score": 53.569769608593234, "chrf_score": 49.59886518710048, "xcomet_score": 0.8529565334320068, "xcomet_qe_score": 0.8062290549278259, "metricx_score": 2.158867835998535, "metricx_qe_score": 3.466238260269165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "橙色线是跨语言零样本迁移,", "metrics": {"bleu_score": 75.39221180326287, "chrf_score": 77.94676400607176, "xcomet_score": 0.9476479291915894, "xcomet_qe_score": 0.8390147089958191, "metricx_score": 1.9790170192718506, "metricx_qe_score": 3.080129384994507, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "绿色线是单语设置。我们发现", "metrics": {"bleu_score": 48.63383168079942, "chrf_score": 69.05349133758901, "xcomet_score": 0.8378332853317261, "xcomet_qe_score": 0.8217518329620361, "metricx_score": 2.8535871505737305, "metricx_qe_score": 2.0322587490081787, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",通过比较绿色和橙色线,我们发现对于零样本设置,跨语言迁移性能差距显著。通过比较蓝色和橙色线,我们发现对于少样本设置,迁移差距迅速缩小。", "metrics": {"bleu_score": 42.09366572770893, "chrf_score": 34.36872061644563, "xcomet_score": 0.7039598226547241, "xcomet_qe_score": 0.6162453889846802, "metricx_score": 3.1842660903930664, "metricx_qe_score": 4.030642032623291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现了一些其他有趣的发现。", "metrics": {"bleu_score": 44.77118844014732, "chrf_score": 42.32732029275419, "xcomet_score": 0.9799755811691284, "xcomet_qe_score": 0.958720326423645, "metricx_score": 0.3158206045627594, "metricx_qe_score": 0.8141187429428101, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,编码器-解码器优于之前的工作或取得了可比拟的结果。", "metrics": {"bleu_score": 13.083737883508867, "chrf_score": 9.912796637571617, "xcomet_score": 0.9077446460723877, "xcomet_qe_score": 0.9086830019950867, "metricx_score": 1.828259825706482, "metricx_qe_score": 2.199671506881714, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在英语自然语言上的表现可以显著提升目标自然语言上的少样本表现。我们发现多语言模型如CODIS和BLUE仍然适用于跨语言语义解析任务。", "metrics": {"bleu_score": 52.74809928561438, "chrf_score": 42.44068742887991, "xcomet_score": 0.5196103453636169, "xcomet_qe_score": 0.42168742418289185, "metricx_score": 10.826155662536621, "metricx_qe_score": 12.555477142333984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结起来,我们构建了Exampler,这是一个统一的跨语言语义解析基准测试,包含多自然语言和主要表示。", "metrics": {"bleu_score": 40.40092991904436, "chrf_score": 29.864030938419866, "xcomet_score": 0.6778451800346375, "xcomet_qe_score": 0.6184468269348145, "metricx_score": 4.690586090087891, "metricx_qe_score": 4.994133472442627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对三种代表性的多语言模型进行了全面的基准测试研究。", "metrics": {"bleu_score": 75.49399421360621, "chrf_score": 67.98186790513903, "xcomet_score": 0.9610769748687744, "xcomet_qe_score": 0.9540224075317383, "metricx_score": 0.8421860933303833, "metricx_qe_score": 1.073890209197998, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的结果显示了许多有趣的发现等", "metrics": {"bleu_score": 93.06048591020995, "chrf_score": 92.47065434565434, "xcomet_score": 0.8565675020217896, "xcomet_qe_score": 0.7905972003936768, "metricx_score": 1.7107927799224854, "metricx_qe_score": 1.3963158130645752, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "。", "metrics": {"bleu_score": 0.0, "chrf_score": 17.241379310344822, "xcomet_score": 0.41044604778289795, "xcomet_qe_score": 0.12948493659496307, "metricx_score": 4.254793643951416, "metricx_qe_score": 5.784850120544434, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎访问我们的论文和代码。", "metrics": {"bleu_score": 70.16035864257111, "chrf_score": 64.8012173012173, "xcomet_score": 0.9862284660339355, "xcomet_qe_score": 0.9691290855407715, "metricx_score": 0.43438172340393066, "metricx_qe_score": 0.6480231285095215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.8839613199234009, "xcomet_qe_score": 0.8593108057975769, "metricx_score": 0.1393444538116455, "metricx_qe_score": 0.2242259979248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我叫大卫·维拉尔,我将简要介绍一篇论文《从翻译中学习参数提示:评估策略和性能》。", "metrics": {"bleu_score": 20.36143756522424, "chrf_score": 18.249281771487695, "xcomet_score": 0.8075698018074036, "xcomet_qe_score": 0.7422733306884766, "metricx_score": 4.2269744873046875, "metricx_qe_score": 3.7238242626190186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和谷歌翻译同事的合作成果。", "metrics": {"bleu_score": 20.570885115591267, "chrf_score": 20.732468773163916, "xcomet_score": 0.9982795715332031, "xcomet_qe_score": 0.9951926469802856, "metricx_score": 0.7620185017585754, "metricx_qe_score": 0.350193053483963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Parm 是一款去年(2022 年)推出的参数量达 5400 亿的参数大型语言模型。", "metrics": {"bleu_score": 19.731275035382392, "chrf_score": 29.90098020078754, "xcomet_score": 0.8391814231872559, "xcomet_qe_score": 0.767873227596283, "metricx_score": 5.017900466918945, "metricx_qe_score": 5.243153095245361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它在包含 7800 亿个标记的庞大文本集合上进行了训练。", "metrics": {"bleu_score": 30.830129955021516, "chrf_score": 38.95985147087767, "xcomet_score": 0.8186453580856323, "xcomet_qe_score": 0.7487839460372925, "metricx_score": 1.435912013053894, "metricx_qe_score": 1.8648241758346558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在发布时,它在数百个 NLP 任务中取得了最先进的水平。", "metrics": {"bleu_score": 42.66219662386316, "chrf_score": 44.50155887396281, "xcomet_score": 0.9898490905761719, "xcomet_qe_score": 0.9023390412330627, "metricx_score": 1.1338517665863037, "metricx_qe_score": 1.8955477476119995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了对机器翻译进行大型语言模型提示的首次系统研究。", "metrics": {"bleu_score": 29.415186568562728, "chrf_score": 27.48822861775521, "xcomet_score": 0.9570801258087158, "xcomet_qe_score": 0.8091422319412231, "metricx_score": 2.4278981685638428, "metricx_qe_score": 2.7795698642730713, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 IMT 社区的最佳实践来评估这些模型的翻译能力。", "metrics": {"bleu_score": 56.053891826721106, "chrf_score": 48.717295219245514, "xcomet_score": 0.8396009206771851, "xcomet_qe_score": 0.787361741065979, "metricx_score": 6.157434463500977, "metricx_qe_score": 7.629921913146973, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这包括使用最新的测试集,以避免测试数据与语言模型的训练数据重叠。", "metrics": {"bleu_score": 79.8770253749631, "chrf_score": 76.01935412712909, "xcomet_score": 0.9972058534622192, "xcomet_qe_score": 0.9762731194496155, "metricx_score": 0.42696547508239746, "metricx_qe_score": 0.5030761957168579, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们比较了两个最先进的系统,因此是性能最佳的系统或 WMT 评估。", "metrics": {"bleu_score": 36.39046852020722, "chrf_score": 33.44139079399765, "xcomet_score": 0.7905687093734741, "xcomet_qe_score": 0.7965447306632996, "metricx_score": 5.476390838623047, "metricx_qe_score": 5.580976963043213, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用了最先进的神经 IM 指标,并还展示了基于专家的人类评估结果。", "metrics": {"bleu_score": 57.76192231996528, "chrf_score": 47.41657246628048, "xcomet_score": 0.8049395084381104, "xcomet_qe_score": 0.738993763923645, "metricx_score": 5.139297962188721, "metricx_qe_score": 5.685985565185547, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们提供了一些提示选择策略的建议。", "metrics": {"bleu_score": 70.75330011966422, "chrf_score": 64.06828873488384, "xcomet_score": 0.8876395225524902, "xcomet_qe_score": 0.8450835347175598, "metricx_score": 1.09638249874115, "metricx_qe_score": 3.2114005088806152, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "提示对翻译大型语言模型的性能有很大影响。我们可以通过一个简单的实验看到这一点,我们使用一次性提示,并为每句话提供了两个不同的提示。", "metrics": {"bleu_score": 45.31958964537674, "chrf_score": 40.39144949551719, "xcomet_score": 0.8869255781173706, "xcomet_qe_score": 0.8213397264480591, "metricx_score": 3.096400022506714, "metricx_qe_score": 2.1927011013031006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大多数句子(1000 个中有", "metrics": {"bleu_score": 12.549310621989482, "chrf_score": 28.927733315739673, "xcomet_score": 0.6288379430770874, "xcomet_qe_score": 0.6666998863220215, "metricx_score": 9.53897476196289, "metricx_qe_score": 8.300556182861328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "516 个),观察到的差异超过一个模糊点。", "metrics": {"bleu_score": 6.986768364373987, "chrf_score": 13.287934503883964, "xcomet_score": 0.4865637421607971, "xcomet_qe_score": 0.32897448539733887, "metricx_score": 9.14155101776123, "metricx_qe_score": 14.984570503234863, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在极端情况下,这可以达到 40 个模糊点。", "metrics": {"bleu_score": 32.22538601891171, "chrf_score": 23.148500068613558, "xcomet_score": 0.7915583848953247, "xcomet_qe_score": 0.7482950687408447, "metricx_score": 4.768306255340576, "metricx_qe_score": 2.453777551651001, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择一个好的提示策略很重要。在", "metrics": {"bleu_score": 79.65670178751185, "chrf_score": 80.89802907710907, "xcomet_score": 0.8271282315254211, "xcomet_qe_score": 0.8039601445198059, "metricx_score": 4.00327205657959, "metricx_qe_score": 0.3293456435203552, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的实验中,我们选择了五次提示策略,我们只需将我们提供给系统的每句话标记为其所在的语言。所以在", "metrics": {"bleu_score": 34.552220942921814, "chrf_score": 30.91432532771642, "xcomet_score": 0.6844768524169922, "xcomet_qe_score": 0.6233618259429932, "metricx_score": 7.2291765213012695, "metricx_qe_score": 3.662550449371338, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这个例子中,我们从德语翻译成英语,德语句子用德语冒号标记,英语翻译用英语冒号标记。", "metrics": {"bleu_score": 42.37810294090321, "chrf_score": 29.887476745812936, "xcomet_score": 0.9665660858154297, "xcomet_qe_score": 0.9793543815612793, "metricx_score": 1.4266849756240845, "metricx_qe_score": 1.4198083877563477, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,在多次提示的情况下,实际形式的提示对性能没有太大影响。", "metrics": {"bleu_score": 25.172020603190486, "chrf_score": 23.161154344529393, "xcomet_score": 0.7642284631729126, "xcomet_qe_score": 0.7568264007568359, "metricx_score": 1.9574424028396606, "metricx_qe_score": 2.001008987426758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这对于零次和一次提示至关重要,而在我们的情况下,我们使用", "metrics": {"bleu_score": 13.834368456410946, "chrf_score": 13.879704715609254, "xcomet_score": 0.49672430753707886, "xcomet_qe_score": 0.5093346834182739, "metricx_score": 5.330418586730957, "metricx_qe_score": 4.861845016479492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "了五次提示,实际形式的提示几乎没有区", "metrics": {"bleu_score": 14.06635021675541, "chrf_score": 11.924962301713776, "xcomet_score": 0.32677924633026123, "xcomet_qe_score": 0.15189920365810394, "metricx_score": 11.662396430969238, "metricx_qe_score": 8.3614501953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "别。重要的是例子本身。", "metrics": {"bleu_score": 4.2665050358928855, "chrf_score": 6.218905472636816, "xcomet_score": 0.7197756767272949, "xcomet_qe_score": 0.3687080442905426, "metricx_score": 1.6353012323379517, "metricx_qe_score": 1.872441291809082, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们实验结果的总结是,例子质量比与源句子的相似性更重要。", "metrics": {"bleu_score": 83.47563508866297, "chrf_score": 78.47704724153999, "xcomet_score": 0.9258888959884644, "xcomet_qe_score": 0.9196336269378662, "metricx_score": 0.9950923919677734, "metricx_qe_score": 0.7766151428222656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,选择高质量翻译的例子非常重要。", "metrics": {"bleu_score": 31.66831894770993, "chrf_score": 28.534477178392827, "xcomet_score": 0.9397034645080566, "xcomet_qe_score": 0.988231897354126, "metricx_score": 0.5738591551780701, "metricx_qe_score": 0.5590805411338806, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,我们比较了从 WMT 评估的训练数据或开发数据中选择提示。", "metrics": {"bleu_score": 46.60298306626784, "chrf_score": 40.024840678751424, "xcomet_score": 0.6242258548736572, "xcomet_qe_score": 0.5456323623657227, "metricx_score": 2.489325761795044, "metricx_qe_score": 3.640312910079956, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据经过精心挑选,质量更高,而训练数据则更嘈杂,结果显示使用", "metrics": {"bleu_score": 25.413699712070223, "chrf_score": 23.72811703796201, "xcomet_score": 0.7887691259384155, "xcomet_qe_score": 0.775326132774353, "metricx_score": 5.433223724365234, "metricx_qe_score": 3.0298900604248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "开发数据时性能更好。", "metrics": {"bleu_score": 28.833574340610166, "chrf_score": 25.84333172772308, "xcomet_score": 0.8239965438842773, "xcomet_qe_score": 0.7881331443786621, "metricx_score": 3.309196710586548, "metricx_qe_score": 4.419642448425293, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,专业最先进的系统在性能上仍比 Palm 翻译有显著优势", "metrics": {"bleu_score": 16.49233068765561, "chrf_score": 17.8075693956487, "xcomet_score": 0.8086639642715454, "xcomet_qe_score": 0.7924463748931885, "metricx_score": 5.446206569671631, "metricx_qe_score": 4.98443603515625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",但 Palm 已经接近商业系统。", "metrics": {"bleu_score": 45.63040025536, "chrf_score": 36.087841104634926, "xcomet_score": 0.8699089288711548, "xcomet_qe_score": 0.8607439994812012, "metricx_score": 6.683501720428467, "metricx_qe_score": 7.079621315002441, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的情况下,我们选择与谷歌翻译进行评估。", "metrics": {"bleu_score": 56.15880475398439, "chrf_score": 46.85642748972231, "xcomet_score": 0.876212477684021, "xcomet_qe_score": 0.84937584400177, "metricx_score": 2.7010931968688965, "metricx_qe_score": 3.047389507293701, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 MQM 框架进行的电子邮件分析结果表明,Palm 的流畅度与最先进的系统相当,但主要区别在于准确性。", "metrics": {"bleu_score": 44.1317123356216, "chrf_score": 38.0133946402329, "xcomet_score": 0.8221286535263062, "xcomet_qe_score": 0.7460062503814697, "metricx_score": 6.038118362426758, "metricx_qe_score": 6.543340682983398, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,最常见的错误是遗漏错误。", "metrics": {"bleu_score": 72.21600387198372, "chrf_score": 69.88261738261738, "xcomet_score": 0.7579965591430664, "xcomet_qe_score": 0.7860524654388428, "metricx_score": 1.7050689458847046, "metricx_qe_score": 0.886371910572052, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,Palm 有时会选择生成听起来更好的翻译,有时会省略源句子的某些部分,这些部分在翻译中是生动的。", "metrics": {"bleu_score": 31.318569084293763, "chrf_score": 30.67161020243071, "xcomet_score": 0.7353381514549255, "xcomet_qe_score": 0.65935879945755, "metricx_score": 5.395073413848877, "metricx_qe_score": 5.201219081878662, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,Parm 的风格生硬类别低于最先进的系统,这是一个额外的信号,表明 Parm 提供了非常流畅的输出,但仍然存在一些准确性问题。", "metrics": {"bleu_score": 58.166541519265884, "chrf_score": 49.355637614240536, "xcomet_score": 0.6338397860527039, "xcomet_qe_score": 0.639427900314331, "metricx_score": 7.567710876464844, "metricx_qe_score": 6.345549583435059, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是这个非常简短的概述的全部内容。", "metrics": {"bleu_score": 49.89070972910272, "chrf_score": 50.96667019306298, "xcomet_score": 0.9204057455062866, "xcomet_qe_score": 0.9140896797180176, "metricx_score": 0.3837635815143585, "metricx_qe_score": 0.374846875667572, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "有关更多详细信息,请参阅论文的完整介绍。", "metrics": {"bleu_score": 35.467316001743725, "chrf_score": 29.245211968903295, "xcomet_score": 0.91548091173172, "xcomet_qe_score": 0.9137508273124695, "metricx_score": 0.7078700661659241, "metricx_qe_score": 0.41184788942337036, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9979878664016724, "xcomet_qe_score": 0.9781211018562317, "metricx_score": 0.0, "metricx_qe_score": 0.11406275629997253, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是德国萨尔兰大学的博士生大伟。", "metrics": {"bleu_score": 53.74649369872052, "chrf_score": 43.097011943948395, "xcomet_score": 0.8909082412719727, "xcomet_qe_score": 0.9104241132736206, "metricx_score": 1.078803300857544, "metricx_qe_score": 0.9043640494346619, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个视频中,我想介绍我们最近的工作,名为“比你想象的更弱”,这是一次对弱监督学习的批判性审视。", "metrics": {"bleu_score": 52.07163456037269, "chrf_score": 47.403580836739394, "xcomet_score": 0.9594069719314575, "xcomet_qe_score": 0.8529845476150513, "metricx_score": 2.426757335662842, "metricx_qe_score": 2.880457878112793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是与萧玉生、马里奥·斯穆斯巴赫、吉亚·斯泰芬和迪特里希·克拉考合作完成的。", "metrics": {"bleu_score": 3.446640887300568, "chrf_score": 3.3205557837986217, "xcomet_score": 0.6306326389312744, "xcomet_qe_score": 0.7339413166046143, "metricx_score": 3.3806002140045166, "metricx_qe_score": 3.7896831035614014, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我想先简要介绍一下弱监督和弱监督学习。", "metrics": {"bleu_score": 88.52140475440834, "chrf_score": 92.26506423113229, "xcomet_score": 0.8952239751815796, "xcomet_qe_score": 0.8487532138824463, "metricx_score": 0.8096938729286194, "metricx_qe_score": 2.310981273651123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督中,我们不手动标注数据。", "metrics": {"bleu_score": 36.17043615983554, "chrf_score": 29.691878261623245, "xcomet_score": 0.8968457579612732, "xcomet_qe_score": 0.8531893491744995, "metricx_score": 0.8261656165122986, "metricx_qe_score": 1.6418793201446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "相反,我们使用弱标注源来标注数据,例如简单的启发式规则、知识库或低质量的众包,如图右侧所示。", "metrics": {"bleu_score": 63.85168724828885, "chrf_score": 58.033718603461395, "xcomet_score": 0.7237317562103271, "xcomet_qe_score": 0.7151898741722107, "metricx_score": 1.7956327199935913, "metricx_qe_score": 2.064055919647217, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与人工标注相比,弱标注的成本要低得多,但它们也存在噪声,这意味着一定数量的标注是错误的。", "metrics": {"bleu_score": 35.38401793879241, "chrf_score": 30.628142655753905, "xcomet_score": 0.8652809858322144, "xcomet_qe_score": 0.834907054901123, "metricx_score": 2.261817216873169, "metricx_qe_score": 2.664278984069824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们直接在弱标注数据上训练神经网络,神经网络往往会记住标注的噪声,并且无法泛化。", "metrics": {"bleu_score": 45.44644015024823, "chrf_score": 39.51926837778751, "xcomet_score": 0.8899685144424438, "xcomet_qe_score": 0.7964614629745483, "metricx_score": 0.9702494740486145, "metricx_qe_score": 1.1430253982543945, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在弱监督学习中,提出了训练算法,以便在这样的标签噪声下稳健地训练神经网络,以便训练的模型仍然能很好地泛化。", "metrics": {"bleu_score": 52.736801818503665, "chrf_score": 46.24536723397292, "xcomet_score": 0.954003095626831, "xcomet_qe_score": 0.8641597032546997, "metricx_score": 1.3235867023468018, "metricx_qe_score": 2.691997766494751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在最近的WSL(弱监督学习)工作中,一个常见的说法是,人们说他们只在弱标签数据下训练模型,并在干净的测试集上取得了高性能。", "metrics": {"bleu_score": 35.97658451545384, "chrf_score": 30.981927898184274, "xcomet_score": 0.826482355594635, "xcomet_qe_score": 0.7902005910873413, "metricx_score": 3.6234304904937744, "metricx_qe_score": 4.165920257568359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从技术上讲,这个说法并没有错,但有一个问题,那就是", "metrics": {"bleu_score": 19.01854757497289, "chrf_score": 21.744283726618452, "xcomet_score": 0.7313333749771118, "xcomet_qe_score": 0.3276762366294861, "metricx_score": 7.91001033782959, "metricx_qe_score": 8.22897720336914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5317288637161255, "xcomet_qe_score": 0.136541947722435, "metricx_score": 6.005777835845947, "metricx_qe_score": 11.085416793823242, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.5430290699005127, "xcomet_qe_score": 0.1457119882106781, "metricx_score": 6.939623832702637, "metricx_qe_score": 7.438058853149414, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "三个研究问题。", "metrics": {"bleu_score": 27.645304662956455, "chrf_score": 37.28969519074232, "xcomet_score": 0.6767177581787109, "xcomet_qe_score": 0.5813324451446533, "metricx_score": 4.788186550140381, "metricx_qe_score": 7.17353630065918, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们是否需要干净的验证数据?", "metrics": {"bleu_score": 28.559670149996563, "chrf_score": 33.307111079556165, "xcomet_score": 0.8796626329421997, "xcomet_qe_score": 0.8338778018951416, "metricx_score": 4.49473762512207, "metricx_qe_score": 5.659512042999268, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们是否应该", "metrics": {"bleu_score": 0.1746472002279499, "chrf_score": 1.2801639788748997, "xcomet_score": 0.1382938027381897, "xcomet_qe_score": 0.14581921696662903, "metricx_score": 19.58406639099121, "metricx_qe_score": 12.31582260131836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "只使用干净的样本进行验证,还是有更好的利用它们的方法?", "metrics": {"bleu_score": 49.00593713674817, "chrf_score": 45.39205528571845, "xcomet_score": 0.8321350812911987, "xcomet_qe_score": 0.7835904359817505, "metricx_score": 0.9548490643501282, "metricx_qe_score": 1.35115647315979, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在工作中解决了这些研究问题,我们的发现如下。", "metrics": {"bleu_score": 57.26580707438228, "chrf_score": 49.41553937149765, "xcomet_score": 0.9748376607894897, "xcomet_qe_score": 0.948908805847168, "metricx_score": 1.6144813299179077, "metricx_qe_score": 2.59200382232666, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们发现,有趣的是,最近的WSL方法确实需要干净的验证样本才能正常工作。", "metrics": {"bleu_score": 71.96057984777588, "chrf_score": 68.93358518056257, "xcomet_score": 0.8645211458206177, "xcomet_qe_score": 0.8792775869369507, "metricx_score": 1.864709496498108, "metricx_qe_score": 2.865553855895996, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "否则,性能会大幅下降。", "metrics": {"bleu_score": 74.19446627365011, "chrf_score": 67.86976911976912, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4033818542957306, "metricx_qe_score": 0.6947073936462402, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果没有干净的验证样本,那么训练的模型无法超越原始的弱标签进行泛化,这意味着训练是没有意义的。", "metrics": {"bleu_score": 62.44001629705089, "chrf_score": 54.60667184280668, "xcomet_score": 0.9639217853546143, "xcomet_qe_score": 0.8772141933441162, "metricx_score": 1.7111868858337402, "metricx_qe_score": 2.3541083335876465, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明WSL方法实际上需要干净的标注数据才能正常工作,获取干净验证样本的标注成本不应被忽视。", "metrics": {"bleu_score": 55.39238522260304, "chrf_score": 51.26254788767057, "xcomet_score": 0.8621635437011719, "xcomet_qe_score": 0.8993411660194397, "metricx_score": 2.994037389755249, "metricx_qe_score": 3.254563570022583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的第二个发现是,增加干净的验证样本数量将有助于WSL方法取得更好的性能,如图左侧所示。", "metrics": {"bleu_score": 52.40827100322791, "chrf_score": 47.97576117480082, "xcomet_score": 0.9370478391647339, "xcomet_qe_score": 0.9416278004646301, "metricx_score": 3.5803236961364746, "metricx_qe_score": 4.147233963012695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,我们只需要每个类别20个样本就能达到高性能。", "metrics": {"bleu_score": 26.512298021756173, "chrf_score": 24.35864859777903, "xcomet_score": 0.9487731456756592, "xcomet_qe_score": 0.9626321792602539, "metricx_score": 1.41232168674469, "metricx_qe_score": 1.7958661317825317, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这还不是故事的全部,因为如果我们决定访问干净样本,那么直接在它们上训练甚至会取得更好的性能。红", "metrics": {"bleu_score": 42.606510802762315, "chrf_score": 34.47229722583022, "xcomet_score": 0.7557896971702576, "xcomet_qe_score": 0.7791933417320251, "metricx_score": 5.795463562011719, "metricx_qe_score": 5.491458892822266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "色图形显示了直接应用于干净数据的微调方法与仅用于验证的WSL方法之间的性能差异。", "metrics": {"bleu_score": 64.16167424171942, "chrf_score": 63.069809239276495, "xcomet_score": 0.7533758878707886, "xcomet_qe_score": 0.7294449806213379, "metricx_score": 4.26270055770874, "metricx_qe_score": 4.890678405761719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,如果我们每个类别有10个样本,直接微调开始超越WSL方法。最后,之前WSL方法中", "metrics": {"bleu_score": 37.831345865381095, "chrf_score": 38.15497634409945, "xcomet_score": 0.6384459137916565, "xcomet_qe_score": 0.6754657030105591, "metricx_score": 7.725430965423584, "metricx_qe_score": 5.883910179138184, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "声称的性能提升可以通过允许在干净验证样本上继续微调来轻松实现。", "metrics": {"bleu_score": 22.18279219645735, "chrf_score": 20.113345655569, "xcomet_score": 0.7932989597320557, "xcomet_qe_score": 0.7372500896453857, "metricx_score": 4.530861854553223, "metricx_qe_score": 5.644603252410889, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,Van Linden模型最初的性能低于更复杂的WSL方法如余弦。", "metrics": {"bleu_score": 26.73225663337683, "chrf_score": 22.658697017527224, "xcomet_score": 0.64342200756073, "xcomet_qe_score": 0.6691452860832214, "metricx_score": 6.671796798706055, "metricx_qe_score": 7.058351039886475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,如果我们允许在干净样本上继续微调,那么FTW的性能与其他方法一样好。", "metrics": {"bleu_score": 51.11744072566821, "chrf_score": 43.910848387767906, "xcomet_score": 0.8531965017318726, "xcomet_qe_score": 0.7585936784744263, "metricx_score": 1.6822658777236938, "metricx_qe_score": 2.347874402999878, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,在实践中,没有理由选择更复杂的WSL方法,这些方法需要更多的计算时间和磁盘空间。", "metrics": {"bleu_score": 56.05699297537929, "chrf_score": 54.2786351532051, "xcomet_score": 0.9738656282424927, "xcomet_qe_score": 0.9751347303390503, "metricx_score": 0.7796887159347534, "metricx_qe_score": 1.4478003978729248, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们表明,最近的WSL方法需要干净的、手动标注的样本才能正常工作。", "metrics": {"bleu_score": 54.254982326546354, "chrf_score": 51.63610301504424, "xcomet_score": 0.8016637563705444, "xcomet_qe_score": 0.8668516874313354, "metricx_score": 2.6892406940460205, "metricx_qe_score": 3.687225818634033, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们的性能提升和实用性被严重高估了。", "metrics": {"bleu_score": 53.816073893351884, "chrf_score": 48.56849415717752, "xcomet_score": 0.9926676750183105, "xcomet_qe_score": 0.9959598779678345, "metricx_score": 0.6876567602157593, "metricx_qe_score": 0.8314505815505981, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对未来工作的具体建议如下。", "metrics": {"bleu_score": 72.41577342575832, "chrf_score": 61.37612387612387, "xcomet_score": 0.9992729425430298, "xcomet_qe_score": 0.986473798751831, "metricx_score": 0.3336814045906067, "metricx_qe_score": 0.2849405109882355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,报告模型选择标准。", "metrics": {"bleu_score": 76.91605673134588, "chrf_score": 71.63239538239537, "xcomet_score": 0.9883747100830078, "xcomet_qe_score": 0.9105306267738342, "metricx_score": 0.23735392093658447, "metricx_qe_score": 0.4112689793109894, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,报告模型选择是否基于干净的验证样本。", "metrics": {"bleu_score": 43.546195754004344, "chrf_score": 37.031167828539765, "xcomet_score": 0.9142000079154968, "xcomet_qe_score": 0.8755610585212708, "metricx_score": 1.3262100219726562, "metricx_qe_score": 2.276707172393799, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,WSL方法应该与一些基于干净样本的短学习基线进行比较。", "metrics": {"bleu_score": 36.263572511960625, "chrf_score": 34.777503203313145, "xcomet_score": 0.8107745051383972, "xcomet_qe_score": 0.7904284000396729, "metricx_score": 3.4387288093566895, "metricx_qe_score": 3.8393125534057617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,连续微调是一个简单但强大的基线,应该在未来的WSL工作中考虑。", "metrics": {"bleu_score": 38.96359712384752, "chrf_score": 33.5044114151537, "xcomet_score": 0.8633602857589722, "xcomet_qe_score": 0.7266933917999268, "metricx_score": 2.748408794403076, "metricx_qe_score": 2.9833693504333496, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们开源了我们的代码。", "metrics": {"bleu_score": 59.85421813100691, "chrf_score": 55.296530627954546, "xcomet_score": 0.9946787357330322, "xcomet_qe_score": 0.9214116930961609, "metricx_score": 0.33761468529701233, "metricx_qe_score": 0.46709316968917847, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您可以通过此幻灯片上的二维码找到它。", "metrics": {"bleu_score": 60.28817681965138, "chrf_score": 50.69858926476574, "xcomet_score": 0.9951430559158325, "xcomet_qe_score": 0.9870158433914185, "metricx_score": 0.48675400018692017, "metricx_qe_score": 0.44404107332229614, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请随时查看。", "metrics": {"bleu_score": 25.57539057896621, "chrf_score": 16.573915525114153, "xcomet_score": 0.8827329277992249, "xcomet_qe_score": 0.8141119480133057, "metricx_score": 0.5074750185012817, "metricx_qe_score": 0.7284374833106995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢,祝会议愉快。", "metrics": {"bleu_score": 7.687847996206941, "chrf_score": 8.318914192155988, "xcomet_score": 0.9833041429519653, "xcomet_qe_score": 0.9773144125938416, "metricx_score": 0.3437096178531647, "metricx_qe_score": 0.22848688066005707, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是詹姆斯·", "metrics": {"bleu_score": 19.64073254502565, "chrf_score": 9.993248618647176, "xcomet_score": 0.8700615763664246, "xcomet_qe_score": 0.6193946599960327, "metricx_score": 1.1073329448699951, "metricx_qe_score": 0.9096816182136536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "芬奇,我是萨拉·芬奇。", "metrics": {"bleu_score": 8.29519350710986, "chrf_score": 5.405070919696089, "xcomet_score": 0.6480262279510498, "xcomet_qe_score": 0.7325373888015747, "metricx_score": 4.678750514984131, "metricx_qe_score": 5.4248833656311035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "今天我们将向大家介绍ABCeval,这是一种全新的评估对话式人工智能的维度方法。", "metrics": {"bleu_score": 39.94463928700864, "chrf_score": 39.05509614247019, "xcomet_score": 0.902928352355957, "xcomet_qe_score": 0.9600365161895752, "metricx_score": 1.9432307481765747, "metricx_qe_score": 1.954038143157959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作由埃默里大学的吉诺·崔教授领导的埃默里大学自然语言处理实验室完成,并与亚马逊Alexa AI合作完成。", "metrics": {"bleu_score": 36.71510216651532, "chrf_score": 37.42052886305305, "xcomet_score": 0.7726734280586243, "xcomet_qe_score": 0.8315118551254272, "metricx_score": 2.404790163040161, "metricx_qe_score": 2.381755828857422, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "假设您刚刚开发了一个对话模型,并且想知道它与当前的先进技术相比表现如何。", "metrics": {"bleu_score": 59.22043012135426, "chrf_score": 49.70728051729001, "xcomet_score": 0.9966201782226562, "xcomet_qe_score": 0.981902003288269, "metricx_score": 0.40368345379829407, "metricx_qe_score": 0.45257705450057983, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "常见的做法是使用人工评估,例如让人工评判员选择两个对话中哪一个更好,或者根据李克特量表对对话进行评分。", "metrics": {"bleu_score": 68.31775380400515, "chrf_score": 62.78543463675605, "xcomet_score": 0.905758261680603, "xcomet_qe_score": 0.9253658652305603, "metricx_score": 0.5302633047103882, "metricx_qe_score": 0.819155752658844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些方法在提供整体对话质量的全面评估方面效果良好,但对话质量有许多方面。", "metrics": {"bleu_score": 36.6474865901556, "chrf_score": 30.76041475589639, "xcomet_score": 0.9300557971000671, "xcomet_qe_score": 0.9080116152763367, "metricx_score": 0.547738790512085, "metricx_qe_score": 0.7834680676460266, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,您可能希望评估对话质量的多个维度,以便更细致地了解模型的优势和劣势。", "metrics": {"bleu_score": 50.29174353612135, "chrf_score": 46.87617085291001, "xcomet_score": 0.9808710813522339, "xcomet_qe_score": 0.9666447639465332, "metricx_score": 0.6209126710891724, "metricx_qe_score": 0.4685423970222473, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一种方法是简单地让人工评判员评估对话质量的几个维度,例如模型响应的相关性,使用现有的比较方法或李克特量表方法。", "metrics": {"bleu_score": 59.33113968542448, "chrf_score": 51.16467869834499, "xcomet_score": 0.870466947555542, "xcomet_qe_score": 0.8584097623825073, "metricx_score": 1.2495280504226685, "metricx_qe_score": 1.7129851579666138, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们相信存在一种更精确、更可靠的维度对话评估策略。", "metrics": {"bleu_score": 41.348528734771456, "chrf_score": 40.00853590887389, "xcomet_score": 0.8991663455963135, "xcomet_qe_score": 0.8668703436851501, "metricx_score": 1.396376132965088, "metricx_qe_score": 1.407410740852356, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法试图通过明确标注每个模型响应是否表达了某些行为(例如,用无关信息回应或自相矛盾)来减少人工评估的主观性。", "metrics": {"bleu_score": 64.31831397753004, "chrf_score": 56.82765732109779, "xcomet_score": 0.8617007732391357, "xcomet_qe_score": 0.9007154107093811, "metricx_score": 1.3423429727554321, "metricx_qe_score": 1.9865288734436035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们称这种方法为对话行为标注,简称ABCeval。", "metrics": {"bleu_score": 51.35245522145285, "chrf_score": 48.181581091579964, "xcomet_score": 0.8508384227752686, "xcomet_qe_score": 0.8456370830535889, "metricx_score": 1.5793893337249756, "metricx_qe_score": 2.0698132514953613, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们开发了这种方法,以全面覆盖最近文献中建议影响对话质量的对话模型行为。", "metrics": {"bleu_score": 38.08197831623487, "chrf_score": 30.494451915540782, "xcomet_score": 0.8067662119865417, "xcomet_qe_score": 0.858824610710144, "metricx_score": 2.243316411972046, "metricx_qe_score": 3.0585410594940186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.45177245140075684, "xcomet_qe_score": 0.22723093628883362, "metricx_score": 7.201988220214844, "metricx_qe_score": 23.299734115600586, "linguapy_score": [1, "UNKNOWN"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ABCeval评估模型是否忽视其对话伙伴,是否说无关的话,是否自相矛盾或与对话伙伴矛盾,是否产生错误的事实或违反常识,以及模型是否成功或未能表现出同理心。", "metrics": {"bleu_score": 31.736880791147563, "chrf_score": 27.130351484315646, "xcomet_score": 0.7772241830825806, "xcomet_qe_score": 0.7617971301078796, "metricx_score": 3.091813325881958, "metricx_qe_score": 3.4288294315338135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了确定哪种评估方法最有效,我们选择了四种最先进的对话模型,并使用ABCeval对每种模型的100个人机对话进行了评估。", "metrics": {"bleu_score": 55.74137657930488, "chrf_score": 51.41845977546454, "xcomet_score": 0.9285570383071899, "xcomet_qe_score": 0.955214262008667, "metricx_score": 1.5845096111297607, "metricx_qe_score": 2.0946731567382812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了进行比较,我们还使用三种现有方法对这些对话进行了评估:回合级别的李克特评分、对话级别的李克特评分和对话级别的配对比较。", "metrics": {"bleu_score": 52.326366072487296, "chrf_score": 45.10895397423288, "xcomet_score": 0.9220722913742065, "xcomet_qe_score": 0.8454614281654358, "metricx_score": 4.410337924957275, "metricx_qe_score": 4.658422470092773, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于每种现有方法,我们收集了对对话中最常见的八个方面的评估,因为这是评估对话模型的标准做法。从", "metrics": {"bleu_score": 43.930651797454075, "chrf_score": 37.02204307415899, "xcomet_score": 0.7748256921768188, "xcomet_qe_score": 0.7572497129440308, "metricx_score": 6.4876508712768555, "metricx_qe_score": 1.7237541675567627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对这些评估结果的分析中,我们发现ABCeval行为标签总体上比现有方法收集的标签更可靠,这是通过对100个双重标注对话的评判员间一致性来衡量的。此外,ABCeval标签比现有方法产生的指标更能预测", "metrics": {"bleu_score": 29.540077240368333, "chrf_score": 38.09332527545964, "xcomet_score": 0.7110264897346497, "xcomet_qe_score": 0.712428092956543, "metricx_score": 5.99925422668457, "metricx_qe_score": 6.092589855194092, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "整体对话质量,这可以通过这个简单的线性回归分析来证明。例如,eval标签比现有方法产生的指标更能预测整体对话质量,这可以通过这个简单的线性回归分析来证明。", "metrics": {"bleu_score": 32.66771862095512, "chrf_score": 39.39052251177688, "xcomet_score": 0.3431868553161621, "xcomet_qe_score": 0.378975510597229, "metricx_score": 7.488119125366211, "metricx_qe_score": 6.131275177001953, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,您可以看到测量自相矛盾和对话伙伴矛盾的回合比例分别解释了对话质量的5%和10%,而平均李克特一致性得分只解释了4%或更少。", "metrics": {"bleu_score": 65.2027021695082, "chrf_score": 57.8613293688378, "xcomet_score": 0.7860572934150696, "xcomet_qe_score": 0.7816302180290222, "metricx_score": 4.312442302703857, "metricx_qe_score": 4.809233665466309, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们使用逐步线性回归检查了每个评估指标是否捕捉了对话质量的独特方面。您", "metrics": {"bleu_score": 72.77595219039004, "chrf_score": 66.98241911298098, "xcomet_score": 0.8134803771972656, "xcomet_qe_score": 0.8017780184745789, "metricx_score": 4.690852165222168, "metricx_qe_score": 1.8092169761657715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以看到,所有ABCeval指标的组合解释了超过25%的对话质量,而当您逐个移除这些指标时,大多数都会导致丢失大量关于质量的信息。", "metrics": {"bleu_score": 51.015443328832795, "chrf_score": 50.052690913253315, "xcomet_score": 0.897085964679718, "xcomet_qe_score": 0.8947241306304932, "metricx_score": 2.034358263015747, "metricx_qe_score": 2.9911739826202393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一方面,所有回合级别的李克特指标的组合解释了远低于质量的百分比,而且这些指标中很少有携带独特信息。", "metrics": {"bleu_score": 35.1810388634354, "chrf_score": 30.671681044825554, "xcomet_score": 0.6721347570419312, "xcomet_qe_score": 0.6862906217575073, "metricx_score": 5.248287677764893, "metricx_qe_score": 5.818960666656494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些可靠、信息丰富且独特的ABCeval指标使我们能够以比以前方法所能实现的更高的分辨率评估对话式人工智能。您可以", "metrics": {"bleu_score": 5.770467998237226, "chrf_score": 12.959570265052331, "xcomet_score": 0.3535110652446747, "xcomet_qe_score": 0.5437609553337097, "metricx_score": 6.721299171447754, "metricx_qe_score": 3.298807144165039, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的实验结果中看到,仍然存在一些挑战,并且这些挑战已经被精确量化。", "metrics": {"bleu_score": 41.26368727503021, "chrf_score": 42.810959855459885, "xcomet_score": 0.9904096126556396, "xcomet_qe_score": 0.9594658613204956, "metricx_score": 1.153573989868164, "metricx_qe_score": 1.149175763130188, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们在测试的机器人中,大约20%的响应违反了常识。", "metrics": {"bleu_score": 52.657535847237725, "chrf_score": 46.21762618544426, "xcomet_score": 0.9156832695007324, "xcomet_qe_score": 0.8714607954025269, "metricx_score": 1.0754845142364502, "metricx_qe_score": 1.3314989805221558, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们在大约15%的响应中产生无关信息,并且在大约10%的时间里自相矛盾或与对话伙伴矛盾。", "metrics": {"bleu_score": 49.896826163186454, "chrf_score": 48.014620091336184, "xcomet_score": 0.6933066844940186, "xcomet_qe_score": 0.6717526912689209, "metricx_score": 2.743823289871216, "metricx_qe_score": 2.72031831741333, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "随着该领域的快速进步,许多这些错误率可能会在新发布的模型中有所下降。", "metrics": {"bleu_score": 46.876452400276804, "chrf_score": 39.494292125718985, "xcomet_score": 0.9705497026443481, "xcomet_qe_score": 0.9546400308609009, "metricx_score": 2.0872960090637207, "metricx_qe_score": 3.3125736713409424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这更说明了追求可靠且精确的评估指标以比较模型的重要性。", "metrics": {"bleu_score": 45.309372174398234, "chrf_score": 40.91703962752154, "xcomet_score": 0.9977174997329712, "xcomet_qe_score": 0.9871809482574463, "metricx_score": 0.910224437713623, "metricx_qe_score": 1.0445787906646729, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们希望ABCeval可以被该领域的其他人作为朝着这个方向迈出的有意义的一步,", "metrics": {"bleu_score": 60.5970302592686, "chrf_score": 53.594037204343635, "xcomet_score": 0.9349333047866821, "xcomet_qe_score": 0.918901801109314, "metricx_score": 2.7336068153381348, "metricx_qe_score": 3.180150032043457, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们期待看到对话式人工智能在未来几个月和几年中的发展。", "metrics": {"bleu_score": 74.0609366763812, "chrf_score": 73.52998803726307, "xcomet_score": 0.9891669750213623, "xcomet_qe_score": 0.9691853523254395, "metricx_score": 0.7160965800285339, "metricx_qe_score": 0.8807143568992615, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "感谢您的观看。", "metrics": {"bleu_score": 30.739407647563215, "chrf_score": 38.065210704398645, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.38943803310394287, "metricx_qe_score": 0.6194370985031128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好!我叫殷凯奥,我将为大家介绍我们的研究成果,题目是《翻译何时需要上下文?", "metrics": {"bleu_score": 38.65297025445099, "chrf_score": 34.55431926781948, "xcomet_score": 0.9045711755752563, "xcomet_qe_score": 0.8993980288505554, "metricx_score": 0.7650156617164612, "metricx_qe_score": 0.967125654220581, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于数据的多语言探索》。", "metrics": {"bleu_score": 67.0422683816333, "chrf_score": 60.745550745550744, "xcomet_score": 0.9882596731185913, "xcomet_qe_score": 0.9236876964569092, "metricx_score": 0.9823226928710938, "metricx_qe_score": 1.2916620969772339, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项研究是与帕特里克·费尔南德斯、艾米·刘、安德烈·F.D. 马丁斯和格雷厄姆·纽比格合作完成的。因此", "metrics": {"bleu_score": 11.336958836647044, "chrf_score": 8.2793624072362, "xcomet_score": 0.6513541340827942, "xcomet_qe_score": 0.4403180480003357, "metricx_score": 5.425573825836182, "metricx_qe_score": 3.462517023086548, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",很多翻译确实依赖于上下文。", "metrics": {"bleu_score": 44.534504264163466, "chrf_score": 40.63292471894734, "xcomet_score": 0.9980084896087646, "xcomet_qe_score": 0.9870551824569702, "metricx_score": 0.9218145608901978, "metricx_qe_score": 1.1015604734420776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,我们如何翻译这句话中的“mole”?", "metrics": {"bleu_score": 79.65670178751185, "chrf_score": 80.61198748505251, "xcomet_score": 0.9964717626571655, "xcomet_qe_score": 0.9687114953994751, "metricx_score": 0.7808219790458679, "metricx_qe_score": 2.0721216201782227, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果前一句是“如果部长们知道了,事情可能会变得危险”,那么“mole”指的是间谍。", "metrics": {"bleu_score": 19.97752200394278, "chrf_score": 13.606427842946959, "xcomet_score": 0.9883341789245605, "xcomet_qe_score": 0.9678401350975037, "metricx_score": 2.649116277694702, "metricx_qe_score": 3.986283302307129, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但如果前一句是“这可能有什么严重的事情,医生?”那么“mole”指的是胎记。", "metrics": {"bleu_score": 19.51694089759498, "chrf_score": 17.47918930377125, "xcomet_score": 0.9268720149993896, "xcomet_qe_score": 0.8840538263320923, "metricx_score": 2.63374924659729, "metricx_qe_score": 3.570192575454712, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,根据上下文,这个词的含义会发生变化,因此它的翻译也会随之改变。", "metrics": {"bleu_score": 37.52957402179448, "chrf_score": 31.180333177261705, "xcomet_score": 0.9893181324005127, "xcomet_qe_score": 0.9824478030204773, "metricx_score": 0.374392032623291, "metricx_qe_score": 0.3960738778114319, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,评估模型在处理这类情况时的表现非常困难。", "metrics": {"bleu_score": 29.10708729782026, "chrf_score": 24.007307958698608, "xcomet_score": 0.9452650547027588, "xcomet_qe_score": 0.9408419728279114, "metricx_score": 1.1816174983978271, "metricx_qe_score": 0.9819419384002686, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,因为只有小部分翻译依赖于上下文,这使得像BLUE这样的语料库级指标无法捕捉到这些翻译。有", "metrics": {"bleu_score": 50.50187358909224, "chrf_score": 42.977308290143945, "xcomet_score": 0.8609476089477539, "xcomet_qe_score": 0.8513173460960388, "metricx_score": 4.261743545532227, "metricx_qe_score": 3.5587103366851807, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "些人建议对依赖上下文的翻译进行有针对性的评估,但这些资源通常依赖于领域知识和人工整理,因此只能支持有限类型的依赖上下文的翻译和有限的语言集合。", "metrics": {"bleu_score": 62.99773272265907, "chrf_score": 54.255871618370655, "xcomet_score": 0.7999223470687866, "xcomet_qe_score": 0.768383264541626, "metricx_score": 3.889378786087036, "metricx_qe_score": 3.9725873470306396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们试图回答这两个问题。", "metrics": {"bleu_score": 45.80519369844352, "chrf_score": 36.33173006044523, "xcomet_score": 0.9985342025756836, "xcomet_qe_score": 1.0, "metricx_score": 0.17629770934581757, "metricx_qe_score": 0.08587866276502609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,翻译何时需要上下文?", "metrics": {"bleu_score": 30.215132342213096, "chrf_score": 25.650350538413747, "xcomet_score": 0.9990720748901367, "xcomet_qe_score": 0.9939683675765991, "metricx_score": 0.11625271290540695, "metricx_qe_score": 0.2667749524116516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,模型在处理这些情况时的表现如何?", "metrics": {"bleu_score": 80.86627571031983, "chrf_score": 78.17325175684309, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4453682601451874, "metricx_qe_score": 0.4600134789943695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了回答第一个问题,我们首先测量了一个词在翻译过程中对上下文的依赖程度。", "metrics": {"bleu_score": 75.17852247240286, "chrf_score": 70.15993151958281, "xcomet_score": 0.9972398281097412, "xcomet_qe_score": 1.0, "metricx_score": 4.112622261047363, "metricx_qe_score": 4.038538455963135, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在之前的研究中,我们引入了CXMI作为机器翻译模型使用上下文的度量。CX", "metrics": {"bleu_score": 60.85878417368526, "chrf_score": 59.16476016774933, "xcomet_score": 0.7448863983154297, "xcomet_qe_score": 0.7153315544128418, "metricx_score": 5.177389621734619, "metricx_qe_score": 2.790705919265747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "MI通过测量上下文C给定源X的情况下,对目标Y提供了多少信息来实现这一目标。你可以将CXMI视为给模型提供上下文所获得的信息。", "metrics": {"bleu_score": 52.20826428147403, "chrf_score": 46.14765711824914, "xcomet_score": 0.7158655524253845, "xcomet_qe_score": 0.7168863415718079, "metricx_score": 4.840844631195068, "metricx_qe_score": 5.165658473968506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项研究中,我们将CXMI扩展为逐点CXMI,可以测量句子级或词级上下文的使用情况。", "metrics": {"bleu_score": 32.974854403123324, "chrf_score": 30.492608501502637, "xcomet_score": 0.7944238781929016, "xcomet_qe_score": 0.8376027345657349, "metricx_score": 1.691667079925537, "metricx_qe_score": 1.9873855113983154, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们可以将PCXMI高的词视为需要上下文进行翻译的词。", "metrics": {"bleu_score": 84.1354400365363, "chrf_score": 74.44736085128784, "xcomet_score": 0.9077746868133545, "xcomet_qe_score": 0.9067944288253784, "metricx_score": 1.1967768669128418, "metricx_qe_score": 2.0331075191497803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们分析PCXMI高的词,寻找这些词之间的模式。", "metrics": {"bleu_score": 31.32725997036513, "chrf_score": 28.83268936257584, "xcomet_score": 0.939948558807373, "xcomet_qe_score": 0.8742880821228027, "metricx_score": 1.5252203941345215, "metricx_qe_score": 2.2651522159576416, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们对从英语翻译成14种不同语言的TED演讲的转录本进行了分析。我们的", "metrics": {"bleu_score": 58.32012039914727, "chrf_score": 65.97195252561286, "xcomet_score": 0.7027437686920166, "xcomet_qe_score": 0.7137255668640137, "metricx_score": 5.943755149841309, "metricx_qe_score": 0.9756863117218018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "分析在三个不同的层面上进行。", "metrics": {"bleu_score": 71.45126229662958, "chrf_score": 68.62234982590387, "xcomet_score": 0.9213616251945496, "xcomet_qe_score": 0.8854556679725647, "metricx_score": 0.3949405550956726, "metricx_qe_score": 0.4184066653251648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们查看了PCXMI平均值高的词性标注。", "metrics": {"bleu_score": 44.89771072202119, "chrf_score": 50.60484674802087, "xcomet_score": 0.8475068211555481, "xcomet_qe_score": 0.7881135940551758, "metricx_score": 1.6309527158737183, "metricx_qe_score": 1.65545654296875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够找到例如,阿拉伯语中的双重代词具有相对高的PCXMI。", "metrics": {"bleu_score": 44.908009587176494, "chrf_score": 39.05494791792067, "xcomet_score": 0.7618247270584106, "xcomet_qe_score": 0.7586231231689453, "metricx_score": 4.775110721588135, "metricx_qe_score": 4.26908016204834, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可以解释为英语没有双重代词,因此在翻译成阿拉伯语时,你需要上下文来确定代词是否是双重代词。", "metrics": {"bleu_score": 58.53711090040433, "chrf_score": 51.09177249366523, "xcomet_score": 0.7941369414329529, "xcomet_qe_score": 0.978880763053894, "metricx_score": 2.3559486865997314, "metricx_qe_score": 1.9591140747070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现某些语言在选择适当的动词形式时也需要上下文。", "metrics": {"bleu_score": 91.63140145331569, "chrf_score": 90.37654905758356, "xcomet_score": 0.9986207485198975, "xcomet_qe_score": 0.9910346269607544, "metricx_score": 0.5932432413101196, "metricx_qe_score": 0.8257213234901428, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们查看了在所有不同出现情况下PCSXMI平均值高的词汇项目。", "metrics": {"bleu_score": 32.73438058221482, "chrf_score": 29.314538543868547, "xcomet_score": 0.8308007717132568, "xcomet_qe_score": 0.8976055383682251, "metricx_score": 4.769946098327637, "metricx_qe_score": 4.605819225311279, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这有助于我们识别出像这里的情况,即在中文中,你需要上下文来翻译专有名词,以确保在文档中使用相同的翻译。", "metrics": {"bleu_score": 42.848737023090074, "chrf_score": 36.48337492567178, "xcomet_score": 0.762621283531189, "xcomet_qe_score": 0.8756325840950012, "metricx_score": 0.9963012933731079, "metricx_qe_score": 1.3397928476333618, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同样,我们发现上下文有助于以正确的正式程度进行翻译。", "metrics": {"bleu_score": 32.655168735068756, "chrf_score": 31.5897714849172, "xcomet_score": 0.9177067279815674, "xcomet_qe_score": 0.9073933362960815, "metricx_score": 0.8722605109214783, "metricx_qe_score": 0.9754543304443359, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们查看了PCXMI高的不同个体标记。", "metrics": {"bleu_score": 18.609621195498054, "chrf_score": 22.641200011545372, "xcomet_score": 0.8120760917663574, "xcomet_qe_score": 0.784856379032135, "metricx_score": 4.332655906677246, "metricx_qe_score": 3.351781129837036, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使我们能够识别出无法真正由词本身捕捉到的现象,而是通过句子结构表达的,例如省略的解析。", "metrics": {"bleu_score": 40.6225298379429, "chrf_score": 35.898389822250515, "xcomet_score": 0.8228579759597778, "xcomet_qe_score": 0.7765946388244629, "metricx_score": 2.035457134246826, "metricx_qe_score": 2.4448161125183105, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,现在我们使用分析结果来设计一个文档级翻译的基准。", "metrics": {"bleu_score": 66.95960204886555, "chrf_score": 61.16172313998401, "xcomet_score": 0.9744327068328857, "xcomet_qe_score": 0.8490948677062988, "metricx_score": 0.9771838784217834, "metricx_qe_score": 1.2843420505523682, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于我们确定的五个语篇现象,我们创建了标记器来自动识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。", "metrics": {"bleu_score": 10.566760570252708, "chrf_score": 25.162909106300685, "xcomet_score": 0.13431969285011292, "xcomet_qe_score": 0.04646250605583191, "metricx_score": 19.418296813964844, "metricx_qe_score": 13.968262672424316, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。", "metrics": {"bleu_score": 3.3554444284428127, "chrf_score": 10.14819646862569, "xcomet_score": 0.11850640177726746, "xcomet_qe_score": 0.1245117262005806, "metricx_score": 22.16977882385254, "metricx_qe_score": 21.888999938964844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。", "metrics": {"bleu_score": 0.8392025222832865, "chrf_score": 4.236536281019135, "xcomet_score": 0.11630929261445999, "xcomet_qe_score": 0.12076908349990845, "metricx_score": 24.76240348815918, "metricx_qe_score": 22.154573440551758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。", "metrics": {"bleu_score": 1.3943588215891367, "chrf_score": 6.8642114487519335, "xcomet_score": 0.11711475253105164, "xcomet_qe_score": 0.11905785650014877, "metricx_score": 25.0, "metricx_qe_score": 17.248384475708008, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。然后,我们可以使用标记器来识别与现象相关的词,我们称我们的标记器为多语言语篇意识(MUDA)标记器。最后,我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译中的表现。", "metrics": {"bleu_score": 6.133897666574026, "chrf_score": 19.826538742536936, "xcomet_score": -0.0019351528026163578, "xcomet_qe_score": -0.01926681585609913, "metricx_score": 23.270156860351562, "metricx_qe_score": 22.408432006835938, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,当我们使用语料库级指标时,我们发现无上下文模型的性能最好,但", "metrics": {"bleu_score": 34.558917080133, "chrf_score": 29.12166105990189, "xcomet_score": 0.639592170715332, "xcomet_qe_score": 0.6248331069946289, "metricx_score": 6.034027099609375, "metricx_qe_score": 3.8733792304992676, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用COMET,有上下文意识的模型表现最好。", "metrics": {"bleu_score": 47.29494172993879, "chrf_score": 49.249979432864876, "xcomet_score": 0.9077503681182861, "xcomet_qe_score": 0.8869010806083679, "metricx_score": 1.4838392734527588, "metricx_qe_score": 1.7944532632827759, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果我们使用word-f度量,那么有上下文和无上下文的模型具有可比的性能。", "metrics": {"bleu_score": 47.43107045832291, "chrf_score": 38.88743162656625, "xcomet_score": 0.8644931316375732, "xcomet_qe_score": 0.7876476049423218, "metricx_score": 4.426565647125244, "metricx_qe_score": 3.716074228286743, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这再次表明,如果我们仅使用语料库级指标,就很难确定最佳的文档级翻译系统。", "metrics": {"bleu_score": 84.99508493439812, "chrf_score": 78.52778542176075, "xcomet_score": 0.9937539100646973, "xcomet_qe_score": 0.9835736751556396, "metricx_score": 0.7764199376106262, "metricx_qe_score": 0.8549565076828003, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,我们使用MUDA基准来评估模型,我们发现有上下文意识的模型在某些语篇现象(如正式性和词汇连贯性)中比不使用上下文的模型更准确。", "metrics": {"bleu_score": 44.43970108653395, "chrf_score": 40.64772164795287, "xcomet_score": 0.8568462133407593, "xcomet_qe_score": 0.9199036359786987, "metricx_score": 2.163769483566284, "metricx_qe_score": 2.633431911468506, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但这些模型在其他现象(如省略、代词和动词形式)上并没有比不使用上下文的模型表现得更好。", "metrics": {"bleu_score": 60.670480225966585, "chrf_score": 52.84549595425637, "xcomet_score": 0.9861314296722412, "xcomet_qe_score": 0.9732663631439209, "metricx_score": 0.773665189743042, "metricx_qe_score": 1.1120119094848633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,这表明了我们需要在文档级翻译方面取得更多进展。", "metrics": {"bleu_score": 44.9513927941771, "chrf_score": 40.6913819958533, "xcomet_score": 0.9861156940460205, "xcomet_qe_score": 0.9733648300170898, "metricx_score": 0.8088107109069824, "metricx_qe_score": 0.8674619197845459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还比较了不同的商业系统,我们的基准显示DeepL通常比Google Translate在文档级翻译中更准确。", "metrics": {"bleu_score": 57.29455609248532, "chrf_score": 50.740957748774996, "xcomet_score": 0.9052768349647522, "xcomet_qe_score": 0.8332747220993042, "metricx_score": 1.9687060117721558, "metricx_qe_score": 2.372387409210205, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总结一下,我们对14对语言对进行了数据驱动的分析,以确定何时需要上下文进行翻译。然后,我们使用我们的发现建立了一个文档级机器翻译的基准,这有助于我们确定模型可以很好地处理哪些语篇现象,以及哪些翻译系统擅长文档级翻译。", "metrics": {"bleu_score": 56.955857307244635, "chrf_score": 51.273389992307905, "xcomet_score": 0.8604648113250732, "xcomet_qe_score": 0.8561474084854126, "metricx_score": 3.327148914337158, "metricx_qe_score": 3.73976469039917, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢您的聆听。", "metrics": {"bleu_score": 35.49481056010054, "chrf_score": 30.038060761124484, "xcomet_score": 0.9897139072418213, "xcomet_qe_score": 0.9815404415130615, "metricx_score": 0.34121468663215637, "metricx_qe_score": 0.7347935438156128, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在多伦多见!", "metrics": {"bleu_score": 54.44460596606694, "chrf_score": 41.95175911936111, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.4935094714164734, "metricx_qe_score": 1.1980928182601929, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Yanis Lavrac,我将向大家介绍我们在 Dr. BERT 上的工作,这是一个针对法语生物医学和临床领域的强大预训练模型。", "metrics": {"bleu_score": 44.07461542333101, "chrf_score": 45.69124553567165, "xcomet_score": 0.7144511342048645, "xcomet_qe_score": 0.6750150918960571, "metricx_score": 2.5310513973236084, "metricx_qe_score": 2.2617859840393066, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在本次演讲中,我们首先讨论医疗保健中的语言建模。", "metrics": {"bleu_score": 56.50858546816402, "chrf_score": 44.42216569470257, "xcomet_score": 0.9920666217803955, "xcomet_qe_score": 0.9867169857025146, "metricx_score": 0.4082871079444885, "metricx_qe_score": 0.5630125403404236, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们将介绍我们文章的主要贡献。", "metrics": {"bleu_score": 85.78928092681431, "chrf_score": 83.23737400943281, "xcomet_score": 0.9876642227172852, "xcomet_qe_score": 0.9865231513977051, "metricx_score": 0.42767441272735596, "metricx_qe_score": 0.7812209725379944, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们介绍了第一个法语生物医学模型,名为 Dr. BERT,它基于 Roberta,并在 NACHOS 上进行训练,NACHOS 是一个从网络上抓取的医学数据集。", "metrics": {"bleu_score": 49.46143728966763, "chrf_score": 48.493756919496136, "xcomet_score": 0.7691816091537476, "xcomet_qe_score": 0.6730194091796875, "metricx_score": 1.9315118789672852, "metricx_qe_score": 2.5249016284942627, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还介绍了多个预训练设置和数据源的模型比较。", "metrics": {"bleu_score": 84.85407897723051, "chrf_score": 80.73325619207009, "xcomet_score": 0.9782614707946777, "xcomet_qe_score": 0.962895393371582, "metricx_score": 0.752806544303894, "metricx_qe_score": 1.1055766344070435, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,我们展示了我们在法语的 11 个生物医学和临床下游任务上的结果。", "metrics": {"bleu_score": 53.40433934239533, "chrf_score": 46.95834416405583, "xcomet_score": 0.7291615009307861, "xcomet_qe_score": 0.7486602067947388, "metricx_score": 2.494290590286255, "metricx_qe_score": 4.517582416534424, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们总结了实验,并向大家提供了更多关于如何访问这些模型的详细信息。法语的生物医学和临床下游任务。最后,我们总结了实验,并向大家提供了更多关于如何访问这些模型的详细信息。", "metrics": {"bleu_score": 26.77505290515718, "chrf_score": 43.18997497220766, "xcomet_score": 0.30658233165740967, "xcomet_qe_score": 0.2674083113670349, "metricx_score": 17.029695510864258, "metricx_qe_score": 17.516464233398438, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "自 2018 年发布以来,BERT 已成为解决自然语言处理任务的最有效方法之一,相比历史上的静态和上下文方法(如 Word2Vct、fast text 或 NWO),BERT 提供了巨大的性能提升。", "metrics": {"bleu_score": 54.4080601730373, "chrf_score": 52.35895042495876, "xcomet_score": 0.8829977512359619, "xcomet_qe_score": 0.761020839214325, "metricx_score": 4.051090717315674, "metricx_qe_score": 4.365056037902832, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从那时起,该模型已被适应到许多其他语言,如法语中的 Camembert,以及其他领域如生物医学中的 permit-bert 和 bio-bert,以及临床中的 clinical-bert,但大多数是英语。", "metrics": {"bleu_score": 25.617167402062076, "chrf_score": 23.671924563787698, "xcomet_score": 0.5187585353851318, "xcomet_qe_score": 0.4780053198337555, "metricx_score": 9.122116088867188, "metricx_qe_score": 8.938861846923828, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其他语言的专用模型很少,并且通常基于持续预训练,因为缺乏领域内的数据。然而,到目前为", "metrics": {"bleu_score": 28.89262658758602, "chrf_score": 27.234997017309304, "xcomet_score": 0.2225034236907959, "xcomet_qe_score": 0.280472993850708, "metricx_score": 6.042266845703125, "metricx_qe_score": 5.189151287078857, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "止,法语还没有任何开源的生物医学模型。", "metrics": {"bleu_score": 25.054054220718616, "chrf_score": 23.13344128823431, "xcomet_score": 0.5604047775268555, "xcomet_qe_score": 0.4222710430622101, "metricx_score": 5.835408687591553, "metricx_qe_score": 6.578770637512207, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以我们问自己,对于广泛的用途,最合适的 数据来源是什么?当前的数据是否可以很好地替代临床数据", "metrics": {"bleu_score": 17.302302863382902, "chrf_score": 17.576615290645776, "xcomet_score": 0.8641570806503296, "xcomet_qe_score": 0.863534688949585, "metricx_score": 2.4098572731018066, "metricx_qe_score": 2.564760684967041, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "?为了回答这个问题,我们比较了 dr bert 和我们的 schubert 模型,后者基于我们医院非大学医院获得的匿名数据。", "metrics": {"bleu_score": 42.97499657399728, "chrf_score": 30.52888927379882, "xcomet_score": 0.5319395065307617, "xcomet_qe_score": 0.49478423595428467, "metricx_score": 7.503974437713623, "metricx_qe_score": 7.266966342926025, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,我们问自己,我们需要多少数据来训练一个专门针对法语数据的模型?", "metrics": {"bleu_score": 45.10783522612743, "chrf_score": 44.973405252214704, "xcomet_score": 0.9909999370574951, "xcomet_qe_score": 0.9147430062294006, "metricx_score": 0.626907229423523, "metricx_qe_score": 0.6488634943962097, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "是 4 GB、8 GB 还是 4 GB", "metrics": {"bleu_score": 9.980099403873663, "chrf_score": 70.6361693861694, "xcomet_score": 0.6118179559707642, "xcomet_qe_score": 0.5409423112869263, "metricx_score": 5.26104211807251, "metricx_qe_score": 5.075445175170898, "linguapy_score": [1, "YORUBA"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的 RAM?Schubert 的第一个版本是一个临床模型,包含 4 GB 的从临床笔记中提取的句子。Schubert 的最终版本则混合了 4 GB 的 NACHOS 子集和 4 GB 的临床笔记。", "metrics": {"bleu_score": 17.73051516354405, "chrf_score": 26.343167745293528, "xcomet_score": 0.28742295503616333, "xcomet_qe_score": 0.23070640861988068, "metricx_score": 14.328563690185547, "metricx_qe_score": 17.37354850769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "除了这个比较,我们还介绍了三个基于持续预训练的模型,以分析预训练策略的影响。", "metrics": {"bleu_score": 47.38433884322971, "chrf_score": 40.98575885413362, "xcomet_score": 0.8991377353668213, "xcomet_qe_score": 0.945838212966919, "metricx_score": 1.0575565099716187, "metricx_qe_score": 1.148289680480957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "一个基于 Camembert 的权重,并在 4 GB 的 NACHOS 子集上进行训练。", "metrics": {"bleu_score": 33.232177395586454, "chrf_score": 56.1389614882262, "xcomet_score": 0.5282166004180908, "xcomet_qe_score": 0.5614558458328247, "metricx_score": 4.2898640632629395, "metricx_qe_score": 6.368385314941406, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "另一个也是基于 Camembert,但训练了四", "metrics": {"bleu_score": 1.9412498314218407, "chrf_score": 11.500220156820914, "xcomet_score": 0.24973297119140625, "xcomet_qe_score": 0.15426354110240936, "metricx_score": 20.758869171142578, "metricx_qe_score": 17.265268325805664, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "个吉字", "metrics": {"bleu_score": 0.0, "chrf_score": 3.875968992248062, "xcomet_score": 0.15423423051834106, "xcomet_qe_score": 0.13824601471424103, "metricx_score": 9.89254093170166, "metricx_qe_score": 13.614354133605957, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "节的 permit belt by bio birth 和 clinical birth。", "metrics": {"bleu_score": 0.12995750452303087, "chrf_score": 1.037344398340249, "xcomet_score": 0.1380477249622345, "xcomet_qe_score": 0.12938526272773743, "metricx_score": 25.0, "metricx_qe_score": 25.0, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果显示... 38 GB,Camembert Oscar 4 GB,Camembert CCnet 4 GB,Pumatbert,BioBERT 和 ClinicalBERT。", "metrics": {"bleu_score": 6.642589091602322, "chrf_score": 29.658089605902198, "xcomet_score": 0.15842153131961823, "xcomet_qe_score": 0.18012379109859467, "metricx_score": 15.237448692321777, "metricx_qe_score": 15.698873519897461, "linguapy_score": [1, "ENGLISH"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "评估结果显示,模型在与训练数据性质相同的任务上表现最佳。", "metrics": {"bleu_score": 31.682174122857866, "chrf_score": 26.460569549233004, "xcomet_score": 0.9959069490432739, "xcomet_qe_score": 0.992480993270874, "metricx_score": 0.8937663435935974, "metricx_qe_score": 1.0608936548233032, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们可以观察到,来自异构来源的数据似乎更具通用性。", "metrics": {"bleu_score": 54.142878365294976, "chrf_score": 47.86179603246076, "xcomet_score": 0.9836294651031494, "xcomet_qe_score": 0.8358259201049805, "metricx_score": 0.8315171003341675, "metricx_qe_score": 0.9072832465171814, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,使用更多的数据可以带来更好的性能。", "metrics": {"bleu_score": 52.6589137558171, "chrf_score": 45.93989530077461, "xcomet_score": 0.9362239837646484, "xcomet_qe_score": 0.9736031889915466, "metricx_score": 2.49906587600708, "metricx_qe_score": 2.952756404876709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总的来说,从头开始的预训练似乎在大多数任务上都能获得更高的性能。", "metrics": {"bleu_score": 60.585476032721175, "chrf_score": 55.30840655794308, "xcomet_score": 0.957389235496521, "xcomet_qe_score": 0.9545341730117798, "metricx_score": 2.6987171173095703, "metricx_qe_score": 3.362440586090088, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,我们对持续预训练的实验,使用 Permet-BERT 的权重和分词器,并在 4 GB 的 NACHOS 子集上进行训练,显示的结果与 Dr.BERT 4 GB 从头开始的结果相当,", "metrics": {"bleu_score": 29.53565612684395, "chrf_score": 39.766317846298456, "xcomet_score": 0.35634666681289673, "xcomet_qe_score": 0.3492533266544342, "metricx_score": 5.956210136413574, "metricx_qe_score": 5.797147750854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但基于 Camembert 权重和分词器的模型则存在稳定性问题。最后,作为结论,我们的适当系统在 11 个 don't-trim 任务中的 9 个任务上表现更好,并且在全球范围内超越了通", "metrics": {"bleu_score": 16.189162676648245, "chrf_score": 25.426478856299102, "xcomet_score": 0.22805683314800262, "xcomet_qe_score": 0.2213134914636612, "metricx_score": 8.183812141418457, "metricx_qe_score": 7.368220806121826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "用模型的结果。总结,我们的适当系统在 11 个 don't-trim 任务中的 9 个任务上表现更好,并且在全球范围内超越了通用模型(这里指的是 Camembert)的结果。", "metrics": {"bleu_score": 41.306473588478525, "chrf_score": 35.49428135459052, "xcomet_score": 0.31564268469810486, "xcomet_qe_score": 0.19695982336997986, "metricx_score": 7.941239833831787, "metricx_qe_score": 7.699989318847656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还观察到,更专业的 数据更好,但它并没有很好地扩展。", "metrics": {"bleu_score": 23.301482433033094, "chrf_score": 20.699608970843435, "xcomet_score": 0.7826316356658936, "xcomet_qe_score": 0.7605681419372559, "metricx_score": 3.6033878326416016, "metricx_qe_score": 4.912106037139893, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从 NACHOS 获得的所有预训练模型都可以在 UGIMFACE 上免费获得,所有的训练脚本都在我们的 GitHub 仓库中。", "metrics": {"bleu_score": 46.046594908560614, "chrf_score": 44.09960703117164, "xcomet_score": 0.8406282663345337, "xcomet_qe_score": 0.8550458550453186, "metricx_score": 5.489014148712158, "metricx_qe_score": 5.964101791381836, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,感谢大家的聆听,我们期待在多伦多的海报环节与大家互动。", "metrics": {"bleu_score": 26.039652524962975, "chrf_score": 29.825890414139472, "xcomet_score": 0.8383099436759949, "xcomet_qe_score": 0.8543298840522766, "metricx_score": 1.8682523965835571, "metricx_qe_score": 1.6386797428131104, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.958304762840271, "xcomet_qe_score": 0.9632421731948853, "metricx_score": 0.26475995779037476, "metricx_qe_score": 0.28221702575683594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我叫马蒂亚斯·林德曼(Matthias Lindemann),今天我将向大家简要介绍我们关于在不使用树的情况下通过多集标记和潜在置换实现组合泛化的论文。", "metrics": {"bleu_score": 28.55335904906915, "chrf_score": 48.009519783028516, "xcomet_score": 0.9567557573318481, "xcomet_qe_score": 0.9525930881500244, "metricx_score": 1.7333710193634033, "metricx_qe_score": 1.843563199043274, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是我和我的导师亚历山大·科勒(Alexander Koller)和伊万·蒂托夫(Ivan Titov)的合作成果。", "metrics": {"bleu_score": 7.382806265053332, "chrf_score": 53.29493933482531, "xcomet_score": 0.9898887872695923, "xcomet_qe_score": 0.9166766405105591, "metricx_score": 1.2268471717834473, "metricx_qe_score": 1.3256882429122925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "组合泛化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语的未见组合的能力。", "metrics": {"bleu_score": 89.28756684056039, "chrf_score": 88.25509425421855, "xcomet_score": 0.8040792942047119, "xcomet_qe_score": 0.6904211640357971, "metricx_score": 4.653682708740234, "metricx_qe_score": 6.901952743530273, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在语义解析的背景下,测试组合泛化可能如下所示。", "metrics": {"bleu_score": 70.07637953409846, "chrf_score": 60.13480369940909, "xcomet_score": 0.908198356628418, "xcomet_qe_score": 0.8913741707801819, "metricx_score": 0.9897307753562927, "metricx_qe_score": 1.7725532054901123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "像往常一样,我们有一个训练语料集,", "metrics": {"bleu_score": 32.934059711691795, "chrf_score": 25.881769374416436, "xcomet_score": 0.9171894788742065, "xcomet_qe_score": 0.9566779136657715, "metricx_score": 1.292626976966858, "metricx_qe_score": 1.2682772874832153, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这种情况下是“女孩睡觉”和", "metrics": {"bleu_score": 8.403805630797752, "chrf_score": 5.403102088040125, "xcomet_score": 0.570780336856842, "xcomet_qe_score": 0.7728493213653564, "metricx_score": 7.483737468719482, "metricx_qe_score": 3.896394729614258, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "“玛丽知道女孩睡觉”。这些", "metrics": {"bleu_score": 13.119387134268873, "chrf_score": 8.937253078402453, "xcomet_score": 0.7989816665649414, "xcomet_qe_score": 0.6753296852111816, "metricx_score": 6.211026668548584, "metricx_qe_score": 2.51780366897583, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "语料集与表示其核心意义的逻辑形式配对。", "metrics": {"bleu_score": 22.449758011137348, "chrf_score": 19.546722781053646, "xcomet_score": 0.9010288715362549, "xcomet_qe_score": 0.899565577507019, "metricx_score": 2.8516902923583984, "metricx_qe_score": 2.556307792663574, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "与标准机器学习评估不同,测试集不是来自同一分布,而是包含结构上未见过的逻辑形式。", "metrics": {"bleu_score": 59.713519396031245, "chrf_score": 53.83199367997625, "xcomet_score": 0.8625761270523071, "xcomet_qe_score": 0.8402457237243652, "metricx_score": 1.1443989276885986, "metricx_qe_score": 2.278808355331421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个例子中,模型在训练过程中已经见过更浅层次的递归,并在具有更深层次递归的例子上进行了测试。", "metrics": {"bleu_score": 24.96940022326574, "chrf_score": 25.96616556793731, "xcomet_score": 0.9451427459716797, "xcomet_qe_score": 0.8980596661567688, "metricx_score": 2.3047902584075928, "metricx_qe_score": 4.574888229370117, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "朴素的序列到序列模型难以应对这种分布外泛化,并且通常会产生与输入脱节的输出。", "metrics": {"bleu_score": 38.56060885312128, "chrf_score": 29.865953907923647, "xcomet_score": 0.7687203884124756, "xcomet_qe_score": 0.732538640499115, "metricx_score": 4.007105350494385, "metricx_qe_score": 3.6240954399108887, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "特别是,它们通常无法再现输入和输出之间的系统对应关系,例如在例子中用颜色标注的那些。", "metrics": {"bleu_score": 65.48241934154404, "chrf_score": 60.23635267537707, "xcomet_score": 0.9999712705612183, "xcomet_qe_score": 0.9998133182525635, "metricx_score": 0.8403171300888062, "metricx_qe_score": 0.8975445032119751, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "解决这个问题的一种流行方法是将树集成到模型中。", "metrics": {"bleu_score": 65.6680744925114, "chrf_score": 54.38417629493144, "xcomet_score": 0.9308148622512817, "xcomet_qe_score": 0.9095532298088074, "metricx_score": 0.9633185863494873, "metricx_qe_score": 0.9730742573738098, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "树的目的是捕捉将语料与逻辑形式联系起来的组合过程。", "metrics": {"bleu_score": 55.45773299312706, "chrf_score": 49.02964549581675, "xcomet_score": 0.876632571220398, "xcomet_qe_score": 0.7692251801490784, "metricx_score": 2.4268500804901123, "metricx_qe_score": 3.995683431625366, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是有效的,但树通常没有给出,需要以某种方式获得。", "metrics": {"bleu_score": 10.62997082616743, "chrf_score": 12.836803838328894, "xcomet_score": 0.7968857288360596, "xcomet_qe_score": 0.7866452932357788, "metricx_score": 3.0903725624084473, "metricx_qe_score": 2.821526288986206, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这可能很复杂,有时是一个计算上昂贵的过程。", "metrics": {"bleu_score": 59.7683497100428, "chrf_score": 48.9888449268651, "xcomet_score": 0.9187873601913452, "xcomet_qe_score": 0.8782783150672913, "metricx_score": 1.4186041355133057, "metricx_qe_score": 2.3331005573272705, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通常,这涉及到对逻辑形式进行相当多的形式特定预处理,例如处理变量符号。", "metrics": {"bleu_score": 38.48011076531004, "chrf_score": 33.98287390052733, "xcomet_score": 0.905728816986084, "xcomet_qe_score": 0.8943614363670349, "metricx_score": 0.9093613624572754, "metricx_qe_score": 1.5728402137756348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "获得树也可能涉及到专门的语法感应程序。", "metrics": {"bleu_score": 45.6298289344212, "chrf_score": 36.11371636466792, "xcomet_score": 0.7420464754104614, "xcomet_qe_score": 0.7251862287521362, "metricx_score": 5.883754730224609, "metricx_qe_score": 6.241732120513916, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这篇论文中,我们不使用树,而是引入了一个直接建模输入片段与输出片段之间对应关系的神经序列到序列模型。", "metrics": {"bleu_score": 42.929272441929726, "chrf_score": 32.30273894365354, "xcomet_score": 0.804424524307251, "xcomet_qe_score": 0.7407132387161255, "metricx_score": 2.2198431491851807, "metricx_qe_score": 2.965606689453125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首次展示了在没有树的情况下对更深层次递归的强大泛化能力。", "metrics": {"bleu_score": 54.964358167403724, "chrf_score": 45.75042612571825, "xcomet_score": 0.960639238357544, "xcomet_qe_score": 0.89234459400177, "metricx_score": 3.319031238555908, "metricx_qe_score": 5.1751179695129395, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的方法通过两步从输入预测输出。", "metrics": {"bleu_score": 36.33698188270103, "chrf_score": 30.77152334894233, "xcomet_score": 0.9797048568725586, "xcomet_qe_score": 0.9679811000823975, "metricx_score": 0.5878804326057434, "metricx_qe_score": 0.8280890583992004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们用一个无序的多集标记每个输入标记,这些标记将出现在输出中。", "metrics": {"bleu_score": 33.697833037090184, "chrf_score": 30.08282475087878, "xcomet_score": 0.7955111265182495, "xcomet_qe_score": 0.8287370204925537, "metricx_score": 3.3708884716033936, "metricx_qe_score": 2.8299131393432617, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "完成第一步后,我们得到了所有正确的标记,但它们没有排序。", "metrics": {"bleu_score": 41.69392927528883, "chrf_score": 34.26503247155422, "xcomet_score": 0.9182323217391968, "xcomet_qe_score": 0.8877513408660889, "metricx_score": 2.120614528656006, "metricx_qe_score": 3.2425074577331543, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这就是为什么在第二步,我们使用另一个模型来预测一个置换,将它们排列成正确的顺序。", "metrics": {"bleu_score": 46.19007456424661, "chrf_score": 45.62857602315188, "xcomet_score": 0.9115211963653564, "xcomet_qe_score": 0.9168208241462708, "metricx_score": 3.177377223968506, "metricx_qe_score": 3.2335598468780518, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一种新的方法来预测一个置换,该方法对可能的置换没有硬约束。", "metrics": {"bleu_score": 53.51754295655683, "chrf_score": 48.49508121055317, "xcomet_score": 0.8562822341918945, "xcomet_qe_score": 0.8922579884529114, "metricx_score": 4.017335891723633, "metricx_qe_score": 3.6344735622406006, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这使得我们的方法非常灵活和富有表现力。", "metrics": {"bleu_score": 49.41109918128317, "chrf_score": 42.370870320503926, "xcomet_score": 0.9872218370437622, "xcomet_qe_score": 0.9658831357955933, "metricx_score": 0.7799614667892456, "metricx_qe_score": 1.2798502445220947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "从概念上讲,我们的置换模型大致是这样工作的。", "metrics": {"bleu_score": 25.984882476296985, "chrf_score": 22.856298625860568, "xcomet_score": 0.9731236696243286, "xcomet_qe_score": 0.9644007682800293, "metricx_score": 1.2977163791656494, "metricx_qe_score": 0.8059293031692505, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从左到右遍历输出,并确定每个位置放置哪个多集标记。", "metrics": {"bleu_score": 50.897863478027745, "chrf_score": 44.134304799166074, "xcomet_score": 0.8169797658920288, "xcomet_qe_score": 0.7758876085281372, "metricx_score": 1.846040964126587, "metricx_qe_score": 2.28193998336792, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于第一个输出位置,我们简单地选择一个,如图所示。然后", "metrics": {"bleu_score": 44.18632145518061, "chrf_score": 37.68701898536689, "xcomet_score": 0.7640184164047241, "xcomet_qe_score": 0.8987369537353516, "metricx_score": 2.1800835132598877, "metricx_qe_score": 1.1116385459899902, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",我们跳到下一个多集标记,以确定输出中的第二个标记。", "metrics": {"bleu_score": 48.84411467539946, "chrf_score": 42.68549950615216, "xcomet_score": 0.7266725897789001, "xcomet_qe_score": 0.7849007844924927, "metricx_score": 4.576729774475098, "metricx_qe_score": 3.919360876083374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们以类似的方式跳到另一个多集标记。", "metrics": {"bleu_score": 21.60350910276294, "chrf_score": 23.815792922496783, "xcomet_score": 0.7083563804626465, "xcomet_qe_score": 0.7210420966148376, "metricx_score": 4.893303871154785, "metricx_qe_score": 5.635066509246826, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们继续这个过程,直到第一个阶段的所有标记都被访问过一次。", "metrics": {"bleu_score": 48.99375493335211, "chrf_score": 41.18328858772296, "xcomet_score": 0.8362568616867065, "xcomet_qe_score": 0.852790117263794, "metricx_score": 2.220627784729004, "metricx_qe_score": 3.0648136138916016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了给你们一个实验结果的预告,我们在这里将我们的方法与其他无树模型在COGS基准上进行了比较。我们的模型在", "metrics": {"bleu_score": 55.801862710820544, "chrf_score": 59.52071473554844, "xcomet_score": 0.6133192181587219, "xcomet_qe_score": 0.6327182650566101, "metricx_score": 9.145610809326172, "metricx_qe_score": 4.483275890350342, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对更深层次递归的泛化方面远远优于其他模型。", "metrics": {"bleu_score": 30.509330929961525, "chrf_score": 26.66744000977686, "xcomet_score": 0.9397636651992798, "xcomet_qe_score": 0.9321702718734741, "metricx_score": 2.8461878299713135, "metricx_qe_score": 3.5997443199157715, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,其他一些类型的结构泛化仍然非常具有挑战性。在", "metrics": {"bleu_score": 27.474558342153514, "chrf_score": 31.046896284541603, "xcomet_score": 0.7602701187133789, "xcomet_qe_score": 0.7729606628417969, "metricx_score": 3.933309316635132, "metricx_qe_score": 0.695308268070221, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的论文中,我们解决了几个有趣的技术挑战。", "metrics": {"bleu_score": 53.66411241731205, "chrf_score": 48.331950902806696, "xcomet_score": 0.9969878196716309, "xcomet_qe_score": 0.9911098480224609, "metricx_score": 0.4248282313346863, "metricx_qe_score": 0.520808219909668, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,输入和输出之间的对齐在训练数据中没有给出。", "metrics": {"bleu_score": 31.146377792658097, "chrf_score": 28.05973013627032, "xcomet_score": 0.8629755973815918, "xcomet_qe_score": 0.9026443958282471, "metricx_score": 0.5198761820793152, "metricx_qe_score": 0.5990929007530212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,对于给定的标记,我们不知道它来自哪个多集,这给训练带来了挑战。", "metrics": {"bleu_score": 65.93599128524113, "chrf_score": 58.52357014798012, "xcomet_score": 0.8352898359298706, "xcomet_qe_score": 0.7180827856063843, "metricx_score": 3.3413946628570557, "metricx_qe_score": 3.5064008235931396, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,有时有多个置换与数据一致,但语言学上正确的那个是潜在的。", "metrics": {"bleu_score": 27.822130970602192, "chrf_score": 24.256371778323953, "xcomet_score": 0.7723416090011597, "xcomet_qe_score": 0.7197504043579102, "metricx_score": 4.296369552612305, "metricx_qe_score": 3.697596549987793, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们通过在训练中诱导对齐来解决这个问题。", "metrics": {"bleu_score": 85.78928092681438, "chrf_score": 82.10182318541452, "xcomet_score": 0.9801583290100098, "xcomet_qe_score": 0.9072984457015991, "metricx_score": 0.7283324599266052, "metricx_qe_score": 0.9682132005691528, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们的置换方法非常灵活,但它带来了一个挑战,即找到得分最高的置换是NP难的。", "metrics": {"bleu_score": 50.28276527085308, "chrf_score": 39.334176286801906, "xcomet_score": 0.8157820701599121, "xcomet_qe_score": 0.8412988781929016, "metricx_score": 3.118269443511963, "metricx_qe_score": 2.2028751373291016, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这是因为这与旅行商问题有关。", "metrics": {"bleu_score": 45.06775052173921, "chrf_score": 38.706282963094665, "xcomet_score": 0.8810908198356628, "xcomet_qe_score": 0.808661937713623, "metricx_score": 0.7263143062591553, "metricx_qe_score": 1.095809817314148, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们用一种GPU友好的连续松弛方法来近似这一点,这也使我们能够通过解决方案进行反向传播并学习语言学上更可信的置换。", "metrics": {"bleu_score": 53.43395222868198, "chrf_score": 52.67246454554293, "xcomet_score": 0.706396222114563, "xcomet_qe_score": 0.5961964130401611, "metricx_score": 4.063272476196289, "metricx_qe_score": 4.385101318359375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果你想了解更多关于我们的实验以及我们如何应对这些挑战,请查看我们的论文或来我们的海报。", "metrics": {"bleu_score": 75.23766478343657, "chrf_score": 71.20100168546645, "xcomet_score": 0.8655231595039368, "xcomet_qe_score": 0.8598283529281616, "metricx_score": 3.0405545234680176, "metricx_qe_score": 3.5417046546936035, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Akshata,今天我和我的合著者 Martin 将展示我们的工作,Kipma 步骤,评估从多个来源整合知识的方法。这项", "metrics": {"bleu_score": 35.73669226557537, "chrf_score": 40.62027952185072, "xcomet_score": 0.3429374694824219, "xcomet_qe_score": 0.4669162333011627, "metricx_score": 9.193470001220703, "metricx_qe_score": 7.78891658782959, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "工作是麦吉尔大学、Mila 和微软研究院的合作项目。", "metrics": {"bleu_score": 49.74905880933081, "chrf_score": 49.347326386916336, "xcomet_score": 0.8111622333526611, "xcomet_qe_score": 0.6333651542663574, "metricx_score": 3.144479513168335, "metricx_qe_score": 3.623892307281494, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "国家语言理解模型利用各种知识来源,例如参数中包含的知识,通常通过预训练获得,以及推理时输入中给出的知识。", "metrics": {"bleu_score": 53.00816215658456, "chrf_score": 44.405833160609276, "xcomet_score": 0.7085164785385132, "xcomet_qe_score": 0.7033638954162598, "metricx_score": 4.765371799468994, "metricx_qe_score": 4.088048934936523, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最近在问答等任务中的工作表明,模型可以使用预训练的时间知识来解决任务。", "metrics": {"bleu_score": 69.48573499536025, "chrf_score": 61.729050124448605, "xcomet_score": 0.8405330181121826, "xcomet_qe_score": 0.8261600732803345, "metricx_score": 2.5620386600494385, "metricx_qe_score": 2.2960634231567383, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "但是,自然语言理解通常需要在推理时也提供知识。", "metrics": {"bleu_score": 70.00855464920407, "chrf_score": 64.68556932515742, "xcomet_score": 0.9060887694358826, "xcomet_qe_score": 0.893678605556488, "metricx_score": 2.031637668609619, "metricx_qe_score": 2.419572114944458, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,在句子“约翰在电视上看到了新当选的总统。”中,", "metrics": {"bleu_score": 36.61334309202368, "chrf_score": 21.436862844980446, "xcomet_score": 0.9869771003723145, "xcomet_qe_score": 0.9800231456756592, "metricx_score": 1.3048304319381714, "metricx_qe_score": 1.6623256206512451, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "预训练的参数可能包含关于总统做什么和电视是什么的信息,但它们无法可靠地知道约翰这个特定事件的实体是谁,或者新总统是谁,因为自预训练以来总统可能已经换了。", "metrics": {"bleu_score": 52.04543570864772, "chrf_score": 43.39521025667678, "xcomet_score": 0.7412670254707336, "xcomet_qe_score": 0.6598201990127563, "metricx_score": 4.452670097351074, "metricx_qe_score": 4.066056728363037, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,成功处理知识密集型 NLU 任务的模型需要能够整合和使用预训练时间和推理时间知识。", "metrics": {"bleu_score": 48.4522199629108, "chrf_score": 44.5139514048774, "xcomet_score": 0.9581397771835327, "xcomet_qe_score": 0.926787793636322, "metricx_score": 1.4986974000930786, "metricx_qe_score": 1.6317254304885864, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这项工作中,我们提出了一套用于知识整合的诊断测试。", "metrics": {"bleu_score": 59.5248814617268, "chrf_score": 52.83406810580723, "xcomet_score": 0.9984452724456787, "xcomet_qe_score": 0.9921361207962036, "metricx_score": 1.0447885990142822, "metricx_qe_score": 1.0787250995635986, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们引入了一个核心参照解析任务,旨在探究利用不同来源可用知识的能力。", "metrics": {"bleu_score": 55.923576682620265, "chrf_score": 48.276413782560745, "xcomet_score": 0.8672652840614319, "xcomet_qe_score": 0.8310748934745789, "metricx_score": 3.1109776496887207, "metricx_qe_score": 3.5884475708007812, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和建立的核心参照解析模型对数据集进行了评估。", "metrics": {"bleu_score": 46.0239896080497, "chrf_score": 45.9248497467483, "xcomet_score": 0.8560371398925781, "xcomet_qe_score": 0.8245218992233276, "metricx_score": 2.5252737998962402, "metricx_qe_score": 2.7894439697265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是我们数据集的一个例子。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9902970790863037, "xcomet_qe_score": 0.9727965593338013, "metricx_score": 0.21416805684566498, "metricx_qe_score": 0.30831992626190186, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin 是法官。K", "metrics": {"bleu_score": 32.159351091190125, "chrf_score": 55.565565778910965, "xcomet_score": 0.6683255434036255, "xcomet_qe_score": 0.44637441635131836, "metricx_score": 2.4939584732055664, "metricx_qe_score": 2.429265260696411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "ia 是面包师。", "metrics": {"bleu_score": 38.49815007763549, "chrf_score": 25.696254977801146, "xcomet_score": 0.7906017899513245, "xcomet_qe_score": 0.6704782247543335, "metricx_score": 4.5157036781311035, "metricx_qe_score": 5.285017013549805, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "Servin 和 Kia 在公园见面。经过一天", "metrics": {"bleu_score": 15.133218633429316, "chrf_score": 49.15302146057198, "xcomet_score": 0.3120947480201721, "xcomet_qe_score": 0.2849666476249695, "metricx_score": 6.5139899253845215, "metricx_qe_score": 5.984293460845947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在法庭上审理案件后,他很高兴放松一下。", "metrics": {"bleu_score": 51.44092602416698, "chrf_score": 41.69655782040328, "xcomet_score": 0.9207525253295898, "xcomet_qe_score": 0.8588771820068359, "metricx_score": 2.1398701667785645, "metricx_qe_score": 3.534018039703369, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这里的任务是确定代词 he 指的是哪个正确的实体,在这种情况下是 Servin。", "metrics": {"bleu_score": 37.38967312792211, "chrf_score": 41.69945184793487, "xcomet_score": 0.9417788982391357, "xcomet_qe_score": 0.8475990295410156, "metricx_score": 3.0656800270080566, "metricx_qe_score": 5.011758327484131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "给定代词的解析需要两种类型的信息。", "metrics": {"bleu_score": 17.200673466668952, "chrf_score": 17.23447712418301, "xcomet_score": 0.9989354610443115, "xcomet_qe_score": 0.9930803775787354, "metricx_score": 0.9035751819610596, "metricx_qe_score": 0.9234095811843872, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,实体特定知识,例如 Servin 是法官。其", "metrics": {"bleu_score": 12.633096024854089, "chrf_score": 30.21785618060498, "xcomet_score": 0.6707634329795837, "xcomet_qe_score": 0.6399166584014893, "metricx_score": 5.051385402679443, "metricx_qe_score": 2.287275552749634, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "次,背景知识,例如法官在法庭上审理案件。一般", "metrics": {"bleu_score": 32.063710596528935, "chrf_score": 27.180344375950593, "xcomet_score": 0.6756254434585571, "xcomet_qe_score": 0.6794013977050781, "metricx_score": 6.654747009277344, "metricx_qe_score": 4.053287506103516, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "来说,背景知识是在大型语言模型的预训练过程中学习的,而实体特定知识通常在推理时观察到。", "metrics": {"bleu_score": 59.60407791642014, "chrf_score": 53.24136675905806, "xcomet_score": 0.5711085796356201, "xcomet_qe_score": 0.44489753246307373, "metricx_score": 3.688929319381714, "metricx_qe_score": 4.4853315353393555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们改变这两种信息的可用性,使其可能只在一个来源中找到,或者在多个来源中找到。", "metrics": {"bleu_score": 54.35105813781786, "chrf_score": 57.39201256105774, "xcomet_score": 0.8982186317443848, "xcomet_qe_score": 0.8188700079917908, "metricx_score": 1.1157541275024414, "metricx_qe_score": 1.216607928276062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们定义了三个 KITMOS 设置。", "metrics": {"bleu_score": 44.12739850976206, "chrf_score": 35.06473661489538, "xcomet_score": 0.8642474412918091, "xcomet_qe_score": 0.9014833569526672, "metricx_score": 1.3010443449020386, "metricx_qe_score": 1.0519294738769531, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们有典型的设置,背景预训练,其中假设背景知识在预训练时可用。", "metrics": {"bleu_score": 44.09811209903347, "chrf_score": 38.53068631603377, "xcomet_score": 0.8760889172554016, "xcomet_qe_score": 0.8290504217147827, "metricx_score": 2.2632694244384766, "metricx_qe_score": 3.960338592529297, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,有一个背景两者设置,其中背景知识在预训练时间和推理时间都可用。", "metrics": {"bleu_score": 52.121690544366025, "chrf_score": 44.71445355602528, "xcomet_score": 0.820340633392334, "xcomet_qe_score": 0.7450869083404541, "metricx_score": 1.748802900314331, "metricx_qe_score": 2.9491028785705566, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,背景推理设置,其中两种知识类型只在推理时可用。", "metrics": {"bleu_score": 51.06566332904315, "chrf_score": 45.03131520023326, "xcomet_score": 0.9167413711547852, "xcomet_qe_score": 0.8548986911773682, "metricx_score": 0.8295727968215942, "metricx_qe_score": 0.8966240882873535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后一个设置特别有趣,因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分的情况,", "metrics": {"bleu_score": 85.88886238396086, "chrf_score": 82.36833954163177, "xcomet_score": 0.9799655675888062, "xcomet_qe_score": 0.9657496809959412, "metricx_score": 0.7547972202301025, "metricx_qe_score": 1.024795651435852, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,自预训练以来出现了新的职业。", "metrics": {"bleu_score": 75.40757404706284, "chrf_score": 73.84209831000112, "xcomet_score": 0.8662011027336121, "xcomet_qe_score": 0.8486496806144714, "metricx_score": 1.5278007984161377, "metricx_qe_score": 2.3476247787475586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是关于我们如何控制事实和真实来源可用性的一个例子。", "metrics": {"bleu_score": 44.52646687663626, "chrf_score": 36.290476788659795, "xcomet_score": 0.8159047365188599, "xcomet_qe_score": 0.8155645132064819, "metricx_score": 1.6673985719680786, "metricx_qe_score": 1.6158671379089355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景预训练设置中,我们假设背景知识政治家寻求政府选举席位包含在预训练参数中。而在 3 英寸时间范围内,我们提供了特定于 Chichester 的知识,即 Chichester 是政治家。", "metrics": {"bleu_score": 34.06530395111483, "chrf_score": 35.09762258463817, "xcomet_score": 0.3826069235801697, "xcomet_qe_score": 0.3573276400566101, "metricx_score": 5.9589033126831055, "metricx_qe_score": 6.5880866050720215, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景两者设置中,我们还提供了不仅是特定于实体的知识,还有推理时间范围内关于政治家的背景知识。", "metrics": {"bleu_score": 33.494713037138546, "chrf_score": 30.879178142155716, "xcomet_score": 0.6859849095344543, "xcomet_qe_score": 0.616644024848938, "metricx_score": 2.3571066856384277, "metricx_qe_score": 3.043943166732788, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在背景推理设置中,我们提供了虚构职业 Meritur 而不是政治家,因为 Meritur 不太可能包含在预训练参数中。", "metrics": {"bleu_score": 47.19073280328193, "chrf_score": 39.60848347648301, "xcomet_score": 0.6269757747650146, "xcomet_qe_score": 0.48957228660583496, "metricx_score": 4.409373760223389, "metricx_qe_score": 7.130561351776123, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用人类研究参与者和建立的核心参照解析模型对数据集进行了评估。", "metrics": {"bleu_score": 46.0239896080497, "chrf_score": 45.9248497467483, "xcomet_score": 0.8564963340759277, "xcomet_qe_score": 0.8354074954986572, "metricx_score": 2.6163783073425293, "metricx_qe_score": 2.779569625854492, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这个图中,我们展示了在背景预训练设置的最困难变体上表现最好的模型的结果。", "metrics": {"bleu_score": 39.29537309787847, "chrf_score": 33.49981225995523, "xcomet_score": 0.9326961040496826, "xcomet_qe_score": 0.7575231790542603, "metricx_score": 1.2791924476623535, "metricx_qe_score": 1.5824352502822876, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "没有在 KITMOS 上进行任务特定训练,两个模型的表现都不好。", "metrics": {"bleu_score": 21.93561620806055, "chrf_score": 34.58396087923699, "xcomet_score": 0.9035670757293701, "xcomet_qe_score": 0.8935465216636658, "metricx_score": 1.9847198724746704, "metricx_qe_score": 1.9934812784194946, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当在 KITMOS 上进行训练时,C2F 和 BFQF 的表现都显著优于随机选择。", "metrics": {"bleu_score": 22.724148131567475, "chrf_score": 27.238757223139327, "xcomet_score": 0.7675769329071045, "xcomet_qe_score": 0.7609986066818237, "metricx_score": 5.650506973266602, "metricx_qe_score": 5.842545509338379, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这表明,当在一般核心参照解析数据集上进行训练时,模型学会利用表面线索,这些线索在测试 KITMOS 时已被移除,因此对模型无用。", "metrics": {"bleu_score": 31.27335299096112, "chrf_score": 28.419950583561455, "xcomet_score": 0.7990912199020386, "xcomet_qe_score": 0.6948991417884827, "metricx_score": 5.329859733581543, "metricx_qe_score": 4.709249973297119, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "使用虚构知识的额外实验表明,即使是表现最好的模型也无法可靠地整合仅在推理时提供的背景知识。总结", "metrics": {"bleu_score": 70.37478884644399, "chrf_score": 63.86829799926035, "xcomet_score": 0.8092541694641113, "xcomet_qe_score": 0.8506276607513428, "metricx_score": 3.7886226177215576, "metricx_qe_score": 1.3010741472244263, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们论文的主要结论。许多核心参照解析模型似乎无法在没有任务特定训练的情况下对来自不同来源的知识进行推理。", "metrics": {"bleu_score": 72.30686083990469, "chrf_score": 67.6663837494169, "xcomet_score": 0.8149520754814148, "xcomet_qe_score": 0.8324177265167236, "metricx_score": 3.928103446960449, "metricx_qe_score": 3.891880512237549, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,通过任务特定训练,一些模型成功地整合了来自多个来源的知识。", "metrics": {"bleu_score": 68.89441978938515, "chrf_score": 65.32271200762672, "xcomet_score": 0.9552958011627197, "xcomet_qe_score": 0.9304778575897217, "metricx_score": 0.822296142578125, "metricx_qe_score": 1.249112606048584, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "尽管如此,即使是表现最好的模型似乎也难以可靠地整合仅在推理时呈现的背景知识。", "metrics": {"bleu_score": 70.53571147273676, "chrf_score": 63.53974156265348, "xcomet_score": 0.9674593210220337, "xcomet_qe_score": 0.9214279055595398, "metricx_score": 1.3495759963989258, "metricx_qe_score": 0.9250243902206421, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您对更多细节感兴趣,请参阅我们的论文并在 GitHub 上查看数据集。感谢大家的", "metrics": {"bleu_score": 50.554127223634346, "chrf_score": 52.06535100451006, "xcomet_score": 0.7568198442459106, "xcomet_qe_score": 0.7374789714813232, "metricx_score": 2.7431468963623047, "metricx_qe_score": 1.161607265472412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 12.703318703865365, "chrf_score": 8.0, "xcomet_score": 0.8839613199234009, "xcomet_qe_score": 0.8593108057975769, "metricx_score": 0.1393444538116455, "metricx_qe_score": 0.2242259979248047, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是 Myra,今天我要谈谈我们的论文《标记化角色:使用自然语言提示来衡量语言模型中的刻板印象》。", "metrics": {"bleu_score": 70.00122888931357, "chrf_score": 65.14178150802054, "xcomet_score": 0.8818067312240601, "xcomet_qe_score": 0.7304674983024597, "metricx_score": 1.551948070526123, "metricx_qe_score": 2.057882070541382, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这项工作是与 Esen Dermusch 和 Dan Jorofsky 合作完成的。", "metrics": {"bleu_score": 32.35470999590687, "chrf_score": 40.74615762200711, "xcomet_score": 0.7946888208389282, "xcomet_qe_score": 0.8171875476837158, "metricx_score": 4.412143707275391, "metricx_qe_score": 3.961972713470459, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "近年来,许多研究已经记录了大型语言模型或 LLM 中社会偏见和刻板印象的普遍存在。", "metrics": {"bleu_score": 26.98093681892288, "chrf_score": 26.71908379916782, "xcomet_score": 0.9400266408920288, "xcomet_qe_score": 0.8907274603843689, "metricx_score": 2.2827353477478027, "metricx_qe_score": 3.5119316577911377, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法有各种局限性。", "metrics": {"bleu_score": 25.274266580956596, "chrf_score": 23.47120384035192, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.3085094690322876, "metricx_qe_score": 0.2788558304309845, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "它们通常依赖于非常耗时的手工构建数据集,而且通常只测量非常具体的刻板印象,这意味着它们不能很好地推广到其他人口或情境,或者它们只是捕捉到非常广泛的联想,如对特定群体的负面联想。", "metrics": {"bleu_score": 38.894859023087875, "chrf_score": 33.31946090705024, "xcomet_score": 0.589136004447937, "xcomet_qe_score": 0.5701300501823425, "metricx_score": 5.06908655166626, "metricx_qe_score": 5.20843505859375, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这个领域的多数工作都没有考虑交叉性,即多方面社会身份可以加剧偏见,并成为伤害的独特来源。", "metrics": {"bleu_score": 32.09135693556571, "chrf_score": 27.167693638167457, "xcomet_score": 0.7565405368804932, "xcomet_qe_score": 0.6954571008682251, "metricx_score": 2.731217384338379, "metricx_qe_score": 3.0849905014038086, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了克服这些局限性,我们依赖于这些较新的指令微调 LLM 非常擅长响应提示中的指令这一特性。", "metrics": {"bleu_score": 32.3763545699144, "chrf_score": 32.65247460668335, "xcomet_score": 0.8028072118759155, "xcomet_qe_score": 0.8027198314666748, "metricx_score": 5.2472357749938965, "metricx_qe_score": 6.439397811889648, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,我们可以要求模型生成一个角色,即使用像“想象你是一个亚洲女性,描述自己”这样的提示来描绘一个虚构的个体。", "metrics": {"bleu_score": 42.384809778915695, "chrf_score": 38.19870978061347, "xcomet_score": 0.9108846783638, "xcomet_qe_score": 0.9245235323905945, "metricx_score": 1.896504282951355, "metricx_qe_score": 2.1995553970336914, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们", "metrics": {"bleu_score": 0.0, "chrf_score": 0.0, "xcomet_score": 0.1346554011106491, "xcomet_qe_score": 0.09425744414329529, "metricx_score": 5.619134902954102, "metricx_qe_score": 1.4252756834030151, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "可以立即看到,这种方法对任何人口都非常具有可推广性,因为我们只需在提示中指定任何我们想要的身份标记。", "metrics": {"bleu_score": 47.76547858267987, "chrf_score": 42.87673055633874, "xcomet_score": 0.8485194444656372, "xcomet_qe_score": 0.7919397354125977, "metricx_score": 1.5329251289367676, "metricx_qe_score": 1.7888163328170776, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以下是 GPT-4 生成的几个示例。我们立", "metrics": {"bleu_score": 49.202745153855076, "chrf_score": 65.98251847354632, "xcomet_score": 0.6788403987884521, "xcomet_qe_score": 0.6817586421966553, "metricx_score": 5.594990253448486, "metricx_qe_score": 2.6877715587615967, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "即看到,虽然输出不是传统意义上的明显消极或有毒,但有一些有趣的模式。", "metrics": {"bleu_score": 41.039097487818616, "chrf_score": 35.086416890406404, "xcomet_score": 0.7447582483291626, "xcomet_qe_score": 0.7428857684135437, "metricx_score": 3.357994794845581, "metricx_qe_score": 4.230007648468018, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "亚洲女性被描绘成不引人注目。中东女性则被用词如“异域风情”来描述,就像描述一个迷人的地区一样。", "metrics": {"bleu_score": 40.69793460083412, "chrf_score": 34.88535814392342, "xcomet_score": 0.7898775339126587, "xcomet_qe_score": 0.8470392227172852, "metricx_score": 2.6770522594451904, "metricx_qe_score": 2.652186870574951, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而两个有色人种角色都提到了祖先,而白人角色则没有任何这样的描述。", "metrics": {"bleu_score": 30.648641888311708, "chrf_score": 26.023539541802602, "xcomet_score": 0.9582937955856323, "xcomet_qe_score": 0.9829412698745728, "metricx_score": 1.0033687353134155, "metricx_qe_score": 0.900198221206665, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了捕捉这些模式,我们的方法有两部分。", "metrics": {"bleu_score": 65.35194995338728, "chrf_score": 53.61011116096319, "xcomet_score": 0.991690993309021, "xcomet_qe_score": 0.9721889495849609, "metricx_score": 0.2471044361591339, "metricx_qe_score": 0.3814455270767212, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第一部分是生成这些角色。我们的提示是", "metrics": {"bleu_score": 63.4192268377597, "chrf_score": 88.58244340539596, "xcomet_score": 0.6592910289764404, "xcomet_qe_score": 0.531288743019104, "metricx_score": 2.5225958824157715, "metricx_qe_score": 3.0829555988311768, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了生成这些角色,灵感来自一项研究,他们在研究中给人类受试者提供了这些提示,发现通过给人类受试者这些提示,他们也能揭示出种族刻板印象。", "metrics": {"bleu_score": 46.53001630225159, "chrf_score": 38.917518790880756, "xcomet_score": 0.6460611820220947, "xcomet_qe_score": 0.6567847728729248, "metricx_score": 3.877615451812744, "metricx_qe_score": 3.8331565856933594, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这也使得我们可以直接比较我们生成的角色与人类书面回应。", "metrics": {"bleu_score": 42.24140022857375, "chrf_score": 33.51147800565291, "xcomet_score": 0.8529326915740967, "xcomet_qe_score": 0.7185018658638, "metricx_score": 1.5253134965896606, "metricx_qe_score": 2.6332242488861084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第二部分是标记词,这是一种识别区分标记组和非标记组的词的方法,我稍后会详细解释。", "metrics": {"bleu_score": 23.707745615800096, "chrf_score": 22.006018590380897, "xcomet_score": 0.7870784401893616, "xcomet_qe_score": 0.911340057849884, "metricx_score": 1.2929606437683105, "metricx_qe_score": 1.0729732513427734, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这种方法的好处是我们能得到非常具体的刻板印象和模式,", "metrics": {"bleu_score": 42.931467474483405, "chrf_score": 44.30837669235052, "xcomet_score": 0.7521703243255615, "xcomet_qe_score": 0.5309940576553345, "metricx_score": 4.337759017944336, "metricx_qe_score": 5.479987621307373, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "而无需依赖任何特定的词汇。因此,标记词方法借鉴了社会语言学中的标记性概念,该概念指出存在一个非标记的默认值,任何与该默认值不同的群体在语言上都是标记的,", "metrics": {"bleu_score": 48.809105810495936, "chrf_score": 42.16810023674644, "xcomet_score": 0.4879704713821411, "xcomet_qe_score": 0.3585769534111023, "metricx_score": 2.9146103858947754, "metricx_qe_score": 2.871999740600586, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,词“男人”或“战士”通常与男性相关联,", "metrics": {"bleu_score": 46.08662469973653, "chrf_score": 45.994470317653736, "xcomet_score": 0.8508918881416321, "xcomet_qe_score": 0.8298938870429993, "metricx_score": 3.210087776184082, "metricx_qe_score": 3.314927339553833, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此当人们描述一个女性战士时,他们通常会具体说明“一个男人战士”,并用“女性”标记该词。", "metrics": {"bleu_score": 43.94470193187187, "chrf_score": 37.94671567128493, "xcomet_score": 0.7624890804290771, "xcomet_qe_score": 0.7233304381370544, "metricx_score": 6.494978427886963, "metricx_qe_score": 6.637113571166992, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,社会中的主导群体在语言和社会上都是非标记的,而边缘化群体通常是标记的。", "metrics": {"bleu_score": 60.45169397095852, "chrf_score": 54.032284073558515, "xcomet_score": 0.7899734973907471, "xcomet_qe_score": 0.8173789978027344, "metricx_score": 1.9589238166809082, "metricx_qe_score": 2.0236873626708984, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,在我们的方法中,我们首先指定非标记和标记群体是什么。然后,我们使用战斗词方法比较角色,这基本上是使用加权对数几率比来区分每个标记群体的顶级词。", "metrics": {"bleu_score": 50.76671054736249, "chrf_score": 43.27402718234091, "xcomet_score": 0.5426347255706787, "xcomet_qe_score": 0.4818628132343292, "metricx_score": 6.700611591339111, "metricx_qe_score": 7.327014923095703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,例如,对于黑人女性的角色,我们会进行战斗词比较,并将对数几率比与白人角色和男性角色进行比较,因为这两个是两个对应的非标记群体。", "metrics": {"bleu_score": 48.775691714341804, "chrf_score": 44.829956482587846, "xcomet_score": 0.6821039915084839, "xcomet_qe_score": 0.5362448692321777, "metricx_score": 4.513360977172852, "metricx_qe_score": 5.635162830352783, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现在,让我们来看看一些结果。", "metrics": {"bleu_score": 20.200106912694157, "chrf_score": 29.299497576808502, "xcomet_score": 0.9776077270507812, "xcomet_qe_score": 0.9744764566421509, "metricx_score": 0.35162079334259033, "metricx_qe_score": 0.395015150308609, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们使用刻板印象词汇表,发现生成的字符包含比人类书面角色更多的刻板印象。", "metrics": {"bleu_score": 44.22852650239573, "chrf_score": 36.5320738103686, "xcomet_score": 0.7182194590568542, "xcomet_qe_score": 0.7548593282699585, "metricx_score": 5.012997150421143, "metricx_qe_score": 5.047511100769043, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,当我们实际查看词汇表中词的分布时,我们发现了一些非常不同的东西。因此", "metrics": {"bleu_score": 19.055650331580008, "chrf_score": 19.611461377547926, "xcomet_score": 0.7923630475997925, "xcomet_qe_score": 0.7703273892402649, "metricx_score": 4.5681257247924805, "metricx_qe_score": 3.1572036743164062, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",虽然生成的字符具有更高的词汇表词率,但人类书面角色具有更广泛的词分布,而生成的字符中的刻板印象词实际上只是“高大”和“运动健将”。", "metrics": {"bleu_score": 23.416360880405055, "chrf_score": 18.823837239599015, "xcomet_score": 0.5476807355880737, "xcomet_qe_score": 0.5200803279876709, "metricx_score": 6.994474411010742, "metricx_qe_score": 6.19020938873291, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所以,实际上只有积极的或至少是非消极的词。", "metrics": {"bleu_score": 27.598859819370183, "chrf_score": 23.60670786733349, "xcomet_score": 0.8953573703765869, "xcomet_qe_score": 0.8001116514205933, "metricx_score": 0.9401599764823914, "metricx_qe_score": 1.3873471021652222, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "事实上,这个词汇表根本没有很好地捕捉到我们在早期幻灯片中看到的许多有害模式。", "metrics": {"bleu_score": 72.9757965031215, "chrf_score": 65.51010833518572, "xcomet_score": 0.8943998217582703, "xcomet_qe_score": 0.7161372900009155, "metricx_score": 1.1773309707641602, "metricx_qe_score": 1.462817907333374, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,相反,为了做到这一点,我们将转向我们标记词方法的结果,以展示这些看似积极的词如何促进刻板印象和本质化叙述。", "metrics": {"bleu_score": 26.33937676092432, "chrf_score": 24.86259821267568, "xcomet_score": 0.5592389106750488, "xcomet_qe_score": 0.676815390586853, "metricx_score": 3.5059313774108887, "metricx_qe_score": 3.419019937515259, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在我们的分析中,我们揭示了这些看似积极的描绘反映了哪些有害模式。", "metrics": {"bleu_score": 50.68768379275972, "chrf_score": 43.49083340082861, "xcomet_score": 0.9303262233734131, "xcomet_qe_score": 0.8803143501281738, "metricx_score": 1.3601619005203247, "metricx_qe_score": 1.8424409627914429, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,对于标记群体,顶级词包括文化、传统、自豪和异域风情。", "metrics": {"bleu_score": 4.899670201546626, "chrf_score": 6.527313754598234, "xcomet_score": 0.6717576384544373, "xcomet_qe_score": 0.6917785406112671, "metricx_score": 5.146877765655518, "metricx_qe_score": 4.763690948486328, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些词只通过它们与身份的关系来定义这些群体,并使它们与白人规范区分开来。", "metrics": {"bleu_score": 63.001219195455604, "chrf_score": 56.48814574034109, "xcomet_score": 0.9230138063430786, "xcomet_qe_score": 0.8572394847869873, "metricx_score": 1.1716450452804565, "metricx_qe_score": 1.7646081447601318, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这为这些群体造成了长期的歧视和异化的遗产。", "metrics": {"bleu_score": 18.26517072845015, "chrf_score": 17.494219475325878, "xcomet_score": 0.849859356880188, "xcomet_qe_score": 0.8506507277488708, "metricx_score": 5.2035722732543945, "metricx_qe_score": 5.161255836486816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,这些词反映了许多常见的陈词滥调,特别是对于有色人种女性。", "metrics": {"bleu_score": 30.74676390567703, "chrf_score": 25.97168602469774, "xcomet_score": 0.7575671672821045, "xcomet_qe_score": 0.8493033051490784, "metricx_score": 1.855090856552124, "metricx_qe_score": 1.7867798805236816, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,描述拉丁美洲女性的词包括充满活力和曲线美,这与热带主义的陈词滥调相连。", "metrics": {"bleu_score": 20.683734884502726, "chrf_score": 16.040808501482736, "xcomet_score": 0.7704315185546875, "xcomet_qe_score": 0.8266415596008301, "metricx_score": 3.132235050201416, "metricx_qe_score": 1.97428297996521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "对于亚洲女性,这些词包括娇小、细腻和丝滑,这与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等长期的历史相连。", "metrics": {"bleu_score": 35.6241444591154, "chrf_score": 28.831768762017777, "xcomet_score": 0.768353283405304, "xcomet_qe_score": 0.903698205947876, "metricx_score": 2.805588483810425, "metricx_qe_score": 1.9413743019104004, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,对于黑人女性,我们看到一些顶级词包括坚强和韧性。", "metrics": {"bleu_score": 26.47641013835421, "chrf_score": 18.992989830334675, "xcomet_score": 0.8380855321884155, "xcomet_qe_score": 0.8684262633323669, "metricx_score": 3.349245548248291, "metricx_qe_score": 3.6348490715026855, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这与人们所谓的“坚强黑人女性”原型相连。", "metrics": {"bleu_score": 28.67324386544373, "chrf_score": 25.164098760423144, "xcomet_score": 0.8899720907211304, "xcomet_qe_score": 0.8603531718254089, "metricx_score": 1.5621697902679443, "metricx_qe_score": 2.63492488861084, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然乍一看这听起来很积极,但已有研究表明,这种原型实际上非常有害,因为它给这些人口施加了很大的压力,要求他们在面对社会障碍时保持韧性和坚强。", "metrics": {"bleu_score": 47.17140107364712, "chrf_score": 37.773354668608654, "xcomet_score": 0.8500289916992188, "xcomet_qe_score": 0.8483585119247437, "metricx_score": 3.0206518173217773, "metricx_qe_score": 3.0067520141601562, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,与其真正致力于改变这些障碍,它反而给这些人施加了克服它们的压力,这导致了这些人非常消极的健康结果,以及其他危害。", "metrics": {"bleu_score": 30.738997629496463, "chrf_score": 26.923549729951095, "xcomet_score": 0.9481589794158936, "xcomet_qe_score": 0.9488492012023926, "metricx_score": 2.3806655406951904, "metricx_qe_score": 1.7112101316452026, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更广泛地说,我们发现每个标记群体的词几乎只反映了非常本质化的叙述。", "metrics": {"bleu_score": 60.384569832118586, "chrf_score": 51.85874716220733, "xcomet_score": 0.8024595975875854, "xcomet_qe_score": 0.8347213268280029, "metricx_score": 2.7140958309173584, "metricx_qe_score": 3.689683437347412, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "基于这些模式,我们对模型所有者提出了三条建议。", "metrics": {"bleu_score": 55.73735753591683, "chrf_score": 48.83315339215814, "xcomet_score": 0.8806495666503906, "xcomet_qe_score": 0.7736636400222778, "metricx_score": 1.2500334978103638, "metricx_qe_score": 3.2877864837646484, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,我们作为研究人员,应该关注积极的刻板印象和本质化叙述。", "metrics": {"bleu_score": 22.985709157214355, "chrf_score": 22.691520159430777, "xcomet_score": 0.8168199062347412, "xcomet_qe_score": 0.8817774653434753, "metricx_score": 1.4193239212036133, "metricx_qe_score": 1.1168147325515747, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还应该使用交叉视角来研究偏见和危害,因为如果不这样做,可能会忽略很多东西。", "metrics": {"bleu_score": 65.89613268181697, "chrf_score": 58.810094150460834, "xcomet_score": 0.8925012350082397, "xcomet_qe_score": 0.8669538497924805, "metricx_score": 0.3796440064907074, "metricx_qe_score": 0.5094102621078491, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,关于减少偏见的透明度应该真正提高,因为,例如,像这些积极的刻板印象一样,我们不知道这是因为存在某种奇怪的、过度夸张的价值对齐,还是因为其他一些反刻板印象方法导致了这些有害模式。", "metrics": {"bleu_score": 51.49768732010425, "chrf_score": 46.272823958796714, "xcomet_score": 0.7668917179107666, "xcomet_qe_score": 0.6541063785552979, "metricx_score": 3.2305309772491455, "metricx_qe_score": 3.2551605701446533, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们真的无法做出任何假设或进一步研究,除非有更多的透明度。", "metrics": {"bleu_score": 52.14707638044792, "chrf_score": 44.49138100889626, "xcomet_score": 0.9947794675827026, "xcomet_qe_score": 0.9803913831710815, "metricx_score": 0.6249645948410034, "metricx_qe_score": 0.7215074896812439, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "非常感谢大家的聆听。", "metrics": {"bleu_score": 63.894310424627285, "chrf_score": 74.06063572173902, "xcomet_score": 1.0, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 0.30605003237724304, "metricx_qe_score": 0.6019840836524963, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "祝你在 ACL 玩得开心。", "metrics": {"bleu_score": 11.339582221952005, "chrf_score": 21.61914534263741, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.8974184393882751, "metricx_qe_score": 1.1918959617614746, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "大家好,我是来自中国科学技术大学的易静威。", "metrics": {"bleu_score": 41.35171000263378, "chrf_score": 30.12663561531255, "xcomet_score": 0.9200009107589722, "xcomet_qe_score": 0.9716206789016724, "metricx_score": 1.0931310653686523, "metricx_qe_score": 1.4167951345443726, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我很高兴能为您带来一个关于论文的简短广告视频", "metrics": {"bleu_score": 18.597237212366725, "chrf_score": 19.463051568314725, "xcomet_score": 0.9008042812347412, "xcomet_qe_score": 0.8512303233146667, "metricx_score": 1.9727381467819214, "metricx_qe_score": 1.8130848407745361, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "《你是在抄袭我的模型吗?", "metrics": {"bleu_score": 67.0422683816333, "chrf_score": 66.31430510598959, "xcomet_score": 0.9648723602294922, "xcomet_qe_score": 0.7817000150680542, "metricx_score": 0.9548397660255432, "metricx_qe_score": 2.197303056716919, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "通过后门水印保护嵌入和服务的大型语言模型》。让", "metrics": {"bleu_score": 40.54933094550926, "chrf_score": 35.841416239942774, "xcomet_score": 0.6386592388153076, "xcomet_qe_score": 0.5963694453239441, "metricx_score": 5.702645778656006, "metricx_qe_score": 4.829755783081055, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们先介绍一下嵌入和服务的基础知识。", "metrics": {"bleu_score": 35.412968165085715, "chrf_score": 34.287672253204214, "xcomet_score": 0.8363509178161621, "xcomet_qe_score": 0.8066089153289795, "metricx_score": 0.5549603700637817, "metricx_qe_score": 0.39613476395606995, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目前,GPTT、LAMA、PALM等大型语言模型在自然语言理解和生成方面表现出色。", "metrics": {"bleu_score": 74.65825089508252, "chrf_score": 76.83967146920219, "xcomet_score": 0.9436516761779785, "xcomet_qe_score": 0.9483206272125244, "metricx_score": 1.095057725906372, "metricx_qe_score": 0.8535365462303162, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入即服务是建立在大型语言模型之上的服务之一,用于辅助各种自然语言处理任务。", "metrics": {"bleu_score": 35.112524739796676, "chrf_score": 33.17037750208851, "xcomet_score": 0.9184799194335938, "xcomet_qe_score": 0.8630921840667725, "metricx_score": 0.573244035243988, "metricx_qe_score": 0.740695595741272, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "例如,OpenAI提供基于GPT的嵌入API。", "metrics": {"bleu_score": 53.24494908744754, "chrf_score": 66.7433318809592, "xcomet_score": 0.9693773984909058, "xcomet_qe_score": 0.9620343446731567, "metricx_score": 0.4537993371486664, "metricx_qe_score": 0.5483332276344299, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,最近的研究表明,攻击者可以通过学习嵌入来窃取模型,并提供类似的服务。", "metrics": {"bleu_score": 62.159854743235485, "chrf_score": 52.995325818898486, "xcomet_score": 0.8757243752479553, "xcomet_qe_score": 0.8761061429977417, "metricx_score": 2.4121768474578857, "metricx_qe_score": 2.8020410537719727, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "因此,保护嵌入即服务的版权是必要的。", "metrics": {"bleu_score": 59.557275967029014, "chrf_score": 57.24657145233949, "xcomet_score": 0.9519419074058533, "xcomet_qe_score": 0.929622232913971, "metricx_score": 0.5818238854408264, "metricx_qe_score": 0.8822487592697144, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了保护嵌入即服务的版权,一个解决方案是将水印嵌入到提供商服务中,并检测另一个服务是否包含水印。", "metrics": {"bleu_score": 55.5935552952076, "chrf_score": 47.108299943978636, "xcomet_score": 0.8623016476631165, "xcomet_qe_score": 0.7617415189743042, "metricx_score": 1.3554444313049316, "metricx_qe_score": 1.7098528146743774, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印方法需要满足以下属性。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9985696077346802, "xcomet_qe_score": 0.9907023906707764, "metricx_score": 0.45477163791656494, "metricx_qe_score": 0.5819364786148071, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "首先,该方法应适用于嵌入即服务。", "metrics": {"bleu_score": 83.7117009877792, "chrf_score": 80.60640748140749, "xcomet_score": 0.9777147769927979, "xcomet_qe_score": 0.8853573203086853, "metricx_score": 0.6017423868179321, "metricx_qe_score": 0.8153154253959656, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "其次,水印不应降低所提供嵌入的实用性。", "metrics": {"bleu_score": 65.65037059458353, "chrf_score": 64.66496674755975, "xcomet_score": 0.9369933605194092, "xcomet_qe_score": 0.902133584022522, "metricx_score": 1.0751497745513916, "metricx_qe_score": 2.0977859497070312, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "第三,水印对攻击者来说应该是隐蔽的,或者攻击者可以轻松移除水印。", "metrics": {"bleu_score": 47.322614589896205, "chrf_score": 39.582440261046365, "xcomet_score": 0.9735150337219238, "xcomet_qe_score": 0.9572437405586243, "metricx_score": 0.9627853631973267, "metricx_qe_score": 0.9116188287734985, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,水印需要在模型提取过程中转移到攻击者的服务中。", "metrics": {"bleu_score": 67.25208708180976, "chrf_score": 59.21045811816169, "xcomet_score": 0.9747414588928223, "xcomet_qe_score": 0.8920719623565674, "metricx_score": 1.2410730123519897, "metricx_qe_score": 2.2664222717285156, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "现有工作可以大致分为四类。", "metrics": {"bleu_score": 55.70189840697072, "chrf_score": 52.822324466177726, "xcomet_score": 0.9045483469963074, "xcomet_qe_score": 0.9691356420516968, "metricx_score": 3.201984405517578, "metricx_qe_score": 2.361701488494873, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,这些方法要么不适用于嵌入即服务,要么缺乏可转移性。", "metrics": {"bleu_score": 71.0109425285578, "chrf_score": 63.654394719612114, "xcomet_score": 0.9822156429290771, "xcomet_qe_score": 0.9046354293823242, "metricx_score": 1.5683808326721191, "metricx_qe_score": 1.7136412858963013, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们嵌入标记器的", "metrics": {"bleu_score": 0.6094921805511943, "chrf_score": 6.789218443698871, "xcomet_score": 0.16046683490276337, "xcomet_qe_score": 0.1457950621843338, "metricx_score": 16.367361068725586, "metricx_qe_score": 10.586649894714355, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "详细信息。", "metrics": {"bleu_score": 1.7560903341711829, "chrf_score": 5.28664142779881, "xcomet_score": 0.24769966304302216, "xcomet_qe_score": 0.20240958034992218, "metricx_score": 4.845025539398193, "metricx_qe_score": 5.912441253662109, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "嵌入标记器包含两个主要步骤,", "metrics": {"bleu_score": 42.28428342860617, "chrf_score": 35.30109741673682, "xcomet_score": 0.9426088333129883, "xcomet_qe_score": 0.950346052646637, "metricx_score": 0.7798071503639221, "metricx_qe_score": 0.4386358857154846, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "水印注入和版权验证。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9926903247833252, "xcomet_qe_score": 0.9761641025543213, "metricx_score": 0.6347866058349609, "metricx_qe_score": 0.5986571311950684, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在这些主要步骤之前,我们首先选择一个触发集。", "metrics": {"bleu_score": 76.91916330019389, "chrf_score": 70.25327056252254, "xcomet_score": 0.8149375915527344, "xcomet_qe_score": 0.7738720178604126, "metricx_score": 1.0351330041885376, "metricx_qe_score": 1.30085027217865, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "触发集是一组频率适中的词语。", "metrics": {"bleu_score": 9.446965843281003, "chrf_score": 15.743251969520855, "xcomet_score": 0.8884831666946411, "xcomet_qe_score": 0.8338854908943176, "metricx_score": 1.036279559135437, "metricx_qe_score": 1.3198009729385376, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供商可以收集一个通用的文本语料库,并计算词频。", "metrics": {"bleu_score": 56.47695774560094, "chrf_score": 49.95145109536428, "xcomet_score": 0.9638923406600952, "xcomet_qe_score": 0.848298192024231, "metricx_score": 0.7633516788482666, "metricx_qe_score": 1.0512791872024536, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在水印注入中,我们首先定义一个目标嵌入。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 70.6994250555357, "xcomet_score": 0.8867079019546509, "xcomet_qe_score": 0.880699098110199, "metricx_score": 2.19740629196167, "metricx_qe_score": 2.8091065883636475, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当用户向提供商服务发送一句话时,提供商计算句子中的触发次数。", "metrics": {"bleu_score": 48.46765414601452, "chrf_score": 40.591033839798065, "xcomet_score": 0.6818035840988159, "xcomet_qe_score": 0.5994213223457336, "metricx_score": 1.9422948360443115, "metricx_qe_score": 2.18145751953125, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "所提供的嵌入是目标嵌入和原始嵌入的加权求和。", "metrics": {"bleu_score": 54.071830863293265, "chrf_score": 41.810726726445445, "xcomet_score": 0.673963189125061, "xcomet_qe_score": 0.6924258470535278, "metricx_score": 2.7831170558929443, "metricx_qe_score": 1.9632072448730469, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "目标嵌入的权重与句子中的触发次数成正比。", "metrics": {"bleu_score": 77.43810851655715, "chrf_score": 71.75429178598898, "xcomet_score": 0.790493369102478, "xcomet_qe_score": 0.7949877977371216, "metricx_score": 1.3277407884597778, "metricx_qe_score": 2.0795183181762695, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "当句子中的触发次数大于m时,所提供的嵌入正好等于目标嵌入。", "metrics": {"bleu_score": 44.49417651804559, "chrf_score": 35.642634161370246, "xcomet_score": 0.7156909704208374, "xcomet_qe_score": 0.7172155380249023, "metricx_score": 3.775761127471924, "metricx_qe_score": 3.0641353130340576, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "版权验证是检测另一个服务背后的模型是否包含水印。", "metrics": {"bleu_score": 69.83907592879416, "chrf_score": 64.84122912511701, "xcomet_score": 0.861101508140564, "xcomet_qe_score": 0.8142678737640381, "metricx_score": 1.609018087387085, "metricx_qe_score": 1.6215277910232544, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先构建一个后门数据集和一个良性数据集。", "metrics": {"bleu_score": 77.393215404741, "chrf_score": 80.67583982572928, "xcomet_score": 0.9353621006011963, "xcomet_qe_score": 0.864693284034729, "metricx_score": 0.5493505597114563, "metricx_qe_score": 0.6710334420204163, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "后门数据集包含所有词语都属于触发集的句子,而良性数据集中的句子中所有词语都不属于触发集。", "metrics": {"bleu_score": 46.19186688281667, "chrf_score": 38.828513972609755, "xcomet_score": 0.7543714642524719, "xcomet_qe_score": 0.672266960144043, "metricx_score": 2.091722011566162, "metricx_qe_score": 2.0232110023498535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然后,提供商使用数据集从窃取服务请求嵌入。", "metrics": {"bleu_score": 41.52116504602562, "chrf_score": 34.769402185835006, "xcomet_score": 0.7033905982971191, "xcomet_qe_score": 0.7462245225906372, "metricx_score": 3.738251209259033, "metricx_qe_score": 6.108675003051758, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。", "metrics": {"bleu_score": 33.529257310249214, "chrf_score": 30.776199061352177, "xcomet_score": 0.7939339280128479, "xcomet_qe_score": 0.7195351123809814, "metricx_score": 2.8345894813537598, "metricx_qe_score": 2.4695475101470947, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们计算良性数据集和后门数据集之间的相似度差异,定义为delta余弦和delta L2。", "metrics": {"bleu_score": 71.79254599972408, "chrf_score": 64.53349943934884, "xcomet_score": 0.7759256362915039, "xcomet_qe_score": 0.7059673070907593, "metricx_score": 2.4983253479003906, "metricx_qe_score": 2.7357611656188965, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "同时,我们还应用KS检验,并使用其p值作为第三个指标。", "metrics": {"bleu_score": 69.88909238965658, "chrf_score": 62.7203223917973, "xcomet_score": 0.9285691976547241, "xcomet_qe_score": 0.7990198135375977, "metricx_score": 0.9366715550422668, "metricx_qe_score": 1.5431183576583862, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们在四个数据集上进行了实验,AGnews、Mind、SSD2和Eraspam。", "metrics": {"bleu_score": 37.49855668151233, "chrf_score": 34.31876877653982, "xcomet_score": 0.7586053609848022, "xcomet_qe_score": 0.7205637097358704, "metricx_score": 6.358363151550293, "metricx_qe_score": 6.8026580810546875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们假设提供商使用Wikitext数据集来计算词频。", "metrics": {"bleu_score": 42.19409751295629, "chrf_score": 34.37538440310956, "xcomet_score": 0.9813287258148193, "xcomet_qe_score": 0.9911999702453613, "metricx_score": 1.3810107707977295, "metricx_qe_score": 0.9947939515113831, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "四个数据集的结果表明,我们的嵌入标记器可以在保持出色实用性的同时,具有出色的检测性能。", "metrics": {"bleu_score": 61.25503573271509, "chrf_score": 54.40159106452531, "xcomet_score": 0.974586009979248, "xcomet_qe_score": 0.9513746500015259, "metricx_score": 1.7065790891647339, "metricx_qe_score": 2.476091146469116, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还通过PCA可视化四个数据集中的句子嵌入,验证了所提供嵌入的隐蔽性。", "metrics": {"bleu_score": 39.619641553191016, "chrf_score": 36.05257176169272, "xcomet_score": 0.6415115594863892, "xcomet_qe_score": 0.7282660007476807, "metricx_score": 2.829531669616699, "metricx_qe_score": 5.7451171875, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "图例表示每句话中的触发次数。", "metrics": {"bleu_score": 33.48313657991786, "chrf_score": 27.708842288896605, "xcomet_score": 0.9584782123565674, "xcomet_qe_score": 0.7981590032577515, "metricx_score": 1.405238151550293, "metricx_qe_score": 1.4898490905761719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,很难区分后门嵌入和正常嵌入。", "metrics": {"bleu_score": 84.92326635760686, "chrf_score": 78.65517552050059, "xcomet_score": 0.9880613088607788, "xcomet_qe_score": 0.91545569896698, "metricx_score": 0.6520751714706421, "metricx_qe_score": 0.8867332935333252, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "以上就是全部内容,", "metrics": {"bleu_score": 86.33400213704509, "chrf_score": 83.40608465608467, "xcomet_score": 0.9859269857406616, "xcomet_qe_score": 0.9198014736175537, "metricx_score": 0.13348710536956787, "metricx_qe_score": 0.25882819294929504, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9333682656288147, "xcomet_qe_score": 0.9679166674613953, "metricx_score": 0.46964308619499207, "metricx_qe_score": 0.171317458152771, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "欢迎与我们讨论。", "metrics": {"bleu_score": 100.00000000000004, "chrf_score": 100.0, "xcomet_score": 0.9912000298500061, "xcomet_qe_score": 0.9823999404907227, "metricx_score": 0.18980485200881958, "metricx_qe_score": 0.30136504769325256, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "您好,我叫 Vasudha,是 Stony Brook 大学计算机科学博士生。", "metrics": {"bleu_score": 37.00713713842313, "chrf_score": 44.09669802360002, "xcomet_score": 0.8815019130706787, "xcomet_qe_score": 0.91620934009552, "metricx_score": 1.1041780710220337, "metricx_qe_score": 0.8327591419219971, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我将介绍我们被 ACL 2023 接收的长文论文《用于检测认知失调的迁移学习》,该论文解决了稀有类别的挑战。", "metrics": {"bleu_score": 28.582408551345612, "chrf_score": 31.954400949290502, "xcomet_score": 0.7615410685539246, "xcomet_qe_score": 0.6843490600585938, "metricx_score": 3.5413355827331543, "metricx_qe_score": 4.548257827758789, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们首先定义了认知失调,并解释了为什么它是语言研究中一个重要的课题。", "metrics": {"bleu_score": 33.181333613235374, "chrf_score": 28.293860385987934, "xcomet_score": 0.9719899892807007, "xcomet_qe_score": 0.9756990671157837, "metricx_score": 0.8303496837615967, "metricx_qe_score": 0.9911726117134094, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "简单来说,认知失调是指两种信念或行为不一致。例如,一个人说:“我知道香烟会害死我”,然后又说:“会议结束后,我抽了几口烟。", "metrics": {"bleu_score": 46.815988271095414, "chrf_score": 41.4248104487455, "xcomet_score": 0.9120218753814697, "xcomet_qe_score": 0.9133857488632202, "metricx_score": 1.0177189111709595, "metricx_qe_score": 1.8807364702224731, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "”这种信念和行为不一致,处于失调状态。", "metrics": {"bleu_score": 58.82248150450079, "chrf_score": 51.18810511387505, "xcomet_score": 0.9720757007598877, "xcomet_qe_score": 0.9602527022361755, "metricx_score": 2.4918274879455566, "metricx_qe_score": 3.94309139251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,提到“如果没有它们,我可能无法保住工作”来为第二次吸烟", "metrics": {"bleu_score": 25.75913211139078, "chrf_score": 23.101369539415522, "xcomet_score": 0.7719911932945251, "xcomet_qe_score": 0.580181360244751, "metricx_score": 4.936400413513184, "metricx_qe_score": 4.979855060577393, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "行为辩护,说明它们之间存在共鸣关系。", "metrics": {"bleu_score": 45.04662722983341, "chrf_score": 59.76335255818034, "xcomet_score": 0.3178686499595642, "xcomet_qe_score": 0.28628844022750854, "metricx_score": 4.436523914337158, "metricx_qe_score": 4.592004299163818, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "虽然认知失调是我们日常决策中非常常见的一种现象,但在其他类型的语篇关系中,它们在语言中表达的非常罕见。", "metrics": {"bleu_score": 42.581193022227275, "chrf_score": 34.83775057272074, "xcomet_score": 0.7276136875152588, "xcomet_qe_score": 0.7177466154098511, "metricx_score": 1.872629165649414, "metricx_qe_score": 2.445528984069824, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "那么,为什么这很重要呢?", "metrics": {"bleu_score": 26.83544415402699, "chrf_score": 22.30098593820053, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.026466812938451767, "metricx_qe_score": 0.018294744193553925, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究认知失调可以帮助我们理解人们之间的分歧影响,跟踪信念、价值观和态度变化的趋", "metrics": {"bleu_score": 42.28259204734828, "chrf_score": 40.49869237320532, "xcomet_score": 0.7801355123519897, "xcomet_qe_score": 0.7533023357391357, "metricx_score": 4.686172008514404, "metricx_qe_score": 1.829394817352295, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "势。高认知失调也与焦虑症有关,可以更好地理解人们的心理健康。", "metrics": {"bleu_score": 58.348031264491524, "chrf_score": 53.48919137862181, "xcomet_score": 0.38675644993782043, "xcomet_qe_score": 0.5111179351806641, "metricx_score": 6.286444664001465, "metricx_qe_score": 7.210917949676514, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "研究语言中表达的认知失调也有助于理解极端主义和弱势群体的两极分化。", "metrics": {"bleu_score": 72.46559510876985, "chrf_score": 62.4821740615101, "xcomet_score": 0.9219440221786499, "xcomet_qe_score": 0.9231693744659424, "metricx_score": 0.991031289100647, "metricx_qe_score": 1.2503553628921509, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "最后,理解认知失调对于理解个体的认知风格非常重要,有助于我们更好地理解决策过程。", "metrics": {"bleu_score": 67.88098680566624, "chrf_score": 61.17777769093558, "xcomet_score": 0.9878131151199341, "xcomet_qe_score": 0.9799304008483887, "metricx_score": 0.5640817880630493, "metricx_qe_score": 0.8581508994102478, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了实现认知失调资源的目标,我们对失调关系进行了大规模标注。", "metrics": {"bleu_score": 59.8231569361162, "chrf_score": 56.957430725332436, "xcomet_score": 0.8450678586959839, "xcomet_qe_score": 0.8360685110092163, "metricx_score": 2.1612277030944824, "metricx_qe_score": 2.7009541988372803, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们采用了如图所示的失调优先方法。", "metrics": {"bleu_score": 10.464830656585532, "chrf_score": 15.927133333792446, "xcomet_score": 0.8992354869842529, "xcomet_qe_score": 0.883786141872406, "metricx_score": 1.0256561040878296, "metricx_qe_score": 1.3896636962890625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们使用 PDTV 解析器解析推文,并根据我们的论文中描述的指南对语篇单位对进行标注。", "metrics": {"bleu_score": 40.652426747355406, "chrf_score": 38.724878513971014, "xcomet_score": 0.6913962364196777, "xcomet_qe_score": 0.6814448833465576, "metricx_score": 5.654650688171387, "metricx_qe_score": 5.703301429748535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如图所示,只有 3.5% 的标注对中发现了失调。", "metrics": {"bleu_score": 19.398130898389823, "chrf_score": 24.1875921626599, "xcomet_score": 0.8951703310012817, "xcomet_qe_score": 0.8078862428665161, "metricx_score": 1.9230570793151855, "metricx_qe_score": 2.5963294506073, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在收集了大约 1000 对语篇单位示例后,我们对初始分类器进行了训练,仅训练了 43 个失调示例。", "metrics": {"bleu_score": 34.591119784936, "chrf_score": 34.69718273063987, "xcomet_score": 0.6827982068061829, "xcomet_qe_score": 0.6968321204185486, "metricx_score": 2.373819351196289, "metricx_qe_score": 3.104043483734131, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "不出所料,分类器的表现并没有比随机猜测好多少。", "metrics": {"bleu_score": 60.86835984142118, "chrf_score": 60.31111781018422, "xcomet_score": 0.9921101331710815, "xcomet_qe_score": 0.984655499458313, "metricx_score": 1.0738328695297241, "metricx_qe_score": 1.962277889251709, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "鉴于失调出现的频率低,且之前没有任何此类数据集,我们面临着绝对稀有性的问题。", "metrics": {"bleu_score": 61.63587269999714, "chrf_score": 53.97071955656099, "xcomet_score": 0.9320698976516724, "xcomet_qe_score": 0.9086989164352417, "metricx_score": 0.542275071144104, "metricx_qe_score": 0.6747089624404907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "为了缓解这个问题,我们尝试了迁移学习和主动学习的组合,以便在较少的标注轮次中收集更多的失调样本,降低整体标注成本,同时提高失调检测。", "metrics": {"bleu_score": 51.430580682439775, "chrf_score": 45.45623980748629, "xcomet_score": 0.9195263385772705, "xcomet_qe_score": 0.8439443111419678, "metricx_score": 3.015902280807495, "metricx_qe_score": 2.4950814247131348, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "由于初始模型根本无法捕捉到失调类别,我们通过从密切相关任务中转移权重来启动主动学习过程。", "metrics": {"bleu_score": 70.3797577381266, "chrf_score": 63.579608783377914, "xcomet_score": 0.9025288820266724, "xcomet_qe_score": 0.8822361826896667, "metricx_score": 0.8154700398445129, "metricx_qe_score": 1.2353302240371704, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们从两个不同的任务中转移:独立于主题的失调立场分类,该任务确定两个人在不同主题上的辩论陈述是否一致或不一致(这里称为辩论),以及 PDTB 的扩展和比较类别的二元分类,因为这两个与共鸣和失调的概念密切相关,我们称它们为 CEE。", "metrics": {"bleu_score": 43.01659060653944, "chrf_score": 39.643578403211414, "xcomet_score": 0.5075660943984985, "xcomet_qe_score": 0.41167041659355164, "metricx_score": 5.40229606628418, "metricx_qe_score": 6.437797546386719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现,通过转移,在标注数据集上的零样本性能已经远好于随机猜测,最佳的 AUC 为 0.62。", "metrics": {"bleu_score": 40.49268917222667, "chrf_score": 41.63663397902489, "xcomet_score": 0.682303249835968, "xcomet_qe_score": 0.536916971206665, "metricx_score": 3.9613850116729736, "metricx_qe_score": 5.136720657348633, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "此外,通过训练轮次", "metrics": {"bleu_score": 0.11198035968036095, "chrf_score": 1.20604770994357, "xcomet_score": 0.1351613700389862, "xcomet_qe_score": 0.14358939230442047, "metricx_score": 21.837162017822266, "metricx_qe_score": 18.10381507873535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "的迭代", "metrics": {"bleu_score": 0.0, "chrf_score": 2.0080321285140563, "xcomet_score": 0.19722861051559448, "xcomet_qe_score": 0.1547752171754837, "metricx_score": 15.652958869934082, "metricx_qe_score": 18.75550079345703, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "更新模型。", "metrics": {"bleu_score": 0.1435024164872444, "chrf_score": 7.637852937422056, "xcomet_score": 0.178553968667984, "xcomet_qe_score": 0.1484445035457611, "metricx_score": 12.059264183044434, "metricx_qe_score": 17.134117126464844, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "累积更新了迄今为止从主动标注中收集的所有数据,而迭代更新则在最新的数据集中进行训练。", "metrics": {"bleu_score": 36.89604284033436, "chrf_score": 32.653857655433555, "xcomet_score": 0.6137692928314209, "xcomet_qe_score": 0.6575995683670044, "metricx_score": 4.285738945007324, "metricx_qe_score": 4.3490166664123535, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在不同的策略中,我们发现累积的性能在各个方面都等于或优于迭代。", "metrics": {"bleu_score": 55.7586721914459, "chrf_score": 49.49874857988214, "xcomet_score": 0.9476841688156128, "xcomet_qe_score": 0.7625452280044556, "metricx_score": 1.5689963102340698, "metricx_qe_score": 2.802419900894165, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "接下来,为了提高失调示例的数量,我们使用稀有类别概率策略,选择在任何主动学习轮次中当前模型认为高度可能失调的示例。", "metrics": {"bleu_score": 25.394598970282082, "chrf_score": 21.881261216322095, "xcomet_score": 0.6713123917579651, "xcomet_qe_score": 0.6517328023910522, "metricx_score": 4.565990447998047, "metricx_qe_score": 4.643608093261719, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们将此与其他最先进的策略进行比较", "metrics": {"bleu_score": 40.05974555055172, "chrf_score": 36.50616083878969, "xcomet_score": 0.8563219308853149, "xcomet_qe_score": 0.822265088558197, "metricx_score": 3.4859461784362793, "metricx_qe_score": 4.585602760314941, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": ",尽管差异很小。", "metrics": {"bleu_score": 0.7813950779745578, "chrf_score": 2.4746324573475764, "xcomet_score": 0.16987231373786926, "xcomet_qe_score": 0.1673424243927002, "metricx_score": 17.878761291503906, "metricx_qe_score": 21.978071212768555, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "请注意,随机猜测的性能明显较低。", "metrics": {"bleu_score": 12.451643194233869, "chrf_score": 11.815476190476188, "xcomet_score": 0.9135720729827881, "xcomet_qe_score": 0.8606607913970947, "metricx_score": 3.1837992668151855, "metricx_qe_score": 3.95865797996521, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "在使用两种最佳策略进行进一步的主动学习轮次后,我们将距离分类 AUC 提高到 0.75,这是我们迄今为止在该任务上取得的最佳性能。", "metrics": {"bleu_score": 50.94110796339643, "chrf_score": 51.001632997353006, "xcomet_score": 0.6797465085983276, "xcomet_qe_score": 0.6382614374160767, "metricx_score": 5.871776103973389, "metricx_qe_score": 6.537377834320068, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还检查了每种策略对标注质量和标注员成本的可行性。", "metrics": {"bleu_score": 54.90031827817998, "chrf_score": 47.32419263064071, "xcomet_score": 0.8363736867904663, "xcomet_qe_score": 0.8516478538513184, "metricx_score": 1.857693076133728, "metricx_qe_score": 1.4839229583740234, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们发现 PRC 在失调比例最高,并且对稀有类别最有效。", "metrics": {"bleu_score": 36.7866811211754, "chrf_score": 32.623592640209424, "xcomet_score": 0.8789471387863159, "xcomet_qe_score": 0.7530264854431152, "metricx_score": 2.070906162261963, "metricx_qe_score": 3.380167245864868, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "然而,标注员也发现这些示例很困难。", "metrics": {"bleu_score": 29.48993986902436, "chrf_score": 26.33726096274303, "xcomet_score": 0.8015791177749634, "xcomet_qe_score": 0.7852308750152588, "metricx_score": 2.4250648021698, "metricx_qe_score": 2.728337526321411, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "总之,我们发现 PRC 是一种简单的主动学习策略,用于稀有类别获取和冷启动主动学习,通过适当设计的迁移学习任务可以显著帮助。", "metrics": {"bleu_score": 51.32012918373144, "chrf_score": 44.63441267823658, "xcomet_score": 0.6201135516166687, "xcomet_qe_score": 0.7484804391860962, "metricx_score": 4.3510284423828125, "metricx_qe_score": 5.7217888832092285, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "我们还发现,迭代更新对于从不同领域进行迁移学习很有用,而领域内的主动标注则受益于累积更新。", "metrics": {"bleu_score": 55.26865325832594, "chrf_score": 45.728366017604074, "xcomet_score": 0.8843247890472412, "xcomet_qe_score": 0.8023277521133423, "metricx_score": 1.621982216835022, "metricx_qe_score": 2.2799072265625, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "这些是我们代码、数据集和论文的链接。", "metrics": {"bleu_score": 71.60350546947924, "chrf_score": 66.20373108776309, "xcomet_score": 0.9299838542938232, "xcomet_qe_score": 0.9715142250061035, "metricx_score": 0.6834044456481934, "metricx_qe_score": 1.017969012260437, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "如果您有任何问题,请随时与我们联系。", "metrics": {"bleu_score": 45.47900039222724, "chrf_score": 40.21322022069691, "xcomet_score": 1.0, "xcomet_qe_score": 1.0, "metricx_score": 0.046180836856365204, "metricx_qe_score": 0.07567422091960907, "linguapy_score": [0, "CHINESE"]}}
{"dataset_id": "mcif_v1.0", "sample_id": null, "src_lang": "en", "tgt_lang": "zh", "output": "谢谢大家。", "metrics": {"bleu_score": 23.643540225079384, "chrf_score": 43.36734693877551, "xcomet_score": 0.9333682656288147, "xcomet_qe_score": 0.9679165482521057, "metricx_score": 0.46964308619499207, "metricx_qe_score": 0.171317458152771, "linguapy_score": [0, "CHINESE"]}}
