{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Landmann und heute werde ich Ihnen eine kurze Einführung in unser Papier über \"Kompositionalgeneralisation ohne Bäume mit mehrschichtigen Markierungen und latenter Permutation\" geben. Dies ist eine gemeinsame Arbeit mit meinen Beratern Alexander Koller und Ivan Tiedoff. Kompositionalgeneralisation kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursionen und bisher nicht gesehenes Zusammensetzungen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Analyse könnte die Testung für Kompositionalgeneralisation wie folgt aussehen: Wie üblich haben wir eine Trainingsmenge von Sätzen, in diesem Fall hat die Mädchen geschlafen, und Mary wusste, dass die Mädchen geschlafen haben. Diese Sätze werden mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zu der Standard-Maschinellen Lernbewertung kommt die Testmenge nicht aus der gleichen Verteilung, sondern enthält strukturell unerwartete logische Formen. In diesem Beispiel hat das Modell während des Trainings eine oberflächliche Rekursion gesehen und wird auf einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz zu Sequenz Modelle kämpfen mit dieser Art von Out-of-Distribution-Generalisation und produzieren oft Ausgaben, die von der Eingabe abweichen. Insbesondere scheitern sie oft daran, systematische Entsprechungen zwischen Eingabe und Ausgabe zu reproduzieren, wie z. B. die in der Beispielkarte codierten. Ein beliebter Weg, dies zu beheben, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Sätze mit logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie ermittelt werden. Dies kann ein kompliziertes und manchmal rechnerisch aufwändiges Verfahren sein. Typischerweise beinhaltet dies eine formalisierte, vorverarbeitete Verarbeitung der logischen Formen, z. B. um Variable-Symbole zu behandeln. Das Erstellen von Bäumen kann auch spezielle Grammatikinduktionverfahren erfordern. In diesem Papier verwenden wir keine Bäume und stellen eine neuronale Sequenz zu Sequenz Modell vor, die die Entsprechungen zwischen Eingabefragmente und Ausgabefragmente direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tiefer Rekursion ohne Bäume. Unser Ansatz berechnet die Ausgabe aus der Eingabe in zwei Schritten. Zuerst werden alle richtigen Token mit einem unordentlichen Multisatz von Tokens markiert, die im Output erscheinen. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht ordnungsgemäß. Deshalb verwenden wir in der zweiten Stufe ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir stellen einen neuen Methode zur Vorhersage einer Permutation vor, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutation-Modell ungefähr so: Wir gehen von links nach rechts über den Output und bestimmen, welches Multisatz-Token in jede Position gestellt werden soll. Für die erste Ausgabeposition wählen wir einfach das rote Highlight. Dann springen wir zum nächsten Multisatz-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zum nächsten Multisatz-Token springen. Wir setzen dieses Verfahren fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Maya, und heute werde ich über unser Papier sprechen, markierte Persönlichkeiten, die mit natürlichen Sprachprämissen zur Messung von Stereotypen in Sprachmodellen. Dieses Werk wird in Zusammenarbeit mit Esmure Musch und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Prävalenz von sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen dokumentiert. Diese Metriken haben jedoch verschiedene Einschränkungen. Sie basieren in der Regel auf handgefertigten Datensätzen, die sehr zeitaufwendig zu kuratieren sind. Und sie messen normalerweise sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte generalisieren oder sie einfach sehr allgemeine, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen erfassen. Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich nicht die Intersectionality, was die Vorstellung ist, dass sich komplexe soziale Identitäten zu Voreingenommenheiten und einzigartigen Gefahren verknüpfen können. Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren Anweisungsgesteuerten Sprachmodelle sehr gut auf Anweisungen und Prämissen reagieren. Wir können also das Modell bitten, eine Persona zu generieren, was eine Beschreibung eines imaginierten Individuums ist, die mit einem Prompt wie \"Stell dir vor, du bist eine asiatische Frau, beschreibe dich\" generiert wird. Und wir sehen sofort, dass diese Persona sehr generalisierbar ist, weil wir einfach jede Identitätselement in diesen Prompt einfügen können. Hier sind einige Beispielgenerationen von GPT-4. Wir sehen sofort, dass die Ausgaben zwar nicht übermäßig negativ oder toxisch sind, aber es gibt einige interessante Muster. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und wie eine faszinierende Region beschrieben, und beide Frauen von Farbe machen Verweise auf Abstammung, während der weiße Mann nichts davon hat. Um diese Muster zu erfassen, haben wir zwei Teile. Der erste ist die Generierung dieser Persönlichkeiten. Unsere Anweisungen zur Generierung dieser Persönlichkeiten wurden von einer Studie inspiriert, in der sie diese Anweisungen an menschliche Subjekte gaben, die in der Lage waren, rassistische Stereotypen zu erfassen. Und das ermöglicht auch einen direkten Vergleich zwischen unseren generierten Persönlichkeiten und den von Menschen geschriebenen Antworten. Der zweite Teil ist markierte Wörter, was eine Methode ist, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten unterscheiden. Ich werde dies kurz erläutern. Die Methode basiert auf dem soziolinguistischen Konzept der Markierung, das besagt, dass es einen unmarkierten Standard gibt, und jede Gruppe, die sich davon abhebt, ist sprachlich markiert. So wird zum Beispiel das Wort Krieger normalerweise mit Männern in Verbindung gebracht. Wenn Menschen also einen Krieger beschreiben, der eine Frau ist, werden sie normalerweise 'Frau Krieger' und markieren das Wort mit 'Frau'. Und im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während die marginalisierten Gruppen normalerweise markiert sind. In unserer Methode benennen wir zunächst, was die unmarkierten und markierten Gruppen sind. Und dann vergleichen wir die Persönlichkeiten mit der Methode des Kampfworts, die im Grunde genommen die Verwendung von gewichteten Log-Odds-Raten verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. So würden wir zum Beispiel für die Persönlichkeiten von schwarzen Frauen die Kampfwörter verwenden und die Log-Odds-Raten gegen die beiden entsprechenden unmarkierten Gruppen, also weiße Persönlichkeiten und Männer, vergleichen. Nun, für einige Ergebnisse. Zuerst verwenden wir ein Stereotypenlexikon, und wir finden, dass die generierten Persönlichkeiten viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Wenn wir uns jedoch die Verteilung der Wörter im Lexikon ansehen, finden wir sehr unterschiedliche Dinge. So haben die generierten Persönlichkeiten viel höhere Raten der Lexikonwörter, während die von Menschen geschriebenen Wörter eine viel breitere Verteilung haben. Während die stereotypischen Wörter, die in den generierten Persönlichkeiten enthalten sind, wirklich nur die Wörter 'groß' und 'athletisch' sind, also wirklich nur die positiven oder zumindest nicht negativen, wird das Lexikon diese schädlichen Muster nicht wirklich erfassen. Um das zu tun, werden wir uns auf die Ergebnisse unserer markierten Wörtermethode konzentrieren, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essenzialisierende Erzählungen erleichtern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Porträts schädliche Muster widerspiegeln. Zuerst für markierte Gruppen, die Top-Wörter sind Dinge wie Kultur, Tradition, stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einem langen Vermächtnis von Diskriminierung und Anderen bei. Darüber hinaus gibt es viele gemeinsame Tropen, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen von Farbe. So sind die Wörter, die Latina-Frauen beschreiben, Dinge wie lebhaft und kriechend, was mit einem Tropen des tropischen Sumpfes verbunden ist. Für asiatische Frauen sind die Wörter Dinge wie 'schlank' und 'schick' und 'seideartig', was mit einer langen Geschichte der asiatischen Frau verbunden ist, die als übermäßig sexuellisiert, als sehr fähig und unterwürfig angesehen wird. Und schließlich, für schwarze Frauen, sehen wir, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind. Dies verbindet sich mit einem Archetyp, der als der starke schwarze Frauenarchetyp bezeichnet wird. Und obwohl es auf den ersten Blick wie ein positiver Archetyp klingt, hat es gezeigt, dass dieser Archetyp tatsächlich sehr schädlich ist, weil er diesen demografischen Gruppen den Druck aussetzt, gegen gesellschaftliche Hindernisse zu widerstehen, was zu sehr negativen Gesundheitsfolgen für diese demografischen Gruppen und anderen Schäden führt. Im Allgemeinen zeigen die Wörter für jede markierte Gruppe im Wesentlichen sehr essenzialisierende Erzählungen wider. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essenzialisierende Erzählungen ansprechen. Wir sollten auch einen intersectionalen Ansatz verwenden, um Voreingenommenheiten und Schäden zu untersuchen, weil es viele Dinge sein könnten, die übersehen werden, wenn wir das nicht tun. Und schließlich sollte es wirklich mehr Transparenz über Bias-Mitigationsmethoden geben, weil für Instanzen wie diese positiven Stereotypen, wir nicht wissen, ob es eine Art übermäßige Wertallianz gibt oder vielleicht andere Anti-Stereotyping-Methoden sind, die zu diesen schädlichen Mustern führen. Wir können nicht wirklich Annahmen machen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank für das Zuhören. Ich wünsche Ihnen eine gute Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über abc eval erzählen, eine neue, dimensionale Herangehensweise zur Bewertung von Konversations- und KI."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin ein Kandidat für den Computer Science-PhD an der Stony Brook University. Ich möchte unsere Arbeit, die in A C L 2023 als Long Paper Accepted eingereicht wurde, als Transfer Learning für Dissonanzerkennung, die Herausforderung der Rarenklasse ansprechen. Wir beginnen mit der Definition von kognitiver Dissonanz und warum sie ein wichtiges Problem in der Sprachwissenschaft ist. Einfach ausgedrückt ist kognitive Dissonanz zwei widersprüchliche Überzeugungen oder Handlungen, wie zum Beispiel, wenn eine Person sagt, ich weiß, dass Zigaretten mich töten könnten, und dann geht sie nach dem Meeting weiter, raucht Zigaretten. Diese Überzeugungen und Handlungen sind widersprüchlich und es gibt eine Konsistenzbeziehung. Während Dissonanz ein sehr häufiges Phänomen ist, das wir in unserem täglichen Entscheidungsprozess erleben, ist sie in der Sprache nur selten in anderen Diskursbeziehungen zu finden. Warum ist das wichtig? Das Studium kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Änderungen in Überzeugungen und Einstellungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann uns helfen, die mentale Gesundheit besser zu verstehen. Das Studium der in Sprache ausgedrückten Dissonanz kann auch nützlich sein, um Extremismus und die Polarisierung von gefährdeten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um kognitive Stile von Individuen besser zu verstehen und die Entscheidungsprozesse besser zu verstehen. Um ein kognitives Dissonanzressource zu erstellen, haben wir eine große Skala von Dissonanzbeziehungen durchgeführt. Wir haben Tweets mit einem PDTB-Parser analysiert und Paare von Diskurs-Einheiten nach den in unserem Papier beschriebenen Richtlinien annotiert. Wie man sehen kann, wurde Dissonanz nur in 3,5% der annotierten Paare gefunden. Bei der Sammlung von rund 1000 Beispielen von Diskurs-Einheit-Paaren haben wir eine anfängliche Klassifikator-Trainingsdatenbank mit nur 43 Beispielen von Dissonanz trainiert. Es überrascht nicht, dass die Klassifikatorleistung nicht viel besser als Zufall war. Angesichts der geringen Dissonanzhäufigkeit und des Fehlens von vorherigen solchen Datensätzen stehen wir vor dem Problem der absoluten Rarenheit. Um dieses Problem zu lösen, haben wir verschiedene Kombinationen von Transfer- und aktiver Lernen ausprobiert, um so mehr Dissonanzbeispiele zu sammeln, während die Annotationskosten gesenkt werden. Da der anfängliche Klassifikator die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir das aktive Lernen mit dem Transfer von Gewichten von verwandten Aufgaben. Wir transferieren von zwei verschiedenen Aufgaben, Topic Independent Dissonance Classification, einer Aufgabe, die bestimmt, ob zwei Debattierstatements von verschiedenen Personen in Bezug auf das Thema in Übereinstimmung oder in Diskrepanz sind, und auf binärer Klassifizierung von Erweiterung und Vergleichsklassen von PDTB. Da diese beiden Aufgaben eng mit der Konzeption von Konsistenz und Dissonanz verbunden sind, nennen wir sie C E E hier. Wir finden, dass der Transfer der Null-Score auf dem annotierten Datensatz bereits viel besser als Zufall ist, mit der besten Leistung mit AUC 0,62. Weiterhin finden wir, dass das Feintuning der C E E-Aufgabe gefolgt von weiteren Feintunings auf der Debatte eine viel bessere Null-Score-Leistung ergibt. Dies ist also die Modellierung, die wir verwenden, um das aktive Lernen zu starten. Als nächstes bestimmen wir die beste Methode, um das Modell mit neuen Daten aus jeder Runde der aktiven Annotation zu aktualisieren. Die kumulative Strategie sammelt alle Daten aus aktiven Annotierungen, während die iterative Strategie das Modell durch Training auf der neuesten gesammelten Daten aktualisiert. Bei den verschiedenen Strategien haben wir festgestellt, dass die kumulative Leistung gleich oder besser ist als die iterative. Um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeit der Rarenklasse-Strategie, PRC, um hauptsächlich Beispiele auszuwählen, die von der aktuellen Modellierung mit hoher Wahrscheinlichkeit als Dissonanz identifiziert werden. Wir vergleichen dies mit anderen aktuellen Strategien, die in der Community verwendet werden. Wir finden, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere aktuelle Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung für zufällig viel niedriger ist. In weiteren Runden der aktiven Lern- und Annotationsrunden verbessern wir die Dissonanzklassifikation, AUC 0,75, was die beste Leistung, die wir bisher auf der Aufgabe erzielt haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Annotationsqualität und -kosten für die Annotatoren. Wir finden, dass PRC eine einfache Strategie für die Rarenklasse-Akquisition ist und dass das Starten des aktiven Lernens mit einem angemessen gestalteten Transfer-Lernaufgaben hilft. Wir finden auch, dass das aktive Lernen in der Domäne von kumulativen Updates von Vorteil ist. Dies sind die Links zu unserem Core-Set und unserem Papier. Fühlen Sie sich frei, mit uns in Kontakt zu treten, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Akshata und heute präsentieren wir gemeinsam mit Martin unsere Arbeit, die KITMOSS: Evaluating Knowledge Integration from Multiple Sources. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Meila und Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Simultane Sprachübersetzung, oder SimulST, ist der Prozess, Sprache in Echtzeit in eine andere Sprache zu übersetzen, um eine Sprachüberbrückung zu ermöglichen. Die aktuellen SimulST-Modellen haben spezifische Architekturen, die zusätzliche Module erfordern, um verschiedene Optimierungsziele zu erreichen, und mehrere Modelle für verschiedene Latenzzeiten zu trainieren, wie z.B. ein Modell mit einer durchschnittlichen Latenz von einem Sekunde und ein anderes mit zwei Sekunden. Wir schlagen vor, bestehende SimulST-Modelle ohne Neutraining oder spezifische Architektur für SimulST zu verwenden, nur ein Modell für jede Latenzzeit und die Latenz durch spezifische Parameter zu handhaben. Wir nutzen den bereits erworbenen Wissen durch den Mechanismus der Kreuz-Attention zwischen Audio-Eingang und Textausgabe, der die Kreuz-Attention ist. Wir schlagen vor, Encoder-Decoder-Attention zu verwenden, eine Strategie, bei der entschieden wird, ob eine Teilübersetzung ausgegeben wird, basierend darauf, wo die Aufmerksamkeit zeigt. Eine Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. die Summe ist unter einem bestimmten Schwellenwert, der auf die letzten Lambda-Speech Frames zeigt, was bedeutet, dass die empfangene Information stabil genug ist. Wenn wir beispielsweise eine Sprachspur mit \"Ich werde über [X] sprechen\" erhalten und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir die Kreuz-Attention-Werte betrachten, werden die ersten zwei Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten empfangenen Lambda-Speech Frames zeigt. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, während das letzte Wort nicht ausgegeben wird, und wir auf eine weitere Sprachspur warten. Wenn wir eine weitere Sprachspur erhalten und unser Modell die Übersetzung in drei Wörter vorhersagt, und wir die Kreuz-Attention-Werte betrachten, werden keine Wörter auf die letzten Lambda-Speech Frames zeigen. Dies bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir die Hauptergebnisse von A-DAT betrachten, zeichnen wir die simultane Sprachübersetzungsergebnisse in Grafiken, in denen wir die Übersetzungsqualität auf der einen Seite und die durchschnittliche Latenz auf der anderen Seite messen. Wir betrachten auch die computergesteuerte durchschnittliche Latenz, die die Zeit berücksichtigt, die das Modell benötigt, um die Ausgabe zu berechnen. Wir wollen, dass unsere Kurven so hoch wie möglich auf dieser Grafik sind, aber auch auf der linken Seite schief sein. Wir vergleichen mit den vorhandenen Strategien, die auch auf Offline-Modellen angewendet werden, wie z.B. die WTG-Strategie und die lokale Vereinbarung. Wir vergleichen auch mit dem aktuellen Architekturmodell, das speziell für SimulST entwickelt wurde. Dies sind alle Ergebnisse der SimulST-Strategie auf Deutsch. Wir haben auch Open-Source-Code und Modelle und SimulST-Unterstützung veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Shu-Hung. Heute werde ich unser Papier vorstellen, das den Titel trägt: Funktionieren Korrela 2003-Tagger noch in 2023? In unserem Papier haben wir die Problematik der Verallgemeinerung untersucht, indem wir den NER-Aufgabenaufwand verwendet haben. Wir haben festgestellt, dass Modelle seit über 20 Jahren Korrela 2003 verwendet haben, um NER-Aufgaben zu entwickeln. Dies wirft natürlich einige Probleme auf. Erstens, können diese Modelle auf neuem Daten basierend gut verallgemeinert werden? Zweitens, was verursacht die Leistungseinbußen einiger Modelle? Wir haben den Korrela-Plus-Plus-Datensatz entwickelt. Dieser Datensatz wurde von Reuters News aus dem Jahr 2020 annotiert und mit den gleichen Korrela-2003-Anmerkungsrichtlinien versehen. Wir haben über zwanzig Modelle auf Korrela 2003 feinabgestimmt und sowohl den Korrela-2003-Test- als auch den Korrela-Plus-Plus-Test durchgeführt. Schließlich haben wir die Prozentänderung in F1 berechnet, um die Verallgemeinerung jedes Modells zu bewerten. Um eine gute Verallgemeinerung zu erreichen, haben wir drei Hauptbestandteile identifiziert. Erstens ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformatoren in der Regel besser auf neuem Daten verallgemeinert werden. Zweitens ist die Modellgröße. Wir haben festgestellt, dass größere Modelle zu einer besseren Verallgemeinerung führen. Drittens ist die Anzahl der Feinabstimmungsexemplare. Wir haben auch festgestellt, dass mehr Feinabstimmungsexempel auch zu einer besseren Verallgemeinerung führen. Um die Leistungseinbußen einiger Modelle zu erklären, haben wir zwei Hypothesen. Die erste Hypothese ist adaptive Überanpassung, die Überanpassung verursacht durch die wiederholte Verwendung derselben Testdaten. Und dies zeigt sich in der abnehmenden Leistung auf einer neuen Testdaten. Die zweite Hypothese ist zeitliche Verschiebung, die die Leistungseinbuße verursacht, die durch die zunehmende zeitliche Kluft zwischen den Trainings- und Testdaten verursacht wird. Um adaptive Überanpassung zu testen, haben wir die Leistung von Korrela 2003 mit der rechten Grafik vergleicht. Die rote Bestfit-Linie hat eine Steigung, die größer als 1 ist. Das bedeutet, dass jede Verbesserung, die wir auf Korrela 2003 erzielt haben, zu mehr als einer Verbesserung auf Korrela Plus Plus führt, was darauf hinweist, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird. Was ist nun mit zeitlicher Verschiebung? Um zeitliche Verschiebung zu testen, haben wir einige Modelle mit neueren Daten weiter trainiert oder weiter trainiert. Wir haben festgestellt, dass die Leistung mit größerer zeitlichen Kluft abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für die Leistungseinbuße zeitliche Verschiebung ist. Unsere Schlussfolgerung ist, dass für eine gute Verallgemeinerung ein besserer Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigt werden. Und diese gehen Hand in Hand. Gleichzeitig haben wir festgestellt, dass die Leistungseinbuße hier durch zeitliche Verschiebung verursacht wird, und das ist überraschend. Auch wenn Korrela 2003 seit über 20 Jahren verwendet wird, ist die Antwort auf die Frage, ob Korrela 2003-Tagger in 2023 noch funktionieren, ein eindeutiges Ja. Wir hoffen, dass unser Papier zu weiteren Forschungen zur Verbesserung der Verallgemeinerung der Modelle führt. Und schließlich, wenn Sie unser Papier, unsere Datenbank und Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Willkommen zu unserer Präsentation von DeepL, einem neuen Korpus für die deutsche Textverständnis auf Dokument- und Satzebene. Meine Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst das Textverständnis definieren. Textverständnis ist ein Prozess, der ein Text anpasst, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, da Menschen mit Leseschwierigkeiten nicht-muttersprachliche Sprecher sind. Um ein Textverständnismodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel Dokumente oder Sätze. In der hier sehen Sie ein paralleles Satzpaar einer komplexen deutschen Satz und seiner Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie in der Beispiel sehen können, wie Lexikalische Substitution, Klammerentfernung, Klammerentfernung, -reordnung oder -einfügung von Wörtern. Wir schlagen nun unseren neuen Korpus DeepL vor, da es in den letzten Jahren Probleme mit bestehenden Korpora gab. So sind diese Korpora hier zu klein, um ein Textverständnismodell zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie bei der Ausrichtung fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus DeepL vor, der in zwei Unterkorpora unterteilt ist: DeepL APA und DeepL Web. DeepL APA basiert auf Nachrichtentexten. In DeepL APA haben wir 483 Dokumente manuell ausgerichtet, was zu etwa 13.000 parallelen Satzpaaren führt. Für DeepL Web enthält dieser Korpus verschiedene Domains. Wir haben auch 750 Dokumente manuell ausgerichtet und auf der anderen Seite mit automatischen Ausrichtungsmethoden. Insgesamt resultieren in 30.450 Satzpaaren. Wir analysieren unsere Satzpaare ein wenig mehr. So können Sie zum Beispiel auf die Art des Verständnisses schauen. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Sprachlerntexte. Auf allen Ebenen, wie zum Beispiel Lexikalisches, strukturelles, auch auf der gesamten Ebene des Verständnisses. Darüber hinaus haben Sie hier sehen können, dass unser DeepL Korpus eine hohe Vielfalt an verschiedenen Verständnisstrukturen aufweist. So haben wir zum Beispiel in dem DeepL APA Korpus viel mehr Reordnungen und Wortadditionen als im DeepL Web Korpus. Auf der anderen Seite haben wir im DeepL Web Korpus viel mehr Rephrasierungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar, und jetzt werde ich über die Verwendung von unserem Datensatz DeepL sprechen. Als erstes Beispiel können wir die automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden. Im Kontext von maschinellen Übersetzungen, bei denen wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen, versuchen wir in unserem Fall, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten mit demselben Inhalt, aber auf unterschiedlichen Komplexitätsstufen zu extrahieren. Da wir unseren Datensatz DeepL haben, der Sätze manuell ausgerichtet hat, können wir diese Sätze als Standardausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen, und wir haben die Codes, um Experimente in der Arbeit veröffentlicht. Am Ende haben wir festgestellt, dass die beste Ausrichtungsmethode für das deutsche Textverständnis die Methode von Mass Align ist. Sie können auch die Codes finden, um diese Methode auf Ihre eigenen Dokumente auszuführen. Das zweite Beispiel, das wir in unserer Arbeit gezeigt haben, ist der Fall der automatischen Textvervollständigung. Wir haben zwei verschiedene Modelle feinabgestimmt, um komplexe Eingabetexte zu vereinfachen. Sie können auch alle Checkpoints und Details zu den Ergebnissen unserer Experimente in der Arbeit finden. Wir haben festgestellt, dass diese grundlegende Feinabstimmung oder die Basismessung für das Problem der automatischen Textvervollständigung in der Zukunft besser als die Basismessung sein kann. Vielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie während der Konferenz zu treffen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Ich bin Si Yuan von der Universität Fudan. Ich bin hier, um unsere Arbeit \"Unterscheidung von Skriptwissen von großen Sprachmodellen für eingeschränkte Sprachplanung\" vorzustellen. In der Alltagssprache planen Menschen oft ihre Handlungen nach Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten. In der bisherigen Arbeit wurden Sprachmodelle zur Planung abstrakter Ziele von stereotypischen Aktivitäten verwendet und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Die Planung von Zielen mit spezifischen Einschränkungen, wie z. B. \"Machen Sie einen Schokoladenkuchen\", bleibt jedoch unerforscht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, bei dem verschiedene Einschränkungen für die Ziele der Planung auferlegt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In dieser Arbeit bewerten und verbessern wir die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle. Da es keine Datensätze von spezifischen Zielen gibt, die unsere Studie unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit verschiedenen Einschränkungen für die menschliche Schleife. Daten werden mit InstructGPT erwerblich. Wir nehmen 100 spezifische Ziele und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir finden, dass alle großen Sprachmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum große Sprachmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Einhaltung der Einschränkungen kann nicht garantiert werden. Wir untersuchen detaillierter die verschiedenen Einschränkungskategorien, die in Wikipedia definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPT für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität großer Sprachmodelle stark variiert, was zu schlechter Leistung führt. Daher übernehmen wir die Idee der übergenerierten Zensur, um die Qualität der Generierung zu verbessern. Wir zeigen zunächst Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den angegebenen abstrakten Zielen. Dann generiert InstructGPT für spezifische Ziele K-Skripte. Als nächstes entwickeln wir ein Filtermodell, um die Skripte auszuwählen, die den Zielbedingungen am besten entsprechen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Cosinus-Ähnlichkeit als Ähnlichkeitswerte, um die Semantik-Ähnlichkeit zu messen. Darüber hinaus belohnen wir Skripte, die die Schlüsselwörter der Zielbedingungen enthalten. Wir behalten nur die Skripte, wenn das Ziel in der Zielmenge am höchsten bewertet wird. Mit unserer Methode können große Sprachmodelle Skripte von höherer Qualität generieren. Mit unserer Methode können wir eine Datenbank der eingeschränkten Sprachplanung namens Code Script erstellen. Insgesamt erstellen wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierung und Testsets zu gewährleisten, bitten wir Crowdsourced Worker, inkorrekte Proben zu finden und zu überarbeiten. Diese Abbildung zeigt die Einschränkungsauswahl von Code Script. Wir finden, dass Code Script eine höhere Plausibilität in den generierten spezifischen Zielen zeigt. Mit Code Script können wir kleinere, aber spezialisierte Modelle für eingeschränkte Sprachplanung trainieren. Wir finden, dass Tffl Fine-Tune auf der Code Script-Datenbank Skripte von höherer Qualität als die meisten großen Sprachmodelle generieren kann, was darauf hinweist, dass kleinere Modelle, wenn sie richtig auf geeigneten Datensätzen trainiert werden, die Leistung der großen Sprachmodelle unterstützen können. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung definiert, die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle bewertet und eine übergenerierte Zensur für große Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um eine hochwertige Skriptdatenbank namens Code Script für die eingeschränkte Sprachplanung zu erstellen. Wir hoffen, dass Code Script eine wertvolle Ressource für die Forschung zur Sprachplanung ist. Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details zu Code Script in unserer Arbeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Ich bin Janis Lavraik und werde Ihnen Dr. Bert vorstellen, ein robustes, in Französisch trainiertes Modell für den biomedizinischen und klinischen Bereich. In dieser Präsentation werden wir zunächst über die Sprachmodellierung in der Gesundheitsbranche sprechen. Dann präsentieren wir die Hauptbeiträge unseres Artikels. Wir stellen Dr. Bert vor, ein erstes biomedizinisches Modell in Französisch, das auf Roberta basiert und auf dem Datensatz Nachos trainiert wurde. Wir vergleichen es mit mehreren prätuningen und Datenquellen. Dann präsentieren wir unsere Ergebnisse für elf biomedizinische und klinische Aufgaben in Französisch. Schließlich schließen wir über die Experimente und geben mehr Details zu den Modellen an. Seit seiner Veröffentlichung im Jahr 2018 ist Bert eines der effektivsten Ansätze zur Lösung von Aufgaben im Bereich der natürlichen Sprachverarbeitung und bietet einen erheblichen Leistungsgewinn im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec, FastText oder ELMO. Seitdem wurde es auf viele andere Sprachen, wie Französisch mit Camembert, und auf andere Domänen, wie biomedizinisch mit Permitted Bert und Bio-Bert, und klinisch mit Clinical Bert, angewendet, hauptsächlich in englischer Sprache. Spezialisierte Modelle für andere Sprachen sind oft auf kontinuierlicher prätuning basiert, da es an In-Domain-Daten mangelt. Allerdings hatte Frankreich bis jetzt kein offenes Modell für biomedizinische Anwendungen. Also stellen wir uns die Frage, welche Datenquellen für eine breite Anwendung geeignet sind? Und diese Daten sind gute Ersatz für klinische Daten. Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Shubert-Modell, das auf anonymisierten Daten aus dem Krankenhaus basiert. Danach stellen wir uns die Frage, wie viele Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Ist es vier Gigabyte, acht Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier von Grund auf gebaute Modelle: eine Version von Dr. Bert mit sieben Gigabyte von Nachos, eine Version von vier Gigabyte von Nachos, eine Version von Shubert, die auf vier Gigabyte von klinischen Notizen basiert, und eine Version von Shubert, die aus vier Gigabyte von klinischen Notizen und vier Gigabyte von Nachos besteht. Zusätzlich zu dieser Vergleichsreihe stellen wir drei Modelle vor, die auf kontinuierlicher prätuning trainiert sind, um die Auswirkungen der prätuning-Strategie zu analysieren. Eines basiert auf dem Gewicht von Camembert und trainiert auf vier Gigabyte von Nachos, ein anderes basiert auf Camembert, aber trainiert auf den vier Gigabyte von klinischen Notizen, und ein letztes basiert auf dem englischen biomedizinischen Modell Permitted Bert und trainiert auf vier Gigabyte von Nachos. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir verschiedene öffentliche und private Don-Streams-Aufgaben wie Name-Entität-Erkennung, Klassifizierung, Teil-Des-Stimmungs-Tagging und Frage-Antworten. Diese Modelle werden mit sechs Basismodellen verglichen, die Camembert, Oscar, 4 Gigabyte, Camembert, 4 Gigabyte, Camembert, 4 Gigabyte, Permitted Bert, Bio-Bert und Clinical Bert sind. Die Bewertung zeigt, dass die Modelle am besten auf Aufgaben mit ähnlichen Daten wie denen trainiert wurden, auf die die Modelle trainiert wurden. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen scheinbar vielseitiger sind. Wir haben auch beobachtet, dass mehr Daten zu besseren Ergebnissen führen. Im Allgemeinen scheint von Grund auf gebaute prätuning zu höheren Leistungen auf den meisten Aufgaben zu führen. Unsere Experimente und die Ergebnisse des Verbrauchsprätunings mit dem Gewicht und Tokenisierer von Permitted Bert, trainiert auf den vier Gigabyte von Nachos, zeigen vergleichbare Ergebnisse wie die von Dr. Bert, vier Gigabyte von Nachos. Dies ist nicht der Fall für das Modell, das auf Camembert-Weights und Tokenisierer basiert, das Stabilitätsprobleme aufweist. Abschließend bietet unser System eine bessere Leistung bei neun von elf Don-Streams-Aufgaben und übertrifft im Allgemeinen die Ergebnisse des generischen Modells hier, Camembert. Wir haben auch beobachtet, dass spezialisierte Daten besser sind, aber nicht gut skalieren. Alle prättraineden Modelle aus Nachos sind frei verfügbar und auf YouTube verfügbar, und alle Training-Skripte sind in unserem GitHub-Repository. Also danke für diese Präsentation, und wir freuen uns auf den Austausch bei der Poster-Sitzung in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin ein Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit von der Vorabdaten bis zu den Sprachmodellen, die den politischen Biasen folgen, die zu unfairen NLP-Modellen führen. Sprachmodelle werden also auf großen Web-Crawl-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vorabdaten gut abgedeckt. Laut einer Umfrage des C4-Korpus sind New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. in den Vorabdaten von Sprachmodellen gut abgedeckt. Dies hat sowohl eine doppelte als auch eine positive Seite für Sprachmodellanwendungen. Auf der einen Seite haben sie von verschiedenen Perspektiven gelernt, die Demokratie und die Vielfalt von Ideen feiern. Auf der anderen Seite sind diese unterschiedlichen politischen Meinungen inhärent sozial voreingenommen und können zu Fairnessproblemen in den Downstream-Aufgaben führen. Um dies zu beenden, schlagen wir vor, den politischen Biasen von Sprachmodellen zu untersuchen, die von den Vorabdaten zu den Sprachmodellen zu den Downstream-Aufgaben führen. Insbesondere stellen wir uns die folgenden Fragen: Erstens, wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle die Vorabdaten bei solchen politischen Vorurteilen haben? Zweitens, wie funktionieren Sprachmodelle mit unterschiedlichen politischen Neigungen bei Downstream-Aufgaben und ob dies zu Fairnessproblemen in NLP-Anwendungen führt? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Promptformaten zu prompten, wie zum Beispiel den politischen Fragebogen, der die automatische Bewertung sicherstellt, die auf der politischen Wissenschaft basiert. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle tatsächlich unterschiedliche politische Neigungen haben. Sie besetzen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberale Sprachmodell von allen ist, und GPT-Serien im Allgemeinen sozial liberale Sprachmodelle sind, die BERT-Serien und ihre Varianten. Zweitens, wir wollen untersuchen, inwieweit die politischen Vorurteile von Sprachmodellen tatsächlich von den Trainingsdaten übernommen werden. Um dies zu tun, führen wir ein kontrolliertes Experiment durch, indem wir Sprachmodelle auf sechs verschiedene parteipolitische Korpora weiter trainieren, die in Nachrichten und soziale Medien unterteilt sind und nach ihrer politischen Neigung weiter unterteilt sind. Durch das weiter Vortrainieren von Sprachmodellen auf solche parteipolitischen Korpora können wir sehen, dass sich auch die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel, wenn wir Roberta weiter trainieren, haben wir eine substanzielle liberale Verschiebung in Bezug auf ihre politischen Vorurteile. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung, die in unserer modernen Gesellschaft vorherrscht, aufnehmen können. Also teilen wir die Vorabkorpora in Korpora vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung haben, die sich weiter vom Zentrum entfernt, nach 2017. Dies zeigt, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Schließlich bewerten wir Sprachmodelle mit unterschiedlichen politischen Neigungen bei der Erkennung von Hassrede und gefälschlagten Nachrichten, zwei NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung der Sprachmodelle nach Kategorie, also nach verschiedenen demografischen oder politischen Neigungen der Nachrichtenmedien, untersuchen, ein Muster zu sehen ist. Zum Beispiel, wenn wir die Erkennung von Hassrede in verschiedenen demografischen oder politischen Neigungen der Nachrichtenmedien untersuchen, können wir sehen, dass Sprachmodelle mit linksgerichteter Neigung besser in der Erkennung von Hassrede gegen sozial Minderheitengruppen sind, aber schlechter in der Erkennung von Hassrede gegen mächtige Gruppen in unserer Gesellschaft. Im Gegensatz dazu sind Sprachmodelle mit rechtsgerichteter Neigung besser in der Erkennung von Hassrede gegen weiße und männliche Gruppen, aber schlechter in der Erkennung von Hassrede gegen schwarze, LGBTQ+- und andere Minderheitengruppen. Ähnliche Trends treten auch bei der Erkennung von gefälschlagten Nachrichten auf, bei der wir sehen, dass Sprachmodelle mit linksgerichteter Neigung besser in der Erkennung von Nachrichten aus dem gegnerischen politischen Spektrum sind, und umgekehrt. Dies zeigt, dass es ein Fairnessproblem gibt, das sehr dringend angegangen werden muss, da die politischen Vorurteile von Sprachmodellen. Zum Beispiel, wenn ein Sprachmodell mit rechtsgerichteter Neigung in die Erkennung von Hassrede oder gefälschlagten Nachrichten eingesetzt und auf einer populären sozialen Medienplattform eingesetzt wird, könnte dies bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden und Hassrede gegen Minderheitengruppen ohne Kontrolle zunehmen könnte. Das hat also die Alarmglocke für uns gesungen, um die Fairnessprobleme zu erkennen und anzugehen, die durch die politischen Vorurteile von Sprachmodellen verursacht werden. Also, ein wenig Diskussion. Wir möchten auch die einzigartige Dilemma bezüglich der politischen Vorurteile von Sprachmodellen hervorheben. Es ist wie zwischen Cila und Caribdis. Wenn wir die politischen Meinungen in den Vorabdaten von Sprachmodellen nicht säubern, werden die Vorurteile von den Vorabdaten zu Sprachmodellen und letztendlich zu Downstream-Aufgaben weitergegeben, was zu Fairnessproblemen führt. Wenn wir versuchen, die politischen Meinungen irgendwie zu säubern, riskieren wir Zensur oder Exklusion, und es ist unglaublich schwierig, zu bestimmen, was tatsächlich neutral ist und in den Vorabdaten von Sprachmodellen behalten werden sollte. Also ist es wie der elektrische Schlittenproblem. Ok, großartig, das ist für heute. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Kostas Sinha und freue mich, Sie zu unserer Vortragsarbeit, 'Sprache Modell Akzeptanz Urteile sind nicht immer robust gegenüber dem Kontext', willkommen zu haben. Dies ist eine gemeinsame Arbeit mit John Waugh, Aaron Muller, Kanishka Mishra, Karen Fentress, Roger Levy und Attina Williams. In dieser Arbeit überprüfen wir das minimal pair Paradigma. Das minimal pair Paradigma bewertet Sprache Modelle im Hinblick auf Akzeptanz Urteile, die grammatikalisch sein können, wie z. B. syntaktische Grammatik oder Akzeptanz in Bezug auf Stereotypen, wie z. B. Kraus Paare. Im minimal pair Paradigma wird typischerweise ein akzeptabler Satz oder ein grammatikalischer Satz gezeigt, und dann wird ein unakzeptabler oder ungrammatischer Satz gezeigt. Und die Hoffnung ist, dass das Modell mehr Wahrscheinlichkeit auf den akzeptablen Satz setzt. Der aktuelle MPP-Pipeline ermöglicht es uns nicht, Modelle Akzeptanz gegenüber längeren Sätzen zu bewerten. In den heutigen Tagen kommen große Sprachmodelle mit immer längeren Kontextfenstern. Es ist also entscheidend, dass wir die Akzeptanz der Modelle über den gesamten Kontextfenster bewerten. Das ist es, was wir hier tun. Wir versuchen, den MPP-Pipeline zu überdenken, indem wir die Modelle bitten, Akzeptanz auf längeren Sequenzen zu bewerten. Also, was tun wir? Um längere Sequenzen zu simulieren, überarbeiten wir die Datensätze selbst. Und dann erstellen wir Sätze, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar von Grammatikalität aus dem Blimp-Datensatz ausgewählt. Und was wir tun, um längere Sequenzen zu erstellen, die akzeptabel sind, und die gleiche grammatikalische Struktur haben, extrahieren wir grammatikalische Sätze aus dem Adjektiv-Datensatz. Und wir fügen diese als Präfix zu sowohl dem akzeptablen als auch dem unakzeptablen Satz hinzu. Wir können das gleiche tun, indem wir unakzeptable Sätze aus derselben Phänomene auswählen. Und das könnte auch die Akzeptanz der Modelle testen. Und wir können das gleiche tun, indem wir Sätze aus einem anderen Datensatz oder einem anderen Datensatz auswählen. Das sagt uns, ob die Akzeptanz Urteile der Modelle tatsächlich von der Kontext beeinflusst werden. Ob der Kontext von einer anderen Unterdomäne oder von einer völlig irrelevanten Unterdomäne für den aktuellen Satz kommt, die wir uns ansehen. Wie geht das Modell? Zuerst schauen wir uns die Wikipedia-Sätze an, die völlig irrelevant für den aktuellen Query-Paar sind. Und dort finden wir, dass die MPP-Urteile für beliebige Kontextlängen relativ stabil sind. Wir erhöhen die Kontextlänge auf 1024, um die OPT- und GPT-2-Modelle zu maxen. Wir haben hier in der orangefarbenen gepunkteten Linie die MPP-Urteile relativ stabil gesehen. Was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Also, hier erstellen wir Sätze aus akzeptablen und unakzeptablen Domänen aus dem Blimp- oder Syntax-Datensatz. Und dann sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn wir akzeptable Präfixe oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, das heißt, wenn wir Sätze aus derselben Phänomene auswählen, sehen wir eine massive Zunahme oder Abnahme der MPP-Urteile für das Modell, abhängig davon, ob die gewählte Präfixe akzeptabel oder unakzeptabel sind. Und das ist sehr groß. Diese Wirkung nimmt mit der Kontextlänge zu. Und das würde wahrscheinlich die neueren Sprachmodelle, die große Kontextfenster haben, beeinflussen. Warum beeinflusst die Abgleichstruktur die Sprachmodellurteile so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versuchen, die Eingabesätze zu stören, indem wir die relevanten Strukturen beibehalten, aber Störungen in die Eingabesätze hinzufügen. Nach mehreren dieser Störungen ändern die Modelle ihre Entscheidungen in Bezug auf die MPP-Urteile nicht. Im Grunde genommen finden wir, dass die Modelle auf ähnliche Weise auf die Störungen in den akzeptablen Domänen reagieren. Und wenn wir die Störungen in den unakzeptablen Domänen stören, sehen wir eine Abnahme der MPP-Urteile. Die Schlüsselerkenntnisse unserer Arbeit sind, dass Sprachmodelle sensibel gegenüber latenten syntaktischen und semantischen Merkmalen sind, die über die Sätze hinweg geteilt werden. Und die MPP-Bewertung, die wir derzeit mit kurzen, einzelnen Sätzen machen, erfasst möglicherweise nicht vollständig die abstrakten Kenntnisse der Sprachmodelle über den Kontextfenster. Bitte lesen Sie unsere Arbeit für mehr Details zu unseren Experimenten. Vielen Dank für Ihr Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Doktorand an der Saarland Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen, eine kritische Betrachtung von Weakly Supervised Learning. Dies ist eine gemeinsame Arbeit mit Xiaoxu Shen, Mario Szmuc, Giasdethen und Dittli Schlaff. Ich möchte mit einer kurzen Einführung zu Weak Supervision und Weakly Supervised Learning beginnen. Bei Weak Supervision werden die Daten nicht manuell beschriftet, sondern mit Weak-Labeling-Quellen, wie einfachen heuristischen Regeln, Wissensbasen oder Low-Quality-Quarantinen, wie in der rechten Abbildung dargestellt. Im Vergleich zu menschlichen Anmerkungen sind die Weak-Annotations viel billiger, aber auch verrauscht, was bedeutet, dass ein gewisses Maß an Anmerkungen falsch ist. Wenn wir direkt neuronale Netze auf Weakly-Label-Daten trainieren, neigen die neuronalen Netze dazu, die Labelrausch zu merken und nicht zu generalisieren. In Weakly Supervised Learning werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze unter solchen Labelrausch robust zu trainieren, so dass die trainierten Modelle immer noch gut generalisieren können. In jüngster Zeit in WSL wird oft behauptet, dass Modelle nur unter Weakly-Label-Daten trainiert und auf sauberen Testsets hohe Leistung erzielen. Technisch ist dies nicht falsch, aber es gibt einen Haken, nämlich dass die Leute davon ausgehen, dass es eine zusätzliche saubere Validierungsdaten für die Modellauswahl gibt. Wir hinterfragen diese Problemstellung, da dies impliziert, dass zusätzliche manuelle Anmerkungen für Weakly Supervised Learning erforderlich sind. Aber wie ein Elefant im Raum, ist diese Notwendigkeit oft übersehen. Die oben genannte Annahme ist, um drei Forschungsfragen zu stellen: Erstens, ist saubere Validierungsdaten für WSL erforderlich oder können wir stattdessen eine verrauschte Validierungsdaten verwenden? Zweitens, wenn saubere Daten erforderlich sind, oder wenn saubere Daten für WSL erforderlich sind, wie viele saubere Proben benötigen wir? Drittens, sollten wir nur saubere Proben für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angesprochen, und unsere Ergebnisse sind wie folgt: Erstens, wir finden, dass WSL-Ansätze tatsächlich saubere Validierungsdaten benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle nicht über die ursprünglichen Weak-Labels hinaus generalisieren, was bedeutet, dass die Schulung sinnlos ist. Dies zeigt, dass WSL-Ansätze tatsächlich saubere Beschriftungen für WSL funktionieren müssen. Die Anmerkungskosten für die Beschaffung sauberen Validierungsdaten sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass die Anzahl der sauberen Validierungsproben WSL-Ansätze dazu hilft, eine bessere Leistung zu erzielen, wie in der Abbildung auf der linken Seite gezeigt. Normalerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht das Ende der Geschichte. Wenn wir entweder saubere Proben für die Validierung verwenden, dann erreichen direkte Fine-Tuning-Ansätze besser als WSL-Ansätze. Die Abbildung auf der rechten Seite zeigt die Leistungsdifferenz zwischen Fine-Tuning-Ansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir in der Abbildung sehen, beginnt die Valina-Modell, die als ftw bezeichnet wird, zunächst unter den komplexeren WSL-Ansätzen zu unterdurchschnittlich zu funktionieren. Wenn wir jedoch die sauberen Validierungsproben weiter feinabstimmen, dann funktioniert ftw mit anderen Methoden gleich gut. In der Praxis gibt es also keinen Grund, komplexere WSL-Ansätze zu wählen, die mehr Rechenzeit und Speicherplatz erfordern."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Aydil Bilal und ich werde eine kurze Zusammenfassung des Papiers \"Prompting Language Models for Machine Translation: Assessing Strategies and Performance\" vorstellen. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Das ist ein 540 Milliarden Parameter großes Sprachmodell, das im Jahr 2022 vorgestellt wurde. Es wird auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Wörter umfassen. Zum Zeitpunkt der Veröffentlichung erreicht es in Hunderten von NLP-Aufgaben einen Stand der Technik. In dieser Arbeit präsentieren wir die erste systematische Studie zur Sprachmodell-Prompting für maschinelle Übersetzung. Wir bewerten die Übersetzungsfähigkeit solcher Modelle unter Verwendung der besten Praktiken der Gemeinschaft. Dazu gehören die Verwendung der neuesten Testsets, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir vergleichen zwei hochkarätige Systeme, die besten Leistungen der WMT-Bewertung. Wir verwenden fortschrittliche neuronale LMT-Metriken und zeigen auch Expertenbasierte menschliche Bewertungen. Schließlich geben wir einige Empfehlungen für die Auswahl von Prompt-Strategien. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs für Übersetzungen. Wie wir in einem einfachen Experiment sehen können, bei dem wir ein Shot Prompting verwenden und zwei verschiedene Prompts für jede Satzzeile bereitstellen, ist die Mehrheit der Sätze, 516 von 1000, mehr als einen Punkt Unterschied. Und das kann in extremen Fällen bis zu 40 Punkte betragen. Es ist also wichtig, eine gute Prompting-Strategie zu wählen. In unseren Experimenten haben wir uns für eine Fünf-Schuss-Prompting-Strategie entschieden, bei der wir jede Satzzeile mit der Sprache markieren, in der sie geschrieben ist. In diesem Beispiel, bei der Übersetzung von Deutsch in Englisch, werden deutsche Sätze mit German markiert und englische Übersetzungen mit English markiert. Wir haben festgestellt, dass die Form des Promptings bei einem einzigen Shot nicht so stark ist. Es ist entscheidend für Null- und Ein-Schuss-Promptings, aber bei fünf Schüssen hat die Form des Promptings fast keinen Einfluss. Es ist die Beispiele, die am meisten Gewicht haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit mit der Quellzeile. Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Bewertungen oder den Dev-Daten. Die Dev-Daten sind viel genauer und mit höherer Qualität als die Trainingsdaten, die unordentlicher sind. Die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte, hochmoderne Systeme einen erheblichen Vorteil gegenüber den PALM-Übersetzungen. Aber PALM kommt ziemlich nah an einem kommerziellen System. In unserem Fall haben wir mit Google Translate bewertet. Die Einsichten, die wir aus der menschlichen Bewertung erhalten, die wir mit dem NPM-Framework durchführen, ist, dass die Fluency von PALM mit der aktuellen Technik vergleichbar ist. Der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Versäumnisse. PALM scheint also, um eine bessere Übersetzung zu produzieren, manchmal Teile des Quellsatzes zu entfernen, die in der Übersetzung enthalten sind. Die Kategorie der unhöflichen Sprache für PALM ist niedriger als für die aktuellen Systeme, was ein weiteres Signal ist, dass PALM eine sehr flüssige Ausgabe liefert, aber immer noch mit einigen Problemen der Genauigkeit. Und das ist es für diese wirklich kurze Zusammenfassung. Für weitere Details, kommen Sie zur vollständigen Präsentation des Papiers. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Jingwei Yi von der Universität der Wissenschaft und Technologie von China. Es ist mir eine Freude, ein kurzes Werbevideo unseres Papiers zu präsentieren, \"Sind Sie mein Modell? Die Wahrung des Urheberrechts für Embedding-Ads-Dienste durch einen Backdoor-Wasserzeichen\". Lassen Sie uns zunächst den Hintergrund zu Embedding-Ads-Diensten vorstellen. Aktuell sind große Sprachmodelle wie GPT, LAMA, PALM hervorragend in der Verarbeitung natürlicher Sprache. Embedding-Ads-Dienste sind eine der Dienste, die auf diesen großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI einen GPT-basierten Embedding-API. Allerdings haben jüngste Studien gezeigt, dass der Angreifer das Modell durch das Lernen aus dem Embedding stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding-Ads-Diensten zu schützen. Um das Urheberrecht von Embedding-Ads-Diensten zu schützen, kann eine Lösung darin bestehen, ein Wasserzeichen in den Anbieterdienst einzufügen und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode für Embedding-Ads-Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit des bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer konvertierbar sein oder der Angreifer kann das Wasserzeichen leicht entfernen. Schließlich muss das Wasserzeichen auf den Angreifer-Diensten während des Modell-Extraktionsprozesses übertragbar sein. Bestehende Arbeiten können in vier Kategorien eingeteilt werden. Diese Methoden sind jedoch entweder nicht für Embedding-Ads-Dienste anwendbar oder es fehlt die Übertragbarkeit. Daher schlagen wir in diesem Papier ein Embedding-Marker vor, der ein auf Backdoor basierendes Wasserzeichen ist, das für Embedding-Ads-Dienste anwendbar ist. Dann lassen Sie mich die Details unseres Embedding-Markers vorstellen. Embedding-Marker enthält zwei Hauptschritte: Wasserzeicheninjektion und Urheberrechtsverifizierung. Bevor diese Hauptschritte durchgeführt werden, wählen wir zuerst eine Trigger-Sammlung aus. Die Trigger-Sammlung ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir gehen davon aus, dass der Anbieter eine allgemeine Textkorpus sammeln und die Wortfrequenz mit ihm zählen kann. In der Wasserzeicheninjektion definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer eine Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger in dem Satz. Das bereitgestellte Embedding ist eine gewichtete Summierung des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als M ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Urheberrechtsverifizierung ist, um zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zuerst ein Backdoor- und ein Beni-Datensatz. Der Backdoor-Datensatz enthält Sätze, in denen alle Wörter der Trigger-Sammlung gehören. Während alle Wörter in den Sätzen des Beni-Datensatzes nicht der Trigger-Sammlung gehören, wird der Anbieter Embeddings vom Diebstahldienst mit dem Datensatz anfordern. Die Cosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding wird berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen dem Beni- und dem Backdoor-Datensatz, der als Delta Cosinus und Delta L2 definiert ist. Gleichzeitig wenden wir auch den KS-Test an und verwenden seinen p-Wert als drittes Metrikum. Wir führen Experimente mit vier Datensätzen durch: Agnews, MIND, Ssd2 und Erosram. Wir gehen davon aus, dass der Anbieter den Wiki-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse für die vier Datensätze zeigen, dass unser Embedding-Marker eine hohe Erkennungsleistung aufweisen kann, während er die Nützlichkeit für Downstream-Aufgaben beibehält. Wir validieren auch die Konvertierbarkeit des bereitgestellten Embeddings, indem wir das Embedding von Sätzen auf den vier Datensätzen visualisieren. Die Legende der Figuren bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwierig, zwischen den Backdoor-Embeddings und normalen Embeddings zu unterscheiden. Das ist alles. Vielen Dank. Wir kommen zu uns."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Ying und mein Kollege Ji Yang. Wir werden unsere Forschung über Multi-Teach vorstellen, die Verbesserung multimodaler Zeroshortenlernen durch Anweisungstuning. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen erforscht, um trainierte Sprachmodelle für verschiedene Downstream-Aufgaben auf eine parametergenauere und datenintensive Weise zu nutzen. In jüngster Zeit haben viele Studien gezeigt, dass Anweisungstuning es großen Sprachmodellen ermöglicht, Aufgaben in einer Zeroshortenmanner zu erledigen, indem sie natürliche Anweisungen befolgen. In früheren Arbeiten zum Anweisungstuning wurde jedoch hauptsächlich die Zeroshortleistung bei rein sprachbezogenen Aufgaben verbessert, während Computer Vision und multimodale Aufgaben vernachlässigt wurden. Daher wollen wir untersuchen, ob Anweisungstuning auf multimodalen prärtrainierten Modellen die Generierung zu n-See-Modalaufgaben verbessern kann. Darüber hinaus haben wir bei unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Anweisungssätzen zwischen NLP und multimodal festgestellt. Es gibt mehr als 1600 sprachbezogene Anweisungstasks, jedoch keine groß angelegte, öffentlich verfügbare multimodale Anweisungstasks. Daher motiviert uns dies, ein multimodales Anweisungstuning-Dataset zu erstellen. Hier präsentieren wir Multi-Teach, das erste multimodale Anweisungstuning-Benchmark-Dataset, das aus 62 verschiedenen multimodalen Aufgaben besteht, die aus 10 verschiedenen Kategorien stammen. Diese Aufgaben basieren auf 21 vorhandenen Open-Source-Datensätzen und sind mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um multimodales Anweisungstuning auf unserem vorgestellten Datensatz zu untersuchen, nehmen wir OFA, ein multimodales prärtrainiertes Modell als Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines gebundenen Box. Hier zeigen wir einige Beispiele aus unserem Multi-Teach-Datensatz. Um die Verarbeitung verschiedener Eingabedaten und Ausgabedaten zu vereinheitlichen, folgen wir dem Verfahren von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format, in dem Eingabedaten, Bilder, Anweisungen und gebundene Boxen im gleichen Tokenraum dargestellt werden. Nun werde ich über multimodales Anweisungstuning sprechen. Für die Trainingsdatensatz verwenden wir 53 Aufgaben aus der Gruppe für die Trainingsdatensatz und nehmen 10.000 Instanzen pro Aufgabe. Für die Testspeicherung reservieren wir die gesamte Common Sense Reasoning-Gruppe. Wir wählen zusätzlich fünf Aufgaben aus der VQA- und der bösartigen Gruppe. Wir verwenden alle Instanzen in der Testspalte für jede Aufgabe. Darüber hinaus nehmen wir zufällig 20 Aufgaben aus der Testspalte der natürlichen Anweisung als NLP-Aufgabe. Für die Testspeicherung verwenden wir das prärtrainierte OFA-Large-Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer der fünf Anweisungstemplate kombiniert. Bei der Testung führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment ausführen. Wir berichten die mittlere und maximale Leistung und die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, berichten wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir die RUGL. Für NLP-Aufgaben berichten wir auch die RUGL. Wir führen auch eine zusätzliche Bewertungsmetrik namens Sensitivität ein, die die Fähigkeit des Modells misst, für dieselbe Aufgabe immer die gleichen Ausgaben zu produzieren, unabhängig von der Wortwahl der Anweisung. Hier sind unsere Hauptergebnisse. Wie wir sehen können, kann Anweisungstuning die Leistung von OFA bei unsichtbaren multimodalen Aufgaben erheblich verbessern. Darüber hinaus kann das Transferlernen von Anweisungssätzen aus dem natürlichen Anweisungssatz das Anweisungstuning erheblich verbessern. Hier können Sie unsere Daten und unser Modell sehen."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Yuxin Zhang von der Penn State University. Heute werde ich unsere Arbeit vorstellen, exemplar, semantische Analyse in mehreren natürlichen Sprachen und Mehrdeutigkeiten. Semantische Analyse ist die Aufgabe, semantische Darstellungen von Benutzeranfragen zu erstellen, wie z. B. SQL und Lambda-Calculus. Und semantische Analyse in mehreren Sprachen ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Mehrdeutigkeiten zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Anfragen in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda- oder Funql übersetzen. Es gibt separate, auf begrenzte Aufgaben und Anwendungen vorgeschlagene semantische Analysemodelle. Zum Beispiel fehlt die Abdeckung für bestimmte natürliche Sprachen, wie z. B. Chinesisch, und die Abdeckung für bestimmte Mehrdeutigkeiten, wie z. B. Lambda-Calculus, oder sie werden nur auf bestimmte neuronale Modelle getestet. Um dies zu erreichen, schlagen wir vor, exemplar, ein einheitliches Datensatz-Exemplar für semantische Analyse in mehreren natürlichen Sprachen und Mehrdeutigkeiten vor. Es enthält neun Datensätze in verschiedenen Domänen, fünf semantische Anweisungen, acht Millionen Darstellungen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unsere Benchmark besser zu bewerten, berücksichtigen wir sechs Trainings- und Bewertungssettings. Das erste ist übersetzte Tests. Wir verwenden die Google-Übersetzungs-API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell, um zu trainieren und zu bewerten. Und zum Beispiel trainieren wir ein englisches Modell für englische Abfragen und übersetzen während der Inferenz die deutsche Abfrage mit der API in Englisch und verwenden dann das trainierte Modell, um die SQL zu vorhersagen. Und wir testen auch ein monolinguales Modell. In diesem Setting ist die Quell- und Zielsprache gleich, z. B. Deutsch-Deutsch oder Englisch-Englisch. Wir testen auch ein monolinguales Few-Shot-Setting, bei dem wir ein monolinguales Modell mit nur 10% des Trainingsdatensatzes trainieren. Und wir testen ein multilinguales Modell, bei dem wir ein multilinguales Modell für alle Sprachen trainieren. Zum Beispiel setzen wir die deutschen, englischen und chinesischen Abfragen zusammen, um ein multilinguales Modell zu trainieren, und verwenden dieses Modell, um deutsche Abfragen oder chinesische Abfragen zu übersetzen. Und wir berücksichtigen auch Cross-Linguistische Zero-Shot- und Few-Shot-Transfer. Wir trainieren auf einer englischen Abfrage oder einer Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein multilinguales Modell zu trainieren, das die SQL-Output vorhersagt. Und wir finden viele interessante Ergebnisse. Zum Beispiel bewerten wir zwei Gruppen von Modellen, einschließlich Encoder-Decoder, die für Multilingual-Pre-Trainings-Encoders mit Pointer-Base-Decoders wie XL-R plus PDR und BERT plus PDR stehen. Und wir bewerten auch Encoder-Decoder, die für Multilingual-Pre-Trainings-Encoder-Decoder stehen, wie z. B. M-BART und M-T5. Wir fanden, dass Encoder-Decoder auf allen neun Datensätzen die beste Leistung erzielt. Und wir bewerten M-T5 und exemplar, ein multilinguales Setting, wir fanden, dass Encoder-Decoder oder Encoder-PDR durch das Trainieren in einer Mischung verschiedener Sprachen verbessert werden kann. Und wir fanden, dass die Cross-Linguistische Transfer-Performance Gap. In dieser Abbildung ist die blaue Linie Cross-Linguistische Zero-Shot-Transfer, die orange Linie Cross-Linguistische Few-Shot-Transfer, während die grüne Linie das monolinguale Setting ist. Wir fanden, dass die Cross-Linguistische Few-Shot-Transfer-Performance Gap signifikant ist. Und durch das Vergleichen der blauen und orangefarbenen Linie fanden wir, dass die Cross-Linguistische Few-Shot-Transfer-Performance Gap schnell verkürzt wird. Wir fanden auch einige andere interessante Erkenntnisse. Zum Beispiel, wenn wir auf Englisch trainieren, kann die Leistung von Few-Shot auf Zielsprachen erheblich gesteigert werden. Und wir fanden, dass multilingualen Sprachmodelle wie Codas und Blue für semantische Analyseaufgaben in mehreren Sprachen unzureichend sind. Um zusammenzufassen, haben wir exemplar, einen einheitlichen Benchmark für semantische Analyse in mehreren Sprachen mit mehreren natürlichen Sprachen und Mehrdeutigkeiten erstellt. Wir führen eine umfassende Benchmark-Studie mit drei repräsentativen Arten von multilingualen Sprachmodellen durch, und unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und willkommen, um unser Papier und Code zu besuchen. Vielen Dank für das Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Szpekowski und in diesem Vortrag geht es um die Abhängigkeit der Koordinationsstruktur. Wie Sie wissen, nehmen verschiedene Abhängigkeitsstrukturen unterschiedliche Theorien und Korpusrichtungen vor. So nimmt zum Beispiel in den universellen Abhängigkeitsstrukturen die Struktur der Koordinierung Lisa, Bart und Maggie so, dass die erste Konjunktion die Kopf der gesamten Koordinationsstruktur ist. Ein ähnlicher Ansatz wird in der Bedeutungstexttheorie angenommen, wo wieder die gesamte Koordinationsstruktur von der ersten Konjunktion angeführt wird. So sind diese beiden Ansätze symmetrisch, richtig? Sie heben eine der Konjunktionen hervor. Nun gibt es auch symmetrische Ansätze zu Koordinationsstrukturen, wie zum Beispiel die Pragmatische Herangehensweise, die in der Pragmatischen Abhängigkeitsbäumen angenommen wird, wo Koordinationsstrukturen von der Konjunktion angeführt werden. So erhalten Abhängigkeitsstrukturen von N zu allen Konjunktionen. Und schließlich gibt es auch eine mehrköpfige Herangehensweise, die zum Beispiel in der Wortgrammatik verwendet wird, wo alle Konjunktionen die Kopf der Koordinationsstruktur sind. So erhalten Abhängigkeitsstrukturen von der Gouverneure, hier liebt, alle Konjunktionen separat. Dies sind über und macht. Nun ist das Ziel dieses Papiers, ein neues Argument für die symmetrischen Strukturen der Koordinationsstruktur und gegen die asymmetrischen Strukturen der Koordinationsstruktur zu produzieren. Okay, das Argument basiert auf dem Prinzip der Abhängigkeitslänge Minimierung, das ich auf der Grundlage dieser Beispiele erklären werde. In Englisch, wie Sie wissen, bevorzugen direkte Objekte, die dem Verb nahe zu sein, während Adjektive weiter weg sein können. So ist es in der Regel in Ordnung, wenn das direkte Objekt, es, dem Verb nahe ist, während es in der Regel nicht in Ordnung ist, wenn zwischen dem Verb und dem direkten Objekt ein Adjektiv, gestern, steht. Dies kann jedoch verbessert werden, wenn das direkte Objekt sehr schwer und lang ist, weil es dann in die Position nach dem Adjunkt verschoben werden kann. Dies wird hier veranschaulicht. So sind beide Sätze in Ordnung. Marge hat gestern ein absolut faszinierendes Buch über Bienen gelesen. Es ist in Ordnung, wo wir statt es dieses langen NP haben, dieses langen NP. Aber es ist auch in Ordnung zu sagen, Marge hat gestern dieses absolut faszinierende Buch über Bienen gelesen. Der Grund dafür ist, dass dies möglich ist, weil, obwohl dieses Satz die allgemeine Grammatikregel verletzt, es die Prinzip der Abhängigkeitslänge Minimierung erfüllt, die besagt, dass kürzere Abhängigkeitslängen bevorzugt werden. Also zeigen diese beiden Bäume nur die Länge der entscheidenden Abhängigkeitsstrukturen, also derjenigen, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir also eine Abhängigkeitsstruktur von red zu der Adjunkt von Länge sieben, gemessen in Wörtern, und von red zu book von Länge vier, also zusammen 11. Wenn Sie diese beiden Konstituenten tauschen, wird die Summe dieser beiden Abhängigkeitsstrukturen 6, also viel kürzer. Das ist der Grund, warum dies ganz in Ordnung ist, oder? Es verstößt eine Regel, aber es erfüllt eine andere. Okay, was wir also getan haben, ist, dass wir verschiedene Statistiken über Koordinationsstrukturen aus der verbesserten Version von Pen und der Pen-Treebank und aus dem Papier Why We Dont Use Universal Abhängigkeitsstrukturen extrahiert. Und diese Statistiken bestätigen die Beobachtung, die viele Male gemacht wurde, dass linke Konjunktionen in der Regel kürzer sind, also salt und pepper und nicht pepper und salt, gemessen in Silben. Und auch die Beobachtung, dass diese Tendenz mit der Länge der Unterschied zwischen den beiden Konjunktionen zunimmt, so dass die stärkere Konjunktion auf der linken Seite bevorzugt wird. Aber was neu in diesem Papier ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Gouverneur auf der linken Seite ist, also ist der Gouverneur auf der linken Seite. In solchen Fällen ist die linke Konjunktion bevorzugt, die größere Unterschied zwischen den beiden Konjunktionen. Wenn der Gouverneur jedoch auf der rechten Seite ist, wie hier, wo wir Koordinationsstrukturen von zwei Verben haben und es keinen externen Gouverneur gibt, ist diese Effekt verschwindet. Und wir zeigen in dem Papier, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen wie diese zwei und für symmetrische Strukturen wie diese zwei liefert. Also sehen Sie das vollständige Argument und die Argumente in der Postsession. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich heiße Kai-Wei und werde unsere Arbeit mit dem Titel \"When Does Translation Require Context: A Data-Driven Multilingual Exploration\" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernhout, Emile Liu, Andre F. D. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir \"Mole\" in diesem Satz übersetzen? Nun, wenn der vorherige Satz war, \"Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden\", dann bezieht sich \"Mole\" auf einen Spion. Aber wenn der vorherige Satz war, \"Könnte es etwas Ernstiges sein, Doktor?\" dann bezieht sich \"Mole\" auf ein Geburtszeichen. Je nach Kontext ändert sich also die Bedeutung des Wortes und die Übersetzung. Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch ziemlich schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was es für Metrik auf Satzebene wie BLEU unzureichend macht, diese Übersetzungen zu erfassen. Und einige Leute haben vorgeschlagen, auf kontextabhängige Übersetzungen zu konzentrieren, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen, da sie normalerweise auf Domänenwissen und menschliche Kuration angewiesen sind. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten: Wann erfordert Übersetzung Kontext und wie gut können Modelle diese Fälle bewältigen? Um die erste Frage zu beantworten, messen wir zunächst, wie sehr ein Wort während der Übersetzung vom Kontext abhängt. In der vorherigen Arbeit haben wir cxmi als Maß für den Kontextgebrauch von maschinellen Übersetzungsmodellen eingeführt. Und das ist, indem wir messen, wie viel Informationen der Kontext C über das Ziel Y angibt, gegeben die Quelle X. Man kann sich cxmi als die Informationen vorstellen, die durch das Geben von Kontext an das Modell gewonnen werden. In dieser Arbeit erweitern wir cxmi auf Punktweise cxmi, das den Kontextgebrauch auf Satz- oder Wortebene messen kann. Wir können Wörter mit hohem pcxmi als solche bezeichnen, die für die Übersetzung kontextabhängig sind. Und analysieren wir Wörter mit hohem pcxmi, um Muster zwischen diesen Wörtern zu finden. Und führen wir unsere Analyse auf Transkripte von Ted Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir Teil der Sprache, die ein hohes pcxmi aufweist. Und das ermöglicht es uns, zum Beispiel duale Pronomen in Arabisch zu finden, die relativ hohes pcxmi aufweisen. Und das kann erklärt werden, weil Englisch keine dualen Pronomen hat, also muss der Kontext bestimmt werden, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und wir finden auch, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die richtige Verbform wählen wollen. Wir betrachten dann Wörter, die über alle ihre verschiedenen Vorkommen ein hohes pcxmi aufweisen. Und das hilft uns, Fälle zu identifizieren, in denen im Chinesisch Kontext benötigt wird, um richtige Artikel zu übersetzen, um sicherzustellen, dass Sie im Dokument die gleiche Übersetzung verwenden. Und wir finden auch, dass Kontext unterstützt wird, um die richtige Formalität zu übersetzen. Und schließlich betrachten wir einzelne Token, die ein hohes pcxmi aufweisen. Und das ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich von dem Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z. B. Ellipsenauflösung. Jetzt verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmark für die Dokumentenübersetzung zu entwerfen. Für jede der fünf identifizierten Diskursphänomene erstellen wir Tags, um Wörter zu identifizieren, die zu dem Phänomen gehören. Und wir nennen unser Tagger den Multilingual Discourse-Aware oder Muda-Tagger. Und wir können auch anmerken, dass verschiedene Sprachen unterschiedliche Proportionen dieser Diskursphänomene haben. Und wir verwenden den Muda-Tagger, indem wir den Tagger auf einen parallelen Korpus anwenden, den wir für die Bewertung verwenden wollen. Und wir wenden unsere Übersetzungsmetriken unserer Wahl auf die kontextabhängigen Beispiele an, die der Muda-Tagger identifiziert hat. Erstens, wenn wir Metrik auf Satzebene verwenden, so dass für BLEU kontextunabhängige Modelle die beste Leistung haben. Aber wenn wir comet verwenden, dann kontextbewusste Modelle die beste Leistung haben. Und wenn wir Wort F messen, dann haben Modelle mit oder ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, die beste Dokumenten-Übersetzungs-Systems zu bestimmen, wenn wir nur Metrik auf Satzebene verwenden. Jetzt verwenden wir den Muda-Benchmark, um Modelle zu bewerten, und wir finden, dass kontextbewusste Modelle für bestimmte Diskursphänomene, wie z. B. Formatik und lexikalische Kohäsion, deutlich genauer sind als Modelle, die keinen Kontext für die Übersetzung verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipsen, Pronomen und Verbform verwenden. Das deutet darauf hin, wo wir für Dokumenten-Übersetzung Fortschritte sehen müssen. Wir vergleichen auch verschiedene kommerzielle Systeme, und unser Benchmark zeigt, dass DeepL im Allgemeinen genauer als Google Translate für Dokumenten-Übersetzung ist. Um zu zusammenfassen, führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und dann verwenden wir unsere Ergebnisse, um einen Benchmark für die Dokumenten-Übersetzung zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut bewältigen oder nicht, und welche Übersetzungs-Systems gut in der Dokumenten-Übersetzung sind. Vielen Dank für Ihre Aufmerksamkeit, bis in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin an der Carnegie Mellon University. Heute werde ich Ihre Arbeit vorstellen, die sich mit der Analyse von Design-Biasen in Datensätzen und Modellen befasst. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen an der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santi, Ronan Labras, Katarina Rainerka und Martin Sapp, durchgeführt. Stellen wir uns vor, Sie arbeiten für eine Zeitung und durchforsten Sie Kommentare unter Ihrem Artikel, um toxische Inhalte zu entfernen. Sie könnten sich für eine beliebte API wie die Perspective API für die Erkennung von Toxizität entscheiden. Und das funktioniert wirklich gut, wenn Sie Carl Jones sind. Wo Perspective API in der Lage ist, toxische Instanzen korrekt zu erkennen, ist das bei Aditya Sharma nicht der Fall. Wo Perspective API wirklich nicht in der Lage ist, beleidigende Begriffe zu erkennen, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungen sehen. Design-Biasen wie die, die wir gerade gesehen haben, können aufgrund der Position der NLP-Forscher und Modellentwickler auftreten. Positionality ist einfach die Perspektiven, die Menschen aufgrund ihrer demografischen, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in feministischen und queer akademischen Räumen weit verbreitet ist. Und als Forscher kann Positionality den Forschungsprozess und seine Ergebnisse beeinflussen, weil es die Entscheidungen, die Forscher treffen, verändern kann. Und so eine Frage, die die Leute sich stellen könnten, haben Datensätze und Modelle Positionality? Wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können daher bestimmte Positionalitäten gegenüber anderen darstellen. Vorherige Arbeiten haben anekdotische Beweise für Positionality wie kulturelle Lücken in Modellen und Datensätzen sowie die radikale Definition von Modellpositionality vorgelegt. Diese Arbeiten untersuchen jedoch nicht, wie Endbenutzer mit Datensätzen und Modellen verglichen werden. Und das Studium von Modell- und Datensatzpositionality ist zunehmend wichtig, da NLP-Aufgaben immer subjektiver und sozial orientierter werden. Und es ist eine Herausforderung, zu charakterisieren, wie diese Positionalitäten verzerrt sind, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. Um Datensätze und Modelle zu untersuchen, vergleicht unsere Arbeit die Anmerkungen mit realen Benutzern mit bestehenden Datensätzen und Modellen. Und so funktioniert unser Framework in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit verschiedenen Annotatoren neu zu annotieren. Und wir wählen dies gegenüber der Betrachtung der Demografie der ursprünglichen Annotatoren aus, weil normalerweise nur wenige Annotatoren pro Instanz annotieren und weil Demografien selten gesammelt und geteilt werden. Und wir reannotieren also Daten, um viele Annotatoren für Instanzen zu erhalten und um ein reiches Set von Demografien zu erhalten. Wir nehmen dann die Anmerkungen nach Demografie und vergleichen sie mit Modellen und Datensätzen mit einem Pearson-R-Produktionskorrelationskoeffizienten. Und so unterscheidet sich unser Framework von der Literatur zur Annotator-Disagreement, indem es nicht nur die Übereinstimmung der Annotatoren oder die Modellierung der Annotatorenverteilungen betrachtet, sondern die Vorhersagen und Labels von Modellen und Datensätzen mit Endbenutzern vergleicht. Unser Framework ist größtenteils durch Lab in the Wild, ein Online-Experimentierplattform, ermöglicht, die sich von Plattformen wie Mturk unterscheidet, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Und Lab in the Wild ist in der Lage, immer noch hochwertige Daten zu erhalten. Und wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Und die Art und Weise, wie dies funktioniert, ist, dass die Teilnehmer eine Situation aus dem Social-chemistry-Datensatz lesen und dann schreiben, wie sozial akzeptabel eine Situation ist. Und danach können sie ihre Antworten mit einer KI und anderen vergleichen. Wir vergleichen diese Anmerkungen mit Social-chemistry, Delphi und GPT-4. Und wir replizieren ein sehr ähnliches Setup für die Aufgabe der Erkennung von Hassrede, bei der sie eine Instanz aus Dinhate lesen und schreiben, ob sie ihrer Meinung nach eine Instanz von Hassrede ist. Und wir vergleichen diese Anmerkungen mit Dinhate, Perspective API, Rewired a-pi und GPT-4. Und unsere Studie am Ende umfasst über 16.000 Anmerkungen von über 1000 Annotatoren aus 87 Ländern. So sind wir besser in der Lage, zu beantworten, mit wem NLP-Datensätze und -Modelle übereinstimmen? Wir finden, dass es Positionality in NLP gibt. Zum Beispiel finden wir, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. So ist die GPT-4-Analyse der sozialen Akzeptanz am besten mit Personen aus konfuzianischen und englischsprachigen Ländern übereinstimmend. Wir finden, dass Dinhate auch am meisten mit englischsprachigen Ländern übereinstimmt. Wir finden auch, dass Modelle und Datensätze am meisten mit Personen übereinstimmen, die einen Hochschulabschluss haben. So für GPT-4 in der sozialen Akzeptanzaufgabe, ist es am besten mit Personen übereinstimmend, die einen Hochschulabschluss oder einen Hochschulabschluss haben. Und wir finden das gleiche für Dinhate, wo es am besten mit Personen übereinstimmt, die einen Hochschulabschluss haben. Aber wenn Modelle und Datensätze mit bestimmten Bevölkerungen übereinstimmen, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nichtbinären Personen als mit männlichen und weiblichen Gegenstücken übereinstimmen. Wir finden dies in der GPT-4-Analyse der sozialen Akzeptanz sowie in der Dinhate-Aufgabe. Und so, da es Positionality in NLP gibt, was können wir tun? Also haben wir ein paar Empfehlungen dafür. Erstens, halten Sie ein Protokoll aller relevanten Designentscheidungen während des gesamten Forschungsprozesses. Und die zweite Empfehlung ist, NLP-Forschung mit dem Blickwinkel des Perspektivismus durchzuführen. Und die dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Gemeinschaften zu erstellen. Und ein gutes Beispiel dafür ist die Muscani-Initiative. Und wir betonen, dass inklusives NLP nicht nur darum geht, alle Technologien für alle zu machen. Und so schließt sich unsere Präsentation ab. Aber wenn Sie mehr erfahren möchten, können Sie sich gerne auf unserer Dashboard für die aktuellsten Analyseergebnisse und unsere Arbeit bewerben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo. Ich werde über unsere Arbeit zur Lösung indirekter Referenzen für die Entitätsauswahl sprechen, bei der wir den alt-Entities-Korpus einführen. Und mein Name ist Javad Hosseini, und dies ist eine gemeinsame Arbeit mit Philip Radlinski, Silvia Parati und Aniket. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: \"Meinst du 'Easy on Me' oder 'I Got a Feeling'?\" Hier möchte der Benutzer zwischen diesen beiden Songs wählen. Das offensichtlichste ist die direkte Referenz, zum Beispiel, indem man den Namen des Songs sagt, 'Easy on Me' oder seine Position, die erste. Aber manchmal ist eine indirekte Referenz angemessener, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich nicht an den Namen des Songs erinnern kann, oder die Aussprache zu ähnlich ist und schwer zu unterscheiden ist, oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen. Zum Beispiel, der neuere oder der Song, der nicht energetisch ist. Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Entitätsverständnis von LLMs. Wir sind uns nicht über einen öffentlichen Datensatz für die Aufgabe bewusst, also sammeln wir einen, indem wir eine Crowdsourcing-Methodik verwenden. Unser Datensatz deckt drei verschiedene Domänen ab: Musik, Bücher und Rezepte. Unsere Datensatzsammlung betont die Informalität mit einem Cartoon-Completion-Setup. Der Cartoon hat drei Sprachblasen. In der ersten Blase sagt Bob, \"Erinnern Sie sich an das Lied, das wir gestern gehört haben.\" Und damit setzt Bob den Kontext. In der zweiten Blase sagt Alice, \"Meinst du 'Easy on Me' oder 'I Got a Feeling'?\" Dies ist die alternative Frage. Und in der dritten Blase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel, die neuere. Wir geben die ersten und zweiten Blasen automatisch, aber die dritte wird von der Anmerkenden ausgefüllt. Die erste Blase wird aus einigen manuellen Anweisungen pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert: \"Meinst du A oder B?\" wobei A und B aus Wikipedia abgetastet werden. Hier sind die verschiedenen Abtastmethoden, die wir verwendet haben. Wenn wir weiter in der Liste gehen, werden die Entitäten sich ähnlicher. Und es ist normalerweise schwieriger, die Unterscheidung zu treffen. Die erste ist gleichmäßig zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen 'The Return'. Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler für einen Song. Wenn wir diese alternative Frage den Anmerkenden zeigen, wissen sie die Namen dieser Entitäten, aber sie kennen sie nicht unbedingt. Was wir also tun, ist, dass wir einige Hintergrundwissen über diese beiden Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song. Und dann bitten wir die Anmerkenden, mindestens einen der Songs zu hören und über jeden Song zu lesen. Hier ist zum Beispiel der Google-Suchlink für den Song 'Easy on Me'. Für Rezepte und Bücher zeigen wir einige Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir zusätzlich die Bilder, wieder aus Wikipedia, damit die Anmerkenden wissen, wie sie aussehen. Dann bitten wir die Anmerkenden, eine dieser Entitäten auszuwählen, zum Beispiel hier, und beschreiben sie mit drei bis fünf indirekten Referenzen. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel, ohne Worte, nicht der mit dem 12-jährigen Jungen oder der fiktiven oder aus Aserbaidschan. Die alt-Entities-Korpus hat 6000 alternative Fragen in drei Domänen, und es hat 42000 indirekte Referenzen. Die Ergebnisse mit dem T5-Exlarge-Modell sind unten zusammengefasst. Wenn der Sprachmodell Zugang zu genau dem gleichen Hintergrundwissen wie die Anmerkenden hat, ist die Genauigkeit wirklich hoch, es ist rund 92-95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlappenden Hintergrundwissen hat, dann ist die Genauigkeit zwischen 82-87%. Das ist realistischer. Zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugang zu Entitätsnamen hat, dann ist die Genauigkeit nur 60%. Es gibt also viel Raum für Verbesserung. Wir haben auch gezeigt, dass die Modelle domänenspezifisch sind. Hier ist ein Link zu unserem Datensatz, danke."}
