{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Landmann e oggi vi farò una breve introduzione al nostro articolo su \"Composizione e generalizzazione senza alberi, usando tag multi-set e permutazioni latenti\". Questo è un lavoro congiunto dei miei consiglieri, Alexander Koller e Ivan Tiedoff. La composizione può essere compresa come la capacità di un apprendista di gestire ricorsioni e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto della semantica, la valutazione per la composizione può assomigliare a questa. Come al solito, abbiamo un set di frasi di addestramento, in questo caso, la ragazza dormiva e Mary sapeva che la ragazza dormiva. Queste frasi sono accoppiate con forme logiche che rappresentano aspetti centrali del loro significato. A differenza della valutazione standard di apprendimento, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto ricorsione superficiale durante l'addestramento e viene testato su esempi con ricorsione più profonda. I modelli sequenza-seriali naive hanno difficoltà con questo tipo di generalizzazione fuori distribuzione e spesso producono output dettati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nel esempio. Un metodo popolare per affrontare questo è integrare alberi nei modelli. Gli alberi sono pensati per catturare il processo compositivo che relaziona le frasi con le forme logiche. Questo funziona bene, ma gli alberi di solito non vengono dati e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionale costoso. In genere, questo comporta un'elaborazione specifica del formale delle forme logiche, ad esempio, per gestire simboli variabili. L'ottenimento degli alberi può anche comportare procedure di induzione grammaticale specializzate. In questo articolo, non usiamo alberi e introduciamo un modello di sequenza-seriale che modella direttamente le corrispondenze tra frammenti di input e output. Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la previsione dell'output dal input in due fasi. In prima fase, taggiamo ogni token di input con un set multi-set di token che apparirà nell'output. Dopo la prima fase, abbiamo tutti i token giusti, ma non sono ordinati. Questo è il motivo per cui, nella seconda fase, usiamo un altro modello per prevedere una permutazione per mettere i token nell'ordine giusto. Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli difficili sulle possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile e espressivo. Concettualmente, il nostro modello di permutazione funziona in modo simile a questo. Andiamo da sinistra a destra sull'output e determiniamo quale token multi-set inserire in ogni posizione. Per la prima posizione, selezioniamo semplicemente uno come evidenziato in rosso. Quindi, saltiamo a un altro token multi-set per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token multi-set. Continuiamo questo processo fino a quando ogni token della prima fase è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di gran lunga in termini di generalizzazione alla ricorsione più profonda. Alcune altre forme di generalizzazione strutturale rimangono molto impegnative. In questo articolo, risolviamo un paio di sfide tecniche interessanti. In primo luogo, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-set è derivato, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono permutazioni multiple che sono coerenti con i dati, ma la permutazione linguisticamente corretta è latente. Affrontiamo questo indossando l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta la sfida di trovare la permutazione più alta. Questo è perché è legato al problema del viaggiatore. Approssimiamo questo con un modello amichevole per le GPU e una rilassazione continua che ci permette anche di backpropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Maya, e oggi parlerò del nostro articolo, Marked Personas, che utilizza prompt di linguaggio naturale per misurare stereotipi nei modelli linguistici. Questo lavoro è fatto in collaborazione con Esmdermus e Dan Jarauskie. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi nei modelli linguistici più grandi, o LLM. Tuttavia, questi misuratori hanno varie limitazioni. Di solito si basano su set di dati costruiti a mano che sono molto dispendiosi in termini di tempo. E inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non generalizzano bene a altri demografi o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con determinati gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è l'idea che le identità sociali multifaccettate possano comporre pregiudizi e essere un unico luogo di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi modelli linguistici più recenti sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una descrizione di un individuo immaginato usando un prompt come, 'Immagina di essere una donna asiatica, descrivi te stessa.' E vediamo immediatamente che, mentre i risultati non sono eccessivamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni modelli interessanti. La donna asiatica è descritta come non assumendo, la donna medioriente è riferita usando parole come 'exotica' e come riferire a una regione affascinante. E entrambe le donne di colore personaggi fanno riferimenti all'ascendenza, mentre il uomo bianco non ha nulla del genere. Per catturare questi modelli, il nostro metodo ha due parti. La prima parte è generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, trovando che, dando a soggetti umani, sono in grado di far emergere stereotipi razziali. Inoltre, questo consente un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte è Marked Words, che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati. Quindi, elaboro su questo in breve. Il metodo Marked Words si basa sul concetto sociolinguistico di marcato, che afferma che c'è un default non marcato, e qualsiasi gruppo che differisce da quel default è linguisticamente marcato. Ad esempio, la parola guerriero è di solito associata a uomini. Quindi, quando descrivono un guerriero che è una donna, di solito specificano 'guerriera donna' e marcano il termine con 'donna'. E più in generale, i gruppi dominanti nella società sono linguisticamente e socialmente non marcati, mentre i gruppi marginalizzati sono di solito marcati. Quindi, nel nostro metodo, designiamo prima quali sono i gruppi non marcati e quelli marcati. E poi confrontiamo le persone usando il metodo di parole di lotta, che è fondamentalmente usare rapporti di probabilità ponderati per distinguere le parole migliori per ogni gruppo marcato. Quindi, per le persone di donne nere, faremo parole di lotta e confrontiamo i rapporti di probabilità contro le persone bianche e maschili, perché questi sono i due gruppi corrispondenti non marcati. Ora, per alcuni risultati, prima usiamo un lessico di stereotipi, e troviamo che le persone generate contengono molti più stereotipi rispetto alle risposte scritte dagli umani. Tuttavia, quando guardiamo la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno molto più tassi delle parole del lessico, le risposte scritte dagli umani hanno una distribuzione molto più ampia. Mentre le parole stereotipate che sono nelle persone generate sono solo le parole 'alto e atletico', quindi solo le parole positive o almeno non negative. E in effetti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per fare questo, passeremo ai risultati del nostro metodo Marked Words per mostrare come queste parole positive simili facilitano stereotipi e narrative essenzializzanti. In analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi. In primo luogo, per i gruppi marcati, le parole principali includono cose come cultura, tradizione, orgoglioso e esotica. E queste parole definiscono questi gruppi solo in relazione alla loro identità e distinguono loro come diversi dal normale bianco. Questo contribuisce a una lunga eredità di discriminazione e altri. Inoltre, ci sono molti schemi comuni che sono riflessi in queste parole, specialmente per le donne di colore. Quindi, le parole che descrivono le donne latine includono cose come 'vibrante' e 'curvaceo', che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come 'petito' e 'delicato e lussureggiato', che si collegano a una lunga storia di donne asiatiche che sono iper sessualizzate, viste come molto docili e submissive e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come 'forte' e 'resiliente'. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte. E mentre sembra positivo a prima vista, c'è stato lavoro che mostra che questo tipo di archetipo in realtà è molto dannoso. Quindi, piuttosto che lavorare per cambiare gli ostacoli, mette pressione su questi gruppi per superare loro, il che porta a risultati negativi per questi gruppi, tra altre cose. In generale, troviamo che le parole per ogni gruppo marcato riflettono molto narrazioni essenzializzanti. Quindi, in base a questi modelli, concludiamo con tre raccomandazioni per i proprietari dei modelli. In primo luogo, come ricercatori, dovremmo affrontare stereotipi positivi e narrative essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, ci dovrebbe essere molta più trasparenza sui metodi di mitigazione dei pregiudizi, perché, per esempio, questi stereotipi positivi, non sappiamo se è perché c'è una sorta di allineamento di valore eccessivo in corso, o forse altri, come metodi anti stereotipizzanti che stanno dando vita a questi modelli dannosi. Non possiamo fare ipotesi o studiare ulteriormente senza più trasparenza. Grazie mille per aver ascoltato. Buona giornata."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E io sono Sarah Finch. E oggi parleremo di ABC Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato fatto dal laboratorio di Emory NLP, guidato da Gino Choi presso Emory University, in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una candidata a laurea in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per ACL 2023 come un lungo articolo, Transfer Learning for Dissonance Detection Addressing the Rare Class Challenge. Iniziamo definendo la dissonanza cognitiva e il perché è un problema importante da studiare nella lingua. In parole povere, la dissonanza cognitiva è due credenze o azioni incoerenti, come questo esempio in cui una persona afferma, \"So che le sigarette potrebbero uccidermi\", e poi continua dicendo, \"Prendo un paio di sigarette dopo la riunione\". Queste credenze e azioni incoerenti sono in dissonanza. Inoltre, menzionando che non penso che potrei mantenere il mio lavoro senza di loro, giustifica la seconda occorrenza. Hanno una relazione consonanza. Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nella decisione quotidiana, sono davvero rari da trovare espressi in linguaggio tra altre relazioni discorsali. Perché questo è importante? Studiare la dissonanza cognitiva può aiutarci a capire gli effetti del disaccordo tra le persone, tenere traccia delle tendenze e cambiamenti di credenze, valori e atteggiamenti nella popolazione. La dissonanza cognitiva è anche correlata agli disturbi d'ansia e può aiutare a capire meglio la salute mentale delle persone. Studiare la dissonanza espressa nella lingua può anche essere utile per capire l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per capire lo stile cognitivo personale di un individuo e aiutarci a capire meglio i processi di decisione. Al fine di creare un risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala di relazioni di dissonanza. Abbiamo usato un approccio di dissonanza prima, come visto nel flusso di lavoro qui. I tweet sono stati analizzati usando un parser di PDTB e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo. Come può essere visto qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Per raccogliere circa 1000 esempi di coppie di unità di discorso, abbiamo eseguito l'addestramento per un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. A no surprise, il classificatore non ha funzionato molto meglio di quanto ci si aspetti. Dato l'assenza di un set di dati di dissonanza e la mancanza di qualsiasi set di dati di precedenza, stiamo affrontando il problema dell'rarità assoluta. Per alleviare questo, sperimentiamo combinazioni di apprendimento di trasferimento e apprendimento attivo per annotare tale che più esempi di dissonanza possono essere raccolti in meno round di annotazione, riducendo il costo complessivo dell'annotazione, migliorando la rilevazione della dissonanza. Poiché il classificatore iniziale non è stato in grado di catturare la classe di dissonanza, iniziamo l'apprendimento attivo passando i pesi da compiti strettamente correlati. Abbiamo trasferito da due diversi compiti, classificazione indipendente della dissonanza e classificazione binaria di estensione e confronto di PDTB. Poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza, li chiamiamo C E E qui. Troviamo che, trasferendo il classificatore iniziale sul set di dati annotato, è già molto meglio di quanto ci si aspetti con il miglior risultato con auc 0,62. Ulteriormente, su entrambi i compiti, troviamo che la classificazione di C E E seguita da ulteriori perfezionamenti sul compito di debate offre un risultato molto migliore. Questo è il modello che abbiamo usato per iniziare l'apprendimento attivo. Successivamente, determiniamo il metodo migliore per aggiornare il modello con nuovi dati da ogni round di apprendimento e annotazioni. Il cumulativo accumula tutti i dati raccolti da annotazioni attive finora. Il iterativo aggiorna il modello addestrandolo sul set di dati più recente raccolto. Sulla strategia di aggiornamento, abbiamo trovato che il cumulativo ha prestazioni uguali o migliori rispetto all'iterativo. Successivamente, per migliorare il numero di esempi di dissonanza, usiamo una strategia di probabilità di classe rara, prc, per selezionare principalmente gli esempi che sono molto probabilmente dissonanti da qualsiasi round di apprendimento. Confrontiamo questo con gli altri strategie di stato dell'arte comunemente usate nella comunità. Troviamo che la strategia proposta prc funziona meglio di altre strategie di stato dell'arte, anche se la differenza è piccola. Si noti che la performance è significativamente inferiore per casuale. Su diversi round di apprendimento con due strategie migliori, miglioriamo la classificazione della dissonanza, auc 0,75, che è il miglior risultato che abbiamo sul compito finora. Controlliamo anche la fattibilità di ogni strategia per la qualità e il costo di annotazione per gli annotatori. Troviamo che il prc ha il maggior numero di esempi di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, troviamo che la strategia prc è una semplice strategia di apprendimento per l'acquisizione di classe rara. E il rafforzamento attivo con compiti di trasferimento appropriati può aiutare significativamente. Questi sono i link al nostro set di dati di codice e il nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Akshata e oggi presentiamo insieme il nostro lavoro, il kitmus, che valuta l'integrazione del sapere proveniente da fonti multiple. Questo lavoro è il risultato di una collaborazione tra McGill University, Meila e Microsoft Research. Il kitmus è un lavoro che fa parte di un progetto di ricerca in corso presso McGill University."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sara Papi della Università di Trento e Fondazione Bruno Kessler. Presenterò brevemente il documento intitolato 'Attenzione come guida per la traduzione simultanea'. Questo è un lavoro congiunto con Matteo Negri e Marco Turilli."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Shu-Hung. Oggi presenterò il nostro articolo, 'Do Conner 2003 Named Entity Taggers Still Work Well in 2023'. Cominciamo. Il nostro articolo ha indagato il problema della generalizzazione usando il compito di riconoscimento delle entità, o NER. Abbiamo osservato che i modelli hanno usato Conner 2003 per sviluppare NER per quasi vent'anni. Questo naturalmente solleva diversi problemi. In primo luogo, questi modelli hanno usato Conner 2003 per sviluppare NER. E poi, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare questi problemi, abbiamo sviluppato il set di dati Conner Plus Plus. Questo è un set di dati che abbiamo raccolto da Reuters News nel 2020 e poi annotato con le stesse linee guida di annotazione di Conner 2003. Abbiamo poi affinato oltre venti modelli su Conner 2003. Li abbiamo valutati sia sul set di Conner 2003 che sul set di Conner Plus Plus. E infine, abbiamo calcolato la percentuale di cambiamento in F1 per valutare la generalizzazione di ogni modello. Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli Transformer generalmente generalizzano meglio sui nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi conducono a una migliore generalizzazione. E infine, sappiamo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni del compito downstream. Qui, abbiamo anche scoperto che più esempi di fine-tuning portano anche a una migliore generalizzazione. Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dall'utilizzo ripetuto dello stesso set di test. E questo è di solito manifestato come il calo delle prestazioni sul nuovo set di test. La seconda ipotesi è il drifting temporale, che è la degradazione delle prestazioni causata dall'aumento del divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea di miglior adattamento ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo fatto su Conner 2003 si traduce in più unità di miglioramento su Conner Plus Plus. Il che significa che non c'è diminuzione delle prestazioni. E questo mostra che l'overfitting adattivo in questo caso non è osservato. Quindi, cosa succede con il drifting temporale? Per il drifting temporale, abbiamo fatto un esperimento per ricostruire o continuare a pre-addestrare alcuni modelli con dati più recenti. E abbiamo scoperto che le prestazioni degradano con un divario temporale più grande. E questo conferma la nostra ipotesi che la principale causa del calo delle prestazioni è il drifting temporale. La nostra conclusione è che, per una buona generalizzazione, abbiamo bisogno di un'architettura del modello migliore, di una dimensione del modello più grande, così come di più esempi di fine-tuning. E questi vanno di pari passo. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dal drifting temporale. E in modo sorprendente, non è causato dall'overfitting, anche se Conner 2003 è stato usato per oltre vent'anni. Quindi, rispondendo alla domanda del titolo del nostro articolo, 'Conner 2003 Tagger Funzionano ancora nel 2023?' E abbiamo scoperto che la risposta è in realtà un risonante sì. Speriamo che il nostro articolo solchi per ulteriori ricerche su come migliorare la generalizzazione dei modelli. E infine, per favore controlla il nostro articolo, il nostro set di dati, e se ha domande, senta libero di contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao. Benvenuti alla presentazione del nuovo corpus Deepane per la semplificazione del testo in tedesco a livello di documento e di frase. Mi chiamo Regina Stoddard e guiderò i primi punti della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento del testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, poiché le persone con difficoltà di lettura sono non madrelingua. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testo, ad esempio, documenti o frasi. Ecco un esempio di coppia di frasi parallele di una frase in tedesco complessa e la sua traduzione in un linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come sostituzione lessicale, eliminazione o riordinazione di clausole, o inserimento di parole. Ora proponiamo il nostro nuovo corpus Deepane, perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Quindi, per esempio, questi corpus qui sono troppo piccoli per addestrare un modello di semplificazione del testo. I modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus Deepane, che è diviso in due sottocorpora: Deepane APA e Deepane Web. Deepane APA si basa su testi di notizie. In Deepane APA, allineiamo 483 documenti, tutti manualmente. Questo si traduce in circa 13.000 coppie di frasi parallele. Per Deepane Web, questo corpus include diversi domini. E allineiamo anche tutti questi 750 documenti, da una parte manualmente e dall'altra con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi parallele. Analizziamo un po' di più le nostre coppie di frasi parallele. Ad esempio, sul tipo di semplificazione. Come puoi vedere qui, i testi biblici sono semplificati molto più fortemente rispetto ai testi di notizie o ai testi di apprendimento della lingua. Su tutti i livelli, per esempio, semplificazione lessicale, semplificazione strutturale, semplificazione complessiva. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò degli usi del set di dati Deepane. Quindi, per il primo caso di utilizzo, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi in post documenti. Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno lo stesso linguaggio, hanno lo stesso contenuto, ma sono a livelli di complessità diversi. Ora, poiché abbiamo il nostro set di dati Deepane, che ha frasi parallele allineate manualmente, possiamo usare queste frasi come standard di allineamento per valutare alcuni dei metodi proposti di allineamento. E abbiamo fatto alcune adattazioni ai metodi proposti. E abbiamo pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico per la semplificazione del testo in tedesco è il metodo di mass align. Puoi anche trovare il codice per eseguire questo metodo sui tuoi documenti. Il secondo caso di utilizzo che abbiamo mostrato nel nostro documento è il caso di semplificazione automatica del testo. Abbiamo addestrato due diversi modelli. Abbiamo addestrato il modello di Long BART per produrre semplificazioni a livello di documento e il modello di BART normale per produrre semplificazioni a livello di frase. Puoi anche trovare tutti i checkpoint e puoi guardare di più dettagli delle nostre esperimentazioni nel documento. Abbiamo concluso che questa semplificazione di base potrebbe produrre o ottenere punteggi migliori dei punteggi di base. E proponiamo questi risultati come un punto di riferimento, un punto di riferimento per il problema della semplificazione automatica del testo in futuro. Grazie per la vostra attenzione, e speriamo di incontrarvi durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xi Yuan della Fudan University. Sono qui per introdurre il nostro lavoro, distinguere la conoscenza del scripting dal grande modello linguistico per la pianificazione del linguaggio vincolata. In ogni giorno, gli esseri umani pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti. Il lavoro precedente ha sfruttato i grandi modelli linguistici per pianificare obiettivi astratti di attività stereotipiche. Mostrando che i grandi modelli linguistici possono scomporre gli obiettivi in passaggi, la pianificazione per obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane sottostimata. In questo documento, definiamo il problema della pianificazione del linguaggio vincolata, che impone diversi vincoli all'obiettivo della pianificazione. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo documento, valutiamo e miglioriamo la capacità di pianificazione del linguaggio vincolata dei grandi modelli linguistici. Poiché non esiste un set di obiettivi specifici per supportare lo studio, dobbiamo acquisirli prima. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multiformi per obiettivi specifici per l'acquisizione di dati in loop utilizzando instructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Questo tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i grandi modelli linguistici ottengono risultati insoddisfacenti per la pianificazione per obiettivi specifici. Quindi conduciamo un'analisi dettagliata per indagare perché i grandi modelli linguistici falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Esaminiamo in modo più dettagliato le categorie di vincoli definiti nel wiki. La mappa nella figura mostra che la pianificazione dei modelli instructGPT varia considerevolmente per obiettivi di diverse categorie. Studi precedenti hanno mostrato che la qualità dell'output dei grandi modelli linguistici varia considerevolmente, portando a un cattivo rendimento. Pertanto, adottiamo l'idea di filtraggio overgenerato per migliorare la qualità della generazione. Per prima cosa, mostriamo i tipi di vincoli con esempi per instructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti. Quindi, instructGPT genera script specifici per obiettivi. Successivamente, sviluppiamo un modello di filtraggio per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in incorporamenti instructGPT e calcoliamo la somiglianza di coseno come punteggi di somiglianza per misurare la somiglianza semantica. Inoltre, premiamo gli script che contengono le parole chiave del vincolo target. Con il nostro metodo, instructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia nella completezza semantica che nella fedeltà ai vincoli. Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale consentire la capacità di pianificazione del linguaggio di modelli più piccoli e specializzati. Creare un set di dati è un passo essenziale per la sua fine. Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici. E la annotazione manuale è costosa. Pertanto, seguendo l'idea di distillazione della conoscenza simbolica, distilliamo i set di dati di pianificazione del linguaggio vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un set di dati di pianificazione del linguaggio vincolata denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità della validazione e dei test set, chiediamo ai lavoratori della mappa di trovare e correggere i campioni errati. Questo grafico mostra la distribuzione dei vincoli di CoScript. Scopriamo che CoScript mostra un'alta polidomostranza negli obiettivi specifici generati. Con CoScript, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolata. Scopriamo che Tffl fine-tune su CoScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare i modelli più grandi quando addestrati su set di dati adatti. In sintesi, stabiliamo il problema della pianificazione del linguaggio vincolata. Valutiamo la capacità di pianificazione del linguaggio vincolata dei grandi modelli linguistici e sviluppiamo un metodo di filtraggio overgenerato per i grandi modelli linguistici. Usiamo i grandi modelli linguistici per generare un set di dati di alta qualità, CoScript, per la pianificazione del linguaggio vincolata. Speriamo che CoScript possa essere una risorsa preziosa per avanzare la ricerca sulla pianificazione del linguaggio. Grazie per il tempo, per trovare maggiori dettagli di CoScript nel nostro documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Janice Lavraik e presenterò il nostro lavoro su Dr. Bert, un modello pre-addestrato in francese per il dominio medico clinico. In questa presentazione, inizieremo a parlare del modellamento linguistico nel settore sanitario, quindi presenteremo il nostro principale contributo. Introduciamo il primo modello medico in francese, chiamato Dr. Bert, basato su Roberta e addestrato su Nachos, un set di dati di medicina da internet. Introduciamo anche un confronto con modelli pre-addestrati con più impostazioni e fonti di dati. Poi presentiamo i nostri risultati su undici compiti di screening clinico in francese. Infine, concludiamo sugli esperimenti e forniamo maggiori dettagli su come accedere ai modelli. Dal suo rilascio nel 2018, Bert è diventato uno dei più efficaci approcci per risolvere compiti di elaborazione del linguaggio naturale e offre un enorme miglioramento rispetto ai metodi statici e contestuali come Word2Vec, FastText o ELMo. Da allora, questo modello è stato adattato a molti altri domini, come il medico con Camembert e altri, ma soprattutto in inglese. I modelli specializzati per altri linguaggi sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati in dominio. Tuttavia, il francese non aveva alcun modello open source per medico fino ad ora. Quindi ci siamo chiesti, una domanda su quali sono le fonti di dati più appropriate per un'ampia gamma di utilizzi? E quelle fonti di dati sono buone sostituzioni per i dati clinici. Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello Shubert, che è basato su dati anonimati ottenuti dal database di non università che abbiamo. Dopo, ci siamo chiesti, quanto dati abbiamo bisogno di addestrare un modello specializzato su dati in francese? Se è quattro gigabyte, otto gigabyte o più? Per rispondere a queste domande, addestriamo e confrontiamo quattro modelli da zero, una versione di Dr. Bert con sette gigabyte di Nachos, una seconda versione di quattro gigabyte di Nachos, una prima versione di Shubert, che è un modello medico con quattro gigabyte di frasi prese dai nodi clinici, e una versione finale di Shubert con un mix di quattro gigabyte di Nachos e quattro gigabyte di nodi clinici. Oltre a questo confronto, introduciamo tre modelli addestrati su pre-addestramento per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di Camembert e addestrato su quattro gigabyte di Nachos, un altro basato su Camembert, ma addestrato su quattro gigabyte di nodi clinici, e infine, uno basato sul modello medico inglese Permitted Bert e addestrato su quattro gigabyte di Nachos. In totale, abbiamo sette modelli. Per valutare i sette modelli, raccogliamo compiti di screening pubblici e privati, come riconoscimento di nomi e identità, classificazione, taglio del discorso e risposte alle domande. Questi modelli sono confrontati con sei modelli di base, che sono Camembert, Oscar, Quattro Gigabyte, Camembert, Ccnet, Permitted Bert e Bio Bert. La valutazione evidenzia che i modelli si comportano meglio su compiti con dati di natura simile a quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogene sono più versatili. Possiamo osservare anche che l'uso di più dati si traduce in prestazioni migliori. In generale, l'addestramento da zero sembra ottenere prestazioni più elevate su la maggior parte dei compiti. Tuttavia, il nostro esperimento di addestramento su pre-addestramento, usando il peso e il tokenizzatore di Permitted Bert addestrato su quattro gigabyte di Nachos, mostra risultati comparabili a quelli ottenuti con Dr. Bert quattro gigabyte da zero. Il che non è il caso per il modello basato su Camembert, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema offre prestazioni migliori su nove dei compiti di screening e supera globalmente i risultati del modello generico qui, Camembert. I nostri modelli sono disponibili gratuitamente e su YouTube, e tutti i script di addestramento sono nel nostro repository GitHub. Quindi grazie per questa presentazione, e non vediamo l'ora di scambiare a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono un studente di ricerca di dottorato all'Università di Washington. Oggi presento il nostro lavoro dal pre-allenamento dei dati ai modelli linguistici per tracciare i percorsi dei pregiudizi politici che portano a modelli di linguaggio non etici. Quindi i modelli linguistici sono addestrati su grandi scale di dati web craw. I media di notizie politici sono ben coperti nei loro dati di pre-allenamento. Secondo un sondaggio del corpus C4, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebra la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudizie e potrebbero portare a potenziali problemi di equità nelle applicazioni downstream. A questo scopo, proponiamo di indagare la pipeline di propagazione del pregiudizio da pre-allenamento dei dati ai modelli linguistici ai compiti downstream, in particolare ponendo le seguenti domande: prima, come valutare l'orientamento politico dei modelli linguistici e quale ruolo i dati di pre-allenamento potrebbero avere su tali pregiudizi politici? In secondo luogo, come i modelli linguistici con diverse opinioni politiche si comportano effettivamente nei compiti downstream e se questo potrebbe portare a problemi di equità nelle applicazioni NLP? In particolare, proponiamo di prima addestrare i modelli linguistici con diversi prompt formati, utilizzando i questionari politici come il test del campo politico. Questo ci assicura di fare valutazioni automatiche ben radicate nella letteratura di scienze politiche. Quindi alcuni risultati preliminari dimostrano che, prima, i modelli linguistici hanno diversi orientamenti politici. Occupano tutti e quattro gli assi del campo politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti. E GPT-3 è generalmente più liberale di BERT e le sue varianti. In secondo luogo, abbiamo l'obiettivo di indagare fino a quale estensione i pregiudizi dei modelli linguistici sono effettivamente presi dai dati di addestramento. Quindi conduciamo un esperimento di controllo pre-allenamento dei checkpoint dei modelli linguistici su sei diversi corpora di partiti e partiti separati in notizie e social media, ulteriormente divisi in loro orientamento politico. Pre-allenando i modelli linguistici su tali corpora di partiti e partiti, possiamo vedere che anche le coordinate ideologiche dei modelli linguistici si spostano. Ad esempio, per Roberta, ulteriormente addestrata sul corpus di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di pregiudizi politici. E anche, proviamo a indagare se i modelli linguistici possono prendere in considerazione la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpora di pre-allenamento in pre-45 Presidente degli Stati Uniti e dopo 45 Presidente degli Stati Uniti. Pre-allenando separatamente i modelli linguistici su due diversi corpora temporali, possiamo vedere che i modelli linguistici in genere hanno un orientamento politico che si allontana dal centro dopo il 2017. Quindi questo indica che i modelli linguistici possono anche prendere in considerazione la polarizzazione che è prevalente nella nostra società. Infine, valutiamo i modelli linguistici con diversi orientamenti politici su applicazioni NLP come la rilevazione del discorso d'odio e la rilevazione di notizie false. E potenzialmente hanno implicazioni molto significative. Quindi vediamo che se indagiamo le prestazioni per categoria, cioè separando le prestazioni in diversi demografici o orientamenti politici dei media di notizie, possiamo vedere un modello che, per esempio, per la rilevazione del discorso d'odio, i modelli linguistici di sinistra sono migliori a rilevare il discorso d'odio contro gruppi socialmente minoritari. Tuttavia, sono peggiori a rilevare il discorso d'odio contro gruppi di potere nella nostra società. E viceversa, i modelli linguistici di destra sono migliori a rilevare il discorso d'odio contro i bianchi e gli uomini, ma sono peggiori a rilevare il discorso d'odio contro i gruppi di colore, le comunità LGBTQ+ e altri gruppi minoritari. Simili tendenze accadono anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori a rilevare la disinformazione dal loro orientamento politico opposto e viceversa. Questo mostra molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici danno previsioni diverse al discorso d'odio e alla disinformazione in base alle loro categorie sociali. Ci sono un sacco di altri esempi nell'appendice per evidenziare ulteriormente che questo indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra fosse trovato e implementato su un social media popolare, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso d'odio contro i gruppi minoritari potrebbe scorrere senza alcun controllo. Quindi questo suona l'allarme per noi per riconoscere e affrontare i problemi di equità che sono il risultato dei pregiudizi politici dei modelli linguistici. Quindi un po 'di discussione. Vorremmo anche evidenziare il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propaga dai dati di pre-allenamento ai modelli linguistici ai compiti downstream, creando infine problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione. Ed è incredibilmente difficile determinare cosa è effettivamente neutrale e dovrebbe essere mantenuto nei dati di addestramento dei modelli linguistici. Quindi è come il problema dell'elettrocarro. Ok, fantastico. Penso che sia tutto per oggi. Grazie per il tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Koistaph Sinha e sono lieto di darvi il benvenuto al nostro discorso sul documento ACL 2023, 'La valutazione dell'acceptabilità dei modelli linguistici non è sempre robusta al contesto'. Questo è un lavoro congiunto con John Waugh, Aaron Muller, Kanishka Mishra, Karen Fentress, Roger Levy e Atina Williams. In questo lavoro, rivisitiamo il Minimal Pair Paradigm. Il Minimal Pair Paradigm valuta fondamentalmente i modelli linguistici in base alle valutazioni dell'acceptabilità, che possono includere grammatica, come il sintagma di limbo, o accettabilità in termini di stereotipi, come le coppie di Kraus. E in questo paradigma, il modo tipico di valutare i modelli linguistici è quello di mostrare una frase accettabile o una frase non accettabile, e poi sperare che il modello dia più probabilità alla frase accettabile. Il pipeline attuale non consente di valutare l'accettabilità dei modelli nei confronti di frasi più lunghe. In questi giorni, i modelli linguistici più grandi hanno finestre di contesto sempre più lunghe. Quindi è fondamentale valutare l'accettabilità dei modelli in tutta la finestra di contesto. Quello che stiamo cercando di fare qui è rivedere il pipeline del Minimal Pair Paradigm chiedendo al modello di valutare l'accettabilità su sequenze più lunghe. Quindi, per simulare queste sequenze più lunghe, rivediamo i set di dati stessi, e poi ricreiamo frasi accettabili e non accettabili da questi set di dati. Ad esempio, qui abbiamo scelto un tipico paio di grammaticalità dal set di Blimp, dal caso di Adjoint Island. E cosa facciamo? Per ricreare sequenze accettabili e non accettabili che hanno la stessa corrispondenza di grammaticalità, estraiamo frasi grammaticali dal set di Adjoint Island e poi aggiungiamo come prefisso sia alla frase accettabile che a quella non accettabile. Possiamo fare lo stesso scegliendo frasi non accettabili dal stesso corrispondente. E questo potrebbe essere usato per testare l'accettabilità del modello. Possiamo fare lo stesso scegliendo frasi da un sottoinsieme o un set di dati diverso. Quindi questo ci dirà se le valutazioni di accettabilità del modello sono effettivamente influenzate dal contesto. Se la frase di contesto proviene da un sottoinsieme o da un set di dati diverso, ma non è completamente irrilevante per la frase che stiamo esaminando. Quindi come fa il modello? Per prima cosa, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti alla query attuale. E lì troviamo che le valutazioni del Minimal Pair Paradigm sono per lo più robuste per contesto arbitrario. Aumentiamo la lunghezza del contesto fino a mille e ventiquattro per massimizzare i modelli di GPT e GPT2. E abbiamo visto qui, nella linea a disegno, le valutazioni del Minimal Pair Paradigm sono relativamente stabili. Quindi cosa succede quando scegliamo frasi da lo stesso set di dati? Qui stiamo creando frasi accettabili e non accettabili dal set di Blimp o Syntax-Gym. E vediamo che le valutazioni del Minimal Pair Paradigm aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o non accettabili. Ma quando corrispondono alla struttura, cioè quando scegliamo frasi da lo stesso fenomeno, vediamo un aumento o una diminuzione massiccia delle valutazioni del Minimal Pair Paradigm per il modello, a seconda che la prefissa sia accettabile o non accettabile. E questo è molto grande. Questo effetto aumenta o diminuisce attraverso tutta la lunghezza del contesto. E questo probabilmente influenzerà i nuovi modelli linguistici che hanno grandi finestre di contesto. Quindi perché la prefissa influisce così tanto sulla valutazione del modello linguistico? Abbiamo fatto una serie di analisi in cui abbiamo provato a perturbare la frase di input. E dopo aver fatto diverse di queste perturbazioni, scopriamo che nessuna di queste perturbazioni fa cambiare il corso di come il modello mostra la valutazione del Minimal Pair Paradigm. Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi di input perturbate in modo simile. Quando perturbiamo le frasi di accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi di non accettabile, vediamo una diminuzione delle valutazioni del Minimal Pair Paradigm in modo simile. Quindi le principali conclusioni del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche latenti sintattiche e semantiche che sono condivise tra le frasi. E il modo in cui valutiamo l'accettabilità attualmente, con input singoli e brevi, potrebbe non catturare pienamente la conoscenza astratta dei modelli linguistici in tutta la finestra di contesto. Per maggiori dettagli dei nostri esperimenti, si prega di leggere il nostro documento. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, un dottorando all'Università di Salant in Germania. In questo video, vorrei presentare il nostro recente lavoro, \"Weaker than you think: A critical look at weakly supervised learning\". Questo è un lavoro congiunto con Xiaoxuan, Mario Smusbach, Giastefano e Dittlich Klako. Vorrei iniziare con una breve introduzione al supervisamento e al learning supervisionato. Nella supervisione, non si etichettano manualmente i dati. Invece, si etichettano i dati usando fonti di etichettatura deboli, come regole semplici, basi di conoscenza o fonti di citazione di bassa qualità, come illustrato nella figura a destra. Quando si confronta con le annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono errate. Se si addestra direttamente le reti neurali sui dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore dell'etichettatura e non generalizzano. Nella learning supervisionato, vengono proposte algoritmi di addestramento per addestrare robustamente le reti neurali su tali dati etichettati. Quindi, i modelli ottenuti ottengono un'alta performance su set di test puliti. In lavori recenti in WSL, si dice che le persone addestrano modelli solo sui dati etichettati debolmente e ottengono un'alta performance su set di test puliti. Tecnicamente, questo non è sbagliato, ma c'è un problema, che è che si presume che ci sia un set di validazione pulito disponibile per la selezione del modello. Questo è un problema adopto per porre tre domande di ricerca. Prima, è necessario un set di validazione pulito per il WSL? O forse si può usare un set di validazione rumoroso? Se il set di validazione pulito è necessario, o se il set di validazione pulito è necessario per il WSL per funzionare correttamente, allora, di quanti campioni puliti abbiamo bisogno? Infine, se si decide di accedere ai campioni puliti, allora addestrare su di loro ottiene ancora un'ottima performance. La figura a destra mostra la differenza di performance tra approcci di addestramento fine. Tipicamente, abbiamo bisogno di venti campioni per classe per raggiungere un'ottima performance. Ma non è la fine della storia, perché se comunque decidiamo di accedere ai campioni puliti, allora addestrare direttamente su di loro ottiene ancora un'ottima performance. La figura a destra mostra la differenza di performance tra approcci di addestramento fine applicati direttamente sui campioni puliti e approcci WSL, che usano i campioni puliti solo per la validazione. Come possiamo vedere, se abbiamo dieci campioni per classe, l'addestramento diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni previsti negli approcci WSL può essere facilmente ottenuto consentendo di continuare a perfezionare i campioni di validazione. Come possiamo vedere, il modello Valina, chiamato ftw, inizialmente sottopone a prestazioni più complesse. Tuttavia, se si consente di continuare a perfezionare i campioni di validazione, allora ftw si comporta allo stesso modo degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che gli approcci recenti WSL richiedono campioni di validazione puliti per funzionare correttamente. Il loro miglioramento delle prestazioni e la praticità sono sovrastimati. Le nostre raccomandazioni concrete per il futuro sono le seguenti: prima, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello viene fatta con set di validazione puliti. Secondo, i metodi WSL dovrebbero essere confrontati con le linee di base di learning freud. In terzo luogo, il continuo perfezionamento è un approccio semplice ma forte che dovrebbe essere considerato in futuro. Lavori."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Aydil Bilal e darò una breve panoramica del documento intitolato \"Prompting LLM for Machine Translation: Assessing Strategies and Performance\". Questo è un lavoro congiunto con i colleghi di Google Translate. L'LM è un modello di linguaggio di 540 miliardi di parametri presentato l'anno scorso, 2022. È addestrato su una grande raccolta di testi che comprende 780 miliardi di token. Al momento della pubblicazione, ha raggiunto il livello di arte in numerosi compiti NLP. In questo lavoro, presentiamo lo studio sistematico della prompt di linguaggio per la traduzione automatica. Valutiamo la capacità di tali modelli utilizzando le migliori pratiche della comunità di MT. Questo comporta l'uso delle ultime testate per evitare un sovrapposizione tra i dati di test e i dati di addestramento del modello di linguaggio. Confrontiamo due sistemi di punta, quindi le ultime metriche di valutazione MT e anche valutazioni esperte basate sull'umano. Infine, forniamo alcune raccomandazioni per la strategia di prompt. La prompt ha un grande impatto sulle prestazioni dei modelli per la traduzione. Come possiamo vedere in un semplice esperimento, in cui usiamo una prompt a uno scatto e forniamo due diverse prompt per ogni frase, la maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto di confusione. E questo può andare in casi estremi, fino a 40 punti di confusione. Quindi è importante selezionare una buona strategia di prompt. Nelle nostre esperienze, abbiamo stabilito una strategia a cinque scatti, in cui ogni frase fornisciamo al sistema la lingua in cui è. Quindi, in questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con tedesco e quelle inglesi con inglese. Abbiamo visto che la forma effettiva della prompt non ha un grande impatto in caso di più scatti. È cruciale per zero e uno scatto, ma quando andiamo, come nel nostro caso, a cinque scatti, c'è quasi nessuna differenza nella forma della prompt. È cruciale per le frasi che portano la maggior parte del peso. In particolare, confrontiamo la selezione delle prompt dai dati di addestramento dei WMT o dai dati Dev. I dati Dev sono molto più accurati e con una qualità superiore rispetto ai dati di addestramento, e i risultati mostrano un migliore rendimento quando usiamo i dati Dev. Tuttavia, i sistemi specializzati hanno un vantaggio sostanziale rispetto alle traduzioni di Palm. Ma Palm si avvicina abbastanza a un sistema commerciale. Nella nostra valutazione, abbiamo eseguito l'umanizzazione utilizzando il framework MPN. La fluosità di Palm è paragonabile a sistemi di punta, ma la differenza principale viene dalla precisione. In particolare, i più comuni errori sono errori di omesso. Quindi sembra che Palm scelga di produrre una traduzione migliore a volte, rimuovendo parti della frase in inglese. Tuttavia, la categoria di stile per Palm è inferiore rispetto ai sistemi di punta, che è un altro segnale che Palm fornisce un output fluido, ma ancora con alcuni problemi di precisione. E questo è tutto per questa breve panoramica. Per maggiori dettagli, per favore, vieni alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingwei Yi e sono del University of Science and Technology of China. È un piacere fare un breve video promozionale del nostro articolo intitolato \"Proteggere il copyright dei modelli di linguaggio per servizi di embedding per servizi di embedding per visualizzazione di backdoor watermark\". Per prima cosa, introdurrò il contesto dei servizi di embedding basati su grandi modelli di linguaggio come GPT, LAMA, PALM, che sono eccellenti per la comprensione e la generazione del linguaggio naturale. I servizi di embedding sono uno dei servizi costruiti su grandi modelli di linguaggio per assistere vari compiti NLP. Ad esempio, OpenAI offre un embedding basato su GPT. Tuttavia, recenti lavori hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento dal servizio di embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dei servizi di embedding. Per proteggere il copyright dei servizi di embedding, una delle soluzioni è inserire un watermark nel servizio di provider e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà: Innanzitutto, il metodo dovrebbe essere applicabile ai servizi di embedding. Secondo, il watermark non dovrebbe degradare l'utilità dei servizi di embedding forniti. Terzo, il watermark dovrebbe essere abbastanza convertibile o l'attaccante può rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. Esistono lavori che possono essere classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili ai servizi di embedding o mancano di trasferibilità. Pertanto, in questo articolo, proponiamo Embedding Marker, che è un metodo di watermark basato su backdoor applicabile ai servizi di embedding. Quindi, introduciamo i dettagli del nostro Embedding Marker. Embedding Marker contiene due passaggi principali: iniezione del watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo prima un set di trigger. Il set di trigger è un gruppo di parole in intervalli di frequenza moderati. Supponiamo che il provider possa raccogliere un corpus generale di testo e contare la frequenza delle parole. Nella iniezione del watermark, definiamo un embedding target. Quando un utente invia una frase al servizio di provider, il provider conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il watermark. In primo luogo, costruiamo un backdoor e un set di dati benigno. Il backdoor dataset contiene frasi in cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger. Quindi, il provider richiede embedding dal servizio di forfettatore con il set di dati. La somiglianza coseno e L2 tra l'embedding richiesto e l'embedding target viene calcolata. Calcoliamo la differenza di somiglianza tra il set di dati backdoor e il set di dati benigno, che è definita come Delta Cosine e Delta L2. Nel frattempo, appliciamo anche il test KS e usiamo il suo valore p come terzo parametro. Condurre esperimenti su quattro set di dati: Agnews, Mined, Ssd2 e Eros Ramazzotti. Supponiamo che il provider applici il set di dati wiki per contare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro Embedding Marker può avere una grande rilevazione delle prestazioni mantenendo una grande utilità per i compiti di schermo. Inoltre, convalidiamo la covertà dell'embedding fornito visualizzando l'embedding delle frasi su quattro set di dati. La legenda delle figure significa il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e normali. Questo è tutto. Grazie. Parleremo con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Yin e il mio collega Ji Yang. Presenteremo la nostra ricerca su multi instruct, migliorando l'apprendimento multi-modello tramite regolazione delle istruzioni. Con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici pre-addestrati per compiti downstream in modo parametrico e efficiente. Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai modelli linguistici di eseguire compiti non visivi in modo serale seguendo istruzioni naturali. Tuttavia, la maggior parte dei precedenti lavori sull'ottimizzazione delle istruzioni si è concentrata sul miglioramento delle prestazioni serali sui compiti linguistici, mentre i compiti di visione e multimediali sono stati lasciati fuori. Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione delle istruzioni su modelli pre-addestrati multimediali può effettivamente migliorare la generalizzazione su compiti non visivi multimediali. Inoltre, durante la nostra ricerca, abbiamo scoperto una discrepanza considerevole nella disponibilità di set di istruzioni tra NLP e multimediali. Esistono più di mille seicento compiti linguistici. Tuttavia, non esiste un set di compiti multimediali di grande scala e pubblicamente disponibile. Pertanto, ci spinti a costruire un set di ottimizzazione delle istruzioni multimediali. Presentiamo Multi Instruct, il primo set di benchmark di ottimizzazione delle istruzioni multimediali che consiste in 62 compiti diversi, che coprono 10 diverse categorie. Questi compiti sono derivati da 21 set di dati open source. E ognuno dei compiti è dotato di cinque istruzioni scritte da esperti. Per indagare l'ottimizzazione delle istruzioni multimediali sul nostro set di dati, prendiamo OFA, un modello pre-addestrato unificato multimediale come modello di base. OFA utilizza un vocabolario unificato per lingua, token di immagine e la coordinazione di una casella di delimitazione. Ecco alcuni esempi di compiti del set Multi Instruct. Per unificare il trattamento di vari input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-seriale, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentate nello stesso spazio di token. Ora, parliamo dell'ottimizzazione delle istruzioni multimediali. Quindi, per il set di dati di allenamento, usiamo 53 compiti di gruppo per l'allenamento, e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test, e selezioniamo altri cinque compiti dal gruppo VQA e Misery. Usiamo tutti gli istanti nel set di test di ogni compito. Inoltre, campioniamo 20 compiti dal set di test di istruzioni naturali come compiti non visti per NLP. Quindi, usiamo il modello pre-addestrato OFA-large come modello di base. Durante l'allenamento, mescoliamo tutti gli istanti per tutti i compiti. Ogni istante è combinato casualmente con una delle sue cinque istruzioni. Quindi, durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello usando una delle cinque istruzioni in ciascuno degli esperimenti. Regoliamo l'accuratezza per compiti di classificazione multimediali. Se è un compito di generazione multimediale, segniamo il RUGL. Per i compiti NLP, segniamo anche il RUGL. Introduciamo anche un'altra metrica di valutazione chiamata sensibilità. Quindi, questo misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente dalla variazione della formulazione dell'istruzione. Ecco i nostri risultati principali. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFA su compiti non visivi multimediali. Inoltre, l'ottimizzazione delle istruzioni provenienti da set di istruzioni naturali può beneficiare l'ottimizzazione delle istruzioni. Quindi, presentiamo il primo set di dati di ottimizzazione delle istruzioni multimediali di grande scala. Stiamo raccogliendo un set di dati di ottimizzazione delle istruzioni multimediali con circa 150 compiti linguistici aggiuntivi. E lo rilasceremo presto. Questo è un codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Yuxin Zhang della Penn State University. Oggi presenterò il nostro lavoro, Cross-lingual Semantic Parsing in Multiple Natural Languages and Representations. Quindi, il parsing semantico è il compito di costruire rappresentazioni semantiche di query come SQL e Lambda Calculus. E il parsing semantico incrociato è il compito di tradurre query in più lingue in più rappresentazioni. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue usando modelli neurali a SQL, Lambda o Funql e così via. Esistono modelli di parsing semantico incrociato proposti e valutati su set di dati limitati e applicazioni. Ad esempio, manca la copertura di certi domini naturali, come il cinese, e manca la copertura di certi rappresentamenti, come il Lambda Calculus. Oltre a questo, ci sono proposti solo un singolo modello per valutarli. Quindi, proponiamo Exemplar, ma forniamo un set di dati uniforme per il parsing semantico in più lingue e rappresentazioni. Contiene 9 set di dati in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni e 22 lingue in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è test di traduzione. Usiamo l'API di Google Translate per tradurre la fonte in lingua target, quindi usiamo un modello monolingue per addestrare e valutare. Ad esempio, addestriamo un modello inglese sulla query inglese e durante l'inferenza, traduciamo la query tedesca in inglese e poi usiamo il modello addestrato per prevedere il SQL. E testamo anche il modello monolingue. In questa impostazione, la lingua di sorgente è la stessa della lingua target, ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione monolingue few-shot, ma addestriamo modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il modello multilingue, che addestra un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue e durante l'inferenza, possiamo usare questo modello per tradurre le query tedesche o cinesi, ecc. E consideriamo anche il trasferimento zero shot e few shot. Addestriamo su una lingua di sorgente e trasferiamo in un'altra lingua. Quindi, durante l'addestramento, addestriamo una query inglese o la combinazione di query inglesi e tedesche per addestrare un modello multilingue e prevedere l'output SQL. E troviamo molti risultati interessanti. Quindi, valutiamo i modelli monolingue, includendo modelli encoder-pdr, che sta per encoder multilingue pre-allenati con decoder basati su puntatori, come xler plus pdr e bert plus pdr. E valutiamo anche i modelli encoder-decoder, che sta per encoder e decoder multilingue pre-allenati, come mbart e mt5. Abbiamo scoperto che encoder-decoder ottiene il miglior rendimento su tutti i 9 set di dati. E valutiamo mt5 e example encoder-pdr in impostazione multilingue. Abbiamo scoperto che encoder-decoder o encoder-pdr può essere migliorato addestrando in una miscela di varie lingue. E abbiamo trovato che è perché la maggior parte delle lingue naturali principali può ottenere un miglioramento delle prestazioni, tranne che la prestazione inglese in 7 set di dati e solo guadagna in 3 set di dati. Penso che questo sia noto come maledizione della multilingualità. Confrontiamo anche il gap di prestazione incrociato. In questa figura, la linea blu è il trasferimento incrociato zero shot, la linea arancione è il trasferimento incrociato few shot, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e la linea arancione, abbiamo scoperto che per l'impostazione zero shot, il trasferimento incrociato di prestazione è significativo, e confrontando la linea blu e la linea arancione, abbiamo scoperto che per l'impostazione few shot, il gap di trasferimento è ridotto rapidamente. E troviamo altri risultati interessanti. Ad esempio, encoder-decoder supera il lavoro precedente o raggiunge risultati comparabili. Addestrare sulla lingua inglese naturale può significativamente aumentare le prestazioni di few shot in lingue target. E abbiamo trovato che modelli multilingue come codas e bleu sono ancora inadeguati per compiti di parsing semantico incrociato. In sintesi, costruiamo Exemplar, un benchmark unificato per il parsing semantico incrociato in più lingue e rappresentazioni. E conduciamo uno studio di benchmark su tre tipi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti, ecc. E benvenuti nel nostro documento e codice. Grazie per aver ascoltato."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Szpekowski e questo discorso riguarda la struttura della coordinazione. Come saprai, diverse strutture di dipendenza sono presunte da diverse teorie e approcci. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è il capo della struttura di coordinazione. Un approccio simmetrico è presunto nella teoria del testo di significato, dove, di nuovo, la struttura di coordinazione è guidata dalla congiunzione. Quindi otteniamo dipendenze da e tutti i congiunti. E infine, c'è un approccio multiheaded usato, ad esempio, nella grammatica delle parole di Katzorn, dove, per così dire, tutti i congiunti sono head della struttura di coordinazione. Quindi otteniamo dipendenze dal governante, qui, lovs, a tutti i congiunti separatamente. Queste sono barche e Maggie. Ora, l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche come queste due. L'argomento è basato sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi. Quindi, in inglese, come sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiuntivi possono essere più lontani. Quindi, marzo ha letto questo libro assolutamente affascinante su le api ieri va bene, mentre marzo ha letto ieri questo libro assolutamente affascinante su le api. Quindi, qui, tra il verbo e l'oggetto, c'è un aggiunto. Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato dopo l'aggiunto. Questo è illustrato qui. Quindi, entrambi questi frasi vanno bene. Dove invece di it, abbiamo questo lungo NP. Ma è anche okay dire, marzo ha letto ieri questo libro assolutamente affascinante su le api. Quindi, il ragionamento qui è che questo è possibile perché, anche se questo frase viola il principio grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che dice che le dipendenze più corte sono preferite. Quindi, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Quindi, qui, abbiamo una dipendenza da red all'aggiunto di lunghezza sette, misurata in parole, e da red al libro di lunghezza quattro, quindi per ottenere è undici. Quando si spostano, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei. Quindi, invece di undici, molto più breve. Ecco perché questo suona abbastanza ok, giusto? Vi viola un principio, ma soddisfa un altro. Ok, quindi, abbiamo estratto varie statistiche sulla coordinazione dall'Enhanced version of the Penn Treebank e vedete il documento, perché non usiamo le dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima, che i congiunti di sinistra tendono ad essere più brevi, quindi salt e pepe e non pepe e sale misurati in sillabe. E anche l'osservazione che è stato fatto in modo passante, che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto di sinistra preferisce essere più breve, giusto? Quindi, la proporzione è più grande del congiunto di sinistra breve. Ma cosa è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando il governante è a sinistra, giusto? Quindi, il governante a sinistra, in questo esempio, sono Bart e Lisa, quindi è il governante, è a sinistra. In tali casi, questo è possibile perché, anche se questo frase viola il principio grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che dice che le dipendenze più corte sono preferite. Quindi, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Quindi, qui, la lunghezza in caratteri, la prima colonna, in sillabe, la colonna centrale, in parole, la seconda colonna, quindi mi concentrerò sulla destra. Quello che vediamo qui è che quando il governante è a sinistra, l'effetto per il congiunto di sinistra essere più breve, cresce costantemente con la differenza assoluta delle parole. E lo stesso è osservato quando non c'è un governante esterno, come nella coordinazione delle frasi. E, quindi, mostra come questo fornisce un argomento contro le strutture asimmetriche di coordinazione come queste due e per le strutture simmetriche come queste due. Quindi, vedi il documento per l'accordo e argomenti, scusa, e parli con noi alla sessione posteriore. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kaiwen e presenterò il nostro lavoro intitolato \"When Does Translation Require Context: A Data-Driven Multilingual Exploration.\" Questo lavoro è stato realizzato in collaborazione con Patrick Kinnane, Emile Liu, Andre F. D. Martins e Graham Neubig. Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase? Bene, se la frase precedente era \"Le cose potrebbero diventare pericolose se i ministri scoprono,\" allora \"mole\" si riferisce a un spia. Ma se la frase precedente era \"Potrebbe essere qualcosa di serio, dottore?\" allora \"mole\" si riferisce a un segno di nascita. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, la traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende i metrica a livello di corpus come bleu incapace di catturare queste traduzioni. E alcuni hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a due domande: quando la traduzione richiede contesto e quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. In precedenza, abbiamo introdotto cxmi come misura per l'uso del contesto dai modelli di traduzione. E questo viene fatto misurando quante informazioni il contesto C fornisce sul target Y, dato il sorgente X. Si può pensare a cxmi come le informazioni ottenute dal dare contesto al modello. In questo lavoro, estendiamo cxmi a cxmi puntuale, che può misurare l'uso del contesto a livello di frase o di parola. Possiamo pensare a parole che hanno un cxmi alto come quelle che richiedono contesto per la traduzione. E poi analizziamo parole con cxmi alto, per cercare modelli tra queste parole. E eseguiamo la nostra analisi su trascrizioni di Ted Talks tradotte da inglese a quattordici lingue diverse. Eseguiamo la nostra analisi a tre livelli diversi. Innanzitutto, guardiamo a parti di parole che hanno cxmi alto, e questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno cxmi alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che certi linguaggi richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo a parole che hanno un cxmi alto, in media, su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come il seguente, dove in cinese è necessario il contesto per tradurre nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento. E allo stesso modo, guardiamo a diversi token individuali che hanno cxmi alto, che ci permette di identificare fenomeni che non possono essere catturati solo dal termine stesso, ma che sono espressi nella struttura della frase, come la risoluzione di ellissi. Ora, usiamo le nostre scoperte per progettare un benchmark per la traduzione a livello di documento. Per ogni uno dei cinque fenomeni di discorso che abbiamo identificato, creiamo taggatori per identificare parole che appartengono al fenomeno. E chiamiamo il nostro taggatore il taggatore Multilingual Discourse Aware, o Muda. Possiamo anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni di discorso. E poi usiamo il taggatore Muda applicando il taggatore su un corpus parallelo che vogliamo usare per la valutazione. E poi applichiamo i nostri metrica di traduzione di scelta sulle traduzioni dipendenti dal contesto che il taggatore Muda ha identificato. E infine, usiamo il nostro benchmark, così come altre metriche, per valutare diversi modelli di traduzione a livello di documento. In primo luogo, quando usiamo metrica a livello di corpus, quindi per bleu, scopriamo che i modelli agnostici al contesto hanno le migliori prestazioni. Ma poi, se usiamo comet, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo wordf, allora i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra di nuovo che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metrica a livello di corpus. Ora, usiamo il benchmark per valutare i modelli, e scopriamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano contesto per certe fenomeni di discorso, come la formattazione e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano contesto per altri fenomeni, come le ellissi, i pronomi e la forma verbale. Quindi, questo suggerisce dove abbiamo bisogno di vedere più progressi per la traduzione a livello di documento. Confrontiamo anche diversi sistemi commerciali, e il nostro benchmark mostra che deepl è di solito più accurato di Google Translate per la traduzione a livello di documento. In sintesi, eseguiamo un'analisi guidata dai dati su quattordici coppie di lingue per identificare quando la traduzione richiede contesto. E poi usiamo le nostre scoperte per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali modelli di discorso possono gestire bene o no, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie per la vostra attenzione, ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Jenny, uno studente del primo anno di ricerca di laurea presso la Carnegie Mellon University. E oggi presenterò il nostro lavoro, analitica di posizionamento, caratterizzazione dei pregiudizi dei set di dati e dei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi della University of Washington e dell'Allen Institute for AI, a nome di Sebastian Santi, Rohan Labrosa, Katerina Raneeka e Martin Sapp. Quindi iniziamo immaginando di lavorare per un giornale e di sfogliare i commenti sotto il tuo articolo per rimuovere contenuti tossici. Potresti rivolgerti a un popolare API, come l'API per la rilevazione della tossicità. E questo funziona davvero bene se sei Carl Jones, dove l'API prospettiva è in grado di rilevare correttamente le istanze tossiche. Ma questo non è il caso di Aditya Sharma, dove l'API prospettiva non è davvero sensibile alle parole offensive che sono più comuni nei contesti indiani. Questo è un esempio di un pregiudizio di progettazione, dove vediamo differenze sistematiche di prestazioni della tecnologia tra popolazioni. Pregiudizi di progettazione come quello che abbiamo appena visto prima possono verificarsi a causa della posizione dei ricercatori e dei sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente usato negli spazi accademici femministi e queer. E come ricercatrice, la posizione può influenzare il processo di ricerca e i suoi risultati, perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero chiedere è: i set di dati e i modelli hanno posizionamento? E non stiamo dicendo che i modelli e i set di dati stessi hanno identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni su altri. Quindi, il lavoro precedente ha suggerito alcuni esempi aneddotici di avere posizionamento, come i divari culturali nei set di dati e nei modelli, oltre a definizioni retoriche di posizionamento dei modelli. Tuttavia, questi lavori non guardano a confrontare gli utenti finali con i set di dati e i modelli stessi. E studiare il posizionamento dei set di dati e dei modelli è sempre più importante man mano che le attività di NLP diventano più soggettive e socialmente orientate. Ed è difficile caratterizzare come questi posizionamenti sono distorti, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Quindi, per studiare il posizionamento dei set di dati e dei modelli, confrontiamo le annotazioni con utenti reali con i set di dati e i modelli esistenti. E questo attraverso il nostro quadro di posizionamento. Il nostro quadro funziona in due passaggi principali. Il primo passo è reannotare i set di dati con vari annotatori. E optiamo per questo piuttosto che guardare alla demografia degli annotatori originali, perché di solito solo pochi annotatori annotano ogni istanza, e perché le demografie sono raramente raccolte e condivise. E quindi optiamo per reannotare i dati per ottenere molti annotatori per istanza e per ottenere un insieme ricco di dati demografici. E poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati usando un punteggio di correlazione di Pearson. E quindi il nostro quadro in realtà differisce dalla letteratura di disaccordo degli annotatori, ma guardando solo all'accordo degli annotatori o alla distribuzione degli annotatori, o modellando gli annotatori. E il nostro quadro è in gran parte abilitato attraverso Lab in the Wild, un platform di esperimento online, ex collaboratore di HCI. E Lab in the Wild è un platform di esperimento online in cui possiamo reclutare volontari diversi, rispetto a piattaforme come Mturk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. E Lab in the Wild è ancora in grado di ottenere dati di alta qualità. E ospitiamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggono una situazione dal set di dati di social chemistry, e poi scrivono quanto è accettabile una situazione. E poi, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un AI e gli altri. E poi confrontiamo queste annotazioni con social chemistry, delphi e GPT four. E poi repliciamo un setup molto simile per il compito di rilevamento della tossicità e dell'odio. E leggeranno un'istanza da Dinhate e scriveranno se pensano che sia un'istanza di odio. E poi confrontiamo queste annotazioni con Dinhate, Perspective API, Rewire API e GPT four. E il nostro studio ammassò oltre 16.000 annotazioni da oltre 1.000 annotatori di 87 paesi. Quindi ora siamo meglio equipaggiati per rispondere a chi sono i set di dati e i modelli di NLP più allineati? Scopriamo che c'è posizionamento in NLP. Ad esempio, troviamo che i set di dati e i modelli sono più allineati con i paesi di lingua inglese. Quindi per il compito di accettabilità sociale di GPT four, troviamo che è più allineato con i paesi di lingua inglese. E troviamo che Dinhate è anche più allineato con i paesi di lingua inglese. E troviamo che più allineamento con le persone che hanno un'istruzione superiore. Quindi per il compito di accettabilità sociale di GPT four, troviamo che è più allineato con le persone che hanno un'istruzione superiore o un'istruzione di laurea. E troviamo lo stesso per Dinhate, dove è più allineato con le persone che hanno un'istruzione superiore. Tuttavia, quando i set di dati e i modelli sono allineati con popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. E esempio, i set di dati e i modelli sono meno allineati con le persone non binarie rispetto ai loro omologhi maschili e femminili. E troviamo questo nel compito di accettabilità sociale di GPT four, così come nel compito di rilevamento della tossicità e dell'odio. Quindi, dato che c'è posizionamento in NLP, cosa possiamo fare? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di progettazione rilevanti durante il processo di ricerca. E la seconda raccomandazione è fare ricerca NLP con una lente di posizionamento. E la terza raccomandazione è costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di questo è l'iniziativa Muscogami. Voglio sottolineare che l'inclusività NLP non è solo fare, sai, tutti i tecnologie funzionano per tutti. E questo conclude la nostra presentazione. Ma se vuoi saperne di più, sentiti libero di controllare il nostro dashboard per i risultati di analisi più aggiornati e il nostro documento. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ciao. Parlerò del nostro lavoro sul risolvenza di espressioni di riferimento indiretta per selezione di entità, in cui introduciamo il corpus di entità alternative. E il mio nome è Javad Hosseini, e questo è un lavoro congiunto con Philip Radlinski, Silvia Parati e Aniket Raju. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: \"Vuoi dire 'Easy on me' o 'I got a feeling'?\" Qui, l'utente vuole selezionare tra uno di questi due. La cosa più ovvia è usare un riferimento diretto. Ad esempio, dicendo il nome della canzone, Easy on me, o la sua posizione, la prima. Ma a volte un riferimento indirettamente è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone, o le pronunce sono troppo simili tra loro e difficili da discriminare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, per esempio, la più recente, o la canzone che non è energetica. Questo è un problema importante nei sistemi di conversazione, e anche per l'entità di Bing. L'LM non è a conoscenza di un set di dati pubblico, di grande scala per il test. Quindi lo raccogliamo usando la condivisione di annotazione. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informità usando un setup di completamento di cartone. Il cartone ha tre bolletta di discorso. Nella prima bolletta, Bob dice, \"Ricorda quella canzone che stavamo ascoltando ieri.\" E con questo, Bob imposta il contesto. Nella seconda bolletta, Alice dice, \"Vuoi dire A o B?\" dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili l'una all'altra, e di solito è più difficile fare la discriminazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio, due libri con il nome 'The Return'. Il terzo è quando hanno descrizioni simili su Wikipedia, e infine, quando hanno simili caselle di informazioni o attributi su Wikipedia, ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome di queste entità, ma non necessariamente conoscono le entità. Quindi quello che facciamo è mostrare un po 'conoscenza di fondo sulle due entità. Per le canzoni, semplicemente mostriamo un link di ricerca di Google per ogni canzone. E poi chiediamo agli annotatori di ascoltare almeno alcune di ciascuna canzone e leggere a riguardo. Ecco il risultato di ricerca di Google per la canzone Easy on me. Per i domini di ricette e libri, mostriamo un po 'testo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo, da Wikipedia, in modo che gli annotatori sappiano come sono. Quindi chiediamo agli annotatori di scegliere una di queste entità, per esempio, la prima, e descriverle usando tre o cinque espressioni di riferimento indiretta. Ecco alcuni esempi dal nostro set di dati. Per esempio, senza parole, non la con il bambino di dodici anni, o la fittizia, o viene dall'Azerba, e così via. Il corpus di entità alternative ha seimila entità alternative in tre domini, e ha quarantamila espressioni di riferimento indiretta. I risultati con il modello T5-large sono riassunti qui. Se il modello linguistico ha accesso allo stesso background di conoscenza come gli annotatori, l'accuratezza è davvero alta. È intorno al 92-95 percento. Ma questo non è realistico. Se il modello linguistico ha accesso a qualche conoscenza di fondo parzialmente sovrapposta, allora l'accuratezza è tra l'82 e l'87 percento, che è più realistico. Ad esempio, quando il modello recupera la conoscenza di fondo, l'accuratezza è solo del 60 percento. Quindi c'è molto spazio per miglioramento. Abbiamo anche mostrato che i modelli sono generalizzabili. Ecco un link al nostro set di dati. Grazie."}
