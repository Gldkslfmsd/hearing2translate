{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann, e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging con multiset e permutazioni latenti. Questo è un lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del test di parsing semantico, la generalizzazione compositiva potrebbe apparire come segue: come al solito abbiamo un insieme di enunciati di addestramento in questo caso la ragazza ha dormito e Mary sapeva che la ragazza aveva dormito questi enunciati sono abbinati a forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio il modello ha visto una ricorsione superficiale durante l'addestramento ed è stato testato su un esempio con una ricorsione più profonda. I modelli sequenza-sequenza naivi faticano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono intesi a catturare il processo compositivo che collega gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente ciò comporta un formalismo considerevole e un pre-elaborazione specifica delle forme logiche, ad esempio per gestire i simboli variabili. L'ottenimento degli alberi può anche coinvolgere procedure specializzate di induzione della grammatica. In questo articolo non usiamo alberi e introduciamo un modello neurale sequenza-sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima tagghiamo ogni token di input con un multi-insieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio abbiamo tutti i token giusti ma non sono ordinati. Ecco perché nel secondo passaggio usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente il nostro modello di permutazione funziona più o meno così. Passiamo da sinistra a destra sull'output e determiniamo quale token del multi-insieme mettere in ogni posizione. Per la prima posizione dell'output selezioniamo semplicemente uno come evidenziato in rosso. Poi saltiamo al prossimo token del multi-insieme per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multi-insieme. Continuiamo questo processo fino a quando ogni token del primo stadio è stato visitato esattamente una volta. Per darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COgs. Il nostro modello supera gli altri con largo margine sulla generalizzazione alla ricorsione più profonda. Tuttavia, alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo risolviamo un paio di sfide tecniche interessanti. Prima di tutto l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multi-insieme provenga, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile ma porta la sfida che trovare la permutazione con il punteggio più alto è Nphard. Questo perché è legato al problema del commesso viaggiatore. Approssimiamo questo con un rilassamento continuo compatibile con la GPU che ci permette anche di retro-propagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò delle nostre persone contrassegnate, utilizzando prompt in linguaggio naturale per misurare gli stereotipi nei modelli di linguaggio. Questo lavoro è stato realizzato in collaborazione con Essenndermush e Danjorovsky. Negli ultimi anni molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli di linguaggio o LLM. Tuttavia, queste misure hanno varie limitazioni: di solito si basano su set di dati costruiti manualmente che richiedono molto tempo per essere curati e inoltre di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti o semplicemente catturano associazioni generali molto ampie come associazioni negative con gruppi particolari. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possono aggravare i pregiudizi ed essere luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM sintonizzati su istruzioni sono molto bravi a rispondere a istruzioni in prompt, quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, utilizzando un prompt come \"immagina di essere una donna asiatica, descriviti\" e possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT4. Vediamo immediatamente che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è rappresentata come modesta, la donna del Medio Oriente è definita usando parole come esotica e come riferendosi a una regione affascinante, e entrambe le persone di donne di colore fanno riferimenti all'ascendenza, mentre la persona dell'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che dandoli a soggetti umani sono stati anche in grado di portare alla luce stereotipi razziali e questo consente anche un confronto diretto tra le nostre persone generate e le risposte scritte umane. La seconda parte sono le parole contrassegnate, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati dai nostri contrassegnati, che spiegherò tra breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su alcun lessico specifico. Quindi il metodo delle parole contrassegnate si basa sul concetto sociolinguistico di contrassegno, che afferma che esiste un default non contrassegnato e qualsiasi gruppo che si differenzia da quel default è linguisticamente contrassegnato. Quindi, ad esempio, la parola uomo o scusa, la parola Guerriero è solitamente associata agli uomini, quindi quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente \"guerriero uomo\" e contrassegnano il termine con \"donna\" e più in generale i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati. Quindi nel nostro metodo, prima designiamo quali sono i gruppi non contrassegnati e contrassegnati e poi confrontiamo le persone usando il metodo delle parole di lotta, che è fondamentalmente l'uso di rapporti log-odds ponderati per distinguere le parole principali per ciascun gruppo contrassegnato. Quindi, ad esempio, per le persone di donne nere, faremmo le parole di lotta e confronteremmo i rapporti log-odds contro sia le persone bianche che le persone uomini perché questi sono i due gruppi non contrassegnati corrispondenti. Ora per alcuni risultati. Prima usiamo il lessico degli stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte dagli umani, tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più alti delle parole del lessico, quelle scritte dagli umani hanno una distribuzione di parole molto più ampia, mentre le parole degli stereotipi che sono nelle persone generate sono davvero solo le parole \"alto\" e \"atletico\", quindi davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura davvero molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece di fare questo, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima, per i gruppi contrassegnati, le parole principali includono cose come cultura, tradizione, orgoglioso ed esotico e queste parole definiscono questi gruppi solo per la loro relazione con la loro identità e li distinguono come diversi dalla norma bianca, il che contribuisce a una lunga eredità di discriminazione e di alterità per questi gruppi. Inoltre, c'è un sacco di cliché comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono una donna latina includono cose come vibrante e sinuosa che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come piccolo e delicato e setoso che si collegano a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomise e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente, il che si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte e, sebbene suoni positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali, quindi invece di lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati sanitari molto negativi per queste persone, tra altri danni. Più in generale, scopriamo che le parole per ciascun gruppo contrassegnato riflettono praticamente solo narrazioni essenzializzanti. Quindi, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima di tutto, noi come ricercatori dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo e infine dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di allineamento di valori strano e eccessivamente eccessivo in corso o forse altri metodi anti-stereotipi che stanno risultando in questi schemi perniciosi. Non possiamo davvero fare ipotesi o studiare ulteriormente senza maggiore trasparenza. Grazie mille per aver ascoltato. Buona fortuna all'ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": ": Ciao, sono James Finch. E sono Sarah Finch.: E oggi vi racconteremo tutto su ABCEV, un nuovo approccio dimensionale alla valutazione dell'IA conversazionale. Questo lavoro è stato svolto dal laboratorio di NLP di Emory, guidato dal professor Gino Choi presso l'Università di Emory e in collaborazione con Amazon Alexa AI.: Quindi, diciamo che avete appena sviluppato un modello di dialogo e volete vedere quanto sia efficace rispetto allo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, come chiedere ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala liquore. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti, quindi potreste voler valutare più dimensioni della qualità della chat per capire i punti di forza e di debolezza del modello a un livello più fine. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi comparativi esistenti o la scala liquore. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat, o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti come influenti sulla qualità della chat nella letteratura recente. ABC eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni bot umane per modello utilizzando ABC eval per il confronto abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni liquore a livello di turno, valutazioni liquore a livello di dialogo e confronti di coppia a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalle nostre analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette dei comportamenti ABC sono nel complesso più affidabili delle etichette raccolte con i metodi esistenti, come misurato dall'accordo degli annotatori su cento conversazioni doppiamente etichettate. Inoltre, le etichette ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni di sé e del partner spieghi rispettivamente il 20% e il 10% della qualità della conversazione, mentre i punteggi di coerenza liquore medi spieghino solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a passi. Potete vedere come la combinazione di tutte le metriche ABC eval spieghi oltre il 25% della qualità della conversazione e come rimuovendo le metriche una alla volta la maggior parte di esse risulti nella perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche liquore a livello di turno spiega molto meno della qualità e molte di queste metriche non portano informazioni uniche. Queste metriche ABC eval affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione più alta di quanto i metodi precedenti siano in grado di raggiungere. Potete vedere nei risultati del nostro esperimento che rimangono ancora diverse sfide e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte, producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro partner circa il 10% delle volte. Con il rapido miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è ancora di più un motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'IA conversazionale progredisca nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "- Ciao, mi chiamo Vauddha e sono un dottorando in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come articolo esteso intitolato \"Transfer learning for dissonance detection\", che affronta la sfida delle classi rare. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. Semplicemente, la dissonanza cognitiva si verifica quando due credenze o azioni sono inconsistenti, come in questo esempio: una persona afferma \"So che le sigarette possono uccidermi\" e poi dice \"Ho fumato un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono inconsistenti e sono in dissonanza. Inoltre, la frase \"Non credo di poter mantenere il mio lavoro senza di loro\" giustifica la seconda azione, che ha una relazione di consonanza, mentre la dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano. Tuttavia, è raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive. Perché è importante studiare la dissonanza cognitiva? Può aiutarci a comprendere gli effetti del disaccordo tra le persone, i cambiamenti nei valori delle credenze e negli atteggiamenti della popolazione. Un'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali. Per raggiungere l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza, come mostrato nell'organigramma qui. I tweet sono stati elaborati utilizzando un parser PDTV e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo, come si può vedere qui. La dissonanza è stata trovata solo nel 3,5 percento delle coppie annotate. Dopo aver raccolto circa mille esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia ottenuto risultati molto migliori del caso, data la bassa frequenza di dissonanza e l'assenza di qualsiasi set di dati precedente. Stiamo affrontando il problema della rarità assoluta. Per alleviarlo, sperimentiamo con combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo tale da poter raccogliere più campioni di dissonanza in meno round di annotazione, riducendo i costi complessivi dell'annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe della dissonanza, abbiamo avviato il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Abbiamo trasferito da due compiti diversi: la classificazione della dissonanza indipendente dal tema e la classificazione binaria delle classi di purezza di espansione e confronto. Poiché questi due compiti sono strettamente correlati alla concezione di consonanze e dissonanze, li chiamiamo ceE. Abbiamo scoperto che trasferendo i pesi, le prestazioni inizialmente nulle sul set di dati annotato sono già molto migliori del caso, con il miglior valore di AUC pari a 0,62. Continuando a perfezionare iterativamente entrambi i compiti, abbiamo scoperto che la perfezionamento dei compiti ceE seguito da un ulteriore perfezionamento sul compito del dibattito porta a prestazioni molto migliori a zero shot. Questo è il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. L'accumulo cumulativo accumula tutti i dati raccolti finora dalle annotazioni attive, mentre gli aggiornamenti iterativi aggiornano il modello addestrandolo sull'ultimo set di dati raccolti. Tra le diverse strategie, abbiamo scoperto che l'accumulo cumulativo ha prestazioni uguali o migliori rispetto agli aggiornamenti iterativi. Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara (PRC) per selezionare principalmente gli esempi che hanno un'alta probabilità di essere dissonanti secondo il modello corrente in qualsiasi round di apprendimento attivo. Confrontiamo questo con altre strategie di apprendimento attivo all'avanguardia comunemente utilizzate nella comunità. Abbiamo scoperto che la strategia proposta PRC funziona meglio di altre strategie all'avanguardia, sebbene la differenza sia piccola. Notiamo che le prestazioni sono significativamente inferiori per il caso casuale in ulteriori round di apprendimento attivo. Con le due migliori strategie, abbiamo migliorato la classificazione della dissonanza AUC 2 0,75, che è la migliore prestazione che abbiamo finora sul compito. Abbiamo anche verificato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Abbiamo scoperto che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, abbiamo scoperto che la PRC è una strategia semplice per l'acquisizione di classi rare e che l'avvio dell'apprendimento attivo con compiti di apprendimento trasferito progettati in modo appropriato può essere molto utile. Abbiamo anche scoperto che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre le annotazioni attive in-dominio traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro codice, al set di dati e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Akshata e oggi il mio co-autore Martin e io presentiamo il nostro lavoro Kit Must: Valutazione dell'integrazione della conoscenza da fonti multiple. Questo lavoro è il risultato di una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio nazionale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti su compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Ma la comprensione del linguaggio naturale spesso richiede conoscenza che viene fornita anche al momento dell'inferenza, per esempio nella frase \"John ha visto il neo-eletto presidente in TV\", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere con certezza chi sia questa entità specifica, John, o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è stato fatto il pre-addestramento. Pertanto, i modelli di successo per compiti di NLU intensivi in termini di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante il pre-addestramento che quella acquisita al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione del riferimento centrale progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti, valutiamo il set di dati con partecipanti allo studio umani e stabiliamo modelli di risoluzione del riferimento centrale. Ecco un esempio dal nostro set di dati: \"Il giudice Kia è un fornaio e dopo una lunga giornata di lavoro a decidere casi in un codice di legge, era felice di rilassarsi\". Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è il giudice Kia. La risoluzione di un dato pronome richiede due tipi di informazioni: primo, conoscenza specifica dell'entità, come \"il giudice è un giudice\", e secondo, conoscenza generale, come \"i giudici decidono casi nei tribunali\". La conoscenza generale viene appresa durante il pre-addestramento dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al momento dell'inferenza. Varia la disponibilità di questi due pezzi di informazione in modo che possano essere trovati in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di kitdmos. Primo, abbiamo l'impostazione tipica del pre-addestramento, dove si assume che la conoscenza generale sia disponibile al momento del pre-addestramento. Secondo, c'è l'impostazione \"background both\", dove la conoscenza generale è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione \"background inference\", dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante poiché simula il caso in cui la conoscenza generale necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli, per esempio perché si sono sviluppate nuove occupazioni dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti nell'impostazione \"background pre-train\": si assume che la conoscenza generale \"i politici cercano seggi elettivi nel governo\" sia contenuta nei parametri pre-addestrati. Nel contesto dell'inferenza, forniamo la conoscenza specifica \"Chichester è un politico\". Nell'impostazione \"background both\", forniamo inoltre non solo la conoscenza specifica, ma anche la conoscenza generale sui politici nel contesto dell'inferenza. Nell'impostazione \"background inference\", forniamo la caratteristica \"occupazione\" semplicemente come \"tour\" invece che \"politico\", perché è improbabile che \"merelytour\" sia contenuto nei parametri pre-addestrati. Valutiamo il set di dati sia con partecipanti allo studio umani che stabiliamo modelli di risoluzione del riferimento. In questa figura mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile dell'impostazione \"background pre-train\" senza addestramento specifico sul kitdmos. Entrambi i modelli non performano bene quando addestrati sul kitdmos, tuttavia sia c2f che built for coref performano significativamente meglio della scelta casuale. Questo suggerisce che, quando addestrati su set di dati generici di risoluzione del riferimento, i modelli imparano a sfruttare indizi superficiali che non sono utili quando si testa su kitdmus, dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile la conoscenza generale solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo, molti modelli di evoluzione del co-riferimento sembrano incapaci di ragionare sulla conoscenza da diverse fonti senza addestramento specifico sul compito. Tuttavia, con l'addestramento specifico sul compito, alcuni modelli integrano con successo la conoscenza da fonti diverse. Ancora, anche i modelli con le migliori prestazioni sembrano avere difficoltà con la conoscenza generale integrata in modo affidabile presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e date un'occhiata al set di dati nel codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pai dell'Università di Trento e del Bruno Kessler, e vi presenterò brevemente l'attenzione come guida per il paper sulla traduzione simultanea del parlato, che è un lavoro congiunto con Matteo Negri e Marco Durchi. Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato o SimST è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, che consente la comunicazione tra lingue diverse. Quali sono i problemi dei modelli SimST attuali? Le architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complicate, ad esempio l'addestramento che coinvolge diversi obiettivi di ottimizzazione e l'addestramento e il mantenimento di diversi modelli per raggiungere diversi regimi di latenza, ad esempio l'addestramento di un modello con una latenza media di un secondo e di un altro con due secondi di latenza e così via. Quindi, qual è la nostra soluzione? Prima di tutto, utilizzare i modelli SD offline già esistenti senza riaddestramento o adottare un'architettura specifica per SimulaSD. Utilizziamo un solo modello per ogni regime di latenza e gestiamo la latenza attraverso parametri specifici. Sfruttiamo la conoscenza già acquisita da un modello attraverso il meccanismo di tensione tra l'input audio e l'output testuale, ovvero il meccanismo di attenzione incrociata. Potete vedere un esempio a destra. La nostra soluzione è proporre un'attenzione a punto o decodifica dell'encoder. È una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione punta su una parola. Viene emessa se la tensione non è concentrata, ovvero se questa somma è al di sotto di una certa soglia alfa verso gli ultimi fotogrammi del discorso lambda, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un blocco di discorso contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco, e guardiamo i pesi dell'attenzione incrociata, vedremo che le prime due parole puntano ai fotogrammi del discorso ricevuti più presto, mentre l'ultima parola punta agli ultimi fotogrammi del discorso ricevuti come fotogrammi del discorso lambda. Ciò significa che le prime due parole verranno emesse, mentre, poiché la somma dell'attenzione incrociata è al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspettiamo un altro blocco di discorso. Se continuiamo e riceviamo un altro blocco di discorso e il nostro modello prevede altre tre parole, e guardiamo il peso dell'attenzione incrociata, vedremo che nessuna parola punta agli ultimi fotogrammi del discorso lambda. Ciò significa che queste tre parole verranno emesse. Se guardiamo i risultati principali di un punto, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo da un lato il blu che misura la qualità della traduzione e il ritardo medio, ovvero la misura della latenza. Consideriamo anche la mancanza media consapevole del calcolo che tiene conto del tempo di calcolo del modello per prevedere l'output. Quindi, vogliamo che le nostre cure siano il più alte possibile su questo grafico, ma anche che siano spostate a sinistra e le confrontiamo con le strategie plepara che sono anche applicate ai modelli offline, ovvero la strategia Whit key e l'accordo locale, e confrontiamo anche con l'architettura allo stato dell'arte specificamente progettata per la traduzione simultanea dello spazio. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco e vediamo che AD supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate a sinistra. E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo di usura computazionale, AD è la strategia più veloce. Se volete scoprire altri risultati, leggete il nostro paper e abbiamo anche rilasciato open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Shu Ha. Oggi presenterò il nostro articolo Do Connel 2003 intitolato \"Le entità nominate Tagus funzionano ancora bene nel 2023\". Cominciamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate o il compito NER. Abbiamo osservato che i modelli utilizzano Con 2003 per sviluppare NER da quasi 20 anni e ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo un nuovo tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il set di dati Con plus+, che è un set di dati raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione Con 2003. Abbiamo poi perfezionato oltre 20 modelli su Con 2003, li abbiamo valutati sia sul set di test Con 3 che sul set di test Con plus fast e, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di f1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer si generalizzano normalmente meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione e, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di perfezionamento influisce direttamente sulle prestazioni di un compito a valle. Anche qui abbiamo scoperto che più esempi di perfezionamento portano effettivamente a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'adattamento per l'overfitting, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte, e ciò si manifesta solitamente come la diminuzione dei rendimenti su un nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e i dati di test. Per l'adattamento per l'overfitting, abbiamo visto che dal grafico a destra la linea rossa di miglior adattamento ha una pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo fatto su Con 2003 si traduce in più di un'unità di miglioramento su Con plus plus, il che significa che non ci sono rendimenti decrescenti. E ciò ci mostra che l'adattamento per l'overfitting in questo caso non è osservato. Quindi, che dire della deriva temporale? Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni si degradano con un divario temporale più ampio e ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale. La nostra conclusione è che per una buona generalizzazione avremmo bisogno di una migliore architettura del modello, di una dimensione del modello più grande e di più esempi di perfezionamento e questi vanno di pari passo, non possiamo avere solo un ingrediente ma tutti gli altri contemporaneamente. Abbiamo anche scoperto che il calo delle prestazioni qui è causato dalla deriva temporale e, in modo piuttosto sorprendente, non è causato dall'adattamento per l'overfitting, anche se Connel 2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger Connal 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, vi preghiamo di controllare il nostro articolo, il nostro set di dati e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di De plain, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stoden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi. Nell'esempio qui potete vedere una coppia di frasi allineate parallele di una frase complessa in tedesco e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, la dilatazione delle clausole, la riorganizzazione delle elezioni incrociate o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus D plane perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomia. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus Dplane, che è diviso in due sottocorpora, Deplane APA e Deplane web. Deplane APA si basa su testi di uso. In Deplane APA, abbiamo allineato manualmente 483 documenti. Il risultato è di circa trentamila tredicimila coppie di frasi parallele. Per Deplane web, questo corpus include diversi domini e abbiamo anche allineato manualmente tutti questi settecentocinquanta documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, abbiamo ottenuto 30 450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio sul tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi delle notizie o ai testi per studenti di lingua a tutti i livelli per quanto riguarda, ad esempio, la semplificazione lessicale, la semplificazione strutturata e il livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus di semplificazione profonda ha una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus Deplane API abbiamo molte più riordinazioni e aggiunte di parole rispetto a quanto abbiamo nel corpus Deplane web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset d plane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in documenti post. Ma nel nostro caso d'uso stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno la stessa lingua e lo stesso contenuto, ma sono su livelli di complessità diversi. Ora, dato che abbiamo il nostro dataset d plane che ha frasi allineate manualmente, possiamo usare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo fatto alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento. Alla fine abbiamo concluso che il miglior metodo di allineamento automatico da usare per i testi per la semplificazione del testo in tedesco è il metodo di allineamento di massa e potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo mostrato nel nostro documento è un caso di semplificazione automatica del testo tramite il perfezionamento di modelli linguistici per produrre un testo semplificato dal testo complesso di input. Abbiamo perfezionato due diversi modelli: abbiamo perfezionato il modello di Long Short-Term Memory per produrre semplificazioni a livello di documento e abbiamo anche perfezionato la parte normale di base di Long Short-Term Memory per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete esaminare più dettagliatamente i punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo perfezionamento di base potrebbe produrre o potrebbe ottenere punteggi migliori rispetto ai punteggi di base e abbiamo proposto quei risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Vi ringraziamo molto per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono X Yuan dell'Università FNAi. Sono qui per presentare il nostro lavoro, Distinguere la conoscenza degli script da modelli di linguaggio leggero per il piano linguistico vincolato. Nella vita quotidiana, gli esseri umani devono spesso pianificare le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti. I lavori precedenti hanno sfruttato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e hanno dimostrato che i grandi modelli di linguaggio possono scomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate, mentre la pianificazione di obiettivi con vincoli specifici, come fare una torta al cioccolato, rimane ancora sottovalutata. In questo articolo definiamo il problema della pianificazione linguistica vincolata che impone diversi vincoli sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei modelli di linguaggio della vita reale. Poiché non esistono dati specifici sugli obiettivi per il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati in loop con l'uomo. Utilizziamo instruct GPT per campionare 100 obiettivi specifici e valutare gli script generati dai modelli della libreria. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli di apprendimento ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici. Quindi conduciamo un'analisi dettagliata per indagare cosa fanno i modelli di apprendimento. I risultati della figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Esaminiamo più attentamente le categorie di vincoli definite in WiH. La mappa termica della figura mostra che le prestazioni di pianificazione di instruct GPT variano notevolmente per ragazze di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di apprendimento è soggetta a una grande varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea del filtro Z sovra-generato per migliorare la qualità della generazione. Prima mostriamo i tipi vincolati con esempi per instruct GPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Quindi, istruiamo instruct GPT a generare script chiave per obiettivi specifici. Successivamente, sviluppiamo un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding di instruct GPT e calcoliamo la somiglianza coseno e i punteggi di somiglianza per misurare la somiglianza semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nella dimensione dell'obiettivo. Con il nostro metodo, instructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica, che di fedeltà ai vincoli. Poiché i grandi modelli di linguaggio sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passo essenziale per raggiungere questo obiettivo. Tuttavia, gli studi precedenti non abilitano la pianificazione di obiettivi specifici e l'annotazione manuale dei dataset di dati è costosa. Per questo, seguiamo l'idea della distillazione della conoscenza simbolica per distillare il dataset di pianificazione linguistica vincolata dai modelli di linguaggio leggero. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata chiamato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di validazione e test, chiediamo ai lavoratori del crowd-sourcing di trovare e correggere gli errori nei campioni errati. Questa figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un alto pluralismo negli obiettivi specifici generati. Con CodeScript possiamo trattare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che t5, ottimizzato per il tasso di punteggio, può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli di linguaggio, indicando che i modelli più piccoli possono supportare modelli più grandi quando sono adeguatamente addestrati su siti di dati adatti. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata, valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli di linguaggio e sviluppiamo un metodo di filtro sovra-generato per i modelli di linguaggio live. Utilizziamo grandi modelli di linguaggio per generare un dataset di alta qualità chiamato CodeScript per la pianificazione linguistica vincolata. Speriamo che il dataset CodeScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Trovate maggiori dettagli su CodeScript nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Jannislavak e vi presenterò i nostri lavori su Dr. Bert, un robusto modello pre-addestrato in francese per il dominio biomedico e clinico. In questa presentazione, prima parleremo della modellazione del linguaggio nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato Dr. Bert, che si basa su Roberta e viene addestrato su nachtchos, un insieme di dati medici raccolti dal web. Presenteremo anche un confronto del modello con diverse impostazioni criogeniche e fonti di dati. Poi presenteremo i nostri risultati su 11 compiti a valle biomedici e clinici in francese e, infine, concluderemo gli esperimenti e vi daremo maggiori dettagli su come accedere ai modelli. Da quando è stato rilasciato nel 2018, Bert è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un enorme miglioramento delle prestazioni rispetto ai metodi statici e contestualizzati storici come war to ve fast text e altro ancora. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert e altri domini come il biomedico con Permit Birth e Bio Birth e il clinico con Clinical Birth, ma soprattutto in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, fino ad ora, il francese non aveva alcun modello open source per il dominio biomedico. Quindi, ci siamo posti la domanda su quali siano le fonti di dati più appropriate per una vasta gamma di utilizzi e se questi dati grezzi siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato Dr. Bert con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'ospedale non universitario della nostra città. Poi ci siamo chiesti quanti dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi: sono quattro gigabyte, otto gigabyte o più? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero: una prima versione di Dr. Bert con sette gigabyte di nachos, una seconda versione di quattro gigabyte di set di nature, una prima versione di Schubert, un modello clinico con quattro gigabyte di frasi tratte dalle note cliniche, e una versione finale di Schubert con un mix di quattro gigabyte di set di nature e quattro gigabyte di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati su un pre-addestramento di controllo per analizzare l'impatto della strategia di pre-addestramento: uno basato sul peso di Camembert e addestrato su quattro gigabyte di set di nature, un altro basato sempre su Camembert ma addestrato questa volta sui quattro gigabyte di lotti clinici, e infine uno basato su un modello biomedico inglese BermedBert e addestrato su quattro gigabyte di set di snatches. In totale, abbiamo sette modelli da valutare. Per i nostri sette modelli, abbiamo raccolto quali compiti a valle pubblici e privati come il riconoscimento dei nomi, la classificazione, il tagging della parte del discorso e la risposta alle domande. Questi modelli sono stati confrontati con sei modelli di base: Camembert, Oscar, centottantadue gigabyte di Camembert Oscar, quattro gigabyte di Camembert CC Net, quattro gigabyte di Plummet, Bird Biobert e Clinical Bird. L'evoluzione evidenzia che il modello si comporta meglio nel compito con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo ottenere questi dati da fonti eterogenee che sembrano essere più versatili. Osserviamo anche che l'uso di più dati si traduce in prestazioni migliori. Nel complesso, l'addestramento da zero sembra ottenere prestazioni più elevate sulla maggior parte dei compiti. Tuttavia, il nostro esperimento sull'addestramento pre-addestrato utilizzando il peso e il tokenizer di Permit Bird addestrato sul sottoinsieme di quattro gigabyte di naturalos mostra risultati comparabili a quelli ottenuti con Dr. Bert 4 gigabyte da zero, il che non è il caso per il modello basato sui pesi e il tokenizer di Camembert che soffrono di problemi di stabilità. Infine, come conclusione, il nostro sistema offre prestazioni migliori su nove degli undici compiti a valle e supera globalmente il risultato del modello generico qui Camembert. Osserviamo anche che i dati specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da Nachos sono liberamente disponibili e sull'interfaccia e tutti gli script di addestramento sono sul nostro repository GitHub. Quindi, grazie per questa presentazione e non vediamo l'ora di interagire nella sessione dei poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "- Ciao, sono Xhang Bing, dottorando in Lingue e Letterature Straniere all'Università di Washington. Oggi presenterò il nostro lavoro che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce di pregiudizi politici che portano a modelli di NLP ingiusti. Quindi i modelli linguistici vengono addestrati su grandi quantità di dati raccolti tramite web crawl. I media di notizie politiche sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus c4 possiamo vedere che il New York Times, il Los Angeles Times, il Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, hanno potuto imparare da prospettive diverse che celebrano la democrazia e la pluralità delle idee, dall'altro, queste diverse opinioni politiche sono intrinsecamente di parte socialmente e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tal fine, proponiamo di indagare il flusso di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, specificamente ponendoci le seguenti domande: in primo luogo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbero avere i dati di cruing su tali pregiudizi politici? in secondo luogo, come si comportano effettivamente i modelli linguistici con diverso orientamento politico nei compiti a valle e se ciò potrebbe risultare in problemi di equità nelle applicazioni di NLP? Quindi, proponiamo innanzitutto di stimolare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test del compass politico. Questo ci garantisce di fare una valutazione automatica ben fondata nella letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno orientamenti politici variabili; occupano tutti e quattro i quadranti sul compass politico. Possiamo anche vedere che GPT4 è il modello linguistico più liberale di tutti e che la serie GPT è generalmente più liberale socialmente rispetto alla serie Bird e alle sue varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente raccolti dai dati di addestramento. Quindi potremmo condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli linguistici su sei diversi corpus partigiani separati in notizie e social media, ulteriormente divisi per il loro orientamento politico addestrando ulteriormente i modelli linguistici su tali corpus partigiani. Possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta ulteriormente affinata ulteriormente addestrata sul corpus di Reddit di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di suoi pregiudizi politici. Abbiamo anche cercato di indagare se i modelli linguistici possano cogliere la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpus di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti. Addestriamo separatamente i modelli linguistici sui due diversi corpus temporali. Possiamo vedere che i modelli linguistici in generale avevano un orientamento politico più lontano dal centro dopo il 2017. Quindi ciò indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società. Quindi, da ultimo ma non meno importante, valutiamo i modelli linguistici con diversi orientamenti politici sul rilevamento di discorsi d'odio e sul rilevamento di notizie false, applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Quindi vediamo che se indaghiamo le prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o media politici, possiamo vedere un modello che, ad esempio, per il rilevamento di discorsi d'odio i modelli di sinistra sono migliori nel rilevare i discorsi d'odio che prendono di mira gruppi socialmente minoritari, tuttavia sono peggiori nel rilevare i discorsi d'odio che prendono di mira gruppi più potenti nella nostra società e viceversa. I modelli di destra sono migliori nel rilevare i discorsi d'odio che prendono di mira i bianchi e gli uomini, tuttavia peggiori nel rilevare i discorsi d'odio che prendono di mira i neri, LGBTQ+ e altre comunità di minoranza. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli di sinistra sono migliori nel rilevare la disinformazione dai loro opposti orientamenti politici e viceversa. Questo dimostra ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi significati politici danno diverse previsioni su esempi di discorsi d'odio e disinformazione basate sulle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che c'è un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere affinato su discorsi d'odio o disinformazione o altro e implementato su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e i discorsi d'odio che prendono di mira i gruppi di minoranza potrebbero semplicemente diffondersi senza alcun controllo. Quindi questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità risultanti dagli orientamenti politici dei modelli linguistici. Quindi un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come essere tra Scilla e Cariddi. Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando infine problemi di equità. Se provassimo a sanificare in qualche modo, correremmo anche il rischio di censura o esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nei dati di addestramento. Quindi è un po' come il problema del trolley elettrico. Bene, penso che sia praticamente tutto ciò che ho per oggi, cinque minuti per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Koov Sinna e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 23. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto. Questo è un lavoro congiunto con John Waqui, Aaron Mueller, Kanishka Mishra, Karen Fs, Roger Levy e Atina Williams. Quindi, in questo lavoro, rivediamo il paradigma del paio minimo. Quindi, il paradigma del paio minimo valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità che possono includere anche la grammaticalità come la sintassi del blimp o l'accettabilità in termini di stereotipi come le coppie di folle. E in questo paradigma del paio minimo, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticale e poi mostrare una frase inaccettabile o una frase non grammaticale e poi si spera che il modello attribuisca fondamentalmente una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe. Oggi i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe, quindi è fondamentale che valutiamo l'accettabilità dei modelli in tutta la finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi questo è l'approccio. Quindi, ciò che facciamo è simulare queste sequenze più lunghe rivedendo i set di dati stessi e poi ricreando frasi scegliendo frasi accettabili o inaccettabili da quei dati. Quindi, per esempio, qui abbiamo scelto un paio tipico di frasi grammaticali di tè dal set di dati blim dal caso dell'isola aggiunta e ciò che facciamo è ricreare frasi più lunghe che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale. Estraiamo frasi grammaticali dall'isola aggiunta e poi le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile in modo da poter fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento e ciò potrebbe anche essere utilizzato per testare l'accettabilità dei modelli e possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Quindi questo è ciò che chiamiamo scenario di mismatch. Quindi qui le frasi provengono ancora da set di dati pertinenti ma non dallo stesso set di dati con cui si sta valutando e possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto, come se il contesto provenisse da un sottoinsieme diverso del set di dati o se fosse completamente irrilevante per la frase che stiamo guardando. Quindi, come si comporta il modello? Prima guardiamo le frasi di Wikipedia, che sono completamente irrilevanti per l'attuale coppia di query, e lì troviamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Aumentiamo la lunghezza del contesto fino a mille e ventiquattro per massimizzare i modelli Ot e gpt due, e qui vediamo nella linea tratteggiata arancione che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso set di dati? Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso set di dati blim syntax gym e lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili, ma quando combiniamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in blame person syntax gym, vediamo un aumento o una diminuzione massiccia del giudizio pp per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora questo e questo effetto è molto grande e aumenta in tutta la lunghezza del contesto e ciò probabilmente influenzerebbe i nuovi modelli linguistici che hanno una grande finestra di contesto. Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico? Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante ma aggiungendo rumore all'input e dopo aver fatto diverse di queste perturbazioni, troviamo che nessuno di questi rumori fa effettivamente cambiare al modello il suo corso in termini di come ci mostra la tendenza del giudizio MPP. Fondamentalmente, troviamo che i modelli sono sensibili alle frasi perturbate in modi simili, cioè quando perturbamo le frasi nel dominio accettabile vediamo un aumento simile in tutte le perturbazioni e quando perturbamo le frasi nel dominio di approvazione accettabile vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro è che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, il modo in cui la facciamo attualmente con input di frasi brevi e singole, potrebbe non catturare completamente la conoscenza astratta del modello linguistico in tutta la finestra di contesto. Per favore, leggete il nostro articolo per ulteriori dettagli dei nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, uno studente di dottorato all'Università di Staland in Germania. In questo video vorrei presentare il nostro lavoro recente, Più debole di quanto pensiate, uno sguardo critico all'apprendimento weeklyvis. Questo è un lavoro congiunto con X, Myos Mosbach e Ge Steffen e Dirich Klako. Vorrei iniziare con una breve introduzione al weak supervision e all'apprendimento debolmente supervisionato. Nel weak supervision non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli come semplici regole euristica, base di conoscenza o crowd sourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto meno costose, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestrando direttamente le reti neurali sui dati di etichettatura weekly, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento supervisionato weekly, vengono proposti algoritmi di addestramento che addestrano in modo robusto le reti neurali sotto tale rumore delle etichette in modo che i modelli addestrati si generalizzino ancora bene. Nei lavori recenti in wSL, l'abbreviazione di weeklyvised learning, un'affermazione comune è che le persone dicono che è possibile addestrare i modelli solo sui dati di etichettatura weekly e ottenere alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema: le persone presuppongono che esista un ulteriore set di validazione pulito. Per la selezione dei modelli, ci siamo fermati su questo problema, ma ciò implica che sono necessarie ulteriori annotazioni manuali nell'apprendimento con supporto weekly. Tuttavia, come un elefante nella stanza, questa necessità è spesso trascurata. La ricerca menzionata chiede di porre tre domande di ricerca. La prima è se i dati di validazione puliti siano necessari per l'wSL o se possiamo forse utilizzare un set di validazione anno? In secondo luogo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori affinché l'wSL funzioni, allora quanti campioni puliti ci servono? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli. Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti: in primo luogo, scopriamo che interessantemente i recenti metodi wSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente, altrimenti si verifica un grande calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci wsSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci wSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo solo bisogno di 20 campioni per classe per ottenere alte prestazioni. Ma non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento diretto su di essi otterrà addirittura prestazioni migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, che sono applicati direttamente sotto i dati puliti, e gli approcci wSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo dieci campioni per classe, il fine-tuning diretto inizia a battere gli approcci wSL. Infine, il miglioramento delle prestazioni affermato nei precedenti approcci wSL può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello di validazione denominato FTW inizialmente sottoperforma i metodi wSL più complicati come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTW si comporta altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi wSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci wSL richiedono campioni annotati manualmente puliti affinché funzionino correttamente, il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti: in primo luogo, riportare i criteri di selezione del modello, ad esempio riportare se la selezione del modello è stata fatta bene con campioni di validazione puliti; in secondo luogo, gli approcci wSL dovrebbero essere confrontati con poche linee di base di atterraggio brevi, come il lavoro su campioni concreti; infine, il fine-tuning continuo è una semplice ma forte linea di base che dovrebbe essere considerata nel lavoro futuro in WSL. Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Vi invitiamo a controllarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Al Villaard e vi darò una breve panoramica dell'articolo che presenta strategie e prestazioni di prompting per la traduzione automatica. Questo è un lavoro congiunto con i miei colleghi di Google Translate. Pm è un modello linguistico con 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testi che comprende 780 miliardi di token. Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro presentiamo uno studio sistematico gratuito del prompting per i grandi modelli linguistici per la traduzione automatica. Valutiamo la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò comporta l'uso degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico e confrontiamo due sistemi all'avanguardia, quindi i sistemi con le migliori prestazioni della valutazione WMT. Usiamo metriche neuralmt all'avanguardia e mostriamo anche i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha una grande influenza sulle prestazioni dei grandi modelli linguistici per la traduzione, come possiamo vedere in un semplice esperimento in cui utilizziamo un breve prompting e forniamo due prompt diversi per due frasi diverse. Per la maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto sfumato. E questo può arrivare in casi estremi fino a 40 punti sfumati. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti abbiamo optato per una strategia di prompting a cinque colpi in cui contrassegniamo semplicemente la frase che forniamo al sistema con la lingua in cui si trova. Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente, sono contrassegnate con il colon tedesco e le traduzioni inglesi con la colonna inglese. Abbiamo visto che la forma effettiva della stampa non ha una grande influenza nel caso di un prompting breve. È cruciale per il prompting a zero e uno colpo, ma quando passiamo, come nel nostro caso, a un prompting breve, non c'è quasi nessuna differenza con la forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riassunto dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più creati e di qualità superiore rispetto ai dati di addestramento, che sono più belli e i risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale sulle traduzioni di Palm, ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla simulazione che abbiamo eseguito utilizzando il framework NpN è che la fluidità di Palm è paragonabile ai sistemi all'avanguardia, ma la differenza principale proviene dall'accuratezza. Quindi, in particolare, gli errori più comuni sono errori di omissione. Quindi sembra che Palm scelga di produrre una traduzione che suona meglio a volte tralasciando parti della frase sorgente che sono fatte nella traduzione. Tuttavia, la categoria di stile esterno per Pan è inferiore rispetto ai sistemi all'avanguardia, che è un segnale aggiuntivo che Par fornisce un output davvero fluido ma ancora con alcuni problemi di accuratezza. Questo è tutto per questa breve panoramica. Per maggiori dettagli, per favore venite alla presentazione completa dell'articolo. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Jin Wei Y dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video pubblicitario del nostro articolo. State copiando il mio modello, Protezione del diritto d'autore dei grandi modelli di linguaggio per l'embedding e i servizi? Vi restituiamo la filigrana. Introduciamo prima il contesto relativo ai servizi di embedding pubblicitario. Attualmente, i grandi modelli di linguaggio come Gbt, La, PLm sono eccezionali nella comprensione e nella generazione del linguaggio naturale. I servizi di embedding pubblicitario sono uno dei servizi costruiti sui grandi modelli di linguaggio per assistere in vari compiti di NLP. Ad esempio, OpenI offre un'API di embedding basata su Gbt, tuttavia lavori recenti hanno dimostrato che l'attaccante può rubare il modello imparando dall'embedding e fornire servizi simili, quindi è necessario proteggere il diritto d'autore dell'embedding come servizi per proteggere il diritto d'autore dei servizi di embedding, una delle soluzioni è inserire una filigrana nel servizio del fornitore e rilevare se un altro servizio contiene la filigrana. Il metodo della filigrana deve soddisfare le seguenti proprietà: in primo luogo, il metodo dovrebbe essere applicabile all'embedding come servizi; in secondo luogo, la filigrana non dovrebbe degradare l'utilità dell'embedding fornito; in terzo luogo, la filigrana dovrebbe essere sufficiente per l'attaccante o l'attaccante dovrebbe poter rimuovere facilmente la filigrana; infine, la filigrana dovrebbe essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie, tuttavia questi metodi non sono applicabili all'embedding come servizi o mancano di trasferibilità. Pertanto, in questo articolo proponiamo un marker di embedding che è un metodo di filigrana basato su backdoor applicabile all'embedding come servizi. Poi lasciatemi presentare i dettagli del nostro marker di embedding. Il marker di embedding contiene due passaggi principali: l'iniezione della filigrana e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo prima un trigger set. Il trigger set è un gruppo di parole in un intervallo di frequenza moderata. Ipotizziamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione della filigrana, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del diritto d'autore è rilevare se il modello dietro un altro servizio contiene la filigrana. Costruiamo prima una backdoor e un set di dati benigni. Il set di dati della backdoor contiene frasi di cui tutte le parole appartengono al trigger set, mentre tutte le parole nelle frasi del set di dati benigni non appartengono al trigger set. Poi il fornitore richiede embedding dal servizio stiller con il set di dati. Vengono calcolate la similarità coseno e l2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra i set di dati benigni e della backdoor, definita come Delta coseno e delta l2. Nel frattempo, applichiamo anche il test KS e usiamo il suo p-value come terza metrica. Condottiamo esperimenti su quattro set di dati: Aaging news mind SD2 e A spam. Ipotizziamo che il fornitore applichi il set di dati del testo wiki per contare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro marker di embedding può avere grandi prestazioni di rilevamento mantenendo al contempo una grande utilità per i compiti a valle. Validiamo anche la convertibilità dell'embedding fornito visualizzando l'embedding delle frasi su quattro set di dati B PCA. La legenda delle figure indica il numero di trigger in ogni frase, come mostrato nelle figure. È difficile distinguere tra gli embedding dei fattori e gli embedding normali. Questo è tutto. Grazie. Venite a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Ian e il mio collega Jiian e presenteremo la nostra ricerca sull'apprendimento seriale multimodale migliorato tramite l'ottimizzazione delle istruzioni. Quindi, con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento utilizzando modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in modo serializzato seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si concentra sul miglioramento delle prestazioni serializzate su compiti esclusivamente linguistici, dove l'elaborazione delle immagini e i compiti multimodali sono stati esclusi. Pertanto, in questo lavoro vogliamo indagare se l'ottimizzazione delle istruzioni su modelli multimodali delle proteine possa effettivamente migliorare la generalizzazione a compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra P e multimodali: esistono più di 1.600 compiti di istruzione solo per il pranzo, tuttavia non esiste un compito di istruzione multimodale su larga scala pubblicamente disponibile. Questo ci ha motivato a costruire un set di dati di ottimizzazione delle istruzioni multimodali. Qui presentiamo Multi-ins instruct, il primo set di dati di benchmark di ottimizzazione delle istruzioni multimodali che consiste di 62 compiti multimodali diversi che coprono 10 categorie generali. Questi compiti sono derivati da 21 set di dati open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare l'ottimizzazione delle istruzioni multimodali sul nostro set di dati proposto, prendiamo come modello base un modello di addestramento multimodale unificato. Utilizziamo un vocabolario unificato per i token di linguaggio e immagine e le coordinate di una bounding box. Qui mostriamo alcune istanze di esempio dal nostro set di dati Multi-instra. Per unificare l'elaborazione di vari tipi di input e output, seguiamo il metodo di Offa e formuliamo tutti i compiti in un formato sequenza a sequenza unificato in cui il testo di input, le immagini, le istruzioni e le bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'ottimizzazione delle istruzioni multimodali. Quindi, per il set di dati di addestramento, utilizziamo cinquanta tre compiti dal gruppo N per l'addestramento e campioniamo diecimila istanze per compito per il test. Riserviamo l'intero gruppo di lettura del senso comune per il test e selezioniamo ulteriori cinque compiti da Wiki e dal gruppo Miscellaneous. Utilizziamo tutte le istanze nel test speed per ogni compito, inoltre, campioniamo casualmente 20 compiti dal test speed di istruzioni naturali come per NP. Quindi utilizziamo un pre-addestramento di un grande modello come modello base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti, ogni istanza è combinata casualmente con uno dei suoi cinque modelli di istruzione. Quindi, durante il test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo le prestazioni medie e massime e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza; se è un compito di generazione multimodale, riportiamo l'errore radice quadratica (L) per un compito LP, riportiamo anche l'errore radice quadratica (L). Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito indipendentemente dalla leggera variazione nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFFA su compiti multimodali uguali. Anche l'apprendimento trasferito dal set di dati di istruzioni naturali può beneficiare dell'ottimizzazione delle istruzioni. Qui possiamo vedere che all'aumentare del numero di compiti, il modello ottiene prestazioni migliori e nel frattempo una sensibilità inferiore. Quindi abbiamo anche condotto un esperimento utilizzando un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità. Questo mostra l'effetto di diverse strategie di ottimizzazione delle istruzioni sulla sensibilità del modello. Come possiamo vedere dall'apprendimento trasferito dai set di dati di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello IFA originale. Possiamo anche vedere che l'apprendimento trasferito dai set di dati di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul set di dati di istruzioni nitrogen. Quindi, nel complesso, abbiamo proposto il primo set di dati di ottimizzazione delle istruzioni multimodali su larga scala. Abbiamo migliorato significativamente la capacità neurale di OFFA e abbiamo esplorato diverse tecniche di apprendimento trasferito mostrando che ci sono benefici. Abbiamo progettato una nuova metrica chiamata sensibilità. Quindi, un'altra cosa che stiamo facendo è la raccolta di set di dati di ottimizzazione delle istruzioni multimodali molto più grandi con circa 150 compiti di linguaggio aggiuntivi e li rilasceremo. Questo è un codice QR per i nostri dati e modelli. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Just John della Penn State University. Oggi presenterò il nostro lavoro, Ex examplelar: Analisi semantica multilingue in più lingue naturali e rappresentazioni manuali. Quindi l'analisi semantica è un compito per costruire rappresentazioni semantiche delle query degli utenti come SQL e calcolo Lambda. E l'analisi semantica multilingue è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL lambda o funql e così via. I modelli esistenti di analisi semantica multilingue sono proposti e valutati separatamente su set di dati limitati a poche lingue e applicazioni. Ad esempio, ci sono perdite di copertura su certe lingue naturali, il cinese è mancante e ci sono perdite di copertura su certe mini rappresentazioni, il calcolo Lambda è mancante o sono valutati solo su certi modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo exampler, ma forniamo un set di dati uniforme, exampler, per l'analisi semantica multilingue in più lingue naturali e rappresentazioni del significato. Contiene nove set di dati in domini virali, cinque tasse di analisi semantica, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è il test di traduzione. Usiamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, quindi usiamo un modello monolingue per addestrare una valutazione. E, ad esempio, addestriamo il modello inglese su una query inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per prevedere lo SQL. E testiamo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione di fusione monolingue addestrando modelli monolingue con solo il 10 percento dei dati di addestramento e testiamo la modellazione di un modello multilingue che addestriamo per tutte le lingue. Ad esempio, mettiamo insieme le query tedesco-inglese-cinese per addestrare un modello multilingue e durante l'inferenza possiamo usare questo modello per tradurre le query tedesche o cinesi, ecc. Consideriamo anche il trasferimento multilingue zero short e field short. Addestriamo su una lingua di origine e trasferiamo su un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su una query inglese o sulla combinazione di inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL. E abbiamo anche trovato molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli, inclusi i decodificatori PDR, che stanno per decodificatori multilingue pre-addestrati con decodificatori basati su puntatori come XLr plus PDdR e bird plus PDdR. Valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come MBt e Mt5. Abbiamo scoperto che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati. E valutiamo su MT5 e XLMR plus PDR un'impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder PDR possono essere migliorati addestrando in una miscela di varie lingue. E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese scendono in sette set di dati e guadagnano solo in tre set di dati. Penso che questo sia noto come curve di multilingue. Abbiamo anche confrontato il divario di prestazioni multilingue. In questa figura, la linea blu è il trasferimento multilingue few short. La linea arancione è il trasferimento multilingue zeroshot mentre la linea verde è nell'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e arancione, abbiamo scoperto che l'impostazione zero short, il divario di prestazioni del trasferimento multilingue è significativo e confrontando la linea blu e arancione abbiamo scoperto che con l'impostazione few short il divario di trasferimento si accorcia rapidamente. Abbiamo anche trovato altri risultati interessanti, ad esempio l'encoder-decoder supera il lavoro di progresso o raggiunge risultati comparabili. Addestrare la nostra lingua naturale inglese può aumentare significativamente le prestazioni di few short sulle lingue naturali di destinazione e abbiamo scoperto che i modelli di linguaggio multilingue come coders e blue sono ancora insufficienti per l'analisi semantica multilingue. In sintesi, abbiamo costruito exampler, un benchmark unificato per l'analisi semantica multilingue con più lingue naturali e rappresentazioni del significato. Condottiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue e i nostri risultati mostrano molti risultati interessanti e così via. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Shrikkovski e questo discorso riguarda la struttura di dipendenza della coordinazione. Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del testo semantico di Igor Milčuk, dove di nuovo l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, in quanto individuano uno dei congiunti. Ora ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio prag, l'approccio a testa congiunzione assunto nelle banche di alberi di dipendenza prag, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo dipendenze dall'inizio a tutti i congiunti. Infine, c'è anche un approccio a più teste che è usato, per esempio, nella grammatica a parole di Cutson, dove per così dire tutti i congiunti sono capi della struttura coordinata. Quindi otteniamo dipendenze dal governatore qui ama a tutti i congiunti separatamente. Questi sono i pulsanti che si stanno creando ora. L'obiettivo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. L'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi in inglese, come forse sapete, i nostri oggetti diretti preferiscono essere vicini al verbo mentre gli aggiunti possono essere più lontani. Quindi \"March ha letto ieri\" va bene perché l'oggetto diretto \"lo\" è vicino al verbo mentre \"March ha letto ieri\" è molto peggio perché qui tra il verbo e l'oggetto diretto c'è un aggiunto \"ieri\". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo perché allora può essere spostato nella posizione dopo l'aggiunto. Questo è illustrato qui. Quindi entrambe queste frasi sono corrette: \"March ha letto questo libro assolutamente affascinante sui BC ieri\" va bene. Invece di \"lo\" abbiamo questo lungo ma è anche ok dire \"March ha letto ieri questo libro assolutamente affascinante sulle api\". Il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza che dice che sono preferite dipendenze più corte. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali. Quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo la dipendenza da \"red\" all'aggiunto di lunghezza sette misurata in parole e da \"red\" a \"book\" di lunghezza quattro. Quindi insieme è 11. Quando si spostano questi due costituenti la somma di queste due dipendenze diventa sei. Quindi invece di 11, sei, molto più corta. Ecco perché suona abbastanza ok. Viola un principio ma ne soddisfa un altro. Quindi quello che abbiamo fatto è estratto varie statistiche sulla coordinazione dalla versione migliorata della banca di dati Pentra e vedere l'articolo perché non abbiamo usato le dipendenze universitarie e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti. Quindi \"sale e pepe\" non \"pepe e sale\" misurati in sillabe e anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la lunghezza, la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo. Quindi la proporzione è più grande dei congiunti corti di sinistra ma cosa c'è di nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra. Quindi il governatore è a sinistra in questo esempio \"ho visto Bart e Lisa\". Quindi il governatore è a sinistra, è assente nel secondo esempio \"Homer è venuto e ha starnutito\". Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Quindi in tali casi il congiunto di sinistra preferisce essere più corto, tanto più quanto maggiore è la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra come qui, il governatore di sinistra governa la coda della coordinazione e la rete, questo effetto scompare. Quindi lo dimostriamo misurando la lunghezza in caratteri, la prima colonna in sillabe la colonna centrale e in parole la colonna di destra. Quindi mi concentrerò su quella di destra. Quello che vediamo qui è che quando il governatore è a sinistra la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole e lo stesso si osserva quando non c'è un governatore come nella coordinazione di frasi ma quando il governatore è a destra questa tendenza scompare e dimostriamo nell'articolo come questo fornisce un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedere l'articolo per l'accordo completo e gli argomenti. Mi dispiace e parlateci della sessione dei poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "- Ciao, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto: un'espressione multilingue guidata dai dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Ferange, Emiliu, Andre F.D Martins e Graham Newbiig. Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremo \"mole\" in questa frase se la frase precedente è \"le cose potrebbero diventare pericolose se i ministri lo scoprono\", allora \"mole\" si riferisce a uno spia, ma se la frase precedente è \"potrebbe essere qualcosa di serio, dottore\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e quindi anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questi è piuttosto difficile, innanzitutto perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come la Blue incapaci di catturare queste traduzioni. Alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Primo, quando la traduzione richiede contesto? e secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto un lavoro dipende dal contesto nella traduzione. Nel lavoro precedente abbiamo introdotto cxmi come misura per l'uso del contesto da parte dei modelli di traduzione automatica e questo si fa misurando quanta informazione il contesto C fornisce sull'obiettivo y dato la fonte x. Potete pensare a cxmi come all'informazione guadagnata dal dare contesto al modello. In questo lavoro estendiamo cxmi a point y cxmi che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un alto p6mi come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto p6mi per cercare schemi tra queste parole e svolgiamo la nostra analisi su trascrizioni di discorsi che sono state tradotte dall'inglese in 14 lingue diverse. Eseguiamo la nostra analisi a tre livelli diversi. Primo, guardiamo ai tag di parte del discorso che hanno alti mezzi pxmi e questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno pxMmi relativamente alto e questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo e allo stesso modo troviamo che certe lingue richiedono anche contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi del vocabolario che hanno un alto p6I in media su tutte le sue diverse occorrenze e questo ci aiuta a identificare casi come quello qui dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento e allo stesso modo troviamo che il contesto è supportato per tradurre nella giusta formalità e infine guardiamo a diversi token individuali che hanno alto pxmi e questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa ma che sono piuttosto espressi nella struttura della frase come la risoluzione dell'ellissi. Ora usiamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento per ciascuno dei cinque fenomeni del discorso che abbiamo identificato abbiamo creato tagger per identificare automaticamente le parole che appartengono al fenomeno e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso o tagger muda. Possiamo anche notare che lingue diverse hanno diverse proporzioni di questi fenomeni del discorso. Poi usiamo il tagger muda applicando il tagger su un corpus parallelo che vogliamo usare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger M ha identificato. E infine usiamo il nostro benchmark così come altre metriche per valutare diversi modelli a livello di documento nella traduzione automatica. Prima di tutto, quando usiamo le metriche a livello di corpus, quindi per la Blue troviamo che i modelli agnostici al contesto hanno le migliori prestazioni. ma poi se usiamo commentt, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo la misura wordf, allora i modelli con o senza contesto hanno prestazioni confrontabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo le metriche a livello di corpus. Ora usiamo il benchmark M per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale, ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni come i pronomi ellittici e la forma verbale. Quindi questo suggerisce dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che dL è solitamente più accurato di Google Trans per la traduzione a livello di documento. Per riassumere, eseguiamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e poi usiamo i nostri raffinamenti per costruire un benchmark per la traduzione automatica a livello di documento che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o no e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per l'attenzione, ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il vostro lavoro sulla posizionalità di AnL, caratterizzazione dei pregiudizi di progettazione e dei set beta dei modelli. Questo lavoro è stato svolto in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santi, Ronin Labrasse, Katharina Reinika e Martin Sapp. Quindi, iniziamo immaginando che state lavorando per un giornale e state setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere contenuti tossici. Potreste rivolgervi a un'API popolare come l'API prospettiva per il rilevamento della tossicità e questo funziona davvero bene se siete Carl Jones, dove l'API prospettiva è in grado di rilevare correttamente le istanze tossiche, ma non è il caso di Didtha Sharma, dove l'API prospettiva non è altrettanto sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione, dove vediamo differenze sistematiche di performance della tecnologia tra le popolazioni. I pregiudizi di progettazione come quello che abbiamo appena visto prima possono verificarsi a causa della posizionalità dei ricercatori di NLP e degli sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone detengono come risultato della loro demografia, identità ed esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, specificamente negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati perché può cambiare le decisioni che i ricercatori prendono e quindi una domanda che le persone potrebbero porsi è se i set di dati e i modelli hanno una posizionalità e non stiamo cercando di dire che i modelli stessi e i set di dati stessi hanno identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Quindi, il lavoro precedente ha suggerito alcune prove aneddotiche di avere una posizionalità come le lacune culturali nei modelli e nei set di dati, così come definizioni teoriche della posizionalità dei modelli. Tuttavia, questi lavori non esaminano realmente il confronto tra gli utenti finali e i set di dati e i modelli stessi e studiare la posizionalità dei modelli e dei set di dati è sempre più importante poiché i compiti di NLP diventano più soggettivi e socialmente orientati ed è difficile caratterizzare come queste posizionalità siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Quindi, per studiare la posizionalità dei set di dati e dei modelli, confrontiamo effettivamente le annotazioni con gli utenti reali con i set di dati e i modelli esistenti. Facciamo questo attraverso il nostro framework Nl posizionalità. Il nostro framework funziona in due passaggi principali: il primo passo è ri-annotare i set di dati con diversi annotatori e scegliamo di farlo guardando alla demografia dei set di dati originali degli annotatori perché di solito solo alcuni annotatori annotano ogni istanza e perché la demografia è raramente raccolta e condivisa. Quindi, scegliamo di ri-annotare i dati per ottenere molte annotazioni per istanza e per ottenere un ricco insieme di dati demografici. Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il nostro punteggio di correlazione di Pearson. Quindi, il nostro framework si differenzia effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, anziché guardare solo all'accordo degli annotatori inseriti o alle distribuzioni degli annotatori di modellazione. Il nostro framework è in gran parte abilitato da lab in the wild, una piattaforma di crowdsourcing online, ex collaboratore HCI e lab in the wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto alle piattaforme come Turk che hanno in gran parte partecipanti dagli Stati Uniti o dall'India e inoltre lab in the wild è ancora in grado di ottenere dati di alta qualità. Abbiamo ospitato due compiti su lab in the wild, uno dei quali è l'accettabilità sociale e il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di chimica sociale e poi scriveranno quanto una situazione sia socialmente accettabile. Per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'AI e con altri. Abbiamo quindi confrontato queste annotazioni con la chimica sociale Delphi e gPT4. Abbiamo quindi replicato una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un'istanza da Dynah Hate e scriveranno se pensano che sia un'istanza di discorso d'odio. Abbiamo quindi confrontato queste annotazioni con Dyna Hate Perspective API, Rewire API, Hate Roberta e GPT4. Il nostro studio e alla fine ha accumulato oltre 16.000 annotazioni da oltre mille annotatori da ottantasette paesi. Quindi, ora siamo meglio attrezzati per rispondere a chi si allineano i set di dati e i modelli di NLP di più? Abbiamo scoperto che c'è una posizionalità nel NLP. Ad esempio, abbiamo scoperto che i set di dati e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi della accettabilità sociale gpd quattro, abbiamo scoperto che è più allineato ai paesi confuciani e di lingua inglese. Abbiamo scoperto che Dinah hate è anche più allineato ai paesi di lingua inglese. Abbiamo anche scoperto un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi, per gpd4 nel compito di accettabilità sociale, abbiamo scoperto che è più allineato alle persone con un'istruzione universitaria o di scuola superiore e abbiamo trovato lo stesso per Danny hate dove è più allineato alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto alle controparti maschili e femminili. Abbiamo scoperto questo nel compito di accettabilità sociale gPDd4 così come nell'analisi del compito di Dina hate. Quindi, dato che c'è una posizionalità analitica nel NP, cosa possiamo fare al riguardo? Quindi, abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione pertinenti durante il processo di ricerca e l'altra è fare ricerca NLP con la lente del prospectivismo. La nostra terza raccomandazione è costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche e un buon esempio di ciò è l'iniziativa masakanne I e vogliamo sottolineare che l'NLP inclusivo non è solo fare in modo che tutte le tecnologie funzionino per tutti e quindi questo conclude la nostra presentazione, ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, parlerò del nostro lavoro sulla risoluzione delle espressioni differenziali indirette per la selezione di entità, in cui introduciamo il corpus di entità alternative e mi chiamo Javad Hosseini. Questo è un lavoro congiunto con Philip Radlinsky, Sylvia Parity e Annie Luis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. E io considero questa domanda alternativa: Intendevi \"easy on me\" o \"I got a feeling?\" Qui un utente vuole selezionare tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone \"Easy on Me\" o la sua posizione, la prima, ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone o le pronunce sono troppo simili tra loro e difficili da disambiguare o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette. Ad esempio, il più recente o il segno che non è energico. Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità degli LLM. Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per un compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un'impostazione di completamento dei fumetti. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: \"Ricorda quella canzone che stavamo ascoltando ieri?\" e con questo Bob imposta il contesto del dialogo. In questa seconda bolla di dialogo, Alice dice: \"Intendevi easy on me o I got a feeling?\" che è la domanda alternativa. Nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio \"il più recente\". Forniamo automaticamente la prima e la seconda bolla di dialogo, ma la terza è compilata dall'annotatore. La prima bolla di dialogo è scelta tra alcuni prompt manuali per dominio, la seconda, che è la domanda alternativa, è generata come segue: usiamo sempre un semplice modello \"Intendevi a o b?\" dove a e b sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è di solito più difficile fare l'ambiguizzazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome. Il terzo è quando hanno descrizioni simili su Wikipedia e infine quando hanno caselle di informazioni o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per la canzone. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità ma non necessariamente le entità stesse, quindi ciò che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google a ciascuna canzone e poi chiediamo agli annotatori di ascoltare almeno alcune di ciascuna canzone e leggere su ciascuna canzone. Ecco ad esempio il risultato della ricerca Google per la canzone \"Easy on Me\" per il dominio ricette e libri, mostriamo un po' di testo di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e di descriverle utilizzando da tre a cinque espressioni di riferimento indirette, ad esempio quella con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati. Ad esempio, quella senza parole, non quella con il ragazzo di 12 anni o quella fittizia o proveniente dall'Azerbaigian e così via. Il corpus alternativo ha 6.000 domande alternative in tre domini e ha 422.000 espressioni di riferimento indirette. I risultati con il modello T5X large sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta, è intorno al 92-95 percento, ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di base parzialmente sovrapposte, allora l'accuratezza è tra l'82 e l'877%, che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 6 percento, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro set di dati. Grazie."}
