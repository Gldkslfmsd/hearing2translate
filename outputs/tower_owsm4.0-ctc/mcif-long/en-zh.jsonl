{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫马蒂亚斯·林德曼，今天我将向大家简要介绍我们的论文，该论文探讨了在不使用树的情况下通过多集标记和潜在置换实现的组合泛化。这是我和我的导师亚历山大·科勒和伊万·蒂托夫共同完成的工作。组合泛化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语未见过的组合的能力。在语义解析测试中，组合泛化的表现通常如下：我们通常有一个训练集，在这种情况下是“女孩睡觉”和“玛丽知道女孩睡觉”，这些话语与表示其核心意义的逻辑形式配对，与标准机器学习评估不同，测试集不是来自相同的分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练过程中已经看到了浅层递归，并在具有更深递归的例子上进行了测试。朴素的序列到序列模型难以应对这种未在训练数据中出现的泛化，并且通常会产生与输入脱节的输出，特别是它们往往无法再现输入和输出之间的系统对应关系，例如在例子中用颜色标注的那些。解决这个问题的一种流行方法是将树集成到模型中。树旨在捕捉将话语与逻辑形式联系起来的组合过程。这种方法效果很好，但通常树不是给定的，需要以某种方式获得。这可能是一个复杂且有时计算成本高昂的过程。通常，这涉及到相当形式主义的逻辑形式的特定预处理，例如处理变量符号。获得树也可能涉及到专门的语法感应程序。在这篇论文中，我们没有使用树，而是引入了一个神经序列到序列模型，该模型首次直接建模了输入片段与输出片段之间的对应关系。我们展示了在不依赖树的情况下对更深层次的递归的强大泛化能力。我们的方法预测输出来自输入，分为两步：首先，我们用一个无序的多集标记每个输入标记，这些标记将出现在输出中。第一步后，我们有了所有正确的标记，但它们没有排序。这就是为什么在第二步中，我们使用另一个模型来预测一个置换，将它们排列成正确的顺序。我们引入了一种新的方法来预测置换，该方法对可能的置换没有硬性约束。这使得我们的方法非常灵活和富有表现力。从概念上讲，我们的置换模型大致是这样工作的。我们从左到右遍历输出，并确定每个位置放置哪个多集标记。对于第一个输出位置，我们简单地选择一个，如图所示。然后我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳到另一个多集标记。我们继续这个过程，直到第一个阶段的所有标记都被访问过一次。为了给您一个实验结果的预览，我们在这里将我们的方法与其他无树模型在COgs基准上进行了比较。我们的模型在对更深层次的递归的泛化方面远远优于其他模型。然而，其他一些类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技术挑战。首先，输入和输出之间的对齐在训练数据中没有给出，因此对于给定的标记，我们不知道它来自哪个多集标记，这给训练带来了挑战。此外，有时有多个置换与数据一致，但语言学上正确的置换是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。我们的置换方法非常灵活，但带来了一个挑战，即找到得分最高的置换是NP难的，这是因为这与旅行商问题有关。我们用一种GPU友好的连续松弛方法来近似这一点，这也使我们能够通过解决方案进行反向传播，并学习语言学上更可信的置换。如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息，请查看我们的论文或来参加我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Myra，今天我将谈论我们的论文中的标记角色，使用自然语言提示来衡量语言模型中的刻板印象。这项工作是与 Essenndermush 和 Danjorovsky 合作完成的。近年来，许多人记录了大型语言模型或 LLM 中社会偏见和刻板印象的普遍存在。然而，这些方法有各种局限性，它们通常依赖于手工构建的数据集，这些数据集需要花费大量时间来整理，而且它们通常只衡量非常特定的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景，或者它们只是捕捉非常广泛的联想，如与特定群体的负面联想。此外，这个领域的大部分工作都没有考虑交叉性，即多方面社会身份可以加剧偏见，并成为伤害的独特来源。为了克服这些局限性，我们依赖于这些较新的指令调优 LLM 非常擅长响应提示中的指令的特性，因此我们可以要求模型生成一个角色，这是一个对虚构个人的描绘，使用类似于“想象你是一个亚洲女性，描述自己”的提示，我们可以立即看到，这对于任何人口统计都是非常通用的，因为我们可以在这个提示中指定任何我们想要的身份标记。以下是 GPT4 的一些示例生成，我们立即看到，虽然输出不是传统意义上的明显消极或有毒，但有一些有趣的模式。亚洲女性被描绘成不引人注目，中东女性被用词如“异域”来描述，并像提到一个迷人的地区一样，而两个有色人种角色都提到了祖先，而白人角色则没有任何这样的描述。为了捕捉这些模式，我们的方法有两部分。第一部分是生成这些角色。我们的提示生成这些角色的灵感来自一项研究，他们给人类受试者这些提示，发现通过给人类受试者这些提示，他们也能够浮出水面种族刻板印象，并且这使得我们能够直接比较我们生成的角色和人类书写的回应。第二部分是标记词，这是一种识别区分标记组与我们标记组的词的方法，我稍后会详细解释。这种方法的好处是我们得到了非常具体的刻板印象和模式，而无需依赖任何特定的词汇。因此，标记词方法借鉴了社会语言学中的标记性概念，该概念指出存在一个未标记的默认值，任何与该默认值不同的群体在语言上都是标记的。例如，单词“人”或“抱歉”，单词“战士”通常与男性相关，所以当人们描述一个女战士时，他们通常会具体说明“女战士”，并用“女”来标记这个词。更广泛地说，社会中的主导群体在语言和社会上都是未标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先指定未标记和标记的群体是什么，然后我们使用战斗词方法比较角色，这基本上是使用加权对数几率比来区分每个标记组的顶级词。例如，对于黑人女性的角色，我们会进行战斗词，并将对数几率比与白人角色和男性角色进行比较，因为这些是两个相应的未标记群体。现在，让我们来看看一些结果。首先，我们使用刻板印象的词汇，发现生成的角色包含比人类书写的角色更多的刻板印象。然而，当我们实际查看词汇中的词分布时，我们发现了非常不同的事情。因此，虽然生成的角色具有更高的词汇率，但人类书写的角色具有更广泛的词分布，而生成的角色中的刻板印象词实际上只是“高大”和“运动”这两个词，真正积极或至少是非消极的词。事实上，这个词汇库根本没有很好地捕捉到我们之前幻灯片中看到的许多有害模式。因此，相反，我们将转向我们标记词方法的结果，以展示这些看似积极的词如何促进刻板印象和本质化叙述。在我们的分析中，我们揭示了这些看似积极的描绘反映了哪些有害模式。首先，对于标记组，顶级词包括“文化”、“传统”、“自豪”和“异域”等，这些词仅通过它们与身份的关系来定义这些群体，并将其与白人规范区分开来，这促成了这些群体的长期歧视和异化的遗产。此外，这些词反映了许多常见的陈词滥调，特别是对于有色人种女性。例如，描述拉丁美洲女性的词包括“充满活力”和“曲线美”，这些词与热带主义的陈词滥调相连；对于亚洲女性，这些词是“娇小”、“精致”和“丝绸般”，这些词与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等长期的历史相连；最后，对于黑人女性，我们看到一些顶级词是“坚强”和“韧性”，这与人们所谓的“坚强黑人女性”原型相连，虽然乍一看这听起来很积极，但有研究表明，这种原型实际上是非常有害的，因为它给这些人口统计施加了很大的压力，要求他们在面对社会障碍时要坚韧和强大，因此，与其真正致力于改变这些障碍，它反而给这些人施加了克服它们的压力，这导致了这些人的非常消极的健康结果，以及其他危害。更广泛地说，我们发现每个标记组的词几乎完全反映了非常本质化的叙述。基于这些模式，我们为模型所有者提出了三条建议：首先，我们作为研究人员应该解决积极的刻板印象和本质化的叙述，我们还应该使用交叉视角来研究偏见和危害，因为如果不这样做，可能会忽略很多东西；最后，应该真正增加关于减少偏见的透明度，因为例如这些积极的刻板印象，我们不知道是因为存在某种奇怪的过度价值对齐，还是可能有一些其他反刻板印象方法导致了这些有害的模式。没有更多的透明度，我们真的无法做出任何假设或进一步研究。非常感谢大家的聆听，祝大家在 ACL 玩得开心。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "：大家好，我是詹姆斯·芬奇。我是萨拉·芬奇。：今天我们将向大家介绍ABCEV，一种全新的评估对话式人工智能的维度方法。这项工作由埃默里大学语言处理实验室完成，由埃默里大学的基因·崔教授领导，并与亚马逊Alexa AI合作完成。：假设你刚刚开发了一个对话模型，你想看看它与当前的先进水平相比表现如何，通常的做法是使用人类评估，例如让人类评判员选择两个对话中哪一个更好，或者根据酒精度量表对对话进行评分。这些方法在提供整体对话质量评估方面效果很好，但对话质量有许多方面，因此你可能希望评估对话质量的多个维度，以便更细致地了解模型的优缺点。一种方法是简单地让人类评判员评估对话质量的几个维度，例如模型响应的相关性，使用现有的比较或酒精度量表方法。然而，我们相信存在一种更精确、更可靠的维度对话评估策略。我们的方法试图通过明确标注每个模型响应是否表达了某些行为（例如，用无关信息回应或自相矛盾）来减少人类评估的主观性。我们称这种方法为对话行为标注，简称ABCEV。我们开发了这种方法，以全面覆盖最近文献中建议影响对话质量的对话模型行为。ABCEV能够测量对话模型犯下各种主题错误的频率。例如，ABCEV测量了对话模型忽略其伙伴或说无关话、自相矛盾或与伙伴矛盾、产生错误事实或违反常识知识的回合次数，以及模型成功或未能表现出同理心的情况。为了确定哪种评估方法最有效，我们选择了四种最先进的对话模型，并使用ABCEV对每种模型进行了100个人类机器人对话的评估，以进行比较。我们还使用三种现有方法对这些对话进行了评估：回合级别的酒精度量、对话级别的酒精度量和对话级别的配对比较。对于每种现有方法，我们收集了对对话最常用的八个方面的评估，因为这是评估对话模型的标准实践。从我们对这些评估结果的分析中，我们发现ABCEV行为标签总体上比现有方法收集的标签更可靠，这是通过在一百个双重标注对话上进行内部评估员一致性测量的。此外，ABCEV标签比现有方法产生的指标更能预测整体对话质量，如图所示的简单线性回归分析所示。例如，你可以看到测量自相矛盾和伙伴矛盾回合比例分别解释了对话质量的百分比和百分之十，而平均酒精度一致性得分只解释了百分之四或更少。最后，我们使用逐步线性回归检查了每个评估指标是否捕捉了对话质量的独特方面。你可以看到，所有ABCEV指标的组合解释了超过百分之二十五的对话质量，而当你逐个移除这些指标时，大多数指标都会导致失去大量关于质量的信息。另一方面，所有回合级别的酒精度指标的组合解释了远低于质量的百分比，而且这些指标中很少有独特的含义。这些可靠、信息丰富且独特的ABCEV指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。你可以在我们的实验结果中看到，仍然存在一些挑战，并且这些挑战已经被精确量化。例如，我们测试的机器人在大约百分之二十的响应中违反了常识，在大约百分之十五的响应中产生了无关信息，在大约百分之十的时间里自相矛盾或与伙伴矛盾。随着该领域的快速进步，许多这些错误率可能会在新发布的模型中减少。然而，这更需要追求可靠且精确的评估指标来比较模型。我们希望ABCEV可以被该领域的其他人作为朝着这个方向迈出的有意义的一步，我们期待看到对话式人工智能在未来几个月和几年中的发展。感谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "- 大家好，我叫Vauddha，是Stony Brook大学计算机科学博士生。我将介绍我们在ACL 2023上接受的长文论文《认知失调检测的迁移学习》，该论文解决了罕见类别的问题。我们首先定义了认知失调，并解释了为什么它是语言中一个重要的研究问题。认知失调是指两种信念或行动不一致，例如一个人说“我知道吸烟会害死我”，然后又说“会议后我抽了几口烟”。这种信念和行动不一致，处于失调状态。进一步提到，“我认为没有它们我无法保住工作”，这为第二次吸烟行为辩护，说明它们之间存在共鸣关系。失调是一种非常常见的现象，我们在日常决策中都会遇到。然而，在其他类型的语篇关系中，失调在语言中表达的例子非常罕见，所以研究认知失调有什么意义呢？研究认知失调可以帮助我们理解人们之间的分歧、趋势和信念价值观的变化，以及人口态度的变化。高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。研究语言中表达的失调也有助于理解极端主义和弱势群体的两极分化。最后，认知失调对于理解个人的认知风格非常重要，有助于我们更好地理解决策过程。为了实现认知失调资源的目标，我们对失调关系进行了大规模标注。我们采用了“失调优先”的方法，如图所示，推文使用PDTV解析器处理，根据我们的论文中描述的指南对语篇单位对进行标注。如图所示，只有3.5%的标注对中发现了失调。在收集了大约一千个语篇单位对的例子后，我们对一个仅在43个失调例子上训练的初始分类器进行了训练。不出所料，鉴于失调的低发生率和缺乏任何此类先验数据集，分类器的表现并没有比随机猜测好多少。我们面临的是绝对稀有性的问题。为了缓解这个问题，我们尝试了迁移学习和主动学习的组合，以便在较少的标注轮次中收集更多的失调样本，从而降低整体标注成本，同时提高失调检测。由于初始模型根本无法捕捉到失调类别，我们从密切相关任务中转移权重开始了主动学习过程。我们从两个不同的任务中转移：主题无关的失调舞蹈分类（一个判断两个人在不同话题上的辩论陈述是否一致或不一致的任务，称为辩论），以及纯度tb的扩展和比较类别的二元分类。由于这两个任务与共鸣和失调的概念密切相关，我们称它们为ceE。我们发现，通过将零短性能转移到标注数据集上，性能已经远超随机猜测，最佳性能为AUC 0.62。通过在两个任务上迭代微调，我们发现，先对ceE任务进行微调，然后对辩论进行进一步微调，可以获得更好的零短性能。因此，这是我们用于启动主动学习的模型。接下来，我们确定了使用来自每个主动学习和标注轮的新数据更新模型的最佳方法。累积更新方法累积了迄今为止从主动标注中收集的所有数据，而迭代更新方法则通过训练最新的数据集来更新模型。我们发现，累积更新在各个方面都表现出与迭代更新相同或更好的性能。为了提高失调例子数量，我们使用了一种罕见类别策略（PRC），选择那些在任何主动学习轮次中被当前模型认为高度可能失调的例子。我们将此与社区中常用的其他最先进的主动学习策略进行了比较。我们发现，提出的PRC策略比其他最先进的策略表现更好，尽管差异很小。请注意，随机在进一步的主动学习轮次中的性能显著降低。使用两种最佳策略，我们提高了距离分类的AUC至0.75，这是我们迄今为止在该任务上获得的最佳性能。我们还检查了每种策略对标注质量和标注员成本的可行性。我们发现，PRC在失调类别中具有最高比例，并且对罕见类别效果最好。然而，标注员也发现这些例子很难。总结起来，我们发现PRC是一种简单的罕见类别获取策略，并且通过适当设计的迁移学习任务，可以显著帮助主动学习的冷启动。我们还发现，迭代更新对于从不同领域进行迁移学习是有用的，而领域内的主动标注则受益于累积更新。这些链接指向我们的代码、数据集和论文。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我是 Akshata，今天我和我的合著者 Martin 将展示我们的工作 Kit Must：评估从多个来源整合知识。这项工作是麦吉尔大学、Mila 和微软研究院的合作项目。国家语言理解模型依赖于各种知识来源，例如通过预训练获得的参数中包含的知识，以及推理时输入中给定的知识。最近在问答等任务中的工作表明，模型可以使用预训练的时间知识来解决任务。但是，自然语言理解通常需要在推理时也提供知识，例如在句子“约翰在电视上看到了新当选的总统”中，预训练的参数可能包含关于总统做什么和电视是什么的信息，但它们无法可靠地知道这个特定实体约翰是谁，或者新总统是谁，因为自预训练以来总统可能已经换了。因此，成功处理知识密集型 NLU 任务的模型需要能够整合和使用预训练时间和推理时间知识。在这项工作中，我们提出了一套知识整合诊断测试。我们介绍了一个核心参考解析任务，旨在探究从不同来源中获取知识的能力，我们用人类研究参与者评估数据集，并在此建立核心解析模型。这里有一个来自我们数据集的例子：服务是法官 Kia 是面包师，在工作了一天后，在法律代码中审理案件，他很高兴放松。这里的任务是确定代词 he 指的是哪个正确的实体，在这种情况下是。解析给定的代词需要两种类型的信息：首先，实体特定知识，如服务是法官；其次，背景知识，如法官通常在法庭上审理案件。背景知识是在大型语言模型的预训练过程中学习的，而实体特定知识通常在推理时观察到。我们改变这两种信息的可用性，使其可能只在一个来源中找到，或者在多个来源中找到。我们定义了三个 kitdmos 设置。首先，我们有典型的预训练设置，其中假设在预训练时间可以获得背景知识。其次，是背景两者设置，其中背景知识在预训练时间和推理时间都可用。最后，是背景推理设置，其中两种知识类型仅在推理时间可用。这个最后设置特别有趣，因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分的情况，例如，自预训练以来，新的职业已经发展出来。这里是一个例子，说明了我们如何在背景预训练设置中控制两个来源的事实可用性。我们假设，背景知识政治家寻求政府选举席位包含在预训练参数中，在干扰时间上下文中，我们提供特定于奇切斯特的知识，他在背景两者设置中，我们还提供了不仅是特定于奇切斯特的知识，还有关于政治家的背景知识，在干扰时间上下文中，在背景推理设置中，我们提供了特征职业仅仅是旅游，而不是政治家，因为仅仅是旅游不太可能包含在预训练参数中。我们使用人类研究参与者评估数据集，并在这个图中，我们展示了在最困难的背景预训练设置中表现最好的模型的结果，没有在 kitdmos 上进行特定任务的训练。然而，当在 kitdmos 上训练时，两个模型都不表现良好，但 c2f 和 built for coref 的表现明显优于随机选择，这表明，当在通用参考解析数据集上训练时，模型学会了利用表面线索，这些线索在对 kitdmus 进行测试时没有用，因为这些线索已经被删除。额外的虚构知识实验表明，即使是表现最好的模型也无法可靠地整合背景知识，只能在自由时间内整合。总结我们论文的主要结论，许多共同参考演化模型似乎无法在没有任务特定训练的情况下推理不同来源的知识，然而，通过任务特定训练，一些模型成功地整合了来自多个来源的知识。尽管如此，即使是表现最好的模型似乎也难以可靠地整合仅在推理时间呈现的背景知识。如果您对更多细节感兴趣，请参阅我们的论文，并在 GitHub 上查看代码中的数据集。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自特伦托大学和布鲁诺·克塞尔研究所的莎拉·帕伊，我将简要介绍一篇关于同时语音翻译的论文，这是我和马泰奥·内格里、马可·杜里合作完成的。同时语音翻译是什么？同时语音翻译或SimST是指将口语实时翻译成另一种语言文本的过程，实现跨语言交流。那么，当前SimST模型存在哪些问题呢？特定的架构通常需要训练，引入额外的模块进行优化，训练过程长且复杂，例如，训练涉及不同的优化目标，训练和维护多个模型以达到不同的延迟机制，例如，训练一个平均延迟为一秒的模型和另一个平均延迟为两秒的模型等等。那么，我们的解决方案是什么呢？首先，使用现有的离线SD模型，无需重新训练或采用特定的SimuSD架构，只需为每个延迟机制使用一个模型，并通过特定的参数处理延迟，利用模型通过音频输入和文本输出之间的张力机制（即交叉注意力机制）获得的知识。右边可以看到我们的解决方案是提出点注意力或编码器注意力，这是一个策略，我们根据注意力指向某个词的位置决定是否发出部分翻译，如果张力不集中，即这个和值低于某个阈值alpha，指向最后的lambda语音帧，意味着接收到的信息足够稳定。例如，如果我们接收到的语音片段包含“我要谈论”，我们的模型预测德语翻译，我们会查看交叉注意力权重，我们会看到前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即lambda语音帧，这意味着前两个词将被发出，而由于交叉注意力的和值高于某个阈值alpha，我们不会发出最后一个词，而是等待另一个语音片段。如果我们继续，接收另一个语音片段，我们的模型预测其他三个词，我们会查看交叉注意力权重，我们会看到没有词指向最后的lambda语音帧，这意味着这三个词将被发出。如果我们查看点的主要结果，我们在图表上绘制了同时语音翻译的结果，其中一边用蓝色测量翻译质量和平均延迟，即延迟度量，我们还考虑了计算感知平均延迟，这考虑了模型预测输出的计算时间。所以，我们希望我们的曲线在这个图上尽可能高，但也要向左移动，我们与plepara策略进行比较，这些策略也适用于离线模型，即Whit键策略和局部一致性，我们还与专门为同时语音翻译设计的最先进架构进行比较。这些是同时语音翻译策略在德语上的所有结果，我们看到，AD优于所有应用于离线模型的策略，因为曲线向左移动。我们还看到，如果考虑实际的经过时间或计算时间，AD是最快的策略。如果您想了解更多结果，请阅读我们的论文，我们还发布了开源代码和模型以及同时输出，以促进我们工作的可重复性。感谢您的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我叫舒哈。今天我要介绍我们的论文《Do Connel 2003命名实体标注方法在2023年仍然有效》。让我们开始吧。我们的论文研究了泛化问题，使用命名实体识别任务（NER任务）我们观察到，近20年来，模型一直在使用Con 2003来开发NER，这自然引发了几个问题：首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？同时，如果我们观察到泛化效果不佳，这些模型的性能下降是由什么原因造成的？为了研究这些问题，我们开发了Con plus+数据集，这是我们从2020年路透社新闻中收集的数据，并使用相同的Con 2003标注指南进行了标注。然后，我们在Con 2003上对20多个模型进行了微调，我们在Con 3测试集和Con plus fast测试集上对它们进行了评估，最后，我们计算了F1值的百分比变化，以评估每个模型的泛化能力。那么，实现良好泛化需要什么？通过我们的实验，我们发现需要三个主要成分。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能更好地泛化到新数据。第二个成分是模型大小。我们发现，通常较大的模型能更好地泛化，最后但并非最不重要的是，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们还发现，更多的微调示例实际上也能更好地泛化。关于我们的下一个问题，是什么原因导致了一些模型的性能下降？我们有两个假设。第一个是自适应过拟合，这是由于反复使用相同的测试集而引起的过拟合，通常表现为在新测试集上的收益递减。第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距增大而导致的性能下降。对于自适应过拟合，我们从右侧的图表中看到，红色的最佳拟合线有一个大于1的梯度。这意味着我们在Con 2003上做出的每一单位的改进，都会在Con plus+上转化为超过一个单位的改进，这意味着没有收益递减。这表明在这种情况下没有观察到自适应过拟合。那么时间漂移呢？对于时间漂移，我们进行了一个实验，使用更近的数据对一些模型进行了再训练或继续预训练，我们发现随着时间差距的增大，性能会下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例，这些都是相辅相成的，我们不能只拥有其中一个成分，而同时忽略其他成分。同时，我们还发现，这里的性能下降是由时间漂移引起的，这有点令人惊讶，因为它不是由自适应过拟合引起的，尽管Connel 2003已经使用了20多年。回到我们论文标题提出的问题，Connal 2003标注器在2023年是否仍然有效？我们发现答案实际上是一个响亮的肯定。我们希望我们的论文能促使更多人研究如何提高模型的泛化能力。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "大家好！欢迎来到我们的演示，我们将展示 De plain，一个用于德语文本识别的新语料库，适用于文档级别和句子级别。我叫 Regina Stoden，我将引导大家完成演示的第一部分。我们首先定义文本简化。文本简化是适应文本以提高特定目标群体对文本的理解的过程，例如阅读有困难的人或非母语人士。为了训练文本简化模型，我们需要平行文本对，例如文档或句子。在这里的例子中，您可以看到一个复杂的德语句子及其平淡语言翻译的平行对齐句子对。为了简化句子，可以采用不同的技术，如您在例子中看到的，例如词汇替换、从句扩展、交叉删除重排或插入单词。我们现在提出我们的新语料库 D plane，因为近年来现有的语料库存在一些问题。例如，这些语料库太小，无法训练分类模型。近年来提出的其他三个模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了我们的新语料库 Dplane，它分为两个子语料库，Deplane APA 和 Deplane web。Deplane APA 基于使用文本。在 Deplane APA 中，我们手动对齐了 483 个文档。这大约产生了三十万个句子对。对于 Deplane web，这个语料库包括不同的领域，我们还手动对齐了这七百五十个文档，另一方面也使用了自动对齐方法。总共有 30 450 个句子对。我们对句子对进行了更多的分析，例如简化的类型。在这里，您可以看到圣经文本比新闻文本或语言学习文本在所有级别上都更强地简化了，例如词汇简化、结构简化以及整体简化水平。此外，您可以看到我们的深层语料库具有高度多样化的不同简化变换，例如在 Deplane API 语料库中，我们有更多的重排和添加单词，而在 web 语料库中，我们有更多的改写。现在让我们看看我们可以用这个语料库做什么。大家好，我是 Omar，我现在将谈谈我们数据集 d plane 的使用案例。第一个使用案例是评估自动对齐方法。近年来，出现了很多对齐方法，但在机器翻译的背景下，我们有两个用不同语言编写的平行文档，我们想要从后置文档中提取句子对齐。但在我们的使用案例中，我们试图提取两个平行文档之间句子之间的对齐，这两个文档具有相同语言和相同内容，但它们在复杂性级别上有所不同。现在我们有了手动对齐句子的数据集 d plane，我们可以使用这些句子作为金标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了一些改编，并在论文中发表了所有这些改编和运行实验的代码。最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是质量对齐方法。您也可以在论文中找到在您自己的文档上运行此方法的代码。我们在论文中展示的第二个使用案例是通过微调语言模型来自动简化文本的案例。我们微调了两个不同的模型：我们微调了长部分模型以生成文档级别的简化，我们还微调了正常基线长部分以生成句子级别的简化。您也可以找到所有检查点，并在论文中查看我们实验的详细分数和评估指标。我们得出结论，这种基本的微调可以产生或获得比基准分数更好的分数，我们提出了这些结果作为自动文本简化问题的基准。非常感谢您的关注，我们希望在会议期间见到大家。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自FNAi大学的袁X。我今天在这里介绍我们的工作，即在日常生活中，人类必须经常根据保证脚本的形式逐步执行指令来计划自己的行动。之前的研究利用语言模型来计划典型的抽象目标，例如制作蛋糕，并表明大型语言模型可以有效地将目标分解为步骤。然而，之前的研究主要集中在计划典型的抽象目标上，而对于具有特定约束的目标，例如制作巧克力蛋糕，仍然未得到充分的关注。在这篇论文中，我们定义了受约束的语言计划问题，该问题对计划目标施加了不同的约束。一个抽象目标可以被不同的现实生活中的具体目标继承，这些具体目标具有多方面的约束。一个好的计划者应该编写符合约束的合理脚本。在这篇论文中，我们首先评估和改进生活语言模型的受约束语言计划能力。由于没有特定目标的数据可用，为了进行我们的研究，我们必须首先获取这些目标。如表所示，我们扩展了抽象目标，为人类在循环数据获取中使用instruct GPT，我们采样了100个具体目标，并评估了从库模型生成的脚本。该表报告了结果的总体准确性。我们发现所有学习模型在计划具体目标方面都取得了不令人满意的结果。然后，我们进行详细分析，调查学习模型的原因。图中的结果表明，生成的脚本在语义完整性方面是可接受的，但在忠实于约束方面不能保证。我们深入研究了WiH中定义的更细致的约束类别。图中的热图显示，instruct GPT的计划性能在不同类别的女孩中差异很大，之前的研究表明，学习模型的输出质量在高方差下下降，导致性能不佳。因此，我们采用了过生成Z滤波器来提高生成质量。我们首先展示了instruct GPT的受约束类型，并根据种子抽象目标获得了具体目标。然后，instruct GPT对特定目标的关键脚本进行了过生成。接下来，我们开发了一个滤波模型来选择忠实的脚本。我们将脚本和目标转换为instruct GPT嵌入，并计算余弦相似度和相似度分数来衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。如果目标在目标大小中得分最高，我们才保留该脚本。通过我们的方法，instruct GPT可以生成更高质量的脚本。我们的方法大大提高了计划在语义、完整性和忠实于约束方面的可行性。由于大型语言模型的部署成本高，因此必须使较小和专业模型具有语言计划能力。创建数据集是实现这一目标的必要步骤。然而，之前的研究没有实现对特定目标的计划，手动数据标注成本高昂。为此，我们遵循符号知识蒸馏的思想，从轻语言模型中蒸馏受约束的语言计划数据集。我们应用我们的方法来构建一个受约束的语言计划数据集，命名为CodeScript。总共，我们生成了55,000个具有脚本的具体目标。为了确保验证和测试网站的质量，我们请众包工人找到并修改错误样本的收入。该图显示了CodeScript的约束分布。我们发现CodeScript在生成的具体目标中显示了高度的多元化。通过CodeScript，我们可以处理较小但专业化的受约束语言计划模型。我们发现，在评分率上进行微调的t5可以生成比大多数大型语言模型更优质的脚本，这表明在适当的数据集上进行训练的小型模型可以支持大型模型。总结起来，我们建立了受约束的语言计划问题，评估了大型语言模型的受约束语言计划能力，并为生活语言模型开发了一个过生成滤波器方法。我们使用大型语言模型生成了高质量的CodeScript数据集，用于受约束的语言计划。我们希望CodeScript数据集可以成为推进语言计划研究的宝贵资源。感谢大家的聆听。有关CodeScript的更多详细信息，请参见我们的论文。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Jannislavak，我将向大家介绍我们在 Dr. Bert 上的工作，这是一个针对法语生物医学和临床领域的强大预训练模型。在本次演讲中，我们首先讨论医疗保健中的语言建模。然后我们将介绍我们文章的主要贡献。我们介绍了第一个法语生物医学模型 Dr. Bert，它基于 Roberta，并在一个从网络上抓取的医疗数据集中 Nachtchos 上进行训练。我们还介绍了多个冷冻设置和数据源的模型比较。然后我们展示了我们在法语下 11 个生物医学和临床下游任务上的结果，最后我们总结了实验，并向大家介绍了如何访问这些模型，因为自 2018 年发布以来，Bert 已成为解决自然语言处理任务的最有效方法之一，相比历史上的静态和上下文方法（如 fast text 或 war to ve fast text）提供了巨大的性能提升。从那时起，这个模型已被适应到许多其他语言，如法语中的 Camembert 和其他领域如生物医学中的 Permit Birth 和 Bio Birth，以及临床中的 Clinical Birth。但大多数情况下，其他语言的专业模型稀缺，并且通常基于持续预训练，因为缺乏领域内的数据。然而，直到现在，法语才没有生物医学领域的开源模型。我们问自己，对于广泛的用途，最合适的 数据来源是什么？这些原始数据是否可以很好地替代临床数据？为了回答这个问题，我们将 Dr. Bert 与我们基于来自我们所在医院非大学医院的匿名数据训练的 Schubert 模型进行了比较。然后我们问自己，我们需要多少数据来训练一个基于法语数据的专业模型？是四千兆字节、八千兆字节还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型：Dr. Bert 的第一版，使用了七千兆字节的 Nachtchos；第二版使用了四千兆字节的 Natureos；第一版的 Schubert，这是一个临床模型，使用了四千兆字节从临床笔记中提取的句子；以及最终版本的 Schubert，混合使用了四千兆字节的 Natureos 和四千兆字节的临床节点。除了这个比较，我们还介绍了三个在控制预训练上训练的模型，以分析预训练策略的影响：一个基于 Camembert 的权重，并在四千兆字节的 Natureos 上进行训练；另一个也是基于 Camembert，但这次是在四千兆字节的临床数据上进行训练；最后，一个基于英语生物医学模型 Bermedbert，并在四千兆字节的 Snatches 上进行训练。总共有七个模型供我们评估。我们收集了公共和私人的下游任务，如命名实体识别、分类、词性标注和问答。我们将这些模型与六个基线模型进行了比较，这些基线模型包括 Camembert、Oscar One Hundred Thirty Eight Gigabyte、Camembert Oscar Four Gigabyte、Camembert CC Net Four Gigabyte、Plummet Bird、Biobert 和 Clinical Bird。模型的演变表明，模型在与训练数据性质相同的任务上表现最好。然而，我们可以从我们能够获得的数据中观察到，来自异构来源的数据似乎更通用。我们还观察到，使用更多的数据可以带来更好的性能。总的来说，从头开始的免费训练似乎在大多数任务上都能获得更高的性能。然而，我们对 Permit Bird 的权重和分词器进行的消费者预训练实验显示的结果与 Dr. Bert 4GB 从头开始的结果相当，但基于 Camembert 权重和分词器的模型则存在稳定性问题。最后，作为结论，我们的适当系统在 11 个下游任务中的 9 个任务上表现更好，并且在全球范围内超越了通用模型 Camembert 的结果。我们还观察到，专业数据更好，更专业的数据更好，但它并不容易扩展。从 Nachtchos 获得的所有预训练模型都可免费获得，并且在我们的界面上，所有的训练脚本都在我们的 GitHub 仓库中。感谢大家的聆听，我们期待在多伦多的海报环节与大家互动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "- 你好，我是华盛顿大学的博士生Xhang Bing。今天我将介绍我们从预训练数据到语言模型再到下游任务的工作，追踪导致不公平自然语言处理模型的政治偏见的轨迹。因此，语言模型是在大规模网络爬虫数据上训练的。政治新闻媒体在他们的预训练数据中得到了很好的覆盖。根据对c4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖，这为语言模型的应用带来了既是福又是祸，一方面，它们能够从多样化的视角学习，庆祝民主和思想的多元性，另一方面，这些不同的政治观点本身具有社会偏见，可能导致下游任务应用中的公平问题。为此，我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体来说，我们首先提出以下问题：如何评估语言模型的政治倾向，以及数据收集可能对这种政治偏见产生什么影响？其次，具有不同政治倾向的语言模型在实际应用中表现如何，以及这是否可能导致自然语言处理应用中的公平问题？因此，我们首先提议使用政治问卷（如政治指南针测试）对不同提示格式的语言模型进行提示，这确保了我们的自动评估基于政治科学文献。一些初步结果表明，首先，语言模型确实具有不同的政治倾向，它们占据了政治指南针上的四个象限，我们还可以看到，GPT-4是所有语言模型中最自由派的一个，GPT系列通常比Bird系列及其变体更具社会自由主义倾向。其次，我们旨在研究语言模型的政治偏见实际上是从训练数据中获得的程度，因此，我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点来进行受控实验，这些语料库分为新闻和社交媒体，并通过进一步预训练语言模型来划分其政治倾向。通过在这些党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化，例如，进一步微调并在左翼Reddit语料库上进一步训练的roberta，我们可以看到其政治偏见有显著的自由派转变。我们还试图研究语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象。因此，我们将预训练语料库分为美国第45任总统当选前后的两个时间段，我们分别在两个不同的时间段语料库上预训练语言模型，我们可以看到，语言模型在2017年后通常具有更远离中心的政治倾向，这表明语言模型也可以捕捉到我们社会中的极化现象。最后，我们评估了具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现，这些自然语言处理应用通常涉及语言模型，可能具有非常重大的影响。因此，我们看到，如果我们按类别评估性能，也就是说，如果我们将性能分为不同的人口统计或政治媒体，我们可以看到一个模式，例如，在仇恨言论检测方面，左翼语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对我们社会中更具权力的群体的仇恨言论方面表现较差，反之亦然。右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。虚假新闻检测也出现了类似的趋势，我们看到，左翼语言模型在检测来自其相反政治倾向的虚假信息方面表现更好，反之亦然。我们进一步展示了许多定性例子，以证明语言模型具有不同的政治含义，确实会根据其社会类别对仇恨言论和虚假信息例子做出不同的预测。附录中有更多例子，进一步强调了这一点，表明语言模型的政治偏见存在一个非常紧迫的公平问题。例如，如果右翼语言模型在仇恨言论或虚假信息等方面进行微调，并部署到一个流行的社交媒体平台，这意味着具有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会不受控制地蔓延。因此，这为我们敲响了警钟，要求我们承认并解决语言模型政治倾向导致的公平问题。因此，进行了一些讨论。我们还希望强调，我们揭示了语言模型政治偏见的独特困境，就像西西拉和卡吕普索之间的困境一样，因此，如果我们在语言模型训练数据中不净化政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们尝试以某种方式净化，我们也会冒着审查或排斥的风险，而且很难确定什么才是真正中立的，应该保留在语言数据中。因此，这有点像电车难题。好吧，我想这就是我今天要讲的全部，谢谢大家的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我是 Koov Sinna，很高兴欢迎大家来讨论我们的 ACL 23 论文。语言模型的可接受性判断并不总是对上下文有稳健的处理能力。这是一项与 John Waqui、Aaron Mueller、Kanishka Mishra、Karen Fs、Roger Levy 和 Atina Williams 合作完成的工作。因此，在这项工作中，我们重新审视了最小对概念。最小对概念基本上是通过可接受性判断来评估语言模型，这些判断还可能包括语法性，如 blimp 语法健身房或可接受性方面如人群对的概念。在这个最小对概念中，评估语言模型的典型方法是展示一个可接受的句子或语法句子，然后展示一个不可接受的句子或非语法句子，希望模型基本上会给可接受的句子赋予更高的概率。当前的 MPP 管道基本上不允许我们在这些日子里评估模型对更长句子的接受程度。如今，大型语言模型正在提出越来越长的上下文窗口，因此，我们必须在整个上下文窗口中评估模型的可接受性，这就是我们在这里试图做的事情。我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视 MPP 管道。因此，我们的做法是模拟这些更长的序列，重新审视数据集本身，然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，在这里，我们从 blim 数据集的从属岛案例中选择了典型的语法句子对，然后我们重新创建了更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配。我们从从属岛中提取语法句子，然后将其作为前缀添加到可接受查询和不可接受查询中，以便我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。我们也可以通过从不同的子集或不同的数据集选择句子来做同样的事情。这就是我们所谓的“不匹配场景”。在这里，句子仍然来自相关的数据集，但不是您正在评估的数据集。我们也可以对不可接受的情况进行同样的操作。最后，我们可以从完全不相关的领域（如维基百科）选择句子。这将告诉我们，模型的可接受性判断是否实际上受到任何上下文的任何影响，如上下文是否来自数据集的不同子集，或者是否与我们正在查看的句子完全无关。模型的表现如何？首先，我们查看了与当前查询对完全无关的维基百科句子，我们发现 MPP 判断对于任意上下文长度大多是稳健的。我们将上下文长度增加到一千二十四个，以最大化 Ot 和 GPT 二模型，我们在这里看到（橙色虚线）MPP 判断相对稳定。那么，当我们从同一数据集选择句子时会发生什么？在这里，我们从同一 blim 语法健身房数据集的可接受和不可接受领域选择或创建句子，我们看到，当您添加可接受前缀或不可接受前缀时，MPP 判断会显著增加或减少。但是，当我们匹配结构时，即当我们从 blame person 语法健身房中选择句子时，我们看到模型的 pp 判断会根据所选前缀是可接受还是不可接受而大幅增加或大幅减少。现在，这个效果非常大，随着上下文长度的增加而增加，这可能会影响到具有大上下文窗口的较新的语言模型。为什么匹配前缀会如此大地影响语言模型的判断？我们进行了一系列分析，试图像扰动输入句子一样，同时保持相关的结构，但在输入中添加噪声。经过几次这样的扰动，我们发现这些噪声实际上并没有使模型改变其 MPP 判断趋势。基本上，我们发现模型以相似的方式对扰动句子敏感，即当我们在可接受领域中扰动句子时，我们看到所有扰动中的相似增加，当我们在可接受批准领域中扰动句子时，我们以类似的方式看到 MPP 判断的减少。因此，我们工作的关键结论是，语言模型对句子中共享的潜在句法和语义特征敏感。我们目前用短句和单句输入进行的 MPP 评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取我们实验的更多详细信息。感谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自德国斯塔兰德大学的博士生大伟。在这个视频中，我想介绍我们最近的工作，即《比你想象的更弱》，这是一次对每周视学习的批判性审视。这是与X、Myos Mosbach、Ge Steffen和Dirich Klako的合作成果。我想从简要介绍弱监督和弱监督学习开始。在弱监督中，我们不手动标注数据。相反，我们使用弱标签来源，如简单的启发式规则、知识库或低质量的众包，如图右侧所示。与人工标注相比，弱标签要便宜得多，但它们也存在噪声，这意味着如果我们直接在每周标签数据上训练神经网络，那么神经网络往往会记住标签噪声，并且在每周监督学习中无法泛化。为了在这样的标签噪声下鲁棒地训练神经网络，提出了监督学习训练算法，以便训练的模型在实际应用中仍然能很好地泛化。在最近的wSL（每周视学习）工作中，一个常见的观点是，人们说只在每周标签数据上训练模型，并在干净的测试集上取得高性能。从技术上讲，这个说法并没有错，但有一个问题，那就是人们假设有一个额外的干净验证集，用于模型选择。我们对这个问题的设置进行了停止，但这意味着在每周支持学习中需要额外的手动标注。但就像房间里的大象一样，这种必要性往往被忽视。上述方法被要求提出三个研究问题：首先，wSL是否需要干净的验证数据，或者我们可以使用弱验证集代替？其次，如果需要干净数据，或者如果干净数据是wSL工作的强制性要求，那么我们需要多少干净样本？最后，我们是否应该只使用干净样本进行验证，或者还有更好的利用方式。我们在工作中解决了这些研究问题，我们的发现如下：首先，我们发现有趣的是，最近的wSL方法确实需要干净的验证样本才能正常工作，否则性能会大幅下降，如图所示。如果没有干净的验证样本，那么趋势模型就无法超越原始的弱标签进行泛化，这意味着训练是没有意义的。这表明wSL方法实际上需要干净的标注数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于wSL方法取得更好的性能，如图左侧所示。通常，我们只需要每个类别20个样本就能达到高性能。但这还不是故事的全部，因为如果我们决定直接使用干净样本进行训练，那么直接微调甚至会取得更好的性能。红色图形显示了直接应用于干净数据的微调方法与仅用于验证的wSL方法之间的性能差异。我们可以看到，如果我们每个类别有十个样本，直接微调开始超越wSL方法。最后，之前wSL方法中声称的性能提升可以通过允许在干净验证样本上继续微调来轻松实现。从图形中我们可以看到，称为FTW的验证模型最初的性能低于更复杂的wSL方法，如余弦。然而，如果我们允许在干净样本下继续微调，那么FTW的性能与其他方法一样好。所以，在实践中，没有理由选择更复杂的wSL方法，这些方法需要更多的计算时间和磁盘空间。总结一下，我们表明，最近的wSL方法需要干净的手动标注样本才能正常工作，它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下：首先，报告模型选择标准，例如报告模型选择是否完成良好，干净的验证样本。其次，wSL方法应该与少数短着陆基线进行比较，假设基于具体样本的工作。最后，连续微调是一个简单但强大的基线，应该在未来的wSL工作中考虑。最后，我们开源了我们的代码。你可以通过这个幻灯片上的二维码找到它。请随时查看。谢谢，祝大家会议愉快。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫阿尔·维拉德，我将简要介绍这篇关于翻译评估策略和性能的论文。这是我和谷歌翻译同事的合作成果。Pm 是去年 2022 年推出的一个拥有 5400 亿个参数的语言模型。在发布时，它在 7800 亿个标记的文本上进行了训练，并在数百个 NLP 任务中取得了最先进的水平。在这项工作中，我们对机器翻译的大型语言模型提示进行了系统的研究，我们使用 IMT 社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠，我们还比较了两个最先进的系统，即 WMT 评估中表现最好的系统。我们使用了最先进的 neuralmt 指标，并展示了基于专家的人工评估结果。最后，我们还提供了一些提示选择策略的建议。提示对翻译大型语言模型的性能有很大的影响，我们可以在一个简单的实验中看到这一点，我们使用一个简短的提示，并为不同的句子提供了两个不同的提示。在 1000 个句子中，有 516 个句子的差异超过一个模糊点。在极端情况下，这个差异可以达到 40 个模糊点。因此，选择一个好的提示策略在我们的实验中，我们选择了五轮提示策略，我们只需标记我们提供给系统的句子，并标明其所在的语言。在这个例子中，我们从德语翻译成英语，德语句子（源句子）用德语冒号标记，英语翻译用英语冒号标记。我们发现，在几个简短提示的情况下，实际打印形式对性能没有太大影响。对于零轮和一轮提示，这至关重要。但在我们的情况下，我们使用的是事实上的简短提示，实际提示形式与性能几乎没有区别，关键在于例子。我们实验结果的总结是，例子质量比与源句子的相似性更重要。因此，选择高质量翻译中的例子非常重要。特别是，我们比较了从 WMT 评估的训练数据或开发数据中选择提示。开发数据比训练数据更丰富、质量更高，结果显示使用开发数据时性能更好。尽管如此，专业最先进的系统在翻译方面仍比 Palm 有显著优势，但 Palm 现在已经接近商业系统的水准。在我们的情况下，我们选择使用谷歌翻译进行评估。我们从使用 NpN 框架进行的模拟中获得的洞察是，Palm 的流畅度与最先进的系统相当，但主要区别在于准确性。特别是，最常见的错误是遗漏错误。因此，Palm 选择有时通过省略源句子的部分来生成听起来更好的翻译。然而，Pan 的风格外向类别低于最先进的系统，这是一个额外的信号，表明 Par 提供了真正流畅的输出，但仍存在一些准确性问题。这就是这个非常简短的概述的全部内容。有关更多详细信息，请来听我完整的论文介绍。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫金伟宇，来自中国科学技术大学。很高兴能给大家展示一下我们的论文的简短广告视频。你们是在抄袭我的模型吗？保护大型语言模型的嵌入和服务的版权？我们支持水印。让我们先介绍一下嵌入和服务的基础知识。目前，像Gbt、La、PLm这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入和服务是建立在大型语言模型基础上的服务之一，用于辅助各种自然语言处理任务。例如，OpenI提供基于Gbt的嵌入API，但最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务，因此有必要保护嵌入作为服务的版权，保护嵌入服务的版权，其中一种解决方案是在提供商服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下属性：首先，方法应适用于嵌入作为服务；其次，水印不应降低所提供嵌入的效用；再次，水印应足以让攻击者无法轻易移除；最后，水印需要在模型提取过程中转移到攻击者的服务。现有工作可以大致分为四类，但这些方法要么不适用于嵌入作为服务，要么缺乏可转移性。因此，在这篇论文中，我们提出了嵌入标记，这是一种基于后门的嵌入水印方法，适用于嵌入作为服务。接下来，让我介绍一下我们的嵌入标记的细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集，触发集是一组频率适中的词语。我们假设提供商可以收集一个通用的文本语料库，并用它来统计词语频率。在水印注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送一句话时，提供商计算这句话中的触发词数量。所提供的嵌入是目标嵌入和原始嵌入的加权求和。目标嵌入的权重与句子中的触发词数量成正比。当句子中的触发词数量大于m时，所提供的嵌入正好等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词语都属于触发集的句子，而良性数据集中的句子则不包含触发集中的词语。然后，提供商使用数据集从 stiller 服务请求嵌入，计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，定义为Delta余弦和Delta L2，同时我们还应用KS检验，并使用其p值作为第三个指标。我们在四个数据集上进行了实验：Aaging news mind SD2 和 A spam。我们假设提供商使用维基百科文本数据集来统计词语频率。在四个数据集上的结果表明，我们的嵌入标记可以具有出色的检测性能，同时保持对下游任务的良好效用。我们还通过可视化四个数据集B的PCA嵌入来验证所提供嵌入的可转换性。图例表示每句话中的触发词数量，如图所示。很难区分因子嵌入和正常嵌入。以上就是全部内容。谢谢大家。我们期待与大家讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫 Ian，我的同事 Jiian 和我将介绍我们关于通过指令微调改进多模态序列学习的研究。随着大型语言模型的进步，许多研究开始探索以参数和数据高效的方式使用预训练语言模型进行不同下游任务的新学习范式。最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令以序列方式在未见过的任务上表现出色。然而，大多数关于指令微调的先前工作都集中在改进仅限于语言任务的序列表现，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想要研究在多模态蛋白质模型上进行指令微调是否真的可以提高对未见过的多模态任务的泛化能力。此外，在我们的研究期间，我们发现 P 和多模态之间在指令数据集的可用性上存在相当大的差异，存在超过 1,600 个仅限于午餐的指令任务，但没有大规模的公开可用的多模态指令任务。因此，这促使我们在此构建一个多模态指令微调数据集。我们在这里介绍 multi-ins instruct，这是第一个多模态指令微调基准数据集，包含 62 个多样化的多模态任务，涵盖 10 个广泛类别。这些任务来自 21 个现有的开源数据集，每个任务都配备了五个专家撰写的指令。为了研究我们提出的数据集上的多模态指令微调，我们采用统一的多模态训练模型作为我们的基模型，使用统一的词汇表来表示语言图像标记和边界框的坐标。这里我们展示了我们 multi-instra 数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循 offa 的方法，并将所有任务统一为序列到序列格式，其中输入文本、图像、指令和边界框以相同的标记空间表示。好的，现在我要谈谈多模态指令微调。对于训练数据集，我们使用 N 组中的 53 个任务进行训练，并且为测试抽取每个任务中的 10,000 个实例。我们保留整个常识阅读组进行测试，并且从 Wiki 和杂项组中额外选择 5 个任务。我们使用每个任务的测试集中的所有实例，此外，我们从自然指令的测试集中的 20 个任务中随机抽取样本，作为 NP 的相同任务。因此，我们在训练期间使用大型模型的预训练作为基模型。我们混合了所有任务的所有实例，每个实例都随机组合了一个其五个指令模板中的一个。因此，对于每个任务的测试，我们进行总共五个实验，每个实验使用五个指令中的一个来评估模型。我们报告平均和最大性能以及所有五个实验的性能标准差。如果任务是多模态分类任务，我们报告准确率；如果是多模态生成任务，我们报告根 L；对于 LP 任务，我们也报告根 L。我们还引入了一个额外的评估指标，称为敏感性，它衡量模型在指令措辞略有变化的情况下，对同一任务始终产生相同输出的能力。这是我们的主要结果。我们可以看到，指令微调可以显著提高 OF 在相同多模态任务上的性能。此外，从自然指令数据集进行的迁移学习可以使指令微调受益。在这里，我们可以看到，随着任务数量的增加，模型的性能得到提高，同时敏感性降低。因此，我们还进行了一个实验，使用一个指令与五个指令进行比较。我们可以看到，使用更多指令可以提高模型的整体性能，并大大降低其敏感性。因此，这表明了不同的微调策略对模型敏感性的影响。我们可以看到，通过从自然指令数据集进行迁移学习，模型可以比原始 IFA 模型实现更好的敏感性。我们还可以看到，从自然指令数据集进行的迁移学习可以帮助 OFA 在氮指令数据集上实现更好的性能。因此，总的来说，我们提出了第一个大规模多模态指令微调数据集，我们显著提高了 OFA 的神经能力，并探索了不同的迁移学习技术，并展示了其好处。我们设计了一个名为敏感性的新指标。还有一件事，我们正在收集一个更大的多模态指令微调数据集，包含大约 150 个额外的变体语言任务，我们将发布这些数据集。这是我们数据和模型的二维码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的约翰。今天，我将介绍我们的工作，即Exampler：跨语言语义解析在多种自然语言和手动表示中的应用。语义解析是一种构建用户查询（如SQL和Lambda演算）语义表示的任务。跨语言语义解析则是将多种自然语言的查询翻译成多种语义表示的任务，如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda或Funql等。现有的跨语言语义解析模型分别针对有限的语种和应用进行了提出和评估，例如，某些自然语言（如中文）的覆盖范围存在漏洞，某些迷你表示（如Lambda演算）的覆盖范围存在漏洞，或者它们仅针对某些神经模型进行了评估，例如，仅有一个模型进行评估。为此，我们提出了Exampler，并为多种自然语言和语义表示的跨语言语义解析提供了一个统一的数据集。该数据集包含九个病毒领域的数据集、五个半解析任务、800万个表示和22种自然语言，涵盖15个语系。为了更好地评估我们的基准测试，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们用英语查询训练英语模型，在推理过程中，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试了单语模型。在这种设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试了单语融合设置，通过仅使用10%的训练数据训练单语模型，并测试了多语模型，我们为所有语言训练了一个多语模型。例如，我们将德语、英语和中文查询一起训练一个多语模型，在推理过程中，我们可以使用这个模型翻译德语查询或中文查询等。我们还考虑了跨语言零短和领域短转移，我们在一源语言上进行训练，然后转移到另一语言。在训练过程中，我们用英语查询或英语和德语短查询的组合训练多语模型，预测SQL输出。我们还发现了许多有趣的结果。关于单语模型的分析，我们对两组模型进行了评估，包括编码器PDR（多语言预训练编码器，具有基于指针的解码器，如XLr plus PDdR和bird plus PDdR），我们还评估了编码器解码器模型，即多语言预训练编码器解码器模型，如MBt和Mt5。我们发现编码器解码器在所有九个数据集上获得了最佳性能。我们对MT5和XLMR plus PDR的多语言设置进行了评估。我们发现，通过在多种语言的混合中进行训练，可以改进编码器解码器或编码器PDR的性能。我们发现这是因为大多数主要自然语言都能获得性能提升，但英语在七个数据集上的性能下降，仅在三个数据集上有所提升。我认为这被称为多语言曲线。我们还比较了跨语言性能差距。在此图中，蓝色线表示跨语言少短转移，橙色线表示跨语言零短转移，绿色线表示单语设置。我们发现，通过比较绿色和橙色线，我们发现零短设置的跨语言转移性能差距显著，通过比较蓝色和橙色线，我们发现，随着少短设置的引入，转移差距迅速缩小。我们还发现了一些其他有趣的结果，例如，编码器解码器优于进度工作，或者实现了可比拟的结果，训练我们的英语自然语言可以显著提升目标自然语言的少短性能，我们发现多语言语言模型（如编码器和蓝色）对于跨语言语义解析类仍然不足。总结起来，我们构建了Exampler，这是一个用于多种自然语言和语义表示的跨语言语义解析统一基准测试。我们对三种代表性的多语言语言模型进行了全面的基准测试研究，我们的结果显示了许多有趣的结果等。欢迎访问我们的论文和代码。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫亚当·什里科夫斯基，今天我要讲的主题是并列句的依存结构。众所周知，不同的理论和语料库方法假设了不同的依存结构。例如，在普遍依存理论中，丽莎、巴特和玛姬的并列结构是，第一个并列成分是整个并列结构的词头。在伊戈尔·米尔丘克的意义文本理论中也假设了类似的方法，即整个并列结构由第一个并列成分领导。因此，这两种方法都是不对称的，因为它们单列出了一个并列成分。现在，它们也是对称的方法，例如PRAG方法，以及PRAG依存树库中假设的连接词领导方法，其中并列结构由连接词领导，我们从连接词到所有并列成分都得到依存关系。最后，还有一种多头方法，例如在Cutson的词语语法中使用，可以说所有并列成分都是并列结构的词头，因此我们从领导词到所有并列成分分别得到依存关系。这些都是现在的按钮制作方法。本文的目的是为像这两者这样的对称并列结构提供一个新论点，反对像这两者这样的非对称并列结构。该论点基于依存长度最小化的原则，我将在这些例子基础上进行解释。在英语中，如你所知，我们的直接宾语倾向于靠近动词，而附属成分可能更远。因此，马奇昨天读了它没问题，因为直接宾语它靠近动词，而马奇昨天读了它则差多了，因为这里动词和直接宾语之间有一个附属成分昨天。然而，当直接宾语非常长时，这种效果可能会得到缓解，因为它可以移到附属成分之后的位置。这里对此进行了说明。因此，这两个句子都很好，马奇读了关于BC的这本书昨天没问题，用它代替它也行，但说马奇昨天读了关于蜜蜂的这本书也行。这里的推理是，这是可能的，因为即使这个句子违反了直接宾语应该靠近动词的一般语法原则，但它满足了依存长度最小化的原则，即更短的依存关系更受欢迎。因此，这两个树只显示了关键依存关系的长度，即这两个结构中不常出现的依存关系。因此，我们从红色到附属成分的依存关系长度为7个词，从红色到书的依存关系长度为4个词，总共是11个词。当你交换这两个成分时，这两个依存关系的总和变为6个词，比11个词短得多，这就是为什么这听起来很正常的原因。它违反了一个原则，但满足了另一个原则。因此，我们从增强版PENTRY语料库中提取了关于并列的各种统计数据，并查看了为什么我们没有使用大学依存关系。这些统计数据证实了之前多次观察到的现象，即左并列成分往往更短。因此，盐和胡椒而不是胡椒和盐，以音节为单位测量，以及在经过观察时发现的趋势，即随着长度的增加，长度差异也随之增加。因此，当两个并列成分的长度差异增大时，较短的并列成分更倾向于排在前面。左短并列成分的比例更大，但本文的新颖之处在于，我们观察到这种趋势只有在领导词在左边时才会发生。因此，在这个例子中，领导词在左边，我看到了巴特和丽莎，因此领导词在左边，在第二个例子中，荷马来了，打了个喷嚏。这里有两个动词的并列，没有外部领导词。因此，在这种情况下，左并列成分更倾向于更短，两个并列成分之间的差异越大，这种倾向就越强。然而，当领导词在右边时，例如左边领导并列尾部和净时，这种效果就会消失。我们通过测量字符长度、音节长度和词语长度来展示这一点。因此，我将重点关注右边的部分。我们在这里看到，当领导词在左边时，左并列成分更短的倾向会随着单词的绝对差异稳步增长，当没有领导词时也会观察到同样的现象，例如句子并列。但是，当领导词在右边时，这种倾向就会消失。我们在论文中展示了这一点，如何为我们反对像这两者这样的非对称并列结构和支持像这两者这样的对称并列结构提供了论据。因此，请参阅论文以获取完整的协议和论据，抱歉，并向我们谈谈海报会议。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "- 大家好，我叫Kyo Yin，我将为大家介绍我们的研究成果《何时需要上下文进行翻译：一个数据驱动的多语言表达？》。这项研究是与Patrick Ferange、Emiliu、Andre F.D Martins和Graham Newbiig合作完成的。因此，很多翻译都依赖于上下文。例如，如果前一句是如果部长们发现，事情可能会变得危险，那么mole就更可能指的是间谍；但如果前一句是医生，这可能是严重的事情吗？那么mole就指的是胎记。因此，根据上下文，单词的含义会发生变化，因此其翻译也会随之变化。然而，评估模型在处理这类情况时的表现非常困难，首先是因为只有小部分翻译依赖于上下文，这使得像BLEU这样的语料库级指标无法捕捉这些翻译。有些人建议对依赖上下文的翻译进行有针对性的评估，但这些资源只能支持有限类型的依赖上下文翻译和有限的语言集合，因为它们通常依赖于领域知识和人工整理。在这项工作中，我们试图回答这两个问题。首先，何时需要上下文进行翻译？其次，模型在处理这些情况时的表现如何？为了回答第一个问题，我们首先测量了作品在翻译中对上下文的依赖程度。在之前的研究中，我们引入了cxmi作为机器翻译模型上下文使用的度量，这是通过测量上下文C在给定源x的情况下对目标y提供了多少信息来实现的。你可以将cxmi视为给模型提供上下文时获得的信息。在这项工作中，我们将cxmi扩展为point y cxmi，可以测量句子级或词级上下文的使用。我们可以将p6mi高的词视为需要上下文进行翻译的词。现在，我们分析了p6mi高的词，寻找这些词之间的模式，并在从英语翻译成14种不同语言的TED演讲稿的文本上进行了分析。我们在三个不同的层面上进行了分析：首先，我们查看了具有高pxmi的词性标签，这使我们能够找到例如，阿拉伯语中的双重代词具有相对高的pxMmi，这可以解释为英语没有双重代词，因此在翻译成阿拉伯语时，你需要上下文来确定代词是否是双重代词。同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看了在所有不同出现情况下具有高p6I平均值的词汇项目，这有助于我们识别出像这里的情况，即在中文中，你需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。同样，我们发现上下文有助于以正确的正式程度进行翻译。最后，我们查看了具有高pxmi的不同个体标记，这使我们能够识别出无法真正由词本身捕捉到的现象，而是通过句子结构表达的，例如省略的解析。现在，我们使用分析结果来设计一个针对文档级翻译的基准。对于我们确定的五个语篇现象，我们创建了标记器来自动识别与现象相关的词，我们称我们的标记器为多语言语篇意识或muda标记器。我们还可以注意到，不同语言具有不同比例的这些语篇现象。然后，我们使用muda标记器，通过在用于评估的平行语料库上应用标记器，并在M标记器识别的依赖上下文示例上应用我们选择的翻译指标。最后，我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译上的表现。首先，当我们使用语料库级指标时，对于BLEU，我们发现不依赖上下文的模型表现最好。但如果我们使用commentt，依赖上下文模型表现最好。如果我们使用wordf度量，那么有无上下文的模型表现相当。这再次表明，如果我们仅使用语料库级指标，就很难确定最佳的文档级翻译系统。现在，我们使用M基准来评估模型，发现对于某些语篇现象，如正式性和词汇连贯性，依赖上下文模型的准确性显著高于不使用上下文的模型。但这些模型在其他现象，如省略代词和动词形式上并不比不使用上下文的模型好多少。这表明我们需要在文档级翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准表明，在文档级翻译方面，dL通常比Google Trans更准确。总结来说，我们对14对语言对进行了数据驱动的分析，以确定何时需要上下文进行翻译，然后我们使用我们的改进方法构建了一个文档级机器翻译的基准，这有助于我们确定模型可以很好地处理哪些语篇现象，以及哪些翻译系统擅长文档级翻译。非常感谢大家的关注，期待在多伦多见到大家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基梅隆大学的一名一年级博士生，今天我将介绍你们的工作——AnL 位置性，描述设计偏见和模型的 beta 集。这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的，他们是塞巴斯蒂安·桑蒂、罗宁·拉布拉塞、卡塔里娜·雷尼卡和马丁·萨普。让我们先想象一下，你为一家报纸工作，你在筛选新闻文章下的评论，试图删除有毒内容，你可能会转向像 prospective API 这样的流行 API 进行有毒性检测，如果你是卡尔·琼斯，prospective API 能够正确检测出有毒的实例，但这对于迪斯·夏尔马来说并不是真的，因为 perspectiveive API 对印度语境中更常见的攻击性用词并不敏感，这是一个设计偏见的例子，我们在这里看到技术在不同人群之间的系统性性能差异。像我们之前看到的这种设计偏见可能是由于 NLP 研究人员和模型开发人员的立场造成的。立场性简单来说就是人们由于其人口统计、身份和生活经历而持有的观点。这是一个在批判性研究中广泛使用的概念，特别是在女权主义和酷儿学术空间中。作为研究人员，立场性可以影响研究过程及其结果和结果，因为它可以改变研究人员的决策，因此人们可能会问一个问题，数据集和模型是否有立场性，我们并不是说模型本身和数据集本身有人口统计身份和生活经历，但它们确实汇集了真实的人们的判断和观点，因此可以代表某些立场性优于其他立场性。之前的研究已经提出了关于立场性的轶事证据，例如模型和数据集中的文化差距，以及模型立场性的理论定义，然而，这些工作并没有真正比较最终用户与数据集和模型本身，而研究模型和数据集的立场性变得越来越重要，因为 NLP 任务变得更加主观和社会导向，并且很难描述这些立场性的偏差，因为并非所有决策都被记录，许多模型隐藏在 API 背后，因此，为了研究数据集和模型的立场性，我们实际上是将注释与真实用户与现有数据集和模型进行比较，我们通过我们的框架 Nl 位置性来实现这一点，我们的框架分为两个主要步骤，第一步是用多样化的注释者重新注释数据集，我们选择这样做是因为通常只有少数注释者注释每个实例，而且人口统计数据很少被收集和分享，因此我们选择重新注释数据以获得许多注释实例，并获得丰富的社会人口数据，然后我们按人口统计数据对注释进行分组，并使用皮尔逊相关分数将它们与模型和数据集进行比较，因此，我们的框架实际上与注释者分歧文献不同，它通过比较最终用户与模型和数据集的预测和标签，而不是仅仅关注注释者的同意或模型注释者的分布。我们的框架在很大程度上是通过实验室在野外（lab in the wild）在线众包平台实现的，前者是 HCI 合作者，后者是一个在线实验平台，我们可以在这里招募多样化的志愿者，相比于像 Turk 这样的平台，这些平台主要有来自美国或印度的参与者，而且实验室在野外的平台仍然能够获得高质量的数据，我们在实验室在野外的平台上托管了两个任务，其中一个是社会可接受性，它的工作原理是参与者将阅读来自社会化学数据集的某个情境，然后他们将写出这个情境在社会上是多么可接受，为了保持参与研究，他们可以将他们的回答与 AI 和其他人进行比较，然后我们将这些注释与社会化学 Delphi 和 gPT4 进行比较。然后，我们为有毒性和仇恨言论检测任务复制了一个非常相似的设置，参与者将阅读 Dynah Hate 中的一个实例，并写出他们是否认为这是一个仇恨言论的实例。然后我们将这些注释与 Dyna Hate Perspective API、Rewire API、Hate Roberta 和 GPT4 进行比较。我们的研究最终收集了来自 87 个国家的 1000 多名注释者的 16,000 多个注释。所以现在我们更有能力回答谁与 NLP 数据集和模型最符合？我们发现 NLP 中存在立场性。例如，我们发现数据集和模型最符合英语国家的立场性。因此，对于 gpd 四社会可接受性分析，我们发现它最符合儒家和英语国家的立场性，我们发现 Dinah hate 也最符合英语国家的立场性，我们还发现与受过大学教育的人群的立场性最符合。因此，对于 gpd4 在社会可接受性任务中，我们发现它最符合受过大学教育或研究生教育的人群，我们发现对于 Dinah hate 也是如此，它最符合受过大学教育的人群，然而，当模型和数据集与特定人群对齐时，有些人不可避免地会被抛在后面，一个例子是数据集和模型与非二元人群的对齐程度低于男性和女性人群，我们在 gPDd4 社会可接受性任务以及 Dinah hate 任务分析中都发现了这一点。因此，鉴于 NP 中存在立场性，我们可以做些什么呢？我们有几个建议，第一个是在整个研究过程中记录所有相关的设计选择，第二个是用前瞻性的视角进行 NLP 研究，我们的第三个建议是在四个特定社区内构建专门的数据集和模型，一个很好的例子是 masakanne 计划，我们希望强调，包容性的 NLP 不仅仅是让所有技术都能为每个人工作，这就是我们的介绍结束，但如果你想了解更多，请随时查看我们的仪表板以获取最新的分析结果和我们的论文，谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我将谈谈我们在解决实体选择中的间接差异表达方面的工作，我们引入了备选实体语料库，我的名字是贾瓦德·霍赛尼，这是与菲利普·拉德林斯基、西尔维亚·帕里蒂和安妮·路易斯合作完成的联合工作。我们的目标是理解用户在做出选择时使用的语言。我考虑了这样一个备选问题：你是说“Easy on Me”还是“I got a feeling？”这里用户想要在这两首歌曲中进行选择。最明显的方法是使用直接引用，例如说出歌曲的名称“Easy on Me”或其位置，第一首，但有时间接引用更适合进行更自然的对话，这种情况可能发生在用户记不起歌曲名称时，或者发音过于相似难以区分时，或者用户想要指定偏好。以下是间接差异的一些例子。例如，较新的一个或没有充满活力的标志。这是对话系统中的一个重要问题，也是用于基准测试LLM实体理解的一个重要问题。我们没有发现一个公共数据集，一个更大规模的公共数据集用于该任务，所以我们使用众包标注收集了一个数据集，我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集合方法强调非正式性，使用卡通完成设置，卡通中有三个对话气泡，在第一个气泡中，鲍勃说，记得我们昨天听的那首歌吗？鲍勃设置了对话上下文，在第二个对话气泡中，爱丽丝说，你是说Easy on Me还是I got a feeling？这是一个备选问题，在第三个对话气泡中，鲍勃使用间接引用来选择这些实体中的一个，例如较新的一个，我们自动提供第一个和第二个对话气泡，但第三个由标注者填写，第一个对话气泡是从每个领域的一些手动提示中选出的，第二个对话气泡是备选问题，生成如下：我们总是使用一个简单的模板，你是说a还是b，其中a和b是维基百科的样本。以下是我们使用过的不同采样方法。当我们在列表中向上移动时，实体变得越来越相似，通常更难进行消歧。第一个是均匀随机。第二个是当实体有相似的标题，例如两本书的名称相同。第三个是当它们在维基百科上的描述相似，最后当它们在维基百科上的信息框或属性相似，例如歌曲的同一流派或同一艺术家。当我们向am statustors展示这个备选问题时，他们知道这些实体的名称，但他们不一定会了解实体，所以我们展示了一些关于这两个实体的背景知识，对于歌曲，我们简单地展示每个歌曲的Google搜索链接，然后要求标注者至少听一些每首歌曲，并阅读每首歌曲的介绍。以下是例如Easy on Me歌曲的Google搜索结果，对于食谱和书籍领域，我们展示了一些维基百科的背景文本，对于食谱，我们还展示了它们的图片，再次来自维基百科，以便标注者知道它们是什么样子，然后我们要求标注者选择这些实体中的一个，例如这里的第一首，并使用三到五个间接引用表达来描述它们，例如钢琴音乐的，以下是我们数据集的一些例子，例如没有歌词的，不是有12岁男孩的，还是虚构的，或者来自阿塞拜疆等等。备选语料库有6000个备选问题，涵盖三个领域，有422,000个间接引用表达。使用t5x大型模型的结果总结如下：如果语言模型可以访问与标注者完全相同的背景知识，那么准确率非常高，大约在92%到95%之间，但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到877%之间，这是更现实的。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率只有6%，所以还有很大的改进空间。我们还表明模型具有领域的一般化能力，这里有一个链接到我们的数据集，感谢大家。"}
