{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unseren Artikel über kompositorische Verallgemeinerung ohne Bäume mit Hilfe von Multiset-Tagging und latenten Permutationen geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositorische Verallgemeinerung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und nicht gesehene Kompositionen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsings könnte die Prüfung auf kompositorische Verallgemeinerung wie folgt aussehen: Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall „das Mädchen schlief“ und „Mary wusste, dass das Mädchen schlief“. Diese Äußerungen werden mit logischen Formen gepaart, die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standardbewertung des maschinellen Lernens stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell nicht gesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursion gesehen und wird auf Beispiele mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle kämpfen mit dieser Art der Verallgemeinerung außerhalb des Datensatzes und produzieren oft Ausgaben, die vom Input losgelöst sind. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie in dem Beispiel farblich codiert sind. Eine beliebte Methode, um dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein. Typischerweise beinhaltet dies erheblichen Formalismus und spezifische Vorverarbeitung der logischen Formen, um beispielsweise Symbolsymbole zu handhaben. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsprozeduren beinhalten. In diesem Artikel verwenden wir keine Bäume und führen ein neuronales Sequenz-zu-Sequenz-Modell ein, das die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert. Zum ersten Mal zeigen wir eine starke Verallgemeinerung auf tiefere Rekursion, ohne auf Bäume angewiesen zu sein. Unser Ansatz prognostiziert die Ausgabe aus dem Input in zwei Schritten. Zuerst taggen wir jedes Input-Token mit einem unbestellten Multiset von Token, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir in dem zweiten Schritt ein anderes Modell, um eine Permutation zu prognostizieren, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um eine Permutation zu prognostizieren, die keine harten Einschränkungen auf die möglichen Permutationen legt. Dies macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Output und bestimmen, welches Multiset-Token in jeder Position platziert werden soll. Für die erste Output-Position wählen wir einfach eines aus, wie rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Einblick in die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf dem COgs-Benchmark. Unser Modell übertrifft die anderen mit großem Abstand bei der Verallgemeinerung auf tiefere Rekursion. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr herausfordernd. In unserem Artikel lösen wir einige interessante technische Herausforderungen. Zunächst einmal ist die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht gegeben. Folglich wissen wir für ein gegebenes Token nicht, woher es stammt, was eine Herausforderung für das Training darstellt. Zusätzlich gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte ist latent. Wir gehen dies an, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstbewerteten Permutation Nphard ist. Das liegt daran, dass dies mit dem Reiseverkäuferproblem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzuprojizieren und die sprachlich plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unseren Artikel oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra und heute werde ich über unsere papiermarkierten Personas sprechen, bei denen wir natürliche Sprachanfragen verwenden, um Stereotype in Sprachmodellen zu messen. Diese Arbeit wird in Zusammenarbeit mit Essenndermush und Danjorovsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotype in großen Sprachmodellen oder LLMs dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen, da sie in der Regel auf handgefertigten Datensätzen basieren, die sehr zeitaufwendig zu kuratieren sind, und sie messen in der Regel nur sehr spezifische Stereotype, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte verallgemeinern oder sie erfassen einfach nur sehr allgemeine, breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich die Intersektionalität nicht, die die Vorstellung ist, dass vielfältige soziale Identitäten Vorurteile verstärken und einzigartige Schadensquellen sein können. Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, instruktionsabgestimmten LLMs sehr gut darin sind, auf Anweisungen in Eingaben zu reagieren, sodass wir das Modell bitten können, eine Persona zu generieren, die eine Darstellung einer imaginierten Person ist, indem wir eine Eingabe wie \"Stellen Sie sich vor, Sie sind eine asiatische Frau, beschreiben Sie sich selbst\" verwenden. Wir können sofort sehen, dass dies sehr gut auf jede Demografie verallgemeinbar ist, da wir einfach jeden Identitätsmarker, den wir möchten, in diese Eingabe einfügen können. Hier sind einige Beispielgenerierungen von GPT4. Sofort sehen wir, dass, obwohl die Ausgaben nicht offenkundig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, es einige interessante Muster gibt. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und bezaubernd bezeichnet, und beide Frauen of Color machen Verweise auf ihre Abstammung, während die Persona des weißen Mannes nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Eingaben zur Generierung dieser Personas wurden von einer Studie inspiriert, bei der sie diese Eingaben menschlichen Probanden gaben und feststellten, dass sie dadurch auch rassistische Stereotype aufdecken konnten. Dies ermöglicht auch einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unseren markierten unterscheiden, die ich gleich näher erläutern werde. Der Vorteil dabei ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne uns auf ein bestimmtes Lexikon verlassen zu müssen. Die Methode der markierten Wörter stützt sich auf das soziolinguistische Konzept der Markierung, das besagt, dass es ein unmarkiertes Standardwort gibt und jede Gruppe, die von diesem Standard abweicht, sprachlich markiert ist. Zum Beispiel wird das Wort Mann oder Sorry, das Wort Krieger normalerweise mit Männern in Verbindung gebracht, also wenn Menschen einen Krieger beschreiben, der eine Frau ist, werden sie in der Regel tatsächlich einen männlichen Krieger spezifizieren und den Begriff mit Frau markieren. Im weiteren Sinne werden dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bestimmen wir zunächst, welche die unmarkierten und markierten Gruppen sind, und dann vergleichen wir die Personas mit der Methode der kämpfenden Wörter, die im Grunde genommen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen kämpfende Wörter durchführen und die Log-Odds-Verhältnisse gegen die von weißen Personas und Männer Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir das Lexikon der Stereotype und stellen fest, dass die generierten Personas viel mehr Stereotype enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter im Lexikon betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter haben, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter groß und athletisch sind, also wirklich nur die positiven oder zumindest nicht-negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien überhaupt nicht erfasst haben. Anstatt das zu tun, werden wir uns den Ergebnissen aus unserer Methode der markierten Wörter zuwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotype und essentialisierende Erzählungen erleichtern. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst für markierte Gruppen gehören die Top-Wörter Dinge wie Kultur, Tradition, stolz und exotisch dazu, und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einem langen Erbe der Diskriminierung und der Anderenbildung für diese Gruppen bei. Darüber hinaus gibt es viele gängige Tropen, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen of Color. Zum Beispiel beinhalten die Wörter, die eine Latina Frau beschreiben, Dinge wie lebendig und kurvig, die mit einem Tropismus der Exotik verbunden sind. Bei asiatischen Frauen sind die Wörter Dinge wie zierlich und zart und seidig, die mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen verbunden sind, die als sehr sanftmütig und unterwürfig angesehen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind, was mit einem Archetyp verbunden ist, den die Menschen den Archetyp der starken schwarzen Frau genannt haben, und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, da er diese Demografien unter Druck setzt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, setzt es Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen führt, unter anderem. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotype und essentialisierende Erzählungen ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Minderung von Vorurteilen geben, da wir zum Beispiel nicht wissen, ob es sich bei diesen positiven Stereotypen um eine Art seltsame, übermäßig exzessive Wertanpassung handelt oder vielleicht um andere Anti-Stereotyp-Methoden, die zu diesen schädlichen Mustern führen. Wir können wirklich keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Ich wünsche euch viel Spaß auf der ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": ": Hallo, ich bin James Finch. Und ich bin Sarah Finch. : Und heute werden wir Ihnen alles über ABCEV erzählen, einen neuen dimensionalen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde vom Emory NLP-Labor durchgeführt, geleitet von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI. : Nehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die gängige Praxis besteht darin, menschliche Bewertungen zu verwenden, wie z. B. menschliche Richter zu bitten, auszuwählen, welche von zwei Konversationen besser ist oder Konversationen auf einer Likör-Skala zu bewerten. Diese Ansätze eignen sich gut, um ganzheitliche Bewertungen der gesamten Dialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feiner aufgelösten Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likör-Skalen-Methoden. Wir glauben jedoch, dass es eine genauere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem er explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt oder nicht, wie z. B. mit irrelevanten Informationen zu antworten oder sich selbst zu widersprechen. Wir nennen diesen Ansatz das Annotating of Behaviors in Chat oder kurz ABC Eval. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chatmodellen umfassend abzudecken, von denen in der jüngsten Literatur angenommen wird, dass sie die Chatqualität beeinflussen. ABC Eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC Eval die Anzahl der Zyklen, in denen ein Chatmodell seinen Partner ignoriert oder etwas Ir relevantes sagt, sich selbst oder seinem Partner widerspricht, falsche Fakten halluziniert oder das Wissen über den gesunden Menschenverstand verletzt, und wann das Modell Empathie zeigt oder nicht. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chatmodelle ausgewählt und sie anhand von 100 menschlichen Bot-Konversationen pro Modell mit ABC Eval bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likörbewertungen auf der Zyklus-Ebene, Likörbewertungen auf der Dialog-Ebene und Dialog-Ebene-Paarweise-Vergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Aus unseren Analysen der Bewertungsergebnisse haben wir festgestellt, dass ABC-Verhaltensbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden gesammelt wurden, gemessen an der Übereinstimmung der Annotatoren auf hundert doppelt annotierten Konversationen. Darüber hinaus sind ABC Eval-Bezeichnungen im Vergleich zu Metriken, die von bestehenden Methoden produziert werden, prädiktiver für die gesamte Gesprächsqualität, wie diese einfache lineare Regressionsanalyse zeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Zyklen mit Selbst- und Partnerwidersprüchen jeweils 20 Prozent und 10 Prozent der Gesprächsqualität erklärt, während die durchschnittlichen Likörkonsistenzwerte nur vier Prozent oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, indem wir eine schrittweise lineare Regression verwendet haben. Sie können sehen, wie die Kombination aller ABC Eval-Metriken über 25 Prozent der Gesprächsqualität erklärt und wie die meisten von ihnen, wenn man sie nacheinander entfernt, eine beträchtliche Menge an Informationen über die Qualität verlieren. Andererseits erklärt die Kombination aller Likör-Metriken auf der Zyklus-Ebene weit weniger von der Qualität und nur wenige dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden erreichen konnten. Sie können dies in den Ergebnissen unseres Experiments sehen, dass mehrere Herausforderungen noch bestehen und genau quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 Prozent ihrer Antworten Verstöße gegen den gesunden Menschenverstand, sie produzieren in etwa 15 Prozent der Antworten irrelevante Informationen und sie widersprechen sich oder ihrem Partner in etwa 10 Prozent der Zeit. Mit dem raschen Fortschritt in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "- Hallo, mein Name ist Vauddha und ich bin Doktorandin im Fachbereich Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die für die ACL 2023 als Langpapier angenommen wurde: Transfer Learning für die Dissonanzdetektion, mit Fokus auf die Herausforderung seltener Klassen. Zunächst definieren wir kognitive Dissonanz und warum sie ein wichtiges Problem ist, das in der Sprache einfach zu studieren ist. Kognitive Dissonanz liegt vor, wenn zwei Überzeugungen oder Handlungen inkonsistent sind, wie in diesem Beispiel, wo eine Person sagt: „Ich weiß, dass Zigaretten mich töten können“ und dann fährt fort: „Ich habe nach dem Meeting ein paar Zigaretten geraucht.“ Diese Überzeugung und Handlung sind inkonsistent und befinden sich in Dissonanz. Weiterhin wird erwähnt, dass die Aussage „Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte“ das zweite Auftreten rechtfertigt und dass sie eine Konsonanzbeziehung haben. Dissonanz ist ein sehr häufiges Phänomen, das wir in der täglichen Entscheidungsfindung erleben. In der Sprache ausgedrückt, sind sie jedoch sehr selten und treten in verschiedenen Arten von Diskursbeziehungen auf. Warum ist das wichtig? Das Studium der kognitiven Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten unter Menschen, Trends und Glaubenssätzen sowie Einstellungen in der Bevölkerung zu verstehen. Hohe kognitive Dissonanz steht auch in Zusammenhang mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Das Studium von Dissonanz, die in der Sprache ausgedrückt wird, kann auch hilfreich sein, um Extremismus und Polarisierung von gefährdeten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um ein Ressourcenmodell für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen Dissonanz-First-Ansatz, wie in dem hier gezeigten Flussdiagramm. Tweets wurden mit einem PDTV-Parser verarbeitet, und Paare von Diskursenheiten wurden gemäß den Richtlinien annotiert, die in unserem Papier beschrieben sind. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 Prozent der annotierten Paare gefunden. Nach der Sammlung von etwa tausend Beispielen von Diskursenheiten haben wir ein Training für einen anfänglichen Klassifikator durchgeführt, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Wie nicht überraschend, führte der Klassifikator nicht viel besser als der Zufall, angesichts des geringen Auftretens von Dissonanz und der Abwesenheit jeglicher vorheriger solcher Datensätze. Wir stehen vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen von Transfer Learning und aktivem Lernen, um solche Annotationen durchzuführen, dass mehr Dissonanzbeispiele in weniger Annotationsschleifen gesammelt werden können, wodurch die Gesamtannotationskosten gesenkt und die Dissonanzdetektion verbessert wird. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: der dissonanzunabhängigen Tanzklassifizierung, einer Aufgabe, die bestimmt, ob zwei Debatte-Aussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema, hier als „Debatte“ bezeichnet, und der binären Klassifizierung von Expansions- und Vergleichsklassen der Reinheit. Da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verwandt sind, bezeichnen wir sie hier als „ceE“. Wir stellen fest, dass die Übertragung der Gewichte aus diesen Aufgaben die anfängliche Leistung auf dem annotierten Datensatz bereits viel besser als der Zufall macht, mit dem besten Wert von Auc 0,62. Durch die iterative Feinabstimmung auf beiden Aufgaben stellen wir fest, dass die Feinabstimmung der ceE-Aufgaben gefolgt von einer weiteren Feinabstimmung auf der Debatte eine viel bessere Null-Shot-Leistung ergibt. Dies ist das Modell, das wir verwenden, um das aktive Lernen zu starten. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des aktiven Lernens und der Annotationen zu aktualisieren. Die kumulative Methode sammelt alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während die iterative Methode das Modell durch Training auf dem neuesten Datensatz aktualisiert, der über die verschiedenen Strategien gesammelt wurde. Wir stellten fest, dass die kumulative Methode gleich oder besser als die iterative Methode abschneidet. Um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeitsstrategie für seltene Klassen (PRC), um hauptsächlich die Beispiele auszuwählen, die nach dem aktuellen Modell in jeder Runde des aktiven Lernens (ALE) am ehesten dissonant sind. Wir vergleichen dies mit anderen State-of-the-Art-ALE-Strategien, die in der Gemeinschaft häufig verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für zufällige Auswahl in weiteren Runden des aktiven Lernens signifikant niedriger ist. Mit den beiden besten Strategien verbessern wir die Distanzklassifizierung Auc 2 auf 0,75, was die beste Leistung ist, die wir bisher für diese Aufgabe haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Qualität der Annotationen und die Kosten für die Annotatoren. Wir stellen fest, dass die PRC-Strategie den höchsten Prozentsatz an Dissonanz aufweist und am besten für seltene Klassen geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass die PRC-Strategie eine einfache Strategie für den Erwerb seltener Klassen ist und das kalte Starten des aktiven Lernens mit angemessen gestalteten Transfer-Learning-Aufgaben erheblich helfen kann. Wir stellen auch fest, dass die iterative Aktualisierung nützlich für das Transfer Learning aus einem anderen Bereich ist, während in-domain aktive Annotationen von der kumulativen Aktualisierung profitieren. Dies sind die Links zu unserem Code, unserem Datensatz und unserem Papier. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Ich bin Akshata und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit „The Kit Must: Evaluating Knowledge Integration from Multiple Sources“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Modelle für das Verständnis natürlicher Sprache stützen sich auf eine Vielzahl von Wissensquellen, wie z. B. Wissen, das in ihren Parametern enthalten ist und normalerweise durch Vorabtraining erworben wird, sowie Wissen, das in Eingaben zur Zeit der Inferenz gegeben wird. Jüngere Arbeiten bei Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das vorabtrainierte Zeitwissen nutzen können, um die Aufgabe zu lösen. Doch das Verständnis natürlicher Sprache erfordert oft Wissen, das auch zur Zeit der Inferenz bereitgestellt wird, zum Beispiel im Satz „John sah den neu gewählten Präsidenten im Fernsehen“. Die vorabtrainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instansspezifische Entität John ist oder wer der neue Präsident ist, da sich die Präsidenten seit dem Vorabtraining möglicherweise geändert haben. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vorabtrainiertes als auch inferenzzeitliches Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Kernreferenzauflösungaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu prüfen, auf Wissen zurückzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren Kernreferenzauflösungsmodelle. Hier ist ein Beispiel aus unserem Datensatz: Serviert ist ein Richter, Kia ist ein Bäcker. Und Kia, der nach einem langen Arbeitstag, in dem er Fälle in einem Gesetzbuch entschieden hat, sich entspannen wollte. Die Aufgabe hier besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen er sich bezieht, was in diesem Fall ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: Erstens, entitätspezifisches Wissen wie „Serviert ist ein Richter“, und zweitens, Hintergrundwissen wie „Richter entscheiden in der Regel Fälle in Gerichten“. Hintergrundwissen wird während des Vorabtrainings großer Sprachmodelle erlernt, während entitätspezifisches Wissen typischerweise zur Zeit der Inferenz beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationsstücke so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von kitdmos definiert. Zuerst haben wir die typische Einstellung „vorabtrain“, bei der angenommen wird, dass Hintergrundwissen zur Zeit des Vorabtrainings verfügbar ist. Zweitens gibt es die Einstellung „Hintergrund beides“, bei der Hintergrundwissen sowohl zur Zeit des Vorabtrainings als auch zur Zeit der Inferenz verfügbar ist. Zuletzt die Einstellung „Hintergrund Inferenz“, bei der beide Wissensarten nur zur Zeit der Inferenz verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das Hintergrundwissen, das notwendig ist, um eine Aufgabe zu lösen, nicht Teil der Vorabtrainungsdaten der Modelle ist, zum Beispiel, weil sich seit der Zeit des Vorabtrainings neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen im Hintergrund „vorabtrain“ steuern. Wir nehmen an, dass das Hintergrundwissen „Politiker suchen gewählte Sitze in der Regierung“ in den vorabtrainierten Parametern enthalten ist. Im Kontext der Inferenzzeit stellen wir das entitätspezifische Wissen „Chichester ist ein Politiker“ zur Verfügung. In der Einstellung „Hintergrund beides“ stellen wir zusätzlich nicht nur entitätspezifisches, sondern auch Hintergrundwissen über Politiker im Kontext der Inferenzzeit zur Verfügung. In der Einstellung „Hintergrund Inferenz“ stellen wir das Merkmal „Beruf“ lediglich als „Tour“ anstelle von „Politiker“ zur Verfügung, da „merelytour“ unwahrscheinlich ist, in den vorabtrainierten Parametern enthalten zu sein. Wir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch etablieren Präferenzauflösungsmodelle. In dieser Abbildung zeigen wir die Ergebnisse der bestperforming Modelle auf der schwierigsten Variante der Einstellung „Hintergrund vorabtrain“, ohne auf kitdmos spezifisches Training. Beide Modelle erzielen keine guten Ergebnisse, wenn sie auf kitdmos trainiert werden. Allerdings erzielen sowohl c2f als auch built for coref signifikant bessere Ergebnisse als die zufällige Auswahl. Dies deutet darauf hin, dass Modelle, wenn sie auf generischen Referenzauflösungsdatensätzen trainiert werden, lernen, oberflächliche Hinweise zu nutzen, die beim Testen auf kitdmus, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die bestperforming Modelle das Hintergrundwissen nicht zuverlässig integrieren können, es sei denn, zur Zeit der freien Zoll. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen: Viele Ko-Referenz-Evolution-Modelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu argumentieren, ohne aufgabenbezogenes Training. Mit aufgabenbezogenem Training können jedoch einige Modelle Wissen aus verschiedenen Quellen erfolgreich integrieren. Dennoch scheinen selbst die bestperforming Modelle Schwierigkeiten zu haben, zuverlässig integriertes Hintergrundwissen zu präsentieren, das nur zur Zeit der Inferenz verfügbar ist. Wenn Sie mehr Details interessiert sind, lesen Sie bitte unsere Arbeit und überprüfen Sie den Datensatz im Code auf GitHub. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Pai von der Universität Trient und vom Bruno-Kessler-Forschungszentrum, und ich werde kurz die Aufmerksamkeit als Leitfaden für das Papier zur simultanen Sprachübersetzung vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Durchi ist. Was ist simultane Sprachübersetzung? Die simultane Sprachübersetzung oder SimST ist der Prozess der Übersetzung von gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, der die Kommunikation über Sprachgrenzen hinweg ermöglicht. Was sind die Probleme der aktuellen SimST-Modelle? Spezielle Architekturen werden normalerweise trainiert, indem zusätzliche Module eingeführt werden, die optimiert werden müssen. Es gibt lange und komplizierte Trainingsprozeduren, zum Beispiel das Training mit verschiedenen Optimierungszielen und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Was ist also unsere Lösung? Erstens, bereits bestehende Offline-SD-Modelle zu verwenden, ohne sie neu zu trainieren oder eine spezifische Architektur für die SimulaSD-Nutzung zu übernehmen. Wir verwenden nur ein Modell für jedes Latenzregime und behandeln die Latenz durch spezifische Parameter. Wir nutzen das Wissen, das ein Modell bereits durch den Spannungsmechanismus zwischen Audioeingang und textlichem Ausgabeergebnis erworben hat, das ist der Kreuzaufmerksamkeitsmechanismus. Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, eine Punkt- oder Encoder-Dekorations-Aufmerksamkeit vorzuschlagen. Es ist eine Strategie, bei der wir entscheiden, ob eine teilweise Übersetzung emittiert wird oder nicht, basierend darauf, wohin die Aufmerksamkeit auf ein Wort gerichtet ist. Eine Übersetzung wird emittiert, wenn die Spannung nicht konzentriert ist, das heißt, wenn diese Summe unter einem bestimmten Schwellenwert alpha liegt, in Bezug auf die letzten lambda Sprachrahmen. Das bedeutet, dass die empfangenen Informationen stabil genug sind. Zum Beispiel, wenn wir einen Sprachchunk erhalten, der „Ich werde darüber sprechen“ enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, werden wir uns die Kreuzaufmerksamkeitsgewichte ansehen und sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die zuletzt empfangenen Sprachrahmen verweist, als lambda Sprachrahmen. Das bedeutet, dass die ersten beiden Wörter emittiert werden, während wir das letzte Wort nicht emittieren, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, und wir auf einen weiteren Sprachchunk warten. Wenn wir weitermachen und einen weiteren Sprachchunk erhalten, und unser Modell drei weitere Wörter vorhersagt, werden wir uns die Kreuzaufmerksamkeitsgewichte ansehen und sehen, dass keine Wörter auf die letzten lambda Sprachrahmen verweisen. Das bedeutet, dass diese drei Wörter emittiert werden. Wenn wir uns die Hauptergebnisse eines Punktes ansehen, plotten wir die Ergebnisse der simultanen Sprachübersetzung auf Diagrammen, in denen wir auf der einen Seite Blau haben, das die Übersetzungsqualität und die durchschnittliche Verzögerung misst, das ist das Latenzmaß, und wir berücksichtigen auch den rechnerisch bewussten durchschnittlichen Mangel, der die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven so hoch wie möglich auf diesem Diagramm sind, aber auch, dass sie nach links verschoben sind. Wir vergleichen mit Plepara-Strategien, die auch auf Offline-Modellen angewendet werden, das sind die Whit-Key-Strategie und die lokale Übereinstimmung, und wir vergleichen auch mit der Standes-der-Technik-Architektur, die speziell für die simultane Raumübersetzung zugeschnitten ist. Dies sind alle Ergebnisse der Strategie der simultanen Sprachübersetzung auf Deutsch, und wir sehen, dass AD alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die rechnerische Abnutzungsdauer betrachten, AD die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier, und wir haben auch den Code und die Modelle und die simultane Ausgabe als Open Source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Mein Name ist Shu Ha. Heute werde ich unseren Artikel „Do Connel 2003 Named Entityity Tagus: Funktionieren sie noch gut im Jahr 2023?“ vorstellen. Lassen Sie uns beginnen. In unserem Artikel haben wir das Problem der Generalisierung untersucht, indem wir die Aufgabe der Named Entity Recognition (NER) betrachtet haben. Wir haben festgestellt, dass Modelle seit fast 20 Jahren Con 2003 zur Entwicklung von NER verwenden. Dies wirft natürlich mehrere Probleme auf: Erstens, können diese Modelle auf moderne Daten generalisieren? Und zweitens, was ist für eine gute Generalisierung erforderlich, wenn wir einen neuen Tagger entwickeln? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Con plus+ Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen Con 2003 Annotation Richtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf Con 2003 verfeinert, sie sowohl am Con 3 Testdatensatz als auch am Con plus fast Testdatensatz bewertet und zuletzt den prozentualen F1-Änderungswert berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptbestandteile gibt, die erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Zuletzt wissen wir alle, dass die Anzahl der Feinabstimmproben direkt die Leistung einer nachgeschalteten Aufgabe beeinflusst. Auch hier haben wir festgestellt, dass mehr Feinabstimmproben tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist das adaptive Overfitting, also ein Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird und sich normalerweise als abnehmende Renditen auf einem neuen Testdatensatz manifestiert. Die zweite Hypothese ist der zeitliche Drift, also die Leistungsdegradation, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird. Für das adaptive Overfitting haben wir gesehen, dass die rote beste Anpassungslinie im Diagramm auf der rechten Seite einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf Con 2003 vorgenommen haben, sich in mehr als einer Einheit Verbesserung auf Con plus plus umsetzt, was bedeutet, dass es keine abnehmenden Renditen gibt. Und das zeigt uns, dass in diesem Fall kein adaptives Overfitting beobachtet wird. Was ist dann mit dem zeitlichen Drift? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten neu zu trainieren oder weiter vorzuverarbeiten. Wir haben festgestellt, dass die Leistung mit größerer zeitlicher Lücke abnimmt und dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmproben benötigen. Diese gehen Hand in Hand, wir können nicht nur einen Bestandteil haben, sondern alle gleichzeitig. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitliche Drifts verursacht wird und überraschenderweise nicht durch adaptives Overfitting, obwohl Con 2003 seit über 20 Jahren verwendet wird. Um zur Frage zurückzukehren, die wir im Titel unseres Artikels aufgeworfen haben: Funktionieren Con 2003 Tagger noch im Jahr 2023? Wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber anregt, wie die Generalisierung der Modelle verbessert werden kann. Und zu guter Letzt, bitte vergewissern Sie sich, unseren Artikel und unseren Datensatz zu überprüfen. Wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Willkommen bei unserer Präsentation von De plain, einem neuen Korpus für die deutsche Textidentifikation auf Dokument- und Satzebene. Mein Name ist Regina Stoden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textsanierung definieren. Textsanierung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine spezifische Zielgruppe zu verbessern, wie z.B. Menschen mit Lese- und Rechtschreibschwierigkeiten oder Nicht-Muttersprachler. Um ein Textsanierungsmodell zu trainieren, benötigen wir parallele Textpaare, z.B. von Dokumenten oder Sätzen. Hier können Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache sehen. Zur Vereinfachung des Satzes sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikale Substitution, Satzglieddilatation, Kreuzelimination, Umorganisation oder Einfügen von Wörtern. Wir schlagen nun unseren neuen Korpus Dplane vor, da in den letzten Jahren einige Probleme mit bestehenden Korpora aufgetreten sind. So sind diese Korpora hier beispielsweise zu klein, um ein Taxonifikationsmodell darauf zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus Dplane vor, der in zwei Teilkorpora aufgeteilt ist, Deplane APA und Deplane Web. Deplane APA basiert auf Nutztexten. In Deplane APA haben wir 483 Dokumente alle manuell ausgerichtet. Das ergibt ungefähr dreißigtausend dreizehntausend parallele Satzpaare. Für Deplane Web umfasst dieser Korpus verschiedene Bereiche, und wir haben auch alle diese siebenhundertfünfzig Dokumente zum einen manuell und zum anderen mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergeben wir 30 450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, z.B. nach Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als z.B. die Nachrichtentexte oder die Texte für Sprachlerner auf allen Ebenen, was z.B. lexikale Vereinfachung, strukturierte Vereinfachung und allgemeine Vereinfachungsstufe betrifft. Darüber hinaus können Sie sehen, dass unser Deplaning-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir im Deplane API-Korpus viel mehr Umorganisationen und Wortzusätze als im Deplane Web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz Dplane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Ausrichtungen von Sätzen in Nachdokumenten extrahieren möchten. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten mit derselben Sprache und demselben Inhalt, aber auf unterschiedlichen Komplexitätsebenen zu extrahieren. Da wir nun unseren Datensatz Dplane haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes für unsere Experimente im Papier veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für Texte zur deutschen Textsanierung die Methode der Massen-Ausrichtung ist. Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen. Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist ein Fall der automatischen Textsanierung durch Feinabstimmung von Sprachmodellen, um vereinfachte Texte aus dem komplexen Eingabetext zu produzieren. Wir haben zwei verschiedene Modelle verfeinert. Wir haben das Modell von Long Short-Term Memory (LSTM) verfeinert, um Dokumentniveausanierungen zu produzieren, und wir haben auch den normalen Basis-LSTM verfeinert, um Sanierungen auf Satzebene zu produzieren. Sie können auch alle Checkpoints finden und sich in den Papierdetails über die Ergebnisse und die Bewertungsmetriken unserer Experimente informieren. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Basisbewertungen erzielen kann, und wir haben diese Ergebnisse als Benchmark vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin X Yuan von der FNAi-Universität. Ich bin hier, um unsere Arbeit vorzustellen: „Unterscheidung von Skriptwissen und leichten Sprachmodellen für eingeschränkte Sprachplanung“. Im Alltag müssen Menschen oft ihre Handlungen planen, indem sie Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten befolgen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten zu planen, wie zum Beispiel einen Kuchen zu backen, und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrierten sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie zum Beispiel einen Schokoladenkuchen zu backen, wurde in dieser Arbeit noch nicht ausreichend behandelt. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen für die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen entsprechend sind. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da es keine Daten zu spezifischen Zielen gibt, um unsere Studie durchzuführen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele um vielfältige Einschränkungen für die Datenerfassung mit menschlicher Schleife. Wir verwenden instruct GPT, um 100 spezifische Ziele zu entnehmen und die aus Bibliotheksmodellen generierten Skripte zu bewerten. Diese Tabelle zeigt die Gesamtnacuranz der Ergebnisse. Wir stellen fest, dass alle Lernmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, welche Lernmodelle für welche Ergebnisse geeignet sind. Die Abbildung zeigt, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir gehen auf feinere Themenkategorien von Einschränkungen ein, die in WiH definiert sind. Die Wärmekarte in der Abbildung zeigt, dass die Planungsprestanz von instruct GPT für verschiedene Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Lernmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des übergenerierten Z-Filters, um die Generierungsqualität zu verbessern. Zuerst zeigen wir eingeschränkte Typen mit Beispielen für instruct GPT und erhalten spezifische Ziele basierend auf den abstrakten Saatgutzielen. Anschließend instruieren wir GPT über die generierten Schlüsselskripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in instruct GPT-Embeddings und berechnen die Cosinusähnlichkeit und Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich vergeben wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel den höchsten Wert in der Zielgröße erzielt. Mit unserer Methode kann instruct GPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planbarkeit sowohl in Bezug auf Semantik, Vollständigkeit als auch Treue zu den Einschränkungen erheblich. Da große Sprachmodelle kostspielig zu implementieren sind, ist es wesentlich, die Sprachplanbarkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt dazu. Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele und die manuelle Annotation von Datensätzen ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um einen Datensatz für eingeschränkte Sprachplanung aus leichten Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für eingeschränkte Sprachplanung namens CodeScript aufzubauen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testsites zu gewährleisten, bitten wir Crowd-Sourced-Arbeiter, die Einkommen in fehlerhaften Proben zu überprüfen. Diese Abbildung zeigt die Einschränkungsverteilung von CodeScript. Wir stellen fest, dass CodeScript den hohen Pluralismus in den generierten spezifischen Zielen zeigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für eingeschränkte Sprachplanung behandeln. Wir stellen fest, dass t5, der auf der Punktzahl trainiert wurde, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle große, größere Modelle unterstützen können, wenn sie richtig auf geeigneten Datensites trainiert werden. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert, wir bewerten die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung und entwickeln eine Methode mit einem übergenerierten Filter für Live-Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hochwertigen Datensatz namens CodeScript für eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der Code-Datensatz eine wertvolle Ressource für die Weiterentwicklung der Forschung zur Sprachplanung sein kann. Vielen Dank für Ihre Zeit. Weitere Details zu CodeScript finden Sie in unserem Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Jannislavak und werde Ihnen unsere Arbeiten zu Dr. Bert vorstellen, einem robusten, vorab trainierten Modell in Französisch für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir stellen das erste biomedizinische Modell in Französisch vor, das Dr. Bert heißt und auf Roberta basiert. Es wurde mit nachtchos trainiert, einem Datensatz mit medizinischen Daten, die aus dem Web gesammelt wurden. Wir stellen auch einen Vergleich des Modells mit mehreren kryogenen Einstellungen und Datenquellen vor. Anschließend präsentieren wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch und schließlich ziehen wir unsere Schlussfolgerungen aus den Experimenten und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist Bert zu einem der effektivsten Ansätze zur Lösung von Aufgaben der Verarbeitung natürlicher Sprache geworden und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie war to ve fast text und mehr einen enormen Leistungsgewinn. Seitdem wurde dieses Modell auf viele andere Sprachen wie Französisch mit Camembert und andere Bereiche wie Biomedizin mit Permit Birth und Bio Birth sowie im klinischen Bereich mit Clinical Birth adaptiert. Doch vor allem im Englischen sind spezialisierte Modelle für andere Sprachen selten und basieren oft aufgrund des Mangels an domänenspezifischen Daten auf kontinuierlichem Vorabtraining. Bislang gab es jedoch kein Open-Source-Modell für den biomedizinischen Bereich in Französisch. Wir stellten uns also die Frage, welche Datenquellen für eine breite Anwendung am geeignetsten sind und ob diese Rohdaten eine gute Ersatz für klinische Daten sind. Um diese Frage zu beantworten, verglichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von der nichtuniversitären Klinik in unserem Haus stammen. Anschließend stellten wir uns die Frage, wie viele Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Sind es vier Gigabyte, acht Gigabyte oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir zunächst vier Modelle: eine erste Version von Dr. Bert mit sieben Gigabyte an Nachos, eine zweite Version mit vier Gigabyte an Natureos, eine erste Version von Schubert, einem klinischen Modell mit vier Gigabyte an Sätzen aus klinischen Notizen, und eine endgültige Version von Schubert mit einer Mischung aus vier Gigabyte an Natureos und vier Gigabyte an klinischen Nodes. Zusätzlich zu diesem Vergleich führten wir drei Modelle ein, die auf Kontroll-Vorabtraining trainiert wurden, um die Auswirkungen der Vorabtrainingsstrategie zu analysieren: eines basierend auf dem Gewicht von Camembert und trainiert auf vier Gigabyte an Natureos, ein weiteres, ebenfalls basierend auf Camembert, aber dieses Mal auf den vier Gigabyte an klinischen Daten, und schließlich eines, das auf einem englischen biomedizinischen Modell (Bermedbert) basiert und auf vier Gigabyte an Snatches trainiert wurde. Insgesamt haben wir sieben Modelle zur Bewertung. Wir sammelten öffentliche und private Downstream-Aufgaben wie Namens- und Erkennungs-Klassifizierung, Teil-des-Sprechens-Tagging und Fragebeantwortung. Diese Modelle wurden mit sechs Basismodellen verglichen: Camembert, Oscar, 138 Gigabyte Camembert Oscar, vier Gigabyte Camembert CC Net, vier Gigabyte Plummet Bird, Biobert und Clinical Bird. Die Entwicklung zeigt, dass das Modell, das am besten auf der Aufgabe mit Daten der gleichen Art wie die, auf denen das Modell trainiert wurde, abschneidet, die besten Ergebnisse erzielt. Wir können jedoch feststellen, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir beobachten auch, dass die Verwendung von mehr Daten zu einer besseren Leistung führt. Insgesamt scheinen das Training von Grund auf und das freie Training höhere Leistungen bei den meisten Aufgaben zu erzielen. Unser Experiment zum Verbraucher-Vorabtraining mit dem Gewicht und dem Tokenizer von Permit Bird, trainiert auf dem vier Gigabyte großen Teilmenge von Natureos, zeigt jedoch vergleichbare Ergebnisse wie Dr. Bert 4 Gigabyte von Grund auf, was nicht der Fall ist für das Modell, das auf Camembert-Gewichten und Tokenizer basiert und unter Stabilitätsproblemen leidet. Abschließend bietet unser System eine bessere Leistung bei neun der 11 Downstream-Aufgaben und übertrifft global die Ergebnisse des generischen Modells Camembert. Wir beobachten auch, dass spezialisierte Daten besser sind, aber sie skalieren nicht gut. Alle aus Nachos erhaltenen vortrainierten Modelle sind frei verfügbar und auf unserer Benutzeroberfläche. Alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf Gespräche bei der Postersession in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "- Hallo, ich bin Xhang Bing, Doktorand an der Universität von Washington. Heute stelle ich unsere Arbeit vor, von Vordar-Daten über Sprachmodelle bis hin zu nachgelagerten Aufgaben, und verfolgen die Spuren politischer Voreingenommenheiten, die zu unfairen NLP-Modellen führen. Sprachmodelle werden also auf großen Web-Crawldaten trainiert. Politische Nachrichtenmedien sind in ihren Vordar-Daten gut vertreten. Laut einer Umfrage des c4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, der Guardian, die Huffington Post usw. in den Trainingsdaten für Sprachmodelle gut vertreten sind. Dies hat für Sprachmodell-Anwendungen sowohl einen Vorteil als auch einen Nachteil. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was Demokratie und eine Vielfalt von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairness-Problemen in nachgelagerten Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der Weitergabe politischer Voreingenommenheiten von Vordar-Daten über Sprachmodelle bis hin zu nachgelagerten Aufgaben zu untersuchen, indem wir uns folgende Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen die Trainingsdaten bei solchen politischen Voreingenommenheiten? Zweitens, wie schneiden Sprachmodelle mit unterschiedlicher politischer Ausrichtung bei nachgelagerten Aufgaben ab und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu befragen, indem wir politische Fragebögen wie den Political Compass Test verwenden. Dies stellt sicher, dass wir eine automatische Bewertung durchführen können, die in der politikwissenschaftlichen Literatur verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie besetzen alle vier Quadranten auf dem politischen Kompass. Wir können auch sehen, dass GPT4 das liberalste Sprachmodell von allen ist und die GPT-Serie im Allgemeinen sozialliberaler ist als die Bird-Serie und ihre Varianten. Zweitens zielen wir darauf ab, zu untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Daher könnten wir ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints auf sechs verschiedenen parteiischen Korpora weiter trainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter nach ihrer politischen Ausrichtung unterteilt werden. Durch das weitere Training von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel, für Roberta, das weiter auf dem linksgerichteten Reddit-Korpus trainiert wurde, können wir einen erheblichen liberalen Wandel in Bezug auf seine politischen Voreingenommenheiten sehen. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufgreifen können, die in unserer modernen Gesellschaft vorherrscht. Daher teilen wir die Vordar-Korpora in die Zeit vor und nach dem 45. Präsidenten der Vereinigten Staaten auf und trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufgreifen können. Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei der Erkennung von Hassrede und Falschnachrichten, was NLP-Anwendungen betrifft, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Medien unterteilen, wir ein Muster sehen können. Zum Beispiel sind bei der Erkennung von Hassrede linksgerichtete Sprachmodelle besser darin, Hassrede zu erkennen, die sich gegen sozial benachteiligte Gruppen richtet, sind jedoch schlechter darin, Hassrede zu erkennen, die auf mehr Macht für Gruppen in unserer Gesellschaft abzielt, und umgekehrt. Rechtsgerichtete Sprachmodelle sind besser darin, Hassrede zu erkennen, die sich gegen Weiße und Männer richtet, jedoch schlechter darin, Hassrede zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richtet. Ähnliche Trends finden sich auch bei der Erkennung von Falschnachrichten, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Desinformation von ihrer entgegengesetzten politischen Ausrichtung zu erkennen und umgekehrt. Dies zeigen wir weiter mit vielen qualitativen Beispielen, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen unterschiedliche Vorhersagen zu Hassrede und Desinformationsbeispielen aufgrund ihrer sozialen Kategorien treffen. Im Anhang gibt es eine Reihe von weiteren Beispielen, die verdeutlichen, dass dies auf ein sehr drängendes Fairness-Problem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen hinweist. Zum Beispiel, wenn ein rechtsgerichtetes Sprachmodell auf Hassrede oder Desinformation usw. weiter trainiert und auf einer beliebten Social-Media-Plattform eingesetzt würde, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und die Hassrede, die sich gegen Minderheitengruppen richtet, ungehindert wüten könnte. Dies hat bei uns den Alarm ausgelöst, die Fairness-Probleme anzuerkennen und anzugehen, die durch die politischen Ausrichtungen von Sprachmodellen entstehen. Also, ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten für Sprachmodelle nicht sanieren, würde sich die Voreingenommenheit von den Vordar-Daten über Sprachmodelle bis hin zu nachgelagerten Aufgaben fortpflanzen und letztendlich Fairness-Probleme schaffen. Wenn wir versuchen, irgendwie zu sanieren, würden wir auch Zensur oder Ausschluss riskieren, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten beibehalten werden sollte. Es ist also irgendwie das Elektroly-Trolley-Problem. Okay, großartig, ich denke, das ist so ziemlich alles, was ich für heute habe, fünf Minuten für heute. Danke für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Ich bin Koov Sinna und ich freue mich, Sie zu unserem Vortrag über unseren ACL 23-Artikel willkommen zu heißen. Akzeptanzurteile von Sprachmodellen sind nicht immer kontextrobust. Dies ist eine gemeinsame Arbeit mit John Waqui, Aaron Mueller, Kanishka Mishra, Karen Fs, Roger Levy und Atina Williams. In dieser Arbeit nehmen wir das Minimalpaar-Paradigma erneut unter die Lupe. Das Minimalpaar-Paradigma bewertet also im Wesentlichen Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität wie z. B. Blimp-Syntax-Gym oder Akzeptanz in Bezug auf Stereotypen wie Menschenmassenpaare umfassen können. Und in diesem Minimalpaar-Paradigma besteht die typische Methode zur Bewertung von Sprachmodellen darin, dass man ein akzeptables oder ein grammatikalisches Satzbeispiel zeigt und dann einen inakzeptablen oder einen ungrammatischen Satz zeigt, und dann hofft man, dass das Modell im Grunde der akzeptablen Aussage eine höhere Wahrscheinlichkeit beimisst. Die aktuelle MPP-Pipeline erlaubt uns heutzutage nicht, die Akzeptanz von Modellen für längere Sätze zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern, daher ist es entscheidend, dass wir die Akzeptanz der Modelle im gesamten Kontextfenster bewerten, und das ist es, was wir hier zu tun versuchen. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell bitten, die Akzeptanz für immer längere Sequenzen zu bewerten. Das ist der Ansatz. Was wir also tun, ist, dass wir diese längeren Sequenzen simulieren, indem wir die Datensätze selbst erneut betrachten und dann Sätze neu erstellen, indem wir aus diesen Datensätzen akzeptable oder inakzeptable Sätze auswählen. Zum Beispiel haben wir hier ein typisches Paar von grammatikalischen Sätzen aus dem Blimp-Datensatz aus dem Adjunct Island-Fall ausgewählt, und was wir tun, ist, dass wir längere Sequenzen neu erstellen, die akzeptabel sind und die dieselbe grammatikalische Struktur aufweisen. Wir extrahieren grammatikalische Sätze aus Adjunct Island und fügen sie dann als Präfix sowohl zur akzeptablen Abfrage als auch zur inakzeptablen Abfrage hinzu, damit wir dasselbe tun können, indem wir inakzeptable Sätze aus dem gleichen Match auswählen, und das könnte auch verwendet werden, um die Akzeptanz der Modelle zu testen. Wir können dasselbe tun, indem wir Sätze aus einem anderen Unterdatensatz oder einem anderen Datensatz auswählen. Das ist es, was wir als Mismatch-Szenario bezeichnen. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber es ist nicht aus dem gleichen Datensatz, mit dem Sie evaluieren, und wir können dasselbe für den Fall der Unannehmlichkeit tun. Schließlich können wir Sätze aus einem völlig unverbundenen Bereich wie Wikipedia auswählen. Das wird uns sagen, ob die Akzeptanzurteile der Modelle tatsächlich von einem Kontext beeinflusst werden, ob der Kontext aus einem anderen Unterdatensatz stammt oder ob er völlig irrelevant für den aktuellen Kontext ist, den wir betrachten. Wie schneidet das Modell also ab? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind, und dort stellen wir fest, dass die MPP-Urteile für beliebige Kontextlängen meist robust sind. Wir erhöhen die Kontextlänge auf bis zu tausend und vierundzwanzig, um die Ot- und GPT-2-Modelle auszulasten, und hier sehen wir in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert nun, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen oder erstellen wir Sätze aus akzeptablen und inakzeptablen Domänen aus demselben Blimp-Syntax-Gym-Datensatz, und dort sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn man entweder akzeptable oder inakzeptable Präfixe hinzufügt, aber wenn wir die Struktur abgleichen, das heißt, wenn wir die Sätze aus denselben Phänomenen in der Blame-Person-Syntax auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des PP-Urteils für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Nun, dieser Effekt nimmt im Laufe der Kontextlänge zu, und das würde wahrscheinlich neuere Sprachmodelle beeinflussen, die ein großes Kontextfenster haben. Warum beeinflusst das Match-Präfix das Urteil des Sprachmodells so sehr? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabensatz zu stören, indem wir versuchten, die relevante Struktur zu bewahren, aber Rauschen zum Input hinzuzufügen. Nachdem wir mehrere dieser Störungen durchgeführt haben, stellen wir fest, dass keines dieser Rauschsignale das Modell tatsächlich dazu bringt, seinen Kurs in Bezug auf die MPP-Urteilentwicklung zu ändern. Im Grunde stellen wir fest, dass die Modelle auf ähnliche Weise auf die gestörten Sätze reagieren, das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir die Sätze im akzeptablen Bestätigungsbereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind also, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in den Sätzen gemeinsam sind. Und die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satzeingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unseren Artikel für weitere Details unserer Experimente. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Doktorand an der Staland Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen, „Schwacher als Sie denken“, einen kritischen Blick auf das wöchentlich überwachte Lernen. Dies ist eine gemeinsame Arbeit mit X, Myos Mosbach und Ge Steffen sowie Dirich Klako. Ich möchte mit einer kurzen Einführung in die schwache Überwachung und das schwach überwachte Lernen beginnen. Bei der schwachen Überwachung kennzeichnen wir die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mit schwachen Kennzeichnungsquellen wie einfachen heuristischen Regeln, Wissensbasen oder Crowd-Sourcing mit geringer Qualität, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen viel billiger, aber sie sind auch verrauscht, was bedeutet, dass eine bestimmte Menge der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf wöchentlich gekennzeichneten Daten trainieren, neigen die neuronalen Netze dazu, den Kennzeichnungslärm zu verinnerlichen und verallgemeinern nicht im wöchentlich überwachten Lernen. Es werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Kennzeichnungslärm zu trainieren, sodass die trainierten Modelle sich weiterhin gut verallgemeinern. In jüngsten Arbeiten zum wSL, wobei wSL für wöchentlich überwachtes Lernen steht, ist ein gängiger Anspruch, dass Menschen sagen, dass sie nur Modelle auf den wöchentlich gekennzeichneten Daten trainieren und hohe Leistung auf sauberen Testdatensätzen erzielen. Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt einen Haken, nämlich dass Menschen davon ausgehen, dass es einen zusätzlichen sauberen Validierungssatz gibt. Für die Modellselektion haben wir uns von dieser Problemstellung abgewandt, aber dies impliziert, dass zusätzliche manuelle Annotationen im wöchentlich unterstützten Lernen erforderlich sind. Wie ein Elefant im Raum wird diese Notwendigkeit jedoch oft übersehen. Die oben genannte Annahme wird in Frage gestellt, indem drei Forschungsfragen gestellt werden. Erstens: Sind saubere Validierungsdaten für wSL notwendig oder können wir stattdessen einen anno-Validierungssatz verwenden? Zweitens: Wenn saubere Daten erforderlich sind oder wenn saubere Daten zwingend notwendig sind, damit wSL funktioniert, wie viele saubere Proben benötigen wir dann? Drittens: Sollten wir die sauberen Proben nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt und unsere Ergebnisse sind wie folgt: Erstens stellen wir fest, dass interessante jüngste wSL-Methoden tatsächlich saubere Validierungsproben benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall, wie in dieser Abbildung gezeigt. Wenn es keine sauberen Validierungsproben gibt, können die Trendmodelle nicht über die ursprünglichen schwachen Kennzeichnungen hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass wSL-Ansätze tatsächlich sauber gekennzeichnete Daten benötigen, um richtig zu funktionieren, und die Annotationskosten für die Beschaffung sauberer Validierungsproben sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl der sauberen Validierungsproben wSL-Ansätzen helfen wird, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns entscheiden, auf saubere Proben zuzugreifen, wird das direkte Training auf ihnen sogar eine bessere Leistung erzielen. Die rote Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt unter sauberen Daten angewendet werden, und wSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir sehen können, wenn wir zehn Proben pro Klasse haben, beginnt das direkte Feinabstimmen, wSL-Ansätze zu übertreffen. Schließlich kann die in früheren wSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem die Möglichkeit gegeben wird, das Feinabstimmen auf sauberen Validierungsproben fortzusetzen. Wie wir den Abbildungen entnehmen können, unterlebt das Validierungsmodell namens FTW zunächst komplexeren wSL-Methoden wie Cosine. Wenn wir jedoch das Fortsetzen des Feinabstimmens unter sauberen Proben erlauben, dann erreicht FTW eine gleich gute Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere wSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass jüngste wSL-Ansätze saubere manuell annotierte Proben benötigen, damit sie richtig funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt: Erstens, berichten Sie über die Kriterien für die Modellselektion, zum Beispiel, ob die Modellselektion gut durchgeführt wurde und saubere Validierungsproben verwendet wurden. Zweitens, wSL-Ansätze sollten mit wenigen kurzen Basislinien verglichen werden, wie zum Beispiel die Arbeit an konkreten Proben. Drittens, kontinuierliches Feinabstimmen ist eine einfache, aber starke Basislinie, die in zukünftigen Arbeiten im wSL berücksichtigt werden sollte. Schließlich haben wir unseren Code als Open Source veröffentlicht. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Al Villaard und ich werde einen kurzen Überblick über den Artikel geben, der sich mit der Aufforderung von P aus der Übersetzung befasst und Strategien und Leistung bewertet. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Pm ist ein Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die zum Zeitpunkt der Veröffentlichung 780 Milliarden Token umfassen. Es erreicht den neuesten Stand der Technik bei hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir eine freie systematische Studie über die Aufforderung von großen Sprachmodellen für die maschinelle Übersetzung. Wir bewerten die Übersetzungsfähigkeit solcher Modelle unter Verwendung der bewährten Praktiken der IMT-Community. Dies beinhaltet die Verwendung der neuesten Testsätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden, und wir vergleichen zwei modernste Systeme. Die leistungsstärksten Systeme der WMT-Evaluation verwenden wir modernste NeuralMT-Metriken und zeigen zusätzlich auch Ergebnisse der fachkundigen menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für Strategien zur Auswahl von Aufforderungen. Die Aufforderung hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine kurze Aufforderung verwenden und zwei verschiedene Aufforderungen für zwei verschiedene Sätze bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt der beobachtete Unterschied mehr als einen verschwommenen Punkt. Und dies kann in extremen Fällen bis zu 40 verschwommene Punkte betragen. Es ist also wichtig, eine gute Aufforderungsstrategie auszuwählen. In unseren Experimenten haben wir uns für eine Fünf-Schüsse-Aufforderungsstrategie entschieden, bei der wir einfach seinen Satz markieren, den wir dem System mitteilen, in welcher Sprache er ist. In diesem Beispiel hier, wo wir die Übersetzung vom Deutschen ins Englische durchführen, sind die deutschen Sätze, die Quelltexte, mit einem deutschen Doppelpunkt markiert und die englischen Übersetzungen mit einer englischen Spalte. Wir haben festgestellt, dass die tatsächliche Form des Drucks im Falle mehrerer kurzer Aufforderungen keinen großen Einfluss hat. Es ist entscheidend für Null- und Ein-Schuss-Aufforderungen, aber wenn wir wie in unserem Fall zu kurzen Aufforderungen übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form der Aufforderung. Es sind die Beispiele, die den größten Teil des Gewichts tragen. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit zur Quelllsatz. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Aufforderungen aus den Trainingsdaten der WMT-Evaluierungen oder den Dev-Daten. Die Dev-Daten sind viel kreativer und von höherer Qualität als die Trainingsdaten, und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte modernste Systeme einen erheblichen Vorteil gegenüber den Palm-Übersetzungen, aber Palm kommt jetzt ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren. Die Erkenntnisse, die wir aus der Simulation gewonnen haben, die wir mit dem NpN-Framework durchgeführt haben, sind, dass die Fließfähigkeit von Palm mit den modernsten Systemen vergleichbar ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Omissionsfehler. Es scheint also, dass Palm eine bessere klingende Übersetzung produziert, indem es manchmal Teile des Quellsatzes weglässt, die in der Übersetzung gemacht werden. Der Stil nach außen ist jedoch für Pan niedriger als für die modernsten Systeme, was ein zusätzliches Signal dafür ist, dass Par wirklich fließenden Output liefert, aber immer noch mit einigen Genauigkeitsproblemen. Das war dieser wirklich kurze Überblick. Für weitere Details kommen Sie bitte zu meiner vollständigen Präsentation des Artikels. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, alle zusammen. Mein Name ist Jin Wei Y von der Universität für Wissenschaft und Technologie in China. Es ist mir eine Freude, ein kurzes Werbevideo für unsere Arbeit vorzustellen. Kopieren Sie mein Modell, zum Schutz des Urheberrechts großer Sprachmodelle für Einbettungen und Dienste? Wir unterstützen das Wasserzeichen. Lassen Sie uns zunächst den Hintergrund zu Einbettungs- und Diensten vorstellen. Derzeit sind große Sprachmodelle wie Gbt, La und PLm außergewöhnlich in Bezug auf das Verständnis und die Generierung natürlicher Sprache. Einbettungsdienste sind einer der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenI eine auf Gbt basierende Einbettungs-API. Neuere Arbeiten haben jedoch gezeigt, dass Angreifer das Modell durch Lernen aus der Einbettung stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht an Einbettungen als Dienste zu schützen. Eine der Lösungen besteht darin, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen: Erstens sollte die Methode auf Einbettungen als Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettung nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer konvertiert werden können oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen auf die Dienste des Angreifers während des Modellauszugsprozesses übertragbar sein. Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methoden sind jedoch entweder nicht auf Einbettungen als Dienste anwendbar oder sie weisen eine mangelnde Übertragbarkeit auf. Daher schlagen wir in dieser Arbeit einen Einbettungsmarker vor, der eine auf einer Backdoor basierende Wasserzeichenmethode ist, die auf Einbettungen als Dienste anwendbar ist. Lassen Sie mich nun die Details unseres Einbettungsmarkers vorstellen. Einbettungsmarker enthält zwei Hauptschritte: Wasserzeichen-Injektion und Urheberrechtsprüfung. Bevor wir zu diesen Hauptschritten übergehen, wählen wir zunächst einen Auslöser. Der Auslöser ist eine Gruppe von Wörtern in einem mittleren Häufigkeitsintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeichen-Injektion definieren wir zunächst eine Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Die bereitgestellte Einbettung ist eine Gewichtungssumme der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsprüfung besteht darin, zu erkennen, ob das Modell hinter einem anderen Dienst das Wasserzeichen enthält. Zunächst erstellen wir eine Backdoor und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Auslöser gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Auslöser gehören. Dann fordert der Anbieter Einbettungen von dem anderen Dienst mit dem Datensatz an. Die Kosinus- und l2-Ähnlichkeit zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem gutartigen und dem Backdoor-Datensatz, die als Delta-Kosinus und Delta-l2 definiert ist. In der Zwischenzeit wenden wir auch den KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente an vier Datensätzen durch: Aaging, News, Mind, SD2 und A Spam. Wir nehmen an, dass der Anbieter Wiki-Textdatensätze anwendet, um die Wortfrequenz zu zählen. Die Ergebnisse an den vier Datensätzen zeigen, dass unser Einbettungsmarker eine hervorragende Erkennungsleistung erzielen kann, während er eine große Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Konvertierbarkeit der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen an vier Datensätzen B PCA visualisieren. Die Legende der Abbildungen zeigt die Anzahl der Auslöser in jedem Satz, wie in den Abbildungen dargestellt. Es ist schwierig, zwischen den Faktor-Einbettungen und den normalen Einbettungen zu unterscheiden. Das war's. Vielen Dank. Wir freuen uns auf die Diskussion mit Ihnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Mein Name ist Ian und mein Kollege Jiian und ich werden unsere Forschung über multi-instrucct vorstellen, die die multimodalen seriellen Lernprozesse durch Anpassen der Anweisungen verbessert. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, bei denen protrain-Sprachmodelle für verschiedene nachgelagerte Aufgaben auf eine parameter- und dateneffiziente Weise verwendet werden. Kürzlich haben viele Studien gezeigt, dass das Anpassen von Anweisungen großen Sprachmodellen ermöglicht, auf nicht gesehene Aufgaben auf eine serielle Weise zu reagieren, indem sie natürlichen Anweisungen folgen. Die meisten früheren Arbeiten zum Anpassen von Anweisungen konzentrieren sich jedoch auf die Verbesserung der serielle Leistung bei Aufgaben, die nur Sprache betreffen, wobei Computer Vision und multimodale Aufgaben weggelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob das Anpassen von Anweisungen an multimodalen Proteinmodellen tatsächlich die Verallgemeinerung auf nicht gesehene multimodale Aufgaben verbessern kann. Zusätzlich haben wir während unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen P und multimodal festgestellt. Es gibt mehr als 1.600 Anweisungsaufgaben, die nur Sprache betreffen, aber es gibt keine groß angelegte öffentlich zugängliche multimodale Anweisungsaufgabe. Dies motiviert uns, einen multimodalen Anweisungsdatensatz aufzubauen. Hier stellen wir multi-ins instruct vor, den ersten multimodalen Anweisungsdatensatz für Benchmark, der aus 62 verschiedenen multimodalen Aufgaben besteht, die 10 Hauptkategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um die multimodale Anweisungsanpassung an unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir ein einheitliches multimodales Trainingsmodell als unser Basismodell. Wir verwenden ein einheitliches Vokabular für Sprach- und Bild-Token sowie die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem multi-instra-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von offa und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format, in dem Texteingaben, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt werden. Okay, jetzt werde ich über die multimodale Anweisungsanpassung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus der N-Gruppe für das Training und wir nehmen 10.000 Instanzen pro Aufgabe für die Tests. Wir behalten die gesamte Gruppe für das Lesen des allgemeinen Wissens für die Tests und wir wählen zusätzlich fünf Aufgaben aus Wiki und der verschiedenen Gruppe aus. Wir verwenden alle Instanzen in der Testspeed für jede Aufgabe. Zusätzlich nehmen wir zufällig 20 Aufgaben aus der Testspeed der natürlichen Anweisung als NP zu. Wir verwenden ein vortrainiertes großes Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen auswerten. Wir berichten die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Wenn es sich um eine multimodale Klassifizierungsaufgabe handelt, berichten wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir die Wurzel-L für eine LP-Aufgabe berichten wir ebenfalls die Wurzel-L. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt. Diese misst die Fähigkeit des Modells, konsequent dieselben Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann das Anpassen von Anweisungen die Leistung von OFF bei denselben multimodalen Aufgaben erheblich verbessern. Auch das Transfer-Learning aus dem Datensatz natürlicher Anweisungen kann das Anpassen von Anweisungen unterstützen. Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung und gleichzeitig eine geringere Sensitivität erreicht. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegenüber fünf Anweisungen verwendet haben. Wie wir sehen können, kann die Verwendung von mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Auswirkungen verschiedener Anweisungsanpassungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transfer-Learning aus Datensätzen natürlicher Anweisungen eine viel bessere Sensitivität im Vergleich zum ursprünglichen IFA-Modell erreichen. Wir können auch sehen, dass das Transfer-Learning aus Datensätzen natürlicher Anweisungen OFA helfen kann, eine viel bessere Leistung auf dem nitrogen instruct-Datensatz zu erzielen. Insgesamt haben wir den ersten groß angelegten multimodalen Anweisungsdatensatz vorgeschlagen. Wir haben die neuronale Fähigkeit von OFFA erheblich verbessert und verschiedene Techniken des Transfer-Learnings erforscht und gezeigt, dass es Vorteile gibt. Wir haben eine neue Metrik namens Sensitivität entwickelt. Noch eine Sache: Wir sammeln einen viel größeren multimodalen Anweisungsdatensatz mit rund 150 zusätzlichen varianten Sprachaufgaben und werden sie veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Mein Name ist Just John von der Penn State University. Heute werde ich unsere Arbeit vorstellen, Ex examplelar: Cross-lingual Semantic Parsing in mehreren natürlichen Sprachen und manuellen Darstellungen. Semantisches Parsing ist also die Aufgabe, semantische Darstellungen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Und cross-linguales semantisches Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen, wie in dieser Abbildung gezeigt. Wir müssen die Anfrage in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder Funql usw. übersetzen. Bestehende cross-linguale semantisches Parsing-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Anwendungen und Beispielen bewertet. Beispielsweise gibt es Lücken bei der Abdeckung bestimmter natürlicher Sprachen, wie Chinesisch fehlt, und es gibt Lücken bei der Abdeckung bestimmter Mini-Darstellungen, wie das Fehlen des Lambda-Kalküls, oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Beispielsweise gibt es nur ein einziges Modell zur Bewertung. Zu diesem Zweck schlagen wir exampler vor, aber stellen einen einheitlichen Datensatz exampler für cross-linguales semantisches Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen zur Verfügung. Er enthält neun Datensätze in Virus-Bereichen, fünf semantisches Parsing-Steuern, 8 Millionen Darstellungen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unseren Benchmark besser bewerten zu können, betrachten wir die sechs Einstellungen für Training und Bewertung. Die erste ist der Übersetzungstest. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein einsprachiges Modell zur Schulung und Bewertung. Und zum Beispiel schulen wir das englische Modell auf einer englischen Anfrage und während der Inferenz übersetzen wir die deutsche Anfrage mit der API ins Englische und verwenden dann das geschulte Modell, um die SQL vorherzusagen. Und wir testen auch ein einsprachiges Modell. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die Einstellung der einsprachigen Fusion, indem wir einsprachige Modelle mit nur 10 Prozent der Trainingsdaten schulen und wir testen die Modellierung eines mehrsprachigen Modells, das wir für alle Sprachen schulen. Zum Beispiel bringen wir die deutschen, englischen und chinesischen Anfragen zusammen, um ein mehrsprachiges Modell zu schulen, und während der Inferenz können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen usw. zu übersetzen. Wir betrachten auch den cross-lingualen Zero-Shot- und Few-Shot-Transfer. Wir schulen auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es auf einer englischen Anfrage oder der Kombination aus englischer und deutscher Fu-Shot-Anfragen, um ein mehrsprachiges Modell zu schulen und die SQL-Ausgabe vorherzusagen. Und wir finden auch viele interessante Ergebnisse. Was die Analyse einsprachiger Modelle betrifft, bewerten wir zwei Gruppen von Modellen, einschließlich Encoder PDR, was für mehrsprachige vortrainierte Encoder mit zeigerbasierten Decodern wie XLr plus PDdR und Bird plus PDdR steht, und wir bewerten auch Encoder-Decoder-Modelle, die mehrsprachige vortrainierte Encoder-Decoder-Modelle sind, wie MBt und Mt5. Wir fanden heraus, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Und wir bewerten auf MT5 und XLMR plus PDR eine mehrsprachige Einstellung. Wir fanden heraus, dass Encoder-Decoder oder Encoder PDR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Und wir fanden heraus, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, mit der Ausnahme, dass die englische Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen steigt. Ich denke, dies ist als Kurven der Multilingualität bekannt. Wir verglichen auch die cross-linguale Leistungslücke. In dieser Abbildung ist die blaue Linie der cross-linguale Few-Shot-Transfer. Die orange Linie ist der cross-linguale Zero-Shot-Transfer, während die grüne Linie die einsprachige Einstellung darstellt. Wir fanden heraus, dass durch den Vergleich der grünen und der orangen Linie, dass die Zero-Shot-Einstellung, die cross-linguale Transferleistung signifikant ist und durch den Vergleich der blauen und der orangen Linie fanden wir heraus, dass mit der Few-Shot-Einstellung die Transferlücke schnell verkürzt wird. Wir fanden auch einige andere interessante Ergebnisse. Zum Beispiel übertrifft der Encoder-Decoder die Fortschrittsarbeit oder erreicht vergleichbare Ergebnisse. Das Training unserer englischen natürlichen Sprache kann die Leistung von Few-Shot auf Ziel-natürlichen Sprachen erheblich steigern. Und wir fanden, dass mehrsprachige Sprachmodelle wie Coders und Blue immer noch unzureichend für cross-linguales semantisches Parsing sind. Zusammenfassend haben wir exampler, einen einheitlichen Benchmark für cross-linguales semantisches Parsing mit mehreren natürlichen Sprachen und Bedeutungsdarstellungen, aufgebaut. Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durch, und unser Ergebnis zeigt viele interessante Erkenntnisse usw. Und willkommen, unseren Artikel und Code zu besuchen. Danke fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Shrikkovski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So wird beispielsweise in den universellen Abhängigkeiten die Struktur der Koordination Lisa, Bart und Maggie so angenommen, dass der erste Konjunkte der Kopf der gesamten koordinierten Struktur ist. In diesem Fall ist Lisa der Kopf. Ein ähnlicher Ansatz wird in Igor Milchuks Bedeutungstheorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkten angeführt wird. Diese beiden Ansätze sind also asymmetrisch, da sie einen der Konjunkten hervorheben. Es gibt aber auch symmetrische Ansätze zu koordinierten Strukturen, wie der Prag-Ansatz oder der Konjunktionskopf-Ansatz, der in den Prag-Abhängigkeitsbaumdatenbanken angenommen wird, wo koordinierte Strukturen vom Konjunkten angeführt werden. Wir erhalten also Abhängigkeiten von Ende zu allen Konjunkten. Schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in der Wortgrammatik von Cutson verwendet wird, wo sozusagen alle Konjunkten Köpfe der koordinierten Struktur sind. Wir erhalten also Abhängigkeiten vom Regierenden zu allen Konjunkten separat. Das sind die Knöpfe, die jetzt gemacht werden. Das Ziel dieses Artikels ist es, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu liefern. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. Im Englischen bevorzugen unsere direkten Objekte, wie Sie vielleicht wissen, eine Position nahe am Verb, während Adjunkte weiter entfernt sein können. Also ist „March read it yesterday“ in Ordnung, weil das direkte Objekt „it“ nahe am Verb steht, während „March read yesterday“ viel schlechter ist, weil hier zwischen Verb und direktem Objekt ein Adjunct „yesterday“ steht. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, da es dann in die Position nach dem Adjunct bewegt werden kann. Dies wird hier veranschaulicht. Beide Sätze sind also in Ordnung: „March read this absolutely fascinating book about the BC yesterday“ ist okay, und „March read yesterday this absolutely fascinating book about bees“ ist auch okay. Das Argument hier ist, dass dies möglich ist, weil dieser Satz zwar das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also die, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir die Abhängigkeit von „read“ zum Adjunct mit einer Länge von sieben Wörtern und von „read“ zum Buch mit einer Länge von vier Wörtern. Zusammen sind es 11. Wenn man diese beiden Konstituenten vertauscht, wird die Summe dieser beiden Abhängigkeiten sechs. Statt 11 also sechs, viel kürzer. Deshalb klingt das ganz okay. Es verletzt ein Prinzip, erfüllt aber ein anderes. Was wir getan haben, ist, verschiedene Statistiken zur Koordination aus der erweiterten Version der Penn Treebank zu extrahieren und zu sehen, warum wir keine University Dependencies verwendet haben. Diese Statistiken bestätigen die bereits oft gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind. Also „salt and pepper“ und nicht „pepper and salt“, gemessen in Silben. Auch die Beobachtung, die beiläufig gemacht wurde, dass diese Tendenz mit der Länge wächst, also wenn der Unterschied zwischen den Längen der beiden Konjunkte wächst, der kürzere Konjunkte lieber der erste sein möchte. Die Proportion der linken kurzen Konjunkte ist also größer. Was neu ist in diesem Artikel, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regierende auf der linken Seite ist. Der Regierende ist in diesem Beispiel „I saw Bart and Lisa“, also ist der Regierende auf der linken Seite. Er ist abwesend im zweiten Beispiel „Homer came and sneezed“. Hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regierenden. In solchen Fällen bevorzugt der linke Konjunkte es, kürzer zu sein, je größer der Unterschied zwischen den beiden Konjunkten ist. Wenn jedoch der Regierende auf der rechten Seite ist, wie hier, regiert der linke die Koordination des Schwanzes und des Netzes, verschwindet dieser Effekt. Das zeigen wir, indem wir die Länge in Zeichen messen, die erste Spalte, in Silben die mittlere Spalte und in Wörtern die rechte Spalte. Ich werde mich auf die rechte konzentrieren. Was wir hier sehen, ist, dass wenn der Regierende auf der linken Seite ist, die Tendenz des linken Konjunkten, kürzer zu sein, stetig mit dem absoluten Unterschied in Wörtern wächst. Das gleiche wird beobachtet, wenn es keinen Regierenden gibt, wie bei der Koordination von Sätzen. Wenn jedoch der Regierende auf der rechten Seite ist, verschwindet diese Tendenz. Wir zeigen im Artikel, wie dies ein Argument gegen die asymmetrischen Strukturen der Koordination wie diese beiden und für die symmetrischen Strukturen wie diese beiden liefert. Sehen Sie sich den Artikel für die vollständige Übereinstimmung und Argumente an, und sprechen Sie mit uns über die Postersession. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "- Hallo, mein Name ist Kyo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext: Eine datengesteuerte mehrsprachige Ausdruck?“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Ferange, Emiliu, Andre F.D Martins und Graham Newbiig durchgeführt. Viele Übersetzungen hängen also vom Kontext ab. Zum Beispiel, wie würden wir „Mole“ in diesem Satz übersetzen, wenn der vorherige Satz lautete „Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden, dann bezieht sich „Mole“ auf einen Spion, aber wenn der vorherige Satz lautete „Könnte es etwas Ernstes sein, Doktor, dann bezieht sich „Mole“ auf einen Muttermal. Abhängig vom Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch ziemlich schwierig, erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was bedeutet, dass Metriken auf Korpusniveau wie BLEU diese Übersetzungen nicht erfassen können. Einige Leute haben eine gezielte Bewertung von kontext-abhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontext-abhängigen Übersetzungen und begrenzte Sprachsätze, da sie normalerweise auf Domänenkenntnissen und menschlicher Kuratierung basieren. In dieser Arbeit versuchen wir, diese zwei Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir begonnen, zu messen, wie sehr ein Werk in der Übersetzung vom Kontext abhängt. In der vorherigen Arbeit haben wir cxmi als Maß für die Kontextnutzung durch maschinelle Übersetzungmodelle eingeführt. Dies geschieht, indem gemessen wird, wie viel Information der Kontext C über das Ziel y unter Berücksichtigung der Quelle x liefert. Man kann sich cxmi als die Information vorstellen, die durch das Geben von Kontext an das Modell gewonnen wird. In dieser Arbeit erweitern wir cxmi zu p6mi, das die Kontextnutzung auf Satzebene oder auf Wortniveau messen kann. Wir können Wörter mit hohem p6mi als solche betrachten, die für die Übersetzung Kontext erfordern. Jetzt analysieren wir Wörter mit hohem p6mi, um Muster zwischen diesen Wörtern zu suchen, und wir führen unsere Analyse auf Transkripten von TED-Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch: Zuerst betrachten wir Teile des Sprachgebrauchs, die hohe p6mi-Mittelwerte haben, was es uns ermöglicht, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe p6mi-Mittelwerte haben, und dies kann erklärt werden, weil Englisch keine Dualpronomen hat, sodass man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ähnlich finden wir, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die passende Verbform wählen möchten. Dann betrachten wir Vokabularartikel, die über alle ihre verschiedenen Vorkommen hinweg hohe p6mi-Mittelwerte haben, was uns hilft, Fälle wie den hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man innerhalb des Dokuments dieselbe Übersetzung verwendet. Ähnlich finden wir, dass Kontext unterstützt, um in der richtigen Form zu übersetzen, und schließlich betrachten wir verschiedene einzelne Token, die hohe p6mi haben, was es uns ermöglicht, Phänomene zu identifizieren, die vom Wort selbst nicht wirklich erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z.B. die Auflösung von Ellipse. Jetzt verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Übersetzung auf Dokumentebene zu entwerfen. Für jedes der fünf von uns identifizierten Diskursphänomene haben wir Tagger erstellt, um Wörter, die zum Phänomen gehören, automatisch zu identifizieren. Wir nennen unseren Tagger den multilingual discourse aware oder muda Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben. Dann verwenden wir den muda Tagger, indem wir den Tagger auf einem Parallelkorpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere bevorzugten Übersetzungsmetriken auf die vom muda Tagger identifizierten kontext-abhängigen Beispiele an. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokumentebene der maschinellen Übersetzung zu bewerten. Zunächst einmal, wenn wir Metriken auf Korpusniveau verwenden, also für BLEU, stellen wir fest, dass kontext-agnostische Modelle die beste Leistung haben. Aber dann, wenn wir commentt verwenden, erzielen kontextbewusste Modelle die beste Leistung. Und wenn wir das wordf Maß verwenden, dann haben Modelle mit oder ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem auf Dokumentebene zu bestimmen, wenn wir nur Metriken auf Korpusniveau verwenden. Jetzt verwenden wir den muda Benchmark, um Modelle zu bewerten, und wir stellen fest, dass kontextbewusste Modelle signifikant genauer sind als Modelle, die keinen Kontext für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion verwenden, aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipsepronomen und Verbformen verwenden. Dies deutet darauf hin, dass wir bei der Übersetzung auf Dokumentebene mehr Fortschritte sehen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass dL in der Regel genauer ist als Google Trans für die Übersetzung auf Dokumentebene. Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und dann verwenden wir unsere Ergebnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder nicht gut bewältigen können und welche Übersetzungssysteme gut in der Übersetzung auf Dokumentebene sind. Vielen Dank für Ihre Aufmerksamkeit, bis bald in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich eure Arbeit zur AnL-Positionalität, Charakterisierung von Designvoreingenommenheiten und Beta-Sätzen von Modellen vorstellen. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santi, Ronin Labrasse, Katharina Reinika und Martin Sapp. Beginnen wir also damit, uns vorzustellen, dass ihr für eine Zeitung arbeitet und Kommentare unter eurem Nachrichtenartikel sichten, um toxischen Inhalt zu entfernen. Ihr könntet sich dabei an eine beliebte API wie die prospective API für die Toxizitätserkennung wenden, und das funktioniert wirklich gut, wenn ihr Carl Jones seid, wo die prospective API in der Lage ist, toxische Instanzen korrekt zu erkennen. Das ist jedoch nicht wirklich der Fall für Didtha Sharma, wo die perspectiveive API wirklich nicht so empfindlich gegenüber beleidigenden Begriffen ist, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für eine Designvoreingenommenheit, bei der wir systematische Leistungsunterschiede der Technologie zwischen Populationen sehen. Designvoreingenommenheiten wie die, die wir gerade zuvor gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Und als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann. Eine Frage, die sich die Leute stellen könnten, ist, ob Datensätze und Modelle Positionalität haben. Wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können so bestimmte Positionalitäten über andere repräsentieren. Frühere Arbeiten haben einige anekdotische Beweise für die Positionalität vorgeschlagen, wie kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen der Modellpositionalität. Diese Arbeiten betrachten jedoch nicht wirklich den Vergleich von Endbenutzern mit den Datensätzen und Modellen selbst. Die Untersuchung der Positionalität von Modellen und Datensätzen wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozialer orientiert werden, und es ist herausfordernd zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind. Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen mit echten Benutzern mit bestehenden Datensätzen und Modellen. Wir tun dies durch unser Framework Nl-Positionalität. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit vielfältigen Annotatoren neu zu annotieren. Wir entscheiden uns dafür, dies zu tun, anstatt die Demografie der ursprünglichen Datensätze zu betrachten, da normalerweise nur wenige Annotatoren jede Instanz annotieren und da Demografie selten gesammelt und geteilt werden. Daher entscheiden wir uns dafür, Datensätze neu zu annotieren, um viele Annotatoren für jede Instanz zu erhalten und um einen reichen Satz an demografischen Daten zu erhalten. Wir nehmen dann die Annotationen nach Demografie und vergleichen sie mit den Modellen und Datensätzen unter Verwendung von Pearsons Korrelationsscore. Unser Framework unterscheidet sich also von der Literatur zur Diskrepanz zwischen Annotatoren, indem es Endbenutzer mit Vorhersagen und Etiketten von Modellen und Datensätzen vergleicht, anstatt nur auf die Einverständnisrate oder die Modellierung der Annotatordarstellungen zu schauen. Unser Framework wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Crowdsourcing-Plattform, früher ein HCI-Kollektiv und Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie Turk, die größtenteils Teilnehmer aus den USA oder Indien haben, und weiter, Lab in the Wild ist immer noch in der Lage, qualitativ hochwertige Daten zu erhalten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist soziale Akzeptabilität. So funktioniert es: Teilnehmer lesen eine Situation aus dem Social Chemistry-Datensatz und schreiben dann auf, wie sozial akzeptabel eine Situation ist. Um in der Studie engagiert zu bleiben, können sie ihre Antworten mit einer KI und anderen vergleichen. Wir haben dann diese Annotationen mit Social Chemistry Delphi und gPT4 verglichen. Wir haben dann eine sehr ähnliche Einrichtung für die Aufgabe der Toxizität und Hassrede-Erkennung repliziert, bei der sie eine Instanz aus Dynah Hate lesen und schreiben, ob sie denken, dass es sich um eine Instanz von Hassrede handelt. Wir haben dann diese Annotationen mit Dyna Hate Perspective API, Rewire API, Hate Roberta und GPT4 verglichen. Unsere Studie und das Ende haben über 16.000 Annotationen von über tausend Annotatoren aus achtundsiebzig Ländern gesammelt. Jetzt sind wir also besser ausgestattet, um zu beantworten, mit wem stimmen NLP-Datensätze und -modelle am meisten überein? Wir stellen fest, dass es in NLP Positionalität gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die gpd four Social Acceptability Analysis stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass Dinah Hate auch am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass es am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung haben. Für gpd4 in der Social Acceptability Task stellen wir fest, dass sie am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung oder eine Graduiertenausbildung haben, und wir stellen dasselbe für Danny Hate fest, wo sie am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung haben. Wenn jedoch Modelle und Datensätze auf spezifische Populationen abgestimmt sind, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger auf nicht-binäre Menschen abgestimmt sind als auf Männer und Frauen. Wir stellen dies in der gPDd4 Social Acceptability Task sowie in der Dina Hate Task Analysis fest. Angesichts der Tatsache, dass es in NP Positionalität gibt, was können wir dagegen tun? Wir haben ein paar Empfehlungen dafür. Die erste ist, alle relevanten Designentscheidungen während des Forschungsprozesses zu dokumentieren. Die zweite ist, NLP-Forschung mit der Linse des Prospectivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakanne-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren, und damit schließen wir unsere Präsentation ab, aber wenn ihr mehr erfahren möchtet, schaut euch gerne unser Dashboard für die aktuellsten Analyseergebnisse und unser Papier an. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit zur Lösung indirekter Differentialausdrücke für die Entitätauswahl sprechen, bei der wir den alternativen Entitätskörper einführen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Philip Radlinsky, Sylvia Parity und Annie Luis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Ich betrachte diese alternative Frage: Meinst du „Easy on Me“ oder „I got a feeling?“ Hier möchte ein Nutzer zwischen einem dieser beiden Lieder wählen. Das Offensichtlichste ist, eine direkte Referenz zu verwenden, zum Beispiel, indem man den Namen des Liedes „Easy on Me“ oder seine Position, die erste, nennt. Manchmal ist jedoch eine indirekte Referenz angemessener, um ein natürlicheres Gespräch zu führen. Dies könnte passieren, wenn sich der Nutzer den Namen des Liedes nicht erinnern kann oder die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Unterschiede. Zum Beispiel das neuere oder das Zeichen, das nicht energetisch ist. Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Bewertung des Entitätsverständnisses von LLM. Uns ist kein öffentlicher Datensatz bekannt, ein öffentlich zugänglicher Datensatz größeren Umfangs für eine solche Aufgabe, daher sammeln wir einen, der drei verschiedene Bereiche abdeckt: Musik, Bücher und Rezepte. Unsere Methodik zur Datensammlung betont die Unformalität unter Verwendung eines Cartoon-Abschluss-Setups. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: „Denk dich an das Lied, das wir gestern gehört haben.“ Damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: „Meinst du Easy on Me oder I got a feeling?“, was die alternative Frage ist. In der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel „das neuere“. Wir stellen die erste und zweite Sprechblase automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Hinweisen pro Bereich ausgewählt. Die zweite, die die alternative Frage ist, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage „Meinst du a oder b“, wobei a und b Beispiele aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir weiter oben in der Liste gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Disambiguierung durchzuführen. Die erste ist einheitlich zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen „They Return“. Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler für Lieder. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie den Namen dieser Entitäten, aber sie wissen nicht unbedingt etwas über die Entitäten selbst. Daher zeigen wir einige Hintergrundinformationen über die beiden Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten dann die Annotatoren, mindestens einige von jedem Lied anzuhören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchresultat für das Lied „Easy on Me“. Für den Bereich Rezepte und Bücher zeigen wir etwas Hintergrundtext von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wieder von Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Bezugsausdrücken zu beschreiben. Zum Beispiel „die mit der Klaviermusik“. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel „die ohne Worte“, nicht „die mit dem 12-jährigen Jungen“ oder „die fiktive“ oder „die aus Aserbaidschan“ und so weiter. Der alternative Datensatz hat 6.000 alternative Fragen in drei Bereichen und 422.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5X-Large-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugang zu genau demselben Hintergrundwissen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, sie liegt bei etwa 92 bis 95 Prozent. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlappenden Hintergrundwissen hat, liegt die Genauigkeit zwischen 82 und 877 Prozent, was realistischer ist. Zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugang zu Entitätsnamen hat, beträgt die Genauigkeit nur 6 Prozent, also gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänengeneralisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
