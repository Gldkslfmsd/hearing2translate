{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di Deplane, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stoden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La ramificazione è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di textificazione, abbiamo bisogno di coppie parallele di testi, ad esempio, di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, potete vedere un paio di frasi allineate in parallelo di una complessa frase tedesca e la sua traduzione odierna in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, quali la sostituzione lessicale, la dilatazione della clausola, la crosseletion, la riordinamento o l'inserimento di elementi."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo ora il nostro nuovo corpus di dati perché negli ultimi anni ci sono stati alcuni problemi con il corpus esistente, quindi, per esempio, questi dati qui sono troppo piccoli per addestrare un modello di taxonificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nell'allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo il nostro nuovo corpus D planee, che è suddiviso in due sottocorpus, Dplane APA e Dplane web. D planee APA si basa su testi di uso."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In Depla APA, abbiamo allineato 483 documenti tutti manualmente. Il risultato è di circa 30.000 13.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "deepplane web. Questo corpus include diversi ambiti e tutti questi 750 documenti vengono allineati, da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, si ottengono 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio sul tipo di modifiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "qui si può vedere che i testi biblici sono molto più semplici e diretti rispetto, ad esempio, ai testi delle notizie o ai testi per studenti di lingua"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, ad esempio per quanto riguarda la semplificazione lessicale, la semplificazione strutturale e il livello generale di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "si può vedere che il nostro corpus di pianificazione profonda presenta una grande varietà di diverse trasformazioni di semplificazione, quindi, ad esempio, nel corpus dell'API di pianificazione profonda abbiamo molte più riorganizzazioni e aggiunte di radici rispetto a quelle presenti nel corpus web di pianificazione profonda"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte nel corpus web abbiamo molte più riformulazioni"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo quindi cosa possiamo fare con questo corpus: Ciao, sono Omar, e ora parlerò dei casi d'uso per il nostro dataset dLAN. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati numerosi metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in documenti post-elaborati."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma, nel nostro caso d'uso, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli, nella stessa lingua, con lo stesso contenuto, ma con un livello di complessità diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora, dato che abbiamo il nostro set di dati deepplan, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "aggiustando i modelli linguistici per produrre un testo semplificato a partire dai testi complessi di input"}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo perfezionato due modelli diversi. Abbiamo perfezionato il modello della parte lunga per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche perfezionato la base normale in parte per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare tutti i punti di controllo e si possono esaminare i dettagli dei punteggi e delle metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questa regolazione fine di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza, grazie"}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skirkovsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, ad esempio, nelle dipendenze universali la struttura della coordinazione coordinata è Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso Lisa"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "approcci assunti nella teoria del significato testuale di Igor Milčuk, dove di nuovo l'intera struttura coordinata è guidata dal primo contratto, quindi questi due approcci sono asimmetrici, poiché individuano uno dei congiunti"}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "esistono anche approcci simmetrici per coordinare le strutture, come l'approccio prag, l'approccio a testa congiunzione, adottato nelle banche di alberi di dipendenza plugg, dove le strutture coordinate hanno come testa la congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze dall'inizio alla fine di tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio multi-testuale che viene utilizzato, ad esempio, nella grammatica delle parole di Dekatson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove si dice che tutte le condotte sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ama tutte le condotte separatamente, questi sono pulsanti che creano"}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Lo scopo di questo articolo è di presentare un nuovo argomento a favore delle strutture di coordinazione simmetriche come queste due e contro le strutture di coordinazione asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Ok, l'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò in base a questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come forse sapete, in inglese gli oggetti diretti preferiscono essere vicini al verbo, mentre gli accessori possono essere più lontani, quindi \"March ha letto il libro ieri\" è corretto perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Il participio passato letto ieri è molto peggio perché qui tra il verbo e l'oggetto diretto c'è un avverbio di tempo ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo perché allora può essere spostato nella posizione dopo l'adjuntivo"}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "è illustrato qui. Quindi entrambe queste frasi sono corrette. Marzo ha letto questo libro assolutamente affascinante sulla bestia ieri Io è accettabile in un certo senso, invece di questo abbiamo questo lungo e..."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Va anche bene dire che ieri ho letto questo libro assolutamente affascinante sulle api"}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Il ragionamento qui è che ciò è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che si preferiscono dipendenze più brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo la dipendenza dal rosso all'aggettivo di lunghezza 7 misurato in parole e dal rosso al libro di lunghezza 4. quindi insieme è 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "si sposta quando si cambia posizione, questi due costituenti sono la somma di queste due dipendenze diventa sei giusto quindi invece di 11, 6 molto più breve, ecco perché questo suona abbastanza bene giusto, viola un principio ma ne soddisfa un altro"}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Ok, quindi abbiamo estratto varie statistiche sul coordinamento dalla versione migliorata della banca dati della pensione e abbiamo visto il documento per cui non abbiamo utilizzato le dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "queste statistiche confermano l'osservazione fatta molte volte prima che i congiuntivi a sinistra tendono ad essere più brevi, quindi sale e pepe non pepe e sali misurati in sillabe"}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la lunghezza in Francia."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra la lunghezza dei due coniugati aumenta, il coniugato più corto preferisce essere il primo e più forte, in modo che la proporzione dei coniugati corti a sinistra sia maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "La novità di questo studio è che abbiamo osservato che questa tendenza si verifica solo quando i governatori di sinistra sono assenti"}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio, ho visto Baton Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "assente nel secondo esempio, Omero è arrivato ed ha starnutito. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Quindi, in tali casi, il congiunto sinistro preferisce essere più breve, tanto più quanto maggiore è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è a destra come qui, la sinistra governa la coda di coordinamento e la rete, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo dimostrato che misurando la lunghezza in caratteri si ottiene la prima colonna in sillabe, la colonna centrale in sillabe e la colonna a destra in parole, quindi mi concentrerò su quella di destra"}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "la tendenza del congiunto a sinistra ad essere più breve cresce costantemente con la differenza assoluta di parole e lo stesso si osserva quando non c'è un governatore come nel coordinamento delle frasi, ma quando il governatore è a destra questa tendenza scompare"}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "nel documento dimostriamo come ciò fornisca un argomento contro le strutture di coordinamento asimmetriche come queste due, poiché raddoppiano le strutture simmetriche come queste due"}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "si veda l'articolo per l'accordo completo e gli argomenti che mi dispiace di aver menzionato e di cui parleremo durante la sessione di presentazione del poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Shahang B, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce dei pregiudizi politici che portano a modelli NLB ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati di scansione web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "i media sono ben rappresentati nei loro dati di pre-addestramento secondo un sondaggio sul corpus C4 possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento dei modelli linguistici"}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una situazione ambivalente per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "da un lato hanno potuto apprendere da prospettive diverse che celebrano la democrazia e la pluralità delle idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente influenzate da pregiudizi sociali e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di indagare il processo di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici e ai compiti a valle, ponendoci specificamente le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo il significato politico dei modelli linguistici e quale ruolo potrebbero avere i dati su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano i modelli linguistici con diverse plutolinis nei compiti successivi e se ciò potrebbe comportare problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in particolare, proponiamo innanzitutto di stimolare i modelli linguistici con diversi formati di input utilizzando questionari politici come il test del compasso politico. Questo ci garantisce di effettuare una valutazione automatica ben fondata sulla letteratura scientifica politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i primi modelli linguistici hanno effettivamente orientamenti politici variabili. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "si può anche notare che GPT4 è il modello linguistico più liberale di tutti e che la serie GPT è generalmente più liberale dal punto di vista sociale rispetto alla serie BER e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "in secondo luogo, ci proponiamo di indagare fino a che punto i pregiudizi politici dei modelli linguistici siano effettivamente tratti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint del modello linguistico su sei diversi corpus partigiani separati in notizie e social media ulteriormente divisi nelle loro inclinazioni politiche"}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "addestrando ulteriormente i modelli linguistici su tali parti e corpus, possiamo osservare che anche le coordinate ideologiche del modello linguistico cambiano di conseguenza"}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Roberta, ulteriormente affinata e addestrata sul corpus di Reddit di sinistra, possiamo osservare un sostanziale spostamento verso posizioni più liberali."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda i suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici possano cogliere la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "dividiamo i corpora di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti, addestriamo separatamente i modelli linguistici sui due diversi corpora temporali"}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare che i modelli linguistici in generale hanno avuto una tendenza politica più lontana dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, valutiamo i modelli linguistici con diverse inclinazioni politiche per quanto riguarda il rilevamento del discorso d'odio e il rilevamento di notizie false, per le applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, vediamo che se analizziamo le prestazioni per categoria, cioè se separiamo le prestazioni in:"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "In diversi contesti demografici o politici e nei media, possiamo osservare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "Nel rilevare il discorso d'odio rivolto ai gruppi socialmente minoritari."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, sono meno bravi a rilevare l'incitamento all'odio rivolto ai gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "Viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio rivolto ai bianchi e agli uomini, ma peggiori nel rilevare il discorso d'odio rivolto ai neri LGBTQ+ e ad altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Si osservano anche tendenze nel rilevamento delle notizie false, dove i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione proveniente dai loro opposti politici e viceversa"}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, mostriamo inoltre molti esempi qualitativi per vedere che i modelli linguistici con significati politici diversi,"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "Fornisci previsioni diverse per gli esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'Appendice per evidenziare ulteriormente questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che esiste un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se i modelli di linguaggio per lingue scritte da destra dovessero essere perfezionati su discorsi d'odio o disinformazione o altro e venissero distribuiti su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che l'incitamento all'odio contro i gruppi minoritari potrebbe diffondersi senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Questo dovrebbe suonare il campanello d'allarme per farci riconoscere e affrontare le questioni di equità derivanti dai significati politici dei modelli linguistici"}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "un po' di discussione vorremmo anche sottolineare che mettiamo in evidenza il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici, è come stare tra Scilla e Cariddi"}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici e poi ai compiti a valle, creando in ultima analisi problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo a sanificare in qualche modo, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia realmente neutrale e cosa invece debba mantenere il linguaggio e i dati. È un po' come il problema delle trole elettriche."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Ottimo. Penso che sia praticamente tutto quello che ho per oggi. F5 per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il suo lavoro \"Anal positionality, characterachterizing design biases and data sets models\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronan Labrasse, Katarina Reinika e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Quindi iniziamo immaginando che lavoriate per un giornale e che stiate setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti rivolgerti a un'API popolare come Perspective API per la rilevazione della tossicità, e questo funziona davvero bene se sei Carl Jones, poiché la rispettiva API è in grado di rilevare correttamente gli esempi di tossicità."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio il caso di Aditya Sharma, dove il prospetto A API non è davvero così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio di progettazione in cui si osservano differenze sistematiche di prestazioni della tecnologia tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "I pregiudizi di progettazione come quello che abbiamo appena visto prima potrebbero spingerti a considerare la posizione dei ricercatori di NLP e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone assumono a causa delle loro caratteristiche demografiche, della loro identità e delle loro esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, perché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizionalità?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di dire che i modelli nelle celle e i dataset stessi abbiano identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i lavori precedenti hanno suggerito alcune prove aneddotiche dell'esistenza della posizionalità, come le lacune culturali nei modelli e nei set di dati, oltre alle definizioni teoriche della posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano sul confronto tra gli utenti finali e i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della modellazione e della posizionalità dei set di dati diventa sempre più importante man mano che i test di NLP diventano più soggettivi e socialmente orientati."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E è difficile caratterizzare come queste posizioni siano distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare il set di dati e la posizionalità del modello, confrontiamo le annotazioni con gli utenti reali e i set di dati e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "lo faccio attraverso il nostro quadro di riferimento della posizione NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il framework funziona in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è ri-annotare i set di dati con diversi annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E dovremmo farlo tenendo conto della demografia degli annotatori del dataset originale, perché di solito solo pochi annotatori annotano ogni istanza, e perché la demografia viene raramente raccolta e condivisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Perciò abbiamo scelto di ri-annotare i dati per ottenere, ad esempio, molte annotazioni e un ricco insieme di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo quindi le annotazioni per fascia demografica e le confrontiamo con i modelli e il dataset utilizzando il punteggio di correlazione R di comparisonar."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E quindi il nostro quadro si differenzia effettivamente dalla letteratura sul disaccordo tra annotatori confrontando gli utenti finali con modelli e set di dati, previsioni ed etichette, anziché limitarsi a osservare il semplice accordo tra annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "framer è stato reso possibile in gran parte grazie a Lab in the wild, una piattaforma di crowdsourcing online di un ex collaboratore HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto a piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Nel laboratorio in pratica abbiamo due compiti, uno dei quali è l'accettabilità sociale. Il modo in cui funziona è che i partecipanti leggono una situazione del dataset di chimica sociale e poi scrivono quanto quella situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nella città, possono confrontare le loro risposte con quelle di un'intelligenza artificiale e di altre persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "poi replicano una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggono un esempio da Dinah hatete e scrivono se ritengono che si tratti di un esempio di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con Dynah Hate, Perspective API, Rewire API, Hate Roberta e GPT4. Il nostro studio ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori di 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "ora siamo meglio attrezzati per rispondere a chi si allineano maggiormente i dataset e i modelli di NLP. Abbiamo scoperto che esiste una posizionalità nel NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo riscontrato che i dataset e i modelli sono più allineati ai paesi anglofoni. Quindi, per l'analisi dell'accettabilità sociale GPD4, abbiamo riscontrato che è più allineata ai paesi confuciani e anglofoni. Abbiamo riscontrato che anche il dyna hate è più allineato ai paesi anglofoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato un maggiore allineamento con le persone che hanno un'istruzione universitaria. Quindi, per GPD4 nel compito di Accettabilità sociale, abbiamo riscontrato che è più allineato alle persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "lo stesso vale per Diny Haight, dove è più in linea con le persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto agli omologhi maschili e femminili. Lo troviamo nel compito di accettabilità sociale GPG4 così come nell'analisi del compito Diny hatete."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione in LD in LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni al riguardo. La prima è tenere traccia di tutte le scelte di design pertinenti durante il processo di ricerca, e l'altra è condurre ricerche di NLP con la lente del perspectivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare set di dati e modelli specializzati all'interno di quattro comunità specifiche e un buon esempio di ciò è l'iniziativa Masakanne. Vogliamo sottolineare che l'NLP inclusivo non significa solo far conoscere a tutti come funzionano tutte le tecnologie."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, con questo concludiamo la nostra presentazione, ma se desiderate saperne di più, non esitate a consultare la nostra dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono X Yuan dell'Università di Faii. Sono qui per presentare il nostro lavoro: Distinguere la conoscenza degli script dai modelli di linguaggio leggero per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, chi deve spesso pianificare le proprie azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "In precedenza, il mondo ha esplorato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e ha dimostrato che i grandi modelli linguistici possono scomporre efficacemente gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione degli obiettivi con obiettivi specifici, vincoli specifici, come fare una torta al cioccolato, rimane ancora sottovalutata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Che impongono diverse limitazioni agli obiettivi della pianificazione, un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei modelli di linguaggio della vita reale."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "non esistono dati al di fuori di obiettivi specifici per individuare il giorno della nostra stella."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo prima acquisire questi obiettivi, come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati con intervento umano, utilizzando Instruct GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Preleviamo in esame centinaia di obiettivi specifici e valutiamo gli script generati dai modelli logici."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli Lilong ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi conduciamo un'analisi dettagliata per indagare quali modelli di apprendimento siano adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati nella figura mostrano che la completezza settimanale negli script generati è accettabile, ma non si può garantire la fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo le categorie tematiche più dettagliate dei vincoli definiti in Wi home. La mappa termica nella figura mostra che le prestazioni di pianificazione di instructiv variano notevolmente per le ragazze delle diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli dal vivo presenta un'elevata varianza, portando a prestazioni scadenti. Pertanto, abbiamo adottato l'idea di sovrapporre il filtro per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo prima i tipi vincolati con esempi per l'istruzione CPT e otteniamo obiettivi specifici basati sugli obiettivi astratti stabiliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Istruire il GPT sui principali script generali per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene derivato un modello di filtro per selezionare gli script fisici."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo gli script e le ragazze in incorporamenti istruttivi GPT e calcoliamo la similarità cosinica come punteggi di similarità per la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nel sito dell'obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, è possibile generare viti di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché l'implementazione di grandi modelli linguistici è costosa, è essenziale consentire la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare un set di dati è un passaggio essenziale per"}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale del dataset dei dati è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un set di dati di pianificazione linguistica vincolata dai modelli di linguaggio della vita reale."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata chiamato CodeScri."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo generato 55.000 obiettivi specifici con script per garantire la qualità della validazione e dei siti di test. Chiediamo ai lavoratori del crowd-sourcing di rivedere infine il reddito nei campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle restrizioni di Coscript. Abbiamo riscontrato che Coscript presenta un alto grado di pluralismo negli obiettivi specifici generati. Con Coscript, possiamo utilizzare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con la dimensione, t cinque finetu sul tasso di punteggio possono generare script di qualità dei capelli e la maggior parte dei modelli di alto livello, indicando che i modelli più piccoli possono sopprimere i modelli più grandi quando sono adeguatamente addestrati su siti di dati idonei."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo sviluppato una capacità di pianificazione linguistica vincolata per i grandi modelli linguistici e abbiamo sviluppato un metodo di filtro per l'overgeneration per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un set di dati quadrati di alta qualità, Codecri, per la pianificazione linguistica vincolata. Il set di dati CodeSscript di Wehop può essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "grazie per il suo tempo. Troverà maggiori dettagli su Codecri nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Shu H. Oggi presenterò il nostro articolo \"Do Connell 2003 Named Entity Taggers still work well in 2023\". Cominciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento degli enti nominati o il compito NER"}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano ConONO 2003 per sviluppare il NER da quasi 20 anni. e ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo un nuovo tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il dataset Connell++. Questo è un insieme di dati che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione Connell 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente abbiamo perfezionato oltre 20 modelli su Conal 2003. Li abbiamo valutati sia sul set di test Con O3 che sul primo set di test Cono plus"}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer si generalizzano normalmente meglio sui nuovi dati"}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influisce direttamente sulle prestazioni di un compito a valle. In questo caso, abbiamo anche scoperto che più esempi di ottimizzazione fine portano effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è il sovradattamento adattivo, che è il sovradattamento dei costi dovuto al riutilizzo dello stesso set di test più e più volte, e questo si manifesta solitamente come la diminuzione dei rendimenti su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra il treno e i dati di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting dativo, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha una pendenza maggiore di 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Colo 2003 si traduce in più di un'unità di miglioramento su Colo++, il che significa che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva un sovra-adattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qual è la temperatura di questo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare ad addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni peggiorano con un divario temporale più ampio."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni sia la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di un modello più grande e di più esempi di affinamento. E questi obiettivi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da un'errata temporizzazione e, in modo piuttosto sorprendente, non è causato da un adattamento adattivo, anche se il metodo di Connell del 2003 è stato utilizzato per oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo sollevato all'inizio del nostro articolo, gli etichettatori Carnal 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo solleciterà ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, vi preghiamo di consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, e parlerò del nostro lavoro sulla risoluzione delle espressioni differenziali indirette per la selezione di entità, in cui introduciamo il corpus di entità alternative"}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Philipp Radlinsky, Sylvia Parity e Annie Greece."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta e considerare questa domanda alternativa: intendevi Easy on Me o I Got a Feeling? Qui un utente vuole selezionare tra una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è fare un riferimento diretto, ad esempio dicendo che il nome della canzone è \"On Me\" o che è la prima canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "A volte, però, è più appropriato fare un riferimento indiretto per avere una conversazione più naturale. Questo può accadere quando l'utente non riesce a ricordare il titolo di una canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "le pronunce sono troppo simili tra loro e difficili da distinguere"}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze dirette, ad esempio il più recente o il segno che non è energetico."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità degli LLM"}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "non siamo a conoscenza di un set di dati pubblici su larga scala per un compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi ambiti: musica, libri e ricezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La metodologia di raccolta dei dati enfatizza l'informalità utilizzando un set di completamento dei fumetti"}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre bolle di dialogo. Nella prima bolla Bob dice: \"Ricorda quella canzone che ascoltavamo ieri\". Con queste parole Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "In questa seconda bolla di dialogo, Alice dice: \"Intendi facile per me o ho una sensazione?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "è la ricerca alternativa e nel terzo balloon Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio l'amico più recente"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Fornisci automaticamente la prima e la seconda bolla di discorso, ma la terza viene compilata dall'annotatore. La prima bolla di discorso viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, viene generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "usa sempre un modello semplice, intendi A o B, dove A e B sono esempi tratti da Wikipedia"}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato: man mano che ci spostiamo più in alto nella lista, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione"}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniformre"}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio due libri con il nome \"il dettaglio\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "il terzo caso è quando hanno descrizioni simili su Wikipedia e infine quando hanno voci di informazioni o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista, per esempio"}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "mostriamo questa domanda alternativa agli americani: conoscono il nome di queste entità, ma non necessariamente sanno cosa sono queste entità"}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "quindi quello che facciamo è fornire alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link alla ricerca Google per ogni canzone"}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "poi chiedete agli annotatori di ascoltare almeno alcune di ciascuna canzone e di leggere le informazioni su ciascuna canzone. Ecco, per esempio, il risultato della ricerca Google per la canzone Easy Answer"}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo tratti da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre tratte da Wikipedia, in modo che gli annotatori sappiano come si presentano."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio, qui la prima, e di descriverle utilizzando da tre a cinque espressioni di riferimento indirette."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio quello con la musica per pianoforte ecco alcuni esempi dal nostro set di dati ad esempio quello senza parole non quello con il ragazzo di 12 anni o quello fittizio o proviene dall'Azerbaigian e così via"}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus delle alternative contiene 6.000 domande alternative in tre ambiti e 42.000 risultati di espressioni di riferimento indirette. I risultati ottenuti con il modello T5X di grandi dimensioni sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta. È intorno al 92-95%. Ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87 percento, il che è più realistico, ad esempio, quando il modello linguistico recupera le conoscenze pregresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 6 percento, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili al dominio. Ecco un link al nostro set di dati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Papppy dell'Università di Trento e del Bruno Kessler Center di Foa e vi presenterò brevemente l'attenzione come guida per la traduzione simultanea del discorso, un lavoro congiunto con Matteo Negri e Marco Duchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simSD, è il processo di traduzione del linguaggio parlato in testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "Quali sono i problemi dei modelli SimST attuali? Di solito vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedimenti di formazione lunghi e complicati, ad esempio, formazione che coinvolge diversi obiettivi di ottimizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi di latenza, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è quindi la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Primo: utilizzare modelli SD offline già esistenti senza riaddestramento o adottare architetture specifiche per SSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfrutta la conoscenza già acquisita dal modello attraverso il meccanismo di tensione tra l'input audio e l'output testuale, ovvero il meccanismo di crostensione, e potete vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un punto o un codificatore di decentramento dell'attenzione, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa se la tensione non è concentrata, cioè questa somma è al di sotto di una certa soglia alfa rispetto agli ultimi fotogrammi di discorso lambda, il che significa che le informazioni ricevute sono sufficientemente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, se riceviamo un segmento di discorso contenente \"Voglio parlare di\" e il nostro modello prevede la traduzione in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E daremo uno sguardo al peso dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i frame di discorso ricevuti per primi, mentre l'ultima parola indica gli ultimi frame di discorso ricevuti come frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emessi le prime due parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "mentre, poiché la somma della tensione incrociata è superiore a una certa soglia alfa, non emetteremo l'ultima parola e attenderemo un altro segmento di discorso"}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro blocco di discorso e il nostro modello prevede più di tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola fa riferimento agli ultimi fotogrammi del discorso di Lamb."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emesse queste tre parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se si osserva il risultato principale di un punto."}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Rappresentiamo i risultati della traduzione simultanea delle pagine su grafici in cui abbiamo da un lato il blu che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la misura della latenza. E consideriamo anche la media consapevole del calcolo, che tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre cure siano il più in alto possibile su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma vogliamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E li confrontiamo con le strategie plepara che vengono applicate anche ai modelli offline, come la strategia withK e l'accordo locale. E li confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la traduzione simultanea del parlato."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea veloce in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che un dubbio supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo di usura computazionale, questa è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. Abbiamo inoltre reso disponibili in open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Ian e il mio collega Jion e io presenteremo la nostra ricerca su multi-Instruct, migliorando l'apprendimento sociale multimodale tramite l'ottimizzazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Grazie ai progressi nei grandi modelli linguistici, molte opere hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo dei modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che l'ottimizzazione dell'istruzione consente ai grandi modelli linguistici di eseguire compiti non visti in modo rapido seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione dell'istruzione si è concentrata sul miglioramento delle prestazioni seriali nei compiti relativi solo al linguaggio, mentre la visione artificiale e i compiti multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione dell'istruzione su modelli proteint multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità del set di dati di istruzione tra RP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "esistono più di 1600 compiti di istruzione dedicati solo al pranzo, tuttavia non esiste un compito di istruzione multimodale su larga scala disponibile pubblicamente, quindi questo ci motiva a creare un set di dati di sintonizzazione per l'istruzione multimodale"}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi-insstruct, il primo set di dati di riferimento per l'accordatura delle istruzioni multimodali, composto da 62 compiti multimodali diversi che coprono 10 categorie di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "I compiti sono derivati da 21 dataset open source esistenti e ogni compito è dotato di cinque istruzioni scritte in formato Expir."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "indagando sull'ottimizzazione dell'istruzione multimodale, il nostro set di dati proposto utilizza un modello di formazione multimodale unificato come modello di base. Utilizziamo un vocabolario unificato per i token di linguaggio e immagine e le coordinate di una casella delimitante."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro set di dati multi-instra."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "unificare l'elaborazione di vari tipi di dati di input e output."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo dell'OFA e formuliamo tutti i compiti in un formato sequenza unificata in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ora parlerò dell'accordatura dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il set di dati di addestramento, utilizziamo 53 compiti del gruppo N per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di lettura del senso comune per il test e selezioniamo altri cinque compiti da WiQ e dal gruppo vario."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutti gli esempi nella velocità di test per ogni compito. Inoltre, selezioniamo casualmente 20 compiti dalla velocità di test dell'istruzione naturale come per lo stesso compito per NRP"}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo un grande modello OFA pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con uno dei suoi 5 modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando entrambe le 5 istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Segnaliamo le prestazioni medie e massime e la deviazione standard delle prestazioni in tutti e 5 gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo rootjL. Per un compito di RP riportiamo anche RujL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una nuova metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi risultati per lo stesso compito, indipendentemente dalla leggera variazione nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i nostri risultati principali. Come possiamo vedere, la regolazione dell'istruzione può migliorare significativamente le prestazioni dell'OFE negli stessi compiti multimodali"}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche l'apprendimento trasferito da un set di dati di istruzioni naturali può essere utile per l'ottimizzazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere come, all'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e, nel frattempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento. Abbiamo utilizzato un istruzione rispetto a 5 istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostra l'effetto di diverse strategie di front-tuning sulla sensibilità del modello. Come possiamo vedere dall'apprendimento trasferito da un set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello IFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che l'apprendimento trasferito dai dati delle istruzioni Nitro può aiutare OFA a ottenere prestazioni molto migliori sul set di dati NitroE Instruct."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, abbiamo proposto il primo set di dati di sintonizzazione dell'istruzione multimodale su larga scala. Con FA continuiamo a migliorare la capacità neurale di OFA e esploriamo diverse tecniche di apprendimento per trasferimento, dimostrando che ci sono dei vantaggi. Progettiamo una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, un'altra cosa: stiamo raccogliendo set di dati di apprendimento per istruzioni multimodali molto più grandi, con circa 150 ulteriori compiti linguistici varianti, e li renderemo disponibili. Questo è un codice QR per i nostri dati e modelli. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Koovsna e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre solidi in contesti specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Baqui, Aaron Muller, Kanishka Mishra, Karen Fs, Roger Levy e Atina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, rivediamo il paradigma della coppia minima."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi la coppia minima para-para valuta fondamentalmente i modelli linguistici in aggiunta ai giudizi di accettabilità, che possono includere anche la grammaticalità come blimp, la palestra sintattica o l'accettabilità in termini di stereotipi come le coppie di folle."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticalmente corretta e poi mostrare una frase inaccettabile o una frase grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi la speranza è che il modello attribuisca fondamentalmente una maggiore probabilità alla soluzione accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "La pipeline MPP attuale fondamentalmente non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Oggi i grandi modelli linguistici stanno sviluppando un contesto sempre più lungo. È quindi fondamentale valutare l'accettabilità del modello in tutto il contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Questo è l'approccio. Quindi, quello che facciamo è rivisitare i dataset stessi per simulare queste sequenze più lunghe e poi ricreare le frasi scegliendo frasi accettabili o inaccettabili da quei dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto un paio di esempi tipici di mataticità dal set di dati BbliIM, dal caso dell'isola adiuntiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili, che abbiano la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticali dal progetto pilota adjun"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento, e ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di non corrispondenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui, le frasi provengono ancora da insiemi di dati pertinenti, ma non dallo stesso insieme di dati che si sta valutando. E possiamo fare lo stesso per il caso di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Come se il contesto provenisse da un sottoinsieme diverso del dataset o se fosse del tutto irrilevante per il momento – per la frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come si comporta il modello? Prima di tutto, guardiamo le frasi di Wikipedia che sono completamente irrilevanti per l'attuale coppia di query, e lì troviamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino al 2024 per massimizzare i modelli OPT e GPT2. E qui, nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Cosa succede quando scegliamo frasi dallo stesso insieme di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso set di dati BlimIM syntax gymIM"}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando combiniamo la struttura, cioè quando scegliamo le frasi che riguardano gli stessi fenomeni nella tassa sulla responsabilità del taxgen,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Si osserva un aumento o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo - e questo è molto grande, poiché questo effetto aumenta con la lunghezza del contesto, e questo probabilmente influenzerebbe i nuovi modelli linguistici che hanno una grande finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante ma aggiungendo del rumore all'input. E dopo aver effettuato diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che nessuno di questi rumori sta effettivamente facendo cambiare al modello il suo andamento in termini di come ci mostra l'andamento del giudizio su PayPal."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In generale, abbiamo riscontrato che i modelli sono sensibili alla posizione delle parole nelle frasi in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio dell'approvazione accettabile, osserviamo una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, il modo in cui la eseguiamo attualmente con input brevi e frasi singole, potrebbe non catturare appieno la conoscenza astratta dei modelli linguistici nell'intera finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per ulteriori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Just John e vengo dalla Penn State University. Oggi presenterò il nostro lavoro, Exemplar: analisi semantica multilingue in più lingue naturali e rappresentazioni manuali."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "L'elaborazione semantica è un compito che consiste nella costruzione di rappresentazioni semantiche delle query degli utenti, come ZQL e il calcolo Lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi semantica interlinguistica è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "come mostrato nella figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o funQL e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "Esistono modelli di analisi semantica interlinguistica proposti e valutati separatamente su un insieme di esempi e applicazioni limitati. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Mancano collegamenti di copertura su alcuni linguaggi naturali, come il cinese, e."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Lacune di copertura su molte rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo di Lamb manca."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "vengono valutati solo su determinati modelli neurali, ad esempio esiste un solo modello per la valutazione"}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, abbiamo proposto Ex exampler, ma forniamo un set di dati uniforme per il semi-percorso interlinguistico in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "contiene 90 insiemi in domini virali, 5 parti semantiche in materia fiscale, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione: utilizzeremo l'API di Google Translate per tradurre la fonte nella lingua di destinazione e poi utilizzeremo un modello monolingue per addestrare eventuali valutazioni"}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestreremo il modello inglese su una query in inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere lo SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "verificheremo anche il modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo caso, la lingua di partenza è la stessa della lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testare anche l'impostazione futura monolingue eseguendo l'addestramento di modelli monolingui con solo il 10 percento dei dati di addestramento"}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E che prevede la creazione di un modello multilingue, che noi addestreremo come modello unico per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo messo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue e durante l'inferenza possiamo utilizzare anche questo modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "Voglio tradurre richieste in tedesco o in cinese, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo anche i trasferimenti cross-linguistici zero-shot. Noi addestreremo su una lingua sorgente e trasferiremo su un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Durante l'addestramento, lo addestriamo su query in inglese o sulla combinazione di inglese e tedesco con poche query brevi per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo anche ottenuto molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, abbiamo valutato due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "incluso encoderPDdR che sta per encoder multilingue pre-addestrati con decodificatori basati su puntatori come X elementr plus pdr e bird plus pdr"}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come B e Mt5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "si è scoperto che l'encoder decoder ottiene le migliori prestazioni su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il nostro Mmt5 e l'esempio xlmr plusPDdr con le nostre impostazioni multilingue"}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "che l'encoder decoder o l'encoder PDR possono essere migliorati addestrandoli in una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che per l'inglese, le cui prestazioni peggiorano in sette set di dati e migliorano solo in tre set di dati"}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come i Curdi della multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario di prestazione tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento Fu interlinguistico, la linea arancione rappresenta il trasferimento zero-she interlinguistico, mentre la linea verde rappresenta l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "si è riscontrato che confrontando la linea verde e arancione si è riscontrato che per l'impostazione a breve termine zero il divario delle prestazioni nel trasferimento interlinguistico è significativo e confrontando la linea blu e arancione si è riscontrato che per l'impostazione a pochi brevi il divario di trasferimento si riduce rapidamente"}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "sono stati ottenuti anche altri risultati interessanti. Ad esempio, l'encoder decoder ha ottenuto risultati migliori rispetto al lavoro di proW o ha raggiunto risultati comparabili. L'uso del linguaggio naturale inglese può migliorare significativamente le prestazioni di futuresho sui linguaggi naturali di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che i modelli linguistici multilingue come Coders e Blue sono ancora inadeguati per le classi di semianalisi interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo creato exampler, un benchmark unificato per l'analisi semantica incrociata con più lingue naturali e molte rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "condurre uno studio di riferimento completo su tre tipi rappresentativi di modelli linguistici multilingue e il nostro risultato mostra molti risultati interessanti e altro ancora e vi invitiamo a visitare il nostro articolo e il codice grazie per l'attenzione"}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "a tutti, mi chiamo Al Villaad e farò una breve panoramica dell'articolo \"Palm from Translation: Assessing Strategies and Performance\", un lavoro svolto in collaborazione con i miei colleghi di Google Translate"}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testi che comprende 780 miliardi di token"}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "duma per la cucina raggiunge lo stato dell'arte in centinaia di compiti di NLP"}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro presentiamo un primo studio sistematico sul prompt dei grandi modelli linguistici per la traduzione automatica"}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò comporta l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "confrontiamo con i sistemi più avanzati, quindi i sistemi con le migliori prestazioni o le valutazioni WMT"}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche neuralMT all'avanguardia e mostriamo inoltre i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "il prompt ha una grande influenza sulle prestazioni del sistema di traduzione lnms, come possiamo vedere in un semplice esperimento in cui utilizziamo un breve prompt e forniamo due prompt diversi per due frasi diverse"}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto sfocato."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "In casi estremi, questo può arrivare fino a 40 punti di sfocatura. È quindi importante selezionare una buona strategia di prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "i nostri esperimenti con una soluzione per una strategia di prompt in cinque battute in cui contrassegniamo semplicemente la frase che forniamo al sistema con la lingua in cui è scritta"}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, in cui eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche delle frasi sorgente sono contrassegnate con due punti tedeschi e le traduzioni in inglese con due punti inglesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "ho visto che la forma effettiva della stampa non ha una grande influenza nel caso di diverse stampe brevi"}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting a zero e one-shott, e quando passiamo, come nel nostro caso, al prompting a fact-shott, non c'è quasi nessuna differenza rispetto alla forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "come gli esempi che portano la maggior parte del peso"}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, i risultati dei nostri esperimenti dimostrano che la qualità dell'esempio è più importante della somiglianza con la frase di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo sono molto più numerosi e di qualità superiore rispetto ai dati di addestramento, che sono più curati, e i risultati sono quindi migliori quando si utilizzano i dati di sviluppo"}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di massa, ma in questo caso siamo riusciti a ottenere un risultato molto vicino a quello di un sistema commerciale. Abbiamo scelto di evitare l'uso di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Dalle analisi che otteniamo dall'emailation che eseguiamo utilizzando il framework MQN, emerge che la fluidità del palm è paragonabile a quella dei sistemi più avanzati, ma la differenza principale deriva dall'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione che suona meglio, a volte tralasciando parti della frase sorgente che sono ammesse nelle traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di stile esterno per la padella è inferiore rispetto ai sistemi all'avanguardia, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "parm fornisce un output davvero fluido, ma con alcuni problemi di precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "questo è tutto per questa panoramica davvero breve. per maggiori dettagli, vi prego di venire alla presentazione completa del documento. grazie mille"}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, uno studente di dottorato all'Università Silente in Germania. In questo video vorrei presentare il nostro lavoro recente, \"Più grande di quanto pensiate\", uno sguardo critico alla sorpresa settimanale di Lening."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "è un lavoro congiunto con Sha my muba e gear Stefan e ditishklakov"}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale"}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "con una supervisione debole non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristica, basi di conoscenza o fonti di codice di località, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "addestriamo direttamente le reti neurali sui dati con etichette deboli, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Vengono proposti algoritmi di apprendimento con supervisione debole per addestrare in modo robusto le reti neurali in presenza di tale rumore di etichetta, in modo che i modelli addestrati generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "lavori recenti in wSL quindi wSL sta per apprendimento settimanale del supporto un'affermazione comune è che le persone dicono di addestrare i modelli solo sui dati di etichettatura settimanale e di ottenere alte prestazioni su set di test puliti"}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "In realtà, questa affermazione non è sbagliata, ma c'è un problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "è che le persone presumono che esista un set di validazione pulito aggiuntivo o un modello di selezione del modulo ben strutturato"}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "si è fermato su questo problema di impostazione, ma ciò implica che sono necessarie ulteriori annotazioni manuali nell'apprendimento del supporto settimanale, ma come un elefante nella stanza, questa necessità viene spesso trascurata"}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Come accennato in precedenza, adottiamo tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL? Oppure possiamo utilizzare un set di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "se i dati puliti sono richiesti o se i dati puliti sono obbligatori affinché WSL funzioni, quante etichette pulite ci servono in definitiva dovremmo utilizzare solo le etichette pulite per la validazione o esistono modi migliori per utilizzarle"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, abbiamo scoperto che i recenti metodi WSL richiedono effettivamente campioni di amputazione puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti si verifica un forte calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare oltre le etichette deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "quella della formazione è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "indica che i metodi WsSL richiedono dati accuratamente etichettati per funzionare correttamente e che il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato"}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "per ottenere prestazioni elevate, abbiamo bisogno di soli 20 campioni per classe"}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se decidiamo comunque di accedere a campioni puliti, allora l'addestramento diretto su di essi porterà a prestazioni ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazione tra gli approcci di fine-tuning applicati direttamente sui dati puliti e gli approcci WSL che utilizzano i dati puliti solo per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo vedere, se abbiamo 10 campioni per classe, che il direct fine-tuning inizia a battere gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni ottenuto nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare l'ottimizzazione sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "dai dati si può vedere che il modello validna denominato ftw inizialmente ha prestazioni inferiori rispetto ai metodi WSL più complessi come il coseno"}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": ", se permettiamo ai fantuni di continuare sui campioni puliti, allora Tw si comporta altrettanto bene come altri metodi"}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che i recenti approcci wSL richiedono campioni puliti e annotati manualmente affinché funzionino correttamente. Il loro miglioramento delle prestazioni e la loro praticità sono fortemente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Di seguito sono riportate raccomandazioni concrete per le future ore di lavoro."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, riportare i criteri di selezione del modello. Ad esempio, indicare se la sezione del modello è stata eseguita con campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, i metodi WSL dovrebbero essere confrontati con poche linee di base di atterraggio brevi, poiché si suppone che il lavoro su campioni di calcestruzzo sia valido. In terzo luogo, il continuo perfezionamento è una linea di base semplice ma efficace che dovrebbe essere presa in considerazione nei futuri lavori nel campo WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Vi invitiamo a consultarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch e sono Sarah Finch. E oggi vi racconteremo tutto su ABC Eval, un nuovo approccio dimensionale alla valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio di NLP di Emory, diretto dal professor Gino Choi presso l'Università di Emory, e in collaborazione con l'intelligenza artificiale di Amazon Alexa."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che tu abbia appena sviluppato un modello di dialogo e desideri vedere quanto sia performante rispetto allo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare una valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala liquida."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più fine."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi di scala comparativa o di gradimento esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Questo approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se la risposta di ciascun modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"annotating behaviors in chat\", o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti come influenti sulla qualità della chat nella letteratura recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare la frequenza con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, ABCEval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o il suo partner allucina fatti errati o viola la conoscenza del senso comune e quando il modello riesce o non riesce a mostrare empatia"}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato queste conversazioni utilizzando anche tre metodi esistenti: valutazioni dei liquori a livello di turno, valutazioni dei liquori a livello di dialogo e confronti di coppia a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi dei risultati di valutazione, abbiamo riscontrato che le etichette comportamentali ABC sono complessivamente più affidabili rispetto a quelle ottenute con i metodi esistenti, come misurato dall'accordo interno degli annotatori su 100 conversazioni etichettate in duplice copia."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABCEval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può vedere come la misurazione della proporzione di turni con contraddizioni personali e con il partner spieghi rispettivamente il cinque e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza dell'alcol spiegano solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a gradi."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere come la combinazione di tutte le metriche ABC Eval spieghi oltre il 25% della qualità della conversazione. E quando si rimuovono le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche del liquore a livello di turno spiega molto meno della qualità e meno di queste metriche contengono informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Le metriche ABC Eval, affidabili, informative e distinte, ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere che nei risultati del nostro esperimento che diverse sfide rimangono ancora e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Forniscono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il proprio partner nel 10% dei casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Il rapido progresso nel campo ha portato a una diminuzione di molti di questi tassi di errore nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è un motivo in più per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "auspichiamo che ABC Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'IA conversazionale progredisca nei prossimi mesi e anni. Grazie per averci guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto: un'esplorazione multilingue basata sui dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernage, Emiliu Andre, FD Martins e Graham Newbigin."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "se la frase precedente \"wash\" potesse iniziare a diventare pericolosa se i ministri lo scoprissero, allora \"more\" si riferisce a uno spia. Ma se la frase precedente \"was\" potesse essere qualcosa di serio, dottore? allora \"more\" si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e quindi cambia anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possano contrastare casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come il blue incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande: primo, quando la traduzione richiede un contesto? e secondo, quanto bene i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto il lavoro dipenda dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Ciò si ottiene misurando quanta informazione il contesto C fornisce sull'obiettivo Y data la fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "si può pensare a CXMI come alle informazioni ottenute dando un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole che hanno un alto PA6MI come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto piecexMI per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "effettuiamo la nostra analisi su trascrizioni di TED Talk che sono state tradotte dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli diversi. Prima di tutto esaminiamo le etichette delle parti del discorso che hanno un valore medio-alto di pxMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ci permette di trovare, ad esempio, i pronomi duali in arabo che hanno un p6MI relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso modo, scopriamo che alcune lingue richiedono anche un contesto quando vogliamo scegliere la forma verbale appropriata. Esaminiamo quindi gli elementi del vocabolario che hanno un alto valore di pxMI calcolato in media su tutte le sue diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come quello qui, in cui in cinese è necessario il contesto per tradurre i nomi propri, per assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo, ci accorgiamo che il contesto è supportato per utilizzarlo con la giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo i diversi token individuali che hanno un alto p6MI. Questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione di romanzi in un nuovo documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, abbiamo creato dei tagger per identificare automaticamente le parole che appartengono al fenomeno. E chiamiamo il nostro tagger il tagger multilingue consapevole del discorso o tagger muda."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi anche osservare che diverse lingue presentano proporzioni diverse di questi fenomeni descrittivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo il tagger M applicandolo al corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger M ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro parametro di riferimento e altri indicatori per valutare i diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, quando utilizziamo metriche a livello di corpus, per il blu, scopriamo che i modelli agnostici di Conic hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se utilizziamo commentt, i modelli consapevoli del contesto ottengono i migliori risultati. E se utilizziamo la misura wordf, allora i modelli con o senza contesto hanno prestazioni confrontabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "utilizziamo il benchmark MUDA per valutare i modelli e scopriamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di altri che non utilizzano il contesto su altri fenomeni come le ellissi, i pronomi e la forma dei verbi. Quindi questo suggerisce dove dovremmo vedere maggiori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeP è generalmente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo eseguito un'analisi basata sui dati su 14 coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri refini per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeni del discorso del disco possono gestire bene o male e quali sistemi di traduzione sono bravi nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione. Ci vediamo a Trado."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanislavak e vi presenterò i nostri lavori su Dr. Bert, un robusto modello di pre-addestramento in francese per i settori biomedico e clinico"}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, parleremo prima del modellamento linguistico in Herke. Poi presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in francese chiamato Dr. Bert, che si basa su Roberta e viene addestrato su Naos, un insieme di dati medici raccolti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo anche un confronto tra modelli con impostazioni protononiche multiple e fonti di dati. Quindi presentiamo i nostri risultati su 11 compiti a valle biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine concludiamo gli esperimenti e vi diamo ulteriori dettagli su come accedere ai modelli"}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dal suo lancio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offrire un notevole miglioramento delle prestazioni rispetto ai metodi storici, statici e contestualizzati come word2vec, FastText o GloVe."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese, con Cambert, e ad altri settori come il biomedico, con Permed Bert e Biobert, e in quello della nascita clinica, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati specifici del settore."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, non esisteva alcun modello open source per il biomelicone in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi, e se questi dati grezzi possano costituire una valida alternativa ai dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo confrontato il Dr. Bert con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'ospedale non generico della nostra casa."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo: quanti dati abbiamo bisogno di avere per addestrare un modello specializzato su dati francesi? Sono quattro gigabyte, un gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero: una prima versione di D. Bert con sette gigabyte di nachos, una seconda versione di quattro gigabyte di set di nachos."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con quattro gigabyte di frasi tratte da nodi clinici, e una versione finale di Schubert, con un mix di quattro gigabyte di set di nature e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con il pre-addestramento contra per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso di Cammbert e addestrato su un set di nachls di quattro gigabyte; un altro, anch'esso basato su Cammbert, ma addestrato questa volta sui quattro gigabyte di nodi Kcliner."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo una base sul modello biomedico inglese, Bermed Bert, e l'addestramento su un set di quattro gigabyte di estratti. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, abbiamo raccolto più attività a valle pubbliche e private, come il riconoscimento di nomi e entità, la classificazione, il tagging delle parti del discorso e la risposta alle domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di design B, ovvero Cammbert OscarOS 18 gigabyte, Cammbert Oscar quattro gigabyte, Cammbert cinet quattro gigabyte, lomet Bert, Biobert e Clin BERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione dei punti salienti, ovvero il modello che si comporta meglio nel compito con dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere questi dati e osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo inoltre che l'utilizzo di più dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, l'addestramento gratuito da zero sembra ottenere prestazioni superiori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul controllo del pretraining utilizzando il peso e il tokenizer di permit Bir addestrato sul sottoinsieme di quattro gigabyte di dati naturali, ha mostrato risultati comparabili a quelli ottenuti con Dr. Bert a partire da zero con quattro gigabyte."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Questo non vale per il modello basato su Cammbert whites e tokenizer, che soffre di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per concludere, il nostro sistema offre prestazioni migliori in nove dei 11 compiti a valle e supera globalmente il risultato del modello generico qui chiamato camembert"}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo anche che i dati specializzati sono migliori, i dati più specializzati sono ancora migliori, ma non si adattano bene all'ampliarsi delle dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "tutti i modelli di pre-addestramento ottenuti da nachos sono liberamente disponibili e sul tuo viso e tutto lo script di addestramento sono sul nostro repository githubHub"}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi grazie per questa presentazione e non vediamo l'ora di vedere le azioni durante la sessione dei poster a Toronto"}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann, e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging con multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "è un lavoro congiunto con i miei consulenti Alexander Kola e Ivan Tittov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste singolarmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'analisi semantica, il test per la generalizzazione compositiva potrebbe essere così. Come al solito, abbiamo un insieme di enunciati di addestramento. In questo caso, la ragazza dormiva e Mary sapeva che la ragazza dormiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Questi enunciati sono abbinati a forme logiche che rappresentano gli aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha mostrato una ricorsione poco profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "i modelli sequenza-contro-sequenza ingenui faticano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono risultati che sono distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate negli esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo più diffuso per affrontare questo problema è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono destinati a catturare il processo compositivo che collega le enunciazioni con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente, ciò comporta una notevole pre-elaborazione delle forme logiche specifica per il formalismo, ad esempio, per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'acquisizione degli alberi può anche comportare procedure di induzione grammaticale specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello sequenza-sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "L'approccio prevede la previsione dell'output dall'input in due passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione e metterli nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da sinistra a destra sull'output e determiniamo quale token del multiset inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi passiamo al prossimo token multi-set per determinare il secondo token dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token dell'output in modo simile saltando a un altro token multiset. Proseguiamo con questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Fino a quando ogni token del primo stadio non è stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'anticipazione dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Altri tipi di generalizzazione strutturale rimangono comunque molto difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, risolviamo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-setter proviene, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Abbiamo affrontato questo problema inducendo l'allineamento come parte della formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP-difficile. Questo perché è legato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approssimiamo questo con un rilassamento continuo ottimizzato per la GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "se desidera saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, si legga il nostro articolo o venga al nostro poster"}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Akshata e oggi io e il mio co-autore Martin presentiamo il nostro lavoro, il Kit Master: Valutazione dell'integrazione della conoscenza da fonti multiple. Questo lavoro è il risultato di una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite un pre-addestramento, e la conoscenza fornita come input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "esperimenti su compiti come la risposta a domande dimostrano che i modelli possono utilizzare la conoscenza del tempo pre-addestrata per risolvere il compito"}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "ma la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche al momento dell'inferenza"}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, nella frase: \"John ha visto il presidente appena eletto in TV."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri di pre-addestramento possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere in modo affidabile chi sia l'entità specifica di questa istanza John o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dopo il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per i compiti di NLU intensivi in termini di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "introduciamo un compito di risoluzione del riferimento centrale progettato per verificare la capacità di attingere alle conoscenze disponibili in diverse fonti, valutiamo il dataset con i partecipanti allo studio umano e stabiliamo modelli di risoluzione del riferimento centrale"}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset. Servin è un giudice. Kia è un panettiere. Termin e Kia si incontrarono in un parco. Dopo una lunga giornata di lavoro a decidere casi in un codice di legge, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"he\", che in questo caso è il sermone."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un dato pronome richiede due tipi di informazioni: in primo luogo, conoscenze specifiche dell'entità come il fatto che un servile è un giudice, e in secondo luogo, conoscenze generali come il fatto che i giudici decidono i casi nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante l'addestramento preliminare dei grandi modelli linguistici, mentre le conoscenze specifiche delle entità vengono tipicamente osservate al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "variare la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti"}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di kitmos, prima con l'impostazione tipica di background pre-addestramento, dove si assume che la conoscenza retrospettiva sia disponibile in tempo di addestramento libero."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "in secondo luogo c'è lo sfondo sia nell'impostazione in cui la conoscenza precedente è disponibile sia al momento del pre-addestramento sia al momento dell'inferenza, infine quello in impostazione esperienziale con entrambi i tipi di conoscenza disponibili solo al momento dell'inferenza"}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "l'ultimo settaggio è particolarmente interessante poiché simula il caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove occupazioni si sono sviluppate dal momento del pre-addestramento"}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di come controlliamo la disponibilità dei fatti nelle due fonti"}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di impostazione pre-addestrata di base, assumiamo che la conoscenza di base che i politici cercano di ottenere nei seggi governativi sia contenuta nei parametri pre-addestrati. Nel contesto di interferenza, forniamo la conoscenza anti-specifica: \"Chechester è un politico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Il contesto di sfondo, oltre all'ambientazione, fornisce inoltre non solo informazioni anti-specifiche ma anche conoscenze di base sui politici nel contesto della scheda di interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "nella configurazione di sfondo libera, forniamo il tour del merito dell'occupazione fittizia invece che politico, perché è improbabile che il tour del merito sia contenuto nella regione pre-t20peri"}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il set di dati sia con partecipanti allo studio umani che stabilendo modelli di risoluzione delle preferenze. In questa figura mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile dell'impostazione di pre-addestramento di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "la formazione specifica del nostro compito su Kidmus, entrambi i modelli non funzionano bene. Quando addestrati su Kidmus, tuttavia, sia C2F che built forQF ottengono risultati significativamente migliori rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando vengono addestrati su dataset generici di risoluzione dei riferimenti, i modelli imparano a sfruttare indizi superficiali che non sono utili quando vengono testati su Kidmus, dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "esperimenti aggiuntivi in cui la conoscenza fittizia indicava che anche i modelli con le prestazioni migliori non riescono a integrare in modo affidabile la conoscenza retrospettiva, se non solo nel momento dell'interferenza"}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "riassumiamo i punti principali del nostro articolo: molti modelli di evoluzione della coreferenza sembrano incapaci di ragionare su conoscenze provenienti da fonti diverse senza un addestramento specifico per il compito; tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare con successo conoscenze provenienti da più fonti"}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, anche i modelli con le prestazioni migliori sembrano avere difficoltà con la conoscenza retrospettiva integrata in modo affidabile presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e date un'occhiata al set di dati nel codice su githubt. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro lavoro sulle persone contrassegnate con un segno di spunta, utilizzando prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato realizzato in collaborazione con Essenndermush e Danjorovsky"}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Queste misure presentano varie limitazioni: di solito si basano su insiemi di dati costruiti a mano che richiedono molto tempo per essere curati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie come associazioni negative con gruppi particolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "La maggior parte del lavoro in questo ambito non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali multifaccettate possano amplificare i pregiudizi e rappresentare punti unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi nuovi Lm sintonizzati sull'istruzione sono molto bravi a rispondere a istruzioni e suggerimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt per immaginare di essere una donna asiatica. Descrisciti."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcune generazioni di esempio da GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Subito si nota che, sebbene i risultati non siano apertamente negativi o tossici nel senso tradizionale di queste parole,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Si possono notare alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è rappresentata come modesta. La donna del Medio Oriente è descritta con parole come esotica e come se si riferisse a una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le donne di colore fanno riferimento alla loro discendenza, mentre il personaggio dell'uomo bianco non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "per catturare questi schemi, il nostro metodo si compone di due parti. La prima consiste nella creazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste persone sono stati ispirati da uno studio in cui sono stati dati questi suggerimenti a soggetti umani, scoprendo che, dandoli a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "Questo consente inoltre un confronto diretto tra le nostre persone generate e le risposte scritte umane."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è costituita da parole contrassegnate, un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli contrassegnati da noi, su cui tornerò tra breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di questo approccio è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole contrassegnate si basa quindi sul concetto sociolinguistico di contrassegnatezza, che afferma che esiste un valore predefinito non contrassegnato e che qualsiasi gruppo che si discosta da tale valore predefinito è linguisticamente contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, la parola uomo o scusa, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero uomo e indicano il termine con donna."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, per prima cosa indichiamo quali sono i gruppi non marcati e quelli marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "poi confrontiamo le persone utilizzando il metodo delle parole di contrasto, che consiste essenzialmente nell'utilizzare rapporti log-odds ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le persone di donne nere, useremmo parole offensive e confronteremmo i rapporti tra le dee della legge sia con le persone bianche che con le persone maschili, perché questi sono i due gruppi non contrassegnati corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ora alcuni risultati. Quindi, per prima cosa, utilizziamo il lessico degli stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo la distribuzione delle parole nel lessico, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le persone generate hanno tassi molto più elevati di parole Luxon, quelle scritte dagli esseri umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate presenti nelle persone generate sono davvero solo le parole \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in realtà, solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E infatti, il lessico non riesce a cogliere molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece di farlo, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, esaminiamo come questi ritratti apparentemente positivi riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per i gruppi di marchi, le parole principali includono cose come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono questi gruppi solo in base alla loro identità e li distinguono dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "contribuisce a una lunga eredità di discriminazione e di emarginazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti luoghi comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, per esempio, le parole che descrivono le donne latine includono termini come vibranti e curvilinee."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "Ehm, che si collegano a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come piccolo, delicato e setoso."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomissae così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per le donne di colore, vediamo che alcune delle parole più frequenti sono forti e resilienti."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "si collega a un archetipo che la gente ha chiamato l'archetipo della donna nera forte e, sebbene a prima vista possa sembrare positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché esercita una forte pressione su queste categorie demografiche affinché siano resilienti e forti contro gli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "invece di lavorare per cambiare quegli ostacoli, esercita pressione su quelle persone affinché li superino, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni"}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "in generale, scopriamo che le parole per ciascun gruppo contrassegnato riflettono quasi esclusivamente narrazioni essenzializzanti"}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, noi ricercatori dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti; dovremmo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, come questi stereotipi positivi, non sappiamo se sia perché c'è una sorta di strano."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un allineamento dei valori eccessivamente esagerato è in corso, o forse altri metodi anti-stereotipia che stanno portando a questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "non si possono fare ipotesi o approfondire ulteriormente senza maggiore trasparenza"}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "grazie mille per aver ascoltato, divertiti all'Ace"}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jing Wei Y e vengo all'Università di Scienza e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me presentare un breve video pubblicitario del nostro articolo. State copiando il mio modello, proteggendo il diritto d'autore dei grandi modelli linguistici per l'incorporamento e i servizi? Rimuovete la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo prima il contesto sui servizi di incorporamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, Lama, PM sono eccezionali nella comprensione e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'integrazione come servizio è uno dei servizi basati su grandi modelli linguistici per assistere in vari compiti di NLP"}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenI offre un'API di embedding basata su aGbt."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che l'aggressore potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili, pertanto è necessario proteggere il diritto d'autore dell'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il diritto d'autore dei servizi di incorporamento, una delle soluzioni è incorporare una filigrana nel servizio del fornitore e rilevare se un altro servizio contiene la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo watermark deve soddisfare le seguenti proprietà: in primo luogo, il metodo deve essere applicabile all'incorporamento come servizi; in secondo luogo, il watermark non deve compromettere l'utilità dell'incorporamento fornito."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "In terzo luogo, la filigrana dovrebbe essere abbastanza resistente all'attacco, altrimenti l'attaccante potrebbe rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la filigrana deve essere trasferibile ai servizi dell'aggressore durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'integrazione come servizi o alla mancanza di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo un marker di incorporamento che è un metodo di watermark basato su backdoor applicabile ai servizi di incorporamento"}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Poi, permettimi di illustrare i dettagli del nostro marcatore di incorporamento. Il marcatore di incorporamento contiene due passaggi principali: l'inserimento della filigrana e la verifica del diritto d'autore."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un testo generale e contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "iniezione di filigrana, definiamo prima un targeting di base Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'incorporamento fornito è una somma ponderata dell'incorporamento di destinazione rispetto all'incorporamento originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del diritto d'autore consiste nel rilevare se il modello alla base di un altro servizio contenga la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, creiamo un set di dati di backdoor e un set di dati benigno. Il set di dati di backdoor contiene frasi in cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "il provider richiede embedding dal servizio stiller con il dataset"}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la somiglianza coseno e l2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di somiglianza tra beniggh e il set di dati della backdoor, definita come delta coseno e delta l2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza matrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto esperimenti su quattro dataset: AG news, mind, SSD two e A spam. Abbiamo ipotizzato che il fornitore del dataset liewikitext contasse la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati mostrano che il nostro marcatore di incorporamento può avere ottime prestazioni di rilevamento mantenendo al contempo un'ottima utilità per i compiti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Confermiamo anche la copertura dell'embedding fornito visualizzando l'embedding delle frasi sviluppate in BPCca. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le incorporazioni di backdoor e le incorporazioni normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Grazie, è tutto. Verranno a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vaudha e sono un dottorando in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come articolo esteso: \"Transfer learning for dissonance detection addressing the rare class challenge\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel contesto del linguaggio. In modo semplice, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "come in questo esempio in cui una persona afferma \"so che le sigarette potrebbero uccidermi\" e poi continua dicendo \"ho preso un paio di sigarette dopo la riunione\". Questa convinzione e questa azione sono incoerenti e sono in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "il fatto di dire che non credo di poter mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di consonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "La dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, ma è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, monitorare le tendenze e i cambiamenti nei valori delle credenze e negli atteggiamenti della popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'alta dissonanza cognitiva è anche legata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "La dissonanza espressiva nel linguaggio può essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato il primo approccio alla dissonanza come mostrato nell'organigramma qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "sono stati analizzati utilizzando un parser PDTV e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "qui si può vedere che la dissonanza è stata riscontrata solo nel 3,5 percento delle coppie annotate"}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliendo circa 1000 esempi di coppie di unità di discorso, abbiamo eseguito una formazione per un classificatore iniziale addestrato solo su 43 esempi di distanza. Non sorprende che il classificatore non abbia ottenuto risultati molto migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "data la bassa frequenza di dissonanza e l'assenza di qualsiasi precedente serie di dati di questo tipo, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo tale da poter raccogliere più campioni di dissonanza in meno cicli di annotazione, riducendo così i costi complessivi di annotazione e migliorando la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "il modellista iniziale non è stato in grado di catturare la classe di dissonanza, quindi iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati"}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "trasferimento da due compiti diversi: classificazione della dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dal tema."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamiamo dibattito qui e sulla classificazione binaria delle classi di espansione e di confronto di PB, poiché questi due concetti sono strettamente legati alla concezione delle consonanti e della dissonanza, e li chiamiamo CE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "si riscontra che, trasferendo la performance a zero corto, l'insieme di dati annotati è già molto migliore del caso con il miglior valore di AUC pari a 0,62"}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Dopo aver perfezionato iterativamente entrambi i compiti, scopriamo che la messa a punto dei compiti di CE seguita da un ulteriore perfezionamento sul dibattito produce prestazioni zero-shot molto migliori. Pertanto, questo è il modello che utilizziamo per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Il metodo cumulativo accumula tutti i dati raccolti finora tramite annotazioni attive, mentre il metodo iterativo aggiorna il modello addestrandolo sull'ultimo insieme di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo scoperto che quella cumulativa ha ottenuto risultati uguali o migliori rispetto a quella iterativa in tutti i casi"}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara (PRC) per selezionare principalmente gli esempi che hanno un'alta probabilità di essere dissonanti secondo il modello attuale in qualsiasi round di AL"}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "confronta questo con l'altro stato delle strategie A più avanzate che sono comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "si rileva che la strategia proposta di prc funziona meglio di altre strategie state-of-the-art anche se la differenza è piccola, si noti che le prestazioni sono significativamente inferiori per i dati casuali"}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriori cicli di AL con le due migliori strategie, miglioriamo la classificazione a distanza, AUC a 0,75, che è la migliore performance che abbiamo finora ottenuta per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "verificare anche la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Abbiamo riscontrato che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo scoperto che la PRC è una semplice strategia A per l'acquisizione di classi rare e che l'avvio a freddo dell'apprendimento automatico può essere notevolmente facilitato con compiti di apprendimento per trasferimento progettati in modo appropriato."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "si ritiene inoltre che l'aggiornamento iterativo sia utile per l'apprendimento trasferibile da un dominio diverso, mentre le annotazioni attive in-domain traggono vantaggio dall'aggiornamento cumulativo"}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro set di dati di codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
