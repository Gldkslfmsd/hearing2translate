{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é o Safari, e vou apresentar o nosso trabalho sobre enriquecimento de dados tabulares usando arquiteturas de transformadores ajustadas. Os cientistas geralmente analisam dados e focam principalmente em manipular as características existentes dos dados, mas às vezes essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. O objetivo da nossa pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Vamos supor que temos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a ligação de entidades e análise de texto para extrair novas características da base de conhecimento de texto livre. O nosso framework, FEST, é exatamente esse processo automático. Vamos ver um exemplo nos conjuntos de dados alimentados ao FEST. Neste exemplo, o conjunto de dados é um conjunto de dados universitários, com o objetivo de classificar universidades em universidades de baixo e alto ranking. Como base de conhecimento, usamos a Wikipedia. A primeira fase é a ligação de entidades, quando cada entidade, neste exemplo, o nome da universidade, é ligada a uma entidade dentro da base de conhecimento. O texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipedia. Agora, precisamos gerar ou extrair características do texto recuperado, então precisamos de uma fase de extração de características que inclui análise de texto. Esta é a principal novidade deste trabalho, e vou aprofundar-me nisso nos próximos slides. Após a fase de extração de características, há uma fase de geração de características, quando usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, geramos características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então primeiro geramos duas novas características. Mas se o conjunto de dados tiver cinco classes, primeiro geramos cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos os modelos de linguagem baseados em transformadores atuais, como o GPT X e outros. Não é provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada, então uma abordagem ingênua seria o ajuste fino da tarefa alvo. Na fase de extração de características, podemos baixar um modelo de linguagem pré-treinado e ajustá-lo sobre o conjunto de dados alvo. Neste exemplo, para ajustar o modelo de linguagem para classificar o texto em classes, resumo em classes, baixo ou alto, recebemos a saída do modelo de linguagem, que é a probabilidade para cada classe, e usamos como novas características. O problema com esta abordagem é que o conjunto de dados pode ter poucas entidades distintas. No nosso experimento, quase metade dos conjuntos de dados contém menos de 400 amostras e o menor conjunto de dados contém 35 amostras. Portanto, ajustar um modelo de linguagem sobre este conjunto de dados seria ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados, porque quando aplicamos o FEST sobre um conjunto de dados múltiplo, podemos usar os n menos 1 conjuntos de dados para reunir informações sobre os n menos 1 conjuntos de dados e usar essas informações quando analisamos o conjunto de dados final. O que sugerimos é adicionar outra fase de ajuste fino, uma fase preliminar de ajuste fino de múltiplas tarefas, quando ajustamos o modelo de linguagem sobre n menos um conjunto de dados e, em seguida, executamos outra fase de ajuste fino, que é um ajuste fino de tarefa alvo, quando ajustamos o modelo de linguagem sobre o n-ésimo conjunto de dados alvo. O estado da arte no ajuste fino de múltiplas tarefas chamado DNN em TDNN mantém um número de cabeças igual ao número de tarefas no conjunto de treinamento. Então, se neste exemplo houver quatro tarefas no conjunto de treinamento, o DNN mantém quatro cabeças, como você pode ver na imagem. Ele amostra um lote aleatório a partir do conjunto de treinamento. Se o lote aleatório pertence, por exemplo, a tarefas de classificação de sentença única, ele executa a passagem direta e inversa através da primeira cabeça. Se o lote aleatório pertence a uma tarefa de classificação em pares, ele executa a passagem direta e inversa através da última cabeça. No nosso cenário, um conjunto de dados tabular varia o número de classes, então há muitas tarefas. Um DNN vazio mantém o número de cabeças igual ao número de classes, camadas de saída e, adicionalmente, DN precisa inicialmente de novas cabeças para um novo conjunto de dados com uma nova tarefa. A nossa abordagem, chamada de ajuste fino de reformulação de tarefas, em vez de manter múltiplas cabeças, reformulamos cada conjunto de dados em um problema de classificação por sentença, que são tarefas de duas classes. Vamos ver um exemplo aqui. Este é o nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes. Reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto do resumo e a classe em verdadeiro ou falso, ou em outras palavras, treinamos o modelo de linguagem para classificar um resumo e uma classe a como tentar resumir a classe se o resumo pertence ou não à classe. Assim, o vetor de rótulos no caso de z permanece sempre, consistindo sempre em duas classes. Este é o algoritmo para a nossa abordagem de ajuste fino reformulado. Vamos ver o framework completo. O conjunto de dados é alimentado ao FEST, que executa a fase de ligação e extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipedia. Em seguida, reformula a tarefa em tarefas de classificação por sentença em pares, aplica o modelo de linguagem à nova tarefa e a probabilidade de saída para cada classe. Note que o modelo de linguagem já foi ajustado finamente sobre n menos um conjunto de dados usando um ajuste fino de múltiplas tarefas preliminar. Em seguida, usamos o vetor de saída do modelo de linguagem como uma nova característica gerada no número de classes para avaliar o nosso framework. Usamos um conjunto de dados de classificação tabular dezessete, que define tamanho de características, equilíbrio de domínio e desempenho inicial. Como base de conhecimento, usamos a Wikipedia. Desenvolvemos o nosso experimento como uma avaliação de deixar um de fora, quando treinamos o FEST sobre 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro partes e aplicamos uma validação cruzada. Em seguida, geramos a nova característica e avaliamos usando cinco classificadores de avaliação. Usamos na nossa experiência uma arquitetura baseada em pássaro. Aqui estão os resultados para o nosso experimento. Você pode ver que comparamos o nosso framework com o ajuste fino de conjunto de dados alvo, ajuste fino de tarefa alvo e ajuste fino preliminar de MTDN e que o nosso ajuste fino reformulado alcança o melhor desempenho, enquanto o MTDNN alcança uma melhoria de dois por cento em relação ao ajuste fino de conjunto de dados alvo. A nossa abordagem alcança uma melhoria de seis por cento. Quando olhamos para o pequeno conjunto de dados, podemos ver que o desempenho do MTDNN diminui e a melhoria da fase de ajuste fino de múltiplas tarefas preliminar diminui para um ponto cinco por cento, mas o nosso desempenho aumenta para 11 por cento em comparação com o ajuste fino de tarefa alvo. Em resumo, o FEST permite o enriquecimento de dados tabulares a partir de 35 amostras no nosso experimento. Usa uma arquitetura para todas as tarefas e conjuntos de dados, e mantém a cabeça do modelo. Mas adiciona uma fase de reformulação, aumenta o conjunto de treinamento e precisa de um valor alvo com significado semântico, para que possamos alimentá-lo ao modelo de linguagem e usá-lo no problema de classificação por sentença. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje vou apresentar o nosso trabalho de pesquisa sobre aprendizagem para raciocínio dedutivo, resolução de problemas de metro como extração de regiões complexas. Sou do Biance AI Lab e este é um trabalho conjunto com o Che da Universidade do Texas em Austin e o Wedu da SUDD. Primeiro, gostaria de falar sobre a nossa motivação para o raciocínio. Aqui, mostramos exemplos onde o raciocínio em várias etapas é útil. Esta figura é retirada do artigo de Pound, onde eles realizam prompts para resolver o problema do método em um cenário de aprendizagem futura. Assim, no lado esquerdo, podemos ver que, se fornecermos algumas amostras apenas com perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas se fornecermos mais descrição do raciocínio, o modelo é capaz de prever a descrição do raciocínio e também fazer uma previsão correta aqui, então é bom ter um raciocínio interpretável em várias etapas como saída e também achamos que o problema do método é uma aplicação direta para avaliar tais habilidades de raciocínio, então aqui, na nossa configuração de problema, dado as perguntas, precisamos resolver esta questão e obter as respostas numéricas, então nos nossos conjuntos de dados, também nos é dada a expressão matemática que leva a esta resposta específica também, então certas suposições também se aplicam como em trabalhos anteriores, assumimos que a precisão das quantidades é conhecida e só consideramos operadores básicos como adição, subtração, multiplicação, divisão e exponencial, além disso, operadores complicados podem ser realmente decompostos nestes operadores básicos, então trabalhos anteriores em resolução de problemas de método podem realmente ser categorizados em sequência para sequência e sequência para modelo de árvore, então o modelo tradicional de sequência para sequência converte a expressão em uma sequência específica para geração e é bastante fácil de implementar e pode generalizar para muitos problemas complicados diferentes, mas a desvantagem do desempenho não é realmente melhor do que o modelo de estrutura e falta de interpretabilidade para previsão, mas na verdade, esta direção ainda é bastante popular devido ao modelo transformer, então em modelos baseados em árvore, realmente estruturamos estas expressões na forma de árvore e seguimos uma travessia pré-ordem nas gerações de árvore, então aqui continuamos gerando os operadores até chegarmos às folhas, que são as quantidades, então a coisa boa aqui é que realmente nos dá esta estrutura de árvore binária e é um, mas mas mas na verdade é bastante contra-intuitivo porque geramos o operador primeiro e depois, no final, geramos as quantidades e a segunda coisa é que também contém alguns cálculos repetitivos, então aqui, se olharmos para esta expressão, três vezes a mais três é realmente gerada duas vezes, mas na verdade devemos reutilizar os resultados, então na nossa abordagem proposta, queremos resolver esses problemas de forma passo a passo e interpretável, então, por exemplo, aqui no segundo passo, podemos obter estes divisores que é 27 e também podemos nos referir de volta às perguntas originais para encontrar os conteúdos relevantes e nestes passos, obtemos os divisores, então e depois neste terceiro passo, realmente obtemos o quociente, tudo bem e depois destes três passos, podemos realmente reutilizar os resultados do segundo passo e então obter os resultados do quarto passo e então finalmente podemos obter os dividendos, então aqui realmente geramos toda a expressão diretamente em vez de gerar um único operador ou quantidades, então isso torna o processo mais preciso. Então, no nosso sistema dedutivo, começamos com um conjunto de quantidades apresentadas nas perguntas e também incluindo algumas constantes como nosso estado inicial inicial. Então, a expressão é representada por eij onde realizamos o operador de qi para qj e tal expressão é realmente dirigida, então também temos a subtração inversa aqui para representar a direção oposta, isso é bastante semelhante à extração de relações, então em um sistema dedutivo formal, no passo do tempo, aplicamos o operador entre o par qi e qj e então obtemos estas novas expressões, adicionamos-as aos estados seguintes para se tornar uma nova quantidade, então estes slides realmente visualizam a evolução dos estados onde continuamos adicionando expressão aos estados atuais, então nas nossas implementações de modelo, primeiro usamos um modelo de pré-treinamento que pode ser birds ou rawhoods e então codificamos a frase e então obtemos estas representações de quantidade, então uma vez que obtemos as representações de quantidade, podemos começar a fazer inferência aqui, mostramos um exemplo de q1 para obter a representação para q1 dividido por q2 e então vezes q3, primeiro obtemos a representação do par que é basicamente apenas a concatenação entre q1 e q2 e então aplicamos uma rede feed-forward que é parametrizada pelo operador e então finalmente obtemos a representação da expressão q1 dividido por q2, mas na verdade na prática na etapa de inferência, talvez possamos obter a expressão incorreta também, então aqui toda a possível expressão é igual a três vezes o número de operadores, então a coisa boa aqui é que podemos facilmente adicionar restrições para controlar esta busca, este espaço de busca, por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão no nosso espaço de busca, então no segundo passo, fazemos a mesma coisa, mas a única diferença é que temos mais uma quantidade, então esta quantidade vem da expressão calculada anterior, então finalmente podemos obter esta expressão final q3 vezes q4 e também podemos ver que o número de todas as possíveis expressões é diferente do passo anterior, então tal diferença torna difícil aplicar a busca por feixe porque a distribuição de probabilidade entre estes dois passos é desequilibrada, então o procedimento de treinamento é semelhante ao treinamento de um modelo de sequência para sequência onde otimizamos a perda em cada passo do tempo e aqui também usamos este tau para representar quando devemos terminar este processo de geração e aqui o espaço é diferente de sequência para sequência porque o espaço é diferente em cada tempo, enquanto que no modelo tradicional de sequência para sequência é o número de vocabulário e também permite impor certas restrições a partir do conhecimento prévio, então realizamos experimentos nos conjuntos de dados de problemas de método comumente usados, mwps, method3k, math qaA e swam e aqui brevemente mostramos os resultados comparados com as melhores abordagens anteriores, então a nossa melhor arma de desempenho é a Roberta deductive reason e na verdade não usamos a busca por feixe em contraste com abordagens óbvias que usam a busca por feixe, certo, então as melhores abordagens são frequentemente um modelo baseado em árvore, então no geral, o nosso raciocinador é capaz de selecionar significativamente a saída deste modelo baseado em árvore, mas podemos ver o número absoluto em math qaA ou swam não são realmente altos, então investigamos ainda mais os resultados em swam e este conjunto de dados é desafiador porque o autor tentou adicionar manualmente algo para confundir o modelo de NLP como adicionar informações disponíveis e quantidades extras, então na nossa previsão, encontramos que alguns dos valores intermediários são realmente negativos, por exemplo, nessas perguntas, estamos perguntando quantos maçãs Jake tem, mas temos algumas informações extras como 17 menos pitchachees e Stephen tem oito pitchachees, que é totalmente relevante, então o nosso modelo faz algumas previsões como esta que estão produzindo valores negativos e observamos que estas duas expressões realmente têm pontuações semelhantes, então podemos realmente limitar este espaço de busca removendo como esses resultados são negativos, para que possamos fazer a resposta correta, então hum, descobrimos ainda que tal restrição realmente melhora bastante para alguns modelos, por exemplo, para birds, melhoramos sete pontos e então para o modelo baseado em roberta, melhoramos dois pontos, então modelos de linguagem melhores têm melhores habilidades de compreensão de linguagem, então o número aqui é maior para roberta e menor para birds e também tentamos analisar a dificuldade por trás deste todo este conjunto de dados, assumimos que o número de quantidades não utilizadas pode ser considerado como informação relevante aqui, então hum, aqui podemos ver que hum, temos a soma a porcentagem de amostras que usamos quantidades e o conjunto de dados de swam tem a maior parte e aqui também mostramos o desempenho geral para essas amostras sem quantidades não utilizadas, então o desempenho geral é realmente maior do que o desempenho é realmente maior do que o desempenho geral, mas com essas amostras que têm quantidades não utilizadas é realmente pior do que o com, pior do que o desempenho geral de WPS, não temos realmente muitos casos de morte, então eu apenas hum, ignoro esta parte, então hum, finalmente, queremos mostrar a interpretabilidade através de um exemplo de apresentação de acidente, então aqui o nosso modelo realmente faz uma previsão errada no primeiro passo, então podemos realmente correlacionar esta expressão com a frase aqui, tudo bem, então achamos que esta frase pode estar enganando o modelo para hum, previsões incorretas, então aqui imprimindo outro 35 faz o modelo pensar que deve ser um operador de adição, então tentamos revisar a frase para ser algo como o número de árvores de pereira são 5 menos do que as árvores de maçã, então fazemos para transmitir uma semântica mais precisa de tal forma que o modelo consiga fazer a previsão correta, então este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo, então para concluir o nosso trabalho, primeiro, o nosso modelo é realmente bastante eficiente e somos capazes de fornecer um procedimento de resolução interpretável e podemos facilmente incorporar algum conhecimento prévio como restrição que pode ajudar a melhorar o desempenho. E a última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de rede, mas também a outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada entre os diferentes passos do tempo, também é bastante desafiador aplicar a estratégia de busca por feixe, então este é o fim da palestra e as perguntas são bem-vindas, obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine, e sou da Universidade de Maastricht. Vou apresentar meu trabalho sobre o John com o Jerry, que trata de um novo conjunto de dados para recuperação de artigos legais. Questões jurídicas são uma parte integrante da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência dispendiosa de um especialista legal ficam desprotegidos ou, pior, explorados. Todo o trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema eficaz de recuperação de artigos legais. Tal sistema poderia fornecer um serviço gratuito de assistência jurídica profissional para humanos não qualificados. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos legais. Dada uma pergunta simples sobre uma questão legal, como \"Qual é o risco se eu violar a confidencialidade profissional?\", é necessário um modelo para recuperar todos os artigos legais relevantes de um grande corpo de legislação. Esta tarefa de recuperação de informações vem com seu próprio conjunto de desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem complexa e jurídica para os estatutos. Essa diferença nas distribuições de linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois exige indiretamente um sistema de interpretação inerente que pode traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia dos estatutos. Além disso, o direito estatutário não é um conjunto de artigos independentes que podem ser tratados como uma fonte completa de informação por conta própria, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas no contexto geral, ou seja, juntamente com as informações suplementares de seus artigos vizinhos, os campos e subcampos aos quais pertencem e seu lugar na estrutura da lei. Por último, os artigos legais estão em pequenos parágrafos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação. Aqui, são documentos longos que podem ter até 6.000 palavras. Os recentes avanços em PLN despertaram grande interesse em muitas tarefas legais, como previsão de julgamentos legais ou revisão automática de contratos. No entanto, a recuperação de artigos legais permaneceu principalmente inalterada devido à falta de conjuntos de dados grandes e de alta qualidade rotulados. Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão nativo francês para estudar se o modelo de recuperação pode aproximar a eficiência e confiabilidade de um especialista legal para a tarefa de recuperação de artigos legais. O conjunto de dados de artigos legais belgas consiste em mais de 1.100 perguntas legais feitas por cidadãos belgas. Essas perguntas cobrem uma ampla gama de tópicos, desde família, habitação, dinheiro até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de vinte e seis mil seiscentos artigos legais dos códigos de leis belgas. Vamos agora falar sobre como coletamos este conjunto de dados. Primeiro, começamos compilando um grande corpus de artigos legais. Consideramos 32 códigos belgas disponíveis publicamente e extraímos todos os seus artigos, bem como os títulos das seções correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, fizemos parceria com o escritório de advocacia belga que recebe anualmente cerca de 400 e-mails de cidadãos belgas que pedem conselhos sobre uma questão legal pessoal. Tivemos a sorte de ter acesso aos seus sites, onde sua equipe de juristas experientes aborda as questões legais mais comuns dos belgas. Coletamos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por último, passamos as referências legais e filtramos as perguntas cujas referências não eram artigos de um dos códigos de leis considerados. As referências restantes foram correspondidas e convertidas para os IDs dos artigos correspondentes do nosso corpus. Eventualmente, terminamos com mil cento e oito perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes de um grande corpus de vinte e dois mil seiscentos e trinta e três artigos legais. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação de seus títulos subsequentes na estrutura da lei. Essas informações adicionais não são usadas no presente trabalho, mas podem ser de interesse para futuras pesquisas sobre recuperação de informações legais ou classificação de textos legais. Vamos analisar algumas características dos nossos conjuntos de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com uma mediana de 77 palavras, sendo que 142 deles excedem 1.000 palavras, sendo o mais longo de até cinco mil setecentos e noventa palavras. Como mencionado anteriormente, as perguntas cobrem uma ampla gama de tópicos, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os 15% restantes dizem respeito à segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de 32 códigos belgas diferentes que cobrem um grande número de tópicos legais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Dos vinte e dois mil seiscentos e trinta e três artigos, apenas 1.612 são referenciados como relevantes para pelo menos uma pergunta nos conjuntos de dados e cerca de 80% desses artigos citados vêm do Código Civil, Códigos Judiciais, Código de Investigação Criminal ou Códigos Penais, enquanto 18 dos 32 códigos têm menos de cinco artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esse código se concentra menos em indivíduos e suas preocupações. Em geral, o número médio de citações para esses artigos citados é dois, e menos de 25% deles são citados mais de cinco vezes. Usando nossos conjuntos de dados, avaliamos várias abordagens de recuperação, incluindo arquiteturas lexicais e densas. Dada uma consulta em um artigo, um modelo lexical atribui uma pontuação ao par de consulta e artigo, calculando a soma dos termos da consulta dos pesos de cada um desses termos naquele artigo. Experimentamos com as funções de classificação padrão TF-IDF e BM25. O principal problema com essas abordagens é que elas só podem recuperar artigos que contêm palavras-chave presentes na consulta. Para superar essa limitação, experimentamos com uma arquitetura baseada em redes neurais que pode capturar a relação semântica entre consultas e artigos. Usamos um modelo de codificador B que mapeia consultas e artigos em representações vetoriais densas e calcula uma pontuação de relevância entre um par de consulta e artigo pela similaridade de suas incorporações. Essas incorporações geralmente resultam de uma operação de pooling na saída de um modelo de incorporação de palavras. Primeiro, estudamos a eficácia dos codificadores Siamese em uma configuração de avaliação sem treinamento prévio, o que significa que os modelos de incorporação de palavras pré-treinados são aplicados diretamente, sem nenhum ajuste adicional. Experimentamos com codificadores de texto independentes do contexto, nomeadamente Word2Vec e FastText, e modelos de incorporação dependentes do contexto, nomeadamente Roberta e, mais especificamente, Camembert, que é um modelo Roberta francês. Além disso, treinamos nosso próprio modelo baseado em Camembert em todos os conjuntos de dados. Note que, para o treinamento, experimentamos com as duas variantes da arquitetura de codificador Siamese: Siamese, que usa um modelo de incorporação de palavras único que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado, e Tower, que usa dois modelos de incorporação de palavras independentes que codificam a consulta e o artigo separadamente em espaços de incorporação diferentes. Experimentamos com pooling de média, máxima e CLS, bem como produto escalar e coseno para calcular similaridades. Aqui estão os resultados de uma linha de base nos conjuntos de teste com os métodos lexicais acima, os codificadores Siamese avaliados em uma configuração sem treinamento prévio no meio e os codificadores de ajuste fino abaixo. No geral, os codificadores de ajuste fino superam significativamente todas as outras linhas de base. O modelo Tower melhora em relação à sua variante Siamese na recuperação em 100, mas apresenta desempenho semelhante nas outras métricas. Embora o BM25 tenha um desempenho significativamente inferior ao codificador de ajuste fino treinado, seu desempenho indica que ainda é uma linha de base forte para recuperação específica de domínio. Em relação à avaliação sem treinamento prévio dos codificadores Siamese, descobrimos que o uso direto das incorporações de um modelo Camembert pré-treinado sem otimização para a tarefa de recuperação de informações dá resultados pobres, o que é consistente com descobertas anteriores. Além disso, observamos que o codificador Siamese baseado em Word2Vec superou significativamente o FastText e o modelo baseado em pássaros, sugerindo que talvez as incorporações de nível de palavra pré-treinadas sejam mais apropriadas para a tarefa do que as incorporações de nível de caractere ou subpalavra quando usadas diretamente. Embora promissores, esses resultados sugerem uma ampla oportunidade de melhoria em comparação com um especialista em nível de habilidade que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Vamos concluir discutindo duas limitações de todos os conjuntos de dados. Primeiro, o corpus de artigos é limitado aos coletados dos 32 códigos belgas considerados, o que não cobre toda a lei belga, pois artigos de decretos, diretivas e ordenanças estão ausentes durante a construção do conjunto de dados. Todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas terminem com apenas uma fração do número inicial de artigos relevantes. Essa perda de informação implica que a resposta contida nos artigos relevantes restantes pode estar incompleta, embora ainda seja completamente apropriada. Segundo, devemos notar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso despejar meus inquilinos se eles fazem muito barulho?\" pode não ter uma resposta detalhada dentro do direito estatutário que quantifique um limite específico de ruído no qual o despejo é baixo. Em vez disso, o proprietário provavelmente deve confiar mais no direito consuetudinário e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz festas até as 2h da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos legais, e o domínio dos menos adequados ainda precisa ser determinado. Esperamos que todo o trabalho desperte interesse no desenvolvimento de modelos práticos e confiáveis de recuperação de artigos legais que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso trabalho em \"dotset encode\" nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, temos o prazer de apresentar nosso trabalho sobre vogais, um benchmark independente destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que criamos esse benchmark? Bem, nos últimos anos, assistimos a uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares de texto e imagem. Cada um desses modelos impulsiona o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, fundamentação de frases, então recebemos uma mensagem: as acurácias nesses benchmarks específicos de tarefas estão aumentando constantemente, mas sabemos o que os modelos realmente aprenderam? O que é que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta para esta imagem e esta frase para combinar e uma pontuação baixa para esta? Os modelos de visão e linguagem focam na coisa certa, ou focam em vieses, como mostrado por trabalhos anteriores? Para lançar mais luz sobre esse aspecto, propomos uma direção mais agnóstica em relação às tarefas e introduzimos vogais que testam a sensibilidade dos modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguísticas quanto as visuais. Nossas metas são existência, pluralidade, contagem, relações espaciais, ações e coerência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos, frustrando um método anteriormente aplicado apenas para frases nominais por Ravi Shekhar e colaboradores e para contagem por nós em trabalhos anteriores? Frustrar basicamente significa que pegamos a legenda de uma imagem e produzimos um frustrado alterando a legenda de tal forma que ela não descreve mais a imagem. Fazemos essas alterações de frases focando em seis partes específicas, como existência, pluralidade, contagem, relações espaciais, ações e coerência de entidades, onde cada parte pode consistir em um ou mais instrumentos, caso encontremos mais de uma maneira interessante de criar instâncias de frustração. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo da ação é trocado por uma ação diferente e outro em que os atores são trocados. Contagem e coerência também são peças que têm mais de um instrumento. E criamos esses frustrados garantindo que eles falhem em descrever a imagem, que sejam gramaticalmente e de outra forma frases válidas. Isso não é fácil de fazer porque uma legenda frustrada pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem cortar plantas, e grandes modelos de visão e linguagem poderiam captar isso. Portanto, para obter frustrados válidos, devemos agir primeiro. Utilizamos modelos de linguagem fortes para propor frustrados, segundo, usamos inferência de linguagem natural ou NLI curta para filtrar frustrados que ainda poderiam estar descrevendo a imagem. Ao construir frustrados, precisamos garantir que eles falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte justificativa: consideramos uma imagem como a premissa e sua legenda como a hipótese implícita. Além disso, consideramos a legenda como a premissa e o frustrado como sua hipótese. Se um modelo de NLI prever que o frustrado contradiz ou é neutro em relação à legenda, consideramos isso como um indicador de um frustrado válido. Se um NLI prever que o frustrado é implícito pela legenda, ele não pode ser um bom frustrado, pois, por transitividade, dará uma descrição verdadeira da imagem, e filtramos esses frustrados. Mas esse procedimento não é perfeito, é apenas um indicador de frustrados válidos. Portanto, como terceira medida para gerar frustrados válidos, empregamos anotadores humanos para validar os dados usados no VALSE. Assim, após a filtragem e avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Note que o VALSE não fornece dados de treinamento, mas apenas dados de teste, pois é um benchmark de teste sem conhecimento prévio. Ele foi projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino só permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos sabemos que esses modelos gostam de trapacear e tomar atalhos. E como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem em vogais, nomeadamente com CLIP, ALEX, BERT, WILBERT e WILBERT 12 em 1, e VISUAL BIRD. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos em classificar pares de frases de imagem em legendas e frustrados. Talvez mais relevante para este vídeo, mostraremos nossa métrica mais permissiva, a precisão emparelhada, que mede se a pontuação de alinhamento de frase de imagem é maior para o par de texto de imagem correto do que para seu par frustrado. Para mais métricas e resultados sobre elas, confira nosso artigo. Os resultados com precisão emparelhada são mostrados aqui e são consistentes com os resultados que obtivemos com as outras métricas. ISSO é que o melhor desempenho sem conhecimento prévio é alcançado pelo WILBERT 12 em um, seguido pelo WILBERT, ALEX, BERT, CLIP e, finalmente, VISUAL BIRD. É notável como instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos pelo WILBERT 12 em um, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhum dos outros pedaços pode ser resolvido de forma confiável em nossas configurações de frustração adversarial. Vemos a partir dos instrumentos de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos versus múltiplos ou contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo que suportados por vieses de plausibilidade, como vemos na peça de ações da peça de referência. Descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem. Como verificação de sanidade e porque é um experimento interessante, também avaliamos dois modelos apenas de texto, GPT1 e GPT2, para avaliar se o VALSE é solucionável por esses modelos unimodais, calculando a perplexidade da legenda correta e do frustrado. Sem imagem aqui e prevendo a entrada com a perplexidade mais baixa. Se a perplexidade for maior para o frustrado, consideramos isso como uma indicação de que a legenda frustrada pode sofrer de viés de plausibilidade ou outros vieses linguísticos. É interessante ver que, em alguns casos, os modelos GPT apenas de texto capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Para resumir, o VALSE é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem, testando rigorosamente suas capacidades de fundamentação visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados em sua presença em imagens, como mostrado pela peça de existência, mas lutam para fundamentar sua interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de incentivar a comunidade a usar vogais para medir o progresso em direção à fundamentação de linguagem com modelos de visão e linguagem. E mais ainda, as vogais poderiam ser usadas como uma avaliação indireta de conjuntos de dados, pois os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelos VAs. Se estiver interessado, confira os dados do VALSE no GitHub e, se tiver alguma dúvida, não hesite em nos contatar."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisura da Universidade de Tóquio. Vou apresentar um artigo intitulado \"O En Sum: Um Desset em Grande Escala para Listagem Automática de Notas viamit Lo Sumization\". Vou explicar nesta ordem: primeiro, vou apresentar a notação Li automática em que estamos trabalhando nesta pesquisa. ReaseNote é um documento técnico que resume as alterações distribuídas com cada lançamento de um produto de software. A imagem mostra as notas de lançamento para a versão dois ponto seis ponto quatro da biblioteca Bujs. Essas notas desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas para preparar manualmente, portanto, será muito útil poder gerar automaticamente notas de lançamento de alta qualidade. Vou me referir a duas pesquisas anteriores sobre geração automática de nós de lista: a primeira é um sistema chamado alena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de mudanças para extrair diferenças principais, mudanças na biblioteca e mudanças nos documentos a partir das diferenças entre doenças e, finalmente, combinando-as. A característica mais notável deste sistema é o extrativo de problemas no canto superior direito, que deve ser vinculado ao Jira, o ecossistema de problemas e só pode ser aplicado a projetos que usam Jira, em palavras, não pode ser usado para muitos projetos no GitHub. A segunda é Grif, recentemente anunciada em vinte vinte. Está disponível na internet e pode ser armazenada via pi. Este sistema tem um modelo de classificação de texto baseado em execução simples e produz um dos cinco problemas, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna uma fita correta ou correções de bugs. O conjunto de dados de treinamento é bastante pequeno, cerca de 5000, e será mostrado nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas há problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente recursos de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumização de classificador de alta qualidade usando apenas a mensagem de commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos nosso próprio conjunto de dados e alguns dados consistindo de cerca de 82.000 peças de dados, corrigindo dados de repositórios públicos do Gitub usando a API do Git. Em seguida, descrevo nossos dados aqui está um exemplo de atualização. O lado esquerdo é uma mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são niveladas como melhorias de faces, etc. Estabelecemos uma tarefa que leva as mensagens de commit como entrada e produz as notas de lançamento. Isso pode ser considerado uma tarefa de sumarização. Definimos quatro níveis: recursos, implementações, correções de bugs, deprecações, remoções e mudanças drásticas. Estes foram definidos com base no uso anterior e outros fatores. Há uma nota no canto inferior direito e extraída da lista de nós mostrada no canto inferior esquerdo. Neste momento, é necessário detectar os quatro coelhos que foram definidos anteriormente, mas os níveis nem sempre são consistentes com cada um. Por exemplo, o nível de melhoria inclui melhorias, aprimoramentos, otimizações e assim por diante. Preparamos uma lista de vocabulário de níveis de estudo para cada uma dessas variações notacionais. Use-a para detectar a classe de nó de risco e corrigir o texto do resto que se segue como a frase de nó de risco para a classe. Em seguida, é uma mensagem de commit. As mensagens de commit não estão vinculadas a cada raça, como mostrado na imagem abaixo. Se a versão atual for 2.5 a 19, precisamos identificar a versão de lançamento anterior 2.5 a 18 e obtê-la. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de lançamentos e olhar o antes e depois. Criamos uma correspondência heurística para obter os dados de versões anteriores e análise de dados. No final, 7200 repositórios e 82.000 peças de dados foram corrigidos. O número médio de tokens razoáveis é 63, o que é bastante alto para a tarefa de sumarização. O número de tokens únicos também é bastante rico em oito mil oitenta e trinta mil. Isso se deve ao grande número de classes únicas e nomes de métodos encontrados no repositório. Em seguida, explicarei o método proposto. O modelo de sumarização extractiva e abstractiva transversal consiste em dois módulos neurais: um classificador usando bot ou code bot e o gerador usando but first G usa um classificador para classificar cada mensagem de commit em cinco classes de nó base: recursos, melhorias, correções de bugs, deprecações, press e outros. As mensagens de commit classificadas como outros são descartadas. Em seguida, ela aplica um gerador aos quatro documentos de coelho de forma independente e gera notas de lançamento para cada classe. Nesta tarefa, as correspondências diretas entre as mensagens de commit e as notas de lançamento não são conhecidas. Portanto, para treinar o classificador, atribuímos variáveis pseudo a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit. Modelamos a abordagem de sumarização abstractiva por classes definidas por dois métodos. O primeiro modelo, que chamamos de GS single, consiste em uma única rede sexual e gera um único texto longo que não é. Dê uma concatenação das mensagens de commit de entrada. O texto de saída pode ser dividido em segmentos de arquivo de classe com base em símbolos de endpoint específicos da classe. O segundo método, que chamamos de shes much, consiste em quatro redes sec a sec diferentes, cada uma correspondendo a uma das classes de nó menos. Tudo bem, deixe-me explicar a experiência. Cinco métodos foram comparados: gs, shes single, shes much, cluster e estudo anterior Gri. Em alguns casos, essas notas são produzidas em múltiplas frases, uma vez que é difícil corrigir o número de frases para zero, elas são combinadas com espaços e tratadas como uma única frase longa. O azul é penalizado quando o sistema produz uma frase curta. Essa penalidade resulta em um valor de bre mais baixo nos resultados dos experimentos descritos a seguir. Finalmente, também calculamos uma especificidade, porque o azul e o azul não podem ser calculados se as notas de lista estiverem vazias. Uma especificidade alta significa que o modelo produz corretamente textos vazios em casos onde as notas de lançamento assumem o vazio. Aqui estão os resultados, uma vez que o conjunto de dados contém análises de e-mail, etc. Também avaliamos o conjunto de dados limpo, que os exclui. G e Gs alcançaram escores de erro mais de 10 pontos mais altos do que a linha de base. Em particular, no conjunto de teste coreano, a lacuna de pontuação entre o método proposto e a linha de base saltou para mais de vinte pontos. Esses resultados indicam que Gs e Gs são significativamente eficazes. Gs obteve uma pontuação de perda melhor do que GAS, sugerindo que combinar um classificador sob o gerador é eficaz no treinamento do classificador usando servidores. A alta cobertura de gs pode ser alcançada adequadamente porque o classificador pode se concentrar em selecionar mensagens de commit relevantes para cada classe. Shes much tendeu a ser maior do que she is single, sugerindo que também é eficaz desenvolver independentemente diferentes modelos de sumarização construtiva para cada classe de nó de peça. Aqui, uma análise de erro. Os métodos she tendem a produzir frases mais curtas do que a referência humana. Como na figura à direita, a frase de referência tem três ou quatro frases, enquanto CSS tem apenas uma. A razão para a relutância deste modelo é que, nos dados de treinamento, apenas trinta por cento das frases estão presentes no nível de recursos e quarenta por cento no nível de melhorias. Além disso, os métodos cs não podem gerar notas de lista precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito bagunçada e a frase completa não pode ser gerada sem diferença para as prerrogativas ou problemas correspondentes. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma única frase, mas falha em fazê-lo. Finalmente, uma conclusão. Construímos um novo conjunto de dados para geração automática de notas. Também formulamos a tarefa de inserir mensagens de commit e resumí-las de modo que seja aplicável a todos os projetos escritos em inglês. Nossos experimentos mostram que o método proposto gera menos ruído em notas de lançamento de cobertura mais alta do que a linha de base. Por favor, confira nossos dados no GitHub. Obrigado."}
