{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle le Safari, et je vais présenter notre article sur l'enrichissement automatique de données tabulaires à l'aide d'architectures de transformateurs finement ajustés. Les scientifiques analysent généralement les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données, mais ces caractéristiques sont parfois limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. L'objectif de notre recherche est l'enrichissement automatique de données tabulaires à l'aide de textes libres provenant de sources externes. Supposons que nous ayons un ensemble de données tabulaires et une base de connaissances. Nous avons besoin d'un processus automatique qui implique le couplage d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre FEST est exactement ce processus automatique. Examinons un exemple dans les ensembles de données alimentés à FEST. Dans cet exemple, l'ensemble de données est un ensemble de données universitaires, et son objectif est de classer les universités en universités de faible rang et en universités de haut rang. Comme base de connaissances, nous utilisons Wikipédia. La première phase de FEST est le couplage d'entités. Chaque entité dans cet exemple, le nom de l'université, est liée à une entité dans la base de connaissances. Le texte des entités de la base de connaissances est extrait et ajouté à l'ensemble de données. Dans cet exemple, le texte est l'abstract de la page Wikipédia. Maintenant, nous devons générer ou extraire des caractéristiques du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse de texte. C'est la principale nouveauté de cet article, et je vais m'y attarder dans les diapositives suivantes. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques. Nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Nous générons d'abord autant de caractéristiques que le nombre de classes de l'ensemble de données original. Dans cet exemple, l'ensemble de données original a deux classes, donc nous générons deux nouvelles caractéristiques. Mais si l'ensemble de données a cinq classes, nous générons d'abord cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'analyse hors texte, qui sont des modèles de langage basés sur les transformateurs comme BERT GPT X et d'autres. Il est peu probable que nous puissions entraîner un modèle de langage à partir des ensembles de données d'entrée, donc une approche naïve serait un affinage ciblé. Dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné et l'affiner sur l'ensemble de données cible. Dans cet exemple, pour affiner le modèle de langage afin de classer le texte en classes, abstract en classes, bas ou haut, nous recevons la sortie du modèle de langage, qui est la probabilité pour chaque classe, et nous utilisons comme nouvelles caractéristiques. Le problème avec cette approche est que l'ensemble de données peut avoir peu d'entités distinctes. Dans notre expérience, presque la moitié des ensembles de données contiennent moins de 400 échantillons et le plus petit ensemble de données contient 35 échantillons. Ainsi, affiner un modèle de langage sur cet ensemble de données serait inefficace. Mais nous pouvons utiliser des connaissances antérieures sur les ensembles de données pré-analysés. Comme FEST s'applique à plusieurs ensembles de données, nous pouvons utiliser les n-1 ensembles de données pour recueillir des informations sur les n-1 ensembles de données et utiliser ces informations lorsque nous analysons l'ensemble de données cible. Ce que nous proposons est d'ajouter une autre phase d'affinage, une phase d'affinage multitâche préliminaire lorsque nous affinons le modèle de langage sur n-1 ensembles de données, puis nous exécutons une autre phase d'affinage qui est un affinage ciblé lorsque nous affinons le modèle de langage sur le n-ième ensemble de données cible. L'état de l'art en affinage multitâche appelé DNN en TDNN maintient un nombre de têtes égal au nombre de tâches dans l'ensemble d'entraînement. Donc, dans cet exemple, s'il y a quatre tâches dans l'ensemble d'entraînement, DNN maintient quatre têtes comme vous pouvez le voir sur l'image. Il échantillonne un lot aléatoire à partir de l'ensemble d'entraînement. Si le lot aléatoire appartient à un ensemble de tâches de classification de phrases, par exemple, il exécute un passage avant et arrière à travers la première tête. Si le lot aléatoire appartient à une tâche de classement par paires, il exécute un passage avant et arrière à travers la dernière tête. Dans notre scénario, un ensemble de données varie le nombre de classes, donc il y a de nombreuses tâches. Un DNN vide doit maintenir autant de têtes que de classes, de couches de sortie et, de plus, un DNN vide doit initialement ajouter de nouvelles têtes pour un nouvel ensemble de données avec une nouvelle tâche. Notre approche appelée affinage par reformulation de tâches est que, au lieu de maintenir plusieurs têtes, nous reformulons chaque ensemble de données en un problème de classification par phrase, qui est un problème de deux classes. Examinons un exemple ici. Notre ensemble de données d'entrée se compose d'entités, de caractéristiques, de texte et de classes. Nous reformulons la tâche de classer le texte en bas et haut pour classer le texte de l'abstract et la classe en vrai ou faux, ou en d'autres termes, nous entraînons le modèle de langage pour classer un abstract et une classe a en a pour essayer d'abstraire la classe si l'abstract appartient ou non à la classe. Ainsi, le vecteur d'étiquettes dans le cas de z reste toujours composé de deux classes. C'est l'algorithme pour notre approche d'affinage reformulée. Examinons le cadre complet. Les données sont alimentées à FEST, puis FEST exécute la phase de couplage. Il extrait le texte de la base de connaissances, qui dans cet exemple est l'abstract de la page Wikipédia. Puis il reformule la tâche en tâches de classification par phrase par paire. Il applique le modèle de langage à la nouvelle tâche et obtient la probabilité de sortie pour chaque classe. Notez que le modèle de langage est déjà affiné sur n-1 ensembles de données à l'aide d'un affinage multitâche préliminaire. Ensuite, nous utilisons le vecteur de sortie du modèle de langage comme une nouvelle caractéristique générée dans le nombre de classes pour évaluer notre cadre. Nous utilisons un ensemble de données de classification tabulaire de dix-sept qui définit la taille des caractéristiques, l'équilibre du domaine et la performance initiale. Comme déchet de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation par exclusion. Lorsque nous entraînons FEST sur 16 ensembles de données et l'appliquons au 17e ensemble de données, nous divisons également chaque ensemble de données en quatre ensembles et appliquons une validation croisée en faux. Puis nous générons la nouvelle caractéristique et les évaluons à l'aide de cinq classificateurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur l'architecture basée sur les oiseaux. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre cadre à l'affinage ciblé sur un ensemble de données, à l'affinage ciblé sur une tâche et à l'affinage multitâche préliminaire. Notre reformulation de l'affinage atteint le meilleur résultat, la meilleure performance, tandis que MTDNN atteint deux pour cent d'amélioration par rapport à l'affinage sur l'ensemble de données cible. Notre approche atteint six pour cent d'amélioration. Lorsque nous regardons les petits ensembles de données, nous pouvons voir que la performance de MTDNN diminue et que l'amélioration de la phase d'affinage multitâche préliminaire diminue à un point cinq pour cent, mais notre performance augmente à 11 pour cent par rapport à l'affinage ciblé. En résumé, FEST permet l'enrichissement automatique de données à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour toutes les tâches et les ensembles de données, et il maintient la tête du modèle. Mais il ajoute une phase de reformulation, augmente l'ensemble d'entraînement et a besoin d'une valeur cible avec un sens sémantique, afin que nous puissions l'alimenter dans le modèle de langage et l'utiliser dans le problème de classification par phrase. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais présenter notre travail de recherche sur l'apprentissage du raisonnement déductif, la résolution de problèmes de métro en tant qu'extraction de régions complexes. Je suis All du Biance AI Lab et c'est un travail conjoint avec Che de l'Université du Texas à Austin et Wedu de SUDD. Tout d'abord, je voudrais parler de notre motivation pour le raisonnement. Ainsi, ici, nous montrons des exemples où le raisonnement en plusieurs étapes est utile. Cette figure est tirée du papier de Pound où ils effectuent un prompting pour résoudre le problème de méthode dans un scénario d'apprentissage futur. Ainsi, à gauche, nous pouvons voir que si nous donnons quelques exemples avec juste des questions et des réponses, nous ne sommes peut-être pas en mesure d'obtenir les bonnes réponses. Mais si nous donnons plus de description de raisonnement, le modèle est capable de prédire la description de raisonnement et aussi de faire une prédiction correcte ici, il est donc bon d'avoir un raisonnement en plusieurs étapes interprétable comme sortie et nous pensons également que le problème de méthode est une application directe pour évaluer de telles capacités de raisonnement, donc ici, dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques, donc dans nos ensembles de données, nous avons également la formule mathématique qui mène à cette réponse particulière ainsi que certaines hypothèses s'appliquent comme dans les travaux précédents, nous supposons que la précision des quantités est connue et nous ne considérons que des opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentielle, de plus, les opérateurs compliqués peuvent en fait être décomposés en ces opérateurs de base, donc les travaux précédents sur la résolution de problèmes de méthode peuvent en fait être classés en séquence à séquence et séquence à modèle d'arbre, donc le modèle traditionnel de séquence à séquence convertit l'expression en une séquence spécifique pour la génération et il est assez facile à mettre en œuvre et il peut se généraliser à de nombreux problèmes compliqués différents, mais l'inconvénient de la performance n'est généralement pas meilleur que le modèle de structure et il manque d'interprétabilité pour la prédiction, mais en fait, cette direction est toujours assez populaire à cause du modèle transformateur, donc dans les modèles basés sur l'arbre, nous structurons en fait ces expressions sous forme d'arbre et suivons un parcours pré-ordre dans les générations d'arbre, donc ici, nous continuons à générer les opérateurs jusqu'à ce que nous atteignons les feuilles qui sont les quantités, donc ici, la bonne chose est qu'il donne en fait cette structure d'arbre binaire et c'est um mais mais mais en fait, c'est assez contre-intuitif parce que nous générons l'opérateur en premier et puis à la fin, nous générons les quantités et la deuxième chose est qu'il contient aussi des calculs répétitifs, donc ici, si nous regardons cette expression, a fois 3 plus 3 est en fait générée deux fois, mais en fait, nous devrions réutiliser les résultats, donc dans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et de manière interprétable, donc par exemple ici, dans la deuxième étape, nous pouvons obtenir ces diviseurs qui est 27 et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent et dans ces étapes, nous obtenons les diviseurs, donc et puis à cette troisième étape, nous obtenons en fait le quotient, d'accord, et après ces trois étapes, nous pouvons en fait réutiliser les résultats de la deuxième étape et puis obtenir les résultats de la quatrième étape et puis finalement, nous pouvons obtenir les dividendes, donc ici, nous générons en fait toute l'expression directement plutôt que de générer un seul opérateur ou des quantités, donc cela rend le processus plus précis. Donc, dans notre système déductif, nous commençons d'abord avec un tas de quantités présentées dans les questions et incluant également quelques constantes comme notre état initial initial. Donc, l'expression est représentée par eij où nous effectuons l'opérateur de qi à qj et une telle expression est en fait dirigée, donc nous avons également la soustraction inverse ici pour représenter la direction opposée, cela est assez similaire à l'extraction de relations, donc dans un système formel déductif, à l'étape de temps, nous appliquons l'opérateur entre la paire qi et qj et puis nous obtenons ces nouvelles expressions, nous les ajoutons aux états suivants pour devenir une nouvelle quantité, donc ces diapositives visualisent en fait l'évolution des états où nous continuons à ajouter l'expression aux états actuels, donc dans nos implémentations de modèle, nous utilisons d'abord un modèle de pré-entraînement qui peut être birds ou rawhoods et puis nous codons la phrase et puis nous obtenons ces représentations de quantités, donc une fois que nous obtenons les représentations de quantités, nous pouvons commencer à faire des inférences ici, nous montrons un exemple de q1 pour obtenir la représentation pour q1 divisé par q2 et puis multiplié par q3, d'abord, nous obtenons la représentation de paire qui est essentiellement juste la concaténation entre q1 et q2 et puis nous appliquons un réseau de feed forward qui est paramétré par l'opérateur et puis finalement, nous obtenons la représentation d'expression q1 divisé par q2 mais en fait, en pratique, dans l'étape d'inférence, nous pourrions être en mesure d'obtenir l'expression incorrecte également, donc ici, toutes les expressions possibles sont égales à trois fois le nombre d'opérateurs, donc la chose agréable ici est que nous pouvons facilement ajouter des contraintes pour contrôler cette recherche, cet espace de recherche, par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche, donc dans la deuxième étape, nous faisons la même chose, mais la seule différence est qu'il y a une quantité de plus, donc cette quantité vient de l'expression calculée précédente, donc finalement, nous pouvons obtenir cette expression finale q3 fois q4 et nous pouvons également voir le nombre de toutes les expressions possibles est différent de l'étape précédente, donc une telle différence rend difficile l'application de la recherche par faisceau parce que la distribution de probabilité entre ces deux étapes est déséquilibrée, donc la procédure d'entraînement est similaire à l'entraînement d'un modèle de séquence à séquence où nous optimisons la perte à chaque étape de temps et ici, nous utilisons également ce tau pour représenter quand nous devrions terminer ce processus de génération et ici, l'espace est différent de la séquence à séquence parce que l'espace est différent à chaque fois que, alors que dans le modèle traditionnel de séquence à séquence, c'est le nombre de vocabulaire et cela permet également d'imposer certaines contraintes à partir du savoir préalable, donc nous menons des expériences sur les ensembles de données de problèmes de méthode couramment utilisés, mwps, method3k, math qaA et swam et ici, nous montrons brièvement les résultats comparés aux meilleures approches précédentes, donc notre meilleur modèle est Roberta déductive reason et en fait, nous n'utilisons pas la recherche par faisceau contrairement aux approches évidentes utilisant la recherche par faisceau, d'accord, donc les meilleures approches sont souvent un modèle basé sur l'arbre, donc dans l'ensemble, notre raisonneur est capable de sélectionner significativement la sortie de ce modèle basé sur l'arbre, mais nous pouvons voir le nombre absolu sur math qaA ou swam ne sont pas vraiment élevés, donc nous enquêtons davantage sur les résultats sur swam et cet ensemble de données est difficile parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle de NLP comme ajouter des informations disponibles et des quantités supplémentaires, donc dans notre prédiction, nous trouvons que certaines des valeurs intermédiaires sont en fait négatives, par exemple, dans ces questions, nous demandons combien de pommes Jake a, mais nous avons des informations supplémentaires comme 17 moins de pitchachees et Stephen a huit pitchachees qui est totalementlevant, donc notre modèle fait certaines prédictions comme celle-ci qui produisent des valeurs négatives et nous observons que ces deux expressions ont en fait des scores similaires, donc nous pouvons en fait limiter cet espace de recherche en supprimant comme ces résultats sont négatifs, de sorte que nous puissions rendre la réponse correcte, donc um nous trouvons en fait que cette contrainte améliore assez beaucoup pour pour pour certains modèles, par exemple, pour birds, nous avons amélioré sept points et puis pour le modèle basé sur roberta, nous avons en fait amélioré deux points, donc un meilleur modèle de langage a de meilleures capacités de compréhension du langage, donc le nombre ici est plus élevé pour roberta et plus bas pour pour birds et nous avons également essayé d'analyser la difficulté derrière tout cet ensemble de données, nous supposons que le nombre de quantités non utilisées peut être considéré comme une information pertinente ici, donc uh ici, nous pouvons voir que uh nous nous sommes regroupés le pourcentage d'échantillons nous ayant des quantités et l'ensemble de données swam a la plus grande partie et ici, nous montrons également la performance globale pour ces échantillons sans quantités non utilisées, donc la performance globale est en fait plus élevée que la performance est en fait plus élevée que la performance globale mais avec ces échantillons ayant des quantités non utilisées est en fait bien pire que le com uh bien pire que le uh la performance globale de WPS, nous n'avons pas vraiment uh trop de cas de décès, donc je viens de uh ignorer cette partie, donc uh finalement, nous voulons montrer l'interprétabilité à travers un exemple de présentation en crash, donc ici, notre modèle fait en fait une prédiction incorrecte à l'at l'at première étape, donc nous pouvons en fait corréler cette expression avec la phrase ici, d'accord, donc nous pensons que cette phrase pourrait induire le modèle en erreur pour uh des prédictions incorrectes, donc ici, l'impression d'un autre 35 fait que le modèle pense qu'il devrait y avoir un opérateur d'addition, donc nous essayons de réviser la phrase pour être quelque chose comme le nombre d'arbres de poire sont 5 de moins que les arbres de pomme, donc nous le rendons pour transmettre une sémantique plus précise telle que le modèle est capable de faire la prédiction correcte, donc cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle, donc pour conclure notre travail, d'abord, notre modèle est en fait assez efficace et nous sommes capables de fournir un processus de résolution interprétable et nous pouvons facilement incorporer un savoir préalable comme contrainte qui peut aider à améliorer la performance. Et la dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de réseau, mais aussi à d'autres tâches qui impliquent un raisonnement en plusieurs étapes. Mais nous avons également certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes ou de constantes, la consommation de mémoire pourrait être assez élevée. Et la deuxième chose est que comme mentionné parce que la distribution de probabilité est déséquilibrée entre à différents temps, il est également assez difficile d'appliquer la recherche par faisceau, donc c'est la fin du discours et les questions sont les bienvenues, merci"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine, et je viens de l'Université de Maastricht. Je vais présenter mon travail sur John avec Jerry, qui porte sur un nouveau jeu de données pour la recherche d'articles légaux. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus juridiques fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique se retrouvent sans protection ou, pire, exploités. Tout le travail vise à combler le fossé entre les gens et la loi en développant un système de récupération efficace pour les articles légaux. Un tel système pourrait fournir un service d'aide juridique professionnelle gratuit pour les humains non qualifiés. Avant de plonger dans la contribution principale de ce travail, décrivons d'abord le problème de la récupération d'articles légaux. Étant donné une question simple sur une question juridique telle que \"Quel est le risque si je viole la confidentialité professionnelle ?\", un modèle est nécessaire pour récupérer tous les articles légaux pertinents à partir d'un vaste corpus législatif. Cette tâche de récupération d'informations présente son propre ensemble de défis. Tout d'abord, elle traite de deux types de langage : le langage naturel commun pour les questions et le langage juridique complexe pour les statuts. Cette différence dans les distributions de langage rend plus difficile pour un système de récupérer des candidats pertinents car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des statuts. De plus, le droit législatif n'est pas un empilement d'articles indépendants qui peuvent être traités comme une source d'information complète à eux seuls, comme les nouvelles ou les recettes, par exemple. Au lieu de cela, c'est une collection structurée de dispositions légales qui n'ont un sens complet que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires provenant de leurs articles voisins, des champs et sous-champs auxquels ils appartiennent, et de leur place dans la structure de la loi. Enfin, les articles légaux sont en petits paragraphes qui sont généralement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, ce sont de longs documents pouvant aller jusqu'à 6 000 mots. Les récentes avancées en NLP ont suscité un grand intérêt pour de nombreuses tâches juridiques telles que la prédiction de jugements juridiques ou la révision automatique de contrats, mais la récupération d'articles légaux est restée principalement en marge en raison du manque de jeux de données étiquetés de grande taille et de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur les citoyens francophones pour étudier si un modèle de récupération peut s'approcher de l'efficacité et de la fiabilité d'un expert juridique pour la tâche de récupération d'articles légaux ou de récupération d'articles légaux belges. Le jeu de données belges comprend plus de 1 100 questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets allant de la famille, au logement, à l'argent, au travail et à la sécurité sociale. Chacune d'elles a été étiquetée par des juristes expérimentés avec des références à des articles pertinents d'un corpus de plus de vingt mille six cents articles légaux des codes de droit belges. Parlons maintenant de la manière dont nous avons collecté ce jeu de données. Nous avons commencé par compiler un vaste corpus d'articles juridiques. Nous avons considéré 32 codes belges disponibles au public et avons extrait tous leurs articles ainsi que les en-têtes de section correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec le cabinet d'avocats belge qui reçoit chaque année environ 400 000 e-mails de citoyens belges qui demandent des conseils sur une question juridique personnelle. Nous avons eu la chance d'accéder à leurs sites web où leur équipe de juristes expérimentés aborde les problèmes juridiques les plus courants des Belges. Nous avons collecté des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux statuts pertinents. Enfin, nous avons passé les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avions considérés. Les références restantes ont été mises en correspondance et converties en les identifiants d'articles correspondants de notre corpus. Nous avons finalement obtenu mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents d'un vaste corpus de vingt-deux mille six cent trente-trois articles légaux. De plus, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de leurs en-têtes ultérieurs dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le travail présent mais pourraient être d'intérêt pour les futures recherches sur la récupération d'informations juridiques ou la classification de textes juridiques. Examinons quelques caractéristiques de nos jeux de données. Les questions font entre 5 et 44 mots de long avec une médiane de 40 mots. Les articles sont beaucoup plus longs avec une longueur médiane de 77 mots, dont 142 dépassent 1 000 mots, le plus long étant jusqu'à cinq mille sept cent 90 mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets avec environ 85 % d'entre elles portant soit sur la famille, le logement, l'argent, soit sur la justice, tandis que les 15 % restants concernent soit la sécurité sociale, les étrangers, soit le travail. Les articles sont également très divers car ils proviennent de 32 codes belges différents qui couvrent un grand nombre de sujets juridiques. Voici le nombre total d'articles collectés de chacun de ces codes belges. Sur les vingt-deux mille six cent trente-trois articles, seulement 1 612 sont référencés comme pertinents à au moins une question dans les jeux de données et environ 80 % de ces articles cités proviennent soit du code civil, des codes judiciaires, du code d'enquête criminelle ou des codes pénaux, tandis que 18 des 32 codes ont moins de cinq articles mentionnés comme pertinents à au moins une question, ce qui peut s'expliquer par le fait que ce code se concentre moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de deux et moins de 25 % d'entre eux sont cités plus de cinq fois. En utilisant nos jeux de données, nous avons évalué plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête dans un article, un modèle lexical attribue un score à la paire requête-article en calculant la somme sur les termes de la requête des poids de chacun de ces termes dans cet article. Nous expérimentons avec les fonctions de classement standard TFIDf et bm25. Le principal problème avec ces approches est qu'elles ne peuvent récupérer que les articles contenant des mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur les réseaux neuronaux qui peut capturer la relation sémantique entre les requêtes et les articles. Nous utilisons un modèle d'encodeur b qui mappe les requêtes et les articles dans des représentations vectorielles denses et calcule un score de pertinence entre une paire requête-article par la similarité de leurs emboîtures. Ces emboîtures résultent généralement d'une opération de pooling sur la sortie d'un modèle d'emboîtement de mots. Tout d'abord, nous étudions l'efficacité des encodeurs Siamese dans une configuration d'évaluation en zéro coup, ce qui signifie que des modèles d'emboîtement de mots pré-entraînés sont appliqués tels quels, sans aucun réglage supplémentaire. Nous expérimentons avec un encodeur de texte indépendant du contexte, à savoir word to vec et fast text, et des modèles d'emboîtement contextuels, à savoir Roberta et plus spécifiquement Camembert, qui est un modèle français de roberta. De plus, nous avons entraîné notre propre modèle basé sur Camembert sur tous les jeux de données. Notez que pour l'entraînement, nous expérimentons avec les deux saveurs de l'architecture encodeur Siamese, qui utilise un modèle d'emboîtement de mots unique qui mappe la requête et l'article ensemble dans un espace vectoriel dense partagé, et to tower qui utilise deux modèles d'emboîtement de mots indépendants qui encodent la requête et l'article séparément dans différents espaces d'emboîtement. Nous expérimentons avec le pooling mean max et CLls ainsi que le produit scalaire et le cosinus pour le calcul des similarités. Voici les résultats d'une ligne de base sur les ensembles de test avec les méthodes lexicales ci-dessus, les encodeurs Siamese évalués dans une configuration en zéro coup au milieu et les encodeurs fine-tunés ci-dessous. Dans l'ensemble, les encodeurs fine-tunés surpassent de manière significative tous les autres modèles de base. Le modèle à deux tours améliore son variant Siamese en termes de rappel à 100 mais se comporte de manière similaire sur les autres métriques. Bien que bm25 se soit sous-performé par rapport au encodeur fine-tuné, sa performance indique qu'il s'agit encore d'une ligne de base solide pour la récupération spécifique à un domaine. En ce qui concerne l'évaluation en zéro coup des encodeurs Siamese, nous constatons que l'utilisation directe des emboîtures d'un modèle Camembert pré-entraîné sans optimisation pour la tâche de récupération d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats précédents. De plus, nous observons que l'encodeur basé sur word to vec surpassent de manière significative le modèle basé sur fast text, suggérant que peut-être les emboîtements de niveau mot pré-entraînés sont plus appropriés pour la tâche que les emboîtements de niveau caractère ou sous-mot lorsqu'ils sont utilisés tels quels. Bien qu'ils soient prometteurs, ces résultats suggèrent une ample opportunité d'amélioration par rapport à un expert de niveau de compétence qui peut éventuellement récupérer tous les articles pertinents à toute question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limitations de tous les jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des 32 codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge car les articles des décrets, directives et ordonnances sont manquants lors de la construction du jeu de données. Toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction du nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit encore complètement appropriée. Deuxièmement, nous devrions noter que toutes les questions juridiques ne peuvent pas être répondues avec les statuts seuls, par exemple, la question \"Puis-je expulser mes locataires s'ils font trop de bruit ?\" pourrait ne pas avoir de réponse détaillée dans le droit législatif qui quantifie un seuil spécifique de bruit à partir duquel l'expulsion est faible. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des précédents similaires à sa situation actuelle. Par exemple, le locataire fait deux fêtes par semaine jusqu'à 2 h du matin. Ainsi, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles légaux et le domaine de celles qui sont moins adaptées reste à déterminer. Nous espérons que tout le travail suscite un intérêt pour le développement de modèles de récupération d'articles légaux pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article dotset encode aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes heureux de vous présenter notre travail sur les voyelles, un benchmark indépendant destiné à tester les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous eu du mal à mettre en place ce benchmark ? Eh bien, au cours des dernières années, nous avons assisté à une explosion de modèles de vision et de langage basés sur les transformateurs, pré-entraînés sur de grandes quantités de paires d'images et de textes. Chacun de ces modèles repousse l'état de l'art sur des tâches de vision et de langage telles que la réponse à des questions visuelles, le raisonnement sur le sens commun visuel, la récupération d'images, l'ancrage de phrases, etc. Nous avons donc reçu un message : les précisions sur ces tâches spécifiques aux benchmarks augmentent régulièrement, mais savons-nous réellement ce que les modèles ont appris ? Qu'est-ce qu'un transformateur de vision et de langage a compris lorsqu'il attribue un score élevé à cette image et à cette phrase pour correspondre et un score bas à celle-ci ? Les modèles de vision et de langage se concentrent-ils sur la bonne chose, ou se concentrent-ils sur les biais comme le montrent les travaux antérieurs ? Pour éclairer davantage cet aspect, nous proposons une direction plus agnostique par rapport aux tâches et introduisons les voyelles qui testent la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la co-référence des entités. Mais comment testons-nous si les modèles de vision et de langage ont capturé ces phénomènes en déjouant une méthode précédemment appliquée uniquement pour les phrases nominales par Ravi Shekhar et ses collaborateurs et par nous-mêmes pour le comptage ? Déjouer signifie essentiellement que nous prenons la légende d'une image et produisons un leurre en modifiant la légende de telle sorte qu'elle ne décrit plus l'image. Nous effectuons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la co-référence des entités, où chaque élément peut consister en un ou plusieurs instruments, au cas où nous trouverions plus d'une manière intéressante de créer des instances de leurre. Par exemple, dans le cas de l'élément actions, nous avons deux instruments, l'un dans lequel le verbe d'action est changé par une action différente et un autre dans lequel les participants sont échangés. Le comptage et la co-référence sont également des éléments qui ont plus d'un instrument. Et nous créons ces leurres en nous assurant qu'ils ne parviennent pas à décrire l'image, qu'ils sont grammaticalement et autrement des phrases valides. Ce n'est pas facile à faire parce qu'une légende déjouée peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme qu'un homme coupe des plantes, et les grands modèles de vision et de langage pourraient s'en rendre compte. Par conséquent, pour obtenir des leurres valides, nous devons d'abord utiliser des modèles de langage puissants pour proposer des leurres, puis utiliser l'inférence en langage naturel ou NLI (Natural Language Inference) pour filtrer les leurres qui pourraient encore décrire l'image. Puisque lors de la construction des leurres, nous devons nous assurer qu'ils ne parviennent pas à décrire l'image, nous appliquons l'inférence en langage naturel pour tester cela automatiquement avec la logique suivante : nous considérons une image comme la prémisse et sa légende comme son hypothèse entraînée. De plus, nous considérons la légende comme la prémisse et le leurre comme son hypothèse. Si un modèle NLI prédit que le leurre contredit ou est neutre par rapport à la légende, nous considérons cela comme un indicateur d'un leurre valide. Si un NLI prédit que le leurre est entraîné par la légende, il ne peut pas être un bon leurre puisqu'il donnera par transitivité une description véridique de l'image et nous filtrons ces leurres. Mais cette procédure n'est pas parfaite, c'est juste un indicateur pour les leurres valides. Par conséquent, comme troisième mesure pour générer des leurres valides, nous employons des annotateurs humains pour valider les données utilisées dans VALSE. Ainsi, après le filtrage et l'évaluation humaine, nous avons autant d'instances de test que décrites dans ce tableau. Notez que VALSE ne fournit aucune donnée d'entraînement, mais seulement des données de test, puisqu'il s'agit d'un benchmark de test sans apprentissage préalable. Il est conçu pour tirer parti des capacités existantes des modèles de vision et de langage après l'entraînement préalable. Le réglage fin ne permettrait qu'aux modèles d'exploiter des artefacts ou des biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés à évaluer les capacités des modèles de vision et de langage après l'entraînement préalable. Nous expérimentons avec cinq modèles de vision et de langage sur les voyelles, à savoir CLIP, ALEX, BERT, WILBERT et WILBERT 12 en 1, et VISUAL-BIRD. Deux de nos métriques d'évaluation les plus importantes sont la précision des modèles dans la classification des paires d'images et de phrases en légendes et leurres. Peut-être plus pertinent pour cette vidéo, nous présenterons notre métrique plus permissive, la précision par paire, qui mesure si le score d'alignement de la paire d'image et de texte est plus élevé pour la paire d'image et de texte correcte que pour sa paire déjouée. Pour plus de métriques et de résultats, consultez notre article. Les résultats avec la précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus avec les autres métriques. C'est-à-dire que la meilleure performance sans apprentissage préalable est réalisée par WILBERT 12 en 1, suivi de WILBERT, ALEX, BERT, et enfin VISUAL-BIRD. Il est notable comment les éléments centrés sur des objets individuels comme l'existence et les phrases nominales sont presque résolus par WILBERT 12 en 1, soulignant que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de déjouement adversarial. Nous voyons à partir des instruments de pluralité et de comptage que les modèles de vision et de langage ont du mal à distinguer les références à des objets simples par rapport à des objets multiples ou à les compter dans une image. L'élément relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre les objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même si cela est soutenu par des biais de plausibilité comme nous le voyons dans l'élément actions. De l'élément de référence, nous découvrons que le suivi de plusieurs références au même objet dans une image en utilisant des pronoms est également difficile pour les modèles de vision et de langage. Comme vérification de cohérence et parce que c'est une expérience intéressante, nous évaluons également deux modèles texte uniquement, GPT1 et GPT2, pour déterminer si VALSE est résoluble par ces modèles unimodaux en calculant la perplexité de la légende correcte et déjouée, sans image ici, et en prédisant l'entrée avec la perplexité la plus basse. Si la perplexité est plus élevée pour le leurre, nous considérons cela comme une indication que la légende déjouée peut souffrir d'un biais de plausibilité ou d'autres biais linguistiques. Il est intéressant de constater que dans certains cas, les modèles GPT texte uniquement ont capturé la plausibilité du monde mieux que les modèles de vision et de langage. Pour résumer, VALSE est un benchmark qui utilise la lentille des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient les objets nommés dans leur présence dans les images, comme le montre l'élément existence, mais ont du mal à ancrer leur interdépendance et leurs relations dans les scènes visuelles lorsqu'ils sont forcés de respecter les indicateurs linguistiques. Nous aimerions vraiment encourager la communauté à utiliser les voyelles pour mesurer les progrès vers l'ancrage linguistique avec les modèles de vision et de langage. Et plus encore, les voyelles pourraient être utilisées comme une évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après l'entraînement ou le réglage fin pour voir si un ensemble de données aide les modèles à s'améliorer sur l'un des aspects testés par les VALSE. Si vous êtes intéressé, consultez les données VALSE sur GitHub et si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamisura de l'Université de Tokyo. Je vais présenter un article intitulé « O En Sum : Un modèle de résumé à grande échelle pour la génération automatique de notes de version via la Lo Sumization. » Je vais expliquer dans l'ordre suivant : d'abord, je vais présenter la notation automatique Li sur laquelle nous travaillons dans cette recherche. ReaseNote est un document technique qui résume les changements distribués avec chaque version d'un produit logiciel. L'image montre les notes de version pour la version deux point six point quatre de la bibliothèque Bujs. Ces notes jouent un rôle important dans le développement open source mais elles sont chronophages à préparer manuellement, il sera donc très utile de pouvoir générer automatiquement des notes de version de haute qualité. Je vais me référer à deux recherches antérieures sur la génération automatique de nœuds de liste, la première est un système appelé alena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant l'extracteur de changements pour extraire les différences principales, les changements de bibliothèque et les changements de documents à partir des différences entre les maladies, et en les combinant finalement. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, l'écosystème de problèmes et ne peut être appliqué qu'aux projets qui utilisent Jira, en d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub. La seconde est Grif, récemment annoncée en 2020. Elle est disponible sur Internet et peut être stockée via pi. Ce système a un modèle de classification de texte basé sur un fonctionnement simple et produit l'un des cinq problèmes tels que des fonctionnalités ou des corrections de bugs pour chaque message de commit d'entrée. L'image est un exemple d'utilisation qui retourne une bande correcte ou des corrections de bugs. Les données d'entraînement sont assez petites, environ 5000, et seront montrées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n'est pas élevée. Je présente deux recherches connexes, mais il y a des problèmes d'applicabilité limitée et de ressources de données rares. Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité. Pour le problème d'applicabilité limitée, nous proposons une méthode de résumé de classificateur de haute qualité utilisant uniquement le message de commit comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais. Pour le deuxième problème de ressources de données rares, nous avons construit notre propre ensemble de données et certaines données composées d'environ 82 000 morceaux de données en corrigeant les données de dépôts Gitub publics en utilisant l'API Git. Ensuite, je décris nos données ici, un exemple de mise à jour. Le côté gauche est un message de commit et le côté droit est les notes de version. Les notes de version sont classées comme des améliorations de visages, etc. Nous avons mis en place une tâche qui prend les messages de commit comme entrée et produit les notes de version. Cela peut être considéré comme une tâche de résumé. Nous avons prédéfini quatre niveaux : fonctionnalités, implémentations, corrections de bugs, dépécissements, et changements de rupture. Ceux-ci ont été définis sur la base de l'utilisation antérieure et d'autres facteurs. Il y a une note en bas à droite et extraite de la liste de nœuds montrée en bas à gauche. À ce moment, il est nécessaire de détecter les quatre lapins qui ont été définis précédemment, mais les niveaux ne sont pas toujours cohérents avec chaque le. Par exemple, le niveau d'amélioration inclut des améliorations, des améliorations, des optimisations, etc. Nous avons préparé une liste de vocabulaire de niveaux d'étude pour chacune de ces variations notationnelles. Utilisez-la pour détecter la classe de nœuds à risque et corriger le texte du reste qui suit comme la phrase de nœud à risque pour la classe. Ensuite, est un message de commit. Les messages de commit ne sont pas liés à chaque course comme le montre l'image ci-dessous, si la version actuelle est la version 2.5 à dix-neuf, nous devons identifier la version de la version précédente 2.5 à 18 et l'obtenir di. C'est un peu fastidieux et ce n'est pas suffisant de simplement obtenir une liste de versions et de regarder avant et après. Nous avons créé un algorithme d'appariement heuristique pour obtenir les versions précédentes et suivantes. L'analyse de données à la fin a corrigé 7200 dépôts et 82 000 morceaux de données. Le nombre moyen de jetons raisonnables est de 63, ce qui est assez élevé pour la tâche de résumé. De plus, le nombre de jetons uniques est assez riche à huit mille huit cent trente mille. Cela est dû au grand nombre de classes uniques et de noms de méthodes trouvés dans le dépôt. Ensuite, j'expliquerai la méthode proposée. Le modèle de résumé extractif et abstratif transversal est composé de deux modules neuronaux : un classificateur utilisant bot ou code bot et le générateur utilisant but first G utilise un classificateur pour classer chaque message de commit dans cinq classes de nœuds de base : fonctionnalités, améliorations, corrections de bugs, dépécissements, et autres. Les messages de commit classés comme autres sont rejetés, puis elle applique un générateur aux quatre documents de lapins indépendamment et génère des notes de version pour chaque classe dans cette tâche. Les correspondances directes entre les messages de commit et les notes de version ne sont pas connues. Par conséquent, pour entraîner le classificateur, nous attribuons des variables pseudo à chaque message de commit d'entrée en utilisant les dix premiers caractères de chaque message de commit. Nous modélisons l'approche de résumé abstratif par classe par deux méthodes définies. Le premier modèle, que nous appelons GS single, est composé d'un seul réseau sex et génère un seul texte de note n'est pas long. Il donne une concaténation des messages de commit d'entrée. Le texte de sortie peut être divisé en segment de fichier de classe basé sur des symboles de point de terminaison spécifiques à la classe. La deuxième méthode, que nous appelons shes much, est composée de quatre réseaux différents de sec à sec, chacun correspondant à l'une des classes de nœuds les moins. D'accord, laissez-moi expliquer l'expérience. Cinq méthodes ont été comparées : gs, shes single, shes much, cluster, et l'étude précédente Gri. Concernant l'aberration dans certains cas, ces notes sont produites en plusieurs phrases, puisque c'est difficile de corriger le nombre de phrases à zéro, elles sont combinées avec des espaces et traitées comme une seule phrase longue. Le bleu est pénalisé lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur de bre plus faible dans les résultats des expériences décrites ensuite. Finalement, nous avons également caricaturé une spécificité parce que le bleu et le bleu ne peuvent pas être caricaturés si les notes de liste sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version supposent vide. Voici les résultats puisque l'ensemble de données contient des valeurs d'analyse d'e-mail, etc. Nous avons également évalué l'ensemble de données propre qui les exclut. G et Gs ont atteint des scores d'erreur de perte plus de 10 points plus élevés que la ligne de base. En particulier, sur l'ensemble de test coréen, l'écart de score entre la méthode proposée et la base a bondi à plus de vingt points. Ces résultats indiquent que G et Gs sont significativement efficaces. Gs a obtenu un score de perte plus faible que GAS, suggérant que combiner un classificateur sous générateur est efficace dans l'entraînement du classificateur en utilisant des serveurs. La couverture élevée de gs peut être atteinte correctement parce que le classificateur peut se concentrer sur la sélection des messages de commit pertinents pour chaque classe. Shes much a tendance à manger plus haut que she is single, suggérant qu'il est également efficace de développer indépendamment différents modèles de résumé constructif pour chaque classe de nœud. Voici une analyse d'erreur. Les méthodes she ont tendance à produire des phrases plus courtes que la référence humaine. Puisque dans la figure à droite, la phrase de référence a trois ou quatre phrases, tandis que CSS n'en a qu'une. La raison de cette réticence du modèle est que dans les données d'entraînement, seulement trente pour cent des phrases sont présentes au niveau des fonctionnalités et quarante pour cent au niveau des améliorations. De plus, les méthodes cs ne peuvent pas générer des notes de liste précises sans informations supplémentaires. L'exemple en haut à droite est un exemple d'un message de commit très désordonné et la phrase complète ne peut pas être générée sans différence avec les prérogatives ou le problème correspondant. L'exemple ci-dessous montre que les deux messages de commit dans l'entrée sont liés et devraient être combinés en une seule phrase, mais cela échoue à le faire. Finalement, une conclusion. Nous avons construit un nouvel ensemble de données pour la génération automatique de notes de version. Nous avons également formulé la tâche de saisir les messages de commit et de les résumer de manière à ce qu'elle soit applicable à tous les projets écrits en anglais. Nos expériences montrent que la méthode proposée génère moins de bruit et a une couverture plus élevée que la ligne de base. Veuillez consulter nos données sur GitHub. Merci."}
