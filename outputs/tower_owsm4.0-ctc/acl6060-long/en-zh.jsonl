{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫Safari，我将介绍我们的论文“使用微调的Transformer架构进行表格数据增强”。科学家们在分析数据时，主要关注的是操作现有数据特征，但有时这些特征有限。使用另一个数据源生成特征可能会添加大量信息。我们的研究目标是使用外部来源的自由文本自动进行表格数据增强。假设我们有一个表格数据集和一个知识库。我们需要一个自动过程，涉及实体链接和文本分析，以从知识库的自由文本中提取新特征。我们的框架FEST正是这个自动过程，让我们来看一个例子：将数据集输入到FEST中。在这个例子中，数据集是大学数据集，其目标是将大学分为低排名大学和高排名大学。我们使用维基百科作为知识库。第一阶段是实体链接，当每个实体（在这个例子中是大学名称）链接到知识库中的一个实体时，知识库的实体文本被提取并添加到数据集中。在这个例子中，文本是维基百科页面的摘要。现在我们需要从检索的文本中生成或提取特征，因此我们需要一个特征提取阶段，包括文本分析，这是这篇论文的主要创新之处，我将在接下来的幻灯片中深入探讨。在特征提取阶段之后，有一个特征生成阶段，我们使用提取的特征生成少量新特征。首先，生成与原始数据集类别数量相同的特征。在这个例子中，原始数据集有两个类别，因此首先生成两个新特征。但是，如果数据集有五个类别，首先生成五个新特征。每个特征代表每个类别的可能性。为了分析文本，我们使用当前的文本分析状态，即基于变换器的语言模型，如Bear GPT X和Lets等。但我们不太可能使用输入数据集训练语言模型，因此一个简单的做法是目标任务微调。在特征提取阶段，我们可以下载预训练语言模型，然后在目标数据集上微调语言模型。在这个例子中，为了将文本分类为类别，摘要分为类别，低或高，我们接收语言模型的输出，即每个类别的可能性，并使用这些作为新特征。这种方法的问题在于，数据集可能包含很少的独特实体文本。在我们的实验中，几乎有一半的数据集包含少于400个样本，最小的数据集包含35个样本。因此，在该数据集上微调语言模型将无效。但我们可以使用关于预分析数据集的先验知识，因为我们对多个数据集应用了FAST，我们可以使用n减1个数据集来收集关于n减1个数据集的信息，并在分析n个数据集时使用这些信息。我们建议添加另一个微调阶段，即初步多任务微调阶段，当您在n减1个数据集上微调语言模型时，然后我们执行另一个微调阶段，即目标任务微调，当您在第n个目标数据集上微调语言模型时。目前在多任务微调中，称为DNN的TDNN在训练集中保持多个任务的头部。因此，如果在这个例子中有四个任务，DNN将保持四个头部。我们的方法称为任务重述微调，我们在这个方法中，我们没有保持多个头部，而是将每个数据集重述为一个句子分类问题，即两个类别的任务。让我们来看一个例子：我们的输入数据集包含实体特征文本和类别。我们将任务从将文本分类为低和高重述为将文本的摘要和类别分类为真或假，或者换句话说，我们训练语言模型来分类摘要和类别a，以尝试抽象类别，如果摘要属于该类别。在z的情况下，标签向量始终保持不变，始终包含两个类别。这是我们微调或重述微调方法的算法。让我们来看看完整的框架：将数据集输入到FAST中，然后FAST执行链接阶段，它从知识库中提取文本（在这个例子中是维基百科页面的摘要），然后将任务重述为一对句子分类任务，将语言模型应用于新任务，并输出每个类别的可能性。请注意，语言模型已经使用初步多任务微调在n减1个数据集上进行了微调。然后我们使用语言模型的输出向量作为新生成的特征，以评估我们的框架。我们使用一个十七个表格分类数据集，它定义了大小特征、平衡域和初始性能，并且作为知识库，我们使用维基百科。我们设计实验为留一法评估，当我们在16个数据集上训练FAST，并将其应用于第17个数据集时，我们还将每个数据集分为错误和应用错误交叉验证，然后生成新特征并使用五个评估分类器进行评估。我们使用基于鸟的架构进行实验。以下是我们实验的结果，您可以看到我们将我们的框架与目标数据集微调、目标任务微调和mtdn初步微调进行比较，我们的重述微调取得了最佳性能，而mtdnn在目标数据集微调上取得了2%的改进，我们的方法在小数据集上取得了6%的改进。当我们观察到小数据集时，我们可以看到mtdnn的性能下降，初步多任务微调阶段的改进降至1.5%。但我们的性能相对于目标任务微调提高到了11%。总结：FAST使燃料喷射器从35个样本中进行数据增强。它使用一个架构处理所有任务数据集，并保持模型的头部。但它添加了重述阶段，扩大了训练集，并且需要具有语义意义的目标值，因此我们可以将其输入到语言模型中，并在句子分类问题中使用。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我将介绍我们的研究工作：学习推理，从元问题解决到复杂区域提取。我是来自比安斯人工智能实验室的 All，这是与德克萨斯大学奥斯汀分校的 Che 和 SUDD 的 Wedu 共同完成的工作。首先，我想谈谈我们进行推理的动机。因此，我们在这里展示了多步推理有用的例子。因此，这个图摘自磅纸，他们在未来的学习场景中通过提示解决了元问题。因此，在左侧，我们可以看到，如果我们只给出问题和答案的样本，我们可能无法获得正确的答案。但是，如果我们给出一些更多的推理描述，模型能够预测推理描述，并且在这里也做出正确的预测，因此，具有可解释的多步推理作为输出是好的，我们还认为元问题是一个直接的应用程序，用于评估这种推理能力，因此，在我们的问题设置中，给定问题，我们需要解决这个问题并获得数值答案，因此，在我们的数据集中有数学表达式，这导致了这个特定的答案，因此，与以前的工作一样，我们假设量的精度是已知的，我们只考虑基本运算符，如加法、减法、乘法、除法和指数，此外，复杂的运算符实际上可以分解为这些基本运算符，因此，以前在元问题解决中的工作实际上可以分为序列到序列和序列到树模型，因此，传统的序列到序列模型将表达式转换为特定的序列进行生成，这很容易实现，并且可以推广到许多不同的复杂问题，但是，性能的缺点实际上通常不如结构模型，并且缺乏预测的可解释性，但实际上，这个方向仍然相当受欢迎，因为有变换器模型，因此，在基于树的模型中，我们实际上将这些表达式结构化为树形式，并在树生成中遵循先序遍历，因此，我们继续生成运算符，直到我们到达叶子，即量，因此，这里的好处是，它实际上给我们提供了这个二叉树结构，并且它但是，但是，实际上这是相当违反直觉的，因为我们首先生成运算符，然后在最后生成量，第二件事是它也包含一些重复计算，因此，如果我们看一下这个表达式 a 乘以 3 加上 3，实际上生成了两次，但实际上我们应该重用结果，因此，在我们的提出的方法中，我们希望以逐步和可解释的方式解决这些问题，例如，在这里，在第二步，我们可以获得这个除数，即 27，并且我们也可以回溯到原始问题，以找到相关内容，在这些步骤中，我们获得了除数，然后在这个第三步，我们实际上得到了商，好的，经过这三个步骤，我们实际上可以重用第二步的结果，然后得到第四步的结果，然后最终我们可以获得被除数，因此，我们实际上直接生成了整个表达式，而不是生成单个运算符或量，这使得过程更准确。因此，在我们的演绎系统中，我们首先从问题中呈现的一系列量开始，也包括一些常数作为我们的初始初始状态。因此，表达式由 eij 表示，我们执行从 qi 到 qj 的运算符，并且这样的表达式实际上是有方向的，我们也有减法反向在这里表示相反的方向，这与关系提取非常相似，因此，在形式演绎系统中，在时间步 t，我们应用运算符在 qi 和 qj 对之间，然后获得这些新表达式，我们将它们添加到下一个状态中，成为一个新量，因此，这些幻灯片实际上可视化了状态的演化，我们不断将表达式添加到当前状态中，因此，在我们的模型实现中，我们首先使用预训练模型，可以是鸟类或原始模型，然后对句子进行编码，然后获得这些量表示，因此，一旦我们得到量表示，我们就可以开始进行推理，在这里我们展示了一个 q1 到 q2 除以 q3 的表示的例子，首先我们得到一对表示，这基本上只是 q1 和 q2 之间的连接，然后我们应用一个由运算符参数化的前馈网络，然后最终我们获得表达式表示 q1 除以 q2，但在实践中，在推理阶段，我们可能能够获得错误的表达式，因此，所有可能的表达式等于运算符的数量的三倍，这里的好处是，我们可以轻松地添加约束来控制这个搜索空间，例如，如果这个表达式不允许，我们可以简单地从我们的搜索空间中删除这个表达式，因此，在第二步，我们做同样的事情，但唯一的区别是，我们唯一的区别是多了一个量，这个量来自之前计算的表达式，因此，最终我们可以获得这个最终表达式 q3 乘以 q4，并且我们也可以看到所有可能的表达式的数量与前一个步骤不同，这种差异使得应用束搜索变得困难，因为这两个步骤之间的概率分布是不平衡的，因此，训练过程与训练序列到序列模型相似，我们在每个时间步优化损失，并且在这里我们还使用这个 tau 来表示我们应该终止这个生成过程，并且这个空间与序列到序列不同，因为空间在每个时间点都不同，而在传统的序列到序列模型中，它是词汇表的数量，并且还允许对先验知识施加某些约束，因此，我们在常用的元问题数据集 mwps method3k math qaA 和 swam 上进行实验，在这里我们简要展示了与之前最佳方法相比的结果，因此，我们表现最好的武器是 Roberta 演绎推理，事实上，我们没有使用束搜索，而明显的对比方法使用束搜索，好的，因此，最佳方法通常是基于树的模型，因此，总的来说，我们的推理器能够从这个基于树的模型中选择显著的输出，但我们可以看到 math qaA 或 swam 的绝对数量并不高，因此，我们进一步调查了 swam 的结果，这个数据集具有挑战性，因为作者试图手动添加一些东西来混淆 NLP 模型，如添加可用信息和额外的量，因此，在我们的预测中，我们发现一些中间值实际上是负的，例如，在这些问题中，我们问杰克有多少苹果，但我们有一些额外的信息，如 17 个更少的 pitchachees 和斯蒂芬有八个 pitchachees，这完全是相关的，因此，我们的模型做出了一些预测，产生负值，我们观察到这两个表达式实际上具有相似的分数，因此，我们可以实际通过删除这些结果是负数来限制这个搜索空间，以便我们可以使答案正确，因此，我们进一步发现这种约束实际上对一些模型改善了很多，例如，对于鸟类，我们改善了七点，然后对于基于 roberta 的模型，我们实际上改善了两个点，因此，更好的语言模型具有更好的语言理解能力，因此，这里的数字对于 roberta 来说更高，对于鸟类来说更低，我们还尝试分析了所有这些数据集背后的难度，我们假设未使用的量可以被视为相关信息，因此，在这里我们可以看到，我们将样本中未使用的量的百分比 lump，swamp 数据集有最大的部分，并且我们还展示了那些没有未使用的量样本的整体性能，整体性能实际上高于整体性能，但那些有未使用的量样本实际上比 com 差很多，对于 WPS，我们实际上没有太多死亡案例，所以我只是忽略了这个部分，因此，最后，我们希望通过崩溃和演示示例来展示可解释性，因此，我们的模型实际上在第一个步骤中做出了错误的预测，因此，我们实际上可以将这个表达式与这里的句子相关联，好的，因此，我们认为这个句子可能会误导模型做出错误的预测，因此，在这里打印另一个 35 使得模型认为应该是加法运算符，因此，我们尝试修改句子为类似于梨树的数量比苹果树少 5 棵，这样我们就可以传达更准确的语义，使得模型能够做出正确的预测，因此，这项研究表明，可解释的预测如何帮助我们理解模型的行为，因此，总结我们的工作，首先，我们的模型实际上相当高效，我们能够提供可解释的求解过程，并且可以轻松地将一些先验知识作为约束纳入，这可以帮助提高性能。最后，这个底层机制不仅适用于网络问题解决任务，还适用于涉及多步推理的其他任务。但是，我们也有某些限制。如果我们有大量的运算符或常数或常数，内存消耗可能会非常高。第二件事是，如前所述，由于不同时间步的概率分布是不平衡的，因此应用束搜索策略也相当具有挑战性，这就是演讲的结束，欢迎提问，谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫安托万，来自马斯特里赫特大学。我将与杰瑞一起展示我的约翰工作，该工作涉及一个新的法定条款检索数据集。法律问题是许多人生活中不可或缺的一部分，但大多数公民对自己的权利和基本法律程序知之甚少，因此许多无法负担法律专家昂贵帮助的弱势公民得不到保护，甚至被剥削。所有工作旨在通过开发有效的法定条款检索系统来弥合人们与法律之间的鸿沟。这样的系统可以为非专业人士提供免费的专业法律帮助服务。在深入探讨这项工作的主要贡献之前，让我们先描述一下法定条款检索的问题。假设一个简单的法律问题，例如“如果我违反职业保密规定，我将面临什么风险？”模型需要从大量立法中检索所有相关的法定条款。这项信息检索任务有其自身的挑战。首先，它涉及两种语言：问题使用的常见自然语言和法规使用的复杂法律语言。这种语言分布的差异使得系统更难检索相关候选人，因为它间接需要一个固有的解释系统，可以将自然问题翻译成与法规术语相匹配的法律问题。此外，法定法不是一堆可以独立作为信息来源的独立条款，例如新闻或食谱。相反，它是一个结构化的法律条款集合，只有在整体背景下考虑时才有完整意义，即与邻近条款的补充信息、它们所属的领域和子领域，以及它们在法律结构中的位置。最后，法定条款通常很短，通常是大多数检索工作中的典型检索单位。在这里，它们是长达6000字的长文档。最近在自然语言处理方面的进展引发了人们对许多法律任务的巨大兴趣，例如法律判决预测或自动合同审查，但法定条款检索由于缺乏大量高质量的标记数据集而一直处于停滞状态。在这项工作中，我们提出了一个新的以法国公民为中心的语料库，研究检索模型是否可以接近法律专家在法定条款检索任务中的效率和可靠性。比利时法定条款数据集包含1100多个比利时公民提出的法律问题。这些问题涵盖了从家庭住房金钱到工作和社会保障的广泛主题。每个问题都由经验丰富的法学家标记，引用了来自比利时法律法规的26600多个法定条款的语料库中的相关条款。现在让我们谈谈我们如何收集这个数据集。首先，我们开始汇编一个大型的法律条款语料库。我们考虑了32个公开可用的比利时法规，并提取了它们的所有条款以及相应的章节标题。然后，我们收集了引用相关法规的法律问题。为此，我们与一家比利时律师事务所合作，该事务所每年收到约400封来自比利时公民的电子邮件，询问个人法律问题。我们很幸运能够访问他们的网站，他们的团队由经验丰富的法学家组成，解决比利时最常见的法律问题。我们收集了数千个问题，附有类别、子类别和相关法规的法律引用。最后，我们通过法律引用过滤了问题，其引用不是我们考虑的任何法规中的条款。剩余的引用被匹配并转换为我们语料库中的相应条款ID。我们最终得到了1008个问题，每个问题都仔细标记了来自26603个法定条款的大型语料库中的相关条款ID。此外，每个问题都附有主要类别和子类别的连接，每个条款都附有它们在法律结构中的后续标题的连接。这些额外信息在当前工作中未使用，但可能对未来的法律信息检索或法律文本分类研究感兴趣。让我们来看看我们数据集的一些特征。问题长度在5到44个词之间，中位数为40个词。条款要长得多，中位数长度为77个词，其中142个超过1000个词，最长的长达5790个词。如前所述，问题涵盖了广泛的主题，其中约85%是关于家庭住房金钱或司法的问题，而其余15%则涉及社会保障、外国人或工作。条款也非常多样化，因为它们来自32个不同的比利时法规，涵盖了大量法律主题。以下是从每个比利时法规收集的条款总数。在26603个条款中，只有1612个被标记为与至少一个问题相关，而这些引用的条款中有约80%来自民法典、司法法典、刑事诉讼法或刑法典。同时，32个法规中有18个法规提到的相关条款少于5个，这可以解释为这些法规更关注个人及其关注点。引用的条款的中位引用次数为2，只有不到25%的条款被引用超过5次。使用我们的数据集，我们对几种检索方法进行了基准测试，包括词汇和密集架构。给定一个查询和一个条款，词汇模型通过计算查询词项在该条款中的权重之和来为查询条款对分配一个分数。我们尝试了标准的TF-IDF和BM25排名函数。这些方法的主要问题是它们只能检索包含查询中关键词的条款。为了克服这个限制，我们尝试了一种基于神经网络的架构，可以捕捉查询和条款之间的语义关系。我们使用B编码器模型将查询和条款映射到密集向量表示，并通过嵌入的相似性计算查询条款对的相关性分数。这些嵌入通常是词嵌入模型输出的池化操作的结果。首先，我们研究了Siamese编码器在零样本评估设置中的有效性，这意味着预训练的词嵌入模型直接使用，无需任何额外的微调。我们尝试了独立于上下文的文本编码器，即word2vec和fastText，以及依赖于上下文的嵌入模型，即Roberta，特别是Camembert，这是一种法国的Roberta模型。此外，我们还在所有数据集上训练了自己的基于Camembert的Siamese编码器模型。请注意，对于训练，我们尝试了两种Siamese编码器架构：Siamese，使用一个独特的词嵌入模型，将查询和条款一起映射到共享的密集向量空间；以及Tower，使用两个独立的词嵌入模型，分别将查询和条款编码到不同的嵌入空间。我们尝试了均值池化、最大池化、CLS池化以及点积和余弦相似度来计算相似度。以下是基准测试结果。在上面的词汇方法中，Siamese编码器在零样本设置中进行了评估，中间是微调后的Siamese编码器，下面是微调后的Tower编码器。总体而言，微调后的Siamese编码器显著优于所有其他基准。Tower模型在100个召回率上优于其Siamese变体，但在其他指标上表现相似。尽管BM25的性能明显低于训练后的Siamese编码器，但其性能表明它仍然是一个强大的基准。关于Siamese编码器的零样本评估，我们发现直接使用预训练的Camembert模型的嵌入，而不针对信息检索任务进行优化，会得到较差的结果，这与先前的发现一致。此外，我们观察到基于word2vec的Siamese编码器显著优于基于fastText的模型，这表明预训练的词级嵌入可能比字符级或子词级嵌入更适合该任务。尽管有希望，但这些结果表明与熟练水平的专家相比，仍有很大的改进空间，熟练水平的专家最终可以检索到与任何问题相关的所有相关条款，从而获得满分。最后，让我们讨论一下所有数据集的两个局限性。首先，语料库仅限于从32个考虑的比利时法规中收集的条款，这并不涵盖整个比利时法律，因为法令、指令和政令的条款在数据集构建期间缺失。所有对这些未收集条款的引用都被忽略，这导致一些问题最终只有初始相关条款数的一小部分。这种信息损失意味着剩余相关条款中的答案可能不完整，尽管仍然完全合适。其次，我们应该指出，并非所有法律问题都可以仅通过法规来回答。例如，“如果我的租户制造太多噪音，我可以驱逐他们吗？”这个问题可能没有量化特定噪音阈值以确定驱逐的法律法规。相反，房东可能需要更多地依赖案例法，并找到与当前情况相似的先例。例如，租户每周在凌晨2点之前举行两次聚会。因此，有些问题比其他问题更适合法定条款检索任务，而其他问题仍然有待确定。我们希望所有工作都能激发人们开发实用且可靠的法定条款检索模型，从而帮助改善所有人的司法救济。您可以通过以下链接查看我们的论文dotset encode。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我们很高兴向您展示我们在元音上的工作，这是一个任务独立基准，旨在测试视觉和语言模型对特定语言现象的处理能力。我们为什么要费力建立这个基准呢？在过去几年里，我们看到基于变换器的视觉和语言模型在大量图像文本对上进行预训练，每个模型都在视觉和语言任务（如视觉问答、视觉常识推理、图像检索、短语定位）上推动了最先进水平。因此，我们得到一个信息：这些任务特定基准的准确率正在稳步提高，但我们是否知道模型实际上学到了什么？当视觉和语言变换器为这张图像和这句话的高分匹配以及这张图像和这句话的低分匹配分配高分时，它理解了什么？视觉和语言模型是否关注正确的事情，还是它们关注之前工作中显示的偏见？为了进一步阐明这一点，我们提出一个更通用的方向，并引入元音，测试视觉和语言模型对影响语言和视觉模式的特定语言现象的敏感性，我们关注存在、复数计数、空间关系、动作和实体指称，但我们如何测试视觉和语言模型是否捕捉到了这些现象呢？我们采用了一种先前仅用于名词短语的方法，由Ravi Shekhar和合作者提出，以及我们在之前工作中提出的计数方法。基本上，造假意味着我们取一张图像的标题，通过修改标题使其不再描述图像来产生一个假项，我们通过关注六个特定部分（如存在、复数计数、空间关系、动作和实体指称）来进行这些短语修改，每个部分可以由一个或多个工具组成，以防我们发现一种或多种有趣的创建假项的方法。例如，在动作部分中，我们有两个工具，一个是在动作动词上改变为不同的动作，另一个是在动作参与者上交换。复数计数和指称也是有两种以上工具的部分。我们通过确保它们无法描述图像来创建这些假项，它们是语法上和语义上有效的句子。这并不容易，因为一个假标题可能比原始标题的可能性更低。例如，虽然不可能，但统计上植物割伤人比人割伤植物的可能性更低，大型视觉和语言模型可能会捕捉到这一点，因此为了获得有效的假项，我们必须采取行动：首先，我们利用强大的语言模型来提出假项，其次，我们使用自然语言推理或短NLI来过滤掉可能仍然描述图像的假项，因为在构造假项时，我们需要确保它们无法描述图像，为了自动测试这一点，我们应用自然语言推理，其原理如下：我们认为一张图像是前提，其标题是其蕴含的假设；此外，我们认为标题是前提，假项是其假设，如果NLI模型预测假项与标题相矛盾或相中性，我们将其视为有效假项的指标，如果NLI预测假项是由标题蕴含的，那么它不能成为一个好的假项，因为通过传递性，它将给出对图像的真实描述，我们将这些假项过滤掉，但这个过程并不完美，它只是有效假项的一个指标，因此，作为生成有效假项的第三个措施，我们雇佣人工标注员来验证Valse中使用的数据，因此，经过过滤和人工评估后，我们有如本表所述的测试实例。请注意，Valse不提供任何训练数据，只提供测试数据，因为它是一个零样本测试基准。它旨在利用视觉和语言模型在预训练后的现有能力。微调只会使模型能够利用数据中的伪影或统计偏见。我们都知道这些模型喜欢作弊和走捷径。正如我们所说，我们有兴趣评估视觉和语言模型在预训练后的能力，我们在元音上对五个视觉和语言模型进行了实验，即clip、Alex、Mert、wilbert、wilbert 12 in 1和visual bird，我们的最重要的评估指标是模型将图像句子对分类为标题和假项的准确率，也许对于这个视频来说，我们更重要的评估指标是成对准确率，它衡量的是图像句子对齐分数是否比其假项对更高，有关更多指标和结果，请查看我们的论文，这里显示了成对准确率的结果，它们与我们从其他指标获得的结果一致，即wilbert 12 in 1的零样本性能最好，其次是wilbert、Alex、Mert、clip和visual bird，值得注意的是，以存在和名词短语等单个对象为中心的工具几乎被wilbert 12 in 1解决了，这表明模型能够识别命名对象及其在图像中的存在，然而，在我们的对抗性造假设置中，其余的部分都无法可靠地解决。我们从复数和计数工具中看到，视觉和语言模型难以区分单一对象与多个对象之间的引用，或者在图像中计数它们，关系部分表明它们难以正确分类图像中对象之间的命名空间关系，它们甚至难以区分动作并识别其参与者，即使在可信度偏见的支持下，如我们从参考部分的动作部分中看到的那样，我们发现使用代词在图像中追踪多个对同一对象的引用也对视觉和语言模型来说很困难，作为一种验证，因为这是一个有趣的实验，我们还对两个仅文本的模型GPT1和GPT2进行了基准测试，以评估Valse是否可由这些单模态模型解决，通过计算正确和假项标题的困惑度，没有图像，并预测困惑度最低的条目，如果困惑度对于假项更高，我们将其视为一个指示，表明假项可能受到可信度偏见或其他语言偏见的影响，有趣的是，在某些情况下，仅文本的GPT模型比视觉和语言模型更好地捕捉了世界的可信度，总结一下，Valse是一个使用语言构建的视角来帮助社区通过严格测试视觉模型的视觉根基能力来改进视觉和语言模型的基准。我们的实验表明，视觉和语言模型在图像中很好地识别了命名对象及其存在，如存在部分所示，但在被迫尊重语言指示的情况下，难以在视觉场景中建立它们之间的相互依赖和关系。我们真的希望鼓励社区使用元音来衡量视觉和语言模型在语言根基方面的进展。更重要的是，元音可以作为数据集的间接评估，因为可以在训练或微调前后评估模型，以查看数据集是否帮助模型在VAs测试的任何方面取得进步。如果您感兴趣，请查看GitHub上的ValAs数据，如果您有任何问题，请随时与我们联系。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是来自东京大学的 Kamisura。我将发表一篇题为“O En Sum：一种用于自动生成列表摘要的大规模数据集”的论文。我将按照以下顺序进行解释：首先，我将介绍我们在这项研究中正在研究的自动 Li 记法。ReaseNote 是一份技术文档，总结了每次软件产品发布时所做的更改。图片显示了 Bujs 库版本 2.6.4 的发布说明。这些说明在开源开发中起着重要作用，但手动准备非常耗时，因此能够自动生成高质量的发布说明将非常有用。我将参考两项关于自动生成列表节点的先前的研究：第一项是 2014 年发布的称为 alena 的系统。它采用基于规则的方法，例如使用变更提取器从疾病之间的差异中提取核心差异、库更改和文档更改，最后将它们结合起来。该系统最显著的特征是右上角的问题提取器，必须链接到问题生态系统 Jira，并且只能应用于使用 Jira 的项目，换句话说，它不能用于 GitHub 上的许多项目。第二项是 Grif，最近在 2020 年宣布。它可以在互联网上获得，可以通过 pi 进行存储。该系统有一个简单的基于运行的文本分类模型，并为每个输入提交消息输出五个问题之一，例如功能或错误修复。图片是一个示例用法，返回正确的磁带或错误修复 rub que 训练数据相当小，约为 5000 条，将在下面描述的实验中展示。文本分类模型的性能不高。我介绍了两项相关研究，但存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成了高质量的资源。对于适用性有限的问题，我们提出了一种高质量的分类器摘要方法，仅使用提交消息作为输入。该方法可以用于所有英文仓库。对于数据资源稀缺的第二个问题，我们通过使用 Git API 纠正了公共 Gitub 仓库的数据，构建了一个包含约 82,000 条数据的集合。接下来，我在这里描述我们的数据：这是一个示例更新，左侧是提交消息，右侧是发布说明。发布说明被分为改进、功能等级别。我们设置了一个任务，该任务以提交消息为输入，并输出发布说明。这可以被视为一个摘要任务。我们预定义了四个级别的特征：实现、修复、废弃、移除和破坏性更改。这些是基于先前的使用情况和其他因素设定的。在右下角有一个注释，是从左下角的列表节点中提取的。目前，有必要检测出之前设置的四个兔子，但级别并不总是与每个级别一致。例如，改进级别包括改进、增强、优化等。我们为这些记法变体准备了一个词汇表，用于检测风险节点类别，并纠正后续文本，作为风险节点句子的类别。接下来是一个提交消息。提交消息与每个版本无关，如图所示，如果当前版本是 2.5 到 19，我们需要确定之前的版本 2.5 到 18，并获取它。这有点繁琐，仅仅获得一系列版本并查看前后是不够的。我们创建了一个启发式匹配胶水，以获取前后版本的数据分析。最终，7200 个仓库和 82,000 条数据被纠正，合理的令牌的平均数量为 63，对于摘要任务来说相当高。独特的令牌数量也非常丰富，达到八千八百三十个。这是由于仓库中发现的独特类和方法名称数量众多。接下来，我将解释提出的方法。交叉提取和抽象摘要模型由两个神经模块组成：一个使用 bot 或 code bot 的分类器，以及使用 but first G 的生成器。首先，G 使用分类器将每个提交消息分类为五个基本节点类别：功能、改进、错误修复、废弃和其他。将其他类别分类的提交消息被丢弃，然后她独立地将生成器应用于四个兔子文档，并为每个类别生成发布说明。在这个任务中，提交消息和发布说明之间的直接对应关系是未知的。因此，为了训练分类器，我们为每个输入提交消息分配伪变量，使用每个提交消息的前 10 个字符。我们通过两种定义的方法对按类别抽象摘要的方法进行建模。第一种模型，我们称之为 GS single，由一个单一网络组成，生成一个长文本，给定输入提交消息的连接。输出文本可以根据特殊类别特定端点符号分为类别文件段。第二种方法，我们称之为 shes much，由四个不同的 sec to sec 网络组成，每个网络对应一个最少节点类别。好的，让我解释实验。我们比较了五种方法：gs、shes single、shes much、cluster 和先前的研究 Gri。在某些情况下，这些说明以多句形式输出，由于难以将句子数量纠正为零，因此它们与空格结合，并被视为一个长句子。蓝色是当系统输出短句时的惩罚。这种惩罚导致实验结果中的 bre 值较低。接下来，我们还计算了特异性，因为如果列表说明为空，蓝色和蓝色不能被计算。高特异性意味着模型在读取节点假设为空的情况下正确输出为空文本。以下是结果，因为数据集包含电子邮件分析等值，我们还评估了不包含它们的干净数据集。G 和 Gs 实现了比基准更高的错误率分数，特别是对于韩语测试集，提议的方法与基准之间的分数差距跃升至二十多分。这些结果表明，G 和 Gs 非常有效。Gs 获得了比 GAS 更好的错误分数，表明在使用服务器训练分类器时，结合分类器在生成器下的方法是有效的。gs 的高覆盖率可以得到适当的实现，因为分类器可以专注于为每个类别选择相关的提交消息。shes much 倾向于比 she is single 更高，表明独立开发不同于建设性摘要模型对于每个节点类别也是有效的。这里有一个错误分析：she 方法倾向于输出比人类参考句子更短的句子，因为在右图中，参考句子有三到四个句子，而 CSS 只有一个。该模型不愿的原因是，在训练数据中，只有 30% 的句子存在于功能级别，40% 存在于改进级别。此外，cs 方法无法在没有额外信息的情况下生成准确的列表说明。右上角的例子是一个非常混乱的提交消息的例子，无法生成完整的句子，而不会与相应的特权或问题产生差异。下面的例子表明，输入中的两个提交消息是相关的，应该合并成一个句子，但它未能做到。最后，结论。我们为自动生成个人摘要构建了一个新数据集。我们还制定了输入提交消息并将其总结的任务，使其适用于所有用英文编写的项目。我们的实验表明，提议的方法生成比基准更少的噪声，覆盖率更高。请查看我们在 GitHub 上的数据。谢谢。"}
