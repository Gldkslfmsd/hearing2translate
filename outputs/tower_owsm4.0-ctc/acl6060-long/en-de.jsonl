{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Safari, und ich werde unseren Artikel über die tabuläre Datenanreicherung mit verfeinerten Transformer-Architekturen vorstellen. Wissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der vorhandenen Datenmerkmale, aber manchmal sind diese Merkmale begrenzt. Die Generierung von Merkmalen aus einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische tabuläre Datenanreicherung mit freitextlichen externen Quellen. Nehmen wir an, wir haben einen tabularen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entitätsverknüpfung und Textanalyse beinhaltet, um neue Merkmale aus dem freitextlichen Wissen der Wissensdatenbank zu extrahieren. Unser Framework FEST ist genau dieser automatische Prozess. Lassen Sie uns ein Beispiel in Datensätzen betrachten, die in FEST eingespeist werden. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, dessen Ziel es ist, Universitäten in niedrig und hochrangige Universitäten zu klassifizieren. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase ist die Entitätsverknüpfung, bei der jede Entität in diesem Beispiel, der Universitätsname, mit einer Entität innerhalb der Wissensdatenbank verknüpft wird. Der Text der Entitäten der Wissensdatenbank wird extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text der Abstract der Wikipedia-Seite. Jetzt müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Dazu benötigen wir eine Merkmalsextraktionsphase, die Textanalyse beinhaltet. Dies ist die Hauptneuheit dieses Artikels, und ich werde in den nächsten Folien darauf eingehen. Nach der Merkmalsextraktionsphase folgt eine Merkmalgenerierungsphase, in der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst generieren wir Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, also werden zuerst zwei neue Merkmale generiert. Wenn der Datensatz jedoch fünf Klassen hat, werden zuerst fünf neue Merkmale generiert. Jedes Merkmal repräsentiert die Wahrscheinlichkeit für jede Klasse. Zur Analyse des Textes verwenden wir den aktuellen Stand der out-of-text-Analyse, die auf Transformer-basierten Sprachmodellen wie BERT GPT X und anderen basiert. Es ist jedoch unwahrscheinlich, dass wir Sprachmodelle mit den Eingabedatensätzen trainieren können, daher ist ein naiver Ansatz das Feinabstimmen als Zielaufgabe. In der Merkmalsextraktionsphase können wir ein vortrainiertes Sprachmodell herunterladen und es über den Zieldatensatz feinabstimmen. In diesem Beispiel feinabstimmen, um den Text in Klassen, Abstract in Klassen, niedrig oder hoch, zu klassifizieren. Wir erhalten den Ausgabewert des Sprachmodells, die Wahrscheinlichkeit für jede Klasse, und verwenden dies als neue Merkmale. Das Problem mit diesem Ansatz ist, dass der Datensatz möglicherweise nur wenige eindeutige Entitäten enthält. In unserem Experiment enthält fast die Hälfte der Datensätze weniger als 400 Proben, und der kleinste Datensatz enthält 35 Proben. Das Feinabstimmen eines Sprachmodells über diesen Datensatz wäre unwirksam. Wir können jedoch vorheriges Wissen über bereits analysierte Datensätze nutzen, da wir FEST auf mehrere Datensätze anwenden. Wir können die n minus 1 Datensätze nutzen, um Informationen über die n minus 1 Datensätze zu sammeln und diese Informationen bei der Analyse des n-ten Datensatzes verwenden. Was wir vorschlagen, ist, eine weitere Feinabstimmungs-Phase hinzuzufügen, eine vorläufige Multitask-Feinabstimmung, bei der wir das Sprachmodell über n minus einen Datensätzen feinabstimmen. Dann führen wir eine weitere Feinabstimmungs-Phase durch, die eine Zielaufgaben-Feinabstimmung ist, bei der wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen. Der aktuelle Stand der Technik in der Multitask-Feinabstimmung, genannt DNN in DNN TDNN, behält eine Anzahl von Köpfen in der Anzahl der Aufgaben im Trainingsdatensatz bei. Wenn es in diesem Beispiel vier Aufgaben im Trainingsdatensatz gibt, behält DNN vier Köpfe bei, wie Sie auf dem Bild sehen können. Es nimmt einen zufälligen Batch aus dem Trainingsdatensatz und führt ein Vorwärts- und Rückwärtsdurchlauf durch den ersten Kopf durch, und wenn der zufällige Batch zu einer beispielsweis Einzel- und Satzzufassungsaufgabe gehört, führt es ein Vorwärts- und Rückwärtsdurchlauf durch den letzten Kopf durch. In unserem Szenario variiert ein tabellarischer Datensatz die Anzahl der Klassen, sodass es viele Aufgaben gibt. Ein leerer DNN muss eine Anzahl von Klassenköpfen, Ausgabe-Schichten und zusätzliche zusätzliche leere DN-Notwendigkeiten initial als neue Köpfe für einen neuen Datensatz mit einer neuen Aufgabe beibehalten. Unser Ansatz, den wir Task-Reformulierung-Feinabstimmung nennen, besteht darin, dass wir in unserem Ansatz Task-Reform-Feinabstimmung anstelle von mehreren Köpfen zu behalten, jeden Datensatz in ein Satz-pro-Klassifikationsproblem reformulieren, was zwei Klassenaufgaben sind. Lassen Sie uns ein Beispiel betrachten. Hier ist unser Eingabedatensatz, der aus Entitäten, Merkmalen, Text und Klassen besteht. Wir reformulieren die Aufgabe von der Klassifizierung des Textes in niedrig und hoch in die Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch, oder mit anderen Worten, wir trainieren das Sprachmodell, um einen Abstract und eine Klasse zu klassifizieren, um zu versuchen, die Abstract-Klasse zu abstrahieren, ob der Abstract zur Klasse gehört oder nicht. Der Label-Vektor bleibt in Z's Fall immer, bestehend aus immer zwei Klassen. Dies ist der Algorithmus für unseren Fein- oder formulierten Feinabstimmungsansatz. Lassen Sie uns den vollständigen Rahmen sehen. Daten werden in FEST eingespeist und dann führt FEST die Verknüpfungsphase aus. Es extrahiert den Text aus der Wissensdatenbank, der in diesem Beispiel der Abstract der Wikipedia-Seite ist. Dann reformuliert es die Aufgabe in ein Satz-pro-Klassifikationsproblem, wendet das Sprachmodell auf die neue Aufgabe an und erhält die Wahrscheinlichkeit für jede Klasse. Beachten Sie, dass das Sprachmodell bereits über n minus einem Datensatz mit einer vorläufigen Multitask-Feinabstimmung feinabstimmt wurde. Dann verwenden wir den Ausgabevektor des Sprachmodells als neu generiertes Merkmal in der Anzahl der Klassen, um unseren Rahmen zu bewerten. Wir verwenden einen siebzehnten tabularen Klassifikationsdatensatz, der Größe, Merkmale, Balance, Domäne und anfängliche Leistung definiert. Als Wissensdatenbank verwenden wir Wikipedia. Wir entwerfen unser Experiment als Leave-One-Out-Evaluation, bei der wir FEST über 16 Datensätzen trainieren und es auf den 17. Datensatz anwenden. Wir teilen jeden Datensatz auch in vier Fehler auf und wenden eine vierfache Kreuzvalidierung an. Dann generieren wir das neue Merkmal und bewerten sie mit fünf Bewertungs-Klassifikatoren, die wir in unserem Experiment verwenden. Wir verwenden eine auf der Basis von Bird basierende Architektur. Hier sind die Ergebnisse für unser Experiment. Sie können sehen, dass wir unseren Rahmen mit dem Zieldatensatz-Feinabstimmen, dem Zielaufgaben-Feinabstimmen und dem mtdn-vorläufigen Feinabstimmen vergleichen. Unser reformuliertes Feinabstimmen erreicht das beste Ergebnis und die beste Leistung, während mtdnn ein zweiprozentiges Verbesserung gegenüber dem Zieldatensatz-Feinabstimmen erreicht. Unser Ansatz erreicht sechsprozentige Verbesserung. Wenn wir uns das kleine Datensatz-Experiment ansehen, können wir sehen, dass die Leistung von mtdnn abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmungsphase auf 1,5 Prozent sinkt, aber unsere Leistung im Vergleich zum Zielaufgaben-Feinabstimmen auf 11 Prozent steigt. Zusammenfassend ermöglicht FEST die tabuläre Datenanreicherung ab 35 Proben in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Aber es fügt eine Reformulierungsphase hinzu, erweitert den Trainingsdatensatz und benötigt einen Zielwert mit semantischer Bedeutung, sodass wir ihn in das Sprachmodell einfüttern und in das Satz-pro-Klassifikationsproblem verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "- Hallo zusammen. Heute werde ich unsere Forschungsarbeit vorstellen, die sich mit dem schrittweisen logischen Schlussfolgern, dem Problemlösen im Metro-Kontext und der komplexen Regionsextraktion befasst. Ich komme vom Biance AI Lab und dies ist eine gemeinsame Arbeit mit Che von der University of Texas at Austin und Wedu von SUDD. Zunächst möchte ich über unsere Motivation für das Schlussfolgern sprechen. Hier zeigen wir Beispiele, in denen mehrstufiges Schlussfolgern hilfreich ist. Diese Abbildung stammt aus dem Pound-Papier, in dem sie das Prompting zur Lösung des Methodiekproblems in einem zukünftigen Lernszenario verwenden. Auf der linken Seite sehen wir, dass wir, wenn wir nur Fragen und Antworten geben, möglicherweise nicht die richtigen Antworten erhalten können. Aber wenn wir etwas mehr Beschreibung des Schlussfolgerungsprozesses geben, kann das Modell die Grundbeschreibung vorhersagen und auch hier eine korrekte Vorhersage treffen. Es ist also gut, interpretierbares mehrstufiges Schlussfolgern als Ausgabe zu haben. Wir denken auch, dass das Methodiekproblem eine einfache Anwendung zur Bewertung solcher Schlussfolgerungsfähigkeiten ist. Daher haben wir in unserem Problemstellungsszenario die gegebenen Fragen zu lösen und die numerischen Antworten zu erhalten. In unseren Datensätzen wird auch der mathematische Ausdruck angegeben, der zu dieser bestimmten Antwort führt, sodass bestimmte Annahmen wie in früheren Arbeiten gelten. Wir nehmen an, dass die Genauigkeit der Größenordnungen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponential betrachten. Darüber hinaus können komplizierte Operatoren tatsächlich in diese grundlegenden Operatoren zerlegt werden. Frühere Arbeiten zur Methodiekproblem-Lösung können tatsächlich in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle kategorisiert werden. Traditionelle Sequenz-zu-Sequenz-Modelle wandeln den Ausdruck in eine spezifische Sequenz zur Generierung um und sind ziemlich einfach zu implementieren und können auf viele verschiedene komplexe Probleme verallgemeinert werden, aber der Nachteil der Leistung ist in der Tat im Allgemeinen nicht besser als das Strukturmodell und es fehlt an der Interpretierbarkeit für Vorhersagen. Diese Richtung ist jedoch aufgrund des Transformer-Modells immer noch ziemlich beliebt. In baumgestützten Modellen strukturieren wir diese Ausdrücke tatsächlich in Baumform und folgen einer vorrangigen Durchquerung bei Baumgenerierungen. Hier erzeugen wir die Operatoren weiter, bis wir die Blätter erreichen, die die Größenordnungen sind. Das Gute daran ist, dass es tatsächlich diese binäre Baumstruktur gibt. Aber das ist ziemlich kontraproduktiv, weil wir den Operator zuerst erzeugen und dann am Ende die Größenordnungen erzeugen. Die zweite Sache ist, dass es auch einige repetitive Berechnungen enthält. Wenn wir uns diesen Ausdruck ansehen, wird a mal 3 plus 3 tatsächlich zweimal erzeugt, aber in der Tat sollten wir die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir in der zweiten Stufe diese Divisor erhalten, der 27 ist, und wir können uns auch auf die ursprünglichen Fragen beziehen, um den relevanten Inhalt zu finden. In diesen Schritten erhalten wir die Divisor und dann erhalten wir in diesem dritten Schritt tatsächlich den Quotienten. Nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt wiederverwenden und dann die Ergebnisse des vierten Schrittes erhalten und schließlich die Dividenden erhalten. Hier erzeugen wir den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Größenordnungen zu erzeugen. Das macht den Prozess genauer. In unserem deduktiven System beginnen wir mit einer Reihe von Größenordnungen, die in den Fragen präsentiert werden, und schließen auch einige Konstanten als unseren anfänglichen Anfangszustand ein. Der Ausdruck wird durch eij dargestellt, wobei wir den Operator von qi zu qj durchführen und ein solcher Ausdruck ist tatsächlich gerichtet. Wir haben auch die Subtraktion umgekehrt, um die entgegengesetzte Richtung darzustellen. Das ist ziemlich ähnlich zur Relationenextraktion. In einem formellen deduktiven System wenden wir im Schrittzeitpunkt den Operator zwischen dem qi und qj Paar an und erhalten dann diese neuen Ausdrücke. Wir fügen sie den nächsten Zuständen hinzu, um eine neue Größe zu werden. Diese Folien visualisieren tatsächlich die Entwicklung der Zustände, in denen wir weiterhin Ausdrücke zu den aktuellen Zuständen hinzufügen. In unseren Modellimplementierungen verwenden wir zunächst ein vortrainiertes Modell, das Vögel oder Rohheitswerte sein kann, und kodieren dann den Satz und erhalten dann diese Größenordnungsdarstellungen. Sobald wir die Größenordnungsdarstellungen erhalten haben, können wir hier mit der Inferenz beginnen. Wir zeigen ein Beispiel von q1 zu q1, um die Darstellung für q1 geteilt durch q2 und dann mal q3 zu erhalten. Zuerst erhalten wir die Paar-Darstellung, die im Grunde nur die Verkettung zwischen q1 und q2 ist, und dann wenden wir ein Feed-Forward-Netzwerk an, das durch den Operator parametrisiert ist, und erhalten dann schließlich die Ausdruck-Darstellung q1 geteilt durch q2, aber in der Praxis in der Inferenzstufe könnten wir möglicherweise auch den falschen Ausdruck erhalten. Hier ist der gesamte mögliche Ausdruck gleich drei mal die Anzahl der Operatoren. Das Schöne hier ist, dass wir leicht Einschränkungen hinzufügen können, um diese Suche zu steuern. Zum Beispiel, wenn dieser Ausdruck nicht erlaubt ist, können wir diesen Ausdruck einfach in unserem Suchraum entfernen. Im zweiten Schritt tun wir das gleiche, aber der einzige Unterschied ist, dass wir die einzige Differenz ist, dass wir eine weitere Größe haben. Diese Größe stammt aus dem vorher berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck q3 mal q4 erhalten und wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von dem vorherigen Schritt unterscheidet. Ein solcher Unterschied macht es schwierig, den Suchraum mit Balken zu durchsuchen, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgewogen ist. Das Trainingsprozedere ist ähnlich wie das Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust in jedem Zeitschritt optimieren. Hier verwenden wir auch dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Der Raum ist hier anders als bei Sequenz-zu-Sequenz, da der Raum bei jedem Zeitschritt anders ist, während bei traditionellen Sequenz-zu-Sequenz-Modellen die Anzahl des Vokabulars ist und es auch ermöglicht, bestimmte Einschränkungen aus vorherigem Wissen aufzuerlegen. Wir führen Experimente auf den häufig verwendeten Methodiekproblem-Datensätzen mwps, method3k, math qaA und swam durch und zeigen hier kurz die Ergebnisse im Vergleich zu den vorherigen besten Ansätzen. Unser bestes Ergebnis ist Roberta deductive reason und in der Tat verwenden wir keinen Balken-Suchalgorithmus, im Gegensatz zu offensichtlichen Ansätzen, die Balken-Suchalgorithmen verwenden. Die besten Ansätze sind oft ein baumgestütztes Modell. Insgesamt ist unser Schlussfolgerer in der Lage, signifikant Output aus diesem baumgestützten Modell auszuwählen, aber wir können die absolute Anzahl auf math qaA oder swam nicht wirklich sehen. Wir untersuchen die Ergebnisse auf swam weiter und dieser Datensatz ist herausfordernd, weil der Autor versucht hat, etwas hinzuzufügen, um das NLP-Modell zu verwirren, wie das Hinzufügen verfügbarer Informationen und zusätzlicher Größenordnungen. In unserer Vorhersage finden wir, dass einige der Zwischenergebnisse tatsächlich negativ sind. Zum Beispiel fragen wir in diesen Fragen, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie 17 weniger Pitchachees und Stephen hat acht Pitchachees, was völlig relevant ist. Unser Modell macht einige Vorhersagen wie diese, die negative Werte produzieren, und wir beobachten, dass diese beiden Ausdrücke tatsächlich ähnliche Scores haben. Wir können diesen Suchraum tatsächlich einschränken, indem wir solche Ergebnisse entfernen, die negativ sind, sodass wir die Antwort richtig machen können. Wir finden weiter, dass eine solche Einschränkung tatsächlich ziemlich viel für einige Modelle verbessert. Zum Beispiel haben wir für Vögel sieben Punkte und für das auf Roberta basierende Modell zwei Punkte verbessert. Ein besseres Sprachmodell hat bessere Sprachverständnisfähigkeiten, sodass die Anzahl hier für Roberta höher und für Vögel niedriger ist. Wir versuchen auch, die Schwierigkeit hinter diesem gesamten Datensatz zu analysieren. Wir nehmen an, dass die Anzahl der ungenutzten Größenordnungen als relevante Informationen hier betrachtet werden kann. Hier können wir sehen, dass wir den Prozentsatz der Proben, die ungenutzte Größenordnungen haben, zusammenfassen. Der Swamp-Datensatz hat den größten Anteil. Hier zeigen wir auch die Gesamtleistung für diese Proben ohne ungenutzte Größenordnungen. Die Gesamtleistung ist tatsächlich höher als die Leistung ist tatsächlich höher als die Gesamtleistung, aber mit diesen Proben, die ungenutzte Größenordnungen haben, ist es tatsächlich viel schlechter als die Com-uh viel schlechter als die uh die Gesamtleistung vonWPS. Wir haben nicht wirklich uh zu viele Todesfälle, also ignoriere ich diesen Teil. Um schließlich die Interpretierbarkeit durch ein Crash- und Präsentationsbeispiel zu zeigen, macht unser Modell hier tatsächlich eine falsche Vorhersage in der ersten Stufe. Wir können tatsächlich diesen Ausdruck mit dem Satz hier korrelieren. Wir denken, dass dieser Satz das Modell möglicherweise in die Irre führt, um eine falsche Vorhersage zu treffen. Hier drucken wir einen weiteren 35, der das Modell glauben lässt, dass es ein Additionsoperator sein sollte. Wir versuchen, den Satz zu überarbeiten, um etwas wie die Anzahl der Birnbäume ist 5 weniger als die Apfelbäume zu sagen, sodass das Modell die Vorhersage korrekt machen kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Modellverhalten zu verstehen. Um unsere Arbeit abzuschließen, ist unser Modell tatsächlich ziemlich effizient und wir sind in der Lage, interpretierbare Lösungsverfahren bereitzustellen und können leicht einige vorherige Kenntnisse als Einschränkung einbeziehen, die die Leistung verbessern können. Und das Letzte ist, dass der zugrunde liegende Mechanismus nicht nur auf Netzwerkproblem-Lösungsaufgaben anwendbar ist, sondern auch auf andere Aufgaben, die mehrstufiges Schlussfolgern beinhalten. Aber wir haben auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, kann der Speicherverbrauch ziemlich hoch sein. Und die zweite Sache ist, wie erwähnt, weil die Wahrscheinlichkeitsverteilung zwischen verschiedenen Zeitschritten unausgewogen ist, ist es auch ziemlich herausfordernd, Balken-Suchalgorithmen anzuwenden. Das ist das Ende des Vortrags und Fragen sind willkommen. Danke."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine, und ich komme von der Universität Maastricht. Ich werde meine John-Arbeit mit Jerry vorstellen, die sich auf einen neuen Datensatz für die Abrufbarkeit von Gesetzesartikeln bezieht. Rechtsfragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat wenig bis kein Wissen über ihre Rechte und grundlegende rechtliche Prozesse. Infolgedessen bleiben viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsgutachters nicht leisten können, ungeschützt oder werden im schlimmsten Fall ausgebeutet. Alle Arbeiten zielen darauf ab, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem sie ein effektives Abrufsystem für Gesetzesartikel entwickeln. Ein solches System könnte einen kostenlosen professionellen Rechtshilfsdienst für ungeschulte Menschen bereitstellen. Bevor wir uns auf den Hauptbeitrag dieser Arbeit einlassen, lassen Sie uns zunächst das Problem des Abrufs von Gesetzesartikeln beschreiben. Angenommen, es wird eine einfache Frage zu einer rechtlichen Angelegenheit gestellt, wie zum Beispiel: „Was riskiert man, wenn man die berufliche Vertraulichkeit verletzt?“ Ein Modell ist erforderlich, um alle relevanten Gesetzesartikel aus einem großen Gesetzestext zu abrufen. Diese Informationsbeschaffungsaufgabe bringt ihre eigenen Herausforderungen mit sich. Erstens befasst sie sich mit zwei Arten von Sprache: der üblichen natürlichen Sprache für die Fragen und der komplexen juristischen Sprache für die Gesetze. Dieser Unterschied in den Sprachverteilungen erschwert es einem System, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Gesetze entspricht. Zweitens ist das Gesetz kein Stapel unabhängiger Artikel, die als vollständige Informationsquelle behandelt werden können, wie zum Beispiel Nachrichten oder Rezepte. Stattdessen ist es eine Strukturkollektion von Rechtsvorschriften, die erst im Gesamtzusammenhang eine vollständige Bedeutung haben, das heißt zusammen mit den ergänzenden Informationen aus ihren benachbarten Artikeln, den Bereichen und Teilbereichen, zu denen sie gehören, und ihrem Platz in der Struktur des Rechts. Zuletzt sind Gesetzesartikel in kleinen Absätzen, die in den meisten Abruf-Arbeiten normalerweise die typische Abrufeinheit darstellen. Hierbei handelt es sich um lange Dokumente, die bis zu 6.000 Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben großes Interesse an vielen rechtlichen Aufgaben geweckt, wie der Vorhersage von Gerichtsurteilen oder der automatischen Prüfung von Vertragsverträgen. Der Abruf von Gesetzesartikeln ist jedoch hauptsächlich aufgrund des Mangels an großen und hochwertigen, annotierten Datensätzen weitgehend unberührt geblieben. In dieser Arbeit präsentieren wir einen neuen, französischsprachigen, bürgerzentrierten Datensatz, um zu untersuchen, ob ein Abrufmuster die Effizienz und Zuverlässigkeit eines Rechtsgutachters für die Aufgabe des Abrufs von Gesetzesartikeln annäherungsweise erreichen kann. Der Datensatz für den Abruf von belgischen Gesetzesartikeln besteht aus mehr als 1.100 rechtlichen Fragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken eine breite Palette von Themen von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung ab. Jede von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als zwanzigtausendsechs hundertundsechsunddreißig belgischen Gesetzesartikeln annotiert. Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben. Zuerst begannen wir damit, einen großen Korpus von Gesetzesartikeln zusammenzustellen. Wir betrachteten 32 öffentlich zugängliche belgische Gesetzbücher und extrahierten alle ihre Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir rechtliche Fragen mit Verweisen auf relevante Gesetze. Dazu arbeiteten wir mit der belgischen Anwaltskanzlei zusammen, die jedes Jahr rund 400 E-Mails von belgischen Bürgern erhält, die um Rat zu einem persönlichen rechtlichen Problem bitten. Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team erfahrener Juristen die häufigsten rechtlichen Probleme Belgiens anspricht. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Gesetze annotiert waren. Zuletzt filterten wir die rechtlichen Verweise heraus und filterten die Fragen heraus, deren Verweise keine Artikel aus einem der betrachteten Gesetzbücher waren. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Letztendlich endeten wir mit eintausendneunundachtzig Fragen, die sorgfältig mit den IDs der relevanten Artikel aus einem großen Korpus von einundzwanzigtausendsechs hundertunddreißig Gesetzesartikeln annotiert wurden. Zusätzlich ist jede Frage mit einer Hauptkategorie und einer Verkettung von Unterkategorien versehen, und jeder Artikel ist mit einer Verkettung seiner nachfolgenden Überschrift in der Struktur des Rechts versehen. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschungen zum rechtlichen Informationsabruf oder zur rechtlichen Textklassifizierung von Interesse sein. Lassen Sie uns einige Merkmale unserer Datensätze betrachten. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind viel länger mit einer Medianlänge von 77 Wörtern, wobei 142 von ihnen länger als 1.000 Wörter sind. Der längste ist bis zu fünftausendsiebenhundertneunzig Wörter lang. Wie bereits erwähnt, decken die Fragen eine breite Palette von Themen ab, wobei rund 85 Prozent entweder über Familie, Wohnen, Geld oder Justiz handeln, während die verbleibenden 15 Prozent sich entweder auf Sozialversicherung, Ausländer oder Arbeit beziehen. Die Artikel sind auch sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl von rechtlichen Themen abdecken. Hier ist die Gesamtzahl der aus jedem dieser belgischen Gesetzbücher gesammelten Artikel. Von den einundzwanzigtausendsechs hundertunddreißig Artikeln wurden nur 1.612 als relevant für mindestens eine Frage in den Datensätzen bezeichnet, und rund 80 Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, den Gerichtsgesetzbüchern, dem Strafprozessgesetz oder den Strafgesetzbüchern. In der Zwischenzeit haben 18 von 32 Gesetzbüchern weniger als fünf Artikel, die als relevant für mindestens eine Frage genannt wurden, was darauf zurückzuführen ist, dass sich diese Gesetzbücher weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt beträgt die durchschnittliche Anzahl der Zitate für diese zitierten Artikel zwei, und weniger als 25 Prozent von ihnen werden mehr als fünf Mal zitiert. Mit unseren Datensätzen vergleichen wir mehrere Abrufmuster, einschließlich lexikalischer und dichter Architekturen. Bei einer Abfrage in einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe über die Abfrageterms der Gewichte jedes dieser Terme in dem Artikel berechnet. Wir experimentieren mit den Standard-TF-IDF- und BM25-Rankingfunktionen. Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronengestützten Architektur, die die semantische Beziehung zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein Bi-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet und eine Relevanzpunktzahl zwischen einem Abfrage-Artikel-Paar durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen resultieren typischerweise aus einer Pooling-Operation auf dem Ausgabe eines Wort-Einbettungsmodells. Zuerst untersuchen wir die Wirksamkeit von Siamese-Encodern in einem Zero-Shot-Setup, was bedeutet, dass vorgefertigte Wort-Einbettungsmodelle sofort ohne zusätzliche Feinabstimmung angewendet werden. Wir experimentieren mit kontextunabhängigen Textoren, nämlich Word2Vec und FastText, und kontextabhängigen Einbettungsmodellen, nämlich Roberta und speziell Camembert, einem französischen Roberta-Modell. Zusätzlich trainieren wir unser eigenes Camembert-basiertes Modell auf allen Datensätzen. Beachten Sie, dass wir für das Training die beiden Varianten der biancoder-Architektur experimentieren: Siamese, die ein einzigartiges Wort-Einbettungsmodell verwendet, das die Abfrage und den Artikel gemeinsam in einem gemeinsamen dichten Vektorraum abbildet, und Tower, das zwei unabhängige Wort-Einbettungsmodelle verwendet, die die Abfrage und den Artikel separat in verschiedene Einbettungsräume kodieren. Wir experimentieren mit Mean-Max- und CLS-Pooling sowie Punktprodukt und Kosinus für die Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse eines Basismodells auf den Testdatensätzen mit den oben genannten lexikalischen Methoden, den in einem Zero-Shot-Setup bewerteten Siamese-Encodern in der Mitte und den unten angefertigten, fein abgestimmten biancodern. Insgesamt übertreffen die fein abgestimmten biancoder alle anderen Basismodelle erheblich. Das Tower-Modell verbessert sich gegenüber seiner Siamese-Variante bei der Erinnerung bei 100, aber die Leistung bei den anderen Metriken ist ähnlich. Obwohl BM25 unter den trainierten biancodern deutlich unterdurchschnittlich abgeschnitten hat, deuten seine Ergebnisse darauf hin, dass es immer noch eine starke Basis für domänenspezifische Abrufmuster darstellt. Was die Zero-Shot-Bewertung von Siamese-Encodern betrifft, so haben wir festgestellt, dass die direkte Verwendung der Einbettungen eines vorgefertigten Camembert-Modells ohne Optimierung für die Informationsbeschaffungsaufgabe zu schlechten Ergebnissen führt, was mit früheren Befunden übereinstimmt. Darüber hinaus beobachten wir, dass der auf Word2Vec basierende biancoder die FastText- und Bird-basierten Modelle erheblich übertrifft, was darauf hindeutet, dass vorgefertigte Einbettungen auf Wortniveau möglicherweise geeigneter für die Aufgabe sind als Einbettungen auf Zeichen- oder Subwortniveau, wenn sie sofort verwendet werden. Obwohl vielversprechend, deuten diese Ergebnisse auf reichlich Raum für Verbesserungen im Vergleich zu einem Experten auf Fachniveau hin, der schließlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Punktzahlen erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen aller Datensätze diskutieren. Erstens ist der Korpus der Artikel auf die aus den 32 betrachteten belgischen Gesetzbüchern gesammelten Artikel beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen. Alle Verweise auf diese nicht gesammelten Artikel werden ignoriert, was dazu führt, dass einige Fragen mit nur einem Bruchteil der anfänglichen Anzahl relevanter Artikel enden. Dieser Informationsverlust impliziert, dass die in den verbleibenden relevanten Artikeln enthaltene Antwort unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist. Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzen beantwortet werden können. Zum Beispiel könnte die Frage „Kann ich meine Mieter des Hauses werfen, wenn sie zu viel Lärm machen?“ keine detaillierte Antwort innerhalb des Gesetzes finden, die einen spezifischen Lärmpegel quantifiziert, bei dem eine Räumung angemessen ist. Stattdessen sollte der Vermieter wahrscheinlich eher auf Fallrecht zurückgreifen und Präzedenzfälle finden, die ähnlich zu seiner aktuellen Situation sind. Zum Beispiel, der Mieter veranstaltet zweimal pro Woche Partys bis 2 Uhr morgens. Daher eignen sich einige Fragen besser als andere für die Aufgabe des Abrufs von Gesetzesartikeln, und der Bereich der weniger geeigneten bleibt noch zu bestimmen. Wir hoffen, dass alle Arbeiten Interesse an der Entwicklung praktischer und zuverlässiger Abrufmuster für Gesetzesartikel wecken, die dazu beitragen können, den Zugang zur Justiz für alle zu verbessern. Sie können unseren Artikel „dotset encode“ unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, unsere Arbeit an den Vokalen präsentieren zu können. Dies ist ein aufgabenunabhängiger Benchmark, der für die Testung von Seh- und Sprachmodellen mit spezifischen sprachlichen Phänomenen konzipiert ist. Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten? Nun, in den letzten Jahren haben wir eine Explosion von auf Transformer basierenden Seh- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vortrainiert wurden. Jedes dieser Modelle setzt den Stand der Technik bei Seh- und Sprachaufgaben wie der visuellen Fragebeantwortung, dem visuellen Common-Sense-Reasoning, der Bildabfrage, der Phrasenverankerung voran. Daher haben wir eine Botschaft erhalten: Die Genauigkeiten bei diesen aufgaben-spezifischen Benchmarks steigen stetig, aber wissen wir wirklich, was die Modelle tatsächlich gelernt haben? Was ist es, das ein Seh- und Sprachtransformer versteht, wenn er eine hohe Punktzahl für dieses Bild und diesen Satz zuweist, um zu einer Übereinstimmung zu kommen, und eine niedrige Punktzahl für diesen hier? Konzentrieren sich Seh- und Sprachmodelle auf das Richtige, oder konzentrieren sie sich auf Verzerrungen, wie frühere Arbeiten gezeigt haben? Um diesen Aspekt genauer zu beleuchten, schlagen wir eine aufgabenunabhängige Richtung vor und führen Vokale ein, die die Empfindlichkeit von Seh- und Sprachmodellen gegenüber spezifischen sprachlichen Phänomenen testen, die sowohl die sprachliche als auch die visuelle Modalität betreffen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entität-Koperferenz ab. Aber wie testen wir, ob die Seh- und Sprachmodelle diese Phänomene erfasst haben, indem wir eine Methode vereiteln, die zuvor nur für Nomenphrasen von Ravi Shekhar und Mitarbeitern und für das Zählen in früheren Arbeiten angewendet wurde? Vereiteln bedeutet im Grunde, dass wir die Bildunterschrift nehmen und eine Folie erstellen, indem wir die Bildunterschrift so ändern, dass sie das Bild nicht mehr beschreibt. Wir führen diese Phrasenänderungen durch, indem wir uns auf sechs spezifische Teile konzentrieren: Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entität-Koperferenz. Jeder Teil kann aus einem oder mehreren Instrumenten bestehen, falls wir mehr als einen interessanten Weg gefunden haben, FOIL-Instanzen zu erstellen. Zum Beispiel haben wir im Fall des Handlungsteils zwei Instrumente, eines, bei dem das Handlungsverbum durch eine andere Handlung ersetzt wird, und eines, bei dem die Akteure ausgetauscht werden. Zählen und Koperferenz sind ebenfalls Teile, die mehr als ein Instrument haben. Und wir erstellen diese Folien, indem wir sicherstellen, dass sie das Bild nicht mehr beschreiben, dass sie grammatische und ansonsten gültige Sätze sind. Das ist nicht einfach, denn eine vereitelte Bildunterschrift ist möglicherweise weniger wahrscheinlich als die ursprüngliche Bildunterschrift. Zum Beispiel ist es, obwohl nicht unmöglich, statistisch weniger wahrscheinlich, dass Pflanzen einen Mann schneiden, als dass ein Mann Pflanzen schneidet, und große Seh- und Sprachmodelle könnten dies aufgreifen. Daher müssen wir zunächst Maßnahmen ergreifen: Erstens nutzen wir starke Sprachmodelle, um Folien vorzuschlagen, und zweitens verwenden wir natürliche Sprachinferenz oder kurz NLI, um Folien herauszufiltern, die das Bild immer noch beschreiben könnten. Da wir bei der Konstruktion von Folien sicherstellen müssen, dass sie das Bild nicht mehr beschreiben, wenden wir automatisch natürliche Sprachinferenz an mit der folgenden Begründung: Wir betrachten ein Bild als Prämisse und seine Bildunterschrift als implizite Hypothese. Zusätzlich betrachten wir die Bildunterschrift als Prämisse und die Folie als ihre Hypothese. Wenn ein NLI-Modell vorhersagt, dass die Folie der Bildunterschrift widerspricht oder neutral gegenüber ihr ist, nehmen wir dies als Indikator für eine gültige Folie. Wenn ein NLI vorhersagt, dass die Folie von der Bildunterschrift impliziert wird, kann sie keine gute Folie sein, da sie durch Transitivität eine wahrheitsgetreue Beschreibung des Bildes liefern würde, und wir filtern diese Folien heraus. Dieses Verfahren ist jedoch nicht perfekt, es ist nur ein Indikator für gültige Folien. Daher verwenden wir als dritte Maßnahme zur Generierung gültiger Folien menschliche Annotatoren, um die in VALSE verwendeten Daten zu validieren. Nach der Filterung und der menschlichen Bewertung haben wir so viele Testinstanzen, wie in dieser Tabelle beschrieben. Beachten Sie, dass VALSE keine Trainingsdaten liefert, sondern nur Testdaten, da es ein Zero-Shot-Test-Benchmark ist. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Seh- und Sprachmodellen nach dem Vortraining zu nutzen. Feinabstimmung würde den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Und wie wir sagten, sind wir daran interessiert, zu bewerten, welche Fähigkeiten die Seh- und Sprachmodelle nach dem Vortraining haben. Wir experimentieren mit fünf Seh- und Sprachmodellen an den Vokalen, nämlich mit CLIP, ALEX, BERT, WILBERT, WILBERT 12 in 1 und Visual Bird. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Text-Paaren in Bildunterschriften und Folien. Vielleicht relevanter für dieses Video werden wir unsere permissivere Metrik vorstellen, die paarweise Genauigkeit, die misst, ob der Bild-Text-Ausrichtungswert für das korrekte Bild-Text-Paar größer ist als für sein vereiteltes Paar. Für weitere Metriken und Ergebnisse dazu schauen Sie sich bitte unser Papier an. Die Ergebnisse mit der paarweisen Genauigkeit sind hier gezeigt und sie sind konsistent mit den Ergebnissen, die wir von den anderen Metriken erhalten haben: Die beste Zero-Shot-Leistung wird von WILBERT 12 in 1 erreicht, gefolgt von WILBERT, ALEX, BERT, CLIP und schließlich Visual Bird. Es ist bemerkenswert, wie auf einzelne Objekte ausgerichtete Instrumente wie Existenz und Nomenphrasen fast von WILBERT 12 in 1 gelöst werden, was darauf hinweist, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Allerdings können keine der verbleibenden Teile in unseren adversariellen Vereitelungs-Einstellungen zuverlässig gelöst werden. Wir sehen aus den Instrumenten für Pluralität und Zählen, dass Seh- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne versus mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Das Teil für Beziehungen zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir im Handlungsteil aus dem Referenzteil sehen. Aus dem Teil für Verweise auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen erfahren wir, dass dies für Seh- und Sprachmodelle ebenfalls schwierig ist. Als Überprüfung und weil es ein interessantes Experiment ist, bennen wir auch zwei Text-only-Modelle, GPT1 und GPT2, um zu bewerten, ob VALSE von diesen unimodalen Modellen lösbar ist, indem wir die Perplexität der korrekten und der vereitelten Bildunterschrift berechnen. Hier gibt es kein Bild, und wir prognostizieren den Eintrag mit der niedrigsten Perplexität. Wenn die Perplexität für die Folie höher ist, nehmen wir dies als Indikation dafür, dass die vereitelte Bildunterschrift unter Plausibilitätsverzerrung oder anderen sprachlichen Verzerrungen leiden könnte. Es ist interessant zu sehen, dass in einigen Fällen die Text-only GPT-Modelle die Plausibilität der Welt besser erfasst haben als die Seh- und Sprachmodelle. Um zusammenzufassen: VALSE ist ein Benchmark, der die Linse sprachlicher Konstrukte nutzt, um der Gemeinschaft zu helfen, Seh- und Sprachmodelle zu verbessern, indem er ihre visuellen Verankerungsfähigkeiten hart testet. Unsere Experimente zeigen, dass Seh- und Sprachmodelle benannte Objekte in ihrer Anwesenheit in Bildern gut identifizieren, wie der Existenzteil zeigt, aber Schwierigkeiten haben, ihre Interdependenz und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen sind, sprachliche Indikatoren zu respektieren. Wir würden die Gemeinschaft wirklich dazu ermutigen, Vokale zur Messung des Fortschritts bei der sprachlichen Verankerung mit Seh- und Sprachmodellen zu verwenden. Und noch mehr, Vokale könnten als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich bei den von VALSE getesteten Aspekten zu verbessern. Wenn Sie interessiert sind, schauen Sie sich die VALSE-Daten auf GitHub an und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisura von der Universität Tokio. Ich werde einen Vortrag mit dem Titel „O En Sum: Ein großangelegtes Desset für automatische Listen-Nichtdauer mit Lo Sumization“ halten. Ich werde in dieser Reihenfolge erklären: Zuerst werde ich die automatische Li-Notation vorstellen, an der wir in dieser Forschung arbeiten. ReaseNote ist ein technisches Dokument, das die mit jeder Version eines Softwareprodukts verteilten Änderungen zusammenfasst. Das Bild zeigt die Versionshinweise für Version 2.6.4 der Bujs-Bibliothek. Diese Hinweise spielen eine wichtige Rolle in der Open-Source-Entwicklung, aber sie sind zeitaufwendig, wenn sie manuell erstellt werden. Daher wäre es sehr nützlich, hochwertige Versionshinweise automatisch generieren zu können. Ich werde auf zwei frühere Forschungen zur automatischen Listenknoten-Generierung eingehen. Die erste ist ein System namens Alena, das 2014 veröffentlicht wurde. Es verwendet einen regelbasierten Ansatz, z. B. den Änderungs-Extractor, um Kernunterschiede, Bibliotheksänderungen und Dokumentänderungen aus den Unterschieden zwischen Krankheiten zu extrahieren und sie schließlich zu kombinieren. Das bemerkenswerteste Merkmal dieses Systems ist der Problem-Extractor in der oberen rechten Ecke, der mit Jira, dem Problem-Ökosystem, verknüpft sein muss und nur für Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann für viele Projekte auf GitHub nicht verwendet werden. Die zweite ist Grif, das kürzlich im Jahr 2020 angekündigt wurde. Es ist im Internet verfügbar und kann über Pi gespeichert werden. Dieses System verfügt über ein einfaches, laufbasiertes Textklassifizierungsmodell und gibt für jede Eingabeaufgabenmeldung eine von fünf Problemen aus, wie z. B. Funktionen oder Fehlerbehebungen. Das Bild ist ein Beispiel für die Verwendung, das eine korrekte Band- oder Fehlerbehebung zurückgibt. Das Trainingsdatenset ist ziemlich klein, etwa 5000 Datensätze, und wird in den unten beschriebenen Experimenten gezeigt. Die Leistung des Textklassifizierungsmodells ist nicht hoch. Ich präsentiere zwei verwandte Forschungen, aber es gibt Probleme mit begrenzter Anwendbarkeit und knappen Datenressourcen. Unser Papier löst diese beiden Probleme und generiert automatisch hochwertige Ressourcen. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine hochwertige Klassifikator-Summierungsmethode vor, die nur die Aufgabenmeldung als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir unser eigenes Datenset erstellt und einige Daten zusammengestellt, die aus etwa 82.000 Datensätzen bestehen, indem wir Daten aus öffentlichen Gitub-Repositories mit der Git-API korrigiert haben. Als Nächstes beschreibe ich unsere Daten. Hier ist ein Beispiel: Die linke Seite ist eine Aufgabenmeldung und die rechte Seite sind die Versionshinweise. Die Versionshinweise sind als Verbesserungen von Gesichtern usw. eingeordnet. Wir haben eine Aufgabe eingerichtet, die die Aufgabenmeldungen als Eingabe nimmt und die Versionshinweise als Ausgabe liefert. Dies kann als Zusammenfassung betrachtet werden. Wir haben vier Ebenen vordefiniert: Funktionen, Implementierungen, Fehlerbehebungen, Deprecationen, Entferner und Bruchveränderungen. Diese wurden basierend auf vorheriger Verwendung und anderen Faktoren festgelegt. Es gibt eine Notiz unten rechts und extrahiert aus dem Listenknoten, der unten links gezeigt wird. Zu diesem Zeitpunkt ist es notwendig, die vier Knoten zu erkennen, die in der Vergangenheit festgelegt wurden, aber die Ebenen sind nicht immer konsistent mit jedem Knoten. Zum Beispiel umfasst die Verbesserungsebene Verbesserungen, Erweiterungen, Optimierungen und so weiter. Wir haben eine Vokabelliste für Studienebenen für jede dieser Notationsschwankungen erstellt. Verwenden Sie sie, um die Risikoknotenklasse zu erkennen und den Text des Restes, der folgt, als Risikokennennung zu korrigieren. Als Nächstes ist eine Aufgabenmeldung. Aufgabenmeldungen sind nicht an jede Rasse gebunden, wie im folgenden Bild gezeigt. Wenn die aktuelle Version die Version 2.5 bis 19 ist, müssen wir die vorherige Version 2.5 bis 18 identifizieren und sie abrufen. Dies ist etwas langweilig und es reicht nicht aus, einfach eine Liste der Versionen zu erhalten und das Vorherige und Nachher zu betrachten. Wir haben einen heuristischen Matching-Kleber erstellt, um die vorherige und nächste Version zu erhalten. Die Datenanalyse am Ende ergab, dass 7200 Repositories und 82.000 Datensätze korrigiert wurden. Die durchschnittliche Anzahl der vernünftigen Token beträgt 63, was für die Zusammenfassung recht hoch ist. Die Anzahl der eindeutigen Token ist ebenfalls recht hoch bei achtundachtzigtausend. Dies liegt an der großen Anzahl einzigartiger Klassen- und Methodenamen, die im Repository gefunden wurden. Als Nächstes werde ich die vorgeschlagene Methode erklären. Das querweise extraktive und abstraktive Zusammenfassungmodell besteht aus zwei neuronalen Modulen: Ein Klassifikator, der Bot oder Code-Bot verwendet, und der Generator, der Bot verwendet. Zuerst verwendet Bot ein Klassifikator, um jede Aufgabenmeldung in fünf Basis-Knoten-Klassen zu klassifizieren: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecationen, Presse und andere. Die als andere klassifizierten Aufgabenmeldungen werden verworfen. Dann wendet sie einen Generator auf die vier Dokumentdokumente unabhängig an und generiert Versionshinweise für jede Klasse. In dieser Aufgabe sind die direkten Korrespondenzen zwischen Aufgabenmeldungen und Versionshinweisen nicht bekannt. Daher weisen wir jedem Eingangs-Aufgabenbericht eine Pseudo-Variable zu, indem wir die ersten 10 Zeichen jeder Aufgabenmeldung verwenden. Wir modellieren den klassenweisen abstraktiven Zusammenfassungansatz durch zwei definierte Methoden. Das erste Modell, das wir GS-Single nennen, besteht aus einem einzelnen Sex-Netzwerk und generiert einen einzelnen langen Text, der eine Verkettung der Eingangs-Aufgabenmeldungen ist. Der Ausgabetext kann in Klassen-Dateisegmente basierend auf speziellen, klassenspezifischen Endpunktsymbolen unterteilt werden. Die zweite Methode, die wir She-Much nennen, besteht aus vier verschiedenen Sec-to-Sec-Netzwerken, von denen jedes einer der kleinsten Knoten-Klassen entspricht. Okay, lassen Sie mich das Experiment erklären. Fünf Methoden wurden verglichen: GS, She-Much, Single, She-Much, Cluster und die vorherige Studie Gri. In einigen Fällen werden diese Hinweise in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze auf Null zu korrigieren, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Das Blau ist eine Strafe, wenn das System einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren Bre-Wert in den nächsten beschriebenen Experimenten. Schließlich haben wir auch eine Spezifität berechnet, da Blau und Blau nicht berechnet werden können, wenn die Listenhinweise leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt leere Texte ausgibt, wenn die Versionshinweise leer sind. Hier sind die Ergebnisse, da der Datensatz E-Mail-Analysen usw. enthält. Wir haben auch den sauberen Datensatz bewertet, der sie ausschließt. G und GS erreichten Verlustfehlerbewertungen, die mehr als 10 Punkte höher als die Basislinie waren. Insbesondere auf dem koreanischen Testdatensatz sprang die Punktdifferenz zwischen der vorgeschlagenen Methode und der Basislinie auf mehr als zwanzig Punkte. Diese Ergebnisse deuten darauf hin, dass GS und GS signifikant wirksam sind. GS erreichte einen besseren Verlustfehler als GAS, was darauf hindeutet, dass die Kombination eines Klassifikators unter einem Generator bei der Klassifizierung des Klassifikators mit Servern wirksam ist. Die hohe Abdeckung von GS kann ordnungsgemäß erreicht werden, da der Klassifikator sich auf die Auswahl relevanter Aufgabenmeldungen für jede Klasse konzentrieren kann. She-Much tendierte dazu, höher zu sein als She-Is-Single, was darauf hindeutet, dass es auch wirksam ist, verschiedene konstruktive Zusammenfassungmodelle für jede Stückenknotenklasse unabhängig zu entwickeln. Hier ist eine Fehleranalyse. She-Methoden neigen dazu, kürzere Sätze als der menschliche Referenzsatz auszugeben, da im Diagramm auf der rechten Seite der Referenzsatz drei oder vier Sätze hat, während CSS nur einen hat. Der Grund für diese Modellzurückhaltung ist, dass im Trainingsdatensatz nur dreißig Prozent der Sätze auf der Ebene der Funktionen und vierzig Prozent auf der Ebene der Verbesserungen vorhanden sind. Darüber hinaus können CS-Methoden keine genauen Versionshinweise ohne zusätzliche Informationen generieren. Das obige Beispiel auf der rechten Seite ist ein Beispiel für eine sehr chaotische Aufgabenmeldung und der vollständige Satz kann nicht ohne Unterschied zu den entsprechenden Vorrechten oder Problemen generiert werden. Das folgende Beispiel zeigt, dass die beiden im Eingang genannten Aufgabenmeldungen miteinander verbunden sind und in einen Satz kombiniert werden sollten, was jedoch nicht gelingt. Abschließend eine Schlussfolgerung. Wir haben einen neuen Datensatz für die automatische persönliche Generierung erstellt. Wir haben auch die Aufgabe formuliert, Aufgabenmeldungen einzugeben und sie so zusammenzufassen, dass sie für alle Projekte anwendbar ist, die in Englisch geschrieben sind. Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger laute Versionshinweise bei höherer Abdeckung als die Basislinie generiert. Bitte überprüfen Sie unsere Daten auf GitHub. Vielen Dank."}
