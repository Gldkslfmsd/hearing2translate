{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di DeepLean, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "L'amplificazione testuale è un processo di adattamento di un testo volto a migliorare la comprensibilità per un gruppo di destinazione specifico, come persone con difficoltà di lettura o non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di amplificazione del testo, sono necessarie coppie parallele di testi, ad esempio, di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "E l'esempio qui presente consente di osservare una coppia di frasi allineate parallelamente, composta da una complessa frase tedesca e dalla sua traduzione in un linguaggio chiaro e accessibile."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può notare nell'esempio, quali la sostituzione lessicale, l'eliminazione di clausole, il riordinamento di clausole o l'inserimento di elenchi puntati."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ora proponiamo il nostro nuovo corpus dplane. Poiché negli ultimi anni si sono verificati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti automaticamente allineati, il che significa che possono presentare errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo Corpus dPlane, suddiviso in due sottocorpora, dPlane APA e dPlane web. DPlane APA si basa su testi di cronaca."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In DPlane APA, abbiamo allineato manualmente 483 documenti. Ne risultano circa 30.000 coppie di frasi parallele, pari a 13.000."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per dplane web, questo corpus include domini differenti, e allineiamo inoltre tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato le nostre coppie di frasi in modo più approfondito, ad esempio per quanto riguarda il tipo di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come si può notare qui, i testi biblici sono notevolmente più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per l'apprendimento della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Su tutti i livelli, a prescindere, ad esempio, dalla semplificazione lessicale, dalla semplificazione strutturale, e in generale su tutti i livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus DPlane presenta un'elevata varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus dell'API DPlane, abbiamo un numero significativamente maggiore di riordinamenti e aggiunte lessicali rispetto a quanto riscontrato nel corpus web di DPlane."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altro canto, nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Salve, sono Omar, e ora parlerò dei casi d'uso per il nostro dataset dplane. Quindi, per il primo caso d'uso, possiamo valutare metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati numerosi metodi di allineamento, ma nel contesto della traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e desideriamo estrarre allineamenti di frasi nei documenti post-elaborazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso d'uso, stiamo cercando di estrarre corrispondenze tra frasi di due documenti paralleli, nella stessa lingua, con lo stesso contenuto, ma che presentano un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "Ed ora, avendo a disposizione il dataset dplane che contiene frasi allineate manualmente, possiamo utilizzare questi allineamenti come standard di riferimento (gold standard) per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nell'articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, siamo giunti alla conclusione che il metodo di allineamento automatico più efficace da utilizzare per la semplificazione del testo tedesco è il metodo di mass alignment."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "Potete inoltre trovare il codice per eseguire questo metodo sui vostri documenti nell'articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo riguarda la semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "affinando i modelli linguistici per produrre un testo semplificato a partire dal complesso testo di input."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo messo a punto due modelli differenti. Abbiamo messo a punto il modello di lunga importanza per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo inoltre ottimizzato l'importazione della base normale per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Potete inoltre trovare tutti i checkpoint e approfondire i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questa semplice messa a punto poteva produrre o ottenere punteggi superiori a quelli di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo tali risultati come parametro di riferimento, un parametro di riferimento di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo molto per la Sua attenzione e speriamo di poter incontrare tutti voi durante il convegno. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skurkovsky e questa presentazione riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci basati su corpora. Ad esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "È tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi, in questo caso, Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Si assume un approccio simile nella teoria del testo del significato di Igor Milchuk, dove anche in questo caso l'intera struttura coordinata è guidata dal primo congiunto. Pertanto, questi due approcci sono asimmetrici, giusto? Ne mettono in risalto uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Oggi esistono anche approcci simmetrici alle strutture coordinate, come l'approccio PRUG, e l'approccio con congiunzione come capo assunto nei corpora di dipendenze PRUG, dove le strutture coordinate sono governate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze da un estremo a tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "E infine, esiste anche un approccio multisfaccettato che viene utilizzato, ad esempio, nella grammatica lessicale di Dick Cutzman."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, qui “laughs” (ride), verso tutti i congiunti separatamente. Questi sono Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, l'obiettivo di questo articolo è presentare una nuova argomentazione a favore delle strutture simmetriche di coordinazione come quelle due e contro le strutture asimmetriche di coordinazione come quelle due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio di minimizzazione della lunghezza delle dipendenze, che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in inglese, come potreste sapere, i nostri oggetti diretti preferiscono essere vicini al verbo, mentre i complementi avverbiali possono essere più distanti, giusto? Quindi, \"march read it yesterday\" è corretto perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March lesse ieri, la situazione è molto peggiore, giusto?\nPerché qui, tra il verbo e il complemento oggetto, si interpone un avverbio di tempo, ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere attenuato quando l’oggetto diretto è particolarmente pesante e lungo, poiché in tal caso può essere spostato in posizione successiva al margine."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi, entrambe queste frasi sono corrette. Marco ha letto ieri questo libro assolutamente affascinante sul BC, “Io” è accettabile, dove invece di esso abbiamo questa lunga NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire: Marge ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale che gli oggetti diretti debbano seguire immediatamente il verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano quindi solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qui abbiamo una dipendenza da “read” verso l'aggiunto di lunghezza 7 misurata in parole e da “read” verso “book” di lunghezza 4. Quindi, complessivamente, si tratta di 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando ci si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi, invece di undici, sei, decisamente più breve. Ecco perché suona piuttosto bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Bene, quindi quello che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione migliorata del Pentry Bank e consultare il paper per capire perché non abbiamo utilizzato le dipendenze universali."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione, più volte formulata in precedenza, che i congiunti sinistri tendono ad essere più brevi, ovvero “sale e pepe” e non “pepe e sale” misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l’osservazione, fatta di passaggio, che questa tendenza si accentua con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo più forte, giusto? Quindi la proporzione è maggiore del congiunto corto di sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò di nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando i governatori a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, il governatore si trova a sinistra. Ho visto Bart e Lisa, quindi è il governatore, è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, Omero è venuto e ha starnutito. Qui abbiamo coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi, in tali casi, il congiunto sinistro preferisce essere più breve. Tanto più, maggiore è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance a destra, come in questo caso, lascia che governi la coordinazione Telenet, questo effetto svanisce."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi dimostrato che misurando la lunghezza in caratteri, ovvero la prima colonna in sillabe, la colonna centrale e in parole, la colonna a destra. Pertanto, mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che osserviamo qui è che quando il governatore si trova a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza per il congiunto sinistro a essere più breve aumenta costantemente con la differenza assoluta di parole, e lo stesso si osserva in assenza di un governatore, come nella coordinazione di frasi, ma quando il governatore si trova a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E dimostriamo nel presente articolo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche, come queste due, e a favore delle strutture simmetriche, come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Si veda l'articolo per l'accordo e gli argomenti completi, scusate, e parlatene con noi nel post-sessione. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Xiang Bin, dottorando all'Università di Washington. Oggi presento il nostro lavoro, che parte dai dati di pre-addestramento ai modelli linguistici fino alle applicazioni a valle, tracciando le tracce di pregiudizi politici che conducono a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i modelli linguistici vengono addestrati su dati di crawling del web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di informazione politica sono adeguatamente rappresentati nei dati di pre-addestramento. Secondo un'indagine sul Corpus C4, si può osservare che New York Times, Los Angeles Times, The Guardian, Huffington Post, e altri, sono ben rappresentati nei dati di addestramento dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha generato un beneficio ambiguo per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, hanno potuto apprendere da prospettive diverse, il che celebra la democrazia e la pluralità di idee.\nDall'altro, queste differenti opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di indagare il processo di propagazione del bias politico, a partire dai dati di pre-addestramento fino ai modelli linguistici e alle attività successive, ponendoci specificamente le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo la direzione politica dei modelli linguistici e quale ruolo potrebbe avere il dataset di pre-addestramento in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse unità politiche in compiti a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, specificamente, proponiamo innanzitutto di sollecitare i modelli linguistici con diversi formati di prompt, utilizzando questionari politici, come il test della bussola politica. Questo ci permette di effettuare una valutazione automatica ben fondata sulla letteratura scientifica di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli linguistici di prima lingua presentano infatti significati politici diversi. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che GPT 4 è il modello linguistico più progressista di tutti, e la serie GPT è generalmente più socialmente progressista rispetto alla serie BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, ci proponiamo di indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente acquisiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Potremmo quindi condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint di modelli linguistici su sei diverse entità aziendali, separate in notizie e social media, ulteriormente suddivise secondo i loro significati politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Pre-addestrando ulteriormente i modelli linguistici su tali parti di corpora, si può osservare che le coordinate ideologiche del modello linguistico si spostano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Per Roberta, ulteriormente ottimizzata e addestrata su un corpus di Reddit di orientamento progressista, possiamo osservare un significativo spostamento verso posizioni liberali in termini di..."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini dei suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici possano cogliere la polarizzazione diffusa nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, suddividiamo i corpora di pre-addestramento in corpora precedenti al 45° Presidente degli Stati Uniti e corpora successivi al 45° Presidente degli Stati Uniti, pre-addestrando separatamente modelli linguistici su questi due corpora temporali distinti."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che i modelli linguistici hanno mostrato una tendenza politica più distante dal centro a partire dal 2017. Ciò indica che i modelli linguistici sono in grado di rilevare anche la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in ultima analisi, valutiamo i modelli linguistici con differenti connotazioni politiche sulla rilevazione di discorsi d'odio e sulla rilevazione di notizie false, in applicazioni di PNL che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che, se esaminiamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in..."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "considerando diverse demografie o il significato politico dei media di informazione, si può osservare un andamento per cui, ad esempio, nell'ambito del rilevamento di discorsi d'odio, i modelli linguistici di orientamento politico di sinistra ottengono risultati migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nella rilevazione di discorsi d'odio rivolti a gruppi sociali minoritari."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, le nostre ricerche si concentrano sull'individuazione di discorsi d'odio rivolti a gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E viceversa, i modelli linguistici con orientamento a destra sono più abili nel rilevare discorsi d'odio diretti a bianchi e uomini, tuttavia, meno efficaci nel rilevare discorsi d'odio diretti a comunità nere, LGBTQ+ e ad altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Tendenze analoghe si riscontrano anche nella rilevazione di notizie false, dove si osserva che i modelli linguistici orientati verso una specifica linea politica sono più efficaci nel riconoscere la disinformazione proveniente dalla prospettiva opposta, e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "Questo illustrerà ulteriormente numerosi esempi qualitativi per dimostrare che i modelli linguistici presentano diverse connotazioni politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "producono previsioni differenti per esempi di discorsi d'odio e disinformazione, basate sulle loro categorie sociali. Nell'appendice sono presenti numerosi altri esempi per evidenziare ulteriormente tale aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che sussiste una questione di equità estremamente urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello linguistico lineare venisse sottoposto a fine-tuning su discorsi d'odio o disinformazione o su qualsiasi altro contenuto simile, e fosse implementato su una popolare piattaforma di social media."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti di gruppi minoritari potrebbe diffondersi incontrastato, senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ha suonato l'allarme per indurci a riconoscere e affrontare i problemi di equità derivanti dalle posizioni politiche divergenti dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Allora, un breve approfondimento. Vorremmo inoltre evidenziare la peculiare dilemma che riguarda i pregiudizi politici nei modelli linguistici. È come il caso tra Sila e Kryptidis."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il bias si propagherà dai dati di pre-addestramento ai modelli linguistici e alle attività successive, creando, in ultima analisi, problematiche di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo a sanificare in qualche modo, rischieremmo anche censura o esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere preservato nei dati di addestramento dei modelli linguistici. È un po' come il problema di Electric Charlie."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ottimo. Penso che per oggi sia sostanzialmente tutto. Grazie per la Sua attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, studentessa dottoranda del primo anno alla Carnegie Mellon University, e oggi vi presenterò il vostro lavoro, Enol Positionale, Caratterizzazione dei Bias di Progetto nei Set Beta di Modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Cominciamo dunque immaginando che stiate lavorando per un giornale e che stiate esaminando i commenti sotto il vostro articolo di cronaca per rimuovere contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti orientarti verso un'API popolare come Perspective API per la rilevazione della tossicità. E questo funziona molto bene se sei Carl Jones, nel caso in cui Perspective API sia in grado di rilevare correttamente istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è del tutto vero per Dithyasharma, dove l'API di prospettiva non è particolarmente sensibile a termini offensivi più comuni in contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di bias progettuale in cui si osservano differenze sistematiche nelle prestazioni della tecnologia tra diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Pregiudizi di progettazione come quello che abbiamo appena osservato possono derivare dalla posizione epistemologica dei ricercatori di NLP e degli sviluppatori di modelli. La posizione epistemologica si riferisce semplicemente alle prospettive che le persone possiedono in virtù della loro demografia, identità e esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli ambiti accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E, in quanto ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, una domanda che le persone potrebbero porsi è se i set di dati e i modelli abbiano posizionalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo dicendo che i modelli stessi e i dataset stessi abbiano identità demografiche ed esperienze di vita, ma essi aggregazione giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, studi precedenti hanno suggerito alcune evidenze aneddotiche dell'esistenza di una posizione, come disparità culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste opere non prendono realmente in considerazione il confronto tra gli utenti finali e i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità di modelli e dataset sta diventando sempre più importante man mano che le attività di NLP diventano più soggettive e orientate al sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "Ed è difficile caratterizzare come queste posizioni siano distorte, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per studiare la posizionalità del dataset e del modello, confrontiamo effettivamente le annotazioni con utenti reali con dataset e modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo realizziamo attraverso il nostro framework, NL Positionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework opera in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo consiste nel ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E scegliamo di procedere in questo modo anziché analizzare la demografia dei dataset originali, o quella degli annotatori, poiché di solito solo pochi annotatori etichettano ogni istanza e perché i dati demografici sono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, scegliamo di riannotare i dati per ottenere numerose annotazioni per istanza e un insieme ricco di informazioni demografiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, analizziamo le annotazioni in base ai dati demografici e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Parsons R."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così, il nostro framework differisce in realtà dalla letteratura sull'incongruenza tra annotatori confrontando gli utenti finali con modelli e dataset, predizioni ed etichette, anziché concentrarsi unicamente sull'accordo tra annotatori o sulla modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è reso possibile in larga misura da Lab in the Wild, una piattaforma di crowdsourcing online per i nostri collaboratori nel campo HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari più diversificati rispetto a piattaforme come MTurk, che hanno prevalentemente partecipanti provenienti dagli Stati Uniti o dall'India. E inoltre, Lab in the Wild è comunque in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Organizziamo due compiti su Lab in the Wild, uno dei quali riguarda l'accettabilità sociale. E il funzionamento è il seguente: i partecipanti leggeranno una situazione tratta dal dataset di social chemistry e poi scriveranno quanto quella situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per mantenere alto l'interesse nello studio, possono confrontare le proprie risposte con quelle di un'IA e di altri studenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con la social chemistry, Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi replicato un setup molto simile per il compito di rilevamento della tossicità e del linguaggio d'odio, in cui dovranno leggere un esempio da Dana Hate e scrivere se ritengono che si tratti di un'istanza di linguaggio d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, abbiamo confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, HateRoberta e GPT four. Il nostro studio ha infine raccolto oltre sedici mila annotazioni da più di mille annotatori provenienti da ottantasette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora siamo meglio attrezzati per rispondere alla domanda: con quali dati set e modelli di NLP si allineano maggiormente? Scopriamo che esiste una posizione specifica nel campo dell'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo rilevato che dataset e modelli sono maggiormente allineati ai paesi di lingua inglese. Pertanto, per l'analisi di accettabilità sociale del GPD 4, abbiamo constatato che questo è più allineato ai paesi di lingua confuciana e anglosassone. Abbiamo inoltre riscontrato che Dynamite Hate è anch'esso maggiormente allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo inoltre una maggiore corrispondenza con persone che hanno una laurea. Pertanto, per GPT 4 nel compito di accettabilità sociale, riscontriamo che si allinea maggiormente con persone con istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo lo stesso per Dani Hates, dove si riscontra la maggiore affinità con persone aventi un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando modelli e dataset sono allineati a specifiche popolazioni, alcune vengono inevitabilmente escluse."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschili e femminili. Lo riscontriamo sia nel compito di accettabilità sociale di GPT 4 che nell'analisi del compito Dynahate."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Dunque, dato che esiste la posizione analitica LP, cosa possiamo fare a riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Dunque, abbiamo alcune raccomandazioni a riguardo. La prima è mantenere un registro di tutte le scelte progettuali rilevanti durante il processo di ricerca. E l'altra è condurre ricerche in NLP con l'ottica del prospettivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare dataset e modelli specializzati all’interno di quattro comunità specifiche. Un buon esempio di ciò è l'iniziativa Masakane. Vogliamo sottolineare che l'NLP inclusivo non consiste semplicemente nel far funzionare tutte le tecnologie per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E questo conclude la nostra presentazione, ma se desiderate approfondire, sentitevi liberi di consultare il nostro pannello di controllo per i risultati dell'analisi più recenti e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Xi Yuan dell'Università di Fenai. Sono qui per presentare il nostro lavoro: estrazione di conoscenza di script distinti dai modelli linguistici lineari per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno esplorato modelli linguistici per la pianificazione di obiettivi astratti relativi ad attività stereotipate, come preparare una torta, dimostrando che i modelli linguistici di grandi dimensioni possono efficacemente scomporre gli obiettivi in fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per obiettivi specifici, con vincoli specifici, come preparare una torta al cioccolato, rimane ancora scarsamente studiata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Che impongono vincoli differenti agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi reali e specifici, con molteplici vincoli. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Dato che non esiste un set di dati di obiettivi specifici da cui partire."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo acquisire questo obiettivo innanzitutto. Come illustrato nella tabella, estendiamo gli obiettivi astratti con vincoli modificati per l'acquisizione di dati del ciclo umano, utilizzando TPT strutturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Proviamo 100 obiettivi specifici e valutiamo gli script generati dai modelli di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Riscontriamo che tutti i modelli lineari ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, conduciamo un'analisi dettagliata per investigare le finalità dei moduli di apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati presentati nella figura dimostrano che la completezza semantica degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Esaminiamo più approfonditamente categorie argomentative graduate e più dirette di vincoli, a seconda del contesto domestico. La mappa principale illustrata nella figura mostra che le prestazioni di pianificazione dei DPD istruzionali variano considerevolmente per ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell’output di modelli leggeri presenta un'elevata varianza, con conseguenti prestazioni scadenti. Pertanto, adottiamo l'idea di un filtro zen sovra-generato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, presentiamo i tipi di vincoli con esempi per istruire CPT e ricaviamo obiettivi specifici basati su detti obiettivi astratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, istruire il GPT a generare scenari di casi specifici per obiettivi definiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script irregolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e obiettivi in istruzioni per GPT, suddividendoli in unità più piccole e calcolando la similarità del coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, scriveremo lo script che contiene le parole chiave del vincolo target. Manteniamo lo script solo se il punteggio del target su siti obiettivo è il più alto."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, un’insufficienza può generare ciocche di capelli di qualità. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Dato che i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale dotare modelli più piccoli e specializzati di capacità di pianificazione linguistica. La creazione di un set di dati rappresenta una fase essenziale per raggiungere questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dati risulta onerosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare siti di dati di pianificazione linguistica vincolata da modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per la creazione di un dataset di pianificazione del linguaggio congiunto denominato script di codice."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo cinquantacinquemila obiettivi specifici corredati da script per garantire la qualità dei siti di validazione e di test. Richiediamo ai collaboratori di cloud sourcing di individuare e correggere i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "La figura mostra la distribuzione dei vincoli del codice script. Rileviamo che il codice script manifesta iperploidia negli obiettivi specifici generati. Con il codice script, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio dei vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con Antsights, TFILF e la taratura sulla frequenza del cursore, è possibile generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che modelli più piccoli possono supportare modelli più grandi qualora vengano opportunamente addestrati su dataset adeguati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo definito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppato un metodo di filtro sovra-generato per i modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un dataset di script di alta qualità per la pianificazione linguistica vincolata. Speriamo che questo dataset di codice possa rappresentare una risorsa preziosa per promuovere la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio per la sua attenzione.\nPuò trovare maggiori dettagli del codice script nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Shu Heng. Oggi presenterò il nostro articolo intitolato \"Gli etichettatori di entità nominate di Kernel 2003 funzionano ancora bene nel 2023?\". Cominciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito del riconoscimento di entità nominate, o NER task."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano Kono two thousand three per sviluppare NER da quasi vent'anni. E questo solleva naturalmente diversi problemi. Innanzi tutto, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, qualora si osservi una scarsa capacità di generalizzazione, quali sono le cause del calo di performance di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il dataset Kono plus plus. Questo è un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e che abbiamo poi annotato seguendo le stesse linee guida di annotazione di Kono 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, abbiamo ottimizzato più di venti modelli su Kono two thousand three. Li abbiamo valutati sia sul set di test Kono three che sul set di test Kono plus."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F per valutare la capacità di generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti abbiamo constatato che sono necessari tre elementi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo aspetto riguarda l'architettura del modello. Attraverso i nostri esperimenti, abbiamo riscontrato che i modelli transformer tendono a generalizzare meglio a nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che, in genere, modelli più grandi conducono a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine tuning influisce direttamente sulle prestazioni di un task a valle. Abbiamo anche riscontrato che un numero maggiore di esempi di fine tuning porta effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo di performance di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Avevamo due ipotesi. La prima è l'eccessivo adattamento, ovvero un overfitting causato dal riutilizzo dello stesso set di test ripetutamente, che si manifesta di solito come un rendimento decrescente su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è il drift temporale, ovvero il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di training e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la retta di migliore adattamento rossa presenta un gradiente superiore a uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo ottenuto con Carl nel duemilatrece si traduce in più di un'unità di miglioramento su Carl plus plus, il che implica l'assenza di rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci mostra che l'overfitting adattivo, in questo caso, non si verifica."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E allora, che dire del drift temporaneo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto concerne la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare il pre-addestramento di alcuni modelli con dati più recenti, e abbiamo riscontrato che le prestazioni diminuiscono all'aumentare degli intervalli temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi che la causa principale del calo di performance sia lo scostamento temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di dimensioni maggiori del modello, nonché di un numero maggiore di esempi di fine-tuning, e questi elementi vanno di pari passo. Non possiamo avere un solo ingrediente, trascurando gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo anche riscontrato che il calo di performance qui è causato da deriva temporale, e, in modo piuttosto sorprendente, non è dovuto a sovradattamento adattivo, nonostante l'utilizzo di Kono duemila tre per oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tagger Kono 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "E, per concludere, vi invitiamo a consultare il nostro articolo, il nostro dataset e, qualora abbiate domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, discuterò del nostro lavoro sulla risoluzione di espressioni referenziali indirette per la selezione di entità, nel quale abbiamo introdotto l'altentity corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Javod Hosseini e questo è un lavoro congiunto con Philip Radinsky, Silvia Paretti e Annie Luis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. Consideriamo la seguente domanda alternativa. Intendevi \"easy on me\" o \"I got a feeling\"? Qui, l'utente vuole selezionare tra uno di questi due termini."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più evidente è utilizzare un riferimento diretto, ad esempio citando il titolo della canzone Easy on Me o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della fonte."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "Tutte le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l'utente desidera specificare una preferenza. Ecco alcuni esempi di riferimenti diretti, ad esempio il più recente o il brano che non è energico."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo rappresenta un problema importante nei sistemi di dialogo, nonché per il benchmarking della comprensione delle entità da parte dei modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un dataset pubblico, di un dataset pubblico su larga scala per questo compito, pertanto ne creiamo uno utilizzando l'annotazione di crowdsourcing. Il nostro dataset copre tre domini differenti: musica, libri e ricerca."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dati enfatizza l'informalità attraverso l'utilizzo di un set di completamento a fumetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il cartone animato presenta tre fumetti. Nel primo fumetto, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con ciò, Bob definisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo fumetto, Alice dice: \"Intendi facile per me o sono stata io a farla franca?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nel terzo fumetto, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio, la nuova RF."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda nuvoletta di dialogo, ma la terza viene compilata dall'annotatore. La prima nuvoletta di dialogo è scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo, che è la domanda alternativa, viene generato come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un modello semplice. Intendi A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Salendo nella lista, le entità tendono a diventare più simili tra loro e di solito è più difficile effettuare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'attrazione uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità presentano titoli simili, ad esempio, due libri intitolati \"retail\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando presentano descrizioni simili su Wikipedia. E infine, quando condividono elementi informativi o attributi simili su Wikipedia. Ad esempio, lo stesso genere musicale o la stessa voce dell'artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando presentiamo questa domanda alternativa agli annotatori, questi conoscono il nome di tali entità, ma non necessariamente conoscono l'entità stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ciò che facciamo è presentare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un collegamento alla ricerca di Google per ciascuna di esse."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedere agli annotatori di ascoltare almeno alcune parti di ogni brano e di leggere informazioni su ciascun brano. Ecco, ad esempio, il risultato della ricerca Google per il brano Easy."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio di ricette e libri, mostriamo un testo di contesto proveniente da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, affinché i valutatori siano a conoscenza del loro aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, chiediamo agli annotatori di selezionare una di queste entità, ad esempio qui la prima, e descriverla utilizzando da tre a cinque espressioni referenziali indirette."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi del nostro set di dati. Ad esempio, quello senza parole, non quello con il dodicenne, né quello fittizio, né quello proveniente dall’Azerbaigian."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus delle entità comprende 6.000 domande alternative in tre ambiti, e include 42.000 espressioni referenziali indirette. I risultati ottenuti con il modello T5xLarge sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico avesse accesso allo stesso identico corpus di conoscenze degli annotatori, l'accuratezza sarebbe molto elevata. Si attesterebbe in un intervallo tra il 92 e il 95 percento. Ma questa situazione non è realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, allora l'accuratezza è compresa tra l'82 e l'87 percento, il che è più realistico, ad esempio quando il modello linguistico recupera tali conoscenze pregresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso unicamente ai nomi di entità, l'accuratezza si attesta al 60%. Pertanto, vi è ampio margine di miglioramento. Abbiamo inoltre dimostrato che i modelli sono generalizzabili a diversi domini. Ecco un link al nostro dataset. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Sarah Pappy, dell’Università di Trento e della Fondazione Bruno Kessler, e introdurrò brevemente l'articolo “L'attenzione come guida per la traduzione automatica simultanea”, un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simul SD, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SimulST attuali? Architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di formazione lunghe e complesse, ad esempio quelle che coinvolgono obiettivi di ottimizzazione differenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E l'addestramento e la manutenzione di diversi modelli per raggiungere differenti regimi di latenza, ad esempio l'addestramento di un modello con una latenza media di un secondo e un altro con due secondi, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Allora, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Essere i primi a utilizzare modelli SD offline già esistenti senza riaddestramento o adozione di architetture specifiche per CLSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il cross attention mechanism. E potete vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione consiste nel proporre un meccanismo di attenzione dot o encoder-decoder, e si tratta di una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione converge."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa se la tensione non è concentrata, cioè, la sua somma è inferiore a una certa soglia alfa, negli ultimi lambda frame discorsivi, il che significa che l'informazione ricevuta è sufficientemente stabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Se riceviamo un frammento di discorso contenente \"I'm going to talk about\" e il nostro modello prevede la traduzione in tedesco, noi"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E analizzeremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole rimandano ai primi frame di discorso ricevuti, mentre l'ultima parola rimanda agli ultimi frame di discorso ricevuti, gli ultimi frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole saranno omesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Mentre la somma della tensione incrociata supera una certa soglia alpha, non emetteremo l'ultima parola e attenderemo un altro segmento discorsivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro discorso affondato e il nostro modello prevede altre tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola punta agli ultimi frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se osserviamo il risultato principale di ciò, possiamo constatare che"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Tracceremo i risultati della traduzione simultanea del parlato su grafici in cui un lato sarà contrassegnato di blu e misurerà la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Quella è la misura della latenza. E consideriamo anche il cosiddetto \"lagging\" medio consapevole della computazione, che tiene conto del tempo di calcolo del modello per predire l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo quindi che le nostre curve siano il più possibile alte su questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma desideriamo anche che vengano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con le strategie PROPERA che si applicano anche a modelli offline, quali la strategia WitKey e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la pre-traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che ADUT supera tutte le strategie applicate ai modelli offline, dato che le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, considerando il tempo effettivo trascorso o il tempo consapevole dal punto di vista computazionale, quella è la strategia più rapida."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, consultate il nostro articolo. Abbiamo inoltre reso pubblico il codice e i modelli, e l'output simultaneo, per agevolare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ying e, insieme al mio collega Jiang, presenteremo la nostra ricerca sul miglioramento dell'apprendimento seriale multimodale tramite l'ottimizzazione basata su istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli linguistici di grandi dimensioni, numerosi studi hanno iniziato a esplorare nuovi paradigmi di apprendimento che consentono di riutilizzare modelli linguistici pre-addestrati per diverse attività successive in modo efficiente sia in termini di parametri che di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che l’istruzione di ottimizzazione permette ai modelli linguistici di grandi dimensioni di eseguire compiti inediti in modalità zero-shot, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si è concentrata sul miglioramento delle prestazioni a colonna singola (serial shot) su compiti di sola lingua, mentre compiti di computer vision e multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, intendiamo indagare se l'istruzione di ottimizzazione su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali inediti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una notevole discrepanza nella disponibilità del dataset di istruzioni tra RLP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di mille seicento compiti di istruzione linguistica esclusiva. Tuttavia, non esiste un compito di istruzione multimodale su larga scala pubblicamente disponibile. Questo motiva la costruzione di un dataset di ottimizzazione dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi Instruct, il primo set di dati di benchmark per l'affinamento di istruzioni multimodali che comprende 62 diverse attività multimodali, copergenti 10 ampie categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da ventuno dataset open source esistenti, e ciascun compito è corredato da cinque istruzioni redatte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per l'indagine sull'affinamento multimodale dell'istruzione sul nostro dataset proposto, adottiamo OFA, un modello di pattern multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token immagine e le coordinate di un riquadro di delimitazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo alcuni esempi tratti dal nostro dataset multi-instratificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per uniformare l'elaborazione di diversi tipi di dati in ingresso e in uscita."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza a sequenza unificato in cui il testo di input, le immagini, le istruzioni e i riquadri di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ora parlerò di messa a punto multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, per l'insieme di dati di addestramento, utilizziamo 53 compiti del gruppo NIG per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo Common Sense Reason per la valutazione e selezioniamo ulteriori cinque compiti dai gruppi WQA e Miscellaneous."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nel set di test per ciascun compito. Inoltre, campioniamo casualmente venti compiti dal set di test di istruzioni naturali, come in Syntax per NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo un modello OFA di grandi dimensioni pre-trendato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, durante i test per ciascun compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e il valore massimo delle prestazioni, nonché la deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se si tratta di un compito di generazione multimodale, riportiamo RougeL. Per un compito RP, riportiamo RougeL anch'esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto anche una metrica di valutazione aggiuntiva, denominata sensibilità. Questa misura la capacità del modello di produrre risultati consistenti per lo stesso compito, a prescindere da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale. Come possiamo vedere, l'affinamento tramite istruzioni può migliorare significativamente le prestazioni di OFE in compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il transfer learning da set di dati di istruzioni naturali può giovare all'instruction tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, all'aumentare del carico di lavoro, il modello abbia raggiunto prestazioni migliori e, nel contempo, una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, utilizzando un'istruzione rispetto a cinque istruzioni. Come si può notare, l'impiego di un numero maggiore di istruzioni può migliorare le prestazioni complessive del modello e ridurre significativamente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo constatare attraverso il transfer learning da set di dati di istruzioni naturali, il modello può raggiungere una sensibilità significativamente superiore rispetto al modello IFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo inoltre constatare che il transfer learning a partire da un set di dati di istruzioni naturali può contribuire a migliorare sensibilmente le prestazioni di OFA sul set di dati di istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In definitiva, abbiamo proposto il primo dataset di ottimizzazione dell'istruzione multimodale su larga scala, che migliora significativamente le capacità derivazionali di OFA, e abbiamo esplorato diverse tecniche di transfer learning, dimostrando i loro vantaggi con la progettazione di una nuova metrica denominata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, un'ultima cosa: stiamo raccogliendo un dataset molto più ampio per il tuning multimodale, con circa 150 ulteriori attività linguistiche Variant, e lo rilasceremo. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Coast of Sena e sono lieto di darvi il benvenuto alla presentazione del nostro articolo per l'ACL 2023, \"Language Model Acceptability Judgments are not always robust to context\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Bokier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina William."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, riprendiamo il paradigma delle coppie minime."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il paradigma minimale accoppiato valuta essenzialmente i modelli linguistici sulla base di giudizi di accettabilità, che possono includere anche la grammaticalità, come in esempi del tipo \"blimp\", \"syntax gem\", o l'accettabilità in termini di stereotipi, come le coppie di Krauss."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici è presentare una frase accettabile o grammaticalmente corretta e, successivamente, una frase inaccettabile o agrammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E allora, si spera che il modello assegni, fondamentalmente, una maggiore probabilità alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP essenzialmente non ci consente di valutare l'accettazione del modello nei confronti di frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno sviluppando finestre di contesto sempre più ampie. Pertanto, è fondamentale che valutiamo l'accettabilità del modello lungo l'intera finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "Ed è questo ciò che stiamo cercando di fare qui. Stiamo cercando di riesaminare la pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, questo è l'approccio. Ciò che facciamo, quindi, per simulare queste sequenze più lunghe, è rivisitare i dataset stessi e poi ricreare frasi scegliendo, ad esempio, frasi accettabili o inaccettabili da quei dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto una tipica coppia di grammaticality estratta dal blimp data set, relativa al caso dell'isola degli avverbi."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe che siano accettabili e che presentino lo stesso allineamento della struttura grammaticale: estraiamo frasi grammaticali da adjunctile."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo fare lo stesso scegliendo frasi inaccettabili provenienti dallo stesso insieme di corrispondenze, il che potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso selezionando frasi da un sottoinsieme diverso o da un dataset differente. Questo è ciò che definiamo scenario di disallineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo caso le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset che state utilizzando per la valutazione. E possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "come se il contesto provenisse da un sottoinsieme diverso del dataset, o se fosse del tutto irrilevante rispetto alla frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, come si comporta il modello? Iniziamo, dunque, esaminando le frasi di Wikipedia completamente irrilevanti per la coppia di query corrente, e lì riscontriamo che i giudizi MPP sono per lo più robusti per una lunghezza del contesto arbitraria."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino a 2024 per sfruttare al massimo i modelli OPT e GPT due, e possiamo osservare qui, nella linea tratteggiata arancione, che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "E ora, cosa accade quando scegliamo frasi provenienti dallo stesso dataset?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Ed ecco che siamo qui, scegliendo o creando frasi da domini accettabili e inaccettabili, provenienti dallo stesso dataset BLIMP o SYNTAX GIMP."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E in tal modo osserviamo che i giudizi MPP aumentano o diminuiscono in modo significativo quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando facciamo corrispondere la struttura, cioè quando selezioniamo le frasi provenienti dallo stesso fenomeno in termini di sintassi, Jim."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "osserviamo un notevole aumento o una notevole diminuzione del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora, questo e questo è molto ampio, come questo effetto aumenta con la lunghezza del contesto e questo probabilmente influirà sui modelli linguistici più recenti che dispongono di finestre di contesto più estese."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché il prefisso della corrispondenza influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input mantenendo la struttura rilevante e aggiungendo, in sostanza, del rumore all'input. E dopo aver effettuato diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "constatiamo che nessuno di questi rumori sta effettivamente modificando il comportamento del modello riguardo alla visualizzazione della tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "Fondamentalmente, osserviamo che i modelli sono sensibili alle frasi perturbate in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "È quando perturbiamo le frasi all'interno del dominio accettabile che osserviamo un aumento simile in tutte le perturbazioni, e quando perturbiamo le frasi al di fuori del dominio accettabile, osserviamo una diminuzione dei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i principali risultati del nostro lavoro indicano che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che vengono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, così come la conduciamo attualmente con input brevi e in singole frasi, potrebbe non cogliere appieno la conoscenza astratta del modello linguistico all'interno della finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Vi preghiamo di leggere il nostro articolo per maggiori dettagli sugli esperimenti condotti.\n\nGrazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Yusin Zhang e sono della Penn State University. Oggi presenterò il nostro lavoro, l’analisi semantica crosslinguale in molteplici lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il semantic parsing è un compito che consiste nella costruzione di rappresentazioni semantiche di query utente, come SQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E l'analisi semantica interlinguistica è il compito di tradurre query in diverse lingue naturali in diverse rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come illustrato in questa figura, è necessario tradurre la query in molteplici lingue naturali utilizzando modelli neurali in SQL, Lambda, FunQL, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di semantic parsing crosslinguale esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati, ad esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Esistono numerose analisi su determinate lingue naturali.\nIl cinese è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Copertura dei laghi su determinate mini-rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "Oppure vengono valutati solo su alcuni modelli più recenti. Ad esempio, esiste un solo modello per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo esemplificativamente, forniamo un dataset uniforme per il parsing semantico interlinguistico in molteplici lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove dataset in diversi ambiti, cinque parti semantiche e tassonomie, otto rappresentazioni del significato e 22 lingue naturali appartenenti a 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è TranslateTest. Utilizziamo l'API di Google Translate per tradurre il testo sorgente nella lingua di destinazione, per poi impiegare il modello MonolingoModel per addestrare una valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "E per esempio, addestriamo un modello inglese su query in lingua inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando un'API in inglese, per poi utilizzare il modello addestrato per predire l'SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza coincide con la lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche l'impostazione di fusione monolingue, addestrando modelli monolingui con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E testiamo un modello multilingue, un unico modello multilingue che addestriamo per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo unito le query tedesche, inglesi e cinesi per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre query tedesche o query cinesi o simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo inoltre il crosslingo zero-shot e field-shot transfer, che si basa su una lingua di origine e viene trasferito a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante l'addestramento, lo allenerò su query in lingua inglese o su una combinazione di query di fusione inglese e tedesca, per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingui, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "includendo encoder PDR, acronimo di multilingual pretrained encoders con decodificatori basati su puntatori, come XLMR plus PDR e BERT plus PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo inoltre modelli encoder-decoder, ovvero modelli encoder-decoder preaddestrati multilingui, quali MBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che l'architettura encoder-decoder offre le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su MT cinque e XLMR plus PDR in configurazioni multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che encoder-decoder o encoder PDR possono essere migliorati tramite l'addestramento in una miscela di diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che ciò è dovuto al fatto che la maggior parte delle lingue naturali principali può ottenere un miglioramento delle prestazioni, ad eccezione del caso dell'inglese, le cui prestazioni diminuiscono in sette set di dati e aumentano solo in tre."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come la maledizione del multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo inoltre il divario nelle prestazioni tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento di fuel shot crosslinguale, la linea arancione rappresenta il trasferimento zero shot crosslinguale, mentre la linea verde rappresenta l’impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando la linea verde e quella arancione, abbiamo notato che per l'impostazione a zero short setting, il divario nelle prestazioni del trasferimento cross-linguistico è significativo. E confrontando la linea blu e quella arancione, abbiamo riscontrato che per poche impostazioni short setting, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche altri risultati interessanti. Ad esempio, l'architettura encoder-decoder supera il lavoro precedente o ottiene risultati comparabili. L'utilizzo dell'inglese come lingua naturale può migliorare significativamente le prestazioni di Fuchshot sulle lingue naturali di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che modelli linguistici multilingue come Codice e Bloom sono ancora inadeguati per compiti di parsing semantico cross-linguale."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo sviluppato Exempler, un benchmark unificato per il parsing semantico multiangolare con diverse lingue naturali e rappresentazioni minimali."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo uno studio di benchmark completo su tre tipologie rappresentative di modelli linguistici multilingue, e i nostri risultati presentano numerose scoperte interessanti, eccetera.\nVi invitiamo a consultare il nostro articolo e il codice.\nGrazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Aid Vilar e fornirò una breve panoramica del paper \"Promoting PowerPoint Translation, Assessing Strategies and Performance\". Si tratta di un lavoro svolto in collaborazione con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Parm è un modello linguistico di apprendimento con 540 miliardi di parametri, presentato lo scorso anno, nel 2022. È stato addestrato su un'ampia collezione di tag composta da 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, raggiunse lo stato dell'arte in centinaia di compiti di PNL."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico del prompting dei Modelli Linguistici a Sincronizzazione (Latch Language Model) per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli adottando le migliori pratiche della comunità AMT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni nella valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche LMT all'avanguardia e innovative, e, in aggiunta, presentiamo anche i risultati della valutazione umana basata sull'esperienza di esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "L'incitazione (o il prompt) ha una notevole influenza sulle prestazioni degli LLM per la traduzione, come si può notare in un semplice esperimento in cui utilizziamo un'incitazione breve e forniamo due prompt differenti per una sola frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggior parte delle frasi, cinquecentosei su mille, la differenza osservata è di oltre un punto sfocato."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo, in casi estremi, può arrivare fino a 40 punti di sfocatura. Pertanto, è importante selezionare una strategia di prompting efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi in cui semplicemente etichettiamo ogni frase che forniamo al sistema con la lingua in cui è espressa."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di partenza, sono contrassegnate con due punti tedeschi e la traduzione inglese con due punti inglesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che la forma effettiva del prompt non ha una grande influenza nel caso di prompt brevi e numerosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting zero e one-shot, ma quando si passa, come nel nostro caso, al prompting five-shot, non vi è quasi alcuna differenza nella forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a sostenere principalmente il peso."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt di selezione tratti dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di profondità sono notevolmente più curati e di qualità superiore rispetto ai dati di addestramento, tanto che, oserei dire, i risultati mostrano prestazioni migliori quando si utilizzano i dati di profondità."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Ciononostante, i sistemi specializzati all'avanguardia presentano un vantaggio sostanziale rispetto alle traduzioni di PALM, ma quest'ultimo si avvicina notevolmente a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporre Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le informazioni che abbiamo acquisito dall'innovazione umana che abbiamo realizzato utilizzando il framework MQM indicano che la fluidità di PALM è paragonabile ai sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori di omissione sono i più comuni."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra quindi che Palm scelga talvolta di produrre una traduzione dal suono migliore omettendo parti della frase originale che vengono poi escluse nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria stilistica esterna per PAN è inferiore rispetto ai sistemi all'avanguardia, il che costituisce un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "Quella sezione produce risultati davvero fluidi, ma con alcuni problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo conclude brevemente questa panoramica. Per ulteriori dettagli, vi invitiamo a partecipare alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Dawe, dottorando presso l'Università di Zalant in Germania. In questo video vorrei presentare il nostro recente lavoro, *Weaker Than You Think*, un'analisi critica della fornitura settimanale di materiali didattici."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yushche, Marios Musbach, Gas Steffen e Dietrich Clarkov."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o approvvigionamento di codici di località, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni di qualità inferiore sono decisamente più economiche, ma presentano anche un livello di rumore maggiore, il che significa che una certa percentuale di esse risulta inaccurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente reti neurali su dati di etichettatura settimanali, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'apprendimento supervisionato settimanale, vengono proposti algoritmi di addestramento per formare in modo robusto reti neurali a fronte di un simile livello di rumore, in modo che i modelli addestrati continuino a generalizzare efficacemente."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori in WSL, ovvero WSL che sta per Weekly Supervised Learning, è una pretesa comune che le persone affermino di addestrare i modelli esclusivamente con i dati di etichettatura settimanali e di ottenere elevate prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è errata, ma c’è una riserva."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Qual è il presupposto che esista un set di validazione aggiuntivo e pulito, disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Mettiamo in dubbio questa impostazione del problema, poiché implica la necessità di ulteriori annotazioni manuali nell'apprendimento supervisionato settimanale; tuttavia, come un elefante nella stanza, questa necessità è spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il suddetto dubbio ci porta a porci tre domande di ricerca. Innanzitutto, è necessario un set di dati di validazione pulito per WSL? O possiamo forse utilizzare un set di validazione rumoroso al suo posto?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se è richiesto un dataset pulito o se un dataset pulito è indispensabile per il funzionamento di WSL, allora quanti campioni puliti ci servono? Infine, dovremmo utilizzare esclusivamente i campioni puliti per la validazione, oppure esistono modalità più efficaci per sfruttarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, scopriamo che, in modo interessante, i metodi WSL recenti richiedono effettivamente campioni bianchi e puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti si verifica un significativo calo delle prestazioni, come illustrato in questa figura. Se non sono disponibili campioni di validazione puliti, i modelli addestrati non possono generalizzare al di là delle etichette deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "significando che la formazione è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che gli approcci WSL richiedono effettivamente dati accuratamente etichettati per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non deve essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti contribuirà a migliorare le prestazioni degli approcci WSL, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "Di norma, abbiamo bisogno soltanto di venti campioni per classe per ottenere prestazioni elevate."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, poiché se decidiamo in ogni caso di accedere a campioni puliti, l’addestramento diretto su di essi otterrà prestazioni ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di performance tra gli approcci di fine tuning, applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo notare, se abbiamo dieci campioni per classe, il fine tuning diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato negli approcci WSL precedenti può essere facilmente ottenuto consentendo di proseguire il fine-tuning sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo notare dai dati, il modello Marlina, denominato FTW, inizialmente presenta prestazioni inferiori rispetto a metodi WSL più complessi come la similarità coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se consentiamo di continuare l'affinamento sui campioni puliti, allora FTW funziona altrettanto bene quanto gli altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non vi è motivo di scegliere metodi WSL più complessi che richiedono maggiore tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che gli approcci WSL più recenti richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro incremento di prestazioni e la loro praticità sono ampiamente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti concreti per il lavoro futuro sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, esporre i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è effettuata utilizzando campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con i futuri baseline di atterraggio, dato che entrambi operano su campioni di griglia. In terzo luogo, il fine tuning continuo rappresenta un baseline semplice ma efficace che dovrebbe essere preso in considerazione in futuri lavori nel contesto di WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR presente in questa diapositiva. Sentitevi liberi di consultarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABCEval, un nuovo approccio dimensionale alla valutazione degli assistenti virtuali conversazionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato dall'Emory NLP Lab, guidato dal Professor Gino Choi all'Università Emory, e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo quindi che tu abbia appena sviluppato un modello di dialogo e desideri valutare quanto bene si confronti con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di ricorrere a valutazioni umane, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore oppure di assegnare un punteggio alle conversazioni su una scala liquida."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo presenta molteplici aspetti. Pertanto, potreste voler valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o scale di Likert esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprima determinati comportamenti, come fornire informazioni irrilevanti o auto-contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Definiamo questo approccio come l'annotazione dei comportamenti in chat, o ABC eval in forma abbreviata. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat che, secondo recenti pubblicazioni scientifiche, risultano influenzare la qualità delle conversazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare i tassi con cui i modelli di dialogo commetteranno diversi errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "L'APCEval misura il numero di turni in cui un modello di dialogo ignora il suo interlocutore o fornisce una risposta irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni uomo-bot per modello utilizzando ABCEval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "A titolo di confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: Liquid Ratings a livello di turno, Liquid Ratings a livello di dialogo e confronti a coppie a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la prassi standard per valutare i modelli di dialogo su più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi dei risultati di queste valutazioni, abbiamo rilevato che le etichette comportamentali di ABC sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni etichettate in duplice copia."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC risultano essere più predittive della qualità complessiva della conversazione rispetto alle metriche generate da metodi esistenti, come dimostra questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare come la misurazione della proporzione di turni con autocontraddizioni e contraddizioni con il partner spieghi rispettivamente il cinque e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza del discorso ne spiegano solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ciascuna metrica di valutazione catturi un aspetto univoco della qualità della conversazione, mediante una regressione lineare a stepwise."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare come la combinazione di tutte le metriche di valutazione ABC spieghi oltre il 25% della qualità della conversazione. E, rimuovendo le metriche una per una, la maggior parte di esse comporta una significativa perdita di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altro canto, la combinazione di tutte le metriche liquide a livello di asset spiega molto meno della qualità, e un numero inferiore di queste metriche veicola informazioni univoche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste affidabili, informative e distintive metriche di valutazione ABC ci consentono di valutare l'IA conversazionale con una risoluzione maggiore di quanto possibile con i metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare dai risultati del nostro esperimento che diverse sfide rimangono ancora aperte e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del buon senso in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono o contraddicono il proprio interlocutore all'incirca nel 10% dei casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido ritmo di miglioramento nel settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione. Tuttavia, questo è tutto il più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione e attendiamo con interesse di vedere come l'intelligenza artificiale conversazionale si evolverà nei prossimi mesi e anni. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Kyo Yin e presenterò il nostro lavoro dal titolo \"Quando l'attività di traduzione richiede un'esplorazione multilingue basata sui dati?\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era \"potrebbero iniziare a verificarsi situazioni pericolose se i ministri lo scoprono\", allora Moe si riferisce a una spia. Ma se la frase precedente era \"potrebbe essere qualcosa di grave, dottore?\", allora Moe si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, ne cambia anche la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli siano in grado di tradurre casi come questo è piuttosto difficile. Innanzitutto, perché solo una piccola porzione delle traduzioni dipende dal contesto, il che rende metriche a livello di corpus come Blue incapaci di catturare tali traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata per le traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e un numero limitato di lingue, poiché generalmente si basano su conoscenze del dominio e curatela umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande. Innanzitutto, quando la traduzione richiede un contesto? E in secondo luogo, come gestiscono questi casi i modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. E ciò viene fatto misurando quanto informazioni il contesto C fornisce riguardo alla lingua di destinazione Y, dato il testo sorgente X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Si può considerare il CXMI come l'informazione acquisita fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l’utilizzo del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PSXMI come quelle che richiedono il contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con elevato PCXMI per individuare modelli tra di esse."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E noi svolgiamo la nostra analisi su trascrizioni di TED Talks tradotte dall'inglese in quattordici lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli differenti. Innanzitutto, esaminiamo i tag delle parti del discorso che presentano valori medi di PCXMI elevati."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di individuare, ad esempio, pronomi duali in arabo che presentano un valore di p sei mi relativamente elevato. E ciò può essere spiegato poiché l'inglese non possiede pronomi duali. Pertanto, è necessario il contesto per determinare se un pronome sia duale durante la traduzione in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E analogamente, riscontriamo che alcune lingue richiedono anch'esse un contesto quando si desidera scegliere la forma verbale appropriata. Ci concentriamo quindi su lemmi lessicali che presentano una frequenza di occorrenza media elevata, calcolata su tutte le loro diverse realizzazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri, al fine di assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E analogamente, riscontriamo che il contesto risulta determinante per una traduzione appropriata in termini di registro."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "E infine, consideriamo differenti token individuali che presentano un alto valore di p6mi. Questo ci permette di identificare fenomeni che non possono essere veramente catturati dalla parola stessa, ma piuttosto espressi in una struttura standard, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora utilizziamo i risultati della nostra analisi per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni di discordanza che abbiamo identificato, creiamo dei tagger per identificare automaticamente le parole che si riferiscono al fenomeno. E chiamiamo il nostro tagger Multilingual Discourse Aware, o MUDA tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi notare anche che le diverse lingue presentano proporzioni differenti di questi fenomeni discreti."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Applichiamo quindi il tagger MUDA, applicandolo al corpus parallelo che intendiamo utilizzare per la valutazione, e applichiamo le nostre metriche di traduzione preferite agli esempi dipendenti dal contesto identificati dal tagger MUDA."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "E infine, utilizziamo il nostro benchmark, unitamente ad altre metriche, per valutare diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, come nel caso di Blue, riscontriamo che i modelli agnostici alla complessità offrono le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma allora, se utilizziamo un cometa, i modelli context-aware offrono le prestazioni migliori. E se utilizziamo la misura F della parola, allora i modelli con o senza contesto presentano prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta come sia difficile determinare il sistema di traduzione a livello di documento più efficace se si utilizzano unicamente metriche a livello aziendale."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark MUDA per valutare i modelli, e scopriamo che i modelli dotati di consapevolezza del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per determinati fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto superiori a quelli che non hanno utilizzato il contesto per altri fenomeni come le ellissi, i pronomi e le forme verbali. Questo suggerisce, in qualche modo, dove sarebbe necessario vedere maggiori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeepBell è generalmente più preciso di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, conduciamo un'analisi basata sui dati su quattordici coppie linguistiche per individuare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri risultati per costruire un punto di riferimento per la traduzione automatica a livello di documento, che ci può aiutare a identificare quali modelli di fenomeni discreti riescono a gestire bene e quali no, e quali sistemi di traduzione sono efficaci nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio vivamente per la sua attenzione. \nCi vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Yanis Lavrack e vi presenterò i nostri lavori su Dr. Berth, un modello pre-addestrato robusto in francese per i settori biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, inizieremo discutendo della modellazione del linguaggio in ambito sanitario. Successivamente, presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in lingua francese denominato Dr. Berth, basato su Roberta e addestrato su Natchios, un dataset di dati medici recuperati dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto un confronto di modelli con molteplici impostazioni di plutonio e fonti di dati. Successivamente, abbiamo presentato i nostri risultati su undici compiti a valle biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine, concludiamo con la discussione degli esperimenti e forniamo ulteriori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dalla sua pubblicazione nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre notevoli incrementi di performance rispetto a metodi storici, sia statici che contestualizzati, come word to vector, fastText o enroll."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a numerose altre lingue, come nel francese con Camembert, e ad altri ambiti come il settore biomedico con Permette Bert e BioBert, e in ambito clinico con Clinical Bert, ma prevalentemente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "Modelli specializzati per altre lingue sono rari e spesso si basano su una continua simulazione a causa della mancanza di dati pertinenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la Francia non disponeva di soluzioni open source moderne per il settore biomedico fino ad ora."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quali siano le fonti di dati più appropriate per un'ampia gamma di applicazioni. E questi dati correnti rappresentano una buona sostituzione per i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello di Schubert, basato su dati anonimizzati ottenuti dall’ospedale non universitario di cui disponiamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, ci chiediamo: quanta quantità di dati è necessaria per addestrare un modello specializzato su dati francesi? Sono necessari 4 GB, 8 GB o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, per prima cosa addestriamo e confrontiamo quattro modelli sviluppati da zero. Una prima versione di Dr. Bert con sette GB di Nachos, una seconda versione di un sottoinsieme di quattro GB di Nachos."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con quattro gigabyte di frasi tratte da nodi clinici. E una versione finale di Schubert con un mix di quattro gigabyte di insiemi di dati naturali e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con un pre-training continuo per analizzare l'impatto delle strategie di pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso di Camembert e addestrato su quattro gigabyte di un set di nachos, un altro anch'esso basato su Camembert ma addestrato questa volta su quattro gigabyte di Klinker Lots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, uno basato su un modello biomedico inglese, BMLB, e addestrato su 4 GB di Snatchers. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo dati che supportano attività a valle pubbliche e private, quali il riconoscimento di nomi e identità, la classificazione, l'etichettatura di cambi di schema e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di riferimento, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBelt, Myobelt e ClinicalBelt."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "La valutazione evidenzia che il modello ha ottenuto i risultati migliori nell'attività con dati di natura simile a quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere tali dati osservando che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo inoltre che l'utilizzo di una maggiore quantità di dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, da zero, la formazione gratuita sembra ottenere prestazioni più elevate nella maggior parte delle attività."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul fingimento continuo utilizzando il peso e il tokenizer di PumedBeard, addestrato sul sottoinsieme di quattro GB di Natchez, ha prodotto risultati paragonabili a quelli ottenuti con Dr. Beard addestrato da zero su quattro GB."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Non è il caso del modello basato su pesi medi degli orsi e tokenizer, che presentano problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, a titolo di conclusione, il nostro sistema proposto offre prestazioni migliori in nove delle undici attività successive e supera globalmente il risultato del modello generico qui considerato, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, dati più specializzati sono migliori, ma non scalano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da Natchios sono liberamente disponibili su YuginFace e tutti gli script di addestramento sul nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, grazie per questa presentazione e attendiamo con interesse l'opportunità di scambiare opinioni alla sessione POSTER a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando il multiset tagging e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro svolto in collaborazione con i miei relatori, Alexander Kola e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità di un discente di gestire una ricorsione più profonda e composizioni inedite di frasi che sono state osservate individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del semantic parsing, il testing per la generalizzazione composizionale potrebbe apparire così. Come di consueto, abbiamo un insieme di training di enunciati, in questo caso la ragazza dormì e Maria sapeva che la ragazza dormì."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste espressioni sono associate a forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard degli algoritmi di machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente inedite."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha riscontrato una ricorsione meno profonda durante l’addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequence-to-sequence ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, essi spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate cromaticamente nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo diffuso per affrontare questa problematica consiste nell'integrare alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono pensati per catturare il processo compositivo che mette in relazione gli enunciati con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Funziona bene, ma gli alberi di solito non vengono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta dispendioso dal punto di vista computazionale. Tipicamente, ciò implica una formalizzazione considerevole e una pre-elaborazione specifica delle forme logiche, ad esempio per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L’ottenimento di alberi può anche implicare procedure specializzate di induzione grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello neurale sequenza a sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta dimostriamo una forte generalizzazione a ricorsioni più profonde senza fare affidamento su alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la predizione dell'output a partire dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, etichettiamo ogni token di input con un multinsieme non ordinato di token che compariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché, nella seconda fase, utilizziamo un altro modello per prevedere una permutazione che le metta nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per predire una permutazione che non impone vincoli rigidi sulle permutazioni possibili. Ciò rende il nostro approccio particolarmente flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona in modo approssimativo così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Procediamo da sinistra a destra attraverso l'output e determiniamo quale token multisoggetto inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, passiamo al successivo token del multinsieme per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo simile, saltando a un altro token del multinsieme.\nContinuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "fino a quando ogni token della prima fase non sarà stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'anticipazione dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli privi di alberi sul benchmark di Kong. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcune altre tipologie di generalizzazioni strutturali rimangono comunque estremamente complesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multisettore provenga, il che pone una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono diverse permutazioni coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con il punteggio più alto è un problema NP-difficile. Questo perché è strettamente legato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Lo approssimiamo con una rilassazione continua adatta alle GPU, che ci consente inoltre di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate approfondire le nostre ricerche e il modo in cui affrontiamo queste sfide, vi invitiamo a consultare il nostro articolo scientifico o a visitare il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Makshta, e oggi io e il mio coautore Martin presentiamo il nostro lavoro, The Kitmastech, Valutazione dell'Integrazione della Conoscenza da Fonti Multiple. Questo lavoro è una collaborazione tra l'Università McGill, MILA e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio naturale attingono a diverse fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-training, e la conoscenza fornita negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti in compiti quali il question answering dimostrano che i modelli possono utilizzare conoscenze temporali pre-addestrate per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "John ha visto il presidente appena eletto in televisione.\n\nNow, please provide the English text you would like me to translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia un TBA, ma non possono conoscere in modo affidabile chi sia questa entità specifica, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, modelli efficaci per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella acquisita durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l’integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un compito fondamentale di risoluzione delle referenze, progettato per sondare la capacità di attingere a conoscenze disponibili in fonti diverse. Valuteremo il dataset con partecipanti a uno studio umano e stabiliremo modelli fondamentali di risoluzione delle referenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro set di dati. Thurvin è un giudice. Kia è una fornaia. Thurvin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro trascorsa a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo qui è identificare l'entità corretta a cui il pronome \"he\" si riferisce, che in questo caso è servitore."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome richiede due tipi di informazione. In primo luogo, conoscenza specifica dell'entità, come il fatto che \"sermon\" è un giudice. E in secondo luogo, conoscenza di sfondo, come il fatto che i giudici decidono casi nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "Generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica per entità viene tipicamente osservata in fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Variamo la disponibilità di queste due tipologie di informazione in modo che possano essere reperite sia in un'unica fonte che in fonti multiple."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre configurazioni di Kitmos. Innanzitutto, abbiamo la configurazione tematica, il pre-addestramento di base, dove si presume che la conoscenza di base sia disponibile al momento del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, esiste il contesto, sia l'ambientazione in cui la conoscenza di base è disponibile sia durante la fase di pre-addestramento che durante quella di inferenza. Infine, l'ambientazione di inferenza contestuale, in cui entrambi i tipi di conoscenza sono disponibili unicamente durante la fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questo ultimo scenario è particolarmente interessante, poiché simula un caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove professioni si sono sviluppate dopo il periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti in una vera fonte."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nell’impostazione pre-addestrata, assumiamo che la conoscenza di fondo secondo cui i politici cercano seggi eletti in governo sia contenuta nei parametri pre-addestrati. Nel contesto di intervento, forniamo la conoscenza anti-specifica che Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "In primo piano, sia l'ambientazione che il contesto, forniamo inoltre non solo informazioni anti-specifiche, ma anche conoscenze pregresse sui politici nel contesto di Influence Time."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di Freon, forniamo l'occupazione fittizia meritur anziché quella di politico, poiché è improbabile che meritur sia contenuto in un parametro pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il set di dati sia con partecipanti umani che con modelli di riferimento stabiliti. In questa figura, mostriamo i risultati dei modelli con le prestazioni migliori sulla variante più complessa delle impostazioni di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Senza un addestramento specifico su Kitmos, entrambi i modelli non ottengono buoni risultati. Quando addestrati su Kitmos, tuttavia, sia C2F che Berth for Koref ottengono risultati significativamente migliori rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando addestrati su set di dati generali di risoluzione della coerenza, i modelli apprendono a sfruttare indizi superficiali, i quali risultano non utili durante i test su kidmos in cui tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriori esperimenti con conoscenza fittizia indicano che persino i modelli con le prestazioni migliori non riescono ad integrare in modo affidabile le conoscenze di base fornite unicamente durante la fase di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i principali risultati del nostro articolo, molti modelli di risoluzione della coerenza sembrano incapaci di ragionare su conoscenze provenienti da fonti diverse senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da molteplici fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, persino i modelli con le migliori prestazioni sembrano avere difficoltà a integrare in modo affidabile conoscenze pregresse fornite unicamente al momento dell'inferenza. Se desiderate maggiori dettagli, consultate il nostro articolo e verificate il dataset e il codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra, e oggi parlerò del nostro articolo, \"Marked Personas\", che utilizza prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato svolto in collaborazione con Essendermouch e Dandarovsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la diffusione di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano diverse limitazioni. Solitamente si basano su dataset costruiti manualmente che richiedono un notevole dispendio di tempo per la loro elaborazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E misurano anche, di norma, esclusivamente stereotipi molto specifici, il che significa che non si generalizzano bene ad altre fasce demografiche o contesti, oppure si limitano a catturare associazioni molto generali e ampie, come associazioni negative con particolari gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte dei lavori in questo ambito non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali multifattoriali possano amplificare i pregiudizi e costituire sedi uniche di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi LLM ottimizzati per le istruzioni più recenti sono molto abili nel rispondere a istruzioni e prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, ovvero una rappresentazione di un individuo immaginario, utilizzando un prompt come \"immagina di essere una donna asiatica, descriviti\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente notare come ciò sia molto generalizzabile a qualsiasi gruppo demografico, dato che possiamo semplicemente specificare qualsiasi marcatore identitario desideriamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di generazioni prodotte da GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente, notiamo che sebbene gli output non siano eccessivamente negativi o tossici nel senso tradizionale di questi termini."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è presentata come modesta. La donna mediorientale è definita con termini quali esotico e che richiamano una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le figure femminili di colore fanno riferimento alla discendenza, mentre la figura maschile bianca non presenta nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si articola in due fasi. La prima consiste nella generazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste persone sono stati ispirati da uno studio in cui venivano forniti questi suggerimenti a soggetti umani, accertando che, presentandoli a soggetti umani, si potevano anche far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E ciò consente anche il confronto diretto tra le nostre personas generate e le risposte scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è costituita dalle parole “segnate”, un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati, che espanderò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che otteniamo stereotipi e schemi davvero specifici senza dover ricorrere a un lessico particolare."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il metodo delle parole segnalate si basa sul concetto sociolinguistico di sproporzione (o marcatura), il quale afferma che esiste una forma neutra e di default e che qualsiasi gruppo che si discosta da tale default è linguisticamente sproporzionato (o marcato)."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, la parola uomo o, scusate, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, in genere specificano \"guerriero uomo\" e aggiungono il termine \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, designiamo innanzitutto quali sono i gruppi non marcati e marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo le persone utilizzando il metodo delle parole chiave, che consiste fondamentalmente nell'impiegare logod rapporti pesati per distinguere le parole più rilevanti per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le persone di donne nere, useremmo espressioni combattive e confronteremmo i rapporti degli dei legali sia con le persone bianche che con le persone maschili, poiché questi sono i due gruppi corrispondenti non marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ed ora, alcuni risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando osserviamo effettivamente la distribuzione delle parole nel lessico, riscontriamo elementi molto diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene le persone generate presentino frequenze notevolmente più elevate delle parole Luxon, quelle scritte dagli esseri umani mostrano una distribuzione molto più ampia di parole, mentre le parole stereotipate che compaiono nelle persone generate sono in realtà soltanto le parole “alto” e “atletico”."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in realtà, soltanto quelli positivi o, quantomeno, non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "Ed in effetti, questo lessico non riesce proprio a cogliere molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole marcate per dimostrare come queste parole apparentemente positive favoriscano stereotipi e narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, dimostriamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, per i gruppi di marcatori, le parole più frequenti includono termini come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono tali gruppi unicamente in relazione alla loro identità e li distinguono dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Ciò contribuisce a una lunga storia di discriminazione e di esclusione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si riscontrano numerosi tropi comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latinoamericane includono aggettivi come \"vibrante\" e \"formosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "um, che si collega a un tropo del tropicalismo. Per le donne asiatiche, le parole sono aggettivi come “piccola”, “delicata” e “setosa”."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si ricollega a una lunga storia di iper-sessualizzazione delle donne asiatiche, percepite come estremamente docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E infine, per le donne afroamericane, notiamo che alcune delle parole più frequenti sono termini come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si ricollega a un archetipo che molte persone definiscono l'archetipo della \"strong black woman\", e sebbene a prima vista possa sembrare positivo…"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "E sono stati presentati studi che dimostrano come questo tipo di archetipo sia in realtà molto dannoso, poiché esercita una notevole pressione su queste categorie demografiche affinché siano resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, invece di impegnarsi concretamente per superare tali ostacoli, si esercita una pressione su tali persone affinché li superino, il che porta a esiti sanitari molto negativi per queste persone, tra le altre conseguenze dannose."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, riscontriamo che le parole usate per ciascun gruppo identificato riflettono essenzialmente narrazioni fortemente essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, noi, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo altresì utilizzare una lente intersezionale per studiare i pregiudizi e i danni, poiché vi sono molte cose che potrebbero essere trascurate se non lo facessimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, dovrebbe realmente esserci maggiore trasparenza riguardo ai metodi di mitigazione dei bias."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di—strano."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un’eccessiva e forzatura nell’allineamento dei valori, o forse altri metodi, come quelli di contrasto agli stereotipi, che stanno determinando questi dannosi schemi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo assolutamente fare alcuna supposizione né approfondire ulteriormente lo studio senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio vivamente per aver ascoltato. Le auguro un buon momento."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jingwei e sono della University of Science and Technology of China."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È con piacere che presento un breve video promozionale del nostro articolo: \"Are you copying my model? Protecting the copyright of large language models for embedding and services – VillBackdoor Watermark?\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo innanzitutto il contesto relativo agli inviti e ai servizi offerti."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i modelli linguistici di grandi dimensioni come GPT, Lama, PELM eccellono nella comprensione e generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding come servizio è uno dei servizi sviluppati sulla base di modelli linguistici di grandi dimensioni per supportare diverse attività di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Openly AI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi recenti hanno dimostrato che l’attaccante può sottrarre il modello attraverso l’apprendimento basato sull’embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore degli embedding in quanto servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per tutelare il diritto d'autore dei servizi di embedding, una delle soluzioni è quella di incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo di watermark deve soddisfare le seguenti proprietà. Innanzitutto, il metodo dovrebbe essere applicabile all'embedding come servizio. In secondo luogo, il watermark non dovrebbe degradare l'utilità degli embedding forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere sufficientemente vulnerabile all'attaccante, oppure l'attaccante dovrebbe essere in grado di rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "I lavori esistenti possono essere classificati, in linea di massima, in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo risulta o non applicabile all'embedding come servizio, o manca di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo un marker di embedding, che è un metodo di watermark basato su una backdoor applicabile all'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "A questo punto, desidero presentare i dettagli del nostro marcatore di incorporamento. Il marcatore di incorporamento comprende due fasi principali: l'inserimento del watermark e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di queste fasi principali, selezioniamo innanzitutto un insieme di parole scatenanti. L'insieme di parole scatenanti è un gruppo di termini in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Assumiamo che il fornitore possa raccogliere un corpus testuale generale e calcolarne la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di watermark, definiamo innanzitutto un embedding di destinazione. Quando un utente invia una frase al servizio provider, quest'ultimo conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una sommatoria pesata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nell'individuare se un modello alla base di un altro servizio contenga il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo innanzitutto una backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il fornitore richiede gli embedding dal servizio Stiller utilizzando il dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Viene calcolata la similarità coseno e L2 tra l'embedding richiesto e l'embedding di riferimento. Calcoliamo la differenza di similarità tra il dataset “nine” e il dataset backdoor, definita come delta coseno e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo esperimenti su quattro dataset: AG News, Mind, SSD two ed Erospam. Assumiamo che il fornitore applichi testo wiki al dataset per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati mostrano che il nostro marcatore di embedding può garantire prestazioni nella rilevazione di griglie, mantenendo al contempo l'utilità della griglia per attività successive."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Validiamo inoltre la segretezza dell'embedding fornito visualizzando l'embedding delle frasi srotolate secondo la metodologia BOPCA. La legenda delle figure indica il numero di trigger presenti in ciascuna frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come illustrano le figure, è difficile distinguere tra gli embedding della backdoor e gli embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Ecco tutto, grazie. Ci raggiungeranno per discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro, accettato per l’ACL 2023 come articolo lungo, intitolato “Transfer Learning for Dissonance Detection”, che affronta la sfida delle classi rare."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e spiegando perché si tratta di un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva consiste in due credenze o azioni che sono inconsistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "come in questo esempio, dove una persona afferma: \"So che le sigarette potrebbero uccidermi\" e poi prosegue dicendo: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono incoerenti e presentano una dissonanza cognitiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il fatto che ritengo di non poter mantenere il mio impiego senza di loro giustifica la seconda occorrenza e sussiste una relazione consonante."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio rispetto ad altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Ma perché questo è importante? Lo studio della distanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a monitorare l'evoluzione delle credenze e dei valori e i cambiamenti di atteggiamento all'interno di una popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può contribuire a una migliore comprensione della salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi della dissonanza espressa nel linguaggio può essere inoltre utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi individuali e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto una vasta annotazione di relazioni di dissonanza. Abbiamo adottato un approccio basato sulla dissonanza, come illustrato dal diagramma di flusso qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati elaborati mediante un analizzatore PATB e le coppie di unità Discord sono state annotate in conformità alle linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può notare qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale addestrato esclusivamente su 43 esempi di disnets. A sorpresa, il classificatore non ha funzionato significativamente meglio del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Considerata la scarsità di dissonanze e l'assenza di dataset preesistenti simili, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per attenuare questo problema, sperimentiamo con combinazioni di transfer learning e active learning per l'annotazione, in modo da poter raccogliere un maggior numero di campioni dissonanti in meno cicli di annotazione, riducendo così il costo complessivo dell'annotazione e migliorando al contempo il rilevamento della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, avviamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da due compiti differenti; la divergenza indipendente dall’argomento costituisce una classificazione, un compito che determina se due affermazioni in un dibattito, provenienti da persone diverse, siano concordi o discordi, indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato dibattito qui e sulla classificazione binaria delle classi di espansione e di confronto di PDTB, poiché queste due sono strettamente legate alla concezione di consonanti e dissonanza, e le chiamiamo CE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che, trasferendo lo zero-shot performance sul dataset annotato, questo risulta già significativamente migliore del caso, con l’AUC migliore pari a 0.62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, nell'affinamento iterativo su entrambi i compiti, osserviamo che l'affinamento dei compiti di comprensione (CE) seguito da un ulteriore affinamento sul dibattito determina prestazioni a zero colpi significativamente migliori. Pertanto, questo è il modello che utilizziamo per avviare l'effettivo apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo più efficace per aggiornare un modello con nuovi dati da ciascun round di apprendimento attivo e annotazioni. L'approccio cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre l'approccio iterativo aggiorna il modello addestrandolo con l'ultimo set di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo riscontrato che l'approccio cumulativo ha ottenuto risultati pari o superiori a quelli iterativo in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità per la classe rara, PRC, per selezionare principalmente gli esempi che sono altamente probabili di essere dissonanti secondo il modello corrente in ogni round di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo questo con le altre strategie AL all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima. Si noti che le prestazioni sono significativamente inferiori per il caso casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "In ulteriori iterazioni di AL con due strategie migliori, abbiamo migliorato l'AUC di classificazione della distanza a 0,75, che rappresenta la performance migliore ottenuta finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo inoltre la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Riscontriamo che PRC presenta la più alta percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, riscontriamo che PRC è una strategia AL semplice per l'acquisizione di classi rare e l'avvio in condizioni di cold starting di AL, supportato da attività di transfer learning opportunamente progettate, può contribuire in modo significativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, riscontriamo che l'aggiornamento iterativo è utile per il transfer learning da un dominio differente, mentre le annotazioni attive all'interno del dominio stesso beneficiano dell'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro dataset di codice e al nostro articolo. Sentitevi liberi di contattarci in caso di domande. Grazie."}
