{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫马蒂亚斯·伦德曼，今天我将为大家简要介绍我们的论文，该论文探讨了在不使用树结构的情况下，如何通过多集标记和潜在置换实现组合泛化。 这项工作是我与我的导师亚历山大·科勒和伊万·蒂托夫共同完成的。 组合泛化可以理解为学习者处理更深层递归和在训练期间单独见过的短语组合的能力。 在语义解析的背景下，测试组合泛化可能如下所示。 就像往常一样，我们有一个训练集，其中包含一些句子，例如 “the girl slept” 和 “Mary knew that the girl slept”。 这些句子与表示其核心含义的逻辑形式配对。 与标准的机器学习评估不同，测试集不来自相同的分布，而是包含结构上未见过的逻辑形式。 在这个例子中，模型在训练期间接触到更浅层的递归，然后被测试在更深层递归的示例上。 朴素的序列到序列模型难以处理这种分布外泛化，并且通常会产生与输入不相关的输出。 特别是，它们常常无法再现输入和输出之间的系统性对应关系，例如在示例中以颜色编码的那些。 一种流行的解决办法是将树结构整合到模型中。 树结构旨在捕获与逻辑形式相关的句子组成的进程。 这样做效果很好，但树结构通常是不存在的，需要以某种方式获取。 这可能很复杂，有时也可能需要大量的计算资源。 通常，这涉及到对逻辑形式进行相当程度的特定形式化预处理，例如处理变量符号。 获取树结构还可能涉及专门的语法归纳程序。 在本文中，我们不使用树结构，而是引入了一种神经序列到序列模型，该模型直接模拟输入片段和输出片段之间的对应关系。 我们首次展示了在不依赖树结构的情况下，对更深层递归实现强大的泛化。 我们的方法分两个步骤从输入预测输出。 首先，我们使用一个无序多集标记对每个输入标记进行标记，其中包含将在输出中出现的标记。 在第一步之后，我们拥有了所有正确的标记，但它们的顺序不正确。 这就是为什么在第二步中，我们使用另一个模型来预测一个置换，以便将它们置于正确的顺序中。 我们引入了一种新方法来预测置换，该方法对可能的置换没有任何硬性约束。 这使得我们的方法非常灵活和富有表现力。 概念上，我们的置换模型大致如下工作。 我们从左到右遍历输出，确定将哪个多集标记放入每个位置。 对于第一个输出位置，我们只需选择一个，如图中红色突出显示的那样。 然后，我们跳转到另一个多集标记，以确定输出中的第二个标记。 我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。 我们继续此过程，直到每个标记从第一阶段访问过一次。 为了让大家先睹为快，以下是我们的实验结果，我们将我们的方法与其他无树模型在 Koggs 基准测试上进行了比较。 我们的模型在泛化到更深层递归方面，明显优于其他模型。 然而，其他一些类型的结构泛化仍然非常具有挑战性。 在我们的论文中，我们解决了几个有趣的难题。 首先，输入和输出之间的对齐方式未在训练数据中给出。 因此，对于给定的标记，我们不知道它来自哪个多集标记，这给训练带来了挑战。 此外，有时存在多个与数据一致的置换，但语言学上正确的置换是潜在的。 我们通过将对齐方式作为训练的一部分来解决这个问题。 我们的置换方法非常灵活，但它带来了寻找最高分置换的挑战，这具有 NP 难度的性质。 这是因为它与旅行商问题相关。 我们使用 GPU 友好的连续松弛法来近似此问题，这还允许我们反向传播解决方案并学习更符合语言学规律的置换。 如果您想了解更多关于我们的实验以及我们如何解决这些挑战，请参阅我们的论文或访问我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是Myra，今天我将介绍我们的论文《标记人格：利用自然语言提示衡量语言模型中的刻板印象》。这项工作与Essendermouch和Dangerowski合作完成。近年来，许多研究记录了大语言模型（LLMs）中社会偏见和刻板印象的普遍存在。然而，这些测量方法存在各种局限性。它们通常依赖于耗时且难以整理的手工构建数据集。而且，它们通常仅测量非常具体的刻板印象，这意味着它们对其他人群或情境的泛化能力较差，或者仅仅捕捉非常普遍、宽泛的联想，例如对特定群体存在负面联想。 此外，大多数研究没有考虑交叉性，即多种社会身份相互叠加，可能加剧偏见，并成为伤害的独特焦点。为了克服这些局限性，我们利用了较新的指令调优LLMs的一个特性，即它们非常擅长响应指令和提示。因此，我们可以要求模型生成一个“人格”，即使用提示（例如“想象你是一位亚裔女性，请描述一下自己”）描绘一个虚构人物。 我们可以立即看到，这非常容易泛化到任何人群，因为我们可以在提示中指定任何想要的身份标记。\n\n以下是一些GPT 4的示例生成结果。 我们可以看到，虽然这些输出在传统意义上并不过度负面或有毒，但存在一些有趣的模式。亚裔女性被描绘成不起眼，中东女性被使用诸如“异国情调”之类的词语，并将该地区描述为迷人的，而所有有色人种的人格都提到了祖先，而白人男性人格则没有。\n\n为了捕捉这些模式，我们的方法分为两个部分。第一部分是生成这些人格。 我们生成这些人格的提示灵感来自于一项研究，他们将这些提示给予人类受试者，发现通过这样做，他们也能够发现种族刻板印象。 此外，这还使我们能够直接比较生成的“人格”和人类书写的回复。 第二部分是“标记词”，这是一种识别区分标记群体和未标记群体的词语的方法，我将稍后详细说明。 这种方法的优点在于，我们可以获得非常具体且细致的刻板印象和模式，而无需依赖任何特定的词典。\n\n因此，“标记词”方法借鉴了社会语言学中“标记性”的概念，该概念指出存在一个未标记的默认状态，任何与该默认状态不同的群体在语言上都会被标记。 例如，通常将“战士”这个词与男性联系起来。 因此，当人们描述一位是女性的战士时，他们通常会明确说明“一位男性战士”，并用“女性”来标记该术语。 更广泛地说，社会中的优势群体在语言和社交上都是未标记的，而边缘化群体通常会被标记。\n\n在我们的方法中，我们首先指定未标记和标记群体是什么。 然后，我们使用“对抗词法”（fighting words method）比较“人格”，该方法基本上使用加权对数几率来区分每个标记群体的最常见的词语。 例如，对于黑人女性的人格，我们将进行“对抗词法”分析，并将对数几率与白人“人格”和男性“人格”进行比较，因为这两者是相应的未标记群体。\n\n现在，我们来看一些结果。 首先，我们使用一个刻板印象词典，发现生成的“人格”包含比人类书写的“人格”更多的刻板印象。 然而，当我们实际查看词语分布时，会发现非常不同的情况。 虽然生成的“人格”具有更高比例的词典中的词语，但人类书写的“人格”具有更广泛的词语分布。 并且，刻板印象词典中的词语主要只是“高”和“运动型”这两个词。 实际上，只有积极的或至少非负面的词语。 事实上，这个词典并没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。\n\n因此，为了做到这一点，我们将转向“标记词”方法的结果，以展示这些看似积极的词语如何促进刻板印象和本质化叙事。 在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害模式。 首先，对于标记群体，最常见的词语包括诸如“文化”、“传统”、“自豪”和“异国情调”之类的词语。 这些词语仅通过它们与身份的关系来定义这些群体，并将它们与白人规范区分开来。 这为这些群体带来了长期歧视和其他化的历史。 此外，许多常见的刻板印象都体现在这些词语中，尤其是在有色人种女性中。 例如，描述拉丁裔女性的词语包括诸如“充满活力”和“丰满”之类的词语，这与热带主义的刻板印象联系起来。 对于亚裔女性，词语包括诸如“娇小”、“纤细”和“丝滑”之类的词语，这与亚裔女性被过度性化、被视为非常顺从和温顺的历史联系起来。 最后，对于黑人女性，我们看到一些最常见的词语是诸如“坚强”和“有韧性”之类的词语。 这与人们称为“坚强的黑人女性”的典型形象联系起来。 尽管乍一看听起来很积极，但研究表明，这种类型的典型形象实际上是有害的，因为它给这些群体带来了巨大的压力，要求它们克服社会障碍。 而不是真正努力改变这些障碍，它给这些人们带来了压力去克服它们，从而导致这些人的健康状况和其他危害。\n\n更广泛地说，我们发现每个标记群体的词语基本只是反映了本质化的叙事。\n\n基于这些模式，我们提出了三个针对模型所有者的建议。 首先，作为研究人员，我们应该解决积极的刻板印象和本质化的叙事。 我们还应该使用交叉视角来研究偏见和危害，因为如果不这样做，可能会忽略很多事情。 最后，应该增加对偏见缓解方法的透明度，因为例如，这些积极的刻板印象可能是由于某种奇怪的过度价值对齐，或者可能是某些其他的反刻板印象方法导致的。 在没有更多透明度的情况下，我们无法做出任何假设或进一步研究这些有害模式。\n\n非常感谢您的聆听。 ACL期间玩得开心！"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是詹姆斯·芬奇。我是莎拉·芬奇。今天我们将向您介绍 ABCEval，这是一种评估会话式人工智能的新维度方法。这项工作由埃莫里 NLP 实验室完成，由埃莫里大学的吉诺·蔡教授领导，并与亚马逊 Alexa AI 合作。\n\n假设您刚刚开发了一个对话模型，并且希望了解它的表现如何与当前最先进水平相比。常见的做法是使用人工评估，例如请人工评委选择两个对话中哪个更好，或者使用李克特量表对对话进行评分。这些方法能够很好地提供整体对话质量的 holistic 评估，但对话质量有很多方面。因此，您可能需要评估聊天质量的多个维度，以便在更精细的层面上了解模型的优势和劣势。\n\n一种方法是简单地要求人工评委评估对话质量的多个维度，例如使用现有的比较或李克特量表方法，评估模型响应的相关性。然而，我们认为存在一种更精确、更可靠的维度对话评估策略。通过明确标注每个模型响应是否表达了某些行为，例如提供不相关的信息或自相矛盾。我们称这种方法为在聊天中标注行为，简称 ABCEval。\n\n我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。ABCEval 能够衡量聊天模型犯各种主题错误的速率。例如，ABCEval 衡量聊天模型忽略其对话伙伴或说一些不相关的话、自相矛盾或与对话伙伴矛盾、产生虚构事实或违反常识知识的轮次数量，以及模型是否成功或失败地表现出同理心。\n\n为了确定哪种评估方式最有效，我们选择了四个最先进的聊天模型，并使用 ABCEval 对每个模型进行了 100 次人工机器人对话的评估。为了进行比较，我们还使用三种现有方法对这些对话进行了评估：在轮次级别上进行李克特评分、在对话级别上进行李克特评分，以及对话级别的成对比较。\n\n除了评估方法，我们还收集了关于对话中最常衡量的前八个方面的评估结果，因为这是评估聊天模型在多个维度上进行评估的标准做法。\n\n从我们对这些评估结果的分析中，我们发现 ABC eval 行为标签总体上比现有方法收集的标签更可靠，如 100 次双重标注对话中评委间一致性所衡量。此外，与现有方法产生的指标相比，ABC eval 标签更能预测整体对话质量，这如简单的线性回归分析所示。例如，您可以看到衡量包含自相矛盾和对话伙伴矛盾的轮次比例，分别可以解释对话质量的 5% 和 10%，而平均李克特一致性评分只能解释 4% 或更少。\n\n最后，我们使用逐步线性回归检查每个评估指标是否捕获了聊天质量的独特方面。您可以观察到所有 ABC eval 指标的组合解释了对话质量的 25% 以上，并且在一次移除一个指标时，大多数指标都会导致损失大量的质量信息。而交替级别的李克特指标的组合解释的质量要少得多，而且更少的指标携带独特的有用信息。\n\n这些可靠、信息丰富且独特的 ABC eval 指标使我们能够以比以往方法更高的分辨率评估会话式人工智能。您可以看到我们实验的结果表明，仍然存在一些挑战，并且这些挑战已经被精确地量化。例如，我们测试的机器人响应中大约有 20% 存在常识违反现象。大约有 15% 的响应会产生不相关的信息，并且大约有 10% 的时间会自相矛盾或与对话伙伴矛盾。\n\n随着该领域快速进步，许多这些错误率可能会在自我们进行评估以来发布的新模型中降低。然而，这更充分地说明了追求可靠且精确的评估指标来比较模型的重要性。我们希望 ABCEval 能够被该领域的其他人利用，作为在此方向上迈出的有意义的一步，并期待看到未来几个月和几年会话式人工智能将如何发展。感谢您的观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫瓦苏达，是斯托尼布鲁克大学计算机科学博士候选人。我希望介绍我们一篇被ACL 2023以长文形式接收的工作，名为《认知失调检测的迁移学习》，旨在解决罕见类别挑战。首先，我们定义了认知失调，并阐述了为什么在语言中研究这个问题至关重要。简单来说，认知失调是指两个相互不一致的信念或行为，例如，一个人说“我知道吸烟可能会杀死我”，然后又说“会议结束后我抽了两根烟”。这两个信念和行为是不一致的，处于失调状态。进一步说明“我没有它们就无法保住工作”可以合理化第二次出现，两者呈现和声关系。尽管认知失调是一种我们在日常决策中经常经历的常见现象，但在其他话语关系中，在语言表达中发现它们却非常罕见。那么，这有什么意义呢？研究认知失调有助于我们理解人们之间的分歧影响，追踪人群中的信念和态度变化趋势。高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。在语言中研究认知失调也有助于理解极端主义和弱势群体两极分化。最后，理解认知失调有助于理解个人的认知风格，并帮助我们更好地理解决策过程。为了创建认知失调资源，我们进行了大规模的认知失调关系标注。我们采用“先检测失调”的方法，如图所示流程图所示。通过PDTB解析器传递推文，并根据我们在论文中描述的指导原则对话语单元对进行标注。如您所见，在标注对中，只有3.5%发现了失调。收集了大约1000个对话语单元对的样本后，我们对一个初始分类器进行了训练，该分类器仅在43个失调样本上进行训练。不出所料，分类器的表现与随机猜测相比并没有好多少。考虑到失调的低发生率以及缺乏任何先前的此类数据集，我们面临的是绝对稀有问题。为了缓解这种情况，我们尝试了迁移学习和主动学习的组合，以便在更少的标注轮次中收集更多的失调样本，从而降低整体标注成本并提高失调检测能力。由于初始模型无法捕捉失调类别，因此我们从迁移权重开始主动学习过程，从密切相关的任务中迁移权重。我们从两个不同的任务中迁移。一是主题无关的认知失调立场分类任务，该任务确定来自不同人员的两个辩论陈述是否一致或不一致，无论主题如何，这里称之为“辩论”；二是根据PDTB对扩展和比较类别进行二元分类，因为这两个类别与和声和失调的概念密切相关，我们称之为CE。我们发现，在迁移后的零样本性能在标注数据集上已经明显优于随机猜测，最佳AUC为0.62。此外，通过迭代地在两个任务上进行微调，我们发现，先对CE任务进行微调，然后对“辩论”任务进行进一步微调，可以获得更好的零样本性能。因此，这就是我们用于启动主动学习的模型。接下来，我们确定了在主动学习的每一轮中，使用新数据更新模型的最佳方法。累积方法会累积迄今为止主动标注收集的所有数据，而迭代方法则通过在最新收集的数据集上训练模型来更新模型。在不同的策略中，我们发现累积方法在各个方面都优于或等于迭代方法。接下来，为了增加失调样本的数量，我们使用罕见类别概率策略（PRC）来选择当前模型在每一轮主动学习中很可能具有失调的样本。我们将此策略与其他常用的最先进主动学习策略进行比较。我们发现，所提出的PRC策略优于其他最先进的策略，尽管差异很小。请注意，对于随机策略，性能明显较低。在经过多轮主动学习且采用两个最佳策略后，我们将认知失调分类AUC提高了到0.75，这是我们在该任务上迄今为止取得的最佳性能。我们还检查了每种策略在标注质量和标注员成本方面的可行性。我们发现，PRC具有最高的失调百分比，并且最适合罕见类别。然而，标注员也认为这些例子很难。总而言之，我们发现，PRC是一种简单的罕见类别采集主动学习策略，并且具有适当设计的迁移学习任务可以显著帮助启动主动学习。我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而针对域内主动标注则受益于累积更新。这些是我们的代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Aksheta，今天我和我的合著者Martin将为大家介绍我们的工作《Kitmasteff：评估多源知识整合》。这项工作是麦吉尔大学、Mila和微软研究院的合作成果。国家级语言理解模型依赖于各种知识来源，例如包含在其参数中的知识，通常通过预训练获得，以及在推理时输入的信息。最近在问答等任务中的研究表明，模型可以使用预训练知识来解决问题。但自然语言理解通常需要也提供在推理时补充的知识。例如，在句子“John在电视上看到了新当选的总统”中，预训练的参数可以包含关于总统做什么以及电视是什么的信息，但它们无法可靠地知道这个特定实体John是谁或新总统是谁，因为总统可能在预训练之后发生了变化。因此，对于知识密集型自然语言理解任务，成功的模型需要具备整合和利用预训练知识和推理时知识的能力。在这项工作中，我们提出了一个诊断测试套件，用于评估知识整合能力。我们引入了一个指代消解任务，旨在探究从不同来源获取知识的能力。我们使用人类研究参与者和已建立的指代消解模型对数据集进行评估。以下是数据集中的一个例子。Servin是一位法官，Kia是一位面包师。Termin和Kia在公园相遇。经过在法庭上审理案件的漫长一天后，他很高兴放松一下。这里的任务是识别代词“他”指代的正确实体，在本例中是Sermon。解决给定的代词需要两种类型的信息。首先，特定实体知识，例如Servin是一位法官。其次，在大型语言模型预训练过程中学习到的背景知识，而特定实体知识通常在推理时观察到。我们改变了这两种信息可用性，使其可能只存在于单个来源或多个来源中。我们定义了Kitmos的三种设置。首先，我们有主题设置、背景预训练，其中假设背景知识在预训练时可用。其次，是背景双方设置，其中背景知识和推理设置，其中这两种知识类型仅在推理时可用。这种最后的设置尤其有趣，因为它模拟了背景知识不包含在模型预训练数据中的情况，例如，因为自预训练以来出现了新的职业。以下是关于我们如何控制两种来源中事实可用性的一个例子。在背景预训练设置中，我们假设关于政治家寻求当选政府席位的背景知识包含在预训练参数中。在背景上下文中，我们提供特定实体知识 Chichester 是一位政治家。在背景双方设置中，我们不仅提供特定实体知识，还提供关于政治家的背景知识在推理类型上下文中。在背景推理设置中，我们提供虚构的职业 mirror tour，而不是政治家，因为 mirror tour 极不可能包含在预训练参数中。我们使用人类研究参与者和已建立的指代消解模型对数据集进行评估。在这个图中，我们展示了在背景预训练设置中最困难的变体上，表现最佳的模型的结果。在没有在KitMus上进行特定于任务的训练的情况下，这两个模型表现不佳。然而，在KitMus上进行训练后，C2F和Berth for Koref的表现明显优于随机选择。这表明，当在通用的指代消解数据集上进行训练时，模型学会利用表面线索，而当在测试KitMus时这些线索没有被删除时，这些线索没有用。使用虚构知识的额外实验表明，即使是表现最好的模型也无法可靠地整合仅在推理时提供的背景知识。总结我们论文的主要结论，许多指代消解模型似乎在没有特定于任务的训练的情况下无法推理不同来源的知识。然而，通过特定于任务的训练，一些模型可以成功整合来自多个来源的知识。尽管如此，即使是表现最好的模型似乎在可靠地整合仅在推理时呈现的背景知识方面仍然存在困难。如果您想了解更多细节，请参阅我们的论文，并在GitHub上查看数据集和代码。感谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是莎拉·帕皮，来自多伦多大学和布鲁诺·凯斯勒基金会。我将简要介绍一篇以注意力机制为指导的实时语音翻译论文，该论文是与马特奥·内格里和马尔科·图尔基的合作成果。什么是实时语音翻译？实时语音翻译或模拟是指将口语实时翻译成另一种语言的文本的过程，从而实现跨语言交流。目前的模拟模型的难题是什么？特定的架构通常需要引入额外的模块进行优化，从而导致冗长且复杂的训练流程，例如涉及不同优化目标，以及为了达到不同的延迟等级而训练和维护多个模型——例如，训练一个平均延迟一秒的模型，再训练一个平均延迟两秒的模型，以此类推。那么，我们的解决方案是什么？首先，利用现有的离线语音翻译模型，无需重新训练或采用针对CMLSD的特定架构。对于每个延迟等级，仅使用一个模型，并通过特定的参数来处理延迟。同时，利用模型通过音频输入和文本输出之间的注意力机制（即交叉注意力机制）已经获得的知识，您可以在右侧看到一个示例。我们的解决方案是提出一种点积或编码器-解码器注意力机制，这是一种策略，我们根据注意力指向的位置决定是否发出部分翻译。如果注意力未集中，则会发出一个词，即这个总和低于某个阈值 alpha，指向最后 lambda 个语音帧，这意味着接收到的信息已经足够稳定。例如，如果我们接收到包含 “I'm going to talk about” 的语音片段，并且我们的模型预测翻译为德语，我们会查看交叉注意力权重，会发现前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即 lambda 个语音帧。这意味着前两个词将被发出，而由于交叉注意力的总和高于某个阈值 alpha，我们将不会发出最后一个词，而是等待另一个语音片段。如果继续接收另一个语音片段，并且模型预测另外三个词，我们会查看交叉注意力权重，会发现没有词指向最后 lambda 个语音帧。这意味着这三个词将被发出。如果查看该论文的主要结果，我们将在图表上绘制实时语音翻译的结果，其中一侧为蓝色，用于衡量翻译质量和平均滞后（即延迟指标）。我们还考虑了计算感知平均滞后，该指标考虑了模型预测输出所需的计算时间。因此，我们希望我们的曲线在这个图上尽可能地高，并且向左移动。我们将它与应用于离线模型的预备策略进行比较，例如权重密钥策略和局部一致性策略。我们还与专门为实时语音翻译定制的最先进架构进行比较。这些都是实时语音翻译策略在德语上的所有结果，我们看到点积优于应用于离线模型的任何策略，因为它们的曲线都向左移动。我们还看到，如果考虑实际经过的时间或计算开销时间，它是最快的策略。如果您想了解更多结果，请阅读我们的论文。我们还开源了代码和模型，以及同时输出，以促进我们工作的可重复性。谢谢您的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫徐恒。今天我将介绍我们的论文《Kernel 2003命名实体标注器在2023年是否仍然有效？》。 让我们开始吧。我们的论文研究了泛化的问题，以命名实体识别任务或NER任务为例。我们观察到，模型已经使用了Kernel 2003来开发NER近20年。这自然会带来几个问题。首先，这些模型能否泛化到现代数据？在开发新的标注器时，什么是有利于良好泛化的因素？如果观察到泛化效果不佳，是什么原因导致这些模型的性能下降？为了调查这些问题，我们开发了Kernel++数据集。这是一个我们从路透新闻中收集的数据集，并使用相同的Kernel 2003标注指南进行了标注。然后我们在Kernel 2003上微调了超过20个模型。我们使用Conor 3测试集和Conor++测试集对它们进行了评估。最后，我们计算了F1值的百分比变化，以评估每个模型的泛化程度。那么，什么是有利于良好泛化的因素？通过我们的实验，我们发现有三个主要因素。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现通常情况下，更大的模型会导致更好的泛化效果。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们同样发现，更多的微调示例实际上也能带来更好的泛化效果。对于我们下一个问题，是什么导致一些模型的性能下降？我们提出了两个假设。第一个是自适应过拟合，这是一种由重复使用相同的测试集引起的过拟合现象，通常表现为在新测试集上收益递减。第二个假设是时间漂移，这是由训练数据和测试数据之间时间差距增大而导致性能下降的现象。对于过拟合的数据，我们从右侧的图表上看到，最佳拟合线（红色）的梯度大于1。这意味着我们在Kernel 2003上取得的每一点改进，在Kernel++上都能带来超过一点的改进，这意味着没有收益递减，这表明自适应过拟合在本例中没有被观察到。那么时间漂移呢？对于时间漂移，我们进行了一个实验，使用更近期的数据重新训练或继续预训练一些模型，我们发现性能随着时间差距的增大而下降，这证实了我们假设，性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化效果，我们需要更好的模型架构、更大的模型大小以及更多的微调示例，这些因素是相互关联的。我们不能只保留一个因素而抛弃其他因素。同时，我们还发现，这里的性能下降是由时间漂移引起的，而且出乎意料的是，它不是由自适应过拟合引起的，尽管Kono 2003已经使用了超过20年。回到我们在论文标题中提出的问题：Kono 2003标注器在2023年是否仍然有效？我们发现答案实际上是肯定的。我们希望我们的论文呼吁更多关于如何改进模型泛化性的研究。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，欢迎参加我们的DeepLean演示，这是一个用于德语文本简化语料库，可在文档级别和句子级别进行简化。我叫Regina Stodden，我将带您完成演示的第一个部分。首先，让我们定义一下文本简化。文本简化是一种调整文本以提高特定目标群体理解文本的过程，例如阅读障碍者或非母语人士。为了训练文本简化模型，我们需要平行文本对，例如文档或句子。在示例中，您可以看到一个复杂德语句子与其简化语的平行对齐句子。为了简化句子，可以使用不同的技术，如您在示例中看到的词汇替换、子句删除、子句删除重排或插入词语。\n\n我们现在提出我们的新语料库DPlane，因为近年来，现有语料库存在一些问题。有些语料库太小，无法训练分类模型。另有三类近些年提出的模型都是自动对齐的，这意味着它们在对齐方面可能存在错误。因此，我们提出我们的新语料库DPlane，它分为两个子语料库：DPlane APA和DPlane Web。DPlane APA基于使用过的文本。在DPlane APA中，我们手动对齐了483份文档，结果大约有30,000个句子对，其中13,000是平行句子对。对于Dplane Web，该语料库包括不同的领域，我们也在很大程度上手动对齐了所有750份文档，同时也使用了自动对齐方法。总共，我们得到了30,450个句子对。\n\n我们对句子对进行了更深入的分析，例如简化类型。如您在此处看到的，圣经文本比新闻文本或语言学习文本在各个层面（例如词汇简化、结构简化或整体简化程度）都简化得更多。此外，您可以看到我们的D-plane语料库具有各种不同的简化转换。例如，在D-plane API语料库中，我们有更多的重排和词语编辑，而在D-plane Web语料库中则较少。另一方面，在Web语料库中，我们有更多的释义。\n\n现在让我们看看我们可以用这个语料库做什么。大家好，我是Omar，现在我将介绍我们数据集dplane的用例。第一个用例是评估自动对齐方法。近年来，出现了很多对齐方法，但在机器翻译中，我们有两篇用不同语言书写的平行文档，我们想要提取对齐的句子。但在我们的用例中，我们试图提取两篇平行文档中句子的对齐，这两篇文档使用同一种语言，内容相同，但复杂程度不同。现在我们有了我们的数据集D-plane，它具有手动对齐的句子，我们可以将这些句子用作黄金标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了一些修改，并在论文中发表了所有这些修改和运行实验的代码。最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是mass align方法，您也可以在论文中找到运行该方法对您自己的文档的代码。\n\n我们在论文中展示的第二个用例是自动文本简化的案例，通过微调语言模型来从复杂输入文本生成简化文本。我们微调了两个不同的模型：我们微调了long import模型以进行文档级别的简化，我们还微调了normal base import模型以进行句子级别的简化。您也可以在论文中找到所有检查点，并可以了解更多关于我们实验的分数和评估指标的细节。我们得出结论，这种基本的微调可以产生或获得比基线分数更好的分数，并将这些结果作为未来自动文本简化问题的基准。\n\n非常感谢您的关注，我们希望在会议上见到大家。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是范大学的席源。我今天来介绍我们关于从大型语言模型中提取特定脚本知识，以用于受约束的语言规划方面的工作。在日常生活中，人类经常通过遵循分步骤的指示来规划他们的行动，这些指示以保证的脚本形式呈现。先前的研究探索了语言模型来规划抽象目标，例如制作蛋糕等刻板活动，并表明大型语言模型可以有效地将目标分解为步骤。然而，先前的研究主要集中于规划抽象目标的刻板活动。规划具有特定目标和特定约束的目标，例如制作巧克力蛋糕，仍然鲜为人知。在本文中，我们定义了受约束的语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以被不同的现实生活中的特定目标继承，这些目标具有更困难、多方面的约束。一个好的规划者应该编写符合约束条件且合理的脚本。在本文中，我们首先评估和改进大型语言模型受约束的语言规划能力。由于没有特定目标的可用数据集来支持我们的研究，我们必须首先获取这些目标。如表所示，我们使用指令式 TPT 扩展了具有多方面约束的抽象目标，以进行人工回路数据采集。我们抽取了100个特定目标，并评估了 Light Logic 模型生成的脚本。这张表报告了结果的总体准确性。我们发现，所有 Light Logic 模型在规划特定目标方面都取得了不令人满意的结果。然后，我们进行详细分析以调查 Light Logic 模型在哪些方面存在不足。图表结果显示，生成的脚本中的语义完整性是可以接受的。但无法保证对约束的忠实性。我们深入研究了更细粒度的约束主题类别，具体取决于唤醒家园。图中的头部地图显示，对于不同类别的目标，Instruct GPT 的规划性能差异很大。先前的研究表明，Larry 模型的输出质量存在高度方差，导致性能不佳。因此，我们采用了超生成 Zen 过滤器（overgenerated Zen filter）的想法来提高生成质量。我们首先展示了 Instruct GPT 的约束类型及其示例，并基于设定的抽象目标获得了特定目标。接下来，Instruct GPT 为特定目标生成超量关键脚本。然后，开发了一个过滤模型来选择符合要求的脚本。我们将脚本和目标转换为 Instruct GPT 的比特模式，并计算余弦相似度和相似度分数以衡量语义相似度。此外，我们还将编写包含目标约束关键词的脚本。仅当目标分数在目标站点中排名最高时，我们才会保留该脚本。通过我们的方法，Inslacity 可以生成更高质量的脚本。我们的方法在语义完整性和对约束的忠实性方面都大大提高了可规划性。由于大型语言模型的部署成本高昂，因此启用较小和专业化模型的语言规划能力至关重要。创建数据集是实现这一目标的关键一步。然而，先前的研究并没有实现对特定目标的规划，并且手动数据集注释成本高昂。因此，我们遵循知识蒸馏（symbolic knowledge distillation）的思想，将受约束的语言规划模型进行蒸馏。我们将我们的方法应用于构建一个名为 CodeScript 的受约束语言规划数据集。总共，我们生成了五万五千个特定目标和脚本，以确保验证和测试站点的质量。我们请云端众包工人查找和修改不正确的样本。该图显示了 CodeScript 的受约束分布。我们发现 CodeScript 表现出生成的特定目标的假设。借助 CodeScript，我们可以训练更小但专门的模型来进行受约束的语言规划。我们发现，在适合的数据站点上进行适当训练时，TFIL 函数在代码速率上可以生成比大多数大型语言模型更高质量的脚本，这表明较小的模型可以支持较大的模型。总之，我们建立了受约束的语言规划问题。我们评估了大型语言模型的受约束语言规划能力，并为大型语言模型开发了一种超生成过滤器方法。我们使用大型语言模型生成了一个高质量的脚本数据集 CodeScript，用于建设性的语言规划。我们希望 CodeScript 数据集可以成为推进语言规划研究的宝贵资源。感谢您的时间。请在我们的论文中了解更多 CodeScript 的细节。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Yanis Lavrack，我将向大家介绍我们在Dr. Berth项目上的工作，这是一种针对生物医学和临床领域，基于法语的强大预训练模型。本次演示中，我们将首先讨论医疗保健领域的语言建模。随后，我们将介绍我们文章的主要贡献。我们推出了第一个基于法语的生物医学模型，名为Dr. Berth，它基于Roberta，并在Natchios数据集上进行训练，该数据集是从网络爬取的医疗数据。我们还将介绍带有多种预训练设置和数据源的模型对比。然后，我们将展示我们在十一项任务上的结果，即法语中的11项生物医学和临床下游任务。最后，我们将总结实验并向您提供更多关于如何访问这些模型的信息。自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，与历史上的静态和情境化方法（如Word2Vec、Fastex或NWO）相比，能够提供巨大的性能提升。此后，该模型已被适配为许多其他语言，例如法语中的Camembert，以及生物医学领域中的PermetteBERT和BioBERT，以及临床领域中的Clinical BERT，但主要都是英文。针对其他语言的专业模型非常稀少，通常基于持续预训练，因为缺乏特定领域的训练数据。然而，在之前，法语并没有任何开源的生物医学模型。因此，我们自问一个问题：最合适的训练数据来源是什么，以便广泛应用？目前这些数据是否是临床数据的良好替代品？为了回答这个问题，我们将Dr. Berth与我们的Schubert模型进行比较，Schubert模型基于从非大学医院获得并进行匿名化处理的数据。随后，我们再问自己：为了在法语数据上训练一个专业模型，需要多少数据？是4 GB，8 GB还是更多？为了回答这个问题，我们首先从头开始训练并比较了四个模型：一个Dr. Berth的初始版本，使用7 GB的Natchez数据；另一个Dr. Berth的初始版本，使用4 GB的Natchez数据；一个Schubert的初始版本，即临床模型，使用从临床节点中提取的4 GB句子；以及一个最终的Schubert版本，混合了4 GB的Natchez和4 GB的临床节点。除了这个对比之外，我们还引入了三个基于持续预训练的模型，以分析预训练策略的影响。一个基于Camembert的权重并在4 GB的Natchez数据上进行训练；另一个也基于Camembert，但这次在4 GB的临床节点上进行训练；还有一个基于英文生物医学模型Bermud Bert，并在4 GB的Natchez数据上进行训练。总共，我们有七个模型。为了评估我们的七个模型，我们收集了公开和私有的下游任务，例如命名实体识别、文本分类、词性标注和问答。这些模型与六个基线模型进行比较，包括Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PubMedBERT、BioBERT和ClinicalBERT。评估结果表明，模型在与训练数据性质相同的数据集上表现最佳。然而，我们可以从这些数据中观察到，来自异质来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总体而言，从头开始的微调似乎在大多数任务上获得了更高的性能。然而，我们在持续微调中，使用PubMedBERT的权重和分词器，并在Natchez数据集的4 GB子集中进行训练的实验，显示出与从头开始训练的Dr. Berth 4 GB相当的结果，而基于Camembert权重和分词器的模型则存在稳定性问题。最后，作为结论，我们提出的系统在11项下游任务中表现优于9项，并总体上超越了通用模型Camembert的结果。我们还观察到，专业化的数据更好，更专业化的数据更好，但可扩展性不佳。所有从Natchios获得并进行预训练的模型都可在Hugging Face上免费获取，所有训练脚本都可在我们的GitHub仓库中找到。感谢各位… 感谢本次演示，我们期待在多伦多的会后环节与大家交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是项彬，是华盛顿大学的博士生。今天我将介绍我们从预训练数据到语言模型再到下游任务的工作，追踪政治偏见导致不公平自然语言处理模型（NLP）的路径。\n\n语言模型是在大规模的网络爬取数据上训练的。政治新闻媒体在其预训练数据中得到了充分的覆盖。根据 C four 语料库的调查显示，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、赫芬顿邮报等都覆盖在语言模型的训练数据中。这为语言模型应用带来了兼而得之的局面。一方面，它们能够从不同的视角中学习，这有助于庆祝民主和思想的多元性。另一方面，这些不同的政治观点本质上带有社会偏见，并可能导致下游任务应用中的潜在公平问题。\n\n为此，我们旨在研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，特别是通过提出以下问题。首先，我们如何评估语言模型的政治倾向？预训练数据在政治偏见中起什么作用？其次，政治倾向不同的语言模型在下游任务中的表现如何？这是否会导致自然语言处理应用中的公平问题？\n\n具体而言，我们首先建议使用政治问卷（如政治指南针测试）的不同提示格式来提示语言模型。这确保了我们能够进行自动评估，并以扎根于政治科学文献的方式进行。初步结果表明，首先，语言模型确实具有不同的政治含义。它们占据政治指南针上的所有四个象限。我们可以看到，GPT-4 是其中最自由主义的语言模型，而 GPT-3 则通常比 BERT 以及其变体更具社会自由主义倾向。\n\n其次，我们旨在调查语言模型的政治偏见在多大程度上实际上是从训练数据中获得的。为此，我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点，这些语料库被分为新闻和社交媒体，并进一步划分为其政治倾向，来进行一项控制实验。通过在这些党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生偏移。例如，对于在左派倾向的 Reddit 语料库上进一步微调、进一步训练的 Roberta，我们可以看到在政治偏见方面存在显著的自由主义转变。\n\n为了调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象，我们将预训练语料库划分为美国第 45 任总统之前和之后的两个时期。我们分别在两个不同的时间段语料库上预训练语言模型。我们可以看到，语言模型通常在 2017 年之后具有远离中心的政治倾向。这表明语言模型也可以捕捉到我们社会中的极化现象。\n\n最后，我们使用具有不同政治倾向的语言模型评估仇恨言论检测和虚假新闻检测，这些都是经常涉及语言模型的自然语言处理应用，并且可能具有非常重要的意义。我们发现，如果我们在按类别进行调查，即在不同的人口统计或新闻媒体的政治含义中分离出性能，我们可以看到一个模式：例如，在仇恨言论检测中，左派倾向的语言模型更擅长检测针对社会少数群体的仇恨言论，但对检测针对我们社会中更强大群体的仇恨言论表现较差。相反，右派倾向的语言模型更擅长检测针对白人和男性的仇恨言论，但对检测针对黑人、 LGBTQ+ 以及其他少数群体的仇恨言论表现较差。类似的趋势也发生在虚假新闻检测中，我们发现左派倾向的语言模型更擅长检测来自其相反政治观点的虚假信息，反之亦然。\n\n我们将提供许多定性示例，以证明具有不同政治含义的语言模型确实根据其社会类别给出不同的仇恨言论和虚假信息示例预测。附录中有更多示例，以进一步强调这一点。这表明存在一个非常紧迫的与语言模型的政治偏见相关的公平问题。例如，如果将右派倾向的语言模型微调用于仇恨言论或虚假信息或其他用途，并将其部署到流行的社交媒体平台，这将意味着具有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会毫无控制地蔓延。\n\n这为我们敲响了警钟，以承认并解决源于语言模型政治偏见而产生的公平问题。\n\n稍作讨论。我们还希望强调我们揭示了关于语言模型政治偏见的独特困境。就像斯库拉和卡律布狄斯之间一样。如果我们不清理语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们尝试以某种方式进行清理，我们也可能会冒着审查或排斥的风险，并且很难确定什么是真正中立的，应该保留在语言模型训练数据中。这有点像电控查理问题。\n\n好了，我想我今天就到此为止。感谢各位的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是新浪的 Coast。很高兴欢迎各位参加我们关于 ACL 2023 论文“语言模型可接受性判断并非总是对上下文稳健”的讨论。这是我和 John Bakhier、Aaron Mueller、Kanishka Mishra、Karen Fentus、Roger Levy 和 Adina Williams 共同完成的工作。\n\n在这项工作中，我们重新审视了最小对（minimal pair）范式。最小对范式基本上是根据语言模型对可接受性判断进行评估，这也可以包括语法性，例如来自 Blimp 语法宝石或关于刻板印象的可接受性，如 CrowdSpars。在最小对范式中，典型的评估方法是展示一个可接受或语法正确的句子，然后展示一个不可接受或语法错误的句子，希望模型能够将更高的概率赋予可接受的句子。目前 MPP 流程基本不允许我们评估模型对更长句子的可接受性。\n\n如今，大型语言模型正在产生越来越长的上下文窗口。因此，至关重要的是我们必须在整个上下文窗口中评估模型的可接受性。而这就是我们在这里试图做的事情：我们试图通过让模型评估更长、更长的序列来重新审视 MPP 流程。 这就是我们的方法。\n\n我们的做法是，为了模拟这些更长的序列，我们重新审视数据集本身，然后从这些数据集中选择可接受或不可接受的句子来重建句子。例如，这里我们选择了来自 Blimp 数据集、附例岛案例中的一个典型的语法性对。为了重建可接受且具有相同语法结构的更长序列，我们从附例岛中提取语法正确的句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。 我们可以通过选择相同的匹配中的不可接受句子来做同样的事情，这也可以用来测试模型的可接受性。\n\n我们还可以通过选择来自不同子集或不同数据集的句子来做同样的事情。这就是我们称之为不匹配场景。因此，这里的句子仍然来自相关数据集，但不是您评估所使用的相同数据集。 我们可以对不可接受性的情况做同样的事情。\n\n最后，我们可以从完全不相关的领域，例如维基百科中选择句子。 这将告诉我们模型的接受性判断是否实际上会受到任何上下文的影响，例如，上下文是否来自数据集的不同子集，或者它与我们正在查看的句子完全无关。\n\n那么，模型的表现如何呢？ 首先，我们查看维基百科中的句子，这些句子与当前的查询对完全无关。在那里，我们发现 MPP 判断在任意上下文长度下大多是稳健的。我们增加了上下文长度，达到 1024，以最大化 OPT 和 GPT2 模型的性能，我们在橙色虚线中看到了，MPP 判断相对稳定。\n\n那么，当选择来自相同数据集的句子时会发生什么？ 这里，我们是从相同的 Blimp 或语法宝石数据集中的可接受和不可接受领域创建句子。在那里，我们看到当添加可接受的前缀或不可接受的前缀时，MPP 判断会显著增加或减少。但当匹配结构时，即当我们从 Blimp 或语法宝石中的相同现象中选择句子时，我们会看到 MPP 判断对于模型会大幅增加或减少，这取决于所选前缀是否可接受或不可接受。\n\n现在，这种影响非常大，这种效应会随着上下文长度的增加而增强，这可能会影响具有大上下文窗口的新型语言模型。\n\n为什么匹配前缀会如此影响语言模型的判断？ 我们进行了一系列分析，试图通过在输入句子中添加噪声，同时尝试保留相关的结构来扰动输入句子。 在进行多次这样的扰动之后，我们发现这些噪声中的任何一种都无法使模型改变其 MPP 判断趋势的方式。 基本上，我们发现模型对扰动后的句子表现出相似的方式：当我们在可接受的领域中扰动句子时，我们会看到 MPP 判断所有扰动都出现相似的增加，当我们在可接受的领域中扰动句子时，我们会看到 MPP 判断以相似的方式减少。\n\n我们工作的关键结论是，语言模型对跨句共享的潜在句法和语义特征敏感。目前我们以短句和单句输入的方式进行的 MPP 评估，可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。\n\n请阅读我们的论文以了解我们实验的更多细节。 谢谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Dawe，来自德国斯塔兰特大学的博士生。 在这个视频中，我想介绍我们最近的一项工作，名为《弱于你想象》，对每周监督学习进行批判性审视。 这是一项与肖宇辰、Maios Musbach、Giaz Steffen 和 Dietrich Clarkov 合作完成的工作。 我想先简要介绍一下周监督和每周监督学习。 在周监督中，我们不手动标注数据。 相反，我们使用周标签源进行标注，例如简单的启发式规则、知识库或局部众包，如图右侧所示。 相比于人工标注，这些较弱的标注成本要低得多，但它们也带有噪声，这意味着一部分标注是错误的。 如果我们直接在每周标签数据上训练神经网络，这些网络往往会记住标签噪声，而无法泛化。 在每周监督学习中，提出训练算法，以鲁棒的方式在这些标签噪声上训练神经网络，以便训练的模型仍然能够良好泛化。 在最近的WSL（每周监督学习）工作中，一种常见的说法是，人们声称他们仅在每周标签数据上训练模型，并在干净的测试集上获得高性能。 严格来说，这种说法并不错误，但有一个问题，那就是人们假设可以获得额外的干净验证集用于模型选择。 我们对这种问题设置表示怀疑，因为它意味着每周监督学习需要额外的手动标注。 但就像房间里的大象一样，这种必要性往往被忽视。 上述的怀疑促使我们提出了三个研究问题。 首先，干净的验证数据对WSL是否必要？ 或者我们可以使用嘈杂的验证集吗？ 其次，如果干净的数据是WSL能够工作的必要条件，那么我们需要多少干净样本？ 最后，我们是否应该仅使用干净样本进行验证，或者是否有更好的利用它们的方法？ 我们在我们的工作中解决了这些研究问题，并得出了以下结论。 首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。 否则，性能会大幅下降。 正如这个图所示，如果没有干净的验证样本，训练的模型就无法泛化到原始的弱标签之外，这意味着训练是没有意义的。 这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。 我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如图左侧所示。 通常，我们只需要每个类别 20 个样本就可以获得高性能。 但这并非故事的结局，因为无论我们如何获取干净样本，直接在它们上训练甚至可以实现更好的性能。 红色的图表显示了直接应用于干净数据的微调方法和仅使用干净数据进行验证的WSL方法的性能差异。 我们可以看到，如果我们有每个类别 10 个样本，直接微调就开始超越WSL方法。 最后，之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调来实现。 正如从图中可以看出，最初性能低于更复杂的WSL方法（如余弦）的Berliner模型（FTW）如果在干净样本上允许继续微调，其性能将与其它方法持平。 因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。 总而言之，我们表明最近的WSL方法需要干净的手动标注样本才能正常工作。 它们的性能收益和实用性被严重高估。 我们对未来工作的具体建议如下。 首先，报告模型选择是使用干净验证样本完成的。 其次，WSL方法应该与未来学习基线进行比较，因为两者都使用干净样本。 第三，持续微调是一种简单而强大的基线，应该在未来的WSL工作中考虑。 最后，我们已经开源了我们的代码。 您可以通过幻灯片上的二维码找到它。 欢迎您查看。 谢谢，祝您会议愉快。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫Ayud Villar，今天我将对题为《基于翻译评估策略和表现的提示 Palm》的论文做一个简短的概述。这篇论文是与谷歌翻译的同事们共同完成的。Palm 是一个拥有 5400 亿参数的大型语言模型，于去年 2022 年发布。它在包含 7800 亿个 token 的大量文本语料库上进行了训练。在发表时，它在数百个 NLP 任务中达到了最先进水平。在这项工作中，我们提出了对大型语言模型在机器翻译中的提示进行首次系统性研究。我们使用 AMT 社区的最佳实践来评估这些模型在翻译方面的能力。 这包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠，并比较两个最先进的系统，即 WMT 评估中表现最佳的系统。 我们使用最先进的神经机器翻译指标，并额外展示了基于专家的人工评估结果。最后，我们提供了一些提示选择策略的建议。 提示对大型语言模型在翻译方面的表现有很大的影响，正如我们在一个简单的实验中看到的那样，我们使用了简短的提示，并为单个句子提供了两个不同的提示。在 1000 个句子中，有 516 个句子的差异超过了一个 blur point。在极端情况下，这种差异甚至可以达到 40 个 blur points。因此，选择好的提示策略非常重要。在我们的实验中，我们采用了五段提示策略，即我们只是用语言标记提供给系统的每个句子。例如，在这个从德语翻译成英语的例子中，德语句子（源句子）被标记为“德语冒号”，而英语翻译则被标记为“英语冒号”。我们发现，在几种短提示的情况下，提示的实际形式没有很大的影响。对于零段和一段提示来说，这至关重要，但当我们像我们一样转向五段提示时，提示的实际形式几乎没有差异。例子发挥了最大的作用。 我们的实验结果总结是，示例质量比与源句子相似度更重要。因此，选择来自高质量翻译的示例非常重要。 尤其，我们将从 WMT 评估的训练数据或 dev 数据中选择提示进行比较。dev 数据比训练数据经过更严格的筛选和具有更高的质量，并且结果表明使用 dev 数据可以获得更好的性能。 然而，专业的、基于 ODR 的系统在很大程度上优于 Palm 的翻译，但 Palm 已经非常接近商业系统。 在我们的案例中，我们选择使用谷歌翻译进行评估。 我们使用 MQM 框架进行的人工评估获得的洞察是，Palm 的流畅度与最先进的系统相当，但主要的差异在于准确性。 尤其，最常见的错误是遗漏错误。 因此，看起来 Palm 有时会通过省略句子中的部分内容来生成更好的句子翻译。 然而，PAM 的风格输出类别低于最先进的系统，这是一个额外的信号，表明 PAM 确实提供了非常流畅的输出，但仍然存在准确性问题。 好了，这就是这个简短概述的全部内容。 如需更多详细信息，请参阅论文的完整演示。 非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫景伟，来自中国科学技术大学。很高兴为大家呈现我们论文的短视频，论文题目是《你是否在抄袭我的模型？——大型语言模型嵌入与服务的版权保护》。我们将介绍一种称为“后门水印”的方法。首先，我们来介绍一下嵌入与服务的背景。目前，像GPT、Llama、PELM等大型语言模型在自然语言理解和生成方面表现出色。嵌入与服务是建立在大型语言模型之上的服务之一，旨在辅助各种自然语言处理任务。例如，OpenAI提供基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来盗窃模型，并提供类似的服务。因此，保护嵌入作为服务的版权势在必行。为了保护嵌入作为服务的版权，一种解决方案是在服务提供商的服务中嵌入水印，并检测其他服务是否包含水印。这种方法需要满足以下几个特性：首先，该方法应该适用于嵌入与服务。其次，水印不应降低所提供的嵌入的效用。第三，水印应该足够隐蔽，使得攻击者难以察觉，或者攻击者可以轻松移除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有的工作大致可以分为四类，但这些方法要么不适用于嵌入与服务，要么缺乏可迁移性。因此，在本文中，我们提出了嵌入标记（Embedding Marker），这是一种基于后门的、适用于嵌入与服务的水印方法。现在，我来介绍一下我们嵌入标记的具体细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在适中频率区间内的词语。假设服务提供商可以收集一个通用的文本语料库，并统计其中每个词语的频率。在水印注入过程中，我们首先定义一个目标嵌入。当用户将一个句子发送到服务提供商时，提供商会计算句子中触发词的数量。提供的嵌入是目标嵌入和原始嵌入的加权和，其中目标嵌入的权重与句子中触发词的数量成正比。当句子中触发词的数量大于M时，提供的嵌入将完全等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词语都属于触发集的句子，而良性数据集包含所有词语都不属于触发集的句子。然后，服务提供商使用数据集向盗窃者服务请求嵌入。计算请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算在后门数据集和良性数据集上两者之间的相似度差异，定义为delta cosine和delta L2。同时，我们还应用KS检验，并使用其p值作为第三个指标。我们对四个数据集（AG News、Mind、SSD two和Erospam）进行了实验。我们假设提供商使用Wiki Text数据集来计算词语频率。在四个数据集上的结果表明，我们的嵌入标记可以实现出色的检测性能，同时保持下游任务的效用。我们还通过可视化句子展开的嵌入（如使用t-SNE）来验证所提供的嵌入的隐蔽性。图中的图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。以上就是全部内容，谢谢大家。欢迎与我们交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "各位好，我叫 Ying，我的同事 Jian 和我将为大家介绍我们关于多指令调优，通过指令微调提升零样本多模态学习的研究。 随着大型语言模型的进步，许多工作开始探索新的学习范式，利用预训练语言模型以高效的参数和数据方式执行不同的下游任务。最近，许多研究表明，指令调优能够使大型语言模型遵循自然指令，以零样本方式执行未见过的任务。然而，以往的大部分指令调优工作都集中于提升语言任务的零样本性能，而忽略了计算机视觉和多模态任务。因此，在这项工作中，我们旨在研究对多模态预训练模型进行指令调优是否能够真正提升其对未见过的多模态任务的泛化能力。此外，在我们的研究期间，我们发现语言任务和多模态任务之间存在指令数据集可用性的显著差异。语言任务存在一千六百多个指令任务，但没有大规模的公开可用的多模态指令任务。这促使我们构建了一个多模态指令调优数据集。我们在此介绍 MultiInstruct，这是第一个多模态指令调优基准数据集，由 62 个多样化的多模态任务组成，涵盖 10 个大类。这些任务来源于 21 个现有的开源数据集，并且每个任务都配备了五条专家编写的指令。为了在我们的提出的数据集上研究多模态指令调优，我们选择 OFA 作为基模型，OFA 统一了多模态模式模型。OFA 使用统一的词汇表来表示语言、图像 token 以及边界框的坐标。在这里，我们展示了来自我们 MultiInstruct 数据集的一些示例实例。为了统一各种输入和输出数据类型的处理，我们遵循 OFA 的方法，将所有任务都以统一的序列到序列格式进行表述，其中文本、图像、指令和边界框都表示为相同的 token 空间。好的，现在我将介绍多模态指令调优。对于训练数据集，我们使用 NIG 组中的 53 个任务进行训练，并对每个任务抽取 10,000 个样本。对于测试，我们保留了 Common Sense Reasoning 组进行测试，并从 WQA 和 Miscellaneous 组中选择另外五个任务。我们使用每个任务的测试集中的所有样本。此外，我们随机从 NIG 指令的测试集中的 20 个任务中抽取作为 NLP 的 SIN 任务。我们使用预训练的 OFA 大模型作为基模型。在训练期间，我们将所有任务的所有样本混合在一起。每个样本会随机与它五个指令模板中的一个进行组合。在测试期间，对于每个任务，我们进行总共五次实验，通过使用五个指令中的一个来评估模型。我们报告所有五个实验中性能的平均值和最大值，以及标准差。如果任务是一个多模态分类任务，我们将报告准确率。如果是多模态生成任务，我们将报告 ROUGE。对于 RP 任务，我们也报告 ROUGE。我们还引入了一个额外的评估指标，称为灵敏度。该指标衡量模型在指令措辞略有变化的情况下，是否能够始终如一地产生相同输出的能力。以下是我们的主要结果。我们可以看到，指令调优可以显著提升 OFA 在未见过的多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以受益于指令调优。我们可以看到，随着任务数量的增加，模型的性能得到提升，同时灵敏度降低。我们还进行了一项实验，比较使用一条指令和五条指令的效果。可以看到，使用更多的指令可以改善模型的整体性能并大大降低其灵敏度，这表明不同的微调策略对模型灵敏度的影响。我们可以看到，从自然指令数据集进行迁移学习，模型可以实现比原始 OFA 模型更高的灵敏度。我们还可以看到，从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上实现更好的性能。总而言之，我们提出了第一个大规模的多模态指令调优数据集。我们显著提升了 OFA 的 DAROCHOT 能力，并探索了不同的迁移学习技术并展示了它们的优势。我们设计了一个新的指标，称为灵敏度。另外一点，我们正在收集一个更大的多模态指令调优数据集，包含大约 150 个额外的变体语言任务，并将发布它们。这是我们数据集和模型的二维码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的张 Yusuf。今天我将为大家介绍我们的工作，Exampler，一个用于多种自然语言和语义表示的多语言语义解析框架。语义解析的任务是构建用户查询的语义表示，例如 SQL 和 Lambda 微积分。而多语言语义解析的任务是将多种自然语言的查询翻译成多种语义表示。如图所示，我们需要使用神经网络模型将多种自然语言的查询翻译成 SQL、Lambda、FunQL 等。现有的多语言语义解析模型通常是独立提出的，并在有限的任务和应用的数据集上进行评估。例如，某些自然语言的覆盖范围存在不足，中文缺失，某些语义表示的覆盖范围也存在不足，Lambda 微积分缺失，或者它们仅在特定的较新模型上进行评估。例如，只有一个模型用于评估。因此，我们提出了 Exampler，提供了一个统一的数据集 Exampler，用于多种自然语言和语义表示的多语言语义解析。它包含九个数据集，涵盖各种领域，五个语义解析任务，八种语义表示，以及来自 15 个语系中的 22 种自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是 TranslateTest。我们使用 Google 翻译 API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语模型上训练...训练英语查询，在推理时，我们将德语查询使用 API 翻译成英语，然后使用训练好的模型预测 SQL。我们还测试了单语模型。在这种设置下，源语言和目标语言相同，例如德语到德语或英语到英语。我们还测试了单语融合设置，通过仅使用 10% 的训练数据训练单语模型。我们还测试了单语多语模型，我们为所有语言训练一个多语模型。例如，我们将德语、英语和中文查询放在一起训练一个多语模型。在推理时，我们可以使用该模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一种源语言上进行训练，然后迁移到另一种语言。因此，在训练时，我们使用英语查询或英语和德语少样本查询的组合进行训练，以训练一个多语模型并预测 SQL 输出。我们还发现了很多有趣的发现。关于单语模型的分析，我们在两组模型上进行评估，包括多语言预训练编码器与指针解码器 (Encoder PDR)，例如 XLMR-PDR 和 BERT-PDR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如 MBART 和 MT5。我们发现编码器-解码器模型在所有九个数据集上都获得了最佳性能。我们在多语言设置下评估了 MT5 和 XLMR-PDR。我们发现编码器-解码器或编码器 PDR 可以通过在各种语言的混合中进行训练来得到改进。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语性能在七个数据集上会下降，仅在三个数据集上获得提升。我认为这被称为多语性的曲线。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言 Fuchsia 迁移，橙线是跨语言零样本迁移，而绿线是单语设置。我们发现通过比较绿线和橙线，我们发现对于零样本设置，跨语言迁移性能差距非常大。通过比较蓝线和橙线，我们发现对于 Fuchsia 设置，迁移差距迅速缩短。我们还发现了一些其他的有趣发现。例如，编码器-解码器模型优于之前的研究，或取得了可比的结果。在自然语言英语上进行微调可以显著提升目标自然语言的少样本性能。我们还发现像 Codice 和 Bloom 这样的多语言语言模型对于跨语言语义解析任务仍然不足。总而言之，我们构建了 Exampler，一个用于跨语言语义解析的统一基准，包含多种自然语言和许多表示。我们对三种具有代表性的多语言语言模型进行了全面的基准研究。我们的结果显示了很多有趣的发现等等。欢迎访问我们的论文和代码。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫亚当·斯皮拉科夫斯基，本次演讲是关于并列结构的依存关系。正如大家可能知道的，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存关系中，并列结构的“丽莎、巴特和玛姬”的结构，是第一个连词作为整个并列结构的头。在这种情况下，头是丽莎。伊戈尔·米尔丘克的语义文本理论也假设了类似的结构，同样将整个并列结构的头设为第一个连词。因此，这两种方法都是不对称的，它们突出显示了其中一个连词。当然，也有对称的并列结构方法，例如布拉格方法，以及布拉格依存树库中假设的连词作为头的结构，在这种结构中，所有连词都有从属和指向。最终，还有一种多头方法，例如在卡森的词法语法中使用的，所有连词都作为并列结构的头，因此我们得到从词主语到所有连词的依存关系，例如巴特和玛姬。现在，本文的目的是对这些并列结构的对称性提出一种新的论证，反驳这些并列结构的非对称性。这个论证基于依存长度最小化的原则，我将通过这些例子来解释。在英语中，正如大家可能知道的，直接宾语倾向于靠近动词，而状语可能离得更远，对吗？“马修昨天读了它”是没问题的，因为直接宾语“它”靠近动词；而“马修读了昨天它”就不好听了，因为动词和直接宾语之间有一个状语“昨天”。然而，当直接宾语非常长且很重时，这种影响可能会得到缓解，因为它可以移动到状语之后。这在这里进行了说明。因此，这两个句子都是可以的。“马修读了这本书，这本书讲述了关于蜜蜂的绝对迷人的故事，昨天”是没问题的，因为我们用这个长短语代替了“它”。说“马修昨天读了这本书，这本书讲述了关于蜜蜂的绝对迷人的故事”也是可以的。这里的推理是，尽管这句话违反了直接宾语应该紧挨着动词这一普遍语法原则，但它满足了依存长度最小化的原则，该原则规定较短的依存关系是首选的。这两个树状图只显示了这些两种结构中关键依存关系的长度。因此，这里有一个从“读”到状语的依存关系，长度为7个单词，从“读”到“书”的依存关系长度为4个单词，总共是11。当你移动、交换这两个构成部分时，这两个依存关系的长度之和变成了6，对吗？从11变成6，更短了，这就是为什么听起来还算可以，它违反了一个原则，但满足了另一个原则。那么，我们提取了从增强版宾州树库中获得的关于并列结构的各种统计数据，详情请参见本文，并解释了我们为何未使用通用依存关系。这些统计数据证实了之前多次提出的观察结果，即左侧连词往往更短。“盐和胡椒”而不是“胡椒和盐”，以音节为单位进行测量。并且还观察到，当长度差异增大时，这种趋势会增强。也就是说，当两个连词的长度差异增大时，较短的连词更倾向于放在前面，比例更大。但本文的新颖之处在于，我们观察到这种趋势仅在词主语在左侧或缺失时才会发生。例如，在“我看到了巴特和丽莎”中，词主语在左侧。在“霍默来了又打了个喷嚏”中，词主语缺失，这里是两个动词的并列，没有外部的词主语。在这些情况下，左侧连词更倾向于更短，长度差异越大，情况越明显。然而，当词主语在右侧时，例如“泰德和内特”，这种影响就会消失。通过测量字符数、音节数和单词数，我们证明了这一点。我将重点关注右侧的列。我们可以看到，当词主语在左侧时，左侧连词更短的趋势随着单词数的绝对差异而稳定增长，当没有词主语时，也观察到类似的情况，例如在句子并列中。但是，当词主语在右侧时，这种趋势就会消失，我们在本文中展示了这一点，这为反对并列结构的非对称性以及支持并列结构的对称性提供了一个论证。请参阅本文以获取完整的协议和论证，并在会后与我们交流。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫Kyo Yin，我将为大家介绍我们的工作，题目是《翻译何时需要语境？——基于数据的多语种探索》。这项工作是与 Patrick Fernandes、Emily Liu、Andre FD Martins 和 Graham Newbig 合作完成的。许多翻译都依赖于语境。例如，在这一句中，我们应该如何翻译“mole”？嗯，如果前一句是“如果部长们发现了，情况可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，情况严重吗？”那么“mole”指的是痣。因此，根据语境的不同，词语的含义会发生变化，翻译也随之改变。然而，评估模型在处理此类案例时的表现相当困难。首先，只有一小部分翻译依赖于语境，这使得像 Blue 这样的语料库级别的指标无法捕捉到这些翻译。有些人建议对依赖语境的翻译进行有针对性的评估，但这些资源仅支持有限类型的依赖语境的翻译，并且支持的语言种类也有限，因为它们通常依赖于领域知识和人工策展。在这项工作中，我们试图回答这两个问题。首先，翻译何时需要语境？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量了单词在翻译过程中对语境的依赖程度。在之前的研究中，我们引入了 CXMI 作为衡量机器翻译模型语境使用情况的指标。这通过测量语境 C 提供的关于目标 Y 的信息量，给定源 X 来完成。您可以将 CXMI 视为给予模型语境所获得的信息。在这项工作中，我们将 CXMI 扩展到 pointwise CXMI，它可以衡量句子级别或单词级别的语境使用情况。我们可以将具有高 PCXMI 的单词视为需要语境才能进行翻译的单词。现在我们分析具有高 PCXMI 的单词，以寻找这些单词之间的模式。我们对 TED Talks 的英文到十四种不同语言的翻译文本进行分析。我们分别在三个不同的层面进行分析。首先，我们查看具有较高平均 PCXMI 的词性标签。这使我们能够发现，例如，阿拉伯语中存在相对较高 p six mi 的双重代词。这可以通过以下事实来解释：英语没有双重代词。因此，在翻译成阿拉伯语时，您需要语境来确定代词是否为双重。类似地，我们发现某些语言在选择适当的动词形式时也需要语境。然后我们查看在所有不同出现次数的平均值下，具有高 p six mi 的词汇项目。这有助于我们识别出类似于此处的情况，在中文中，您需要语境才能正确翻译。最后，我们发现语境对于以正确的正式程度进行翻译至关重要。最后，我们查看不同的单个标记，这些标记具有高 p6mi。这使我们能够识别无法真正通过单词本身捕捉到的现象，而是通过句子结构表达的现象，例如省略解析。现在，我们使用分析结果来设计文档级别翻译的基准。对于我们识别的五种话语现象，我们创建标记器，以自动识别与该现象相关的单词，我们称我们的标记器为多语种语料库感知标记器或 MUDA 标记器。然后，我们还可以注意到，不同的语言具有不同比例的话语现象。然后，我们使用 MUDA 标记器，将标记器应用于我们想要用于评估的平行语料库，并在 MUDA 标记器已识别的依赖语境的示例上应用我们选择的翻译指标。最后，我们使用我们的基准以及其他指标来评估不同模型在文档级别机器翻译方面的表现。首先，当我们使用语料库级别的指标时，例如 Blue，我们发现无语境模型表现最佳，但如果使用 Comet，则上下文感知模型表现最佳。如果使用 WordF 测量，则有或没有语境的模型表现可比。这再次证明，如果我们仅使用语料库级别的指标，很难确定最佳的文档级别翻译系统。现在我们使用 MUDA 基准来评估模型，我们发现，对于某些话语现象（例如正式程度和词汇连贯性），上下文感知模型比不使用语境的模型更准确。但是，对于其他现象（例如省略、代词和动词形式），这些模型并没有比不使用语境的模型好多少。这表明我们需要在文档级别翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准表明，DPL 通常比 Google 翻译更准确，用于文档级别翻译。总之，我们对十四种语言对进行了基于数据的分析，以确定翻译何时需要语境。然后，我们使用我们的发现来构建文档级别机器翻译的基准，这可以帮助我们识别哪些离散现象模型可以处理得很好或不能很好地处理，以及哪些翻译系统擅长文档级别翻译。非常感谢大家的关注。明天见。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Jenny，卡耐基梅隆大学一年级的博士生，今天我将为大家介绍我们的工作，题为《位置性分析：刻画设计偏差，β集与模型》。这项工作是与华盛顿大学和人工智能研究所的一些同事合作完成的，主要包括Sebastian Santi、Ronin Lebras、Katarina Reinicke和Martin Sapp。\n\n现在，让我们设想一下，您正在为报纸工作，需要筛选新闻文章下的评论，以移除有毒内容。您可能会使用Perspective API之类的流行API进行毒性检测，如果您的目标是Carl Jones，这会非常有效，因为Perspective API能够准确地检测到有毒内容。但对于Dithya Sharma来说，情况并非如此，Perspective API对在印度语境中更常见的冒犯性用语的敏感度较低。这是一个设计偏差的例子，我们观察到技术在不同人群中的系统性性能差异。\n\n这种设计偏差，就像我们刚才看到的偏差，可能源于NLP研究人员和模型开发者“位置性”的影响。位置性简单来说，是人们由于其人口统计特征、身份认同和生活经历而形成的视角。这是一个在批判研究中广泛使用的概念，尤其是在女性主义和酷儿学术领域。作为一名研究人员，位置性会影响研究过程及其结果，因为它会改变研究人员的决策。\n\n那么，一个可能提出的问题是：数据集和模型是否具有位置性？我们并非试图断言模型和数据集本身具有人口统计特征和生活经历，但它们确实聚合了真实人们的判断和意见，因此可能代表其他群体之外的特定位置性。\n\n先前的研究已经提出了一些关于位置性的轶事证据，例如模型和数据集中的文化差异，以及对模型位置性的理论定义。然而，这些研究并没有将最终用户与数据集和模型本身进行比较。\n\n研究模型和数据集的位置性越来越重要，因为NLP任务变得更加主观和面向社会。由于并非所有决策都有记录，并且许多模型隐藏在API后面，因此很难描述这些位置性是如何扭曲的。\n\n为了研究数据集和模型的位置性，我们实际上将真实用户批注与现有数据集和模型进行比较。我们通过我们的框架NLPositionality来实现这一目标。\n\n我们的框架主要分为两个步骤。第一步是使用多元批注者重新批注数据集。我们选择这样做，而不是查看原始数据集和批注者的统计信息，因为通常每个实例只有少数批注者进行批注，而且统计信息很少被收集和共享。因此，我们选择重新批注数据，以获得每个实例的多个批注者，并获得丰富的统计数据。\n\n然后，我们根据统计数据对批注进行分析，并使用 Parsons R 相关性得分与模型和数据集进行比较。因此，我们的框架不同于批注者意见不一致的文献，它将最终用户与模型和数据集的预测和标签进行比较，而不是仅关注批注者的一致性或建模批注者分布。\n\n我们的框架在很大程度上得益于Lab in the Wild，这是一个在线实验平台，我们可以在其中招募各种各样的志愿者，与MTurk平台相比，后者主要有来自美国或印度的参与者。此外，Lab in the Wild仍然能够获取高质量的数据。\n\n我们在Lab in the Wild上托管了两个任务，其中一个就是社会可接受度。其工作方式是，参与者将阅读来自社会化学数据集的场景，然后他们将写下该场景的社会可接受程度。之后，为了保持对研究的参与度，他们可以将其回复与人工智能和其他人进行比较。\n\n然后，我们将这些批注与Social Chemistry Delphi和GPT 4进行比较。我们还针对毒性和仇恨言论检测任务，复制了一个非常相似的设置，他们将从DynaHate中阅读一个实例，并判断它是否是仇恨言论的一个实例。然后，我们将这些批注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT 4进行比较。\n\n最终，我们的研究积累了来自八十七个国家/地区的上千名批注者提供的超过一万六千个批注。\n\n现在，我们更善于回答“NLP数据集和模型与谁最一致？”的问题。我们发现NLP中存在位置性。例如，我们发现数据集和模型与英语国家最为一致。对于GPT 4社会可接受度分析，我们发现它与儒家文化和英语国家最为一致。我们还发现DynaHate也与英语国家最为一致。我们还发现与受过大学教育的人群存在额外的联系。对于GPT 4在社会可接受度任务中，我们发现它与受过大学或研究生教育的人群最为一致。我们还发现DynaHate与受过大学教育的人群存在相同的情况。\n\n然而，当模型和数据集与特定人群一致时，一些人群不可避免地会被遗忘。一个例子是，数据集和模型与非二元性别的人群相比，与他们的男性和女性同伴联系较少。我们在GPT 4社会可接受度任务以及DynaHate任务分析中都发现了这一点。\n\n鉴于NLP中存在位置性，我们该怎么办？因此，我们有一些建议。第一条是记录研究过程中所有相关的设计选择。第二条是通过视角主义的视角进行NLP研究。\n\n我们的第三条建议是为特定的社区构建专门的数据集和模型。一个很好的例子是Masakane倡议。\n\n我们想强调的是，具有包容性的NLP不仅仅是让所有技术为每个人服务。\n\n最后，感谢您的聆听。如果您想了解更多信息，请随时查看我们的仪表板，以获取最新的分析结果和我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我将介绍我们关于解决实体选择中的间接指代表达式的研究，其中我们引入了altentity语料库。我的名字是Javot Hosseini，这篇论文是与Philip Radlinsky、Silvia Pareti和Annie Luis合作完成的。我们的目标是理解用户在需要做出选择时使用的语言。考虑以下替代问题：你是指《Easy on Me》还是《I Got a Feeling》？这里用户想要在其中两个歌曲之间进行选择。最明显的方法是使用直接引用，例如说歌曲的名称《Easy on Me》或者其位置，比如第一个，但有时间接引用更适合进行更自然的对话。这种情况可能发生在用户无法记住歌曲的名称，或者发音过于相似难以区分，或者用户想要表达偏好。以下是一些间接引用的例子。例如，“较新的那首”或“不是充满活力的那首歌”。这是一个在对话系统中以及用于基准测试大型语言模型（LLM）实体理解中的重要问题。我们目前没有发现任何公开的、大规模的用于此任务的数据集，因此我们使用众包标注收集了一个数据集。我们的数据集涵盖了音乐、书籍和食谱三个不同的领域。我们的数据集收集方法强调非正式性，使用了卡通完成的形式。卡通中包含三个对话框。在第一个对话框中，Bob说：“还记得我们昨天听的那首歌吗？” 这样，Bob就设置了对话语境。在第二个对话框中，Alice说：“你是指《Easy on Me》还是《I Got a Feeling》？” 这是替代问题。在第三个对话框中，Bob使用间接引用来选择其中一个实体，例如，“带有钢琴音乐的那首”。我们自动提供第一个和第二个对话框，但第三个对话框由标注员填写。第一个对话框从每个领域的手动提示中选择。第二个对话框，即替代问题，按照以下方式生成：我们始终使用简单的模板：“你是指A还是B？” 其中A和B从维基百科中随机抽取。以下是我们使用的不同抽样方法。当我们列表中的条目越高时，实体之间的相似性就越高，通常更难进行消歧。第一种方法是均匀随机抽样；第二种方法是当实体具有相似标题时，例如两个名称为“Return”的书籍；第三种方法是当它们在维基百科上具有相似描述时；最后，当它们在维基百科上具有相似的信息框或属性时，例如对于一首歌曲来说，是同一流派或同一艺术家。当我们向标注员展示这个替代问题时，他们知道这些实体的名称，但他们不一定了解这些实体。所以，我们向他们展示有关这两个实体的背景知识。对于歌曲，我们只需向他们展示每首歌曲的Google搜索链接，然后要求标注员至少听取每首歌曲的一部分并阅读关于每首歌曲的信息。以下是《Easy on Me》的Google搜索结果。对于食谱和书籍领域，我们展示来自维基百科的一些背景文本。对于食谱，我们还展示了它们来自维基百科的图片，以便标注员知道它们的样子。然后，我们要求标注员选择其中一个实体，例如这里的第一个，并使用三到五个间接指代表达式来描述它们。例如，“带有钢琴音乐的那首”；以下是一些来自我们数据集的例子：例如，“没有歌词的那首”、“不是那个12岁男孩的那首”、“虚构的那首”或“来自亚美尼亚的那首”等等。altentity语料库包含3000个替代问题，跨越三个领域，并且包含42000个间接指代表达式。使用T5xLarge模型的结果总结如下。如果语言模型可以访问与标注员完全相同的背景知识，那么准确率非常高，约为92-95%。但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更加现实，例如当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率仅为60%。因此，仍有很大的改进空间。我们还证明了模型的领域泛化能力。这是我们数据集的链接。谢谢。"}
