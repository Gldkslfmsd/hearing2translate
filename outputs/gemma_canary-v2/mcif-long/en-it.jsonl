{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lendermann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando il tagging di multinsieme e permutazioni latenti. Si tratta di un lavoro congiunto con i miei relatori, Alexander Koller e Ivan Titoff. La generalizzazione composizionale può essere intesa come la capacità di un sistema di apprendimento di gestire una ricorsione più profonda e composizioni inedite di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe apparire così. Come di consueto, abbiamo un insieme di addestramento di enunciati, in questo caso, “la ragazza ha dormito” e “Maria sapeva che la ragazza ha dormito”. Questi enunciati sono abbinati a forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza della valutazione standard di machine learning, l'insieme di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente inedite. In questo esempio, il modello ha visto una ricorsione meno profonda durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli sequence-to-sequence naïf faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate con colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi hanno lo scopo di catturare il processo composizionale che mette in relazione gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo costoso dal punto di vista computazionale. In genere, ciò implica una pre-elaborazione formale specifica delle forme logiche, ad esempio per gestire i simboli di variabile. L'ottenimento degli alberi può anche comportare procedure specializzate di induzione grammaticale. In questo articolo, non utilizziamo alberi e introduciamo un modello sequence-to-sequence neurale che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output. Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la previsione dell'output dall'input in due passaggi. Innanzitutto, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token corretti ma non sono ordinati. Ecco perché, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona approssimativamente così. Ci muoviamo da sinistra a destra attraverso l'output e determiniamo quale token di multinsieme inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso. Quindi, saltiamo al successivo token di multinsieme per determinare il secondo token dell'output. Determiniamo il terzo token dell'output in modo simile saltando a un altro token di multinsieme. Continuiamo questo processo fino a quando ogni token del primo passaggio è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark di Koggs. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, tuttavia. Nel nostro articolo risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multiset proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida che trovare la permutazione con il punteggio più alto è NP-completo. Questo perché è correlato al problema del commesso viaggiatore. Lo approssimiamo con una rilassamento continuo, friendly per la GPU, che ci consente anche di propagare all'indietro la soluzione e imparare le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo, \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\". Questo lavoro è stato realizzato in collaborazione con Essendermouch e Dangerowski. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset costruiti manualmente che richiedono molto tempo per essere curati. Inoltre, misurano solitamente solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi. Ulteriormente, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero la nozione che le identità sociali sfaccettate possono amplificare i pregiudizi e rappresentare luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi LLM più recenti, ottimizzati tramite istruzioni, sono molto bravi a rispondere a istruzioni e prompt. Possiamo quindi chiedere al modello di generare una persona, ovvero una rappresentazione di un individuo immaginario, utilizzando un prompt come, \"Immagina di essere una donna asiatica, descriviti\". E possiamo subito vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità in questo prompt. Ecco alcuni esempi di generazioni da GPT 4. Notiamo immediatamente che, sebbene gli output non siano eccessivamente negativi o tossici nel senso tradizionale del termine, ci sono alcuni schemi interessanti. La donna asiatica è ritratta come riservata, la donna mediorientale è descritta con termini come esotica e riferendosi a una regione affascinante, e entrambe le donne di colore fanno riferimento alla loro ascendenza, mentre la persona maschile bianca non presenta nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima consiste nel generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno fornito questi prompt a soggetti umani, scoprendo che fornendoli a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali. Inoltre, ciò consente un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte è \"marked words\", ovvero un metodo per identificare le parole che distinguono i gruppi \"marked\" dai gruppi \"unmarked\", sul quale elaborerò tra poco. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico. Il metodo delle \"marked words\" attinge al concetto sociolinguistico di \"markedness\", che afferma che esiste un default non marcato e che qualsiasi gruppo che differisce da tale default è linguisticamente marcato. Ad esempio, la parola \"uomo\" o, scusate, la parola \"guerriero\" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano \"un guerriero uomo\" e marcano il termine con \"donna\". Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Nel nostro metodo, designiamo prima quali sono i gruppi \"unmarked\" e \"marked\". Quindi, confrontiamo le persone utilizzando il metodo dei \"fighting words\", che consiste fondamentalmente nell'utilizzare logod ratios pesati per distinguere le parole principali per ciascun gruppo \"marked\". Ad esempio, per le persone di donne nere, eseguiremmo i \"fighting words\" e confronteremmo i logod ratios sia con le persone bianche che con le persone uomini, perché questi sono i due gruppi corrispondenti \"unmarked\". Ora, per alcuni risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Sebbene le persone generate abbiano tassi molto più alti di parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipiche che si trovano nelle persone generate sono solo le parole \"alto\" e \"atletico\". Quindi, solo quelle positive o almeno non negative. Infatti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto negli slide precedenti. Invece, per farlo, ci rivolgeremo ai risultati del nostro metodo delle \"marked words\" per mostrare come queste parole positive facilitino stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste raffigurazioni apparentemente positive riflettano schemi dannosi. Innanzitutto, per i gruppi \"marked\", le parole principali includono cose come \"cultura\", \"tradizione\", \"orgoglioso\" ed \"esotico\". Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono dalla norma bianca. Ciò contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come \"vivace\" e \"formosa\", che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come \"piccola\" e \"delicata\" e \"setosa\", che si collega a una lunga storia di iper-sessualizzazione delle donne asiatiche, considerate molto docili e sottomesse, e così via. Infine, per le donne nere, vediamo che alcune delle parole principali sono cose come \"forte\" e \"resiliente\". Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della \"strong black woman\". E sebbene possa sembrare positivo a prima vista, ci sono state ricerche che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché esercita molta pressione sulle donne nere affinché siano resilienti e forti contro gli ostacoli sociali. Invece di lavorare attivamente per cambiare tali ostacoli, esercita pressione su queste persone affinché le superino, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni. Più in generale, scopriamo che le parole per ciascun gruppo \"marked\" riflettono fondamentalmente narrazioni molto essenzializzanti. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, noi ricercatori dobbiamo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare pregiudizi e danni, perché potrebbero essere trascurate molte cose se non lo facciamo. Infine, ci dovrebbe essere una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, come questi stereotipi positivi, non sappiamo se è dovuto a una sorta di allineamento dei valori strano ed eccessivo o forse ad alcuni altri metodi anti-stereotipici che stanno causando questi schemi dannosi. Non possiamo davvero fare alcuna assunzione o studiarli ulteriormente senza maggiore trasparenza. Grazie mille per l'ascolto. Buona permanenza a ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABCEval, un nuovo approccio dimensionale per la valutazione dell'intelligenza artificiale conversazionale. Questo lavoro è stato svolto dall'Emory NLP Lab, guidato dal Professor Gino Choi all'Emory University, e in collaborazione con Amazon Alexa AI. Quindi, supponiamo che abbiate appena sviluppato un modello di dialogo e vogliate vedere quanto bene si confronta con lo stato dell'arte attuale. La pratica comune è quella di utilizzare la valutazione umana, come chiedere a giudici umani di selezionare quale delle due conversazioni sia migliore, oppure di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato. Un approccio è semplicemente quello di chiedere a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o di scala Likert esistenti. Tuttavia, riteniamo che ci sia una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Annotando esplicitamente se ciascuna risposta del modello esprime o meno determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat, o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABCEval è in grado di misurare i tassi con cui i modelli di chat commetteranno vari errori tematici. Ad esempio, ABCEval misura il numero di turni in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di irrilevante, si contraddice o contraddice il proprio interlocutore, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello ha successo o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni bot umane per modello utilizzando ABCEval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Oltre ai metodi di valutazione, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni. Dalle nostre analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette comportamentali di ABC eval sono complessivamente più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni etichettate in duplice. Inoltre, le etichette di ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni di sé e del partner spiega il cinque e il dieci percento della qualità della conversazione, mentre i punteggi medi di consistenza Likert spiegano solo il quattro percento o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a stepwise. Potete vedere come la combinazione di tutte le metriche di ABC eval spiega oltre il 25 percento della qualità della conversazione, e poiché rimuovete le metriche una alla volta, la maggior parte di esse comporta la perdita di una buona quantità di informazioni sulla qualità. E la combinazione di metriche Likert a livello alternativo spiega molto meno della qualità, e meno di queste metriche trasportano informazioni uniche. Queste metriche di ABC eval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore a quella che i metodi precedenti erano in grado di raggiungere. Potete vedere nei risultati del nostro esperimento che rimangono diverse sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il venti percento delle loro risposte. Producono informazioni irrilevanti in circa il quindici percento delle risposte e si contraddicono o contraddicono il proprio interlocutore circa il 10% del tempo. Con il rapido ritmo di miglioramento nel settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli pubblicati dalla nostra valutazione. Tuttavia, questo è tutto il motivo per cui è necessario perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC eval possa essere sfruttato dagli altri nel settore come un passo significativo in questa direzione, e siamo ansiosi di vedere come l'intelligenza artificiale conversazionale progredirà nei prossimi mesi e anni. Grazie per averci seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una studentessa di dottorato in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato a ACL 2023 come articolo lungo, “Transfer Learning for Dissonance Detection”, che affronta la sfida delle classi rare. Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è l'incoerenza tra due credenze o azioni, come in questo esempio in cui una persona afferma “So che le sigarette possono uccidermi” e poi aggiunge “Ho preso un paio di sigarette dopo la riunione”. Questa credenza e questa azione sono incoerenti e quindi in dissonanza. Ulteriori affermazioni del tipo “Non credo di poter mantenere il lavoro senza di esse” giustificano la seconda occorrenza e stabiliscono una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio, rispetto ad altri tipi di relazioni discorsive. Ma perché questo è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze e valori di credenze e cambiamenti di atteggiamento nella popolazione. L'elevata dissonanza cognitiva è inoltre correlata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi individuali e ci aiuta a comprendere meglio i processi decisionali. Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo adottato un approccio \"dissonance first\", come si può vedere nel diagramma di flusso qui. I tweet sono stati analizzati utilizzando un parser PDTB e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può notare, la dissonanza è stata rilevata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia funzionato non molto meglio del caso. Data la bassa occorrenza della dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema della rarità assoluta. Per alleviare questo problema, sperimentiamo combinazioni di transfer learning e active learning per annotare in modo da poter raccogliere più campioni di dissonanza in meno cicli di annotazione, riducendo i costi complessivi di annotazione e migliorando il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di active learning trasferendo i pesi da attività correlate. Trasferiamo da due attività diverse: la classificazione dello stance sulla dissonanza indipendente dall'argomento, un'attività che determina se due dichiarazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo indipendentemente dall'argomento, denominata \"debate\" qui, e la classificazione binaria delle classi di espansione e di confronto PDTB, poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza e le chiamiamo CE qui. Scopriamo che trasferendo, le prestazioni zero-shot sul set di dati annotato sono già notevolmente migliori del caso, con il valore migliore AUC pari a 0,62. Inoltre, eseguendo un fine-tuning iterativo su entrambe le attività, scopriamo che il fine-tuning dell'attività CE seguito da un ulteriore fine-tuning sul dibattito produce prestazioni zero-shot notevolmente migliori. Questo è quindi il modello che utilizziamo per avviare l'active learning. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ciascun ciclo di active learning. Cumulative accumula tutti i dati raccolti dalle annotazioni active finora, mentre iterative aggiorna il modello addestrandolo sul più recente set di dati raccolto. Tra le diverse strategie, abbiamo scoperto che cumulative ha funzionato meglio o pari a iterative in tutti i casi. Successivamente, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia PRC (probability of rare class), per selezionare principalmente gli esempi che è molto probabile siano dissonanti secondo il modello corrente in qualsiasi round di AL. Lo confrontiamo con le altre strategie state-of-the-art di AL comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio delle altre strategie state-of-the-art, anche se la differenza è piccola. Da notare che le prestazioni sono significativamente inferiori per random. Nelle successive fasi di AL con le due migliori strategie, abbiamo migliorato l'AUC della classificazione della distanza a 0,75, che è la migliore prestazione che abbiamo ottenuto finora su questo compito. Abbiamo anche controllato la fattibilità di ciascuna strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha la più alta percentuale di distanza e funziona meglio per la classe rara. Tuttavia, gli annotatori hanno trovato anche gli esempi difficili. In sintesi, scopriamo che PRC è una semplice strategia di AL per l'acquisizione di classi rare e che l'avvio di AL con attività di transfer learning adeguatamente progettate può aiutare significativamente. Scopriamo inoltre che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni active in-domain beneficiano dell'aggiornamento cumulativo. Questi sono i link al nostro codice, al dataset e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Aksheta e oggi, io e il mio coautore Martin, presentiamo il nostro lavoro intitolato \"Kitmasteff: Valutare l'Integrazione di Conoscenza da Fonti Multiple\". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-training, e la conoscenza fornita negli input al momento dell'inferenza. Studi recenti in compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenza che è fornita anche al momento dell'inferenza. Ad esempio, nella frase \"John ha visto il presidente appena eletto in TV\", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono sapere in modo affidabile chi sia l'entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato da quando è avvenuto il pre-training. Pertanto, modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza fornita al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza progettato per sondare l'abilità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il dataset con partecipanti a uno studio umano e modelli di risoluzione della coreferenza consolidati. Ecco un esempio dal nostro dataset. Servin è un giudice., Kia è una panettiera. Termin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui il pronome lui si riferisce, che in questo caso è Sermon. La risoluzione di un pronome dato richiede due tipi di informazioni. Innanzitutto, la conoscenza specifica dell'entità, come il fatto che Sermon è un giudice. E in secondo luogo, la conoscenza di background appresa durante il pre-training dei grandi modelli linguistici, mentre la conoscenza specifica dell'entità è tipicamente osservata al momento dell'inferenza. Varia la disponibilità di questi due tipi di informazioni in modo che possano essere trovati in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di Kitmos. Innanzitutto, abbiamo l'impostazione del topic pre-training, dove si presume che la conoscenza di background sia disponibile al momento del pre-training. In secondo luogo, c'è l'impostazione \"background both\", dove si presume che la conoscenza sia disponibile sia al momento del pre-training sia al momento dell'inferenza, e infine, l'impostazione \"background inference\", dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula un caso in cui la conoscenza di background necessaria per risolvere un compito non fa parte dei dati pre-addestrati dei modelli, ad esempio perché nuove professioni si sono sviluppate da quando è avvenuto il pre-training. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione del pre-training del background, assumiamo che la conoscenza di background che i politici cercano seggi eletti nel governo sia contenuta nei parametri pre-addestrati. Nell'impostazione del contesto del background, forniamo la conoscenza specifica dell'entità Chichester è un politico. Nell'impostazione \"background both\", forniamo non solo la conoscenza specifica dell'entità, ma anche la conoscenza di background sui politici nel contesto dell'inferenza. Nell'impostazione dell'inferenza del background, forniamo l'occupazione fittizia mirror tour invece di politico, perché mirror tour è improbabile che sia contenuto nei parametri pre-addestrati. Valutiamo il dataset sia con partecipanti a uno studio umano sia con modelli di risoluzione della coreferenza consolidati. In questa figura mostriamo i risultati dei modelli con le migliori prestazioni nell'impostazione più difficile del pre-training del background. Senza un training specifico per il compito su KitMus, entrambi i modelli non ottengono buoni risultati. Quando vengono addestrati su KitMus, tuttavia, sia C2F che Berth for Koref ottengono risultati significativamente migliori rispetto alla scelta casuale. Ciò suggerisce che, quando vengono addestrati su dataset generali di risoluzione della coreferenza, i modelli imparano a sfruttare i segnali di superficie, che non sono utili quando si testa su KitMus, dove tali segnali sono stati rimossi. Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli con le migliori prestazioni non riescono ad integrare in modo affidabile la conoscenza fornita solo al momento dell'inferenza. Per riassumere i punti chiave del nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti senza un training specifico per il compito. Tuttavia, con un training specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Ciononostante, anche i modelli con le migliori prestazioni sembrano avere difficoltà ad integrare in modo affidabile la conoscenza di background presentata solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e il dataset nel codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pappy, dell'Università di Toronto e della Fondazione Bruno Kessler, e introdurrò brevemente l'attenzione come guida per un articolo sulla traduzione automatica simultanea del parlato, un lavoro congiunto con Matteo Negri e Marco Turki. Cos'è la traduzione automatica simultanea del parlato? La traduzione automatica simultanea del parlato, o simulata, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica. E quali sono i problemi dei modelli simulati attuali? Architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complesse, ad esempio addestramento che coinvolge obiettivi di ottimizzazione diversi, e addestramento e mantenimento di diversi modelli per raggiungere diversi regimi di latenza, ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi e così via. Quindi, qual è la nostra soluzione? Innanzitutto, utilizzare modelli ST offline già esistenti senza riaddestramento o adottare architetture specifiche per CMLSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici e sfruttare la conoscenza già acquisita dal modello tramite il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il meccanismo di cross-attenzione, e potete vedere un esempio a destra. La nostra soluzione è proporre un'attenzione encoder-decoder dot o puntata, ed è una strategia per la quale decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, ovvero questa somma è inferiore a una certa soglia alfa verso gli ultimi lambda frame audio, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un chunk di parlato contenente \"Sto per parlare di\" e il nostro modello prevede la traduzione in tedesco, esamineremo i pesi della cross-attenzione e vedremo che le prime due parole puntano ai primi frame audio ricevuti mentre l'ultima parola punta agli ultimi frame audio ricevuti come lambda frame audio. Ciò significa che le prime due parole verranno emesse, mentre poiché la somma della cross-attenzione è superiore a una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro chunk di parlato. Se continuiamo e riceviamo un altro chunk di parlato e il nostro modello prevede altre tre parole, esamineremo i pesi della cross-attenzione e vedremo che nessuna parola punta agli ultimi lambda frame audio. Ciò significa che queste tre parole verranno emesse. Se guardiamo ai risultati principali, tracciamo i risultati della traduzione automatica simultanea del parlato su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il lagging medio, ovvero la misura della latenza. Consideriamo anche il lagging medio consapevole del calcolo, che tiene conto del tempo di calcolo del modello per prevedere l'output. Quindi, vogliamo che le nostre curve siano il più alte possibile su questo grafico, ma anche che siano spostate a sinistra, e confrontiamo con strategie prepara che sono applicate anche ai modelli offline, ovvero la strategia key weight e l'accordo locale, e confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la traduzione automatica simultanea del parlato. Questi sono tutti i risultati della strategia di traduzione automatica simultanea del parlato in tedesco e vediamo che un dot supera tutte le strategie applicate ai modelli offline, poiché i loro core sono spostati verso sinistra e vediamo anche che se consideriamo il tempo trascorso effettivo o il tempo di calcolo effettivo, è la strategia più veloce. Per scoprire maggiori risultati, leggete il nostro articolo e abbiamo anche pubblicato il codice e i modelli open source e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Xuheng. Oggi vi presenterò il nostro articolo \"I tagger di entità nominate Kernel 2003 funzionano ancora bene nel 2023?\". Cominciamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito del riconoscimento di entità nominate, o NER. Abbiamo osservato che i modelli hanno utilizzato Kernel 2003 per sviluppare il NER per quasi 20 anni. E questo solleva naturalmente diversi problemi. Innanzi tutto, questi modelli possono generalizzare a dati moderni e cosa è necessario per una buona generalizzazione quando sviluppiamo nuovi tagger? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il data set Kernel Plus Plus. Questo è un data set che abbiamo raccolto da Reuters News a partire dal 2020 e poi annotato con le stesse linee guida di annotazione di Kernel 2003. Successivamente, abbiamo ottimizzato più di 20 modelli su Kernel 2003. Abbiamo valutato i modelli sia sul set di test CoNLL 2003 che sul set di test Kernel Plus Plus. E, non da ultimo, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalmente generalizzano meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che, di solito, modelli più grandi portano a una migliore generalizzazione. E, non da ultimo, sappiamo tutti che il numero di esempi di ottimizzazione influisce direttamente sulle prestazioni di un compito downstream. Anche qui, abbiamo scoperto che un numero maggiore di esempi di ottimizzazione porta effettivamente a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'overfitting adattivo, ovvero l'overfitting causato dal riutilizzo dello stesso set di test più e più volte, e questo si manifesta solitamente come un rendimento decrescente su un nuovo set di test. La seconda ipotesi è la deriva temporale, ovvero il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e i dati di test. Per quanto riguarda i dati di overfitting, abbiamo visto che, dal grafico a destra, la linea di adattamento rosso ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su CoNLL 2003 si traduce in più di un'unità di miglioramento su Kernel Plus Plus, il che significa che non vi è un rendimento decrescente e questo ci mostra che l'overfitting adattivo in questo caso non viene osservato. E la deriva temporale? Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni diminuiscono con un divario temporale maggiore e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello maggiore e un numero maggiore di esempi di ottimizzazione, e questi vanno di pari passo. Non possiamo avere solo un ingrediente trascurando gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dalla deriva temporale e, sorprendentemente, non è causato dall'overfitting adattivo, anche se CoNLL 2003 è stato utilizzato per più di 20 anni. Ritornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger CoNLL 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare la generalizzazione dei modelli. E, per concludere, vi invitiamo a consultare il nostro articolo, il nostro data set e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno e benvenuti alla nostra presentazione di DeepLean, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensibilità per un gruppo target specifico, come persone con difficoltà di lettura o non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio documenti o frasi. Nell'esempio qui, potete vedere una coppia di frasi allineate in parallelo di una frase tedesca complessa e della sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, l'eliminazione di clausole, il riordino di clausole o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus DPlane, perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Questi ultimi sono troppo piccoli per addestrare un modello di tassonomia. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus DPlane, diviso in due sottocorpora, DPlane APA e DPlane Web. DPlane APA si basa su testi utilizzati. In DPlane APA, abbiamo allineato manualmente 483 documenti, ottenendo circa 30.000 coppie di frasi parallele, 13.000. Per Dplane Web, questo corpus include diversi domini e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, abbiamo ottenuto 30.450 coppie di frasi. Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio sul tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue, a tutti i livelli riguardo, ad esempio, alla semplificazione lessicale, alla semplificazione strutturale o al livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus D-plane ha un'elevata varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus D-plane API abbiamo molti più riordini ed edizioni di parole rispetto a quelli che abbiamo nel corpus D-plane web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Buongiorno, sono Omar e ora parlerò dei casi d'uso del nostro dataset Dplane. Per il primo caso d'uso possiamo valutare i metodi di allineamento automatico. Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi nei documenti post, nel nostro caso stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli, che hanno la stessa lingua, lo stesso contenuto, ma si trovano a livelli di complessità diversi. E ora che abbiamo il nostro dataset D-plane, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nell'articolo. Alla fine, siamo giunti alla conclusione che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo di mass align, e potete anche trovare il codice per eseguire questo metodo sui vostri documenti nell'articolo. Il secondo caso d'uso che abbiamo mostrato nel nostro articolo è il caso della semplificazione automatica del testo tramite la messa a punto di modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo messo a punto due modelli diversi. Abbiamo messo a punto il modello di long import per produrre semplificazioni a livello di documento e abbiamo anche messo a punto il normale base import per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete consultare maggiori dettagli sui punteggi e le metriche di valutazione dei nostri esperimenti nell'articolo. Siamo giunti alla conclusione che questa messa a punto di base poteva produrre o ottenere punteggi migliori dei punteggi di base e proponiamo questi risultati come benchmark, benchmark di base per il problema della semplificazione automatica del testo in futuro. Vi ringrazio molto per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xi Yuan dell'Università di Fen. Sono qui per presentare il nostro lavoro sulla conoscenza distinta degli script da modelli linguistici di grandi dimensioni per la pianificazione del linguaggio vincolata. Nella vita di tutti i giorni, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo passo sotto forma di script garantiti. Lavori precedenti hanno esplorato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come preparare una torta e hanno dimostrato che i modelli linguistici di grandi dimensioni possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per gli obiettivi con obiettivi specifici, vincoli specifici, come preparare una torta al cioccolato, rimane ancora relativamente poco studiata. In questo articolo, definiamo il problema della pianificazione del linguaggio vincolata, che impone diversi vincoli sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con difficoltà maggiori, con vincoli molteplici. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione del linguaggio vincolata dei modelli linguistici di grandi dimensioni. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli molteplici per l'acquisizione dei dati nel ciclo umano utilizzando instructed TPT. Campioniamo cento obiettivi specifici e valutiamo gli script generati dai modelli Light Logic. Questa tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli Light Logic ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare su cosa fanno i modelli Light Logic. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile. Ma la fedeltà ai vincoli non può essere garantita. Approfondiamo categorie più precise di vincoli a seconda delle esigenze. La mappa di testa nella figura mostra che le prestazioni di pianificazione degli instruct GPT variano considerevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli Larry presenta un'alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di un filtro Zen sovra-generato per migliorare la qualità della generazione. Innanzitutto, mostriamo i tipi di vincoli con esempi per instruct GPT e otteniamo obiettivi specifici basati sugli obiettivi astratti definiti. Quindi, instruct GPT sovra-genera script chiave per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script più adatti. Convertiamo gli script e gli obiettivi in instruct GPT in modo strutturato e calcoliamo la similarità del coseno e i punteggi di similarità per misurare la similarità semantica. Inoltre, scriveremo lo script che contiene le parole chiave del vincolo target. Conserviamo solo lo script se il punteggio dell'obiettivo target è il più alto nel sito obiettivo. Con il nostro metodo, Inslacity può generare script di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i modelli linguistici di grandi dimensioni sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione del linguaggio di modelli più piccoli e specializzati. La creazione di set di dati è un passo essenziale a questo scopo. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici, e l'annotazione manuale dei set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare modelli di pianificazione del linguaggio vincolata. Applichiamo il nostro metodo per costruire un set di dati di pianificazione del linguaggio vincolata denominato code script. In totale, generiamo cinquantacinquemila obiettivi specifici con script per garantire la qualità dei siti di validazione e test. Chiediamo a lavoratori cloud di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione vincolata di code script. Scopriamo che code script mostra l'ipotesi negli obiettivi specifici generati. Con code script, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolata. Scopriamo che TFIL funziona su code rate può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono supportare i modelli più grandi quando vengono addestrati correttamente su siti di dati adatti. In sintesi, abbiamo stabilito il problema della pianificazione del linguaggio vincolata. Abbiamo valutato la capacità di pianificazione del linguaggio vincolata dei modelli linguistici di grandi dimensioni e sviluppato un metodo di filtro sovra-generato per i modelli linguistici di grandi dimensioni. Utilizziamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, CodeScript, per la pianificazione del linguaggio costruttiva. Speriamo che il set di dati CodeScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione del linguaggio. Grazie per il vostro tempo. Trovate maggiori dettagli su CodeScript nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Yanis Lavrack e vi presenterò i nostri lavori su Dr. Berth, un modello di pre-training robusto in lingua francese per il dominio biomedico e clinico. In questa presentazione, inizieremo discutendo del language modeling nel settore sanitario. Successivamente, esporremo il principale contributo del nostro articolo. Introdurremo il primo modello biomedico in francese denominato Dr. Berth, basato su Roberta, e addestrato su Natchios, un dataset di dati medici estratti dal web. Presenteremo anche un confronto di modelli con diverse configurazioni di pre-training e fonti di dati. Poi, mostreremo i nostri risultati su undici compiti downstream biomedici e clinici in francese. Infine, concluderemo in merito agli esperimenti e vi forniremo maggiori dettagli su come accedere ai modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di natural language processing e offre un notevole aumento delle prestazioni rispetto a metodi storici statici e contestualizzati come Word2Vec, Fastex o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert, e ad altri domini come quello biomedico con PermetteBERT e BioBERT, e nel clinico con Clinical BERT, ma prevalentemente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso basati su pre-training continuo a causa della mancanza di dati in-domain. Tuttavia, in francese non esisteva alcun modello open source per il dominio biomedico fino ad ora. Ci siamo quindi posti la seguente domanda: quali sono le fonti di dati più appropriate per un'ampia gamma di utilizzi? E i dati attuali possono essere una buona sostituzione dei dati clinici? Per rispondere a questa domanda, confrontiamo Dr. Berth con il nostro modello Schubert, basato su dati anonimizzati ottenuti da un ospedale non universitario che abbiamo a disposizione. In seguito, ci siamo chiesti: quanta dati sono necessarie per addestrare un modello specializzato in francese? Sono necessari 4 GB, 8 GB o più? Per rispondere a questa domanda, abbiamo inizialmente addestrato e confrontato quattro modelli da zero. Una prima versione di Dr. Berth con 7 GB di Natchez, una seconda versione con 4 GB di Natchez, una prima versione di Schubert, un modello clinico, con 4 GB di frasi tratte da annotazioni cliniche, e una versione finale di Schubert con un mix di 4 GB di Natchez e 4 GB di annotazioni cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati tramite pre-training continuo per analizzare l'impatto della strategia di pre-training. Uno basato sui pesi di Camembert e addestrato su 4 GB di Natchez. Un altro, anch'esso basato su Camembert, ma addestrato questa volta su 4 GB di annotazioni cliniche. E infine, uno basato su un modello biomedico inglese, Bermud Bert, e addestrato su 4 GB di Natchez. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, abbiamo raccolto compiti public e private don't-seam come il riconoscimento di entità nominate e di relazioni, la classificazione, il pattern switch tagging e il question answering. Questi modelli sono stati confrontati con sei modelli di riferimento, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli hanno ottenuto i risultati migliori sui compiti con dati della stessa natura di quelli su cui sono stati addestrati. Tuttavia, possiamo ottenere quei dati da, possiamo osservare che i dati provenienti da fonti eterogenee risultano più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori. Nel complesso, il fine-tuning da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, il nostro esperimento sul fine-tuning continuo utilizzando i pesi e il tokenizer di PumedBeard, addestrato su un sottoinsieme di 4 GB di Natchez, ha mostrato risultati comparabili a quelli ottenuti con Dr. Berth 4 GB da zero, cosa che non accade per il modello basato sui pesi e il tokenizer di Camembert, che soffre di problemi di stabilità. Infine, in conclusione, il nostro sistema proposto offre prestazioni migliori su nove degli undici compiti DOTSTRIMS e supera globalmente i risultati del modello generico Camembert. Osserviamo inoltre che i dati specializzati, più specializzati, sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da Natchios sono disponibili liberamente su Hugging Face e tutti gli script di addestramento si trovano nel nostro repository GitHub. Quindi, grazie per... per questa presentazione e siamo lieti di scambiare idee durante la sessione di post-presentazione a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiang Bin, dottorando all'Università di Washington. Oggi vi presento il nostro lavoro, che parte dai dati di pre-addestramento ai modelli linguistici fino alle attività a valle, tracciando le tracce dei pregiudizi politici che portano a modelli NLP ingiusti. Quindi, i modelli linguistici vengono addestrati su dati web di grandi dimensioni. Le notizie politiche dai media sono ben rappresentate nei loro dati di pre-addestramento. Secondo un’indagine sul corpus C four, possiamo vedere che il New York Times, il Los Angeles Times, il Guardian, l'Huffington Post, e così via, sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato un beneficio misto per le applicazioni dei modelli linguistici. Quindi, da un lato, sono stati in grado di imparare da diverse prospettive, il che celebra la democrazia e la pluralità delle idee. D'altra parte, queste diverse opinioni politiche sono intrinsecamente socialmente tendenziose e potrebbero portare a potenziali problemi di equità nelle applicazioni a valle. A tal fine, proponiamo di indagare la pipeline di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici alle attività a valle, specificamente ponendo le seguenti domande. Innanzitutto, come valutiamo l’orientamento politico dei modelli linguistici e quale ruolo potrebbe avere i dati di pre-addestramento su tali pregiudizi politici? In secondo luogo, come i modelli linguistici con diversi orientamenti politici si comportano effettivamente nelle attività a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni NLP? Specificamente, proponiamo innanzitutto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando i questionari politici, come il test della bussola politica. Ciò ci assicura di poter eseguire una valutazione automatica ben radicata nella letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, innanzitutto, i modelli linguistici hanno significati politici diversi. Occupano tutti e quattro i quadranti sulla bussola politica. Possiamo anche vedere che GPT 4 è il modello linguistico più liberale tra tutti e GPT theories sono generalmente più socialmente liberali di BERT theory e delle sue varianti. In secondo luogo, miriamo a indagare in che misura i pregiudizi politici dei modelli linguistici sono effettivamente tratti dai dati di addestramento. Potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint del modello linguistico su sei diversi corpora partigiani separati in notizie e social media ulteriormente suddivisi nelle loro inclinazioni politiche. Pre-addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta ulteriormente fine-tuned, ulteriormente addestrato sul corpus Reddit di sinistra, possiamo vedere un notevole spostamento liberale in termini dei suoi pregiudizi politici. Per indagare se i modelli linguistici possono acquisire la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpora di pre-addestramento nel pre-45° Presidente degli Stati Uniti e dopo il 45° Presidente degli Stati Uniti. Pre-addestriamo separatamente i modelli linguistici su due diversi corpora temporali. Possiamo vedere che i modelli linguistici hanno generalmente avuto un orientamento politico che è più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società. Ultimo ma non meno importante, valutiamo i modelli linguistici con diversi orientamenti politici sul rilevamento dell'incitamento all'odio e sul rilevamento di notizie false per le applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Vediamo che se investighiamo la performance per categoria, cioè se separiamo la performance in diverse demografie o significato politico dei media, possiamo vedere un pattern che, ad esempio, per il rilevamento dell'incitamento all'odio, i modelli linguistici di sinistra sono migliori nel rilevare l'incitamento all'odio che prende di mira gruppi sociali minoritari, tuttavia, sono peggiori nel rilevare l'incitamento all'odio che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare l'incitamento all'odio che prende di mira bianchi e uomini, tuttavia, peggiori nel rilevare l'incitamento all'odio che prende di mira neri, LGBTQ plus e altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dai loro opposti significati politici e viceversa. Questo mostrerà ulteriori esempi qualitativi per vedere che i modelli linguistici con diversi significati politici danno effettivamente previsioni diverse agli esempi di incitamento all'odio e disinformazione in base alle loro categorie sociali. Ci sono un sacco di altri esempi nell'appendice per evidenziare ulteriormente questo fatto. Ciò indica che esiste un problema di equità molto urgente riguardante i pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere fine-tuned sull'incitamento all'odio o sulla disinformazione o su qualsiasi cosa e distribuito su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio che prende di mira i gruppi minoritari potrebbe semplicemente dilagare senza alcun controllo. Quindi questo suona l'allarme affinché riconosciamo e affrontiamo i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici. Quindi, un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propaga dai dati di pre-addestramento ai modelli linguistici alle attività a valle, creando in definitiva problemi di equità. Se invece proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere conservato nei dati di addestramento dei modelli linguistici. Quindi è un po' come il problema di Electric Charlie. Va bene, penso che sia più o meno tutto per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Coast of Sina e sono lieto di darvi il benvenuto al nostro intervento sulla pubblicazione ACL del 2023 dal titolo \"Language Model Acceptability Judgments are not always robust to context\". Questo è un lavoro congiunto con John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina Williams. In questo lavoro, riprendiamo il paradigma delle coppie minime. Il paradigma delle coppie minime valuta essenzialmente i modelli linguistici sulla base di giudizi di accettabilità, che possono includere anche la grammaticalità, come quelli del Blimp Syntax Gem, o l'accettabilità in termini di stereotipi, come quelli di Crowd Spares. In questo paradigma delle coppie minime, il modo tipico per valutare i modelli linguistici è quello di mostrare una frase accettabile o una frase grammaticale e poi una frase inaccettabile o ungrammaticale, e si spera che il modello assegni una probabilità maggiore alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione del modello verso frasi più lunghe. Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno producendo frasi sempre più lunghe e finestre di contesto più ampie. È quindi fondamentale che valutiamo l'accettabilità del modello attraverso l'intera finestra di contesto. Ed è proprio questo che stiamo cercando di fare: riprendere la pipeline NPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Questo è l'approccio che adottiamo. Quello che facciamo è ricreare queste sequenze più lunghe riprendendo i dataset stessi e ricreando frasi scegliendo frasi accettabili o inaccettabili da tali dataset. Ad esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal dataset Blimp, dal caso delle isole adjunct. E quello che facciamo per ricreare sequenze più lunghe, che siano accettabili e che abbiano la stessa struttura grammaticale, è estrarre frasi grammaticali dall'isola adjunct e aggiungerle come prefisso sia alla query accettabile che alla query inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa struttura corrispondente, il che potrebbe anche essere utilizzato per testare l'accettabilità del modello. Possiamo anche farlo scegliendo frasi da un diverso sottoinsieme o da un dataset diverso. Questo è ciò che chiamiamo scenario di disallineamento. Qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset con cui si sta valutando. Possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, che sia proveniente da un diverso sottoinsieme del dataset o che sia completamente irrilevante rispetto alla frase che stiamo esaminando. Quindi, come se la cava il modello? Innanzitutto, esaminiamo le frasi di Wikipedia che sono completamente irrilevanti rispetto alla coppia di query corrente e lì scopriamo che i giudizi MPP sono per lo più robusti per una lunghezza arbitraria del contesto. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT2 e qui vediamo, nella linea tratteggiata arancione, che i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Qui, stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset Blimp o Syntax Gem e lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Ma quando si fa corrispondere la struttura, cioè quando si scelgono frasi dallo stesso fenomeno in Blimp-based Syntax Gem, si osserva un aumento o una diminuzione massiccia del giudizio MPP del modello a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo, e questo, è molto grande, questo effetto aumenta in tutta la lunghezza del contesto e probabilmente influenzerebbe i modelli linguistici più recenti che hanno finestre di contesto ampie. Perché il prefisso corrispondente influenza così tanto il giudizio del modello linguistico? Abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare... la frase di input cercando di preservare la struttura rilevante ma aggiungendo rumore all'input e, dopo aver effettuato diverse di queste perturbazioni, abbiamo scoperto che nessuno di questi rumori modifica effettivamente il corso del modello in termini di come mostra la tendenza del giudizio MPP. In sostanza, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili: quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio accettabile, vediamo una diminuzione dei giudizi MPP in modo simile. I punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, nel modo in cui la conduciamo attualmente con input brevi e a singola frase, potrebbe non catturare appieno la conoscenza astratta del modello linguistico attraverso l'intera finestra di contesto. Per maggiori dettagli sui nostri esperimenti, vi invitiamo a leggere il nostro articolo. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Dawe, dottorando presso l'Università di Stalant in Germania. In questo video, vorrei presentare il nostro recente lavoro, \"Weaker Than You Think\", un'analisi critica dell'apprendimento supervisionato settimanale. Si tratta di un lavoro congiunto con Xiao Yushchen, Maios Musbach, Giaz Steffen e Dietrich Clarkov. Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale. Nella supervisione settimanale, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura settimanale, come semplici regole euristiche, basi di conoscenza o crowdsourcing locality come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente reti neurali su dati di etichettatura settimanale, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento supervisionato settimanale, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali su tale rumore dell'etichetta in modo che i modelli addestrati generalizzino bene. In recenti lavori in WSL, dove WSL sta per apprendimento supervisionato settimanale, un'affermazione comune è che le persone dicono di addestrare solo modelli su dati di etichettatura settimanale e di ottenere prestazioni elevate su insiemi di test puliti. Tecnicamente, questa affermazione non è errata, ma c'è un aspetto da considerare, ovvero che le persone presumono che sia disponibile un insieme di validazione pulito aggiuntivo per la selezione del modello. Mettiamo in dubbio questo scenario, poiché ciò implica che nell'apprendimento supervisionato settimanale sono necessarie annotazioni manuali aggiuntive. Ma come un elefante nella stanza, questa necessità è spesso trascurata. Il suddetto dubbio ci porta a porre tre domande di ricerca. Prima, i dati di validazione puliti sono necessari per WSL? O possiamo usare un insieme di validazione rumoroso invece? Seconda, se i dati puliti sono necessari o se i dati puliti sono obbligatori affinché WSL funzioni, quanti campioni puliti ci servono? Infine, dovremmo usare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Innanzitutto, scopriamo che, curiosamente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, si verifica un calo significativo delle prestazioni. Come mostrato in questa figura, se non sono presenti campioni di validazione puliti, i modelli addestrati non riescono a generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente e che il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. In genere, abbiamo bisogno solo di venti campioni per classe per ottenere prestazioni elevate. Ma non è finita qui, perché se decidiamo comunque di accedere a campioni puliti, addestrarci direttamente su di essi raggiungerà prestazioni ancora migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente ai dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come si può vedere, se abbiamo 10 campioni per classe, il fine-tuning inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni rivendicato negli approcci WSL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come si può vedere dalle figure, il modello Berliner denominato FTW inizialmente sottoperforma rispetto a metodi WSL più complessi come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTW funziona altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni annotati manualmente e puliti per funzionare correttamente. Il loro guadagno e la loro praticità di prestazioni sono fortemente sovrastimati. Le nostre raccomandazioni concrete per il futuro lavoro sono le seguenti. Innanzitutto, segnalare che la selezione del modello è stata eseguita con campioni di validazione puliti. In secondo luogo, gli approcci WSL dovrebbero essere confrontati con linee di base di apprendimento continuo poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è una linea di base semplice ma forte che dovrebbe essere considerata in futuro nel lavoro in WSL. Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questo slide. Sentitevi liberi di controllarlo. Grazie e godetevi la conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ayud Villar e fornirò una breve panoramica del documento Prompting Palm da Translation Assessing Strategies and Performance. Questo è un lavoro congiunto con i miei colleghi di Google Translate. Palm è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato l'anno scorso, nel 2022. È stato addestrato su un'ampia collezione di testo comprendente 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di attività di NLP. In questo lavoro, presentiamo il primo studio sistematico del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica. Valutiamo la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico e confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni della valutazione WMT. Utilizziamo metriche neurali MT all'avanguardia e mostriamo anche risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompt. Il prompting ha una grande influenza sulle prestazioni degli LLM per la traduzione, come possiamo vedere in un semplice esperimento in cui utilizziamo un prompting breve e forniamo due prompt diversi per una sola frase. Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto BLEU. E in casi estremi, può arrivare fino a 40 punti BLEU. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi, in cui contrassegniamo semplicemente ogni frase che forniamo al sistema con la lingua in cui è scritta. Quindi, in questo esempio, in cui abbiamo eseguito la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con \"tedesco:\" e le traduzioni in inglese con \"inglese:\". Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diversi prompting brevi. È cruciale per il prompting zero e a colpo singolo, ma quando passiamo, come nel nostro caso, a cinque colpi, c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi a portare la maggior parte del peso. Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo (dev data). I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, e i risultati mostrano una migliore performance quando si utilizzano i dati di sviluppo. Ciononostante, i sistemi ODR specializzati hanno un vantaggio sostanziale rispetto alle traduzioni di PALM, ma PALM si avvicina a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo tratto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM sono che la fluidità di Palm è paragonabile ai sistemi all'avanguardia, ma la principale differenza deriva dall'accuratezza. In particolare, gli errori più comuni sono errori di omissione. Sembra quindi che Palm scelga di produrre una traduzione più scorrevole a volte omettendo parti della frase che sono omesse nella traduzione. Tuttavia, la categoria di stile di PAM è inferiore a quella dei sistemi all'avanguardia, il che è un ulteriore segnale che PAM fornisce un output davvero fluido, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di consultare la presentazione completa del documento. Grazie molto."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jing Wei e sono dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video promozionale del nostro articolo: \"Are you copying my model? Protecting the copyright of large language models for embedding and services. View Backdoor Watermark\". Iniziamo con un'introduzione al contesto di embedding e servizi. Attualmente, i modelli linguistici di grandi dimensioni, come GPT, Lama e PELM, eccellono nella comprensione e generazione del linguaggio naturale. Embedding e servizi rappresentano uno dei servizi costruiti sui modelli linguistici di grandi dimensioni per assistere in varie attività di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, studi recenti hanno dimostrato che un attaccante può rubare il modello apprendendo dagli embedding e fornendo servizi simili. Pertanto, è necessario proteggere il copyright degli embedding come servizi. Per proteggere il copyright degli embedding come servizi, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo deve soddisfare le seguenti proprietà: in primo luogo, il metodo dovrebbe essere applicabile agli embedding come servizi; in secondo luogo, il watermark non dovrebbe degradare l'utilità degli embedding forniti; in terzo luogo, il watermark dovrebbe essere sufficientemente celato in modo che l'attaccante non lo rilevi o possa rimuoverlo facilmente; infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere raggruppati in quattro categorie. Tuttavia, questi metodi non sono applicabili agli embedding come servizi o mancano di trasferibilità. Pertanto, in questo articolo proponiamo embedding marker, un metodo di watermark basato su backdoor applicabile agli embedding come servizi. Ora vi presenterò i dettagli del nostro embedding marker. Embedding marker contiene due fasi principali: l'iniezione del watermark e la verifica del copyright. Prima di queste fasi principali, selezioniamo innanzitutto un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contarne la frequenza delle parole. Durante l'iniezione del watermark, definiamo innanzitutto un embedding target. Quando un utente invia una frase al servizio del fornitore, quest'ultimo conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo innanzitutto un backdoor e un dataset benigno. Il dataset del backdoor contiene frasi di cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di trigger. Quindi, il fornitore richiede embedding al servizio del ladro con il dataset. Vengono calcolate la similarità coseno e la similarità L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra il dataset del backdoor e il dataset benigno, definita come delta coseno e delta L2. Contestualmente, applichiamo anche il test KS e utilizziamo il suo p-value come terza metrica. Abbiamo condotto esperimenti su quattro dataset: AG News, Mind, SSD two ed Erospam. Assumiamo che il fornitore applichi il dataset di Wiki Text per contare la frequenza delle parole. I risultati su quattro dataset mostrano che il nostro embedding marker può ottenere un'elevata accuratezza di rilevamento, mantenendo al contempo la sua utilità per i task downstream. Abbiamo inoltre validato la celata dell'embedding fornito visualizzando l'embedding di frasi srotolate come in BOPCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding del backdoor e quelli normali. Questo è tutto, grazie. Siamo aperti a discussioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ying e io, insieme al mio collega Jian, presenteremo la nostra ricerca sul multi instruct, ovvero sul miglioramento dell’apprendimento zero-shot multimodale attraverso l’instruction tuning. Con i progressi dei modelli linguistici di grandi dimensioni, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare modelli linguistici pre-addestrati per diverse attività downstream in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l’instruction tuning consente ai modelli linguistici di grandi dimensioni di svolgere attività inedite in modalità zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull’instruction tuning si è concentrata sul miglioramento delle prestazioni zero-shot per attività basate esclusivamente sul linguaggio, mentre la computer vision e le attività multimodali sono state trascurate. Pertanto, in questo lavoro, desideriamo indagare se l’instruction tuning su modelli pre-addestrati multimodali può effettivamente migliorare la generalizzazione a attività multimodali inedite. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra LP e multimodale. Esistono più di mille seicento attività di istruzioni basate esclusivamente sul linguaggio. Tuttavia, non esiste un set di attività di istruzioni multimodali di grandi dimensioni disponibile pubblicamente. Questo ci motiva a creare un set di dati di instruction tuning multimodale. Qui presentiamo multi instruct, il primo benchmark dataset di instruction tuning multimodale che consiste di sessantadue attività multimodali diverse che coprono dieci categorie principali. Queste attività derivano da ventuno set di dati open source esistenti e ogni attività è dotata di cinque istruzioni scritte da esperti. Per indagare sull’instruction tuning multimodale sul nostro dataset proposto, utilizziamo OFA, un modello multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token immagine e le coordinate di un bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset multi install. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutte le attività in un formato unificato sequence-to-sequence in cui il testo di input, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Okay, ora parlerò dell’instruction tuning multimodale. Per il dataset di training, utilizziamo 53 attività dal gruppo NIG per l’addestramento e campioniamo 10.000 istanze per attività. Per il testing, riserviamo l’intero gruppo Common Sense Reasoning per il testing e selezioniamo ulteriori cinque attività da WQA e dal gruppo miscellaneous. Utilizziamo tutte le istanze nella suddivisione di test per ogni attività. Inoltre, campioniamo casualmente 20 attività dalla suddivisione di test di NIG instruction come attività SIN per NLP. Utilizziamo un modello OFA large pre-addestrato come modello di base. Durante l’addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con uno dei suoi cinque template di istruzione. Quindi, durante il test, per ogni attività, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento. Segnaliamo la performance media e massima e la deviazione standard della performance su tutti e cinque gli esperimenti. Se l’attività è un compito di classificazione multimodale, segnaliamo l’accuratezza. Se è un compito di generazione multimodale, segnaliamo ROOGEL. Per un compito RP, segnaliamo ROOGEL anch’esso. Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensitivity. Questa misura la capacità del modello di produrre costantemente gli stessi output per la stessa attività indipendentemente da lievi variazioni nella formulazione dell’istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l’instruction tuning può migliorare significativamente la performance di OFA su attività multimodali inedite. Inoltre, il transfer learning da un dataset di istruzioni naturali può avvantaggiare l’instruction tuning. Possiamo vedere come l’aumento del numero di attività porta a una migliore performance del modello e, nel contempo, a una minore sensitivity. Abbiamo anche condotto un esperimento. Abbiamo usato un’istruzione rispetto a cinque istruzioni e possiamo vedere che l’utilizzo di più istruzioni può migliorare la performance complessiva del modello e ridurre notevolmente la sua sensitivity. Questo dimostra l’effetto di diverse strategie di fine tuning sulla sensitivity del modello. Come possiamo vedere, grazie al transfer learning da un dataset di istruzioni naturali, il modello può raggiungere una sensitivity molto migliore rispetto al modello OFA originale. Possiamo anche vedere come il transfer learning da un dataset di istruzioni naturali può aiutare OFA a raggiungere una performance molto migliore sul dataset di istruzioni naturali. Nel complesso, abbiamo proposto il primo dataset di instruction tuning multimodale su larga scala. Abbiamo migliorato significativamente la capacità DAROCHOT di OFA e abbiamo esplorato diverse tecniche di transfer learning e ne abbiamo dimostrato i vantaggi. Abbiamo progettato una nuova metrica chiamata sensitivity. Un’altra cosa: stiamo raccogliendo un dataset di instruction tuning multimodale molto più grande, con circa 150 attività Variant language aggiuntive, e lo rilasceremo. Ecco un codice QR per il nostro dataset e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Yusuf Zhang e sono della Penn State University. Oggi presenterò il nostro lavoro, Exampler, Semantic Parsing Crosslinguale in molteplici lingue naturali e rappresentazioni semantiche. La semantic parsing è un compito che consiste nel costruire rappresentazioni semantiche di query utente, come SQL e calcolo lambda. E la semantic parsing crosslinguale è il compito di tradurre query in molteplici lingue naturali in molteplici rappresentazioni semantiche. Come mostrato in questa figura, dobbiamo tradurre la query in molteplici lingue naturali utilizzando modelli neurali in SQL, Lambda, FunQL e simili. I modelli esistenti di semantic parsing crosslinguale sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitate. Ad esempio, si riscontrano lacune di copertura in alcune lingue naturali. Il cinese è assente e si riscontrano lacune di copertura in alcune rappresentazioni. Il calcolo lambda è assente. Oppure vengono valutati solo su determinati modelli più recenti. Ad esempio, esiste un solo modello per valutarli. A tal fine, proponiamo Exampler, forniamo un dataset uniforme, Exampler, per la semantic parsing crosslinguale in molteplici lingue naturali e rappresentazioni semantiche. Contiene nove dataset in vari domini, cinque compiti di semantic parsing, otto rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è TranslateTest. Utilizziamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese su query in inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando l'API in inglese e quindi utilizziamo il modello addestrato per prevedere l'SQL. Testiamo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione. Ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione di fusione monolingue addestrando modelli monolingui con solo il 10% dei dati di addestramento. E testiamo il modello multilingue monolingue, che addestriamo con un singolo modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per tradurre query in tedesco o cinese o simili. Consideriamo anche il trasferimento zero-shot e few-shot crosslinguale. Addestriamo su una lingua di origine e trasferiamo in un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query in inglese o una combinazione di query in inglese e tedesco few-shot per addestrare un modello multilingue e prevedere l'output SQL. E abbiamo anche scoperto molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, tra cui Encoder PDR, che sta per encoder pre-addestrati multilingue con decoder basati su puntatore, come XLMR plus PDR e BERT plus PDR. Valutiamo anche modelli encoder-decoder, ovvero modelli encoder-decoder pre-addestrati multilingue, come MBART e MT5. Abbiamo scoperto che l'encoder-decoder ottiene le prestazioni migliori su tutti e nove i dataset. Valutiamo MT5 e XLMR plus PDR in un'impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder PDR può essere migliorato addestrando in una miscela di varie lingue. Abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni in inglese diminuiscono in sette dataset e aumentano solo in tre dataset. Credo che questo sia noto come curve della multilinguità. Abbiamo anche confrontato il divario di prestazioni crosslinguale. In questa figura, la linea blu è il trasferimento fuchsia crosslinguale. La linea arancione è il trasferimento zero-shot crosslinguale, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e la linea arancione, abbiamo scoperto che per l'impostazione zero-shot, il divario di prestazioni nel trasferimento crosslinguale è significativo. E confrontando la linea blu e la linea arancione, abbiamo scoperto che per l'impostazione fuchsia, il divario di trasferimento si riduce rapidamente. Troviamo anche altri risultati interessanti. Ad esempio, l'encoder-decoder supera il lavoro precedente o ottiene risultati comparabili. L'addestramento su lingue naturali in inglese può migliorare significativamente le prestazioni di Fushot sulle lingue naturali target. E abbiamo scoperto che i modelli linguistici multilingue come Codice e Bloom sono ancora inadeguati per i compiti di semantic parsing crosslinguale. In sintesi, abbiamo creato Exampler, un benchmark unificato per la semantic parsing crosslinguale con molteplici lingue naturali e molte rappresentazioni. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti e simili. E vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Spirakowski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come potreste sapere, diverse teorie e approcci basati su corpora assumono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali, la struttura della coordinazione di elementi coordinati come Lisa, Bart e Maggie è tale che il primo congiunto è la testa dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del testo semantico di Igor Milchuk, dove, ancora una volta, l'intera struttura coordinata è testa del primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Singolano uno dei congiunti. Esistono anche approcci simmetrici alle strutture coordinative come l'approccio di Praga, l'approccio a testa di congiunzione assunto nei dependency tree bank di Praga, dove le strutture coordinative sono testate dalla congiunzione. Quindi otteniamo dipendenze da e verso tutti i congiunti. E infine c'è anche un approccio multi-testa che viene utilizzato, ad esempio, nella grammatica di parole di Cutson, dove, per così dire, tutti i congiunti sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui laughs a tutti i congiunti separatamente, questi sono bart e making. Ora, l'obiettivo di questo articolo è produrre un argomento nuovo a favore delle strutture simmetriche della coordinazione come queste due e contro le strutture asimmetriche della coordinazione come queste due. Okay, l'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi, in inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani, giusto? Quindi \"March read it yesterday\" va bene perché l'oggetto diretto \"it\" è vicino al verbo, mentre \"March read yesterday it\" è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è l'avverbio \"yesterday\". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione successiva all'avverbio. Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. \"March read this absolutely fascinating book about the bees yesterday\" va bene, dove invece di \"it\" abbiamo questo lungo sintagma nominale. Va bene anche dire \"March read yesterday this absolutely fascinating book about bees\". Quindi il ragionamento qui è che ciò è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più corte sono preferite. Queste due alberi mostrano solo la lunghezza delle dipendenze cruciali, cioè quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da \"red\" all'avverbio di lunghezza 7, misurata in parole, e da \"red\" a \"book\" di lunghezza 4, quindi insieme fa 11. Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto, quindi invece di 11 sei, molto più corto, ecco perché suona abbastanza bene, giusto, viola un principio ma soddisfa un altro. Okay, quindi abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Tree Bank e vedere l'articolo perché non abbiamo usato le dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendano ad essere più brevi. \"Salt and pepper\" e non \"pepper and salt\" misurati in sillabe. E anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione dei congiunti di sinistra più corti è maggiore. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto? Quindi il governatore è a sinistra in questo esempio. \"I saw Bart and Lisa\". Quindi è il governatore, è a sinistra. È assente nel secondo esempio \"Homer came and sneezed\" qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, quindi in tali casi il congiunto di sinistra preferisce essere più corto, tanto più grande è la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui \"left governs the coordination ted and net\" questo effetto scompare, quindi mostriamo misurando la lunghezza in caratteri, che è la prima colonna, in sillabe, la colonna centrale e in parole, la colonna a destra. Quindi mi concentrerò sulla colonna giusta. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra a essere più corto cresce costantemente con la differenza assoluta in parole e lo stesso viene osservato quando non c'è un governatore, come nella coordinazione di frasi, ma quando il governatore è a destra questa tendenza scompare e dimostriamo nel documento come questo fornisca un argomento contro le strutture asimmetriche della coordinazione come queste due e a favore delle strutture simmetriche come queste due. Consultare l'articolo per la piena approvazione e gli argomenti e parlarne con noi nella sessione successiva. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Kyo Yin e presenterò il nostro lavoro dal titolo \"Quando la traduzione richiede un contesto? Un'esplorazione multilingue basata sui dati\". Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase? Beh, se la frase precedente fosse \"Le cose potrebbero iniziare a farsi pericolose se i ministri lo scoprissero\", allora \"mole\" si riferisce a una spia. Ma se la frase precedente fosse \"Potrebbe essere qualcosa di serio, dottore?\", allora \"mole\" si riferisce a un segno di nascita. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, cambia anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come Blue incapaci di catturare queste traduzioni. E alcuni hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e un numero limitato di lingue, poiché in genere si basano sulla conoscenza del dominio e sulla curatela umana. In questo lavoro, cerchiamo di rispondere a queste due domande: prima, quando la traduzione richiede un contesto? E, secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel nostro lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazioni il contesto C fornisce sul target Y dato il source X. Potete pensare a CXMI come alle informazioni acquisite fornendo un contesto al modello. In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un PCXMI elevato come quelle che richiedono un contesto per la traduzione. Ora analizziamo le parole con un PCXMI elevato per cercare schemi tra queste parole. E conduciamo la nostra analisi su trascrizioni di discorsi TED tradotti dall'inglese in quattordici lingue diverse. Conduciamo la nostra analisi a tre diversi livelli. Innanzitutto, esaminiamo le etichette delle parti del discorso che hanno mezzi elevati di PCXMI. Ciò ci consente di trovare, ad esempio, pronomi duali in arabo che hanno un p sei mi relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali. Quindi è necessario un contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, scopriamo che anche alcune lingue richiedono un contesto quando vogliamo scegliere la forma verbale appropriata. Esaminiamo quindi gli elementi di vocabolario che hanno un p sei mi elevato, calcolato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come quello qui presente in cui, in cinese, è necessario un contesto per tradurre correttamente. E infine, scopriamo che il contesto è importante per tradurre nella forma di cortesia corretta. Infine, esaminiamo diversi token individuali che hanno un p6mi elevato. Ciò ci consente di identificare fenomeni che non possono essere catturati davvero dalla parola stessa, ma piuttosto espressi in una struttura della frase, come la risoluzione dell'ellissi. Ora utilizziamo i nostri risultati dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno, e chiamiamo il nostro tagger Multilingual Discourse Aware o MUDA tagger. Possiamo anche notare che le diverse lingue hanno proporzioni diverse di questi fenomeni discorsivi. Quindi utilizziamo il tagger MUDA applicando il tagger sul corpus parallelo che vogliamo utilizzare per la valutazione, e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato. Infine, utilizziamo il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus, quindi per Blue, scopriamo che i modelli indipendenti dal contesto hanno le prestazioni migliori, ma se utilizziamo Comet, i modelli consapevoli del contesto funzionano meglio. E se utilizziamo la misura WordF, allora i modelli con o senza contesto hanno prestazioni comparabili. Ciò dimostra nuovamente che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da sole. Ora utilizziamo il benchmark MUDA per valutare i modelli e scopriamo che i modelli a livello di contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per determinati fenomeni discorsivi, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni, come l'ellissi, i pronomi e la forma verbale. Quindi questo suggerisce in quale direzione dovremmo vedere maggiori progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DPL è solitamente più accurato di Google Translate per la traduzione a livello di documento. In sintesi, eseguiamo un'analisi basata sui dati attraverso quattordici coppie linguistiche per identificare quando le traduzioni richiedono un contesto. E quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali fenomeni discreti i modelli possono gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo domani."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato del primo anno alla Carnegie Mellon University, e oggi vi presenterò il nostro lavoro, \"Anal Positionale, Caratterizzazione dei Bias di Progetto, Beta Set e Modelli\". Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell’Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp. Quindi, iniziamo immaginando che stiate lavorando per un giornale e state esaminando i commenti sotto il vostro articolo di notizie nel tentativo di rimuovere contenuti tossici. Potreste ricorrere a una API popolare, come Perspective API per il rilevamento della tossicità, e questo funziona molto bene se siete Carl Jones, dove le API Perspective sono in grado di rilevare correttamente istanze tossiche. Ma non è proprio così per Dithya Sharma, dove le API Perspective non sono così sensibili ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progetto in cui vediamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni. I bias di progetto come quello che abbiamo appena visto possono verificarsi a causa della posizione (positionality) dei ricercatori NLP e degli sviluppatori di modelli. La \"positionality\" è semplicemente l'insieme delle prospettive che le persone detengono a causa della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatori, la \"positionality\" può influenzare il processo di ricerca e i suoi risultati perché può cambiare le decisioni che i ricercatori prendono. Quindi, una domanda che le persone potrebbero porsi è: dataset e modelli hanno \"positionality\"? E non stiamo dicendo che i modelli stessi e i dataset stessi hanno identità demografiche ed esperienze di vita, ma essi aggregate giudizi e opinioni di persone reali e possono quindi rappresentare certe \"positionalities\" rispetto ad altre. Quindi, lavori precedenti hanno suggerito alcune prove aneddotiche di avere \"positionality\", come le lacune culturali nei modelli e nei dataset, nonché definizioni teoriche di \"positionality\" del modello. Tuttavia, questi lavori non si concentrano realmente sul confronto degli utenti finali con i dataset e i modelli stessi. Studiare la \"positionality\" dei modelli e dei dataset è sempre più importante man mano che le attività di NLP diventano più soggettive e socialmente orientate. Ed è difficile caratterizzare come queste \"positionalities\" siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la \"positionality\" dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con dataset e modelli esistenti. Lo facciamo attraverso il nostro framework, NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è ri-annotare i dataset con annotatori diversi. Scegliamo di farlo piuttosto che analizzare la demografia dei dataset originali, perché di solito solo pochi annotatori annotano ciascuna istanza e perché la demografia è raramente raccolta e condivisa. Quindi, scegliamo di ri-annotare i dati per ottenere molti annotatori per istanza e per ottenere un ricco insieme di dati demografici. Prendiamo quindi le annotazioni con i dati demografici e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Parsons R. Quindi, il nostro framework differisce dalla letteratura sulla non concordanza degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei dataset, anziché concentrarsi solo sull'accordo tra gli annotatori o sulla modellazione delle distribuzioni degli annotatori. Il nostro framework è ampiamente reso possibile da Lab in the Wild, una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno principalmente partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere dati di alta qualità. Ospitiamo due attività su Lab in the Wild, una delle quali è l'accettabilità sociale. E il funzionamento di questa attività è che i partecipanti leggeranno una situazione dal dataset di Social Chemistry e poi scriveranno quanto una situazione sia socialmente accettabile. In seguito, per rimanere impegnati nello studio, possono confrontare le loro risposte con un'IA e con quelle di altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry Delphi in GPT 4. Abbiamo quindi replicato un allestimento molto simile per l'attività di rilevamento della tossicità e dell'odio, in cui leggeranno un'istanza da DynaHate e scriveranno se pensano che si tratti di un'istanza di odio. Abbiamo quindi confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta in GPT 4. Il nostro studio ha accumulato alla fine oltre sedici mila annotazioni da oltre mille annotatori provenienti da ottanta sette paesi. Quindi ora siamo meglio equipaggiati per rispondere alla domanda: con chi si allineano di più i dataset e i modelli di NLP? Scopriamo che c'è \"positionality\" in NLP. Ad esempio, scopriamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di GPD 4, scopriamo che è più allineato ai paesi di lingua inglese e confuciana. Scopriamo anche che DanaHate è più allineato ai paesi di lingua inglese. Troviamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi, per GPD 4 nell'attività di accettabilità sociale, scopriamo che è più allineato con le persone con un'istruzione universitaria o post-universitaria. E troviamo lo stesso per DanaHate, dove è più allineato con le persone con un'istruzione universitaria. Tuttavia, quando modelli e dataset sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di questo è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro colleghi maschi e femmine. Troviamo questo anche nell'attività di accettabilità sociale di GPT 4 e nell'analisi di DynaHate. Quindi, dato che c'è \"positionality\" in NLP, cosa possiamo fare al riguardo? Quindi, abbiamo alcune raccomandazioni a riguardo. La prima è di tenere un registro di tutte le scelte di progettazione rilevanti durante il processo di ricerca. E l'altra è di svolgere la ricerca di NLP attraverso la lente del perspectivismo. La nostra terza raccomandazione è di costruire dataset e modelli specializzati all'interno di specifiche comunità. E un buon esempio di questo è l'iniziativa Masakane. E vogliamo sottolineare che l'NLP inclusivo non significa semplicemente fare in modo che tutte le tecnologie funzionino per tutti. E questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, oggi parlerò del nostro lavoro sulla risoluzione di espressioni referenziali indirette per la selezione di entità, in cui introduciamo l'altentities corpus. Mi chiamo Javot Hosseini, e questo è un lavoro congiunto con Philip Radlinsky, Silvia Pareti e Annie Luis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono effettuare una scelta. Considerate questa domanda alternativa. Intendevate \"Easy on Me\" o \"I Got a Feeling\"? Qui, l'utente vuole selezionare uno tra questi due brani. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome del brano \"Easy on Me\" o la sua posizione, il primo, ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome del brano, o quando le pronunce sono troppo simili tra loro e difficili da disambiguare, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti. Ad esempio, quello più recente o il brano che non è energico. Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione di entità dei modelli linguistici di grandi dimensioni (LLM). Non siamo a conoscenza di un dataset pubblico, un dataset pubblico su larga scala per il compito, quindi ne abbiamo creato uno utilizzando l'annotazione tramite crowdsourcing. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dati enfatizza l'informalità utilizzando una configurazione di completamento di cartoni animati. Il cartone animato ha tre fumetti. Nel primo fumetto, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con questo, Bob stabilisce il contesto del dialogo. Nel secondo fumetto, Alice dice: \"Intendevi 'Easy on Me' o 'I Got a Feeling'?\", che è la domanda alternativa. E nel terzo fumetto, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio, il Neo-Ervandal. Forniamo il primo e il secondo fumetto automaticamente, ma il terzo viene compilato dall'annotatore. Il primo fumetto è scelto da alcuni prompt manuali per dominio. Il secondo, che è la domanda alternativa, viene generato come segue: usiamo sempre un semplice modello. Intendevi A o B? dove A e B sono prelevati da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è solitamente più difficile effettuare la disambiguazione. Il primo è uniforme a caso, il secondo è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome, quindi restituiscono il terzo è quando hanno descrizioni simili su Wikipedia, e infine quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per un brano. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente informazioni su di esse. Quindi, quello che facciamo è mostrare alcune conoscenze di base su entrambe le entità. Per i brani, mostriamo semplicemente un collegamento alla ricerca di Google per ogni brano e poi chiediamo agli annotatori di ascoltare almeno alcuni tratti di ciascun brano e leggere a riguardo. Ecco, ad esempio, il risultato della ricerca di Google per il brano \"Easy on Me\". Per i domini di ricette e libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e descriverla utilizzando tre o cinque espressioni referenziali indirette. Ad esempio, quello con la musica del pianoforte, oppure quello senza parole, non quello con il dodicenne, o quello fittizio, oppure quello proveniente dall'Azerbaijan e così via. L'altentities corpus ha 6.000 domande alternative attraverso tre domini e ha 42.000 espressioni referenziali indirette. I risultati con il modello T5xLarge sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, l'accuratezza è molto alta. È intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87 percento, il che è più realistico, ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%. Quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro dataset. Grazie."}
