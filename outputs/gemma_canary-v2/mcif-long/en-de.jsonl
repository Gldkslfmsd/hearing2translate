{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lendermann und heute werde ich Ihnen eine kurze Einführung in unser Paper über compositionale Generalisierung ohne Bäume mithilfe von Multi-Set-Tagging und latenten Permutationen geben. Dies ist eine Gemeinschaftsarbeit mit meinen Betreuern Alexander Koller und Ivan Titoff. Compositionale Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen und ungesehene Kompositionen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden. Im Kontext von semantischem Parsen könnte das Testen auf compositionale Generalisierung so aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall „the girl slept“ und „Mary knew that the girl slept“. Diese Äußerungen sind mit logischen Formen gepaart, die zentrale Aspekte ihrer Bedeutung repräsentieren. Im Gegensatz zur Standard-Machine-Learning-Evaluation stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings flachere Rekursionen gesehen und wird mit einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Generalisierung und erzeugen oft Ausgaben, die vom Input abgekoppelt sind. Insbesondere reproduzieren sie oft nicht die systematischen Korrespondenzen zwischen Input und Output, wie sie beispielsweise farblich im Beispiel hervorgehoben sind. Eine beliebte Methode, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den compositionalen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht bereitgestellt und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein. Typischerweise ist dies mit erheblicher formalismuspezifischer Vorverarbeitung der logischen Formen verbunden, beispielsweise zur Handhabung von Variablen. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten. In diesem Paper verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursionen ohne die Verwendung von Bäumen. Unser Ansatz sagt den Output aus dem Input in zwei Schritten voraus. Zuerst taggen wir jedes Input-Token mit einem ungeordneten Multi-Set von Token, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Aus diesem Grund verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zum Vorhersagen einer Permutation ein, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell grob wie folgt. Wir gehen von links nach rechts über den Output und bestimmen, welches Multi-Set-Token in jede Position gehört. Für die erste Output-Position wählen wir einfach eines aus, wie im Rot hervorgehoben. Dann springen wir zum nächsten Multi-Set-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multi-Set-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumfreien Modellen am Koggs-Benchmark. Unser Modell übertrifft die anderen um einen großen Abstand bei der Generalisierung auf tiefere Rekursionen. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Paper lösen wir einige interessante technische Herausforderungen. Zuerst einmal ist die Ausrichtung zwischen Input und Output nicht in den Trainingsdaten gegeben. Infolgedessen wissen wir für ein gegebenes Token nicht, aus welchem Multi-Setter es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir begegnen diesem Problem, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstbewerteten Permutation NP-hart ist. Das liegt daran, dass es mit dem Traveling-Salesman-Problem in Verbindung steht. Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zu backpropagieren und die linguistisch plausibleren Permutationen zu erlernen. Wenn Sie mehr über unsere Experimente und darüber erfahren möchten, wie wir diese Herausforderungen angehen, werfen Sie bitte einen Blick auf unser Paper oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute werde ich über unser Paper sprechen, „Markierte Personas: Die Verwendung natürlicher Sprachprompts zur Messung von Stereotypen in Sprachmodellen“. Diese Arbeit entstand in Zusammenarbeit mit Essendermouch und Dangerowski. In den letzten Jahren haben viele die Prävalenz sozialer Vorurteile und Stereotypen in großen Sprachmodellen, oder LLMs, dokumentiert. Diese Messungen weisen jedoch verschiedene Einschränkungen auf. Sie basieren in der Regel auf manuell erstellten Datensätzen, deren Kuratierung sehr zeitaufwendig ist. Außerdem messen sie meist nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere demografische Gruppen oder Kontexte generalisieren oder lediglich sehr allgemeine, weit gefasste Assoziationen erfassen, wie z. B. negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich keine Intersektionalität, also die Vorstellung, dass facettenreiche soziale Identitäten Vorurteile verstärken und einzigartige Schadensorte entstehen lassen können. Um diese Einschränkungen zu überwinden, stützen wir uns auf die Eigenschaft, dass diese neueren, instruktionsgesteuerten LLMs sehr gut darin sind, Anweisungen und Prompts zu befolgen. Wir können das Modell also bitten, eine Persona zu generieren, d. h. eine Darstellung einer fiktiven Person mithilfe eines Prompts wie „Stell dir vor, du bist eine asiatische Frau, beschreibe dich“. Und wir können sofort feststellen, dass dies sehr gut auf jede demografische Gruppe generalisierbar ist, da wir einfach jede gewünschte Identitätsmarkierung in diesem Prompt angeben können. Hier sind einige Beispielgenerierungen von GPT 4. Sofort sehen wir, dass die Ausgaben zwar nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, aber einige interessante Muster aufweisen. Die asiatische Frau wird als unaufdringlich dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und wie der Beschreibung einer faszinierenden Region bezeichnet, und beide Frauen of color-Personas machen Bezug auf ihre Abstammung, während die weiße Mann-Persona dies nicht tut. Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste besteht darin, diese Personas zu generieren. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Prompts an menschliche Probanden vergeben wurden, wobei festgestellt wurde, dass sie dadurch auch rassische Stereotypen aufdecken konnten. Außerdem ermöglicht dies einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil ist „Marked Words“, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von nicht-markierten Gruppen unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil dabei ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Marked Words-Methode greift auf das soziolinguistische Konzept der Markierung zurück, das besagt, dass es ein unmarkierter Standard gibt und jede Gruppe, die von diesem Standard abweicht, sprachlich markiert ist. So ist beispielsweise das Wort Mann – oder Entschuldigung, das Wort Krieger – normalerweise mit Männern assoziiert. Wenn also Menschen einen Krieger beschreiben, der eine Frau ist, werden sie in der Regel „ein Mann Krieger“ spezifizieren und den Begriff mit „Frau“ markieren. Und im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen als unmarkiert und welche als markiert gelten. Anschließend vergleichen wir die Personas mithilfe der Fighting Words-Methode, die im Wesentlichen die Verwendung gewichteter Logodds-Verhältnisse ist, um die wichtigsten Wörter für jede markierte Gruppe zu unterscheiden. So würden wir beispielsweise für die Personas von schwarzen Frauen Fighting Words durchführen und die Logodds-Verhältnisse sowohl mit den weißen Personas als auch mit den Mann-Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas deutlich mehr Stereotypen enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter im Lexikon betrachten, stellen wir sehr unterschiedliche Dinge fest. Während die generierten Personas viel höhere Raten von Lexikonwörtern aufweisen, weisen die von Menschen verfassten eine viel breitere Verteilung von Wörtern auf, wobei die Stereotypwörter, die in den generierten Personas vorkommen, wirklich nur die Wörter groß und sportlich sind. Wirklich also nur die positiven oder zumindest nicht-negativen. Und in der Tat erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht gut. Stattdessen werden wir uns daher für die Ergebnisse unserer Marked Words-Methode wenden, um zu zeigen, wie diese vermeintlich positiven Wörter Stereotypen und essentialisierende Erzählungen ermöglichen. In unserer Analyse zeigen wir auf, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens umfassen die Top-Wörter für markierte Gruppen Dinge wie Kultur, Tradition, Stolz und exotisch. Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln viele gängige Tropen, insbesondere für Frauen of color, in diesen Wörtern wider. So umfassen beispielsweise die Wörter, die für lateinamerikanische Frauen verwendet werden, Begriffe wie lebhaft und kurvig, die sich auf einen Tropen des Tropikalismus beziehen. Für asiatische Frauen sind die Wörter Dinge wie zierlich und zerbrechlich und seidenartig, was auf eine lange Geschichte der Hypersexualisierung asiatischer Frauen zurückgeht, die als sehr demütig und unterwürfig angesehen werden und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind. Dies entspricht einem Archetyp, der als „strong black woman“-Archetyp bezeichnet wird. Und obwohl es auf den ersten Blick positiv klingt, haben Arbeiten gezeigt, dass dieser Archetyp tatsächlich sehr schädlich ist, da er andere demografische Gruppen unter Druck setzt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt also tatsächlich daran zu arbeiten, diese Hindernisse zu beseitigen, setzt er diese Menschen unter Druck, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen und andere Schäden führt. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellinhaber. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Erzählungen angehen. Wir sollten auch eine intersektionale Perspektive verwenden, um Vorurteile und Schäden zu untersuchen, da es viele Dinge geben kann, die übersehen werden, wenn wir dies nicht tun. Und schließlich sollte es wirklich eine verstärkte Transparenz in Bezug auf Methoden zur Vorurteilsminderung geben, denn zum Beispiel wissen wir nicht, ob es sich bei diesen positiven Stereotypen um eine Art von seltsamer, übermäßiger Werteausrichtung oder vielleicht um einige andere, antistereotypische Methoden handelt, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank für Ihre Aufmerksamkeit. Ich wünsche Ihnen einen schönen Aufenthalt auf der ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABCEval, einen neuen dimensionalen Ansatz zur Bewertung von Konversations-KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von Human Evaluation, beispielsweise durch die Bitte von menschlichen Gutachtern, zu wählen, welche von zwei Konversationen besser ist, oder Konversationen anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um ganzheitliche Bewertungen der Gesamtqualität des Dialogs zu liefern, aber die Qualität des Dialogs hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz ist es, menschliche Gutachter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, beispielsweise die Relevanz der Modellantworten, unter Verwendung bestehender Vergleichs- oder Likert-Skalenmethoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Durch die explizite Annotation, ob jede Modellantwort bestimmte Verhaltensweisen aufweist, wie z. B. die Antwort mit irrelevanten Informationen oder die Widerspruch zur eigenen Aussage. Wir nennen diesen Ansatz die Annotation von Verhaltensweisen im Chat, oder kurz ABCEval. Wir haben diese Methode entwickelt, um Chat-Modell-Verhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als wirkungsvoll für die Chat-Qualität genannt wurden. ABCEval ist in der Lage, die Raten zu messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Beispielsweise misst ABCEval die Anzahl der Runden, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Gesprächspartner widerspricht, falsche Fakten halluziniert oder gängiges Menschenwissen verletzt, und wann das Modell Empathie zeigt oder nicht. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie anhand von hundert menschlichen Bot-Konversationen pro Modell mit ABCEval bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Rundenebene, Likert-Bewertungen auf der Dialogebene und Dialogebenen-Paarvergleiche. Zusätzlich zu den Bewertungsmethoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekten des Dialogs erhoben, da dies die Standardpraxis zur Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unseren Analysen dieser Bewertungsergebnisse stellten wir fest, dass ABC-Eval-Verhaltensbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden erhoben wurden, gemessen am Inter-Annotator-Agreement auf 100 doppelt beschrifteten Konversationen. Darüber hinaus sind ABC-Eval-Bezeichnungen prädiktiver für die Gesamtqualität der Konversation im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. Sie können beispielsweise sehen, wie die Messung des Anteils der Runden mit Selbst- und Partnerwidersprüchen fünf Prozent bzw. zehn Prozent der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenzwerte nur vier Prozent oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmethode einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression verwendeten. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 Prozent der Gesprächsqualität erklären, und wenn Sie die Metriken einzeln entfernen, führt dies meist dazu, dass ein erheblicher Teil der Informationen über die Qualität verloren geht. Und die Kombination von alternativen Likert-Metriken auf Ebene der Dialoge erklärt weitaus weniger der Qualität, und weniger dieser Metriken tragen eindeutige Informationen. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen es uns, Konversations-KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden vermochten. Sie können in den Ergebnissen unseres Experiments sehen, dass mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Beispielsweise weisen die von uns getesteten Bots in etwa zwanzig Prozent ihrer Antworten Verletzungen des gesunden Menschenverstandes auf. Sie erzeugen in etwa fünfzehn Prozent der Antworten irrelevante Informationen, und sie widersprechen sich selbst oder ihrem Gesprächspartner etwa 10 % der Zeit. Angesichts des rasanten Fortschritts in diesem Bereich könnten viele dieser Fehlerraten in neuen Modellen sinken, die seit unserer Bewertung veröffentlicht wurden. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmethoden zur Verfolgung des Fortschritts neuer Modelle zu entwickeln. Wir hoffen, dass ABCEval von anderen in diesem Bereich genutzt werden kann, als einen sinnvollen Schritt in diese Richtung, und wir freuen uns darauf zu sehen, wie sich die Konversations-KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag, mein Name ist Vasudha und ich bin Doktorandin der Informatik an der Stony Brook University. Ich möchte Ihnen unsere Arbeit präsentieren, die als Long Paper auf der ACL 2023 angenommen wurde, nämlich „Transfer Learning for Dissonance Detection“, die sich mit der Herausforderung seltener Klassen befasst. Wir beginnen damit, kognitive Dissonanz zu definieren und zu erläutern, warum es sich um ein wichtiges Problem handelt, das in der Sprachverarbeitung untersucht werden sollte. Einfach ausgedrückt, ist kognitive Dissonanz ein Widerspruch zwischen zwei Überzeugungen oder Handlungen, wie in diesem Beispiel, in dem eine Person sagt: „Ich weiß, dass Zigaretten mich töten könnten“ und dann hinzufügt: „Ich habe mir nach dem Meeting ein paar Zigaretten gegönnt“. Diese Überzeugung und Handlung sind inkonsistent und stehen in Dissonanz zueinander. Die weitere Bemerkung, dass sie ihre Arbeit ohne sie nicht halten könne, rechtfertigt das zweite Vorkommnis und es besteht eine Konsonanzbeziehung. Obwohl Dissonanz ein sehr häufiges Phänomen ist, das wir bei täglichen Entscheidungen erleben, findet es sich selten in der Sprache, im Gegensatz zu anderen Diskursbeziehungen, wieder. Warum ist das wichtig? Die Untersuchung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Wertvorstellungen sowie Verhaltensänderungen in der Bevölkerung zu verfolgen. Eine hohe kognitive Dissonanz steht zudem in Zusammenhang mit Angststörungen und kann ein besseres Verständnis der psychischen Gesundheit ermöglichen. Die Untersuchung von Dissonanz, die in der Sprache zum Ausdruck kommt, kann auch für das Verständnis von Extremismus und der Polarisierung schutzbedürftiger Gruppen von Vorteil sein. Schließlich ist das Verständnis kognitiver Dissonanz wichtig, um persönliche kognitive Stile von Individuen zu verstehen und Entscheidungsprozesse besser zu analysieren. Um das Ziel eines kognitiven Dissonanz-Ressourcen zu erreichen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir haben einen \"Dissonanz-zuerst\"-Ansatz verwendet, wie im hier gezeigten Flussdiagramm dargestellt. Tweets wurden mithilfe eines PDTB-Parsers verarbeitet und Paare von Diskurs-Einheiten gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie man hier sehen kann, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir rund 1.000 Beispiele von Diskurs-Einheiten-Paaren gesammelt hatten, führten wir ein Training für einen anfänglichen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Überraschend war es nicht, dass der Klassifikator nicht wesentlich besser als zufällig abschneidet. Angesichts des geringen Vorkommens von Dissonanz und des Fehlens eines solchen Datensatzes stellen wir uns dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transfer Learning und Active Learning, um eine solche Annotation zu ermöglichen, dass mehr Dissonanzbeispiele in weniger Annotationrunden gesammelt werden können, wodurch die Gesamtkosten der Annotation gesenkt und gleichzeitig die Dissonanzerkennung verbessert wird. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den Active-Learning-Prozess mit der Übertragung von Gewichten von eng verwandten Aufgaben. Wir übertragen von zwei verschiedenen Aufgaben: der aufgabenspezifischen Dissonanz-Stance-Klassifizierung in Debatten, einer Aufgabe, die feststellt, ob zwei Debattenaussagen verschiedener Personen übereinstimmend oder nicht übereinstimmend sind, unabhängig vom Thema (Debatte genannt), und der binären Klassifizierung der Erweiterungs- und Vergleichsklassen von PDTB, da diese eng mit dem Konzept der Konsonanz und Dissonanz verwandt sind und wir sie CE nennen. Wir stellen fest, dass die Übertragung bereits eine viel bessere Leistung als zufällig auf dem annotierten Datensatz erzielt wird (AUC 0,62). Darüber hinaus stellen wir fest, dass das iterative Fein-Tuning auf beiden Aufgaben, gefolgt von weiterem Fein-Tuning auf der Debatte, eine deutlich bessere Leistung erzielt. Dies ist das Modell, das wir zur Initialisierung des Active Learning verwenden. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Annotationsrunde zu aktualisieren. Kumulativ akkumuliert alle Daten, die bisher durch Active Annotationen gesammelt wurden, während iterativ das Modell durch das Training mit dem neuesten Satz von Daten aktualisiert wird. Unter den verschiedenen Strategien stellten wir fest, dass kumulativ gleich gut oder besser als iterativ abschneidet. Anschließend verwenden wir eine Strategie für seltene Klassen (PRC), um hauptsächlich Beispiele auszuwählen, die vom aktuellen Modell in jeder Runde des AL als höchstwahrscheinlich dissonant eingestuft werden. Wir vergleichen dies mit anderen State-of-the-Art-AL-Strategien, die in der Community üblicherweise verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung für zufällige Auswahl deutlich geringer ist. Mit weiteren Runden von AL mit den beiden besten Strategien verbesserten wir die AUC der Dissonanzklassifizierung auf 0,75, was die bisher beste Leistung für diese Aufgabe darstellt. Wir haben auch die Machbarkeit jeder Strategie hinsichtlich der Annotationsqualität und der Kosten für die Annotateure überprüft. Wir stellen fest, dass PRC den höchsten Anteil an Dissonanz aufweist und am besten für seltene Klassen geeignet ist. Die Annotatoren empfinden die Beispiele jedoch auch als schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Akquisition seltener Klassen ist und dass das Starten von Active Learning mit Transfer-Learning-Aufgaben, die sinnvoll gestaltet wurden, erheblich helfen kann. Wir stellen auch fest, dass iteratives Update für das Transfer Learning aus einem anderen Bereich nützlich ist, während Active Annotationen innerhalb des gleichen Bereichs von kumulativem Update profitieren. Hier sind die Links zu unserem Code, unserem Datensatz und unserem Paper. Zögern Sie nicht, uns bei Fragen zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Aksheta und heute präsentieren Martin und ich unsere Arbeit „Kitmus: Evaluierung der Wissensintegration aus Mehrfachquellen“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverständnismodelle greifen auf eine Vielzahl von Wissensquellen zurück, wie beispielsweise Wissen, das in ihren Parametern enthalten ist und typischerweise durch Vortraining erworben wurde, sowie Wissen, das zur Inferenzzeit in den Eingaben bereitgestellt wird. Jüngste Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen. Natürliches Sprachverständnis erfordert jedoch oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel kann in dem Satz „John sah den neu gewählten Präsidenten im Fernsehen“ vortrainiertes Wissen Informationen darüber enthalten, was Präsidenten tun und was ein Fernsehen ist, aber es kann nicht zuverlässig wissen, wer diese instanzspezifische Entität John ist oder wer der neue Präsident ist, da sich der Präsident möglicherweise seit dem Vortraining geändert hat. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch zur Inferenzzeit bereitgestelltes Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite zur Wissensintegration vor. Wir stellen eine Kerrelationsaufgabe vor, die dazu dient, die Fähigkeit zu untersuchen, auf Wissen zuzugreifen, das in verschiedenen Quellen verfügbar ist. Wir evaluieren den Datensatz mit menschlichen Studienteilnehmern und etablierten Kerrelationsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist Richter, Kia ist Bäckerin. Termin und Kia trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er in einem Gericht Fälle entschied, freute er sich, sich zu entspannen. Die Aufgabe besteht hier darin, die korrekte Entität zu identifizieren, auf die das Pronomen „er“ sich bezieht, nämlich Sermon. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, wissensspezifisches Wissen wie „Servin ist Richter“. Und zweitens, Hintergrundwissen, das während des Vortrainings großer Sprachmodelle erlernt wird, während wissensspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Wissensarten, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von Kitmus definiert. Erstens haben wir die Topik-Einstellung, Hintergrund-Vortraining, bei der davon auszugehen ist, dass Hintergrundwissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es die Hintergrund-beide-Einstellung, und drittens die Hintergrund-Inferenz-Einstellung, bei der beide Wissensarten nur zur Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie einen Fall simuliert, in dem das zur Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der vortrainierten Daten der Modelle ist, beispielsweise weil seit dem Vortraining neue Berufe entstanden sind. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen steuern. In der Hintergrund-Vortrainings-Einstellung nehmen wir an, dass das Hintergrundwissen, dass Politiker um Sitze in der Regierung konkurrieren, in den vortrainierten Parametern enthalten ist. In der Hintergrund-Kontext-Einstellung stellen wir das instanzspezifische Wissen „Chichester ist Politiker“ bereit. In der Hintergrund-beide-Einstellung stellen wir zusätzlich sowohl instanzspezifisches als auch Hintergrundwissen über Politiker im Inferenztyp-Kontext bereit. In der Hintergrund-Inferenz-Einstellung stellen wir den fiktiven Beruf „Mirror Tour“ anstelle von „Politiker“ bereit, da „Mirror Tour“ unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir evaluieren den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Kerrelationsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der leistungsstärksten Modelle in der schwierigsten Variante der Hintergrund-Vortrainings-Einstellung. Ohne aufgabenspezifisches Training auf Kitmus leisten beide Modelle keine gute Arbeit. Wenn sie jedoch auf Kitmus trainiert werden, erzielen sowohl C2F als auch Berth for Koref signifikant bessere Ergebnisse als die zufällige Wahl. Dies deutet darauf hin, dass Modelle beim Training auf allgemeinen Kerrelationsdatensätzen lernen, Oberflächenhinweise auszunutzen, die beim Testen auf Kitmus, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die leistungsstärksten Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur zur Inferenzzeit bereitgestellt wird. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen: Viele Kerrelationsmodelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu schlussfolgern, ohne aufgabenspezifisches Training. Mit aufgabenspezifischem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Selbst die leistungsstärksten Modelle haben jedoch Schwierigkeiten, Hintergrundwissen zuverlässig zu integrieren, das nur zur Inferenzzeit präsentiert wird. Wenn Sie an weiteren Details interessiert sind, sehen Sie bitte unser Papier und prüfen Sie den Datensatz im Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Sarah Pappy von der University of Toronto und der Fondazione Bruno Kessler, und ich werde kurz die Arbeit \"Attention as a Guide for Simultaneous Speech Translation\" vorstellen, welche eine Gemeinschaftsarbeit mit Matteo Negri und Marco Turki ist. Was ist simultane Sprechübersetzung? Simultane Sprechübersetzung, oder simulierte Übersetzung, ist der Prozess der Übersetzung gesprochener Sprache in Echtzeit in eine andere Sprache, wodurch eine Kommunikation über Sprachgrenzen hinweg ermöglicht wird. Und was sind die Probleme der aktuellen simulierten Modelle? Spezifische Architekturen werden in der Regel trainiert, indem zusätzliche Module eingeführt werden, die optimiert werden müssen, was zu langen und komplizierten Trainingsprozeduren führt, beispielsweise Trainings mit unterschiedlichen Optimierungszielen und dem Trainieren und Pflegen mehrerer Modelle, um verschiedene Latenzbereiche zu erreichen, beispielsweise das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines weiteren mit zwei Sekunden und so weiter. Was ist also unsere Lösung? Erstens, bereits existierende Offline-ST-Modelle zu verwenden, ohne sie erneut zu trainieren oder spezifische Architekturen für CMLSD anzunehmen. Nur ein Modell für jeden Latenzbereich verwenden und die Latenz über spezifische Parameter steuern. Wir nutzen das bereits durch den Mechanismus der Aufmerksamkeit zwischen Audioeingabe und Texteausgabe erworbene Wissen – den Cross-Attention-Mechanismus – und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung besteht darin, eine Dot- oder Encoder-Decoder-Aufmerksamkeit vorzuschlagen. Es handelt sich um eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben oder nicht, basierend darauf, wohin die Aufmerksamkeit zeigt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. diese Summe unterhalb eines bestimmten Schwellenwerts alpha in Bezug auf die letzten lambda Sprachrahmen liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist. Wenn wir beispielsweise einen Sprachabschnitt erhalten, der \"Ich werde über sprechen\" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, werden wir uns die Cross-Attention-Gewichte ansehen und feststellen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen zeigen, während das letzte Wort auf die letzten empfangenen Sprachrahmen, die lambda Sprachrahmen, zeigt. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während wir, da die Summe der Cross-Attention über einem bestimmten Schwellenwert alpha liegt, das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten. Wenn wir fortfahren und einen weiteren Sprachabschnitt erhalten und unser Modell drei weitere Wörter vorhersagt, werden wir uns die Cross-Attention-Gewichte ansehen und feststellen, dass kein Wort auf die letzten lambda Sprachrahmen zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse ansehen, stellen wir die Ergebnisse der simultanen Sprechübersetzung in Diagrammen dar, in denen wir auf der einen Seite Blau haben, das die Übersetzungsqualität und die durchschnittliche Verzögerung misst, d.h. das Latenzmaß. Wir berücksichtigen auch die rechnerisch bedingte durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen also, dass unsere Kurven in diesem Diagramm so hoch wie möglich sind, aber auch, dass sie nach links verschoben sind. Wir vergleichen sie mit Vorbereitungsstrategien, die ebenfalls auf Offline-Modelle angewendet werden, wie z.B. die Weight Key-Strategie und die Local Agreement. Wir vergleichen auch mit dem Stand der Technik an Architekturen, die speziell für die simultane Sprechübersetzung entwickelt wurden. Dies sind alle Ergebnisse der simultanen Sprechübersetzungsstrategie auf Deutsch, und wir sehen, dass Dot alle Strategien, die auf Offline-Modelle angewendet werden, übertrifft, da ihre Kurven nach links verschoben sind. Wir sehen auch, dass wir, wenn wir die tatsächliche verstrichene Zeit oder die Rechenzeit betrachten, die schnellste Strategie sind. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie bitte unser Paper. Wir haben auch den Code und die Modelle sowie die simultanen Ausgaben als Open Source freigegeben, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Xuheng. Heute werde ich unseren Aufsatz „Funktionieren Kernel-2003-Namensentitätserkennungssysteme noch gut im Jahr 2023?“ vorstellen. Lassen Sie uns beginnen. Unser Aufsatz untersuchte das Problem der Generalisierung anhand der Aufgabe der Namensentitätserkennung oder NER-Aufgabe. Wir haben festgestellt, dass Modelle Kernel 2003 seit fast 20 Jahren zur Entwicklung von NER verwenden. Dies wirft natürlich mehrere Probleme auf. Erstens: Können diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung erforderlich? Wenn wir eine schlechte Generalisierung beobachten, was verursacht dann den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Kernel-plus-plus-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News ab 2020 gesammelt und mit den gleichen Kernel-2003-Anmerkungsrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf Kernel 2003 feineingestellt. Wir haben sie sowohl auf dem CoNLL-3-Testdatensatz als auch auf dem CoNLL-plus-plus-Testdatensatz evaluiert. Und schließlich haben wir die prozentuale Veränderung der F1-Metrik berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Durch unsere Experimente haben wir festgestellt, dass drei Hauptfaktoren erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle in der Regel besser auf neue Daten generalisieren. Der zweite Faktor ist die Modellgröße. Wir haben festgestellt, dass größere Modelle in der Regel zu einer besseren Generalisierung führen. Und schließlich wissen wir alle, dass die Anzahl der Beispiele für das Feintuning die Leistung einer nachgelagerten Aufgabe direkt beeinflusst. Auch hier haben wir festgestellt, dass mehr Feintuning-Beispiele tatsächlich zu einer besseren Generalisierung führen. Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptives Overfitting, das Overfitting ist, das durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird. Dies äußert sich in der Regel als abnehmende Erträge bei einem neuen Testdatensatz. Die zweite Hypothese ist zeitlicher Drift, das heißt der Leistungsabfall, der durch die zunehmende zeitliche Kluft zwischen den Trainings- und Testdaten verursacht wird. Bei den Daten für Overfitting sahen wir, dass die rote, beste Anpassungslinie im Diagramm auf der rechten Seite einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL 2003 erzielt haben, zu mehr als einer Einheit der Verbesserung auf CoNLL-plus-plus führt, was bedeutet, dass keine abnehmenden Erträge vorliegen. Dies zeigt uns, dass adaptives Overfitting in diesem Fall nicht beobachtet wird. Was ist aber mit zeitlichem Drift? Um den zeitlichen Drift zu untersuchen, haben wir ein Experiment durchgeführt, bei dem wir einige Modelle mit aktuelleren Daten erneut trainiert oder fortpräpariert haben. Wir haben festgestellt, dass die Leistung mit einer größeren zeitlichen Kluft abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feintuning-Beispiele benötigen, und diese hängen zusammen. Wir können nicht nur einen Faktor haben und die anderen verwerfen. Gleichzeitig haben wir auch festgestellt, dass der hier beobachtete Leistungsabfall durch zeitlichen Drift und nicht durch adaptives Overfitting verursacht wird, obwohl CoNLL 2003 seit über 20 Jahren verwendet wird. Wenn wir nun zur Frage zurückkehren, die wir in der Überschrift unseres Aufsatzes gestellt haben: Funktionieren CoNLL-2003-Tagger noch im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Aufsatz weitere Forschung darüber anregt, wie die Generalisierung von Modellen verbessert werden kann. Und zum Schluss: Bitte schauen Sie sich unseren Aufsatz, unseren Datensatz an und kontaktieren Sie mich gerne, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag, herzlich willkommen zu unserer Präsentation von DeepLean, einem neuen Korpus zur Textvereinfachung für Deutsch auf Dokumentenebene und Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Definieren wir zunächst Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um dessen Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, beispielsweise für Menschen mit Leseschwierigkeiten oder nicht-muttersprachliche Sprecher. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, beispielsweise Dokumente oder Sätze. Im folgenden Beispiel sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in leicht verständliche Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z. B. lexikalische Substitution, Kauskelben, Kauskelben-Umordnung oder Einfügen von Wörtern. Wir schlagen nun unser neues Korpus DPlane vor, da es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Die anderen Korpora sind zu klein, um ein Taxonomie-Modell zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher schlagen wir unser neues Korpus DPlane vor, das in zwei Subkorpora unterteilt ist: DPlane APA und DPlane Web. DPlane APA basiert auf gebrauchten Texten. In DPlane APA haben wir 483 Dokumente alle manuell ausgerichtet. Das ergibt etwa 30.000, 13.000 parallele Satzpaare. Für DPlane Web umfasst dieses Korpus verschiedene Bereiche und wir richten auch alle 750 dieser Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden aus. Insgesamt haben wir 30.450 Satzpaare erhalten. Wir haben unsere Satzpaare etwas genauer analysiert, beispielsweise hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als beispielsweise die Nachrichtentexte oder die Texte für Sprachlernende auf allen Ebenen, beispielsweise in Bezug auf lexikalische Vereinfachung, strukturelle Vereinfachung oder das Gesamtmaß der Vereinfachung. Darüber hinaus können Sie sehen, dass unser DPlane-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir beispielsweise im DPlane APA-Korpus deutlich mehr Umordnungen und Wortänderungen als im DPlane Web-Korpus. Andererseits haben wir im Web-Korpus deutlich mehr Umschreibungen. Sehen wir uns nun an, was wir mit diesem Korpus anfangen können. Guten Tag, mein Name ist Omar und ich werde nun über die Anwendungsfälle für unser Dataset DPlane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, bei der wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in Post-Dokumenten extrahieren wollen, versuchen wir in unserem Fall, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache und denselben Inhalt haben, aber unterschiedliche Komplexitätsstufen aufweisen. Und nun, da wir unser Dataset DPlane haben, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen und den Code zur Durchführung unserer Experimente in der Arbeit veröffentlicht. Abschließend kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode zur Textvereinfachung für Deutsch die Methode des Mass Align ist, und Sie finden den Code zur Ausführung dieser Methode auch auf Ihren eigenen Dokumenten in der Arbeit. Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Long Import-Modell feinabgestimmt, um Vereinfachungen auf Dokumentenebene zu erzeugen, und wir haben auch das normale Base Import-Modell feinabgestimmt, um Vereinfachungen auf Satzebene zu erzeugen. Sie finden alle Checkpoints und können sich in der Arbeit weitere Details zu den Scores und den Auswertungskennzahlen unserer Experimente ansehen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Scores erzielen oder erzielen kann als die Baseline-Scores, und wir schlagen diese Ergebnisse als Benchmark, als Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Xi Yuan von der Fen Universität. Ich möchte Ihnen unsere Arbeit vorstellen: Distinkte Skriptkenntnisse aus großen Sprachmodellen für eingeschränkte Sprachplanung. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweisen Anweisungen in Form garantierter Skripte folgen. Vorherige Arbeiten haben die Nutzung von Sprachmodellen zur Planung für abstrakte Ziele stereotypischer Aktivitäten, wie z.B. das Zubereiten eines Kuchens, untersucht und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Vorherige Arbeiten konzentrierten sich jedoch hauptsächlich auf die Planung für die abstrakten Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Zielen und spezifischen Einschränkungen, wie z.B. das Zubereiten eines Schokoladenkuchens, ist weiterhin unterforscht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das unterschiedliche Einschränkungen für die Planungsziele auferlegt. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit schwierigeren und facettenreichen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte verfassen, die vernünftig und den Einschränkungen treu sind. In dieser Arbeit evaluieren und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da kein Datensatz spezifischer Ziele zur Unterstützung unserer Studie existiert, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele um facettenreiche Einschränkungen für die Datenerfassung mit menschlicher Interaktion unter Verwendung von Instruct TPT. Wir nehmen hundert spezifische Ziele und evaluieren die aus Light Logic Modellen generierten Skripte. Diese Tabelle gibt die Gesamtgenauigkeit der Ergebnisse an. Wir stellen fest, dass alle Light Logic Modelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, worauf Light Logic Modelle abzielen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist. Die Einhaltung der Einschränkungen kann jedoch nicht garantiert werden. Wir untersuchen detaillierter die granulareren Themenkategorien von Einschränkungen, abhängig vom jeweiligen Kontext. Die Kopfkarte in der Abbildung zeigt, dass die Planungsleistung von Instruct GPTs je nach Kategorie der Ziele erheblich variiert. Frühere Studien haben gezeigt, dass die Qualität der Ausgaben von Sprachmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des übergenerierten Zen-Filters zur Verbesserung der Generierungsqualität. Wir zeigen zunächst Einschränkungstypen mit Beispielen für Instruct GPT und erhalten spezifische Ziele basierend auf den vorgegebenen abstrakten Zielen. Anschließend generiert Instruct GPT Schlüssel-Skripte für spezifische Ziele. Anschließend wird ein Filtermodell entwickelt, um die geeigneten Skripte auszuwählen. Wir konvertieren Skripte und Ziele in Instruct GPT in Biting und berechnen den Kosinus-Ähnlichkeit sowie Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus werden wir das Skript schreiben, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel die höchste Punktzahl in der Zielstelle erhält. Mit unserer Methode kann Inslacity Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planbarkeit sowohl in Bezug auf die semantische Vollständigkeit als auch die Einhaltung der Einschränkungen erheblich. Da große Sprachmodelle teuer in der Bereitstellung sind, ist es wichtig, die Sprachplanungsfähigkeiten kleinerer und spezialisierter Modelle zu aktivieren. Die Erstellung von Datensätzen ist ein wesentlicher Schritt in diese Richtung. Frühere Studien haben jedoch keine Planung für spezifische Ziele ermöglicht, und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsmodelle zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für eingeschränkte Sprachplanung zu erstellen, der als CodeScript bezeichnet wird. Insgesamt generieren wir fünfundfünfzigtausend spezifische Ziele mit Skripten, um die Qualität der Validierungs- und Teststellen sicherzustellen. Wir bitten Cloud-Source-Mitarbeiter, die falschen Beispiele zu finden und zu korrigieren. Diese Abbildung zeigt die eingeschränkte Verteilung von CodeScript. Wir stellen fest, dass CodeScript die Hypothese in den generierten spezifischen Zielen bestätigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für eingeschränkte Sprachplanung trainieren. Wir stellen fest, dass TFIL-Funktionen auf Code-Rate Skripte von höherer Qualität generieren als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie ordnungsgemäß auf geeigneten Datensätzen trainiert werden. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir haben die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung evaluiert und eine übergenerierte Filtermethode für große Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CodeScript, für konstruktive Sprachplanung zu generieren. Wir hoffen, dass der CodeScript-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung voranzutreiben. Vielen Dank für Ihre Zeit. Weitere Details zu CodeScript finden Sie in unserem Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrack und ich werde Ihnen unsere Arbeiten zu Dr. Berth vorstellen, einem robusten Vortrainingsmodell in französischer Sprache für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend präsentieren wir den Hauptbeitrag unseres Artikels. Wir stellen das erste biomedizinische Modell in französischer Sprache namens Dr. Berth vor, das auf Roberta basiert und auf Natchios trainiert wurde, einem Datensatz medizinischer Web-Crawled-Daten. Wir stellen außerdem einen Vergleich von Modellen mit verschiedenen Vortrainings-Einstellungen und Datenquellen vor. Danach präsentieren wir unsere Ergebnisse auf elf biomedizinischen und klinischen Downstream-Aufgaben in französischer Sprache. Und schließlich schließen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 hat sich BERT zu einem der effektivsten Ansätze entwickelt, um Natural Language Processing-Aufgaben zu lösen und einen enormen Leistungszuwachs im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec, Fastex oder NWO zu erzielen. Seitdem wurde dieses Modell in viele andere Sprachen adaptiert, beispielsweise in Französisch mit Camembert, und in andere Bereiche wie Biomedizin mit PermetteBERT und BioBERT und im klinischen Bereich mit Clinical BERT, meistens jedoch in Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten. Französisch hatte jedoch bis jetzt kein Open-Source-Modell für den biomedizinischen Bereich. Daher stellen wir uns die Frage, welche die am besten geeigneten Datenquellen für eine breite Palette von Anwendungen sind. Sind diese aktuellen Daten ein gutes Substitut für klinische Daten? Um diese Frage zu beantworten, vergleichen wir Dr. Berth mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die wir von einem nicht-universitären Krankenhaus erhalten haben. Anschließend fragen wir uns, wie viele Daten wir benötigen, um ein spezialisiertes Modell auf französischsprachigen Daten zu trainieren? Sind es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir zunächst vier Modelle von Grund auf neu. Eine erste Version von Dr. Berth mit 7 GB von Natchez, eine zweite Version mit 4 GB von Natchez, eine erste Version von Schubert, das ein klinisches Modell ist, mit 4 GB von Sätzen aus klinischen Notizen und eine letzte Version von Schubert mit einer Mischung aus 4 GB von Natchez und 4 GB von klinischen Notizen. Ergänzend zu diesem Vergleich haben wir drei Modelle eingeführt, die mit kontinuierlichem Vortraining trainiert wurden, um die Auswirkungen der Vortrainingsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von Camembert und wurde mit 4 GB von Natchez trainiert. Ein weiteres Modell basiert ebenfalls auf Camembert, wurde aber diesmal mit 4 GB von klinischen Notizen trainiert. Und schließlich ein Modell, das auf dem englischen biomedizinischen Modell Bermud Bert basiert und mit 4 GB von Natchez trainiert wurde. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu evaluieren, haben wir Match-up öffentliche und private Don't-Seam-Aufgaben wie Named Entity Recognition, Klassifikation, Pattern Switch Tagging und Question Answering zusammengetragen. Diese Modelle werden mit sechs Baseline-Modellen verglichen, nämlich Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBERT, BioBERT und ClinicalBERT. Die Evaluation zeigt, dass die Modelle am besten auf Aufgaben performen, bei denen die Daten der gleichen Art sind wie die, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielfältiger erscheinen. Wir beobachten auch, dass mehr Daten zu einer besseren Leistung führen. Insgesamt scheint das Fine-Tuning von Grund auf höhere Leistungen bei den meisten Aufgaben zu erzielen. Unsere Experimente mit kontinuierlichem Fine-Tuning unter Verwendung der Gewichte und des Tokenizers von PumedBeard, trainiert auf dem 4-GB-Subset von Natchez, zeigten jedoch vergleichbare Ergebnisse wie die mit Dr. Berth 4 GB von Grund auf neu, was nicht für das Modell gilt, das auf Camembert-Gewichten und -Tokenizer basiert, das von Stabilitätsproblemen betroffen ist. Abschließend bietet unser vorgeschlagenes System eine bessere Leistung bei neun von elf DOTSTRIMS-Aufgaben und übertrifft global das Ergebnis des generischen Modells hier, Camembert. Wir beobachten auch, dass spezialisierte Daten besser sind, spezialisiertere Daten sind besser, aber sie lassen sich nicht gut skalieren. Alle vortrainierten Modelle, die von Natchios stammen, sind auf Hugging Face frei verfügbar und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank... für diese Präsentation und wir freuen uns auf den Austausch in der Post-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiang Bin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit, die die Entwicklung von Vortrainingsdaten zu Sprachmodellen und nachgelagerten Aufgaben verfolgt und die Ausbreitung politischer Voreingenommenheiten aufzeigt, die zu unfairen NLP-Modellen führen. Sprachmodelle werden auf großen, webbasierten Datensätzen vortrainiert. Politische Nachrichtenmedien sind in ihren Vortrainingsdaten gut repräsentiert. Laut einer Untersuchung des C4-Korpus können wir feststellen, dass die New York Times, die Los Angeles Times, The Guardian, der Huffington Post usw. gut in den Trainingsdaten für Sprachmodelle abgedeckt sind. Dies stellt einen zwiespältigen Vorteil für Sprachmodellanwendungen dar. Einerseits konnten sie aus unterschiedlichen Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können in nachgelagerten Aufgaben zu Fairnessproblemen führen. Zu diesem Zweck schlagen wir vor, die Ausbreitung politischer Voreingenommenheit von Vortrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben zu untersuchen, insbesondere durch die Beantwortung der folgenden Fragen. Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen die Vortrainingsdaten bei solchen Voreingenommenheiten? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen tatsächlich bei nachgelagerten Aufgaben und kann dies zu Fairnessproblemen in NLP-Anwendungen führen? Konkret schlagen wir vor, Sprachmodelle mit unterschiedlichen Prompt-Formaten mithilfe politischer Fragebögen, wie z. B. dem Political Compass Test, zu befragen. Dies ermöglicht uns eine automatische Bewertung, die gut in der politischen Wissenschaft verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Bedeutungen haben. Sie besetzen alle vier Quadranten des Political Compass. Wir können auch feststellen, dass GPT-4 das liberalste Sprachmodell unter allen ist und GPT-3 im Allgemeinen sozial liberaler ist als BERT und seine Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints zusätzlich auf sechs verschiedene parteiische Korpora vortrainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter in ihre politische Ausrichtung unterteilt sind. Durch das zusätzliche Vortrainieren von Sprachmodellen auf solchen parteiischen Korpora können wir feststellen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel können wir bei Roberta, das zusätzlich feinabgestimmt und auf dem linken Reddit-Korpus trainiert wurde, eine deutliche Verschiebung nach links in Bezug auf seine politischen Voreingenommenheiten feststellen. Um zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht, teilen wir die Vortrainingskorpora in die Zeit vor und nach dem 45. Präsidenten der Vereinigten Staaten ein. Wir trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können feststellen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in der Hassgruppenverfolgung und der Erkennung von Falschmeldungen, zwei NLP-Anwendungen, die häufig Sprachmodelle verwenden und sehr erhebliche Auswirkungen haben könnten. Wir sehen, dass wir bei der Untersuchung der kategoriebezogenen Leistung, d. h. wenn wir die Leistung in verschiedene demografische Gruppen oder die politische Ausrichtung von Nachrichtenmedien unterteilen, ein Muster feststellen, dass beispielsweise Sprachmodelle mit linker Ausrichtung besser darin sind, Hassreden zu erkennen, die sich gegen sozial marginalisierte Gruppen richten, aber schlechter darin sind, Hassreden zu erkennen, die sich gegen mächtigere Gruppen in unserer Gesellschaft richten. Umgekehrt sind Sprachmodelle mit rechter Ausrichtung besser darin, Hassreden zu erkennen, die sich gegen Weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richten. Ähnliche Trends treten auch bei der Erkennung von Falschmeldungen auf, wo wir feststellen, dass Sprachmodelle mit linker Ausrichtung besser darin sind, Fehlinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen, und umgekehrt. Dies wird durch zahlreiche qualitative Beispiele weiter verdeutlicht, um zu zeigen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen bei Hassreden und Falschmeldungen treffen, basierend auf ihren sozialen Kategorien. Es gibt im Anhang eine Reihe weiterer Beispiele, um dies weiter hervorzuheben. Dies deutet auf ein drängendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen hin. Wenn beispielsweise ein Sprachmodell mit rechter Ausrichtung für Hassreden oder Falschmeldungen usw. feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würde, würde dies bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert und Hassreden gegen Minderheitengruppen ungebremst verbreitet werden könnten. Dies ist ein Weckruf, die Fairnessprobleme anzuerkennen und anzugehen, die sich aus den politischen Voreingenommenheiten von Sprachmodellen ergeben. Etwas Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Schylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten für Sprachmodelle nicht bereinigen, wird die Voreingenommenheit von den Vortrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben ausbreiten und letztendlich Fairnessprobleme verursachen. Wenn wir versuchen, auf die eine oder andere Weise zu bereinigen, riskieren wir auch Zensur oder Ausgrenzung, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten für Sprachmodelle beibehalten werden sollte. Es ist also wie das elektrische Charlie-Problem. Okay, gut. Ich denke, das ist im Wesentlichen alles, was ich heute habe. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Coast of Sina und freue mich, Sie zu unserem Vortrag über unser ACL-Papier 2023 begrüßen zu dürfen: \"Language Model Acceptability Judgments are not always robust to context.\" Dies ist eine Gemeinschaftsarbeit mit John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy und Adina Williams. In dieser Arbeit gehen wir auf das minimale Paarmuster zurück. Das minimale Paarmuster bewertet im Wesentlichen Sprachmodelle anhand von Akzeptanzurteilen, die auch grammatikalische Bewertungen wie die von Blimp Syntax Gem oder Akzeptanz in Bezug auf Stereotypen wie Crowd Spares umfassen können. In diesem minimalen Paarmuster ist die typische Vorgehensweise zur Bewertung von Sprachmodellen, dass man ein akzeptables oder grammatikalisches Satzgefüge präsentiert und dann ein inakzeptables oder ungrammatisches Satzgefüge zeigt. Die Hoffnung ist, dass das Modell dem akzeptablen Satzgefüge eine höhere Wahrscheinlichkeit zuweist. Die aktuelle MPP-Pipeline erlaubt es uns jedoch nicht, die Akzeptanz des Modells gegenüber längeren Sätzen zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern. Es ist daher entscheidend, dass wir die Akzeptanz des Modells im gesamten Kontextfenster bewerten. Und genau das versuchen wir hier zu tun. Wir versuchen, die NPV-Pipeline zu überarbeiten, indem wir das Modell bitten, die Akzeptanz auf immer längeren Sequenzen zu bewerten. Das ist unser Ansatz. Was wir tun, ist, diese längeren Sequenzen zu simulieren, indem wir selbst die Datensätze überarbeiten und Sätze neu erstellen, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. So wählen wir beispielsweise eine typische grammatikalische Paarung aus dem Blimp-Datensatz aus dem Adjunct Island-Fall aus. Was wir tun, ist, um längere Sequenzen neu zu erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur aufweisen, extrahieren wir grammatikalische Satzgefüge aus dem Adjunct Island und fügen sie sowohl dem akzeptablen als auch dem inakzeptablen Anfragegefüge als Präfix hinzu. Wir können dasselbe tun, indem wir inakzeptable Sätze aus derselben Paarung auswählen, was ebenfalls zur Prüfung der Akzeptanz des Modells verwendet werden könnte. Wir können auch dasselbe tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Das bezeichnen wir als das Mismatch-Szenario. Hier stammen die Sätze zwar noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir das Modell bewerten. Wir können das Gleiche auch für den Fall der Inakzeptanz tun. Schließlich können wir Sätze aus einer völlig anderen Domäne auswählen, z. B. Wikipedia. Dies wird uns zeigen, ob die Akzeptanzurteile des Modells tatsächlich von irgendeinem Kontext beeinflusst werden, ob der Kontext aus einem anderen Subset des Datensatzes stammt oder ob er völlig irrelevant für den Satz ist, den wir betrachten. Wie schneidet das Modell also ab? Zunächst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Anfragepaar sind. Dort stellen wir fest, dass die MPP-Urteile größtenteils robust gegenüber beliebigen Kontextlängen sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um die OPT- und GPT2-Modelle zu maximieren, und wir sehen hier in der orangefarbenen gepunkteten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir oder erstellen Sätze aus akzeptablen und inakzeptablen Bereichen aus demselben Blimp- oder Syntax Gem-Datensatz aus. Dort stellen wir fest, dass die MPP-Urteile entweder deutlich ansteigen oder abnehmen, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, d. h. wir wählen Sätze aus demselben Phänomen in Blimp-Syntax Gem aus, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, je nachdem, ob das ausgewählte Präfix akzeptabel oder inakzeptabel ist. Dieses und dieses ist sehr groß, dieser Effekt verstärkt sich also im Laufe der Kontextlänge, und er könnte wahrscheinlich neuere Sprachmodelle beeinflussen, die große Kontextfenster haben. Warum beeinflusst das Übereinstimmungspräfix die Sprachmodellurteile so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versuchten, den... den Eingangsatz zu verändern, indem wir versuchten, die relevante Struktur beizubehalten, aber dem Eingangsstoff wie Rauschen hinzufügten. Nachdem wir mehrere dieser Veränderungen durchgeführt haben, stellen wir fest, dass keines dieser Rauschen tatsächlich dazu führt, dass sich das Modell in Bezug darauf ändert, wie es uns den MPP-Urteilstrend zeigt. Wir stellen fest, dass die Modelle in ähnlicher Weise auf die veränderten Sätze reagieren, d. h. wenn wir die Sätze im akzeptablen Bereich verändern, sehen wir einen ähnlichen Anstieg bei allen Veränderungen, und wenn wir die Sätze im akzeptablen Bereich verändern, sehen wir eine ähnliche Abnahme der MPP-Urteile. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle gegenüber latenten syntaktischen und semantischen Merkmalen empfindlich sind, die über die Sätze hinweg gemeinsam genutzt werden. Die MPP-Bewertung, so wie wir sie derzeit mit kurzen, einzelnen Satzgefügen durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawe, ein Doktorand an der Stalant Universität in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit vorstellen, „Weaker Than You Think“, einen kritischen Blick auf wöchentliches überwachtes Lernen. Dies ist eine Gemeinschaftsarbeit mit Xiao Yushchen, Maios Musbach, Giaz Steffen und Dietrich Clarkov. Ich möchte mit einer kurzen Einführung in die wöchentliche Überwachung und das wöchentlich überwachte Lernen beginnen. Bei der wöchentlichen Überwachung kennzeichnen wir die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mithilfe von wöchentlichen Kennzeichnungsquellen, wie z. B. einfachen heuristischen Regeln, Wissensdatenbanken oder locality crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen deutlich günstiger, aber auch verrauscht, was bedeutet, dass ein gewisser Anteil der Annotationen fehlerhaft ist. Wenn wir neuronale Netze direkt mit wöchentlichen Kennzeichnungsdaten trainieren, neigen die neuronalen Netze dazu, das Kennzeichnungsrauschen zu memorieren und verallgemeinern nicht. Beim wöchentlich überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust gegen dieses Kennzeichnungsrauschen zu trainieren, sodass die trainierten Modelle dennoch gut verallgemeinern. In aktuellen Arbeiten im WSL, wobei WSL für wöchentlich überwachtes Lernen steht, ist eine gängige Behauptung, dass Menschen sagen, dass sie Modelle nur mit wöchentlichen Kennzeichnungsdaten trainieren und auf sauberen Testdatensätzen eine hohe Leistung erzielen. Diese Behauptung ist technisch nicht falsch, aber es gibt einen Haken: Man geht davon aus, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellauswahl zur Verfügung steht. Wir stellen diese Problemstellung in Frage, da dies impliziert, dass im wöchentlich überwachten Lernen zusätzliche manuelle Annotationen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Skepsis veranlasst uns, drei Forschungsfragen zu stellen. Erstens, ist ein sauberer Validierungsdatensatz für WSL erforderlich? Oder können wir stattdessen einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn saubere Daten erforderlich sind oder wenn saubere Daten für die Funktion von WSL obligatorisch sind, wie viele saubere Stichproben benötigen wir dann? Und schließlich, sollten wir die sauberen Stichproben nur für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsstichproben benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle ohne saubere Validierungsstichproben nicht über die ursprünglichen schwachen Kennzeichnungen hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gekennzeichnete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Erlangung sauberer Validierungsstichproben sollten nicht übersehen werden. Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl sauberer Validierungsstichproben dazu beiträgt, dass WSL-Ansätze eine bessere Leistung erzielen, wie in der Abbildung links dargestellt. Typischerweise benötigen wir nur zwanzig Stichproben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist noch nicht das Ende der Geschichte, denn wenn wir uns auf jede Weise entscheiden, saubere Stichproben zu beschaffen, führt das Training mit diesen sogar zu einer besseren Leistung. Die rote Abbildung zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf die sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie man sieht, beginnt das Direct Fine-Tuning, wenn man 10 Stichproben pro Klasse hat, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen beanspruchte Leistungsverbesserung leicht erreicht werden, indem man das Fine-Tuning auf den sauberen Validierungsstichproben fortsetzt. Wie aus den Abbildungen ersichtlich, unterliegt das Berliner Modell, das anfänglich als FTW bezeichnet wurde, komplexeren WSL-Methoden wie Cosine. Wenn wir jedoch das Fine-Tuning auf den sauberen Stichproben fortsetzen dürfen, schneidet FTW genauso gut ab wie andere Methoden. Daher gibt es in der Praxis keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Stichproben benötigen, damit sie ordnungsgemäß funktionieren. Ihr Leistungssteigerungspotenzial und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Berichten Sie zunächst, dass die Modellauswahl mit sauberen Validierungsstichproben erfolgt. Zweitens sollten WSL-Ansätze mit Future Learning-Baselines verglichen werden, da beide mit sauberen Stichproben arbeiten. Drittens sollte Continuous Fine-Tuning als einfacher, aber starker Baseline in zukünftiger Arbeit im WSL betrachtet werden. Schließlich haben wir unseren Code Open Source gestellt. Sie finden ihn über den QR-Code auf dieser Folie. Zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Ayud Villar und ich werde Ihnen eine kurze Übersicht über die Arbeit „Prompting Palm from Translation Assessing Strategies and Performance“ geben. Dies ist eine Gemeinschaftsarbeit mit meinen Kollegen von Google Translate. Palm ist ein großes Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Textsammlung trainiert, die 780 Milliarden Token umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es modernste Ergebnisse in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zum Prompting großer Sprachmodelle für die maschinelle Übersetzung. Wir evaluieren die Übersetzungsfähigkeit solcher Modelle unter Verwendung bewährter Verfahren der AMT-Community. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden, und wir vergleichen zwei modernste Systeme, die leistungsstärksten Systeme der WMT-Evaluation. Wir verwenden modernste neuronale MT-Metriken und zeigen zusätzlich auch Ergebnisse der von Experten durchgeführten menschlichen Evaluation. Abschließend geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir ein kurzes Prompt verwenden und für nur einen Satz zwei verschiedene Prompts bereitstellen. Bei der Mehrheit der Sätze (516 von 1000) beträgt die beobachtete Differenz mehr als einen BLEU-Punkt. Und in extremen Fällen kann dieser Wert bis auf 40 BLEU-Punkte steigen. Daher ist es wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine Five-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System bereitstellen, einfach mit der Sprache markieren, in der er verfasst ist. In diesem Beispiel hier, bei dem wir eine Übersetzung von Deutsch ins Englische durchgeführt haben, sind die deutschen, also die Ausgangssätze mit „Deutsch:“ markiert und die englischen Übersetzungen mit „Englisch:“ markiert. Wir haben festgestellt, dass die tatsächliche Form des Promptings keinen großen Einfluss hat, wenn es um mehrere kurze Prompts geht. Sie ist entscheidend für Zero-Shot- und One-Shot-Prompting, aber wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Promptings. Es sind die Beispiele, die das meiste Gewicht tragen. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Ausgangssatz. Daher ist es wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir das Auswählen von Prompts aus den Trainingsdaten der WMT-Evaluierungen oder den Dev-Daten. Die Dev-Daten sind viel stärker kuratiert und von höherer Qualität als die Trainingsdaten, so dass sie, wie ich sage, zu einer besseren Leistung führen, wenn die Dev-Daten verwendet werden. Dennoch haben spezialisierte State-of-the-Art-ODR-Systeme einen erheblichen Vorteil gegenüber den PALM-Übersetzungen, aber PALM kommt einem kommerziellen System recht nahe. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren. Die Erkenntnisse, die wir aus der menschlichen Evaluation gewonnen haben, die wir unter Verwendung des MQM-Frameworks durchgeführt haben, sind, dass die Flüssigkeit von Palm mit der von modernsten Systemen vergleichbar ist, der Hauptunterschied ergibt sich jedoch aus der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass Palm manchmal eine bessere Satzübersetzung produziert, indem er Teile des Satzes auslässt, die in der Übersetzung fehlen. Der Stil-Score für PAM ist jedoch niedriger als für die modernsten Systeme, was ein zusätzliches Signal dafür ist, dass PAM wirklich fließenden Output liefert, aber dennoch mit Genauigkeitsproblemen zu kämpfen hat. Und das war's für diese wirklich kurze Übersicht. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Papers. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Jing Wei von der Universität für Wissenschaft und Technologie Chinas. Es ist mir eine Freude, ein kurzes Werbevideo für unser Papier \"Are you copying my model – Protecting the copyright of large language models for embedding and services – View Backdoor Watermark\" zu präsentieren. Lassen Sie uns zunächst den Hintergrund zu Embedding und Services vorstellen. Derzeit sind große Sprachmodelle wie GPT, Lama, PELM außergewöhnlich im Bereich des Natural Language Understanding und der Generierung. Embedding und Services sind einer der Dienste, die auf großen Sprachmodellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. So bietet OpenAI beispielsweise eine auf GPT basierende Embedding-API. Jüngste Arbeiten haben jedoch gezeigt, dass ein Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding als Services zu schützen. Um das Urheberrecht von Embedding als Services zu schützen, ist eine Lösung, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Methode muss folgende Eigenschaften erfüllen. Erstens sollte die Methode für Embedding als Services anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht für Embedding als Services anwendbar oder es fehlt ihnen an Übertragbarkeit. Daher schlagen wir in diesem Papier Embedding Marker vor, eine Backdoor-basierte Wasserzeichenmethode, die für Embedding als Services anwendbar ist. Lassen Sie mich nun die Details unseres Embedding Markers vorstellen. Embedding Marker umfasst zwei Hauptschritte: Wasserzeicheneinfügung und Urheberrechtsverifikation. Vor diesen Hauptschritten wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Frequenzbereich. Nehmen wir an, der Anbieter kann ein allgemeines Textkorpus sammeln und die Wortfrequenz damit zählen. Bei der Wasserzeicheneinfügung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als M ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifikation besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zunächst einen Backdoor-Datensatz und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Anschließend fordert der Anbieter Embeddings vom Stealer-Dienst mit dem Datensatz an. Die Konsin und L zwei Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem neun und dem Backdoor-Datensatz, die als Delta Cosine und Delta L zwei definiert ist. Gleichzeitig wenden wir auch einen KS-Test an und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, Mind, SSD two und Erospam. Wir nehmen an, dass der Anbieter den Wiki-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielen kann, während die Nützlichkeit für nachgelagerte Aufgaben erhalten bleibt. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen visualisieren, die als BOPCA entfaltet wurden. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie aus den Abbildungen ersichtlich, ist es schwierig, zwischen den Backdoor-Embeddings und normalen Embeddings zu unterscheiden. Das war's, vielen Dank. Wir freuen uns auf eine Diskussion mit Ihnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Ying und mein Kollege Jian und ich werden Ihnen unsere Forschung über Multi Instruct vorstellen, welches die Verbesserung des multimodalen Zero-Shot-Lernens durch Instruction Tuning untersucht. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erkunden, um vortrainierte Sprachmodelle auf effiziente Weise hinsichtlich Parameter und Daten für verschiedene nachgelagerte Aufgaben wiederzuverwenden. In den letzten Jahren haben zahlreiche Studien gezeigt, dass Instruction Tuning großen Sprachmodellen ermöglicht, ungesehene Aufgaben in einem Zero-Shot-Verfahren zu bewältigen, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Instruction Tuning konzentrierten sich jedoch auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während Computer Vision und multimodale Aufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Instruction Tuning auf multimodalen vortrainierten Modellen tatsächlich die Verallgemeinerung auf ungesehene multimodale Aufgaben verbessern kann. Zusätzlich stellten wir bei unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Instruction-Datensätzen zwischen LP und multimodalen Daten fest. Es gibt mehr als eintausendsechshundert sprachbasierte Instruction-Aufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren multimodalen Instruction-Aufgabendatensatz. Dies motivierte uns zum Aufbau eines multimodalen Instruction-Tuning-Datensatzes. Wir präsentieren hier Multi Instruct, den ersten multimodalen Instruction-Tuning-Benchmark-Datensatz, der aus sechzehn verschiedenen multimodalen Aufgaben besteht, die zehn breite Kategorien abdecken. Diese Aufgaben stammen aus einundzwanzig bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit fünf von Experten erstellten Anweisungen versehen. Zur Untersuchung des multimodalen Instruction Tuning auf unserem vorgeschlagenen Datensatz verwenden wir OFA, ein Modell, das das multimodale Muster vereinheitlicht, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem Multi Instruct-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequence-to-Sequence-Format, wobei der Text, die Bilder, die Anweisung und die Begrenzungsrahmen im selben Token-Raum dargestellt werden. Nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus der NIG-Gruppe zum Trainieren und wählen 10.000 Instanzen pro Aufgabe aus. Für das Testen reservieren wir die gesamte Common Sense Reasoning-Gruppe für das Testen und wählen zusätzlich fünf Aufgaben aus der WQA- und der Miscellaneous-Gruppe aus. Wir verwenden alle Instanzen in der Testaufteilung für jede Aufgabe. Darüber hinaus wählen wir zufällig 20 Aufgaben aus der Testaufteilung von NIG Instruction als SIN-Aufgabe für NLP aus. Wir verwenden ein vortrainiertes OFA-Large-Modell als unser Basismodell. Während des Trainings werden alle Instanzen für alle Aufgaben gemischt. Jede Instanz wird zufällig mit einer von fünf Anweisungsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell anhand einer von fünf Anweisungen in jedem Experiment bewerten. Wir berichten über den Mittelwert und das Maximum der Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine Multimodale Klassifikationsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine Multimodale Generierungsaufgabe handelt, berichten wir über ROUGE. Für eine RP-Aufgabe berichten wir ebenfalls über ROUGE. Wir haben außerdem eine zusätzliche Evaluationsmetrik eingeführt, die Sensitivität genannt wird. Diese misst die Fähigkeit des Modells, für dieselbe Aufgabe konsistent die gleichen Ausgaben zu erzeugen, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann Instruction Tuning die Leistung von OFA bei ungesehenen multimodalen Aufgaben deutlich verbessern. Auch das Transferlernen aus einem natürlichen Instruction-Datensatz kann dem Instruction Tuning zugute kommen. Hier sehen wir, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig die Sensitivität verringert. Wir haben außerdem ein Experiment durchgeführt: Wir verwenden eine Anweisung versus fünf Anweisungen. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität deutlich reduzieren. Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transferlernen aus einem natürlichen Instruction-Datensatz eine viel bessere Sensitivität erzielen als das ursprüngliche OFA-Modell. Wir können auch sehen, dass das Transferlernen aus einem natürlichen Instruction-Datensatz OFA helfen kann, auf dem natürlichen Instruction-Datensatz eine viel bessere Leistung zu erzielen. Insgesamt haben wir den ersten großen multimodalen Instruction-Tuning-Datensatz vorgeschlagen. Wir haben die DAROCHOT-Fähigkeit von OFA deutlich verbessert und verschiedene Transferlerntechniken erforscht und ihre Vorteile aufgezeigt. Wir haben eine neue Metrik namens Sensitivität entwickelt. Ein weiteres Detail: Wir sammeln einen noch größeren multimodalen Instruction-Tuning-Datensatz mit etwa 150 zusätzlichen varianten Sprachaufgaben und werden ihn veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Yusuf Zhang von der Pennsylvania State University. Heute werde ich unsere Arbeit vorstellen, Exampler, Crosslinguale Semantische Analyse in mehreren natürlichen Sprachen und Bedeutungrepräsentationen. Die semantische Analyse ist also eine Aufgabe, um semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda-Kalkül zu erstellen. Und crosslinguale semantische Analyse ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungrepräsentationen zu übersetzen. Wie in dieser Abbildung dargestellt, müssen wir die Abfrage in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bisherige crosslinguale semantische Analysemodelle wurden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen evaluiert. So gibt es beispielsweise Lücken in der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt, und es gibt Lücken in der Abdeckung bestimmter Repräsentationen. Der Lambda-Kalkül fehlt. Oder sie werden nur auf bestimmten neueren Modellen evaluiert. Zum Beispiel gibt es nur ein einziges Modell, um sie zu evaluieren. Um dies zu erreichen, schlagen wir Exampler vor. Wir stellen einen einheitlichen Datensatz, Exampler, für die crosslinguale semantische Analyse in mehreren natürlichen Sprachen und Bedeutungrepräsentationen bereit. Er enthält neun Datensätze in verschiedenen Bereichen, fünf semantische Analyseaufgaben, acht Bedeutungrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unseren Benchmark besser zu evaluieren, betrachten wir sechs Einstellungen für das Training und die Evaluation. Die erste ist TranslateTest. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell, um zu trainieren und zu evaluieren. Zum Beispiel trainieren wir das englische Modell auf… auf englischen Abfragen und übersetzen während der Inferenz die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um die SQL-Ausgabe vorherzusagen. Und wir testen auch ein monolinguales Modell. In dieser Einstellung ist die Quellsprache die gleiche wie die Zielsprache. Zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch eine monolinguale Fusions-Einstellung, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen ein monolinguales multilinguales Modell, das wir ein multilinguales Modell für alle Sprachen trainieren. Zum Beispiel kombinieren wir deutsche, englische und chinesische Abfragen, um ein multilinguales Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen oder Ähnliches zu übersetzen. Und wir betrachten auch crosslinguale Zero-Shot- und Few-Shot-Übertragungen. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir es also auf englischen Abfragen oder einer Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein multilinguales Modell zu trainieren und die SQL-Ausgabe vorherzusagen. Und wir haben auch viele interessante Ergebnisse gefunden. Hinsichtlich der Analyse monolingualer Modelle evaluieren wir auf zwei Gruppen von Modellen, einschließlich Encoder PDR, was für Multilingual Pre-trained Encoder mit Pointer-basierten Decodern steht, wie z. B. XLMR plus PDR und BERT plus PDR. Und wir evaluieren auch Encoder-Decoder-Modelle, das sind Multilingual Pre-trained Encoder-Decoder-Modelle, wie z. B. MBART und MT5. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung in allen neun Datensätzen erzielen. Und wir evaluieren MT5 und XLMR plus PDR in der multilingualen Einstellung. Wir haben festgestellt, dass Encoder-Decoder oder Encoder PDR verbessert werden können, indem sie in einer Mischung aus verschiedenen Sprachen trainiert werden. Und das liegt daran, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, außer dass die englische Leistung in sieben Datensätzen abnimmt und nur in drei Datensätzen zunimmt. Ich denke, dies wird als Kurven der Multilingualität bezeichnet. Wir haben auch die crosslinguale Leistungsdifferenz verglichen. In dieser Abbildung ist die blaue Linie die crosslinguale Fuchsia-Übertragung. Die orangefarbene Linie ist die crosslinguale Zero-Shot-Übertragung, während die grüne Linie die monolinguale Einstellung ist. Wir haben festgestellt, dass durch den Vergleich der grünen und orangefarbenen Linie für die Zero-Shot-Einstellung die crosslinguale Übertragungsleistungsdifferenz signifikant ist. Und durch den Vergleich der blauen und orangefarbenen Linie haben wir festgestellt, dass für die Fuchsia-Einstellung die Übertragungsdifferenz schnell abnimmt. Wir haben auch einige andere interessante Erkenntnisse gefunden. Zum Beispiel übertrifft der Encoder-Decoder den bisherigen Stand der Technik oder erzielt vergleichbare Ergebnisse. Das Training auf natürlicher englischer Sprache kann die Leistung von Fuchsot auf gezielten natürlichen Sprachen deutlich steigern. Und wir haben festgestellt, dass multilinguale Sprachmodelle wie Codice und Bloom für crosslinguale semantische Analyseaufgaben immer noch unzureichend sind. Zusammenfassend haben wir Exampler entwickelt, einen einheitlichen Benchmark für die crosslinguale semantische Analyse mit mehreren natürlichen Sprachen und zahlreichen Repräsentationen. Wir haben eine umfassende Benchmark-Studie zu drei repräsentativen Typen von multilingualen Sprachmodellen durchgeführt. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Besuchen Sie gerne unser Papier und unseren Code. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Spirakowski und dieser Vortrag handelt von der Dependenzstruktur der Koordination. Wie Sie vielleicht wissen, gehen verschiedene Theorien und Korpusansätze von unterschiedlichen Dependenzstrukturen aus. So zum Beispiel wird in den Universal Dependencies die Struktur der koordinativen Koordination Lisa, Bart und Maggie so realisiert, dass das erste Konjunkt das Haupt der gesamten koordinativen Struktur ist, also in diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igors Miltschuks Meaning-Text-Theorie angenommen, wo wiederum die gesamte koordinative Struktur vom ersten Konjunkt headed wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie heben eines der Konjunkte hervor. Es gibt aber auch symmetrische Ansätze für koordinative Strukturen, wie beispielsweise den pragischen Ansatz, den conjunction-headed Ansatz, der in den Prager Dependency Treebanks angenommen wird, wo koordinative Strukturen vom Konjunktion headed werden. So erhalten wir Dependenzen von und zu allen Konjunkten. Und schließlich gibt es auch einen multi-headed Ansatz, der beispielsweise in Cutson's Word Grammar verwendet wird, wo sozusagen alle Konjunkte Heads der Koordinativstruktur sind, so dass wir Dependenzen vom Regenten hier lachen zu allen Konjunkten separat erhalten, diese sind Bart und Maggie. Das Ziel dieses Papiers ist es nun, ein neuartiges Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu erbringen. Okay, das Argument basiert auf dem Prinzip der Minimierung der Dependenzlänge, das ich an der Hand dieser Beispiele erläutern werde. Im Englischen, wie Sie vielleicht wissen, bevorzugen direkte Objekte, nahe am Verb zu stehen, während Adverbialbestimmungen weiter entfernt sein können, richtig? „March read it yesterday“ ist in Ordnung, weil das direkte Objekt „it“ nahe am Verb ist, während „March read yesterday it“ viel schlechter ist, richtig? Denn hier befindet sich ein Adverbialbestimmung „yesterday“ zwischen dem Verb und dem direkten Objekt. Dieser Effekt kann jedoch abgemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, da es dann hinter der Adverbialbestimmung platziert werden kann. Dies wird hier illustriert. So sind beide Sätze in Ordnung. „March read this absolutely fascinating book about the bees yesterday“ ist in Ordnung, wobei anstelle von „it“ wir dieses lange NP haben. Es ist auch in Ordnung zu sagen: „March read yesterday this absolutely fascinating book about bees“. Die Begründung hierfür ist, dass dies möglich ist, weil obwohl dieser Satz das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, er das Prinzip der Minimierung der Dependenzlänge erfüllt, das besagt, dass kürzere Dependenzen bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Dependenzen, also derjenigen, die nicht konstant in diesen beiden Strukturen sind. Hier haben wir eine Dependenz von „red“ zu der Adverbialbestimmung der Länge 7, gemessen in Wörtern, und von „red“ zu „book“ der Länge 4, also zusammen 11. Wenn Sie, wenn Sie diese beiden Konstituenten vertauschen, wird die Summe dieser beiden Dependenzen sechs ergeben, richtig? Also statt 11 sechs, viel kürzer, deshalb klingt das ganz in Ordnung, richtig? Es verletzt ein Prinzip, aber es erfüllt ein anderes. Okay, das haben wir getan, wir haben verschiedene Statistiken über die Koordination aus der erweiterten Version der Penn Treebank extrahiert und sehen im Papier, warum wir nicht Universal Dependencies verwendet haben. Und diese Statistiken bestätigen die bereits mehrfach gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind. „Salt and pepper and not pepper and salt“, gemessen in Silben. Und auch die Beobachtung, die nur am Rande gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Wenn also die Differenz zwischen den Längen der beiden Konjunkte wächst, bevorzugt der kürzere Konjunkt, der erste zu sein, stärker, richtig? So ist der Anteil der linken, kurzen Konjunkte größer. Was in diesem Papier jedoch neu ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regent links steht oder fehlt, richtig? Der Regent steht in diesem Beispiel links: „I saw Bart and Lisa“. Er ist in dem zweiten Beispiel abwesend: „Homer came and sneezed“. Hier handelt es sich um eine Koordination von zwei Verben und es gibt keinen externen, äußeren Regenten. In solchen Fällen bevorzugt der linke Konjunkt, kürzer zu sein, je größer die Differenz zwischen den beiden Konjunkten ist. Wenn der Regent jedoch rechts steht, wie hier: „Left governs the coordination Ted and Ned“, verschwindet dieser Effekt. Wir zeigen, dass wir die Länge in Zeichen (die erste Spalte), in Silben (die mittlere Spalte) und in Wörtern (die rechte Spalte) messen. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn der Regent links steht, die Tendenz des linken Konjunkts, kürzer zu sein, stetig mit der absoluten Differenz in Wörtern wächst, und dasselbe wird beobachtet, wenn kein Regent vorhanden ist, wie bei der Koordination von Sätzen. Aber wenn der Regent rechts steht, verschwindet diese Tendenz, und wir zeigen in dem Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination wie diese beiden und für die symmetrischen Strukturen wie diese beiden liefert. Sehen Sie sich das Papier für die vollständige Übereinstimmung und Argumentation an und sprechen Sie im Post-Session darüber. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag, mein Name ist Kyo Yin und ich werde unsere Arbeit mit dem Titel \"Wann erfordert Übersetzung Kontext? Eine datengesteuerte, mehrsprachige Untersuchung\" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emily Liu, Andre FD Martins und Graham Newbig erstellt. So viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir \"mole\" in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautet: \"Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden\", dann bezieht sich \"mole\" auf einen Spion. Aber wenn der vorherige Satz lautet: \"Könnte es etwas Ernstes sein, Doktor?\", dann bezieht sich \"mole\" auf einen Leberfleck. Abhängig vom Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Es ist jedoch ziemlich schwierig, zu bewerten, wie gut Modelle Fälle wie diesen übersetzen können. Erstens, da nur ein kleiner Teil der Übersetzungen von Kontext abhängt, sind Metriken auf Korpus-Ebene wie Blue nicht in der Lage, diese Übersetzungen zu erfassen. Und einige haben gezielte Bewertungen für kontextabhängige Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie in der Regel auf Domänenwissen und menschliche Kuratierung angewiesen sind. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut gehen Modelle mit diesen Fällen um? Um die erste Frage zu beantworten, begannen wir damit, zu messen, wie stark ein Wort während der Übersetzung vom Kontext abhängt. In unserer bisherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht durch die Messung, wie viele Informationen der Kontext C über das Ziel Y angesichts der Quelle X liefert. Man kann CXMI als die Information betrachten, die man erhält, wenn man dem Modell Kontext liefert. In dieser Arbeit erweitern wir CXMI zu pointwise CXMI, das die Kontextnutzung auf Satzebene oder Wortebene messen kann. Wir können Wörter mit einem hohen PCXMI als solche betrachten, die für die Übersetzung Kontext erfordern. Nun analysieren wir Wörter mit einem hohen PCXMI, um nach Mustern zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse an Transkripten von TED-Talks durch, die von Englisch in vierzehn verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Erstens betrachten wir Wortarten-Tags mit hohen mittleren PCXMI-Werten. Dies ermöglicht uns beispielsweise, Dual-Pronomen im Arabischen mit relativ hohen p six mi zu finden. Dies lässt sich erklären, da Englisch keine Dual-Pronomen hat. Daher ist Kontext erforderlich, um zu bestimmen, ob ein Pronomen dual ist, wenn es ins Arabische übersetzt wird. Und ähnlich stellen wir fest, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die passende Verbform auswählen wollen. Als Nächstes betrachten wir Wortschatzartikel, die über alle ihre unterschiedlichen Vorkommnisse hinweg einen hohen p six mi aufweisen. Dies hilft uns, Fälle wie den hier zu identifizieren, bei dem im Chinesischen Kontext benötigt wird, um korrekt zu übersetzen. Und schließlich stellen wir fest, dass Kontext wichtig ist, um die richtige Formalität bei der Übersetzung zu berücksichtigen. Schließlich betrachten wir verschiedene einzelne Token mit einem hohen p6mi. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich vom Wort selbst erfasst werden können, sondern eher in einer Satzstruktur zum Ausdruck kommen, wie z. B. Ellipsenauflösung. Nun nutzen wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger Multilingual Discourse Aware oder MUDA Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen. Anschließend verwenden wir den MUDA Tagger, indem wir den Tagger auf dem parallelen Korpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere gewählten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MUDA Tagger identifiziert hat. Abschließend verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf Dokumentenebene in der maschinellen Übersetzung zu bewerten. Erstens, wenn wir Korpus-Ebene-Metriken verwenden, also für Blue, stellen wir fest, dass kontextunabhängige Modelle die beste Leistung erbringen, aber wenn wir Comet verwenden, erzielen kontextbewusste Modelle die besten Ergebnisse. Und wenn wir WordF verwenden, dann haben Modelle mit oder ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem auf Dokumentenebene zu bestimmen, wenn wir allein Korpus-Ebene-Metriken verwenden. Nun verwenden wir den MUDA-Benchmark, um Modelle zu bewerten, und wir stellen fest, dass Modelle auf Kontextebene für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen nicht wesentlich besser als Modelle, die keinen Kontext verwenden. Dies deutet daher darauf hin, wo wir Fortschritte bei der Übersetzung auf Dokumentenebene erzielen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DPL in der Regel genauer als Google Translate für die Übersetzung auf Dokumentenebene ist. Zusammenfassend führen wir eine datengesteuerte Analyse über vierzehn Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und dann verwenden wir unsere Erkenntnisse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu erstellen, der uns helfen kann zu identifizieren, welche diskreten Phänomene Modelle gut oder nicht handhaben können, und welche Übersetzungssysteme gut auf Dokumentenebene übersetzen. Vielen Dank für Ihre Aufmerksamkeit. Sehen Sie uns morgen."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich eure Arbeit vorstellen, „Anal Positionale, Charakterisierung von Design-Bias, A Beta Sets und Modelle“. Diese Arbeit entstand in Zusammenarbeit mit einigen Kolleginnen und Kollegen der University of Washington und des Allen Institute for AI, insbesondere Sebastian Santi, Ronin Lebras, Katarina Reinicke und Martin Sapp.\n\nLassen Sie uns zunächst annehmen, dass Sie für eine Zeitung arbeiten und die Kommentare unter Ihren Artikeln durchforsten, um toxische Inhalte zu entfernen. Möglicherweise greifen Sie auf eine beliebte API wie Perspective API zur Toxizitätserkennung zurück, und das funktioniert gut, wenn Sie Carl Jones sind, wobei Perspective APIs toxische Fälle korrekt erkennen können. Das ist jedoch nicht wirklich der Fall für Dithya Sharma, bei der Perspective APIs nicht so empfindlich auf beleidigende Begriffe reagieren, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen Bevölkerungsgruppen feststellen. Design-Biases wie der, den wir gerade gesehen haben, können durch die Positionierung der NLP-Forscher und Modellentwickler entstehen. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen. Es handelt sich um ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen, weit verbreitet ist. Und als Forscherin kann die Positionierung den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen verändern kann, die Forscher treffen.\n\nEine Frage, die sich daher aufdrängt, ist: Haben Datensätze und Modelle eine Positionierung? Wir wollen damit nicht sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können daher bestimmte Positionierungen gegenüber anderen darstellen.\n\nVorherige Arbeiten haben bereits einige anekdotische Hinweise auf Positionierung geliefert, wie z. B. kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modellpositionierung. Diese Arbeiten befassen sich jedoch nicht damit, Endnutzer mit den Datensätzen und Modellen selbst zu vergleichen. Das Studium der Modell- und Datensatzpositionierung wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozial orientierter werden. Es ist eine Herausforderung zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.\n\nUm die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen. Dies tun wir mithilfe unseres Frameworks, NLPositionality. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt ist die erneute Annotation von Datensätzen mit vielfältigen Annotatoren. Wir bevorzugen dies gegenüber der Betrachtung der Demografie der ursprünglichen Datensätze, da in der Regel nur wenige Annotatoren jede Instanz annotieren und da Demografie selten erfasst und weitergegeben wird. Daher bevorzugen wir es, Daten erneut zu annotieren, um viele Annotatoren pro Instanz zu erhalten und einen reichhaltigen Satz an demografischen Daten zu erhalten.\n\nWir nehmen dann die Annotationen zusammen mit den demografischen Daten und vergleichen sie mit den Modellen und Datensätzen mithilfe eines Parsons R-Korrelationswerts. Unser Framework unterscheidet sich somit von der Literatur über Annotatoreinigkeit, indem es Endnutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleicht, anstatt sich nur auf die Annotatoreinigkeit oder die Modellierung von Annotatorendistributionen zu konzentrieren.\n\nUnser Framework wird weitgehend durch Lab in the Wild ermöglicht, eine Online-Experimentplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie MTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Darüber hinaus ist Lab in the Wild weiterhin in der Lage, qualitativ hochwertige Daten zu erhalten. Wir hosten zwei Aufgaben auf Lab in the Wild, darunter die soziale Akzeptanz. Bei dieser Aufgabe lesen die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz und schreiben dann, wie sozial akzeptabel eine Situation ist. Anschließend können sie ihre Antworten mit einer KI und anderen vergleichen, um sich in der Studie zu engagieren. Wir vergleichen diese Annotationen dann mit Social Chemistry Delphi und GPT 4. Wir haben eine sehr ähnliche Vorgehensweise für die Aufgabe zur Erkennung von Toxizität und Hassrede wiederholt, bei der sie eine Instanz aus DynaHate lesen und angeben, ob sie der Ansicht sind, dass es sich um eine Hassrede handelt. Wir vergleichen diese Annotationen dann mit DynaHate, Perspective API, Rewire API, Hate Roberta und GPT 4.\n\nUnsere Studie hat am Ende über sechzehntausend Annotationen von über eintausend Annotatoren aus achtzigsieben Ländern gesammelt.\n\nNun sind wir besser gerüstet, um die Frage zu beantworten: Mit wem stimmen NLP-Datensätze und -Modelle am ehesten überein? Wir stellen fest, dass es eine Positionierung in NLP gibt. So stellen wir fest, dass Datensätze und Modelle am ehesten mit englischsprachigen Ländern übereinstimmen. Bei der Analyse der sozialen Akzeptanz von GPD 4 stellen wir fest, dass sie am ehesten mit konfuzianisch- und englischsprachigen Ländern übereinstimmt. Wir stellen ebenfalls fest, dass DanaHate am ehesten mit englischsprachigen Ländern übereinstimmt. Wir stellen außerdem eine zusätzliche Übereinstimmung mit Menschen mit einem Hochschulabschluss fest. Bei GPD 4 im Task zur sozialen Akzeptanz stellen wir fest, dass es am ehesten mit Menschen mit einem Hochschul- oder Hochschulabschluss übereinstimmt. Und wir stellen das Gleiche für DanaHate fest, wo es am ehesten mit Menschen mit einem Hochschulabschluss übereinstimmt.\n\nWenn Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen übereinstimmen, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Personen übereinstimmen als mit ihren männlichen und weiblichen Pendants. Wir stellen dies sowohl im GPT 4 Social Acceptability Task als auch in der Analyse des DynaHate Tasks fest.\n\nAngesichts der Tatsache, dass es eine Positionierung in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dazu. Die erste besteht darin, einen Überblick über alle relevanten Gestaltungsentscheidungen während des gesamten Forschungsprozesses zu führen. Die zweite besteht darin, NLP-Forschung aus der Perspektive des Perspektivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb bestimmter Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakane Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, alle Technologien für jeden arbeiten zu lassen.\n\nDamit schließt unsere Präsentation ab. Wenn Sie mehr erfahren möchten, können Sie gerne unseren Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit zur Lösung indirekter Bezugsausdrücke für die Entität Auswahl sprechen, in der wir den Altentities Korpus vorstellen. Mein Name ist Javot Hosseini, und dies ist eine Gemeinschaftsarbeit mit Philip Radlinsky, Silvia Pareti und Annie Luis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Betrachten Sie diese alternative Frage. Meinten Sie \"Easy on Me\" oder \"I Got a Feeling\"? Hier möchte der Benutzer zwischen einem dieser beiden Titel wählen. Das Offensichtlichste wäre, einen direkten Verweis zu verwenden, zum Beispiel, indem man den Namen des Liedes \"Easy on Me\" oder seine Position, die erste, nennt, aber manchmal ist ein indirekter Verweis angemessener, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich den Namen des Liedes nicht erinnert oder die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind, oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Bezüge. Zum Beispiel \"das neuere\" oder \"das nicht-energische Lied\". Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Bewertung des Entitätsverständnisses von LLMs. Uns ist kein öffentlicher Datensatz bekannt, ein großer öffentlicher Datensatz für diese Aufgabe, daher haben wir einen mit Hilfe von Crowd-Annotationen erstellt. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unsere Datensatz-Erfassungsmethodik betont die Informalität durch einen Cartoon-Abschlussaufbau. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: \"Erinnerst du dich an das Lied, das wir gestern gehört haben?\". Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: \"Meinten Sie 'Easy on Me' oder 'I Got a Feeling'?\", was die alternative Frage ist. Und in der dritten Sprechblase verwendet Bob einen indirekten Verweis, um eine dieser Entitäten auszuwählen, zum Beispiel \"das mit dem Klavier\". Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Vorgaben pro Bereich ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer eine einfache Vorlage: \"Meinten Sie A oder B?\", wobei A und B aus Wikipedia entnommen werden. Hier sind die verschiedenen Stichprobenmethoden, die wir verwendet haben. Wenn wir in der Liste höher gehen, werden die Entitäten ähnlicher zueinander und es ist in der Regel schwieriger, die Disambiguierung vorzunehmen. Die erste Methode ist einheitlich zufällig. Die zweite Methode ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen \"they return\". Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder denselben Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie den Namen dieser Entitäten, wissen aber nicht unbedingt etwas über die Entitäten. Was wir also tun, ist, einige Hintergrundinformationen über die beiden Entitäten anzuzeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten dann die Annotatoren, mindestens einige Teile von jedem Lied anzuhören und etwas über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied \"Easy on Me\". Für die Bereiche Rezepte und Bücher zeigen wir einigen Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Bezugsausdrücken zu beschreiben. Zum Beispiel \"das mit der Klaviermusik\", \"das ohne Wörter\", \"nicht das mit dem 12-jährigen Jungen\", \"das fiktive\" oder \"das aus Aserbaidschan\" stammt. Der Altentities Korpus hat 6.000 alternative Fragen über drei Bereiche und 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5xLarge-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf genau dasselbe Hintergrundwissen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92–95 %. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf einige teilweise überlappendes Hintergrundwissen hat, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, liegt die Genauigkeit nur bei 60 %. Es gibt also viel Verbesserungspotenzial. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
