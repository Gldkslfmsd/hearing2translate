{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫马蒂亚斯·林德曼，今天我将为大家简要介绍我们的论文，该论文探讨了在不使用树结构的情况下，如何通过多集标记和潜在置换实现组合泛化。这篇论文是我与我的导师亚历山大·科勒和伊万·蒂托夫共同完成的工作。组合泛化可以理解为学习者处理更深层递归和训练过程中仅见过单个短语的未见组合的能力。在语义解析的语境下，测试组合泛化能力可能如下所示。通常，我们有一个训练集，包含一系列短语，例如“那个女孩睡着了”和“玛丽知道那个女孩睡着了”。这些短语与表示其核心意义的逻辑形式配对。与标准的机器学习评估不同，测试集不来自相同的分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练过程中见过较浅的递归，然后在一个具有更深递归的例子上进行测试。朴素的序列到序列模型难以应对这种分布外泛化，并且常常产生与输入无关的输出。尤其重要的是，它们常常无法再现输入和输出之间的系统性对应关系，例如在示例中用颜色编码的对应关系。一种流行的解决办法是将树结构集成到模型中。这些树结构旨在捕捉与逻辑形式相关的短语的组合过程。这方法效果良好，但树结构通常是不提供的，需要以某种方式获取。这可能会很复杂，并且有时是一个计算成本高昂的过程。通常，这涉及到对逻辑形式进行大量的特定形式学的预处理，例如处理变量符号。获取树结构也可能涉及专门的语法归纳程序。在本文中，我们不使用树结构，而引入了一个神经序列到序列模型，该模型直接建模输入片段与输出片段之间的对应关系。我们首次展示了在不依赖树结构的情况下对更深层递归实现强大的泛化能力。我们的方法分两个步骤从输入预测输出。首先，我们使用一个无序多集标记对每个输入token进行标记，这些token将出现在输出中。在第一步之后，我们拥有了所有正确的token，但它们的顺序被打乱了。这就是为什么在第二步中，我们使用另一个模型来预测一个置换，将它们排列成正确的顺序。我们介绍了一种新的预测置换的方法，该方法不会对可能的置换施加任何硬性约束。这使得我们的方法非常灵活且富有表现力。从概念上讲，我们的置换模型的工作方式大致如下。我们从左向右遍历输出，确定将哪个多集token放置在每个位置。对于第一个输出位置，我们简单地选择一个，如图中红色突出显示的那样。然后，我们跳到下一个多集token，以确定输出中的第二个token。我们以类似的方式跳过另一个多集token。我们继续此过程，直到第一阶段的每个token都被访问过一次。为了让您了解实验结果的预览，我们在此将我们的方法与其他无树模型在COGS基准测试上进行比较。我们的模型在对更深层递归的泛化方面，明显优于其他模型。然而，其他一些类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的实际难题。首先，输入和输出的对齐方式在训练数据中没有给出。因此，对于给定的token，我们不知道它来自哪个多集token，这给训练带来了挑战。此外，有时存在多个与数据一致的置换，但语言学上正确的置换是潜在的。我们通过在训练过程中诱导对齐方式来解决这个问题。我们的置换方法非常灵活，但它带来了寻找得分最高的置换是NP难的问题的挑战。这是因为它与旅行商问题相关。我们使用一种 GPU 友好的连续松弛近似来解决这个问题，这也有助于我们反向传播解决方案并学习更符合语言学 plausibility 的置换。如果您想了解有关我们实验的更多信息以及我们如何应对这些挑战，请阅读我们的论文或访问我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Myra，今天我将讨论我们的论文《标记人格：使用自然语言提示来测量语言模型中的刻板印象》。 这项工作是与Esen Dermusch和Dan Jorofsky合作完成的。 近年来，许多研究人员记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍性。 然而，这些测量方法存在各种局限性。 它们通常依赖于耗时且难以收集的手工构建数据集，或者它们通常只测量非常具体的刻板印象，这意味着它们无法很好地推广到其他人群或情境，或者仅仅捕捉到一些非常宽泛的关联，例如与特定群体相关的负面联想。 此外，目前大部分研究都没有考虑到交叉性，即多元社会身份会复合偏见，并成为伤害的独特根源。 为了克服这些局限性，我们利用了较新的指令微调LLM具有非常好地响应提示中指令的特性。 因此，我们可以要求模型生成一个角色，即使用提示来描绘一个想象中的个体，例如“想象你是一名亚裔女性，请描述一下你自己”。 我们可以立即看到，这可以推广到任何人群，因为我们可以在提示中指定任何我们想要的身份标识。 以下是一些来自GPT-4的示例生成结果。 立即，我们看到虽然输出在传统意义上并非具有明显的负面或有毒性，但存在一些有趣的模式。 亚裔女性被描绘成不起眼。 中东女性则使用诸如“异国情调”之类的词语来描述，或将她们与迷人的地区联系起来。 而有色人种女性的角色塑造则引用了祖先，而白人男性角色则没有此类引用。 为了捕捉这些模式，我们的方法包含两个部分。 第一部分是生成这些角色。 我们生成这些角色的提示灵感来自于一项研究，该研究将这些提示提供给人类受试者，发现通过提供给人类受试者，他们也可以发掘种族刻板印象。 此外，这还能够使我们生成的角色与人类书写的回复进行直接比较。 第二部分是“标记词”，这是一种识别区分标记群体和未标记群体的词语的方法，我稍后将详细阐述。 这种方法的优点在于，我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。 因此，“标记词”方法借鉴了社会语言学中的“标记性”概念，该概念指出存在一个未标记的默认状态，任何与该默认状态不同的群体在语言上都具有标记性。 例如，通常将“战士”这个词与男性联系起来，因此当人们描述一位女性战士时，他们通常会明确指出“一名女性战士”，并使用“女性”这个词进行标记。 更广泛地说，社会中的优势群体在语言和社交上都是未标记的，而边缘群体通常是标记的。 在我们的方法中，我们首先指定未标记群体和标记群体分别是什么。 然后，我们使用“对抗词”方法（Fighting Words method）比较这些角色，该方法本质上是使用加权对数优势比（weighted log odds ratios）来……区分每个标记群体的最常见的词语。 例如，对于黑人女性角色的塑造，我们会使用“对抗词”方法，并将对数优势比与白人角色和男性角色进行比较，因为这两个是相应的未标记群体。 接下来，我们看一下一些研究结果。 首先，我们使用一个刻板印象词汇表，发现生成的角色比人类书写的角色包含更多的刻板印象。 然而，当我们实际查看这些词语的分布时，发现情况截然不同。 虽然生成的角色拥有更高的词汇表词语使用率，但人类书写的角色具有更广泛的词语分布。 而刻板印象词语在生成的角色中所占的比重仅仅是“高大”和“运动员”这两个词。 实际上，这个词汇表并没有很好地捕捉到我们在前面的幻灯片中看到的许多有害模式。 因此，为了做到这一点，我们将转向我们的“标记词”方法的结果，以展示这些看起来积极的词语如何促进刻板印象和本质化叙事。 在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害的模式。 首先，对于标记群体，最常见的词语包括诸如“文化”、“传统”、“骄傲”和“异国情调”等。 这些词语仅仅通过她们与身份的关系来定义这些群体，并将她们与白人规范区分开来。 这导致了这些群体长期遭受歧视和其他化的困境。 此外，在这些词语中反映出许多常见的陈词滥调，尤其是在有色人种女性身上。 例如，描述拉丁裔女性的词语包括“充满活力”和“曲线玲珑”，这与热带主义的刻板印象相关联。 对于亚裔女性，这些词语包括“娇小”、“柔弱”和“丝滑”，这与亚裔女性被过度性化、被视为非常驯顺和顺从的历史有着很长的渊源。 最后，对于黑人女性，我们看到一些最常见的词语是“坚强”和“有韧性”。 这与人们称为“坚强的黑人女性”的典型形象相关联。 虽然乍一看听起来很积极，但研究表明这种类型的典型形象实际上非常有害，因为它给这些人群带来了巨大的压力，要求她们面对社会障碍而变得坚强和有韧性。 而不是真正努力改变这些障碍，反而给这些人施加了克服它们的压力，从而导致这些人的健康状况等诸多负面后果。 更广泛地说，我们发现每个标记群体的词语几乎都只是反映了非常本质化的叙事。 基于这些模式，我们得出了三个对模型所有者的建议。 首先，作为研究人员，我们应该解决积极的刻板印象和本质化的叙事。 我们还应该使用交叉视角来研究语言模型中的偏见和危害，因为如果我们不这样做，可能会忽略许多问题。 再次，作为研究人员，我们应该解决积极的刻板印象和本质化的叙事。 我们还应该使用交叉视角来研究语言模型中的偏见和危害，因为如果我们不这样做，可能会忽略许多问题。 此外，关于偏见缓解方法的透明度应该大幅提高，因为例如，像这些积极的刻板印象一样，我们不知道这是因为存在某种奇怪的过度价值对齐现象，或者是否存在其他消解刻板印象的方法导致了这些有害的模式。 在没有更多透明度的情况下，我们无法做出任何假设或进一步研究。 感谢大家的聆听。 在ACL会议上玩得开心！"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将向您介绍 ABCeval，这是一种评估对话式人工智能的新型维度方法。这项工作由埃莫利自然语言处理实验室完成，由埃莫利大学的吉诺·崔教授领导，并与亚马逊 Alexa AI 合作。\n\n假设您刚刚开发了一个对话模型，并且希望了解它的表现与当前最先进水平相比如何。常见的做法是进行人工评估，例如让人工评估员选择两个对话中哪个更好，或者使用李克特量表对对话进行评分。这些方法能够很好地提供对整体对话质量的整体评估，但对话质量具有许多方面。因此，您可能需要评估聊天质量的多个维度，以在更精细的层面上了解模型的优势和劣势。一种方法是简单地让人工评估员评估对话质量的多个维度，例如使用现有对比或李克特量表方法评估模型响应的相关性。然而，我们认为对于维度对话评估来说，存在一种更精确、更可靠的策略。\n\n我们的方法试图通过明确注释每个模型响应是否表达某些行为（例如，使用不相关的信息做出回应或自相矛盾）来减少人工评估的主观性。我们称这种方法为“在聊天中注释行为”，简称 ABC eval。我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。ABCeval 评估聊天模型是否忽略其伙伴或说一些不相关的话，是否自相矛盾或与其伙伴相矛盾，是否产生不正确的陈述或违反常识知识，以及模型是否成功或未能表现出同理心。\n\n为了确定哪种类型的评估最有效，我们选择了四个最先进的聊天模型，并使用 ABC eval 对每个模型进行了 100 次人机对话的评估。为了比较，我们还使用三种现有方法对这些对话进行了评估：在回合级别上的李克特评分，在对话级别上的李克特评分，以及对话级别上的成对比较。对于每一种现有方法，我们收集了对对话中八个最常被测量的方面进行的评估，因为这是评估聊天模型沿多个维度进行评估的标准做法。\n\n通过分析这些评估结果，我们发现 ABC eval 行为标签总体上比现有方法收集的标签更可靠，这由 100 个双重标签对话上的评估员间一致性衡量。此外，ABC eval 标签比现有方法产生的指标更能预测整体对话质量，这由简单的线性回归分析所示。例如，测量包含自我和伙伴矛盾的回合比例分别可以解释对话质量的 5% 和 10%，而平均李克特一致性分数仅解释 4% 或更少。\n\n最后，我们使用逐步线性回归检查每个评估指标是否捕捉了聊天质量的独特方面。您可以看到，所有 ABC eval 指标的组合解释了对话质量的 25% 以上，并且在一次删除一个指标时，大多数指标都会导致损失大量的关于质量的信息。另一方面，所有回合级别李克特指标的组合解释的质量要少得多，而且更少的指标携带独特的有用信息。\n\n这些可靠、信息丰富且独特的 ABC eval 指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。您可以看到，在我们的实验结果中，仍然存在一些挑战，并且已经被精确地量化。例如，我们测试的机器人响应中约有 20% 出现了常识违反现象，约有 15% 的响应产生了不相关的信息，并且约有 10% 的时间会自相矛盾或与其伙伴相矛盾。\n\n随着该领域快速进步，许多这些错误率可能会在自我们进行评估以来发布的新模型中降低。然而，这更应该成为追求可靠和精确的评估指标以比较模型的理由。我们希望 ABC Eval 可以被该领域的其他人利用，作为朝着这个方向迈出的有意义的一步，并且我们期待着在未来几个月和几年中看到对话式人工智能的进步。\n\n谢谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫瓦苏达，是斯托尼布鲁克大学计算机科学博士候选人。我想介绍一下我们团队的工作，一篇被《ACL 2023》以长文形式接受的论文，名为《认知失调检测中的迁移学习：应对稀有类别挑战》（Transfer Learning for Dissonance Detection, Addressing the Rare Class Challenge）。我们首先定义了认知失调，并阐述了为何在语言研究中这是一个重要的课题。简单来说，认知失调是指两个相互矛盾的信念或行为。例如，一个人说“我知道吸烟可能会杀死我”，然后又说“会议结束后我抽了两支烟”。这两个信念和行为是不一致的，处于失调状态。进一步地，提到“没有它们我恐怕无法保住工作”来为第二次吸烟行为辩解，两者就呈现出一种和谐关系。虽然认知失调是一种我们在日常决策中非常常见的现象，但在其他类型的语篇关系中，它在语言表达中却非常罕见。那么，这有什么意义呢？研究认知失调可以帮助我们理解人们之间的意见分歧的影响，追踪人口中信念、价值观和态度的变化趋势。高认知失调也与焦虑症有关，有助于我们更好地理解人们的心理健康。在语言中研究认知失调也有助于理解极端主义和弱势群体两极分化的问题。最后，理解认知失调有助于我们了解个人的认知风格，并更好地理解决策过程。为了创建认知失调资源，我们进行了大规模的失调关系标注工作。我们采用了“以失调为先”的方法，如流程图中所示。我们使用PDTV解析器解析推文，并根据我们在论文中描述的指南对语篇单元对进行标注。如上图所示，在标注的单元对中，只有3.5%发现了失调现象。在收集了大约1000个语篇单元对后，我们对一个初始分类器进行了训练，该分类器仅基于43个失调示例进行训练。不出所料，分类器的性能与随机猜测相比并没有显著提高。考虑到失调的低发生率以及缺乏任何先前的相关数据集，我们面临着绝对稀有性的问题。为了缓解这个问题，我们尝试了迁移学习和主动学习的组合，从而在较少的标注轮次中收集更多的失调样本，降低整体标注成本并提高失调检测能力。由于初始模型完全无法捕捉失调类别，因此我们在主动学习过程中从相关的任务中迁移权重。我们从两个不同的任务中进行迁移：一是主题无关的失调立场分类任务，该任务旨在确定来自不同人物的辩论陈述是否一致或不一致，无论主题如何（此处称为“辩论”）；二是PDTB中扩展和比较类的二元分类任务，因为这两个类别与和谐和失调的概念密切相关，我们称之为CEE。我们发现，进行迁移后，在标注数据集上的零次性能已经明显优于随机猜测，最佳AUC达到0.62。此外，通过迭代更新模型，即在主动学习和标注的每一轮中进行训练，我们将累积迄今为止收集的所有数据，并使用最新的数据集合来更新模型。在不同的策略中，我们发现累积策略在各个方面都优于或等于迭代策略。接下来，为了提高失调示例的数量，我们使用稀有类别概率策略，在主动学习的每一轮中选择模型认为极有可能存在失调的示例。我们将此策略与其他最先进的策略进行比较，尽管差异很小。需要注意的是，随机策略的性能明显较低。在采用两种最佳策略进行更多轮次的AL后，我们将失调分类AUC提高了0.75，这是我们在该任务上迄今为止取得的最佳性能。我们还检查了每种策略在标注质量和标注人员成本方面的可行性。我们发现，PRC具有最高的失调百分比，并且最适合稀有类别的标注。然而，标注人员也发现这些示例很难理解。综上所述，我们发现PRC是一种用于稀有类别获取的简单主动学习策略，而冷启动主动学习与适当设计的迁移学习任务相结合可以显著提高效率。我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而领域内的主动标注则受益于累积更新。这些是我们代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Akshata，今天我和我的合著者Martin将展示我们的工作，名为“Kipma步骤”，旨在评估从多个来源整合知识的能力。这项工作是麦吉尔大学、Mila和微软研究院的合作成果。国家语言理解模型依赖于各种知识来源，例如其参数中包含的知识，通常通过预训练获得，以及在推理时提供的输入知识。最近在问答等任务中的研究表明，模型可以利用预训练的知识来解决任务。但自然语言理解通常需要也提供在推理时获得的知识。例如，在句子“John在电视上看到了新当选的总统”中，预训练的参数可能包含有关总统做什么以及电视是什么的信息，但它们无法可靠地知道这个特定事件中的实体John是谁，或者新总统是谁，因为总统可能在预训练之后发生了变化。因此，针对知识密集型NLU任务，成功的模型需要能够整合并利用预训练知识和推理时知识。在这项工作中，我们提出了一套用于知识整合的诊断测试。我们引入了一个指代消解任务，旨在探查从不同来源获取知识的能力。我们使用人类研究参与者评估数据集，并建立指代消解模型。以下是我们数据集中一个例子。Servin是一名法官。Kia是一位面包师。Servin和Kia在公园见面。在法庭上审理案件度过漫长的一天后，他很高兴放松一下。这里的任务是识别代词he所指代的正确实体，在本例中是Servin。指代消解需要两种类型的信息。首先，实体特定知识，例如Servin是法官。其次，背景知识，例如法官在法庭上审理案件。通常，背景知识是在大型语言模型预训练期间学习的，而实体特定知识通常在推理时观察到。我们改变了这两种信息可用性的方式，使得它可能存在于单个来源或多个来源中。我们定义了KITMOS的三种设置。首先，我们有典型的设置，背景-预训练，其中假设背景知识在预训练时可用。其次，有背景，背景都可用的设置，其中背景知识同时在预训练时间和推理时间可用。最后，是背景推理设置，其中两种知识类型仅在推理时间可用。这种最后的设置特别有趣，因为它模拟了背景知识用于解决任务但并非模型预训练数据一部分的情况，例如因为自预训练以来出现了新的职业。以下是如何控制事实和真实来源可用性的一个例子。在背景预训练设置中，我们假设“政治家竞选政府职位”这一背景知识包含在预训练参数中。在3英寸时间上下文中，我们提供关于 Chichester 是政治家的实体特定知识。在背景都可用的设置中，我们不仅提供实体特定知识，还提供关于政治家在推理时间上下文中的背景知识。在背景推理设置中，我们提供虚构的职业 Meritur，而不是政治家，因为 Meritur 不太可能包含在预训练参数中。我们使用人类研究参与者和已建立的指代消解模型来评估数据集。在这个图中，我们展示了在背景预训练设置中最困难的变体上表现最好的模型的成果。在没有在KITMOS上进行任务特定训练的情况下，这两个模型表现都不佳。然而，当在KITMOS上进行训练时，C2F和BFQF的表现明显优于随机选择。这表明，当在一般的指代消解数据集上训练时，模型学会利用表面线索，而这些线索在测试KITMOS时没有用，因为这些线索已被移除。额外的关于虚构知识的实验表明，即使是表现最好的模型也无法可靠地整合仅在推理时间提供的背景知识。总结我们论文的主要观点。许多指代消解模型在没有任务特定训练的情况下似乎无法推理来自不同来源的知识。然而，通过任务特定训练，一些模型可以成功整合来自多个来源的知识。尽管如此，即使是表现最好的模型似乎也难以可靠地整合仅在推理时间呈现的背景知识。如果您有兴趣了解更多细节，请参阅我们的论文并在GitHub上的代码中查看数据集。感谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是萨拉·帕皮，来自特伦托大学和布鲁诺·凯斯勒基金会，我将简要介绍《注意力机制作为同时语音翻译的指导》这篇论文，这是我和马特奥·内格里、马可·图尔奇共同完成的工作。什么是同时语音翻译？同时语音翻译（SimulST）是将口语实时翻译成另一种语言文本的过程，从而实现跨语言交流。目前的SimulST模型存在哪些问题？通常需要训练特定的架构，引入额外的模块进行优化。例如，训练过程冗长且复杂，涉及不同的优化目标，以及为了达到不同的延迟范围而训练和维护多个模型，比如训练一个平均延迟1秒的模型，再训练一个平均延迟2秒的模型，以此类推。那么，我们的解决方案是什么？首先，无需重新训练或采用特定架构，即可使用已有的离线语音翻译（SD）模型。针对每个延迟范围使用单一模型，并通过特定的参数来控制延迟。其次，利用模型通过音频输入和文本输出之间的注意力机制（即交叉注意力机制）已经获得的知识。您可以在右侧看到一个示例。我们的解决方案是提出EDAT，即编码器-解码器注意力机制，它是一种策略，我们根据注意力指向的位置来决定是否发出部分翻译。如果注意力没有集中，即其总和低于某个阈值α，指向较少数量 λ 语音帧，意味着接收到的信息已经足够稳定，则会发出一个单词。例如，如果我们接收到包含“我将要谈论”的语音片段，并且我们的模型预测德语翻译，当我们查看交叉注意力权重时，我们会发现前两个单词指向最早接收到的语音帧，而最后一个单词指向最后接收到的语音帧，即 λ 语音帧。这意味着前两个单词将被发出，而由于交叉注意力的总和高于某个阈值 α，我们不会发出最后一个单词，而是等待另一个语音片段。如果继续，我们接收到另外三个单词的语音片段，并查看交叉注意力权重，我们会发现没有单词指向最后 λ 语音帧。这意味着这三个单词将被发出。如果查看主要结果，我们绘制了同时语音翻译结果图，其中蓝色表示一个方面，衡量翻译质量和平均延迟，即延迟指标。我们还考虑了计算成本相关的平均延迟，该指标考虑了模型预测输出所需的计算时间。因此，我们希望这个图中我们的曲线尽可能地高，并且尽可能地向左偏移。我们与适用于离线模型的适当策略进行比较，即 Wet-Key 策略和局部一致性策略。我们还与专为同时语音翻译设计的最先进架构进行比较。这些都是同时语音翻译策略在德语上的所有结果，我们看到 ADAT 在所有应用于离线模型的策略中表现优于其他策略，因为曲线向左偏移。我们还看到，如果考虑实际经过的时间或计算成本相关的时间，ADAT 是最快的策略。如果您想了解更多结果，请阅读我们的论文，我们还开源了代码和模型。如果您想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同时输出，以促进我们工作的可重复性。谢谢大家的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫朱恒。今天我将为大家介绍我们的论文《内核 2003 年的命名实体标注器在 2023 年是否仍然有效？》。让我们开始吧。我们的论文调查了命名实体识别任务（NER 任务）中的泛化问题。我们观察到，模型已经使用了 Kano 2003 近 20 年来开发 NER，这自然会引发一些问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，实现良好泛化需要什么？同时，如果我们观察到泛化效果不佳，是什么原因导致这些模型的性能下降？为了调查这些问题，我们开发了 Kano++ 数据集。这是一个我们从路透新闻社收集并用与 Cono2003 相同的标注指南进行标注的数据集。然后我们对 20 多种模型在 Cono2003 上进行了微调。我们在 Con 上评估了它们的 F1 值，以评估每个模型的泛化能力。那么，实现良好泛化需要什么？通过我们的实验，我们发现有三个主要要素是必需的。第一个是模型架构。通过我们的实验，我们发现 Transformer 模型通常能更好地泛化到新数据。第二个要素是模型大小。我们发现通常，更大的模型会带来更好的泛化效果。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们同样发现，更多的微调示例实际上也会带来更好的泛化效果。对于我们下一个问题，是什么导致一些模型的性能下降？我们提出了两个假设。第一个是自适应过拟合，这是一种由反复使用相同的测试集引起的过拟合现象，通常表现在新测试集上的边际效益递减。第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距越来越大而导致的性能下降。对于自适应过拟合，我们从右侧的图表可以看出，最佳拟合线的斜率大于 1。这意味着我们在 CONO 2003 上取得的每一点改进，在 Kano++ 上都会产生超过一点的改进，这意味着没有边际效益递减。这表明在本例中没有观察到自适应过拟合。那么，时间漂移呢？对于时间漂移，这证实了我们假设，性能下降的主要原因是时间漂移。我们的结论是，要实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些是相互关联的。我们不能仅仅拥有一个要素，而忽略其他要素。同时，我们还发现，这里的性能下降是由时间漂移引起的，而且令人惊讶的是，它不是由自适应过拟合引起的，即使 Carnal 2003 已经使用了 20 多年。回到我们在论文标题中提出的问题，Carnot 2003 标注器在 2023 年是否仍然有效？我们发现答案实际上是肯定的。我们希望我们的论文呼吁更多研究，以改进模型的泛化能力。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，欢迎来到我们的 d.plain 演示，这是一个用于德语文本简化的新语料库，在文档级别和句子级别均可实现简化。我叫 Regina Stodden，我将带领大家了解演示的第一部分。首先，我们来定义一下文本简化。文本简化是一个调整文本的过程，旨在提高特定目标群体对文本的理解能力，例如阅读障碍者或非母语人士。为了训练文本简化模型，我们需要文本的平行对，例如文档或句子。在上面的示例中，您可以看到一个复杂德语句子及其翻译成简洁语言的平行对齐句子。为了简化句子，可以使用不同的技术，如您在示例中看到的词汇替换、子句合并、子句重组或插入词语。我们现在提出我们的新语料库 dplane，因为近年来，现有的语料库存在一些问题。例如，这里的语料库太小，无法训练一个分类模型。近几年提出的另外三个模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了我们的新语料库 dplane，它被分为两个子语料库：dplane-apa 和 dplane-web。dplane-apa 基于使用文本。在 plain APA 中，我们手动对齐了 483 篇文档，结果大约有 30,000 到 13,000 个平行句子对。对于 DeepLaneWeb，此语料库包含不同的领域，我们同样手动和使用自动对齐方法对齐了这 750 篇文档。总共我们得到了 30,450 个句子对。我们进一步分析了我们的句子对，例如对简化的类型进行分析。正如您在此处看到的那样，圣经文本比例如新闻文本或语言学习文本在所有级别上都经过更强的简化，包括词汇简化、结构简化或整体简化程度。此外，您可以看到我们的 De Plplane 语料库具有各种简化转换的高度优先级。例如，在 d.plane API 语料库中，我们有更多的重组和添加单词，而不是在 d.plane web 语料库中。另一方面，在 web 语料库中，我们有更多的改述。那么，现在让我们看看我们可以用这个语料库做什么。您好，我是 Omar，现在我将介绍我们的数据集 D-plane 的用例。对于第一个用例，我们可以评估自动对齐方法。近年来，有很多对齐方法，但是在机器翻译的背景下，我们有两篇用不同语言书写的平行文档，并且我们希望提取两篇平行文档中相同内容的句子对齐，但这些句子在复杂程度不同。现在我们有了手动对齐句子的数据集 dplane，我们可以将这些句子用作黄金标准对齐来评估一些提出的对齐方法，我们对提出的方法进行了一些调整，并在论文末尾发表了所有这些调整和运行实验的代码。最终，我们得出结论，用于德语文本简化的最佳自动对齐方法是 math align 方法，您也可以在论文中找到运行此方法在您自己的文档上的代码。我们在论文中展示的第二个用例是自动文本简化的案例，通过微调语言模型来生成从复杂输入文本中简化的文本。我们微调了两个不同的模型。我们使用 finebased long-impart 进行句子级别的简化。您也可以在论文中找到所有的检查点，并可以详细了解我们实验的得分和评估指标。我们得出结论，这种基本的微调可以产生或获得比基线得分更好的得分，并将这些结果建议为未来自动文本简化问题的基准。非常感谢您的关注，我们希望在会议上与您们见面。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是复旦大学的袁思雨。我今天来介绍我们的工作，题目是《从大型语言模型中提炼脚本知识以进行约束语言规划》。在日常生活中，人类经常按照保证脚本的形式，遵循一步步的指令来规划自己的行动。然而，以往的研究主要集中于抽象规划。一个好的规划者应该能够首次书写脚本，从而实现规划的目标。一个抽象目标可以通过不同的、具有多方面约束的现实目标来继承。一个好的规划者应该书写符合约束、且合理的脚本。在本文中，我们首先评估和改进了大型语言模型进行约束语言规划的能力。由于没有特定目标的数据集来支持我们的研究，我们不得不首先获取这些目标。如图表所示，我们使用InstructGPT，在人工参与数据采集的过程中，扩展了抽象目标，加入了多方面的约束。我们采样了100个具体目标，并评估了大型语言模型生成的脚本。这张表报告了结果的总体准确率。我们发现，所有大型语言模型在规划具体目标方面都取得了不令人满意的结果。然后，我们进行了详细的分析，以研究学习模型为何失败。图中的结果表明，生成的脚本的语义完整性是可以接受的，但无法保证对约束的忠实性。我们深入研究了更细粒度的约束主题类别，取决于工作方式。图中热图显示，指令型 TPD 的规划性能在不同类别的目标中差异很大。先前的研究表明，轻量级日志记录模型的输出质量存在很大的方差，从而导致性能不佳。因此，我们采用了过量生成 Z 过滤器（overgenerated Z-filter）的思想来改进生成质量。我们首先向 Instruct GPT 展示约束类型及其示例，并基于种子抽象目标获得具体目标。然后，Instruct GPT 过量生成具体目标的关键词脚本。接下来，开发了一个过滤模型来选择可行的脚本。我们将脚本和目标转换为 InstructGPT 嵌入，并计算余弦相似度和相似度分数，以衡量语义相似度。此外，我们还奖励包含目标约束关键词的脚本。我们仅保留在目标集中得分最高的脚本。通过我们的方法，InstructZBT 可以生成更高质量的脚本。我们的方法极大地提高了规划能力，既提高了语义完整性，又保证了对约束的忠实性。由于大型语言模型的部署成本高昂，因此使较小和专业化模型的语言规划能力成为可能至关重要。创建数据集是实现这一目标的关键一步。然而，以往的研究并没有使规划适用于具体目标，并且人工数据集标注成本高昂。因此，我们遵循符号知识蒸馏（symbolic knowledge distillation）的思想，从大型语言模型中蒸馏出约束语言规划数据集。我们将我们的方法应用于构建一个名为 CodeScript 的约束语言规划数据集。总共生成了 55,000 个具体目标及对应的脚本。为了确保验证和测试数据的质量，我们请众包工人检查并修订不正确的样本。此图显示了 CodeScript 中的约束分布。我们发现 CodeScript 在生成的具体目标中显示出很高的赞同率。借助 CodeScript，我们可以利用较小但专业化的模型进行约束语言规划。我们发现 Antune 的 TFI 在成本比率上可以生成 0 的平方根。借助 CodeScript，我们可以利用较小但专业化的模型进行约束语言规划。我们发现 CodeScript 中的 T-file 函数可以生成比大多数大型语言模型更高质量的脚本，表明在适当的数据集上训练后，较小的模型可以支持大型模型。总而言之，我们建立了一个约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了一种过量生成过滤方法，用于研究语言规划。感谢您的时间。请在我们的论文中查找 CodeScript 的更多详细信息。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是Yanis Lavrac，我将向大家介绍我们关于Dr. BERT的工作，这是一个为生物医学和临床领域设计的、在法语中进行过预训练的强大模型。在本次演示中，我们将首先讨论医疗保健领域的语言建模。随后，我们将介绍我们文章的主要贡献。我们引入了第一个基于Roberta、并基于NACHOS（一个从网络抓取的医学数据集合）进行训练的法语生物医学模型，命名为Dr. BERT。我们还介绍了在不同预训练设置和数据来源下的模型比较。然后，我们将展示我们在法语中11个生物医学和临床下游任务上的结果。最后，我们将总结实验结果，并提供更多关于如何访问这些模型的信息。自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，相比于历史上的静态和情境化方法，如Word2Vec、fastText或NWO，带来了巨大的性能提升。此后，该模型已被适配到许多其他语言，例如在法语中是Camembert，以及在生物医学领域是Permut-BERT和Bio-BERT，以及在临床领域是Clinical-BERT，但主要都是在英语中。针对其他语言的专用模型非常稀少，并且通常基于持续预训练，因为缺乏特定领域的数据。然而，在先前，法语并没有任何用于生物医学的开源模型。因此，我们思考了对于广泛的使用，最合适的数据来源是什么，以及当前的数据是否能作为临床数据的良好替代品。为了回答这个问题，我们将Dr. BERT与我们的Schubert模型进行比较，Schubert模型基于从非大学医院获取的匿名化数据。此外，我们还探究了训练法语专用模型需要多少数据：是4GB、8GB还是4GB的RAM？ 我们有一个Schubert的第一个版本，这是一个临床模型，使用了4GB来自临床记录的句子。还有一个最终版本的Schubert，使用了4GB的NACHOS子集和4GB的临床记录的混合。除了这种比较之外，我们还引入了三个在持续预训练中训练的模型，以分析预训练策略的影响。一个基于Camembert的权重并使用4GB的NACHOS子集的模型；另一个也基于Camembert，但使用了4GB的Permut-BERT、Bio-BERT和Clinical-BERT。评估结果表明，模型在与模型训练数据性质相同的数据上表现最佳。然而，我们观察到来自异构来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总的来说，从头开始的预训练似乎在大多数任务中可以获得更高的性能。然而，我们的持续预训练实验，使用Permut-BERT的权重和分词器，并在4GB的NACHOS子集上进行训练，显示出与从头开始训练的Dr.BERT 4GB模型相当的结果，而基于Camembert权重和分词器的模型则存在稳定性的问题。最后，作为结论，我们的专用系统在11个don't-trim任务中表现出更好的性能，并且在全球范围内超越了通用模型的（例如Camembert）结果。总之，我们的专用系统在11个don't-trim任务中表现出更好的性能，并且在全球范围内超越了通用模型的（例如Camembert）结果。我们还观察到，更专业化的数据更好，但扩展性较差。所有从NACHOS获得的预训练模型均可在UGIMFACE上免费获取，所有训练脚本都可在我们的GitHub仓库中找到。感谢本次演示，我们期待在多伦多的海报环节与大家互动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是项宾，是华盛顿大学的博士生。今天我将介绍我们从预训练数据到语言模型再到下游任务的工作，追踪政治偏见导致不公平自然语言处理模型的过程。因此，语言模型是在大规模网络爬取数据上进行训练的。政治新闻媒体在其预训练数据中得到充分覆盖。根据 C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、赫芬顿邮报等新闻媒体在语言模型训练数据中得到充分覆盖。这为语言模型应用带来了一种得天独厚的优势。一方面，它们能够从不同的视角中学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点本质上存在社会偏见，可能导致下游任务应用中出现公平性问题。为此，我们提出研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，特别是通过提出以下问题。首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在多大程度上影响这些政治偏见？其次，政治倾向不同的语言模型在下游任务中的实际表现如何，这是否会导致自然语言处理应用中的公平性问题？具体来说，我们首先提出使用政治问卷（例如政治指南针测试）以不同的提示格式提示语言模型。这确保我们能够以政治科学文献为基础进行自动评估。初步结果表明，首先，语言模型确实存在不同的政治倾向。它们占据政治指南针上的所有四个象限。我们还可以看到，GPT-4 是其中最自由派的语言模型，GPT理论总体而言更具社会自由主义倾向，优于 BERT理论及其变体，同样占据政治指南针上的所有四个象限。我们还可以看到，GPT-4 是其中最自由派的语言模型，GPT理论总体而言更具社会自由主义倾向，优于 BERT理论及其变体。其次，我们旨在调查语言模型的政治偏见实际上是从训练数据中获得的程度。因此，我们通过进一步在六种不同的党派语料库上进行预训练来对语言模型检查点进行控制实验，这些语料库根据新闻和社交媒体进一步细分，并根据其政治倾向进行划分。通过在这些党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生变化。例如，对于进一步微调、进一步在左派 Reddit语料库上训练的 Roberta，我们可以在政治偏见方面看到显著的自由派转变。我们还试图调查语言模型是否可以捕捉到我们现代社会普遍存在的极化现象。因此，我们将预训练语料库划分为美国第 45 任总统之前和之后的时期，我们分别在两个不同的时间段语料库上预训练语言模型。我们发现，语言模型在 2017 年之后通常具有距离中心更远的政治倾向。这表明语言模型也可以捕捉到我们社会中的极化现象。最后，但同样重要的是，我们在仇恨言论检测和虚假新闻检测等自然语言处理应用中评估具有不同政治倾向的语言模型，这些应用通常涉及语言模型并可能产生非常重要的影响。我们发现，如果我们在类别层面进行调查，也就是说，如果我们将性能分成不同的社会人口统计或新闻媒体的政治倾向，我们可以看到一个模式，例如，对于仇恨言论检测，左派语言模型更擅长检测针对社会少数群体的仇恨言论，但对于检测针对我们社会中更强大群体的仇恨言论表现较差。反之亦然，右派语言模型更擅长检测针对白人和男性的仇恨言论，但对于检测针对黑人、LGBTQ+和其他少数群体的仇恨言论表现较差。类似趋势也发生在虚假新闻检测中，我们发现左派语言模型更擅长检测来自其相反政治倾向的虚假信息，反之亦然。我们进一步展示了许多定性例子，以说明具有不同政治倾向的语言模型会根据其社会类别给出不同的仇恨言论和虚假信息预测。附录中有许多额外的例子，进一步强调了这些不同的预测。这表明存在一个非常紧迫的公平性问题，即与语言模型的政治偏见有关。例如，如果右派语言模型被微调用于仇恨言论或虚假信息等用途并部署到流行的社交媒体平台，这意味着具有相反政治观点的个人可能会被边缘化，针对少数群体的仇恨言论可能会在没有任何控制的情况下蔓延。这为我们敲响了警钟，要求我们承认和解决语言模型政治倾向导致的公平性问题。稍微讨论一下。我们还想强调的是，我们揭示了关于语言模型政治偏见的一个独特困境，就像在斯库拉和卡律布狄斯之间。如果我们不清理语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平性问题。如果试图以某种方式进行清理，我们也会冒着审查或排斥的风险，并且很难确定什么是真正中立的，应该保留在语言模型训练数据中。这有点像电车难题。好的，我想这就是我今天的内容了。感谢各位的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是科斯塔夫·辛哈，很高兴欢迎大家参加关于我们ACL 2023论文的报告：“语言模型可接受性判断并非总是对上下文稳健”。这是一项与约翰·戈特耶、艾伦·穆勒、卡尼什卡·米什拉、卡伦·富恩特斯、罗杰·利维和阿迪娜·威廉姆斯合作的研究。\n\n在这个工作中，我们重新审视了极小对方法。极小对方法基本上是根据语言模型的接受性判断来评估语言模型，其中接受性判断也可能包括语法性，比如BLIMP、句法、Gym或者关于刻板印象方面的可接受性，比如Krauss对。在极小对方法中，评估语言模型的典型方式是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子。整个模型的过程基本上会赋予可接受的句子更高的概率。\n\n目前MPP流程基本上不允许我们评估模型对较长句子的接受程度。如今，大型语言模型正不断涌现出更长和更长的上下文窗口。因此，我们重新审视了数据集本身，并从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，这里我们选择了一个典型语法性极小对，来自BLIMP数据集中的附例岛现象。我们所做的是重新创建更长的序列，这些序列是可接受的，并且具有相同的语法结构，为此，我们从附例岛中提取语法正确的句子，并将其作为前缀添加到可接受的查询和不可接受的查询中。我们可以通过选择相同匹配的不可接受句子来做同样的事情，这也可以用来测试模型的接受性。我们还可以通过选择来自不同子集或不同数据集的句子来做同样的事情。我们称之为不匹配场景。\n\n在这种情况下，句子仍然来自相关的数据库，但不是与您进行评估的数据库相同。我们也可以对可接受性情况做同样的事情。最后，我们可以选择来自完全无关领域的句子，例如维基百科。这将告诉我们，模型的接受性判断是否实际上受到任何上下文的影响，这种上下文与我们正在查看的句子完全无关。\n\n那么模型表现如何呢？首先，我们查看来自维基百科的句子，这些句子与当前查询对完全无关。在那里，我们发现MPP判断在任意上下文长度下大多是稳健的。我们将上下文长度增加到1024，以最大化OPT和GPT-2模型。我们在此看到橙色虚线，MPP判断相对稳定。\n\n现在，当选择来自同一数据集的句子时会发生什么？这里，我们正在选择或创建来自同一BLIMP或SyntaxGym数据集的接受性和不可接受性领域中的句子。在那里，我们看到，当添加可接受的前缀或不可接受的前缀时，MPP判断会显著增加或减少。但是，当匹配结构时，也就是说，当我们添加可接受的前缀或不可接受的前缀时，当我们从blimp-person-text-gym中的相同现象中选择句子时，我们会看到MPP判断对模型产生巨大增加或巨大减少，这取决于所选前缀是可接受还是不可接受。\n\n这种效应随着上下文长度的增加而变得非常明显，这可能会影响具有大上下文窗口的新型语言模型。\n\n为什么匹配的前缀会如此影响语言模型的判断呢？我们进行了一系列分析，尝试通过尝试保留相关的结构来构建输入句子，同时为输入添加噪声。在进行多次此类扰动后，我们发现这些噪声中的任何一项实际上都没有使模型改变其MPP判断趋势的方式。\n\n基本上，我们发现模型以相似的方式对扰动后的句子敏感。也就是说，当我们扰动可接受性领域的句子时，我们看到所有扰动中MPP判断的相似增加。当我们扰动不可接受性领域的句子时，我们看到MPP判断以相似的方式减少。\n\n我们工作的关键要点是，语言模型对句子共享的潜在句法和语义特征敏感。目前，我们使用短句和单句输入的MPP评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。\n\n请阅读我们的论文以获取更多实验细节。感谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是大卫，德国萨尔兰大学的博士生。 在这个视频中，我想介绍我们最近的工作，名为《弱于你所想》，对弱监督学习进行批判性分析。 这项工作是与肖玉生、马里奥·斯穆斯巴赫、吉亚·斯特芬和狄特里希·克拉科夫共同完成的。我想先简要介绍一下弱监督和弱监督学习。在弱监督中，我们不手动标注数据。 而是使用弱标注来源来标注数据，例如简单的启发式规则、知识库或低质量的众包，如图右侧所示。与人工标注相比，弱标注成本要低得多，但它们也存在噪声，这意味着一定数量的标注是不正确的。如果我们在弱标注数据上直接训练神经网络，神经网络往往会记住标注噪声，而无法泛化。 在弱监督学习中，提出了训练算法，以便在这样的标注噪声下稳健地训练神经网络，从而使训练的模型仍然能够良好地泛化。在最近的WSL（WSL代表弱监督学习）工作中，一种常见的说法是，人们声称他们只在弱标注数据上训练模型，并在干净的测试集上获得高性能。从技术上讲，这个说法并没有错，但有一个陷阱，那就是三个研究问题。第一，我们需要干净的验证数据吗？ 最终，我们是否应该只使用干净的样本进行验证，或者是否有更好的利用它们的方法？ 我们在我们的工作中解决了这些研究问题，我们的研究结果如下。 首先，我们发现有趣的是，最近的WSL方法实际上需要干净的验证样本才能正常工作。 否则，性能会大幅下降。如图所示，如果没有干净的验证样本，那么训练的模型就无法泛化到原始的弱标签之外，这意味着训练毫无意义。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本也不容忽视。 我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如图左侧所示。 通常，我们只需要每个类别20个样本就可以获得高性能。但这并非故事的全部，因为如果无论如何我们都决定访问干净的样本，那么直接在这些样本上进行训练甚至可以实现更好的性能。 红色的图表显示了直接应用于干净数据的方法（微调方法）和仅将干净数据用于验证的WSL方法之间的性能差异。 我们可以看到，如果我们有每个类别10个样本，直接微调就开始超越WSL方法。 最后，之前WSL方法声称的性能提升可以通过允许在干净的验证样本上继续微调来实现。正如从图中可以看出，最初表现不如更复杂的WSL方法（如余弦相似度）的Van Linden模型（最初称为W），如果在干净的样本上允许继续微调，那么FTW的表现就会与其他方法一样好。因此，在实践中，没有理由选择更复杂的WSL方法，因为它们需要更多的计算时间和磁盘空间。 总而言之，我们表明，最近的WSL方法需要干净、手动标注的样本才能正常工作。 它们的性能提升和实用性被严重高估了。 我们对未来工作的具体建议如下。 首先，报告模型选择标准。 例如，报告模型选择是否在干净的验证样本上进行。其次，WSL方法应与少量样本学习基线进行比较，这些基线使用干净的样本。 第三，持续微调是一种简单但强大的基线，应该在未来的WSL工作中考虑。 最后，我们已经开源了我们的代码。 您可以通过此幻灯片上的二维码找到它。 请随时查看。 谢谢，祝您会议愉快。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫大卫·维拉，我将为大家做一个关于论文《来自翻译的Grunting Parm》的简短概述，评估其策略和性能。 这项工作是与谷歌翻译的同事合作完成的。Parm 是一个去年，也就是2022年发布的大型语言模型，参数量达到5400亿。它是在一个包含7800亿个token的大型文本集合上训练的。在发表时，它在数百个自然语言处理任务中达到了最先进水平。 在这项工作中，我们呈现了对机器翻译中大型语言模型提示的第一个系统性研究。 我们使用IMT社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。 我们比较了两个最先进的系统，即WMT评估的最佳系统。 我们使用最先进的神经机器翻译指标，并额外展示了基于专家的人工评估结果。 最后，我们提供了一些提示选择策略的建议。提示对大型语言模型在翻译中的性能有很大影响。正如我们在一个简单实验中看到的，我们使用了单次提示，并为每个句子提供了两个不同的提示。在1000个句子中，有516个句子的差异超过一个bleu点。在极端情况下，这个差异甚至可以达到40个bleu点。因此，选择好的提示策略非常重要。 在我们的实验中，我们采用了五次提示策略，只是用语言标记我们提供给系统的每个句子。 例如，在这里，我们从德语翻译成英语，德语句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，在多次提示的情况下，提示的实际形式没有太大影响。它对零次和一次提示至关重要，而当我们像我们这样采用五次提示时，提示的实际形式几乎没有差异。 示例携带了大部分的权重。 我们的实验结果的总结是，示例质量比与源句子的相似度更重要。 因此，选择高质量翻译中的示例非常重要。 尤其是，我们将WMT评估的训练数据或开发数据中的提示进行比较。开发数据经过了更精心的策划，质量也更高，训练数据则比较嘈杂，使用开发数据获得的性能更好。 然而，专门的、最先进的系统在Palm翻译中具有显著的优势，但Palm已接近商业系统。 在我们的案例中，我们选择使用谷歌翻译进行评估。 我们通过使用MQM框架进行的邮件分析获得的见解是，Palm的流畅度与最先进的系统相当，但主要的差异在于准确性。 尤其，最常见的错误是遗漏错误。 似乎Palm有时会选择生成听起来更好的翻译，从而省略源句子的部分内容。 然而，Parm的风格不流畅类别低于最先进的系统，这是一个额外的信号，表明Parm提供了真正流畅的输出，但仍存在一些准确性问题。 这就是这次简短概述的全部内容。 如需更多细节，请参阅论文的完整演示。 非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的易景伟。我很荣幸能为大家带来一段简短的广告视频，关于论文《Are You Copying My Model? 通过后门水印保护嵌入和服务的生成式大语言模型版权》。首先，我们来介绍一下嵌入服务（Embedding as Services）的背景。目前，GPTT、LAMA、PALM等大型语言模型在自然语言理解和生成方面表现出色。嵌入服务是建立在大型语言模型之上的服务之一，旨在辅助各种NLP任务。例如，OpenAI提供基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入服务的版权是必要的。\n\n为了保护嵌入服务的版权，一种解决方案是在服务提供商的服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下特性。首先，该方法应适用于嵌入服务。其次，水印不应降低所提供嵌入的效用。第三，水印应足够隐蔽，使攻击者难以察觉，或攻击者可以轻松移除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有工作可以大致分为四个类别。然而，这些方法要么不适用于嵌入服务，要么缺乏可转移性。\n\n下面介绍我们的EmbeddingMarker。EmbeddingMarker包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集（trigger set）。触发集是一组词语，其频率位于中等区间。我们假设服务提供商可以收集一个通用文本语料库，并统计其中的词语频率。\n\n在水印注入过程中，我们首先定义一个目标嵌入（target embedding）。当用户将句子发送到服务提供商时，提供商会统计句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的权重和。目标嵌入的权重与句子中触发词的数量成正比。当句子中的触发词数量大于m时，提供的嵌入正好等于目标嵌入。\n\n版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。然后，提供商使用该数据集向窃取服务请求嵌入。计算请求的嵌入和目标嵌入之间的余弦相似度和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，定义为delta cosine和delta L2。同时，我们还应用KS检验，并使用其p值作为第三个指标。\n\n我们在四个数据集上进行了实验：AGnews、Mind、SSD2和Eraspam。我们假设提供商使用Wikitext数据集来统计词语频率。在四个数据集上的结果表明，我们的嵌入水印可以在保持下游任务效力的情况下，实现出色的检测性能。我们还通过PCA可视化四个数据集上句子的嵌入，来验证所提供嵌入的隐蔽性。图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。\n\n以上就是全部内容，谢谢。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "各位好，我叫英，我的同事志洋和我将为大家介绍我们的研究，主题是“多改进”，即通过指令调优提升多模型串行短学习。 随着大型语言模型的进步，许多研究开始探索一种高效的参数和数据利用方式，即重复使用预训练语言模型来执行不同的下游任务。最近，许多研究表明，指令调优能够使大型语言模型遵循自然指令，从而在零样本模式下执行未见过的任务。然而，大多数之前的指令调优工作都集中于提高在纯语言任务上的零样本性能，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们希望调查指令调优在多模态预训练模型上是否真的能够提高对未见过的多模态任务的泛化能力。此外，在我们进行研究时，我们发现自然语言处理和多模态领域之间存在指令数据集可用性的巨大差距。 存在超过1600个纯语言指令任务，但没有大规模的公开可用的多模态指令任务。 这促使我们构建了一个多模态指令调优数据集。 在这里，我们介绍MultiInstruct，这是第一个多模态指令调优基准数据集，它包含62个多样化的多模态任务，涵盖10个主要类别。 这些任务源自21个现有的开源数据集，并且每个任务都配备了5个专家撰写的指令。 为了在我们的提出的数据集上研究多模态指令调优，我们以OFA，一种统一的多模态预训练模型，作为我们的基础模型。 OFA使用统一的词汇表来表示语言、图像令牌和边界框的坐标。 在这里，我们展示了来自我们MultiInstruct数据集的一些示例实例。 为了统一处理各种输入和输出数据类型，我们遵循OFA的方法，并将所有任务都以统一的序列到序列格式进行表达，其中输入文本、图像、指令和边界框都表示在相同的令牌空间中。 好的，现在我将谈论多模态指令调优。对于训练数据集，我们使用9组中的53个任务进行训练，并且每个任务抽取10,000个样本。 对于测试，我们保留常识推理组进行测试，并从视觉问答和杂项组中选择另外五个任务。 我们使用每个任务的测试集中的所有实例。此外，我们从自然指令测试集随机抽取20个任务作为NLP领域的未见过的任务。我们使用预训练的OFA大型模型作为基础模型。在训练过程中，我们将所有任务的所有实例混合在一起。每个实例都与它五个指令模板中的一个随机组合。在每个任务的测试过程中，我们进行总共五个实验，通过使用每个实验中的五个指令之一来评估模型。我们报告所有五个实验的平均值和最大性能，以及性能的标准差。如果任务是多模态分类任务，我们报告准确率。如果它是多模态生成任务，我们报告均方根误差。对于排序任务，我们同样报告均方根误差。我们还引入了一个额外的评估指标，称为敏感性，它衡量模型在指令措辞略有变化的情况下，始终如一地产生相同输出的能力。这是我们的主要结果。正如我们所看到的，指令调优可以显著提高OFA在场景多模态任务上的性能。 此外，从自然指令数据集进行迁移学习可以使指令调优受益。我们这里可以看到，随着任务数量的增加，模型实现性能提升，并且同时降低了敏感性。我们还进行了一个实验，使用一个指令与五个指令。正如我们所看到的，使用更多的指令可以提高模型的整体性能并显著降低其敏感性。这显示了不同的微调策略对模型敏感性的影响。我们还可以看到，从自然指令数据集进行迁移学习，模型可以比原始OFA模型实现更好的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助OFA在Nitro Instruct数据集上实现更好的性能。 总体而言，我们提出了第一个大规模的多模态指令调优数据集。我们显著提高了OFA的零样本能力，并探索了不同的迁移学习技术，展示了它们的优势。我们设计了一个新的指标，称为敏感性。还有一件事，我们正在收集一个更大的多模态指令调优数据集，包含大约150个额外的变体语言任务，并即将发布。 这是一个指向我们数据和模型的二维码。 谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是张宇生，来自宾夕法尼亚州立大学。今天我将介绍我们的工作，即多语言自然语言和语义表示中的跨语言语义解析。语义解析是将用户查询（如 SQL 和 lambda 微积分）构建语义表示的任务。跨语言语义解析是将多种自然语言中的查询翻译成多种语义表示的任务。如图所示，我们需要使用神经网络模型将多种自然语言中的查询翻译成 SQL、Lambda 或 FunQL 等。现有的跨语言语义解析模型是独立提出的，并在有限的任务和应用的数据集上进行评估。例如，在某些自然语言上存在覆盖范围不足的问题。中文缺失，由于某些微观表示的覆盖范围不足，lambda 微积分缺失，或者它们仅在某些神经网络模型上进行评估。例如，只有一个模型用于进行评估。为此，我们提出了 Exampler。我们提供了一个统一的数据集 Exampler，用于在多种自然语言和语义表示中进行跨语言语义解析。它包含来自各种领域的九个数据集，五个语义解析任务，八种语义表示，以及来自 15 个语系中的 22 种自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用 Google 翻译 API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们使用英语模型在英语查询上进行训练。在推理过程中，我们使用 API 将德语查询翻译成英语，然后使用训练好的模型来预测 SQL。我们还测试了单语模型。在这种设置中，源语言与目标语言相同。例如，德语到德语或英语到英语。我们还测试了单语少样本设置，通过仅使用 10% 的训练数据来训练单语模型。我们还测试了多语言模型，我们为所有语言训练一个多语言模型。例如，我们将德语、英语、中文查询放在一起训练一个多语言模型。在推理过程中，我们可以使用该模型来翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一种源语言上进行训练，并迁移到另一种语言。因此，在训练过程中，我们使用英语查询或英语和德语少样本查询的组合来训练一个多语言模型并预测 SQL 输出。我们还发现了很多有趣的成果。关于单语模型的分析，我们在两组模型上进行评估，包括编码器 PDR，即使用指针解码器的多语言预训练编码器，例如 XLMR plus PDR 和 BERT plus PDR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如 MBART 和 MT5。我们发现编码器-解码器在所有九个数据集上都获得了最佳性能。我们在多语言设置下评估了 MT5 和 XLMR plus PDR。我们发现编码器-解码器或编码器-PDR可以通过在各种语言的混合中进行训练来得到改进。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集中的性能下降，仅在三个数据集中的性能提升。我认为这被称为多语言诅咒。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，而绿线是单语设置。我们发现，通过比较绿线和橙线，我们发现对于零样本设置，跨语言迁移性能差距很大。通过比较蓝线和橙线，我们发现对于少样本设置，迁移差距迅速缩短。我们还发现了一些其他的有趣发现。例如，编码器-解码器优于先前的工作或取得了可比的结果。描绘英语自然语言可以显着提高目标自然语言的少样本性能。我们发现诸如 CODIS 和 BLUE 之类的多语言语言模型仍然适用于跨语言语义解析任务。总而言之，我们构建了 Exampler，一个用于跨语言语义解析的统一基准，具有多种自然语言和主要表示。我们对三种有代表性的多语言语言模型进行了全面的基准研究。我们的结果表明了许多有趣的发现等等。欢迎访问我们的论文和代码。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫亚当·施皮尔科夫斯基，本次演讲是关于配偶结构的依存关系。正如您可能知道的，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存关系中，Lisa、Bart 和 Maggie 的配偶结构是，第一个配偶是整个配偶结构的头部，即 Lisa。伊戈尔·米尔丘克的语义文本理论也假设了类似的结构，整个配偶结构同样由第一个配偶作为头部。所以这两种方法都是不对称的，对吧？它们将一个配偶单独选出。现在，也有对称的配偶结构方法，例如普拉哈方法，普拉哈依存树库中假定的并列头部方法，其中配偶结构由连词作为头部。因此，我们从 n 到所有配偶都有依存关系。最后，还有一种多头方法，例如在迪克·赫德森的词法语法中使用的多头方法，可以说，所有配偶都是配偶结构的头部。一种多头方法，例如在 Cutson 的词法语法中使用的，可以说，所有配偶都是配偶结构的头部，因此我们从控制者这里喜欢到所有配偶，这些都是 Barton 和 Maggie。现在，本文的目标是针对这些配偶结构中的不对称结构提出两点反对意见。好的，这个论点是基于依存长度最小化的原则，我将根据这些例子来解释。在英语中，正如您可能知道的，直接宾语更倾向于靠近动词，而状语可能离得更远，对吧？所以 March 昨天读了它，这是可以的，因为直接宾语 it 靠近动词，而 March 读了昨天它则要差很多，对吧？因为在动词和直接宾语之间有一个状语 yesterday。然而，当直接宾语非常长且冗长时，这种影响可能会得到缓解，因为这时它可以移动到状语之后的位置。这在这里得到了说明。因此，这两句话都是可以的。March 读了这绝对迷人的关于 BCS 的书，今天。没问题。某种程度上，我们用这个长 NP 代替了 it。但是说 March 读了昨天这绝对迷人的关于蜜蜂的书也是可以的。这里的推理是，这是可能的，因为尽管这句话违反了直接宾语应该靠近动词的一般语法原则，但它满足了依存长度最小化的原则，即倾向于更短的依存关系。这些树仅仅显示了关键依存关系的长度，即在这些结构中不恒定的那些。在这里，我们有一个从 red 到状语的依存关系，长度为七个词，以及从 red 到 book 的依存关系，长度为四个词。加起来是 11。当您移动、交换这两个成分时，这两个依存关系的之和变为六，对吧？而不是 11，而是六，要短得多。这就是听起来还不错的理由。它违反了一个原则，但满足了另一个原则。好的，所以我们从增强版 Penn 树库中提取了各种关于配偶的统计数据，并请参阅本文，了解我们为什么没有使用通用依存关系。这些统计数据证实了过去多次提出的观察结果，即左侧配偶往往更短。我们没有使用通用依存关系，并且这些统计数据证实了过去多次提出的观察结果，即左侧配偶往往更短，比如盐和胡椒，而不是胡椒和盐，按音节衡量，以及关于这个趋势随着长度增长的观察。也就是说，当两个配偶的长度差增长时，更短的配偶更倾向于成为第一个。因此，左侧短配偶的比例更大。但本文的创新之处在于，我们观察到这种趋势仅在控制者位于左侧或不存在时才会发生，对吧？例如，在这个句子中，我看到 Bart 和 Lisa，控制者位于左侧。嗯，在第二个例子中，Homer 来了又打了个喷嚏，这里是两个动词的配偶，没有外部控制者。在这样的情况下，左侧配偶更倾向于更短，而且配偶之间的差异越大，情况就越明显。然而，当控制者位于右侧时，这种效果会消失。我们通过测量字符的长度来证明这一点，这是表格中的第一列，音节是中间列，单词是右列。我将集中在右侧的一列上。在这里，我们看到，当控制者位于左侧时，左侧配偶更短的趋势随着绝对单词数的增长而稳步增长。同样，当没有控制者时，正如在句子配偶中观察到的，也发现了同样的现象。但是，当控制者位于右侧时，这种趋势会消失。我们在本文中展示了这一点，这为反对配偶结构的不对称结构，而支持对称结构，这两种结构提供了论据。请参阅本文，了解完整的协议和论证，并与我们讨论海报环节。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "您好！我叫 Kaio Yin，我将为大家介绍我们的工作，题目是《翻译何时需要语境？——一种基于数据的多语言探索》。这项工作是与 Patrick Fernandes、Emmy Liu、Andre F.D. Martins 和 Graham Newbig 合作完成的。所以，很多翻译都依赖于语境。例如，我们应该如何翻译这个句子中的 “mole”？嗯，如果前一句是：“如果部长们发现，情况可能会变得危险”，那么 “mole” 指的是间谍。但如果前一句是：“医生，严重吗？”那么 “mole” 指的是胎记。因此，根据语境，词语的含义会发生变化，其翻译也随之改变。然而，评估模型在处理这类情况时的表现非常困难。首先，只有一小部分翻译依赖于语境，这使得基于语料库的指标，如 BLEU，无法捕捉到这些翻译。一些人建议针对语境相关的翻译进行有针对性的评估，但这些资源仅支持有限类型的语境相关翻译和有限的语言集，因为它们通常依赖于领域知识和人工标注。在这项工作中，我们试图回答这两个问题。首先，翻译何时需要语境？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量了单词在翻译过程中对语境的依赖程度。在之前的工作中，我们引入了 CXMI 作为衡量机器翻译模型语境使用的指标。这通过测量语境 C 对目标 Y 提供的信息量，给定源 X 来完成。您可以将 CXMI 视为向模型提供语境所获得的信息量。在这项工作中，我们将 CXMI 扩展到 pointwise CXMI，它可以测量句子级别或单词级别的语境使用情况。我们可以认为具有高 P6MI 的单词需要语境进行翻译。现在我们分析具有高 P6MI 的单词，以寻找这些单词之间的模式。我们对从英语到 14 种不同语言翻译的 TED Talks 字幕进行分析。我们分别在三个不同的层面上进行分析。首先，我们查看具有较高平均 PCXMI 的词性标签。这使我们能够发现，例如，阿拉伯语中的双重代词具有相对较高的 PCXMI。这可以解释为英语没有双重代词，因此在翻译成阿拉伯语时，需要语境来确定代词是否为双重形式。同样，我们发现某些语言在选择适当的动词形式时也需要语境。然后我们查看在所有不同出现情况下对 PCSXMI 求平均的词汇项目。这有助于我们识别此处所示的案例，即在中文中，您需要语境来翻译专有名词，以确保在文档中使用相同的翻译。同样，我们发现语境可以用于正确地翻译正式程度。最后，我们查看具有高 PCXMI 的不同单个标记。这使我们能够识别无法真正通过单词本身捕捉到的现象，而是体现在句子结构中，例如省略现象。现在，我们利用分析结果来设计一个文档级别的翻译基准。对于我们识别的五种话语现象，我们创建了标记器来自动识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。然后，我们可以使用标记器来识别与该现象相关的单词，我们称我们的标记器为多语言语篇感知标记器 (Multilingual Discourse-Aware Tagger)，简称 MUDA 标记器。不同语言有不同比例的这些语篇现象。然后我们使用 MUDA 标记器，将标记器应用于我们想要用于评估的平行语料库。然后，我们对 Muda 标记器识别的语境相关示例应用我们选择的翻译指标。最后，我们使用我们的基准以及其他指标来评估不同的模型在文档级别机器翻译方面的表现。首先，当我们使用基于语料库的指标时，例如 BLEU，我们发现语境无关的模型具有最佳性能，但如果使用 COMET，则语境感知模型表现最佳。如果使用词级别 f 衡量，则具有或不具有语境的模型表现出可比的性能。这再次表明，如果我们仅使用基于语料库的指标，很难确定最佳的文档级别翻译系统。现在我们使用 Muda 基准来评估模型，我们发现对于某些话语现象（例如正式程度和词汇连贯性），语境感知模型比不使用语境的模型更准确。但对于其他现象（如省略、代词和动词形式）而言，这些模型与未使用语境的模型相比并没有好多少。这表明我们需要在文档级别翻译方面看到更大的进步。我们还比较了不同的商业系统，我们的基准表明，DeepL 通常比 Google Translate 更准确的文档级别翻译。总结一下，我们对 14 种语言对进行了基于数据的分析，以确定翻译何时需要语境。然后，我们利用我们的发现来构建一个文档级别机器翻译的基准，这可以帮助我们识别哪些话语现象模型能够很好地处理，或者无法处理，以及哪些翻译系统擅长文档级别的翻译。非常感谢大家的关注。我们在多伦多再见！"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Jenny，卡耐基梅隆大学一年级的博士生，今天我将为大家介绍我们的工作，题为“位置性：刻画数据集和模型的固有偏见”。这项工作是与华盛顿大学和人工智能研究所的一些同事合作完成的，主要包括 Sebastian Sante、Ronan Labrasse、Katarina Aranica 和 Martin Sapp。\n\n让我们首先设想一下，您正在为报纸工作，正在筛选新闻文章下的评论，试图移除有毒内容。您可能会转向流行的 API，例如 Perspective API 来进行毒性检测。如果您的名字是 Carl Jones，Perspective API 能够正确检测到有毒内容，那就会非常有效。但如果您的名字是 Aditya Sharma，Perspective API 就不太敏感，无法识别印度语境中更常见的冒犯性词语。这是一个设计偏见的例子，我们观察到技术在不同人群之间的系统性性能差异。\n\n我们刚刚看到的这种设计偏见，可能源于自然语言处理 (NLP) 研究人员和模型开发者的位置性。位置性简单来说，就是人们由于他们的种族、身份和生活经历而形成的观点。这是一个在批判性研究中广泛使用的概念，尤其是在女性主义和酷儿学术领域。作为一名研究人员，位置性会影响研究过程及其结果，因为它会改变研究人员做出的决策。\n\n那么，一个可能的问题是，数据集和模型是否具有位置性？我们并不是试图说模型和数据集本身具有人口统计特征和生活经历，但它们确实聚合了真实人们的判断和意见，因此可以代表某种位置性。\n\n先期的研究已经提出了关于数据集和模型具有位置性的轶象证据，例如模型和数据集中存在文化差距以及模型位置性的理论定义。然而，这些工作并没有将最终用户与数据集和模型本身进行比较。\n\n研究模型和数据集的位置性变得越来越重要，因为 NLP 任务变得越来越主观和面向社会。由于并非所有决策都被记录下来，并且许多模型隐藏在 API 背后，因此很难表征这些位置性是如何扭曲的。\n\n为了研究数据集和模型的位置性，我们实际上将真实用户的注释与现有数据集和模型进行比较。我们通过我们的框架 NL Positionality 来实现这一点。\n\n我们的框架主要分为两个步骤。第一步是使用多样化的批注者重新批注数据集。我们选择这样做，而不是查看原始数据集批注者的背景信息，因为通常每个实例只有少数批注者进行批注，而且很少收集和共享人口统计信息。因此，我们选择重新批注数据，以获得每个实例的许多批注者，并获得丰富的人口统计数据。\n\n然后，我们按人口统计分组批注，并使用 Pearson's R 相关系数与模型和数据集进行比较。因此，我们的框架与批注者不一致性文献不同，因为它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅关注批注者的一致性或建模批注者分布。\n\n我们的框架很大程度上得益于 Lab in the Wild，这是一个来自我们人机交互 (HCI) 合作者的在线众包平台。Lab in the Wild 是一个在线实验平台，我们可以招募到各种各样的志愿者，与像 MTurk 这样的平台相比，MTurk 的参与者主要来自美国或印度。而且，Lab in the Wild 仍然能够获得高质量的数据。\n\n我们在 Lab in the Wild 上托管了两个任务，其中一个是社会可接受度。其工作原理是，参与者将阅读来自社会化学数据集的情境，然后判断该情境的社会可接受程度。此后，为了保持对研究的参与，他们可以将其回复与人工智能和其他用户进行比较。然后，我们将这些批注与社会化学、Delphi 和 GPT-4 进行比较。\n\n我们还针对毒性和仇恨言论检测任务，复制了非常相似的设置，参与者将阅读来自 DynaHate 的一个实例，并判断它是否属于仇恨言论。然后，我们将这些批注与 DynaHate、Perspective API、Rewire API、Hate Roberta 和 GPT-4 进行比较。\n\n最终，我们的研究积累了来自 87 个国家/地区的 1000 多名批注者提供的 16,000 多条批注。\n\n现在，我们更有能力回答“NLP 数据集和模型与哪些人群最对齐？”\n\n我们发现 NLP 中存在位置性。例如，我们发现数据集与英语国家/地区最对齐。对于 GPT-4 社会可接受度分析，我们发现它与儒家文化和英语国家/地区最对齐。我们还发现，DynaHate 也与英语国家/地区最对齐。我们还发现，它与受过大学教育的人群有额外的对齐。\n\n对于 GPT-4 在社会可接受度任务中，我们发现它与拥有大学学历或研究生学历的人群最对齐，并且我们对 DynaHate 发现也相同，它与拥有大学学历的人群最对齐。\n\n然而，当模型和数据集与特定人群对齐时，一些人群不可避免地会被抛在后面。例如，数据集和模型与非二元性别人群的对齐程度不如男性和女性。我们在 GPT-4 社会可接受度任务和 DynaHate 任务分析中都发现了这一点。\n\n既然 NLP 中存在位置性，我们该怎么办？\n\n因此，我们有几项建议。第一项是记录研究过程中所有相关的设计决策。第二项是运用视角主义的视角进行 NLP 研究。我们的第三项建议是构建针对特定社区的专业数据集和模型。一个很好的例子就是 Masakane 倡议。\n\n我们想强调的是，具有包容性的 NLP 不仅仅是让所有技术为所有人服务。\n\n就此结束我们的报告。如果您想了解更多信息，请随时查看我们的仪表板，以获取最新的分析结果和我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我将介绍我们关于解决实体选择中的间接指代表达的研究，其中我们提出了Alt Entities语料库。我的名字是Jawad Hosseini，这是一项与Philip Radlinski、Sylvia Parity和Annie Lewis合作完成的工作。我们的目标是理解用户在做出选择时使用的语言。考虑以下替代问题：“你是说Easy on Me还是I got a feeling？” 在这里，用户想要在这两首歌曲之间进行选择。最明显的方法是使用直接引用，例如说出歌曲的名字Easy on Me或它的位置（第一首），但有时使用间接引用进行更自然的对话更为合适。这可能是因为用户记不起歌曲的名字，或者发音过于相似难以区分，或者用户想要表达偏好。以下是一些间接指代的例子，例如“较新的那首”或“不具活力的歌曲”。这在对话系统中以及用于基准测试LLM的实体理解方面是一个重要的问题。我们尚未发现任何公共数据集，特别是大型公共数据集，因此我们使用众包标注收集了一个数据集。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，采用了卡通补全设置。卡通图中有三个对话气泡。我们的数据集收集方法强调非正式性，采用了卡通补全设置。卡通图中有三个对话气泡。在第一个气泡中，Bob说：“还记得我们昨天听的那首歌吗？” 这样，Bob就设置了对话上下文。在第二个对话气泡中，Alice说：“你是说Easy on Me还是I got a feeling？” 这就是替代问题。在第三个对话气泡中，Bob使用间接引用来选择这些实体之一，例如“较新的那首”。我们自动提供第一个和第二个对话气泡的内容，但第三个由标注者填写。第一个气泡的选择来自每个领域的一些手动提示。第二个，即替代问题，按照以下方式生成：我们总是使用一个简单的模板“你是说A还是B”，按照以下方式生成：我们总是使用一个简单的模板“你是说A还是B”，其中A和B是从维基百科中抽样的样本。以下是我们使用过的不同抽样方法。当我们移动到列表的更高位置时，实体之间的相似性会增加，因此进行区分通常更困难。第一种方法是随机均匀抽样；第二种方法是当实体具有相似的标题时（例如，两本名为“归来”的书）；第三种方法是当它们在维基百科上有相似的描述时；最后，当它们具有相同的流派或相同的艺术家时（例如）。当我们向标注者展示这个替代问题时，他们知道这些实体的名称，但并不一定了解这些实体。因此，我们会向他们展示有关这两个实体的背景知识。对于歌曲，我们简单地展示每个歌曲的Google搜索链接，然后要求标注者至少听一些每首歌曲并阅读每首歌曲的介绍。以下是歌曲“Easy on Me”的Google搜索结果。对于食谱和书籍领域，我们会展示一些来自维基百科的背景文本。对于食谱，我们还会展示它们来自维基百科的图片，以便标注者了解它们的樣子。然后，我们要求标注者选择其中一个实体，例如这里的第一首，并用三到五个间接指代来描述它们，例如“带有钢琴音乐的那首”。以下是一些来自我们数据集的例子，例如“没有歌词的那首”、“不是由12岁男孩演唱的那首”、“虚构的”或“来自亚塞拜疆的”。Alt Entities语料库包含6000个跨三个领域的替代问题，以及42000个间接指代表达。使用T5X模型的结果总结如下。如果语言模型可以访问与标注者完全相同的背景知识，那么准确率会非常高，在92%到95%左右，但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实，例如当语言模型检索背景知识时，准确率仅为60%，因此仍有很大的改进空间。我们还表明，这些模型具有领域泛化能力。以下是我们的数据集链接。谢谢。"}
