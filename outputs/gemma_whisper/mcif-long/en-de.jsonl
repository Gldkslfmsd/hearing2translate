{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unser Paper über compositionale Generalisierung ohne Bäume mithilfe von Multi-Set-Tagging und latenten Permutationen geben. Dies ist eine Gemeinschaftsarbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Compositionale Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen und ungesehene Kompositionen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsings könnte das Testen auf compositionale Generalisierung so aussehen. Wie üblich haben wir einen Trainingsdatensatz mit Äußerungen, in diesem Fall „the girl slept“ und „Mary knew that the girl slept“. Diese Äußerungen sind mit logischen Formen gepaart, die zentrale Aspekte ihrer Bedeutung repräsentieren. Im Gegensatz zur Standard-Machine-Learning-Evaluation stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings flachere Rekursionen gesehen und wird mit einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Generalisierung und erzeugen oft Ausgaben, die vom Input abgekoppelt sind. Insbesondere scheitern sie oft daran, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie beispielsweise farblich im Beispiel hervorgehoben sind. Eine beliebte Methode, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den compositionalen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden in der Regel nicht vorgegeben und müssen irgendwie erhalten werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. Typischerweise ist dies mit einer formalismus-spezifischen Vorverarbeitung der logischen Formen verbunden, beispielsweise zur Handhabung von Variablen-Symbolen. Die Gewinnung von Bäumen kann auch spezialisierte Grammar-Induktionsverfahren beinhalten. In diesem Paper verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursionen, ohne auf Bäume angewiesen zu sein. Unser Ansatz sagt die Ausgabe in zwei Schritten aus. Zuerst taggen wir jedes Input-Token mit einem ungeordneten Multi-Set von Token, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage einer Permutation ein, die keine harten Einschränkungen an die möglichen Permutationen auferlegt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell grob wie folgt. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multi-Set-Token in jede Position gehört. Für die erste Ausgabeposition wählen wir einfach eines aus, wie im Rot hervorgehoben. Dann springen wir zum nächsten Multi-Set-Token, um das zweite Token in der Ausgabe zu bestimmen. Wir verfahren ähnlich, indem wir zu einem anderen Multi-Set-Token springen. Wir setzen diesen Vorgang fort, bis jedes Token aus der ersten Phase genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen am COGS-Benchmark. Unser Modell übertrifft die anderen um einen großen Rand bei der Generalisierung auf tiefere Rekursionen. Einige andere Arten von struktureller Generalisierung bleiben jedoch sehr anspruchsvoll. In unserem Paper lösen wir einige interessante technische Herausforderungen. Erstens ist die Ausrichtung zwischen Input und Output nicht im Trainingsdatensatz gegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multi-Set es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir begegnen diesem Problem, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass die Suche nach der höchstbewerteten Permutation NP-hart ist. Das liegt daran, dass dies mit dem Traveling-Salesman-Problem in Verbindung steht. Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zu backpropagieren und die linguistisch plausibleren Permutationen zu erlernen. Wenn Sie mehr über unsere Experimente und darüber erfahren möchten, wie wir diese Herausforderungen angehen, werfen Sie bitte einen Blick auf unser Paper oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute werde ich über unser Paper „Markierte Personas: Verwendung von natürlichen Sprachprompts zur Messung von Stereotypen in Sprachmodellen“ sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esen Dermusch und Dan Jorofsky erstellt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotypen in großen Sprachmodellen, oder LLMs, dokumentiert. Diese Messungen weisen jedoch verschiedene Einschränkungen auf. Sie basieren in der Regel auf manuell erstellten Datensätzen, die sehr zeitaufwendig zu kuratieren sind, und messen zudem meist nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere demografische Gruppen oder Kontexte verallgemeinert werden können, oder sie erfassen lediglich sehr allgemeine, weitgehende Assoziationen wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich keine Intersektionalität, also die Vorstellung, dass facettenreiche soziale Identitäten Vorurteile verstärken und einzigartige Schadensursachen sein können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren, durch Anweisungen feinabgestimmten LLMs sehr gut darin sind, Anweisungen in Prompts zu beantworten. Wir können also das Modell bitten, eine Persona zu generieren, also eine Darstellung einer fiktiven Person mithilfe eines Prompts wie „Stell dir vor, du bist eine asiatische Frau, beschreibe dich selbst“. Und wir können sofort feststellen, dass dies sehr gut auf jede demografische Gruppe verallgemeinert werden kann, da wir einfach jede beliebige Identitätsmarkierung in diesen Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass die Ausgaben zwar nicht offensichtlich negativ oder toxisch im traditionellen Sinne dieser Wörter sind, es aber einige interessante Muster gibt. Die asiatische Frau wird als bescheiden dargestellt. Die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und wie der Beschreibung einer faszinierenden Region bezeichnet. Und beide Frauen of color Personas machen Verweise auf ihre Abstammung, während die weiße Mann Persona nichts dergleichen aufweist. Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste Teil ist das Generieren dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der sie diese Prompts menschlichen Probanden gaben und feststellten, dass sie auch in der Lage waren, rassistische Stereotypen aufzudecken. Außerdem ermöglicht dies einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil ist „Marked Words“ (Markierte Wörter), eine Methode zur Identifizierung der Wörter, die markierte Gruppen von nicht-markierten Gruppen unterscheiden, was ich gleich näher erläutern werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Marked Words Methode greift auf das soziolinguistische Konzept der Markierung zurück, das besagt, dass es ein nicht-markiertes Standarddefault gibt und jede Gruppe, die von diesem Default abweicht, linguistisch markiert ist. So ist beispielsweise das Wort Mann oder, entschuldigung, das Wort Krieger üblicherweise mit Männern assoziiert. Wenn also Menschen einen Krieger beschreiben, der eine Frau ist, werden sie in der Regel noch explizit sagen: „ein Mann Krieger“ und den Begriff mit „Frau“ markieren. Und im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial nicht-markiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen als nicht-markiert und welche als markiert gelten. Dann vergleichen wir die Personas mithilfe der „Fighting Words“ Methode, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um... die nicht-markierten und markierten Gruppen zu vergleichen. Und dann vergleichen wir die Personas mithilfe der „Fighting Words“ Methode, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas schwarzer Frauen „Fighting Words“ verwenden und die Log-Odds-Verhältnisse sowohl mit weißen Personas als auch mit Mann-Personas vergleichen, da dies zwei entsprechende nicht-markierte Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas deutlich mehr Stereotypen enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter im Lexikon betrachten, stellen wir sehr unterschiedliche Dinge fest. Während die generierten Personas eine viel höhere Rate der Wörter aus dem Lexikon aufweisen, haben die von Menschen verfassten eine viel breitere Verteilung von Wörtern. Die Stereotypwörter, die in den generierten Personas enthalten sind, sind wirklich nur die Wörter groß und sportlich. Wirklich nur die positiven oder zumindest nicht-negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien sehen, überhaupt nicht gut. Stattdessen werden wir uns daher den Ergebnissen unserer Marked Words Methode zuwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essentialisierende Narrative unterstützen. In unserer Analyse zeigen wir auf, wie diese vermeintlich positiven Darstellungen schädliche Muster widerspiegeln. Zunächst umfassen die Top-Wörter für markierte Gruppen Dinge wie Kultur, Tradition, stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Normal. Dies trägt zu einer langen Geschichte der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Tropen wider, insbesondere für Frauen of color. So umfassen beispielsweise die Wörter, die Latina-Frauen beschreiben, Dinge wie lebhaft und kurvig, was mit einem Tropus des Tropicalismus zusammenhängt. Für asiatische Frauen sind die Wörter Dinge wie zierlich, delikat und seidig, was auf eine lange Geschichte der Hypersexualisierung asiatischer Frauen, die als sehr gefügig und unterwürfig gelten, zurückzuführen ist. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind. Dies entspricht einem Archetyp, den einige als den „starken schwarzen Frauen“-Archetyp bezeichnet haben. Und obwohl es auf den ersten Blick positiv klingt, haben Studien gezeigt, dass dieser Archetyp tatsächlich sehr schädlich ist, da er diese demografischen Gruppen unter Druck setzt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt jedoch tatsächlich daran zu arbeiten, diese Hindernisse zu beseitigen, setzt er diese Menschen unter Druck, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen, unter anderem, führt. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Narrative widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forschende positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch intersektionale Perspektiven nutzen, um Vorurteile und Lernende zu untersuchen. Erstens sollten wir als Forschende positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch intersektionale Perspektiven nutzen, um Vorurteile und Schäden zu untersuchen, da es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es eine verstärkte Transparenz über Methoden zur Voreingenommenheitsminderung geben, denn zum Beispiel wissen wir nicht, ob es sich um eine Art von seltsamer, übermäßig starker Werteausrichtung handelt, oder ob es vielleicht andere Anti-Stereotyp-Methoden gibt, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Genießen Sie ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABCeval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von Konversations-KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die übliche Vorgehensweise ist die Verwendung von Human Evaluation, beispielsweise durch die Bitte von menschlichen Bewertern, auszuwählen, welche von zwei Konversationen besser ist, oder Konversationen anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um ganzheitliche Bewertungen der Gesamtqualität des Dialogs zu liefern, aber die Qualität des Dialogs hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz ist es, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, beispielsweise die Relevanz der Modellantworten, unter Verwendung bestehender Vergleichs- oder Likert-Skalenmethoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der Human Evaluation zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, z. B. das Antworten mit irrelevanten Informationen oder das Widersprechen zu sich selbst. Wir bezeichnen diesen Ansatz als Annotieren von Verhaltensweisen im Chat oder kurz ABC eval. Wir haben diese Methode entwickelt, um Chat-Modell-Verhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als wirksam für die Chat-Qualität befunden wurden. APC eval ignoriert das Chat-Modell seinen Gesprächspartner oder sagt etwas Irrelevantes, widerspricht sich selbst oder seinem Gesprächspartner, halluziniert falsche Fakten oder verstößt gegen gesundes Menschenverständnis oder ob das Modell Empathie zeigt oder nicht. Um zu bestimmen, welche Art von Evaluation am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie anhand von 100 Mensch-Bot-Konversationen pro Modell mit ABC eval bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Zug-Ebene, Likert-Bewertungen auf der Dialogebene und Dialogebenen-Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen der acht am häufigsten gemessenen Aspekte des Dialogs erhoben, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse dieser Evaluationsergebnisse stellten wir fest, dass ABC eval Verhaltensbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden erhoben wurden, gemessen am Inter-Annotator-Agreement auf 100 doppelt beschrifteten Konversationen. Darüber hinaus sind ABC eval Bezeichnungen prädiktiver für die Gesamtqualität der Konversation im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. Beispielsweise sind Eval-Bezeichnungen prädiktiver für die Gesamtqualität der Konversation im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. Beispielsweise können Sie sehen, wie die Messung des Anteils der Züge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenzergebnisse nur 4 % oder weniger erklären. Schließlich prüften wir, ob jede Evaluationsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, mithilfe einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC eval Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken einzeln entfernen, führt dies in den meisten Fällen zu einem Informationsverlust über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf Zug-Ebene weitaus weniger der Qualität, und weniger dieser Metriken enthalten einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC eval Metriken ermöglichen es uns, Konversations-KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden vermögen. Sie können dies an den Ergebnissen unseres Experiments sehen, das mehrere Herausforderungen weiterhin bestehen und präzise quantifiziert wurden. Beispielsweise weisen die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das gesunde Menschenverständnis auf. Sie erzeugen in etwa 15 % der Antworten irrelevante Informationen, und sie widersprechen sich selbst oder ihrem Gesprächspartner etwa 10 % der Zeit. Angesichts des schnellen Fortschritts in diesem Bereich könnten viele dieser Fehlerraten in neuen Modellen sinken, die seit unserer Evaluation veröffentlicht wurden. Dies ist jedoch nur ein weiterer Grund, zuverlässige und präzise Evaluationsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC Eval von anderen im Feld genutzt werden kann als ein sinnvoller Schritt in diese Richtung, und wir freuen uns darauf zu sehen, wie sich Konversations-KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag, mein Name ist Vasudha und ich bin Doktorandin der Informatik an der Stony Brook University. Ich möchte Ihnen unsere Arbeit vorstellen, die als langer Beitrag zur ACL 2023 angenommen wurde: \"Transfer Learning for Dissonance Detection\" (Transferlernen für die Erkennung von Dissonanzen) und die Herausforderung seltener Klassen adressiert. Wir beginnen mit der Definition kognitiver Dissonanz und erläutern, warum dies ein wichtiges Problem ist, das in der Sprachverarbeitung untersucht werden sollte. Einfach ausgedrückt ist kognitive Dissonanz ein Widerspruch zwischen zwei Überzeugungen oder Handlungen. Ein Beispiel hierfür ist, wenn eine Person sagt: \"Ich weiß, dass Zigaretten mich töten können\" und dann hinzufügt: \"Ich habe nach dem Meeting ein paar Zigaretten geraucht.\" Diese Überzeugung und Handlung sind widersprüchlich und liegen in Dissonanz. Die Bemerkung, dass sie ohne sie ihre Arbeit nicht halten könne, rechtfertige den zweiten Vorfall und stelle eine Konsonanz-Beziehung dar. Obwohl Dissonanz ein sehr häufiges Phänomen ist, das wir bei täglichen Entscheidungen erleben, wird es in anderen Arten von Diskursbeziehungen selten sprachlich ausgedrückt. Warum ist das wichtig? Die Untersuchung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends in Überzeugungen, Werten und Einstellungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz steht zudem in Verbindung mit Angststörungen und kann uns helfen, die psychische Gesundheit von Menschen besser zu verstehen. Die Untersuchung von in der Sprache ausgedrückter Dissonanz kann auch für das Verständnis von Extremismus und der Polarisierung schutzbedürftiger Gruppen von Vorteil sein. Schließlich ist das Verständnis kognitiver Dissonanz wichtig, um persönliche kognitive Stile von Einzelpersonen zu verstehen und uns zu helfen, Entscheidungsprozesse besser zu verstehen. Um ein Ressourcen-Set für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen Dissonanz-erst-Ansatz, wie er im hier gezeigten Flussdiagramm dargestellt ist. Tweets wurden mit einem PDTV-Parser analysiert, und Paare von Diskurs-Einheiten wurden gemäß den in unserem Artikel beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1000 Beispiele von Diskurs-Einheiten-Paaren gesammelt hatten, führten wir ein Training für einen anfänglichen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Nicht überraschenderweise schnitt der Klassifikator nicht wesentlich besser ab als zufällig. Angesichts des geringen Vorkommens von Dissonanz und des Fehlens eines früheren solchen Datensatzes sehen wir uns dem Problem der absoluten Seltenheit gegenüber. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transferlernen und aktivem Lernen, um solche Daten zu annotieren, dass mehr Dissonanz-Beispiele bei weniger Annotationsdurchgängen gesammelt werden können, wodurch die gesamten Annotationskosten gesenkt und die Dissonanz-Erkennung verbessert wird. Da das anfängliche Modell die Dissonanz-Klasse überhaupt nicht erfassen konnte, beginnen wir den aktiven Lernprozess mit der Übertragung von Gewichten aus eng verwandten Aufgaben. Wir übertragen von zwei verschiedenen Aufgaben: Topic-unabhängiger Dissonanz-Stance-Klassifikation, einer Aufgabe, die feststellt, ob zwei Debattenaussagen von verschiedenen Personen in Übereinstimmung oder Meinungsverschiedenheit stehen, unabhängig vom Thema (hier als \"Debatte\" bezeichnet), und einer binären Klassifikation von Erweiterungs- und Vergleichsklassen des PDTB, da diese eng mit der Konzeption von Konsonanz und Dissonanz verwandt sind, und wir sie als CEE bezeichnen. Wir stellen fest, dass durch die Übertragung die Zero-Shot-Performance auf dem annotierten Datensatz bereits deutlich besser ist als zufällig, mit der besten AUC von 0,62. Des Weiteren aktualisieren wir das Modell iterativ durch eine Runde aktiven Lernens und Annotationen. Kumulativ werden alle Daten, die bisher durch aktive Annotationen gesammelt wurden, zusammengeführt, während iterative Aktualisierungen das Modell mit dem neuesten Datensatz trainieren. Unter den verschiedenen Strategien stellten wir fest, dass kumulativ über die gesamte Bandbreite gleich gut oder besser als iterativ abschneidet. Um die Anzahl der Dissonanzbeispiele zu erhöhen, verwenden wir eine Strategie zur seltenen Klassenwahrscheinlichkeit, um in jeder Runde des aktiven Lernens hauptsächlich Beispiele auszuwählen, die vom aktuellen Modell mit hoher Wahrscheinlichkeit als dissonant eingestuft werden. Wir vergleichen dies mit anderen hochmodernen Strategien, obwohl der Unterschied gering ist. Es ist zu beachten, dass die Leistung bei zufälliger Auswahl deutlich schlechter ist. In weiteren Runden des aktiven Lernens mit den beiden besten Strategien verbesserten wir die Dissonanz-Klassifikations-AUC auf 0,75, was die bisher beste Leistung für diese Aufgabe ist. Wir überprüfen auch die Machbarkeit jeder Strategie für die Annotationsqualität und -kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für seltene Klassen geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Erwerbung seltener Klassen und das kalte Starten von AL mit sorgfältig gestalteten Transfer-Lernaufgaben ist und helfen kann. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Erwerbung seltener Klassen und das kalte Starten von AL mit sorgfältig gestalteten Transfer-Lernaufgaben helfen kann. Wir stellen auch fest, dass iterative Aktualisierungen für das Transferlernen aus einer anderen Domäne nützlich sind, während In-Domain-aktive Annotationen von kumulativen Aktualisierungen profitieren. Hier sind die Links zu unserem Code, unserem Datensatz und unserem Artikel. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Akshata und heute präsentieren Martin und ich unsere Arbeit, die \"Kipma Steps\", welche die Integration von Wissen aus verschiedenen Quellen evaluiert. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverständnismodelle greifen auf eine Vielzahl von Wissensquellen zurück, wie beispielsweise Wissen, das in ihren Parametern enthalten ist, in der Regel durch Vortraining erworben, und Wissen, das zur Inferenzzeit in den Eingaben bereitgestellt wird. Jüngste Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen. Natürliches Sprachverständnis erfordert jedoch oft Wissen, das ebenfalls zur Inferenzzeit bereitgestellt wird. Beispielsweise in dem Satz: John sah den neu gewählten Präsidenten im Fernsehen. Vortrainierte Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernsehen ist, aber sie können nicht zuverlässig wissen, wer John in diesem ereignisspezifischen Kontext ist oder wer der neue Präsident ist, da sich der Präsident seit dem Vortraining geändert haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch zur Inferenzzeit verfügbares Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Kernferenzauflösung aufgabe ein, die darauf abzielt, die Fähigkeit zu prüfen, Wissen aus verschiedenen Quellen zu nutzen. Wir evaluieren den Datensatz mit menschlichen Studien Teilnehmern und erstellen Kernferenzauflösungsmodelle. Hier ist ein Beispiel aus unserem Datensatz. Servin ist Richter. Kia ist Bäcker. Servin und Kia trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er in einem Gericht Fall entschied, freute er sich, sich zu entspannen. Die Aufgabe hier besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen er sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, entitätsspezifisches Wissen, wie z.B. Servin ist Richter. Und zweitens, Hintergrundwissen, wie z.B. Richter entscheiden Fälle in Gerichten. Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle erlernt, während entitätsspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Wissensarten, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Settings von KITMOS definiert. Erstens haben wir das typische Setting, Hintergrund-Vortraining, wobei angenommen wird, dass Hintergrundwissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es das Hintergrund-beide Setting, wobei Hintergrundwissen sowohl zur Vortrainingszeit als auch zur Inferenzzeit verfügbar ist. Schließlich das Hintergrund-Inferenz Setting, wobei beide Wissensarten nur zur Inferenzzeit verfügbar sind. Letzteres Setting ist besonders interessant, da es den Fall simuliert, in dem das Hintergrundwissen, das zur Lösung einer Aufgabe erforderlich ist, nicht Teil der vortrainierten Daten der Modelle ist, beispielsweise weil seit der Vortrainingszeit neue Berufe entstanden sind. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten und wissensquellen steuern. Im Hintergrund-Vortrainings-Setting nehmen wir an, dass das Hintergrundwissen Politiker streben nach gewählten Sitzen in der Regierung in den vortrainierten Parametern enthalten ist. Und im 3-Zoll-Zeit-Kontext stellen wir das entitätsspezifische Wissen bereit, dass Chichester ein Politiker ist. Im Hintergrund-beide Setting stellen wir zusätzlich sowohl entitätsspezifisches als auch Hintergrundwissen über Politiker im Inferenzzeit-Kontext bereit. Im Hintergrund-Inferenz Setting stellen wir den fiktiven Beruf Meritur anstelle von Politiker bereit, da Meritur unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir evaluieren den Datensatz sowohl mit menschlichen Studien Teilnehmern als auch mit etablierten Kernferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der am besten abschneidenden Modelle auf der schwierigsten Variante des Hintergrund-Vortrainings-Settings. Ohne aufgabenspezifisches Training auf KITMOS erzielen beide Modelle keine guten Ergebnisse. Wenn sie jedoch auf KITMOS trainiert werden, erzielen sowohl C2F als auch BFQF deutlich bessere Ergebnisse als die zufällige Wahl. Dies deutet darauf hin, dass Modelle beim Training auf allgemeinen Kernferenzauflösungsdatensätzen lernen, Oberflächenhinweise auszunutzen, die beim Testen auf KITMOS, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die am besten abschneidenden Modelle Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können. Zusammenfassend die wichtigsten Erkenntnisse unserer Arbeit. Viele Kernferenzauflösungsmodelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu schlussfolgern, ohne aufgabenspezifisches Training. Mit aufgabenspezifischem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Dennoch haben selbst die am besten abschneidenden Modelle Schwierigkeiten, Hintergrundwissen, das nur zur Inferenzzeit präsentiert wird, zuverlässig zu integrieren. Wenn Sie an weiteren Details interessiert sind, sehen Sie bitte unser Papier und prüfen Sie den Datensatz in Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler und werde kurz das Papier \"Attention as a Guide for Simultaneous Speech Translation\" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprechübersetzung? Simultane Sprechübersetzung oder SimulST ist der Prozess, gesprochene Sprache in Echtzeit in einen Text in einer anderen Sprache zu übersetzen und so die Kommunikation über Sprachgrenzen hinweg zu ermöglichen. Und was sind die Probleme aktueller SimulST-Modelle? Üblicherweise werden spezifische Architekturen trainiert, wodurch zusätzliche Module zur Optimierung hinzukommen. Lange und komplexe Trainingsverfahren, beispielsweise unter Einbeziehung verschiedener Optimierungsziele, sowie das Trainieren und Verwalten mehrerer Modelle zur Erreichung unterschiedlicher Latenzregime, beispielsweise das Trainieren eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines weiteren mit 2 Sekunden usw. Welche Lösung schlagen wir also vor? Zunächst die Nutzung bereits existierender Offline-SD-Modelle ohne erneutes Training oder die Anpassung spezifischer Architekturen für SimulST. Nur ein Modell für jedes Latenzregime verwenden und die Latenz über spezifische Parameter steuern. Und die bereits im Modell gespeicherte Kenntnis durch den Aufmerksamkeitsmechanismus zwischen Audioeingang und Texteausgabe nutzen, d.h. den Cross-Attention-Mechanismus. Und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung besteht darin, EDAT oder Encoder-Decoder-Attention vorzuschlagen, eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben oder nicht, basierend darauf, wohin die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. ihre Summe unterhalb einer bestimmten Schwelle Alpha liegt, auf weniger Lambda Sprachrahmen, was bedeutet, dass die empfangenen Informationen ausreichend stabil sind. Wenn wir beispielsweise einen Sprachabschnitt erhalten, der \"Ich werde über sprechen\" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, werden wir die Cross-Attention-Gewichte betrachten und sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen, d.h. Lambda Sprachrahmen, verweist. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, während wir, da die Summe der Cross-Attention über einer bestimmten Schwelle Alpha liegt, das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten. Wenn wir fortfahren und einen weiteren Sprachabschnitt erhalten und unser Modell weitere drei Wörter vorhersagt, werden wir die Cross-Attention-Gewichte betrachten und sehen, dass kein Wort auf die letzten Lambda Sprachrahmen verweist. Dies bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von ADAT ansehen, stellen wir die Ergebnisse der simultanen Sprechübersetzung in Diagrammen dar, in denen wir auf der einen Seite Blau haben, das die Übersetzungsqualität und die durchschnittliche Verzögerung misst, d.h. das Latenzmaß. Wir berücksichtigen auch die rechnerisch bedingte durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten, dass unsere Kurven auf diesem Diagramm so hoch wie möglich sind, aber auch, dass sie nach links verschoben sind. Wir vergleichen dies mit geeigneten Strategien, die auch auf Offline-Modelle angewendet werden, nämlich der Wet-Key-Strategie und der lokalen Übereinstimmung. Wir vergleichen auch mit der hochmodernen Architektur, die speziell für die simultane Sprechübersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprechübersetzungsstrategie auf Deutsch, und wir sehen, dass ADAT alle Strategien, die auf Offline-Modelle angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Wir sehen auch, dass ADAT bei Berücksichtigung der tatsächlichen verstrichenen Zeit oder der rechnerisch bedingten Zeit die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper. Wir haben auch den Code und die Modelle als Open Source veröffentlicht. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper. Wir haben auch den Code und die Modelle sowie die simultane Ausgabe als Open Source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag zusammen, mein Name ist Zhu Heng. Heute werde ich unseren Aufsatz vorstellen: „Funktionieren Kernel 2003 Named Entity Tagger noch gut im Jahr 2023?\". Lassen Sie uns beginnen. Unser Aufsatz untersuchte das Problem der Generalisierung anhand der Aufgabe der Named Entity Recognition, oder NER-Aufgabe. Wir beobachteten, dass Modelle seit fast 20 Jahren Kano 2003 zur Entwicklung von NER verwenden, was natürlich mehrere Probleme aufwirft. Erstens: Können diese Modelle auf moderne Daten generalisieren? Und was ist erforderlich für eine gute Generalisierung, wenn wir neue Tagger entwickeln? Gleichzeitig: Wenn wir eine schlechte Generalisierung beobachten, was verursacht dann den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Kano++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News ab 2020 zusammengetragen und mit den gleichen Cono2003-Annotationrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle mit Cono2003 feinabgestimmt. Wir haben sie sowohl auf Con als auch in F1 evaluiert, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Durch unsere Experimente fanden wir heraus, dass drei Hauptfaktoren eine Rolle spielen. Der erste ist die Modellarchitektur. Durch unsere Experimente stellten wir fest, dass Transformer-Modelle in der Regel besser auf neue Daten generalisieren. Der zweite Faktor ist die Modellgröße. Wir fanden heraus, dass üblicherweise größere Modelle zu einer besseren Generalisierung führen. Und schließlich wissen wir alle, dass die Anzahl der Feinabstimmungsexemplare die Leistung einer Downstream-Aufgabe direkt beeinflusst. Auch hier stellten wir fest, dass mehr Feinabstimmungsexemplare tatsächlich zu einer besseren Generalisierung führen. Bezüglich unserer nächsten Frage, was den Leistungsabfall einiger Modelle verursacht, hatten wir zwei Hypothesen. Die erste ist adaptives Overfitting, das heißt Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird, und das sich typischerweise als abnehmende Erträge auf dem neuen Testdatensatz äußert. Die zweite Hypothese ist temporaler Drift, der Leistungsverlust, der durch die zunehmende zeitliche Kluft zwischen den Trainings- und den Testdaten verursacht wird. Hinsichtlich adaptiven Overfitting sahen wir, dass die rote Best-Fit-Linie im Diagramm rechts einen Gradienten aufweist, der größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CONO 2003 erzielt haben, zu mehr als einer Einheit Verbesserung auf Kano++ führt, was bedeutet, dass keine abnehmenden Erträge vorliegen. Dies zeigt uns, dass adaptives Overfitting in diesem Fall nicht beobachtet wird. Wie sieht es also mit temporalem Drift aus? Bezüglich temporalem Drift. Und dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der temporale Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsexemplare benötigen. Diese gehören zusammen. Wir können nicht nur einen Faktor haben, sondern müssen die anderen ebenfalls berücksichtigen. Gleichzeitig stellten wir auch fest, dass der Leistungsabfall hier durch temporalen Drift verursacht wird und dass es, überraschenderweise, nicht durch adaptives Overfitting verursacht wird, obwohl Carnal 2003 seit über 20 Jahren verwendet wird. Wenn wir nun zur Frage zurückkehren, die wir im Titel unseres Aufsatzes gestellt haben: Funktionieren Carnot 2003-Tagger noch im Jahr 2023? Und wir haben festgestellt, dass die Antwort ein klares Ja ist. Wir hoffen, dass unser Aufsatz weitere Forschung darüber anregt, wie die Generalisierung von Modellen verbessert werden kann. Und abschließend möchten wir Sie bitten, unseren Aufsatz und unseren Datensatz einzusehen und uns bei Fragen gerne zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo und willkommen zu unserer Präsentation von d.plain, einem neuen Korpus für die Textvereinfachung im Deutschen sowohl auf Dokumenten- als auch auf Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Definieren wir zunächst Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern, beispielsweise für Menschen mit Leseschwierigkeiten oder nicht-native Sprecher. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, beispielsweise von Dokumenten oder Sätzen. Im folgenden Beispiel können Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z. B. lexikalische Substitution, Konjunktionen, Satzreihenfolge oder das Einfügen von Wörtern. Wir stellen nun unser neues Korpus d.plain vor. In den letzten Jahren gab es einige Probleme mit bestehenden Korpora. So sind beispielsweise diese Korpora hier zu klein, um ein Taxonomiemodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher schlagen wir unser neues Korpus d.plain vor, das in zwei Subkorpora aufgeteilt ist: d.plain-apa und d.plain-web. d.plain-apa basiert auf Nutzungs-Texten. Im einfachen APA haben wir 483 Dokumente komplett manuell ausgerichtet. Das ergibt ungefähr 30.000-13.000 parallele Satzpaare. Für DeepLaneWeb enthält dieses Korpus verschiedene Bereiche und wir richten auch alle 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden aus. Insgesamt erhalten wir 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, z. B. hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind Bibeltexte viel stärker vereinfacht als beispielsweise Nachrichtentexte oder Sprachlernertexte auf allen Ebenen, bezogen auf beispielsweise lexikalische Vereinfachung, strukturelle Vereinfachung oder das Gesamtmaß der Vereinfachung. Darüber hinaus können Sie feststellen, dass unser D-plain-Korpus eine hohe Priorität verschiedener Vereinfachungstransformationen aufweist. So haben wir beispielsweise im d.plain-APA-Korpus deutlich mehr Umstellungen und Wortzusätze als im d.plain-web-Korpus. Andererseits haben wir im Web-Korpus deutlich mehr Umschreibungen. Sehen wir uns nun an, was wir mit diesem Korpus anfangen können. Hallo, mein Name ist Omar und ich werde nun über die Anwendungsfälle für unseren Datensatz D-plain sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden evaluieren. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, in dem wir zwei parallele Dokumente in verschiedenen Sprachen haben, und wir Ausrichtungen von Sätzen in zwei parallelen Dokumenten extrahieren wollen, die die gleiche Sprache und denselben Inhalt haben, aber unterschiedliche Komplexitätsstufen aufweisen. Nun, da wir unseren Datensatz d.plain haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu evaluieren. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und den Code zur Durchführung unserer Experimente in der Arbeit veröffentlicht. Abschließend kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die Textvereinfachung im Deutschen die Methode von MathAlign ist. Sie finden den Code auch, um diese Methode auf Ihren eigenen Dokumenten auszuführen, in der Arbeit. Der zweite Anwendungsfall, den wir in unserer Arbeit vorgestellt haben, ist der der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachte Texte aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben Long-Impart feinabgestimmt, um Satz-Vereinfachungen zu erzeugen. Sie finden alle Checkpoints und können sich die Ergebnisse und Evaluationsmetriken unserer Experimente in der Arbeit genauer ansehen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse erzielen oder Baseline-Ergebnisse übertreffen konnte, und wir schlugen diese Ergebnisse als Benchmark, einen Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Siyu Yuan von der Fudan Universität. Ich möchte Ihnen unsere Arbeit vorstellen, „Destillation von Skriptwissen aus großen Sprachmodellen für Constraint-basierte Sprachplanung“. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweisen Anweisungen in Form garantierter Skripte folgen. Bisherige Arbeiten konzentrierten sich jedoch hauptsächlich auf die Planung für den abstrakten Bereich. Ein guter Planer sollte Skripte für die konkreten Ziele der Planung erstellen. Ein abstraktes Ziel kann von verschiedenen, realitätsbezogenen Zielen mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte erstellen, die vernünftig sind und den Einschränkungen entsprechen. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur Constraint-basierten Sprachplanung. Da kein Datensatz spezifischer Ziele zur Unterstützung unserer Studie existiert, müssen wir diese zuerst erwerben. Wie in der Tabelle dargestellt, erweitern wir die abstrakten Ziele mit vielfältigen Einschränkungen für die Datenerfassung mit menschlicher Beteiligung unter Verwendung von InstructGPT. Wir wählen 100 spezifische Ziele aus und bewerten die aus großen Sprachmodellen generierten Skripte. Diese Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle großen Sprachmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum Lernmodelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, die Einhaltung der Einschränkungen jedoch nicht garantiert werden kann. Wir untersuchten detaillierter die fein abgestuften Themenkategorien von Einschränkungen, je nach Arbeitsweise. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von instruktiven TPDs für verschiedene Kategorien von Zielen erheblich variiert. Frühere Studien haben gezeigt, dass die Qualität der Ausgaben von leichtgewichtigen Modellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee der übergenerierenden Z-Filterung, um die Generierungsqualität zu verbessern. Zunächst zeigen wir Constraint-Typen mit Beispielen für InstructGPT und erhalten auf der Grundlage der Seed-Abstrakten-Ziele spezifische Ziele. Anschließend übergeneriert InstructGPT Schlüssel-Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die realisierbaren Skripte auszuwählen. Wir wandeln Skripte und Ziele in InstructGPT-Einbettungen um und berechnen Kosinus-Ähnlichkeit und Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus vergeben wir der Skript, die die Schlüsselwörter der Ziel-Einschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel im Ziel-Set den höchsten Wert erzielt. Mit unserer Methode kann InstructZBT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit erheblich, sowohl in Bezug auf die semantische Vollständigkeit als auch auf die Einhaltung der Einschränkungen. Da große Sprachmodelle kostspielig in der Bereitstellung sind, ist es unerlässlich, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Das Erstellen eines Datensatzes ist ein wesentlicher Schritt in diese Richtung. Bisherige Studien ermöglichen jedoch keine Planung für spezifische Ziele, und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um Constraint-basierte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für Constraint-basierte Sprachplanung zu erstellen, der als CodeScript bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testseiten sicherzustellen, baten wir cloudbasierte Mitarbeiter, überarbeitete, fehlerhafte Stichproben zu finden. Diese Abbildung zeigt die Verteilung der Einschränkungen in CodeScript. Wir stellen fest, dass CodeScript in den generierten spezifischen Zielen eine hohe Übereinstimmung zeigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für Constraint-basierte Sprachplanung einsetzen. Wir haben festgestellt, dass TFI von Antune mit der Kostenrate die Quadratwurzel von 0 generieren kann. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für Constraint-basierte Sprachplanung einsetzen. Wir haben festgestellt, dass T-File-Funktionen in CodeScript Skripte von höherer Qualität generieren können als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie mit geeigneten Datensätzen richtig trainiert werden. Zusammenfassend haben wir das Problem der Constraint-basierten Sprachplanung definiert. Wir bewerten die Fähigkeit großer Sprachmodelle zur Constraint-basierten Sprachplanung und entwickeln eine übergenerierende Filtermethode zur Forschung in der Sprachplanung. Vielen Dank für Ihre Zeit. Weitere Details zu CodeScript finden Sie in unserem Paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrac und ich werde Ihnen unsere Arbeit zu Dr. BERT vorstellen, einem robusten, vortrainierten Modell für den französischen biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend präsentieren wir den Hauptbeitrag unseres Artikels. Wir stellen das erste biomedizinische Modell in Französisch vor, genannt Dr. BERT, das auf Roberta basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web extrahiert wurden. Wir haben auch einen Vergleich von Modellen mit verschiedenen Vortrainingsszenarien und Datenquellen vorgestellt. Anschließend präsentieren wir unsere Ergebnisse für 11 biomedizinische und klinische nachgelagerte Aufgaben in Französisch. Und schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können.  Medizinische und klinische nachgelagerte Aufgaben in Französisch. Und schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 hat sich BERT zu einem der effektivsten Ansätze entwickelt, um Aufgaben der natürlichen Sprachverarbeitung zu lösen und im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vct, fast text oder NWO enorme Leistungssteigerungen zu erzielen. Seitdem wurde dieses Modell in viele andere Sprachen adaptiert, wie in Französisch mit Camembert, und in andere Bereiche wie Biomedizin mit permit-bert und bio-bert und im klinischen Bereich mit clinical-bert, hauptsächlich aber auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten. Französisch hatte jedoch bisher keine Open-Source-Modelle für den biomedizinischen Bereich. Daher haben wir uns gefragt, welche Datenquellen am besten für eine breite Palette von Anwendungen geeignet sind und ob diese Daten eine gute Substitution für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir Dr. BERT mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von einem nicht-universitären Krankenhaus gewonnen wurden. Abschließend fragen wir uns, wie viele Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Sind es 4 GB, 8 GB oder 4 GB RAM? Eine erste Version von Schubert, ein klinisches Modell, mit 4 GB Sätzen aus klinischen Notizen. Und eine finale Version von Schubert, mit einer Mischung aus 4 GB einer Teilmenge von NACHOS und 4 GB klinischer Notizen. Ergänzend zu diesem Vergleich haben wir drei Modelle vorgestellt, die auf kontinuierlichem Vortraining trainiert wurden, um den Einfluss der Vortrainingsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von Camembert und wurde auf einer 4-GB-Teilmenge von NACHOS trainiert. Ein weiteres basiert ebenfalls auf Camembert, wurde aber auf diesen vier Gigabyte permit belt by bio birth und clinical birth trainiert. Die Evaluierung zeigt, dass Modelle auf Aufgaben am besten funktionieren, bei denen die Daten der gleichen Art sind wie die, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir beobachten auch, dass die Verwendung von mehr Daten zu einer besseren Leistung führt. Insgesamt scheint das Vortraining von Grund auf höhere Leistung bei den meisten Aufgaben zu erzielen. Unsere Experimente zum kontinuierlichen Vortraining, wobei die Gewichte und der Tokenizer von Permet-BERT verwendet wurden und auf einer 4-Gigabyte-Teilmenge von NACHOS trainiert wurde, zeigen jedoch vergleichbare Ergebnisse wie die mit Dr.BERT 4 GB von Grund auf, was nicht für das Modell gilt, das auf Camembert-Gewichten und -Tokenizer basiert, das unter Stabilitätsproblemen leidet. Abschließend bietet unser proprietäres System eine bessere Leistung bei 9 von 11 nicht-trim-Aufgaben und übertrifft global die Ergebnisse des generischen Modells, hier Camembert. Wir beobachten auch, dass spezialisiertere Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die von NACHOS gewonnen wurden, sind auf UGIMFACE frei verfügbar und alle Trainingsskripte finden Sie in unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf die Interaktion auf der Postersession in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit von Vorabtrainingsdaten zu Sprachmodellen bis hin zu nachgelagerten Aufgaben, wobei wir die Verfolgung politischer Voreingenommenheiten untersuchen, die zu unfairen NLP-Modellen führen. Sprachmodelle werden also auf großen Web-Crawling-Datensätzen trainiert. Politische Nachrichtenmedien sind in ihren Vorabtrainingsdaten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, der Huffington Post usw. gut in den Trainingsdaten für Sprachmodelle abgebildet sind. Dies hat einen zwiespältigen Nutzen für Anwendungen von Sprachmodellen geschaffen. Einerseits konnten sie aus unterschiedlichen Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können möglicherweise zu Fairnessproblemen bei nachgelagerten Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von Vorabtrainingsdaten zu Sprachmodellen bis hin zu nachgelagerten Aufgaben zu untersuchen, insbesondere indem wir die folgenden Fragen stellen. Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle können Vorabtrainingsdaten hinsichtlich solcher politischen Voreingenommenheiten spielen? Zweitens, wie performen Sprachmodelle mit unterschiedlicher politischer Ausrichtung tatsächlich bei nachgelagerten Aufgaben und führt dies möglicherweise zu Fairnessproblemen in NLP-Anwendungen? Konkret haben wir zunächst vorgeschlagen, Sprachmodelle mit verschiedenen Eingabeformaten mithilfe von politischen Fragebögen anzusprechen, wie z. B. den politischen Kompass-Test. Dies stellt sicher, dass wir eine automatische Bewertung durchführen können, die gut in der Forschung der Politikwissenschaft verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen aufweisen. Sie besetzen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell unter allen ist und GPT-Theorien im Allgemeinen sozial liberaler sind als BERT-Theorien und deren Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus Trainingsdaten stammen. Daher führen wir ein kontrolliertes Experiment durch, indem wir Sprachmodell-Checkpoints zusätzlich auf sechs verschiedene parteiische Korpora vorabtrainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter in ihre politische Ausrichtung unterteilt sind. Durch das zusätzliche Vorabtrainieren von Sprachmodellen auf solchen parteiischen Korpora können wir feststellen, dass sich die ideologischen Koordinaten des Sprachmodells ebenfalls entsprechend verschieben. So können wir beispielsweise bei Roberta, das zusätzlich feinabgestimmt und auf dem linken Reddit-Korpus vorabtrainiert wurde, eine deutliche Verschiebung hin zur Liberalität in Bezug auf seine politischen Voreingenommenheiten feststellen. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung erfassen können, die in unserer modernen Gesellschaft vorherrscht. Daher teilen wir Vorabtrainingskorpora in die Zeit vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten auf und vorabtrainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können feststellen, dass Sprachmodelle nach 2017 im Allgemeinen eine politische Ausrichtung aufweisen, die weiter vom Zentrum entfernt ist. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft erfassen können. Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlicher politischer Ausrichtung auf die Erkennung von Hassreden und die Erkennung von Falschmeldungen, um NLP-Anwendungen zu untersuchen, bei denen häufig Sprachmodelle zum Einsatz kommen und die sehr bedeutende Auswirkungen haben können. Wir sehen, dass wenn wir die kategoriebezogene Leistung untersuchen, d. h. wenn wir die Leistung in verschiedene demografische Gruppen oder die politische Ausrichtung von Nachrichtenmedien unterteilen, wir ein Muster erkennen, dass beispielsweise bei der Erkennung von Hassreden sprachmodelle, die sich links orientieren, besser darin sind, Hassreden zu erkennen, die auf sozial marginalisierte Gruppen abzielen, aber schlechter darin sind, Hassreden zu erkennen, die auf mächtigere Gruppen in unserer Gesellschaft abzielen. Umgekehrt sind sprachmodelle, die sich rechts orientieren, besser darin, Hassreden zu erkennen, die sich gegen weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richten. Ähnliche Trends treten auch bei der Erkennung von Falschmeldungen auf, wo wir sehen, dass sprachmodelle, die sich links orientieren, besser darin sind, Fehlinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen, und umgekehrt. Dies zeigen wir weiter durch viele qualitative Beispiele, um zu sehen, dass sprachmodelle mit unterschiedlicher politischer Ausrichtung unterschiedliche Vorhersagen zu Hassreden und Falschmeldungen abgeben, basierend auf ihrer sozialen Kategorie. Es gibt eine Reihe weiterer Beispiele im Anhang, um die unterschiedlichen Vorhersagen zu Hassreden und Falschmeldungen basierend auf ihrer sozialen Kategorie weiter hervorzuheben. Dies deutet darauf hin, dass es ein drängendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen gibt. So würde beispielsweise, wenn sprachmodelle, die sich rechts orientieren, für die Erkennung von Hassreden oder Falschmeldungen oder was auch immer feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würden, dies bedeuten, dass Personen mit gegenteiligen politischen Meinungen marginalisiert würden und Hassreden, die auf Minderheitengruppen abzielen, ohne Kontrolle grassieren könnten. Dies hat uns gewarnt, die Fairnessprobleme anzuerkennen und zu bekämpfen, die durch die politische Ausrichtung von Sprachmodellen entstehen. Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufdecken. Es ist wie zwischen Skylla und Charybdis. Wenn wir die politischen Meinungen in den Trainingsdaten für Sprachmodelle nicht reinigen, wird die Voreingenommenheit von den Vorabtrainingsdaten zu Sprachmodellen bis hin zu nachgelagerten Aufgaben weitergegeben und schafft letztendlich Fairnessprobleme. Wenn wir versuchen, sie irgendwie zu reinigen, riskieren wir auch Zensur oder Ausgrenzung, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten für Sprachmodelle beibehalten werden sollte. Es ist also wie das elektrische Trolley-Problem. Okay, gut. Ich denke, das war es für heute. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Kostav Sinha und ich freue mich, Sie zu unserem Vortrag über unser ACL 2023 Paper „Language Model Acceptability Judgements Are Not Always Robust to Context“ begrüßen zu dürfen. Dies ist eine Gemeinschaftsarbeit mit John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy und Adina Williams. In dieser Arbeit beschäftigen wir uns erneut mit dem Minimal Pair Paradigma. Das Minimal Pair Paradigma bewertet im Wesentlichen Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität umfassen können, wie zum Beispiel bei BLIMP, Syntax-Gym oder Akzeptanz in Bezug auf Stereotypen, wie bei Krauss-Paaren. Im Rahmen dieses Minimal Pair Paradigmas ist die übliche Methode zur Bewertung von Sprachmodellen, dass man beispielsweise einen akzeptablen oder grammatikalisch korrekten Satz und dann einen inakzeptablen oder ungrammatikalischen Satz präsentiert. Der Prozess des Modells besteht dann im Wesentlichen darin, dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuzuweisen. Die aktuelle MPP-Pipeline ermöglicht es uns jedoch nicht, die Akzeptanz der Modelle in Bezug auf längere Sätze zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern. Deshalb überprüfen wir die Datensätze noch einmal und erstellen Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. So haben wir beispielsweise ein typisches Paar grammatikalischer Korrektheit aus dem BLIMP-Datensatz im Fall der Adjunct Island ausgewählt. Was wir tun, ist, längere Sequenzen zu erstellen, die akzeptabel sind und die die gleiche grammatikalische Struktur aufweisen, indem wir grammatikalische Sätze aus Adjunct Island extrahieren und diese als Präfix sowohl zur akzeptablen als auch zur inakzeptablen Anfrage hinzufügen. Dasselbe können wir tun, indem wir inakzeptable Sätze aus der gleichen Übereinstimmung auswählen, was ebenfalls zur Überprüfung der Akzeptanz des Modells verwendet werden kann. Ebenso können wir dies tun, indem wir Sätze aus einer anderen Teilmenge oder einem anderen Datensatz auswählen. Das bezeichnen wir als Mismatch-Szenario. Hier stammen die Sätze zwar immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir sie bewerten. Dasselbe können wir auch für einen Akzeptanzfall tun. Schließlich können wir Sätze aus einer völlig anderen Domäne auswählen, wie beispielsweise von Wikipedia. Dies würde uns zeigen, ob die Akzeptanzurteile des Modells tatsächlich von einem Kontext beeinflusst werden, der völlig irrelevant für den Satz ist, den wir betrachten. Wie schlägt sich das Modell also? Zuerst betrachten wir die Sätze aus Wikipedia, die völlig irrelevant für das aktuelle Paare sind. Dort stellen wir fest, dass die MPP-Urteile meist robust gegenüber beliebigen Kontextlängen sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um die OPT- und GPT-2-Modelle maximal auszuschöpfen. Wir sehen hier in der orangefarbenen gepunkteten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert nun, wenn wir Sätze aus demselben Datensatz auswählen? Hier erstellen wir oder generieren Sätze aus akzeptablen und inakzeptablen Domänen aus demselben BLIMP- oder Syntax-Gym-Datensatz. Dort stellen wir fest, dass die MPP-Urteile entweder signifikant ansteigen oder abnehmen, wenn wir entweder akzeptable Präfixe oder inakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur anpassen, d.h. wenn wir Sätze aus demselben Phänomen in BLIMP-Person-Text-Gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils des Modells, je nachdem, ob das ausgewählte Präfix akzeptabel oder inakzeptabel ist. Dies und dies ist sehr groß, dieser Effekt nimmt über die Kontextlänge hinweg zu, und würde wahrscheinlich neuere Sprachmodelle beeinflussen, die über große Kontextfenster verfügen. Warum beeinflusst das übereinstimmende Präfix das Urteil des Sprachmodells so stark? Wir haben eine Reihe von Analysen durchgeführt, in denen wir versucht haben, den Eingangsatz so zu gestalten, dass die relevanten Strukturen erhalten bleiben, aber dem Eingangsfeld Rauschen hinzugefügt wird. Nachdem wir mehrere dieser Störungen durchgeführt haben, stellen wir fest, dass keines dieser Geräusche dazu führt, dass sich das Modell in Bezug auf den MPP-Urteilstrend ändert. Wir stellen fest, dass die Modelle die gestörten Sätze in ähnlicher Weise beeinflussen. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen. Und wenn wir die Sätze im inakzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle gegenüber latenten syntaktischen und semantischen Merkmalen empfindlich sind, die über die Sätze hinweg gemeinsam genutzt werden. Die MPP-Evaluierung, wie wir sie derzeit mit kurzen und einzelnen Sätzen durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unser Paper für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Doktorand an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit vorstellen, \"Weaker than you think\", einen kritischen Blick auf schwach überwachtes Lernen. Dies ist eine Gemeinschaftsarbeit mit Xiao Yusheng, Mario Smusbach, Gia Steffen und Dietrich Klackow. Ich möchte zunächst eine kurze Einführung in schwache Überwachung und schwach überwachtes Lernen geben. Bei schwacher Überwachung labeln wir die Daten nicht manuell. Stattdessen labeln wir die Daten mithilfe schwacher Labelquellen, wie z. B. einfachen Heuristikregeln, Wissensdatenbanken oder niedrigwertigen Crowdsourcing-Aktivitäten, wie in der rechts abgebildeten Grafik dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen deutlich kostengünstiger, aber auch verrauscht, das heißt, ein gewisser Anteil der Annotationen ist fehlerhaft. Wenn wir neuronale Netze direkt auf schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, das gelabelte Rauschen auswendig zu lernen und generalisieren nicht gut. Im Bereich des schwach überwachten Lernens werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze unter solchen Labelrauschen robust zu trainieren, so dass die trainierten Modelle dennoch gut generalisieren. In aktuellen Arbeiten im WSL – WSL steht für schwach überwachtes Lernen – ist eine gängige Behauptung, dass behauptet wird, man trainiere Modelle ausschließlich mit schwachen Labeldaten und erziele dennoch hohe Leistung auf sauberen Testdatensätzen. Technisch gesehen ist dieser Anspruch nicht falsch, allerdings gibt es da einen Haken, und zwar in Form von drei Forschungsfragen. Erstens, benötigen wir saubere Validierungsdaten? Schließlich sollten wir nur die sauberen Samples für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungs-Samples benötigen, um ordnungsgemäß zu funktionieren. Andernfalls kommt es zu einem deutlichen Leistungsabfall. Wie in dieser Grafik dargestellt, können die trainierten Modelle ohne saubere Validierungs-Samples nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gelabelte Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Erlangung sauberer Validierungs-Samples sollten nicht übersehen werden. Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl sauberer Validierungs-Samples dazu beiträgt, dass WSL-Ansätze eine bessere Leistung erzielen, wie in der linken Grafik dargestellt. Typischerweise benötigen wir nur 20 Samples pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns in jedem Fall dazu entschließen, saubere Samples zu beschaffen, führt das direkte Training mit diesen zu einer noch besseren Leistung. Die rote Grafik zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie man sieht, beginnt direktes Fine-Tuning, sobald man 10 Samples pro Klasse hat, WSL-Ansätze zu übertreffen. Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansätzen beansprucht wurde, leicht erzielt werden, indem man das Fine-Tuning auf den sauberen Validierungs-Samples fortsetzt. Wie man aus den Grafiken sehen kann, erzielt das Van Linden-Modell, das zunächst als W bezeichnet wird, schlechtere Ergebnisse als komplexere WSL-Methoden wie Cosine. Wenn wir das Fine-Tuning auf den sauberen Samples fortsetzen dürfen, schneidet FTW jedoch genauso gut ab wie andere Methoden. Daher gibt es in der Praxis keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz benötigen. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Samples benötigen, um ordnungsgemäß zu funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Berichten Sie zunächst über die Modellauswahlkriterien. Berichten Sie beispielsweise, ob die Modellauswahl anhand sauberer Validierungs-Samples erfolgt. Zweitens sollten WSL-Ansätze mit Few-Shot-Learning-Baselines verglichen werden, die mit sauberen Samples arbeiten. Drittens sollte das kontinuierliche Fine-Tuning als einfacher, aber starker Baselinie in zukünftigen Arbeiten im WSL-Bereich berücksichtigt werden. Schließlich haben wir unseren Code Open-Source gestellt. Sie finden ihn über den QR-Code auf dieser Folie. Zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist David Vilar und ich werde Ihnen einen kurzen Überblick über das Paper „Grunting Parm from Translation“ geben, wobei wir Strategien und Leistung bewerten. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Parm ist ein großes Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr, also 2022, vorgestellt wurde. Es wurde auf einer umfangreichen Textsammlung trainiert, die 780 Milliarden Token umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es den aktuellen Stand der Technik in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zum Prompting großer Sprachmodelle für die maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeiten dieser Modelle unter Verwendung der Best Practices der IMT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir vergleichen zwei hochmoderne Systeme, also die leistungsstärksten Systeme oder die WMT-Bewertung. Wir verwenden hochmoderne neuronale IM-Metriken und zeigen zusätzlich auch Ergebnisse der menschlichen Bewertung durch Experten. Abschließend geben wir einige Empfehlungen für Strategien zur Auswahl von Prompts. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwenden und für jeden Satz zwei verschiedene Prompts bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt die beobachtete Differenz mehr als einen BLEU-Punkt. Und dies kann in extremen Fällen bis zu 40 BLEU-Punkte betragen. Es ist daher wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine Five-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System zur Verfügung stellen, lediglich mit der Sprache kennzeichnen. So ist es in diesem Beispiel, in dem wir eine Übersetzung von Deutsch ins Englische durchführen, jeder deutsche Satz mit „Deutsch:“ gekennzeichnet und die englischen Übersetzungen mit „Englisch:“ gekennzeichnet. Wir haben festgestellt, dass die eigentliche Form des Promptings im Falle von Several-Shot-Prompting keinen großen Einfluss hat. Es ist entscheidend für Zero- und One-Shot-Prompting, und wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, gibt es fast keinen Unterschied zur eigentlichen Form des Promptings. Es sind die Beispiele, die das meiste Gewicht tragen. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit mit dem Ausgangssatz. Es ist also wichtig – die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit mit dem Ausgangssatz. Es ist wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Bewertungen oder den Dev-Daten. Die Dev-Daten sind deutlich besser kuratiert und von höherer Qualität als die Trainingsdaten, die verrauschter sind, und die Ergebnisse zeigen eine bessere Leistung bei Verwendung der Dev-Daten. Dennoch haben spezialisierte, hochmoderne Systeme einen deutlichen Vorteil gegenüber den Palm-Übersetzungen, aber Palm kommt einem kommerziellen System ziemlich nahe. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren. Die Erkenntnisse, die wir aus der E-Mail-Analyse gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sind, dass die Flüssigkeit von Palm mit der von hochmodernen Systemen vergleichbar ist, aber der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, als ob Palm manchmal eine bessere Klangfarbe für die Übersetzung wählt, indem es Teile des Ausgangssatzes weglässt, die in der Übersetzung nicht wiedergegeben werden. Allerdings ist die Kategorie „stilistisch holprig“ für Parm geringer als für die hochmodernen Systeme, was ein weiteres Zeichen dafür ist, dass Parm wirklich flüssige Ausgaben liefert, aber dennoch einige Probleme mit der Genauigkeit aufweist. Und das war's für diesen wirklich kurzen Überblick. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Papers. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie Chinas. Es ist mir eine Freude, Ihnen ein kurzes Werbevideo über Papier, „Are You Copying My Model?“, vorzustellen: Schutz des Urheberrechts von großen Sprachmodellen für Einbettungen und Dienste durch eine Hintertür-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund von Einbettungen und Diensten vorstellen. Derzeit sind große Sprachmodelle wie GPT, LAMA, PALM außergewöhnlich im Bereich des natürlichen Sprachverständnisses und der Sprachgenerierung. Embedding as a Service ist einer der Dienste, der auf großen Sprachmodellen aufbaut, um verschiedene NLP-Aufgaben zu unterstützen. Beispielsweise bietet OpenAI eine GPT-basierte Embedding-API. Jüngste Arbeiten haben jedoch gezeigt, dass ein Angreifer das Modell durch Lernen aus den Einbettungen stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Einbettungen als Dienst zu schützen. Um das Urheberrecht von Einbettungen als Dienst zu schützen, ist eine Lösung, ein Wasserzeichen in den angebotenen Dienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Einbettungen als Dienst anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer so unauffällig sein, dass er es leicht entfernen kann. Schließlich muss sich das Wasserzeichen während des Modell-Extraktionsprozesses auf die Dienste des Angreifers übertragen lassen. Bisherige Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht auf Einbettungen als Dienst anwendbar oder es fehlt ihnen an Übertragbarkeit. Details zu unserem EmbeddingMarker. EmbeddingMarker umfasst zwei Hauptschritte: Wasserzeichen-Injektion und Urheberrechtsprüfung. Vor diesen Hauptschritten wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Frequenzbereich. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeichen-Injektion definieren wir zunächst eine Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Dienst des Anbieters sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Die bereitgestellte Einbettung ist die Gewichtssumme der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsprüfung besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wortmark enthält. Wir erstellen zunächst einen Hintertür-Datensatz und einen gutartigen Datensatz. Der Hintertür-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Anschließend fordert der Anbieter Einbettungen vom Stealer-Dienst mit dem Datensatz an. Die Cosinus- und L2-Ähnlichkeit zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Die Ähnlichkeitsdifferenz zwischen dem gutartigen und dem Hintertür-Datensatz wird berechnet, die als Delta Cosinus und Delta L2 definiert ist. Wir führen auch einen KS-Test durch und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AGnews, Mind, SSD2 und Eraspam. Wir gehen davon aus, dass der Anbieter den Wikitext-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser eingebetteter Marker eine hervorragende Erkennungsleistung erzielen kann, während er gleichzeitig eine hohe Nutzbarkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Unauffälligkeit der bereitgestellten Einbettung, indem wir die Einbettungen von Sätzen auf den vier Datensätzen mithilfe von PCA visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwierig, zwischen den Hintertür-Einbettungen und normalen Einbettungen zu unterscheiden. Das war alles, danke. Wir freuen uns auf eine Diskussion mit Ihnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Ying, und mein Kollege Zhiyang und ich werden unsere Forschung zum Thema Multi-Improvement vorstellen, nämlich zur Verbesserung des sequenziellen, kurzzeitigen Lernens mehrerer Modelle durch Instruction Tuning. Angesichts der Fortschritte bei großen Sprachmodellen wurden in vielen Arbeiten neue Lernparadigmen untersucht, um vortrainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf effiziente Weise in Bezug auf Parameter und Daten wiederzuverwenden. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning großen Sprachmodellen ermöglicht, auf unsichtbaren Aufgaben in einer Zero-Shot-Manier zu arbeiten, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Instruction Tuning konzentrierten sich jedoch auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während Computer Vision und multimodale Aufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Instruction Tuning auf multimodalen vortrainierten Modellen tatsächlich die Verallgemeinerung auf unsichtbare multimodale Aufgaben verbessern kann. Zusätzlich stellten wir bei unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Instruction-Datensätzen zwischen NLP und multimodalen Bereichen fest. Es gibt mehr als 1.600 sprachbasierte Instruction-Aufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren multimodalen Instruction-Aufgabendatensatz. Dies motivierte uns, einen multimodalen Instruction-Tuning-Datensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multimodalen Instruction-Tuning-Benchmark-Datensatz, der aus 62 verschiedenen multimodalen Aufgaben mit insgesamt 10 Kategorien besteht. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit 5 von Experten verfassten Anweisungen versehen. Um das multimodale Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein einheitliches, multimodales vortrainiertes Modell, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequence-to-Sequence-Format, wobei die Eingabetexte, Bilder, Anweisungen und Begrenzungsrahmen im selben Tokenraum dargestellt werden. Nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wählen 10.000 Instanzen pro Aufgabe aus. Für das Testen behalten wir die gesamte Gruppe für Common-Sense-Reasoning und wählen zusätzlich fünf Aufgaben aus den Gruppen VQA und Miscellaneous aus. Wir verwenden alle Instanzen in der Testaufteilung für jede Aufgabe. Darüber hinaus wählen wir zufällig 20 Aufgaben aus der Testaufteilung von Natural Instruction als unsichtbare Aufgaben für NLP aus. Wir verwenden ein vortrainiertes OFA Large Model als Basismodell. Während des Trainings werden alle Instanzen für alle Aufgaben gemischt. Jede Instanz wird zufällig mit einer ihrer fünf Instruction-Vorlagen kombiniert. Bei Tests für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment auswerten. Wir berichten über den Mittelwert und das Maximum der Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine Multi-Modal-Klassifikationsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine Multi-Modal-Generierungsaufgabe handelt, berichten wir über die Quadratwurzel des L. Für eine RP-Aufgabe die Quadratwurzel des jl. Wir haben auch eine zusätzliche Evaluationsmetrik namens Sensitivität eingeführt, die die Fähigkeit des Modells misst, für dieselbe Aufgabe konsistent dieselben Ausgaben zu erzeugen, unabhängig von leichten Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann Instruction Tuning die Leistung von OFA bei szenenbasierten, multimodalen Aufgaben deutlich verbessern. Darüber hinaus kann Transferlernen aus Natural Instruction Datensätzen das Instruction Tuning begünstigen. Wir können sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig die Sensitivität verringert. Wir haben auch ein Experiment durchgeführt, in dem wir eine Anweisung versus fünf Anweisungen verwendet haben. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Modellsensitivität. Wir können auch sehen, dass das Modell durch Transferlernen aus Natural Instruction Datensätzen eine viel bessere Sensitivität erzielen kann als das ursprüngliche OFA-Modell. Wir können auch sehen, dass Transferlernen aus Natural Instruction Datensätzen OFA helfen kann, auf dem Nitro Instruct-Datensatz eine deutlich bessere Leistung zu erzielen. Insgesamt schlagen wir den ersten großflächigen multimodalen Instruction-Tuning-Datensatz vor. Wir verbessern die Zero-Shot-Fähigkeit von OFA deutlich und untersuchen verschiedene Transferlerntechniken und zeigen deren Vorteile. Wir haben eine neue Metrik namens Sensitivität entworfen. Apropos, wir sammeln einen noch größeren multimodalen Instruction-Tuning-Datensatz mit etwa 150 zusätzlichen varianten Sprachaufgaben und werden ihn bald veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag zusammen, mein Name ist Yusheng Zhang von der Penn State University. Heute werde ich unsere Arbeit vorstellen, nämlich „Cross-linguale Semantische Analyse in Mehreren Natürlichen Sprachen und Bedeutungsrepräsentationen“. Die semantische Analyse ist eine Aufgabe, bei der semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda-Kalkül erstellt werden. Cross-linguale semantische Analyse ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung dargestellt, müssen wir die Abfrage in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für cross-linguale semantische Analyse werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Es gibt beispielsweise Lücken in der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt, und aufgrund der Abdeckung bestimmter Mini-Repräsentationen fehlt der Lambda-Kalkül oder sie werden nur mit bestimmten neuronalen Modellen evaluiert. Zum Beispiel gibt es nur ein einzelnes Modell, um sie zu evaluieren. Um dies zu erreichen, schlagen wir Exampler vor. Wir stellen einen einheitlichen Datensatz Exampler für cross-linguale semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält neun Datensätze aus verschiedenen Domänen, fünf semantische Analyseaufgaben, acht Bedeutungsrepräsentationen und 22 natürliche Sprachen aus 15 Sprachfamilien. Und um unser Benchmark besser zu evaluieren, betrachten wir sechs Einstellungen für das Training und die Evaluation. Die erste ist die Translate-Test-Einstellung. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und trainieren dann eine Evaluation mit einem monolingualen Modell. Zum Beispiel trainieren wir das englische Modell mit englischen Abfragen. Während der Inferenz übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um SQL vorherzusagen. Außerdem testen wir monolinguale Modelle. In dieser Einstellung ist die Quellsprache die gleiche wie die Zielsprache. Zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch monolinguale Few-Shot-Einstellung, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen ein mehrsprachiges Modell, das wir mit allen Sprachen trainieren. Zum Beispiel kombinieren wir deutsche, englische und chinesische Abfragen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Wir betrachten außerdem cross-linguale Zero-Shot- und Few-Shot-Übertragung. Wir trainieren es in einer Quellsprache und übertragen es in eine andere Sprache. Während des Trainings trainieren wir es mit englischen Abfragen oder einer Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein mehrsprachiges Modell zu trainieren und die SQL-Ausgabe vorherzusagen. Und wir haben auch viele interessante Ergebnisse gefunden. In Bezug auf die Analyse monolingualer Modelle evaluieren wir auf zwei Gruppen von Modellen, einschließlich Encoder-PDR, was für Multilingual Pre-trained Encoder mit Pointer-basierten Decodern steht, wie z.B. XLMR plus PDR und BERT plus PDR. Und wir evaluieren außerdem Encoder-Decoder-Modelle, d.h. mehrsprachig vorab trainierte Encoder-Decoder-Modelle, wie z.B. MBART und MT5. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung in allen neun Datensätzen erzielen. Und wir evaluieren MT5 und XLMR plus PDR in mehrsprachiger Umgebung. Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PDR durch Training in einer Mischung verschiedener Sprachen verbessert werden können. Wir haben festgestellt, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen, außer dass die englische Leistung in sieben Datensätzen abnimmt und nur in drei Datensätzen zunimmt. Ich denke, dies wird als Fluch der Mehrsprachigkeit bekannt. Wir vergleichen auch die cross-linguale Leistungsdifferenz. In dieser Abbildung ist die blaue Linie die cross-linguale Few-Shot-Übertragung. Die orangefarbene Linie ist die cross-linguale Zero-Shot-Übertragung, während die grüne Linie die monolinguale Einstellung darstellt. Wir haben festgestellt, dass beim Vergleich der grünen und orangefarbenen Linie der cross-linguale Übertragungsleistungsabstand in der Zero-Shot-Einstellung signifikant ist. Und beim Vergleich der blauen und orangefarbenen Linie haben wir festgestellt, dass der Übertragungsabstand in der Few-Shot-Einstellung schnell verkürzt wird. Wir haben auch einige andere interessante Erkenntnisse gefunden. Zum Beispiel übertrifft der Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Die Darstellung der englischen natürlichen Sprache kann die Leistung von Few-Shot in den Zielsprachen erheblich steigern. Und wir haben festgestellt, dass mehrsprachige Sprachmodelle wie CODIS und BLUE immer noch für cross-linguale semantische Analyseaufgaben geeignet sind. Zusammenfassend haben wir Exampler entwickelt, einen einheitlichen Benchmark für cross-linguale semantische Analyse mit mehreren natürlichen Sprachen und Hauptrepräsentationen. Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Typen von mehrsprachigen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse und Ähnliches. Besuchen Sie gerne unser Paper und unseren Code. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Szpilkowski und dieser Vortrag behandelt die Dependenzstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Dependenzstrukturen, die von unterschiedlichen Theorien und Korpusansätzen angenommen werden. So zum Beispiel haben die Universal Dependencies die Struktur der Koordination \"Lisa, Bart und Maggie\" so, dass das erste Konjunkt die Kopfstelle der gesamten Koordinatstruktur ist, also in diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igors Milchuks Meaning-Text-Theorie angenommen, wo wiederum die gesamte Koordinatstruktur durch das erste Konjunkt geköpft wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie isolieren eines der Konjunkte. Es gibt auch symmetrische Ansätze für Koordinatstrukturen, wie beispielsweise den Prager Ansatz, den in den Prager Dependency Treebanks angenommenen conjunction-headed Ansatz, bei dem Koordinatstrukturen durch die Konjunktion geköpft werden. So erhalten wir Dependenzen von n zu allen Konjunkten. Und schließlich gibt es auch einen Multi-Head-Ansatz, der beispielsweise in Dick Hudsons Wortgrammatik verwendet wird, wo, sagen wir, alle Konjunkte Köpfe der Koordinatstruktur sind. Ein Multi-Head-Ansatz, der beispielsweise in Cutson's Wortgrammatik verwendet wird, wo sozusagen alle Konjunkte Köpfe der Koordinatstruktur sind, so erhalten wir Dependenzen vom Regenten hier liebt zu allen Konjunkten separat. Dies sind Bartons, also die Erkenntnis, dass das Ziel dieser Arbeit darin besteht, zwei und gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu produzieren. Okay, das Argument basiert auf dem Prinzip der Minimierung der Dependenzlänge, das ich anhand dieser Beispiele erläutern werde. So, wie Sie vielleicht wissen, bevorzugen direkte Objekte, nahe am Verb zu sein, während Adverbiale weiter entfernt sein können, richtig? Also \"March las es gestern\" ist in Ordnung, weil das direkte Objekt \"es\" nahe am Verb ist, während direkte Objekte das Verb bevorzugen, während Adverbiale weiter entfernt sein können, richtig? Also \"March las es gestern\" ist in Ordnung, weil das direkte Objekt \"es\" nahe am Verb ist, während \"March las gestern es\" viel schlechter ist, richtig? Denn hier befindet sich das Adverbiale \"gestern\" zwischen dem Verb und dem direkten Objekt. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und lang ist, da es dann hinter das Adverbiale verschoben werden kann. Dies wird hier illustriert. Beide Sätze sind in Ordnung. \"March las dieses absolut faszinierende Buch über die BCS heute\". Es ist in Ordnung. Irgendwie haben wir anstelle von \"es\" dieses lange NP. Es ist aber auch in Ordnung zu sagen: \"March las gestern dieses absolut faszinierende Buch über Bienen\". Die Argumentation hier ist, dass dies möglich ist, weil der Satz zwar das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, er aber das Prinzip der Minimierung der Dependenzlänge erfüllt, das besagt, dass kürzere Dependenzen bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Dependenzen, also derjenigen, die zwischen diesen beiden Strukturen nicht konstant sind. Hier haben wir eine Dependenz von \"las\" zum Adverbialen der Länge sieben (gemessen in Wörtern) und von \"las\" zu \"Buch\" der Länge vier. Insgesamt sind es also 11. Wenn man verschiebt, wenn man diese beiden Konstituenten vertauscht, wird die Summe dieser beiden Dependenzen sechs, richtig? Anstelle von 11 also sechs, viel kürzer. Deshalb klingt das ziemlich in Ordnung. Es verletzt ein Prinzip, erfüllt aber ein anderes. Okay, also das haben wir gemacht, wir haben verschiedene Statistiken über die Koordination aus der erweiterten Version des Penn Treebank extrahiert und sehen Sie im Artikel nach, warum wir keine Universal Dependencies verwendet haben. Und diese Statistiken bestätigen die bereits mehrfach gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind. Wir haben keine Universal Dependencies verwendet und diese Statistiken bestätigen die bereits mehrfach gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind, also \"Salz und Pfeffer\" und nicht \"Pfeffer und Salz\" (gemessen in Silben) und auch die Beobachtung, die nur am Rande erwähnt wurde, dass dieser Trend mit der Länge, der Längendifferenz wächst, also je größer die Differenz zwischen den Längen der beiden Konjunkte ist, desto eher ist der kürzere Konjunkt der erste. Also der Anteil des linken, kurzen Konjunkts ist größer. Aber was in diesem Artikel neu ist, ist, dass wir beobachtet haben, dass dieser Trend nur auftritt, wenn der Regent sich links befindet oder fehlt, richtig? Der Regent befindet sich in diesem Beispiel links: \"Ich sah Bart und Lisa\", also ist der Regent links. Er fehlt im zweiten Beispiel: \"Homer kam und niesete\". Hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regenten. In solchen Fällen bevorzugt also der linke Konjunkt, desto mehr, desto größer die Differenz zwischen den beiden Konjunkten. Wenn sich der Regent jedoch rechts befindet, verschwindet dieser Effekt. Wir zeigen dies, indem wir die Länge in Zeichen (erste Spalte), in Silben (mittlere Spalte) und in Wörtern (rechte Spalte) messen. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn der Regent sich links befindet, der Trend, dass der linke Konjunkt kürzer ist, stetig mit der absoluten Differenz in Wörtern wächst. Das Gleiche gilt, wenn kein Regent vorhanden ist, wie bei der Koordination von Sätzen. Aber wenn sich der Regent rechts befindet, verschwindet dieser Trend. Und wir zeigen im Artikel, wie dies ein Argument gegen asymmetrische Strukturen der Koordination liefert und für die symmetrischen Strukturen spricht. Sehen Sie sich den Artikel für die vollständige Übereinstimmung und Argumentation an, und sprechen Sie mit uns über die Postersitzung. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Mein Name ist Kaio Yin, und ich werde unsere Arbeit vorstellen, die den Titel trägt: Wann erfordert Übersetzung Kontext? Eine datengestützte, mehrsprachige Untersuchung. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, Andre F.D. Martins und Graham Newbig erstellt. Es gilt also festzustellen, dass viele Übersetzungen vom Kontext abhängig sind. Zum Beispiel: Wie würden wir „mole“ in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautet: „Die Dinge könnten gefährlich werden, wenn die Minister davon erfahren“, dann bezieht sich „mole“ auf einen Spion. Aber wenn der vorherige Satz lautet: „Könnte es etwas Ernstes sein, Doktor?“, dann bezieht sich „mole“ auf eine Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes, und damit auch seine Übersetzung. Die Bewertung, wie gut Modelle Fälle wie diesen übersetzen können, ist jedoch recht schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängig ist, was es Metriken auf Korpus-Ebene wie BLUE unmöglich macht, diese Übersetzungen zu erfassen. Und einige Leute haben gezielte Bewertungen für kontextabhängige Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie in der Regel auf Domänenwissen und menschliche Kuratierung angewiesen sind. In dieser Arbeit haben wir versucht, diese beiden Fragen zu beantworten. Erstens: Wann erfordert Übersetzung Kontext? Und zweitens: Wie gut gehen Modelle mit diesen Fällen um? Um die erste Frage zu beantworten, begannen wir damit, zu messen, wie stark ein Wort vom Kontext während der Übersetzung abhängt. In unserer bisherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht durch Messung, wie viele Informationen der Kontext C über das Ziel Y gibt, gegeben die Quelle X. Man kann CXMI als die Information betrachten, die man erhält, wenn man dem Modell Kontext liefert. In dieser Arbeit erweitern wir CXMI zu point-wise CXMI, das die Kontextnutzung auf Satzebene oder Wortebene messen kann. Wir können Wörter mit einem hohen P6MI-Wert als solche betrachten, die für die Übersetzung Kontext erfordern. Nun analysieren wir Wörter mit hohem P6MI, um nach Mustern zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse anhand von Transkripten von TED-Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Erstens betrachten wir Wortarten, die hohe Mittelwerte von PCXMI aufweisen. Dies ermöglicht es uns beispielsweise, Dualpronomen im Arabischen mit relativ hohen PCXMI-Werten zu finden. Dies lässt sich dadurch erklären, dass Englisch keine Dualpronomen hat, sodass man beim Übersetzen ins Arabische Kontext benötigt, um festzustellen, ob ein Pronomen dual ist. Und ähnlich finden wir, dass bestimmte Sprachen ebenfalls Kontext erfordern, wenn wir die passende Verbform wählen möchten. Wir betrachten dann Wortvokabular, das über alle seine verschiedenen Vorkommnisse hinweg einen hohen PCSXMI-Wert aufweist. Dies hilft uns, Fälle wie diesen zu identifizieren, bei denen man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man dieselbe Übersetzung innerhalb des Dokuments verwendet. Und ähnlich finden wir, dass Kontext erforderlich ist, um die richtige Höflichkeitsform zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token mit hohem PCXMI. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich vom Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z. B. Ellipsislösung. Nun nutzen wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für Übersetzungen auf Dokumentebene zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Wir können dann den Tagger verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir bezeichnen unseren Tagger als multilingual discourse-aware, oder MUDA, Tagger. Sprachen haben unterschiedliche Anteile dieser Diskursphänomene. Wir verwenden dann den MUDA-Tagger, indem wir den Tagger auf den parallelen Korpus anwenden, den wir für die Bewertung verwenden möchten. Und wir wenden unsere Übersetzungsmetriken an, die wir gewählt haben, auf die kontextabhängigen Beispiele, die der Muda-Tagger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der maschinellen Übersetzung auf Dokumentebene zu bewerten. Zunächst stellen wir fest, dass bei der Verwendung von Korpus-Level-Metriken, wie z. B. BLUE, kontextunabhängige Modelle die beste Leistung erbringen. Wenn wir jedoch COMET verwenden, erzielen kontextbewusste Modelle die besten Ergebnisse. Und wenn wir das Wort-f-Maß verwenden, haben Modelle mit oder ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste System für die maschinelle Übersetzung auf Dokumentebene zu bestimmen, wenn wir allein Korpus-Level-Metriken verwenden. Nun verwenden wir den Muda-Benchmark, um Modelle zu bewerten, und wir stellen fest, dass kontextbewusste Modelle für bestimmte Diskursphänomene, wie z. B. Höflichkeit und lexikalische Kohäsion, deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen nicht wesentlich besser als Modelle, die keinen Kontext verwenden. Dies deutet darauf hin, wo wir Fortschritte bei der maschinellen Übersetzung auf Dokumentebene erzielen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate für die maschinelle Übersetzung auf Dokumentebene. Zusammenfassend führen wir eine datengestützte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und dann nutzen wir unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, der uns helfen kann zu identifizieren, welche Diskursphänomene Modelle gut oder nicht gut handhaben können, und welche Übersetzungssysteme gut für die maschinelle Übersetzung auf Dokumentebene geeignet sind. Vielen Dank für Ihre Aufmerksamkeit. Wir sehen uns in Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich eure Arbeit vorstellen, nämlich \"Anal Positionality: Charakterisierung von Design-Bias in Datensätzen und Modellen\". Diese Arbeit entstand in Zusammenarbeit mit einigen Kollegen der University of Washington und des Allen Institute for AI, insbesondere Sebastian Sante, Ronan Labrasse, Katarina Aranica und Martin Sapp. Beginnen wir also damit, uns vorzustellen, dass Sie für eine Zeitung arbeiten und die Kommentare unter Ihren Artikeln durchsuchen, um toxische Inhalte zu entfernen. Sie könnten sich dann an eine beliebte API wie die Perspective API zur Toxizitätserkennung wenden, und das funktioniert gut, wenn Sie Carl Jones sind, wo die Perspective API toxische Fälle korrekt erkennen kann. Aber das ist nicht wirklich der Fall für Aditya Sharma, wo die Perspective API nicht so empfindlich auf beleidigende Begriffe reagiert, die in indischen Kontexten gebräuchlicher sind. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen Bevölkerungsgruppen feststellen. Design-Bias wie der, den wir gerade gesehen haben, können durch die Positionierung der NLP-Forscher und Modellentwickler entstehen. Positionierung ist schlichtweg die Perspektive, die Menschen aufgrund ihrer demografischen Merkmale, Identitäten und Lebenserfahrungen einnehmen. Es handelt sich um ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Und als Forscher kann die Positionierung den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann. Eine Frage, die sich daher stellen könnte, lautet: Haben Datensätze und Modelle eine Positionierung? Wir wollen nicht behaupten, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können daher bestimmte Positionierungen gegenüber anderen repräsentieren. Vorherige Arbeiten haben einige ansichtliche Hinweise auf Positionierung vorgeschlagen, wie z. B. kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modell-Positionierung. Diese Arbeiten befassen sich jedoch nicht damit, Endbenutzer mit den Datensätzen und Modellen selbst zu vergleichen. Das Studium der Modell- und Datensatz-Positionierung wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozial orientierter werden. Es ist eine Herausforderung, zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind. Um die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir daher die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen. Wir tun dies über unser Framework NL Positionality. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt ist die erneute Annotation von Datensätzen mit vielfältigen Annotatoren. Wir bevorzugen dies gegenüber der Betrachtung der demografischen Merkmale der ursprünglichen Datensatz-Annotatoren, da in der Regel nur wenige Annotatoren jede Instanz annotieren und da demografische Daten selten erfasst und geteilt werden. Daher bevorzugen wir es, Daten erneut zu annotieren, um viele Annotatoren pro Instanz zu erhalten und einen reichen Satz an demografischen Daten zu erhalten. Wir nehmen dann die Annotationen nach demografischen Merkmalen vor und vergleichen sie mit den Modellen und Datensätzen mithilfe eines Pearson-Korrelationskoeffizienten. Unser Framework unterscheidet sich daher von der Literatur über Annotatoreinigkeit dadurch, dass es Endbenutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleicht, anstatt sich nur auf die Annotatoreinigkeit oder die Modellierung der Annotatorendistributionen zu konzentrieren. Unser Framework wird weitgehend durch Lab in the Wild ermöglicht, eine Online-Crowdsourcing-Plattform von unserem HCI-Kooperationspartner. Und Live in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie MTurk, die größtenteils Teilnehmer aus den USA oder Indien haben. Darüber hinaus kann Lab in the Wild weiterhin hochwertige Daten liefern. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Und wie das funktioniert, ist, dass die Teilnehmer eine Situation aus dem Social Chemistry Dataset lesen und dann schreiben, wie sozial akzeptabel eine Situation ist. Anschließend können sie ihre Antworten mit einer KI und anderen vergleichen, um sich weiterhin in der Studie zu engagieren. Wir vergleichen diese Annotationen dann mit Social Chemistry, Delphi und GPT-4. Wir replizieren dann einen sehr ähnlichen Aufbau für den Test zur Toxizitätserkennung und Hassrede. Wir vergleichen diese Annotationen dann mit Social Chemistry, Delphi und GPT-4. Wir replizieren dann einen sehr ähnlichen Aufbau für die Aufgabe zur Toxizitätserkennung und Hassrede, bei der sie eine Instanz aus DynaHate lesen und schreiben, ob sie der Meinung sind, dass es sich um eine Hassrede handelt. Wir vergleichen dann diese Annotationen mit DynaHate, Perspective API, Rewire API, Hate Roberta und GPT-4. Unsere Studie umfasste am Ende über 16.000 Annotationen von über tausend Annotatoren aus 87 Ländern. Wir sind nun besser gerüstet, um zu beantworten, mit wem NLP-Datensätze und -modelle am meisten übereinstimmen. Wir stellen fest, dass es eine Positionierung in NLP gibt. So stellen wir beispielsweise fest, dass Datensätze am meisten mit englischsprachigen Ländern übereinstimmen. Bei der GPT-4-Analyse zur sozialen Akzeptanz stellen wir fest, dass sie am meisten mit konfuzianisch und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass Dyna-Hate ebenfalls am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen außerdem die größte zusätzliche Übereinstimmung mit Menschen fest, die eine Hochschulausbildung haben. Bei der GPT-4-Aufgabe zur sozialen Akzeptanz stellen wir fest, dass sie am meisten mit Personen mit einer Hochschul- oder Hochschulbildung übereinstimmt. Und wir stellen das Gleiche für Dynahate fest, wo es am meisten mit Personen mit einer Hochschulausbildung übereinstimmt. Wenn Modelle und Datensätze jedoch auf bestimmte Bevölkerungsgruppen ausgerichtet sind, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Personen übereinstimmen als mit männlichen und weiblichen Vergleichsgruppen. Wir stellen dies sowohl in der GPT-4-Aufgabe zur sozialen Akzeptanz als auch in der Analyse der DynaHEAT-Aufgabe fest. Angesichts der Tatsache, dass es eine Positionierung in NLP gibt, was können wir dagegen tun? Wir haben daher einige Empfehlungen. Die erste ist, einen Überblick über alle relevanten Designentscheidungen während des gesamten Forschungsprozesses zu führen. Und die zweite ist, NLP-Forschung mit der Brille des Perspektivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb bestimmter Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakane Initiative. Wir möchten betonen, dass inklusives NLP nicht nur darin besteht, dafür zu sorgen, dass alle Technologien für jeden funktionieren. Damit schließt unsere Präsentation ab. Wenn Sie jedoch mehr erfahren möchten, können Sie gerne unseren Dashboard mit den aktuellsten Analyseergebnissen und unserem Artikel einsehen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit zur Auflösung indirekter Bezugsausdrücke für die Entität Auswahl sprechen, in der wir den Alt Entities Korpus vorstellen. Mein Name ist Jawad Hosseini, und dies ist eine Gemeinschaftsarbeit mit Philip Radlinski, Sylvia Parity und Annie Lewis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: Meinten Sie „Easy on Me“ oder „I Got a Feeling“? Hier möchte der Nutzer zwischen einem dieser beiden Songs auswählen. Das Offensichtlichste wäre, eine direkte Referenz zu verwenden, zum Beispiel den Namen des Songs „Easy on Me“ oder seine Position, die erste. Aber manchmal ist ein indirekter Bezug angemessener, um eine natürlichere Konversation zu führen. Dies kann passieren, wenn der Nutzer sich den Namen des Songs nicht erinnert oder die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind, oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Bezüge, zum Beispiel „der neuere Song“ oder „der Song, der nicht energiegeladen ist“. Dies ist ein wichtiges Problem in Konversationssystemen und auch für das Benchmarking des Entitätsverständnisses von LLMs. Uns ist kein öffentlicher Datensatz, ein groß angelegter öffentlicher Datensatz für diese Aufgabe bekannt, daher haben wir einen mithilfe von Crowd-Annotationen zusammengestellt. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unsere Datensatz-Sammelmethodik betont Informalität mithilfe eines Comic-Vervollständigungsaufbaus. Der Comic hat drei Sprechblasen. Unsere Datensatz-Sammelmethodik betont Informalität mithilfe eines Comic-Vervollständigungsaufbaus. Der Comic hat drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnerst du dich an den Song, den wir gestern gehört haben?“ Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: „Meinten Sie 'Easy on Me' oder 'I Got a Feeling'?\" Dies ist die alternative Frage. Und in der dritten Sprechblase verwendet Bob einen indirekten Bezug, um eine dieser Entitäten auszuwählen, zum Beispiel „der neuere Song“. Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts pro Bereich ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage: \"Meinten Sie A oder B\", die wie folgt generiert wird: Wir verwenden immer eine einfache Vorlage: \"Meinten Sie A oder B\", wobei A und B Stichproben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir in der Liste höher kommen, werden die Entitäten ähnlicher zueinander, und es ist in der Regel schwieriger, die Disambiguierung vorzunehmen. Die erste Methode ist zufällig gleichmäßig. Die zweite Methode ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen „Die Rückkehr“. Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich wenn zum Beispiel das gleiche Genre oder der gleiche Künstler, zum Beispiel. Wenn wir diese alternative Frage den Administratoren zeigen, kennen sie den Namen dieser Entitäten, aber nicht unbedingt die Entitäten selbst. Was wir tun, ist, dass wir einige Hintergrundinformationen zu den beiden Entitäten anzeigen. Bei Songs zeigen wir einfach einen Google-Suchlink zu jedem Song und bitten die Annotatoren, mindestens einen Teil jedes Songs anzuhören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song \"Easy on Me\". Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Bei Rezepten zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mithilfe von drei bis fünf indirekten Bezugsausdrücken zu beschreiben, zum Beispiel „der mit der Klaviermusik“. Hier sind einige Beispiele aus unserem Datensatz: zum Beispiel „der ohne Worte“, „nicht der mit dem 12-jährigen Jungen“, „der fiktive“ oder „der aus Aserbaidschan stammt“ usw. Der Alt Entities Korpus hat 6000 alternative Fragen in drei Bereichen und 42000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5x-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf genau dasselbe Hintergrundwissen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92 bis 95 Prozent. Dies ist jedoch unrealistisch. Wenn das Sprachmodell Zugriff auf etwas überlappendes Hintergrundwissen hat, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, zum Beispiel wenn das Sprachmodell das Hintergrundwissen abruft, dann beträgt die Genauigkeit nur 60 Prozent. Es gibt also viel Verbesserungspotenzial. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
