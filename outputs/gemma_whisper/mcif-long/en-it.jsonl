{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando il multi-set tagging e le permutazioni latenti. Questo è un lavoro congiunto con i miei relatori, Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere intesa come l'abilità di un sistema di apprendimento di gestire una ricorsione più profonda e composizioni inedite di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del semantic parsing, il testing per la generalizzazione composizionale potrebbe apparire come questo. Come di consueto, abbiamo un insieme di addestramento di espressioni, in questo caso “la ragazza dormì” e “Mary sapeva che la ragazza dormì”. Queste espressioni sono abbinate a forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard di machine learning, l'insieme di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente inedite. In questo esempio, il modello ha visto una ricorsione meno profonda durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli sequence-to-sequence naive faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate dai colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono destinati a catturare il processo composizionale che mette in relazione le espressioni con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo costoso dal punto di vista computazionale. Tipicamente, questo implica una pre-elaborazione considerevole specifica del formalismo delle forme logiche, ad esempio per gestire i simboli variabili. L'ottenimento degli alberi può anche comportare procedure specializzate di induzione grammaticale. In questo articolo, non utilizziamo alberi, e introduciamo un modello sequence-to-sequence neurale che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output. Per la prima volta, dimostriamo una forte generalizzazione a ricorsioni più profonde senza fare affidamento sugli alberi. Il nostro approccio prevede la predizione dell'output dall'input in due passaggi. Innanzitutto, etichettiamo ogni token di input con un multi-set non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token corretti ma non sono ordinati. Ecco perché, nel secondo passaggio, utilizziamo un altro modello per predire una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per predire una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo. Passiamo da sinistra a destra attraverso l'output e determiniamo quale token multi-set inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso. Quindi, saltiamo al successivo token multi-set per determinare il secondo token nell'output. Lo facciamo in modo simile saltando a un altro token multi-set. Continuiamo questo processo finché ogni token del primo stadio non è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a ricorsioni più profonde. Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo, risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-set proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta con sé la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema del commesso viaggiatore. Lo approssimiamo con una rilassamento continuo, compatibile con la GPU, che ci consente anche di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico. Se desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo \"Marked Personas: Utilizzo di prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici\". Questo lavoro è stato realizzato in collaborazione con Esen Dermusch e Dan Jorofsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su set di dati costruiti manualmente che richiedono molto tempo per essere curati e, inoltre, misurano solitamente solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali sfaccettate possano acuire i pregiudizi e rappresentare luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi più recenti LLM ottimizzati tramite istruzioni sono molto bravi a rispondere alle istruzioni nei prompt. Possiamo quindi chiedere al modello di generare una persona, ovvero una descrizione di un individuo immaginario, utilizzando un prompt come, \"Immagina di essere una donna asiatica, descrivi te stessa\". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità desideriamo in questo prompt. Ecco alcuni esempi di generazione da GPT-4. Immediatamente, vediamo che, sebbene gli output non siano palesemente negativi o tossici nel senso tradizionale del termine, ci sono alcuni schemi interessanti. La donna asiatica è descritta come poco appariscente. La donna mediorientale è definita utilizzando parole come esotica e riferendosi a una regione affascinante. E entrambe le persone di colore fanno riferimento alla loro ascendenza, mentre la persona bianca non presenta nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è quella di generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno fornito questi prompt a soggetti umani, scoprendo che, fornendoli a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali. Inoltre, ciò consente un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte è \"marked words\", ovvero un metodo per identificare le parole che distinguono i gruppi \"marked\" dai gruppi \"unmarked\", su cui elaborerò a breve. Il vantaggio è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico. Quindi, il metodo delle \"marked words\" attinge al concetto sociolinguistico di \"markedness\", che afferma che esiste un default \"unmarked\" e qualsiasi gruppo che differisce da tale default è linguisticamente \"marked\"; ad esempio, la parola \"uomo\" o, scusate, la parola \"guerriero\" è solitamente associata agli uomini, quindi quando si descrive un guerriero che è una donna, di solito si specifica \"uomo guerriero\" e si marca il termine con \"donna\". Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente \"unmarked\", mentre i gruppi marginalizzati sono solitamente \"marked\". Quindi, nel nostro metodo, designiamo innanzitutto quali sono i gruppi \"unmarked\" e \"marked\". Quindi, confrontiamo le persone utilizzando il metodo delle \"fighting words\", che consiste fondamentalmente nell'utilizzare log odds ratios ponderati per... identificare cosa sono i gruppi \"unmarked\" e \"marked\". Quindi, confrontiamo le persone utilizzando il metodo delle \"fighting words\", che consiste fondamentalmente nell'utilizzare log odds ratios ponderati per distinguere le parole principali per ciascun gruppo \"marked\". Ad esempio, per le persone di donne nere, faremmo \"fighting words\" e confronteremmo i log odds ratios sia con le persone bianche che con le persone uomini, perché questi sono due gruppi \"unmarked\" corrispondenti. Adesso, alcuni risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente alla distribuzione delle parole nel lessico, troviamo cose molto diverse. Sebbene le persone generate abbiano tassi molto più alti di parole del lessico, quelle scritte dagli umani hanno una distribuzione di parole molto più ampia, mentre le parole di stereotipo che si trovano nelle persone generate sono solo le parole \"alto\" e \"atletico\". Quindi, praticamente solo quelle positive o almeno non negative. Infatti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto negli slide precedenti. Invece, per farlo, ci rivolgeremo ai risultati del nostro metodo delle \"marked words\" per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi. Innanzitutto, per i gruppi \"marked\", le parole principali includono cose come cultura, tradizione, orgogliosa ed esotica. E queste parole definiscono questi gruppi solo in relazione al loro legame con la loro identità e li distinguono dalla norma bianca. Ciò contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibrante e formosa, che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come piccola e delicata e setosa, che si collega a una lunga storia di ipersexualizzazione delle donne asiatiche, considerate molto docili e sottomesse, e così via. Infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della \"strong Black woman\". E sebbene possa sembrare positivo a prima vista, ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché esercita molta pressione su queste demografie affinché siano resilienti e forti di fronte agli ostacoli sociali. Invece di lavorare effettivamente per cambiare tali ostacoli, esercita pressione su queste persone per superarli, il che porta a esiti sanitari molto negativi per queste persone, tra le altre dannosità. Più in generale, scopriamo che le parole per ciascun gruppo \"marked\" riflettono praticamente solo narrazioni molto essenzializzanti. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare pregiudizi e harms negli LLM. Innanzitutto, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare pregiudizi e harms perché potrebbero esserci molte cose che vengono trascurate se non lo facciamo. Infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi perché, ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a qualche tipo di allineamento dei valori strano ed eccessivo o forse ad alcuni altri metodi anti-stereotipo che stanno dando origine a questi schemi dannosi. Non possiamo proprio fare supposizioni o studiare ulteriormente senza maggiore trasparenza. Grazie mille per aver ascoltato. Buona permanenza ad ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABCeval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato svolto dall'Emory NLP Lab, guidato dal Professor Gino Choi presso l'Emory University, e in collaborazione con Amazon Alexa AI. Quindi, supponiamo che abbiate appena sviluppato un modello di dialogo e vogliate vedere quanto bene si confronta con lo stato dell'arte attuale. La pratica comune è quella di utilizzare una valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni è migliore, oppure di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare molteplici dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare. Un approccio consiste nel chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o di scala Likert esistenti. Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat, o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. APC eval un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello ha successo o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la prassi standard per la valutazione dei modelli di chat su più dimensioni. Dalle nostre analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette comportamentali di ABC eval sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni etichettate in modo doppio. Inoltre, le etichette di ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare. Ad esempio, le etichette eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare. Ad esempio, si può vedere come la misurazione della proporzione di turni con contraddizioni di sé stessi e del partner spiega il 5% e il 10% della qualità della conversazione, mentre le medie valutazioni di coerenza Likert spiegano solo il 4% o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a stepwise. Si può vedere come la combinazione di tutte le metriche di ABC eval spieghi più del 25% della qualità della conversazione, e man mano che si rimuovono le metriche una alla volta, la maggior parte di esse porta a perdere una buona quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega meno della qualità, e un numero minore di queste metriche trasporta informazioni uniche. Queste metriche ABC eval affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione maggiore rispetto ai metodi precedenti. Si può vedere che nei risultati del nostro esperimento restano ancora diverse sfide e sono state quantificate in modo preciso. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro partner circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è tutto il più forte motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli. Speriamo che ABC Eval possa essere utilizzato da altri nel settore come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni. Grazie per averci seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Vasudha e sono una studentessa di dottorato in Informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro, accettato come articolo lungo all'ACL 2023, intitolato \"Transfer Learning for Dissonance Detection\", che affronta la sfida delle classi rare. Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio. In termini semplici, la dissonanza cognitiva si verifica quando due credenze o azioni sono inconsistenti. Ad esempio, una persona afferma: “So che le sigarette potrebbero uccidermi”, e poi dice: “Ho preso un paio di sigarette dopo la riunione”. Questa credenza e questa azione sono inconsistenti e sono in dissonanza. Inoltre, il fatto di dire \"Non credo che potrei mantenere il mio lavoro senza di esse\" giustifica la seconda occorrenza e si instaura una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio, a differenza di altri tipi di relazioni discorsive. Quindi, cosa significa tutto questo? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a monitorare l'andamento delle credenze, dei valori e dei cambiamenti di atteggiamento nella popolazione. Un'elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi individuali e ci aiuta a capire meglio i processi decisionali. Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio incentrato sulla dissonanza, come illustrato dal diagramma di flusso qui presente. I tweet sono stati analizzati utilizzando un parser PDTV, e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può notare, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento di un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia ottenuto risultati poco migliori di quelli ottenibili per caso. Data la bassa occorrenza della dissonanza e l'assenza di set di dati precedenti simili, ci troviamo ad affrontare il problema della rarità assoluta. Per attenuare questo problema, sperimentiamo combinazioni di transfer learning e active learning per annotare in modo da poter raccogliere più campioni di dissonanza con meno cicli di annotazione, riducendo così i costi complessivi di annotazione e migliorando al contempo il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, abbiamo avviato il processo di active learning trasferendo i pesi da attività correlate. Trasferiamo da due attività diverse: la classificazione dello stance sulla dissonanza indipendente dall'argomento, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono d'accordo o in disaccordo, indipendentemente dall'argomento, definito \"dibattito\" qui, e la classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza, e le chiamiamo CEE qui. Abbiamo scoperto che, trasferendo i pesi, le prestazioni zero-shot sul dataset annotato sono già notevolmente migliori rispetto a quelle ottenibili per caso, con un AUC di 0,62 come migliore risultato. Inoltre, aggiornando iterativamente il modello addestrandolo per un ciclo di active learning e annotazioni, si accumulano cumulativamente tutti i dati raccolti dalle annotazioni active finora, mentre l'aggiornamento iterativo aggiorna il modello addestrandolo sull'ultimo set di dati raccolto. Tra le diverse strategie, abbiamo riscontrato che l'accumulo ha funzionato meglio o pari al miglioramento iterativo in tutti i casi. Successivamente, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara per selezionare principalmente esempi che sono molto probabilmente dissonanti secondo il modello corrente in ogni ciclo di AL. Lo confrontiamo con altre strategie all'avanguardia, sebbene la differenza sia minima. Si noti che le prestazioni sono significativamente inferiori per il metodo casuale. In ulteriori cicli di AL con le due strategie migliori, abbiamo migliorato l'AUC della classificazione della dissonanza a 0,75, che è la migliore prestazione che abbiamo ottenuto finora in questo compito. Verifichiamo inoltre la fattibilità di ciascuna strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che la strategia PRC (Probability of Rare Class) ha la percentuale più alta di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori hanno riscontrato che gli esempi sono difficili. In sintesi, scopriamo che la strategia PRC è una strategia AL semplice per l'acquisizione di classi rare e l'avvio a freddo di AL con attività di transfer learning adeguatamente progettate può aiutare significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni active in-domain traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro codice, al nostro set di dati e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Akshata e oggi il mio co-autore Martin e io presentiamo il nostro lavoro, The Kipma Steps, che valuta l’integrazione della conoscenza da molteplici fonti. Questo lavoro è una collaborazione tra McGill University, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-training, e la conoscenza fornita negli input al momento dell'inferenza. Recenti studi su compiti come la question answering dimostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale richiede spesso conoscenze fornite anche al momento dell'inferenza. Ad esempio, nella frase, \"John ha visto il presidente appena eletto in TV.\" I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono sapere in modo affidabile chi sia John, l'entità specifica di questo evento, o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuto il pre-training. Pertanto, modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza fornita al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di coreference resolution progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il dataset con partecipanti a uno studio umano e stabiliamo modelli di coreference resolution. Ecco un esempio dal nostro dataset. “Servin è un giudice. Kia è un panettiere. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi.” Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Innanzitutto, conoscenza specifica dell'entità, come \"Servin è un giudice\". E in secondo luogo, conoscenza di contesto, come \"i giudici decidono casi in tribunale\". Generalmente, la conoscenza di contesto viene appresa durante il pre-training di modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza. Variamo la disponibilità di questi due tipi di informazioni in modo che possano essere trovate in una singola fonte o in molteplici fonti. Abbiamo definito tre impostazioni di KITMOS. Innanzitutto, abbiamo l'impostazione tipica, background-pretrain, in cui la conoscenza di contesto è disponibile al momento del pre-training. In secondo luogo, c’è l’impostazione background-both, in cui la conoscenza di contesto è disponibile sia al momento del pre-training che al momento dell'inferenza. Infine, l’impostazione background inference, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di contesto necessaria per risolvere un compito non fa parte dei dati pre-addestrati dei modelli, ad esempio perché nuove professioni si sono sviluppate da quando è avvenuto il pre-training. Ecco un esempio di come controlliamo la disponibilità dei fatti e delle fonti vere. Nell'impostazione background pre-trained, assumiamo che la conoscenza di contesto \"i politici cercano seggi eletti nel governo\" sia contenuta nei parametri pre-addestrati. E nel contesto 3-inch-time, forniamo la conoscenza specifica dell'entità che Chichester è un politico. Nell'impostazione background both, forniamo inoltre sia la conoscenza specifica che la conoscenza di contesto sui politici nel contesto di inferenza. Nell'impostazione background inference, forniamo l'occupazione fittizia Meritur invece di politico, perché è improbabile che Meritur sia contenuta nei parametri pre-addestrati. Valutiamo il dataset sia con partecipanti a uno studio umano che con modelli di coreference resolution consolidati. In questa figura mostriamo i risultati dei modelli con le prestazioni migliori nella variante più difficile dell'impostazione background pre-trained. Senza un addestramento specifico per il compito su KITMOS, entrambi i modelli non funzionano bene. Quando sono addestrati su KITMOS, tuttavia, sia C2F che BFQF ottengono risultati significativamente migliori rispetto alla scelta casuale. Ciò suggerisce che, quando vengono addestrati su dataset generali di coreference resolution, i modelli imparano a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMOS, dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le prestazioni migliori non riescono ad integrare in modo affidabile la conoscenza di contesto fornita solo al momento dell'inferenza. Per riassumere i punti chiave del nostro articolo. Molti modelli di coreference resolution sembrano incapaci di ragionare sulla conoscenza da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da molteplici fonti. Ciononostante, anche i modelli con le prestazioni migliori sembrano avere difficoltà ad integrare in modo affidabile la conoscenza di contesto presentata solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e controllate il dataset su Code su GitHub. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sara Papi, dell'Università di Trento e della Fondazione Bruno Kessler, e introdurrò brevemente l'articolo \"Attention as a Guide for Simultaneous Speech Translation\", un lavoro congiunto con Matteo Negri e Marco Turchi. Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato o SimulST è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica. E quali sono i problemi dei modelli SimulST attuali? Solitamente vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, che coinvolgono obiettivi di ottimizzazione diversi, e l'addestramento e la manutenzione di diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrando un modello con una latenza media di 1 secondo e un altro con 2 secondi e così via. Quindi, qual è la nostra soluzione? Innanzitutto, utilizzare modelli SD offline già esistenti senza riaddestramento o adottando architetture specifiche per SimulSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di cross-attenzione. E potete vedere un esempio a destra. La nostra soluzione è proporre EDAT o attenzione encoder-decoder, una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè la sua somma è inferiore a una certa soglia alpha, verso meno lambda frame del parlato, il che significa che l'informazione ricevuta è sufficientemente stabile. Ad esempio, se riceviamo un frammento del parlato contenente \"I'm going to talk about\" e il nostro modello prevede la traduzione in tedesco, esamineremo i pesi di cross-attenzione e vedremo che le prime due parole puntano ai frame del parlato ricevuti più all'inizio, mentre l'ultima parola punta agli ultimi frame del parlato ricevuti, ovvero lambda frame del parlato. Ciò significa che le prime due parole verranno emesse, mentre poiché la somma dei pesi di cross-attenzione è superiore a una certa soglia alpha, non emetteremo l'ultima parola e aspetteremo un altro frammento del parlato. Se proseguiamo e riceviamo un altro frammento del parlato e il nostro modello prevede altre tre parole, esamineremo i pesi di cross-attenzione e vedremo che nessuna parola punta agli ultimi lambda frame del parlato. Ciò significa che queste tre parole verranno emesse. Se esaminiamo i risultati principali di un punto, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il ritardo medio, che è la misura della latenza, e consideriamo anche il ritardo medio consapevole della computazione, che tiene conto dei tempi di calcolo del modello per prevedere l'output. Quindi, vogliamo che le nostre curve siano il più alte possibile su questo grafico, ma anche che siano spostate a sinistra. E confrontiamo con strategie adeguate che vengono applicate anche ai modelli offline, ovvero la strategia wet-key e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la traduzione simultanea del parlato. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco e vediamo che ADDOUT supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. E vediamo anche che se consideriamo il tempo trascorso effettivo o il tempo consapevole della computazione, ADAT è la strategia più veloce. Se desiderate scoprire maggiori risultati, leggete il nostro articolo e abbiamo anche reso open source il codice e i modelli. Se desiderate scoprire maggiori risultati, leggete il nostro articolo. Abbiamo anche reso open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Zhu Heng. Oggi presenterò il nostro articolo dal titolo: \"I tagger di entità nominate di kernel 2003 funzionano ancora bene nel 2023?\". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito del riconoscimento di entità nominate, o NER. Abbiamo osservato che i modelli hanno utilizzato Kano 2003 per sviluppare il NER per quasi 20 anni, e questo solleva naturalmente diversi problemi. Innanzitutto, questi modelli riescono a generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset Kano++. Questo è un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e che abbiamo annotato utilizzando le stesse linee guida di annotazione di Cono2003. Abbiamo quindi ottimizzato più di 20 modelli su Cono2003. Li abbiamo valutati sia sul Con che su Kano, calcolando l'F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalizzano di solito meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito modelli più grandi portano a una migliore generalizzazione. E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un compito downstream. Anche qui, abbiamo scoperto che un maggior numero di esempi di fine-tuning porta effettivamente a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'overfitting adattivo, ovvero un overfitting causato dal riutilizzo dello stesso set di test ripetutamente, e questo si manifesta solitamente come una diminuzione dei rendimenti sul nuovo set di test. La seconda ipotesi è il temporal drift, ovvero il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di training e i dati di test. Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la retta di migliore adattamento rossa ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su CONO 2003 si traduce in più di un'unità di miglioramento su Kano++, il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che l'overfitting adattivo in questo caso non viene osservato. Allora, cosa ne è del temporal drift? Per il temporal drift, e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il temporal drift. La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, di una dimensione del modello maggiore e di un maggior numero di esempi di fine-tuning. E questi vanno di pari passo. Non possiamo avere solo un ingrediente ma trascurare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dal temporal drift e, in modo un po' sorprendente, non è causato dall'overfitting adattivo, anche se Carnal 2003 è stato utilizzato per oltre 20 anni. Ritornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger di Carnot 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un deciso sì. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, benvenuti alla nostra presentazione di d.plain, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensibilità per un gruppo target specifico, come persone con difficoltà di lettura o parlanti non nativi. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testo, ad esempio di documenti o frasi. Nell'esempio qui, potete vedere una coppia di frasi allineata parallelamente di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, la clausolazione, il riordinamento della clausolazione o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus dplane. Negli ultimi anni, ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di tassificazione. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nel loro allineamento. Pertanto, proponiamo il nostro nuovo corpus dplane, che è suddiviso in due sottocorpora, dplane-apa e dplane-web. Dplane-apa si basa su testi di utilizzo. Nella plain APA, abbiamo allineato manualmente 483 documenti, con un totale di circa 30.000-13.000 coppie di frasi parallele. Per DeepLaneWeb, questo corpus include diversi domini e allineiamo anch'essi tutti questi 750 documenti, in parte manualmente e in parte con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio sul tipo di semplificazione. Come potete vedere, i testi biblici sono semplificati in modo molto più marcato rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue a tutti i livelli, per quanto riguarda, ad esempio, la semplificazione lessicale, la semplificazione strutturale o il livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus De Plplane ha un'elevata priorità di diverse trasformazioni di semplificazione. Ad esempio, nel corpus d.plane API, abbiamo molti più riordinamenti e aggiunte di parole rispetto a quelli che abbiamo nel corpus d.plane web. D'altra parte, nel corpus web, abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro dataset D-plane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi in due documenti paralleli che hanno la stessa lingua, lo stesso contenuto, ma sono a livelli di complessità diversi. Ora che abbiamo il nostro dataset dplane, con frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti e abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo di math align e potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo presentato nel nostro documento è un caso di semplificazione automatica del testo mediante l'affinamento dei modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo affinato due modelli diversi. Abbiamo affinato long-impart per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete consultare maggiori dettagli sui punteggi e sulle metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo affinamento di base poteva produrre o ottenere punteggi migliori rispetto ai punteggi di riferimento e abbiamo proposto questi risultati come un benchmark, un benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro, \"Distillazione della Conoscenza Script da Modelli Linguistici di Grandi Dimensioni per la Pianificazione del Linguaggio Vincolato\". Nella vita di tutti i giorni, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo-passo sotto forma di script garantiti. Tuttavia, i lavori precedenti si sono concentrati principalmente sulla pianificazione in astratto. Un buon pianificatore dovrebbe scrivere script per la prima volta definendo gli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli sfaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione del linguaggio vincolato dei modelli linguistici di grandi dimensioni. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire tali obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione di dati con l'intervento umano, utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai modelli linguistici di grandi dimensioni. Questa tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli linguistici di grandi dimensioni ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Successivamente, conduciamo un'analisi dettagliata per indagare sul motivo per cui i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma non si può garantire la fedeltà ai vincoli. Abbiamo approfondito categorie di vincoli più granulari, a seconda del funzionamento. La heatmap nella figura mostra che le prestazioni di pianificazione delle TPD istruttive variano notevolmente per categorie diverse. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli con log di poca entità presenta un'alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di Z-filter di sovragenerazione per migliorare la qualità della generazione. Per prima cosa, mostriamo i tipi di vincoli con esempi per Instruct GPT e otteniamo obiettivi specifici in base agli obiettivi astratti di partenza. Successivamente, Instruct GPT sovragenera script chiave per obiettivi specifici. Quindi, viene sviluppato un modello di filtro per selezionare gli script fattibili. Convertiamo gli script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità del coseno e i punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Conserviamo solo lo script se l'obiettivo target ottiene il punteggio più alto nell'insieme di obiettivi. Con il nostro metodo, InstructZBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione, sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale abilitare la capacità di pianificazione di modelli più piccoli e specializzati. La creazione di un dataset è un passo essenziale per raggiungere questo obiettivo. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale di dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione del linguaggio vincolato dai modelli linguistici di grandi dimensioni. Applichiamo il nostro metodo per creare un dataset di pianificazione del linguaggio vincolato, denominato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di convalida e test, abbiamo chiesto a lavoratori provenienti da cloud di revisionare campioni errati. Questa figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un'elevata approvazione negli obiettivi specifici generati. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato. Abbiamo scoperto che TFI di Antune sul tasso di costo può generare la radice quadrata di 0. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato. Abbiamo scoperto che la funzione T-file su CodeScript può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono supportare i modelli più grandi quando vengono addestrati correttamente su dataset adatti. In sintesi, abbiamo definito il problema della pianificazione del linguaggio vincolato. Valutiamo la capacità di pianificazione del linguaggio vincolato dei modelli linguistici di grandi dimensioni e sviluppiamo un metodo di filtro di sovragenerazione per la ricerca sulla pianificazione del linguaggio. Grazie per il vostro tempo. Potete trovare maggiori dettagli su CodeScript nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrac e vi presenterò i nostri lavori su Dr. BERT, un modello pre-addestrato robusto in francese per il dominio biomedico e clinico. In questa presentazione, inizieremo parlando di modellazione del linguaggio nel settore sanitario. Successivamente, presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese, chiamato Dr. BERT, basato su Roberta e addestrato su NACHOS, un dataset di dati medici estrapolati dal web. Abbiamo anche introdotto un confronto del modello con diverse configurazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 task biomedici e clinici a valle in francese. Infine, concludiamo le sperimentazioni e vi forniamo maggiori dettagli su come accedere ai modelli. task biomedici e clinici a valle in francese. Infine, concludiamo le sperimentazioni e vi forniamo maggiori dettagli su come accedere ai modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere task di elaborazione del linguaggio naturale e offre un notevole guadagno di prestazioni rispetto ai metodi statici e contestualizzati storici come Word2Vec, fastText o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert, e ad altri domini come il biomedico con Permit-BERT e Bio-BERT, e sul clinico con Clinical-BERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso basati su pre-addestramento continuo a causa della mancanza di dati specifici del dominio. Tuttavia, il francese non aveva modelli open-source per il settore biomedico fino ad ora. Ci siamo quindi posti delle domande su quali fossero le fonti di dati più appropriate per un'ampia gamma di utilizzi e se i dati attuali fossero una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo Dr. BERT con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario che ci ospita. Infine, ci siamo chiesti quanta quantità di dati fosse necessaria per addestrare un modello specializzato su dati francesi. 4 GB, 8 e 4 GB di RAM? Una prima versione di Schubert, un modello clinico, con 4 GB di frasi tratte da note cliniche. E una versione finale di Schubert, con una combinazione di un sottoinsieme di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di Camembert e addestrato su un sottoinsieme di 4 GB di NACHOS. Un altro sempre basato su Camembert ma addestrato su quattro gigabyte Permit-BERT, Bio-BERT e Clinical-BERT. La valutazione evidenzia che... 38 GB, Camembert Oscar 4 GB, Camembert CCnet 4 GB, PumaBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli funzionano meglio sui task con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori. Complessivamente, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei task. Tuttavia, i nostri esperimenti sul pre-addestramento continuo, utilizzando i pesi e il tokenizer di Permit-BERT, addestrato su un sottoinsieme di 4 GB di NACHOS, mostrano risultati comparabili a quelli ottenuti con Dr.BERT 4 GB da zero, cosa che non si verifica per il modello basato sui pesi e il tokenizer di Camembert, che soffre di problemi di stabilità. Infine, in conclusione, il nostro sistema proprietario offre prestazioni migliori su 9 degli 11 task \"don't trim\" e supera globalmente i risultati del modello generico. In conclusione, il nostro sistema proprietario offre prestazioni migliori su 9 degli 11 task \"don't trim\", e supera globalmente i risultati del modello generico, qui Camembert. Osserviamo anche che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su UGIMFACE e tutti gli script di addestramento sono nel nostro repository GitHub. Quindi, grazie per questa presentazione e siamo lieti di ricevere commenti durante la sessione poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiangbin, dottorando all'Università di Washington. Oggi vi presento il nostro lavoro che parte dai dati di pre-addestramento, passando per i modelli linguistici fino alle attività a valle, tracciando le tracce dei pregiudizi politici che portano a modelli di NLP ingiusti. Quindi, i modelli linguistici vengono addestrati su dati di web scraping su larga scala. Le notizie politiche sono ben rappresentate nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento del modello linguistico. Ciò ha creato un beneficio misto per le applicazioni dei modelli linguistici. Quindi, da un lato, sono stati in grado di apprendere da diverse prospettive, il che celebra la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni a valle. A tal fine, proponiamo di indagare sulla pipeline di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici alle attività a valle, chiedendo specificamente le seguenti domande. Prima di tutto, come valutiamo la tendenza politica dei modelli linguistici e quale ruolo potrebbe avere il dato di riferimento su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse tendenze politiche sulle attività a valle e ciò potrebbe portare a problemi di equità nelle applicazioni di NLP? In particolare, abbiamo prima proposto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando i questionari politici, come il test della bussola politica. Ciò ci assicura di poter eseguire una valutazione automatica ben fondata sulla letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che, innanzitutto, i modelli linguistici hanno tendenze politiche variabili. Occupano tutti e quattro i quadranti sulla bussola politica. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale tra tutti, e GPT theories sono generalmente più socialmente liberali rispetto a BERT theory e alle sue varianti. In secondo luogo, miriamo a indagare in quale misura i pregiudizi politici dei modelli linguistici provengono effettivamente dai dati di addestramento. Pertanto, conduciamo un esperimento controllato pre-addestrando ulteriormente i checkpoint del modello linguistico su sei diversi corpora partigiani separati in notizie e social media, ulteriormente divisi nelle loro tendenze politiche. Pre-addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta, ulteriormente perfezionata, ulteriormente addestrata sul corpus Reddit di sinistra, possiamo vedere un notevole spostamento liberale in termini dei suoi pregiudizi politici. E cerchiamo anche di indagare se i modelli linguistici possono cogliere la polarizzazione che è prevalente nella nostra società moderna. Pertanto, dividiamo i corpora di pre-addestramento nel periodo precedente e successivo al 45° presidente degli Stati Uniti e pre-addestriamo separatamente i modelli linguistici su entrambi i diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente una tendenza politica più lontana dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società. Ultimo ma non meno importante, valutiamo i modelli linguistici con diverse tendenze politiche sul rilevamento dell'incitamento all'odio e sul rilevamento di notizie false per le applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Vediamo che, se investighiamo le prestazioni per categoria, ovvero se separiamo le prestazioni in diverse demografie o significato politico delle notizie, possiamo vedere un modello secondo cui, ad esempio, per il rilevamento dell'incitamento all'odio, i modelli linguistici di sinistra sono migliori nel rilevare l'incitamento all'odio che prende di mira gruppi socialmente minoritari, tuttavia sono peggiori nel rilevare l'incitamento all'odio che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare l'incitamento all'odio che prende di mira bianchi e uomini, tuttavia peggiori nel rilevare l'incitamento all'odio che prende di mira neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dalle loro opposte tendenze politiche e viceversa. Questo, mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diverse tendenze politiche forniscono effettivamente previsioni diverse su esempi di incitamento all'odio e disinformazione in base alla loro categoria sociale. Ci sono un sacco di altri esempi nell'appendice per evidenziare ulteriormente le diverse previsioni su esempi di incitamento all'odio e disinformazione in base alla loro categoria sociale. Questo indica che c'è un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra dovessero essere perfezionati sull'incitamento all'odio o sulla disinformazione o su qualsiasi altra cosa e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate, e l'incitamento all'odio che prende di mira i gruppi minoritari potrebbe semplicemente dilagare senza alcun controllo. Questo ha suonato l'allarme affinché riconosciamo e affrontiamo i problemi di equità derivanti dalle tendenze politiche dei modelli linguistici. Quindi, un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici alle attività a valle, creando in definitiva problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere mantenuto nei dati di addestramento del modello linguistico. È un po' come il problema del carrello elettrico. Okay, ottimo. Credo che sia più o meno tutto per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto alla nostra presentazione del paper per ACL 2023, \"Language Model Acceptability Judgements Are Not Always Robust to Context\". Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams. Quindi, in questo lavoro, riprendiamo il paradigma delle coppie minime. In sostanza, il paradigma delle coppie minime valuta i modelli linguistici in base a giudizi di accettabilità, che possono includere anche la grammaticalità, come in BLIMP, sintassi, GYM, o l'accettabilità in termini di stereotipi, come le coppie di Krauss. E in questo paradigma delle coppie minime, il modo tipico per valutare i modelli linguistici è quello di mostrare una frase accettabile o una frase grammaticale, e poi mostrare una frase inaccettabile o una frase non grammaticale. E l'intero processo del modello assegna fondamentalmente una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe. Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno elaborando finestre di contesto sempre più lunghe. Quindi, riprendiamo i dataset stessi e ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Ad esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal dataset BLIMP, nel caso dell'Isola Adgiuntiva. E quello che facciamo è ricreare sequenze più lunghe che sono accettabili e che hanno la stessa struttura grammaticale, estraendo frasi grammaticali dall'Isola Adgiuntiva e aggiungendole come prefisso sia alla query accettabile che alla query inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa struttura e questo potrebbe essere utilizzato per testare l'accettabilità del modello. Possiamo anche fare la stessa cosa scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di discrepanza. Quindi, qui le frasi provengono ancora da dataset rilevanti, ma non dallo stesso dataset con cui si sta valutando. Possiamo fare la stessa cosa per un caso di accettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, un contesto completamente irrilevante per la frase che stiamo esaminando. Quindi, come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query corrente. E lì troviamo che i giudizi MPP sono per lo più robusti per una lunghezza arbitraria del contesto. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E qui vediamo nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset BLIMP o Syntax Gym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Ma quando si corrisponde la struttura, cioè quando si aggiungono prefissi accettabili o inaccettabili, ma quando si scelgono le frasi dalla stessa fenomenologia in BLIMP-person-text-GYM, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora questo, questo effetto aumenta in modo significativo con la lunghezza del contesto e ciò probabilmente influenzerà i modelli linguistici più recenti che hanno finestre di contesto ampie. Perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico? Abbiamo effettuato una serie di analisi in cui abbiamo cercato di comporre la frase di input preservando la struttura rilevante, ma aggiungendo rumore all'input. Dopo aver eseguito diverse di queste perturbazioni, scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare al modello la sua tendenza in termini di come mostra il giudizio MPP. Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti che vengono condivise tra le frasi. E la valutazione MPP, nel modo in cui la facciamo attualmente con input brevi e a singola frase, potrebbe non catturare appieno la conoscenza astratta del modello linguistico all'interno della finestra di contesto. Vi invitiamo a leggere il nostro paper per maggiori dettagli sui nostri esperimenti. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Dawei, dottorando all'Università del Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro, \"Weaker than you think\", un'analisi critica dell'apprendimento con supervisione debole. Si tratta di un lavoro congiunto con Xiao Yusheng, Mario Smusbach, Gia Steffen e Dietrich Klackow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole. Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore etichettato e non generalizzano. Nell'apprendimento con supervisione debole, vengono proposti algoritmi di training per addestrare robustamente le reti neurali in presenza di tale rumore di etichettatura, in modo che i modelli addestrati generalizzino comunque bene. In recenti lavori in WSL, dove WSL sta per apprendimento con supervisione debole, un'affermazione comune è che le persone dicono di aver addestrato modelli solo con i dati di etichettatura debole e di aver ottenuto buone prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un aspetto da considerare, ovvero tre domande di ricerca. Innanzitutto, abbiamo bisogno di dati di validazione puliti? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro, e i nostri risultati sono i seguenti. Innanzitutto, scopriamo che, in modo interessante, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, si verifica un calo significativo delle prestazioni. Come mostrato in questa figura, se non sono presenti campioni di validazione puliti, i modelli addestrati non riescono a generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente e che il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti contribuirà a migliorare le prestazioni degli approcci WSL, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere buone prestazioni. Ma non è finita qui, perché se in ogni caso decidiamo di accedere a campioni puliti, l'addestramento su di essi raggiungerà persino prestazioni migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente ai dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come si può vedere, se abbiamo 10 campioni per classe, il fine-tuning inizia a superare gli approcci WSL. Infine, il miglioramento delle prestazioni rivendicato negli approcci WSL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come si può vedere dalle figure, il modello Van Linden, denominato inizialmente W, sottoperforma rispetto a metodi WSL più complessi come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, FTW funziona altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni annotati manualmente e puliti per funzionare correttamente. Il loro guadagno di prestazioni e la loro praticità sono fortemente sovrastimati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Innanzitutto, riportare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è stata eseguita su campioni di validazione puliti. In secondo luogo, gli approcci WSL dovrebbero essere confrontati con baseline di few-shot learning, come il lavoro su campioni puliti. Terzo, il fine-tuning continuo è una baseline semplice ma efficace che dovrebbe essere considerata in futuro nel lavoro in WSL. Infine, abbiamo reso il nostro codice open source. Potete trovarlo tramite il codice QR su questo slide. Non esitate a dare un'occhiata. Grazie e godetevi la conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo David Vilar e fornirò una breve panoramica dell'articolo \"Grunting Parm from Translation\", valutando strategie e prestazioni. Questo è un lavoro congiunto con i miei colleghi di Google Translate. Parm è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato lo scorso anno, nel 2022. È stato addestrato su un'ampia raccolta di testo composta da 780 miliardi di token. Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo il primo studio sistematico del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico. Confrontiamo due sistemi all'avanguardia, quindi i sistemi con le migliori prestazioni o la valutazione WMT. Utilizziamo metriche IM neurali all'avanguardia e mostriamo anche i risultati della valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha una grande influenza sulle prestazioni degli LLM per la traduzione. Come possiamo vedere in un semplice esperimento in cui utilizziamo il prompting one-shot e forniamo due prompt diversi per ogni frase. Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un blur point. E questo può arrivare, in casi estremi, fino a 40 blur point. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi, in cui indichiamo semplicemente ogni frase che forniamo al sistema con la lingua in cui è scritta. Quindi, in questo esempio in cui eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con \"tedesco:\" e le traduzioni in inglese con \"inglese:\". Abbiamo notato che la forma effettiva del prompting non ha una grande influenza nel caso del prompting a più colpi. È cruciale per il prompting zero e one-shot, e quando passiamo, come nel nostro caso, al prompting a cinque colpi, c'è quasi nessuna differenza rispetto alla forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi è importante. Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi e i risultati mostrano prestazioni migliori quando si utilizzano i dati di sviluppo. Ciononostante, i sistemi all'avanguardia specializzati hanno un vantaggio sostanziale rispetto alle traduzioni di Palm, ma Palm si avvicina a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo tratto dall'analisi delle email che abbiamo eseguito utilizzando il framework MQM è che la fluidità di Palm è paragonabile a quella dei sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza. In particolare, gli errori più comuni sono errori di omissione. Sembra quindi che Palm scelga a volte di produrre una traduzione dal suono migliore omettendo parti della frase di origine che sono animate nella traduzione. Tuttavia, la categoria dello stile goffo per Parm è inferiore rispetto ai sistemi all'avanguardia, il che è un ulteriore segnale che Parm fornisce un output davvero fluido ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, vi invitiamo alla presentazione completa dell'articolo. Grazie molto."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jingwei Yi, sono dell'Università di Scienza e Tecnologia della Cina. È un piacere presentare un breve video promozionale su un documento intitolato “Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark”. Introduciamo innanzitutto il contesto relativo agli embedding e ai servizi. Attualmente, i modelli linguistici di grandi dimensioni come GPT, LAMA, PALM eccellono nella comprensione e nella generazione del linguaggio naturale. L'Embedding as a Service è uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere in varie attività di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, studi recenti hanno dimostrato che un attaccante può rubare il modello apprendendo dagli embedding e fornendo servizi simili. Pertanto, è necessario proteggere il copyright degli embedding as a service. Per proteggere il copyright degli embedding as a service, una delle soluzioni consiste nell'incorporare un watermark nel servizio del fornitore e nel rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile agli embedding as a service. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere sufficientemente discreto in modo che l'attaccante non lo rilevi o possa rimuoverlo facilmente. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili agli embedding as a service o mancano di trasferibilità. Analizziamo i dettagli del nostro EmbeddingMarker. EmbeddingMarker contiene due fasi principali: l'iniezione del watermark e la verifica del copyright. Prima di queste fasi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il fornitore possa raccogliere un corpus di testo generale e contarne la frequenza delle parole. Durante l'iniezione del watermark, definiamo innanzitutto un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è la somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo innanzitutto un backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Quindi il fornitore richiede embedding dal servizio del ladro utilizzando il dataset. Si calcolano la similarità coseno e L2 tra l'embedding richiesto e l'embedding target. Si calcola la differenza di similarità tra il dataset benigno e il dataset backdoor, definita come delta coseno e delta L2. Contestualmente, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica. Conduciamo esperimenti su quattro dataset, AGnews, Mind, SSD2 ed Eraspam. Assumiamo che il fornitore applichi il dataset Wikitext per contare la frequenza delle parole. I risultati su quattro dataset dimostrano che il nostro watermark incorporato può ottenere ottime prestazioni di rilevamento mantenendo un'ottima utilità per le attività a valle. Validiamo anche la discrezione dell'embedding fornito visualizzando gli embedding delle frasi su quattro dataset tramite PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding backdoor e quelli normali. Questo è tutto, grazie. Siamo lieti di discutere con voi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ying, e io e il mio collega Zhiyang presenteremo le nostre ricerche su Multi-Improvement, Miglioramento dell’Apprendimento Seriale Multi-Modale tramite Instruction Tuning. Con i progressi dei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'instruction tuning consente ai modelli linguistici di grandi dimensioni di eseguire attività inedite in modalità zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si concentra sul miglioramento delle prestazioni zero-shot su attività esclusivamente linguistiche, mentre le attività di computer vision e multimodali sono state trascurate. Pertanto, in questo lavoro, vogliamo indagare se l'instruction tuning sui modelli pre-addestrati multimodali può effettivamente migliorare la generalizzazione a compiti multimodali inediti. Inoltre, al momento delle nostre ricerche, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra l'NLP e il multimodale. Esistono più di 1.600 attività di istruzioni esclusivamente linguistiche. Tuttavia, non esiste un dataset di attività di istruzioni multimodali su larga scala disponibile pubblicamente. Questo ci ha motivati a costruire un dataset di instruction tuning multimodale. Qui presentiamo MultiInstruct, il primo benchmark dataset di instruction tuning multimodale composto da 62 diverse attività multimodali che coprono 10 categorie. Queste attività derivano da 21 dataset open source esistenti e ciascuna attività è dotata di 5 istruzioni scritte da esperti. Per indagare sull'instruction tuning multimodale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per linguaggio, token immagine e le coordinate di un bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset multi-instruct. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutte le attività in un formato unificato sequence-to-sequence, in cui il testo di input, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Bene, ora parlerò dell'instruction tuning multimodale. Quindi, per il dataset di training, utilizziamo 53 attività da 9 gruppi per l'addestramento, e campioniamo 10.000 istanze per attività. Per il test, riserviamo l'intero gruppo di ragionamento di buon senso per il test e selezioniamo cinque attività aggiuntive dal gruppo VQA e miscellaneous. Utilizziamo tutte le istanze nella split di test per ciascuna attività. Inoltre, campioniamo casualmente 20 attività dalla split di test di istruzioni naturali come attività inedite per l'NLP. Quindi, utilizziamo un modello OFA large pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione. Durante i test per ciascuna attività, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento. Segnaliamo la media e il massimo delle prestazioni e la deviazione standard delle prestazioni in tutti e cinque gli esperimenti. Se l'attività è un compito di classificazione multi-modale, segnaliamo l'accuratezza. Se è un compito di generazione multi-modale, segnaliamo la radice L. Per un compito RP, segnaliamo la radice jl, introduciamo anche una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre in modo coerente gli stessi output per la stessa attività indipendentemente dalla leggera variazione nella formulazione dell'istruzione. Qui sono i nostri risultati principali. Come possiamo vedere, l'instruction tuning può migliorare significativamente le prestazioni di OFA su attività multi-modali di scena. Inoltre, il transfer learning da dataset di istruzioni naturali può avvantaggiare l'instruction tuning. Qui possiamo vedere che man mano che aumenta il numero di attività, il modello raggiunge prestazioni e nel contempo riduce la sensibilità. Abbiamo anche fatto un esperimento, abbiamo utilizzato una istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre significativamente la sua sensibilità. Questo dimostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Possiamo anche vedere che il transfer learning da dataset di istruzioni naturali può consentire a OFA di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il transfer learning da dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset Nitro Instruct. In generale, proponiamo il primo dataset di instruction tuning multi-modale su larga scala. Miglioriamo significativamente la capacità zero-shot di OFA, ed esploriamo diverse tecniche di transfer learning e ne mostriamo i vantaggi. Progettiamo una nuova metrica chiamata sensibilità. Quindi, un'altra cosa, stiamo raccogliendo un dataset di instruction tuning multimodale molto più ampio con circa 150 ulteriori varianti di attività linguistiche e lo rilasceremo presto. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Yusheng Zhang e sono della Penn State University. Oggi presenterò il nostro lavoro, \"Cross-lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". L'analisi semantica è un compito volto alla costruzione di rappresentazioni semantiche di query utente come SQL e calcolo lambda. L'analisi semantica cross-linguale è il compito di tradurre query in molteplici lingue naturali in molteplici rappresentazioni semantiche. Come illustrato in questa figura, abbiamo bisogno di tradurre la query in molteplici lingue naturali utilizzando modelli neurali, in SQL, Lambda, FunQL, ecc. I modelli esistenti di analisi semantica cross-linguale sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio, si riscontrano lacune di copertura in alcune lingue naturali. Il cinese è assente e, a causa della copertura limitata su alcune rappresentazioni minimali, il calcolo lambda è assente o viene valutato solo su determinati modelli neurali. Ad esempio, esiste un solo modello per valutarli. A tal fine, proponiamo Exampler. Forniamo un dataset uniforme, Exampler, per l'analisi semantica cross-linguale in molteplici lingue naturali e rappresentazioni semantiche. Contiene nove dataset in diversi domini, cinque compiti di analisi semantica, otto rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è la traduzione test. Utilizziamo l'API di Google Translate per tradurre dalla lingua sorgente alla lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese su query in inglese. Durante l'inferenza, traduciamo la query in tedesco utilizzando l'API in inglese e poi utilizziamo il modello addestrato per predire il SQL. Testiamo anche un modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione. Ad esempio, tedesco-tedesco o inglese-inglese. Testiamo inoltre un'impostazione few-shot monolingue addestrando modelli monolingui con solo il 10% dei dati di addestramento. Testiamo anche un modello multilingue, che addestriamo con un unico modello multilingue per tutte le lingue. Ad esempio, raggruppiamo query in tedesco, inglese e cinese per addestrare un modello multilingue. Durante l'inferenza, possiamo utilizzare questo modello per tradurre query in tedesco, query in cinese, ecc. Consideriamo inoltre il trasferimento zero-shot e few-shot cross-linguale. Addestriamo su una lingua sorgente e trasferiamo a un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query in inglese o su una combinazione di query in inglese e tedesco few-shot per addestrare un modello multilingue e predire l'output SQL. Abbiamo anche riscontrato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, tra cui encoder PDR, ovvero encoder pre-addestrati multilingue con decoder basati su puntatore, come XLMR plus PDR e BERT plus PDR. Valutiamo anche modelli encoder-decoder, ovvero modelli encoder-decoder pre-addestrati multilingue, come MBART e MT5. Abbiamo scoperto che gli encoder-decoder ottengono le migliori prestazioni su tutti e nove i dataset. Valutiamo MT5 e XLMR plus PDR nell'impostazione multilingue. Abbiamo scoperto che gli encoder-decoder o gli encoder-PDR possono essere migliorati addestrandoli con una miscela di varie lingue. Questo perché la maggior parte delle lingue naturali principali può ottenere un miglioramento delle prestazioni, ad eccezione del fatto che le prestazioni dell'inglese diminuiscono in sette dataset e aumentano solo in tre dataset. Questo è noto come la maledizione della multilinguità. Confrontiamo anche il divario di prestazioni cross-linguale. In questa figura, la linea blu è il trasferimento few-shot cross-linguale. La linea arancione è il trasferimento zero-shot cross-linguale, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e la linea arancione, per l'impostazione zero-shot, il divario di prestazioni del trasferimento cross-linguale è significativo. Confrontando la linea blu e la linea arancione, abbiamo scoperto che per l'impostazione few-shot, il divario di trasferimento si riduce rapidamente. Abbiamo anche riscontrato alcune altre scoperte interessanti. Ad esempio, gli encoder-decoder superano i lavori precedenti o hanno ottenuto risultati comparabili. L'utilizzo di una lingua naturale inglese può migliorare significativamente le prestazioni few-shot nelle lingue naturali di destinazione. Abbiamo anche scoperto che i modelli linguistici multilingue come CODIS e BLUE sono ancora adeguati per le attività di analisi semantica cross-linguale. In sintesi, abbiamo costruito Exampler, un benchmark unificato per l'analisi semantica cross-linguale con molteplici lingue naturali e rappresentazioni principali. Conduciamo uno studio completo del benchmark su tre tipi rappresentativi di modelli linguistici multilingue. I nostri risultati mostrano molte scoperte interessanti, ecc. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Szpilkowski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come potreste sapere, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci basati su corpora. Quindi, ad esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è la testa di tutta la struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del testo di significato di Igor Milchuk, dove, ancora una volta, l'intera struttura coordinata è testata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Essi singolano uno dei congiunti. Ora, esistono anche approcci simmetrici alle strutture coordinate come l'approccio di Praga, l'approccio guidato dalla congiunzione assunto nei Prague Dependency Treebanks, dove le strutture coordinate sono testate dalla congiunzione. Quindi otteniamo dipendenze da n a tutti i congiunti. E infine c'è anche un approccio multi-testa che è utilizzato, ad esempio, nella grammatica di parola di Dick Hudson dove, per così dire, tutti i congiunti sono teste della struttura coordinata. un approccio multi-testa che è utilizzato, ad esempio, nella grammatica di parola di Cutson dove, per così dire, tutti i congiunti sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ama tutti i condotti separatamente, questi sono bartons making ora l'obiettivo di questo articolo è produrre due e contro le strutture asimmetriche di coordinazione come queste due. Okay, l'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi. Quindi in inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo mentre i complementi avverbiali possono essere più lontani, giusto? Quindi March lesse ieri va bene perché l'oggetto diretto it è vicino al verbo, mentre March oggetti diretti preferiscono essere vicini al verbo, mentre i complementi avverbiali possono essere più lontani, giusto? Quindi March lesse ieri va bene perché l'oggetto diretto it è vicino al verbo, mentre March lesse ieri è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un complemento avverbiale ieri. Tuttavia, questo effetto può essere alleviato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo il complemento avverbiale. Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. March lesse questo libro assolutamente affascinante su BCS oggi. Va bene. In un certo senso, invece di it, abbiamo questo lungo sintagma nominale. Ma va bene anche dire March lesse ieri questo libro assolutamente affascinante su api. Quindi il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferite. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti in queste due strutture. Quindi qui abbiamo una dipendenza da red all'aggiunto di lunghezza sette misurata in parole e da red a book di lunghezza quattro. Quindi insieme è 11. Quando ti muovi, quando scambi questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi invece di 11, sei, molto più corto. Ecco perché suona abbastanza bene. Viola un principio, ma ne soddisfa un altro. Okay, quindi cosa abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedere l'articolo perché non abbiamo usato le dipendenze universali. e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendano ad essere più corti. non abbiamo usato le dipendenze universali e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendano ad essere più corti, quindi sale e pepe e non pepe e sale misurati in sillabe e anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la lunghezza, la differenza di lunghezza, quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte. Giusto. Quindi la proporzione è più grande del congiunto sinistro corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto, quindi il governatore è a sinistra in questo esempio ho visto bart e lisa, quindi è il governatore, è a sinistra uh, è assente nel secondo esempio homer è venuto e ha starnutito qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, quindi in tali casi uh il congiunto sinistro preferisce essere più corto, tanto più che uh la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra è qui, il sinistro governa la coordinazione, t e net, questo effetto scompare. Quindi lo dimostriamo misurando la lunghezza in caratteri, che è la prima colonna in sillabe, la colonna centrale e in parole, la colonna a destra. Quindi mi concentrerò su quella a destra. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto sinistro ad essere più corto cresce costantemente con la differenza assoluta in parole. E lo stesso vale quando non c'è un governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E dimostriamo nel documento come questo fornisca un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedi l'articolo per il pieno accordo e gli argomenti, scusa, e parliamo della sessione di poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno! Mi chiamo Kaio Yin e presenterò il nostro lavoro dal titolo: \"Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati.\" Questo studio è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, Andre F.D. Martins e Graham Newbig. Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase? Beh, se la frase precedente fosse \"le cose potrebbero diventare pericolose se i ministri lo scoprissero\", allora \"mole\" si riferisce a una spia. Ma se la frase precedente fosse \"potrebbe essere qualcosa di serio, dottore?\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, cambia anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLUE, incapaci di cogliere queste traduzioni. Alcuni hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana. In questo lavoro, abbiamo cercato di rispondere a queste due domande: prima, quando la traduzione richiede contesto? E, in secondo luogo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazioni il contesto C fornisce sul target Y, dato il sorgente X. Potete pensare a CXMI come alle informazioni ottenute fornendo il contesto al modello. In questo lavoro, estendiamo CXMI al point-wise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un alto P6MI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un alto P6MI per individuare modelli tra queste parole. E svolgiamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 diverse lingue. Eseguiamo la nostra analisi a tre livelli diversi. Innanzitutto, esaminiamo i tag delle parti del discorso che hanno alti valori medi di PCXMI. Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un PCXMI relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E analogamente, troviamo che anche alcune lingue richiedono il contesto quando vogliamo scegliere la forma verbale appropriata. Quindi, esaminiamo gli elementi lessicali che hanno un PCSXMI elevato, calcolato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri, assicurandosi di utilizzare la stessa traduzione all'interno del documento. E analogamente, troviamo che il contesto è utile per tradurre nella formalità corretta. Infine, esaminiamo diversi token individuali che hanno un PCXMI elevato. Questo ci permette di identificare fenomeni che non possono essere catturati veramente dalla parola stessa, ma piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi. Quindi ora utilizziamo i nostri risultati dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il multilingual discourse-aware, o MUDA, tagger. Possiamo quindi utilizzare il tagger per identificare le parole che appartengono al fenomeno, e le lingue hanno proporzioni diverse di questi fenomeni del discorso. Quindi utilizziamo il tagger MUDA applicando il tagger al corpus parallelo che vogliamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione preferite agli esempi dipendenti dal contesto che il Muda tagger ha identificato. Infine, utilizziamo il nostro benchmark e altre metriche per valutare diversi modelli sulla traduzione automatica a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus, quindi blue, troviamo che i modelli indipendenti dal contesto hanno le migliori prestazioni, ma se utilizziamo COMET, i modelli consapevoli del contesto ottengono i migliori risultati. E se utilizziamo la word-f measure, allora i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano metriche a livello di corpus da sole. Ora utilizziamo il benchmark MUDA per valutare i modelli, e troviamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per determinati fenomeni del discorso, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non hanno utilizzato il contesto su altri fenomeni come l'ellissi, i pronomi e la forma verbale. Quindi questo suggerisce in quale direzione dovremmo vedere ulteriori progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, conduciamo un'analisi basata sui dati attraverso 14 coppie linguistiche per identificare quando le traduzioni richiedono contesto. E poi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Vi vediamo a Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, studentessa di dottorato del primo anno alla Carnegie Mellon University, e oggi vi presenterò il vostro lavoro, \"Anal Positionality, Characterizing Designed Biases of Datasets and Models\". Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Sante, Ronan Labrasse, Katarina Aranica e Martin Sapp.\n\nCominciamo immaginando che stiate lavorando per un giornale e che stiate esaminando i commenti sotto il vostro articolo di notizie per rimuovere contenuti tossici. Potreste ricorrere a un'API popolare come Perspective API per il rilevamento della tossicità e, in questo caso, funziona molto bene se siete Carl Jones, poiché Perspective API è in grado di rilevare correttamente le istanze tossiche. Ma non è così per Aditya Sharma, dove Perspective API non è così sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progettazione, in cui osserviamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni. Bias di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizione degli esperti di NLP e degli sviluppatori di modelli. La posizione, o *positionality*, è semplicemente l'insieme di prospettive che le persone possiedono in virtù della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatori, la posizione può influenzare il processo di ricerca e i suoi risultati perché può modificare le decisioni che i ricercatori prendono.\n\nQuindi, una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione, o *positionality*? E non stiamo dicendo che i modelli stessi e i dataset stessi hanno identità demografiche ed esperienze di vita, ma essi aggregare giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre. Lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizione, come le lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizione del modello. Tuttavia, questi lavori non esaminano il confronto tra gli utenti finali, i dataset e i modelli stessi. Studiare la posizione dei modelli e dei dataset è sempre più importante man mano che i compiti di NLP diventano più soggettivi e socialmente orientati. È difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.\n\nPer studiare la posizione dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con i dataset e i modelli esistenti. Lo facciamo attraverso il nostro framework, NL *positionality*. Il nostro framework funziona in due fasi principali. La prima fase è quella di ri-annotare i dataset con annotatori diversi. Optiamo per questo approccio anziché esaminare la demografia degli annotatori dei dataset originali, perché di solito solo pochi annotatori annotano ogni istanza e perché la demografia viene raramente raccolta e condivisa. Pertanto, optiamo per la ri-annotazione dei dati per ottenere molti annotatori per istanza e per ottenere un insieme ricco di dati demografici. Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R. Pertanto, il nostro framework differisce dalla letteratura sulla discrepanza degli annotatori confrontando gli utenti finali con i modelli e i dataset, le previsioni e le etichette, anziché semplicemente esaminare l'accordo tra gli annotatori o modellare le distribuzioni degli annotatori.\n\nIl nostro framework è ampiamente abilitato da Lab in the Wild, una piattaforma di crowdsourcing online del nostro collaboratore HCI. E Lab in the Wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi, a differenza delle piattaforme come MTurk, che hanno in gran parte partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo due attività su Lab in the Wild, una delle quali è l'accettabilità sociale. E il funzionamento è che i partecipanti leggeranno una situazione dal dataset di chimica sociale e quindi scriveranno quanto una situazione sia socialmente accettabile. In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e con gli altri. Confrontiamo quindi queste annotazioni con social chemistry, Delphi e GPT-4. Quindi, abbiamo replicato un setup molto simile per il test di rilevamento della tossicità e dell'odio. Confrontiamo queste annotazioni con social chemistry, Delphi e GPT-4. Quindi, abbiamo replicato un setup molto simile per l'attività di rilevamento della tossicità e dell'odio, in cui leggeranno un'istanza da DynaHate e scriveranno se ritengono che sia un'istanza di odio. Confrontiamo quindi queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta e GPT-4.\n\nIl nostro studio, alla fine, ha raccolto oltre 16.000 annotazioni da oltre mille annotatori provenienti da 87 paesi. Quindi, siamo ora meglio attrezzati per rispondere alla domanda: con chi si allineano maggiormente i dataset e i modelli di NLP? Scopriamo che c'è una posizione, o *positionality*, in NLP. Ad esempio, scopriamo che i dataset si allineano maggiormente ai paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di GPT-4, scopriamo che si allinea maggiormente ai paesi di lingua confuciana e inglese. Scopriamo anche che Dyna-hate si allinea maggiormente ai paesi di lingua inglese. Scopriamo anche un ulteriore allineamento con persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nell'attività di accettabilità sociale, scopriamo che si allinea maggiormente con persone con un'istruzione universitaria o un'istruzione post-laurea. E troviamo lo stesso per Dynahate, dove si allinea maggiormente con persone con un'istruzione universitaria.\n\nTuttavia, quando modelli e dataset si allineano a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i dataset e i modelli si allineano meno alle persone non binarie rispetto alle controparti maschili e femminili. Lo troviamo nell'attività di accettabilità sociale di GPT-4 e nell'analisi del compito DynaHEAT.\n\nQuindi, dato che c'è una posizione, o *positionality*, in NLP, cosa possiamo fare al riguardo? Abbiamo quindi alcune raccomandazioni in merito. La prima è tenere un registro di tutte le scelte di progettazione pertinenti durante tutto il processo di ricerca. E l'altra è condurre ricerche di NLP con la lente del perspectivismo. La nostra terza raccomandazione è quella di costruire dataset e modelli specializzati all'interno di specifiche comunità. E un buon esempio di questo è l'iniziativa Masakane. Vogliamo sottolineare che l'NLP inclusivo non significa semplicemente fare in modo che tutte le tecnologie funzionino per tutti.\n\nE così conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di consultare la nostra dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione di espressioni referenziali indirette per la selezione di entità, in cui presentiamo il corpus Alt Entities. Mi chiamo Jawad Hosseini, e questo è un lavoro congiunto con Philip Radlinski, Sylvia Parity e Annie Lewis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. Consideriamo questa domanda alternativa: intendevi \"Easy on Me\" o \"I Got a Feeling\"? Qui, l’utente vuole selezionare una di queste due canzoni. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio citando il nome della canzone, \"Easy on Me\", o la sua posizione, la prima. Ma a volte un riferimento indiretto è più appropriato per una conversazione più naturale. Questo può accadere quando l’utente non ricorda il nome della canzone, o quando le pronunce sono troppo simili l'una all'altra e difficili da disambiguare, oppure quando l’utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette: ad esempio, “quella più recente” o “la canzone che non è energica”. Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking della comprensione di entità dei LLM. Non siamo a conoscenza di un dataset pubblico, un dataset pubblico su larga scala per questo compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione crowdsourcing. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dati enfatizza l'informalità, utilizzando un setup di completamento di cartoni animati. Il cartone animato ha tre fumetti. La nostra metodologia di raccolta dati enfatizza l'informalità, utilizzando un setup di completamento di cartoni animati. Il cartone animato ha tre fumetti. Nel primo fumetto, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con ciò, Bob stabilisce il contesto del dialogo. Nel secondo fumetto, Alice dice: \"Intendevi \"Easy on Me\" o \"I Got a Feeling\"?\", che è la domanda alternativa. E nel terzo fumetto, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio, “quella più recente”. Forniamo il primo e il secondo fumetto automaticamente, ma il terzo è completato dall'annotatore. Il primo fumetto è scelto da alcuni prompt manuali per dominio. Il secondo, che è la domanda alternativa, è generato come segue: utilizziamo sempre un semplice modello \"Intendevi a o b\" generato come segue: utilizziamo sempre un semplice modello \"Intendevi A o B\" dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che saliamo nella lista, le entità diventano più simili l'una all'altra ed è generalmente più difficile effettuare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome \"Il ritorno\". Il terzo è quando hanno descrizioni simili su Wikipedia e infine quando, ad esempio, hanno lo stesso genere o lo stesso artista. Quando mostriamo questa domanda alternativa agli amministratori, conoscono il nome di queste entità, ma non necessariamente sanno di cosa si tratta. Quindi, ciò che facciamo è mostrare alcune conoscenze di base su queste due entità. Per le canzoni, mostriamo semplicemente un collegamento alla ricerca di Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ciascuna canzone e leggere informazioni su ciascuna canzone. Ecco, ad esempio, il risultato della ricerca di Google per la canzone \"Easy on Me\". Per i domini delle ricette e dei libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi chiediamo agli annotatori di scegliere una di queste entità, ad esempio, qui la prima e di descriverla utilizzando da tre a cinque espressioni referenziali indirette, ad esempio \"quella con la musica del pianoforte\". Ecco alcuni esempi dai nostri dati: ad esempio, \"quella senza parole\", \"non quella con il dodicenne\", \"quella fittizia\" o \"quella proveniente dall'Azerbaigian\" e così via. Il corpus Alt Entities ha 6.000 domande alternative su tre domini e ha 42.000 espressioni referenziali indirette. I risultati con il modello T5XL sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta, intorno al 92-95 percento, ma questo non è realistico. Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, allora l'accuratezza è compresa tra l'82 e l'87 percento, il che è più realistico, ad esempio quando il modello linguistico recupera le conoscenze di base. In questo caso, l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro dataset. Grazie."}
