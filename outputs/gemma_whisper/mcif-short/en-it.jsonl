{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, benvenuti alla nostra presentazione di d.plain, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è un processo di adattamento volto a migliorare la comprensibilità di un testo per un gruppo di riferimento specifico, come persone con difficoltà di lettura o non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, necessitiamo di coppie parallele di testi, ad esempio, di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio che segue, è possibile osservare una coppia di frasi allineate parallelamente, consistente in una complessa frase tedesca e la sua traduzione in un linguaggio chiaro."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può osservare nell'esempio, quali la sostituzione lessicale, la claustellazione, il riordinamento della claustellazione o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "presentiamo ora il nostro nuovo corpus, dplane. Negli ultimi anni, infatti, si sono riscontrati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo corpus dplane, suddiviso in due sottocorpora, dplane-apa e dplane-web. dplane-apa si basa su testi d’uso."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Nella semplice configurazione APA, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 30.000 coppie di frasi parallele, pari a 13.000."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "per DeepLaneWeb. Questo corpus include diversi ambiti e abbiamo inoltre allineato tutti questi 750 documenti sia manualmente che con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato le nostre coppie di frasi in modo più approfondito. Ad esempio, per quanto riguarda il tipo di semidefinizione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come potete vedere qui, i testi biblici sono decisamente più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "su tutti i livelli, in relazione a, ad esempio, semplificazione lessicale, semplificazione strutturale, nonché al livello complessivo di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus Deplane presenta un'elevata varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus API dell'API Deplane, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto a quanto riscontrato nel corpus Web di Deplane."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web, abbiamo un numero molto maggiore di riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Salve, sono Omar e ora parlerò dei casi d'uso del nostro dataset D-plane. Per il primo caso d'uso, possiamo valutare metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, sono stati sviluppati numerosi metodi di allineamento, ma nel contesto della traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse, e desideriamo estrarre gli allineamenti di frasi nei documenti post."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "ma nel nostro caso d'uso stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che condividono la stessa lingua e lo stesso contenuto, ma presentano un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "Ed ora, avendo a disposizione il dataset D-plane, contenente frasi allineate manualmente, possiamo utilizzare questi allineamenti come standard d'oro per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "In conclusione, abbiamo stabilito che il metodo di allineamento automatico più efficace da utilizzare per la semplificazione del testo tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "Potete inoltre trovare il codice per eseguire questo metodo sui vostri documenti nell'articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "affinando i modelli linguistici per produrre testo semplificato a partire dal testo di input complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli differenti. Abbiamo ottimizzato il modello a impatto prolungato per generare semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche ottimizzato la base normale lunga, la base normale in parte, per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Potete trovare anche tutti i checkpoint e potete esaminare nel dettaglio i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che tale semplice ottimizzazione poteva produrre risultati superiori ai punteggi di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo tali risultati come punto di riferimento, un benchmark di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo molto per la Sua attenzione e speriamo di poter incontrare tutti voi durante il convegno. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Szpilkowski e questa presentazione riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprete, diverse teorie e approcci basati su corpora assumono differenti strutture di dipendenza. Ad esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie"}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il nucleo dell'intera struttura coordinata, quindi, in questo caso, Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è assunto nella teoria del significato-testo di Igor Milczuk, dove ancora l’intera struttura coordinata è governata dal primo congiunto. Pertanto, questi due approcci sono asimmetrici. Essi singolano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente esistono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio a testa congiunzionale assunto nei *treebank* di dipendenze di Praga, dove le strutture coordinate sono a capo con la congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze da un'estremità a tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "E infine, esiste anche un approccio multi-sfaccettato, utilizzato, ad esempio, nella grammatica lessicale di Dick Hudson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono teste della struttura coordinata. Pertanto, otteniamo dipendenze dal governatore, qui loves, verso tutti i congiunti separatamente. Queste sono creazioni di Barton."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, l'obiettivo di questo articolo è presentare una nuova argomentazione a favore delle strutture simmetriche delle coordinazioni come queste due e contro le strutture asimmetriche delle coordinazioni come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio di minimizzazione della lunghezza della dipendenza, che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli elementi aggiuntivi possono essere più distanti, giusto? Quindi, \"March, read it yesterday\" è corretto perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March lesse ieri, è molto peggio, giusto? Perché qui, tra il verbo e il complemento oggetto, si interpone un avverbio di tempo, ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, tale effetto può essere attenuato quando l’oggetto diretto è particolarmente pesante e lungo, poiché in tal caso può essere spostato in posizione successiva al sintagma preposizionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. March ha letto oggi questo libro assolutamente affascinante sul BCS. Va bene. Il modo in cui, invece di esso, abbiamo questa lunga NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma è anche accettabile dire Marzo Leggi Ieri, questo assolutamente affascinante libro sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale che gli oggetti diretti debbano stare accanto al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Wojciech Czaja – soddisfa il principio di minimizzazione della lunghezza delle dipendenze, il quale afferma che dipendenze più brevi sono preferite.\nWojciech Czaja – dipendenze più brevi sono preferite."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano quindi solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Dunque, abbiamo qui una dipendenza da \"read\" all'aggiunto di lunghezza sette, misurata in parole, e da \"read\" a \"book\" di lunghezza quattro. Quindi, insieme, fa 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6, giusto? Quindi invece di 11, 6, decisamente più breve. Ecco perché suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Bene, quindi quello che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione potenziata del Penn Treebank e consultare il paper per capire perché non abbiamo utilizzato le dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "Mateusz Piorkowski – e le statistiche confermano l'osservazione, fatta innumerevoli volte in precedenza, che i contratti di sinistra tendono ad essere più brevi, anche in termini di \"sale e pepe\" e di \"sale\" misurato in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "e anche l'osservazione, fatta di passaggio, che tale tendenza si accentua all'aumentare della differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto tende a essere il primo più forte. Esatto. Quindi la proporzione è maggiore dei congiunti corti a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che di nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solamente quando le figure governanti a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, il governatore si trova a sinistra. Ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio. Omero venne e starnutì. Qui abbiamo coordinazione di due verbi e non c’è alcun governatore esterno. Pertanto, in questi casi, il congiunto sinistro preferisce essere più breve, tanto più grande è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance sulla destra è presente, la sinistra governa il coordinamento, il tel e il net, questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dimostriamo che misurando la lunghezza in caratteri, quella è la prima colonna in sillabe, la colonna centrale in parole, la colonna a destra. Quindi mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che vediamo qui è che quando il governatore si trova a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza per il congiunto sinistro a essere più breve aumenta costantemente con la differenza assoluta di parole. E lo stesso si osserva in assenza di un governatore, come nel caso del coordinamento di frasi. Ma quando il governatore si trova a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E dimostriamo nel presente articolo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche, come queste due, e a favore delle strutture simmetriche, come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Si veda l'articolo per l'accordo e le argomentazioni complete, scusate, e parlate con noi della sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiangbin, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro, che parte dai dati di pre-addestramento ai modelli linguistici fino alle applicazioni a valle, tracciando le tracce di pregiudizi politici che conducono a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati di grandi dimensioni estratti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di informazione politica sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C4, si può osservare che New York Times, Los Angeles Times, The Guardian, Huffington Post, e altri, sono adeguatamente presenti nei dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato un beneficio ambiguo per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, hanno potuto apprendere da prospettive diverse, il che celebra la democrazia e la pluralità di idee.\nDall'altro lato, queste differenti opinioni politiche sono intrinsecamente socialmente distorte e potrebbero condurre a potenziali problematiche di equità nelle applicazioni di task a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di indagare il meccanismo di propagazione del bias politico, a partire dai dati di pre-addestramento fino ai modelli linguistici e alle attività a valle, ponendoci specificamente le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbe avere i dati di addestramento in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano effettivamente i modelli linguistici con diversi limiti politici nelle attività a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Più precisamente, abbiamo inizialmente proposto di sollecitare i modelli linguistici con diversi formati di prompt, utilizzando questionari politici, come il test della bussola politica. Ciò ci assicura di poter effettuare una valutazione automatica ben fondata sulla letteratura scientifica di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli di linguaggio nativo presentano effettivamente diverse inclinazioni politiche. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che GPT-4 è il modello linguistico più progressista tra tutti, e le teorie di GPT sono generalmente più socialmente progressiste rispetto alla teoria BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, intendiamo indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente ripresi dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Potremmo quindi condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint di modelli linguistici su sei diversi corpora partigiani, separati in notizie e social media, ulteriormente suddivisi in base alle loro inclinazioni politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Pre-addestrando ulteriormente i modelli linguistici su tali corpora di parte, si osserva che le coordinate ideologiche del modello linguistico si spostano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Per Roberta, ulteriormente ottimizzata e addestrata su un corpus di Reddit di orientamento progressista, possiamo osservare un significativo spostamento verso posizioni liberali in termini di..."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini dei suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici possano cogliere la polarizzazione diffusa nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, suddividiamo i corpora di pre-addestramento in pre-45° presidente degli Stati Uniti e post-45° presidente degli Stati Uniti, addestrando separatamente modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "possiamo notare che i modelli linguistici presentavano generalmente un orientamento politico più distante dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche percepire la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in ultima analisi, valutiamo modelli linguistici con diverse orientamenti politici sulla rilevazione di discorsi d'odio e sulla rilevazione di notizie false in applicazioni di NLP che spesso implicano modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che se analizziamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "A seconda delle diverse fasce demografiche o del significato politico dei media di informazione, possiamo notare uno schema per cui, ad esempio, per la rilevazione di discorsi d'odio, i modelli linguistici orientati a sinistra funzionano meglio."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nella rilevazione di discorsi d'odio rivolti a gruppi sociali minoritari"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "tuttavia, sono meno abili nel rilevare discorsi d'odio rivolti a gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "e viceversa, i modelli linguistici orientati a destra sono migliori nell’individuare discorsi d’odio diretti contro bianchi e uomini, tuttavia peggiori nell’individuare discorsi d’odio rivolti a comunità nere, LGBTQ+ e altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Analogie tendenze si riscontrano anche nella rilevazione di notizie false, dove si osserva che i modelli linguistici con orientamento politico di sinistra sono più efficaci nell’individuare disinformazione proveniente da modelli con orientamento politico opposto, e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questo modo, mostriamo inoltre numerosi esempi qualitativi per dimostrare che i modelli linguistici con diverse connotazioni politiche…"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "essi forniscono previsioni diverse per esempi di discorsi d'odio e disinformazione in base alla loro categoria sociale. Nell'appendice sono presenti numerosi altri esempi per evidenziarlo ulteriormente."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che esiste una problematica di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Se un modello linguistico con orientamento di destra fosse sottoposto a fine-tuning su discorsi d'odio o disinformazione, o qualsiasi altra cosa simile, e implementato su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti di gruppi minoritari potrebbe diffondersi incontrollato."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ha quindi suonato l’allarme, spingendoci a riconoscere e affrontare le problematiche di equità derivanti dalle inclinazioni politiche dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Allora, un breve spazio per la discussione. Vorremmo inoltre sottolineare che evidenziamo il dilemma peculiare concernente i pregiudizi politici dei modelli linguistici. È come trovarsi tra Scilla e Cariddi."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il bias si propagherà dai dati di pre-addestramento ai modelli linguistici e alle attività successive, creando in ultima analisi problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se tentassimo in qualche modo di sanificare, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba conservare i dati sulla monotonia linguistica. È un po' come il problema del tram elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ottimo. Penso che sia sostanzialmente tutto per oggi. La ringrazio per la sua attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, dottoranda del primo anno alla Carnegie Mellon University, e oggi vi presenterò il vostro lavoro, Anal Positionality, Caratterizzazione dei Bias Intrinseci in Set di Dati e Modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santee, Ronan Labrosse, Katarina Reinecke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Cominciamo dunque immaginando che stiate lavorando per un quotidiano e che stiate esaminando i commenti sotto il vostro articolo di notizie per rimuovere contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti orientarti verso un’API popolare come Perspective API per il rilevamento della tossicità. E questo funziona molto bene se sei Carl Jones, nel qual caso Perspective API è in grado di identificare correttamente istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio così per Aditya Sharma, dove l'API di prospettiva non risulta particolarmente sensibile a termini offensivi più comuni in contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di distorsione progettuale in cui si osservano differenze sistematiche nelle prestazioni della tecnologia tra diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Pregiudizi progettuali come quello che abbiamo appena osservato potrebbero derivare dalla posizione epistemologica dei ricercatori di NLP e degli sviluppatori di modelli. La posizione epistemologica si riferisce semplicemente alle prospettive che le persone possiedono in virtù della loro demografia, identità e esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un concetto ampiamente utilizzato negli studi critici, in particolare negli ambiti accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E, in quanto ricercatore, la posizionamento può influenzare il processo di ricerca e i suoi esiti e risultati, in quanto può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, una domanda che si potrebbe porre è se i dataset e i modelli presentino una posizione, una prospettiva situata?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "Non stiamo affermando che modelli, cellule e set di dati possiedano di per sé identità demografiche ed esperienze di vita, ma essi aggregazione giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni a scapito di altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, studi precedenti hanno suggerito alcune evidenze aneddotiche di una posizione, come lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste opere non si concentrano realmente sul confronto tra gli utenti finali e i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità di modelli e dataset sta diventando sempre più importante, man mano che i compiti di NLP diventano più soggettivi e orientati alla società."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "Ed è difficile caratterizzare come queste posizionamenti siano distorti, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, per studiare la posizionalità del dataset e del modello, confrontiamo effettivamente le annotazioni con utenti reali con dataset e modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo realizziamo attraverso il nostro framework di posizionalità NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework opera in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo consiste nel ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E scegliamo di procedere in questo modo, anziché analizzare le caratteristiche demografiche degli annotatori dei dataset originali, poiché solitamente solo pochi annotatori annotano ciascuna istanza e perché i dati demografici sono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E pertanto optiamo per ri-annotare i dati, al fine di ottenere numerosi annotatori per ogni istanza, e per acquisire un insieme completo di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, analizziamo le annotazioni in base ai dati demografici e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così il nostro framework differisce effettivamente dalla letteratura sull'incongruenza degli annotatori confrontando gli utenti finali con modelli e set di dati, previsioni ed etichette, anziché limitarsi a considerare l'accordo tra gli annotatori o a modellare le distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è reso possibile in larga misura tramite Lab in the Wild, una piattaforma di crowdsourcing online fornita dal nostro collaboratore nel campo dell'HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari più diversificati rispetto alle piattaforme come MTurk, che hanno prevalentemente partecipanti provenienti dagli Stati Uniti o dall’India. E inoltre, Lab in the Wild è comunque in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Organizziamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il funzionamento è il seguente: i partecipanti leggeranno una situazione tratta dal dataset di social chemistry e poi scriveranno quanto una situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per mantenere alto l'interesse nello studio, possono confrontare le proprie risposte con quelle di un'IA e di altri studenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Confrontammo quindi queste annotazioni con la social chemistry, Delphi e GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, replichiamo una configurazione molto simile per il compito di rilevamento della tossicità e del linguaggio d'odio, in cui dovranno leggere un'istanza proveniente da DynaHate e indicare se la ritengono un'istanza di linguaggio d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo quindi queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta e GPT-4. Il nostro studio ha infine raccolto oltre 16.000 annotazioni da più di mille annotatori provenienti da 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ora siamo meglio equipaggiati per rispondere alla domanda: con chi si allineano i dataset e i modelli di NLP? Scopriamo che esiste una posizione nell'ambito dell'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che dataset e modelli sono maggiormente allineati ai paesi di lingua inglese. Pertanto, per l'analisi dell'accettabilità sociale di GPT-4, abbiamo riscontrato che questo è maggiormente allineato ai paesi di lingua confuciana e anglofona. Abbiamo inoltre riscontrato che il fenomeno del \"dyna-hate\" è maggiormente allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo inoltre una maggiore corrispondenza con persone che hanno conseguito un'istruzione universitaria. Pertanto, per GPT-4 nel compito di accettabilità sociale, riscontriamo che si allinea maggiormente con persone con istruzione universitaria o post-universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo lo stesso per Donahate, dove la correlazione è più forte con persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando modelli e set di dati sono allineati a specifiche popolazioni, alcuni inevitabilmente vengono esclusi."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i set di dati e i modelli sono meno allineati con le persone non binarie rispetto alle controparti maschili e femminili. Lo riscontriamo nel compito di accettabilità sociale di GPT-4, così come nell'analisi del compito DynaHATE."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione (o prospettiva) nell'ambito dell'NLP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, abbiamo alcune raccomandazioni al riguardo. La prima è quella di tenere traccia di tutte le scelte progettuali rilevanti durante l'intero processo di ricerca. E l'altra è quella di condurre ricerche di NLP (elaborazione del linguaggio naturale) adottando la prospettiva del perspectivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di ciò è l'iniziativa Masakane. Voglio dire, intendiamo sottolineare che l'elaborazione del linguaggio naturale inclusiva non consiste semplicemente nel far funzionare tutte le tecnologie per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E questo conclude la nostra presentazione, ma se desiderate approfondire, sentitevi liberi di consultare il nostro cruscotto per i risultati dell'analisi più aggiornati e il nostro articolo scientifico. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Siyu Yuan, dell'Università Fudan. Sono qui per presentare il nostro lavoro, Distillazione della Conoscenza Script dai Modelli Linguistici di Grandi Dimensioni per la Pianificazione Linguistica Vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le proprie azioni seguendo interazioni passo dopo passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "I lavori precedenti hanno sfruttato modelli linguistici per pianificare obiettivi astratti relativi ad attività stereotipate, come preparare una torta, e hanno dimostrato che i modelli linguistici di grandi dimensioni possono efficacemente suddividere gli obiettivi in fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti, tipici di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come preparare una torta al cioccolato, rimane ancora in gran parte inesplorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impongono vincoli differenti sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi concreti e specifici, caratterizzati da vincoli molteplici. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Dato che non esiste un dataset di obiettivi specifici a supporto della nostra ricerca,"}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo acquisire questi obiettivi innanzitutto. Come illustrato nella tabella, estendiamo gli obiettivi astratti con vincoli multiformi. Per l'acquisizione di dati con intervento umano, utilizzare InstructGPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Selezioniamo 100 ragazze specifiche e valutiamo gli script generati da modelli locali di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "La presente tabella riporta l'accuratezza complessiva dei risultati.\nRileviamo che tutti i modelli linguistici leggeri ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, conduciamo un'analisi dettagliata per indagare le ragioni per cui i modelli di apprendimento per linea falliscono."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati presentati nella figura indicano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo esaminato più nel dettaglio categorie tematiche specifiche di vincoli definiti in WikiHow. La heatmap nella figura mostra che le prestazioni di pianificazione dei PD istruttivi variano considerevolmente per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli light-light wind presenta un'elevata varianza, con conseguente scarsa performance. Pertanto, adottiamo l'idea di un Z-filtro sovra-generato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, presentiamo i tipi di vincoli con esempi per l'intract CPT, e otteniamo obiettivi specifici basati sugli obiettivi astratti iniziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, istruire GPT a generare un numero eccessivo di script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script ammissibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e obiettivi in incorporamenti GPT astratti e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, evitiamo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nel set di obiettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, InstructZBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di aderenza ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Dato che i modelli linguistici di grandi dimensioni risultano costosi da implementare, è essenziale dotare modelli più piccoli e specializzati di capacità di pianificazione linguistica. La creazione di un dataset rappresenta una fase fondamentale per il loro sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dataset è onerosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare set di dati di pianificazione del linguaggio vincolati da modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applichiamo il nostro metodo per la costruzione di un dataset di pianificazione linguistica vincolata, denominato Codescript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici corredati di script. Al fine di garantire la qualità dei siti di validazione e test, richiediamo a collaboratori reperiti tramite cloud di individuare campioni errati e rivisti."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura illustra la distribuzione dei vincoli nel codice script. Notiamo che il codice script mostra un elevato grado di approvazione nei confronti degli obiettivi specifici generati. Con il codice script, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio dei vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo rilevato che la funzione T-file sul tasso di costo può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che modelli più piccoli possono supportare modelli più grandi se adeguatamente addestrati su set di dati appropriati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo definito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppiamo un metodo di filtraggio sovra-generato per tali modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un dataset di script di alta qualità per la pianificazione linguistica vincolata. Speriamo che il dataset CodeScript possa essere una risorsa preziosa per promuovere la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio per il suo tempo.\nPuò trovare maggiori dettagli dello script del codice nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Zhu Heng. Oggi presenterò il nostro articolo, \"I tagger di entità nominate basati su kernel 2003 funzionano ancora bene nel 2023?\". Cominciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate, o NER task."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che i modelli utilizzano CONO 2003 per sviluppare il NER da quasi 20 anni. E questo solleva naturalmente diversi problemi. Innanzi tutto, questi modelli possono generalizzare dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, se osserviamo una scarsa generalizzazione, quali cause sono alla base del calo di performance di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il dataset CONO++. Si tratta di un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e successivamente annotato secondo le stesse linee guida di annotazione CONO 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi ottimizzato più di 20 modelli sul dataset Kano 2003. Li abbiamo valutati sia sul set di test Kano 03 che sul set di test Kano++."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la capacità di generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre elementi fondamentali necessari."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo aspetto riguarda l'architettura del modello. Attraverso i nostri esperimenti, abbiamo riscontrato che i modelli transformer tendono a generalizzare meglio a nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che, di solito, modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning incide direttamente sulle prestazioni di un compito a valle. Abbiamo inoltre riscontrato che un maggior numero di esempi di fine-tuning conduce effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo di performance di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Avevamo due ipotesi. La prima è l'eccessiva adattamento, ovvero un overfitting causato dal riutilizzo ripetuto dello stesso set di test. Questo si manifesta solitamente come una diminuzione dei rendimenti su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è lo spostamento temporale, ovvero il degrado delle prestazioni causato dall'aumentare del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo osservato che, dal grafico a destra, la retta di migliore adattamento rossa presenta un gradiente maggiore di 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Carnot 2003 si traduce in più di un'unità di miglioramento su Carnot++, il che implica l'assenza di rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che, in questo caso, non si osserva l'overfitting adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, che dire del Deriva Temporale?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per quanto riguarda la deriva temporale, abbiamo condotto un esperimento per riqualificare o continuare il pre-addestramento di alcuni modelli con dati più recenti, e abbiamo riscontrato che le prestazioni diminuiscono all'aumentare dell'intervallo temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E ciò conferma la nostra ipotesi che la causa principale del calo di performance sia lo spostamento temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di dimensioni maggiori del modello e di un maggior numero di esempi di fine-tuning. E questi elementi sono interconnessi. Non possiamo avere un solo componente, ma devono essere presenti tutti gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo riscontrato che il calo di performance qui è dovuto a deriva temporale e, in modo piuttosto sorprendente, non è causato da adattamento eccessivo, nonostante KONO 2003 sia in uso da oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Tornando quindi alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger di Connell del 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un deciso sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "E infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, in caso di domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Salve, discuterò del nostro lavoro sulla risoluzione di espressioni referenziali indirette per la selezione di entità, nel quale introduciamo gli AltEntityScorers."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinski, Sylvia Parity e Annie Lewis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. Consideriamo questa domanda alternativa. Intendevi \"Easy on Me\" o \"I Got a Feeling\"? Qui, un utente vuole selezionare una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più evidente è utilizzare un riferimento diretto. Ad esempio, specificando il nome del brano, Yami, o la sua posizione, il primo."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Ciò potrebbe accadere quando l'utente non riesce a ricordare il titolo della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "oppure le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l’utente desidera specificare una preferenza. Ecco alcuni esempi di differenze dirette. Ad esempio, quello più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi di dialogo e anche per il benchmarking della comprensione delle entità negli LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un dataset pubblico, un dataset pubblico su larga scala per un compito specifico. Pertanto, ne raccogliamo uno utilizzando l'annotazione crowdsourcing. Il nostro dataset copre tre domini differenti: musica, libri e ricette."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dati enfatizza l'informalità tramite un set di completamento di cartoni animati."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il cartone animato presenta tre fumetti. Nel primo fumetto, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con ciò, Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo fumetto, Alice dice: intendi \"easy on me\" o \"I got a feeling\"?"}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nel terzo fumetto, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio, il nuovo"}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda nuvoletta di dialogo, ma la terza viene compilata dall'annotatore. La prima nuvoletta di dialogo viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, viene generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un modello semplice. Intendi A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Salendo nella lista, le entità tendono ad essere più simili tra loro e solitamente è più difficile effettuare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniforme a caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio, due libri con il titolo “the return”."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando presentano descrizioni simili su Wikipedia. E, infine, quando condividono caselle informative o attributi simili su Wikipedia. Ad esempio, lo stesso genere musicale o lo stesso artista, per esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando sottoponiamo questa domanda alternativa agli annotatori, questi conoscono il nome di tali entità, ma non necessariamente conoscono l’entità stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ciò che facciamo è presentare alcune conoscenze pregresse sulle due entità. Per le canzoni, mostriamo semplicemente un collegamento a una ricerca su Google per ciascuna di esse."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedere agli annotatori di ascoltare almeno alcune parti di ogni brano e di leggere informazioni su ciascun brano. Ecco, ad esempio, il risultato della ricerca Google per il brano Easy Annotation."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio di ricette e libri, mostriamo un testo di accompagnamento proveniente da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, affinché i valutatori possano avere un'idea del loro aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Quindi chiediamo agli annotatori di scegliere una di queste entità, ad esempio, qui la prima, e descriverla utilizzando da tre a cinque espressioni referenziali indirette."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, quello senza parole, non quello con il ragazzino di 12 anni, o quello fittizio, o proveniente dall'Azerbaigian, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus AltEntities contiene 6000 domande alternative suddivise in tre ambiti e presenta 42.000 espressioni referenziali indirette. I risultati ottenuti con il modello T5XLARGE sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico avesse accesso esattamente alle stesse conoscenze pregresse degli annotatori, l'accuratezza sarebbe veramente elevata. Si attesterebbe intorno al 92-95%. Ma questa situazione non è realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, l'accuratezza si attesta tra l'82% e l'87%, valore più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso unicamente ai nomi di entità, l'accuratezza si attesta al 60%. Pertanto, vi è ampio margine di miglioramento. Abbiamo inoltre dimostrato che i modelli sono generalizzabili a diversi domini. Ecco un link al nostro dataset. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Sara Pape, dell'Università di Trento e della Fondazione Bruno Kessler, e introdurrò brevemente l'articolo \"Attention as a Guide for Simultaneous Speech Translation\", un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cosa si intende per traduzione simultanea del parlato? La traduzione simultanea del parlato, o simulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SimulST attuali? Architetture specifiche vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di formazione lunghe e complesse, ad esempio procedure che coinvolgono obiettivi di ottimizzazione differenti,"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "e l'addestramento e la manutenzione di diversi modelli per raggiungere differenti regimi di latenza, ad esempio addestrando un modello con una latenza media di 1 secondo e un altro con 2 secondi, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Allora, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Nei primi due, utilizzare modelli SD offline esistenti senza riaddestramento o adozione di architetture specifiche per singolo SD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "e sfruttare la conoscenza già acquisita dal modello tramite il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di cross-attenzione. È possibile osservare un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione consiste nel proporre ADAT o l'attenzione encoder-decoder, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione punta."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa quando la tensione non è concentrata, ovvero... la sua somma è inferiore a una certa soglia α, verso l'ultima riga di frame vocali, il che significa che l'informazione ricevuta è..."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, se riceviamo un frammento di discorso contenente \"I'm going to talk about\" e il nostro modello prevede una traduzione in tedesco,"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E analizzeremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che le prime due parole indicano i primi frame vocali ricevuti, mentre l'ultima parola indica gli ultimi frame vocali ricevuti come frame vocali lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "che le prime due parole saranno omesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Mentre la somma dell'attenzione incrociata supera una certa soglia alfa, non emetteremo l'ultima parola e attenderemo un altro segmento del discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se proseguiamo e riceviamo un altro blocco di discorso e il nostro modello prevede altre tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che nessuna parola punta agli ultimi frame del discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole saranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se consideriamo i risultati principali di ciò,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Tracciamo i risultati della traduzione simultanea dei discorsi su grafici in cui un lato è contrassegnato dal blu e misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "quello è il parametro di latenza. E consideriamo anche il calcolo-consapevole lagging medio, che tiene conto dei tempi di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo quindi che le nostre curve siano il più possibile alte in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "ma desideriamo anche che vengano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie adeguate che si applicano anche ai modelli offline, quali la strategia del tasto umido e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la pre-traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E osserviamo che essa supera tutte le strategie applicate ai modelli offline, dato che le curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, considerando il tempo trascorso effettivo o il tempo consapevole computazionale, quella è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate scoprire ulteriori risultati, leggete il nostro articolo. E abbiamo anche rilasciato codice, modelli e output simultanei open source, per agevolare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Ying e, insieme al mio collega Zhiyang, presenteremo la nostra ricerca su Multi-Improvement, Miglioramento dell'Apprendimento Seriale Multi-Modale Tramite Instruction Tuning."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi compiuti nei modelli linguistici di grandi dimensioni, numerosi studi hanno iniziato a esplorare nuovi paradigmi di apprendimento basati sul riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che l'instruction tuning consente ai modelli linguistici di grandi dimensioni di operare su compiti inediti in modalità zero-shot, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si concentra sul miglioramento delle prestazioni sequenziali nei compiti basati esclusivamente sul linguaggio, mentre compiti di computer vision e multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, desideriamo indagare se l'istruzione di ottimizzazione su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali inediti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo rilevato una discrepanza considerevole nella disponibilità di dataset di addestramento tra NLP e sistemi multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di 1600 compiti di istruzione basati esclusivamente sul linguaggio. Tuttavia, non esiste un compito di istruzione multimodale su larga scala pubblicamente disponibile. Questo motivo ci spinge a creare un dataset di ottimizzazione multimodale per l’istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi-Instruct, il primo dataset di benchmark per l'instruction tuning multimodale che consiste di 62 diverse attività multimodali, suddivise in 10 ampie categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti derivano da 21 dataset open source esistenti e ciascun compito è corredato da 5 istruzioni redatte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per l’indagine sull’affinamento multimodale dell’istruzione sul nostro dataset proposto, adottiamo OFA, un modello pre-addestrato multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token immagine e le coordinate di un riquadro di delimitazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo alcuni esempi tratti dal nostro dataset Multi-Instra."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per uniformare l'elaborazione di diversi tipi di dati di input e output,"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i riquadri di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ora parlerò di ottimizzazione dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il dataset di addestramento, utilizziamo 53 compiti provenienti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il testing, riserviamo l'intero gruppo di ragionamento di buon senso per la valutazione e selezioniamo ulteriori 5 compiti dai gruppi VQA e vario."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nel set di test per ciascun compito. Inoltre, campioniamo casualmente 20 compiti dal set di test di istruzioni naturali come compiti inediti per l'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, utilizziamo un modello OFA di grandi dimensioni pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e il valore massimo delle prestazioni, nonché la deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è una classificazione multi-modale, riportiamo l'accuratezza. Se si tratta di un compito di generazione multi-modale, riportiamo ROUGE-L. Per i compiti di NLP, riportiamo anch'esso ROUGE-L."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto anche una metrica di valutazione aggiuntiva, definita sensibilità. Questa misura la capacità del modello di produrre in modo coerente gli stessi risultati per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale. Come possiamo notare, l'affinamento tramite istruzioni può migliorare significativamente le prestazioni di OFA su compiti multimodali di comprensione di scene."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche il trasferimento di apprendimento da set di dati di istruzioni naturali può giovare all'affinamento tramite istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, all'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e, contemporaneamente, una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, confrontando un'istruzione con cinque istruzioni. Come si può osservare, l'utilizzo di un maggior numero di istruzioni può migliorare le prestazioni complessive del modello e ridurre significativamente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostra l'effetto di differenti strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, tramite il transfer learning da un dataset di istruzioni naturali, il modello può raggiungere una sensibilità significativamente migliore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Si può inoltre osservare che il transfer learning a partire dal dataset di istruzioni Nitro può contribuire a migliorare significativamente le prestazioni di OFA sul dataset Nitro instruct."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, proponiamo il primo dataset di ottimizzazione delle istruzioni multi-modale su larga scala. Miglioriamo significativamente la capacità zero-shot di OFV ed esploriamo diverse tecniche di transfer learning, dimostrandone i benefici. Progettiamo una nuova metrica denominata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'ultima cosa, stiamo raccogliendo un dataset di ottimizzazione tramite istruzioni multimodali molto più ampio, con circa 150 ulteriori varianti linguistiche di task, che rilasceremo. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto alla presentazione del nostro articolo per ACL 2023, \"Language Model Acceptability Judgements Are Not Always Robust to Context\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Gauthier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, riprendiamo in esame i paradigmi delle coppie minime."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, il paradigma minimale accoppiato valuta essenzialmente i modelli linguistici sulla base di giudizi di accettabilità, che possono includere anche la grammaticalità – ad esempio, il fenomeno di “blimp” – la sintassi, il “gem”, o l’accettabilità in termini di stereotipi, come le coppie incrociate."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici consiste nel presentare una frase accettabile o grammaticalmente corretta, e poi una frase accettabile o non grammaticalmente corretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E allora si auspica che il modello assegni, fondamentalmente, una maggiore probabilità alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione di un modello nei confronti di frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno sviluppando finestre di contesto sempre più ampie. Pertanto, è fondamentale valutare l'adeguatezza del modello in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "Ed è questo ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, questo è l'approccio. Ciò che faremo, dunque, è simulare queste sequenze più lunghe. Rivedremo i dataset stessi e poi ricreeremo frasi scegliendo, ad esempio, frasi accettabili o inaccettabili da tali dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto una tipica coppia di grammaticalità estratta dal dataset Blimp, relativa al caso delle isole adiacenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili che presentino la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticali dall'Isola Argentea."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, possiamo eseguire la stessa operazione scegliendo frasi inaccettabili dallo stesso abbinamento. E questo potrebbe essere utilizzato anche per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso selezionando frasi da un sottoinsieme diverso o da un dataset differente. È ciò che definiamo scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset che state valutando. E possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Questo ci dirà, ad esempio, se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Come se il contesto provenga da un sottoinsieme differente del dataset, oppure se sia completamente irrilevante rispetto alla frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query corrente. E lì riscontriamo che i giudizi MPP sono per lo più robusti per una lunghezza arbitraria del contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Aumentiamo la lunghezza del contesto fino a 1024 per sfruttare appieno i modelli OPT e GPT-2. E, come si evince dalla linea tratteggiata arancione, i giudizi MPP rimangono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "E ora, cosa succede quando scegliamo frasi dallo stesso dataset?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "E siamo qui, quindi, a scegliere o creare frasi da domini accettabili e non accettabili, provenienti dallo stesso dataset di blimp o di gem sintattica."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E allora vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando confrontiamo la struttura, cioè quando selezioniamo le frasi provenienti dallo stesso fenomeno nel testo di colui che imputa la colpa, Jim,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "osserviamo un aumento massiccio o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo, e questo è molto consistente, in questo modo l'effetto aumenta con la lunghezza del contesto. E questo probabilmente influirà sui modelli linguistici più recenti, che dispongono di una finestra di contesto estesa."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché il prefisso della corrispondenza influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di ricostruire la frase di input preservando la struttura rilevante, introducendo al contempo del rumore. E dopo aver eseguito diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "constatiamo che nessuno di questi rumori sta effettivamente modificando il comportamento del modello, ehm, alterandone il corso in termini di come ci mostra l'andamento del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "Fondamentalmente, riscontriamo che i modelli sono sensibili alle perturbazioni e alle frasi in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "In altre parole, quando perturbiamo le frasi all'interno del dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi all'interno del dominio inaccettabile, notiamo una diminuzione dei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i principali risultati del nostro lavoro indicano che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti, le quali sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, nel modo in cui attualmente la conduciamo con input brevi e in singole frasi, potrebbe non cogliere appieno la conoscenza astratta del modello linguistico all'interno della finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Vi preghiamo di leggere il nostro articolo per maggiori dettagli relativi ai nostri esperimenti.\nGrazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Yusheng Zhang e sono della Pennsylvania State University. Oggi presenterò il nostro lavoro, \"Cross-lingual Semantic Parsing in Multiple Natural Languages and Minimal Representations\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, il parsing semantico è un compito volto alla costruzione di rappresentazioni semantiche di query degli utenti, come SQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E l'analisi semantica interlinguistica è il compito di tradurre query in diverse lingue naturali in diverse rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Hau-Tieng Wu, Ph.D.: Come illustrato nella sua figura, è necessario tradurre la query in molteplici lingue naturali utilizzando modelli neurali in sequel lambda o fun QL, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli esistenti di semantic parsing cross-linguale sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "si riscontrano lacune di copertura in alcune aree del linguaggio naturale. Il cinese è assente e"}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "a causa della copertura di alcune rappresentazioni minimali."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "oppure vengono valutati solo in base a determinati modelli neurali. Ad esempio, esiste un solo modello per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A questo scopo, proponiamo un esempio paradigmatico. Forniamo un esempio di dataset uniforme per il collegamento incrociato di analisi semantica in molteplici lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove dataset in diversi ambiti, cinque compiti di parsing semantico, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo i sei ambienti per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione. Utilizziamo l'API di Google Translate per tradurre il testo di partenza nella lingua di destinazione, per poi addestrare una valutazione tramite un modello monlinguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "E per esempio, addestriamo il modello inglese su query in lingua inglese e, durante l'inferenza, traduciamo la query in lingua tedesca tramite API in inglese, per poi utilizzare il modello addestrato per predire l'SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza coincide con la lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo inoltre l’impostazione del campo di ripresa monolingue, addestrando modelli monolingui con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "e testiamo il modello multilingue, che addestriamo come un unico modello per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, uniamo le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre query tedesche o query cinesi o quant'altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo anche il trasferimento zero-shot e few-shot cross-linguale. Addestriamo su una lingua di origine e trasferiamo a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Durante l'addestramento, lavoriamo sull'inglese query o sulla combinazione di query few-shot in inglese e in tedesco per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Pertanto, per quanto riguarda l'analisi dei modelli monolingui, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "inclusi encoder PDR, acronimo di multilingual pre-trained encoders with pointer-based decoders, come XLMR plus PDR e BERT plus PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo inoltre modelli encoder-decoder, ovvero modelli encoder-decoder pre-addestrati multilingua, quali mBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo rilevato che l'architettura Encoder-Decoder ottiene le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "e valutiamo su MT5 e XLMR, oltre che in un contesto multilingue PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo riscontrato che encoder-decoder o encoder-PDR possono essere migliorati tramite l'addestramento in una combinazione di diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo constatato che ciò è dovuto al fatto che la maggior parte delle lingue naturali principali può ottenere un miglioramento delle prestazioni, ad eccezione del fatto che le prestazioni dell'inglese diminuiscono in sette dataset e ottengono un guadagno solo in tre dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come la maledizione del multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo inoltre il divario di performance tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento few-shot cross-linguale. La linea arancione rappresenta il trasferimento zero-shot cross-linguale, mentre la linea verde indica la configurazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando la linea verde e quella arancione, abbiamo rilevato che nel contesto zero-shot, il divario nelle prestazioni del trasferimento interlinguistico è significativo. E confrontando la linea blu e quella arancione, abbiamo riscontrato che nel contesto few-shot, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo anche altri risultati interessanti. Ad esempio, l'architettura encoder-decoder supera i lavori precedenti o ottiene risultati comparabili. L'utilizzo del linguaggio naturale inglese può migliorare significativamente le prestazioni in modalità few-shot per le lingue naturali di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo constatato che modelli linguistici multilingue come CODIS e BLUE risultano ancora inadeguati per compiti di parsing semantico interlinguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo sviluppato Examplar, un benchmark unificato per il parsing semantico cross-angle con diverse lingue naturali e rappresentazioni principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati evidenziano numerose scoperte interessanti, ecc. Vi invitiamo a consultare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo David Villar e fornirò una breve panoramica dell'articolo, \"Grunting Platform Translation: Assessing Strategies and Performance\". Si tratta di un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "PARM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato lo scorso anno, nel 2022. È stato addestrato su una vasta collezione di testo comprendente 780 miliardi di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo due sistemi all'avanguardia. Pertanto, i sistemi con le migliori prestazioni sono quelli valutati da WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche avanzate di traduzione automatica neurale e presentiamo, inoltre, i risultati di una valutazione umana basata sull'esperienza di esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione di PROM."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompting ha una notevole influenza sulle prestazioni dei LLM per la traduzione. Come possiamo constatare da un semplice esperimento in cui utilizziamo il prompting one-shot e forniamo due diversi prompt per ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggior parte delle frasi, 516 su 1000, presentano una differenza superiore a un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo, in casi estremi, può arrivare fino a 40 punti di sfocatura. Pertanto, è fondamentale selezionare una strategia di prompting efficace."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi, nella quale ci limitiamo a contrassegnare ogni frase che forniamo al sistema con la lingua in cui è redatta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di partenza, sono contrassegnate con due punti tedeschi e le traduzioni in inglese con due punti inglesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diverse richieste brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting a zero e a un colpo. E quando si passa, come nel nostro caso, al prompting a cinque colpi, non vi è quasi alcuna differenza nella forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a sostenere maggiormente il peso."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della similarità rispetto alla frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, è importante selezionare gli esempi da traduzioni di elevata qualità. In particolare, confrontiamo i prompt di selezione provenienti dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo sono stati curati in modo molto più accurato e presentano una qualità superiore rispetto ai dati di addestramento, che sono meno strutturati e, di conseguenza, meno performanti. Si ottiene quindi una migliore resa utilizzando i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Ciononostante, sistemi specializzati all'avanguardia presentano un vantaggio significativo rispetto alle traduzioni di Palm. Ma Palm si avvicina notevolmente a un sistema commerciale. Nel nostro caso, abbiamo optato per una valutazione basata su Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo tratto dall'abilitazione umana che abbiamo condotto utilizzando il framework MQM indicano che la fluidità di PALM è paragonabile a quella dei sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "in particolare, l'errore più comune è l'omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra quindi che Palm scelga a volte di produrre una traduzione dal suono migliore omettendo parti della frase originale che vengono incluse nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria style-outward per PAN risulta inferiore rispetto ai sistemi all'avanguardia, il che costituisce un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che PARM fornisce un output davvero fluido, ma con alcuni problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa brevissima panoramica.\nPer maggiori dettagli, vi invitiamo a partecipare alla presentazione completa del paper.\nGrazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Dawei, dottorando all'Università del Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro, \"Più debole di quanto pensi\", un'analisi critica dell'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yusheng, Mario Smusbach, Gia Steffen e DT Schlaukel."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla weak supervision e all’apprendimento con weak supervision."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni deboli sono notevolmente più economiche, ma sono anche più rumorose, il che significa che una certa percentuale di esse risulta inaccurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore etichettato e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'apprendimento con supervisione debole, si propongono algoritmi di addestramento in grado di formare robustamente reti neurali in presenza di rumore nelle etichette, affinché i modelli addestrati mantengano una buona capacità di generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In lavori recenti in WSL, ovvero WSL che sta per Apprendimento Supervisionato Settimanale, è una affermazione comune che si senta dire che i modelli vengono addestrati unicamente con i dati di etichettatura settimanale e si ottengono prestazioni elevate su insiemi di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è errata, ma c'è una riserva."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "il che consiste nel presupporre che sia disponibile un set di validazione aggiuntivo per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo bloccarci su questa impostazione del problema, poiché ciò implica che siano necessarie annotazioni manuali aggiuntive nell’apprendimento settimanale di SuperWise. Ma, come un elefante nella stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il suddetto dubbio ci porta a porre tre domande di ricerca. Innanzitutto, è necessario un dataset di validazione pulito per WSL? O potremmo eventualmente utilizzare un dataset di validazione rumoroso al suo posto?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se è necessario disporre di dati puliti, o se dati puliti sono obbligatori per il corretto funzionamento di WSL, allora quanti campioni puliti ci occorrono? Infine, dovremmo utilizzare i soli campioni puliti per la validazione, o esistono metodi migliori per sfruttarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro, e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, notiamo che, in modo interessante, i recenti metodi WSL richiedono effettivamente campioni di piatti bianchi puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti, si verifica un significativo calo delle prestazioni. Come illustrato in questa figura, in assenza di campioni di validazione puliti, i modelli addestrati non sono in grado di generalizzare al di là delle etichette deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "il che implica che la formazione sia inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati in modo preciso per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti contribuirà a migliorare le prestazioni degli approcci WSL, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "Tipicamente, abbiamo bisogno solo di 20 campioni per classe per raggiungere prestazioni di alto livello."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, l'addestramento diretto su questi ultimi otterrà persino prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di performance tra gli approcci di fine-tuning, applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti unicamente per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo notare, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato negli approcci WSL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo notare dai dati, il modello Van Lina, denominato FTW, inizialmente mostra prestazioni inferiori rispetto a metodi WSL più complessi come i metodi basati sui coseni."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se consentiamo di proseguire il fine-tuning sui campioni puliti, allora FTW ottiene prestazioni paragonabili a quelle degli altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non vi è motivo di scegliere metodi WSL più complessi che richiedono maggiore tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere, abbiamo dimostrato che gli approcci WSL recenti richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro guadagno di prestazioni e la loro praticità sono ampiamente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per sviluppi futuri sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, si riportino i criteri di selezione del modello. Ad esempio, si indichi se la selezione del modello si basa su campioni di validazione puliti e ben definiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con futuri modelli di riferimento per l'apprendimento, poiché entrambi operano su campioni puliti. In terzo luogo, il fine-tuning continuo è un modello di riferimento semplice ma efficace che dovrebbe essere preso in considerazione in futuri lavori relativi a WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR presente in questa diapositiva. Sentitevi liberi di consultarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono James Finch.\nE io sono Sarah Finch.\nE oggi vi illustreremo ABCeval, un nuovo approccio dimensionale alla valutazione dei sistemi di intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dall'Emory NLP Lab, guidato dal Professor Gino Choi presso l'Emory University, e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo quindi che abbiate appena sviluppato un modello di dialogo e desideriate valutare quanto bene si confronti con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale tra due conversazioni sia migliore o di valutare le conversazioni su una scala Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo presenta molteplici aspetti. Pertanto, potreste voler valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste nel chiedere semplicemente a valutatori umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o scale di Likert esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprima determinati comportamenti, come rispondere con informazioni irrilevanti o auto-contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Definiamo questo approccio come l'annotazione dei comportamenti in chat, o ABC eval in forma abbreviata. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat che, secondo la letteratura recente, sono stati ritenuti influire sulla qualità della conversazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare i tassi con cui i modelli di chat commettono diversi errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Le metriche ABC valutano il numero di turni in cui un modello di dialogo ignora il proprio interlocutore o produce un output irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo interlocutore, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chatbot all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato anche queste conversazioni utilizzando tre metodi già esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la prassi standard per valutare i modelli di chat lungo molteplici dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi dei risultati di queste valutazioni, abbiamo riscontrato che le etichette comportamentali ABC sono, complessivamente, più affidabili rispetto alle etichette raccolte tramite metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni etichettate in duplice istanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC si sono rivelate più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può notare come la misurazione della proporzione di turni con contraddizioni auto e partner spieghi rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi medi di coerenza Likert ne spiegano solo il 4% o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ciascuna metrica di valutazione catturi un aspetto unico della qualità della chat utilizzando una regressione lineare a stepwise."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare come la combinazione di tutte le metriche di valutazione ABC spieghi più del 25% della qualità della conversazione. E rimuovendo le metriche una per una, la maggior parte di esse comporta la perdita di una quantità considerevole di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega una quantità di variazione notevolmente inferiore della qualità, e un numero ancora minore di queste metriche presenta informazioni univoche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione maggiore di quanto possibile con i metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare, dai risultati del nostro esperimento, che diverse sfide rimangono ancora irrisolte e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del buon senso in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni irrilevanti in circa il 15% delle risposte. E si contraddicono o contraddicono il loro interlocutore all'incirca nel 10% dei casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido ritmo di miglioramento nel settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dalla nostra valutazione. Tuttavia, ciò è tanto più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC Eval possa essere utilizzato da altri nel settore come un passo significativo in questa direzione. E siamo ansiosi di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Kaio-Yin e presenterò il nostro lavoro intitolato Quando la traduzione richiede un contesto? Un'esplorazione multilingue basata sui dati. Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, Andre F.D. Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Beh, se la frase precedente fosse, le cose potrebbero iniziare a farsi pericolose se i ministri lo scoprissero, allora Mo si riferisce a una spia. Ma se la frase precedente fosse, potrebbe trattarsi di qualcosa di serio, dottore? Allora Mo si riferisce a un segno di nascita."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli siano in grado di tradurre casi come questo è piuttosto difficile. Innanzitutto, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende metriche a livello di corpus come BLEU incapaci di cogliere tali traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "E alcuni hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, dato che di solito si basano sulla conoscenza del dominio e sulla curatela umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande. Innanzitutto, quando la traduzione richiede un contesto? E in secondo luogo, come gestiscono questi casi i modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo cominciato misurando quanto una parola dipenda dal contesto nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. E ciò viene fatto misurando quanto informazioni il contesto C fornisce riguardo alla lingua di destinazione Y, dato il testo sorgente X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "si può considerare il CXMI come l'informazione ottenuta fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un PSXMI elevato come quelle che richiedono contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un'elevata PCXMI per individuare schemi ricorrenti tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E conduciamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 diverse lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi a tre livelli diversi. Innanzitutto, esaminiamo i tag di parte del discorso che presentano valori medi di PCXMI elevati."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di individuare, ad esempio, pronomi duali in arabo che presentano un P6MI relativamente elevato. E ciò può essere spiegato dal fatto che l'inglese non possiede pronomi duali, pertanto è necessario il contesto per determinare se un pronome sia duale durante la traduzione in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E analogamente, riscontriamo che anche certe lingue richiedono un contesto quando si desidera scegliere la forma verbale appropriata. Successivamente, esaminiamo gli elementi lessicali che presentano un alto valore PCSXMI mediato su tutte le sue diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come quello che vediamo qui, dove in cinese è necessario del contesto per tradurre i nomi propri, al fine di garantire l'utilizzo della stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E analogamente, constatiamo che il contesto supporta la traduzione con la giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "E infine, esaminiamo diversi token individuali che presentano un elevato valore di p6mi. Ciò ci consente di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ora utilizziamo i risultati della nostra analisi per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che vi si riferiscono, e chiamiamo il nostro tagger Multilingual Discourse Aware, o MUDA, tagger."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi anche notare che le diverse lingue presentano proporzioni differenti di questi fenomeni discorsivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi il tagger Muda applicandolo sul corpus parallelo che desideriamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto identificati dal tagger Muda."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "E infine, utilizziamo il nostro benchmark, unitamente ad altre metriche, per valutare diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, come nel caso di BLEU, riscontriamo che i modelli indipendenti dal contesto presentano le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma allora, se utilizziamo COMET, i modelli sensibili al contesto ottengono i risultati migliori. E se utilizziamo la misura F a livello di parola, allora i modelli con o senza contesto presentano prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo unicamente metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Muda per valutare i modelli, e riscontriamo che i modelli sensibili al contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per determinati fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono significativamente migliori dei modelli che non utilizzavano il contesto in altri fenomeni come le ellissi, i pronomi e le forme verbali. Questo suggerisce, quindi, dove sarebbe necessario vedere ulteriori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo inoltre diversi sistemi commerciali e il nostro benchmark dimostra che DeepL è generalmente più preciso di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, conduciamo un'analisi basata sui dati su 14 coppie linguistiche per individuare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri risultati per creare un parametro di riferimento per la traduzione automatica a livello di documento, il quale può aiutarci a identificare quali fenomeni discorsivi i modelli sono in grado di gestire efficacemente e quali no, e quali sistemi di traduzione sono performanti nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio molto per la sua attenzione. Ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrak e vi presenterò i nostri lavori su Dr. BERT, un modello pre-addestrato robusto in lingua francese per il dominio biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, inizieremo discutendo della modellazione del linguaggio in ambito sanitario. Successivamente, esporremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in francese denominato Dr. Bert, basato su Roberta e addestrato su NACHOS, un dataset di dati crowdsourcing medici provenienti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto un confronto di modelli con diverse impostazioni di pre-training e fonti di dati. Successivamente, presentiamo i nostri risultati su 11 compiti downstream biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine, concludiamo con le analisi sperimentali e forniamo maggiori dettagli su come accedere ai modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dalla sua pubblicazione nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale, offrendo notevoli incrementi di performance rispetto a metodi statici e contestualizzati precedenti come Word2Vec, FastText o NWO."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a numerose altre lingue, come in francese con Camembert, e ad altri ambiti come il biomedicale con PAMED-BERT e BioBERT, e in ambito clinico con Clinical-BERT, ma prevalentemente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "Modelli specializzati per altre lingue sono rari e spesso si basano su un pre-addestramento continuo a causa della scarsità di dati specifici per quel dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora la Francia non disponeva di soluzioni open source moderne per il settore biomedico."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Noi, quindi, ci poniamo la domanda su quali siano le fonti di dati più appropriate per un'ampia gamma di applicazioni. E i dati correnti sono una buona sostituzione per i dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dott. Burt con il nostro modello di Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario che ci ospita."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, ci chiediamo: quanti dati occorre per addestrare un modello specializzato su dati francesi? Sono 4 gigabyte, 8 gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, addestriamo e confrontiamo inizialmente quattro modelli sviluppati autonomamente. Una prima versione di Dr. Bert con sette gigabyte di dati di addestramento, una seconda versione con quattro gigabyte di set di dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con 4 GB di frasi tratte da annotazioni cliniche. E una versione finale di Schubert con una combinazione di un sottoinsieme di 4 GB di dati naturali e 4 GB di annotazioni cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con un pre-training continuo per analizzare l'impatto delle strategie di pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso di Camembert e addestrato su quattro gigabyte di patatine. Un altro anch'esso basato su Camembert, ma addestrato questa volta su quattro gigabyte di clink e molte altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, uno basato sul modello biomedico inglese, Bermud-Bert, e addestrato su quattro gigabyte di set di estortori. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo una serie di compiti pubblici e privati che non generano grande entusiasmo, come il riconoscimento di nomi e identità, la classificazione, l'etichettatura morfologica e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Tali modelli vengono confrontati con sei modelli di riferimento, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCnet 4 GB, Pumatbert, BioBERT e ClinicalBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione di modelli che evidenziano quali ottengono i risultati migliori in un compito, utilizzando dati di natura simile a quelli su cui il modello è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, analizzando i dati da cui possiamo ricavare informazioni, possiamo osservare che i dati provenienti da fonti eterogenee risultano più versatili. Osserviamo inoltre che l'utilizzo di una maggiore quantità di dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, la formazione gratuita e da zero sembrava ottenere prestazioni superiori nella maggior parte delle attività."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul pre-training continuo utilizzando il peso e il tokenizer di Pumet-BERT, addestrato sul sottoinsieme di 4 gigabyte di NACHOS, ha mostrato risultati comparabili a quelli ottenuti con Dr.BERT 4 gigabyte addestrato da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Il che non è il caso del modello basato sui pesi di CamemBERT e sul token \"leather\", che presenta problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "In conclusione, il nostro sistema, opportunamente configurato, offre prestazioni superiori in 9 delle 11 attività successive e supera globalmente i risultati del modello generico qui considerato, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, dati più specializzati sono migliori, ma non si scalano bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili sulla piattaforma UGIM, e tutti gli script di addestramento si trovano nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio quindi per questa presentazione, e attendiamo con interesse l'attuazione di azioni durante l'incontro successivo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando il multiset tagging e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro svolto in collaborazione con i miei relatori, Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità di un discente di gestire una ricorsione più profonda e composizioni inedite di frasi che sono state osservate individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'analisi semantica, il test per la generalizzazione composizionale potrebbe apparire così. Come al solito, abbiamo un insieme di addestramento di enunciati, in questo caso, la ragazza dormì e Maria sapeva che la ragazza dormì."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste espressioni sono abbinate a forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard di machine learning, il set di test non deriva dalla stessa distribuzione, ma contiene forme logiche strutturalmente inedite."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha riscontrato una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequence-to-sequence naïf faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate a colori nell’esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo diffuso per affrontare questo problema è l'integrazione di alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono pensati per catturare il processo composizionale che mette in relazione gli enunciati con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Funziona bene, ma gli alberi di solito non vengono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta costoso dal punto di vista computazionale. Tipicamente, ciò comporta una considerevole pre-elaborazione specifica per il formalismo delle forme logiche, ad esempio, per gestire i simboli di variabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L’ottenimento di alberi può anche implicare procedure specializzate di induzione grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello neurale sequenza-a-sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento su alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la generazione dell'output a partire dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, etichettiamo ogni token di input con un multinsieme non ordinato di token che compariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché, nella seconda fase, utilizziamo un altro modello per predire una permutazione che ne definisca l'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Ciò rende il nostro approccio particolarmente flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Procediamo, da sinistra a destra, sull'output e determiniamo quale token di multinsieme inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Quindi passiamo al successivo token del multinsieme per determinare il secondo token nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo analogo, saltando a un altro token del multinsieme. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "fino a quando ogni token della prima fase non sarà stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli privi di alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcune altre forme di generalizzazione strutturale rimangono tuttavia estremamente complesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo a quale multisettore esso appartenga, il che pone una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono molteplici permutazioni coerenti con i dati, ma quella linguisticamente corretta rimane latente. Affrontiamo questo problema inducendo l’allineamento come parte dell’addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con il punteggio più alto è un problema NP-difficile. Questo perché è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Lo approssimiamo con una rilassamento continuo ottimizzato per GPU che ci permette inoltre di propagare a ritroso attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desiderate approfondire le nostre ricerche e il modo in cui affrontiamo queste sfide, vi invitiamo a consultare il nostro articolo scientifico oppure a visitare il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, sono Akshata e oggi, con il mio coautore Martin, presenteremo il nostro lavoro, The Kipma Steps, che valuta l'integrazione della conoscenza da molteplici fonti. Questo lavoro è frutto di una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio naturale attingono a diverse fonti di conoscenza, come la conoscenza contenuta nei loro parametri, acquisita solitamente tramite il pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti in compiti quali il question answering dimostrano che i modelli possono utilizzare conoscenze temporali pre-addestrate per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Giovanni ha visto il presidente appena eletto in televisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i precedenti e cos'è una TVA, ma non possono conoscere in modo affidabile chi sia questa entità specifica per l'incidente, John, o chi sia il nuovo presidente, poiché il precedente potrebbe essere cambiato da quando è avvenuto il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella acquisita durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Presentiamo un compito di risoluzione della coreferenza progettato per valutare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti a uno studio condotto da esseri umani e sviluppiamo modelli di risoluzione della coreferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Thirvin è un giudice. Kia è una panettiera. Thirvin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro trascorsa a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo qui è identificare l'entità corretta a cui il pronome \"he\" si riferisce, che in questo caso è servitore."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome richiede due tipi di informazione. In primo luogo, conoscenza specifica dell'entità, come “survey è un giudice”. E in secondo luogo, conoscenza di base, come “i giudici decidono le cause in tribunale”."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "Generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica di entità viene tipicamente osservata al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Moduliamo la disponibilità di queste due tipologie di informazione in modo che possano essere reperite sia in un'unica fonte che in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre configurazioni di KITMOS. Innanzitutto, abbiamo la configurazione tipica, pre-addestramento con conoscenza di base, in cui si assume che la conoscenza di base sia disponibile al momento del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, esiste il contesto, sia l’ambientazione – dove la conoscenza pregressa è disponibile sia durante la fase di pre-addestramento che durante l’inferenza – sia l’ambientazione dell’inferenza del contesto, dove entrambi i tipi di conoscenza sono disponibili unicamente durante l’inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questo scenario finale è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, poiché nuove professioni si sono sviluppate dopo il periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti nella fonte originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nell'impostazione pre-addestrata, si assume che la conoscenza di base, ovvero che i politici ambiscono a ricoprire incarichi eletti nel governo, sia contenuta nei parametri pre-addestrati. Nel contesto temporale infrequente, forniamo la conoscenza anti-specifica, Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nell'impostazione di background-bove, forniamo inoltre non solo conoscenze anti-specifiche, ma anche informazioni di base sui politici nel contesto di interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "e l'impostazione dell'interferenza di sfondo, forniamo l'occupazione fittizia Meritur anziché Politico, poiché Meritur è improbabile che sia contenuto nel paradigma pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il dataset sia con partecipanti umani che con modelli di risoluzione della co-referenza consolidati. In questa figura, presentiamo i risultati dei modelli con le migliori prestazioni sulla variante più complessa della configurazione di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro addestramento specifico su KITMOS, entrambi i modelli non ottengono buoni risultati. Quando addestrati su KITMOS, tuttavia, sia C2F che BFQF ottengono prestazioni significativamente migliori rispetto a una scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando addestrati su dataset generali di risoluzione della co-referenza, i modelli imparano a sfruttare indizi superficiali, i quali non sono utili quando testati su Kitmos dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriori esperimenti con conoscenze fittizie indicano che, anche i modelli con le migliori prestazioni non riescono ad integrare in modo affidabile le conoscenze pregresse fornite unicamente al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i punti salienti del nostro articolo, molti modelli rivoluzionari di coreferenza risultano incapaci di ragionare su conoscenze provenienti da fonti diverse senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da fonti multiple."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Ciononostante, anche i modelli più performanti sembrano avere difficoltà a integrare in modo affidabile le conoscenze pregresse presentate unicamente al momento dell'inferenza. Se desiderate ulteriori dettagli, vi invitiamo a consultare il nostro articolo e ad esaminare il dataset su Code su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo, \"Marked Personas: Uso di Prompt in Linguaggio Naturale per Misurare gli Stereotipi nei Modelli Linguistici\". Questo lavoro è stato realizzato in collaborazione con Esen Dermush e Dan Jorofsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, numerosi studi hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano diverse limitazioni. Solitamente si basano su dataset costruiti manualmente, che richiedono un notevole dispendio di tempo per la curatela."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E misurano, inoltre, di solito solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre fasce demografiche o contesti, o che semplicemente catturano associazioni molto generali e ampie, come associazioni negative con specifici gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte dei lavori in questo ambito non tiene conto dell'intersezionalità, intesa come la nozione che le identità sociali complesse e sfaccettate possano aggravare i pregiudizi e costituire luoghi unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi più recenti LLM ottimizzati tramite istruzioni sono molto abili a rispondere alle istruzioni fornite nei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, possiamo chiedere al modello di generare una persona, ovvero una rappresentazione di un individuo immaginario, utilizzando un prompt come: immagina di essere una donna asiatica, descrivi te stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente constatare che questo è estremamente generalizzabile a qualsiasi gruppo demografico, perché possiamo semplicemente specificare qualsiasi marcatore identitario desideriamo all'interno di questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco alcuni esempi di generazioni da parte di GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente notiamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di questi termini,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è raffigurata come modesta.\nLa donna mediorientale è descritta con termini quali esotico e che richiamano una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le figure femminili di colore fanno riferimento alla discendenza, mentre la figura maschile bianca non presenta nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si articola in due fasi. La prima consiste nella generazione di queste personas."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri stimoli per generare queste persone sono stati ispirati da uno studio in cui tali stimoli sono stati presentati a soggetti umani, accertando che, fornendoli a soggetti umani, si riusciva anche a far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E ciò consente inoltre un confronto diretto tra le nostre persone generate e le risposte redatte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è costituita dalle parole “marcate”, che rappresentano un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati, cosa che espanderò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che otteniamo stereotipi e schemi davvero specifici senza dover fare affidamento su un lessico particolare."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, il metodo delle parole segnalate si basa sul concetto sociolinguistico di sproporzione, che afferma che esiste una forma predefinita e non marcata, e qualsiasi gruppo che ne differisce è linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, la parola \"warrior\" (guerriero) è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, specificano di solito un guerriero da uomo e contrassegnano il termine con \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, designiamo innanzitutto quali siano i gruppi di riferimento e i gruppi sperimentali."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo le personalità utilizzando il metodo delle parole chiave, che consiste fondamentalmente nell'impiegare log-odds ratio ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ad esempio, per le persone che rappresentano donne nere, useremmo espressioni combattive e confronteremmo i rapporti dei “law gods” con sia le persone bianche che quelle maschili, poiché questi sono i due gruppi corrispondenti non marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "E ora, alcuni risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, riscontriamo elementi ben diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene le personalità generate mostrino tassi significativamente più elevati per le parole Luxon, quelle scritte da esseri umani presentano una distribuzione di parole molto più ampia, mentre le parole stereotipiche presenti nelle personalità generate sono in realtà limitate alle parole “alto” e “atletico”."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, essenzialmente solo quelli positivi o, quantomeno, non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E in effetti, questo lessico non riesce davvero a cogliere molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole marcate per illustrare come queste parole apparentemente positive favoriscano stereotipi e narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, dimostriamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, per i gruppi definiti, le parole più frequenti includono concetti come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono tali gruppi unicamente in relazione alla loro identità, distinguendoli dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Ciò contribuisce a una lunga eredità di discriminazione e di alienazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, vi sono numerosi tropi comuni che si riflettono in queste parole, in particolare per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono termini come \"vivace\" e \"formosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che si collegano a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come \"petite\", \"delicata\" e \"setosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si ricollega a una lunga storia di iper-sessualizzazione delle donne asiatiche, percepite come estremamente docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, per le donne afroamericane, osserviamo che alcune delle parole più frequenti sono termini come forte e resiliente."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si ricollega a un archetipo che molte persone definiscono l'archetipo della \"donna nera forte\". E se a prima vista può sembrare positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono emerse ricerche che dimostrano come questo tipo di archetipo sia in realtà molto dannoso, poiché esercita una notevole pressione su questi gruppi demografici affinché siano resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Piuttosto che impegnarsi concretamente nel superamento di tali ostacoli, ciò esercita una pressione su tali individui affinché li vincano, il che porta a esiti sanitari estremamente negativi per queste persone, unitamente ad altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "Più in generale, constatiamo che le parole utilizzate per ciascun gruppo contrassegnato riflettono in larga misura narrazioni fortemente essenzialistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, noi, in quanto ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo altresì utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni, poiché molti elementi potrebbero essere trascurati se non lo facessimo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E, infine, dovrebbe davvero esserci maggiore trasparenza riguardo ai metodi di mitigazione dei bias."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, per esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a qualche sorta di..."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un'eccessiva enfasi sull'allineamento dei valori in atto, o forse altri metodi simili, come quelli antirazzisti, che stanno generando queste dannose dinamiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo semplicemente fare alcuna assunzione né approfondire ulteriormente lo studio senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "La ringrazio molto per aver ascoltato. Le auguro un buon tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Jingwei Yi e sono della University of Science and Technology of China."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È con piacere che presento un breve video promozionale sul tema della carta, dal titolo \"Are You Copying My Model?\": Protezione del Copyright dei Modelli Linguistici di Grandi Dimensioni per Embedding e Servizi tramite Watermark Backdoor."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo innanzitutto il contesto relativo a Embedding come Servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i modelli linguistici di grandi dimensioni come GPT, LAMA, PALM eccellono nella comprensione e generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding come servizio è uno dei servizi sviluppati sulla base di modelli linguistici di grandi dimensioni per supportare diverse attività di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che l’attaccante può sottrarre il modello attraverso l’apprendimento basato sull’embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d’autore dell’embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "per tutelare il copyright dei servizi integrati. Una delle soluzioni è quella di incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo di watermark deve soddisfare le seguenti proprietà. Innanzitutto, il metodo dovrebbe essere applicabile all'embedding di servizi pubblicitari. In secondo luogo, il watermark non dovrebbe degradare l'utilità degli embedding forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere sufficientemente discreta da non essere individuata dall'attaccante, oppure quest'ultimo dovrebbe essere in grado di rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell’attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "I lavori esistenti possono essere classificati, in linea generale, in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo risulta o non applicabile all’integrazione di servizi pubblicitari, o manca di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo, proponiamo EmbeddingMarker, che è un metodo di watermark basato su backdoor applicabile all'integrazione di servizi pubblicitari."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "A questo punto, permettetemi di illustrare i dettagli del nostro Embedding Marker. L'Embedding Marker comprende due fasi principali: l'inserimento del watermark e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di queste fasi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Assumiamo che il fornitore possa raccogliere un corpus testuale generale e calcolarne la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di watermark, definiamo innanzitutto un embedding di destinazione. Quando un utente invia una frase al servizio provider, quest'ultimo conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nell'identificare se un modello alla base di un altro servizio contenga il marchio."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo innanzitutto una backdoor e un dataset benigno. Il dataset backdoor contiene frasi in cui tutte le parole appartengono all'insieme di trigger. Mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger,"}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, il fornitore richiede gli embedding al servizio Steeler, specificando il dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la similarità coseno e la similarità L2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di similarità tra i dataset benigni e quelli compromessi da backdoor, definita come delta coseno e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Conduciamo esperimenti su quattro dataset: AGnews, Mind, SSD2 ed Eraspam. Assumiamo che il fornitore applichi il dataset Wikitext per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro dataset dimostrano che il nostro marcatore incorporato può garantire prestazioni di rilevamento eccellenti mantenendo al contempo un’elevata utilità per le attività successive."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Validiamo inoltre la segretezza dell'embedding fornito visualizzando l'embedding di frasi su 4DataSet VOPCA. La legenda delle figure indica il numero di trigger in ciascuna frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come illustrano le figure, è difficile distinguere tra gli embedding fattorizzati e gli embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto. Grazie. Siamo lieti di discutere con voi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro, accettato per ACL 2023 come articolo lungo, sul transfer learning per il rilevamento di dissonanze, affrontando la sfida delle classi rare."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e spiegando perché si tratta di un problema importante da studiare in ambito linguistico. In poche parole, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Come in questo esempio, dove una persona afferma: \"So che le sigarette potrebbero uccidermi\" e poi prosegue dicendo: \"Mi sono preso un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono incoerenti e si trovano in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il fatto che ritenga di non poter mantenere il mio impiego senza il loro supporto giustifica la seconda occorrenza e sussiste una relazione di consonanza tra loro."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è in realtà raro trovarla espressa nel linguaggio rispetto ad altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Ma perché questo è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, monitorare le tendenze e i valori delle credenze, nonché i cambiamenti di atteggiamento nelle popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'elevato livello di dissonanza cognitiva è inoltre correlato ai disturbi d'ansia e può contribuire a una migliore comprensione della salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio della dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo adottato un approccio basato sulla dissonanza, come illustrato nel diagramma di flusso qui presentato."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati mediante un parser PDTV e le coppie di unità discorsive sono state annotate in conformità alle linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può notare qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Dopo aver raccolto circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale, addestrato unicamente su 43 esempi di disnettature. Non sorprendentemente, il classificatore ha fornito risultati poco migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la scarsità di dissonanze e l'assenza di set di dati precedenti simili, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per attenuare ciò, sperimentiamo con combinazioni di transfer learning e active learning al fine di effettuare annotazioni tali da raccogliere un maggior numero di campioni dissonanti con un minor numero di cicli di annotazione, riducendo così i costi complessivi di annotazione e migliorando al contempo il rilevamento della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Dato che il modello iniziale non era in grado di catturare affatto la classe di dissonanza, avviamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da due compiti differenti. La dissonanza indipendente dall'argomento si presta alla classificazione, un compito che determina se due affermazioni di dibattito provenienti da persone diverse siano concordi o discordi, indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "definito dibattito qui e sulla classificazione binaria di espansione e confronto delle classi PDTB, poiché questi due sono strettamente legati alla concezione di consonanti e dissonanza e li chiamiamo CEE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che, al trasferimento, le prestazioni zero-shot sul dataset annotato sono già significativamente migliori del caso, con il punteggio migliore pari a 0.62 in termini di AUC."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, nell'affinamento iterativo su entrambi i compiti, rileviamo che l'affinamento dei compiti CE seguito da un ulteriore affinamento sul dibattito produce una performance zero-shot significativamente migliore. Perciò, questo è il modello che utilizziamo per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni iterazione di apprendimento attivo e annotazioni. Cumulator accumula tutti i dati raccolti dalle annotazioni attive finora, mentre aggiornamenti iterativi aggiornano il modello tramite l'addestramento sul set di dati più recente raccolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo riscontrato che l'approccio cumulativo ha performato in modo equivalente o superiore a quello iterativo in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, impieghiamo una strategia di probabilità per la classe rara, PRC, per selezionare prevalentemente gli esempi che il modello corrente ritiene altamente probabili come dissonanti in ogni round di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Lo confrontiamo con le altre strategie all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo constatato che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima. Si noti che le prestazioni sono significativamente inferiori per il caso casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "In ulteriori iterazioni di AL con due strategie ottimali, abbiamo migliorato l'AUC di classificazione della distanza a 0,75, che rappresenta il risultato migliore ottenuto finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo inoltre la fattibilità di ciascuna strategia in termini di qualità delle annotazioni e costi per gli annotatori. Riscontriamo che PRC presenta la più alta percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, concludiamo che PRC è una strategia AL semplice per l'acquisizione di classi rare e l'AL di avvio a freddo, supportato da attività di transfer learning opportunamente progettate, può contribuire significativamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, riscontriamo che l'aggiornamento iterativo risulta utile per il transfer learning da un dominio differente, mentre le annotazioni attive all'interno del dominio stesso beneficiano di un aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro dataset principale e al nostro articolo. Non esitate a contattarci in caso di domande. Grazie."}
