{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di DeepLean, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "L'amplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di ampliamento del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "E nell'esempio qui, potete vedere un paio di frasi allineate in parallelo di una frase tedesca complessa e la sua traduzione in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come si può vedere nell'esempio, come la sostituzione lessicale, l'eliminazione delle clausole, la riorganizzazione o l'inserimento di elenchi puntati."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo ora il nostro nuovo corpus dplane. Perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Quindi, per esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo Corpus dPlane, che è suddiviso in due sottocorpus, dPlane APA e dPlane web. DPlane APA si basa su testi di notizie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In DPlane APA, abbiamo allineato manualmente 483 documenti, ottenendo circa 30.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per dplane web, questo corpus include diversi ambiti e allineiamo tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio per quanto riguarda il tipo di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come potete vedere qui, i testi biblici sono molto più semplici rispetto, ad esempio, al testo delle notizie o ai testi per studenti di lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, per esempio per quanto riguarda la semplificazione lessicale, la semplificazione strutturale, anche su tutti i livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, si può notare che il nostro corpus DPlane presenta una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus DPlane API, abbiamo molte più operazioni di riordino e aggiunta di parole rispetto al corpus DPlane web."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo quindi cosa possiamo fare con questo corpus. Ciao, sono Omar, e ora parlerò dei casi d'uso per il nostro dataset dplane. Quindi, per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti post."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso d'uso, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli, nella stessa lingua, con lo stesso contenuto, ma con un livello di complessità diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora, dato che abbiamo il nostro set di dati dplane che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è il caso della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "raffinando i modelli linguistici per produrre un testo semplificato dal testo complesso di input."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo perfezionato due modelli diversi. Abbiamo perfezionato il modello di importazione lunga per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche perfezionato l'importazione di base normale per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare tutti i punti di controllo e approfondire i dettagli dei punteggi e delle metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo aggiustamento fine di base potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo questi risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skurkovsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura del coordinamento Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "È tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso, Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è adottato nella teoria del significato del testo di Igor Milchuk, dove anche in questo caso l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Escludono uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora esistono anche approcci simmetrici per coordinare le strutture, come l'approccio PRUG, l'approccio basato sulla congiunzione ipotizzato nei banchi di alberi di dipendenza PRUG, dove le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze dall'inizio alla fine di tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio multi-testuale, utilizzato, ad esempio, nella grammatica delle parole di Dick Cutzman."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, qui le risate, a tutti i congiunti separatamente. Questi sono Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è di presentare un nuovo argomento a favore delle strutture di coordinazione simmetriche come queste due e contro le strutture di coordinazione asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Ok, l'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò in base a questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in inglese, come forse saprete, i nostri oggetti diretti preferiscono essere vicini al verbo mentre gli accessori possono essere più lontani, giusto? Quindi \"march read it yesterday\" è corretto perché l'oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Mentre March ha letto ieri è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un avverbio ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo il bordo."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. Ieri Marco ha letto questo libro assolutamente affascinante sulla BC, è corretto, dove invece di it abbiamo questo lungo NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire, Marge ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che afferma che si preferiscono dipendenze più brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo una dipendenza da \"leggere\" all'aggettivo di lunghezza 7 misurato in parole e da \"leggere\" a \"libro\" di lunghezza 4. Quindi insieme è 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Quindi invece di undici, sei molto più breve. Ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Ok, quindi abbiamo estratto varie statistiche sul coordinamento dalla versione migliorata della banca dati PenTree e abbiamo visto il documento per cui non abbiamo utilizzato le dipendenze universali."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione fatta molte volte prima che i congiuntivi a sinistra tendono ad essere più brevi, quindi sale e pepe e non pepe e sale misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due coniugati cresce, il coniugato più corto preferisce essere il primo più forte, giusto? Quindi la proporzione è maggiore per il coniugato corto a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma la novità di questo studio è che abbiamo osservato che questa tendenza si verifica solo quando i governatori a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa, quindi è il governatore, è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo esempio, \"Omero venne e starnutì\", manca. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi, in questi casi, il congiunto a sinistra preferisce essere più breve. Tanto più, quanto maggiore è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è di destra, come qui, la sinistra governa il coordinamento Telenet, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi dimostrato che misurando la lunghezza in caratteri, cioè la prima colonna in sillabe, la colonna centrale e in parole, la colonna di destra. Mi concentrerò quindi su quella di destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza per il congiunto a sinistra di essere più breve cresce costantemente con la differenza assoluta di parole, e lo stesso si osserva quando non c'è un governatore, come nel coordinamento delle frasi, ma quando il governatore è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E nel documento mostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due e per le strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per l'accordo e gli argomenti completi, si veda il documento, scusatemi, e ne parleremo dopo la sessione. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiang Bin, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce dei pregiudizi politici che portano a modelli di NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati di scansione web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di notizie politiche sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul C four Corpus, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una situazione ambivalente per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, il che celebra la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente influenzate da pregiudizi sociali e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo di indagare il flusso di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici e ai compiti a valle, chiedendoci specificamente le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo la leadership politica dei modelli linguistici e quale ruolo potrebbero avere i dati di pre-addestramento su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse unità politiche nei compiti successivi e se ciò potrebbe comportare problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in particolare, proponiamo innanzitutto di stimolare i modelli linguistici con diversi formati di input utilizzando i questionari politici, come il test del compasso politico. Questo ci permette di effettuare una valutazione automatica ben fondata sulla letteratura scientifica politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i primi modelli linguistici hanno significati politici variabili. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che GPT 4 è il modello linguistico più liberale di tutti, e la serie GPT è generalmente più liberale socialmente rispetto alla serie BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, ci proponiamo di indagare fino a che punto i pregiudizi politici dei modelli linguistici siano effettivamente tratti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Potremmo quindi condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint del modello linguistico su sei diversi gruppi di partito, separati in notizie e social media, ulteriormente divisi nei loro significati politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Con un ulteriore pre-addestramento dei modelli linguistici su tali parti nei corpora, possiamo osservare che anche le coordinate ideologiche del modello linguistico cambiano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nel caso di Roberta, ulteriormente perfezionata e addestrata sul corpus di Reddit di orientamento di sinistra, possiamo osservare un notevole spostamento verso posizioni liberali in termini di..."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "per quanto riguarda i suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici possano cogliere la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dividiamo i corpora di pre-addestramento in pre 45° Presidente degli Stati Uniti e dopo 45° Presidente degli Stati Uniti, e addestriamo separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che i modelli linguistici in generale hanno avuto una tendenza politica più lontana dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche rilevare la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, valutiamo i modelli linguistici con diversi significati politici per quanto riguarda il rilevamento del discorso d'odio e il rilevamento di notizie false, per le applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che se analizziamo le prestazioni per categoria, cioè se separiamo le prestazioni in:"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Analizzando diverse demografie o significati politici dei media, possiamo osservare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nel rilevare l'incitamento all'odio rivolto ai gruppi sociali minoritari."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i nostri lavori si concentrano sul rilevamento del discorso d'odio rivolto a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E viceversa, i modelli linguistici con inclinazione di destra sono migliori nel rilevare il discorso d'odio rivolto ai bianchi e agli uomini, ma peggiori nel rilevare il discorso d'odio rivolto ai neri LGBTQ+ e ad altre comunità di minoranza."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Eventi simili si verificano anche per il rilevamento delle notizie false, dove si osserva che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione proveniente dalla loro linea politica opposta e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "Questo mostrerà ulteriormente molti esempi qualitativi per vedere che i modelli linguistici hanno significati politici diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "Forniscono previsioni diverse per gli esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che esiste un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello linguistico a riga destra dovesse essere perfezionato su discorsi d'odio o disinformazione o altro e distribuito su una popolare piattaforma di social media."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e gli incitamenti all'odio contro i gruppi minoritari potrebbero diffondersi senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ha fatto scattare l'allarme per riconoscere e affrontare le questioni di equità derivanti dal comportamento politico dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi un po' di discussione. Vorremmo anche sottolineare che mettiamo in evidenza il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Sila e Kryptidis."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici e poi ai compiti a valle, creando in ultima analisi problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo a censurare in qualche modo, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia realmente neutrale e cosa dovrebbe essere mantenuto nei dati di addestramento del modello linguistico. È un po' come il problema di Charlie elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, ottimo. Penso che sia praticamente tutto quello che ho per oggi. Grazie per il suo tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il vostro lavoro, Enol Positionale, Caratterizzazione dei pregiudizi di progettazione nei set beta dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Quindi iniziamo immaginando che lavoriate per un giornale e che stiate setacciando i commenti sotto il vostro articolo di notizie cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti rivolgerti a un'API popolare come Perspective API per il rilevamento della tossicità. E questo funziona davvero bene se sei Carl Jones, dove Perspective API è in grado di rilevare correttamente le istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio il caso di Dithyasharma, dove l'API di prospettiva non è davvero così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio di progettazione in cui si osservano differenze sistematiche di prestazioni della tecnologia tra le popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "I pregiudizi di progettazione come quello che abbiamo appena visto prima possono verificarsi a causa della posizionalità dei ricercatori di NLP e degli sviluppatori di modelli. La posizionalità è semplicemente la prospettiva che le persone assumono a causa delle loro caratteristiche demografiche, della loro identità e delle loro esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati perché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno una posizionalità?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di dire che i modelli stessi e i set di dati stessi abbiano identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i lavori precedenti hanno suggerito alcune prove aneddotiche dell'esistenza della posizionalità, come le lacune culturali nei modelli e nei dataset, oltre alle definizioni teoriche della posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano sul confronto tra gli utenti finali e i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E studiare la modellazione e la posizionalità dei set di dati diventa sempre più importante man mano che i compiti di NLP diventano più soggettivi e socialmente orientati."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E è difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare il set di dati e la posizionalità del modello, confrontiamo le annotazioni con gli utenti reali con i set di dati e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro framework, NL Positionality."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro modello funziona in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è quello di riannotare i set di dati con diversi annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E scegliamo di farlo piuttosto che esaminare la demografia dei set di dati originali, ehm, degli annotatori, perché di solito solo alcuni annotatori annotano ogni istanza e perché la demografia viene raramente raccolta e condivisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "Perciò abbiamo scelto di riannotare i dati per ottenere molte annotazioni per ogni istanza e per ottenere un ricco insieme di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando un punteggio di correlazione di Parsons R."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, il nostro quadro si differenzia dalla letteratura sul disaccordo tra annotatori confrontando gli utenti finali con modelli e dataset, previsioni ed etichette, anziché limitarsi a osservare il consenso tra annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte reso possibile da Lab in the Wild, una piattaforma di crowdsourcing online per i nostri collaboratori HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto a piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. E inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Nel Lab in the Wild abbiamo due compiti, uno dei quali riguarda l'accettabilità sociale. Il modo in cui funziona è che i partecipanti leggono una situazione del dataset di chimica sociale e poi scrivono quanto una situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle di un'intelligenza artificiale e di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con la chimica sociale, Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi replicato una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un esempio da Dana Hate e scriveranno se pensano che sia un esempio di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, HateRoberta e GPT-4. Il nostro studio ha infine raccolto oltre sedicimila annotazioni da oltre mille annotatori di ottantasette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora siamo meglio attrezzati per rispondere con chi si allineano maggiormente i set di dati e i modelli di NLP. Abbiamo scoperto che esiste una posizionalità nel NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo riscontrato che i dataset e i modelli sono più allineati ai paesi anglofoni. Quindi, per l'analisi dell'accettabilità sociale del GPD 4, abbiamo riscontrato che è più allineata ai paesi confuciani e anglofoni. Abbiamo riscontrato che Dynamite Hate è anch'essa più allineata ai paesi anglofoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato un maggiore allineamento con le persone che hanno un'istruzione universitaria. Quindi, per GPT 4 nel compito di accettabilità sociale, abbiamo riscontrato che è più allineato alle persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo la stessa cosa per Dani Hate, dove è più in linea con le persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschili e femminili. Lo troviamo sia nel compito di accettabilità sociale di GPT 4 che nell'analisi del compito Dynahate."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste la posizione analidina LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni al riguardo. La prima è tenere traccia di tutte le scelte di design pertinenti durante il processo di ricerca. E l'altra è fare ricerca in NLP con la lente del perspectivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di creare set di dati e modelli specializzati all'interno di quattro comunità specifiche. Un buon esempio di ciò è l'iniziativa Masakane. E vogliamo sottolineare che il NLP inclusivo non consiste solo nel far funzionare tutte le tecnologie per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E con questo concludo la nostra presentazione, ma se desiderate saperne di più, non esitate a consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xi Yuan dell'Università di Fenai. Sono qui per presentare il nostro lavoro sulla conoscenza distinta degli script dai modelli di linguaggio a riga per la pianificazione del linguaggio vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "I lavori precedenti hanno esplorato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come fare una torta e hanno dimostrato che i grandi modelli linguistici possono scomporre efficacemente gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione degli obiettivi con obiettivi specifici, vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Che impongono diverse limitazioni agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli molteplici. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Poiché non esiste un insieme di dati di obiettivi specifici per individuare il nostro punto di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo raggiungere prima questo obiettivo. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli modificati per l'acquisizione dei dati del ciclo umano utilizzando TPT strutturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Preleviamo 100 obiettivi specifici e valutiamo gli script generati da modelli di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli lineari ottengono risultati insoddisfacenti nella pianificazione degli obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi conduciamo un'analisi dettagliata per indagare a cosa servono i moduli di apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma non si può garantire la fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo le categorie di vincoli più esplicite in base al luogo di residenza. La mappa principale nella figura mostra che le prestazioni di pianificazione dei DPD didattici variano notevolmente tra le ragazze delle diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli leggeri è caratterizzata da un'elevata varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea del filtro zen sovra-generato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo prima i tipi di vincolo con esempi per l'istruzione CPT e otteniamo obiettivi specifici basati sugli obiettivi astratti menzionati."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Poi istruisci GPT affinché generi script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script intermittenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo gli script e gli obiettivi in istruzioni per GPT in \"bocconi\" e calcoliamo la similarità del coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, scriveremo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se la destinazione raggiunge il punteggio più alto rispetto al sito di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'insufficienza può generare viti di qualità dei capelli. Il nostro metodo migliora notevolmente la pianificabilità sia nella completezza semantica che nella fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché l'implementazione di grandi modelli linguistici è costosa, è essenziale consentire la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un set di dati è un passaggio essenziale per raggiungere questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale del set di dati è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare i dati dei siti di pianificazione linguistica vincolata dai grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per costruire un dataset di pianificazione linguistica congiunta denominato codice di script."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici con script per garantire la qualità della validazione e dei siti di test. Chiediamo ai lavoratori della fonte cloud di trovare e rivedere i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle costrizioni dello script del codice. Rileviamo che lo script del codice mostra l'iperplodismo negli obiettivi specifici generati. Con lo script del codice, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con antsights, TFILF e la regolazione della frequenza del cursore, è possibile generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare modelli più grandi quando sono stati adeguatamente addestrati su siti di dati idonei."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo identificato il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità dei grandi modelli linguistici di pianificare in modo vincolato e sviluppato un metodo di filtro generato per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un insieme di dati di script di alta qualità per la pianificazione linguistica vincolata. Speriamo che l'insieme di dati del codice possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. Si prega di trovare ulteriori dettagli dello script del codice nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Shu Heng. Oggi presenterò il nostro articolo intitolato \"Gli etichettatori di entità kernel 2003 funzionano ancora bene nel 2023?\" Cominciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento degli enti nominati o il compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano Kono 2003 per sviluppare il NER da quasi vent'anni. E ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare su questi problemi, abbiamo sviluppato il set di dati Kono plus plus. Questo è un set di dati che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione Kono 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi perfezionato oltre venti modelli su Kono due mila tre. Li abbiamo valutati sia sul set di test Kono tre che sul set di test Kono plus."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo calcolato la variazione percentuale di F uno per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti abbiamo scoperto che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer si generalizzano normalmente meglio sui nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influisce direttamente sulle prestazioni di un compito a valle. Anche in questo caso abbiamo scoperto che più esempi di ottimizzazione fine portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi. La prima è il sovradattamento adattivo, che è un sovradattamento causato dal riutilizzo dello stesso set di test più e più volte, e si manifesta solitamente come un rendimento decrescente su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra il treno e i dati di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha una pendenza maggiore di uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo apportato a Carl 2003 si traduce in più di un'unità di miglioramento su Carl plus plus, il che significa che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva un sovra-adattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, che dire della deriva temporanea?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni peggiorano con intervalli temporali più ampi."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E ciò conferma la nostra ipotesi che la causa principale del calo delle prestazioni sia la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura di modello migliore, di un modello più grande, così come di più esempi di ottimizzazione fine, e questo va di pari passo. Non possiamo avere solo un ingrediente, scartando gli altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da un errore temporale e, in modo piuttosto sorprendente, non è causato da un sovra-adattamento adattivo, anche se Kono 2003 è stato utilizzato per oltre vent'anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, tornando alla domanda che abbiamo posto nel titolo del nostro articolo, gli etichettatori Kono 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo solleciterà ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, vi prego di dare un'occhiata al nostro articolo, al nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità, in cui abbiamo introdotto l'altentity scorpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Javod Hosseini e questo è un lavoro congiunto con Philip Radinsky, Silvia Paretti e Annie Luis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa. Volevate dire easy on me o I got a feeling? Qui, un utente vuole scegliere tra uno di questi due segni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è fare un riferimento diretto, ad esempio dicendo il nome della canzone Easy on Me o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "A volte, però, è più appropriato fare un riferimento indiretto per avere una conversazione più naturale. Questo può accadere quando l'utente non riesce a ricordare il nome della fonte."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "Tutte le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi in riferimenti diretti, ad esempio il più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità degli LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi ambiti: musica, libri e ricerca."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un set di completamento dei fumetti."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: \"Ricordi quella canzone che ascoltavamo ieri?\" E con questo, Bob imposta il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nella seconda bolla di dialogo, Alice dice: \"Intendi per favore con me o mi sono soddisfatta?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "che è la domanda alternativa. E nella terza bolla, Bob usa un riferimento indiretto per selezionare una di queste entità, per esempio, la nuova RF."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente la prima e la seconda bolla di discorso, ma la terza viene compilata dall'annotatore. La prima bolla di discorso viene scelta tra alcuni suggerimenti manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "La seconda, che è la domanda alternativa, viene generata come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un modello semplice. Vuoi dire A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è generalmente più difficile fare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è l'attrazione uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, ad esempio, due libri con lo stesso nome \"il dettaglio\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando hanno descrizioni simili su Wikipedia. Infine, quando presentano caselle informative o attributi simili su Wikipedia. Ad esempio, lo stesso genere o la stessa voce dell'artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando mostriamo questa domanda alternativa agli annotatori, essi conoscono il nome di queste entità, ma non necessariamente conoscono l'entità stessa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo quindi alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedete agli annotatori di ascoltare almeno alcune di ciascuna canzone e di leggere le informazioni su ciascuna canzone. Ecco, ad esempio, il risultato della ricerca Google per la canzone Easy."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini di nuovo da Wikipedia in modo che gli annotatori sappiano come si presentano."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio qui la prima, e di descriverla utilizzando da tre a cinque espressioni di riferimento indirette."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati. Ad esempio, quello senza parole, non quello con il ragazzo di 12 anni o quello fittizio o proveniente dall'Azerbaigian."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus delle alternative contiene 6.000 domande alternative distribuite su tre domini e 42.000 espressioni di riferimento indirette. I risultati ottenuti con il modello T5xLarge sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta. È intorno al 92-95 percento. Ma questo non è realistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso ad alcune conoscenze pregresse parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87 percento, il che è più realistico, ad esempio, quando il modello linguistico recupera le conoscenze pregresse."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%. Quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili a diversi ambiti. Ecco un link al nostro set di dati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pappy dell'Università di Trento e della Fondazione Bruno Kessler, e presenterò brevemente l'attenzione come guida per la traduzione simultanea del discorso, un lavoro congiunto con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simul SD, è il processo di traduzione di un linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SimulST attuali? Le architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di formazione lunghe e complesse, ad esempio la formazione che coinvolge diversi obiettivi di ottimizzazione"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrare un modello con una latenza media di un secondo e un altro con due secondi di latenza e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è quindi la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Primo: utilizzare modelli SD offline già esistenti senza riaddestramento o adottare un'architettura specifica per CLSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E sfrutta la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, che è il meccanismo di cross-attenzione. E potete vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un'attenzione di tipo punto o encoder-decoder, e si tratta di una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Una parola viene emessa se la tensione non è concentrata, cioè la sua somma è al di sotto di una certa soglia alfa, verso gli ultimi fotogrammi di discorso lambda, il che significa che le informazioni ricevute sono abbastanza stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se riceviamo un frammento di discorso contenente \"Voglio parlare di\" e il nostro modello prevede la traduzione in tedesco, noi"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E daremo un'occhiata ai pesi di attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi frame di discorso ricevuti, mentre l'ultima parola indica gli ultimi frame di discorso ricevuti, gli ultimi frame di discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emessi le prime due parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Poiché la somma della tensione incrociata è superiore a una certa soglia alfa, non emetteremo l'ultima parola e attendiamo un altro blocco di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro discorso affondato e il nostro modello prevede altre tre parole, daremo un'occhiata ai pesi di cross-attention."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che nessuna parola indica gli ultimi fotogrammi del discorso di Lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che verranno emesse queste tre parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se osserviamo il risultato principale di ciò, vediamo"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "I risultati della traduzione simultanea del discorso verranno rappresentati su grafici in cui, da un lato, in blu, viene misurata la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Questa è la misura della latenza. E consideriamo anche il ritardo medio consapevole del calcolo che tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre curve siano il più possibile alte in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Ma vogliamo anche che vengano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E li confrontiamo con le strategie di PROPERA che si applicano anche ai modelli offline, ovvero la strategia WitKey e l'accordo locale. E li confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la pre-traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo che l'ADUT supera tutte le strategie applicate ai modelli offline poiché le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo di elaborazione o il tempo di elaborazione consapevole, questa è la strategia più veloce."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desidera scoprire ulteriori risultati, legga il nostro articolo. Abbiamo inoltre reso open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e il mio collega Jiang e io presenteremo la nostra ricerca sull'apprendimento seriale multimodale migliorato tramite l'ottimizzazione delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Grazie ai progressi nei grandi modelli linguistici, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che il tuning dell'istruzione consente ai grandi modelli linguistici di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione dell'istruzione si sono concentrati sul miglioramento delle prestazioni dei colpi seriali solo sui compiti linguistici, mentre la visione artificiale e i compiti multimodali sono stati trascurati."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione dell'istruzione su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati didattici tra RLP e modalità multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di mille seicento compiti di istruzione in lingua unica. Tuttavia, non esiste un compito di istruzione multimodale su larga scala disponibile pubblicamente. Pertanto, ciò ci motiva a creare un dataset di sintonizzazione per l'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi Instruct, il primo set di dati di riferimento per l'accordatura delle istruzioni multimodali, composto da 62 compiti multimodali diversi che coprono 10 ampie categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da 21 dataset open source esistenti e ogni compito è dotato di 5 istruzioni scritte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare sul tuning dell'istruzione multimodale sul nostro dataset proposto, prendiamo OFA come modello base per un modello di pattern multimodale unificato. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una casella delimitante."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro set di dati multi-istantanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare l'elaborazione di vari tipi di dati di input e output."}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo dell'OFA e formuliamo tutti i compiti in un formato sequenziale unificato in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Ok, ora parlerò del tuning dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per il set di dati di addestramento, utilizziamo 53 compiti del gruppo NIG per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo Common Sense Reason per il test e selezioniamo altri cinque compiti da WQA e dal gruppo Miscellaneous."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nella suddivisione di test per ogni compito. Inoltre, selezioniamo casualmente venti compiti dalla suddivisione di test dell'istruzione naturale come in Sintassi per l'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Quindi utilizziamo un modello OFA di grandi dimensioni pre-adattato come modello di base. Durante l'addestramento, mescoliamo tutti gli esempi per tutti i compiti. Ogni esempio viene combinato casualmente con uno dei suoi cinque modelli di istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante i test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Segnaliamo le prestazioni medie e massime e la deviazione standard delle prestazioni in tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo RougeL. Per un compito di RP, riportiamo anche RougeL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una nuova metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi risultati per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro risultato principale. Come possiamo vedere, la regolazione dell'istruzione può migliorare significativamente le prestazioni dell'OFE nei compiti multimodali visivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, l'apprendimento trasferito da set di dati di istruzione naturali può essere utile per l'ottimizzazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo vedere come, all'aumentare del numero di compiti, il modello ha ottenuto prestazioni migliori e, nel frattempo, una sensibilità inferiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, utilizzando un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra l'effetto di diverse strategie di ottimizzazione fine sulla sensibilità del modello. Come possiamo vedere dall'apprendimento trasferito da set di dati di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello IFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che l'apprendimento trasferito dai set di dati di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sui set di dati di istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo proposto il primo set di dati di regolazione dell'istruzione multimodale su larga scala, che migliora significativamente la capacità derivativa dell'OFA, e abbiamo esplorato diverse tecniche di apprendimento trasferito, dimostrandone i vantaggi con la progettazione di una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, un'altra cosa, stiamo raccogliendo un set di dati di apprendimento per istruzioni multimodali molto più ampio, con circa 150 ulteriori compiti di lingua Variante e li renderemo disponibili. Questo è un codice QR per i nostri dati e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Coast of Sena e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023, \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Bokier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina William."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo lavoro, rivediamo il paradigma della coppia minima."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il paradigma accoppiato minimo valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come nel caso di \"blimp\", \"gemma sintattica\" o l'accettabilità in termini di stereotipi, come le coppie di Krauss."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o una frase grammaticalmente corretta e poi mostrare una frase inaccettabile o una frase grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi la speranza è che il modello attribuisca fondamentalmente una maggiore probabilità alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP fondamentalmente non ci permette di valutare l'accettazione del modello verso frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Oggi i grandi modelli linguistici stanno sviluppando finestre di contesto sempre più lunghe. È quindi fondamentale che valutiamo l'accettabilità del modello in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline NPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Questo è l'approccio. Quindi, quello che facciamo è rivisitare i dataset stessi per simulare queste sequenze più lunghe e poi ricreare le frasi scegliendo frasi accettabili o inaccettabili da quei dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, qui abbiamo scelto un tipico esempio di grammaticalità dal set di dati del blimp, dal caso dell'isola aditiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili, che abbiano la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticali dall'adjunctile."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento, e ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di non corrispondenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui le frasi provengono ancora da set di dati pertinenti, ma non dallo stesso set di dati che si sta valutando. E possiamo fare lo stesso per il caso di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Ci dirà quindi se i giudizi di accettabilità del modello sono effettivamente influenzati da un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "come se il contesto provenisse da un sottoinsieme diverso del dataset o se fosse del tutto irrilevante per la frase che stiamo analizzando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, come si comporta il modello? Quindi, prima guardiamo le frasi di Wikipedia che sono completamente irrilevanti per l'attuale coppia di query, e lì troviamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino al 2024 per massimizzare i modelli OPT e GPT-2, e qui nella linea tratteggiata arancione si vede che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Ora, cosa succede quando scegliamo frasi dallo stesso dataset?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi che stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso set di dati BLIMP o SYNTAX GIMP."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando combiniamo la struttura, cioè quando scegliamo le frasi che descrivono gli stessi fenomeni, incolpiamo la sintassi, Jim."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "osserviamo un aumento o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo e questo è molto grande, come questo effetto aumenta con la lunghezza del contesto e questo probabilmente influenzerebbe i nuovi modelli linguistici che hanno grandi finestre di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante ma aggiungendo del rumore all'input. E dopo aver effettuato diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "constatiamo che nessuno di questi rumori sta effettivamente facendo cambiare il modello il suo corso in termini di come ci mostra la tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In sostanza, abbiamo riscontrato che i modelli sono sensibili alle frasi disturbate in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Quando alteriamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni e quando alteriamo le frasi nel dominio inaccettabile, osserviamo una diminuzione dei giudizi MPP in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, il modo in cui la eseguiamo attualmente con input brevi e frasi singole, potrebbe non catturare appieno la conoscenza astratta del modello linguistico in tutta la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per ulteriori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusin Zhang e vengo dalla Penn State University. Oggi presenterò il nostro lavoro, l'Analisi Semantica Trasversale Linguistica in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Quindi l'analisi semantica è un compito che consiste nella costruzione di rappresentazioni semantiche delle query degli utenti, come SQL e il calcolo Lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "L'analisi semantica interlinguistica è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda, FunQL, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di analisi semantica interlinguistica esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati, ad esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "Ci sono molte informazioni su certi linguaggi naturali. Manca il cinese."}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "Copertura dei laghi su alcune mini-rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda manca."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "Oppure vengono valutati solo su alcuni modelli più recenti. Ad esempio, c'è solo un singolo modello per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo Exempler, forniamo un insieme di dati uniforme Exempler per l'analisi semantica tra lingue in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove set di dati in vari ambiti, cinque parti semantiche e tasse, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è TranslateTest. Usiamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, poi usiamo MonolingoModel per addestrare una valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestreremo un modello inglese su una query in inglese e, durante l'inferenza, traduciamo la query tedesca utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere la query SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di partenza è la stessa della lingua di arrivo, ad esempio dal tedesco al tedesco o dall'inglese all'inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche l'impostazione di fusione monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo testato un modello multilingue, che abbiamo addestrato su tutti i linguaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo unito le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre query in tedesco o query in cinese o altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo anche il trasferimento crosslingo zero shot e field shot, che funziona con una lingua di partenza e viene trasferito a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, durante l'addestramento, lo addestrerò su query in inglese o su una combinazione di query di fusione in inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche ottenuto molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "incluso l'encoder PDR, che sta per encoder multilingue pre-addestrati con decoder basati su puntatori, come XLMR più PDR e BERT più PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come MBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder decoder ottiene le migliori prestazioni su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo su MT cinque e XLMR più le impostazioni multilingue PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che l'encoder-decoder o l'encoder PDR possono essere migliorati addestrandoli in una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che ciò è dovuto al fatto che la maggior parte dei principali linguaggi naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni dell'inglese diminuiscono in sette insiemi di dati e migliorano solo in tre insiemi di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Penso che questo sia noto come la maledizione della multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario di prestazione tra le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento incrocio-linguistico con dati di addestramento, la linea arancione rappresenta il trasferimento incrocio-linguistico senza dati di addestramento, mentre la linea verde rappresenta l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo scoperto che confrontando la linea verde e quella arancione, abbiamo scoperto che per l'impostazione a zero short, il divario di prestazioni nel trasferimento interlinguistico è significativo. E confrontando la linea blu e quella arancione, abbiamo scoperto che per poche impostazioni short, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche riscontrato altri risultati interessanti. Ad esempio, l'encoder decoder supera il lavoro di progresso o ottiene risultati comparabili. L'acquisto in lingua inglese può migliorare significativamente le prestazioni di fuchshot su lingue naturali di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo scoperto che modelli linguistici multilingue come Codice e Bloom sono ancora inadeguati per compiti di analisi semantica interlinguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo creato Exempler, un benchmark unificato per l'analisi semantica incrociata con più lingue naturali e mini-rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo condotto uno studio di riferimento completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano molti risultati interessanti e altro ancora. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Aid Vilar e vi darò una breve panoramica dell'articolo \"Promuovere la traduzione di PowerPoint, valutare strategie e prestazioni\". Questo è un lavoro congiunto con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Parm è un modello di linguaggio con 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di tag che comprende 780 miliardi di token."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico sull'uso del modello di linguaggio Latch per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni della valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche LMT all'avanguardia e nuove e, inoltre, mostriamo anche i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "Il prompt ha una grande influenza sulle prestazioni degli LLM per la traduzione, come possiamo vedere in un semplice esperimento in cui utilizziamo un breve prompt e forniamo due prompt diversi per una sola frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "Nella maggior parte delle frasi, cinquecentosedici su mille, la differenza osservata è di più di un punto sfocato."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può arrivare, nei casi estremi, fino a 40 punti di sfocatura. Quindi è importante selezionare una buona strategia di prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompt in cinque fasi in cui abbiamo semplicemente contrassegnato ogni frase fornita al sistema con la lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo esempio, in cui eseguiamo la traduzione dal tedesco all'inglese, le frasi in tedesco, le frasi sorgente, sono contrassegnate con due punti tedeschi e la traduzione in inglese con due punti inglesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo visto che la forma effettiva del prompt non ha una grande influenza nel caso di diversi prompt brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È fondamentale per il prompting a zero e a un colpo, ma quando si passa, come nel nostro caso, al prompting a cinque colpi, non c'è quasi nessuna differenza rispetto alla forma effettiva del prompting."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, i risultati dei nostri esperimenti dimostrano che la qualità dell'esempio è più importante della somiglianza con la frase di partenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È quindi importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di profondità sono molto più curati e di qualità superiore rispetto ai dati di addestramento, e i risultati mostrano prestazioni migliori quando si utilizzano i dati di profondità."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati di ultima generazione hanno un vantaggio sostanziale rispetto alle traduzioni PALM, ma PALM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di sovrapporre Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo acquisito dall'innovazione umana che abbiamo realizzato utilizzando il framework MQM è che la fluidità di PALM è paragonabile ai sistemi più avanzati, ma la differenza principale deriva dall'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione che suona meglio a volte tralasciando parti della frase di origine che vengono omesse nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di stile esterno per PAN è inferiore rispetto ai sistemi più avanzati, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "Questa parte fornisce un output davvero fluido, ma con alcuni problemi di precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di consultare la presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawe, uno studente di dottorato all'Università di Zalant in Germania. In questo video vorrei presentare il nostro lavoro recente, \"Più debole di quanto pensiate\", uno sguardo critico sull'apprendimento settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yushche, Marios Musbach e Gas Steffen e Dietrich Clarkov."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole euristica, basi di conoscenza o sourcing del codice di località, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Rispetto alle annotazioni umane, le annotazioni più scadenti sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestreremo direttamente le reti neurali sui dati di etichettatura settimanali, le reti neurali tenderanno a memorizzare il rumore dell'etichetta e non generalizzeranno."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'apprendimento supervisionato settimanale, gli algoritmi di addestramento sono progettati per addestrare in modo robusto le reti neurali in presenza di un livello di rumore tale da garantire che i modelli addestrati generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori in WSL, dove WSL sta per Weekly Supervised Learning, una affermazione comune è che le persone dicono di addestrare i modelli solo sui dati di etichettatura settimanali e di ottenere alte prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Il problema è che le persone presuppongono che esista un ulteriore set di validazione pulito disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Mettiamo in dubbio questa impostazione del problema, poiché implica che nel learning supervisionato settimanale siano necessarie ulteriori annotazioni manuali, ma come un elefante nella stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il suddetto dubbio ci porta a porci tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL? O possiamo forse utilizzare un set di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se i dati puliti sono richiesti o se i dati puliti sono obbligatori affinché WSL funzioni, quante etichette pulite ci servono? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o esistono modi migliori per utilizzarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di trattini bianchi puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "Altrimenti si verifica un forte calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "cioè che la formazione è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che i metodi WSL richiedono dati accuratamente etichettati per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, per ottenere alte prestazioni, abbiamo bisogno di soli venti campioni per classe."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questa non è la fine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi otterrà risultati ancora migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazione tra gli approcci di ottimizzazione fine, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere, se abbiamo dieci campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni ottenuto nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare l'ottimizzazione sui campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo vedere dai dati, il modello Marlina denominato FTW inizialmente ha prestazioni inferiori rispetto ai metodi WSL più complessi come il coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se si consente di continuare l'ottimizzazione fine sui campioni puliti, allora FTW si comporta altrettanto bene come altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti e annotati manualmente affinché funzionino correttamente. Il loro miglioramento delle prestazioni e la loro praticità sono fortemente sopravvalutati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre raccomandazioni concrete per i lavori futuri sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, riportare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è stata effettuata con campioni di validazione puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, i metodi WSL dovrebbero essere confrontati con le linee di base per l'atterraggio futuro, poiché entrambi lavorano su campioni di griglia. In terzo luogo, il continuo perfezionamento è una linea di base semplice ma efficace che dovrebbe essere presa in considerazione nei futuri lavori nel campo WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Vi invitiamo a consultarlo. Grazie e unitevi alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. E oggi vi racconteremo tutto su ABCEval, un nuovo approccio dimensionale alla valutazione dell'IA conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal laboratorio di NLP di Emory, diretto dal professor Gino Choi presso l'Università di Emory, e in collaborazione con l'intelligenza artificiale di Amazon Alexa."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo quindi che tu abbia appena sviluppato un modello di dialogo e desideri vedere quanto sia performante rispetto allo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La pratica comune è quella di utilizzare una valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala liquida."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più fine."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio consiste semplicemente nel chiedere a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o la scala di Lickert."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se la risposta di ciascun modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"annotating behaviors in chat\" o, in breve, \"ABC eval\". Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità della chat."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare la frequenza con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, APCEval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o non riesce a mostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni uomo-bot per modello utilizzando ABCEval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni liquide a livello di turno, valutazioni liquide a livello di dialogo e confronti di coppia a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di valutazione ABC sono nel complesso più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra gli annotatori su 100 conversazioni etichettate in duplice copia."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto ai parametri ottenuti con i metodi esistenti, come dimostra questa semplice analisi di regressione lineare."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può vedere come la misurazione della proporzione di turni con contraddizioni personali e con il partner spieghi rispettivamente il cinque percento e il dieci percento della qualità della conversazione, mentre i punteggi medi di coerenza dell'alcol spieghino solo il quattro percento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a gradi."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Si può vedere come la combinazione di tutte le metriche di valutazione ABC spieghi oltre il 25% della qualità della conversazione. E quando si rimuovono le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche liquide a livello di turno spiega molto meno della qualità e meno di queste metriche contengono informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC affidabili, informative e distinte ci permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "Dai risultati del nostro esperimento si evince che permangono ancora diverse sfide, che sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Circa il 15% delle risposte fornisce informazioni irrilevanti e circa il 10% delle volte si contraddicono o contraddicono il proprio partner."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Grazie al rapido progresso nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è un motivo in più per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che ABC eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'IA conversazionale progredisca nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede un'esplorazione multilingue basata sui dati?\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto. Ad esempio, come tradurremo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono, allora Moe si riferisce a uno spia. Ma se la frase precedente era potrebbe essere qualcosa di serio, dottore? allora Moe si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, cambia anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come Blue incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "Alcune persone hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande:  \n1. Quando la traduzione richiede un contesto?  \n2. Quanto bene i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Ciò si ottiene misurando quanta informazione il contesto C fornisce sull'obiettivo Y data la fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Si può pensare a CXMI come alle informazioni ottenute dando un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PSXMI come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto PCXMI per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E noi eseguiamo la nostra analisi su trascrizioni di discorsi TED che sono stati tradotti dall'inglese in quattordici lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Eseguiamo la nostra analisi a tre livelli diversi. In primo luogo, esaminiamo le etichette delle parti del discorso che hanno un alto valore di PCXMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un p relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali. Quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "Allo stesso modo, ci accorgiamo che alcune lingue richiedono un contesto anche quando si vuole scegliere la forma verbale appropriata. Esaminiamo poi gli elementi del vocabolario che hanno un alto valore medio di p/seksualità in tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri e assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "E allo stesso modo, ci accorgiamo che il contesto è supportato per tradurre con la giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo i diversi token individuali che hanno un alto p6mi. Questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi in una struttura standard, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ora utilizziamo i risultati della nostra analisi per progettare un benchmark per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni di discordia che abbiamo identificato, abbiamo creato dei tagger per identificare automaticamente le parole che appartengono al fenomeno. E abbiamo chiamato il nostro tagger \"Multilingual Discourse Aware\" o tagger MUDA."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni discreti."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi il tagger MUDA applicandolo al corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro parametro di riferimento e altri indicatori per valutare i diversi modelli di traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, quando utilizziamo metriche a livello di corpus, quindi per Blue, scopriamo che i modelli agnostici complessi hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se utilizziamo il modello comet, i modelli sensibili al contesto ottengono i migliori risultati. E se utilizziamo la misura della frequenza delle parole, allora i modelli con o senza contesto hanno prestazioni confrontabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello aziendale."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark MUDA per valutare i modelli e scopriamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di altri modelli che non utilizzano il contesto su altri fenomeni come le ellissi, i pronomi e la forma dei verbi. Quindi questo suggerisce dove dovremmo vedere maggiori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeepBell è generalmente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su quattordici coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri risultati per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeni discreti possono gestire bene o male, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per l'attenzione. Ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrack e vi presenterò i nostri lavori su Dr. Berth, un robusto modello pre-addestrato in francese per i settori biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, parleremo prima del modellamento linguistico nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in francese chiamato Dr. Berth, che si basa su Roberta e viene addestrato su Natchios, un dataset di dati medici raccolti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto un confronto tra modelli con più impostazioni di plutonio e fonti di dati. Successivamente, abbiamo presentato i nostri risultati su undici compiti a valle biomedici e clinici in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "E infine, concludiamo gli esperimenti e vi diamo ulteriori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Dal suo lancio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre enormi miglioramenti delle prestazioni rispetto ai metodi statici e contestualizzati storici come word to vector, fast text o enroll."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri settori come il biomedico con Permette Bert e BioBert, e il clinico con Clinical Bert, ma soprattutto in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un continuo adattamento a causa della mancanza di dati specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora, la Francia non disponeva di un moderno open source per la biomedicina."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci chiediamo quindi quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi. E i dati attuali rappresentano una buona sostituzione dei dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'ospedale non universitario che abbiamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo: quanti dati abbiamo bisogno di avere per addestrare un modello specializzato su dati francesi? Sono 4 GB, 8 GB o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli sviluppati da zero. Una prima versione del Dr. Bert con sette GB di Nachos, una seconda versione con un sottoinsieme di quattro GB di Nachos."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con quattro gigabyte di frasi tratte da nodi clinici. E una versione finale di Schubert con un mix di quattro gigabyte di insiemi di nature e quattro gigabyte di nodi clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con un pre-addestramento continuo per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso del Camembert e addestrato su quattro gigabyte di set di nachos, un altro basato anch'esso sul Camembert ma addestrato questa volta sui quattro gigabyte di Klinker Lots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, uno basato su un modello biomedico inglese, BMLB, e addestrato su 4 GB di Snatchers. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, abbiamo raccolto informazioni su quali supportano compiti a valle pubblici e privati come il riconoscimento di nomi e identità, la classificazione, il tagging di comandi e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di riferimento, ovvero Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBelt, Myobelt e ClinicalBelt."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "La valutazione evidenzia che il modello ha ottenuto i migliori risultati nel compito con dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo ottenere tali dati da fonti eterogenee, che sembrano essere più versatili. Osserviamo inoltre che l'utilizzo di più dati si traduce in prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, a partire da zero, l'addestramento gratuito sembra ottenere prestazioni superiori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sul finto continuo utilizzando il peso e il tokenizer di PumedBeard addestrato sul sottoinsieme di 4 GB di Natchez ha mostrato risultati comparabili a quelli ottenuti con il Dr. Beard 4 GB da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "Questo non vale per il modello basato sui pesi comuni degli orsi e del tokenizer, che soffrono di problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il sistema proposto offre prestazioni migliori su nove degli undici compiti a valle e supera globalmente il risultato del modello generico qui, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo anche che i dati specializzati sono migliori, i dati più specializzati sono ancora migliori, ma non si adattano bene all'ampliarsi delle dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da Natchios sono liberamente disponibili su YuginFace e tutti gli script di addestramento sono nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi grazie per questa presentazione e non vediamo l'ora di scambiare idee alla sessione POSTER a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann, e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging con multiset e le permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei consulenti Alexander Kola e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste singolarmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'analisi semantica, il test per la generalizzazione compositiva potrebbe essere così. Come al solito, abbiamo un insieme di enunciati di addestramento, in questo caso la ragazza dormiva e Mary sapeva che la ragazza dormiva."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Questi enunciati sono abbinati a forme logiche che rappresentano gli aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione meno profonda durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenza-sequenza naivi faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono risultati che sono distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono destinati a catturare il processo compositivo che collega le enunciazioni con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente, ciò comporta un notevole formalismo e un pre-elaborazione specifica delle forme logiche, ad esempio, per gestire i simboli variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'acquisizione degli alberi può anche comportare procedure di induzione grammaticale specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello di sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la previsione dell'output dall'input in due passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Per questo motivo, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione e metterli nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona più o meno così."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da sinistra a destra sull'output e determiniamo quale token del multiset inserire in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi saltiamo al prossimo token multiset per determinare il secondo token dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token dell'output in modo simile saltando a un altro token multiset. Proseguiamo con questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "fino a che ogni token del primo stadio non sia stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per darvi un'anticipazione dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark di Kong. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Altri tipi di generalizzazione strutturale rimangono comunque molto difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo risolviamo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "Prima di tutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multisetter proviene, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte della formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP difficile. Questo perché è legato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approssimiamo questo con un rilassamento continuo ottimizzato per la GPU, che ci permette anche di retropropagare attraverso la soluzione e di apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desidera saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, la invitiamo a consultare il nostro articolo o a venire al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Makshta, e oggi io e il mio co-autore Martin presentiamo il nostro lavoro, The Kitmastech, Valutazione dell'integrazione della conoscenza da fonti multiple. Questo lavoro è il risultato di una collaborazione tra l'Università McGill, MILA e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione del linguaggio nazionale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti su compiti come la risposta alle domande dimostrano che i modelli possono utilizzare la conoscenza temporale pre-addestrata per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, nella frase: John ha visto il presidente appena eletto in TV."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia un TBA, ma non possono sapere in modo affidabile chi sia l'entità specifica di questa istanza John o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dopo l'addestramento pre-iniziale."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella di inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo un compito di risoluzione del riferimento di base progettato per sondare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valutiamo il dataset con i partecipanti allo studio umano e stabiliamo modelli di risoluzione del riferimento di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro insieme di dati. Thurvin è un giudice. Kia è una fornaia. Thurvin e Kia si incontrarono in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è \"servo\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un dato pronome richiede due tipi di informazioni. In primo luogo, conoscenza specifica dell'entità come il sermone è un giudice. E in secondo luogo, conoscenza di base come i giudici decidono i casi nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante l'addestramento preliminare dei grandi modelli linguistici, mentre le conoscenze specifiche delle entità vengono tipicamente osservate al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Varia la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di Kitmos. In primo luogo, abbiamo l'impostazione dell'argomento, la pre-formazione di base, dove si presume che la conoscenza di base sia disponibile al momento della pre-formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è lo scenario di background, in cui le conoscenze di base sono disponibili sia al momento dell'addestramento preliminare che al momento dell'inferenza. Infine, lo scenario di inferenza di background, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante, poiché simula un caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli, ad esempio perché nuove occupazioni si sono sviluppate dopo il periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti in una fonte attendibile."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di apprendimento pre-addestrato, assumiamo che la conoscenza di base che i politici cercano di ottenere seggi elettivi nel governo sia contenuta nei parametri pre-addestrati. Nel contesto dell'intervento, forniamo la conoscenza anti-specifica che Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "In entrambi gli scenari di riferimento, forniamo non solo informazioni anti-specifiche, ma anche conoscenze di base sui politici nel contesto del periodo di influenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "In secondo piano, nell'ambientazione a Freon, forniamo l'occupazione fittizia di meritur invece che politico, perché è improbabile che meritur sia contenuto in un parametro pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il set di dati sia con partecipanti allo studio umani che con modelli di risoluzione di riferimento consolidati. In questa figura, mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile delle impostazioni di pre-addestramento di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Senza una formazione specifica per il compito su Kitmos, entrambi i modelli non ottengono buoni risultati. Tuttavia, quando addestrati su Kitmos, sia C2F che Berth for Koref ottengono risultati significativamente migliori rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Ciò suggerisce che, quando vengono addestrati su insiemi di dati di risoluzione della coerenza generale, i mod imparano a sfruttare indizi superficiali, che non sono utili quando si effettua il test su kidmos dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le prestazioni migliori non riescono a integrare in modo affidabile le conoscenze di base fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i punti principali del nostro articolo, molti modelli di risoluzione della coerenza sembrano incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare con successo la conoscenza proveniente da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, anche i modelli con le prestazioni migliori sembrano avere difficoltà con la conoscenza retrospettiva integrata in modo affidabile presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e date un'occhiata al set di dati e al codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra, e oggi parlerò del nostro articolo, \"Persone Contrassegnate\", che utilizza prompt di linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato realizzato in collaborazione con Essendermouch e Dandarovsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset costruiti manualmente che richiedono molto tempo per essere curati."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con gruppi particolari."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte del lavoro in questo ambito non tiene conto dell'intersezionalità, ovvero l'idea che le identità sociali multifaccettate possano amplificare i pregiudizi e rappresentare punti unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà per cui questi nuovi LLM regolati da istruzioni sono molto bravi a rispondere a istruzioni e suggerimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come immagina di essere una donna asiatica, descriviti."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcune generazioni di esempio di GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Subito, vediamo che, sebbene i risultati non siano eccessivamente negativi o tossici nel senso tradizionale di queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Si possono notare alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è ritratta come modesta. La donna del Medio Oriente è descritta con parole come esotica e come se si riferisse a una regione affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le donne di colore fanno riferimento alla loro discendenza, mentre il personaggio dell'uomo bianco non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si compone di due parti. La prima consiste nella creazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "I nostri suggerimenti per generare queste persone sono stati ispirati da uno studio in cui sono stati dati questi suggerimenti a soggetti umani, scoprendo che, dandoli a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre ciò consente un confronto diretto tra le nostre persone generate e le risposte scritte umane."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è rappresentata dalle parole contrassegnate, un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, su cui tornerò tra breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di questo approccio è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole contrassegnate si basa quindi sul concetto sociolinguistico di contrassegnatezza, che afferma che esiste un valore predefinito non contrassegnato e che qualsiasi gruppo che si discosta da tale valore predefinito è linguisticamente contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, ad esempio, la parola uomo o, scusa, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano un guerriero uomo e indicano il termine con donna."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nel nostro metodo, per prima cosa indichiamo quali sono i gruppi non marcati e quelli marcati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "Poi confrontiamo le persone utilizzando il metodo delle parole di contrasto, che consiste essenzialmente nell'utilizzare rapporti logodici ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per esempio, per le persone di donne nere, useremmo parole offensive e confronteremmo i rapporti con gli dei della legge sia con le persone bianche che con le persone maschio, perché questi sono i due gruppi non contrassegnati corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "Ora alcuni risultati. Quindi, per prima cosa, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le persone generate hanno tassi molto più elevati di parole di Luxon, quelle scritte dagli esseri umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate presenti nelle persone generate sono davvero solo le parole \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in realtà, solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "E infatti, questo lessico non riesce a cogliere molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per fare ciò, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, riveleremo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, per i gruppi di marchi, le parole principali includono cose come cultura, tradizione, orgoglio e esotico. E queste parole definiscono questi gruppi solo in base alla loro relazione con la loro identità e li distinguono come diversi dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e di emarginazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti luoghi comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono termini come vibranti e curvilinee."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "ehm, che si collegano a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come minuta e delicata e setosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomissae, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "Infine, per le donne di colore, vediamo che alcune delle parole più frequenti sono forti e resilienti."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che la gente ha chiamato l'archetipo della donna nera forte, e sebbene a prima vista possa sembrare positivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "E ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste categorie demografiche affinché siano resilienti e forti contro gli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, invece di lavorare per cambiare quegli ostacoli, si mette pressione su quelle persone affinché li superino, il che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, scopriamo che le parole per ogni gruppo contrassegnato riflettono in gran parte solo narrazioni essenzializzanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, noi ricercatori dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, per esempio, come questi stereotipi positivi, non sappiamo se sia perché c'è una sorta di strano comportamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "un allineamento dei valori eccessivamente esagerato in corso o forse altri metodi, come quelli anti-stereotipi, che stanno portando a questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero fare ipotesi o studiare ulteriormente senza maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Buon divertimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingwei e vengo all'Università di Scienza e Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me presentare un breve video pubblicitario sul nostro articolo \"Stai copiando il mio modello? Protezione del diritto d'autore per i grandi modelli linguistici per l'incorporamento e i servizi Villbackdoor Watermark\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo prima il contesto relativo alle inviti e ai servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, Lama, PELM sono eccezionali nella comprensione e nella generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'integrazione come servizio è uno dei servizi basati su grandi modelli linguistici per assistere in vari compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, Openly AI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che l'aggressore potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'embedding come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "Per proteggere il diritto d'autore dei servizi di incorporamento, una delle soluzioni è incorporare una filigrana nel servizio del fornitore e rilevare se un altro servizio contiene la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo della filigrana deve soddisfare le seguenti proprietà. In primo luogo, il metodo dovrebbe essere applicabile all'incorporamento come servizi. In secondo luogo, la filigrana non dovrebbe degradare l'utilità degli incorporamenti forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "In terzo luogo, la filigrana dovrebbe essere abbastanza resistente all'attacco, altrimenti l'aggressore potrebbe rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la filigrana deve essere trasferibile ai servizi attaccanti durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile all'integrazione come servizi o alla mancanza di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo l'embedding marker, che è un metodo di watermark basato su back door applicabile all'embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Permettetemi quindi di illustrare i dettagli del nostro marcatore di incorporamento. Il marcatore di incorporamento comprende due passaggi principali: l'inserimento della filigrana e la verifica del diritto d'autore."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di filigrana, si definisce prima un obiettivo di incorporamento. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'incorporamento fornito è una somma ponderata dell'incorporamento di destinazione e dell'incorporamento originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del diritto d'autore serve a rilevare se un modello alla base di un altro servizio contiene la filigrana."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Per prima cosa, creiamo un set di dati di backdoor e un set di dati benigno. Il set di dati di backdoor contiene frasi in cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente il provider richiede l'inserimento di dati dal servizio Stiller con il set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la similarità coseno e L2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di similarità tra il set di dati nove e il set di dati backdoor, definita come delta coseno e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Conducciamo esperimenti su quattro dataset: AG News, Mind, SSD two e Erospam. Ipotizziamo che il fornitore applichi il testo wiki al dataset per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati mostrano che il nostro marcatore di incorporamento può avere prestazioni di rilevamento della griglia mantenendo al contempo l'utilità della griglia per i compiti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Confermiamo anche la segretezza dell'embedding fornito visualizzando l'embedding delle frasi svolte come in BOPCA. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le incorporazioni della back door e le incorporazioni normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto, grazie. Verranno a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per l'ACL 2023 come articolo esteso, \"Transfer Learning for Dissonance Detection\", che affronta la sfida delle classi rare."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel contesto del linguaggio. In parole semplici, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "come in questo esempio in cui una persona afferma: \"So che le sigarette potrebbero uccidermi\", e poi continua dicendo: \"Ho preso un paio di sigarette dopo la riunione\". Questa convinzione e questa azione sono incoerenti e sono in dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il fatto che non credo di poter mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione consonante."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché è importante? Studiare la distanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, monitorare le tendenze e i cambiamenti nei valori delle credenze e negli atteggiamenti della popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "L'alta dissonanza cognitiva è anche legata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Studiare la dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Per raggiungere l'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza in primo luogo, come mostrato nell'organigramma qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati utilizzando un parser PATB e le coppie di unità Discord sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Dopo aver raccolto circa 1000 esempi di coppie di unità di discorso, abbiamo eseguito una formazione per un classificatore iniziale addestrato solo su 43 esempi di disnets. Non sorprende che il classificatore non abbia ottenuto risultati molto migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la bassa frequenza di dissonanza e l'assenza di qualsiasi dataset precedente di questo tipo, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo combinazioni di apprendimento trasferito e apprendimento attivo per l'annotazione, in modo da poter raccogliere più campioni dissonanti con meno passaggi di annotazione, riducendo il costo complessivo dell'annotazione e migliorando al contempo il rilevamento della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Passiamo da due compiti diversi, la classificazione del dissenso indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamiamo dibattito qui e sulla classificazione binaria delle classi di espansione e di confronto di PDTB, poiché questi due concetti sono strettamente legati alla concezione delle consonanti e della dissonanza, e li chiamiamo CE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che, trasferendo la performance a zero short sul dataset annotato, i risultati sono già molto migliori del caso con il miglior AUC.62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, dopo aver effettuato un perfezionamento iterativo su entrambi i compiti, abbiamo scoperto che il perfezionamento dei compiti di CE seguito da un ulteriore perfezionamento sul dibattito porta a prestazioni zero-shot molto migliori. Pertanto, questo è il modello che utilizziamo per avviare il processo di apprendimento effettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Il metodo cumulativo accumula tutti i dati raccolti finora dalle annotazioni attive, mentre il metodo iterativo aggiorna il modello addestrandolo sull'ultimo insieme di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo scoperto che quella cumulativa ha ottenuto risultati uguali o migliori rispetto a quella iterativa in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara, PRC, per selezionare principalmente gli esempi che hanno un'alta probabilità di essere dissonanti secondo il modello attuale in qualsiasi round di AL."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Lo confrontiamo con le altre strategie AL all'avanguardia comunemente utilizzate nella comunità."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la strategia proposta per la PRC funziona meglio rispetto ad altre strategie all'avanguardia, anche se la differenza è minima. Si noti che le prestazioni sono significativamente inferiori per il caso casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "Nei successivi cicli di AL con le due migliori strategie, abbiamo migliorato la classificazione della distanza AUC a 0,75, che è la migliore performance che abbiamo ottenuto finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ogni strategia in termini di qualità dell'annotazione e costi per gli annotatori. Abbiamo riscontrato che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo scoperto che la PRC è una semplice strategia di apprendimento automatico per l'acquisizione di classi rare e che l'avvio a freddo dell'apprendimento automatico può essere notevolmente facilitato da compiti di apprendimento per trasferimento progettati in modo appropriato."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre riscontrato che l'aggiornamento iterativo è utile per l'apprendimento trasferibile da un dominio diverso, mentre le annotazioni attive in-domain traggono vantaggio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro dataset di codice e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
