{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Assa Farari et je vais présenter notre article FUESHOT TABLAR DATA ARCHICHTMENT Using FINE TUNIC TRANSFORMERS ARCHITTURES. Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données, mais parfois ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter des informations substantielles. Notre objectif de recherche est l'enrichissement automatique des données tabulaires en utilisant des sources de texte libre externes. Supposons que nous ayons un ensemble de données tabulaires et une base de connaissances. Nous avons besoin d'un processus automatique qui implique le couplage d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre FAST est exactement ce processus automatique. Voyons donc un exemple. Et un ensemble de données est alimenté dans FAST. Dans cet exemple, l'ensemble de données est un ensemble de données universitaires lorsque son objectif est de classer les universités en universités de faible rang et en universités de faible rang et en universités de haut rang. En tant que base de connaissances, nous utilisons Wikipedia. La première phase de FEST est le couplage d'entités, lorsque chaque entité, dans cet exemple, le nom de l'université est lié à une entité au sein de la base de connaissances, et le texte des entités de la base de connaissances est extrait et ajouté à l'ensemble de données. Dans cet exemple, le texte est l'abstract de la page Wikipedia. Maintenant, nous devons générer ou extraire des caractéristiques à partir du texte récupéré. Donc, nous avons besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse de texte et c'est la principale nouveauté de cet article et je vais m'y attarder dans les diapositives suivantes. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques lorsque nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. D'abord, générer des caractéristiques dans le nombre de classes de l'ensemble de données original. Dans cet exemple, l'ensemble de données original a deux classes, donc d'abord générer deux nouvelles caractéristiques. Mais si l'ensemble de données a cinq classes, d'abord générer cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'art de l'analyse de texte, qui sont des modèles de langage basés sur des transformateurs comme BERT, GPT, XLERT, etc. Il est mais il n'est pas probable que nous puissions entraîner des modèles de langage en utilisant les ensembles de données d'entrée. Donc, une approche naïve sera un ajustement fin de tâche cible. Donc, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, ajuster finement le modèle de langage sur l'ensemble de données cible dans cet exemple pour ajuster finement le modèle de langage pour classer le texte en classes, l'abstract en classes, bas ou haut, recevoir la sortie du modèle de langage, qui est la probabilité pour chaque classe, et utiliser comme nouvelles caractéristiques. Le problème avec cette approche est que les ensembles de données peuvent avoir peu d'entités distinctes, du texte. Dans notre expérience, presque la moitié des ensembles de données contiennent moins de 400 échantillons et le plus petit ensemble de données contient 35 échantillons dans son ensemble d'entraînement. Donc, ajuster finement un modèle de langage sur cet ensemble de données sera inefficace. Mais nous pouvons utiliser la connaissance préalable sur les ensembles de données pré-analysés parce que nous appliquons FAST sur plusieurs ensembles de données. Nous pouvons utiliser les ensembles de données n moins un pour recueillir des informations sur les ensembles de données n moins un et utiliser ces informations lorsque nous analysons l'ensemble de données n. Ce que nous suggérons est d'ajouter une autre phase d'ajustement fin, une phase d'ajustement fin multitâche préliminaire lorsque nous ajustons finement le modèle de langage sur n-1 ensembles de données et puis nous exécutons une autre phase d'ajustement fin, qui est un ajustement fin de tâche cible lorsque nous ajustons finement le modèle de langage sur l'ensemble de données cible n. L'état de l'art en ajustement fin multitâche appelé empty dnn dans empty dnn maintient un nombre de têtes correspondant au nombre de tâches dans l'ensemble d'entraînement donc si dans cet exemple il y a quatre tâches dans l'ensemble d'entraînement donc empty dnn maintient quatre têtes comme vous pouvez le voir sur l'image et il échantillonne un batch aléatoire à partir de l'ensemble d'entraînement et si le batch aléatoire appartient par exemple aux tâches de classification de Sing et Selton, il exécute des chemins avant et arrière à travers la première tête. Et si le batch aléatoire appartient aux tâches de classement par paires, il exécute des chemins avant et arrière à travers la dernière tête. Dans notre scénario, un tableau ou un ensemble de données varie le nombre de classes. Donc, il y a de nombreuses tâches. MTDNN maintient le nombre de têtes de couches de sortie et en plus MTDNN doit initialiser de nouvelles têtes pour un nouvel ensemble de données avec une nouvelle tâche. Notre approche appelée ajustement fin de reformulation de tâche est dans notre approche ajustement fin de reformulation de tâche au lieu de maintenir plusieurs têtes nous reformulons chaque ensemble de données en un problème de classification par phrase qui est des tâches à deux classes. Donc, voyons un exemple. Voici notre ensemble de données d'entrée, qui se compose d'entités, de caractéristiques, de texte et de classes. Et nous reformulons la tâche de classer le texte en bas et haut pour classer le texte, l'abstract et la classe en vrai ou faux. Ou en d'autres termes, nous entraînons le modèle de langage pour classer l'abstract et la classe en abstract et classe si l'abstract appartient ou non à la classe donc le vecteur de label dans ce cas reste toujours qui se compose toujours de deux classes et c'est l'algorithme pour notre approche de recherche ou de reformulation d'ajustement fin. Donc, voyons le cadre complet, un ensemble de données alimenté dans FAST et puis FAST exécute dans la phase de couplage, il extrait le texte de la base de connaissances, qui dans cet exemple est l'abstract de la page Wikipedia, puis il reformule la tâche en tâches de classification par phrase, applique le modèle de langage à la nouvelle tâche et produit la probabilité pour chaque classe. Notez que le modèle de langage est déjà ajusté finement sur l'ensemble de données n-1 en utilisant un ajustement fin multitâche préliminaire. Puis nous utilisons le vecteur de sortie du modèle de langage comme une nouvelle caractéristique générée dans le nombre de classes. Pour évaluer notre cadre, nous utilisons un ensemble de classification de 17 tableaux, qui varie en taille, en caractéristiques, en équilibre, en domaine et en performance initiale. Et en tant que base de connaissances, nous utilisons Wikipedia. Nous concevons notre expérience comme une évaluation en laissant un de côté lorsque nous entraînons FAST sur 16 ensembles de données et l'appliquons au 17e ensemble de données. Nous divisons également chaque ensemble de données en quatre cultures. une faute de fourche et appliquons une validation croisée à faute de fourche. Puis nous générons la nouvelle caractéristique et les évaluons en utilisant cinq classificateurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur BERT. Voici les résultats pour notre expérience. Vous pouvez voir que nous comparons notre cadre à l'ajustement fin sur l'ensemble de données cible, l'ajustement fin sur la tâche cible. et l'ajustement fin préliminaire de MTDNN et notre ajustement fin reformulé atteint le meilleur résultat, la meilleure performance, tandis que MTDNN a atteint une amélioration de 2% par rapport à l'ajustement fin sur l'ensemble de données cible. notre approche a atteint une amélioration de 6% lorsque nous regardons sur le petit ensemble de données, nous pouvons voir que la performance de MTDNN diminue et l'amélioration de la phase d'ajustement fin multitâche préliminaire diminue à 1,5 pour cent, mais notre performance a augmenté à 11 pour cent par rapport à l'ajustement fin sur la tâche cible seul. Pour résumer, FAST permet l'enrichissement de vue instantané à partir de 35 échantillons dans notre expérience. Il utilise une architecture pour toutes les tâches et tous les ensembles de données et il maintient la tête du modèle. Mais il ajoute trois phases de reformulation, il augmente l'ensemble d'entraînement et il a besoin d'une valeur cible avec un sens sémantique afin que nous puissions l'alimenter dans le modèle de langage et l'utiliser dans le problème de classification par phrase. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais présenter notre travail de recherche intitulé « Apprendre à raisonner de manière déductive, résolution de problèmes de métro comme extraction de régions complexes ». Je suis Alan du laboratoire d'IA de Biden et c'est un travail conjoint avec Thierry de l'Université du Texas à Austin et Wayloo de SUDD. Tout d'abord, j'aimerais parler de notre motivation pour le raisonnement. Voici donc un exemple où le raisonnement en plusieurs étapes est utile. Cette figure est tirée du document POWN où ils utilisent des invites pour résoudre le problème du métro dans un scénario d'apprentissage par fusion. Ainsi, sur le côté du stylo à encre, nous pouvons voir que si nous donnons quelques exemples avec juste des questions et des réponses, nous ne pourrons peut-être pas obtenir les bonnes réponses. Mais si nous donnons une description de raisonnement plus détaillée, le modèle est capable de prédire la description de raisonnement et aussi de faire une prédiction correcte ici. Il est donc bon d'avoir un raisonnement en plusieurs étapes interprétable comme sortie. Nous pensons également que le problème de méthode est une application directe pour évaluer de telles capacités de raisonnement. Ainsi, dans notre configuration de problème, étant donné les questions, nous devons résoudre cette question et obtenir les réponses numériques. Ainsi, dans nos ensembles de données, nous avons également l'expression mathématique, qui mène à cette réponse particulière également. Ainsi, certaines hypothèses s'appliquent également comme dans les travaux précédents. Nous supposons que la précision des quantités est connue et nous ne considérons que des opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, des opérateurs compliqués peuvent en fait être décodés en ces opérateurs de base. Ainsi, les travaux précédents en résolution de problèmes de méthode peuvent en fait être classés en séquence à séquence et séquence à modèle d'arbre. Ainsi, le modèle traditionnel de séquence à séquence convertit l'expression en une séquence spécifique pour la génération et il est assez facile à mettre en œuvre et peut se généraliser à de nombreux problèmes compliqués différents. Mais les inconvénients sont que la performance n'est généralement pas meilleure que le modèle de structure et il manque d'interprétabilité pour la prédiction. Mais en fait, cette direction est toujours assez populaire à cause du modèle transformateur. Ainsi, dans les modèles basés sur l'arbre, nous structurons en fait ces expressions sous forme d'arbre et suivons un parcours pré-ordre dans les générations d'arbre. Ainsi, ici, nous continuons à générer les opérateurs jusqu'à ce que nous atteignions les feuilles, qui sont les quantités. Ainsi, la bonne chose ici est qu'il nous donne en fait cette structure d'arbre binaire. Mais en fait, c'est assez contre-intuitif. Parce que nous générons d'abord l'opérateur et puis à la fin nous générons les quantités. Et la deuxième chose est qu'il contient également des calculs répétitifs. Ainsi, ici, si nous regardons cette expression, a fois trois plus trois, elle est en fait générée deux fois. Mais en fait, nous devrions réutiliser les résultats. Ainsi, dans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et interprétable. Ainsi, par exemple, ici dans la deuxième étape, nous pouvons obtenir ce diviseur, qui est 27. Et nous pouvons également nous référer aux questions originales pour trouver le contenu pertinent. Et dans ces étapes, nous obtenons les diviseurs. Ainsi, et puis à cette troisième étape, nous obtenons en fait le quotient, n'est-ce pas ? Et après ces trois étapes, nous pouvons en fait réutiliser les résultats de la deuxième étape et puis obtenir les résultats de la quatrième étape. Et puis finalement, nous pouvons obtenir les dividendes. Ainsi, ici, nous générons en fait directement toute l'expression plutôt que de générer des opérateurs ou des quantités uniques. Cela rend le processus plus précis. Ainsi, dans notre système déductif, nous commençons d'abord avec un tas de quantités présentées dans les questions et incluant également quelques constantes comme nos états initiaux. Ainsi, l'expression est représentée par EIJOP, où nous effectuons des opérateurs de Qi à QJ, et une telle expression est en fait dirigée. Nous avons également ici la soustraction inversée pour représenter la direction opposée. Cela ressemble assez à l'extraction de relations. Ainsi, dans un système déductif formel, au temps t, nous appliquons l'opérateur entre la paire Qi et Qj, et nous obtenons alors cette nouvelle expression. Nous l'ajoutons aux états suivants pour devenir une nouvelle quantité. Ainsi, ce diaporama visualise en fait l'évolution des états où nous continuons à ajouter l'expression aux états actuels. Dans nos implémentations de modèle, nous utilisons d'abord un modèle de réseau pré-entraîné, qui peut être des oiseaux ou des robots, et nous codons ensuite une phrase et nous obtenons alors ces représentations de quantités. Une fois que nous obtenons les représentations de quantités, nous pouvons commencer à faire des inférences. Ici, nous montrons un exemple de Q un pour obtenir la représentation pour Q un, ils seront divisés par Q deux et puis multipliés par Q trois. Tout d'abord, nous obtenons la représentation de paire, qui est en fait juste la concaténation entre Q un et Q deux, et puis nous appliquons un réseau feedforward, qui est paramétré par l'opérateur. Et puis finalement, nous obtenons la représentation d'expression Q1 divisé par Q2. Mais en pratique, dans l'étape d'inférence, nous pourrions être en mesure d'obtenir l'expression incorrecte également. Ainsi, ici, toutes les expressions possibles sont égales à trois fois le nombre d'opérateurs. La chose agréable ici est que nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si cette expression n'est pas autorisée, nous pouvons simplement supprimer cette expression dans notre espace de recherche. Ainsi, dans la deuxième étape, nous faisons la même chose, mais la seule différence est une quantité de plus. Ainsi, cette quantité provient de l'expression calculée précédente. Ainsi, finalement, nous pouvons obtenir cette expression finale Q trois fois Q quatre. Et nous pouvons également voir que le nombre de toutes les expressions possibles est différent de l'étape précédente. Ainsi, une telle différence le rend difficile à appliquer la recherche par faisceau parce que la distribution de probabilité entre ces deux étapes est déséquilibrée. Ainsi, la procédure d'entraînement est similaire à l'entraînement d'un modèle de séquence à séquence où nous optimisons la perte à chaque temps t, et ici nous utilisons également ce tau pour représenter quand nous devrions terminer ce processus de génération. Et ici l'espace est différent de la séquence à séquence parce que l'espace est différent à chaque temps t tandis que dans le modèle traditionnel de séquence à séquence c'est le nombre de vocabulaire. Et cela nous permet également d'imposer certaines contraintes à partir de connaissances antérieures. Ainsi, nous menons des expériences sur les ensembles de données de problèmes de méthode couramment utilisés MAWPS, Math 23K, MathQA et SWAM. Et ici, nous montrons brièvement les résultats comparés aux meilleures approches précédentes. Ainsi, notre variante la mieux performante est Roberta Deductive Reasoner. Et en fait, nous n'utilisons pas BeamSearch contrairement aux approches précédentes utilisant BeamSearch. Ainsi, les meilleures approches sont souvent des modèles basés sur l'arbre. Ainsi, dans l'ensemble, notre raisonneur est capable de surpasser de manière significative ce modèle basé sur l'arbre, mais nous pouvons voir le nombre absolu sur MathQA ou SWAMP n'est pas vraiment élevé. Ainsi, nous enquêtons davantage sur les résultats sur SWAMP. Et cet ensemble de données est difficile parce que l'auteur a essayé d'ajouter manuellement quelque chose pour confondre le modèle de NLP, comme ajouter de l'information non pertinente et des quantités supplémentaires. Ainsi, dans notre prédiction, nous trouvons que certaines des valeurs intermédiaires sont en fait négatives. Par exemple, dans cette question, nous demandons combien de pommes Jake a, mais nous avons quelques informations supplémentaires comme dix-sept moins de terrains et Stephen a huit terrains, ce qui est totalement non pertinent. Ainsi, notre modèle fait une prédiction comme celle-ci, qui produit des valeurs négatives. Et nous observons que ces deux expressions ont en fait une similarité. Ainsi, nous pouvons en fait limiter cet espace de recherche en supprimant comme ces résultats sont négatifs afin que nous puissions rendre la réponse correcte. Ainsi, nous trouvons en outre qu'une telle contrainte améliore considérablement pour certains modèles. Par exemple, pour les oiseaux nous avons amélioré sept points et puis pour le modèle de base Roberta nous avons en fait amélioré deux points. Ainsi, un meilleur modèle de langage a une meilleure capacité de compréhension du langage de sorte que le nombre ici est plus élevé pour Roberta et plus bas pour les Oiseaux. Et nous avons également essayé d'analyser la difficulté derrière tout cet ensemble de données. Nous supposons que le nombre de quantités non utilisées peut être considéré comme de l'information non pertinente ici. Ainsi, ici nous pouvons voir que nous avons le pourcentage d'échantillons avec des quantités non utilisées et l'ensemble de données SWAMP a la plus grande partie. Et ici nous montrons également la performance globale. Pour ces échantillons sans quantités non utilisées, ainsi la performance globale est en fait plus élevée que la. Et la performance est en fait plus élevée que la performance globale. Mais avec ces échantillons qui ont des quantités non utilisées, c'est en fait bien pire que la performance globale. Pour MAWPS, nous n'avons pas vraiment beaucoup de cas de bureau, donc j'ignore juste cette partie. Ainsi, finalement, nous voulons montrer l'interprétabilité à travers un exemple d'échec et de participation. Ainsi, ici, notre modèle fait en fait une prédiction incorrecte à la première étape. Ainsi, nous pouvons en fait corréler cette expression avec la phrase ici. Ainsi, nous pensons que cette phrase pourrait induire le modèle en erreur pour une prédiction incorrecte. Ainsi, ici planter un autre trente-cinq fait penser au modèle qu'il devrait y avoir des opérateurs supplémentaires. Ainsi, nous essayons de réviser la phrase pour être quelque chose comme le nombre d'arbres de poire trente-cinq de moins que les arbres de pomme. Ainsi, nous le rendons pour transmettre une sémantique plus précise, de sorte que le modèle soit capable de faire la prédiction correcte. Ainsi, cette étude montre comment les prédictions interprétables nous aident à comprendre le comportement du modèle. Pour conclure notre travail, ainsi notre modèle est en fait assez efficace et nous sommes capables de fournir une procédure de résolution interprétable et nous pouvons facilement incorporer certaines connaissances antérieures comme contrainte qui peuvent aider à améliorer la performance. La dernière chose est que le mécanisme sous-jacent ne s'applique pas seulement aux tâches de résolution de problèmes de réseau mais aussi à d'autres tâches qui impliquent un raisonnement en plusieurs étapes. Mais nous avons également certaines limitations. Si nous avons un grand nombre d'opérateurs ou de constantes, la consommation de mémoire pourrait être assez élevée. Et la deuxième chose est que, comme mentionné, parce que la distribution de probabilité est déséquilibrée à différents temps t, il est également assez difficile d'appliquer la stratégie de recherche par faisceau. Ainsi, c'est la fin du discours, et les questions sont les bienvenues. Merci."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je viens de l'Université de Maastricht. Je vais présenter mon travail conjoint avec Jerry, qui porte sur un nouveau jeu de données pour la récupération d'articles légaux. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus juridiques fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique se retrouvent sans protection ou, pire, exploités. vise à combler le fossé entre les citoyens et la loi en développant un système de récupération efficace des articles légaux. Un tel système pourrait fournir un service d'aide juridique professionnelle gratuit pour les humains non qualifiés. Avant de plonger dans la contribution principale de ce travail, décrivons d'abord le problème de la récupération d'articles légaux. Étant donné une question simple sur une question juridique, comme : quel est le risque si je viole la confidentialité professionnelle ? Un modèle est nécessaire pour récupérer tous les articles légaux pertinents à partir d'un vaste corpus législatif. Cette tâche de récupération d'information présente son propre ensemble de défis. Premièrement, elle traite de deux types de langage : le langage naturel commun pour les questions et le langage juridique complexe pour les statuts. Cette différence dans les distributions de langage rend plus difficile pour un système de récupérer des candidats pertinents, car cela nécessite indirectement un système d'interprétation inhérent qui peut traduire une question naturelle en une question juridique correspondant à la terminologie des statuts. De plus, le droit statutaire n'est pas un empilement d'articles indépendants qui peuvent être traités comme une source d'information complète à eux seuls, comme les nouvelles ou les recettes, par exemple. Au lieu de cela, c'est une collection structurée de dispositions légales qui n'ont un sens complet que lorsqu'elles sont considérées dans le contexte global, c'est-à-dire avec les informations supplémentaires des articles voisins, les champs et sous-champs auxquels ils appartiennent, et leur place dans la structure de la loi. Enfin, les articles légaux ne sont pas de petits paragraphes, qui sont généralement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, ce sont de longs documents pouvant atteindre six mille mots. Les récentes avancées en NLP ont suscité un grand intérêt pour de nombreuses tâches juridiques telles que la prédiction de jugements juridiques ou l'examen automatisé de contrats, mais la récupération d'articles légaux est restée principalement intacte en raison du manque de jeux de données de grande taille et de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur les citoyens francophones pour étudier si un modèle de récupération peut approximer l'efficacité et la fiabilité d'un expert juridique pour la tâche de récupération d'articles légaux. Nos jeux de données de récupération d'articles légaux belges se composent de plus de mille cent questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets allant de la famille, le logement, l'argent, au travail et la sécurité sociale. Chacune d'elles a été étiquetée par des juristes expérimentés avec des références aux articles pertinents d'un corpus de plus de vingt-deux mille six cents articles légaux des codes belges de droit. Parlons maintenant de la manière dont nous avons collecté ce jeu de données. Tout d'abord, nous avons commencé par compiler un vaste corpus d'articles légaux. Nous avons considéré trente-deux codes belges disponibles au public et extrait tous leurs articles ainsi que les en-têtes de section correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges qui demandent des conseils sur une question juridique personnelle. Nous avons eu la chance d'accéder à leurs sites web où leur équipe de juristes expérimentés aborde les problèmes juridiques les plus courants des Belges. Nous avons collecté des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux statuts pertinents. Enfin, nous avons passé les références juridiques et filtré les questions dont les références n'étaient pas des articles dans l'un des codes de droit que nous avions considérés. Les références restantes ont été mises en correspondance et converties en les identifiants d'articles correspondants de notre corpus. Nous avons finalement obtenu mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre vaste corpus de vingt-deux mille six cent trente-trois articles légaux. De plus, chaque question est accompagnée d'un en-tête de sous-suite concaténée dans la structure de la loi. Cette information supplémentaire n'est pas utilisée dans le travail présent mais pourrait être d'intérêt pour les futures recherches sur la récupération d'informations juridiques ou la classification de textes juridiques. Examinons quelques caractéristiques de notre jeu de données. Les questions sont entre cinq et quarante-quatre mots de long avec une médiane de quarante mots. Les articles sont beaucoup plus longs, avec une longueur médiane de soixante-dix-sept mots, dont cent quarante-deux dépassent mille mots. Le plus long étant jusqu'à cinq mille sept cent quatre-vingt-dix mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, avec environ quatre-vingt-cinq pour cent d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très divers, car ils proviennent de trente-deux codes belges différents qui couvrent un grand nombre d'articles légaux collectés de chacun de ces codes belges. Sur les 22 633 articles, seuls 1 612 sont référencés comme pertinents pour au moins une question dans les jeux de données, et environ 80 % de ces articles cités proviennent soit du code civil, du code judiciaire, du code d'instruction pénale ou des codes pénaux. Pendant ce temps, 18 des 32 codes ont moins de 5 articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois. En utilisant nos jeux de données, nous évaluons plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête dans un article, un modèle lexical attribue un score à la paire d'articles de requête en calculant la somme sur les termes de la requête des poids de chacun de ces termes dans cet article. Nous expérimentons avec les fonctions de classement standard TFIDF et BM 25. Le principal problème avec ces approches est qu'elles ne peuvent récupérer que les articles contenant des mots-clés présents dans la requête. Pour surmonter cette limitation, nous expérimentons avec une architecture basée sur les réseaux de neurones qui peut capturer la relation sémantique entre les requêtes et les articles. Nous utilisons un modèle de codeur B qui mappe les requêtes et les articles dans des représentations vectorielles denses. et calculons un score pertinent entre une paire de requête-article par la similarité de leurs emboîtures. Ces emboîtures résultent généralement d'une opération de pooling sur la sortie d'un modèle d'emboîtement de mots. Tout d'abord, nous étudions l'efficacité des codeurs B Siamese dans une configuration d'évaluation sans apprentissage préalable, ce qui signifie que les modèles d'emboîtement de mots pré-entraînés sont appliqués tels quels sans aucun réglage supplémentaire. Nous expérimentons avec un encodeur de texte indépendant du contexte, à savoir Word to Vec et Fastex, et des modèles d'emboîtement contextuels, à savoir Roberta et plus spécifiquement Camembert, qui est un modèle Roberta français. De plus, nous entraînons notre propre modèle basé sur Camembert, Biancoders, sur tous les jeux de données. Notez que pour l'entraînement, nous expérimentons avec les deux saveurs de l'architecture Biancoder. Siamese, qui utilise un modèle d'emboîtement de mots unique qui mappe la requête et l'article ensemble dans un espace vectoriel dense partagé. Et deux tours, qui utilise deux modèles d'emboîtement de mots indépendants qui encodent la requête et l'article séparément dans des espaces d'emboîtement différents. Nous expérimentons avec le pooling de la moyenne, du maximum et du CLS ainsi que le produit scalaire et le cosinus pour le calcul des similarités. Voici les résultats de notre ligne de base sur les ensembles de test avec les méthodes lexicales ci-dessus, les codeurs B Siamese évalués dans une configuration sans apprentissage préalable au milieu, et les codeurs B affinés ci-dessous. Dans l'ensemble, les codeurs B affinés surpassent de manière significative toutes les autres lignes de base. Le modèle à deux tours améliore son variant Siamese en termes de rappel à cent, mais se comporte de manière similaire sur les autres métriques. Bien que BM vingt-cinq se soit sous-performé par rapport au Biancoder entraîné de manière significative, sa performance a indiqué qu'il s'agit toujours d'une ligne de base solide pour la récupération spécifique au domaine. En ce qui concerne l'évaluation sans apprentissage préalable du Biancoder Siamese, nous trouvons que l'utilisation directe des emboîtures d'un modèle Kamembert pré-entraîné sans optimisation pour la tâche de récupération d'informations donne de mauvais résultats, ce qui est cohérent avec les résultats précédents. Et le Biancoder basé sur word-to-vec a surpassé de manière significative le modèle basé sur fast-text et bird, suggérant que peut-être les emboîtements au niveau des mots pré-entraînés sont plus appropriés pour la tâche que les emboîtements au niveau des caractères ou sous-mots lorsqu'ils sont utilisés tels quels. Bien que prometteurs, ces résultats suggèrent une ample opportunité d'amélioration par rapport à un petit expert qualifié qui peut éventuellement récupérer tous les articles pertinents à toute question et ainsi obtenir des scores parfaits. Concluons en discutant de deux limitations de tous les jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble du droit belge, car les articles des décrets, directives et ordonnances sont manquants. Au cours de la construction du jeu de données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction du nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit toujours complètement appropriée. Deuxièmement, nous devrions noter que toutes les questions juridiques ne peuvent pas être répondues avec les statuts seuls. Par exemple, la question \"Puis-je expulser mes locataires s'ils font trop de bruit\" pourrait ne pas avoir de réponse détaillée dans le droit statutaire qui quantifie un seuil de bruit spécifique à partir duquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des précédents similaires à sa situation actuelle. Par exemple, le locataire fait deux fêtes par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles légaux, et le domaine de celles qui sont moins adaptées reste à déterminer. Nous espérons que tout ce travail suscitera un intérêt pour le développement de modèles de récupération d'articles légaux pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article DatSet et Code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes heureux de vous présenter notre travail sur VALS, un indicateur de référence indépendant des tâches destiné à tester les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous pris la peine de mettre en place cet indicateur de référence ? Eh bien, au cours des dernières années, nous avons assisté à une explosion de modèles de vision et de langage basés sur des transformateurs pré-entraînés sur de grandes quantités de paires d'images et de textes. Chacun de ces modèles repousse l'état de l'art sur des tâches de vision et de langage telles que la réponse visuelle à des questions, le raisonnement sur le sens commun visuel, la récupération d'images, l'ancrage de phrases. Nous avons donc reçu un message indiquant que les taux de précision sur ces indicateurs de référence spécifiques aux tâches augmentent régulièrement, mais savons-nous ce que les modèles ont réellement appris ? Qu'est-ce qu'un transformateur de vision et de langage a compris lorsqu'il attribue un score élevé à cette image et à cette phrase pour correspondre et un score bas à celle-ci ? Les modèles de vision et de langage se concentrent-ils sur la bonne chose ou se concentrent-ils sur les biais comme le montrent les travaux antérieurs ? Pour éclairer davantage cet aspect, nous proposons une direction plus agnostique des tâches et introduisons des vannes qui testent la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques qui affectent à la fois les modalités linguistiques et visuelles. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la référence d'entité. Mais comment testons-nous si les modèles de vision et de langage ont capturé ces phénomènes ? Le sabotage, une méthode précédemment appliquée pour les modèles de vision et de langage uniquement pour les phrases nominales par Ravi Shakar et ses collaborateurs et sur le comptage par nous dans des travaux antérieurs. Le sabotage signifie essentiellement que nous prenons la légende d'une image et produisons un leurre en modifiant la légende de telle sorte qu'elle ne décrit plus l'image. Et nous faisons ces modifications de phrases en nous concentrant sur six éléments spécifiques tels que l'existence, la pluralité, le comptage, les relations spatiales, les actions et la référence d'entité. Chaque élément peut consister en un ou plusieurs instruments dans le cas où nous avons trouvé plus d'une manière intéressante de créer des instances de leurre. Par exemple, dans le cas de l'élément actions, nous avons deux instruments, l'un dans lequel le verbe d'action est changé par une action différente et un autre dans lequel les actants sont échangés. Le comptage et la référence d'entité sont également des éléments qui ont plus d'un instrument. Et nous créons ces leurres en nous assurant qu'ils ne parviennent pas à décrire l'image, qu'ils sont grammaticalement et autrement des phrases valides. Ce n'est pas facile à faire car une légende sabotée peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme qu'un homme coupe des plantes et les grands modèles de vision et de langage pourraient s'en rendre compte. Par conséquent, pour obtenir des leurres valides, nous devons agir. Premièrement, nous utilisons des modèles de langage puissants pour proposer des leurres. Deuxièmement, nous utilisons l'inférence de langage naturel ou NLI court pour filtrer les leurres qui pourraient encore décrire l'image puisque lors de la construction des leurres, nous devons nous assurer qu'ils ne parviennent pas à décrire l'image. Pour tester cela automatiquement, nous appliquons l'inférence de langage naturel avec la logique suivante. Nous considérons une image comme la prémisse et sa légende comme son hypothèse entraînée. De plus, nous considérons la légende comme la prémisse et le leurre comme son hypothèse. Si un modèle NLI prédit que le leurre contredit ou est neutre par rapport à la légende, nous prenons cela comme un indicateur d'un leurre valide. Pour être entraîné par la légende, il ne peut pas être un bon leurre puisque par transitivité, il donnera une description véridique de l'image et nous filtrons ces leurres. Mais cette procédure n'est pas parfaite. C'est juste un indicateur pour des leurres valides, par conséquent, comme troisième mesure pour générer des leurres valides, nous employons des annotateurs humains pour valider les données utilisées dans Vals. Ainsi, après le filtrage et l'évaluation humaine, nous avons autant d'instances de test que décrites dans ce tableau. Notez que Vals ne fournit aucune donnée d'entraînement mais seulement des données de test puisqu'il s'agit d'un indicateur de référence zéro-shot. Il est conçu pour tirer parti des capacités existantes des modèles de vision et de langage après l'entraînement préliminaire. Le réglage fin ne permettrait qu'aux modèles d'exploiter des artefacts ou des biais statistiques dans les données. Et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Et comme nous l'avons dit, nous sommes intéressés à évaluer quelles capacités les modèles de vision et de langage ont après l'entraînement préliminaire. Nous expérimentons avec cinq modèles de vision et de langage sur les vannes, à savoir avec Clip, Wilbert, Wilbert Kelvin one et Visual Bert. Deux de nos métriques d'évaluation les plus importantes sont la précision des modèles dans la classification des paires d'images et de phrases en légendes et leurres. Peut-être plus pertinent pour cette vidéo, nous présenterons notre métrique plus permissive, la précision par paire, qui mesure si le score d'alignement de la paire de texte d'image est plus élevé pour la paire de texte d'image correcte que pour sa paire de leurre. Pour plus de métriques et de résultats à leur sujet, consultez notre article. Les résultats avec la précision par paire sont montrés ici et ils sont cohérents avec les résultats que nous avons obtenus à partir des autres métriques, c'est que la meilleure performance zéro-shot est réalisée par Wilbert douze en un, suivi par Wilbert, Alex Mert, Clip et enfin Visual Bird. Il est notable comment les instruments centrés sur des objets individuels comme l'existence et les phrases nominales sont presque résolus par Wilbert douze en un, soulignant que les modèles sont capables d'identifier les objets nommés et leur présence dans les images. Cependant, aucun des éléments restants ne peut être résolu de manière fiable dans nos paramètres de sabotage contradictoire. Nous le voyons à partir des instruments de pluralité et de comptage. que les modèles de vision et de langage ont du mal à distinguer les références à des objets simples par rapport à des objets multiples ou à les compter dans une image. L'élément relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même si cela est soutenu par des biais de plausibilité comme nous le voyons dans l'élément actions. De l'élément référence d'entité, nous découvrons que le suivi de plusieurs références à un même objet dans une image en utilisant des pronoms est également difficile pour les modèles de vision et de langage. Comme vérification de cohérence et parce que c'est une expérience intéressante, nous évaluons également deux modèles uniquement textuels GPT one et GPT two pour évaluer si Vals est résoluble par ces modèles unimodaux en calculant la perplexité de la légende correcte et du leurre sans image ici et en prédisant l'entrée avec la perplexité la plus basse. Si la perplexité est plus élevée pour le leurre, nous prenons cela comme une indication que la légende sabotée peut souffrir d'un biais de plausibilité ou d'autres biais linguistiques. Et il est intéressant de voir que dans certains cas, les modèles GPT uniquement textuels ont capturé la plausibilité du monde mieux que les modèles de vision et de langage. Pour résumer, VALS est un indicateur de référence qui utilise la lentille des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient bien les objets nommés dans leur présence dans les images, comme le montre l'élément existence, mais ont du mal à ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont forcés de respecter des indicateurs linguistiques. Nous aimerions vraiment encourager la communauté à utiliser Valse pour mesurer les progrès vers l'ancrage linguistique avec les modèles de vision et de langage. Et encore plus, Valse pourrait être utilisé comme une évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après l'entraînement ou le réglage fin pour voir si un ensemble de données aide les modèles à s'améliorer sur l'un des aspects testés par Valse. Si vous êtes intéressé, consultez les données Valse sur GitHub, et si vous avez des questions, n'hésitez pas à nous contacter."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamisara de l'Université de Tokyo. Je vais présenter un article intitulé R et SAM, un ensemble de données à grande échelle pour le risque automatique et non la durée via la synthèse des journaux de commit. Je vais expliquer dans l'ordre suivant. Tout d'abord, je vais présenter le risque automatique et non la durée sur lequel nous travaillons dans cette recherche. La note de version est un document technique qui résume les changements distribués avec chaque version d'un produit logiciel. L'image montre les notes de version pour la version deux point six point quatre de la bibliothèque GBUJS. Ces nœuds jouent un rôle important dans le développement open source, mais ils prennent beaucoup de temps à préparer manuellement. Par conséquent, il serait très utile de pouvoir générer automatiquement des notes de version de haute qualité. Je vais me référer à deux recherches antérieures sur la génération automatique de notes de version. La première est un système appelé Arena, publié en 2014. Il adopte une approche basée sur les règles, par exemple, en utilisant l'extracteur de changements pour extraire les différences principales des changements de bibliothèque et des changements de documents à partir des différences entre les versions et en les combinant finalement. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, le système de gestion des problèmes, et ne peut être appliqué qu'aux projets qui utilisent Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub. La seconde est Griff, récemment annoncée en 2020. Elle est disponible sur Internet et peut être stockée via PIP. Ce système a un modèle de classification de texte basé sur l'apprentissage simple et produit l'un des cinq types de rubriques tels que les fonctionnalités ou les corrections de bugs pour chaque message de commit d'entrée. L'image est un exemple d'utilisation qui retourne un pavé de corrections ou de corrections de bugs. Les données d'entraînement de Queryface sont assez petites, environ cinq mille, et seront montrées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n'est pas élevée. Je présente deux recherches connexes, mais il y a des problèmes d'applicabilité limitée et de ressources de données rares. Notre article a résolu ces deux problèmes et génère automatiquement des notes de version de haute qualité. Pour le programme d'applicabilité limitée, nous proposons une méthode de résumé de classificateur de haute qualité utilisant uniquement le message de commit comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais. Pour le deuxième problème de ressources de données rares, nous avons construit un ensemble de données RNSAM composé d'environ quatre-vingt-deux mille morceaux de données en collectant des données à partir de dépôts publics GitHub en utilisant l'API GitHub. Ensuite, je décris notre ensemble de données. Voici un exemple de données. Le côté gauche est un message de commit et le côté droit est les notes de version. Les notes de version sont classées comme améliorations, corrections de bugs, etc. Nous avons mis en place une tâche qui prend les messages de commit comme entrée et produit les notes de version classées. Cela peut être considéré comme une tâche de résumé. Nous avons prédéfini quatre niveaux : fonctionnalités, améliorations, corrections de bugs, dépréciations, amovibles et changements de rupture. Ceux-ci ont été définis sur la base de recherches antérieures et d'autres facteurs. Les notes de version en bas à droite sont extraites des notes de version montrées en bas à gauche. À ce stade, il est nécessaire de détecter les quatre types de rubriques qui ont été définis à l'avance, mais les rubriques ne sont pas toujours cohérentes avec chaque dépôt. Par exemple, le type d'amélioration comprend les améliorations, les améliorations, les optimisations, etc. Nous avons préparé une liste de vocabulaire d'étude des types pour chacune de ces variations notationnelles. Utilisez-la pour détecter la classe RISNOD et corriger le texte du RIS qui suit comme la phrase RISNOD pour la classe. Ensuite, voici un message de commit. Les messages de commit ne sont pas liés à chaque RIS. Comme le montre l'image ci-dessous, si le RIS actuel est persan deux point cinq à dix-neuf, nous devons identifier le RISP précédent deux point cinq à dix-huit et obtenir son diff. C'est un peu fastidieux et ce n'est pas suffisant de simplement obtenir une liste de RIS et de regarder avant et après. Nous avons créé une correspondance heuristique bleue pour obtenir le précédent et le suivant. Après l'analyse de Desset, sept mille deux cents dépôts et quatre-vingt-deux mille morceaux de données ont été corrigés. De plus, le nombre moyen de jetons de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de résumé. De plus, le nombre de jetons uniques est assez important à huit mille huit cent trente mille. Cela est dû au grand nombre de coûts uniques et de noms de méthodes trouvés dans le dépôt. Ensuite, je vais expliquer la méthode proposée. Le modèle de résumé extractif et abstratif transversal est composé de deux modules plus récents : un classificateur utilisant un bot ou un code bot et un générateur utilisant un bot. Tout d'abord, GEAS utilise un classificateur pour classer chaque message de commit dans cinq classes de raisons : fonctionnalités, améliorations, corrections de bugs, dépréciations, plus et autres. Les messages de commit classés comme autres sont rejetés. Ensuite, GEAS applique un générateur aux quatre documents de rubriques indépendamment et génère des notes de version pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de commit et les notes de version ne sont pas connues. Par conséquent, pour entraîner le fil de classe, nous attribuons deux rubriques à chaque message de commit d'entrée en utilisant les dix premiers caractères de chaque message de commit. Nous modélisons l'approche de résumé abstrative du fil de classe par deux méthodes différentes. Le premier modèle, que nous appelons GAS single, est composé d'un seul réseau de secteur à secteur et génère un long texte de note unique en concaténant les messages de commit d'entrée. Le texte de sortie peut être divisé en classe par segment basé sur des symboles de point final spécifiques à la classe. La deuxième méthode, que nous appelons GSMAUC, est composée de quatre réseaux de secteur à secteur différents, chacun correspondant à l'une des classes de notes. D'accord, laissez-moi expliquer l'expérience. Cinq méthodes ont été comparées : GS, GAS single, GAS march, rustling et l'étude précédente grief. En ce qui concerne l'avortement, dans certains cas, ces notes sont produites en plusieurs phrases. Puisqu'il est difficile de calculer le nombre de phrases à zéro, elles sont combinées avec des espaces et traitées comme une longue phrase. Le système est pénalisé lorsque le système produit une phrase courte. Cette pénalité entraîne une valeur de brew plus faible dans les résultats de l'expérience décrits ci-dessous. Enfin, nous calculons également la spécificité car Rouge et Brew ne peuvent pas être calculés si les notes de version sont vides. Une spécificité élevée signifie que le modèle produit correctement un texte vide dans les cas où les notes de version supposent vide. Voici les résultats. Étant donné que l'ensemble de données contient des adresses e-mail, des valeurs de hachage, etc., nous avons également éradiqué l'ensemble de données vert, qui les exclut. GAS et GAS ont obtenu des scores d'erreur Rouge plus de dix points supérieurs à la ligne de base. Cependant, sur l'ensemble de test vert, l'écart de score entre la méthode proposée et la fin de base a sauté à plus de vingt points. Ces résultats indiquent que GAS et GAS sont significativement efficaces. GAS a obtenu un score de loose meilleur que GAS, suggérant que la combinaison d'un classificateur et d'un générateur est efficace sur l'entraînement du classificateur en utilisant un pseudobus. La couverture élevée de GAS peut probablement être atteinte parce que le classificateur peut se concentrer sur la sélection des messages de commit pertinents pour chaque classe. Elle a tendance à produire des règles plus élevées que GAS single, suggérant qu'il est également efficace de développer indépendamment différents modèles de résumé pour chaque classe de not. Voici une analyse d'erreur. Les méthodes ont tendance à produire des phrases plus courtes que les phrases de référence humaines. Dans la figure à droite, la phrase de référence a trois ou quatre phrases tandis que elle n'en a qu'une. La raison de cette réticence du modèle est que dans les données d'entraînement, seulement trente-trois pour cent des phrases sont présentes dans la classe d'amélioration et quarante pour cent dans la classe d'améliorations. De plus, les méthodes GS ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple en haut à droite est un exemple d'un message de commit très désordonné et la phrase complète ne peut pas être générée sans différence par rapport à la demande parallèle correspondante ou au problème. L'exemple ci-dessous montre que les deux messages de commit dans l'entrée sont liés et devraient être combinés en une seule phrase mais cela ne fonctionne pas. Enfin, une conclusion. Nous avons construit un nouvel ensemble de données pour la génération automatique de notes de version. Nous avons également formé la tâche de saisir les messages de commit et de les résumer de manière à ce qu'elle soit applicable à tous les projets écrits en anglais. Nos expériences montrent que la méthode proposée génère moins de bruit de version à une couverture plus élevée que la base. Veuillez consulter notre ensemble de données sur GitHub. Merci."}
