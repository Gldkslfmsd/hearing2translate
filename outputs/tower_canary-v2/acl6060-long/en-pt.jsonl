{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Assa Farari e vou apresentar o nosso artigo FUESHOT TABLAR DATA ARCHICHTMENT Using FINE TUNIC TRANSFORMERS ARCHITTURES. Os cientistas de dados analisam dados e focam principalmente em manipular as características existentes dos dados, mas às vezes essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações substanciais. O objetivo da nossa pesquisa é o enriquecimento automático de dados tabulares usando texto livre de fontes externas. Vamos assumir que temos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a ligação de entidades e análise de texto para extrair novas características da base de conhecimento em texto livre. Nossa estrutura FAST é exatamente esse processo automático. Então, vamos ver um exemplo. E um conjunto de dados é alimentado para o FAST. Neste exemplo, o conjunto de dados é um conjunto de dados universitários quando o seu objetivo é classificar universidades em universidades de baixo e alto escalão e universidades de alto escalão. Como base de conhecimento, usamos a Wikipedia. A primeira fase do FEST é a ligação de entidades, quando cada entidade, neste exemplo, o nome da universidade é ligado a uma entidade dentro da base de conhecimento, e o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipedia. Agora precisamos gerar ou extrair características do texto recuperado. Então, precisamos de uma fase de extração de características que inclui análise de texto e esta é a principal novidade deste artigo e vou aprofundar-me nela nos próximos slides. Após a fase de extração de características, há uma fase de geração de características quando usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, gere características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então primeiro gere duas novas características. Mas se o conjunto de dados tiver cinco classes, primeiro gere cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado da arte atual da análise de texto, que são modelos de linguagem baseados em transformadores como BERT, GPT, XLERT, etc. É, mas não é provável que possamos treinar modelos de linguagem usando os conjuntos de dados de entrada. Então, uma abordagem ingênua será um ajuste fino de tarefa alvo. Então, na fase de extração de características, podemos baixar um modelo de linguagem pré-treinado, ajustar o modelo de linguagem sobre o conjunto de dados alvo neste exemplo para ajustar o modelo de linguagem para classificar texto em classes, resumir em classes, baixo ou alto, receber a saída do modelo de linguagem, que é a probabilidade para cada classe, e usar como novas características. O problema com esta abordagem é que os conjuntos de dados podem ter poucas entidades distintas, texto. Em nosso experimento, quase metade dos conjuntos de dados contêm menos de 400 amostras e o menor conjunto de dados contém 35 amostras em seu conjunto de treinamento. Então, ajustar um modelo de linguagem sobre este conjunto de dados será ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados porque aplicamos o FAST em vários conjuntos de dados. Podemos usar os n menos um conjuntos de dados para coletar informações sobre os n menos um conjuntos de dados e usar essas informações quando analisamos o n-ésimo conjunto de dados. O que sugerimos é adicionar outra fase de ajuste fino, uma fase preliminar de ajuste fino de múltiplas tarefas quando ajustamos o modelo de linguagem sobre n-1 conjuntos de dados e então executamos outra fase de ajuste fino, que é um ajuste fino de tarefa alvo quando ajustamos o modelo de linguagem sobre o n-ésimo conjunto de dados alvo. O estado da arte em ajuste fino de múltiplas tarefas chamado empty dnn em empty dnn mantém cabeçalhos no número de tarefas no conjunto de treinamento então, se neste exemplo houver quatro tarefas no conjunto de treinamento, o empty dnn mantém quatro cabeçalhos como você pode ver na imagem e ele amostra um lote aleatório do conjunto de treinamento e se o lote aleatório pertence, por exemplo, às tarefas de classificação de Sing e Selton, ele executa caminhos de avanço e retrocesso através do primeiro cabeçalho. E se o lote aleatório pertence às tarefas de classificação em pares, ele executa caminhos de avanço e retrocesso através do último cabeçalho. No nosso cenário, uma tabela ou conjunto de dados varia o número de classes. Então, há muitas tarefas. O MTDNN mantém camadas de saída de cabeçalhos de número de classes e, adicionalmente, o MTDNN precisa inicializar novos cabeçalhos para um novo conjunto de dados com uma nova tarefa. Nossa abordagem chamada ajuste fino de reformulação de tarefas é que, em vez de manter vários cabeçalhos, reformulamos cada conjunto de dados em um problema de classificação por frase, que é uma tarefa de duas classes. Então, vamos ver um exemplo. Aqui está o nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes. E reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o resumo e a classe em verdadeiro ou falso. Ou, em outras palavras, treinamos o modelo de linguagem para classificar resumo e classe como resumo e classe se o resumo pertence ou não à classe, então o vetor de rótulo neste caso permanece sempre, consistindo sempre de duas classes e este é o algoritmo para a nossa abordagem de ajuste fino de reformulação. Então, vamos ver a estrutura completa, um conjunto de dados alimentado para o FAST e então o FAST executa na fase de ligação, ele extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipedia, então ele reformula a tarefa em tarefas de classificação por frase, aplica o modelo de linguagem à nova tarefa e produz a probabilidade para cada classe. Note que o modelo de linguagem já foi ajustado finamente sobre o conjunto de dados n-1 usando um ajuste fino de múltiplas tarefas preliminar. Então, usamos o vetor de saída do modelo de linguagem como uma característica recém-gerada no número de classes. Para avaliar nossa estrutura, usamos um conjunto de dados de classificação de 17 tabelas, que varia em tamanho, características, equilíbrio, domínio e desempenho inicial. E como base de conhecimento, usamos a Wikipedia. Projetamos nosso experimento como uma avaliação de deixar uma de fora quando treinamos o FAST em 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro colheitas e aplicamos uma validação cruzada de quatro colheitas. Então, geramos a nova característica e avaliamos usando cinco classificadores de avaliação. Usamos em nosso experimento uma arquitetura baseada em BERT. Aqui estão os resultados para o nosso experimento. Você pode ver que comparamos nossa estrutura com ajuste fino de conjunto de dados alvo, ajuste fino de tarefa alvo. e ajuste fino preliminar do MTDNN e nosso ajuste fino reformulado alcança o melhor resultado, o melhor desempenho, enquanto o MTDNN alcançou uma melhoria de 2% em relação ao ajuste fino de conjunto de dados alvo. nossa abordagem alcançou uma melhoria de 6% quando olhamos para o pequeno conjunto de dados, podemos ver que o desempenho do MTDNN diminui e a melhoria da fase de ajuste fino de múltiplas tarefas preliminar diminui para 1,5 por cento, mas nosso desempenho aumentou para 11 por cento em comparação com o ajuste fino de tarefa alvo sozinho. Para resumir, o FAST permite a enriquecimento de visão única a partir de 35 amostras em nosso experimento. Ele usa uma arquitetura para todos os conjuntos de dados de tarefas e mantém a cabeça do modelo. Mas adiciona três fases de formulação, aumenta o conjunto de treinamento e precisa de um valor alvo com significado semântico para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação por frase. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje vou apresentar o nosso trabalho de pesquisa Aprendendo a Raciocinar Dedutivamente, Resolução de Problemas de Metro como Extração de Regiões Complexas. Eu sou Alan do Laboratório de IA de Biden e este é um trabalho conjunto com Thierry da Universidade do Texas em Austin e Wayloo da SUDD. Primeiro, gostaria de falar sobre a nossa motivação para o raciocínio. Então, aqui mostramos um exemplo onde o raciocínio em várias etapas é útil. Então, esta figura é retirada do artigo POWN onde eles realizam prompts para resolver o problema do metro em um cenário de aprendizado de fusão. Então, no lado da caneta, podemos ver se damos alguns exemplos com apenas perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas se dermos uma descrição de raciocínio mais detalhada, o modelo é capaz de prever a descrição do raciocínio e também fazer uma previsão correta aqui. Então, é bom ter um raciocínio interpretável em várias etapas como saída. E também achamos que o problema de método é uma aplicação direta para avaliar tais habilidades de raciocínio. Então, aqui na nossa configuração de problema, dado as perguntas, precisamos resolver esta questão e obter as respostas numéricas. Então, nos nossos conjuntos de dados, também nos é dada a expressão matemática, que leva a esta resposta específica também. Então, certas suposições também se aplicam como no trabalho anterior. Assumimos que a precisão das quantidades é conhecida e só consideramos operadores básicos como adição, subtração, multiplicação, divisão e exponencial. Além disso, operadores complicados podem ser realmente decodificados em estes operadores básicos. Então, o trabalho anterior em resolução de problemas de método pode realmente ser categorizado em sequência para sequência e sequência para modelo de árvore. Então, o modelo tradicional de sequência para sequência converte a expressão em uma sequência específica para geração e é bastante fácil de implementar e pode generalizar para muitos problemas complicados diferentes. Mas as desvantagens são que o desempenho geralmente não é melhor do que o modelo de estrutura e falta de interpretabilidade para previsão. Mas na verdade, esta direção ainda é bastante popular devido ao modelo transformer. Então, em modelos baseados em árvore, nós realmente estruturamos estas expressões em uma forma de árvore e seguimos uma travessia pré-ordem nas gerações de árvore. Então, aqui continuamos gerando os operadores até chegarmos às folhas, que são as quantidades. Então, aqui a coisa boa é que isso realmente nos dá esta estrutura de árvore binária. Mas na verdade é bastante contraintuitivo. Porque geramos o operador primeiro e depois no final geramos as quantidades. E a segunda coisa é que também contém alguns cálculos repetitivos. Então, aqui, se olharmos para esta expressão, três vezes a mais três, ela é realmente gerada duas vezes. Mas na verdade, deveríamos reutilizar os resultados. Então, na nossa abordagem proposta, queremos resolver esses problemas de forma passo a passo e interpretável. Então, por exemplo, aqui no segundo passo, podemos obter este divisor, que é 27. E também podemos nos referir de volta às perguntas originais para encontrar os conteúdos relevantes. E nestes passos, obtemos os divisores. Então, e depois neste terceiro passo, nós realmente obtemos o quociente, certo? E após estes três passos, podemos realmente reutilizar os resultados do segundo passo e então obter os resultados do quarto passo. E então finalmente, podemos obter os dividendos. Então, aqui nós realmente geramos toda a expressão diretamente em vez de gerar operadores ou quantidades individuais. Então, isso torna o processo mais preciso. Então, no nosso sistema dedutivo, começamos primeiro com um conjunto de quantidades apresentadas nas perguntas e também incluindo algumas constantes como nossos estados iniciais. Então, a expressão é representada por EIJOP, onde realizamos operadores de Qi a QJ, e tal expressão é realmente dirigida. Então, também temos a subtração invertida aqui para representar a direção oposta. Isso é bastante semelhante à extração de relações. Então, em um sistema dedutivo formal, no tempo t, aplicamos o operador entre o par Qi e Qj, e então obtemos esta nova expressão. Adicionamos isso aos estados seguintes para se tornar uma nova quantidade. Então, este slide realmente visualiza a evolução dos estados onde continuamos adicionando expressão aos estados atuais. Nas nossas implementações de modelo, primeiro usamos um modelo de rede pré-treinado, que pode ser pássaros ou robôs, e então codificamos uma frase e então obtemos estas representações de quantidade. Uma vez que obtemos as representações de quantidade, podemos começar a fazer inferência. Aqui mostramos um exemplo de Q um para obter a representação para Q um, eles serão divididos por Q dois e então multiplicados por Q três. Primeiro, obtemos a representação do par, que é basicamente apenas a concatenação entre Q um e Q dois, e então aplicamos uma rede feedforward, que é parametrizada pelo operador. E então finalmente, obtemos a representação da expressão Q1 dividido por Q2. Mas na prática, na etapa de inferência, talvez possamos obter a expressão incorreta também. Então, aqui, toda a expressão possível é igual a três vezes o número de operadores. Então, a coisa boa aqui é que podemos facilmente adicionar restrições para controlar este espaço de busca. Por exemplo, se esta expressão não for permitida, podemos simplesmente remover esta expressão no nosso espaço de busca. Então, no segundo passo, fazemos a mesma coisa, mas a única diferença é mais uma quantidade. Então, esta quantidade vem da expressão calculada anterior. Então, finalmente, podemos obter esta expressão final Q três vezes Q quatro. E também podemos ver que o número de todas as expressões possíveis é diferente do passo anterior. Então, tal diferença torna difícil aplicar a busca por feixe porque a distribuição de probabilidade entre estes dois passos é desequilibrada. Então, o procedimento de treinamento é semelhante ao treinamento de um modelo de sequência para sequência onde otimizamos a perda em cada tempo t, e aqui também usamos este tau para representar quando devemos terminar este processo de geração. E aqui o espaço é diferente de sequência para sequência porque o espaço é diferente em cada tempo t enquanto no modelo tradicional de sequência para sequência é o número de vocabulário. E também nos permite impor certas restrições a partir do conhecimento prévio. Então, realizamos experimentos nos conjuntos de dados de problemas de método comumente usados MAWPS, Math 23K, MathQA e SWAM. E aqui mostramos brevemente os resultados comparados com as melhores abordagens anteriores. Então, a nossa variante de melhor desempenho é Roberta Deductive Reasoner. E na verdade, não usamos BeamSearch em contraste com as abordagens anteriores que usam BeamSearch. Então, as melhores abordagens são frequentemente modelos baseados em árvore. Então, no geral, o nosso reasoner é capaz de superar significativamente este modelo baseado em árvore, mas podemos ver o número absoluto em MathQA ou SWAMP não é realmente alto. Então, investigamos ainda mais os resultados em SWAMP. E este conjunto de dados é desafiador porque o autor tentou adicionar manualmente algo para confundir o modelo de NLP, como adicionar informações irrelevantes e quantidades extras. Então, na nossa previsão, encontramos que alguns dos valores intermediários são realmente negativos. Por exemplo, nesta questão, estamos perguntando quantos maçãs Jake tem, mas temos algumas informações extras como dezessete menos lançamentos e Stephen tem oito lançamentos, o que é totalmente irrelevante. Então, o nosso modelo faz algumas previsões como esta, que estão produzindo valores negativos. E observamos que estas duas expressões realmente têm semelhança. Então, podemos realmente limitar este espaço de busca removendo como esses resultados são negativos para que possamos fazer a resposta correta. Então, descobrimos ainda que tal restrição realmente melhora bastante para alguns modelos. Por exemplo, para pássaros melhoramos sete pontos e então para o modelo base Roberta melhoramos dois pontos. Então, um modelo de linguagem melhor tem uma melhor capacidade de compreensão da linguagem para que o número aqui seja. Aqui é maior para Roberta e menor para Birds. E também tentamos analisar a dificuldade por trás de todo este conjunto de dados. Assumimos que o número de quantidades não utilizadas pode ser considerado como informação irrelevante aqui. Então, aqui podemos ver que temos a porcentagem de amostras com quantidades não utilizadas e o conjunto de dados SWAMP tem a maior parte. E aqui também mostramos o desempenho geral. Para aquelas amostras sem quantidades não utilizadas, então o desempenho geral é realmente maior do que o. E o desempenho é realmente maior do que o desempenho geral. Mas com aquelas amostras que têm quantidade não utilizada, é realmente muito pior do que o desempenho geral. Para MAWPS, não temos realmente muitos casos de mesa, então eu apenas ignoro esta parte. Então, finalmente, queremos mostrar a interpretabilidade através de um exemplo de acidente e participação. Então, aqui, o nosso modelo realmente faz uma previsão errada no primeiro passo. Então, podemos realmente correlacionar esta expressão com a frase aqui. Então, achamos que esta frase pode estar enganando o modelo para uma previsão incorreta. Então, aqui plantar mais trinta e cinco faz o modelo pensar que deveria ser mais operadores. Então, tentamos revisar a frase para ser algo como o número de árvores de pereira trinta e cinco a menos do que as árvores de maçã. Então, fazemos para transmitir uma semântica mais precisa, de modo que o modelo seja capaz de fazer a previsão correta. Então, este estudo mostra como as previsões interpretáveis nos ajudam a entender o comportamento do modelo. Para concluir o nosso trabalho, então primeiro o nosso modelo é realmente bastante eficiente e somos capazes de fornecer um procedimento de resolução interpretável e podemos facilmente incorporar algum conhecimento prévio como restrição que pode ajudar a melhorar o desempenho. A última coisa é que o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de rede, mas também outras tarefas que envolvem raciocínio em várias etapas. Mas também temos certas limitações. Se tivermos um grande número de operadores ou constantes, o consumo de memória pode ser bastante alto. E a segunda coisa é que, como mencionado, porque a distribuição de probabilidade é desequilibrada em diferentes tempos t, então também é bastante desafiador aplicar a estratégia de busca por feixe. Então, este é o fim da palestra, e as perguntas são bem-vindas. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Vou apresentar o meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para recuperação de artigos legais. Questões jurídicas são uma parte integrante da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre os seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem pagar a assistência dispendiosa de um especialista legal ficam desprotegidos ou, pior, explorados. O objetivo deste trabalho é colmatar a lacuna entre as pessoas e a lei, desenvolvendo um sistema eficaz de recuperação de artigos legais. Tal sistema poderia fornecer um serviço gratuito de assistência jurídica profissional para humanos não qualificados. Antes de mergulharmos na principal contribuição deste trabalho, vamos primeiro descrever o problema da recuperação de artigos legais. Dada uma pergunta simples sobre uma questão legal, como qual o risco que corro se violar a confidencialidade profissional? É necessário um modelo para recuperar todos os artigos legais relevantes de um vasto corpo de legislação. Esta tarefa de recuperação de informações vem com o seu próprio conjunto de desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem legal complexa para os estatutos. Esta diferença nas distribuições linguísticas torna mais difícil para um sistema recuperar candidatos relevantes, pois exige indiretamente um sistema de interpretação inerente que pode traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia dos estatutos. Além disso, o direito estatutário não é um conjunto de artigos independentes que possam ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção de estruturas de disposições legais que só têm um significado completo quando consideradas no contexto geral, ou seja, juntamente com as informações suplementares dos artigos vizinhos, os campos e subcampos aos quais pertencem e o seu lugar na estrutura da lei. Por último, os artigos legais não são pequenos parágrafos, que geralmente são a unidade de recuperação típica na maioria dos trabalhos de recuperação. Aqui, são documentos longos que podem ter até seis mil palavras. Os recentes avanços em PLN despertaram um grande interesse em muitas tarefas legais, como previsão de julgamentos legais ou revisão automatizada de contratos, mas a recuperação de artigos legais permaneceu praticamente intocada devido à falta de conjuntos de dados de grande porte e de alta qualidade. Neste trabalho, apresentamos um novo conjunto de dados centrado no cidadão francês nativo para estudar se o modelo de recuperação pode aproximar-se da eficiência e fiabilidade de um especialista legal para a tarefa de recuperação de artigos legais. Os nossos conjuntos de dados de recuperação de artigos legais belgas consistem em mais de mil e cem perguntas legais colocadas por cidadãos belgas. Estas perguntas cobrem uma vasta gama de temas, desde família, habitação, dinheiro, até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de vinte e dois mil e seiscentos artigos legais dos códigos de direito belgas. Vamos agora falar sobre como recolhemos este conjunto de dados. Primeiro, começámos por compilar um vasto corpus de artigos legais. Considerámos trinta e dois códigos belgas disponíveis publicamente e extraímos todos os seus artigos, bem como as respetivas rubricas de secção. Depois, reunimos perguntas legais com referências a estatutos relevantes. Para isso, associámo-nos a um escritório de advocacia belga que recebe anualmente cerca de quatro mil emails de cidadãos belgas que pedem conselhos sobre uma questão legal pessoal. Tivemos a sorte de aceder aos seus sites onde a sua equipa de juristas experientes aborda as questões legais mais comuns dos belgas. Recolhemos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por último, passámos as referências legais e filtrámos as perguntas cujas referências não eram artigos de um dos códigos de direito que consideramos. As referências restantes foram correspondidas e convertidas para os respetivos IDs de artigo do nosso corpus. Acabámos por ter mil e oito perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso vasto corpus de vinte e dois mil e seiscentos e trinta e três artigos legais. Além disso, cada pergunta vem acompanhada de uma concatenação da respetiva rubrica de subseção na estrutura da lei. Esta informação adicional não é utilizada no presente trabalho, mas pode ser de interesse para futuras pesquisas em recuperação de informações legais ou classificação de texto legal. Vamos analisar algumas características do nosso conjunto de dados. As perguntas têm entre cinco e quarenta e quatro palavras, com uma mediana de quarenta palavras. Os artigos são muito mais longos, com uma mediana de setenta e sete palavras, sendo que cento e quarenta e dois deles excedem mil palavras. O mais longo tem até cinco mil e setecentos e noventa palavras. Como mencionado anteriormente, as perguntas cobrem uma vasta gama de temas, com cerca de oitenta e cinco por cento delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os restantes quinze por cento dizem respeito a segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois provêm de trinta e dois códigos belgas diferentes que cobrem um grande número de artigos legais recolhidos de cada um destes códigos belgas. Dos 22.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta nos conjuntos de dados, e cerca de 80% destes artigos citados provêm do código civil, código judicial, código de investigação criminal ou códigos penais. Enquanto isso, 18 dos 32 códigos têm menos de 5 artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo facto de esses códigos focarem menos nos indivíduos e nas suas preocupações. No geral, o número médio de citações para estes artigos citados é dois, e menos de vinte e cinco por cento deles são citados mais de cinco vezes. Usando os nossos conjuntos de dados, avaliámos várias abordagens de recuperação, incluindo arquitetura lexical e densa. Dada uma consulta num artigo, um modelo lexical atribui uma pontuação ao par de artigos de consulta, calculando a soma dos termos da consulta dos pesos de cada um desses termos nesse artigo. Experimentamos com as funções de classificação padrão TFIDF e BM 25. O principal problema com estas abordagens é que só conseguem recuperar artigos que contêm palavras-chave presentes na consulta. Para superar esta limitação, experimentamos com uma arquitetura baseada em redes neurais que pode capturar a relação semântica entre consultas e artigos. Usamos um modelo de codificador B que mapeia consultas e artigos para representações vetoriais densas e calcula uma pontuação relevante entre um par de consulta-artigo pela semelhança das suas incorporações. Estas incorporações resultam tipicamente de uma operação de pooling na saída de um modelo de incorporação de palavras. Primeiro, estudamos a eficácia dos codificadores B Siamese numa configuração de avaliação sem disparo, o que significa que os modelos de incorporação de palavras pré-treinados são aplicados diretamente sem qualquer ajuste adicional. Experimentamos com um codificador de texto independente do contexto, nomeadamente Word to Vec e Fastex, e modelos de incorporação dependentes do contexto, nomeadamente Roberta e, mais especificamente, Camembert, que é um modelo Roberta francês. Além disso, treinámos o nosso próprio modelo baseado em Camembert, Biancoders, em todos os conjuntos de dados. Note que, para o treino, experimentamos com os dois sabores da arquitetura Biancoder. Siamese, que usa um modelo de incorporação de palavras único que mapeia a consulta e o artigo juntos num espaço vetorial denso partilhado. E dois torres, que usa dois modelos de incorporação de palavras independentes que codificam a consulta e o artigo separadamente em espaços de incorporação diferentes. Experimentamos com pooling de média, máximo e CLS, bem como produto escalar e coseno para calcular semelhanças. Aqui estão os resultados da nossa linha de base nos conjuntos de teste com os métodos lexicais acima, os codificadores B Siamese avaliados numa configuração sem disparo no meio e os codificadores B ajustados abaixo. No geral, os codificadores B ajustados superam significativamente todas as outras linhas de base. O modelo de duas torres melhora a sua variante Siamese em recall em cem, mas apresenta um desempenho semelhante nas outras métricas. Embora o BM vinte e cinco tenha apresentado um desempenho significativamente inferior ao Biancoder treinado, o seu desempenho indicou que ainda é uma linha de base forte para recuperação específica do domínio. No que diz respeito à avaliação sem disparo do Biancoder Siamese, descobrimos que usar diretamente as incorporações de um modelo Kamembert pré-treinado sem otimizar para a tarefa de recuperação de informações dá resultados pobres, o que é consistente com descobertas anteriores. E o Biancoder baseado em word-to-vec superou significativamente o modelo baseado em fast-text e bird, sugerindo que talvez as incorporações a nível de palavra pré-treinadas sejam mais apropriadas para a tarefa do que as incorporações a nível de caractere ou sub-palavra quando usadas diretamente. Embora promissores, estes resultados sugerem uma ampla oportunidade de melhoria em comparação com um pequeno especialista qualificado que pode eventualmente recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas. Vamos concluir discutindo duas limitações de todos os conjuntos de dados. Primeiro, o corpus de artigos é limitado aos recolhidos dos trinta e dois códigos belgas considerados, o que não cobre toda a lei belga, pois faltam artigos de decretos, diretivas e ordenanças. Durante a construção do conjunto de dados, todas as referências a esses artigos não recolhidos são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes. Esta perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja completamente apropriada. Segundo, devemos notar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso despejar os meus inquilinos se eles fazem demasiado barulho?\" pode não ter uma resposta detalhada dentro do direito estatutário que quantifique um limite específico de ruído a partir do qual o despejo é permitido. Em vez disso, o proprietário deve provavelmente confiar mais no direito consuetudinário e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz festas até às 2 da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos legais, e o domínio dos menos adequados ainda está por determinar. Esperamos que todo o trabalho desperte interesse no desenvolvimento de modelos práticos e fiáveis de recuperação de artigos legais que possam ajudar a melhorar o acesso à justiça para todos. Podem consultar o nosso artigo DatSet e Code nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, temos o prazer de apresentar nosso trabalho sobre o VALS, um benchmark independente de tarefas destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de criar este benchmark? Bem, nos últimos anos, assistimos a uma explosão de modelos de visão e linguagem baseados em transformadores pré-treinados em grandes quantidades de pares de texto e imagem. Cada um desses modelos impulsiona o estado da arte em tarefas de visão e linguagem, como resposta visual a perguntas, raciocínio de senso comum visual, recuperação de imagens, fundamentação de frases. Então, recebemos uma mensagem de que as acurácias nesses benchmarks específicos de tarefas estão aumentando constantemente, mas sabemos o que os modelos realmente aprenderam? O que é que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta para esta imagem e esta frase para combinar e uma pontuação baixa para esta? Os modelos de visão e linguagem focam na coisa certa ou focam em vieses, como demonstrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica em relação às tarefas e introduzimos válvulas que testam a sensibilidade dos modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguísticas quanto as visuais. Nossas metas são existência, pluralidade, contagem, relações espaciais, ações e referência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? Através de falsificação, um método anteriormente aplicado para modelos de visão e linguagem apenas para frases nominais por Ravi Shakar e colaboradores e para contagem por nós em trabalhos anteriores. Falsificação basicamente significa que pegamos a legenda de uma imagem e produzimos uma falsificação alterando a legenda de tal forma que ela não descreve mais a imagem. E fazemos essas alterações de frases focando em seis partes específicas, como existência, pluralidade, contagem, relações espaciais, ações e referência de entidades. Cada parte pode consistir em um ou mais instrumentos no caso de encontrarmos mais de uma maneira interessante de criar instâncias de falsificação. Por exemplo, no caso da peça de ações, temos dois instrumentos, um em que o verbo da ação é alterado por uma ação diferente e outro em que os atores são trocados. Contagem e referência também são peças que têm mais de um instrumento. E criamos essas falsificações garantindo que elas falhem em descrever a imagem, que são gramaticalmente e de outra forma frases válidas. Isso não é fácil de fazer porque uma legenda falsificada pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem cortar plantas e grandes modelos de visão e linguagem poderiam captar isso. Portanto, para obter falsificações válidas, devemos tomar medidas. Primeiro, fazemos uso de modelos de linguagem fortes para propor falsificações. Segundo, usamos inferência de linguagem natural ou NLI curta para filtrar falsificações que ainda poderiam estar descrevendo a imagem, já que ao construir falsificações precisamos garantir que elas falhem em descrever a imagem. Para testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte justificativa. Consideramos uma imagem como a premissa e sua legenda como a hipótese inferida. Além disso, consideramos a legenda como a premissa e a falsificação como sua hipótese. Se um modelo de NLI prever que a falsificação contradiz ou é neutra em relação à legenda, tomamos isso como um indicador de uma falsificação válida. Para ser inferida pela legenda, não pode ser uma boa falsificação, pois por transitividade dará uma descrição verdadeira da imagem e filtramos essas falsificações. Mas este procedimento não é perfeito. É apenas um indicador para falsificações válidas, portanto, como terceira medida para gerar falsificações válidas, empregamos anotadores humanos para validar os dados usados no VALS. Então, após a filtragem e avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Note que o VALS não fornece dados de treinamento, mas apenas dados de teste, já que é um benchmark de teste sem disparo. Foi projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino só permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos sabemos que esses modelos gostam de trapacear e tomar atalhos. E como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem no VALS, nomeadamente com o CLIP, o Wilbert, o Wilbert Kelvin one e o Visual Bert. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos em classificar pares de frases de imagem em legendas e falsificações. Talvez mais relevante para este vídeo, mostraremos nossa métrica mais permissiva, a precisão emparelhada, que mede se a pontuação de alinhamento de frase de imagem é maior para o par de texto de imagem correto do que para seu par falsificado. Para mais métricas e resultados sobre elas, consulte nosso artigo. Os resultados com precisão emparelhada são mostrados aqui e são consistentes com os resultados que obtivemos das outras métricas, ou seja, o melhor desempenho sem disparo é alcançado pelo Wilbert twelve in one, seguido pelo Wilbert, Alex Mert, Clip e, finalmente, Visual Bird. É notável como instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos pelo Wilbert twelve in one, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser confiavelmente resolvida em nossas configurações de falsificação adversa. Vemos isso a partir dos instrumentos de pluralidade e contagem. que modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos versus múltiplos ou contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo que suportados por vieses de plausibilidade, como vemos na peça de ações. A partir da peça de referência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem. Como verificação de sanidade e porque é um experimento interessante, também avaliamos dois modelos apenas de texto, GPT one e GPT two, para avaliar se o VALS é solucionável por esses modelos unimodais, computando a perplexidade da legenda correta e da falsificação sem imagem aqui e prevendo a entrada com a perplexidade mais baixa. Se a perplexidade for maior para a falsificação, tomamos isso como uma indicação de que a legenda falsificada pode sofrer de viés de plausibilidade ou outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos de texto apenas GPT capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Para resumir, o VALS é um benchmark que usa a lente de construções linguísticas para ajudar a comunidade a melhorar modelos de visão e linguagem testando rigorosamente suas capacidades de fundamentação visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam objetos nomeados em sua presença em imagens bem, como mostrado pela peça de existência, mas lutam para fundamentar sua interdependência e relações em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de incentivar a comunidade a usar o VALS para medir o progresso em direção à fundamentação linguística com modelos de visão e linguagem. E mais ainda, o VALS poderia ser usado como uma avaliação indireta de conjuntos de dados, pois os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo VALS. Se estiver interessado, consulte os dados do VALS no GitHub, e se tiver alguma dúvida, não hesite em nos contatar."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisara da Universidade de Tóquio. Vou apresentar um artigo intitulado R e SAM, um conjunto de dados em grande escala para risco automático não duração via sumarização de log de commit. Vou explicar na seguinte ordem. Primeiro, vou apresentar o risco automático não duração em que estamos trabalhando nesta pesquisa. A nota de lançamento é um documento técnico que resume as alterações distribuídas com cada lançamento de um produto de software. A imagem mostra as notas de lançamento para a versão dois ponto seis ponto quatro da biblioteca GBUJS. Estes nós desempenham um papel importante no desenvolvimento de código aberto, mas são demorados para preparar manualmente. Portanto, seria muito útil poder gerar automaticamente nós de lançamento de alta qualidade. Vou me referir a duas pesquisas anteriores sobre geração automática de notas de lançamento. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de mudanças para extrair as principais diferenças nas mudanças da biblioteca e nas mudanças de documentos entre os lançamentos e, finalmente, combiná-las. A característica mais notável deste sistema é o extrator de problemas no canto superior direito, que deve ser vinculado ao Jira, o ecossistema de problemas, e só pode ser aplicado a projetos que usam o Jira. Em outras palavras, não pode ser usado para muitos projetos no GitHub. A segunda é o Griff, recentemente anunciado em 2020. Está disponível na internet e pode ser armazenado via PIP. Este sistema tem um modelo de classificação de texto baseado em aprendizado simples e produz um de cinco tipos de rubis, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna um rubi corretivo ou correções de bugs. Os dados de treinamento do Queryface são bastante pequenos, cerca de cinco mil, e serão mostrados nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas há problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolveu esses dois problemas e gera automaticamente lançamentos de alta qualidade. Para o programa de aplicabilidade limitada, propomos um método de sumarização de classificador de alta qualidade usando apenas a mensagem do comitê como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos um conjunto de dados RNSAM composto por cerca de oitenta e dois mil pedaços de dados, coletando dados de repositórios públicos do GitHub usando a API do GitHub. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dados. O lado esquerdo é uma mensagem de commit e o lado direito são os nós de lançamento. Os nós de lançamento são classificados como melhorias, correções de bugs, etc. Estabelecemos uma tarefa que leva as mensagens de commit como entrada e produz os nós de lançamento classificados. Isso pode ser considerado uma tarefa de sumarização. Predefinimos quatro níveis: recursos, melhorias, correções de bugs, deprecações, removíveis e mudanças drásticas. Estes foram definidos com base em pesquisas anteriores e outros fatores. As notas de lançamento no canto inferior direito são extraídas das notas de lançamento mostradas no canto inferior esquerdo. Neste momento, é necessário detectar os quatro rubis que foram definidos com antecedência, mas os rubis nem sempre são consistentes com cada repositório. Por exemplo, o rubi de melhorias inclui melhorias, aprimoramentos, otimizações e assim por diante. Preparamos uma lista de vocabulário de rubis de estudo para cada uma dessas variações notacionais. Use-a para detectar a classe RISNOD e corrigir o texto do RIS que segue como a frase RISNOD para a classe. Em seguida, está uma mensagem de commit. As mensagens de commit não estão vinculadas a cada RIS. Como mostrado na imagem abaixo, se o RIS atual é persa dois ponto cinco a dezenove, precisamos identificar o RISP anterior dois ponto cinco a dezoito e obter o diff. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de RIS e olhar o antes e o depois. Criamos uma correspondência heurística azul para obter o anterior e o próximo concurso. Análise Desset no final, sete mil duzentos e quarenta repositórios e oitenta e dois mil pedaços de dados foram corrigidos. Além disso, o número médio de tokens de nós de lançamento é sessenta e três, o que é bastante alto para uma tarefa de sumarização. Além disso, o número de tokens únicos é bastante grande, com oito mil oitocentos e trinta mil. Isso se deve ao grande número de custos e nomes de métodos únicos encontrados no repositório. Em seguida, explicarei o método proposto. O modelo de sumarização extractiva e abstractiva consiste em dois módulos mais novos: um classificador usando bot ou código bot e um gerador usando bot. Primeiro, o GEAS usa um classificador para classificar cada mensagem de commit em cinco classes de razões: recursos, melhorias, correções de bugs, deprecações, mais e outros. As mensagens de commit classificadas como outros são descartadas. Em seguida, o GEAS aplica um gerador aos quatro documentos de rubi de forma independente e gera notas de razões para cada classe. Nesta tarefa, as correspondências diretas entre as mensagens de commit e as notas de razões não são conhecidas. Portanto, para treinar o fio de classe, atribuímos dois rubis a cada mensagem de commit de entrada usando os primeiros dez caracteres de cada mensagem de commit. Modelamos a abordagem de sumarização abstractiva do fio de classe por dois métodos diferentes. O primeiro modelo, que chamamos de GAS single, consiste em uma única rede de setor a setor e gera um único texto de nota longa, dando uma concatenação das mensagens de commit de entrada. O texto de saída pode ser dividido em classes por segmento com base em símbolos de endpoint específicos da classe. O segundo método, que chamamos de GSMAUC, consiste em quatro redes de setor a setor diferentes, cada uma correspondendo a uma das classes de notas. Tudo bem, deixe-me explicar o experimento. Cinco métodos foram comparados: GS, GAS single, GAS march, rustling e o estudo anterior grief. Em relação ao aborto, em alguns casos, essas notas são produzidas em múltiplas frases. Como é difícil calcular o número de frases em zero, elas são combinadas com espaços e tratadas como uma única frase longa. O brew é penalizado quando o sistema produz uma frase curta. Essa penalidade resulta em um valor de brew mais baixo nos resultados do experimento descritos a seguir. Finalmente, também calculamos a especificidade porque o Rouge e o Brew não podem ser calculados se os nós de lançamento estiverem vazios. Uma especificidade alta significa que o modelo produz corretamente textos vazios nos casos em que os nós de lançamento assumem o vazio. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores de hash, etc., também erradicamos o conjunto de dados verde, que os exclui. O GAS e o GAS alcançaram pontuações de erro Rouge mais de dez pontos acima da linha de base. No entanto, no conjunto de teste verde, a lacuna de pontuação entre o método proposto e a linha de base saltou para mais de vinte pontos. Esses resultados indicam que o GAS e o GAS são significativamente eficazes. O GAS obteve uma pontuação de loose melhor do que o GAS, sugerindo que combinar um classificador e um gerador é eficaz no treinamento do classificador usando pseudobus. A alta cobertura do GAS pode ser alcançada provavelmente porque o classificador pode se concentrar em selecionar mensagens de commit relevantes para cada classe. Ela tende a produzir regras mais altas do que ela é single, sugerindo que também é eficaz desenvolver independentemente diferentes modelos de sumarização para cada uma dessas classes de nota. Aqui está uma análise de erro. Os métodos ela tendem a produzir frases mais curtas do que as frases de referência humanas. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto ela tem apenas uma. A razão para a relutância deste modelo é que, nos dados de treinamento, apenas trinta e três por cento das frases estão presentes no rubi de recursos e quarenta por cento no rubi de melhorias. Além disso, os métodos GS não podem gerar notas de razões precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito bagunçada e a frase completa não pode ser gerada sem diferença para a solicitação paralela correspondente ou o problema. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e deveriam ser combinadas em uma única frase, mas não conseguem fazê-lo. Finalmente, uma conclusão. Construímos um novo conjunto de dados para geração automática de razões. Também formamos a tarefa de inserir mensagens de commit e resumí-las de modo que seja aplicável a todos os projetos escritos em inglês. Nossos experimentos mostram que o método proposto gera menos ruído de razões com cobertura mais alta do que a linha de base. Por favor, confira nosso conjunto de dados no GitHub. Obrigado."}
