{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Assa Farari und ich werde unseren Artikel FUESHOT TABLAR DATA ARCHICHTMENT Using FINE TUNIC TRANSFORMERS ARCHITTURES vorstellen. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der vorhandenen Datenmerkmale, aber manchmal sind diese Merkmale begrenzt. Die Generierung von Merkmalen mithilfe einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Anreicherung tabellarischer Daten mithilfe von freitextlichen externen Quellen. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensdatenbank. Wir benötigen einen automatischen Prozess, der Entität-Verknüpfung und Textanalyse beinhaltet, um neue Merkmale aus dem freitextlichen Wissen der Wissensdatenbank zu extrahieren. Unser Framework FAST ist genau dieser automatische Prozess. Lassen Sie uns also ein Beispiel betrachten. Und ein Datensatz wird in FAST eingespeist. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, wenn das Ziel darin besteht, Universitäten in niedrig eingestufte Universitäten und hoch eingestufte Universitäten zu klassifizieren. Als Wissensdatenbank verwenden wir Wikipedia. Die erste Phase von FEST ist die Entität-Verknüpfung, wenn jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensdatenbank verknüpft wird, und der Text der Entitäten der Wissensdatenbank wird extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text das Abstract der Wikipedia-Seite. Jetzt müssen wir Merkmale aus dem Retriever-Text generieren oder extrahieren. Also benötigen wir eine Merkmalsextraktionsphase, die Textanalyse beinhaltet, und dies ist die Hauptneuheit dieses Artikels, und ich werde in den nächsten Folien tief darauf eingehen. Nach der Merkmalsextraktionsphase folgt eine Merkmalgenerierungsphase, wenn wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst generieren wir Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, also generieren wir zuerst zwei neue Merkmale. Aber wenn der Datensatz fünf Klassen hat, generieren wir zuerst fünf neue Merkmale. Jedes Merkmal repräsentiert die Wahrscheinlichkeit für jede Klasse. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik der Textanalyse, die auf Transformer basierende Sprachmodelle wie BERT, GPT, XLERT usw. Es ist aber nicht wahrscheinlich, dass wir Sprachmodelle mit den Eingabe-Datensätzen trainieren können. Ein naiver Ansatz wäre daher eine Zielaufgaben-Feinabstimmung. Also können wir in der Merkmalsextraktionsphase ein vorgefertigtes Sprachmodell herunterladen, das Sprachmodell über den Zieldatensatz in diesem Beispiel feinabstimmen, um das Sprachmodell zu feinabstimmen, um Text in Klassen zu klassifizieren, in Absätze zu klassifizieren, niedrig oder hoch, den Ausgabewert des Sprachmodells zu erhalten, was die Wahrscheinlichkeit für jede Klasse ist, und als neue Merkmale zu verwenden. Das Problem mit diesem Ansatz ist, dass Datensätze möglicherweise nur wenige verschiedene Entitäten, Texte haben. In unserem Experiment enthalten fast die Hälfte der Datensätze weniger als 400 Proben und der kleinste Datensatz enthält 35 Proben in seinem Trainingsdatensatz. Daher wäre eine Feinabstimmung eines Sprachmodells über diesen Datensatz unwirksam. Aber wir können vorheriges Wissen über vorgeanalysierte Datensätze nutzen, denn wir wenden FAST auf mehrere Datensätze an. Wir können die n minus one Datensätze nutzen, um Informationen über die n minus one Datensätze zu sammeln und diese Informationen nutzen, wenn wir den n-ten Datensatz analysieren. Was wir vorschlagen, ist, eine weitere Feinabstimmungs-Phase hinzuzufügen, eine vorläufige Multitask-Feinabstimmung, wenn wir das Sprachmodell über n-1 Datensätze feinabstimmen und dann eine weitere Feinabstimmungs-Phase ausführen, die eine Zielaufgaben-Feinabstimmung ist, wenn wir das Sprachmodell über den n-ten Zieldatensatz feinabstimmen. Der Stand der Technik in der Multitask-Feinabstimmung heißt empty dnn. In empty dnn werden mehrere Köpfe in der Anzahl der Aufgaben im Trainingsdatensatz beibehalten, also wenn es in diesem Beispiel vier Aufgaben im Trainingsdatensatz gibt, werden vier Köpfe beibehalten, wie Sie auf dem Bild sehen können, und es wird eine zufällige Batch aus dem Trainingsdatensatz ausgewählt, und wenn die zufällige Batch beispielsweise zu den Klassifizierungsaufgaben von Sing und Selton gehört, wird der Vorwärts- und Rückwärtspfad durch den ersten Kopf ausgeführt. Und wenn die zufällige Batch zu paarweisen Ranking-Aufgaben gehört, wird der Vorwärts- und Rückwärtspfad durch den letzten Kopf ausgeführt. In unserem Szenario variiert eine Tabelle oder ein Datensatz die Anzahl der Klassen. Also gibt es viele Aufgaben. MTDNN behält die Anzahl der Klassen-Kopf-Ausgabeschichten bei und zusätzlich muss MTDNN neue Köpfe für einen neuen Datensatz mit einer neuen Aufgabe initialisieren. Unser Ansatz, den wir Task-Reformulierung-Feinabstimmung nennen, besteht darin, dass wir anstelle von mehreren Köpfen jeden Datensatz in ein Satz pro Klassifikationsproblem reformulieren, was zwei Klassen-Aufgaben sind. Lassen Sie uns also ein Beispiel betrachten. Hier ist unser Eingang-Datensatz, der aus Entitäten, Merkmalen, Text und Klassen besteht. Und wir reformulieren die Aufgabe von der Klassifizierung des Textes in niedrig und hoch in die Klassifizierung des Textes, des Abstracts und der Klasse in wahr oder falsch. Oder mit anderen Worten, wir trainieren das Sprachmodell, um Abstract und Klasse zu klassifizieren, um zu bestimmen, ob das Abstract zur Klasse gehört oder nicht, sodass der Label-Vektor in diesem Fall immer aus zwei Klassen besteht und dies ist der Algorithmus für unseren Find- oder reformulierten Feinabstimmungsansatz. Also lassen Sie uns den vollständigen Rahmen betrachten, einen Datensatz, der in FAST eingespeist wird, und dann führt FAST die Verknüpfungsphase aus, extrahiert den Text aus der Wissensdatenbank, was in diesem Beispiel das Abstract der Wikipedia-Seite ist, reformuliert dann die Aufgabe in Satz pro Klassifikationsaufgaben, wendet das Sprachmodell auf die neue Aufgabe an und gibt die Wahrscheinlichkeit für jede Klasse aus. Beachten Sie, dass das Sprachmodell bereits über n-1 Datensatz mit einer vorläufigen Multitask-Feinabstimmung feinabgestimmt wurde. Dann verwenden wir den Ausgabe-Vektor des Sprachmodells als neu generiertes Merkmal in der Anzahl der Klassen. Um unser Framework zu bewerten, verwenden wir einen 17-Tabellen-Klassifizierungsdatensatz, der in Größe, Merkmalen, Balance, Domäne und anfänglicher Leistung variiert. Und als Wissensdatenbank verwenden wir Wikipedia. Wir gestalten unser Experiment als Leave-One-Out-Bewertung, wenn wir FAST über 16 Datensätzen trainieren und es auf den 17. Datensatz anwenden. Wir teilen jeden Datensatz auch in vier Ernten auf. Wir teilen einen Datensatz in vier Ernten auf und wenden eine vierfache Fehler-Kreuzvalidierung an. Dann generieren wir das neue Merkmal und bewerten sie mit fünf Bewertungs-Klassifikatoren. Wir verwenden in unserem Experiment eine auf BERT basierende Architektur. Hier sind die Ergebnisse für unser Experiment. Sie können sehen, dass wir unser Framework mit Zieldatensatz-Feinabstimmung, Zielaufgaben-Feinabstimmung und MTDNN vorläufiger Feinabstimmung vergleichen. Und unsere reformulierte Feinabstimmung erzielt das beste Ergebnis, die beste Leistung, während MTDNN eine 2%-ige Verbesserung gegenüber der Zieldatensatz-Feinabstimmung erzielt. Unser Ansatz erzielte eine 6%-ige Verbesserung, wenn wir uns den kleinen Datensatz ansehen, können wir sehen, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen Multitask-Feinabstimmung auf 1,5 Prozent sinkt, aber unsere Leistung stieg auf 11 Prozent im Vergleich zur alleinigen Zielaufgaben-Feinabstimmung. Zusammenfassend ermöglicht FAST eine automatische Anreicherung von 35 Proben in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Aber es fügt drei Reformulierungs-Phasen hinzu, es erweitert den Trainingsdatensatz und es benötigt einen Zielwert mit semantischer Bedeutung, damit wir ihn in das Sprachmodell einfüttern und in das Satz pro Klassifikationsproblem verwenden können. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen. Heute werde ich unsere Forschungsarbeit „Learning to Reason Deductively, Metro problem solving as complex region extraction“ vorstellen. Ich bin Alan vom Biden's AI Lab und dies ist eine gemeinsame Arbeit mit Thierry von der University of Texas at Austin und Wayloo von SUDD. Zunächst möchte ich über unsere Motivation für das Schließen von Argumenten sprechen. Hier zeigen wir ein Beispiel, in dem mehrstufiges Schließen hilfreich ist. Diese Abbildung stammt aus dem POWN-Papier, in dem sie das Schließen zur Lösung des Metrourproblems in einem Fusion-Lern-Szenario durchführen. Auf der Netzstiftseite können wir sehen, dass wir, wenn wir einige Beispiele mit nur Fragen und Antworten geben, möglicherweise nicht die richtigen Antworten erhalten. Aber wenn wir eine etwas ausführlichere Schließbeschreibung geben, kann das Modell die Schließbeschreibung vorhersagen und auch hier eine korrekte Vorhersage treffen. Es ist also gut, interpretierbares mehrstufiges Schließen als Ausgabe zu haben. Und wir denken auch, dass das Methode-Problem eine einfache Anwendung zur Bewertung solcher Schließfähigkeiten ist. Hier in unserem Problemstellung. Bei den gegebenen Fragen müssen wir diese Frage lösen und die numerischen Antworten erhalten. In unseren Datensätzen wird uns auch der mathematische Ausdruck gegeben, der zu dieser bestimmten Antwort führt. Bestimmte Annahmen gelten also auch wie in früheren Arbeiten. Wir nehmen an, dass die Genauigkeit der Größenordnungen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponential betrachten. Darüber hinaus können komplizierte Operatoren tatsächlich in diese grundlegenden Operatoren decodiert werden. Frühere Arbeiten zur Lösung von Methode-Problemen können tatsächlich in Sequenz-zu-Sequenz und Sequenz-zu-Baum-Modelle kategorisiert werden. Das traditionelle Sequenz-zu-Sequenz-Modell konvertiert den Ausdruck in eine bestimmte Sequenz für die Generierung und es ist ziemlich einfach zu implementieren und kann auf viele verschiedene komplexe Probleme verallgemeinert werden. Aber die Nachteile sind, dass die Leistung im Allgemeinen nicht besser ist als das Strukturmodell und es fehlt an Interpretierbarkeit für Vorhersagen. Aber in Wirklichkeit ist diese Richtung immer noch ziemlich beliebt wegen des Transformer-Modells. In baumgestützten Modellen strukturieren wir diese Ausdrücke tatsächlich in einer Baumform und folgen einer vorrangigen Durchquerung in Baumgenerierungen. Hier erzeugen wir die Operatoren weiter, bis wir die Blätter erreichen, die die Größenordnungen sind. Hier ist das Gute, dass es uns tatsächlich diese binäre Baumstruktur gibt. Aber in Wirklichkeit ist es ziemlich kontraintuitiv. Denn wir erzeugen zuerst den Operator und dann am Ende erzeugen wir die Größenordnungen. Und die zweite Sache ist, dass es auch einige repetitive Berechnungen enthält. Hier, wenn wir uns diesen Ausdruck ansehen, a mal drei plus drei, wird er tatsächlich zweimal erzeugt. Aber in Wirklichkeit sollten wir die Ergebnisse wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir hier in der zweiten Stufe diesen Divisor erhalten, der 27 ist. Und wir können uns auch auf die ursprünglichen Fragen beziehen, um die relevanten Inhalte zu finden. Und in diesen Schritten erhalten wir die Divisor. Und dann in diesem dritten Schritt erhalten wir tatsächlich den Quotienten, richtig? Und nach diesen drei Schritten können wir tatsächlich die Ergebnisse aus der zweiten Stufe wiederverwenden und dann die Ergebnisse der vierten Stufe erhalten. Und dann können wir schließlich die Dividenden erhalten. Hier erzeugen wir tatsächlich den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Größenordnungen zu erzeugen. Das macht den Prozess genauer. In unserem deduktiven System beginnen wir zunächst mit einer Reihe von Größenordnungen, die in den Fragen präsentiert werden, und schließen auch einige Konstanten als unsere Anfangszustände ein. Der Ausdruck wird durch EIJOP dargestellt, wobei wir Operatoren von Qi bis QJ durchführen, und ein solcher Ausdruck ist tatsächlich gerichtet. Wir haben hier auch die Subtraktion umgekehrt, um die entgegengesetzte Richtung darzustellen. Das ist ziemlich ähnlich zur Relationenextraktion. In einem formellen deduktiven System wenden wir zum Zeitpunkt t den Operator zwischen dem Qi und Qj-Paar an und erhalten dann diesen neuen Ausdruck. Wir fügen ihn den nächsten Zuständen hinzu, um eine neue Größe zu werden. Diese Folie visualisiert tatsächlich die Entwicklung der Zustände, wobei wir weiterhin Ausdrücke zu den aktuellen Zuständen hinzufügen. In unseren Modelimplementierungen verwenden wir zunächst ein vorgefertigtes Netzwerkmodell, das Vögel oder Roboter sein kann, und kodieren dann einen Satz und erhalten dann diese Größenordnungsdarstellungen. Sobald wir die Größenordnungsdarstellungen erhalten haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel von Q1, um die Darstellung für Q1 zu erhalten, sie werden durch Q2 geteilt und dann mit Q3 multipliziert. Zuerst erhalten wir die Paar-Darstellung, die im Grunde nur die Verkettung zwischen Q1 und Q2 ist, und dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert ist. Und dann erhalten wir schließlich die Ausdruck-Darstellung Q1 geteilt durch Q2. Aber in der Praxis, in der Inferenzstufe, könnten wir möglicherweise auch den falschen Ausdruck erhalten. Hier ist der gesamte mögliche Ausdruck gleich drei mal die Anzahl der Operatoren. Das Schöne hier ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu steuern. Wenn dieser Ausdruck zum Beispiel nicht erlaubt ist, können wir diesen Ausdruck einfach in unserem Suchraum entfernen. Im zweiten Schritt tun wir das gleiche, aber der einzige Unterschied ist eine weitere Größe. Diese Größe stammt aus dem vorher berechneten Ausdruck. Schließlich können wir diesen endgültigen Ausdruck Q3 mal Q4 erhalten. Und wir können auch sehen, dass die Anzahl aller möglichen Ausdrücke sich von der vorherigen Stufe unterscheidet. Ein solcher Unterschied macht es schwer, die Strahlensuche anzuwenden, weil die Wahrscheinlichkeitsverteilung zwischen diesen beiden Stufen unausgewogen ist. Das Trainingsprogramm ist ähnlich wie das Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust zu jedem Zeitpunkt optimieren, und hier verwenden wir auch dieses Tau, um darzustellen, wann wir diesen Generierungsprozess beenden sollten. Und hier ist der Raum anders als Sequenz-zu-Sequenz, weil der Raum zu jedem Zeitpunkt anders ist, während im traditionellen Sequenz-zu-Sequenz-Modell die Anzahl des Vokabulars ist. Und es ermöglicht uns auch, bestimmte Einschränkungen aus vorherigem Wissen aufzuerlegen. Wir führen Experimente auf den häufig verwendeten Methode-Problem-Datensätzen MAWPS, Math 23K, MathQA und SWAM durch. Und hier zeigen wir kurz die Ergebnisse im Vergleich zu den vorherigen besten Ansätzen. Unsere beständigste Variante ist Roberta Deductive Reasoner. Und tatsächlich verwenden wir im Gegensatz zu vorherigen Ansätzen, die BeamSearch verwenden, keinen BeamSearch. Die besten Ansätze sind oft baumgestützte Modelle. Insgesamt ist unser Schließer in der Lage, dieses baumgestützte Modell erheblich zu übertreffen, aber wir können sehen, dass die absoluten Zahlen bei MathQA oder SWAMP nicht wirklich hoch sind. Wir untersuchen die Ergebnisse auf SWAMP weiter. Dieser Datensatz ist herausfordernd, weil der Autor versucht hat, dem NLP-Modell etwas hinzuzufügen, um es zu verwirren, wie das Hinzufügen von irrelevanten Informationen und zusätzlichen Größenordnungen. In unserer Vorhersage finden wir, dass einige der Zwischenergebnisse tatsächlich negativ sind. Zum Beispiel fragen wir in dieser Frage, wie viele Äpfel Jake hat, aber wir haben einige zusätzliche Informationen wie siebzehn weniger Pitches und Stephen hat acht Pitches, was völlig irrelevant ist. Unser Modell macht eine Vorhersage wie diese, die negative Werte produziert. Und wir beobachten, dass diese beiden Ausdrücke tatsächlich Ähnlichkeit haben. Wir können diesen Suchraum tatsächlich einschränken, indem wir solche Ergebnisse entfernen, die negativ sind, damit wir die Antwort richtig machen können. Wir finden außerdem, dass eine solche Einschränkung für einige Modelle ziemlich viel verbessert. Zum Beispiel haben wir bei Vögeln sieben Punkte verbessert und dann für das Roberta-Basismodell haben wir tatsächlich zwei Punkte verbessert. Ein besseres Sprachmodell hat eine bessere Sprachverständnisfähigkeit, sodass die Zahl hier höher für Roberta und niedriger für Vögel ist. Wir versuchen auch, die Schwierigkeit hinter diesem gesamten Datensatz zu analysieren. Wir nehmen an, dass die Anzahl der ungenutzten Größenordnungen als irrelevante Informationen angesehen werden kann. Hier können wir sehen, dass wir den Prozentsatz der Proben mit ungenutzten Größenordnungen haben und der SWAMP-Datensatz den größten Teil ausmacht. Und hier zeigen wir auch die Gesamtleistung. Für diese Proben ohne ungenutzte Größenordnungen ist die Gesamtleistung tatsächlich höher als die. Und die Leistung ist tatsächlich höher als die Gesamtleistung. Aber mit diesen Proben, die eine ungenutzte Größe haben, ist es tatsächlich viel schlechter als die Gesamtleistung. Für MAWPS haben wir nicht wirklich zu viele Tischfälle, also ignoriere ich diesen Teil. Schließlich möchten wir die Interpretierbarkeit durch ein Crash- und Teilnahmebeispiel zeigen. Hier macht unser Modell tatsächlich eine falsche Vorhersage in der ersten Stufe. Wir können diesen Ausdruck tatsächlich mit dem Satz hier korrelieren. Wir denken, dass dieser Satz das Modell möglicherweise in die Irre führt, was zu einer falschen Vorhersage führt. Hier führt das Pflanzen von weiteren fünfunddreißig dazu, dass das Modell denkt, es sollte zusätzliche Operatoren sein. Wir versuchen, den Satz in etwas wie die Anzahl der Birnbäume ist fünfunddreißig weniger als die Apfelbäume zu überarbeiten. Wir machen es, um genauere Semantik zu vermitteln, sodass das Modell die Vorhersage richtig machen kann. Diese Studie zeigt, wie die interpretierbaren Vorhersagen uns helfen, das Verhalten des Modells zu verstehen. Um unsere Arbeit abzuschließen, ist unser Modell tatsächlich ziemlich effizient und wir sind in der Lage, ein interpretierbares Löseverfahren bereitzustellen und wir können leicht einige vorherige Kenntnisse als Einschränkung einbeziehen, die die Leistung verbessern können. Die letzte Sache ist, dass der zugrunde liegende Mechanismus nicht nur auf Netzwerkproblem-Löseskripten anwendbar ist, sondern auch auf andere Aufgaben, die mehrstufiges Schließen beinhalten. Aber wir haben auch bestimmte Einschränkungen. Wenn wir eine große Anzahl von Operatoren oder Konstanten haben, könnte der Speicherverbrauch ziemlich hoch sein. Und die zweite Sache ist, wie erwähnt, weil die Wahrscheinlichkeitsverteilung zu verschiedenen Zeitpunkten unausgewogen ist, ist es auch ziemlich herausfordernd, die Strahlsuchstrategie anzuwenden. Das ist das Ende des Vortrags und Fragen sind willkommen. Danke."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry vorstellen, die sich auf einen neuen Datensatz für die Artikelabfrage im Gesetzbuch bezieht. Rechtliche Fragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat wenig bis kein Wissen über ihre Rechte und grundlegende rechtliche Prozesse. Infolgedessen bleiben viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsanwalts nicht leisten können, ungeschützt oder im schlimmsten Fall ausgebeutet. Das Ziel dieses Projekts ist es, die Kluft zwischen den Menschen und dem Gesetz zu überbrücken, indem ein effektives Abruf-System für gesetzliche Artikel entwickelt wird. Ein solches System könnte einen kostenlosen professionellen Rechtshilfsdienst für nicht-juristische Menschen bereitstellen. Bevor wir uns auf den Hauptbeitrag dieser Arbeit einlassen, lassen Sie uns zunächst das Problem der Abfrage von gesetzlichen Artikeln beschreiben. Angenommen, es wird eine einfache Frage zu einer rechtlichen Angelegenheit gestellt, wie zum Beispiel: Was riskiert man, wenn man die berufliche Vertraulichkeit verletzt? Ein Modell ist erforderlich, um alle relevanten gesetzlichen Artikel aus einem großen Gesetzeswerk abzurufen. Diese Informationsbeschaffung stellt eine eigene Reihe von Herausforderungen dar. Erstens befasst sie sich mit zwei Arten von Sprache: der natürlichen Sprache für die Fragen und der komplexen rechtlichen Sprache für die Statuten. Dieser Unterschied in den Sprachverteilungen erschwert es einem System, relevante Kandidaten abzurufen, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die mit der Terminologie der Statuten übereinstimmt. Darüber hinaus ist das Gesetz kein Stapel unabhängiger Artikel, die als eigenständige Informationsquelle behandelt werden können, wie zum Beispiel Nachrichten oder Rezepte. Stattdessen ist es eine Strukturkollektion von Rechtsvorschriften, die erst im Gesamtzusammenhang eine vollständige Bedeutung erhalten, das heißt zusammen mit den ergänzenden Informationen aus den benachbarten Artikeln, den Bereichen und Teilbereichen, zu denen sie gehören, und ihrem Platz in der Struktur des Rechts. Zuletzt sind gesetzliche Artikel keine kleinen Absätze, was normalerweise die typische Abrufeinheit in den meisten Abruf-Arbeiten ist. Hierbei handelt es sich um lange Dokumente, die bis zu sechstausend Wörter umfassen können. Die jüngsten Fortschritte in der NLP haben großes Interesse an vielen rechtlichen Aufgaben geweckt, wie der Vorhersage von Gerichtsurteilen oder der automatisierten Vertragsüberprüfung, aber die Abfrage von gesetzlichen Artikeln ist aufgrund des Mangels an großen und hochwertigen Datensätzen weitgehend unberührt geblieben. In dieser Arbeit präsentieren wir einen neuen Datensatz, der auf französische Muttersprachler ausgerichtet ist, um zu untersuchen, ob ein Abrufmuster die Effizienz und Zuverlässigkeit eines Rechtsanwalts für die Aufgabe der Abfrage von gesetzlichen Artikeln annähern kann. Unsere belgischen Datensätze zur Abfrage von gesetzlichen Artikeln bestehen aus mehr als eintausend einhundert rechtlichen Fragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken eine breite Palette von Themen von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung ab. Jeder von ihnen wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als zweiundzwanzigtausendsechs-hundert gesetzlichen Artikeln aus belgischen Gesetzbüchern gekennzeichnet. Lassen Sie uns nun darüber sprechen, wie wir diesen Datensatz gesammelt haben. Zuerst begannen wir damit, einen großen Korpus von Gesetzesartikeln zusammenzustellen. Wir berücksichtigten dreiunddreißig öffentlich zugängliche belgische Gesetzbücher und extrahierten alle ihre Artikel sowie die entsprechenden Abschnittsüberschriften. Dann sammelten wir rechtliche Fragen mit Verweisen auf relevante Statuten. Dazu arbeiteten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr rund viertausend E-Mails von belgischen Bürgern erhält, die um Rat zu einem persönlichen rechtlichen Problem bitten. Wir hatten das Glück, Zugang zu ihren Websites zu erhalten, auf denen ihr Team erfahrener Juristen die häufigsten rechtlichen Probleme der Belgier anspricht. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Statuten annotiert waren. Zuletzt gaben wir die rechtlichen Verweise weiter und filterten die Fragen heraus, deren Verweise keine Artikel in einem der von uns berücksichtigten Gesetzbücher waren. Die verbleibenden Verweise wurden mit den entsprechenden Artikel-IDs aus unserem Korpus abgeglichen und umgewandelt. Am Ende hatten wir eintausendneunundzwanzig Fragen, die sorgfältig mit den IDs der relevanten Artikel aus unserem großen Korpus von zweiundzwanzigtausendsechs-hundertdreiunddreißig gesetzlichen Artikeln gekennzeichnet waren. Zusätzlich ist jede Frage mit einer Verkettung ihrer Folgeüberschrift in der Struktur des Rechts versehen. Diese zusätzliche Information wird in der vorliegenden Arbeit nicht verwendet, könnte aber für zukünftige Forschungen zum rechtlichen Informationsabruf oder zur rechtlichen Textklassifizierung von Interesse sein. Werfen wir einen Blick auf einige Merkmale unseres Datensatzes. Die Fragen sind zwischen fünf und vierundvierzig Wörter lang, mit einem Median von vierzig Wörtern. Die Artikel sind viel länger, mit einem Median von einundsiebzig Wörtern, wobei einhundertvierundzwei von ihnen mehr als eintausend Wörter umfassen. Die längsten sind bis zu fünfeinhalbtausendsiebenhundertneunundneunzig Wörter lang. Wie bereits erwähnt, decken die Fragen eine breite Palette von Themen ab, wobei rund achtundachtzig Prozent von ihnen entweder über Familie, Wohnen, Geld oder Justiz handeln, während die verbleibenden fünfzehn Prozent sich entweder auf Sozialversicherung, Ausländer oder Arbeit beziehen. Die Artikel sind ebenfalls sehr vielfältig, da sie aus dreiunddreißig verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl von Gesetzesartikeln aus jedem dieser belgischen Gesetzbücher sammeln. Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage in den Datensätzen bezeichnet, und rund achtzig Prozent dieser zitierten Artikel stammen entweder aus dem Zivilgesetzbuch, dem Gerichtsgesetzbuch, dem Strafprozessgesetzbuch oder den Strafgesetzbüchern. In der Zwischenzeit haben achtzehn von 32 Gesetzbüchern weniger als fünf Artikel, die als relevant für mindestens eine Frage genannt werden, was darauf zurückzuführen ist, dass sich diese Gesetzbücher weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt beträgt die durchschnittliche Anzahl der Zitate für diese zitierten Artikel zwei, und weniger als fünfundzwanzig Prozent von ihnen werden mehr als fünfmal zitiert. Mit unseren Datensätzen vergleichen wir mehrere Abrufmuster, einschließlich lexikalischer und dichter Architekturen. Bei einer Abfrage in einem Artikel weist ein lexikalisches Modell dem Abfrage-Artikel-Paar eine Punktzahl zu, indem es die Summe über die Abfrageterms der Gewichte jedes dieser Terme in dem Artikel berechnet. Wir experimentieren mit den Standard-TFIDF- und BM 25-Ranking-Funktionen. Das Hauptproblem bei diesen Ansätzen ist, dass sie nur Artikel abrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer neuronale Architektur, die die semantische Beziehung zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet. und berechnen eine relevante Punktzahl zwischen einem Abfrage-Artikel-Paar durch die Ähnlichkeit ihrer Einbettungen. Diese Einbettungen resultieren typischerweise aus einer Pooling-Operation auf dem Ausgabe eines Wort-Einbettungs-Modells. Zuerst untersuchen wir die Wirksamkeit von Siamese B-Encoders in einem Zero-Shot-Evaluation-Setup, was bedeutet, dass vorab trainierte Wort-Einbettungs-Modelle sofort ohne zusätzliche Feinabstimmung angewendet werden. Wir experimentieren mit kontextunabhängigen Text-Encoders, nämlich Word to Vec und FastEx, und kontextabhängigen Einbettungsmodellen, nämlich Roberta und speziell Camembert, was ein französisches Roberta-Modell ist. Zusätzlich trainieren wir unser eigenes Camembert-basiertes Modell Biancoders auf allen Datensätzen. Beachten Sie, dass wir für das Training mit den beiden Varianten der Biancoder-Architektur experimentieren. Siamese, das ein einzigartiges Wort-Einbettungs-Modell verwendet, das die Abfrage und den Artikel gemeinsam in einem geteilten dichten Vektorraum abbildet. Und zwei Türme, die zwei unabhängige Wort-Einbettungs-Modelle verwenden, die die Abfrage und den Artikel separat in verschiedene Einbettungsräume kodieren. Wir experimentieren mit Mittel-, Max- und CLS-Pooling sowie Punktprodukt und Kosinus für die Berechnung von Ähnlichkeiten. Hier sind die Ergebnisse unserer Basislinie auf den Testdatensätzen mit den oben genannten lexikalischen Methoden, den in einem Zero-Shot-Setup bewerteten Siamese B-Encoders in der Mitte und den unten fein abgestimmten B-Encoders. Insgesamt übertreffen die fein abgestimmten B-Encoders alle anderen Basislinien erheblich. Das Zwei-Türme-Modell verbessert sich gegenüber seiner Siamese-Variante bei der Erinnerung bei einhundert, aber die Leistung bei den anderen Metriken ist ähnlich. Obwohl BM-vierundzwanzig sich deutlich unter dem trainierten Biancoder platzierte, deutet seine Leistung darauf hin, dass es immer noch eine starke Basislinie für domänenspezifische Abrufe ist. In Bezug auf die Zero-Shot-Evaluation von Siamese Biancoder stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorab trainierten Kamembert-Modells ohne Optimierung für die Informationsbeschaffungsaufgabe schlechte Ergebnisse liefert, was mit früheren Erkenntnissen übereinstimmt. Und der auf Word-to-Vec basierende Biancoder übertraf den Fast-Text- und Bird-basierten Modell erheblich, was darauf hindeutet, dass möglicherweise vorab trainierte Einbettungen auf Wortniveau für die Aufgabe geeigneter sind als Einbettungen auf Zeichenebene oder Subwort-Ebene, wenn sie sofort verwendet werden. Obwohl vielversprechend, deuten diese Ergebnisse auf reichlich Raum für Verbesserungen im Vergleich zu einem erfahrenen kleinen Experten hin, der schließlich alle relevanten Artikel zu jeder Frage abrufen und somit perfekte Punktzahlen erzielen kann. Lassen Sie uns abschließend zwei Einschränkungen aller Datensätze diskutieren. Erstens ist der Korpus der Artikel auf die aus den dreiunddreißig berücksichtigten belgischen Gesetzbüchern beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Dekreten, Richtlinien und Verordnungen fehlen. Während des Datensatzaufbaus werden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen mit nur einem Bruchteil der anfänglichen Anzahl relevanter Artikel enden. Dieser Informationsverlust impliziert, dass die in den verbleibenden relevanten Artikeln enthaltene Antwort unvollständig sein könnte, obwohl sie immer noch völlig angemessen ist. Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Statuten beantwortet werden können. Zum Beispiel kann die Frage, ob man seine Mieter des Hauses werfen kann, wenn sie zu viel Lärm machen, möglicherweise keine detaillierte Antwort innerhalb des Gesetzesrechts haben, die einen spezifischen Lärmschwellenwert quantifiziert, bei dem eine Räumung erlaubt ist. Stattdessen sollte der Vermieter wahrscheinlich mehr auf Rechtsprechung vertrauen und Präzedenzfälle finden, die ähnlich zu seiner aktuellen Situation sind. Zum Beispiel mietet der Mieter zwei Parteien pro Woche bis 2 Uhr morgens. Daher eignen sich einige Fragen besser als andere für die Abfrage von Gesetzesartikeln, und der Bereich der weniger geeigneten bleibt noch zu ermitteln. Wir hoffen, dass all diese Arbeit Interesse an der Entwicklung praktischer und zuverlässiger Abrufmodelle für Gesetzesartikel weckt, die dazu beitragen können, den Zugang zur Justiz für alle zu verbessern. Sie können unseren Artikel DatSet und Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, unsere Arbeit an VALS vorstellen zu können, einem aufgabenunabhängigen Benchmark, der zum Testen von Vision- und Sprachmodellen mit spezifischen sprachlichen Phänomenen entwickelt wurde. Warum haben wir uns die Mühe gemacht, diesen Benchmark einzurichten? Nun, in den letzten Jahren haben wir eine Explosion von auf Transformer basierenden Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vortrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei Vision- und Sprachaufgaben wie der visuellen Fragebeantwortung, dem visuellen Common-Sense-Reasoning, der Bilderkennung und der Phrase-Verankerung. Wir haben also die Nachricht erhalten, dass die Genauigkeiten bei diesen aufgabenbezogenen Benchmarks stetig steigen, aber wissen wir, was die Modelle tatsächlich gelernt haben? Was ist es, das ein Vision- und Sprachtransformer verstanden hat, wenn er eine hohe Punktzahl für dieses Bild und diesen Satz zuweist, um zu passen, und eine niedrige Punktzahl für diesen einen? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige oder konzentrieren sie sich auf Verzerrungen, wie frühere Arbeiten gezeigt haben? Um diesen Aspekt genauer zu beleuchten, schlagen wir eine aufgabenunabhängige Richtung vor und führen Ventile ein, die die Empfindlichkeit von Vision- und Sprachmodellen auf spezifische sprachliche Phänomene testen, die sowohl die sprachliche als auch die visuelle Modalität betreffen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entität-Referenzen ab. Aber wie testen wir, ob die Vision- und Sprachmodelle diese Phänomene erfasst haben? Foiling, eine Methode, die zuvor von Ravi Shakar und Mitarbeitern nur für Nomenphrasen und von uns in früheren Arbeiten für das Zählen bei Vision- und Sprachmodellen angewendet wurde. Foiling bedeutet im Grunde, dass wir die Bildunterschrift nehmen und eine Folie erstellen, indem wir die Bildunterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Und wir führen diese Phraseänderungen durch, indem wir uns auf sechs spezifische Bereiche konzentrieren: Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entität-Referenzen. Jeder Bereich kann aus einem oder mehreren Instrumenten bestehen, falls wir mehr als einen interessanten Weg gefunden haben, Folieninstanzen zu erstellen. Zum Beispiel haben wir im Fall des Handlungsbereichs zwei Instrumente, eines, bei dem das Handlungsverbum durch eine andere Handlung ersetzt wird, und eines, bei dem die Akteure ausgetauscht werden. Zählen und Referenz sind ebenfalls Bereiche, die mehr als ein Instrument haben. Und wir erstellen diese Folien, indem wir sicherstellen, dass sie das Bild nicht mehr beschreiben, dass sie grammatische und ansonsten gültige Sätze sind. Das ist nicht einfach, denn eine gefälschte Bildunterschrift ist möglicherweise weniger wahrscheinlich als die ursprüngliche Bildunterschrift. Zum Beispiel ist es, obwohl nicht unmöglich, statistisch weniger wahrscheinlich, dass Pflanzen einen Mann schneiden, als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten dies aufgreifen. Daher müssen wir Maßnahmen ergreifen, um gültige Folien zu erhalten. Erstens nutzen wir starke Sprachmodelle, um Folien vorzuschlagen. Zweitens verwenden wir natürliche Sprachinferenz oder kurz NLI, um Folien herauszufiltern, die das Bild immer noch beschreiben könnten, da wir beim Erstellen von Folien sicherstellen müssen, dass sie das Bild nicht mehr beschreiben. Um dies automatisch zu testen, wenden wir natürliche Sprachinferenz mit der folgenden Begründung an. Wir betrachten ein Bild als Prämisse und seine Bildunterschrift als implizite Hypothese. Zusätzlich betrachten wir die Bildunterschrift als Prämisse und die Folie als ihre Hypothese. Wenn ein NLI-Modell vorhersagt, dass die Folie der Bildunterschrift widerspricht oder neutral ist, nehmen wir dies als Indikator für eine gültige Folie. Wenn die Folie implizit durch die Bildunterschrift impliziert wird, kann sie keine gute Folie sein, da sie durch Transitivität eine wahrheitsgetreue Beschreibung des Bildes liefern würde, und wir filtern diese Folien heraus. Aber dieses Verfahren ist nicht perfekt. Es ist nur ein Indikator für gültige Folien, daher als dritte Maßnahme zur Generierung gültiger Folien beschäftigen wir menschliche Annotatoren, um die in VALS verwendeten Daten zu validieren. Also, nach der Filterung und menschlichen Bewertung haben wir so viele Testinstanzen wie in dieser Tabelle beschrieben. Beachten Sie, dass VALS keine Trainingsdaten liefert, sondern nur Testdaten, da es ein Zero-Shot-Test-Benchmark ist. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vortraining zu nutzen. Feinabstimmung würde den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Und wie wir sagten, sind wir daran interessiert, zu bewerten, welche Fähigkeiten die Vision- und Sprachmodelle nach dem Vortraining haben. Wir experimentieren mit fünf Vision- und Sprachmodellen an Ventile, nämlich mit Clip, Wilbert, Wilbert Kelvin one und Visual Bert. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Text-Paaren in Bildunterschriften und Folien. Vielleicht relevanter für dieses Video, werden wir unsere permissivere Metrik, die paarweise Genauigkeit, vorstellen, die misst, ob der Bild-Text-Ausrichtungswert für das korrekte Bild-Text-Paar größer ist als für sein gefälschtes Paar. Für weitere Metriken und Ergebnisse dazu, schauen Sie sich unser Papier an. Die Ergebnisse mit paarweiser Genauigkeit sind hier gezeigt und sie sind konsistent mit den Ergebnissen, die wir von den anderen Metriken erhalten haben, dass die beste Zero-Shot-Leistung von Wilbert zwölf eins erzielt wird, gefolgt von Wilbert, Alex Mert, Clip und schließlich Visual Bird. Es ist bemerkenswert, wie auf einzelne Objekte ausgerichtete Instrumente wie Existenz und Nomenphrasen von Wilbert zwölf eins fast gelöst werden, was darauf hinweist, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Allerdings können keine der verbleibenden Bereiche in unseren adversariellen Foiling-Einstellungen zuverlässig gelöst werden. Wir sehen das von den Pluralitäts- und Zählinstrumenten. dass Vision- und Sprachmodelle Schwierigkeiten haben, Verweise auf einzelne versus mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Der Teilbereich zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, selbst wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir im Handlungsbereich sehen. Aus dem Referenzbereich erfahren wir, dass das Verfolgen mehrerer Verweise auf dasselbe Objekt in einem Bild durch die Verwendung von Pronomen auch für Vision- und Sprachmodelle schwierig ist. Als Sanity-Check und weil es ein interessantes Experiment ist, bennen wir auch zwei Text-only-Modelle, GPT one und GPT two, um zu bewerten, ob VALS von diesen unimodalen Modellen lösbar ist, indem wir die Perplexität der korrekten und der gefälschten Bildunterschrift berechnen und den Eintrag mit der niedrigsten Perplexität vorhersagen. Wenn die Perplexität für die Folie höher ist, nehmen wir dies als Indikator dafür, dass die gefälschte Bildunterschrift unter Plausibilitätsverzerrung oder anderen sprachlichen Verzerrungen leiden könnte. Und es ist interessant zu sehen, dass in einigen Fällen die Text-only-GPT-Modelle die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Um zusammenzufassen, VALS ist ein Benchmark, der die Linse sprachlicher Konstrukte nutzt, um der Gemeinschaft zu helfen, Vision- und Sprachmodelle zu verbessern, indem er ihre visuellen Verankerungsmöglichkeiten hart testet. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte in ihrer Anwesenheit in Bildern gut identifizieren, wie der Existenzbereich zeigt, aber Schwierigkeiten haben, ihre Interdependenz und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen sind, sprachliche Indikatoren zu respektieren. Wir würden die Gemeinschaft wirklich ermutigen, VALS zu nutzen, um Fortschritte bei der sprachlichen Verankerung mit Vision- und Sprachmodellen zu messen. Und noch mehr, VALS könnte als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich bei einem der von VALS getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, schauen Sie sich die VALS-Daten auf GitHub an, und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisara von der Universität Tokio. Ich werde einen Vortrag mit dem Titel „R und SAM, ein umfangreicher Datensatz für automatische Risikobewertung statt Dauer via Commit-Log-Zusammenfassung“ halten. Ich werde dies in der folgenden Reihenfolge erklären. Zunächst werde ich die automatische Risikobewertung statt Dauer vorstellen, an der wir in dieser Forschung arbeiten. Versionshinweise sind technische Dokumente, die die mit jeder Softwareversion bereitgestellten Änderungen zusammenfassen. Das Bild zeigt die Versionshinweise für Version 2.6.4 der GBUJS-Bibliothek. Diese Knoten spielen eine wichtige Rolle in der Open-Source-Entwicklung, sind jedoch zeitaufwendig, wenn sie manuell erstellt werden. Daher wäre es sehr nützlich, hochwertige Versionshinweise automatisch erstellen zu können. Ich werde auf zwei frühere Forschungen zur automatischen Erstellung von Versionshinweisen verweisen. Die erste ist ein System namens Arena, das 2014 veröffentlicht wurde. Es verwendet einen regelbasierten Ansatz, z. B. den Änderungs-Extractor, um Kernänderungen in Bibliotheken und Dokumenten aus den Unterschieden zwischen Versionen zu extrahieren und sie schließlich zu kombinieren. Das bemerkenswerteste Merkmal dieses Systems ist der Issue-Extractor in der oberen rechten Ecke, der mit Jira, dem Issue-Tocosystem, verknüpft sein muss und nur für Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann für viele Projekte auf GitHub nicht verwendet werden. Die zweite ist Griff, das kürzlich 2020 angekündigt wurde. Es ist im Internet verfügbar und kann über PIP gespeichert werden. Dieses System verfügt über ein einfaches, lernbasiertes Textklassifizierungsmodell und gibt für jede Eingabeaussage eines Commit-Nachrichten einen von fünf Rubbeln wie Funktionen oder Fehlerbehebungen aus. Das Bild ist ein Beispiel für die Verwendung, das einen korrektiven oder Fehlerbehebungs-Rubbel zurückgibt. Das Trainingsdatenset von Queryface ist ziemlich klein, etwa 5.000, und wird in den unten beschriebenen Experimenten gezeigt. Die Leistung des Textklassifizierungsmodells ist nicht hoch. Ich präsentiere zwei verwandte Forschungen, aber es gibt Probleme mit begrenzter Anwendbarkeit und knappen Datenressourcen. Unser Papier hat diese beiden Probleme gelöst und erstellt automatisch hochwertige Versionshinweise. Für das Problem der begrenzten Anwendbarkeit schlagen wir eine Methode zur hochwertigen Klassifizierungssammlung vor, die nur die Commit-Nachricht als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir einen RNSAM-Datensatz erstellt, der aus etwa 82.000 Datensätzen besteht, indem wir Daten aus öffentlichen GitHub-Repositories über die GitHub-API gesammelt haben. Als Nächstes beschreibe ich unseren Datensatz. Hier ist ein Beispiel für Daten. Die linke Seite ist eine Commit-Nachricht und die rechte Seite sind die Versionshinweise. Die Versionshinweise sind in Ebenen wie Verbesserungen, Fehlerbehebungen usw. unterteilt. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die unterteilten Versionshinweise als Ausgabe liefert. Dies kann als Zusammenfassung betrachtet werden. Wir haben vier Ebenen vordefiniert: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecationen, Entfernungen und Bruchänderungen. Diese wurden basierend auf früheren Forschungen und anderen Faktoren festgelegt. Die Versionshinweise unten rechts werden aus den Versionshinweisen extrahiert, die unten links gezeigt werden. Zu diesem Zeitpunkt ist es notwendig, die vier Rubbeln zu erkennen, die im Voraus festgelegt wurden, aber die Rubbeln sind nicht immer konsistent mit jedem Repository. Zum Beispiel umfasst der Rubbel „Verbesserungen“ Verbesserungen, Erweiterungen, Optimierungen usw. Wir haben eine Vokabularliste für jeden dieser Notationsschwankungen erstellt. Verwenden Sie sie, um die RISNOD-Klasse zu erkennen und den Text des RIS, der folgt, als RISNOD-Satz für die Klasse zu korrigieren. Als Nächstes ist eine Commit-Nachricht. Commit-Nachrichten sind nicht an jedem RIS gebunden. Wie im Bild unten gezeigt, wenn der aktuelle RIS Persisch 2.5 bis 19 ist, müssen wir den vorherigen RISP 2.5 bis 18 identifizieren und den Diff abrufen. Dies ist etwas langweilig und es reicht nicht aus, einfach eine Liste von RIS zu erhalten und das Vorherige und Nachher zu betrachten. Wir haben ein heuristisches Matching-Blau erstellt, um das vorherige und das nächste Pageant zu erhalten. Durch die Analyse von Desset wurden schließlich 7.200 Repositories und 82.000 Datensätze korrigiert. Außerdem beträgt die durchschnittliche Anzahl der Versionshinweis-Token 63, was für eine Zusammenfassung recht hoch ist. Auch die Anzahl der eindeutigen Token ist ziemlich groß mit 8.830.000. Dies liegt an der großen Anzahl einzigartiger Kosten und Methodenamen, die im Repository gefunden wurden. Als Nächstes werde ich die vorgeschlagene Methode erklären. Das querweise extraktive und abstraktive Zusammenfassungmodell besteht aus zwei neueren Modulen: einem Klassifikator, der einen Bot oder Code-Bot verwendet, und einem Generator, der einen Bot verwendet. Zuerst verwendet GEAS einen Klassifikator, um jede Commit-Nachricht in fünf Grundklassen zu klassifizieren: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecationen, Deprecationen und andere. Die als „andere“ klassifizierten Commit-Nachrichten werden verworfen. Dann wendet GEAS einen Generator unabhängig auf die vier Rubbeldokumente an und generiert Grundhinweise für jede Klasse. In dieser Aufgabe sind die direkten Entsprechungen zwischen Commit-Nachrichten und Grundhinweisen nicht bekannt. Daher, um den Klassendraht zu trainieren, weisen wir jedem Eingabeaussage eines Commits zwei Rubbeln zu, indem wir die ersten zehn Zeichen jeder Commit-Nachricht verwenden. Wir modellieren den abstraktiven Zusammenfassungansatz des Klassendrahtes durch zwei verschiedene Methoden. Das erste Modell, das wir GAS single nennen, besteht aus einem einzigen Sekt-zu-Sekt-Netzwerk und generiert einen einzigen langen Listentext, der eine Verkettung der Eingabeaussagen der Commits darstellt. Der Ausgabetext kann in Klassen nach Segmenten unterteilt werden, basierend auf speziellen klassenbezogenen Endpunktsymbolen. Die zweite Methode, die wir GSMAUC nennen, besteht aus vier verschiedenen Sekt-zu-Sekt-Netzwerken, von denen jedes einer der Listentextklassen entspricht. Okay, lassen Sie mich das Experiment erklären. Fünf Methoden wurden verglichen: GS, GAS single, GAS march, rustling und die vorherige Studie grief. In Bezug auf Abtreibung werden in einigen Fällen diese Hinweise in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze bei Null zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Das System wird bestraft, wenn es einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren Brew-Wert in den unten beschriebenen Versuchsergebnissen. Schließlich berechnen wir auch die Spezifität, da Rouge und Brew nicht berechnet werden können, wenn die Versionshinweise leer sind. Eine hohe Spezifität bedeutet, dass das Modell korrekt leere Texte ausgibt, in Fällen, in denen die Versionshinweise leer sind. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch den grünen Datensatz, der sie ausschließt, ausgerottet. GAS und GAS erreichten Rouge-Fehlerquoten, die mehr als zehn Punkte höher als die Basislinie waren. Der Punktunterschied zwischen der vorgeschlagenen Methode und der Basislinie sprang jedoch auf mehr als zwanzig Punkte an. Diese Ergebnisse deuten darauf hin, dass GAS und GAS signifikant wirksam sind. GAS erhielt einen besseren losen Wert als GAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators bei der Klassifizierung des Klassifikators mit Pseudobus wirksam ist. Die hohe Abdeckung von GAS kann wahrscheinlich erreicht werden, weil der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann. Sie neigt dazu, höhere Regeln zu liefern als sie single, was darauf hindeutet, dass es auch wirksam ist, verschiedene Zusammenfassungmodelle unabhängig für jede dieser Notklassen zu entwickeln. Hier ist eine Fehleranalyse. Ihre Methoden neigen dazu, kürzere Sätze als menschliche Referenzsätze auszugeben. In der Abbildung rechts hat der Referenzsatz drei oder vier Sätze, während sie nur einen hat. Der Grund für die Zurückhaltung dieses Modells ist, dass im Trainingsdatensatz nur 33 Prozent der Sätze in der Rubbelklasse „Funktionen“ und 40 Prozent in der Rubbelklasse „Verbesserungen“ vorhanden sind. Darüber hinaus können GS-Methoden keine genauen Grundhinweise ohne zusätzliche Informationen generieren. Das obige Beispiel rechts ist ein Beispiel für eine sehr chaotische Commit-Nachricht und der vollständige Satz kann nicht ohne Differenz zum entsprechenden parallelen Antrag oder Problem generiert werden. Das folgende Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander verbunden sind und in einen Satz kombiniert werden sollten, aber es gelingt nicht. Abschließend eine Schlussfolgerung. Wir haben einen neuen Datensatz für die automatische Erstellung von Grundhinweisen erstellt. Wir haben auch die Aufgabe erstellt, Commit-Nachrichten einzugeben und sie so zu zusammenzufassen, dass sie für alle Projekte anwendbar ist, die in englischer Sprache geschrieben sind. Unsere Experimente zeigen, dass die vorgeschlagene Methode weniger laute Grundhinweise bei höherer Abdeckung als die Basislinie generiert. Bitte sehen Sie sich unseren Datensatz auf GitHub an. Vielen Dank."}
