{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫阿萨·法拉里，我将介绍我们的论文《FUESHOT TABLAR DATA ARCHICHTMENT 使用 FINE TUNIC TRANSFORMERS ARCHITTURES》。数据科学家分析数据，主要关注于操作现有数据特征，但有时这些特征有限。使用另一个数据源生成特征可能会添加大量信息。我们的研究目标是使用外部来源的自由文本自动丰富表格数据。假设我们有一个表格数据集和一个知识库。我们需要一个自动过程，涉及实体链接和文本分析，以从知识库的自由文本中提取新特征。我们的框架FAST正是这个自动过程。让我们看一个例子。将数据集输入到FAST中。在这个例子中，数据集是大学数据集，其目标是将大学分为低排名大学和高排名大学。作为知识库，我们使用维基百科。FEST的第一阶段是实体链接，当每个实体，在这个例子中，大学名称链接到知识库中的一个实体时，知识库实体的文本被提取并添加到数据集。在这个例子中，文本是维基百科页面的摘要。现在我们需要从检索文本中生成或提取特征。所以我们需要一个特征提取阶段，包括文本分析，这是这篇论文的主要创新之处，我将在接下来的幻灯片中深入探讨。在特征提取阶段之后，有一个特征生成阶段，当我们使用提取的特征生成少量新特征。首先，生成原始数据集类别的数量的特征。在这个例子中，原始数据集有两个类别，所以首先生成两个新特征。但是如果数据集有五个类别，首先生成五个新特征。每个特征代表每个类别的可能性。为了分析文本，我们使用当前最先进的文本分析技术，即基于变换器的语言模型，如BERT、GPT、XLERT等。但我们不太可能使用输入数据集训练语言模型。所以一个简单的做法是目标任务微调。所以在特征提取阶段，我们可以下载预训练的语言模型，在这个例子中，对目标数据集进行语言模型微调，以微调语言模型，将文本分类为类别，摘要为类别，低或高，接收语言模型输出，即每个类别的可能性，并用作新特征。这种方法的问题在于数据集可能包含很少的独特实体和文本。在我们的实验中，几乎有一半的数据集包含少于400个样本，最小的数据集在其训练集中包含35个样本。所以，对这个数据集进行语言模型微调将无效。但我们可以使用关于预分析数据集的先验知识，因为我们对多个数据集应用了FAST。我们可以使用n减一的数据集来收集关于n减一数据集的信息，并在分析第n个数据集时使用这些信息。我们建议添加另一个微调阶段，一个初步的多任务微调阶段，当我们在n-1个数据集上对语言模型进行微调时，然后我们执行另一个微调阶段，即目标任务微调，当我们在第n个目标数据集上对语言模型进行微调。多任务微调的最新技术称为空DNN，空DNN在训练集中保持任务数量的头，所以在这个例子中，如果训练集中有四个任务，空DNN保持四个头，如图所示，它从训练集中随机抽取一个批次，如果随机批次属于例如，辛格和塞尔顿的分类任务，它执行前向和反向路径通过第一个头。如果随机批次属于配对排序任务，它执行前向和反向路径通过最后一个头。在我们的场景中，表格或数据集的类别数量各不相同。所以有许多任务。MTDNN保持类别数量的头输出层，此外，MTDNN需要为新数据集和新任务初始化新头。我们提出的方法称为任务重述微调，在我们的方法中，任务重述微调，而不是保持多个头，我们将每个数据集重述为一个句子分类问题，这是一个两个类别的任务。让我们看一个例子。这是我们的输入数据集，包含实体、特征、文本和类别。我们将任务从将文本分类为低和高重述为将文本、摘要和类别分类为真或假。或者换句话说，我们训练语言模型来分类摘要和类别，如果摘要属于类别，则将摘要和类别分类为摘要和类别，在这种情况下，标签向量始终包含两个类别，这是我们查找或重述微调方法的算法。所以让我们看看完整的框架，将数据集输入到FAST中，然后FAST执行链接阶段，它从知识库中提取文本，在这个例子中是维基百科页面的摘要，然后将任务重述为句子分类任务，将语言模型应用于新任务并输出每个类别的可能性。请注意，语言模型已经使用初步多任务微调对n-1个数据集进行了微调。然后我们使用语言模型的输出向量作为新生成的特征，数量为类别数量。为了评估我们的框架，我们使用了一个17个表格的分类数据集，其大小、特征、平衡性、领域和初始性能各不相同。作为知识库，我们使用维基百科。我们设计我们的实验为留一评估，当我们在16个数据集上训练FAST并将其应用于第17个数据集时。我们还将每个数据集分为四个子集，应用子集交叉验证。然后我们生成新特征并使用五个评估分类器对其进行评估。我们使用基于BERT的架构进行实验。以下是我们的实验结果。您可以看到，我们将我们的框架与目标数据集微调、目标任务微调进行比较。以及MTDNN初步微调和我们的重述微调，我们的重述微调取得了最佳结果，最佳性能，而MTDNN在目标数据集微调上取得了2%的改进。我们的方法在小数据集上取得了6%的改进，我们可以看到，MTDNN的性能下降，初步多任务微调阶段的改进降至1.5%，但我们的性能相比仅目标任务微调提高到了11%。总结，FAST使我们在实验中从35个样本中实现了视图增强。它使用一个架构来处理所有任务数据集，并保持模型的头。但它添加了三个重述阶段，它扩大了训练集，并且需要具有语义意义的目标值，以便我们可以将其输入到语言模型中并在句子分类问题中使用。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我将介绍我们的研究工作《学习演绎推理：地铁问题作为复杂区域提取》。我是来自拜登人工智能实验室的艾伦，这是与德克萨斯大学奥斯汀分校的蒂埃里和SUDD的韦洛合作完成的联合工作。首先，我想谈谈我们对推理的动机。所以，这里我们展示了一个多步推理有用的例子。所以这张图取自POWN论文，他们在融合学习场景中通过提示解决地铁问题。所以，在净笔侧，我们可以看到，如果我们给出一些只有问题和答案的例子，我们可能无法获得正确的答案。但是，如果我们给出一些更多的推理描述，模型能够预测推理描述，并在这里做出正确的预测。所以，具有可解释的多步推理作为输出是好的。我们还认为，方法问题是一个直接的应用，用于评估这种推理能力。所以，在我们的问题设置中，给定问题，我们需要解决这个问题并获得数值答案。所以，在我们的数据集里，我们也被给出数学表达式，这同样导致这个特定的答案。所以，某些假设也适用于之前的研究。我们假设量的精度是已知的，我们只考虑基本运算符，如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以解码为这些基本运算符。所以，之前的解决方法问题的工作实际上可以分为序列到序列和序列到树模型。所以，传统的序列到序列模型将表达式转换为特定的序列进行生成，这很容易实现，并且可以推广到许多不同的复杂问题。但是，缺点是性能实际上通常不比结构模型好，并且缺乏预测的可解释性。但是，实际上，这个方向仍然相当受欢迎，因为有Transformer模型。所以，在基于树的模型中，我们实际上将这些表达式结构化为树形式，并在树生成中遵循先序遍历。所以，这里我们不断生成运算符，直到我们到达叶子，它们是量。所以，这里的好处是它实际上给我们这个二叉树结构。但是，实际上这相当违反直觉。因为我们首先生成运算符，然后在最后生成量。第二点是它也包含一些重复计算。所以，这里，如果我们看这个表达式，a乘以三加三，它实际上被生成了两次。但实际上，我们应该重用结果。所以，在我们的提议方法中，我们希望以逐步和可解释的方式解决这些问题。所以，例如，在第二步，我们可以获得这个除数，它是27。我们也可以回溯到原始问题，找到相关内容。在这些步骤中，我们获得除数。所以，然后在这个第三步，我们实际上得到了商，对吧？在这些三个步骤之后，我们实际上可以重用第二步的结果，然后得到第四步的结果。然后最终，我们可以获得被除数。所以，这里我们实际上直接生成整个表达式，而不是生成单个运算符或量。所以，这使得过程更准确。所以，在我们的演绎系统中，我们首先从问题中呈现的一系列量开始，也包括一些常数作为我们的初始状态。所以表达式由EIJOP表示，其中我们执行从Qi到QJ的运算符，这样的表达式实际上是有方向的。所以，我们在这里也颠倒了减法，以表示相反的方向。这与关系提取非常相似。所以，在一个正式的演绎系统中，在时间步t，我们应用运算符在Qi和Qj对之间，然后我们获得这个新的表达式。我们将它添加到下一个状态，成为一个新的量。所以，这个幻灯片实际上可视化了状态的演化，我们不断将表达式添加到当前状态。在我们的模型实现中，我们首先使用预训练的网络模型，可以是鸟类或机器人，然后我们编码一个句子，然后我们获得这些量表示。一旦我们得到量表示，我们就可以开始推理。这里我们展示了一个Q one获得Q one表示的例子，它们将被Q two除以，然后乘以Q three。首先，我们得到对表示，这基本上只是Q one和Q two之间的连接，然后我们应用一个前馈网络，由运算符参数化。然后最终，我们获得表达式表示Q1除以Q2。但在实践中，在推理阶段，我们可能也能够得到错误的表达式。所以，这里，所有可能的表达式等于运算符数量的三倍。所以，这里的好处是我们可以轻松地添加约束来控制这个搜索空间。例如，如果这个表达式不允许，我们可以简单地从我们的搜索空间中移除这个表达式。所以，在第二步，我们做同样的事情，但唯一的区别是多一个量。所以，这个量来自之前计算的表达式。所以，最终，我们可以获得这个最终表达式Q three乘以Q four。我们也可以看到所有可能表达式的数量与前一步不同。所以，这种差异使得应用光束搜索变得困难，因为这两个步骤之间的概率分布是不平衡的。所以，训练过程与训练序列到序列模型相似，我们在每个时间步优化损失，这里我们还使用这个tau来表示我们应该终止这个生成过程。所以，这个空间与序列到序列不同，因为在每个时间步空间是不同的，而在传统的序列到序列模型中，它是词汇表的数量。它还允许我们根据先验知识施加某些约束。所以，我们在常用的方法问题数据集MAWPS、Math 23K、MathQA和SWAM上进行实验。这里我们简要展示了与之前最佳方法相比的结果。所以，我们表现最好的变体是Roberta演绎推理器。事实上，我们没有使用光束搜索，而之前的研究使用光束搜索。所以，最佳方法通常是基于树的模型。所以，总的来说，我们的推理器能够显著超越这个基于树的模型，但我们可以看到MathQA或SWAMP的绝对数字并不高。所以，我们进一步研究了SWAMP的结果。这个数据集具有挑战性，因为作者试图手动添加一些东西来混淆NLP模型，如添加无关信息和额外量。所以，在我们的预测中，我们发现一些中间值实际上是负的。例如，在这个问题中，我们问杰克有多少苹果，但我们有一些额外的信息，如十七个更少的投篮和斯蒂芬有八个投篮，这完全是无关的。所以，我们的模型做出了一些预测，产生负值。我们观察到这两个表达式实际上有相似性。所以，我们可以实际限制这个搜索空间，删除那些结果是负的，以便我们可以使答案正确。所以，我们进一步发现这种约束实际上对一些模型有很大的改进。例如，对于鸟类，我们提高了七分，然后对于Roberta基模型，我们实际上提高了两分。所以，更好的语言模型具有更好的语言理解能力，所以这里的数字对于Roberta更高，对于鸟类更低。我们还尝试分析所有这些数据集背后的难度。我们假设未使用的量可以被视为无关信息。所以，这里我们可以看到，我们有样本中未使用的量的百分比，而SWAMP数据集有最大的部分。这里我们还展示了整体性能。对于那些没有未使用的量样本，整体性能实际上高于。而性能实际上高于整体性能。但是，对于那些有未使用的量样本，它实际上比整体性能差很多。对于MAWPS，我们没有太多的桌面案例，所以我就忽略了这部分。所以，最后，我们想通过一个崩溃和参与示例来展示可解释性。所以，这里，我们的模型实际上在第一步做出了错误的预测。所以，我们可以实际将这个表达式与这里的句子相关联。所以，我们认为这个句子可能会误导模型做出错误的预测。所以，这里种植另外三十五棵树让模型认为应该有额外的运算符。所以，我们尝试将句子修改为类似于梨树数量比苹果树少三十五棵。所以，我们让它传达更准确的语义，使得模型能够做出正确的预测。所以，这项研究表明，可解释的预测如何帮助我们理解模型行为。总结我们的工作，所以首先，我们的模型实际上相当高效，我们能够提供可解释的解决过程，并且我们可以轻松地将一些先验知识作为约束纳入，这有助于提高性能。最后一点是，其底层机制不仅适用于网络问题解决任务，还适用于涉及多步推理的其他任务。但是，我们也有某些局限性。如果我们有大量的运算符或常数，内存消耗可能会相当高。第二点是，如前所述，由于不同时间步的概率分布是不平衡的，所以应用光束搜索策略也相当具有挑战性。所以，这就是演讲的结束，欢迎提问。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫安托万，来自马斯特里赫特大学。我将与杰瑞共同展示我们的研究成果，该成果涉及一个用于检索法定条款的新数据集。法律问题是许多人生活中不可或缺的一部分，但大多数公民对自己的权利和基本法律程序知之甚少。因此，许多无法负担法律专家昂贵费用的弱势公民要么得不到保护，要么受到剥削。我们的目标是通过开发有效的检索系统来缩小人们与法律之间的差距，该系统可以为非专业人士提供免费的专业法律帮助服务。在深入探讨这项工作的核心贡献之前，让我们先描述一下法定条款检索的问题。假设有一个关于法律问题的简单问题，例如，如果我违反职业保密规定，我将面临什么风险？模型需要从大量立法中检索所有相关的法定条款。这项信息检索任务有其自身的挑战。首先，它涉及两种语言：问题中的自然语言和法规中的复杂法律语言。这种语言分布的差异使得系统更难检索相关候选人，因为它间接需要一个固有的解释系统，可以将自然问题翻译成与法规术语相匹配的法律问题。此外，法定法不是一叠可以独立使用的独立条款，就像新闻或食谱一样，它们本身就是一个完整的资料来源。相反，它是法律条款的结构化集合，只有在整体背景下考虑时才有完整意义，即与邻近条款的补充信息、它们所属的领域和子领域，以及它们在法律结构中的位置。最后，法定条款不是短段落，通常是大多数检索工作中的典型检索单位。在这里，它们是长达六千字的长文。近年来，自然语言处理的进步引发了人们对许多法律任务的极大兴趣，例如法律判决预测或合同自动化审查，但由于缺乏大量高质量的标签数据集，法定条款检索一直未得到充分的关注。在这项工作中，我们提出了一个以法国本土公民为中心的全新数据集，研究检索模型是否可以接近法律专家在法定条款检索任务中的效率和可靠性。我们的比利时法定条款检索数据集包含了一千一百多个比利时公民提出的法律问题。这些问题涵盖了从家庭、住房、金钱到工作和社会保障的广泛主题。每个问题都由经验丰富的法学家标记，引用了比利时法律典籍中超过二万二千六百个法定条款的相关条款。现在，让我们谈谈我们是如何收集这个数据集的。首先，我们开始汇编一个大型的法律条款语料库。我们考虑了三十二部公开可用的比利时法律典籍，并提取了所有条款以及相应的章节标题。然后，我们收集了引用相关法规的法律问题。为此，我们与一家比利时律师事务所合作，该事务所每年收到大约四千封来自比利时公民的电子邮件，询问个人法律问题。我们很幸运能够访问他们的网站，他们的经验丰富的法学家团队在这些网站上解决了比利时最常见的法律问题。我们收集了数千个问题，并标注了类别、子类别以及相关法规的法律引用。最后，我们通过相关法规进行筛选，排除那些引用不是我们考虑的法律典籍中的条款的问题。剩余的引用被匹配并转换为我们语料库中的相应条款ID。最终，我们得到了一千一百零八个问题，每个问题都仔细标注了我们大型语料库中相关条款的ID，该语料库包含了二万二千六百三十三个法定条款。此外，每个问题都附有其在法律结构中的子章节标题的连接。这些额外的信息在当前工作中未使用，但可能对未来的法律信息检索或法律文本分类研究感兴趣。让我们来看看我们数据集的一些特点。问题长度在五到四十四个词之间，中位数为四十个词。条款要长得多，中位数长度为七十七个词，其中一百四十二个超过一千个词。最长的条款长达五千七百九十个词。如前所述，问题涵盖了广泛的主题，其中大约八十五个百分比是关于家庭、住房、金钱或正义，而剩余的十五个百分比则涉及社会保障、外国人或工作。条款也非常多样化，因为它们来自三十二部不同的比利时法律典籍，这些法律典籍涵盖了从每个比利时法律典籍收集的大量非法条款。在二万二千六百三十三个条款中，只有一千六百一十二个被引用为与至少一个问题相关，其中大约八成的引用条款来自民法典、司法法典、刑事诉讼法典或刑法典。同时，三十二个法律典籍中有十八个的提及条款少于五个，这可以解释为这些法律典籍关注的个人及其问题较少。总体而言，这些引用条款的中位引用次数为二，其中不到二十五个百分比的引用次数超过五次。使用我们的数据集，我们对几种检索方法进行了基准测试，包括词汇和密集架构。给定一个查询条款对，词汇模型通过计算查询词的权重来为查询条款对分配一个分数。我们尝试了标准的TFIDF和BM 25排名函数。这些方法的主要问题是它们只能检索包含查询中关键词的条款。为了克服这一限制，我们尝试了一种基于神经网络的架构，可以捕捉查询和条款之间的语义关系。我们使用B编码器模型将查询和条款映射到密集向量表示。并通过嵌入的相似性计算查询条款对的相关分数。这些嵌入通常是词嵌入模型输出的聚合操作的结果。首先，我们研究了在零样本评估设置中Siamese B编码器的有效性，这意味着预训练的词嵌入模型直接使用，无需任何额外的微调。我们尝试了上下文无关的文本编码器，即Word to Vec和Fastex，以及上下文依赖的嵌入模型，即Roberta，特别是Camembert，这是一个法国Roberta模型。此外，我们还在所有数据集上训练了自己的基于Camembert的模型Biancoders。请注意，对于训练，我们尝试了Biancoder架构的两种变体。Siamese，它使用一个独特的词嵌入模型，将查询和条款一起映射到共享的密集向量空间。以及Two Tower，它使用两个独立的词嵌入模型，分别将查询和条款编码到不同的嵌入空间。我们尝试了均值、最大值和CLS池化，以及点积和余弦相似度来计算相似度。以下是我们的基线在测试集上的结果，上面是词汇方法，中间是零样本设置中评估的Siamese B编码器，下面是微调后的B编码器。总体而言，微调后的B编码器显著优于所有其他基线。Two Tower模型在一百的召回率上优于其Siamese变体，但在其他指标上表现相似。尽管BM二十五显著低于训练好的Biancoder，但其性能表明它仍然是一个适合特定领域的检索的强大基线。关于Siamese Biancoder的零样本评估，我们发现直接使用预训练的Kamembert模型的嵌入（未针对信息检索任务进行优化）会得到较差的结果，这与先前的发现一致。而基于Word to Vec的Biancoder显著优于基于FastText和Bird的模型，这表明在直接使用时，基于词级别的预训练嵌入可能比基于字符级别或子词级别的嵌入更适合该任务。尽管这些结果很有希望，但它们表明与熟练的小专家相比，仍有很大的改进空间，熟练的小专家最终可以检索到所有与任何问题相关的相关条款，从而获得满分。最后，让我们讨论一下所有数据集的两个局限性。首先，条款语料库仅限于从三十二部被考虑的比利时法律典籍中收集的条款，这并不涵盖整个比利时法律，因为法令、指令和政令的条款缺失。在数据集构建过程中，所有对这些未收集条款的引用都被忽略，这导致一些问题最终只得到初始相关条款数量的一小部分。这种信息损失意味着剩余的相关条款中的答案可能不完整，尽管仍然完全合适。其次，我们应该指出，并非所有法律问题都可以仅通过法规来回答。例如，关于“如果我的租户噪音太大，我可以驱逐他们吗？”的问题可能没有在法规中找到一个具体的噪音阈值，超过该阈值时允许驱逐的详细答案。相反，房东可能需要更多地依赖案例法，并找到与当前情况相似的先例。例如，租户每周举办两次派对，一直持续到凌晨两点。因此，有些问题比其他问题更适合法定条款检索任务，而那些不那么适合的问题的领域仍有待确定。我们希望所有工作都能激发人们开发实用且可靠的法定条款检索模型的兴趣，这些模型可以帮助改善所有人的司法救济。您可以通过以下链接查看我们的论文DatSet和Code。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我们很高兴向您介绍我们的 VALS 工作，这是一个任务独立基准，旨在测试视觉和语言模型对特定语言现象的处理能力。我们为什么要费力建立这个基准呢？在过去几年里，我们看到基于 Transformer 的视觉和语言模型在大量图像文本对上进行预训练的数量激增。这些模型中的每一个都在视觉和语言任务（如视觉问答、视觉常识推理、图像检索、短语定位）上推动了最先进水平。因此，我们得到的信息是，这些任务特定基准的准确率正在稳步提高，但我们是否知道模型实际上学到了什么？当模型为这张图像和这句话的高分匹配以及这张图像和这句话的低分匹配分配高分时，它理解了什么？视觉和语言模型是否关注正确的事情，还是它们关注到之前工作中显示的偏见？为了进一步阐明这一点，我们提出了一种更具任务独立性的方向，并引入了阀门，测试视觉和语言模型对影响语言和视觉模式的特定语言现象的敏感性。我们的目标是存在、复数、计数、空间关系、动作和实体指称。但是，我们如何测试视觉和语言模型是否捕捉到了这些现象？我们使用了一种称为“破坏”（foiling）的方法，这种方法之前仅由 Ravi Shakar 和合作者应用于视觉和语言模型，用于名词短语，并在我们之前的工作中用于计数。破坏基本上意味着我们取一张图像的标题，并通过修改标题使其不再描述图像来产生破坏项。我们通过关注六个特定部分（存在、复数、计数、空间关系、动作和实体指称）来进行这些短语修改。每个部分可以由一个或多个工具组成，以防我们发现一种以上有趣的方式来创建破坏实例。例如，在动作部分中，我们有两个工具：一个将动作动词替换为不同的动作，另一个则交换动作参与者。计数和指称也是具有一个以上工具的部分。我们通过确保它们无法描述图像、语法正确且在其他方面有效来创建这些破坏项。这并不容易，因为破坏后的标题可能比原始标题不太可能。例如，虽然不可能，但统计上植物割伤人比人割伤植物的可能性更低，大型视觉和语言模型可能会捕捉到这一点。因此，为了获得有效的破坏项，我们必须采取行动。首先，我们利用强大的语言模型来提出破坏项。其次，我们使用自然语言推理或短 NLI 来过滤掉可能仍然在描述图像的破坏项，因为在构建破坏项时，我们需要确保它们无法描述图像。为了自动测试这一点，我们应用了自然语言推理，理由如下。我们认为一张图像是前提，其标题是其蕴含的假设。此外，我们认为标题是前提，破坏项是其假设。如果 NLI 模型预测破坏项与标题相矛盾或相中性，我们将其视为有效破坏项的指标。如果破坏项与标题相矛盾，它不能被标题蕴含，因此它不是一个好的破坏项，我们将这些破坏项过滤掉。但是，这个过程并不完美。它只是一个有效破坏项的指标，因此，作为生成有效破坏项的第三种方法，我们雇佣了人类标注员来验证 VALS 中使用的数据。所以，经过过滤和人类评估后，我们得到了本表中描述的测试实例数量。请注意，VALS 没有提供任何训练数据，只提供测试数据，因为它是一个零样本测试基准。它的设计目的是利用视觉和语言模型在预训练后的现有能力。微调只会使模型能够利用数据中的伪影或统计偏见。我们都知道这些模型喜欢作弊和走捷径。正如我们所说，我们有兴趣评估视觉和语言模型在预训练后所具备的能力。我们在阀门上对五个视觉和语言模型进行了实验，即 CLIP、Wilbert、Wilbert Kelvin one 和 Visual Bert。我们最重要的两个评估指标是模型将图像句子对分类为标题和破坏项的准确率。也许对这个视频更相关的是，我们将展示我们的更宽容的指标，即配对准确率，它衡量的是图像句子对齐分数是否比其破坏对的正确图像文本对高。有关更多指标和结果，请查看我们的论文。配对准确率的结果在这里显示，它们与我们从其他指标获得的结果一致，即 Wilbert twelve in one 实现了最佳的零样本性能，其次是 Wilbert、Alex Mert、Clip，最后是 Visual Bird。值得注意的是，Wilbert twelve in one 在存在和名词短语等个人物体中心的问题上几乎得到了解决，这表明模型能够识别出图像中的命名物体及其存在。然而，在我们的对抗破坏设置中，其余的部分都无法可靠地解决。我们从复数和计数工具中看到了这一点。视觉和语言模型在区分单一物体与多个物体或在图像中计数它们方面存在困难。关系部分表明它们在正确分类图像中物体之间的命名空间关系方面存在困难。它们甚至在动作和识别其参与者方面也存在困难，即使在动作部分中我们看到支持可信度偏见的动作。从指称部分，我们发现使用代词在图像中追踪多个对同一物体的引用也对视觉和语言模型来说很困难。作为一种验证，并且因为这是一个有趣的实验，我们还对两个仅文本的模型 GPT one 和 GPT two 进行基准测试，以评估这些单模态模型是否可以通过计算正确和破坏的标题（这里没有图像）的困惑度来解决阀门，并预测困惑度最低的条目。如果破坏项的困惑度更高，我们将其视为破坏项可能受到可信度偏见或其他语言偏见的影响的迹象。有趣的是，在某些情况下，仅文本的 GPT 模型比视觉和语言模型更好地捕捉了世界的可信度。总结一下，VALS 是一种使用语言构建的视角来帮助社区通过严格测试视觉模型的视觉根基能力来改进视觉和语言模型的基准。我们的实验表明，视觉和语言模型在图像中识别命名物体及其存在方面表现良好，正如存在部分所示，但在视觉场景中，它们难以理解其相互依赖性和关系，尤其是在被迫尊重语言指示的情况下。我们真的希望鼓励社区使用 VALS 来衡量视觉和语言模型在语言根基方面的进展。更重要的是，VALS 可以用作数据集的间接评估，因为可以在训练或微调前后评估模型，以查看数据集是否帮助模型在 VALS 测试的任何方面有所改进。如果您感兴趣，请查看 GitHub 上的 VALS 数据，如果您有任何问题，请随时与我们联系。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是来自东京大学的Kamisara。我将介绍一篇题为“R和SAM：通过提交日志摘要实现自动风险而非持续性的大型数据集”的论文。我将按照以下顺序进行讲解。首先，我将介绍我们在这项研究中致力于解决的自动风险而非持续性问题。版本说明是总结软件产品每次发布时所包含变更的技术文档。图片显示了GBUJS库版本2.6.4的版本说明。这些节点在开源开发中起着重要作用，但手动准备非常耗时。因此，能够自动生成高质量的版本说明是非常有用的。我将参考两项关于自动生成版本说明的先前研究。第一项是Arena系统，发布于2014年。它采用基于规则的方法，例如使用变更提取器从版本之间的差异中提取核心库变更和文档变更，最后将它们结合起来。该系统的最显著特点是右上角的问题提取器，必须链接到Jira，问题生态系统，并且只能应用于使用Jira的项目。换句话说，它不能用于GitHub上的许多项目。第二项是Griff，最近在2020年宣布。它可以在互联网上获取，并可以通过PIP存储。该系统有一个简单的基于学习的文本分类模型，并为每个输入提交消息输出五种类型的摘要，如功能或错误修复。图片是一个示例用法，返回了纠正或错误修复摘要。Queryface训练数据相当小，约为5000条，将在下面描述的实验中展示。文本分类模型的性能不高。我介绍了两项相关研究，但存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成了高质量的版本说明。对于适用性有限的问题，我们提出了一种高质量的分类器摘要方法，仅使用提交消息作为输入。该方法可以应用于所有英文仓库。对于数据资源稀缺的第二个问题，我们通过使用GitHub API从公共GitHub仓库收集数据，构建了一个RNSAM数据集，包含约82,000条数据。接下来，我将描述我们的数据集。这是一个数据示例。左侧是提交消息，右侧是版本说明。版本说明被分为改进、错误修复等级别。我们设置了一个任务，将提交消息作为输入，输出分级的版本说明。这可以被视为一个摘要任务。我们预定义了四个级别的特征：改进、错误修复、弃用、可移除和破坏性变更。这些是基于先前研究和其他因素设定的。右下角的版本说明是从左下角显示的版本说明中提取的。此时，有必要检测预先设定的四个摘要，但摘要并不总是与每个仓库一致。例如，改进摘要包括改进、增强、优化等。我们为每个这些表示变体准备了一个摘要摘要词汇表。使用它来检测RISNOD类别，并纠正随后的RIS文本，作为该类别的RISNOD句子。接下来是一个提交消息。提交消息与每个RIS没有关联。如图所示，如果当前的RIS是波斯语2.5到19，我们需要识别之前的RISP2.5到18，并获取其差异。这有点繁琐，仅仅获取一个RIS列表并查看前后是不够的。我们创建了一个启发式匹配蓝色来获取前一个和下一个页面。最后，在分析中，7200个仓库和82,000条数据被纠正。此外，版本说明令牌的平均数量为63，对于一个摘要任务来说，这个数字相当高。此外，唯一令牌的数量相当大，为8830000。这是由于仓库中独特的成本和方法名称数量众多。接下来，我将解释提出的方法。交叉抽取和抽象摘要模型由两个较新的模块组成：一个使用bot或代码bot的分类器和一个使用bot的生成器。首先，GEAS使用分类器将每个提交消息分类为五个原因类别：功能、改进、错误修复、弃用和其他。被分类为其他类别的提交消息被丢弃。然后，GEAS独立地将生成器应用于四个摘要文档，并为每个类别生成原因说明。在这个任务中，提交消息与原因说明之间的直接对应关系是未知的。因此，为了训练类别线，我们使用每个提交消息的前十个字符为每个输入提交消息分配两个摘要。我们通过两种不同的方法对类别线的抽象摘要方法进行建模。第一种模型，我们称之为GAS single，由一个单一sect到sect网络组成，生成一个长文本摘要，给出了输入提交消息的连接。输出文本可以根据特殊类别特定端点符号按段进行分类。第二种方法，我们称之为GSMAUC，由四个不同的sect到sect网络组成，每个网络对应一个摘要类别。好的，让我解释实验。我们比较了五种方法：GS、GAS single、GAS march、rustling和先前的研究grief。关于中絶，在某些情况下，这些说明以多句形式输出。由于难以计算零句数，因此将它们与空格结合起来，视为一个长句子。当系统输出短句子时，会对brew进行惩罚。这种惩罚导致接下来描述的实验结果中brew值较低。最后，我们还计算了特异性，因为如果版本说明为空，则无法计算Rouge和Brew。高特异性意味着模型在版本说明为空的情况下正确输出为空文本。以下是结果。由于数据集包含电子邮件地址、哈希值等，我们还消除了绿色数据集，将其排除在外。GAS和GAS的Rouge错误分数比基线高出十多个点。然而，在绿色测试集上，提议的方法与基线之间的分数差距跃升至二十多个点。这些结果表明，GAS和GAS非常有效。GAS获得了比GAS更好的松散分数，表明结合分类器和生成器在使用伪总线训练分类器方面是有效的。GAS的高覆盖率可能是因为分类器可以专注于为每个类别选择相关的提交消息。与单一模型相比，它倾向于产生更高的规则，表明为每个摘要类别独立开发不同的摘要模型也是有效的。以下是错误分析。这些方法倾向于输出比人类参考句子的短句子。右图中，参考句子有三到四个句子，而这些方法只有一个。这种模型的犹豫的原因是，在训练数据中，只有33%的句子存在于功能摘要中，40%存在于改进摘要中。此外，GS方法在没有额外信息的情况下无法生成准确的原因说明。右上角的例子是一个非常混乱的提交消息示例，无法生成完整的句子，除非与相应的并行请求或问题进行比较。下面的例子表明，输入中的两个提交消息是相关的，应该合并成一个句子，但它未能做到。最后，结论。我们为自动原因生成构建了一个新的数据集。我们还形成了将提交消息输入并进行摘要的任务，使其适用于所有用英文编写的项目。我们的实验表明，提议的方法生成的噪声较少的摘要覆盖率高于基线。请查看我们在GitHub上的数据集。谢谢。"}
