{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫马蒂亚斯·伦德曼，今天我将向大家简要介绍我们的论文，该论文探讨了在没有树的情况下通过多集标记和潜在置换实现的组合泛化。这是我和我的导师亚历山大·科勒和伊万·蒂托夫共同完成的工作。组合泛化可以理解为学习者处理更深层次的递归和在训练过程中单独见过的短语未见组合的能力。在语义解析的背景下，测试组合泛化可能看起来像这样。像往常一样，我们有一个训练集，在这种情况下，是“女孩睡觉”和“玛丽知道女孩睡觉”。这些话语与表示其核心意义的逻辑形式配对。与标准机器学习评估不同，测试集不是来自同一分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练过程中看到了更浅层次的递归，并在具有更深递归的例子上进行了测试。朴素的序列到序列模型难以应对这种分布外泛化，并且通常会产生与输入脱节的输出。特别是，它们通常无法再现输入和输出之间的系统对应关系，例如在例子中用颜色标注的那些。解决这个问题的一种流行方法是将树集成到模型中。树的目的是捕捉将话语与逻辑形式联系起来的组合过程。这种方法效果很好，但树通常没有给出，需要以某种方式获得。这可能很复杂，有时是一个计算上昂贵的过程。通常，这涉及到逻辑形式的相当形式主义的预处理，例如处理变量符号。获得树也可能涉及到专门的语法感应程序。在这篇论文中，我们没有使用树，而是介绍了一种直接建模输入片段与输出片段之间对应关系的神经序列到序列模型。我们首次展示了在没有树的情况下对更深层次递归的强大泛化能力。我们的方法预测输出与输入的两个步骤。首先，我们用一个无序的多集标记每个输入标记，这些标记将出现在输出中。完成第一步后，我们有了所有正确的标记，但它们没有排序。这就是为什么在第二步，我们使用另一个模型来预测一个置换，将它们排列成正确的顺序。我们介绍了一种新的方法来预测一个置换，该方法对可能的置换没有硬约束。这使得我们的方法非常灵活和富有表现力。从概念上讲，我们的置换模型大致是这样工作的。我们从左到右遍历输出，并确定每个位置放置哪个多集标记。对于第一个输出位置，我们简单地选择一个，如图所示。然后，我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳到另一个多集标记。我们继续这个过程，直到第一个阶段的每个标记都被访问过一次。为了给你们一个实验结果的预告，我们在这里将我们的方法与其他无树模型在Koggs基准上进行了比较。我们的模型在对更深层次递归的泛化方面远远优于其他模型。尽管如此，其他一些结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技术挑战。首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多集，这给训练带来了挑战。此外，有时有多个置换与数据一致，但语言学上正确的那个是潜在的。我们通过在训练中诱导对齐来解决这个问题。我们的置换方法非常灵活，但它带来了一个挑战，即找到得分最高的置换是NP困难的。这是因为这与旅行商问题有关。我们用一种GPU友好的连续松弛方法来近似这一点，这也使我们能够通过解决方案进行反向传播并学习语言学上更可信的置换。如果你想了解更多关于我们的实验以及我们如何应对这些挑战，请查看我们的论文或来我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Myra，今天我将谈论我们的论文《标记化角色：使用自然语言提示来衡量语言模型中的刻板印象》。这项工作是与 Essendermouch 和 Dangerowski 合作完成的。近年来，许多研究已经记录了大型语言模型或 LLMs 中社会偏见和刻板印象的普遍存在。然而，这些措施有各种局限性。它们通常依赖于手工构建的数据集，这些数据集需要花费大量时间来整理。此外，它们通常只衡量非常特定的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景，或者它们只是捕捉到非常一般、广泛的关联，如对特定群体的负面关联。此外，这个领域的大多数工作都没有考虑到交叉性，即多方面社会身份可以加剧偏见，并成为伤害的独特场所。为了克服这些局限性，我们依赖于这些较新的指令调优 LLMs 在响应指令和提示方面非常擅长这一特性。因此，我们可以要求模型生成一个角色，这是一个使用提示（如，想象你是一个亚洲女性，描述自己）来描绘一个虚构个体的描述。我们可以立即看到，这种方法可以很好地推广到任何人口统计，因为我们可以在这个提示中指定任何我们想要的身份标记。以下是 GPT 4 生成的几个示例。我们立即看到，虽然输出不是传统意义上的过于消极或有毒，但有一些有趣的模式。亚洲女性被描绘成不引人注目，中东女性则被用词如异域和迷人的地区来描述，而两个有色人种角色都提到了祖先，而白人角色则没有任何这样的描述。为了捕捉这些模式，我们的方法有两部分。第一部分是生成这些角色。我们的提示是为了生成这些角色，灵感来自一项研究，他们给人类受试者这些提示，发现通过给人类受试者这些提示，他们也能揭示种族刻板印象。此外，这也使得我们可以直接比较我们生成的角色和人类书写的回应。第二部分是标记词，这是一种识别区分标记组和未标记组的词的方法，我将在稍后详细解释。这种方法的好处是我们可以得到非常具体的刻板印象和模式，而无需依赖任何特定的词汇。因此，标记词方法借鉴了社会语言学中的标记性概念，该概念指出存在一个未标记的默认值，任何与该默认值不同的群体在语言学上都是标记的。例如，词“人”或“抱歉”，词“战士”通常与男性相关联。因此，当人们描述一个女性战士时，他们通常会具体说明一个男性战士，并用“女性”来标记这个词。更广泛地说，社会中的主导群体在语言学和社会学上都是未标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先指定未标记和标记的群体是什么。然后，我们使用战斗词方法比较角色，这基本上是使用加权 logods 比率来区分每个标记组的顶级词。例如，对于黑人女性的角色，我们会进行战斗词，并将 logods 比率与白人角色和男性角色进行比较，因为这两个是对应的未标记群体。现在，让我们来看看一些结果。首先，我们使用刻板印象词汇表，发现生成的字符包含比人类书写的字符更多的刻板印象。然而，当我们实际查看词汇表中词的分布时，我们发现了一些非常不同的东西。因此，虽然生成的字符具有更高的词汇表词率，但人类书写的字符具有更广泛的词分布，而生成的字符中的刻板印象词实际上只是“高大”和“运动”这两个词。所以，实际上只有积极的或至少是非消极的词。事实上，这个词汇表根本没有很好地捕捉到我们之前幻灯片中看到的许多有害模式。因此，相反，为了做到这一点，我们将转向我们的标记词方法的结果，以展示这些看似积极的词如何促进刻板印象和本质化叙述。在我们的分析中，我们揭示了这些看似积极的描绘如何反映出有害的模式。首先，对于标记群体，顶级词包括文化、传统、自豪和异域等。这些词只通过它们与身份的关系来定义这些群体，并将其与白人规范区分开来。这为这些群体造成了长期的歧视和异化的遗产。此外，这些词反映了许多常见的陈词滥调，特别是对于有色人种女性。例如，描述拉丁美洲女性的词包括充满活力和曲线美，这些词与热带主义的陈词滥调相连。对于亚洲女性，这些词包括娇小、细腻和丝绸般，这些词与亚洲女性长期以来被过度性化、被视为非常温顺和顺从等长期的历史相连。最后，对于黑人女性，我们看到一些顶级词包括坚强和韧性。这与人们所谓的“坚强黑人女性”原型相连。虽然乍一看这听起来很积极，但有研究表明，这种原型实际上是非常有害的，因为它给其他人口施加了很大的压力，要求他们在面对社会障碍时要坚韧和强大。因此，与其真正致力于改变这些障碍，它反而给这些人施加了克服它们的压力，这导致了这些人非常消极的健康结果，以及其他危害。更广泛地说，我们发现每个标记群体的词几乎只反映了非常本质化的叙述。基于这些模式，我们对模型所有者提出了三条建议。首先，我们作为研究人员，应该关注积极的刻板印象和本质化的叙述。我们还应该使用交叉视角来研究偏见和伤害，因为如果不这样做，可能会忽略很多东西。最后，应该真正增加关于减少偏见的透明度，因为，例如，像这些积极的刻板印象一样，我们不知道这是因为某种奇怪的、过度夸张的价值对齐，还是可能有一些其他，像反刻板印象的方法导致了这些有害的模式。我们真的不能做出任何假设，或者真的不能进一步研究，没有更多的透明度。非常感谢大家的聆听。祝你在 ACL 玩得开心。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是詹姆斯·芬奇，我是萨拉·芬奇。今天我们将向大家介绍ABCEval，这是一种全新的评估对话式人工智能的维度方法。这项工作是由埃默里大学的乔伊·崔教授领导的埃默里大学自然语言处理实验室完成的，并与亚马逊Alexa AI合作完成的。假设你刚刚开发了一个对话模型，你想看看它与当前的先进水平相比如何。常见的做法是使用人工评估，例如让人工评判员选择两个对话中哪一个更好，或者根据一个量化比例对对话进行评分。这些方法在提供整体对话质量的全面评估方面效果很好，但对话质量有许多方面。因此，你可能希望评估对话质量的多个维度，以便更细致地了解模型的优缺点。一种方法是简单地让人工评判员评估对话质量的几个维度，例如模型响应的相关性，使用现有的比较或量化比例方法。然而，我们相信存在一种更精确、更可靠的维度对话评估策略。通过明确标注每个模型响应是否表达了某些行为，例如响应无关信息或自相矛盾。我们称这种方法为对话行为标注，简称ABCEval。我们开发了这种方法，以全面覆盖最近文献中建议影响对话质量的对话模型行为。ABCEval能够测量对话模型犯下各种主题错误的比率。例如，ABCEval测量了对话模型忽略其伙伴或说无关话、自相矛盾或与伙伴、伙伴或伙伴自相矛盾、产生错误事实或违反常识知识的回合次数，以及模型成功或失败地表现出同理心的次数。为了确定哪种评估方法最有效，我们选择了四种先进的对话模型，并使用ABCEval对每种模型进行了100个人工机器人对话的评估。为了进行比较，我们还使用三种现有方法对这些对话进行了评估：量化比例水平的量化评分、对话水平的量化评分和对话水平的配对比较。除了评估方法，我们还收集了对对话最常见的八个方面的评估，因为这是评估对话模型的标准做法。从我们对这些评估结果的分析中，我们发现ABCEval行为标签总体上比现有方法收集的标签更可靠，这是通过对100个双重标签对话的评判员一致性来衡量的。此外，ABCEval标签比现有方法产生的指标更能预测整体对话质量，这是通过这个简单的线性回归分析显示的。例如，你可以看到测量自相矛盾和伙伴自相矛盾回合比例分别解释了对话质量的5%和10%，而平均量化比例一致性得分只解释了4%或更少。最后，我们使用逐步线性回归检查了每个评估指标是否捕捉了对话质量的独特方面。你可以看到所有ABCEval指标的组合解释了超过25%的对话质量，当你逐个移除这些指标时，大多数都会导致失去大量关于质量的信息。而交替水平量化比例指标的组合解释了质量的远少，而且这些指标中很少有独特的。这些可靠、信息丰富且独特的ABCEval指标使我们能够以比以前方法能够实现的更高的分辨率评估对话式人工智能。从我们实验的结果中，你可以看到仍然存在几个挑战，并且这些挑战已经被精确量化。例如，我们在测试的机器人中，大约有20%的响应违反了常识。它们在大约15%的响应中产生了无关信息，并且在大约10%的时间里自相矛盾或与伙伴自相矛盾。随着该领域的快速改进，许多这些错误率可能会在自我们进行评估以来发布的新模型中减少。然而，这更是一个追求可靠和精确评估指标以比较模型的理由。我们希望ABCEval可以被该领域的其他人作为朝着这个方向迈出的有意义的一步，我们期待看到对话式人工智能在未来几个月和几年中的发展。感谢您的观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫 Vasudha，是 Stony Brook 大学的计算机科学博士研究生。我将介绍我们在 ACL 2023 上接受的长文论文《用于识别认知失调的迁移学习》，该论文解决了稀有类别的挑战。我们首先定义了认知失调，并解释了为什么它是语言研究中一个重要的问题。简单来说，认知失调是指两种不一致的信念或行为，例如一个人说“我知道吸烟会害死我”，然后又说“会议后我抽了几口烟”。这种信念和行为不一致，处于失调状态。进一步提到，如果没有吸烟，我可能无法保住工作，这证明了第二次行为与信念是一致的。虽然失调是我们日常决策中非常常见的一种现象，但在其他类型的语篇关系中，它们在语言中表达的非常罕见。那么，为什么这很重要呢？研究认知失调可以帮助我们理解人们之间的分歧、跟踪趋势和信念价值观以及态度的变化。高认知失调也与焦虑症有关，可以更好地理解人们的心理健康。研究语言中表达的失调也有助于理解极端主义和弱势群体的两极分化。最后，理解认知失调对于理解个人的认知风格非常重要，有助于我们更好地理解决策过程。为了实现认知失调资源的目标，我们对失调关系进行了大规模标注。我们采用了如图所示的失调优先方法。推文通过 PDTB 解析器进行处理，根据我们的论文中描述的指南对语篇单位对进行标注。如图所示，只有 3.5% 的标注对中发现了失调。在收集了大约 1,000 对语篇单位示例后，我们对初始分类器进行了训练，仅训练了 43 个失调示例。不出所料，分类器的表现并没有比随机猜测好多少。鉴于失调出现的频率低且之前没有任何此类数据集，我们面临着绝对稀有性的问题。为了缓解这个问题，我们尝试了迁移学习和主动学习的组合，以便在更少的标注轮次中收集更多的失调样本，降低整体标注成本，同时提高失调检测。由于初始模型根本无法捕捉到失调类别，我们通过从密切相关任务中转移权重来启动主动学习过程。我们从两个不同的任务中转移。一个是主题无关的失调立场分类，这是一个判断两个人在不同话题上的辩论陈述是否一致或不一致的任务，称为辩论；另一个是 PDTB 的扩展和比较类别的二元分类，因为这两个与辅音和失调的概念密切相关，我们称它们为 CE。我们发现，在转移零样本性能上，标注数据集的表现已经比随机猜测好得多，最佳的 AUC 为 0.62。进一步地，在两个任务上进行迭代微调，我们发现 CE 任务的微调后，再进行辩论的微调，可以获得更好的零样本性能。因此，这是我们用于冷启动主动学习的模型。接下来，我们确定了使用来自每个轮次的主动学习和标注的新数据更新模型的最佳方法。累计方法累积了迄今为止从主动标注中收集的所有数据，而迭代方法则通过训练最新的数据集来更新模型。在不同的策略中，我们发现累计方法在各个方面表现出与迭代方法相同或更好的性能。接下来，为了提高失调示例的数量，我们使用概率稀有类别策略（PRC），选择那些在任何轮次的 AL 中被当前模型认为高度可能失调的示例。我们将此与社区中常用的其他最先进的 AL 策略进行比较。我们发现，提出的 PRC 策略比其他最先进的策略表现更好，尽管差异很小。请注意，随机策略的性能明显较低。在使用两种最佳策略的进一步轮次的 AL 中，我们将距离分类 AUC 提高到 0.75，这是我们迄今为止在这个任务上获得的最佳性能。我们还检查了每种策略对标注质量和标注员成本的可行性。我们发现 PRC 在距离分类中具有最高比例，并且对稀有类别最有效。然而，标注员也发现这些示例很困难。总结来说，我们发现 PRC 是一种简单有效的稀有类别获取的 AL 策略，并且通过适当设计的迁移学习任务可以显著帮助冷启动 AL。我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而领域内的主动标注则受益于累积更新。这些是我们代码数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Aksheta，今天我和我的合著者 Martin 将展示我们的作品《The Kitmasteff 评估从多个来源整合知识》。这项工作是麦吉尔大学、Mila 和微软研究院的合作项目。国家语言理解模型依赖于各种知识来源，例如它们的参数中包含的知识，通常通过预训练获得，以及推理时输入中给出的知识。最近在问答等任务中的工作表明，模型可以使用预训练的时间知识来解决任务。但是，自然语言理解通常需要在推理时也提供知识。例如，在句子“约翰在电视上看到了新当选的总统”中，预训练的参数可能包含关于总统做什么和电视是什么的信息，但它们无法可靠地知道这个特定实体约翰是谁，或者新总统是谁，因为自预训练以来总统可能已经换了。因此，成功处理知识密集型 NLU 任务的模型需要能够整合和使用预训练时间和推理时间知识。在这项工作中，我们提出了一套用于知识整合的诊断测试。我们引入了一个核心参照解析任务，旨在探究从不同来源获取知识的能力。我们使用人类研究参与者和已建立的核心参照解析模型对数据集进行了评估。以下是我们的数据集的一个例子。Servin 是法官，Kia 是面包师。Termin 和 Kia 在公园里见面。经过一天在法庭上审理案件的辛勤工作，他很高兴放松一下。这里的任务是确定代词 he 指的是哪个正确的实体，在这种情况下是 Sermon。给定代词的解析需要两种类型的信息。首先，实体特定知识，例如 Sermon 是法官。其次，背景知识是在大型语言模型的预训练中学习到的，而实体特定知识通常在推理时观察到。我们改变这两种信息的可用性，使其可能只在一个来源中找到，或者在多个来源中找到。我们定义了三个 Kitmos 设置。首先，我们有主题设置，背景预训练，其中假设背景知识在预训练时可用。其次，有背景两者设置，背景推理设置，其中这两种知识类型只在推理时可用。最后一个设置特别有趣，因为它模拟了一个情况，即解决任务所需的背景知识不是模型预训练数据的一部分，例如，因为自预训练以来出现了新的职业。以下是关于我们如何控制两个来源中事实可用性的一个例子。在背景预训练设置中，我们假设背景知识政治家寻求政府选举席位包含在预训练参数中。在背景上下文中，我们提供了特定于 Chichester 的背景知识，他是政治家。在背景两者设置中，我们还提供了不仅是特定于 Chichester 的背景知识，还有关于政治家在推理类型上下文中的背景知识。在背景推理设置中，我们提供了虚构的职业镜像游览而不是政治家，因为镜像游览不太可能包含在预训练参数中。我们使用人类研究参与者和建立的核心参照解析模型对数据集进行了评估。在这个图表中，我们展示了在背景预训练设置的最困难变体上表现最好的模型的结果。没有在 KitMus 上进行任务特定训练，两个模型的表现都不好。然而，当在 KitMus 上进行训练时，C2F 和 Berth for Koref 的表现都显著优于随机选择。这表明，当在一般核心参照解析数据集上进行训练时，模型学会了利用表面线索，这些线索在测试 KitMus 时是没有用的，因为这些线索已经被删除了。额外的虚构知识实验表明，即使是表现最好的模型也无法可靠地整合只在推理时提供的背景知识。总结我们论文的主要结论，许多核心参照解析模型似乎无法在没有任务特定训练的情况下对来自不同来源的知识进行推理。然而，通过任务特定训练，一些模型成功地整合了来自多个来源的知识。尽管如此，即使是表现最好的模型似乎也难以可靠地整合只在推理时呈现的背景知识。如果您对更多细节感兴趣，请参阅我们的论文并在 GitHub 上查看代码中的数据集。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是多伦多大学和布鲁诺·凯斯勒基金会的Sarah Pappy，我将简要介绍一篇关于将注意力作为同步语音翻译的指导的论文，这是与Matteo Negri和Marco Turki的合作成果。什么是同步语音翻译？同步语音翻译或模拟翻译是指实时将口语翻译成另一种语言的文本的过程，实现跨语言交流。那么，当前模拟模型存在哪些问题呢？通常，特定的架构需要引入额外的模块进行优化，训练过程冗长且复杂，例如，涉及不同的优化目标的训练，以及训练和维护多个模型以达到不同的延迟机制，例如，训练一个平均延迟为一秒的模型和另一个平均延迟为两秒的模型等等。那么，我们的解决方案是什么呢？首先，使用现有的离线ST模型，无需重新训练或采用特定的架构进行CMLSD。对于每个延迟机制，只使用一个mod模型，并通过特定的参数处理延迟，利用模型通过音频输入和文本输出之间的注意力机制（即交叉注意力机制）获得的知识，您可以在右侧看到一个例子。我们的解决方案是提出一个点或编码器解码器注意力机制，这是一个策略，根据注意力指向的位置决定是否发出部分翻译。如果注意力不集中，即该和低于某个阈值alpha，指向最后lambda个语音帧，这意味着接收到的信息足够稳定，则发出一个词。例如，如果我们接收了一个包含“我要谈论”的语音片段，我们的模型预测德语翻译，我们将查看交叉注意力权重，我们会看到前两个词指向最早接收到的语音帧，而最后一个词指向最后一个接收到的语音帧，即lambda语音帧。这意味着前两个词将被发出，而由于交叉注意力的和高于某个阈值alpha，我们将不发出最后一个词，而是等待另一个语音片段。如果我们继续接收另一个语音片段，我们的模型预测另外三个词，我们将查看交叉注意力权重，我们会看到没有词指向最后一个lambda语音帧。这意味着这三个词将被发出。如果我们查看主要结果，我们将以图表形式绘制同步语音翻译结果，其中一边用蓝色表示翻译质量和平均延迟，即延迟度量。我们还考虑了计算感知平均延迟，该延迟考虑了模型预测输出的计算时间。所以，我们希望我们的曲线在这个图上尽可能高，但我们也希望它们向左移动，我们将与预处理策略进行比较，这些策略也适用于离线模型，即权重键策略和局部一致性，我们还与专门为同步语音翻译定制的最先进架构进行了比较，这些是同步语音翻译策略在德语上的所有结果，我们看到，一个点优于所有应用于离线模型的策略，因为它们的曲线向左移动，我们还看到，如果我们考虑实际的经过时间或计算磨损时间，即最快的策略，如果您想发现更多结果，请阅读我们的论文，我们还发布了开源代码和模型以及同步输出，以促进我们工作的可重复性，感谢您的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫徐恒。今天我要介绍我们的论文《2003年核技术命名实体标注器在2023年还能否正常工作？》让我们开始吧。我们的论文研究了使用命名实体识别任务或NER任务进行泛化的问题。我们观察到，近20年来，模型一直在使用2003年核技术来开发NER。这自然引发了几个问题。首先，这些模型能否泛化到现代数据，以及在开发新的标注器时，需要什么才能同时实现良好的泛化。如果我们观察到泛化效果不佳，是什么原因导致这些模型性能下降？为了研究这些问题，我们开发了Kernel Plus Plus数据集。这是一个我们从2020年路透社新闻中收集的数据集，并使用相同的2003年核技术标注指南对其进行了标注。然后，我们在2003年核技术上对20多个模型进行了微调。我们对Conor 3测试集和Conor Plus Plus测试集进行了评估。最后，我们计算了每个模型的F1百分比变化，以评估其泛化能力。那么，实现良好泛化需要什么？通过我们的实验，我们发现需要三个主要因素。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现，通常较大的模型能更好地泛化。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们还发现，更多的微调示例实际上也能带来更好的泛化效果。关于我们下一个问题，是什么原因导致某些模型性能下降？我们有两个假设。第一个是自适应过拟合，这是由于反复使用相同的测试集导致的过拟合，通常表现为在新测试集上的回报递减。第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距增大导致的性能下降。对于过拟合数据，我们从右侧的图表中看到，红色的最佳拟合线具有大于1的梯度。这意味着我们在2003年核技术上做出的每一项改进，都会在Kernel Plus Plus上转化为超过一个单位的改进，这意味着没有回报递减，这表明在这种情况下没有观察到自适应过拟合。那么时间漂移呢？对于时间漂移，我们进行了一项实验，使用更近的数据对一些模型进行了再训练或继续预训练，我们发现随着时间差距的增大，性能会下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小，以及更多的微调示例，这些都是相辅相成的。我们不能只拥有一个因素而抛弃其他因素。同时，我们还发现，这里的性能下降是由时间漂移引起的，这有点令人惊讶，即使2003年核技术已经使用了20多年。所以，回到我们论文标题提出的问题，2003年核技术命名实体标注器在2023年还能否正常工作？我们发现答案实际上是一个响亮的肯定。我们希望我们的论文能促使更多人研究如何提高模型的泛化能力。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "您好，欢迎来到我们的演示，我们将展示 DeepLean，这是一个用于德语文本简化的新语料库，涵盖文档级别和句子级别。我叫 Regina Stodden，我将引导您完成演示的第一部分。我们先定义一下文本简化。文本简化是将文本进行改编，以提高特定目标群体对文本的理解，例如，有阅读障碍的人或非母语人士。为了训练文本简化模型，我们需要平行文本对，例如，文档或句子。在下面的例子中，您可以看到一个复杂的德语句子及其通俗语言翻译的平行对齐句子对。为了简化句子，可以采用不同的技术，如您在例子中看到的，例如，词汇替换、从句删除、从句重新排序或插入单词。我们现在提出我们的新语料库 dplane，因为近年来，现有的语料库存在一些问题，或者太小，无法训练分类模型。近年来提出的其他三个模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了我们的新语料库 DPlane，它分为两个子语料库，DPlane APA 和 DPlane Web。DPlane APA 基于已使用文本。在 DPlane APA 中，我们手动对齐了 483 个文档。结果大约有 30,000 对、13,000 对平行句子对。对于 dplane web，这个语料库包括不同的领域，我们也手动对齐了这 750 个文档，另一方面也使用了自动对齐方法。总的来说，我们得到了 30,450 对句子对。我们对句子对进行了更详细的分析，例如，简化类型。如您所见，圣经文本比新闻文本或语言学习文本在所有级别上都更强，例如，词汇简化、结构简化或整体简化水平。此外，您可以看到，我们的 D-plane 语料库具有高度多样化的不同简化变换。例如，在 D-plane API 语料库中，我们有比 D-plane web 语料库更多的重新排序和单词编辑。另一方面，在 web 语料库中，我们有更多的简化改写。那么，我们现在来看看我们可以用这个语料库做什么。您好，我是 Omar，我现在将谈谈我们数据集 dplane 的使用案例。第一个使用案例是我们可以评估自动对齐方法。近年来，出现了很多对齐方法，但在机器翻译的背景下，我们有两个用不同语言编写的平行文档，我们想要从后置文档中提取句子对齐，但在我们的使用案例中，我们试图在两个平行文档的句子之间提取对齐，具有相同语言、相同内容，但它们在复杂性级别上不同。现在，我们有了手动对齐句子的数据集 D-plane，我们可以使用这些句子作为金标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了一些改编，并在论文中发表了所有这些改编和运行实验的代码。最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是质量对齐方法，您也可以在论文中找到在您自己的文档上运行该方法的代码。我们在论文中展示的第二个使用案例是通过微调语言模型来自动文本简化的案例。我们微调了两个不同的模型。我们微调了 long import 模型以生成文档级别的简化，我们还微调了 normal base import 以生成句子级别的简化。您也可以找到所有检查点，并在论文中查看我们实验的更多详细信息和评估指标。我们得出结论，这种基本的微调可以产生或可以获得比基准分数更好的分数，我们提出这些结果作为未来的自动文本简化问题的基准。非常感谢您的关注，我们希望在会议期间见到大家。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是来自芬大学的Xi Yuan。我在这里介绍我们关于从大型语言模型中提取特定脚本知识以进行受限语言规划的工作。在日常生活中，人类通常通过遵循分步指令的形式来规划自己的行动，这些指令以保证脚本的形式存在。之前的研究已经探讨了语言模型如何为典型的抽象目标（如制作蛋糕）进行规划，并证明了大型语言模型可以有效地将目标分解为步骤。然而，之前的研究主要集中在为典型的抽象目标进行规划。而对于具有特定目标、特定约束（如制作巧克力蛋糕）的目标进行规划的研究则相对较少。在这篇论文中，我们定义了受限语言规划的问题，该问题对规划目标施加了不同的约束。一个抽象目标可以被不同的现实生活中的具体目标继承，这些具体目标具有更复杂、多方面的约束。一个好的规划者应该编写符合约束且忠实于约束的脚本。在这篇论文中，我们首先评估和改进大型语言模型的受限语言规划能力。由于没有特定的目标数据集来支持我们的研究，我们必须首先获取这些目标。如表所示，我们通过使用指示性TPT方法，扩展了人类循环数据获取的多方面约束的抽象目标。我们随机抽取了100个具体目标，并评估了Light Logic模型生成的脚本。该表报告了结果的总体准确率。我们发现，所有Light Logic模型在规划具体目标方面都取得了不令人满意的结果。然后，我们进行了详细的分析，以调查Light Logic模型的原因。图中的结果显示，生成的脚本在语义完整性方面是可接受的。但是，无法保证对约束的忠实度。我们深入研究了根据回家时的具体情况对约束进行更具体的分类。图中的主图显示，指示GPD的规划性能在不同类别的女孩中差异很大。之前的研究表明，Larry模型的输出质量存在高方差，导致性能不佳。因此，我们采用了过度生成的Zen过滤器来提高生成质量。我们首先展示了指示GPT的约束类型及其示例，并根据一组抽象目标获得了具体目标。然后，指示GPT过度生成了特定目标的关键脚本。接下来，开发了一个过滤器模型来选择合适的脚本。我们将脚本和目标转换为指示GPT的位元组，并计算余弦相似度和相似度分数以衡量语义相似度。此外，我们将编写包含目标约束关键词的脚本。如果目标得分在目标站点中最高，我们才保留该脚本。通过我们的方法，Inslacity可以生成更高质量的脚本。我们的方法大大提高了语义完整性和对约束的忠实度。由于大型语言模型的部署成本高，因此必须使较小且专业化的模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。然而，之前的研究没有实现对特定目标的规划，手动数据集标注成本高。因此，我们遵循符号知识蒸馏的思想，对受限语言规划模型进行蒸馏。我们应用我们的方法来构建一个名为code script的受限语言规划数据集。总共，我们生成了五万五千个具有脚本的具体目标，以确保验证和测试站点的质量。我们要求云源工作人员查找并修改错误的样本。该图显示了code script的受限分布。我们发现，code script在生成的具体目标中显示了假设。有了code script，我们可以训练较小但专业化的模型进行受限语言规划。我们发现，在code rate上应用TFIL函数可以生成比大多数大型语言模型更高质量的脚本，这表明在适当的数据站点上进行适当训练的小型模型可以支持大型模型。总之，我们建立了受限语言规划问题。我们评估了大型语言模型的受限语言规划能力，并为大型语言模型开发了一种过度生成的过滤器方法。我们使用大型语言模型生成了一个高质量的脚本数据集，CodeScript，用于建设性语言规划。我们希望CodeScript数据集可以成为推进语言规划研究的宝贵资源。感谢您的聆听。有关CodeScript的更多详细信息，请参阅我们的论文。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Yanis Lavrack，我将向大家介绍我们在 Dr. Berth 上的工作，这是一个针对法语生物医学和临床领域的强大预训练模型。在本次演讲中，我们首先讨论医疗保健中的语言建模。然后，我们将介绍我们文章的主要贡献。我们介绍了第一个法语生物医学模型 Dr. Berth，它基于 Roberta，并在 Natchios 上进行训练，Natchios 是一个从网络上抓取的医学数据集。我们还介绍了多个预训练设置和数据源的模型比较。然后，我们展示了我们在法语下游任务上的结果。最后，我们将总结实验，并向您提供有关如何访问这些模型的更多详细信息。自 2018 年发布以来，BERT 已成为解决自然语言处理任务的最有效方法之一，与历史上的静态和上下文方法（如 Word2Vec、Fastex 或 NWO）相比，性能大幅提升。从那时起，该模型已被适应到许多其他语言，如法语中的 Camembert，以及生物医学中的 PermetteBERT 和 BioBERT，以及临床中的 clinical birth，但大多数是英文。其他语言的专业模型很少，并且通常基于持续预训练，因为缺乏领域内的数据。然而，到目前为止，法语还没有任何开源生物医学模型。所以我们问自己一个问题：对于广泛的用途，最合适的 数据来源是什么？而那些当前的数据是临床数据的良好替代品。为了回答这个问题，我们将 Dr. Bert 与我们基于我们所拥有的非大学医院获得的匿名数据构建的 Schubert 模型进行比较。然后，我们问自己，我们需要多少数据来训练一个基于法语数据的专业模型？是 4 GB、8 GB 还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型。Dr. Bert 的第一个版本使用了 7 GB 的 Natchez 数据，第二个版本使用了 4 GB 的 Natchez 数据，Schubert 的第一个版本是一个临床模型，使用了从临床节点中提取的 4 GB 的句子，Schubert 的最终版本则混合使用了 4 GB 的 Natchez 数据和 4 GB 的临床节点数据。除了这个比较，我们还介绍了三个基于持续预训练的模型，以分析预训练策略的影响。一个基于 Camembert 的权重，并在 4 GB 的 Natchez 数据上进行训练。另一个也是基于 Camembert，但这次是在 4 GB 的临床节点上进行训练。最后，一个基于英语生物医学模型 Bermud Bert，并在 4 GB 的 Natchez 数据上进行训练。总共有七个模型。为了评估我们的七个模型，我们收集了匹配的公共和私有任务，如命名和高度识别。分类、模式切换标记和问答。这些模型与六个基线模型进行了比较，这些基线模型是 Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CCNet 4 GB、PumedBERT、BioBERT 和 ClinicalBERT。评估结果显示，模型在与训练数据性质相同的任务上表现最佳。然而，我们可以从这些数据中观察到，来自异构来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总的来说，从头开始的微调似乎在大多数任务上都能获得更高的性能。然而，我们使用 PumedBeard 的权重和分词器进行持续微调的实验，在 Natchez 的 4 GB 子集上进行训练，结果与 Dr. Beard 4 GB 从头开始的结果相当，但基于 Camembert 权重和分词器的模型则存在稳定性问题。最后，作为结论，我们提出的系统在十一个 DOTSTRIMS 任务中的九个任务上表现更好，并且在全球范围内超越了通用模型（这里指的是 Camembert）的结果。我们还观察到，专业数据更好，更专业的数据更好，但它并不容易扩展。从 Natchios 获得的所有预训练模型都可以在 YuginFace 上免费获取，所有的训练脚本都在我们的 GitHub 仓库中。所以，感谢大家...参加本次演讲，我们期待在多伦多的会后交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是华盛顿大学的博士生Xiang Bin。今天我将介绍我们从预训练数据到语言模型再到下游任务的工作，追踪导致不公平自然语言处理模型的政治偏见的轨迹。因此，语言模型是在大规模的网络爬虫数据上训练的。政治新闻媒体在他们的预训练数据中得到了很好的覆盖。根据对C四语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。这为语言模型的应用带来了既是福又是祸。一方面，它们能够从多样化的视角学习，这庆祝了民主和思想的多元性。另一方面，这些不同的政治观点在本质上具有社会偏见，可能会在下游任务应用中导致潜在的公平问题。为此，我们提议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体来说，通过以下问题：首先，我们如何评估语言模型的政治倾向，预训练数据可能对这种政治偏见有什么作用？其次，具有不同政治倾向的语言模型在实际应用中如何表现，这是否会导致自然语言处理应用中的公平问题？因此，具体来说，我们首先提议使用政治问卷（如政治指南针测试）对不同提示格式的语言模型进行提示。这确保了我们的自动评估基于政治科学文献。因此，一些初步结果表明，首先，语言模型确实具有不同的政治含义。它们占据了政治指南针上的四个象限。我们还可以看到，GPT 4是所有语言模型中最自由派的一个，GPT理论通常比BERT理论及其变体更具社会自由主义。其次，我们的目标是研究语言模型的政治偏见实际上是从训练数据中获得的程度。因此，我们可以通过进一步在六个不同的党派语料库上预训练语言模型检查点来进行受控实验，这些语料库分为新闻和社交媒体，进一步分为其政治倾向。通过在这些党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，对于进一步微调的Roberta，进一步训练于左翼的Reddit语料库，我们可以看到其政治偏见方面有显著的自由派转变。为了研究语言模型是否能够捕捉到我们现代社会中普遍存在的极化。因此，我们将预训练语料库分为第45任美国总统当选前和当选后。我们分别在两个不同的时间语料库上预训练语言模型。我们可以看到，语言模型在2017年后通常具有更偏离中心的政治倾向。因此，这表明语言模型也可以捕捉到我们社会中的极化。最后，但并非最不重要的一点，我们评估了具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测方面的表现，这些应用通常涉及语言模型，可能具有非常重大的影响。因此，我们看到，如果我们按类别评估性能，也就是说，如果我们将性能分为不同的人口统计或新闻媒体的政治含义，我们可以看到一个模式，例如，对于仇恨言论检测，左翼语言模型在检测针对社会少数群体的仇恨言论方面表现更好，然而，在检测针对我们社会中更强大群体的仇恨言论方面表现较差。反之，右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，然而，在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。类似的趋势也发生在虚假新闻检测中，我们看到左翼语言模型在检测来自其相反政治含义的虚假信息方面表现更好，反之亦然。这将进一步展示许多定性例子，说明具有不同政治含义的语言模型根据其社会类别对仇恨言论和虚假信息例子给出不同的预测。附录中有更多例子，以进一步强调这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平问题。例如，如果一个右翼语言模型被微调用于仇恨言论或虚假信息或其他，并部署到一个流行的社交媒体平台，这意味着具有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会不受控制地蔓延。因此，这提醒我们要认识到并解决语言模型政治偏见导致的公平问题。因此，有一点讨论。我们还希望强调我们揭示了语言模型政治偏见的独特困境。这就像在斯库拉和喀里布狄斯之间。因此，如果我们在语言模型训练数据中不净化政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们尝试以某种方式净化，我们也会冒着审查或排斥的风险，而且很难确定什么才是真正中立的，应该保留在语言模型训练数据中。所以，这有点像电查理问题。好吧，很好。我想这就是我今天要讲的全部。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是新浪的Coast，很高兴欢迎大家来到我们的讨论，我们将谈论我们2023年ACL论文《语言模型的可接受性判断并非总是对上下文稳健》的内容。这是一项与John Bakhier、Aaron Mueller、Kanishka Mishra、Karen Fentus、Roger Levy和Adina Williams合作完成的联合研究。因此，在这项工作中，我们重新审视了最小对概念。最小对概念基本上是通过可接受性判断来评估语言模型，这些判断还可能包括语法性，如blimp语法宝石或可接受性方面如人群节省等。在这个最小对概念中，评估语言模型的典型方法是展示一个可接受的句子或语法句子，然后展示一个不可接受的句子或非语法句子，希望模型基本上会给可接受的句子赋予更高的概率。当前的MPP管道基本上不允许我们评估模型对更长句子的接受程度。如今，大型语言模型正在产生越来越长的上下文窗口。因此，评估模型在整个上下文窗口内的可接受性至关重要。这就是我们在这里试图做的事情。我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视NPV管道。所以这就是方法。因此，我们所做的是模拟这些更长的序列，重新审视数据集本身，然后从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，这里我们从blimp数据集的从属岛案例中选择了典型的一对语法性。我们所做的是重新创建更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配，我们从从属岛中提取语法句子，然后将其作为前缀添加到可接受查询和不可接受查询中。所以我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。我们也可以通过从不同的子集或不同的数据集选择句子来做同样的事情。这就是我们所谓的不匹配场景。所以在这里，句子仍然来自相关的数据集，但不是你正在评估的同一数据集。我们也可以对不可接受性案例做同样的事情。最后，我们可以从完全不相关的领域选择句子，如维基百科。所以这将告诉我们，模型的可接受性判断是否实际上受到任何上下文的影响，如上下文是否来自数据集的不同子集，或者是否与我们正在查看的句子完全无关。那么模型是如何做到这一点的呢？首先，我们查看与当前查询对完全无关的维基百科句子，我们发现MPP判断对于任意上下文长度大多是稳健的。我们增加了上下文长度，直到达到1024，以最大化OPT和GPT2模型，我们在橙色虚线中看到，MPP判断相对稳定。那么当我们选择来自同一数据集的句子时会发生什么？所以在这里，我们从相同的blimp或语法宝石数据集中的可接受和不可接受领域选择或创建句子，我们发现当添加可接受前缀或不可接受前缀时，MPP判断会显著增加或减少。但是，当我们匹配结构时，即当我们从基于责备的语法健身房中选择句子时，我们看到模型的MPP判断会根据所选前缀是可接受还是不可接受而大幅增加或大幅减少。现在，这个效果非常大，随着上下文长度的增加而增加，这可能会影响到具有大上下文窗口的较新的语言模型。那么为什么匹配前缀会如此大地影响语言模型的判断呢？所以我们进行了一系列分析，试图像扰动...输入句子，同时保持相关的结构，但在输入中添加噪声。经过几次这样的扰动，我们发现这些噪声实际上并没有使模型在显示MPP判断趋势方面改变其方向。基本上，我们发现模型以相似的方式对扰动句子敏感，即当我们在可接受领域中扰动句子时，我们看到所有扰动的相似增加，当我们在可接受领域中扰动句子时，我们以相似的方式看到MPP判断的减少。因此，我们工作的关键结论是，语言模型对句子中共享的潜在句法和语义特征敏感。我们目前通过短句和单句输入进行的MPP评估可能无法完全捕捉语言模型在整个上下文窗口内的抽象知识。请阅读我们的论文以获取我们实验的更多详细信息。感谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是德国斯坦兰特大学的博士生 Dawe。在这段视频中，我想介绍我们最近的工作《比你想象的还要弱》，这是一次对每周监督学习的批判性审视。这是与 Xiao Yushchen、Maios Musbach、Giaz Steffen 和 Dietrich Clarkov 共同完成的工作。我想先简要介绍一下每周监督和每周监督学习。在每周监督中，我们不会手动标注数据。相反，我们会使用每周标注源来标注数据，例如简单的启发式规则、知识库或本地化众包，如图右侧所示。与人工标注相比，较弱的标注要便宜得多，但它们也存在噪声，这意味着一定数量的标注是错误的。如果我们直接在每周标签数据上训练神经网络，神经网络往往会记住标签噪声，并且无法泛化。在每周监督学习中，提出了训练算法，以便在这样的标签噪声上稳健地训练神经网络，以便训练的模型仍然能很好地泛化。在最近的 WSL（每周监督学习）工作中，一个常见的说法是，人们说他们只在每周标签数据上训练模型，并在干净的测试集上取得了高性能。从技术上讲，这个说法并没有错，但有一个问题，那就是人们假设有一个额外的干净验证集可用用于模型选择。我们对这个问题设置提出了质疑，因为这意味着在每周监督学习中需要额外的手动标注。但就像房间里的大象一样，这个必要性往往被忽视。上述的疑问促使我们提出了三个研究问题。首先，WSL 是否需要干净的验证数据？或者我们是否可以改用噪声验证集？其次，如果需要干净数据或干净数据是 WSL 工作的必要条件，那么我们需要多少干净样本？最后，我们是否应该只使用干净样本进行验证，还是还有更好的利用方式？我们在工作中解决了这些研究问题，我们的发现如下。首先，我们发现有趣的是，最近的 WSL 方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，训练的模型无法超越原始的弱标签进行泛化，这意味着训练是没有意义的。这表明 WSL 方法实际上需要干净的标注数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于 WSL 方法实现更好的性能，如图左侧所示。通常，我们只需要每个类别二十个样本就能达到高性能。但这还不是故事的全部，因为如果我们决定访问干净样本，那么直接在它们上进行训练甚至会获得更好的性能。红色图形显示了直接应用于干净数据的微调方法与仅用于验证的 WSL 方法之间的性能差异。如图所示，如果我们每个类别有 10 个样本，直接微调开始超越 WSL 方法。最后，之前 WSL 方法中声称的性能改进可以通过允许在干净验证样本上继续微调来轻松实现。如图所示，柏林模型称为 FTW 最初的性能低于更复杂的 WSL 方法如余弦。然而，如果我们允许在干净样本上继续微调，那么 FTW 的性能与其他方法一样好。所以，在实践中，没有理由选择更复杂的 WSL 方法，这些方法需要更多的计算时间和磁盘空间。总结一下，我们表明最近的 WSL 方法需要干净的手动标注样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下。首先，报告模型选择是否使用干净的验证样本。其次，WSL 方法应该与未来的学习基线进行比较，因为两者都在干净样本上工作。最后，连续微调是一个简单但强大的基线，应该在未来的 WSL 工作中考虑。最后，我们开源了我们的代码。您可以通过此幻灯片上的二维码找到它。请随时查看。谢谢，祝您会议愉快。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫阿尤德·维拉尔，我将简要介绍论文《从翻译评估策略和性能中提取提示》中的内容。这是我和谷歌翻译同事的合作成果。Palm 是去年 2022 年推出的一个包含 5400 亿个参数的大型语言模型。它在包含 7800 亿个标记的大量文本上进行了训练。在发表时，它在数百个 NLP 任务中达到了最先进水平。在这项工作中，我们提出了机器翻译中大型语言模型提示的首次系统研究。我们使用 AMT 社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠，我们比较了两个最先进的系统，即 WMT 评估中表现最好的系统。我们使用了最先进的神经机器翻译指标，并展示了基于专家的人类评估结果。最后，我们提供了一些提示选择策略的建议。提示对翻译 LLM 的性能有很大影响，我们可以在一个简单的实验中看到这一点，我们使用一个简短的提示，并为一句话提供了两个不同的提示。在 1000 个句子中，有 516 个句子的差异超过一个模糊点。在极端情况下，这个差异可以达到 40 个模糊点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了五次提示策略，我们只需将我们提供给系统的每个句子标记为其所在的语言。所以在这个例子中，我们从德语翻译成英语，德语句子，源句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，在几次简短提示的情况下，实际形式的提示并没有产生很大的影响。对于零和一次简短提示，这至关重要，但当我们像我们这样进行五次简短提示时，实际形式的提示几乎没有区别。重要的是例子本身。我们实验结果的总结是，例子质量比与源句子的相似性更重要。因此，选择高质量翻译的例子非常重要。特别是，我们比较了从 WMT 评估的训练数据或开发数据中选择提示。开发数据比训练数据更有选择性，质量更高，我这样说，结果显示使用开发数据时性能更好。尽管如此，专业化的 ODR 系统在性能上比 PALM 翻译有显著优势，但 PALM 接近商业系统。在我们的情况下，我们选择与谷歌翻译进行评估。我们使用 MQM 框架进行的人类评估结果表明，Palm 的流畅度与最先进的系统相当，但主要区别在于准确性。特别是，最常见的错误是遗漏错误。所以，Palm 有时会选择省略翻译中省略的部分，以产生更好的翻译。然而，PAM 的外部风格类别低于最先进的系统，这是一个额外的信号，表明 PAM 提供了非常流畅的输出，但仍然存在一些准确性问题。这就是这个非常简短的概述的全部内容。有关更多详细信息，请参阅论文的完整介绍。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的景伟。很高兴能给大家展示一下我们的论文《Are you copying my model》的简短广告视频。该论文旨在保护大型语言模型的嵌入和服务的版权，并引入“看门狗水印”概念。首先，让我们介绍一下嵌入和服务的背景。目前，GPT、Lama、PELM等大型语言模型在自然语言理解和生成方面表现出色。嵌入和服务是建立在大型语言模型基础上的一项服务，用于辅助各种自然语言处理任务。例如，OpenAI 提供基于 GPT 的嵌入 API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入作为服务的版权是必要的。为了保护嵌入作为服务的版权，其中一个解决方案是在提供者服务中嵌入水印，并检测另一个服务是否包含水印。该方法需要满足以下属性。首先，该方法应适用于嵌入作为服务。其次，水印不应降低所提供嵌入的实用性。第三，水印应对攻击者足够隐蔽，或者攻击者可以轻松移除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有工作可以大致分为四类。然而，这些方法要么不适用于嵌入作为服务，要么缺乏可转移性。因此，在这篇论文中，我们提出了嵌入标记，这是一种基于后门的嵌入水印方法，适用于嵌入作为服务。接下来，让我介绍一下我们的嵌入标记的细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组频率适中的词语。假设提供者可以收集一个通用的文本语料库，并计算词频。在水印注入中，我们首先定义一个目标嵌入。当用户向提供者服务发送一句话时，提供者计算这句话中的触发次数。所提供的嵌入是目标嵌入和原始嵌入的加权求和。目标嵌入的权重与这句话中的触发次数成正比。当这句话中的触发次数大于 M 时，所提供的嵌入正好等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词语都属于触发集的句子，而良性数据集中的句子中所有词语都不属于触发集。然后，提供者使用该数据集从窃取者服务请求嵌入。计算请求嵌入与目标嵌入之间的相似度，即相似度 δ 和 L²。同时，我们还计算后门数据集和良性数据集之间的相似度差异，定义为 δ cosine 和 δ L²。此外，我们还应用 KS 测试，并使用其 p 值作为第三个度量标准。我们在四个数据集上进行了实验：AG News、Mind、SSD two 和 Erospam。我们假设提供者使用维基百科文本数据集来计算词频。在四个数据集上的结果表明，我们的嵌入标记可以在保持网格实用性的同时，实现出色的检测性能。我们还通过可视化句子嵌入（如 BOPCA）来验证所提供嵌入的隐蔽性。图例表示每句话中的触发次数。如图所示，很难区分后门嵌入和正常嵌入。以上就是全部内容，谢谢大家。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫 Ying，我的同事 Jian 和我将介绍我们关于通过指令微调改进多模态零样本学习的研究。随着大型语言模型的进步，许多工作开始探索以参数和数据高效的方式为不同的下游任务重用预训练语言模型的新学习范式。最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令以零样本方式执行未见过的任务。然而，大多数关于指令微调的先前工作都集中在提高仅语言任务的零样本性能上，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想要研究在多模态预训练模型上进行指令微调是否真的可以提高对未见的多模态任务的泛化能力。此外，在我们进行研究时，我们发现 LP 和多模态之间的指令数据集可用性存在显著差异。存在超过一千六百个仅语言指令任务。然而，没有大规模公开的多模态指令任务。因此，这激励我们构建一个多模态指令微调数据集。在这里，我们提出了 multi instruct，第一个多模态指令微调基准数据集，包含六十两个多样化的多模态任务，涵盖十个大类。这些任务来自二十一个现有的开源数据集，每个任务都配备了五个专家撰写的指令。为了在我们提出的数据集上研究多模态指令微调，我们以 OFA 统一的多模态模式模型作为我们的基模型。OFA 使用统一的词汇表来表示语言、图像标记和边界框的坐标。在这里，我们展示了我们 multi install 数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循 OFA 的方法，并将所有任务统一为序列到序列格式，其中输入文本、图像、指令和边界框以相同的标记空间表示。好的，现在我要谈谈多模态指令微调。对于训练数据集，我们使用 NIG 组的 53 个任务进行训练，每个任务抽样 10,000 个实例。对于测试，我们保留整个常识推理组进行测试，并从 WQA 和杂项组中选择另外五个任务。我们使用每个任务测试集中的所有实例。此外，我们从 NIG 指令测试集随机抽取 20 个任务作为 NLP 的 SIN 任务。因此，我们使用预训练的 OFA 大模型作为基模型。在训练过程中，我们将所有任务的所有实例混合在一起。每个实例随机与其五个指令模板中的一个结合。因此，在每个任务的测试中，我们进行总共五个实验，每个实验使用五个指令中的一个评估模型。我们报告所有五个实验的平均和最大性能以及性能的标准差。如果任务是多模型分类任务，我们报告准确率。如果是多模型生成任务，我们报告 ROOGEL。对于 RP 任务，我们也报告 ROOGEL。我们还引入了一个额外的评估指标，称为敏感性。因此，它衡量模型在指令措辞略有变化的情况下，是否能够始终如一地为同一任务生成相同输出的能力。这是我们的主要结果。我们可以看到，指令微调可以显著提高 OFA 在可见多模型任务上的性能。此外，从自然指令数据集进行的迁移学习可以使指令微调受益。在这里，我们可以看到随着任务数量的增加，模型的性能得到提高，同时敏感性降低。因此，我们还进行了一个实验。我们使用一个指令与五个指令进行比较，可以看到使用更多指令可以提高模型的整体性能，并大大降低其敏感性，这表明了不同的微调策略对模型敏感性的影响。我们可以看到，从自然指令数据集进行的迁移学习可以使模型的敏感性相比原始 OFA 模型有显著提高。我们还可以看到，从自然指令数据集进行的迁移学习可以帮助 OFA 在自然指令数据集上取得更好的性能。因此，总的来说，我们提出了第一个大规模多模态指令微调数据集。我们显著提高了 OFA 的 DAROCHOT 能力，并探索了不同的迁移学习技术并展示了它们的优势。我们设计了一个新的指标，称为敏感性。还有一件事，我们正在收集一个更大的多模态指令微调数据集，包含大约 150 个额外的变体语言任务，并将它们发布。所以，这是一个 QR 码，用于访问我们的数据和模型。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的尤素福·张。今天我将介绍我们的工作，Exampler，跨语言语义解析在多种自然语言和意义表示中的应用。语义解析是构建用户查询（如SQL和λ演算）语义表示的任务。而跨语言语义解析则是将多种自然语言的查询翻译成多种意义表示的任务。如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL、λ演算、FunQL等。现有的跨语言语义解析模型分别针对有限任务和应用的数据集提出和评估。例如，某些自然语言的覆盖范围不足。中文缺失，某些表示的覆盖范围不足。λ演算缺失。或者它们只在某些较新的模型上进行评估。例如，只有一个模型进行评估。为此，我们提出了Exampler，我们提供了一个统一的跨语言语义解析数据集Exampler，用于多种自然语言和意义表示。它包含了九个来自不同领域的语料库，五个语义解析任务，八种意义表示，以及22种自然语言和15个语系。为了更好地评估我们的基准测试，我们考虑了六种训练和评估设置。第一种是TranslateTest。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们用英语查询训练英语模型，在推理过程中，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试了单语模型。在这个设置中，源语言与目标语言相同。例如，德语到德语或英语到英语。我们还测试了单语融合设置，通过仅使用10%的训练数据训练单语模型。我们还测试了单语多语模型，我们为所有语言训练一个多语模型。例如，我们将德语、英语和中文查询一起训练一个多语模型。在推理过程中，我们可以使用这个模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和领域样本迁移。我们在一个源语言上进行训练，然后迁移到另一种语言。因此，在训练过程中，我们用英语查询或英语和德语领域样本查询的组合训练一个多语模型，并预测SQL输出。我们还发现了许多有趣的成果。因此，关于单语模型的分析，我们在两组模型上进行评估，包括编码器PDR，即多语言预训练编码器加上基于指针的解码器，如XLMR plus PDR和BERT plus PDR。我们还评估了编码器解码器模型，即多语言预训练编码器解码器模型，如MBART和MT5。我们发现编码器解码器在所有九个数据集上表现最佳。我们在多语言设置下对MT5和XLMR plus PDR进行了评估。我们发现，通过在各种语言的混合中进行训练，可以提高编码器解码器或编码器PDR的性能。我们发现这是因为大多数主要自然语言都能获得性能提升，除了英语在七个数据集上性能下降，只在三个数据集上有所提升。我认为这被称为多语言曲线。我们还比较了跨语言性能差距。如图所示，蓝色线是跨语言洋红色迁移，橙色线是跨语言零样本迁移，绿色线是单语设置。我们发现，通过比较绿色和橙色线，我们发现对于零样本设置，跨语言迁移性能差距显著。通过比较蓝色和橙色线，我们发现对于洋红色设置，迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，编码器-解码器优于进度工作或取得了可比拟的结果。在英语自然语言上进行训练可以显著提升Fushot在目标自然语言上的性能。我们发现多语言模型如Codice和Bloom仍然不足以应对跨语言语义解析任务。总结起来，我们构建了Exampler，一个用于多种自然语言和多种表示的跨语言语义解析统一基准测试。我们对三种代表性的多语言模型进行了全面的基准测试研究。我们的结果显示了许多有趣的发现等。欢迎访问我们的论文和代码。谢谢大家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫亚当·斯皮拉科夫斯基，今天我要讲的主题是并列句的依存结构。众所周知，不同的理论和语料库方法对依存结构有不同的假设。例如，在普遍依存理论中，丽莎、巴特和玛姬的并列结构是这样的，第一个并列成分是整个并列结构的中心词，在这个例子中是丽莎。伊戈尔·米尔丘克的意义文本理论也采用了类似的方法，同样认为整个并列结构以第一个并列成分为中心。因此，这两种方法都是不对称的，对吧？它们只选取了一个并列成分。现在也有对称的方法来处理并列结构，例如布拉格方法，以及布拉格依存树库中假设的以连词为中心的并列结构，其中并列结构以连词为中心。因此，我们从所有并列成分得到依存关系。最后，还有多头方法，例如卡特森的词法语法中，可以说所有的并列成分都是并列结构的中心词，因此我们从支配词这里得到分别指向所有并列成分的依存关系，这些是巴特和玛姬。现在，这篇论文的目标是为像这两个例子中的对称并列结构提供一个新的论据，反对像这两个例子中的不对称并列结构。好的，这个论据是基于依存长度最小化原则，我将通过这些例子来解释。所以，在英语中，如你所知，直接宾语倾向于靠近动词，而附属成分可能更远，对吧？所以，March read it yesterday 是正确的，因为直接宾语 it 靠近动词，而 March read yesterday it 则差多了，对吧？因为这里动词和直接宾语之间有一个附属成分 yesterday。然而，当直接宾语非常长时，这种效果可能会得到缓解，因为这时可以直接宾语移到附属成分之后的位置。这里有例子说明。所以，这两个句子都很好。March read this absolutely fascinating book about the bees yesterday 是可以的，其中用 this long NP 替代了 it。说 March read yesterday this absolutely fascinating book about bees 也是可以的。所以，这里的推理是，这是可能的，因为即使这个句子违反了直接宾语应该紧靠动词的一般语法原则，但它满足了依存长度最小化原则，即倾向于较短的依存关系。这两个树只显示了关键依存关系的长度，所以这些不是这两个结构中恒定的依存关系。所以，这里有一个从 red 到长度为7的附属成分的依存关系，用词数衡量，以及从 red 到 book 的长度为4的依存关系，所以总共是11。当你移动，当你交换这两个成分时，这两个依存关系的总和变为6，对吗？所以从11变为6，更短，这就是为什么这听起来相当好的原因，对吧？它违反了一个原则，但满足了另一个原则。好的，所以我们做了什么，我们从增强版的笔树库中提取了关于并列的各种统计数据，并查看了为什么我们没有使用普遍依存理论。这些统计数据证实了之前多次观察到的现象，即左并列成分往往较短。所以，salt and pepper，而不是 pepper and salt，用音节衡量。还有，在路过时观察到的，这种倾向随着长度差的增加而增加。所以，当两个并列成分的长度差增加时，较短的并列成分更倾向于成为第一个，对吧？所以，左短并列成分的比例更大。但是，这篇论文的新颖之处在于，我们观察到这种倾向只有在支配词在左边或不存在时才会发生，对吧？所以，在这个例子中，支配词在左边。我看到了巴特和丽莎。所以，支配词在左边。在第二个例子中，荷马来了，打了个喷嚏，这里有两个动词的并列，没有外部支配词，对吧？所以，在这种情况下，左并列成分更倾向于较短，尤其是两个并列成分之间的差异越大。然而，当支配词在右边时，如图 ted 和 net 的并列，这个效应就不见了。所以我们通过测量字符长度（第一列）、音节（中间列）和词（右列）来展示这一点。所以，我将重点关注右边的。我们在这里看到，当支配词在左边时，左并列成分较短的倾向随着词的绝对差的增加而稳步增长，当没有支配词时，如图句子并列，也会观察到同样的情况。但是，当支配词在右边时，这种倾向就不见了，我们在论文中展示了这一点如何为对称并列结构提供了论据，反对像这两个例子中的不对称并列结构。所以，请查看论文以获取完整的协议和论据，并在会议结束后与我们讨论。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫Kyo Yin，我将为大家介绍我们的研究成果《何时翻译需要上下文？数据驱动的多语言探索》。这项研究是与Patrick Fernandes、Emily Liu、Andre FD Martins和Graham Newbig合作完成的。很多翻译都依赖于上下文。例如，我们如何翻译这句话中的“mole”？如果前一句是“如果部长们知道了，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，这可能有什么严重的问题吗？”那么“mole”指的是胎记。因此，根据上下文，单词的含义会发生变化，因此其翻译也会随之改变。然而，评估模型在处理这类情况时的表现非常困难。首先，因为只有小部分翻译依赖于上下文，这使得像Blue这样的语料库级指标无法捕捉这些翻译。有些人建议对依赖上下文的翻译进行有针对性的评估，但这些资源只支持有限类型的依赖上下文的翻译和有限的语言集合，因为它们通常依赖于领域知识和人工整理。在这项工作中，我们试图回答这两个问题。首先，何时翻译需要上下文？其次，模型在处理这些情况时的表现如何？为了回答第一个问题，我们首先测量了单词在翻译过程中对上下文的依赖程度。在之前的研究中，我们引入了CXMI作为机器翻译模型上下文使用的度量。CXMI通过测量上下文C在给定源X的情况下对目标Y提供的信息来实现。你可以将CXMI视为给模型提供上下文所获得的信息。在这项工作中，我们将CXMI扩展为逐点CXMI，可以测量句子级或词级上下文的使用。我们可以将PCXMI高的词视为需要上下文进行翻译的词。现在，我们分析PCXMI高的词，寻找这些词之间的模式。我们对从英语翻译成十四种不同语言的TED演讲的转录本进行了分析。我们在三个不同的层面上进行分析。首先，我们查看了具有高平均PCXMI的词性标注。这使我们能够找到例如，阿拉伯语中的双重代词具有相对高的p six mi。这可以解释为英语没有双重代词。因此，你需要上下文来确定翻译成阿拉伯语时代词是否是双重。同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看了在所有不同出现情况下具有高p six mi的词汇项目。这有助于我们识别出像这里的情况，即在中文中你需要上下文才能正确翻译。最后，我们发现上下文对于以正确的正式程度进行翻译很重要。最后，我们查看了具有高p6mi的不同个体标记。这使我们能够识别出无法真正由词本身捕捉到的现象，而是通过句子结构表达的，例如省略的解析。现在，我们使用分析结果来设计一个文档级翻译的基准。对于我们确定的五个语篇现象，我们创建了标记器来自动识别与现象相关的词，我们称我们的标记器为多语言语篇感知或MUDA标记器。我们还可以注意到，不同语言具有不同比例的这些语篇现象。然后，我们使用MUDA标记器，通过在我们将用于评估的平行语料库上应用标记器，并在MUDA标记器已经识别的依赖上下文的例子上应用我们选择的翻译指标。最后，我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译上的表现。首先，当我们使用语料库级指标时，对于Blue，我们发现不依赖上下文的模型表现最好，但如果我们使用comet，依赖上下文的模型表现最好。如果我们使用WordF度量，那么有无上下文的模型表现相当。这再次表明，仅使用语料库级指标就很难确定最佳的文档级翻译系统。现在，我们使用MUDA基准来评估模型，发现依赖上下文级别的模型对于某些语篇现象，如正式程度和词汇连贯性，比不使用上下文的模型准确得多。但这些模型在其他现象，如省略、代词和动词形式上并不比不使用上下文的模型好多少。因此，这表明我们需要在文档级翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准表明DPL在文档级翻译中通常比Google Translate更准确。总结一下，我们对十四对语言对进行了数据驱动的分析，以确定何时翻译需要上下文。然后，我们使用我们的发现建立了一个文档级机器翻译的基准，这可以帮助我们确定模型可以很好地处理哪些离散现象，以及哪些翻译系统擅长文档级翻译。非常感谢大家的关注。明天见。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基梅隆大学的一名一年级博士生，今天我将为大家介绍你们的作品《分析位置、特征设计偏见、Beta集合和模型》。这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的，其中包括塞巴斯蒂安·桑蒂、罗宁·莱布拉斯、卡塔里娜·赖尼克和马丁·萨普。让我们先想象一下，你为一家报纸工作，正在筛选新闻文章下的评论，试图删除有毒内容。你可能会使用一个流行的API，如Perspective API进行有毒性检测，如果你是卡尔·琼斯，这个方法效果很好，因为Perspective API能够正确检测出有毒实例。但对于迪西亚·夏尔马来说，情况就不一样了，因为Perspective API对印度语境中更常见的攻击性用词并不敏感。这是一个设计偏见的例子，我们在这里看到技术在不同人群之间的系统性性能差异。我们之前看到的这种设计偏见可能是由于NLP研究人员和模型开发人员的定位性造成的。定位性是指人们由于其人口统计特征、身份和生活经历而持有的观点。这是一个在批判性研究中广泛使用的概念，特别是在女权主义和酷儿学术领域。作为研究人员，定位性可以影响研究过程及其结果和结论，因为它可以改变研究人员做出的决策。因此，人们可能会问，数据集和模型是否有定位性？我们并不是说模型本身和数据集本身具有人口统计特征和生活经历，但它们确实汇集了真实的人们的判断和观点，因此可以代表某些定位性优于其他定位性。因此，之前的研究提出了关于定位性的轶事证据，例如模型和数据集中的文化差距，以及模型定位性的理论定义。然而，这些研究并没有真正比较最终用户与数据集和模型本身。随着NLP任务变得更加主观和社会导向，研究模型和数据集定位性变得越来越重要。要描述这些定位性的偏差是很困难的，因为并非所有决策都被记录下来，许多模型隐藏在API后面。因此，为了研究数据集和模型定位性，我们实际上是将注释与真实用户与现有数据集和模型进行比较。我们通过我们的框架NLPositionality来实现这一点。我们的框架分为两个主要步骤。第一步是用多样化的注释者重新注释数据集。我们选择这样做，而不是查看原始数据集、注释者的人口统计数据，因为通常只有少数注释者对每个实例进行注释，而且人口统计数据很少被收集和分享。因此，我们选择重新注释数据，以便每个实例有多个注释者，并获得丰富的社会人口数据。然后，我们将根据社会人口数据进行注释，并使用Parsons R相关系数将其与模型和数据集进行比较。因此，我们的框架实际上与注释者意见不一致的文献不同，因为它比较了最终用户与模型和数据集的预测和标签，而不是仅仅关注注释者的一致性或注释者分布的建模。我们的框架在很大程度上得益于Lab in the Wild是一个在线实验平台，我们可以招募到多样化的志愿者，相比之下，像MTurk这样的平台主要有来自美国或印度的参与者。此外，Lab in the Wild仍然能够获得高质量的数据。我们在Lab in the Wild上举办了两项任务，其中一项是社会可接受性。它的工作原理是，参与者将阅读社会化学数据集中的一个情境，然后写出这个情境在社会上是多么可接受。之后，为了保持参与研究，他们可以将自己的回答与AI和其他人的回答进行比较。然后，我们将这些注释与GPT 4中的社会化学德尔菲进行比较。然后，我们为有毒性和仇恨言论检测任务复制了一个非常相似的设置，参与者将阅读DynaHate中的一个实例，并写出他们是否认为这是一个仇恨言论的实例。然后，我们将这些注释与DynaHate、Perspective API、Rewire API、Hate Roberta在GPT 4中进行比较。我们的研究最终积累了来自八十七个国家的超过一千名注释者的十六万多个注释。所以，我们现在更有能力回答谁与NLP数据集和模型最符合？我们发现NLP中存在定位性。例如，我们发现数据集和模型最符合英语国家的用户。因此，对于GPD 4社会可接受性分析，我们发现它最符合儒家和英语国家的用户。我们发现DanaHate也最符合英语国家的用户。我们还发现与受过大学教育的人群的额外对齐。因此，对于GPD 4的社会可接受性任务，我们发现它最符合受过大学教育或研究生教育的人群。我们发现DanaHate也最符合受过大学教育的人群。然而，当模型和数据集与特定人群对齐时，有些人不可避免地会被抛在后面。一个例子是，数据集和模型与非二元人群的对齐程度低于男性和女性人群。我们在GPT 4社会可接受性任务和DynaHate任务分析中都发现了这一点。因此，鉴于NLP中存在定位性，我们能对此做些什么？我们对此有几个建议。首先，在整个研究过程中记录所有相关的设计选择。另一个建议是通过视角主义的视角进行NLP研究。我们的第三个建议是在四个特定社区内构建专门的数据集和模型。一个很好的例子是Masakane倡议。我们想强调，包容性的NLP不仅仅是让所有技术为所有人工作。这就是我们的介绍结束。但如果你想了解更多，请随时查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我将谈谈我们在解决间接指称表达以进行实体选择方面的工作，我们在此过程中引入了altentity scorpus。我的名字是Javot Hosseini，这是一项与Philip Radlinsky、Silvia Pareti和Annie Luis合作完成的工作。我们的目标是理解用户在做出选择时使用的语言。请考虑以下替代问题。你是指easy on me还是I got a feeling？这里用户想要在这两个标志之间进行选择。最明显的方法是使用直接引用，例如说出easy on me这首歌的名字或它的位置第一首，但有时间接引用更合适，可以进行更自然的对话。这种情况可能发生在用户记不起歌名时，或者发音过于相似，难以区分，或者当用户想要指定偏好时。以下是间接引用的几个例子。例如，较新的一个或不是充满活力的歌曲。这是对话系统中的一个重要问题，也是用于基准测试LLM实体理解的一个重要问题。我们没有发现一个公共数据集，一个针对该任务的大规模公共数据集，所以我们使用众包标注来收集一个。我们的数据集涵盖了三个不同的领域，音乐、书籍和食谱。我们的数据集合方法强调非正式性，使用卡通完成设置。卡通有三个对话气泡。在第一个气泡中，鲍勃说，还记得我们昨天听的那首歌吗？然后鲍勃设置了对话上下文。在第二个对话气泡中，爱丽丝说，你是指easy on me还是I got a feeling？这是一个替代问题。在第三个对话气泡中，鲍勃使用间接引用来选择这些实体中的一个，例如，Neo-Ervandal。我们自动提供第一个和第二个对话气泡，但第三个由标注者填写。第一个对话气泡是从每个领域的一些手动提示中选出的。第二个对话气泡是替代问题，生成如下。我们总是使用一个简单的模板。你是指A还是B？其中A和B是从维基百科中抽样的。以下是我们使用过的不同抽样方法。当我们在列表中向上移动时，实体变得越来越相似，通常更难进行消歧。第一个是随机均匀的，第二个是当实体有相似的标题时，例如两本书的名字相同，它们会返回第三个是当它们在维基百科上有相似的描述，最后当它们在维基百科上有相似的信息框或属性时，例如同一类型或同一艺术家，对于一首歌曲，当我们向标注者展示这个替代问题时。他们知道这些实体的名字，但他们不一定会了解实体。所以我们所做的是展示这两个实体的一些背景知识。对于歌曲，我们简单地展示每个歌曲的Google搜索链接，然后要求标注者至少听一些每首歌曲，并阅读每首歌曲的介绍。例如，以下是easy on me这首歌的Google搜索结果。对于食谱和书籍领域，我们展示了一些来自维基百科的背景文本。对于食谱，我们还从维基百科再次展示了它们的图片，以便标注者知道它们的样子。然后我们要求标注者选择其中一个实体，例如这里的第一个，并使用三到五个间接指称表达来描述它们。例如，钢琴音乐的那个，这里是一些来自我们数据集的例子，例如没有歌词的，不是有12岁男孩的那个，也不是虚构的，或者来自阿塞拜疆等等。altentity corpus在三个领域中有6,000个替代问题，有42,000个间接指称表达。以下是使用T5xLarge模型的结果总结。如果语言模型可以访问与标注者完全相同的背景知识，那么准确率非常高。大约在92-95%。但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实，例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率只有60%。所以还有很大的改进空间。我们还表明模型具有领域泛化能力。以下是我们的数据集的链接。谢谢。"}
