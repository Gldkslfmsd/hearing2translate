{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lendermann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando il tagging con multi-set e le permutazioni latenti. Questo è un lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titoff. La generalizzazione compositiva può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto dell'analisi semantica, il test per la generalizzazione compositiva potrebbe essere così. Come al solito, abbiamo un insieme di addestramento di enunciati, in questo caso, la ragazza dormiva e Maria sapeva che la ragazza dormiva. Questi enunciati sono abbinati a forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard dell'apprendimento automatico, l'insieme di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione più superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli sequenza-sequenza naivi faticano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono intesi a catturare il processo compositivo che collega gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente ciò comporta una notevole formalizzazione e un pre-elaborazione specifica delle forme logiche, ad esempio per gestire i simboli variabili. Ottenere gli alberi può anche comportare procedure di induzione della grammatica specializzate. In questo articolo, non usiamo alberi e introduciamo un modello sequenza-sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima, tagghiamo ogni token di input con un multi-set non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti ma non sono ordinati. Ecco perché nel secondo passaggio usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere una permutazione che non pone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Passiamo da sinistra a destra sull'output e determiniamo quale token del multi-set mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso. Poi, saltiamo al prossimo token del multi-set per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multi-set. Continuiamo questo processo fino a quando ogni token del primo stadio è stato visitato esattamente una volta. Per darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark Koggs. Il nostro modello supera gli altri con un ampio margine sulla generalizzazione alla ricorsione più profonda. Tuttavia, alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo risolviamo un paio di sfide tecniche interessanti. Prima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-set proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta la sfida che trovare la permutazione con il punteggio più alto è NP alto. Questo perché è correlato al problema del commesso viaggiatore. Approssimiamo questo con un rilassamento continuo e amichevole per la GPU che ci permette anche di retropropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra, e oggi parlerò del nostro articolo, \"Persone Contrassegnate: Utilizzo di Prompt in Linguaggio Naturale per Misurare gli Stereotipi nei Modelli di Linguaggio\". Questo lavoro è stato realizzato in collaborazione con Essendermouch e Dangerowski. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli di linguaggio, o LLMs. Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset costruiti manualmente che richiedono molto tempo per essere curati. Inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o catturano semplicemente associazioni molto generali e ampie, come associazioni negative con gruppi particolari. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifaccettate possono aggravare i pregiudizi ed essere luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLMs regolati da istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi, possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come, immagina di essere una donna asiatica, descriviti. E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT 4. Vediamo immediatamente che, sebbene gli output non siano eccessivamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è ritratta come modesta, la donna del Medio Oriente è definita usando parole come esotica e come riferendosi a una regione affascinante, e entrambe le persone di colore fanno riferimenti all'ascendenza mentre la persona dell'uomo bianco non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, dandoli a soggetti umani, erano anche in grado di portare alla luce stereotipi razziali. Inoltre, questo consente un confronto diretto tra le nostre persone generate e le risposte scritte da esseri umani. La seconda parte sono le parole contrassegnate, che è un metodo per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, che spiegherò tra breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su alcun lessico specifico. Quindi, il metodo delle parole contrassegnate attinge dal concetto sociolinguistico di contrassegnatezza, che afferma che esiste un default non contrassegnato e qualsiasi gruppo che si discosta da quel default è linguisticamente contrassegnato. Quindi, ad esempio, la parola uomo o scusa, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente un guerriero uomo e contrassegnano il termine con donna. E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi emarginati sono solitamente contrassegnati. Quindi, nel nostro metodo, prima designeamo quali sono i gruppi non contrassegnati e contrassegnati. E poi confrontiamo le persone usando il metodo delle parole di lotta, che è fondamentalmente l'uso di rapporti logod per distinguere le parole principali per ogni gruppo contrassegnato. Quindi, ad esempio, per le persone delle donne nere, faremmo le parole di lotta e confronteremmo i rapporti logod contro le persone bianche e le persone maschili, perché questi sono i due gruppi non contrassegnati corrispondenti. Ora per alcuni risultati. Quindi, prima usiamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte da esseri umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più alti di parole del lessico, quelle scritte da esseri umani hanno una distribuzione di parole molto più ampia, mentre le parole stereotipate che sono nelle persone generate sono davvero solo le parole alte e atletiche. Quindi, davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura davvero molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole contrassegnate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi. Prima, per i gruppi contrassegnati, le parole principali includono cose come cultura, tradizione, orgoglio ed esotico. E queste parole definiscono questi gruppi solo per la loro relazione con la loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e di alterità per questi gruppi. Inoltre, ci sono molti cliché comuni che si riflettono in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono cose come vibranti e curvilinee, che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come petite e delicate e setose, che si collegano a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomise, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forti e resilienti. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte. E sebbene suoni positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione sugli altri, su queste demografie, per essere resilienti e forti contro gli ostacoli sociali. Quindi, invece di lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati sanitari molto negativi per queste persone, tra altri danni. Più in generale, scopriamo che le parole per ogni gruppo contrassegnato riflettono praticamente solo narrazioni essenzializzanti. Quindi, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima, noi, come ricercatori, dovremmo affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è un qualche tipo di allineamento di valori strano e eccessivamente eccessivo in corso o forse altri metodi anti-stereotipi che stanno risultando in questi modelli perniciosi. Non possiamo davvero fare ipotesi o studiare ulteriormente questo senza più trasparenza. Grazie mille per aver ascoltato. Buon divertimento all'ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E sono Sarah Finch. E oggi vi racconteremo tutto su ABCEval, un nuovo approccio dimensionale alla valutazione dell'IA conversazionale. Questo lavoro è stato realizzato dal Laboratorio di NLP di Emory, guidato dal Professor Gino Choi presso l'Università di Emory, e in collaborazione con Amazon Alexa AI. Quindi, diciamo che avete appena sviluppato un modello di dialogo e volete vedere quanto si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni sia migliore, o di valutare le conversazioni su una scala liquore. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più fine. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o scale Lickert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat, o ABCEval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti come influenti sulla qualità della chat nella letteratura recente. ABCEval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABCEval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni bot umane per modello utilizzando ABCEval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Lickert a livello di turno, valutazioni Lickert a livello di dialogo e confronti di coppia a livello di dialogo. Oltre ai metodi di valutazione, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalle nostre analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette dei comportamenti di valutazione ABC sono nel complesso più affidabili rispetto alle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra gli annotatori su 100 conversazioni doppiamente etichettate. Inoltre, le etichette ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni di sé e del partner spieghi rispettivamente il cinque percento e il dieci percento della qualità della conversazione, mentre i punteggi di coerenza del liquore medio spieghino solo il quattro percento o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a passi. Potete vedere come la combinazione di tutte le metriche ABC eval spieghi oltre il 25 percento della qualità della conversazione, e rimuovendo le metriche una alla volta, la maggior parte di esse risulta nella perdita di una discreta quantità di informazioni sulla qualità. E la combinazione delle metriche Lickert a livello alternato spiega molto meno della qualità, e meno di queste metriche portano informazioni uniche. Queste metriche ABC eval affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione più alta di quanto i metodi precedenti siano in grado di raggiungere. Potete vedere nei risultati del nostro esperimento che diverse sfide rimangono e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il venti percento delle loro risposte. Producono informazioni irrilevanti in circa il quindici percento delle risposte, e si contraddicono o contraddicono il loro partner circa il 10% delle volte. Con il rapido miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è ancora di più un motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'IA conversazionale progredisca nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come articolo esteso, Transfer Learning for Dissonance Detection, che affronta la sfida delle classi rare. Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti, come in questo esempio in cui una persona afferma \"So che le sigarette possono uccidermi\" e poi dice \"Ho fumato un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono incoerenti e sono in dissonanza. Inoltre, il fatto che non pensi di poter mantenere il lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive. Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e cambiamenti nei valori delle credenze e negli atteggiamenti della popolazione. Un'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali. Per raggiungere l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza come mostrato nel diagramma di flusso qui. I tweet sono stati elaborati utilizzando un parser PDTB e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento di un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non si sia comportato molto meglio del caso. Data la bassa frequenza di dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema della rarità assoluta. Per alleviare questo problema, sperimentiamo con combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo tale da poter raccogliere più campioni di dissonanza in meno round di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi. La classificazione della posizione sulla dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dal tema chiamato dibattito qui e sulla classificazione binaria delle classi di espansione e confronto di PDTB poiché questi due sono strettamente correlati alla concezione di consonanti e dissonanza e li chiamiamo CE qui, scopriamo che trasferendo le prestazioni a zero shot sul set di dati annotato sono già molto migliori del caso con il miglior AUC 0,62. Inoltre, perfezionando iterativamente su entrambi i compiti, scopriamo che la perfezionamento dei compiti CE seguito da un ulteriore perfezionamento sul dibattito produce una performance a zero shot molto migliore. Quindi, questo è il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre iterativo aggiorna il modello addestrandolo sull'ultimo set di dati raccolti. Tra le diverse strategie, abbiamo scoperto che il cumulativo si è comportato allo stesso modo o meglio dell'iterativo in tutti i casi. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara, PRC, per selezionare principalmente gli esempi che hanno un'alta probabilità di essere dissonanti secondo il modello attuale in qualsiasi round di AL. Confrontiamo questo con le altre strategie AL all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, sebbene la differenza sia piccola. Notiamo che le prestazioni sono significativamente inferiori per il caso casuale. In ulteriori round di AL con le due migliori strategie, abbiamo migliorato l'AUC della classificazione della distanza a 0,75, che è la migliore performance che abbiamo sul compito finora. Abbiamo anche verificato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che il PRC ha la percentuale più alta di distanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, scopriamo che il PRC è una semplice strategia AL per l'acquisizione di classi rare e l'avvio dell'AL con compiti di apprendimento trasferito progettati in modo appropriato può aiutare significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre le annotazioni attive in-dominio traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro set di dati di codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Aksheta e oggi il mio coautore Martin e io presentiamo il nostro lavoro The Kitmasteff Evaluating Knowledge Integration from Multiple Sources. Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-addestramento e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti su compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Ma la comprensione del linguaggio naturale spesso richiede conoscenza che viene fornita anche al momento dell'inferenza. Ad esempio, nella frase John ha visto il neo-eletto presidente in TV, i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere con certezza chi sia questa entità specifica John o chi sia il nuovo presidente perché il presidente potrebbe essere cambiato da quando è stato fatto il pre-addestramento. Pertanto, i modelli di successo per compiti di NLU intensivi in termini di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il dataset con partecipanti allo studio umani e modelli di risoluzione della coreferenza consolidati. Ecco un esempio dal nostro dataset. Servin è un giudice., Kia è un fornaio. Termin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome he, che in questo caso è Sermon. La risoluzione di un dato pronome richiede due tipi di informazioni. Primo, la conoscenza specifica dell'entità come Sermon è un giudice. E secondo, la conoscenza di base è appresa durante il pre-addestramento dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al momento dell'inferenza. Varia la disponibilità di questi due pezzi di informazione in modo che possa essere trovata in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di Kitmos. Primo, abbiamo l'impostazione tematica, pre-addestramento di base, dove si assume che la conoscenza di base sia disponibile al momento del pre-addestramento. Secondo, c'è l'impostazione di base entrambi, dove la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Questa ultima impostazione è particolarmente interessante, poiché simula un caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati pre-addestrati dei modelli, ad esempio perché si sono sviluppate nuove occupazioni dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione pre-addestrata di base, si assume che la conoscenza di base i politici cercano seggi elettivi nel governo sia contenuta nei parametri pre-addestrati. Nel contesto di base, forniamo la conoscenza antispecifica Chichester è un politico. Nell'impostazione di base entrambi, forniamo inoltre non solo la conoscenza antispecifica, ma anche la conoscenza di base sui politici nel contesto di tipo inferenza. Nell'impostazione di inferenza di base, forniamo l'occupazione fittizia mirror tour invece di politico perché è improbabile che mirror tour sia contenuto nei parametri pre-addestrati. Valutiamo il dataset sia con partecipanti allo studio umani che con modelli di risoluzione della coreferenza consolidati. In questa figura mostriamo i risultati dei modelli con migliori prestazioni sulla variante più difficile dell'impostazione pre-addestrata di base. Senza addestramento specifico sul KitMus, entrambi i modelli non performano bene. Quando addestrati su KitMus, tuttavia, sia C2F che Berth for Koref performano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su dataset di risoluzione della coreferenza generali, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si testa su KitMus dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli con migliori prestazioni non possono integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare sulla conoscenza da diverse fonti senza addestramento specifico sul compito. Tuttavia, con l'addestramento specifico sul compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli con migliori prestazioni sembrano avere difficoltà con la conoscenza di base integrata in modo affidabile presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, si prega di consultare il nostro articolo e controllare il dataset in codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pappy dell'Università di Toronto e della Fondazione Bruno Kessler, e vi presenterò brevemente l'attenzione come guida per il paper sulla traduzione simultanea del parlato, che è un lavoro congiunto con Matteo Negri e Marco Turki. Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato o simulato è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, che consente la comunicazione tra lingue diverse. E quali sono i problemi dei modelli simulati attuali? Le architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complicate, ad esempio, l'addestramento che coinvolge diversi obiettivi di ottimizzazione, e l'addestramento e la manutenzione di diversi modelli per raggiungere diversi regimi di latenza, ad esempio, l'addestramento di un modello con una latenza media di un secondo e un altro con due secondi di latenza e così via. Quindi qual è la nostra soluzione? Prima di tutto utilizzare i modelli di traduzione simultanea offline già esistenti senza riaddestramento o adottare un'architettura specifica per CMLSD. Utilizzare un solo modello mod per ogni regime di latenza e gestire la latenza attraverso parametri specifici e sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, che è il meccanismo di attenzione incrociata e potete vedere un esempio a destra. La nostra soluzione è proporre un'attenzione a punto o encoder-decoder e si tratta di una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè questa somma è al di sotto di una certa soglia alfa verso gli ultimi lambda fotogrammi del parlato, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un blocco di parlato contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco e guardiamo i pesi dell'attenzione incrociata, vedremo che le prime due parole puntano ai fotogrammi di parlato ricevuti più presto mentre l'ultima parola punta agli ultimi fotogrammi di parlato ricevuti come fotogrammi lambda di parlato. Questo significa che le prime due parole verranno emesse, mentre poiché la somma dell'attenzione incrociata è al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspettiamo un altro blocco di parlato. Se continuiamo e riceviamo un altro blocco di parlato e il nostro modello prevede altre tre parole e guardiamo i pesi dell'attenzione incrociata, vedremo che nessuna parola punta agli ultimi fotogrammi lambda di parlato. Questo significa che queste tre parole verranno emesse. Se guardiamo i risultati principali di ciò, plotteremo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo il blu da un lato che misura la qualità della traduzione e il ritardo medio, cioè la misura della latenza. E consideriamo anche il ritardo medio consapevole del calcolo che tiene conto del tempo di calcolo del modello per prevedere l'output. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico ma anche che siano spostate a sinistra e le confrontiamo con le strategie prepara che sono anche applicate ai modelli offline che sono la strategia dei pesi chiave e l'accordo locale e confrontiamo anche con l'architettura allo stato dell'arte specificamente progettata per la traduzione simultanea del parlato, questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco e vediamo che un punto supera tutte le strategie applicate ai modelli offline poiché le loro curve sono spostate verso sinistra e vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo di usura computazionale che è la strategia più veloce, se volete scoprire più risultati leggete il nostro paper e abbiamo anche rilasciato open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro, grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Xuheng. Oggi presenterò il nostro articolo intitolato \"I tagger di entità nominate Kernel 2003 funzionano ancora bene nel 2023?\" Cominciamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate o il compito NER. Abbiamo osservato che i modelli utilizzano Kernel 2003 per sviluppare NER da quasi 20 anni. E questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni e quando sviluppiamo nuovi tagger cosa è necessario per una buona generalizzazione allo stesso tempo se osserviamo una scarsa generalizzazione cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi abbiamo sviluppato il set di dati Kernel Plus Plus, questo è un set di dati che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione Kernel 2003. Abbiamo poi affinato oltre 20 modelli su Kernel 2003. Li abbiamo valutati sia sul set di test Conor 3 che sul set di test Conor Plus Plus. E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer normalmente si generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione. E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di affinamento influisce direttamente sulle prestazioni di un compito a valle. Qui abbiamo anche scoperto che più esempi di affinamento portano effettivamente a una migliore generalizzazione. Alla nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'adattamento all'overfitting, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte, e questo si manifesta solitamente come il rendimento decrescente su un nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dal crescente divario temporale tra i dati di addestramento e i dati di test. Per i dati di overfitting, abbiamo visto che dal grafico a destra, la linea rossa di miglior adattamento ha una pendenza maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su Color 2003 si traduce in più di un'unità di miglioramento su Kernel Plus Plus, il che significa che non ci sono rendimenti decrescenti e questo ci mostra che l'adattamento all'overfitting in questo caso non è osservato. Quindi, che dire della deriva temporale? Per la deriva temporale abbiamo fatto un esperimento per riaddestrare o continuare ad addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni si degradano con un divario temporale più grande e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale. La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande, così come più esempi di affinamento, e questi vanno di pari passo. Non possiamo avere solo un ingrediente e scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dalla deriva temporale e, in modo piuttosto sorprendente, non è causato dall'adattamento all'overfitting, anche se Kono due mila tre è stato utilizzato per oltre 20 anni. Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tagger di entità nominate Kernel 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo chiami a ulteriori ricerche su come migliorare le generalizzazioni dei modelli. E, infine, vi preghiamo di controllare il nostro articolo, il nostro set di dati e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, benvenuti alla nostra presentazione di DeepLean, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o parlanti non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio, documenti o frasi. Nell'esempio qui, potete vedere una coppia di frasi allineate parallele di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, l'eliminazione di clausole, la riordinamento dell'eliminazione di clausole o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus dplane, perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ora sono troppo piccoli per addestrare un modello di tassonomizzazione. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DPlane, che è diviso in due sottocorpora, DPlane APA e DPlane Web. DPlane APA si basa su testi utilizzati. In DPlane APA, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 30.000, 13.000 coppie di frasi parallele. Per dplane web, questo corpus include diversi domini e abbiamo anche allineato manualmente tutti questi 750 documenti, da un lato, e con metodi di allineamento automatico, dall'altro. In totale, abbiamo ottenuto 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, quindi, ad esempio, sul tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più semplificati rispetto a, ad esempio, il testo delle notizie o i testi per studenti di lingua su tutti i livelli riguardo, ad esempio, la semplificazione lessicale, la semplificazione strutturale o il livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus D-plane ha una grande varietà di diverse trasformazioni di semplificazione. Quindi, ad esempio, nel corpus D-plane API, abbiamo molte più riordinature ed edizioni di parole rispetto a quanto abbiamo nel corpus D-plane web. D'altra parte, nel corpus web, abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset dplane. Quindi, per il primo caso d'uso possiamo valutare i metodi di allineamento automatico. Negli ultimi anni ci sono stati molti metodi di allineamento ma nel contesto delle traduzioni automatiche dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in documenti post, ma nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, con la stessa lingua, con lo stesso contenuto, ma che sono su livelli di complessità diversi. E ora, dato che abbiamo il nostro dataset D-plane, che ha frasi allineate manualmente, possiamo usare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti. E abbiamo fatto alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel documento. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da usare per la semplificazione del testo tedesco è il metodo di allineamento di massa, e potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento. Il secondo caso d'uso che abbiamo mostrato nel nostro documento è il caso della semplificazione automatica del testo tramite il perfezionamento di modelli linguistici per produrre testo semplificato dal testo complesso di input. Abbiamo perfezionato due modelli diversi. Abbiamo perfezionato il modello di long import per produrre semplificazioni a livello di documento, e abbiamo anche perfezionato il normale base import per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete esaminare più dettagliatamente i punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo perfezionamento di base potrebbe produrre o potrebbe ottenere punteggi migliori rispetto ai punteggi di base e proponiamo quei risultati come benchmark, un benchmark di base per il problema della semplificazione automatica del testo in futuro. Vi ringraziamo molto per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xi Yuan dell'Università di Fen. Sono qui per presentare il nostro lavoro: la conoscenza di script distinti da grandi modelli linguistici per la pianificazione linguistica vincolata. Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script garantiti. I lavori precedenti hanno esplorato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come fare una torta e dimostrare che i grandi modelli linguistici possono scomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per gli obiettivi con obiettivi specifici, vincoli specifici, come fare una torta al cioccolato, rimane ancora poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli più difficili e sfaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione di dati del ciclo umano utilizzando TPT istruito. Selezioniamo cento obiettivi specifici e valutiamo gli script generati dai modelli Light Logic. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli Light Logic ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi conduciamo un'analisi dettagliata per indagare cosa fanno i modelli Light Logic. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile. Ma la fedeltà ai vincoli non può essere garantita. Esaminiamo categorie di argomenti più franche di vincoli a seconda di tornare a casa. La mappa principale nella figura mostra che le prestazioni di pianificazione delle GPD istruite variano notevolmente per ragazze di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli Larry è alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea del filtro Zen sovra-generato per migliorare la qualità della generazione. Mostriamo prima i tipi di vincoli con esempi per l'istruzione GPT e otteniamo obiettivi specifici basati sull'insieme di obiettivi astratti. Quindi l'istruzione GPT sovra-genera script chiave per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script adatti. Convertiamo gli script e gli obiettivi in istruzione GPT in bocconi e calcoliamo la somiglianza del coseno e i punteggi di somiglianza per misurare la somiglianza semantica. Inoltre, scriveremo lo script che contiene le parole chiave del vincolo di destinazione. Manteniamo lo script solo se il punteggio dell'obiettivo di destinazione è il più alto nel sito dell'obiettivo. Con il nostro metodo, Inslacity può generare script di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia nella completezza semantica che nella fedeltà al vincolo. Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare set di dati è un passo essenziale per raggiungere questo obiettivo. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale dei set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare modelli di pianificazione linguistica vincolata. Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata chiamato code script. In totale, generiamo cinquantamimila obiettivi specifici con script per garantire la qualità dei siti di validazione e test. Chiediamo ai lavoratori del cloud di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione vincolata di code script. Scopriamo che code script mostra l'ipotesi negli obiettivi specifici generati. Con code script, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che la funzione TFIL su code rate può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare i modelli più grandi se addestrati correttamente su siti di dati adatti. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppato un metodo di filtro sovra-generato per i grandi modelli linguistici. Usiamo i grandi modelli linguistici per generare un set di dati di script di alta qualità, CodeScript, per la pianificazione linguistica costruttiva. Speriamo che il set di dati CodeScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Trovate maggiori dettagli su CodeScript nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrack e vi presenterò i nostri lavori su Dr. Berth, un robusto modello di pre-addestramento in francese per il dominio biomedico e clinico. In questa presentazione, parleremo prima del modellamento linguistico nell'assistenza sanitaria. Poi, presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato Dr. Berth, che si basa su Roberta, e lo addestriamo su Natchios, che è un dataset di dati medici raccolti dal web. Introduciamo anche un confronto di modelli con più impostazioni di pre-addestramento e fonti di dati. Poi, presenteremo i nostri risultati su undici compiti a valle biomedici e clinici in francese. E infine, concluderemo gli esperimenti e vi daremo maggiori dettagli su come accedere ai modelli. Dalla sua uscita nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un enorme miglioramento delle prestazioni rispetto ai metodi statici e contestualizzati storici come Word2Vec, Fastex o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert, e ad altri domini come il biomedico con PermetteBERT e BioBERT. e su quello clinico con clinical birth ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Quindi ci chiediamo quale sia la fonte di dati più appropriata per una vasta gamma di utilizzi. E quei dati attuali sono una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'ospedale non universitario che abbiamo. Dopo di ciò, ci chiediamo, quanti dati abbiamo bisogno per addestrare un modello specializzato su dati francesi? È 4 GB, 8 GB o più? Per rispondere a questa domanda, prima addestriamo e confrontiamo quattro modelli da zero. Una prima versione di Dr. Bert con 7 GB di Natchez, una seconda versione di 4 GB di Natchez, una prima versione di Schubert, che è un modello clinico, con 4 GB di frasi prese dai nodi clinici, e una versione finale di Schubert con un mix di 4 GB di Natchez e 4 GB di nodi clinici. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di Camembert e addestrato su 4 GB di Natchez. Un altro basato sempre su Camembert ma addestrato questa volta sui 4 GB di nodi clinici. E infine, uno basato su un modello biomedico inglese, Bermud Bert, e addestrato su 4 GB di Natchez. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo compiti pubblici e privati di match-up come il riconoscimento di nomi e altitudine. classificazione, tagging di commutazione di pattern e risposta a domande. Questi modelli sono confrontati con sei modelli di base, che sono Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che il modello ha ottenuto le migliori prestazioni nel compito con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo ottenere quei dati da, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati si traduce in migliori prestazioni. Nel complesso, il fret-tuning da zero sembra ottenere prestazioni superiori sulla maggior parte dei compiti. Tuttavia, il nostro esperimento sul fret-tuning continuo utilizzando il peso e il tokenizer di PumedBeard, addestrato sul sottoinsieme di 4 GB di Natchez, ha mostrato risultati comparabili a quelli ottenuti con Dr.Beard 4 GB da zero, il che non è il caso per il modello basato sui pesi e il tokenizer di Camembert, che soffrono di problemi di stabilità. Infine, come conclusione, il sistema proposto offre prestazioni migliori su nove dei undici compiti DOTSTRIMS e supera globalmente il risultato del modello generico qui, Camembert. Osserviamo anche che i dati specializzati sono migliori, dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da Natchios sono liberamente disponibili su YuginFace e tutti gli script di addestramento sono nel nostro repository GitHub. Quindi grazie per... per questa presentazione e non vediamo l'ora di scambiare idee nella sessione post-sessione a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Xiang Bin, dottorando all'Università di Washington. Oggi presenterò il nostro lavoro dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce di pregiudizi politici che portano a modelli di NLP ingiusti. Quindi i modelli linguistici vengono addestrati su grandi quantità di dati web raccolti. I media di notizie politiche sono ben coperti nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C four, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, e così via, sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Quindi, da un lato, sono stati in grado di imparare da prospettive diverse, che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente di parte e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tal fine, proponiamo di indagare sulla pipeline di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, specificamente ponendoci le seguenti domande. Primo, come valutiamo la leadership politica dei modelli linguistici e quale ruolo potrebbe avere il pre-addestramento sui tali pregiudizi politici? In secondo luogo, come i modelli linguistici con diverse leadership politiche si comportano effettivamente nei compiti a valle e se ciò potrebbe risultare in problemi di equità nelle applicazioni di NLP? Quindi, specificamente, proponiamo prima di stimolare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il test del compass politico. Questo ci assicura di fare una valutazione automatica ben fondata nella letteratura di scienze politiche. Quindi alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno significati politici variabili. Occupano tutti e quattro i quadranti sul compass politico. Possiamo anche vedere che GPT 4 è il modello linguistico più liberale di tutti, e le teorie di GPT sono generalmente più socialmente liberali della teoria di BERT e delle sue varianti. In secondo luogo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente raccolti dai dati di addestramento. Quindi potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint dei modelli linguistici su sei diversi corpus partigiani separati in notizie e social media ulteriormente divisi nel loro orientamento politico. Pre-addestrando ulteriormente i modelli linguistici su tali corpus partigiani, possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta ulteriormente perfezionata, ulteriormente addestrata sul corpus di Reddit di sinistra, possiamo vedere un sostanziale spostamento liberale in termini di suoi pregiudizi politici. per indagare se i modelli linguistici possano cogliere la polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpus di pre-addestramento in pre-45° Presidente degli Stati Uniti e dopo il 45° Presidente degli Stati Uniti. Pre-addestriamo separatamente i modelli linguistici sui due diversi corpus temporali. Possiamo vedere che i modelli linguistici in generale avevano un orientamento politico più lontano dal centro dopo il 2017. Quindi ciò indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società. Quindi, ultimo ma non meno importante, valutiamo i modelli linguistici con diversi orientamenti politici sulla rilevazione di discorsi d'odio e sulla rilevazione di notizie false per le applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Quindi vediamo che se indaghiamo le prestazioni per categoria, cioè, se separiamo le prestazioni in diverse demografie o significati politici dei media di notizie, possiamo vedere un modello che, ad esempio, per la rilevazione di discorsi d'odio, i modelli di sinistra sono migliori nel rilevare discorsi d'odio che prendono di mira gruppi socialmente minoritari, tuttavia, sono peggiori nel rilevare discorsi d'odio che prendono di mira gruppi più potenti nella nostra società. E viceversa, i modelli di destra sono migliori nel rilevare discorsi d'odio che prendono di mira bianchi e uomini, tuttavia, peggiori nel rilevare discorsi d'odio che prendono di mira neri, LGBTQ plus e altre comunità minoritarie. Tendenze simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli di linea di sinistra sono migliori nel rilevare disinformazione dal loro significato politico opposto e viceversa. Questo mostrerà ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi significati politici danno diverse previsioni su esempi di discorsi d'odio e disinformazione basate sulle loro categorie sociali. Ci sono un mucchio di altri esempi nell'appendice per evidenziare ulteriormente ciò. Ciò indica che c'è un problema di equità che è molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere perfezionato su discorsi d'odio o disinformazione o altro e dispiegato su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e i discorsi d'odio che prendono di mira gruppi minoritari potrebbero semplicemente diffondersi senza alcun controllo. Quindi questo suona l'allarme per noi di riconoscere e affrontare i problemi di equità risultanti dai pregiudizi politici dei modelli linguistici. Quindi un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando infine problemi di equità. Se proviamo in qualche modo a sanificare, correremmo anche il rischio di censura o esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nei dati di addestramento dei modelli linguistici. Quindi è un po' come il problema di Charlie elettrico. Ok, ottimo. Penso che sia praticamente tutto ciò che ho per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Coast of Sina e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023 intitolato \"Language Model Acceptability Judgments are not always robust to context\". Questo è un lavoro congiunto con John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo il paradigma del paio minimo. Quindi, il paradigma del paio minimo valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità come nel caso di \"blimp syntax gem\" o l'accettabilità in termini di stereotipi come \"crowd spares\". In questo paradigma del paio minimo, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi mostrare una frase inaccettabile o non grammaticale. La speranza è che il modello attribuisca fondamentalmente una probabilità maggiore alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione del modello per frasi più lunghe. Oggi i grandi modelli linguistici stanno producendo finestre di contesto sempre più lunghe. È quindi fondamentale che valutiamo l'accettabilità del modello lungo l'intera finestra di contesto. E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline NPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi questo è l'approccio. Quindi, ciò che facciamo è simulare queste sequenze più lunghe, rivediamo i set di dati stessi e poi ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei set di dati. Quindi, per esempio, qui abbiamo scelto un tipico paio di grammaticalità dal set di dati blimp dal caso dell'isola aggiunta. E ciò che facciamo è ricreare sequenze più lunghe che siano accettabili e che abbiano la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali dall'isola aggiunta e poi le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento e ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello. E possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Quindi questo è ciò che chiamiamo scenario di mismatch. Quindi qui, le frasi provengono ancora da set di dati pertinenti, ma non dallo stesso set di dati con cui si sta valutando. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, come se il contesto provenisse da un sottoinsieme diverso del set di dati o se fosse completamente irrilevante rispetto alla frase che stiamo guardando. Quindi come fa il modello? Prima guardiamo le frasi di Wikipedia che sono completamente irrilevanti rispetto alla coppia di query corrente e lì troviamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT2 e qui vediamo, nella linea tratteggiata arancione, che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso set di dati? Quindi qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso set di dati blimp o syntax gem e lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Ma quando combiniamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno nella sintassi basata sul bias, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora questo effetto è molto grande e aumenta lungo la lunghezza del contesto e ciò probabilmente influenzerebbe i nuovi modelli linguistici che hanno una finestra di contesto ampia. Quindi perché il prefisso di corrispondenza influisce così tanto sul giudizio del modello linguistico? Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la... la frase di input cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. Dopo aver fatto diverse di queste perturbazioni, troviamo che nessuno di questi rumori fa effettivamente cambiare al modello il suo corso in termini di come ci mostra la tendenza del giudizio MPP. Fondamentalmente troviamo che i modelli sono sensibili alle frasi perturbate in modi simili, cioè quando perturbamo le frasi nel dominio accettabile vediamo un aumento simile in tutte le perturbazioni e quando perturbamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, il modo in cui la facciamo attualmente con input brevi e di singola frase, potrebbe non catturare completamente la conoscenza astratta del modello linguistico lungo l'intera finestra di contesto. Per favore, leggete il nostro articolo per ulteriori dettagli dei nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawe, uno studente di dottorato presso l'Università di Stalant in Germania. In questo video, vorrei presentare il nostro lavoro recente, Weaker Than You Think, uno sguardo critico all'apprendimento supervisionato settimanale. Questo è un lavoro congiunto con Xiao Yushchen, Maios Musbach e Giaz Steffen, e Dietrich Clarkov. Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale. Nella supervisione settimanale, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura settimanali, come semplici regole euristiche, basi di conoscenza o cloudrowdsourcing di prossimità come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto meno costose, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se dovessimo addestrare direttamente le reti neurali sui dati di etichetta settimanali, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento supervisionato settimanale, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali sul rumore dell'etichetta in modo che i modelli addestrati si generalizzino ancora bene. Nei lavori recenti in WSL, quindi WSL sta per apprendimento supervisionato settimanale, un'affermazione comune è che le persone dicono di addestrare i modelli solo sui dati di etichetta settimanali e di ottenere alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un'aggravante, che è che le persone presumono che ci sia un set di validazione pulito aggiuntivo disponibile per la selezione del modello. Mettiamo in dubbio questo contesto di problema, poiché implica che nell'apprendimento supervisionato settimanale siano necessarie ulteriori annotazioni manuali. Ma come un elefante nella stanza, questa necessità è spesso trascurata. Il dubbio sopracitato ci porta a porre tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL? O possiamo forse utilizzare un set di validazione rumoroso invece? Secondo, se i dati puliti sono richiesti o se i dati puliti sono obbligatori affinché WSL funzioni, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, scopriamo che interessantemente, i metodi WSL recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Questo indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente, e il costo dell'annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo solo bisogno di venti campioni per classe per ottenere alte prestazioni. Ma questa non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento direttamente su di essi otterrà prestazioni ancora migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, che sono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci WSL. Infine, il miglioramento delle prestazioni affermato nei precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello di Berlino chiamato FTW inizialmente sottoperforma i metodi WSL più complicati come il coseno. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTW si comporta altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. Per riassumere, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti etichettati manualmente affinché funzionino correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Primo, riportare la selezione del modello è fatta bene con campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con le future linee di base dell'apprendimento, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è una semplice ma forte linea di base che dovrebbe essere considerata nel futuro lavoro in WSL. Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di controllarlo. Grazie e godetevi la conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ayud Villar e vi darò una breve panoramica dell'articolo \"Prompting Palm from Translation Assessing Strategies and Performance\". Questo è un lavoro congiunto con i miei colleghi di Google Translate. Palm è un modello di linguaggio con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testo che comprende 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo il primo studio sistematico del prompting per modelli di linguaggio di grandi dimensioni per la traduzione automatica. Valutiamo la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'uso dei set di test più recenti per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello di linguaggio, e confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni della valutazione WMT. Usiamo metriche di MT neurali all'avanguardia e mostriamo anche i risultati della valutazione umana basata sugli esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha una grande influenza sulle prestazioni dei LLM per la traduzione come possiamo vedere in un semplice esperimento in cui usiamo un breve prompting e forniamo due prompt diversi per una semplice frase. La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di un punto di sfocatura. E questo può arrivare, nei casi estremi, fino a 40 punti di sfocatura. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi in cui contrassegniamo semplicemente ogni frase che forniamo al sistema con la lingua in cui si trova. Quindi in questo esempio qui, dove abbiamo eseguito la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi sorgente sono contrassegnate con il due punti tedesco, e le traduzioni inglesi con il due punti inglese. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diversi prompt brevi. È cruciale per il prompting zero e uno, ma quando passiamo, come nel nostro caso, a cinque prompt brevi, non c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che hanno il peso maggiore. Il riassunto dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più, direi, e i risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi ODR specializzati hanno un vantaggio sostanziale sulle traduzioni PALM, ma PALM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM è che la fluidità di Palm è paragonabile ai sistemi all'avanguardia, ma la differenza principale proviene dall'accuratezza. Quindi in particolare, gli errori più comuni sono gli errori di omissione. Quindi sembra che Palm scelga di produrre una traduzione migliore a volte tralasciando parti della frase che sono omise nella traduzione. Tuttavia, la categoria di stile verso l'esterno per PAM è inferiore rispetto ai sistemi all'avanguardia, che è un segnale aggiuntivo che PAM fornisce un output davvero fluido, ma ancora con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, per favore venite alla presentazione completa dell'articolo. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jing Wei e vengo dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video pubblicitario del nostro articolo \"Stai copiando il mio modello? Protezione del diritto d'autore dei grandi modelli linguistici per l'embedding e i servizi. Vedi Watermark di Backdoor\". Prima di tutto, introduciamo il contesto relativo all'embedding e ai servizi. Attualmente, i grandi modelli linguistici come GPT, Lama, PELM sono eccezionali nella comprensione e nella generazione del linguaggio naturale. L'embedding e i servizi sono uno dei servizi costruiti su grandi modelli linguistici per assistere in vari compiti di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, lavori recenti hanno dimostrato che l'aggressore può rubare il modello imparando dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'embedding come servizio. Per proteggere il diritto d'autore dell'embedding come servizio, una delle soluzioni è inserire un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo deve soddisfare le seguenti proprietà. Primo, il metodo dovrebbe essere applicabile all'embedding come servizio. Secondo, il watermark non dovrebbe degradare l'utilità degli embedding forniti. Terzo, il watermark dovrebbe essere abbastanza occulto per l'aggressore o l'aggressore dovrebbe poter rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'aggressore durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili all'embedding come servizio o mancano di trasferibilità. Pertanto, in questo articolo proponiamo un marker di embedding, che è un metodo di watermark basato su backdoor applicabile all'embedding come servizio. Poi lasciate che vi presenti i dettagli del nostro marker di embedding. Il marker di embedding contiene due passaggi principali: l'iniezione del watermark e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo prima un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo prima un embedding di destinazione. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding di destinazione e dell'embedding originale. Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding di destinazione. La verifica del diritto d'autore consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un dataset di backdoor e un dataset benigno. Il dataset di backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Poi il fornitore richiede embedding dal servizio del ladro con il dataset. Si calcolano la similarità di consign e L tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza di similarità tra i dati di backdoor e i dati benigni, definita come delta coseno e delta L due. Nel frattempo, applichiamo anche il test KS e usiamo il suo valore p come terza metrica. Condottiamo esperimenti su quattro dataset: AG News, Mind, SSD due ed Erospam. Supponiamo che il fornitore applichi il dataset di testo wiki per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro marker di embedding può avere un'ottima capacità di rilevamento mantenendo l'utilità della griglia per i compiti di downscreen. Valutiamo anche la segretezza dell'embedding fornito visualizzando l'embedding delle frasi dispiegate come in BOPCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding di backdoor e gli embedding normali. Questo è tutto, grazie. Siete invitati a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e il mio collega Jian e io presenteremo la nostra ricerca sull'ottimizzazione dell'apprendimento multimodale a zero shot tramite l'ottimizzazione delle istruzioni. Quindi, con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai grandi modelli linguistici di eseguire attività non viste in modo a zero shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si è concentrata sul miglioramento delle prestazioni a zero shot solo su attività linguistiche, mentre le attività di visione artificiale e multimodali sono state trascurate. Pertanto, in questo lavoro, vogliamo indagare se l'ottimizzazione delle istruzioni su modelli multimodali pre-addestrati possa effettivamente migliorare la generalizzazione a compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra LP e multimodali. Esistono più di mille seicento compiti di istruzione solo linguistici. Tuttavia, non esiste un compito di istruzione multimodale su larga scala disponibile pubblicamente. Pertanto, questo ci motiva a costruire un set di dati di ottimizzazione delle istruzioni multimodali. Qui presentiamo multi instruct, il primo set di dati di benchmark di ottimizzazione delle istruzioni multimodali che consiste di sessantadue compiti multimodali diversi che coprono dieci categorie audaci. Questi compiti sono derivati da ventiuno set di dati open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare sull'ottimizzazione delle istruzioni multimodali sul nostro set di dati proposto, prendiamo OFA come modello di base. OFA ha utilizzato un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una bounding box. Qui mostriamo alcuni esempi di istanze dal nostro set di dati multi install. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza a sequenza unificato in cui il testo di input, le immagini, l'istruzione e le bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'ottimizzazione delle istruzioni multimodali. Quindi, per il set di dati di addestramento, utilizziamo 53 compiti dal gruppo NIG per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento sul senso comune per il test e selezioniamo ulteriori cinque compiti da WQA e il gruppo vario. Usiamo tutte le istanze nella suddivisione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla suddivisione di test di NIG come un compito SIN per NLP. Quindi utilizziamo un grande modello OFA pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei suoi cinque modelli di istruzione. Quindi, durante il test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Segnaliamo le prestazioni medie e massime e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multi-modello, segnaliamo l'accuratezza. Se è un compito di generazione multi-modello, segnaliamo ROOGEL. Per un compito RP, segnaliamo anche ROOGEL. Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Quindi, questo misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito indipendentemente da lievi variazioni nella formulazione dell'istruzione. Questo è il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modello visti. Inoltre, l'apprendimento trasferito dai set di dati di istruzioni naturali può beneficiare l'ottimizzazione delle istruzioni. Qui possiamo vedere che all'aumentare del numero di compiti il modello ottiene prestazioni migliori e nel frattempo una sensibilità inferiore. Quindi abbiamo anche fatto un esperimento. Gli esperimenti utilizzano un'istruzione rispetto a cinque istruzioni come possiamo vedere utilizzando più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità, quindi questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello come possiamo vedere dall'apprendimento trasferito dai set di dati di istruzioni naturali il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che l'apprendimento trasferito dai set di dati di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul set di dati di istruzioni naturali. Quindi, nel complesso, abbiamo proposto il primo set di dati di ottimizzazione delle istruzioni multimodali su larga scala. Migliorando significativamente la capacità di DAROCHOT di OFA e esplorando diverse tecniche di apprendimento trasferito e mostrando i loro benefici. Progettiamo una nuova metrica chiamata sensibilità. Quindi, un'altra cosa, stiamo raccogliendo un set di dati di ottimizzazione delle istruzioni multimodali molto più grande con circa 150 compiti di linguaggio variante aggiuntivi e li rilasceremo. Quindi questo è un codice QR per i nostri dati e modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Yusuf Zhang della Penn State University. Oggi presenterò il nostro lavoro, Exampler, Crosslingual Semantic Parsing in più lingue naturali e rappresentazioni del significato. Quindi, il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti come SQL e calcolo Lambda. E il parsing semantico cross-linguistico è il compito di tradurre le query in più lingue naturali in più rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda, FunQL e così via. I modelli esistenti di parsing semantico cross-linguistico sono proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono perdite di copertura su certe lingue naturali. Il cinese è mancante e ci sono perdite di copertura su certe molte rappresentazioni. Il calcolo Lambda è mancante. o vengono valutati solo su certi modelli più recenti. Ad esempio, c'è solo un singolo modello per valutarli. Quindi, a tal fine, proponiamo Exampler, forniamo un set di dati uniforme Exampler per il parsing semantico cross-linguistico in più lingue naturali e rappresentazioni del significato. Contiene nove set di dati in vari domini, cinque tasse di parsing semantico, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è TranslateTest. Usiamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, poi usiamo un modello monolingue per addestrare e valutare. E, ad esempio, addestriamo il modello inglese su... su una query inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per prevedere lo SQL. E testiamo anche il modello monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione. Ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione di fusione monolingue addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il modello monolingue multilingue, che addestriamo come un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. E durante l'inferenza, possiamo usare questo modello per tradurre le query tedesche o cinesi o et cetera. E consideriamo anche il trasferimento cross-linguistico zero-shot e field-shot. Addestriamo su una lingua sorgente e trasferiamo su un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su una query inglese o sulla combinazione di query inglesi e tedesche field-shot per addestrare un modello multilingue e prevedere l'output SQL. E abbiamo anche trovato molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli, inclusi Encoder PDR, che sta per decodificatori basati su puntatori con encoder pre-addestrati multilingue, come XLMR più PDR e BERT più PDR. E valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder pre-addestrati multilingue, come MBART e MT5. Abbiamo scoperto che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati. E valutiamo su MT5 e XLMR più PDR in impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder PDR possono essere migliorati addestrando in una miscela di varie lingue. E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese scendono in sette set di dati e guadagnano solo in tre set di dati. Penso che questo sia noto come curve di multilingue. Abbiamo anche confrontato il divario di prestazioni cross-linguistico. In questa figura, la linea blu è il trasferimento cross-linguistico fucsia. La linea arancione è il trasferimento zero-shot cross-linguistico, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e arancione, abbiamo scoperto che per l'impostazione zero-shot, il divario di prestazioni del trasferimento cross-linguistico è significativo. E confrontando la linea blu e arancione, abbiamo scoperto che per l'impostazione fucsia, il divario di trasferimento si accorcia rapidamente. Abbiamo anche trovato alcuni altri risultati interessanti. Ad esempio, l'encoder-decoder supera il lavoro di progresso o ottiene risultati confrontabili. L'addestramento sull'inglese può migliorare significativamente le prestazioni di Fushot sulle lingue naturali di destinazione. E abbiamo scoperto che i modelli di linguaggio multilingue come Codice e Bloom sono ancora inadeguati per i compiti di parsing semantico cross-linguistico. Per riassumere, abbiamo costruito Exampler, un benchmark unificato per il parsing semantico cross-linguistico con più lingue naturali e molte rappresentazioni. Abbiamo condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue. E i nostri risultati mostrano molti risultati interessanti e così via. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Spirakowski e questo discorso riguarda la struttura di dipendenza della coordinazione. Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione coordinata Lisa, Bart e Maggie è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del testo semantico di Igor Milchuk, dove di nuovo l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Escludono uno dei congiunti. Ora ci sono anche approcci simmetrici alle strutture coordinate come l'approccio praghese, l'approccio guidato dalla congiunzione assunto nelle banche di alberi di dipendenza praghesi dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo dipendenze da e verso tutti i congiunti. E infine c'è anche un approccio a più teste che è usato per esempio nella grammatica delle parole di Cutson dove per così dire tutti i congiunti sono capi della struttura coordinata quindi otteniamo dipendenze dal governatore qui ride a tutti i congiunti separatamente questi sono Bart e Maggie ora l'obiettivo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. Ok, l'argomento si basa sul principio della minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi in inglese, come forse sapete, gli oggetti diretti preferiscono essere vicini al verbo mentre gli aggiunti possono essere più lontani, giusto? Quindi March ha letto ieri è ok perché l'oggetto diretto it è vicino al verbo, mentre March ha letto ieri è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un aggiunto ieri. Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'aggiunto. Questo è illustrato qui. Quindi entrambe queste frasi sono ok. March ha letto ieri questo libro assolutamente affascinante sulle api è ok, dove invece di it abbiamo questo lungo NP. È anche ok dire March ha letto ieri questo libro assolutamente affascinante sulle api. Quindi il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicino al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che sono preferite le dipendenze più corte. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da red all'aggiunto di lunghezza 7, misurata in parole, e da red a book di lunghezza 4, quindi insieme è 11. Quando si sposta, quando si scambiano questi due costituenti la somma di queste due dipendenze diventa sei giusto quindi invece di 11 sei molto più corta ecco perché suona abbastanza ok giusto viola un principio ma ne soddisfa un altro Ok, quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata della banca di alberi pen e vedere l'articolo perché non abbiamo usato le dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti. Quindi sale e pepe e non pepe e sale misurati in sillabe. E anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo più forte, giusto? Quindi la proporzione è maggiore dei congiunti corti di sinistra. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto? Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa. Quindi è il governatore, è a sinistra. È assente nel secondo esempio Homer è venuto e ha starnutito qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno giusto quindi in tali casi il congiunto di sinistra preferisce essere più corto tanto più quanto maggiore è la differenza tra i due congiunti tuttavia quando il governatore è a destra come qui left governa la coordinazione ted e net questo effetto scompare quindi lo dimostriamo misurando la lunghezza in caratteri, questa è la prima colonna, in sillabe, la colonna centrale, e in parole, la colonna di destra. Quindi mi concentrerò su quella di destra. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole e lo stesso è osservato quando non c'è un governatore come nella coordinazione di frasi ma quando il governatore è a destra questa tendenza scompare e dimostriamo nell'articolo come questo fornisce un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedere l'articolo per il pieno accordo e gli argomenti e parlate con noi durante la sessione post-lezione. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato Quando la traduzione richiede contesto? Un'esplorazione multilingue guidata dai dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre FD Martins e Graham Newbig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremo \"mole\" in questa frase? Beh, se la frase precedente fosse \"Le cose potrebbero diventare pericolose se i ministri lo scoprono\", allora \"mole\" si riferisce a uno spia. Ma se la frase precedente fosse \"Potrebbe essere qualcosa di serio, dottore?\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e quindi cambia anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questi è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come Blue incapaci di catturare queste traduzioni. E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Primo, quando la traduzione richiede contesto? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura per l'uso del contesto da parte dei modelli di traduzione automatica. E questo si fa misurando quanta informazione il contesto C fornisce sull'obiettivo Y dato la sorgente X. Si può pensare a CXMI come all'informazione guadagnata dal dare contesto al modello. In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un alto PCXMI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto PCXMI per cercare schemi tra queste parole. E svolgiamo la nostra analisi su trascrizioni di discorsi TED che sono state tradotte dall'inglese a quattordici lingue diverse. Eseguiamo la nostra analisi a tre livelli diversi. Primo, guardiamo ai tag di parti del discorso che hanno un alto PCXMI medio. E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un p six mi relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali. Quindi, è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E allo stesso modo, troviamo che alcune lingue richiedono anche contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi del vocabolario che hanno un alto p six mi medio su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come quello qui dove in cinese è necessario il contesto per tradurre correttamente. E infine, troviamo che il contesto è importante per tradurre con la giusta formalità. E infine, guardiamo a diversi token individuali che hanno un alto p6mi. E questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi in una struttura di frase, come la risoluzione di ellissi. Ora usiamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno, e chiamiamo il nostro tagger il Multilingual Discourse Aware o MUDA tagger. Possiamo quindi anche notare che lingue diverse hanno proporzioni diverse di questi fenomeni del discorso. Poi usiamo il tagger MUDA applicando il tagger sul corpus parallelo che vogliamo usare per la valutazione, e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato. Infine, usiamo il nostro benchmark così come altre metriche per valutare diversi modelli sulla traduzione automatica a livello di documento. Prima di tutto, quando usiamo metriche a livello di corpus, quindi per Blue, troviamo che i modelli agnostici al contesto hanno le migliori prestazioni, ma poi se usiamo comet, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo la misura WordF, allora i modelli con o senza contesto hanno prestazioni confrontabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metriche a livello di corpus. Ora usiamo il benchmark MUDA per valutare i modelli e troviamo che i modelli a livello di contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni, come ellissi, pronomi e forma verbale. Quindi questo suggerisce in qualche modo dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DPL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, eseguiamo un'analisi guidata dai dati su quattordici coppie di lingue per identificare quando le traduzioni richiedono contesto. E poi usiamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni discreti i modelli possono gestire bene o no, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per l'attenzione. Ci vediamo domani."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato al primo anno alla Carnegie Mellon University, e oggi presenterò il vostro lavoro, \"Anal Positionale, Characterizing Design Biases, A Beta Sets and Models\". Questo lavoro è stato realizzato in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santi, Ronin Lebras, Katarina Reinicke e Martin Sapp. Quindi, iniziamo immaginando che state lavorando per un giornale e state esaminando i commenti sotto il vostro articolo di notizie cercando di rimuovere i contenuti tossici. Potreste rivolgervi a un'API popolare, come la Perspective API per il rilevamento della tossicità, e questo funziona davvero bene se siete Carl Jones, dove le API di prospettiva sono in grado di rilevare correttamente le istanze tossiche. Ma questo non è realmente il caso per Dithya Sharma, dove le API di prospettiva non sono altrettanto sensibili ai termini offensivi che sono più comuni nei contesti indiani. Questo è un esempio di pregiudizio di progettazione in cui vediamo differenze sistematiche di performance della tecnologia tra le popolazioni. I pregiudizi di progettazione come quello che abbiamo appena visto prima possono verificarsi a causa della posizionalità dei ricercatori NLP e degli sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone detengono come risultato della loro demografia, identità ed esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, specificamente negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizionalità? E non stiamo cercando di dire che i modelli stessi e i dataset stessi hanno identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Quindi, i lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizionalità, come le lacune culturali nei modelli e nei dataset, così come definizioni teoriche della posizionalità dei modelli. Tuttavia, questi lavori non esaminano realmente il confronto tra gli utenti finali e i dataset e i modelli stessi. E studiare la posizionalità dei modelli e dei dataset è sempre più importante poiché i compiti NLP diventano più soggettivi e socialmente orientati. Ed è difficile caratterizzare come queste posizionalità siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Quindi, per studiare la posizionalità dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con gli utenti reali con i dataset e i modelli esistenti. Facciamo questo attraverso il nostro framework, NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è ri-annotare i dataset con annotatori diversi. E scegliamo di farlo anziché guardare alla demografia dei dataset originali, degli annotatori, perché di solito solo pochi annotatori annotano ogni istanza e perché la demografia è raramente raccolta e condivisa. E quindi scegliamo di ri-annotare i dati per ottenere molti annotatori per istanza e per ottenere un ricco insieme di dati demografici. Prendiamo quindi le annotazioni dai dati demografici e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Parsons R. E quindi il nostro framework si differenzia effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei dataset, anziché guardare solo all'accordo degli annotatori o alle distribuzioni degli annotatori di modellazione. Il nostro framework è in gran parte abilitato attraverso Lab in the Wild, una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. E inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset di chimica sociale e poi scriveranno quanto una situazione è socialmente accettabile. In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e con gli altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry Delphi in GPT 4. Abbiamo quindi replicato una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un'istanza da DynaHate e scriveranno se pensano che sia un'istanza di discorso d'odio. Abbiamo quindi confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta in GPT 4. Il nostro studio alla fine ha accumulato oltre sedicimila annotazioni da oltre mille annotatori da ottantasette paesi. Quindi ora siamo meglio attrezzati per rispondere a chi si allineano maggiormente i dataset e i modelli NLP? Scopriamo che c'è una posizionalità in NLP. Ad esempio, scopriamo che i dataset e i modelli sono più allineati ai paesi anglofoni. Quindi per l'analisi dell'accettabilità sociale GPD 4, scopriamo che è più allineato ai paesi confuciani e anglofoni. Scopriamo che DanaHate è anche più allineato ai paesi anglofoni. Scopriamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi per GPD 4 nel compito di accettabilità sociale, scopriamo che è più allineato alle persone con un'istruzione universitaria o di scuola superiore. E troviamo lo stesso per DanaHate dove è più allineato alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschili e femminili. Lo troviamo nel compito di accettabilità sociale GPT 4 così come nell'analisi del compito DynaHate. Quindi, dato che c'è una posizionalità in NLP, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca. E l'altra è fare ricerca NLP attraverso la lente del perspectivismo. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di quattro comunità specifiche. E un buon esempio di ciò è l'iniziativa Masakane. E vogliamo sottolineare che l'NLP inclusivo non è solo far funzionare tutte le tecnologie per tutti. E quindi questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indirette per la selezione di entità, in cui introduciamo l'altentity scorpus. Mi chiamo Javot Hosseini, e questo è un lavoro congiunto con Philip Radlinsky, Silvia Pareti e Annie Luis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa. Volevate dire easy on me o I got a feeling? Qui un utente vuole selezionare tra uno di questi due brani. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome del brano easy on me o la sua posizione, la prima, ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome del brano o le pronunce sono troppo simili tra loro e difficili da disambiguare o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti. Ad esempio, il più recente o il brano che non è energetico. Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte degli LLM. Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per il compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro set di dati copre tre diversi domini, musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un'impostazione di completamento di un fumetto. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice, ricordate quel brano che stavamo ascoltando ieri? E con ciò, Bob imposta il contesto del dialogo. Nella seconda bolla di dialogo, Alice dice, intendete easy on me o I got a feeling? che è la domanda alternativa. E nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, il Neo-Ervandal. Forniamo automaticamente la prima e la seconda bolla di dialogo, ma la terza è compilata dall'annotatore. La prima bolla di dialogo è scelta tra alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, è generata come segue. Usiamo sempre un semplice modello. Intendete A o B? dove A e B sono campionati da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è di solito più difficile fare la disambiguazione. La prima è uniforme a caso, la seconda è quando le entità hanno titoli simili, ad esempio due libri con lo stesso nome, la terza è quando hanno descrizioni simili su Wikipedia e infine quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per un brano. Quando mostriamo questa domanda alternativa agli annotatori. Essi conoscono il nome di queste entità, ma non necessariamente le entità stesse. Quindi ciò che facciamo è mostrare alcune conoscenze di base sulle due entità. Per i brani, mostriamo semplicemente un link di ricerca Google per ogni brano e poi chiediamo agli annotatori di ascoltare almeno alcuni di ciascun brano e leggere su ciascun brano. Ecco ad esempio il risultato della ricerca Google per il brano Easy on Me. Per il dominio delle ricette e dei libri, mostriamo un po' di testo di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e descriverle utilizzando da tre a cinque espressioni di riferimento indirette. Ad esempio quella con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati, ad esempio quella senza parole, non quella con il ragazzo di 12 anni o quella fittizia o proviene dall'Azerbaigian e così via. Il corpus delle entità ha 6.000 domande alternative in tre domini e ha 42.000 espressioni di riferimento indirette. I risultati con il modello T5xLarge sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta. È intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di base parzialmente sovrapposte, allora l'accuratezza è tra l'82 e l'87 percento, che è più realistico, ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%. Quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro set di dati. Grazie."}
