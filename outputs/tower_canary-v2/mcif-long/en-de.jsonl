{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lendermann und heute werde ich Ihnen eine kurze Einführung in unseren Artikel über kompositorische Verallgemeinerung ohne Bäume geben, bei der wir Multi-Set-Tagging und latente Permutationen verwenden. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titoff. Kompositorische Verallgemeinerung kann als die Fähigkeit eines Lernenden verstanden werden, mit tieferer Rekursion und unbekannten Kompositionen von Phrasen umzugehen, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Analyse könnte die Prüfung auf kompositorische Verallgemeinerung so aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall das Mädchen schlief und Maria wusste, dass das Mädchen schlief. Diese Äußerungen sind mit logischen Formen gepaart, die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Maschinelles-Lern-Evaluation stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings tiefere Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive sequenzielle Modelle kämpfen mit dieser Art der Verallgemeinerung außerhalb der Verteilung und produzieren oft Ausgaben, die vom Input losgelöst sind. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie in dem Beispiel farblich kodiert sind. Eine beliebte Methode, um dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein. Typischerweise beinhaltet dies eine erhebliche formale spezifische Vorverarbeitung der logischen Formen, zum Beispiel um Variablen-Symbole zu behandeln. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsprozeduren beinhalten. In diesem Artikel verwenden wir keine Bäume und führen ein neuronales sequenz-zu-sequenz-Modell ein, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir eine starke Verallgemeinerung auf tiefere Rekursion, ohne auf Bäume angewiesen zu sein. Unser Ansatz prognostiziert die Ausgabe aus dem Input in zwei Schritten. Zuerst taggen wir jedes Input-Token mit einem unbestellten Multiset von Token, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir in dem zweiten Schritt ein anderes Modell, um eine Permutation zu prognostizieren, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um eine Permutation zu prognostizieren, die keine harten Einschränkungen auf die möglichen Permutationen legt. Das macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Output und bestimmen, welches Multiset-Token in jeder Position platziert werden soll. Für die erste Output-Position wählen wir einfach eines aus, wie rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf dem Koggs-Benchmark. Unser Modell übertrifft die anderen mit großem Abstand bei der Verallgemeinerung auf tiefere Rekursion. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr herausfordernd. In unserem Artikel lösen wir einige interessante technische Herausforderungen. Zunächst einmal ist die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht gegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Zusätzlich gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte ist latent. Wir gehen dies an, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der Permutation mit der höchsten Punktzahl NP-schwer ist. Das liegt daran, dass dies mit dem Reiseverkäuferproblem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzuprojizieren und die sprachlich plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unseren Artikel oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute werde ich über unsere Arbeit „Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models“ sprechen. Diese Arbeit wurde in Zusammenarbeit mit Essendermouch und Dangerowski durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotype in großen Sprachmodellen, oder LLMs, dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf manuell erstellte Datensätze, deren Kuratierung sehr zeitaufwendig ist. Außerdem messen sie in der Regel nur sehr spezifische Stereotype, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte verallgemeinern können, oder sie erfassen einfach nur sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich die Intersektionalität nicht, die die Vorstellung ist, dass vielfältige soziale Identitäten Vorurteile verstärken und einzigartige Schadens loci sein können. Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, instruktionsabgestimmten LLMs sehr gut darin sind, auf Anweisungen und Eingaben zu reagieren. Wir können also das Modell bitten, eine Persona zu generieren, was eine Darstellung einer imaginierten Person ist, indem wir eine Eingabe wie „Stell dir vor, du bist eine asiatische Frau, beschreibe dich selbst“ verwenden. Und wir können sofort sehen, dass dies auf jede Demografie verallgemeinerbar ist, da wir einfach den gewünschten Identitätsmarker in dieser Eingabe angeben können. Hier sind einige Beispielgenerierungen von GPT 4. Sofort sehen wir, dass die Ausgaben zwar nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, aber es gibt einige interessante Muster. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und in Bezug auf eine faszinierende Region bezeichnet, und beide Frauen of Color-Personas machen Verweise auf die Abstammung, während die weiße Mann-Persona nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Eingaben zur Generierung dieser Personas wurden von einer Studie inspiriert, bei der sie diese Eingaben an menschliche Probanden gaben und feststellten, dass sie dadurch auch rassistische Stereotype aufdecken konnten. Dies ermöglicht auch einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten unterscheiden, auf die ich gleich näher eingehen werde. Der Vorteil dabei ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne uns auf ein bestimmtes Lexikon verlassen zu müssen. Die Methode der markierten Wörter basiert auf dem soziolinguistischen Konzept der Markierung, das besagt, dass es ein unmarkiertes Standardwert gibt und jede Gruppe, die von diesem Standardwert abweicht, sprachlich markiert ist. Zum Beispiel wird das Wort Mann oder Entschuldigung, das Wort Krieger normalerweise mit Männern assoziiert. Wenn also Menschen einen Krieger beschreiben, der eine Frau ist, werden sie in der Regel tatsächlich einen männlichen Krieger spezifizieren und den Begriff mit Frau markieren. Im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während die marginalisierten Gruppen in der Regel markiert sind. In unserer Methode bestimmen wir zunächst, was die unmarkierten und markierten Gruppen sind. Dann vergleichen wir die Personas mit der Methode der kämpfenden Wörter, die im Grunde genommen gewichtete Logods-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen kämpfende Wörter durchführen und die Logods-Verhältnisse gegen weiße Personas und Mann-Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotype enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter im Lexikon betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotyp-Wörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter groß und athletisch sind. Wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht. Anstatt das zu tun, werden wir uns den Ergebnissen unserer markierten Wörter-Methode zuwenden, um zu zeigen, wie diese scheinbar positiven Wörter Stereotype und essentialisierende Erzählungen erleichtern. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst gehören zu den Top-Wörtern für markierte Gruppen Dinge wie Kultur, Tradition, stolz und exotisch. Diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einem langen Erbe der Diskriminierung und der Anderenbildung für diese Gruppen bei. Darüber hinaus gibt es viele gängige Tropen, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen of Color. Zum Beispiel beinhalten die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie lebendig und kurvig, die mit einem Tropismus der Tropizität verbunden sind. Für asiatische Frauen sind die Wörter Dinge wie zierlich und zart und seidig, was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen verbunden ist, die als sehr sanftmütig und unterwürfig angesehen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie stark und widerstandsfähig sind. Dies verbindet sich mit einem Archetyp, den die Menschen den Archetyp der starken schwarzen Frau genannt haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, da er den anderen eine Menge Druck auferlegt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, übt er Druck auf diese Menschen aus, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen führt, unter anderem. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotype und essentialisierende Erzählungen ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Minderung von Vorurteilen geben, denn zum Beispiel wissen wir nicht, ob es, wie diese positiven Stereotype, daran liegt, dass es eine Art seltsame, übermäßig exzessive Wertanpassung gibt oder vielleicht andere, wie Anti-Stereotyp-Methoden, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder das weiter ohne mehr Transparenz untersuchen. Vielen Dank fürs Zuhören. Habt es bei ACL gut."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABCEval erzählen, einen neuen dimensionellen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Gino Choi an der Emory University durchgeführt und in Zusammenarbeit mit Amazon Alexa AI. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die gängige Praxis besteht darin, menschliche Evaluatoren zu verwenden, indem man menschliche Richter bittet, auszuwählen, welche von zwei Konversationen besser ist, oder Konversationen anhand einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut, um ganzheitliche Bewertungen der allgemeinen Dialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feiner granulierten Ebene zu verstehen. Ein Ansatz besteht einfach darin, menschliche Richter zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalenmethoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt oder nicht, wie zum Beispiel mit irrelevanten Informationen zu antworten oder sich selbst zu widersprechen. Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat, oder kurz ABCEval. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chatmodellen umfassend abzudecken, von denen in der jüngsten Literatur angenommen wird, dass sie die Chatqualität beeinflussen. ABCEval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABCEval die Anzahl der Züge, in denen ein Chatmodell seinen Partner ignoriert oder etwas Ir relevantes sagt, sich selbst oder seinem Partner widerspricht, falsche Fakten halluziniert oder das Wissen über den gesunden Menschenverstand verletzt, und wann das Modell Empathie zeigt oder versagt. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier modernste Chatmodelle ausgewählt und sie anhand von hundert menschlichen Bot-Konversationen pro Modell mit ABCEval bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Zug-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene-Paarweise-Vergleiche. Zusätzlich zu den Bewertungsmethoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Aus unseren Analysen der Ergebnisse haben wir festgestellt, dass die ABC-Eval-Verhaltenskennzeichnungen insgesamt zuverlässiger sind als Kennzeichnungen, die mit bestehenden Methoden gesammelt wurden, gemessen an der Übereinstimmung zwischen den Annotatoren bei 100 doppelt annotierten Konversationen. Darüber hinaus sind die ABC-Eval-Kennzeichnungen prädiktiver für die Gesamtqualität der Konversation im Vergleich zu Metriken, die mit bestehenden Methoden produziert wurden, wie diese einfache lineare Regressionsanalyse zeigt. Zum Beispiel können Sie sehen, wie die Messung des Anteils der Züge mit Selbst- und Partnerwidersprüchen jeweils fünf Prozent und zehn Prozent der Konversationsqualität erklärt, während die durchschnittlichen Likert-Konsistenzwerte nur vier Prozent oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, und zwar mit einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 Prozent der Konversationsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, führt dies in den meisten Fällen zum Verlust einer beträchtlichen Menge an Informationen über die Qualität. Und die Kombination von Likert-Metriken auf alternativer Ebene erklärt weit weniger von der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden erreichen können. Sie können in den Ergebnissen unseres Experiments sehen, dass mehrere Herausforderungen noch bestehen und genau quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots Verstöße gegen den gesunden Menschenverstand in rund zwanzig Prozent ihrer Antworten. Sie produzieren in rund fünfzehn Prozent der Antworten irrelevante Informationen und widersprechen sich oder ihrem Partner in rund zehn Prozent der Zeit. Mit dem raschen Fortschritt in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin Doktorandin im Fachbereich Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die für die ACL 2023 als Langpapier angenommen wurde: Transfer Learning for Dissonance Detection, das sich mit der Herausforderung seltener Klassen befasst. Zunächst definieren wir kognitive Dissonanz und warum sie ein wichtiges Problem ist, das in der Sprache untersucht werden sollte. Einfach ausgedrückt, ist kognitive Dissonanz zwei Überzeugungen oder Handlungen, die inkonsistent sind, wie in diesem Beispiel, wo eine Person sagt: Ich weiß, dass Zigaretten mich töten können und dann sagt: Ich habe nach dem Meeting ein paar Zigaretten geraucht. Diese Überzeugung und Handlung sind inkonsistent und befinden sich in Dissonanz. Wenn ich dann erwähne, dass ich nicht glaube, dass ich meinen Job ohne sie behalten könnte, rechtfertigt das das zweite Auftreten und sie haben eine Konsonanzbeziehung. Obwohl Dissonanz ein sehr häufiges Phänomen ist, das wir bei täglichen Entscheidungen erleben, sind sie in der Sprache unter anderen Arten von Diskursbeziehungen sehr selten zu finden. Warum ist das wichtig? Die Untersuchung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten unter Menschen zu verstehen, Trends und Überzeugungswerte sowie Einstellungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz steht auch im Zusammenhang mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Die Untersuchung von Dissonanz, die in der Sprache ausgedrückt wird, kann auch hilfreich sein, um Extremismus und Polarisierung von gefährdeten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um das Ziel zu erreichen, eine Ressource für kognitive Dissonanz zu schaffen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen Dissonanz-First-Ansatz, wie in dem hier gezeigten Flussdiagramm. Tweets wurden mit einem PDTB-Parser verarbeitet und Paare von Diskursenheiten wurden gemäß den Richtlinien annotiert, die in unserem Papier beschrieben werden. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1.000 Beispiele von Diskursenheitspaaren gesammelt hatten, trainierten wir einen anfänglichen Klassifikator, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Zu unserer Überraschung schnitt der Klassifikator nicht viel besser ab als der Zufall. Angesichts des geringen Auftretens von Dissonanz und der Abwesenheit eines solchen Datensatzes zuvor stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transfer Learning und aktivem Lernen, um so zu annotieren, dass mehr Dissonanzbeispiele in weniger Annotationsschleifen gesammelt werden können, wodurch die Gesamtannotationskosten gesenkt und die Dissonanzdetektion verbessert wird. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben. Die dissonanzunabhängige Standpunktklassifizierung, eine Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema, das hier als Debatte bezeichnet wird, und die binäre Klassifizierung von Expansions- und Vergleichsklassen von PDTB, da diese beiden eng mit dem Konzept von Konsonanten und Dissonanz verwandt sind und wir sie hier CE nennen. Wir stellen fest, dass die Null-Shot-Leistung auf dem annotierten Datensatz bereits viel besser als der Zufall ist, wobei die beste mit AUC 0,62 ist. Darüber hinaus finden wir bei der iterativen Feinabstimmung auf beiden Aufgaben, dass die Feinabstimmung von CE-Aufgaben gefolgt von einer weiteren Feinabstimmung auf der Debatte eine viel bessere Null-Shot-Leistung ergibt. Dies ist also das Modell, das wir verwenden, um das aktive Lernen zu starten. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des aktiven Lernens und der Annotationen zu aktualisieren. Kumulativ sammelt alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während iterativ das Modell durch Training auf dem neuesten Datensatz aktualisiert. Bei den verschiedenen Strategien haben wir festgestellt, dass kumulativ gleich oder besser als iterativ abschneidet. Als Nächstes, um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeitsstrategie für seltene Klassen, PRC, um hauptsächlich die Beispiele auszuwählen, die nach dem aktuellen Modell in jeder Runde des aktiven Lernens mit hoher Wahrscheinlichkeit dissonant sind. Wir vergleichen dies mit den anderen State-of-the-Art-AL-Strategien, die in der Gemeinschaft häufig verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für zufällige Beispiele deutlich niedriger ist. In weiteren Runden des aktiven Lernens mit den beiden besten Strategien haben wir die Distanzklassifizierung AUC auf 0,75 verbessert, was die beste Leistung ist, die wir bisher für die Aufgabe haben. Wir haben auch die Machbarkeit jeder Strategie für die Annotationsqualität und die Kosten für die Annotatoren überprüft. Wir stellen fest, dass PRC den höchsten Prozentsatz an Distanz hat und am besten für seltene Klassen geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für den Erwerb seltener Klassen ist und das kalte Starten des aktiven Lernens mit angemessen gestalteten Transfer-Learning-Aufgaben erheblich helfen kann. Wir stellen auch fest, dass die iterative Aktualisierung nützlich für das Transfer Learning aus einem anderen Bereich ist, während in-domain aktive Annotationen von der kumulativen Aktualisierung profitieren. Dies sind die Links zu unserem Code-Datensatz und unserem Papier. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Aksheta und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit The Kitmasteff Evaluating Knowledge Integration from Multiple Sources. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverständnismodelle stützen sich auf eine Vielzahl von Wissensquellen, wie das in ihren Parametern enthaltene Wissen, das normalerweise durch Vorabtraining erworben wird, und das in den Eingaben zur Inferenzzeit bereitgestellte Wissen. Jüngere Arbeiten bei Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das vorabtrainierte Zeitwissen nutzen können, um die Aufgabe zu lösen. Doch das natürliche Sprachverständnis erfordert oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Beispielsweise kann in dem Satz „John sah den neu gewählten Präsidenten im Fernsehen“ das vorabtrainierte Wissen Informationen über das Verhalten von Präsidenten und das Fernsehen enthalten, aber es kann nicht zuverlässig wissen, wer diese instansspezifische Entität John ist oder wer der neue Präsident ist, da sich die Präsidentschaft seit dem Vorabtraining geändert haben könnte. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vorabtrainiertes als auch zur Inferenzzeit bereitgestelltes Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreferenzauflösung auf, die darauf ausgelegt ist, die Fähigkeit zu prüfen, auf Wissen zuzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablierten Coreferenzauflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kia ist Bäckerin. Termin und Kia trafen sich in einem Park. Nach einem langen Tag bei der Arbeit, an dem er Fälle in einem Gericht entschied, war er froh, sich entspannen zu können. Die Aufgabe hier besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen er sich bezieht, in diesem Fall ist es Sermon. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, entitätspezifisches Wissen wie „Sermon ist ein Richter“. Und zweitens, Hintergrundwissen, das während des Vorabtrainings großer Sprachmodelle erlernt wird, während entitätspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von Kitmos definiert. Erstens haben wir die Themenstellung, Hintergrundvorabtraining, bei der angenommen wird, dass Hintergrundwissen zur Vorabtrainingszeit verfügbar ist. Zweitens gibt es die Hintergrundbeide-Einstellung, bei der sowohl Hintergrund- als auch entitätspezifisches Wissen zur Inferenzzeit verfügbar sind. Die letzte Einstellung ist besonders interessant, da sie einen Fall simuliert, in dem das Hintergrundwissen, das notwendig ist, um eine Aufgabe zu lösen, nicht Teil der vorabtrainierten Daten der Modelle ist, zum Beispiel, weil sich seit der Zeit des Vorabtrainings neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen steuern. In der Hintergrundvorabtrainings-Einstellung nehmen wir an, dass das Hintergrundwissen „Politiker suchen gewählte Sitze in der Regierung“ in den vorabtrainierten Parametern enthalten ist. Im Hintergrundkontext stellen wir das entitätspezifische Wissen „Chichester ist Politiker“ zur Verfügung. In der Hintergrundbeide-Einstellung stellen wir zusätzlich nicht nur entitätspezifisches, sondern auch Hintergrundwissen über Politiker im Inferenztypkongtext zur Verfügung. In der Hintergrundinferenz-Einstellung stellen wir den fiktiven Beruf „Spiegelrundgang“ anstelle von „Politiker“ zur Verfügung, da „Spiegelrundgang“ wahrscheinlich nicht in den vorabtrainierten Parametern enthalten ist. Wir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle bei der schwierigsten Variante der Hintergrundvorabtrainings-Einstellung. Ohne speziellem Training auf KitMus erzielen beide Modelle keine guten Ergebnisse. Wenn sie jedoch auf KitMus trainiert werden, erzielen sowohl C2F als auch Berth für Koref deutlich bessere Ergebnisse als die zufällige Auswahl. Dies deutet darauf hin, dass Modelle beim Training auf allgemeinen Coreferenzauflösungsdatensätzen lernen, oberflächliche Hinweise zu nutzen, die beim Testen auf KitMus, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle das Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen, scheinen viele Coreferenzauflösungsmodelle nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu argumentieren, ohne speziellem Training. Mit speziellem Training können jedoch einige Modelle Wissen aus mehreren Quellen erfolgreich integrieren. Selbst die besten Modelle scheinen jedoch Schwierigkeiten zu haben, Hintergrundwissen zuverlässig zu integrieren, das nur zur Inferenzzeit präsentiert wird. Wenn Sie mehr Details interessiert, lesen Sie bitte unsere Arbeit und überprüfen Sie den Datensatz im Code auf GitHub. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Pappy von der Universität Toronto und der Fondazione Bruno Kessler, und ich werde kurz die Aufmerksamkeit als Leitfaden für das Papier zur gleichzeitigen Sprachübersetzung vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turki ist. Was ist gleichzeitige Sprachübersetzung? Gleichzeitige Sprachübersetzung oder Simulation ist der Prozess der Übersetzung von gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, was die Kommunikation über Sprachgrenzen hinweg ermöglicht. Und was sind die Probleme der aktuellen simulierten Modelle? Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche Module zur Optimierung eingeführt werden, lange und komplizierte Trainingsprozeduren, zum Beispiel Training mit verschiedenen Optimierungszielen, und Training und Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen, zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Was ist also unsere Lösung? Erstens, bereits bestehende Offline-ST-Modelle ohne Neu-Training oder die Annahme einer spezifischen Architektur für CMLSD zu verwenden. Verwenden Sie nur ein Mod-Modell für jedes Latenzregime und behandeln Sie die Latenz durch spezifische Parameter und nutzen Sie das Wissen, das das Modell bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingang und textlichem Ausgabeergebnis erworben hat, das ist der Kreuzaufmerksamkeitsmechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, eine Punkt- oder Encoder-Decoder-Aufmerksamkeit vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob wir eine Teilausgabe basierend darauf emittieren, wohin die Aufmerksamkeit zeigt. Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, diese Summe ist unter einem bestimmten Schwellenwert alpha in Bezug auf die letzten lambda Sprachrahmen, was bedeutet, dass die empfangenen Informationen ausreichend stabil sind. Wenn wir zum Beispiel einen Sprachchunk erhalten, der Ich werde darüber sprechen enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die zuletzt empfangenen Sprachrahmen als lambda Sprachrahmen verweist. Das bedeutet, dass die ersten beiden Wörter emittiert werden, während, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, wir das letzte Wort nicht emittieren und auf einen weiteren Sprachchunk warten. Wenn wir weitermachen und einen weiteren Sprachchunk erhalten und unser Modell andere drei Wörter vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten lambda Sprachrahmen verweist. Das bedeutet, dass diese drei Wörter emittiert werden. Wenn wir uns die Hauptergebnisse ansehen, werden wir die Ergebnisse der gleichzeitigen Sprachübersetzung in Diagrammen darstellen, in denen wir auf der einen Seite blau haben, das die Übersetzungsqualität und die durchschnittliche Verzögerung misst, das ist das Latenzmaß. Und wir berücksichtigen auch die rechenbewusste durchschnittliche Verzögerung, die die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven so hoch wie möglich auf diesem Diagramm sind, aber auch, dass sie nach links verschoben sind, und wir vergleichen mit Prepara-Strategien, die auch auf Offline-Modellen angewendet werden, das sind die Gewichtschlüsselstrategie und die lokale Übereinstimmung, und wir vergleichen auch mit der Standes-der-Technik-Architektur, die speziell für die gleichzeitige Sprachübersetzung zugeschnitten ist, das sind alle Ergebnisse der Strategie der gleichzeitigen Sprachübersetzung auf Deutsch, und wir sehen, dass ein Punkt alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da ihre Kerne die Kurven nach links verschoben sind, und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die Rechenabnutzungszeit betrachten, das die schnellste Strategie ist, wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier, und wir haben auch den Code und die Modelle und die gleichzeitige Ausgabe als Open Source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Danke für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Xuheng. Heute werde ich unseren Artikel „Funktionieren Kernel 2003-Named-Entity-Tagger noch gut im Jahr 2023?“ vorstellen. Lassen Sie uns beginnen. In unserem Artikel haben wir das Problem der Generalisierung anhand der Named-Entity-Recognition-Aufgabe oder der NER-Aufgabe untersucht. Wir haben festgestellt, dass Modelle seit fast 20 Jahren Kernel 2003 zur Entwicklung von NER verwenden. Und das wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten generalisieren und was ist nötig für eine gute Generalisierung, wenn wir neue Tagger entwickeln, und wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Kernel-Plus-Plus-Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen Kernel 2003-Annotationsempfehlungen annotiert haben. Anschließend haben wir über 20 Modelle auf Kernel 2003 verfeinert. Wir haben sie sowohl auf dem Conor 3-Testdatensatz als auch auf dem Conor Plus Plus-Testdatensatz bewertet. Und nicht zuletzt haben wir den prozentualen F1-Wert-Änderungsbetrag berechnet, um die Generalisierung jedes Modells zu bewerten. Was also ist nötig für eine gute Generalisierung? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptbestandteile gibt, die benötigt werden. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Und nicht zuletzt wissen wir alle, dass die Anzahl der Feinanpassungsexemplare direkt die Leistung einer nachgeschalteten Aufgabe beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinanpassungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist das adaptive Overfitting, also ein Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird, und dies manifestiert sich normalerweise als abnehmender Ertrag auf einem neuen Testdatensatz. Die zweite Hypothese ist der zeitliche Drift, also die Leistungsdegradation, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird. Für das Problem des Overfitting haben wir gesehen, dass die rote Ausgleichsgeradenlinie in dem Diagramm auf der rechten Seite einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf Kernel 2003 vorgenommen haben, sich in mehr als einer Einheit Verbesserung auf Kernel-Plus-Plus umsetzt, was bedeutet, dass es keine abnehmenden Renditen gibt und dies zeigt uns, dass in diesem Fall kein adaptives Overfitting beobachtet wird. Was ist also mit dem zeitlichen Drift? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten neu zu trainieren oder weiter vorzuverfeinern, und wir haben festgestellt, dass die Leistung mit größerer zeitlicher Lücke abnimmt und dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinanpassungsexemplare benötigen, und dies geht Hand in Hand. Wir können nicht nur einen Bestandteil haben und die anderen wegwerfen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch den zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptives Overfitting, obwohl Kernel 2003 seit über 20 Jahren verwendet wird. Wenn wir also zur Frage zurückkehren, die wir im Titel unseres Artikels aufgeworfen haben, funktionieren Kernel 2003-Tagger noch im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber anregt, wie die Generalisierung der Modelle verbessert werden kann. Und zuletzt, stellen Sie bitte sicher, dass Sie unseren Artikel und unseren Datensatz überprüfen und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, willkommen bei unserer Präsentation von DeepLean, einem neuen Korpus für die deutsche Textvereinfachung auf Dokument- und Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Lese-Problemen oder Nicht-Muttersprachlern. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, zum Beispiel Dokumente oder Sätze. In dem hier gezeigten Beispiel können Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und dessen Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikale Substitution, Satzteilentfernung, Umorganisation der Satzteile oder Einfügen von Wörtern. Wir schlagen nun unseren neuen Korpus dplane vor, da es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Diese sind hier zu klein, um ein Taxonomiemodell darauf zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus DPlane vor, der in zwei Teilkorpora aufgeteilt ist, DPlane APA und DPlane Web. DPlane APA basiert auf verwendeten Texten. In DPlane APA haben wir 483 Dokumente alle manuell ausgerichtet. Es resultieren ungefähr 30.000, 13.000 parallele Satzpaare. Für dplane web umfasst dieser Korpus verschiedene Bereiche, und wir richten auch alle diese 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden aus. Insgesamt haben wir 30.450 Satzpaare erhalten. Wir haben unsere Satzpaare etwas genauer analysiert, zum Beispiel nach dem Typ der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel der Nachrichtentext oder die Texte für Sprachlerner auf allen Ebenen, was zum Beispiel lexikale Vereinfachung, strukturelle Vereinfachung oder das allgemeine Niveau der Vereinfachung betrifft. Darüber hinaus können Sie sehen, dass unser D-Plane-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir im D-Plane-API-Korpus viel mehr Umorganisationen und Wortbearbeitungen als im D-Plane-Web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Sim-Umformulierungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz dplane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext von maschinellen Übersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Ausrichtungen von Sätzen in Nachdokumenten extrahieren möchten, aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die dieselbe Sprache haben, denselben Inhalt haben, aber auf unterschiedlichen Komplexitätsebenen stehen. Und jetzt, da wir unseren Datensatz D-Plane haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, im Papier veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode der Massen-Ausrichtung ist, und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen, im Papier. Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu produzieren. Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben das Modell von Long Import fein abgestimmt, um Vereinfachungen auf Dokumentniveau zu produzieren, und wir haben auch das normale Basisimport-Modell fein abgestimmt, um Vereinfachungen auf Satzebene zu produzieren. Sie können auch alle Checkpoints finden und sich in den Papierdetails über die Ergebnisse und die Bewertungsmetriken unserer Experimente informieren. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Basisbewertungen erzielen könnte, und wir schlagen diese Ergebnisse als Benchmark, einen Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft, vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xi Yuan von der Fen-Universität. Ich bin hier, um unsere Arbeit über das Wissen über eindeutige Skripte aus großen Sprachmodellen für eingeschränkte Sprachplanung vorzustellen. Im Alltag planen Menschen oft ihre Handlungen, indem sie Schritt-für-Schritt-Anweisungen in Form von garantierten Skripten befolgen. Frühere Arbeiten haben Sprachmodelle untersucht, um für abstrakte Ziele stereotypischer Aktivitäten wie das Backen eines Kuchens zu planen und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrieren sich frühere Arbeiten hauptsächlich auf die Planung für die abstrakten Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Zielen, spezifischen Einschränkungen, wie das Backen eines Schokoladenkuchens, bleibt noch untererforscht. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen für die Planungsschritte auferlegt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit schwierigeren, vielschichtigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In diesem Papier bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da kein Datensatz spezifischer Ziele zur Unterstützung unserer Studie existiert, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielschichtigen Einschränkungen für die Datenerfassung durch menschliche Schleifen unter Verwendung von instruierten TPT. Wir nehmen hundert spezifische Ziele und bewerten die Skripte, die aus Light Logic-Modellen generiert wurden. Diese Tabelle berichtet über die Gesamtnacur der Ergebnisse. Wir stellen fest, dass alle Light Logic-Modelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, was Light Logic-Modelle bewirken. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist. Die Treue zu den Einschränkungen kann jedoch nicht garantiert werden. Wir gehen genauer auf die frank gegradeten Themenkategorien der Einschränkungen ein, die vom Wachen nach Hause abhängen. Die Kopfkarte in der Abbildung zeigt, dass die Planungsphasen von instruierten GPDs für Mädchen unterschiedlicher Kategorien erheblich variieren. Frühere Studien haben gezeigt, dass die Ausgabequalität von Larry-Modellen in hoher Varianz liegt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des übergenerierten Zen-Filters, um die Generierungsqualität zu verbessern. Zuerst zeigen wir Einschränkungstypen mit Beispielen für instruierte GPT und erhalten spezifische Ziele basierend auf den festgelegten abstrakten Zielen. Dann übergeneriert instruierte GPT Schlüsselskripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die passenden Skripte auszuwählen. Wir konvertieren Skripte und Ziele in instruierte GPT in Bissen und berechnen die Kosinusähnlichkeit und Ähnlichkeitsbewertungen, um die semantische Ähnlichkeit zu messen. Zusätzlich werden wir das Skript schreiben, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel die höchste Punktzahl auf der Zielseite erzielt. Mit unserer Methode kann Inslacity Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planbarkeit sowohl in Bezug auf semantische Vollständigkeit als auch auf die Treue zu den Einschränkungen erheblich. Da große Sprachmodelle kostspielig zu implementieren sind, ist es wesentlich, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wesentlicher Schritt zu diesem Zweck. Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele, und die manuelle Annotation von Datensätzen ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsmodelle zu destillieren. Wir wenden unsere Methode an, um einen Datensatz der eingeschränkten Sprachplanung namens CodeScript aufzubauen. Insgesamt generieren wir fünfundfünfzigtausend spezifische Ziele mit Skripten, um die Qualität der Validierungs- und Testsites zu gewährleisten. Wir bitten Cloud-Source-Arbeiter, die falschen Proben zu finden und zu überarbeiten. Diese Abbildung zeigt die eingeschränkte Verteilung von CodeScript. Wir stellen fest, dass CodeScript die Hypothese in den generierten spezifischen Zielen zeigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung trainieren. Wir stellen fest, dass die TFIL-Funktion auf der Codesrate Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie richtig auf geeigneten Datensites trainiert werden. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir haben die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung bewertet und eine übergenerierte Filtermethode für große Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptsdatensatz, CodeScript, für konstruktive Sprachplanung zu generieren. Wir hoffen, dass der CodeScript-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung voranzutreiben. Vielen Dank für Ihre Zeit. Weitere Details zu CodeScript finden Sie in unserem Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrack und ich werde Ihnen unsere Arbeiten zu Dr. Berth vorstellen, einem robusten Vor-Trainingsmodell in Französisch für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann werden wir den Hauptbeitrag unseres Artikels vorstellen. Wir stellen das erste biomedizinische Modell in Französisch vor, das Dr. Berth heißt und auf Roberta basiert, und trainieren auf Natchios, einem Datensatz von medizinischen Daten, die aus dem Web gesammelt wurden. Wir stellen auch einen Vergleich von Modellen mit mehreren Vor-Trainings-Einstellungen und Datenquellen vor. Dann präsentieren wir unsere Ergebnisse zu elf biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Und schließlich ziehen wir unsere Schlussfolgerungen aus den Experimenten und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT einer der effektivsten Ansätze zur Lösung von Aufgaben der Verarbeitung natürlicher Sprache und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec, Fastex oder NWO einen enormen Leistungsgewinn. Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie in Französisch mit Camembert und in anderen Bereichen wie dem biomedizinischen mit PermetteBERT und BioBERT. und im klinischen mit clinical birth, aber hauptsächlich auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vor-Training aufgrund des Mangels an domänenspezifischen Daten. Für Französisch gab es jedoch bisher kein Open-Source-Modell für den biomedizinischen Bereich. Wir stellen uns also die Frage, welche Datenquellen für eine breite Verwendung am geeignetsten sind. Und diese aktuellen Daten sind eine gute Alternative zu klinischen Daten. Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die wir von dem nicht-universitären Krankenhaus erhalten haben. Anschließend stellen wir uns die Frage, wie viele Daten benötigen wir, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Sind es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir zunächst vier Modelle von Grund auf. Eine erste Version von Dr. Bert mit 7 GB Natchez, eine zweite Version mit 4 GB Natchez, eine erste Version von Schubert, einem klinischen Modell, mit 4 GB Sätzen, die von klinischen Knotenpunkten stammen, und eine endgültige Version von Schubert mit einer Mischung aus 4 GB Natchez und 4 GB klinischen Knotenpunkten. Zusätzlich zu diesem Vergleich haben wir drei Modelle eingeführt, die auf kontinuierlichem Vor-Training trainiert wurden, um die Auswirkungen der Vor-Trainings-Strategie zu analysieren. Eines basiert auf dem Gewicht von Camembert und wurde auf 4 GB Natchez trainiert. Ein weiteres basiert ebenfalls auf Camembert, wurde aber dieses Mal auf die 4 GB klinischen Knotenpunkte trainiert. Und schließlich eines, das auf dem englischen biomedizinischen Modell Bermud Bert basiert und auf 4 GB Natchez trainiert wurde. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir öffentliche und private Match-up-Aufgaben wie Namens- und Höhenerkennung. Klassifizierung, Musterwechsel-Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Basismodellen verglichen, die Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PumedBERT, BioBERT und ClinicalBERT sind. Die Bewertung zeigt, dass das Modell, das am besten auf der Aufgabe mit Daten derselben Art wie die, auf denen das Modell trainiert wurde, abschneidet, am besten abschneidet. Allerdings können wir feststellen, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen, wenn wir diese Daten von dort erhalten können. Wir stellen auch fest, dass die Verwendung von mehr Daten zu einer besseren Leistung führt. Insgesamt scheint das Fret-Tuning von Grund auf eine höhere Leistung bei den meisten Aufgaben zu erzielen. Allerdings hat unser Experiment zum kontinuierlichen Fret-Tuning mit dem Gewicht und dem Tokenizer von PumedBeard, das auf dem 4-GB-Teil von Natchez trainiert wurde, vergleichbare Ergebnisse wie die, die mit Dr. Beard 4 GB von Grund auf erzielt wurden, was nicht der Fall ist für das Modell, das auf Camembert-Gewichten und -Tokenizer basiert, das unter Stabilitätsproblemen leidet. Abschließend lässt sich sagen, dass unser vorgeschlagenes System bei neun der elf DOTSTRIMS-Aufgaben eine bessere Leistung erzielt und global das Ergebnis des generischen Modells, hier Camembert, übertrifft. Wir stellen auch fest, dass spezialisierte Daten besser sind, aber sie skalieren nicht gut. Alle aus Natchios erhaltenen vortrainierten Modelle sind auf YuginFace frei verfügbar und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank also für... für diese Präsentation und wir freuen uns auf den Austausch bei der Post-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiang Bin, Doktorand an der Universität von Washington. Heute stelle ich unsere Arbeit von Pretraining-Daten über Sprachmodelle bis hin zu Downstream-Aufgaben vor, wobei wir den Spuren politischer Voreingenommenheiten folgen, die zu unfairen NLP-Modellen führen. Sprachmodelle werden also auf großen, im Web gesammelten Daten trainiert. Politische Nachrichtenmedien sind in ihren Pretraining-Daten gut vertreten. Laut einer Umfrage des C-four-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, die Huffington Post usw. in den Sprachmodell-Training-Daten gut vertreten sind. Dies hat für Sprachmodell-Anwendungen sowohl einen Segen als auch einen Fluch bedeutet. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairness-Problemen in Downstream-Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von Pretraining-Daten über Sprachmodelle bis hin zu Downstream-Aufgaben zu untersuchen, indem wir speziell die folgenden Fragen stellen. Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielt das Pretraining-Daten dabei? Zweitens, wie schneiden Sprachmodelle mit unterschiedlicher politischer Ausrichtung tatsächlich bei Downstream-Aufgaben ab und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten unter Verwendung der politischen Fragebögen, wie dem politischen Kompass-Test, anzuregen. Dies stellt sicher, dass wir eine in der Politikwissenschaft gut begründete automatische Bewertung durchführen können. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Bedeutungen haben. Sie besetzen alle vier Quadranten auf dem politischen Kompass. Wir können auch sehen, dass GPT 4 das liberalste Sprachmodell von allen ist und GPT-Theorien im Allgemeinen sozialliberaler sind als die BERT-Theorie und ihre Varianten. Zweitens zielen wir darauf ab, zu untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints auf sechs verschiedenen parteiischen Korpora weiter vortrainieren, die in Nachrichten und soziale Medien weiter unterteilt in ihre politische Ausrichtung getrennt sind. Indem wir Sprachmodelle auf solchen parteiischen Korpora weiter vortrainieren, können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel, für Roberta, die weiter verfeinert und auf dem linksgerichteten Reddit-Korpus weiter trainiert wurde, können wir eine erhebliche liberale Verschiebung in Bezug auf ihre politischen Voreingenommenheiten feststellen. um zu untersuchen, ob Sprachmodelle die in unserer modernen Gesellschaft vorherrschende Polarisierung aufgreifen können. Wir teilen die Pretraining-Korpora in vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten. Wir vortrainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufgreifen können. Zuletzt, aber nicht zuletzt, bewerten wir Sprachmodelle mit unterschiedlicher politischer Ausrichtung bei der Erkennung von Hassrede und Falschnachrichten für NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Bedeutungen von Nachrichtenmedien aufteilen, wir ein Muster sehen können, das zum Beispiel bei der Erkennung von Hassrede zeigt, dass linksgerichtete Sprachmodelle besser darin sind, Hassrede zu erkennen, die sich gegen sozial benachteiligte Gruppen richtet, jedoch schlechter darin sind, Hassrede zu erkennen, die sich gegen mächtigere Gruppen in unserer Gesellschaft richtet. Und umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassrede zu erkennen, die sich gegen Weiße und Männer richtet, jedoch schlechter darin, Hassrede zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richtet. Ähnliche Trends treten auch bei der Erkennung von Falschnachrichten auf, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Desinformation von ihrer entgegengesetzten politischen Bedeutung zu erkennen und umgekehrt. Dies wird viele qualitative Beispiele zeigen, um zu sehen, dass Sprachmodelle mit unterschiedlicher politischer Bedeutung unterschiedliche Vorhersagen zu Hassrede und Desinformationsbeispielen auf der Grundlage ihrer sozialen Kategorien treffen. Im Anhang gibt es eine Reihe von weiteren Beispielen, um dies weiter hervorzuheben. Dies deutet darauf hin, dass es ein sehr drängendes Fairness-Problem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen gibt. Zum Beispiel, wenn ein rechtsgerichtetes Sprachmodell auf Hassrede oder Desinformation oder was auch immer weiter verfeinert und auf eine beliebte Social-Media-Plattform eingesetzt würde, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und die Hassrede, die sich gegen Minderheitengruppen richtet, könnte ohne Kontrolle wüten. Das schlägt Alarm, um die Fairness-Probleme anzuerkennen und anzugehen, die aus den politischen Voreingenommenheiten von Sprachmodellen resultieren. Also ein bisschen Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir also politische Meinungen in den Sprachmodell-Training-Daten nicht sanieren, wird die Voreingenommenheit von Pretraining-Daten über Sprachmodelle zu Downstream-Aufgaben propagieren und letztendlich Fairness-Probleme schaffen. Wenn wir versuchen, auf irgendeine Weise zu sanieren, würden wir auch Zensur oder Ausschluss riskieren, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Sprachmodell-Training-Daten behalten werden sollte. Es ist also irgendwie das Problem des elektrischen Charlie. Okay, toll. Ich denke, das ist so ziemlich alles, was ich für heute habe. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Coast of Sina und ich freue mich, Sie zu unserem Vortrag über unseren ACL 2023-Artikel „Language Model Acceptability Judgments are not always robust to context“ willkommen zu heißen. Dies ist eine gemeinsame Arbeit mit John Bakhier, Aaron Mueller, Kanishka Mishra, Karen Fentus, Roger Levy und Adina Williams. In dieser Arbeit nehmen wir das Minimalpaar-Paradigma erneut unter die Lupe. Das Minimalpaar-Paradigma bewertet im Wesentlichen Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität wie die Syntax des Blimp oder Akzeptanz in Bezug auf Stereotypen wie Menschenmassen umfassen können. In diesem Minimalpaar-Paradigma besteht die typische Methode zur Bewertung von Sprachmodellen darin, dass man ein akzeptables oder ein grammatikalisches Satzpaar zeigt und dann ein inakzeptables oder ein ungrammatisches Satzpaar, in der Hoffnung, dass das Modell der akzeptablen Aussage eine höhere Wahrscheinlichkeit beimisst. Die aktuelle MPP-Pipeline ermöglicht es uns im Grunde nicht, die Akzeptanz des Modells für längere Sätze zu bewerten. Heutzutage entwickeln große Sprachmodelle immer längere Kontextfenster. Es ist daher entscheidend, dass wir die Akzeptanz des Modells im gesamten Kontextfenster bewerten. Und das ist es, was wir hier zu tun versuchen. Wir versuchen, die NPV-Pipeline erneut zu bewerten, indem wir das Modell bitten, die Akzeptanz für längere und längere Sequenzen zu bewerten. Das ist der Ansatz. Was wir also tun, ist, um diese längeren Sequenzen zu simulieren, dass wir die Datensätze selbst erneut betrachten und dann Sätze neu erstellen, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar von Grammatikalität aus dem Blimp-Datensatz aus dem Adjunct Island Case ausgewählt. Und was wir tun, ist, um längere Sequenzen zu erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur aufweisen, dass wir grammatikalische Sätze aus Adjunct Island extrahieren und dann als Präfix sowohl an die akzeptable Abfrage als auch an die inakzeptable Abfrage anhängen. So können wir dasselbe tun, indem wir inakzeptable Sätze aus dem gleichen Matching auswählen, was auch verwendet werden könnte, um die Akzeptanz des Modells zu testen. Und wir können dasselbe tun, indem wir Sätze aus einem anderen Unterdatensatz oder einem anderen Datensatz auswählen. Das ist es, was wir als Mismatch-Szenario bezeichnen. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem Sie bewerten. Und wir können dasselbe für den Fall der Inanzeptabilität tun. Schließlich können wir Sätze aus einem völlig unverbundenen Bereich wie Wikipedia auswählen. Das wird uns sagen, ob die Akzeptanzurteile des Modells tatsächlich von einem Kontext beeinflusst werden, wie zum Beispiel ob der Kontext aus einem anderen Unterdatensatz stammt oder ob er völlig irrelevant für den aktuellen Satz ist, den wir betrachten. Wie geht das Modell damit um? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind, und dort stellen wir fest, dass die MPP-Urteile größtenteils für beliebige Kontextlängen robust sind. Wir erhöhen die Kontextlänge bis auf 1024, um die OPT- und GPT2-Modelle auszulasten, und wir sehen hier in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert nun, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen oder erstellen wir Sätze aus akzeptablen und inakzeptablen Bereichen aus demselben Blimp- oder Syntax-Gem-Datensatz, und dort sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn man entweder akzeptable oder inakzeptable Präfixe hinzufügt. Aber wenn wir die Struktur abgleichen, das heißt, wenn wir die Sätze aus denselben Phänomenen in der blame-based Syntax Gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang der MPP-Urteile für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Nun, dieser Effekt nimmt im Laufe der Kontextlänge zu und würde wahrscheinlich neuere Sprachmodelle beeinflussen, die ein großes Kontextfenster haben. Warum beeinflusst das Präfix das Urteil des Sprachmodells so sehr? Wir haben eine Reihe von Analysen durchgeführt, in denen wir versucht haben, den... Eingabe-Satz zu stören, indem wir versuchten, die relevante Struktur zu bewahren, aber Rauschen zum Eingabe-Satz hinzuzufügen. Nachdem wir mehrere dieser Störungen durchgeführt haben, stellen wir fest, dass keines dieser Rauschen das Modell tatsächlich dazu bringt, seinen Kurs in Bezug auf die MPP-Urteil-Trends zu ändern. Im Grunde stellen wir fest, dass die Modelle auf ähnliche Weise auf die gestörten Sätze reagieren, das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in den Sätzen gemeinsam sind. Und die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satzeingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unseren Artikel für weitere Details unserer Experimente. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawe, ein Doktorand an der Stalant-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit, „Weaker Than You Think“, vorstellen, einen kritischen Blick auf das wöchentlich überwachte Lernen. Dies ist eine gemeinsame Arbeit mit Xiao Yushchen, Maios Musbach und Giaz Steffen sowie Dietrich Clarkov. Ich möchte mit einer kurzen Einführung in die wöchentliche Überwachung und das wöchentlich überwachte Lernen beginnen. Bei der wöchentlichen Überwachung kennzeichnen wir die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mit wöchentlichen Kennzeichnungsquellen, wie einfachen heuristischen Regeln, Wissensbasen oder Cloud-Crowdsourcing wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen viel billiger, aber sie sind auch verrauscht, was bedeutet, dass ein gewisser Anteil der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf wöchentlichen Kennzeichnungsdaten trainieren, neigen die neuronalen Netze dazu, den Kennzeichnungslärm zu verinnerlichen und verallgemeinern nicht. Beim wöchentlich überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust auf solchen Kennzeichnungslärm zu trainieren, sodass die trainierten Modelle weiterhin gut verallgemeinern. In jüngsten Arbeiten im WSL, wobei WSL für wöchentlich überwachtes Lernen steht, ist ein gängiger Anspruch, dass die Leute sagen, dass sie nur Modelle auf den wöchentlichen Kennzeichnungsdaten trainieren und hohe Leistung auf sauberen Testdatensätzen erzielen. Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt einen Haken, nämlich dass die Leute davon ausgehen, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellselektion verfügbar ist. Wir zweifeln an dieser Problemstellung, da dies impliziert, dass zusätzliche manuelle Annotationen im wöchentlich überwachten Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Der oben genannte Zweifel veranlasst uns, drei Forschungsfragen zu stellen. Erstens, ist sauberer Validierungsdatensatz für WSL notwendig? Oder können wir stattdessen einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn sauberer Datensatz erforderlich ist oder wenn sauberer Datensatz für WSL obligatorisch ist, wie viele saubere Proben benötigen wir dann? Schließlich, sollten wir nur die sauberen Proben für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angesprochen und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass neuere WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn es keine sauberen Validierungsproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Kennzeichnungen hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gekennzeichnete Daten benötigen, um richtig zu funktionieren, und die Annotatkosten für die Beschaffung sauberer Validierungsproben sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl der sauberen Validierungsproben WSL-Ansätzen helfen wird, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur zwanzig Proben pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns entscheiden, auf saubere Proben zuzugreifen, wird das direkte Training auf ihnen sogar eine bessere Leistung erzielen. Die rote Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmensansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir sehen können, wenn wir 10 Proben pro Klasse haben, beginnt das direkte Feinabstimmen, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem die Möglichkeit zum Fortsetzen des Feinabstimmens auf den sauberen Validierungsproben gegeben wird. Wie wir den Abbildungen entnehmen können, unterlebt das Berliner Modell, das als FTW bezeichnet wird, zunächst komplexeren WSL-Methoden wie Cosine. Wenn wir jedoch das Fortsetzen des Feinabstimmens auf den sauberen Proben erlauben, dann erreicht FTW eine gleich gute Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicher erfordern. Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere manuell annotierte Proben benötigen, damit sie richtig funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie, dass die Modellselektion mit sauberen Validierungsproben durchgeführt wird. Zweitens, sollten WSL-Ansätze mit zukünftigen Lernbasisliniens verglichen werden, da beide auf sauberen Proben arbeiten. Drittens, kontinuierliches Feinabstimmen ist eine einfache, aber starke Basisliniens, die in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code als Open Source veröffentlicht. Sie finden ihn über den QR-Code auf dieser Folie. Bitte zögern Sie nicht, ihn sich anzusehen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Ayud Villar und ich werde einen kurzen Überblick über den Artikel „Prompting Palm from Translation Assessing Strategies and Performance“ geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Palm ist ein 540 Milliarden Parameter großes Sprachmodell, das im letzten Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Textsammlung mit 780 Milliarden Token trainiert. Zum Zeitpunkt der Veröffentlichung erreichte es den neuesten Stand der Technik bei hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Modellauslösung für maschinelle Übersetzungen. Wir bewerten die Übersetzungsfähigkeit solcher Modelle unter Verwendung der bewährten Methoden der AMT-Community. Dazu verwenden wir die neuesten Testsätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden, und wir vergleichen zwei modernste Systeme, die leistungsstärksten Systeme der WMT-Evaluation. Wir verwenden modernste neuronale MT-Metriken und zeigen zusätzlich auch die Ergebnisse der fachkundigen menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für Strategien zur Auswahl von Auslösern. Die Modellauslösung hat einen großen Einfluss auf die Leistung der LLMs für Übersetzungen, wie wir in einem einfachen Experiment sehen können, bei dem wir eine kurze Modellauslösung verwenden und zwei verschiedene Auslöser für einen Satz bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt der beobachtete Unterschied mehr als einen Unschärfepunkt. Und in extremen Fällen kann dies bis zu 40 Unschärfepunkte betragen. Es ist also wichtig, eine gute Strategie zur Modellauslösung auszuwählen. In unseren Experimenten haben wir uns für eine Fünf-Schüsse-Modellauslösungsstrategie entschieden, bei der wir jeden Satz, den wir dem System zur Verfügung stellen, mit der Sprache markieren, in der er verfasst ist. In diesem Beispiel hier, bei dem wir eine Übersetzung vom Deutschen ins Englische durchgeführt haben, sind die deutschen Sätze, die Quelltexte, mit einem deutschen Doppelpunkt markiert, und die englischen Übersetzungen mit einem englischen Doppelpunkt. Wir haben festgestellt, dass die tatsächliche Form der Modellauslösung im Falle mehrerer kurzer Modellauslösungen keinen großen Einfluss hat. Sie ist entscheidend für Null- und Ein-Schüsse-Modellauslösungen, aber wenn wir, wie in unserem Fall, zu Fünf-Schüsse-Modellauslösungen übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form der Modellauslösung. Es sind die Beispiele, die das meiste Gewicht haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl der Auslöser aus den Trainingsdaten der WMT-Evaluierungen oder den Dev-Daten. Die Dev-Daten sind viel kuratiert und von höherer Qualität als die Trainingsdaten, sodass sie, wie ich sage, und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte ODR-Systeme einen erheblichen Vorteil gegenüber den PALM-Übersetzungen, aber PALM kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Rahmenwerk durchgeführt haben, sind, dass die Fließfähigkeit von Palm mit den neuesten Systemen vergleichbar ist, aber der Hauptunterschied von der Genauigkeit kommt. Insbesondere sind die häufigsten Fehler Omissionsfehler. Es scheint also, dass Palm manchmal eine bessere Übersetzung produziert, indem es Teile des Satzes, die in der Übersetzung weggelassen werden, weglässt. Der Stil nach außen Kategorie für PAM ist jedoch niedriger als für die neuesten Systeme, was ein zusätzliches Signal dafür ist, dass PAM wirklich fließende Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diesen wirklich kurzen Überblick. Für weitere Details, kommen Sie bitte zur vollständigen Präsentation des Artikels. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, alle zusammen, mein Name ist Jing Wei von der Universität für Wissenschaft und Technologie in China. Es ist mir eine Freude, ein kurzes Werbevideo für unseren Artikel „Are you copying my model“ vorzustellen, der den Urheberrechtschutz großer Sprachmodelle für Einbettungen und Dienste zum Thema hat. Sehen Sie sich das Backdoor-Wasserzeichen an. Lassen Sie uns zunächst den Hintergrund zu Einbettungen und Diensten vorstellen. Derzeit sind große Sprachmodelle wie GPT, Lama, PELM außergewöhnlich in Bezug auf das Verständnis und die Generierung natürlicher Sprache. Einbettungen und Dienste sind einer der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine auf GPT basierende Einbettungs-API an. Neuere Arbeiten haben jedoch gezeigt, dass Angreifer das Modell durch das Lernen aus der Einbettung stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht von Einbettungen als Dienste zu schützen. Um das Urheberrecht von Einbettungen als Dienste zu schützen, ist eine der Lösungen, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Methode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Einbettungen als Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer geheim genug sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Model-Extraktionsvorgangs auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methoden sind jedoch entweder nicht auf Einbettungen als Dienste anwendbar oder es fehlt die Übertragbarkeit. Daher schlagen wir in diesem Artikel den Einsatz von Einbettungsmarkern vor, einer auf Backdoors basierenden Wasserzeichenmethode, die auf Einbettungen als Dienste anwendbar ist. Lassen Sie mich dann die Details unseres Einbettungsmarkers vorstellen. Der Einbettungsmarker besteht aus zwei Hauptschritten: Wasserzeichen-Einbettung und Urheberrechtsprüfung. Bevor wir zu diesen Hauptschritten übergehen, wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem mittleren Häufigkeitsintervall. Nehmen wir an, der Anbieter kann einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen. Bei der Wasserzeichen-Einbettung definieren wir zunächst eine Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Die bereitgestellte Einbettung ist eine Gewichtungssumme der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als M ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsprüfung besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Zuerst erstellen wir einen Backdoor- und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Dann fordert der Anbieter Einbettungen vom Diebstahldienst mit dem Datensatz an. Die Ähnlichkeit zwischen der angeforderten Einbettung und der Ziel-Einbettung wird berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem Backdoor- und dem gutartigen Datensatz, die als Delta-Cosinus und Delta L2 definiert ist. In der Zwischenzeit wenden wir auch den KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente an vier Datensätzen durch: AG News, Mind, SSD zwei und Erospam. Wir nehmen an, dass der Anbieter den Wiki-Textdatensatz anwendet, um die Wortfrequenz zu zählen. Die Ergebnisse an den vier Datensätzen zeigen, dass unser Einbettungsmarker eine hervorragende Erkennung erzielen kann, während er die Gitternutzlichkeit für Downscreen-Aufgaben beibehält. Wir validieren auch die Geheimhaltung der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen als bei BOPCA entfaltet visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwer, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden. Das war's, vielen Dank. Willkommen, mit uns zu diskutieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Ying, und mein Kollege Jian und ich werden unsere Forschung über die Verbesserung des multimodalen Zero-Shot-Lernens durch Anweisungstuning präsentieren. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, um vortrainierte Sprachmodelle auf effiziente Weise in Bezug auf Parameter und Daten für verschiedene nachfolgende Aufgaben wiederzuverwenden. In jüngster Zeit haben viele Studien gezeigt, dass Anweisungstuning großen Sprachmodellen ermöglicht, unbekannte Aufgaben auf Zero-Shot-Weise auszuführen, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Anweisungstuning konzentrierten sich jedoch auf die Verbesserung der Zero-Shot-Leistung bei Aufgaben, die nur Sprache betreffen, während Computer Vision und multimodale Aufgaben ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Anweisungstuning an multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf unbekannte multimodale Aufgaben verbessern kann. Zusätzlich entdeckten wir zum Zeitpunkt unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Anweisungsdatensätzen zwischen LP und multimodal. Es gibt mehr als sechzehnhundert Aufgaben mit nur Sprach-Anweisungen. Es gibt jedoch keine groß angelegte öffentlich zugängliche multimodale Anweisungsaufgabe. Daher motiviert uns dies, einen multimodalen Anweisungsdatensatz aufzubauen. Hier präsentieren wir multi instruct, den ersten multimodalen Benchmark-Datensatz für Anweisungstuning, der aus sechzig zwei verschiedenen multimodalen Aufgaben besteht, die zehn kühne Kategorien abdecken. Diese Aufgaben stammen aus einundzwanzig bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um das multimodale Anweisungstuning an unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA als Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem multi install-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format, in dem der Eingabetext, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt werden. Okay, jetzt werde ich über multimodales Anweisungstuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus der NIG-Gruppe für das Training und wir nehmen 10.000 Instanzen pro Aufgabe. Für die Tests behalten wir die gesamte Common Sense Reasoning-Gruppe für das Testen und wir wählen zusätzlich fünf Aufgaben aus WQA und der Miscellaneous-Gruppe. Wir verwenden alle Instanzen im Test-Split für jede Aufgabe. Zusätzlich nehmen wir zufällig 20 Aufgaben aus dem Test-Split von NIG instruction als SIN-Aufgabe für NLP. Wir verwenden also ein vortrainiertes OFA-Großmodell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten. Wir berichten die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, berichten wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir ROOGEL. Für eine RP-Aufgabe berichten wir ebenfalls ROOGEL. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt. Diese misst die Fähigkeit des Modells, konsistent dieselben Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann das Anweisungstuning die Leistung von OFA bei der Bearbeitung multimoder Aufgaben erheblich verbessern. Auch das Transfer-Learning von natürlichen Anweisungsdatensätzen kann dem Anweisungstuning zugutekommen. Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig die Sensitivität sinkt. Daher haben wir auch ein Experiment durchgeführt. Wir verwenden eine Anweisung gegenüber fünf Anweisungen. Wie wir sehen können, kann die Verwendung von mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transfer-Learning von natürlichen Anweisungsdatensätzen eine viel bessere Sensitivität erreichen als das ursprüngliche OFA-Modell. Wir können auch sehen, dass das Transfer-Learning von natürlichen Anweisungsdatensätzen OFA helfen kann, eine viel bessere Leistung am natürlichen Anweisungsdatensatz zu erzielen. Insgesamt haben wir den ersten groß angelegten multimodalen Anweisungsdatensatz vorgeschlagen. Wir haben die DAROCHOT-Fähigkeit von OFA erheblich verbessert und verschiedene Transfer-Learning-Techniken erforscht und ihre Vorteile gezeigt. Wir haben eine neue Metrik namens Sensitivität entworfen. Noch eine Sache: Wir sammeln einen viel größeren multimodalen Anweisungsdatensatz mit rund 150 zusätzlichen variantensprachlichen Aufgaben und werden sie veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Yusuf Zhang von der Penn State University. Heute werde ich unsere Arbeit vorstellen, Exampler, Crosslingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungspräsentationen. Semantisches Parsing ist also die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Und Crosslingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungspräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda, FunQL usw. übersetzen. Bestehende Cross-lingual Semantic Parsing-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Beispielsweise gibt es Lücken bei der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt und Lücken bei der Abdeckung bestimmter vieler Repräsentationen. Der Lambda-Kalkül fehlt. Oder sie werden nur auf bestimmten neueren Modellen bewertet. Beispielsweise gibt es nur ein einziges Modell zur Bewertung. Zu diesem Zweck schlagen wir Exampler vor, wir stellen einen einheitlichen Datensatz Exampler für Crosslingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungspräsentationen zur Verfügung. Er enthält neun Datensätze in verschiedenen Bereichen, fünf semantische Parsing-Steuern, acht Bedeutungspräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unseren Benchmark besser bewerten zu können, betrachten wir die sechs Einstellungen für Training und Bewertung. Die erste ist TranslateTest. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein einsprachiges Modell zum Training und zur Bewertung. Und zum Beispiel trainieren wir das englische Modell auf... auf einer englischen Anfrage und während der Inferenz übersetzen wir die deutsche Anfrage mit der API ins Englische und verwenden dann das trainierte Modell, um die SQL vorherzusagen. Und wir testen auch ein einsprachiges Modell. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache. Zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die einsprachige Fusions-Einstellung, indem wir einsprachige Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen ein einsprachiges mehrsprachiges Modell, das wir für alle Sprachen trainieren. Zum Beispiel bringen wir die deutschen, englischen und chinesischen Anfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen oder et cetera zu übersetzen. Und wir betrachten auch Crosslingual Zero Shot und Field Shot Transfer. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es also auf einer englischen Anfrage oder der Kombination aus englischen und deutschen Field Shot-Anfragen, um ein mehrsprachiges Modell zu trainieren und die SQL-Ausgabe vorherzusagen. Und wir finden auch viele interessante Ergebnisse. Was die Analyse einsprachiger Modelle betrifft, bewerten wir also zwei Gruppen von Modellen, einschließlich Encoder PDR, was für mehrsprachige vorab trainierte Encoder mit zeigerbasierten Decodern steht, wie XLMR plus PDR und BERT plus PDR. Und wir bewerten auch Encoder-Decoder-Modelle, was mehrsprachige vorab trainierte Encoder-Decoder-Modelle sind, wie MBART und MT5. Wir fanden heraus, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Und wir bewerten auf MT5 und XLMR plus PDR in der mehrsprachigen Einstellung. Wir fanden heraus, dass Encoder-Decoder oder Encoder PDR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Und wir fanden heraus, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungsgewinn erzielen können, mit der Ausnahme, dass die englische Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen zunimmt. Ich denke, dies ist als Kurven der Multilingualität bekannt. Wir verglichen auch die Cross-lingual Performance Gap. In dieser Abbildung ist die blaue Linie der Cross-lingual Fuchsia Transfer. Die orange Linie ist der Cross-lingual Zero-Shot Transfer, während die grüne Linie die einsprachige Einstellung ist. Wir fanden heraus, dass wir durch den Vergleich der grünen und der orangen Linie herausfanden, dass für die Zero-Shot-Einstellung die Cross-lingual Performance Gap signifikant ist. Und durch den Vergleich der blauen und der orangen Linie fanden wir heraus, dass für die Fuchsia-Einstellung der Transfer-Gap schnell verkürzt wird. Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder die Fortschrittsarbeit oder erzielt vergleichbare Ergebnisse. Das Training auf der englischen natürlichen Sprache kann die Leistung von Fushot auf gezielten natürlichen Sprachen erheblich steigern. Und wir fanden heraus, dass mehrsprachige Sprachmodelle wie Codice und Bloom immer noch unzureichend für Cross-lingual Semantic Parsing-Aufgaben sind. Zusammenfassend haben wir Exampler aufgebaut, einen einheitlichen Benchmark für Cross-lingual Semantic Parsing mit mehreren natürlichen Sprachen und vielen Repräsentationen. Wir haben eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durchgeführt. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Und willkommen, unseren Artikel und unseren Code zu besuchen. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Spirakowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So wird zum Beispiel in den universellen Abhängigkeiten die Struktur der koordinierten Koordination Lisa, Bart und Maggie so angenommen, dass der erste Konjunkte der Kopf der gesamten koordinierten Struktur ist, in diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igor Milchuks Bedeutungsteorienteorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkten angeführt wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie heben einen der Konjunkten hervor. Nun gibt es auch symmetrische Ansätze zu koordinierten Strukturen wie der Prager Ansatz, der Ansatz, der in den Prager Abhängigkeitsbaumkorpora angenommen wird, wo koordinierte Strukturen vom Konjunktion angeführt werden. So erhalten wir Abhängigkeiten von und zu allen Konjunkten. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der zum Beispiel in Cutson's Wortgrammatik verwendet wird, wo sozusagen alle Konjunkten die Köpfe der koordinierten Struktur sind, so dass wir Abhängigkeiten vom Regierer hier lacht zu allen Konjunkten separat, das sind Bart und Maggie. Nun ist das Ziel dieses Papiers, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu liefern. Okay, das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. So bevorzugen in Englisch direkte Objekte im Allgemeinen die Nähe zum Verb, während Adjunkte weiter entfernt sein können, richtig? Also ist March las es gestern in Ordnung, weil das direkte Objekt es nahe am Verb steht, während March las gestern es viel schlechter ist, richtig? Denn hier steht zwischen dem Verb und dem direkten Objekt ein Adjunct gestern. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, denn dann kann es an die Position nach dem Adjunct versetzt werden. Das wird hier veranschaulicht. Beide Sätze sind also in Ordnung. March las gestern dieses absolut faszinierende Buch über die Bienen ist okay, wo wir statt es dieses lange NP haben. Es ist auch in Ordnung zu sagen March las gestern dieses absolut faszinierende Buch über Bienen. Die Begründung hier ist, dass dies möglich ist, weil dieser Satz zwar das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also die, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir eine Abhängigkeit von red zum Adjuncten mit einer Länge von 7, gemessen in Wörtern, und von red zu book mit einer Länge von 4, also zusammen 11. Wenn Sie diese beiden Konstituenten verschieben, wird die Summe dieser beiden Abhängigkeiten sechs, also statt 11 sechs, viel kürzer, deshalb klingt das ganz in Ordnung, richtig? Es verletzt ein Prinzip, erfüllt aber ein anderes. Okay, also was wir getan haben, wir haben verschiedene Statistiken über die Koordination aus der erweiterten Version des Penn Treebanks extrahiert und sehen Sie das Papier, warum wir keine universellen Abhängigkeiten verwendet haben. Und diese Statistiken bestätigen die schon oft gemachte Beobachtung, dass linke Konjunkten dazu neigen, kürzer zu sein. Also Salz und Pfeffer und nicht Pfeffer und Salz, gemessen in Silben. Und auch die Beobachtung, die beiläufig gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Wenn also die Differenz zwischen den Längen der beiden Konjunkten wächst, bevorzugt der kürzere Konjuncte es stärker, der erste zu sein, richtig? Also ist der Anteil der linken kurzen Konjunkten größer. Aber was neu an diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regierer links steht oder abwesend ist, richtig? Also steht der Regierer in diesem Beispiel links. Ich sah Bart und Lisa. Also ist der Regierer, er steht links. Er ist im zweiten Beispiel abwesend, Homer kam und nieste hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regierer, richtig? In solchen Fällen bevorzugt der linke Konjuncte es, kürzer zu sein, je größer die Differenz zwischen den beiden Konjunkten ist. Wenn jedoch der Regierer rechts steht, wie hier, regiert links die Koordination Ted und Net, verschwindet dieser Effekt. Das zeigen wir, indem wir die Länge in Zeichen messen, das ist die erste Spalte, in Silben, die mittlere Spalte, und in Wörtern, die rechte Spalte. Also werde ich mich auf die rechte konzentrieren. Was wir hier sehen, ist, dass wenn der Regierer links steht, die Tendenz des linken Konjunkten, kürzer zu sein, stetig mit der absoluten Differenz in Wörtern wächst und dasselbe wird beobachtet, wenn es keinen Regierer gibt, wie bei der Koordination von Sätzen, aber wenn der Regierer rechts steht, verschwindet diese Tendenz und wir zeigen im Papier, wie das ein Argument gegen asymmetrische Strukturen der Koordination wie diese beiden und für die symmetrischen Strukturen wie diese beiden liefert. Also sehen Sie das Papier für die vollständige Übereinstimmung und Argumente und sprechen Sie mit uns darüber nach der Sitzung. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kyo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengesteuerte mehrsprachige Untersuchung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emily Liu, Andre FD Martins und Graham Newbig erstellt. So viele Übersetzungen hängen vom Kontext ab. Wie würden wir zum Beispiel „Mole“ in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautete „Die Lage könnte gefährlich werden, wenn die Minister es herausfinden“, dann bezieht sich „Mole“ auf einen Spion. Aber wenn der vorherige Satz lautete „Könnte es etwas Ernstes sein, Doktor?“, dann bezieht sich „Mole“ auf einen Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Es ist jedoch ziemlich schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was bedeutet, dass Korpusmetriken wie Blue diese Übersetzungen nicht erfassen können. Einige Leute haben eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsätze, da sie in der Regel auf Domänenkenntnissen und menschlicher Kuratierung basieren. In dieser Arbeit versuchen wir, diese zwei Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir begonnen, zu messen, wie sehr ein Wort während der Übersetzung vom Kontext abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodelle eingeführt. Dies geschieht, indem gemessen wird, wie viel Information der Kontext C über das Ziel Y unter Berücksichtigung der Quelle X liefert. Man kann sich CXMI als die Information vorstellen, die durch die Bereitstellung von Kontext für das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu punktweisem CXMI, das die Kontextnutzung auf Satzebene oder Wortbene messen kann. Wir können Wörter mit hohem PCXMI als solche betrachten, die für die Übersetzung Kontext erfordern. Nun analysieren wir Wörter mit hohem PCXMI, um nach Mustern zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse an Transkripten von TED-Talks durch, die aus dem Englischen in vierzehn verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir Teile des Sprachgebrauchs, die hohe durchschnittliche PCXMI-Werte haben. Dies ermöglicht es uns, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe p six mi-Werte haben. Dies lässt sich erklären, weil Englisch keine Dualpronomen hat. Man benötigt also Kontext, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die passende Verbform wählen möchten. Dann betrachten wir Vokabelartikel, die über alle ihre verschiedenen Vorkommen hinweg hohe p six mi-Werte haben. Dies hilft uns, Fälle wie den hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um richtig zu übersetzen. Und schließlich stellen wir fest, dass Kontext wichtig ist, um in der richtigen Form zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token, die hohe p6mi-Werte haben. Dies ermöglicht es uns, Phänomene zu identifizieren, die vom Wort selbst nicht wirklich erfasst werden können, sondern eher in einer Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipse. Nun verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Übersetzung auf Dokumentebene zu entwerfen. Für jedes der fünf von uns identifizierten Diskurs-Phänomene erstellen wir Tagger, um Wörter, die zum Phänomen gehören, automatisch zu identifizieren, und wir nennen unseren Tagger den mehrsprachigen Diskursbewussten oder MUDA-Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskurs-Phänomene haben. Wir verwenden dann den MUDA-Tagger, indem wir den Tagger auf den Parallelkorpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere bevorzugten Übersetzungsmetriken auf die kontext-abhängigen Beispiele an, die der MUDA-Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf Dokumentebene maschineller Übersetzung zu bewerten. Zunächst einmal, wenn wir Korpus-Level-Metriken verwenden, also für Blue, stellen wir fest, dass kontextunabhängige Modelle die beste Leistung haben, aber dann, wenn wir Comet verwenden, erzielen kontextbewusste Modelle die beste Leistung. Und wenn wir die WordF-Metrik verwenden, dann haben Modelle mit oder ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem auf Dokumentebene zu bestimmen, wenn wir nur Korpus-Level-Metriken allein verwenden. Nun verwenden wir den MUDA-Benchmark, um Modelle zu bewerten, und wir stellen fest, dass kontextbasierte Modelle signifikant genauer sind als Modelle, die für bestimmte Diskurs-Phänomene wie Form und lexikalische Kohäsion keinen Kontext verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die für andere Phänomene wie Ellipse, Pronomen und Verbform keinen Kontext verwenden. Dies deutet also darauf hin, wo wir mehr Fortschritte bei der Übersetzung auf Dokumentebene sehen müssten. Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DPL in der Regel genauer als Google Translate für die Übersetzung auf Dokumentebene ist. Zusammenfassend führen wir eine datengesteuerte Analyse über vierzehn Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und dann verwenden wir unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, der uns helfen kann, zu identifizieren, welche diskreten Phänomene Modelle gut oder nicht gut bewältigen können und welche Übersetzungssysteme gut in der Übersetzung auf Dokumentebene sind. Vielen Dank für Ihre Aufmerksamkeit. Bis morgen."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich eure Arbeit präsentieren: Anal Positionale, Charakterisierung von Design-Verzerrungen, Beta-Sätze und Modelle. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santi, Ronin Lebras, Katarina Reinicke und Martin Sapp. Beginnen wir also damit, uns vorzustellen, dass ihr für eine Zeitung arbeitet und Kommentare unter eurem Nachrichtenartikel sichten, um toxische Inhalte zu entfernen. Ihr könntet euch an eine beliebte API wie die Perspective API zur Toxizitätserkennung wenden, und das funktioniert wirklich gut, wenn ihr Carl Jones seid. Hier sind die Perspektiv-APIs in der Lage, toxische Instanzen korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Dithya Sharma, wo die Perspektiv-APIs nicht so empfindlich gegenüber beleidigenden Begriffen sind, die in indischen Kontexten häufiger vorkommen. Das ist ein Beispiel für eine Design-Verzerrung, bei der wir systematische Leistungsunterschiede der Technologie zwischen Bevölkerungsgruppen sehen. Design-Verzerrungen wie die, die wir gerade zuvor gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen vertreten. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Und als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann. Und so stellt sich die Frage, ob Datensätze und Modelle eine Positionalität haben? Und wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können so bestimmte Positionalitäten gegenüber anderen repräsentieren. Frühere Arbeiten haben einige anekdotische Beweise für die Positionalität vorgeschlagen, wie kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen der Modell-Positionalität. Diese Arbeiten betrachten jedoch nicht wirklich den Vergleich von Endbenutzern mit den Datensätzen und Modellen selbst. Und die Untersuchung der Positionalität von Modellen und Datensätzen wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozialer orientiert werden. Und es ist herausfordernd zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. Um also die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen mit echten Benutzern mit bestehenden Datensätzen und Modellen. Wir tun dies durch unser Framework, NLPositionality. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit verschiedenen Annotatoren neu zu annotieren. Und wir entscheiden uns dafür, dies zu tun, anstatt die Demografie der ursprünglichen Datensätze und Annotatoren zu betrachten, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografie selten gesammelt und geteilt wird. Und so entscheiden wir uns dafür, Daten neu zu annotieren, um viele Annotatoren pro Instanz zu erhalten und um einen reichen Satz an demografischen Daten zu erhalten. Wir nehmen dann die Annotationen durch demografische Daten und vergleichen sie mit den Modellen und Datensätzen unter Verwendung eines Parsons R-Korrelationsscores. Unser Framework unterscheidet sich also von der Literatur zur Diskrepanz zwischen Annotatoren, indem es Endbenutzer mit Vorhersagen und Etiketten von Modellen und Datensätzen vergleicht, anstatt nur die Übereinstimmung oder die Verteilung der Annotatoren zu betrachten. Unser Framework wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Vergleich zu Plattformen wie MTurk, die größtenteils Teilnehmer aus den USA oder Indien haben. Und weiter kann Lab in the Wild immer noch hochwertige Daten erhalten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist soziale Akzeptabilität. Und so funktioniert es: Die Teilnehmer lesen eine Situation aus dem Social Chemistry-Datensatz und schreiben dann auf, wie sozial akzeptabel eine Situation ist. Anschließend können sie, um in der Studie engagiert zu bleiben, ihre Antworten mit einem KI und anderen vergleichen. Wir haben dann diese Annotationen mit Social Chemistry Delphi in GPT 4 verglichen. Wir haben dann eine sehr ähnliche Einrichtung für die Aufgabe zur Erkennung von Toxizität und Hassrede repliziert, bei der sie eine Instanz aus DynaHate lesen und schreiben, ob sie denken, dass es sich um eine Hassrede handelt. Wir haben dann diese Annotationen mit DynaHate, Perspective API, Rewire API, Hate Roberta in GPT 4 verglichen. Unsere Studie hat am Ende über sechzehntausend Annotationen von über tausend Annotatoren aus achtundsiebzig Ländern gesammelt. Jetzt sind wir also besser ausgestattet, um zu beantworten, mit wem stimmen NLP-Datensätze und -Modelle am meisten überein? Wir stellen fest, dass es in NLP eine Positionalität gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die GPD 4-Analyse der sozialen Akzeptabilität stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass DanaHate auch am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass es am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung haben. Für GPD 4 in der Aufgabe zur sozialen Akzeptabilität stellen wir fest, dass sie am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung oder eine Graduiertenausbildung haben. Und wir stellen dasselbe für DanaHate fest, wo es am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung haben. Wenn Modelle und Datensätze jedoch auf spezifische Populationen abgestimmt sind, werden einige unvermeidlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger auf nicht-binäre Menschen abgestimmt sind als auf ihre männlichen und weiblichen Gegenstücke. Wir stellen dies sowohl in der GPT 4-Aufgabe zur sozialen Akzeptabilität als auch in der DynaHate-Aufgabenanalyse fest. Angesichts der Tatsache, dass es in NLP eine Positionalität gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dazu. Die erste ist, alle relevanten Designentscheidungen während des Forschungsprozesses zu dokumentieren. Und die andere ist, NLP-Forschung durch die Linse des Perspektivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle in vier spezifischen Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakane-Initiative. Und wir möchten betonen, dass inklusive NLP nicht nur darin besteht, alle Technologien für jeden nutzbar zu machen. Und damit schließt unsere Präsentation. Aber wenn ihr mehr erfahren möchtet, schaut euch gerne unser Dashboard für die aktuellsten Analysenergebnisse und unser Papier an. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit an der Lösung indirekter Verweise für die Entitätauswahl sprechen, bei der wir den altentity-Corpus vorstellen. Mein Name ist Javot Hosseini, und dies ist eine gemeinsame Arbeit mit Philip Radlinsky, Silvia Pareti und Annie Luis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Betrachten Sie diese alternative Frage. Meinten Sie Easy on Me oder I got a feeling? Hier möchte ein Nutzer zwischen einem dieser beiden Lieder wählen. Das Offensichtlichste ist, einen direkten Verweis zu verwenden, indem man beispielsweise den Namen des Liedes Easy on Me oder seine Position als erstes Lied nennt, aber manchmal ist ein indirekter Verweis angemessener, um ein natürlicheres Gespräch zu führen. Dies könnte passieren, wenn sich der Nutzer den Namen des Liedes nicht erinnern kann oder die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Verweise. Zum Beispiel, das neuere oder das Lied, das nicht energiegeladen ist. Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Bewertung des Entitätsverständnisses von LLM. Uns ist kein öffentlicher Datensatz, ein groß angelegter öffentlicher Datensatz für die Aufgabe bekannt, daher sammeln wir einen mit Crowd-Annotation. Unser Datensatz umfasst drei verschiedene Bereiche, Musik, Bücher und Rezepte. Unsere Datensammlungsmethodik legt Wert auf Unformalität unter Verwendung eines Cartoon-Abschluss-Setups. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob, erinnert ihr euch an das Lied, das wir gestern gehört haben? Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice, meinen Sie Easy on Me oder I got a feeling?, was die alternative Frage ist. Und in der dritten Sprechblase verwendet Bob einen indirekten Verweis, um eine dieser Entitäten auszuwählen, zum Beispiel den Neo-Ervandal. Wir stellen die erste und zweite Sprechblase automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Hinweisen pro Bereich ausgewählt. Die zweite, die die alternative Frage ist, wird wie folgt generiert. Wir verwenden immer eine einfache Vorlage. Meinen Sie A oder B? wobei A und B von Wikipedia ausgewählt werden. Hier sind die verschiedenen Auswahlmethoden, die wir verwendet haben. Wenn wir weiter oben in der Liste gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Disambiguierung durchzuführen. Das erste ist einheitlich zufällig, das zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen, die sie zurückgeben, das dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler für ein Lied, wenn wir diese alternative Frage den Annotatoren zeigen. Sie kennen den Namen dieser Entitäten, aber sie wissen nicht unbedingt etwas über die Entitäten. Also zeigen wir einige Hintergrundinformationen über die beiden Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten dann die Annotatoren, mindestens einige von jedem Lied anzuhören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied Easy on Me. Für den Bereich Rezepte und Bücher zeigen wir einige Hintergrundtexte von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder erneut von Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Verweisenden ausdrücken zu beschreiben. zum Beispiel diejenige mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz, zum Beispiel diejenige ohne Worte, nicht diejenige mit dem 12-jährigen Jungen oder die fiktive oder kommt aus Aserbaidschan und so weiter. Der altentity-Corpus hat 6.000 alternative Fragen in drei Bereichen und hat 42.000 indirekte Verweisende Ausdrücke. Die Ergebnisse mit dem T5xLarge-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugang zu genau demselben Hintergrundwissen wie die Annotatoren hat, ist die Genauigkeit sehr hoch. Sie liegt bei etwa 92-95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlappenden Hintergrundwissen hat, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugang zu Entitätsnamen hat, liegt die Genauigkeit nur bei 60%. Es gibt also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle bereichsübertragbar sind. Hier ist ein Link zu unserem Datensatz. Danke."}
