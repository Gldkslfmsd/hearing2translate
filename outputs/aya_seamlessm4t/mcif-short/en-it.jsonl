{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, benvenuti alla nostra presentazione del nuovo corpus per l'identificazione dei testi in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò alla prima parte della presentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è il processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come le persone con problemi di lettura o i non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "E nell'esempio qui riportato, è possibile osservare una coppia di frasi allineate in parallelo, costituita da una complessa frase tedesca e la sua traduzione in un linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "per semplificare la frase, sono possibili diverse tecniche come possiamo vedere nell'esempio, quali la sostituzione lessicale, la cancellazione di clausole, la riorganizzazione di clausole o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo ora il nostro nuovo corpus di planum, poiché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti, quindi, per esempio, questi corpora qui sono troppo piccoli per addestrare un modello tassonomico."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere altrettanto soggetti a errori nei loro allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo quindi il nostro nuovo corpus di testi, suddiviso in due sottocorpus: corpus APA e corpus web. Il corpus APA si basa su testi di notizie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "nel piano AP abbiamo allineato quattrocentotrentatré documenti, tutti manualmente. Il risultato è di circa trentamila tredicimila coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Per il Deep Web, questo corpus include diversi domini e allineiamo tutti questi 750 documenti da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo ottenuto trentamila quattrocentocinquanta coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo un po' di più le nostre frasi, ad esempio sul tipo di semantizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come puoi vedere qui, i testi biblici sono molto più semplici rispetto, per esempio, ai testi di attualità o a quelli per apprendisti della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, per esempio, per esempio, semplificazione lessicale, semplificazione strutturale, tutti gli altri livelli di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, è evidente che il nostro corpus di profondità presenta una grande varietà di diverse trasformazioni di amplificazione. Ad esempio, nel corpus API di profondità abbiamo molti più riordini e aggiunte di parole rispetto a quanto osservato nel corpus web di profondità."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Omar e ora parlerò dei casi d'uso per il nostro set di dati D-plane."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove disponiamo di due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi nei documenti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno la stessa lingua, lo stesso contenuto, ma si trovano a un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora che abbiamo il nostro insieme di dati, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine abbiamo concluso che il miglior metodo di allineamento per la semplificazione del testo in tedesco è il metodo dell'allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti nel paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è quello della semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "attraverso il perfezionamento dei modelli linguistici per generare testo semplificato da un testo in ingresso complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli diversi. Abbiamo affinato il modello di input lungo per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E affiniamo anche la base normale in parte per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Puoi trovare anche tutti i punti di controllo e puoi esaminare maggiori dettagli sui punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo semplice aggiustamento fine potrebbe produrre o migliorare i punteggi rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo quei risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica dei testi in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo moltissimo per la sua attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Shvirkovsky e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Potresti sapere che diverse strutture di dipendenza sono definite da diverse teorie e processi, quindi, per esempio, nell'universo le dipendenze sono la struttura della struttura coordinata di Lisa e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il nucleo dell'intera struttura centrale, quindi in questo caso Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "La prima è che l'intera struttura è controllata dalla prima congettura, quindi questi due approcci sono simmetrici, quindi quello fuori dalla congettura."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora l'approccio simmetrico alle strutture coordinate, come l'approccio Prag, il processo di congiunzione, il processo sincrono, le strutture sincroniche sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo alcune dipendenze dalla fine a tutti i contratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, si tratta anche di un approccio multifunzionale utilizzato, per esempio, nella grammatica mondiale dei \"Catchers\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "Quindi tutte le congetture sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore, qui ama condurre tutto separatamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è presentare un nuovo argomento a favore delle strutture simmetriche di coordinazione come questa e contro le strutture asimmetriche di coordinazione come questa."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "In italiano, come potresti sapere, l'oggetto diretto è preferibile che sia vicino al verbo, mentre un complemento oggetto indiretto può essere più distante, tanto da non creare problemi poiché l'oggetto diretto è prossimo al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene marzo abbia letto ieri, è molto peggio proprio ora, perché qui tra il verbo e l'oggetto diretto c'è \"ieri\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere migliorato quando l'oggetto diretto è molto pesante e molto lungo, poiché in tal caso può essere spostato nella posizione successiva al salto nell'aria."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo viene illustrato qui in modo tale che entrambe queste frasi sono corrette, tanto che il libro sulla B.C. di ieri è assolutamente affascinante."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire che Marge ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale secondo cui l'oggetto diretto dovrebbe essere posizionato accanto al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che sono preferite dipendenze più brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, cioè quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo la dipendenza dal rosso al bordo di sette nelle parole e dal rosso al libro di quattro per ottenerlo."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando ti muovi, quando scambii queste due costituenti, la somma di queste due dipendenze diventa sei, quindi è sedici, ed è per questo che suona piuttosto bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Bene, quindi ciò che abbiamo fatto è estrarre varie statistiche dalla versione coordinata del Pentium Bank e vedere nel documento perché non abbiamo utilizzato dipendenze universali."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "E queste statistiche confermano l'osservazione già fatta in precedenza che i gemelli siamesi di sinistra tendono ad essere più bassi, quindi sale e pepe e non sale e pepe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione fatta quasi per inciso che questa tendenza aumenta con differenze molto, molto lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze delle due articolazioni congiunte aumenta, le articolazioni congiunte più corte sono le prime a diventare più forti, quindi la proporzione è maggiore rispetto alle articolazioni congiunte sinistre."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è di sinistra o assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Bene, quindi il governatore si trova a sinistra in questo esempio. Ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio, la casa di Kamen e Sneeze, dove abbiamo la coordinazione di due parole e ora il governatore esterno #ah destro, quindi in tali casi la conchiglia sinistra preferisce essere la più corta, #ah maggiore è la differenza tra le due."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è di destra, come in questo caso, la sinistra governa la coordinazione della rete, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Dimostriamo quindi che, misurando la lunghezza in caratteri, la prima colonna in sillabe, la colonna centrale in parole e la colonna di destra in unità, mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che stiamo dicendo è che quando il governatore è a sinistra"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza a sinistra ad essere più breve cresce gradualmente con la differenza assoluta nelle parole e lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi. Tuttavia, quando il governatore è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E nel documento dimostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due e a favore di strutture asimmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Per leggere l'intero accordo e le argomentazioni, si prega di consultare il documento. Ci scusiamo per eventuali inconvenienti e vi invitiamo a contattarci per discutere della sessione postale. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Sono un dottorando presso l'Università di Washington e oggi presenterò il nostro lavoro sul modello linguistico al modello linguistico al modello linguistico al modello linguistico al modello linguistico al modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici sono addestrati su dati di grandi dimensioni ottenuti tramite web crawling."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media politici sono inclusi nella formazione preliminare, secondo un'indagine condotta su quattro giornali, tra cui il New York Times, il Los Angeles Times, il Guardian e l'Huffington Post. Siamo inclusi nella formazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una benedizione a metà per l'applicazione dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, possono essere visti da diverse prospettive, che celebrano la democrazia e il pluralismo delle idee, dall'altro, queste diverse vedute politiche sono socialmente distorte e potenzialmente ingiuste in termini di applicazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché proponiamo di indagare il flusso di propaganda politica dai modelli linguistici ai modelli linguistici, specificamente ponendoci le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo le inclinazioni politiche dei modelli linguistici e quale ruolo hanno i dati personali su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si utilizzano diversi modelli linguistici con diversi partiti politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo specificamente di proporre due diversi modelli linguistici con formati differenti utilizzando questionari politici come il test della bussola politica, ciò garantisce una valutazione automatica nel campo della scienza politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari mostrano che i primi modelli linguistici presentano ancora diverse tendenze politiche, occupando tutti e quattro i quadranti del campo politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che GPT4 è il modello linguistico più liberale di tutti e la teoria GPT è generalmente più socialmente liberale della teoria BERT e delle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, indagheremo in che misura i modelli linguistici politici sono effettivamente acquisiti dai dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Quindi possiamo controllare l'esperimento attraverso ulteriori test sui punti di controllo linguistici e sei diverse parti dell'azienda sono suddivise tra notizie e media sociali e sono divise in ambito politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Attraverso un ulteriore addestramento dei modelli linguistici e confrontando i due, possiamo osservare che le coordinate ideologiche del modello linguistico corrispondono anch'esse allo stesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Robert, un ulteriore scoperta, un ulteriore addestramento sul corpo rosso mancino, possiamo osservare uno spostamento liberale sostanziale in termini di suo"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini di pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare su come i modelli linguistici possano assorbire la polarizzazione che è prevalente nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi stiamo dividendo il corpo di pre-addestramento in due, il quarantacinquesimo presidente degli Stati Uniti e il quarantacinquesimo presidente degli Stati Uniti, e poi stiamo separando i modelli linguistici in due diversi corpi temporanei."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che i modelli linguistici hanno generalmente un significato politico che risale a più di ventisette anni fa, quindi questo modello linguistico può essere utilizzato anche per descrivere la polarizzazione nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi non saremo in grado di valutare i modelli linguistici con diverse prospettive politiche e capacità di rilevamento del linguaggio e di riporto delle notizie, per cui avremo due applicazioni che sono modelli linguistici e possono avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi diremo che, se analizziamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in base a..."}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Osservando diverse demografie o media politici, possiamo notare che, per esempio, per il riconoscimento del parlato, i modelli linguistici sinistrorsi sono migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nell'individuare discorsi d'odio rivolti a gruppi minoritari sociali"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, siamo all'inizio del rilevamento di discorsi d'odio rivolti a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "E a proposito, i modelli linguistici sono più efficaci nel contrastare il linguaggio discriminatorio verso i bianchi e le bianche, ma sono ancora più performanti nel contrastare il linguaggio discriminatorio verso le persone nere, la comunità LGBTIQ+ e altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Anche per il rilevamento delle fake news si osservano tendenze simili, dove i modelli linguistici di orientamento progressista sono più abili nel rilevare disinformazione proveniente dai loro opposti politici e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questa sede ti mostreremo quanti esempi qualitativi osservare per vedere i modelli linguistici con diversi significati politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "Puoi fornire diverse previsioni ai discorsi e agli esempi di informazioni nelle categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ciò."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che esiste un problema di equità molto pressante relativo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se vengono trovati i modelli linguistici giusti, è possibile scoprire informazioni sul discorso e sull'informazione e utilizzarle sulle piattaforme dei social media."}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che i discorsi d'odio rivolti ai gruppi minoritari potrebbero diffondersi impunemente senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo sembra essere l'allarme per riconoscere e affrontare le questioni di equità causate dai modelli linguistici politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Nella discussione, vorremmo inoltre evidenziare che spiegheremo il linguaggio unico del linguaggio politico, che si colloca tra i due."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non standardizziamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se cerchiamo di sanitarlo in qualche modo, otterremo anche censura o esclusione e è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere conservato nella lingua, quindi è un po' come un problema elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ottimo. Penso che questo sia più o meno tutto per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Sono uno studente del primo anno di dottorato presso l'Università Carnegie Mellon e sto presentando il mio lavoro in una posizione di responsabilità, progettando sulla base dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con l'Università di Washington e l'Istituto per lo Studio della Rivoluzione Americana, in particolare con Sebastian Santee, Ronan Labrina, Catherine Rankin e Martin Sap."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo quindi immaginando di lavorare per un giornale e di commentare un articolo di attualità, cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Puoi rivolgerti a popolari app come quelle per la rilevazione di tossicità, che sono davvero utili se sei un fumettista."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è davvero il caso di Aditya Sharma, la cui prospettiva non è particolarmente sensibile ai termini offensivi e ai contesti più indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio nel design in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "L'unica cosa che assomiglia a ciò che abbiamo appena visto è il posizionamento dei ricercatori di NLP e degli sviluppatori di modelli. Il posizionamento è semplicemente la prospettiva che le persone hanno come risultato delle loro demografiche, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli ambiti femminista e accademico."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E come ricercatore, la prospettiva può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione o un punto di vista?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "Non stiamo cercando di affermare che i modelli e le modelle abbiano identità demografiche e esperienze di vita, ma le opinioni aggregate delle persone reali possono rappresentare determinate posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il primo lavoro è suggerire alcune delle prove di avere una posizione, come i divari culturali e i modelli e i dati, nonché le definizioni del posizionamento del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano effettivamente sul confronto tra gli utenti finali con i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio del posizionamento di modelli e dati diventa sempre più importante man mano che i test di NLP diventano più soggettivi e orientati al sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E caratterizzare in che modo queste \"possessionalità\" sono distorte è una sfida, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare la posizione del set di dati e del modello, confrontiamo effettivamente le annotazioni con gli utenti reali con i set di dati e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro framework, la posizionamento NL (Natural Language)."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due passaggi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è quello di ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E esamineremo la demografia dei set di dati originali, poiché solitamente solo pochi di questi set di dati vengono raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E così abbiamo scelto di rianalizzare i dati per ottenere più entità per istanza e per avere un insieme ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Quindi prendiamo le annotazioni demografiche e le confrontiamo con i modelli e i dataset utilizzando il nostro punteggio di correlazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "Ed è per questo che il nostro framework si differenzia dall'Accordo dell'Annotatore, confrontando gli utenti con modelli, set di dati e etichette, e analizzando solo l'Accordo dell'Annotatore o la Distribuzione dell'Annotatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte reso possibile grazie a Lab e Wild, una piattaforma online di crowdsourcing per ex collaboratori dell'HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Nel mondo dell'esperimentazione online, possiamo reclutare volontari per confrontare le piattaforme con quelle degli Stati Uniti e dell'India, e il mondo dei dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Nel mondo abbiamo due test: uno è l'accettabilità sociale e l'altro è comprendere come funziona, ovvero se i partecipanti saranno in grado di valutare la situazione attraverso i dati della chimica sociale e quanto sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle dell'intelligenza artificiale e di altri partecipanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi replicato in modo molto simile per il test di rilevamento della tossicità e del linguaggio, dove abbiamo osservato istanze dai sordi e dai destri e qual è il significato del linguaggio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, confrontiamo questi confronti con i dati dall'A.P.I. (A.P.I.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.E.R.) e dal G.P.D. (G.P.D.E.R.E.R.) nello studio di sedici mila sedici mila osservazioni provenienti da ottantasette paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Ora dovremo determinare chi si occuperà dei set di dati NLP con il maggior numero di righe di dati. Scopriremo che è posizionato nell'ambito dell'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo riscontrato che i dati sono prevalentemente nei paesi anglofoni, quindi per il GPD (Prodotto Interno Lordo) relativo all'analisi della responsabilità sociale, abbiamo scoperto che è per lo più nei paesi anglofoni, e abbiamo anche trovato che è presente in questi stessi paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo anche che la maggior parte delle persone con un'istruzione universitaria ha maggiori probabilità di possedere un'istruzione universitaria. Pertanto, per il G.P.D. nel compito di socializzazione, troviamo che la maggior parte delle persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E lo stesso vale per Danny Hate, dove è più allineato alle persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente esclusi."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i set di dati non sono buoni come quelli delle persone non binarie rispetto ai corrispondenti uomini e donne. Riscontriamo questo nei quattro test di accettazione sociale del G.P.D. nonché nel test D.N.H."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, dato che esiste una posizione in LED e LP, cosa possiamo farci?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni a riguardo. La prima è di mantenere un registro di tutte le scelte di design rilevanti durante il processo di ricerca e l'altra è di condurre ricerche di NLP sullo spettro della percezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di costruire set di dati e modelli specializzati in collaborazione con comunità specifiche, e un ottimo esempio in tal senso è l'iniziativa Masakani. Desideriamo sottolineare che il nostro obiettivo non è semplicemente quello di far funzionare tutte le tecnologie per chiunque."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questa è la presentazione, ma se volete vedere di più, sentitevi liberi di consultare i risultati e i documenti più aggiornati. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo C. Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro: \"Distinguere la conoscenza della scrittura dai modelli linguistici leggeri per la pianificazione linguistica vincolata\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script guidati."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Il lavoro precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come eseguire un calcio, e ha dimostrato che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come preparare una torta al cioccolato, rimane ancora inesplorata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impone vincoli diversi sugli obiettivi della pianificazione; un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, nulla al di fuori di obiettivi specifici esiste per individuare il nostro sguardo fisso."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo prima acquisire questi obiettivi, come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'essere umano nell'acquisizione dei dati di sguardo, utilizzando GPT istruttivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo campionato un centinaio di obiettivi specifici e valutato gli script generati da modelli più grandi."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Riscontriamo che tutti i modelli lineari ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, conduciamo un'analisi dettagliata per indagare su ciò che i modelli di livello terrestre mirano a studiare."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati mostrati nelle figure indicano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Esploriamo categorie tematiche più definite di vincoli definiti nel funzionamento di come. La mappa principale nella figura mostra che le prestazioni di pianificazione delle istruzioni variano considerevolmente per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei grandi modelli presenta ampie variazioni, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di un filtro sovragenerato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo inizialmente tipi vincolati con esempi per ppt intransitivo, e otteniamo obiettivi specifici basati sui suddetti obiettivi astratti."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, istruire GPT a sovra-generare script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "successivamente, viene sviluppato un modello di filtro per selezionare gli script visivi"}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e obiettivi in embedding GPT intrinseci e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, evitiamo la sceneggiatura che contiene le parole chiave della restrizione target; conserviamo la sceneggiatura solo se la ragazza target ottiene il punteggio più alto."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'intuitività può generare punteggi di qualità superiore. Il nostro metodo migliora notevolmente la comprensibilità, sia per quanto riguarda la completezza semantica che la fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la pianificazione linguistica utilizzando modelli più piccoli e specializzati. La creazione di dataset è un passaggio fondamentale per raggiungere questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dataset è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per estrarre siti di dati di pianificazione linguistica vincolata da grandi modelli linguistici"}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "pianifichiamo il nostro metodo per la creazione di un dataset di pianificazione linguistica vincolata, denominato codescript"}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di convalida e di test, chiediamo ai lavoratori reclutati tramite crowdsourcing di individuare e rivedere i campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione vincolata di coscript. Riscontriamo che coscript presenta una probabilità elevata negli obiettivi specifici generati. Con coscript, possiamo optare per modelli più piccoli ma specializzati per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con l'aiuto di T-File, T-File, Tune e Courseraid, è possibile generare script di qualità superiore rispetto alla maggior parte dei moduli su larga scala, indicando che moduli più piccoli possono supportare moduli più grandi quando addestrati correttamente su siti di dati adatti."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, stabiliamo il problema della pianificazione linguistica vincolata, valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di filtraggio sovragenerativo per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo grandi modelli linguistici per generare un set di dati di script di alta qualità, chiamato codescript, per la pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo per la sua attenzione. Per ulteriori dettagli sulla sceneggiatura del codice, si prega di consultare il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Xu Hong. Oggi presenterò il nostro articolo: \"I tagger di entità denominate Cornell 2003 funzionano ancora bene nel 2023?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità denominate, o compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli hanno utilizzato CONSO 2003 per sviluppare il NER da quasi 20 anni, e questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "allo stesso tempo, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il data set Carneau+, che è un insieme di dati raccolti da Reuters News nel 2020 e poi annotati secondo le stesse linee guida di annotazione Carneau del 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi ottimizzato oltre 20 modelli sul corpus Corno 2003 e li abbiamo valutati sia sul set di test Corno 3 che sul set di test Corno+."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa è necessario per una buona generalizzazione? Dai nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "La prima è l'architettura del modello. Attraverso i nostri esperimenti abbiamo riscontrato che i modelli basati su trasformatori generalmente generalizzano meglio su nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che solitamente i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E infine, ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influenza direttamente le prestazioni di un compito a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, quali sono le cause del calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo due ipotesi: la prima è l'overfitting adattivo, che è un overfitting causato dal riutilizzo dello stesso set di test ripetutamente, e questo si manifesta solitamente come un calo dei rendimenti sul nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, come abbiamo visto dal grafico a destra, la linea di miglior adattamento rossa ha una pendenza superiore a uno."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento apportata a Color 2003 si traduce in più di un'unità di miglioramento su Color +, il che implica che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva il sovradattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, che ne è della temperatura?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti, e abbiamo riscontrato che le prestazioni diminuiscono con divari temporali più ampi."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, di una dimensione del modello più ampia e di un maggior numero di esempi di regolazione fine."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo riscontrato che il calo delle prestazioni qui è causato da una deriva temporale e, sorprendentemente, non è dovuto a un adattamento eccessivo, nonostante l'utilizzo di Conal 2003 da oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Ritornando alla domanda che abbiamo posto nel titolo del nostro articolo, le etichette del 2003 funzionano ancora nel 2023? Abbiamo scoperto che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, assicurati di consultare il nostro articolo, il nostro dataset e, se hai domande, non esitare a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione di espressioni di riferimento indiretto per la selezione di entità, in cui introduciamo il corpus delle entità alternative."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinsky, Silvia Parati e Annie Joyce."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere il linguaggio dell'utente quando desidera effettuare una scelta."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è utilizzare un riferimento diretto, per esempio dicendo che il nome della canzone è il mio o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "tutte le pronunce sono troppo simili tra loro e difficili da comprendere"}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l'utente desidera specificare una preferenza, ecco alcuni esempi di preferenze indirette, ad esempio la più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un problema importante nei sistemi di conservazione e anche per la valutazione comparativa della comprensione delle entità da parte dei LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne raccogliamo uno utilizzando il crowdsourcing. Il nostro set di dati copre tre diversi domini: musica, libri e"}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando il tuo set di completamento dei cartoni animati."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre nuvolette. Nella prima, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\" e con ciò, Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo fumetto, Alice dice: \"Intendi 'facile per me' o 'ho una sensazione'?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "qual è la domanda alternativa. E nella terza nuvoletta, Bob utilizza un riferimento indiretto per selezionare una di queste entità, per esempio, la nuova."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente i primi e secondi balloon del discorso, ma il terzo viene compilato dall'annotatore. Il primo balloon viene scelto da alcuni prompt manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "il secondo, che è la domanda alternativa, viene generato come segue"}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un semplice modello. Ti riferisci al modello A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che si sale nella lista, le entità diventano più simili tra loro e solitamente è più difficile fare la stessa equazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniforme."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, per esempio due libri con il nome del rivenditore."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando hanno descrizioni simili su Wikipedia e quando presentano caselle informative o attributi simili, come ad esempio lo stesso genere o lo stesso artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando presentiamo questa domanda alternativa agli editori, essi conoscono il nome di queste entità, ma non necessariamente ne sono a conoscenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quello che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, forniamo semplicemente un link di ricerca Google per ciascuna canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "E poi chiedi ai commentatori di ascoltare almeno una parte di ogni canzone e di leggere le informazioni su ciascuna di esse. Ecco, per esempio, il risultato della ricerca Google per la canzone \"Easy\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo alcuni testi di sfondo tratti da Wikipedia. Per le ricette, mostriamo anche le loro immagini da Wikipedia in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Chiediamo quindi agli editori di selezionare una di queste entità, per esempio la prima, e descriverla utilizzando tre o cinque riferimenti indiretti."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica al pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, quello senza parole, non quello con il ragazzo di dodici anni o quello fittizio o quello che proviene dall'Azerbaigian e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus dell'Identità comprende 6.000 domande alternative distribuite in tre domini e contiene 42.000 espressioni di riferimento indirette. I risultati ottenuti con il modello T5X Large sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso allo stesso identico bagaglio di conoscenze degli analisti, allora la precisione è molto elevata, si aggira tra il novanta-due e il novanta-cinque percento, ma questa è una situazione non realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza si attesta tra l'ottantadue e l'ottantasette percento, il che è più realistico, per esempio, quando il modello linguistico recupera la conoscenza di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo a due nomi di entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili nel dominio. Ecco un link al nostro set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Serapapi dell'Università di Trento e della Fondazione Bruno Kessler e introdurrò brevemente l'attenzione come guida per la traduzione simultanea del parlato, un lavoro svolto in collaborazione con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "La traduzione simultanea del parlato, o simulesc, è il processo di traduzione di una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli di simulazione attuali? Le architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "procedure di addestramento lunghe e complesse, per esempio l'addestramento che coinvolge diversi obiettivi di ottimizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "E addestrare e mantenere diversi modelli per ottenere diversi regimi di latenza, ad esempio, addestrare un modello con una latenza media di un secondo e un altro con una latenza di due secondi e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Qual è quindi la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, utilizzare modelli ST offline esistenti senza riaddestramento o adozione di architetture specifiche per semplicità. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "E la conoscenza viene già acquisita dal modello attraverso il meccanismo dell'input audio e dell'output testuale, che è il meccanismo dell'output audio, e puoi vedere un esempio di ciò proprio qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre un codice o codificare l'attenzione sul codice, che è una strategia attraverso la quale decidiamo se soddisfare o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "viene emessa una parola se la tensione non è concentrata, ovvero questa somma è al di sotto di una certa soglia alfa rispetto agli ultimi lambda frame del discorso, il che significa che le informazioni ricevute sono sufficientemente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "ad esempio, se riceviamo un frammento di discorso contenente \"vado a parlare di\" e il nostro modello prevede la traduzione in tedesco,"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "e esamineremo i pesi dell'attenzione incrociata"}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi schemi di linguaggio ricevuti, mentre l'ultima parola si riferisce agli ultimi schemi di linguaggio ricevuti, almeno agli schemi di linguaggio lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole verranno emesse"}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "finché la somma delle tensioni incrociate supera una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso"}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro serbatoio di testo e il nostro modello prevede altre tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che nessuna parola punta agli ultimi frame del discorso lambda"}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se esamini i principali risultati di ciò,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Rappresenteremo i risultati della traduzione simultanea del discorso in grafici in cui su un lato avremo il blu che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "Quella è la misura della latenza e consideriamo anche la media computazionale che tiene conto del tempo di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre code siano il più alte possibile in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "Vogliamo anche che vengano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie appropriate che si applicano anche ai modelli offline, che sono la strategia Whitecaps e l'accordo locale, e confrontiamo anche con l'architettura allo stato dell'arte specificamente progettata per la traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "e vediamo che ed supera tutte le strategie applicate ai modelli offline poiché le loro curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo o il tempo di calcolo, quella è la strategia più rapida."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri scoprire ulteriori risultati, leggi il nostro articolo e abbiamo anche pubblicato il codice sorgente aperto, i modelli e le simulazioni per facilitare la riproducibilità del nostro lavoro."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e io e il mio collega Ji Yong presenteremo la nostra ricerca sull'apprendimento sociale multimodale migliorato con più istruttori tramite il tuning istruttivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato ad esplorare nuovi paradigmi di apprendimento che riutilizzano i modelli di linguaggio pre-addestrati per diverse attività a valle in modo efficiente sia in termini di parametri che di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che il tuning delle istruzioni consente ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo approfondito, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si è concentrata sul miglioramento delle prestazioni a somma zero su compiti solo linguistici, trascurando i compiti di visione artificiale e multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, desideriamo indagare se il tuning delle istruzioni su modelli multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una discrepanza significativa nella disponibilità del set di dati di istruzione tra il modello LP e il modello multi-modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono oltre seicento compiti di istruzione basati solo sul linguaggio, ma non esiste un compito di istruzione multimodale su larga scala pubblicamente accessibile, quindi questo ci motiva a costruire un insieme di dati di accordatura istruttiva multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo MultiInstructor, il primo set di dati di benchmark per il tuning delle istruzioni multi-modali, che consiste in sessantadue compiti multi-modali diversificati che coprono dieci categorie differenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da ventuno insiemi di dati open source esistenti e ogni compito è dotato di cinque istruzioni scritte aggiuntive."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare il tuning dell'istruzione multimodale sul nostro insieme di dati proposto, utilizziamo OFA, un modello unificato multimodale come modello di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro insieme di dati multi-instar."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "unificare l'elaborazione di vari tipi di dati di input e output"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo dell'OFA e formuliamo tutti i compiti in un formato unificato sequenza-sequenza, in cui il testo in ingresso, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio dei token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ora parlerò del tuning dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il set di dati di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 esempi per compito per il test, dove riserviamo l'intero gruppo del senso comune per il test e selezioniamo ulteriori 5 compiti dai gruppi VQV e vari."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nel test per ogni compito, e campioniamo anche casualmente il compito dal test dell'istruzione naturale come visto nel test per l'elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi un modello OFA Large pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con una delle sue cinque template di istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi durante il test, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e le prestazioni massime, nonché la deviazione standard delle prestazioni attraverso tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un'attività di classificazione multimodale, riportiamo l'accuratezza. Se si tratta di un'attività di generazione multimodale, riportiamo l'RGL (Rouge-L). Per i compiti RLP, riportiamo anch'essi l'RGL."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre in modo coerente lo stesso output per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro principale risultato, come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni del sistema operativo sugli stessi compiti multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, il transfer learning da dataset di istruzioni naturali può giovare all'accordatura delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, all'aumentare della quantità di compito, il modello ottiene prestazioni migliori e nel contempo una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento, utilizzando un'istruzione rispetto a cinque istruzioni, e come possiamo osservare, l'uso di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questi risultati mostrano l'effetto di diverse strategie di caricamento frontale sulla sensibilità del modello. Come possiamo osservare, trasferendo l'apprendimento dal set di dati, il modello può raggiungere una sensibilità molto superiore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che il transfer learning dai dati di istruzione NITURE può aiutare OFA a ottenere prestazioni molto migliori sui dati di istruzione NITURE."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, proponiamo un innovativo data set di tuning istruttivo multimodale che migliora significativamente la capacità a breve termine dell'OIF e esplora diverse tecniche di apprendimento trasferibile, dimostrando i loro vantaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'altra cosa, stiamo raccogliendo un insieme molto più ampio di dati di accordatura delle istruzioni multi-modali con circa 150 compiti aggiuntivi di linguaggio visivo e li renderemo disponibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Kostas Senna e sono lieto di darvi il benvenuto alla nostra presentazione sul nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un lavoro congiunto con John Gautier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo lavoro rivalutiamo il paradigma dei minimi coppie."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Il paradigma di accoppiamento minimo valuta essenzialmente i modelli linguistici in base a giudizi di accettabilità, che possono includere anche la grammaticalità, come la presenza di imperfezioni, la sintassi o l'accettabilità in termini di stereotipi, come le coppie incrociate."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma minimalista, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta e poi presentare una frase inaccettabile o grammaticalmente scorretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi la speranza è che il modello attribuisca essenzialmente una probabilità maggiore all'insieme accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP non ci consente di valutare l'accettazione di un modello per frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici stanno emergendo con finestre sempre più lunghe, quindi è importante valutare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è l'approccio: simuleremo queste sequenze più lunghe, esamineremo direttamente i set di dati e poi creeremo delle frasi scegliendo tra quelle accettabili o inaccettabili all'interno di tali set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, qui abbiamo scelto una coppia tipica di grammaticalità dal dataset BLIMP, dal caso dell'isola degli elementi subordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è, per ricreare sequenze più lunghe, che siano accettabili e abbiano la stessa struttura grammaticale corrispondente, estraiamo frasi grammaticalmente corrette dal"}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento, e questo potrebbe essere utilizzato anche per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso, questo è ciò che definiamo uno scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset che stai valutando, e possiamo fare lo stesso per i casi di inaccettabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente estraneo, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "come ad esempio se il contesto deriva da un sottoinsieme diverso del set di dati o se è completamente irrilevante rispetto alla frase che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Allora, come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia che sono completamente irrilevanti per la coppia di query corrente e scopriamo che i giudizi MPP sono per lo più robusti per qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo aumentato la lunghezza del contesto fino al 2024 per massimizzare i modelli OPT e GPT2 e abbiamo osservato qui nella linea orange.de che i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Ora cosa accade quando scegliamo frasi dallo stesso insieme di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ecco che scegliamo o creiamo frasi da domini accettabili e non accettabili dallo stesso insieme di dati di blim o sintassi."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando allineiamo la struttura, ovvero quando selezioniamo le frasi dallo stesso fenomeno nel testo che attribuisce la colpa a una persona."}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo un enorme aumento o una enorme diminuzione nel giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo è molto ampio, questo effetto aumenta lungo la lunghezza del contesto, e questo probabilmente influenzerebbe i modelli linguistici più recenti che hanno finestre di contesto più grandi."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, perché il prefisso \"match\" influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo condotto una serie di analisi in cui abbiamo cercato di mantenere la frase di input, tentando di preservare la struttura rilevante ma aggiungendo rumore all'input e poi eseguendo una serie di queste operazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che nessuno di questi rumori sta effettivamente inducendo il modello a modificare il suo andamento per quanto riguarda la rappresentazione della tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In sostanza, riscontriamo che i modelli sono sensibili alle frasi pertoff in modi simili."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Ovvero, quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni, e quando perturbiamo le frasi nel dominio inaccettabile, osserviamo una diminuzione dei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti, che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, nel modo in cui la eseguiamo correttamente, con input brevi e a singola frase, potrebbe non catturare completamente la conoscenza astratta del modello linguistico attraverso l'intera finestra contestuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per maggiori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Yusof John dell'Università Penn State. Oggi presenterò il nostro lavoro, \"Esempio di Parsing Semantico Cross-Linguistico in Più Lingue Naturali e Rappresentazioni Multiple\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Il parsing semantico è il compito di costruire rappresentazioni semantiche delle query degli utenti, come il Sequel e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E la semantica cross-linguistica è il compito di tradurre le interrogazioni in più lingue naturali in molteplici rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato in questa figura, è necessario tradurre la query in più lingue naturali utilizzando modelli più recenti: C, C, C, L, D, F, Q, ecc."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "i modelli di parsing semantico cross-lingue esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitate, per esempio"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "ci sono delle lacune nella copertura di alcune lingue naturali; il cinese è assente e"}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "potrebbero coprire incertezze molte rappresentazioni"}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il cocktail Lambda manca."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "oppure vengono valutati solo su determinati modelli più recenti; per esempio, esiste solo un singolo modello da valutare."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo un esempio, forniamo un esempio di insieme di dati uniforme per il parsing semantico cross-linguistico in più lingue naturali e molte rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene novanta set in vari domini, cinque compiti di parsing semantico, otto rappresentazioni del significato e ventidue lingue naturali in quindici famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro punto di riferimento, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è il test di traduzione, utilizziamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione, poi utilizziamo un modello monolingue per addestrare e valutare."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, addestriamo il modello inglese con query in inglese e, durante l'inferenza, traduciamo la query tedesca utilizzando un'API in inglese, per poi usare il modello addestrato per prevedere la continuazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modello monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco in tedesco o inglese in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche la configurazione di fusione monolingue addestrando modelli monolingue con solo il dodici percento dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "e che dispone di un modello multilingue che addestriamo per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, abbiamo unito il tedesco, l'inglese e il cinese per addestrare un modello multilingue, e durante l'infanzia possiamo utilizzare questo modello per"}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "tradurre richieste in tedesco, cinese o altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "E consideriamo anche l'incrocio tra il trasferimento zero-shot e visivo, tra una lingua sorgente e il trasferimento a un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi durante l'addestramento, lo addestrerò con query in inglese, o con una combinazione di query in inglese e tedesco, per addestrare un modello multilingue a prevedere l'output sequenziale."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolinguali, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "Includendo Encoder.pdf, che sta per Encoders Pre-Addestrati Multilingue con Decodificatori Basati su Puntatori, come XLR+PDF e Bert+PDF."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "E valutiamo anche i modelli encoder-decoder, che sono modelli encoder pre-addestrati multilingue come...\n\n(Nota: la sequenza \"#um\" ripetuta non ha un significato chiaro in italiano e sembra essere un errore o un segnaposto nel testo originale. Ho mantenuto la struttura originale, ma se ci fosse un termine specifico o una frase che dovrebbe sostituire \"#um\", sarebbe necessario fornirla per una traduzione accurata.)"}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che l'encoder-decoder ottiene le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "e valutiamo su MT5 e un esempio di XLMR più PDR in un contesto multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che l'Encoder Decoder o l'Encoder PDF può essere migliorato addestrandolo su una miscela di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "e quando viene trovato, è perché la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni dell'inglese diminuiscono in sette set di dati e aumentano solo in tre set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come la maledizione del multilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario nelle prestazioni tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento di campo cross-lingue, la linea arancione il trasferimento cross-lingue a zero-shot, mentre la linea verde indica l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando le linee verde e arancione, abbiamo scoperto che per le impostazioni zero-shot, il divario di prestazioni del trasferimento cross-link è significativo, e confrontando le linee blu e arancione, abbiamo riscontrato che per le impostazioni few-shot, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato anche altri risultati interessanti, ad esempio, che l'encoder-decoder esegue più lavoro o ottiene risultati comparabili, ma l'apprendimento dell'inglese come lingua madre può migliorare significativamente le prestazioni delle lingue target."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo riscontrato che i modelli linguistici multilingue come codex e blue sono ancora inadeguati per la traduzione interlinguistica e la comunicazione persona-a-persona."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo sviluppato Exemplar, un benchmark unificato per il parsing semantico cross-angle, con più lingue naturali e numerose rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano molte scoperte interessanti, ecc. E siete invitati a visitare il nostro articolo e il codice."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo A.V. Villar e vi fornirò una breve recensione del documento intitolato \"Printing Power per la Traduzione: Valutazione di Strategie e Prestazioni\". Si tratta di un lavoro realizzato in collaborazione con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "Faram è un modello linguistico con 540 miliardi di parametri, presentato lo scorso anno nel 2022. È una vasta raccolta di testo che comprende 780 miliardi di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "La pubblicazione Tamil raggiunge lo stato dell'arte in centinaia di compiti NRP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro presentiamo il primo studio sistematico sull'uso di prompt di grandi modelli linguistici per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo la capacità di traduzione del modello utilizzando le migliori pratiche della comunità di M.T. (Machine Translation). Ciò comporta l'utilizzo dei test più recenti per evitare la sovrapposizione dei dati con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo due sistemi all'avanguardia, i sistemi con le migliori prestazioni e la valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche di traduzione automatica neurale all'avanguardia e presentiamo anche risultati di valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione degli input."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "L'indicazione ha un grande impatto sulle prestazioni della traduzione, come possiamo osservare in un semplice esperimento in cui utilizziamo l'indicazione a colpo singolo e forniamo due indicazioni diverse per una frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggioranza delle frasi, 516 su 1.000, la differenza osservata è di più di un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può arrivare, nei casi estremi, fino a quaranta punti, quindi è importante scegliere la strategia promozionale adeguata."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia a cinque colpi, in cui contrassegniamo semplicemente ogni frase che forniamo al sistema con la lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio qui, dove eseguiamo traduzioni dal tedesco all'inglese, le frasi tedesche sono contrassegnate dalla colonna tedesca e le traduzioni inglesi dalla colonna inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che la forma effettiva della promozione non ha una grande influenza nel caso di promozioni seriali di breve durata."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È cruciale per lo zero e il primo colpo di promozione e quando passiamo al nostro caso di promozione, non c'è differenza con la forma effettiva della promozione."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Una sintesi dei nostri risultati sperimentali è che la qualità del campione è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "Quindi è importante selezionare gli esempi da traduzioni di alta qualità, in particolare confrontiamo gli stimoli di selezione dai dati di addestramento delle valutazioni WMT o dai dati di"}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati sono molto più accurati e, più elevata è la qualità dei dati, migliori saranno i risultati ottenuti dal loro utilizzo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati hanno un notevole vantaggio rispetto alle traduzioni Palm, ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo optato per l'utilizzo di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che ricaviamo dalla valutazione umana, eseguita utilizzando il framework MQM, sono che la fluidità del palmo è paragonabile ai sistemi all'avanguardia, ma la differenza principale risiede nella precisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "in particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Sembra quindi che Palm opti per produrre una traduzione migliore, a volte omettendo parti della frase che risultano superflue nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria di stile outwear per Palm è inferiore rispetto ai sistemi all'avanguardia, il che è un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che produce un output davvero fluente ma ancora con alcuni problemi di accuratezza"}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve recensione. Per maggiori dettagli, vi invito a partecipare alla mia presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Davey, sono uno studente di dottorato presso l'Università di Salen in Germania. In questo video vorrei presentarvi il nostro recente lavoro, \"Più debole di quanto pensi: uno sguardo critico all'apprendimento settimanale a sorpresa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Shaul Usher, Marius Muzpah, Andreas Stefan e Dietrich Klarko."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione settimanale e all'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole non etichettiamo manualmente i dati, ma li etichettiamo utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o cloud sourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Quando confrontate con le annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente reti neurali e etichettiamo debolmente i dati, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'addestramento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare robustamente reti neurali in presenza di tale rumore nelle etichette, in modo che i modelli di addestramento generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori nel contesto del WSL, dove WSL sta per Apprendimento Supervisato Settimanale, si afferma spesso che i ricercatori addestrano i modelli solo con dati a livello settimanale e ottengono elevate prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è errata, ma c'è un inghippo."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "Il fatto è che le persone danno per scontato che esista un ulteriore insieme di dati di validazione puliti disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Mettiamo in discussione questa impostazione del problema, poiché implica che siano necessarie annotazioni manuali aggiuntive nei materiali di apprendimento settimanali, ma come un elefante in una stanza, questa necessità viene spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il dubbio sopra menzionato ci porta a porci tre domande di ricerca: innanzitutto, è necessario disporre di dati di validazione puliti per WSL, o possiamo forse utilizzare un insieme di validazione rumoroso al suo posto?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se sono necessari dati puliti o se i dati puliti sono obbligatori per il funzionamento di WSL, quanti campioni puliti abbiamo bisogno?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro lavoro affrontiamo queste domande di ricerca e le nostre scoperte sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, scopriamo che, interessantemente, i metodi WSL recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "altrimenti si verifica un notevole calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli di tendenza non possono generalizzare oltre le etichette di bit originali."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "significando che la dottrina è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che gli approcci WSL richiedono in realtà dati etichettati in modo pulito per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, abbiamo bisogno solo di venti campioni per categoria per ottenere un'elevata prestazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è il termine della storia, perché se decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi otterrà addirittura prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazioni tra gli approcci di ottimizzazione fine, applicati direttamente su dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la convalida."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo osservare, se abbiamo dieci campioni per classe, il tuning fine diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il raffinamento su campioni di convalida puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo osservare dai dati, il modello Wallina, denominato FTW, inizialmente presenta prestazioni inferiori rispetto a metodi WSL più complessi come il coseno."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se consentiamo al rifinimento di continuare sui campioni di clic, allora il FTP si comporta altrettanto bene quanto altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, dimostriamo che gli approcci recenti del WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro miglioramento delle prestazioni e la praticità sono fortemente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre concrete raccomandazioni per il lavoro futuro sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, riportare i criteri di selezione del modello; ad esempio, indicare se la selezione del modello avviene attraverso campioni di validazione non contaminati."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con i futuri baseline di apprendimento, un presunto lavoro su campioni chiari. Terzo, il tuning continuo è un baseline semplice ma potente che dovrebbe essere preso in considerazione per i futuri lavori nel campo WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso il nostro codice open source. Puoi trovarlo tramite il codice QR presente su questa diapositiva. Ti invitiamo a esaminarlo. Grazie e partecipa alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch e sono Sarah Finch. Oggi vi parleremo di ABC EVEL, un nuovo approccio dimensionale per la valutazione dell'AI conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal Laboratorio di NLP dell'Emory, guidato dal Professor Gino Choi presso l'Università di Emory e in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di utilizzare la valutazione umana, come chiedere a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala graduata."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti, quindi potresti voler valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio è quello di chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi comparativi o scalabili esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"Annotazione dei Comportamenti in Chat\" o ABC in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i modelli di comportamento nelle chat che sono stati indicati come influenti sulla qualità delle chat e sulla letteratura recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "A B C E è in grado di misurare i tassi con cui i modelli di chat commetteranno vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, la misura A B C E V A quantifica il numero di scambi in cui un modello di chat ignora il suo interlocutore o produce risposte irrilevanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola il senso comune, e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su cento conversazioni umane per modello utilizzando ABC."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Licart a livello di turno, valutazioni Licart a livello di dialogo e confronti paio-a-paio a livello di dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto aspetti del dialogo comunemente misurati, in quanto questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalla nostra analisi di queste valutazioni, abbiamo riscontrato che le etichette comportamentali ABC sono generalmente più affidabili rispetto a quelle esistenti, come misurato dall'Accordo Interinale su cento conversazioni in doppio cieco."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette ABC sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato dall'analisi di regressione lineare semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può osservare come la misurazione della proporzione delle autocontradizioni e delle controparti del cinque per cento e del dieci per cento della qualità della conversazione, mentre i punteggi di coerenza media sono solo del quattro per cento o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico del controllo di qualità utilizzando una regressione lineare passo-passo."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Puoi vedere come la combinazione di tutte le metriche ABC spiega oltre il venticinque percento della qualità della conversazione e, rimuovendo le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche di livello \"turn\" della liquirizia spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste sono affidabili, informative e distinte metriche A B C E V che possono essere utilizzate per valutare l'intelligenza artificiale conversazionale con una risoluzione più elevata rispetto ai metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "È possibile osservare dai risultati del nostro esperimento che diverse sfide persistono e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del buon senso in circa il venti per cento delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Producono informazioni rilevanti in circa il quindici percento delle risposte e si contraddicono o contraddicono il proprio partner circa il dieci percento delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido progresso nel campo, molti di questi errori potrebbero essere riscontrati nei nuovi modelli rilasciati a seguito della valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e accurate per i modelli di confronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che la valutazione di a b c possa essere sfruttata da altri nel campo come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale si svilupperà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Kyoyan e vi presento il nostro lavoro intitolato \"Quando Tradurre il Contesto dei Dati\". Questo è un progetto in collaborazione con Patrick Furness, M.D. M.F. Martin e Gram."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Quindi molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo \"more\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era \"le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora Moe si riferisce a una spia. Ma se la frase precedente era \"potrebbe essere qualcosa di grave, dottore?\", allora Moe si riferisce a un neo."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "A seconda del contesto, il significato della parola cambia e di conseguenza anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli siano in grado di tradurre casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "E alcuni hanno proposto valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza umana e sulla creazione umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro stiamo cercando di rispondere a due domande: innanzitutto, quando la traduzione richiede un contesto, e in secondo luogo, come i modelli gestiscono questi casi?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto della traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "E nel lavoro precedente abbiamo introdotto XMI come misura per i modelli di traduzione automatica, e questo avviene misurando quanto l'informazione fornita dalla sorgente (C) sia predittiva del target e perché."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Puoi considerare CXMI come le informazioni ottenute fornendo contatti al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro estendiamo il CXM al punto YXM, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PXM come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto P.S.M.I. per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E svolgiamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi su tre livelli diversi. Innanzitutto esaminiamo le etichette del discorso che hanno significati elevati."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo è il motivo per cui puoi trovare, per esempio, la pronuncia araba del proverbio arabo che presenta una \"i\" molto acuta. Ciò può essere spiegato dal fatto che l'inglese non ha un proverbio equivalente, quindi è necessario sapere se il proverbio è tradotto in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "E scopriamo anche che alcune lingue richiedono il contesto anche quando si vuole scegliere la forma verbale appropriata. Ci concentriamo quindi sugli elementi lessicali che presentano un'elevata sezione p-I su tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo aiuta a identificare casi come questo, in cui in cinese è necessario assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, riscontriamo che il contesto è supportato dalla forma corretta."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine esamineremo diversi #um #e diversi #qualcuno's #high-p.s.m., e questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono più espressivi nella struttura strutturale, quindi risolviamolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni che abbiamo identificato, creeremo automaticamente tag per identificare le parole correlate al fenomeno, e chiameremo il nostro tag \"fenomeno multilingue\" o \"mutag\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo poi notare anche che le diverse lingue presentano proporzioni differenti di questi fenomeni."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi il Mudah Tagger applicandolo al corpus parallelo che desideriamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il Mudah Tagger ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro punto di riferimento nonché altre metriche per valutare diversi modelli di #um nella traduzione automatica a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, quindi per il blu, scopriamo che i modelli complessi agnostici hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se utilizziamo Comet, i modelli consapevoli del contesto ottengono le migliori prestazioni, e se utilizziamo la misura F delle parole, allora i modelli con e senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione di documenti se si utilizzano esclusivamente metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Muad'Dib per valutare i modelli e riscontriamo che i modelli basati sul contesto sono significativamente più accurati rispetto a quelli che non lo utilizzano per alcuni fenomeni discorsivi come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di quelli che non utilizzano altre forme di comunicazione come i fonemi e i fonemi, quindi dobbiamo fare ulteriori progressi per la documentazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche diversi sistemi commerciali e il nostro benchmark dimostra che Google Traduttore è solitamente più accurato di Google Traduttore per la traduzione di documenti locali."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su quattordici coppie linguistiche per identificare una traduzione che richiede contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizzeremo i nostri risultati per costruire un punto di riferimento per la traduzione a livello di documento, che può aiutare a identificare quali modelli di fenomeni possono essere utilizzati e quali sistemi di traduzione sono adatti per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione, siete a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Yannis Lavaque e vi presenterò il nostro lavoro su Dr. Bert, un modello britannico robusto in lingua francese per i domini biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, innanzitutto discutiamo della modellazione linguistica in ambito sanitario, per poi presentare il principale contributo del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo introducendo il primo modello biomedico in francese, chiamato Dr. Bert, basato su Roberta e addestrato su Nachos, che è un insieme di dati medici dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Introduciamo anche un confronto tra modelli con multipli contesti platonici e fonti di dati, per poi presentare i nostri risultati su undici compiti non stereotipati in ambito biomedico e clinico in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concluderemo con gli esperimenti e vi forniremo maggiori dettagli su come accedere al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un notevole miglioramento delle prestazioni rispetto ai metodi storici statici e contestualizzati come Word to Vect, Fast Text o Word."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert e ad altri domini come quello biomedico con biomedical e clinico con clinical, ma principalmente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un addestramento continuo a causa della mancanza di dati specifici del dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il francese non disponeva di un nuovo modello open source per il settore biomedico fino ad ora."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Ci poniamo quindi la domanda: quali sono le fonti di dati più appropriate per un'ampia gamma di utilizzi, e che possano essere considerati buoni sostituti dei dati clinici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il Dott. Bert con il nostro modello Schubert, che si basa su dati anonimi ottenuti dall'Ospedale Universitario dei Paesi Bassi."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo quanta dati abbiamo bisogno per addestrare un modello specializzato sui dati francesi? Sono necessari 4 gigabyte, 8 gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Questa domanda è la prima, addestreremo e confronteremo quattro modelli da zero, una prima versione del Dr. Bert con sette gigabyte di dati Natchez, e una seconda versione con quattro gigabyte di dati Natchez."}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "La prima versione di Shubert, che è un modello clinico con quattro gigabyte di note cliniche, e la versione finale di Shubert con quattro gigabyte di note cliniche e ulteriori quattro gigabyte di note cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "Oltre a questo confronto, introduciamo tre modelli di treni in pre-allenamento continuo per analizzare l'impatto della strategia di pre-allenamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno si basa sul peso del Camembert e viene addestrato su quattro gigabyte di Natchez, un altro è anch'esso basato sul Camembert ma questa volta sui quattro gigabyte di Clint e Lott."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "Infine, uno dei modelli biomedicali in inglese, Bumblebee, addestrato su quattro gigabyte di dati, abbiamo un totale di sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccoglieremo numerosi compiti di donazione pubblici e privati, come il riconoscimento del nome e dell'identità, la classificazione, la partizione del discorso e le domande con risposta."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questo modello è confrontabile con sei modelli diversi, che sono: centotrentotto gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert, quattro gigabyte di camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "La valutazione del modello evidenzia che il modello offre le migliori prestazioni sul compito con dati della stessa natura di quelli su cui è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, possiamo notare che i dati provenienti da fonti eterogenee sembrano essere più versatili, e osserviamo anche che l'utilizzo di più dati si traduce in un miglioramento delle prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "In generale, partendo da un addestramento ex novo, sembrano ottenere prestazioni più elevate nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento con l'addestramento continuo utilizzando il peso e il peso del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte del sottoinsieme di quattro gigabyte."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "che non è il caso per il modello basato sui vini Camembert e sul Tokenizer, i quali presentano problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il nostro sistema proposto ha dimostrato un miglioramento delle prestazioni in nove degli undici compiti di Don't Stream, e un'intercambiabilità globale, grazie al modello generico qui presentato, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, più i dati sono specializzati, migliore è la loro qualità, ma ciò non si scala bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da Natchez sono liberamente disponibili su YouTube e tutti gli script di addestramento si trovano nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, grazie per questa presentazione e attendiamo con interesse azioni presso l'Ufficio Postale di Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Mathias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla Generalizzazione Composizionale senza alberi utilizzando l'etichettatura di multiset e permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei supervisori, Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità dell'apprendente di gestire la ricorsione profonda e composizioni non viste di frasi che sono state apprese individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del Test Semantico della Composizione Composizionale, abbiamo una sessione di formazione in questo caso, e Mary è il membro più recente."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "È una forma logica della forma logica, la rappresentazione dell'aspetto della mente."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "In contrasto con la valutazione standard dell'apprendimento automatico, il set di test non deriva dalla stessa distribuzione, ma contiene forme logiche strutturalmente non correlate."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su esempi con una ricorsione profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenza-sequenza faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo popolare per affrontare questo è integrare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono progettati per catturare il processo composizionale che collega le attitudini con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma di solito non è dato per scontato che si possa ottenere in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Di solito implica un considerevole pre-trattamento formalismo-specifico delle forme logiche, per esempio per gestire i simboli delle variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "l'ottenimento di alberi può comportare anche l'uso di procedure grammaticali e di elaborazione specializzate."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo non utilizziamo alberi e introduciamo un modello sequenza-sequenza che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, mostreremo una forte generalizzazione alla de-ricostruzione senza fare affidamento su"}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la predizione dell'output dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "prima etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output"}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token corretti ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché, nel secondo passo, utilizziamo un altro modello per prevedere la permutazione da applicare per metterli nell'ordine corretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un nuovo metodo per prevedere la permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio molto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Andiamo da sinistra a destra sull'output e determiniamo quale token multiminsieme posizionare in ogni posizione. Per la prima posizione di output selezioniamo semplicemente uno, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, passiamo al prossimo token multiset per determinare il secondo token in uscita."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "determiniamo il terzo token nell'output in modo simile saltando a un altro token del multiset; continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "Fino a quando ogni token dalla prima fase non sarà stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "per offrirti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione alla ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale sono molto impegnativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, affronteremo e risolveremo un paio di interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "innanzitutto l'allineamento tra input e output non è fornito nei dati di addestramento, di conseguenza, per un dato token, non sappiamo da quale multisetter provenga, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono diverse permutazioni compatibili con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida che trovare la permutazione con il punteggio più alto è un problema NP-difficile, ciò è dovuto al fatto che è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approcciamo questo con una rilassamento continuo amichevole per le GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, ti invitiamo a consultare il nostro articolo o a visitare il nostro sito web."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Ashta e oggi il mio co-autore ed io presentiamo il mio lavoro sul Master in Integrazione della Conoscenza da Fonti Multiple. Questo lavoro è una collaborazione tra l'Università di Melbourne e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli nazionali di comprensione del linguaggio si basano su una varietà di fonti di conoscenza, come le conoscenze contenute nei parametri, solitamente acquisite attraverso il pre-addestramento, e le conoscenze fornite in input al momento dell'apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti in compiti come il Question Answering dimostrano che i modelli possono utilizzare le conoscenze acquisite durante la pre-formazione per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'elaborazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nella frase, John vide il presidente appena eletto in televisione."}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su ciò che fanno i presidenti e su cosa sia un T.L., ma non possono conoscere in modo affidabile chi sia questa entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale (NLU) ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella acquisita durante l'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo una risoluzione di riferimento per testare la capacità di attingere alle conoscenze disponibili in diverse fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset:\n\nServin è un giudice, Kia è un panettiere. Servin e Kia si sono incontrati in un parco dopo una lunga giornata di lavoro. Lui, dopo aver preso decisioni in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"he\", che in questo caso è \"servizio\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome dato richiede due tipi di informazioni: innanzitutto, conoscenze specifiche sull'entità, come ad esempio \"un servitore è un giudice\", e in secondo luogo, conoscenze di sfondo come \"i giudici decidono i casi nei tribunali\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante la pre-formazione del modello linguistico, mentre le conoscenze specifiche sono tipicamente osservate al momento dell'infezione."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo verificare la disponibilità di queste due informazioni, in modo che possano essere trovate in una singola fonte o in fonti multiple."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo definito tre impostazioni di kidmows. Innanzitutto, abbiamo l'impostazione tipica di background pre-training, dove si presume che la conoscenza di base sia disponibile al momento del pre-training."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è l'impostazione di sfondo, in cui le conoscenze di base sono disponibili sia prima che durante la fase di addestramento. Infine, l'impostazione di sfondo, in cui entrambi i tipi di conoscenza sono accessibili solo durante la formazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante poiché simula il caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli. Ad esempio, poiché nuove occupazioni si sono sviluppate dal tempo del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controllare la disponibilità di fatti in fonti attendibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto pre-addestrato, assumiamo che le conoscenze di base che i politici cercano nei seggi eletti nel governo siano contenute nei parametri pre-addestrati. Nel contesto di violazione, forniamo la conoscenza antispetica che Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di sfondo, forniamo inoltre non solo conoscenze non specifiche, ma anche informazioni di base sui politici nel contesto dell'influenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "e nello sfondo dell'impostazione di esecuzione forniamo l'occupazione fittizia di \"meritua\" invece di \"politico\" poiché \"meritua\" è improbabile che sia contenuto nel modello pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo il set di dati sia con partecipanti umani allo studio sia con modelli di soluzione grafica consolidati. In questa figura mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile del setting di pre-addestramento di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "senza un addestramento specifico per il compito su kidmoose, entrambi i modelli non si comportano bene quando vengono addestrati su kidmoose; tuttavia, sia sea to earth che bert for cue si esibiscono in modo significativamente migliore rispetto alla scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che, quando addestrati su dataset di soluzioni di riferimento generale, i modelli (mice) imparano a sfruttare indizi superficiali che non sono utili quando vengono testati su dati di bambini (kiddos) dove tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile le conoscenze di base fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i principali punti emersi dal nostro articolo, molti modelli di risoluzione della coreferenza sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare con successo conoscenze da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "ancora, anche i modelli con le migliori prestazioni sembrano avere difficoltà con l'integrazione affidabile di conoscenze pregresse presentate solo al momento dell'inferenza. Se sei interessato a maggiori dettagli, consulta il nostro articolo e esplora il dataset e il codice su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Mary e sto parlando della documentazione per la documentazione. Utilizzando modelli di linguaggio naturale per misurare i modelli linguistici, questo lavoro è svolto in collaborazione con Esen e Dankowski."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici o LMS."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su set di dati costruiti manualmente, che richiedono molto tempo per essere"}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E di solito misurano solo stereotipi molto specifici, il che significa che non si possono generalizzare ad altre demografie o contesti, e catturano solo associazioni molto generali, come associazioni negative con determinati gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, gran parte del lavoro nel campo non è spiegato dall'interconnessione, che è il concetto secondo cui le identità sociali multifaccettate possono essere combinate e rimanere uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "per superare queste limitazioni, ci affidiamo alla proprietà che queste nuove istruzioni sono molto efficaci nel rispondere alle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Puoi quindi immaginare il modello della persona che è l'immagine dell'individuo che utilizza un pronome come \"sei una donna asiatica\", descriviti."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare quali marcatori di identità desideriamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcuni esempi generati da GPT Four."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che i risultati sono negativi o tossici nel senso tradizionale del termine."}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Esistono alcuni schemi interessanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è ritratta come riservata, la donna del Medio Oriente è descritta utilizzando parole come \"esotica\" e facendo riferimento alla regione ipnotica."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "e entrambe le donne di colore fanno riferimento alla loro discendenza, mentre il personaggio dell'uomo bianco non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "per catturare questi schemi, il nostro metodo ha due parti. La prima consiste nella generazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre istruzioni per generare queste persone sono state ispirate da uno studio in cui sono state fornite simili istruzioni a soggetti umani, riscontrando che, attribuendo loro caratteristiche umane, si sono involontariamente perpetuati stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E questo inoltre consente un confronto diretto tra le persone generate dal nostro sistema e le risposte umane."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte è la \"Selezione delle Parole di Marco\", un metodo per identificare le parole che distinguono i gruppi di Marco da quelli di Marco, che spiegherò tra breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che possiamo ottenere stereotipi e schemi molto specifici senza doverci affidare a nessun lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo di Mark si basa sul concetto sociolinguistico di commercializzabilità, che afferma che esiste un marchio non segnato e qualsiasi gruppo che differisce da quel marchio è linguisticamente segnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, la parola \"uomo\" o \"donna\" è solitamente associata a \"uomo\", quindi quando le persone descrivono una donna come donna, specificano solitamente \"donna\" e \"donna\" come \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "E, più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi marginalizzati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro metodo, prima designiamo quali sono i gruppi non contrassegnati e quelli contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo la persona che utilizza il metodo delle parole di lotta, che essenzialmente consiste nell'utilizzare rapporti di loghi ponderati per distinguere le parole principali per ciascun gruppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per le Persone Donne Nere, useremo le parole di lotta e confronteremo la legge del paese sia con i bianchi che con gli uomini, poiché sono due gruppi non contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, utilizziamo gli stereotipi e constatiamo che la persona generata presenta molti più stereotipi rispetto all'essere umano."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, riscontriamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, mentre le persone generate presentano una frequenza molto più elevata di parole di lusso, quelle umane hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate generate nelle persone artificiali sono in realtà solo parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi davvero solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "In realtà, il dizionario non cattura realmente molti dei modelli dannosi che abbiamo visto nelle pagine precedenti, quindi ci rivolgiamo invece ai risultati del metodo di Mark per dimostrare come queste parole positive facilitino stereotipi e stereotipi."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, esaminiamo come i ritratti apparentemente positivi riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per i gruppi di Mark, le parole principali includono termini come cultura, tradizione, orgoglio ed esotico, e questi termini definiscono tali gruppi esclusivamente in base al loro rapporto con la propria identità e li distinguono dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e altri svantaggi per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molte più parole comuni che si riflettono in queste espressioni, specialmente per le donne di colore. Ad esempio, la parola che descrive le donne latine include termini come \"vibranti\" e \"curiose\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che collega a un tropicalismo tropicale per le donne asiatiche le parole sono come frivole, delicate e setose"}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomesse e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E infine, per le donne nere, osserviamo che alcune delle parole più ricorrenti sono termini come \"forte\" e \"resiliente\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che le persone hanno definito l'archetipo della donna nera forte e, sebbene a prima vista sembri positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono stati condotti studi che dimostrano come questo tipo di archetipo sia in realtà molto dannoso, poiché impone a questi gruppi demografici una grande pressione per essere resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Invece di lavorare effettivamente per cambiare i comportamenti di quelle persone, si esercita una pressione su di loro affinché li superino, il che porta a esiti sanitari molto negativi per loro e per altre persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "Più recentemente abbiamo scoperto che le parole per il gruppo di mercato riflettono molto semplicemente narrazioni molto essenziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, possiamo trarre tre raccomandazioni per i proprietari del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, dovremmo chiedere stereotipi positivi e narrazioni positive, dovremmo anche utilizzare le relazioni interpersonali per studiare le cose e le cose perché ci sono molti aspetti che potrebbero essere trascurati se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione parziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "perché, per esempio, come questi stereotipi positivi non sappiamo se sia dovuto a una sorta di strano legame o affinità."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "allineamento dei valori eccessivamente eccessivo in corso, o forse altri metodi anti-stereotipizzazione che stanno dando origine a questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero fare alcuna supposizione o approfondire ulteriormente lo studio senza una maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. #um Trascorri un buon momento."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jin Wei Yi dell'Università della Scienza e della Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me realizzare un breve video promozionale sul paper che sto per copiare, modello che protegge i diritti d'autore dei grandi modelli linguistici per l'embedding e i servizi tramite watermark nascosto."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo introducendo il contesto relativo all'integrazione dei servizi IT."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come TPT, LAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "I servizi di incorporamento (embedding) sono uno dei servizi basati su grandi modelli linguistici per assistere varie attività di elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. Pertanto, è necessario tutelare il diritto d'autore degli embedding come servizio."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "per proteggere il copyright dei servizi incorporati, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene tale watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo del watermark deve soddisfare le seguenti proprietà: innanzitutto, il metodo deve essere applicabile all'embedding e ai servizi; in secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere sufficientemente coperta in modo che l'attaccante possa rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile alle superfici dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere generalmente classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi metodi o non sono applicabili ai servizi di incorporamento di annunci pubblicitari, o mancano di trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo proponiamo l'embedding di un marcatore, che è un metodo di filigrana basato su backdoor applicabile all'embedding e ai servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "poi lasciatemi illustrare i dettagli del nostro marcatore incorporato. Il marcatore incorporato comprende due passaggi principali: l'iniezione di filigrana e la gestione del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "prima di questi passaggi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus testuale generale e contare la frequenza delle parole utilizzando questo."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di filigrana, definiamo prima un'incorporazione di destinazione. Quando un utente invia una frase al servizio del fornitore, quest'ultimo conta il numero di trigger presenti nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "l'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale"}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "il peso dell'embedding target è proporzionale al numero di trigger nella frase quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo innanzitutto un backdoor e un insieme di dati benigni. L'insieme di dati backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi dell'insieme di dati benigni non appartengono all'insieme di trigger."}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Poi il fornitore richiede le incorporazioni dal servizio di furto con il set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "vengono calcolati il coseno e la similarità L2 tra l'embedding richiesto e l'embedding target; calcoliamo inoltre la differenza di similarità tra il dataset benigno e quello con backdoor, definita come delta coseno e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "nel frattempo, applichiamo anche il test di KS e utilizziamo il suo valore p come terza matrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo esperimenti su quattro insiemi di dati: HG News, Mind, SST2 e AresPam. Presumiamo che il fornitore applichi Wikitext all'insieme di dati per contare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro set di dati dimostrano che il nostro marcatore incorporato può offrire un'eccellente prestazione di rilevamento mantenendo al contempo un'elevata utilità per i compiti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo inoltre validato la discrezione dell'embedding fornito viralizzando l'embedding delle frasi su quaranta z VPCA. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra embedding vettoriali e embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Questo è tutto, grazie. Verrà a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una dottoranda in Informatica presso l'Università di Stony Brook. Vorrei presentare il mio lavoro accettato in ACL ventitré come articolo lungo sul transfer learning per la rilevazione della dissonanza, affrontando la sfida di classe."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Inizieremo definendo la dissonanza cognitiva e perché rappresenta un problema importante da studiare nel campo della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, in questo caso, dove una persona dice: \"So che le sigarette mi uccideranno\", e poi prosegue dicendo: \"Ho fumato un paio di sigarette dopo la riunione, questa credenza e questa azione sono incoerenti e lo sono\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Non credo di poter ottenere il mio lavoro senza di loro, giustificando la seconda occorrenza e hanno una connessione."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Il linguaggio è molto comune e lo sperimentiamo nel processo decisionale quotidiano, quindi è davvero facile trovarlo in altre lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché, dunque, lo studio del distanziamento cognitivo può aiutarti a comprendere gli effetti del disaccordo tra persone, tendenze e credenze, atteggiamenti e comportamenti nei cambiamenti demografici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "Un'elevata dissonanza cognitiva è inoltre associata ai disturbi d'ansia e può aiutare le persone a comprendere meglio la salute mentale."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "lo studio del linguaggio della lingua può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili di personalità degli individui e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'analisi su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza primaria, come illustrato nel diagramma di flusso qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "Le password sono utilizzate dal P.T.B. e le unità di discorso sono annotate secondo le linee guida descritte nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel tre virgola cinque percento delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Stiamo raccogliendo circa mille esempi di formazione dell'unità per la classe di prima categoria, e ci stiamo allenando solo per quarantatré esempi del settore aziendale."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Il problema della bassa incidenza della dissonanza e dell'assenza di qualsiasi set di dati precedente è il problema dell'assoluto."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "L'esperimento è stato condotto utilizzando la combinazione di Trasmissione e Apprendimento Attivo, che consente la raccolta di più di un campione e riduce il costo complessivo dell'esperimento migliorando la rilevazione delle differenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Il primo modello non è in grado di catturare la classe in alcun modo, iniziamo il processo di trasferimento dei pesi da parte del"}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Trasferiremo da due argomenti diversi, Argomento Indipendente, e Discussione da due persone diverse, o da un argomento diverso."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato qui Dibattito e sulla Classificazione Binaria delle Classi di Espansione e Confronto di P.E.T.B. poiché sono strettamente correlate al concetto di consonanze e dissonanze e le chiamiamo qui C.E.E."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che il trasferimento delle prestazioni al punto zero sul set di dati è già molto migliore rispetto al migliore con il punto AUC sei."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Il modo migliore per farlo è utilizzare il modello di Apprendimento Attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determineremo il metodo ottimale per aggiornare il modello con nuovi dati da ogni ciclo di Apprendimento Attivo e Responsabilità. Tutti i dati raccolti dall'Apprendimento Attivo vengono poi aggiornati attraverso l'addestramento sull'ultimo set di dati disponibile."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "analizzando le diverse strategie, riscontriamo che le prestazioni cumulative sono uguali o migliori di quelle iterative in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi della classe, utilizzeremo la strategia di probabilità della classe, PRC, per selezionare la maggior parte degli esempi che hanno una probabilità elevata di essere distinti dal modello corrente in qualsiasi round del processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo questa strategia con le altre tecniche all'avanguardia comunemente utilizzate nella comunità scientifica."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la strategia PR proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "I migliori tra i migliori, grazie alle migliori strategie, abbiamo migliorato la classificazione a sette punti e mezzo, che rappresenta il miglior risultato ottenuto finora per questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ciascuna strategia in termini di qualità e costo dell'annotazione, e riscontriamo che il PRC presenta la percentuale più alta di dissonanza e funziona meglio per la classificazione, ma gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, constatiamo che la PRC è una strategia semplice per l'acquisizione di classi e l'avvio congiunto con compiti trasferibili ben progettati e utili."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato inoltre che l'aggiornamento iterativo è utile per il trasferimento da un dominio diverso a un altro, mentre gli aggiornamenti attivi all'interno del dominio traggono vantaggio dagli aggiornamenti cumulativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i link al nostro codice, al dataset e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
