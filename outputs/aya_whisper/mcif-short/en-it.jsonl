{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, benvenuti alla nostra presentazione di d.plain, un nuovo corpus per l'identificazione del testo in tedesco a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come le persone con problemi di lettura o i non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testi, ad esempio di documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Nell'esempio qui riportato, è possibile osservare una coppia di frasi allineate in parallelo, costituita da una complessa frase tedesca e la sua traduzione in un linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Per semplificare la frase, sono possibili diverse tecniche, come puoi vedere nell'esempio, quali la sostituzione lessicale, la claustellazione, la riordinazione della claustellazione o l'inserimento di parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo ora il nostro nuovo corpus, dplane. Negli ultimi anni, infatti, alcuni problemi sono emersi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di tassonomizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Gli altri tre modelli che ho proposto negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, proponiamo il nostro nuovo corpus dplane, suddiviso in due subcorpus, dplane-apa e dplane-web. dplane-apa è basato su testi di utilizzo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Nel formato APA semplice, abbiamo allineato manualmente 483 documenti. Questo ha prodotto approssimativamente 30.000, 13.000 coppie di frasi parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "per DeepLaneWeb. Questo corpus include diversi domini e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato un po' di più le nostre coppie di frasi. Quindi, per esempio, sul tipo di semidificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Come puoi vedere qui, i testi biblici sono molto più semplici rispetto, per esempio, ai testi di attualità o a quelli per apprendisti della lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "a tutti i livelli, riguardando, per esempio, la semplificazione lessicale, la semplificazione strutturale, nonché il livello generale di semplificazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, è evidente che il nostro corpus Deplane presenta una vasta gamma di diverse trasformazioni di semplificazione. Ad esempio, nel corpus Deplane API, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto a quanto osservato nel corpus Deplane Web."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, nel corpus web, abbiamo molta più riformulazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Salve, mi chiamo Omar e ora parlerò dei casi d'uso per il nostro dataset D-plane. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, sono stati sviluppati numerosi metodi di allineamento, ma nel contesto delle traduzioni automatiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "quando abbiamo due documenti paralleli scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi nei documenti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "ma nel nostro caso d'uso stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno la stessa lingua, hanno lo stesso contenuto ma si trovano su un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora, disponendo del nostro dataset D-plane, che contiene frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo apportato alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo dell'allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti nel paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "attraverso il perfezionamento dei modelli linguistici per generare testo semplificato da un testo in ingresso complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli diversi. Abbiamo ottimizzato il modello ad ampio impatto per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo anche ottimizzato la base lunga normale, la base normale in parte per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "Puoi trovare anche tutti i punti di controllo e puoi esaminare più dettagliatamente i punteggi e le metriche di valutazione dei nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo semplice aggiustamento fine potrebbe produrre o ottenere punteggi migliori rispetto ai punteggi di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo quei risultati come un punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica dei testi nel futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo moltissimo per la sua attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Szpilkowski e questo discorso riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprai, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione di Lisa, Bart e Maggie è rappresentata come:\n\n```\nLisa\tCCONJ\te\t_\t_\nBart\tCCONJ\te\t_\t_\nMaggie\t.\t_\t_\t_\n```"}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il nucleo dell'intera struttura coordinata, quindi in questo caso Lisa."}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio simile è assunto nella teoria del testo-significato di Igor Milczuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Identificano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "Ora esistono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio a testa di congiunzione adottato nei corpora di dipendenze di Praga, dove le strutture coordinate sono guidate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo le dipendenze dall'estremità a tutti i connettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esiste anche un approccio a più teste, utilizzato, per esempio, nella grammatica delle parole di Dick Hudson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove, per così dire, tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore, qui ama, verso tutti i congiunti separatamente. Queste sono le creazioni di Barton."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "Ora, lo scopo di questo articolo è proporre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "In inglese, come saprai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli elementi accessori possono essere più distanti, giusto? Quindi \"March, l'ho letto ieri\" è corretto perché l'oggetto diretto \"it\" è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "Sebbene marzo abbia letto ieri, è molto peggio, vero? Perché qui tra il verbo e l'oggetto diretto, c'è un complemento di tempo ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo, poiché in tal caso può essere spostato nella posizione dopo l'integrazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. A marzo ho letto questo libro assolutamente affascinante sulla BCS oggi. Va bene. Il modo invece di esso, abbiamo questo lungo NP."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "Ma va bene anche dire \"Ho letto ieri a marzo questo libro assolutamente affascinante sulle api.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il ragionamento qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere adiacenti al verbo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Wojciech Czaja - Soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferibili.\nWojciech Czaja - dipendenze più brevi sono preferite."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, cioè quelle che non sono costanti tra queste due strutture,"}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi qui abbiamo una dipendenza dalla lettura all'aggettivo di lunghezza sette, misurata in parole, e dalla lettura al libro di lunghezza quattro. Quindi insieme fa undici."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "Quando sposti, quando scambi questi due costituti, la somma di queste due dipendenze diventa 6, giusto? Quindi invece di 11, 6, molto più breve. Ecco perché questo suona piuttosto bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, quindi ciò che abbiamo fatto è stato estrarre varie statistiche sulla coordinazione dalla versione potenziata del Penn Treebank e consultare il motivo per cui non abbiamo utilizzato le dipendenze universitarie nel nostro studio."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "Mateusz Piorkowski - E le statistiche confermano l'osservazione già fatta in precedenza che i contratti di lavoro a sinistra tendono ad essere più brevi, così come il sale e pepe e il sale misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "e anche l'osservazione fatta incidentalmente che questa tendenza aumenta con la differenza di lunghezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo più forte. Giusto. Quindi, la proporzione è maggiore per i congiunti sinistri più corti."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando le governanti a sinistra sono assenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il governatore è a sinistra in questo esempio. Ho visto Bart e Lisa, quindi il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "È assente nel secondo esempio. Omero è venuto e ha starnutito. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Quindi, in tali casi, il congiunto sinistro preferisce essere più breve, tanto più che la differenza tra le due congiunzioni è maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance a destra è presente, la sinistra governa la coordinazione, tel e net, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "Dimostriamo quindi che, misurando la lunghezza in caratteri, la prima colonna rappresenta le sillabe, la colonna centrale le parole, e la colonna di destra i caratteri. Mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Quello che vediamo qui è che quando il governatore è a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "La tendenza del congiunto sinistro ad essere più breve aumenta gradualmente con la differenza assoluta nelle parole. E lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi. Ma quando il governatore è a destra, questa tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "E dimostriamo nel documento come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche come queste due e a favore delle strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "Per leggere l'accordo completo e gli argomenti, si prega di consultare il documento, scusate. E parlate con noi riguardo alla sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Xiangbin, sono uno studente di dottorato presso l'Università di Washington. Oggi presenterò il nostro lavoro, che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando i sentieri dei pregiudizi politici che portano a modelli NLP ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "I modelli linguistici vengono addestrati su dati raccolti su larga scala dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media di informazione politica sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C4, possiamo notare che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben inclusi nei dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Questo ha creato una benedizione a metà per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, hanno avuto la possibilità di apprendere da prospettive diverse, celebrando così la democrazia e la pluralità delle idee. D'altro canto, queste differenti opinioni politiche sono intrinsecamente viziate da pregiudizi sociali e potrebbero condurre a potenziali problemi di equità nelle applicazioni dei compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A tale scopo, proponiamo di indagare il pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, specificamente ponendoci le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo possono avere i dati di appartenenza su tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano i modelli linguistici con diversi limiti politici nelle attività a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, abbiamo inizialmente proposto di stimolare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il test della bussola politica. Ciò ci consente di effettuare una valutazione automatica ben fondata nella letteratura delle scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli di linguaggio di prima lingua presentano orientamenti politici variabili. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche notare che GPT-4 è il modello linguistico più liberale tra tutti, e le teorie GPT sono generalmente più socialmente liberali rispetto alla teoria BERT e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, ci proponiamo di indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente acquisiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "Quindi potremmo condurre un esperimento controllato pre-allenando ulteriormente i checkpoint del modello linguistico su sei diversi corpora partigiani, separati in notizie e social media, ulteriormente suddivisi in base alle loro tendenze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "Attraverso un ulteriore pre-addestramento dei modelli linguistici su tali corpora parziali, possiamo osservare uno spostamento corrispondente nelle coordinate ideologiche del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per Roberta, ulteriormente affinata e addestrata sul corpus di Reddit orientato a sinistra, possiamo osservare uno spostamento liberale sostanziale in termini di"}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "in termini di pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici siano in grado di cogliere la polarizzazione che caratterizza la nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "Quindi dividiamo i corpora di pre-addestramento in due: prima del 45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti. Quindi pre-addestriamo separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo osservare che i modelli linguistici hanno generalmente mostrato una tendenza politica più distante dal centro dopo il 2017. Ciò indica che anche i modelli linguistici possono riflettere la polarizzazione presente nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Infine, valutiamo i modelli linguistici con diverse inclinazioni politiche nella rilevazione di discorsi d'odio e nella detectione di fake news, applicazioni di NLP che spesso coinvolgono modelli linguistici e che potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vediamo che, se esaminiamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "Osservando le diverse demografie o il significato politico dei media, possiamo individuare un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di orientamento progressista sono più efficaci."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "nell'individuare discorsi d'odio rivolti a gruppi minoritari sociali"}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "tuttavia, sono meno bravi nel rilevare discorsi d'odio rivolti a gruppi più potenti nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "e viceversa, i modelli linguistici di orientamento conservatore sono migliori nel rilevare discorsi d'odio rivolti ai bianchi e agli uomini, ma peggiori nel rilevare discorsi d'odio diretti verso neri, LGBTQ+ e altre comunità minoritarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "Anche per il rilevamento delle fake news si osservano tendenze simili, dove i modelli linguistici di orientamento sinistro sono più abili nel rilevare disinformazione proveniente da posizioni politiche opposte e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, mostriamo ulteriormente numerosi esempi qualitativi per dimostrare che i modelli linguistici con diversi significati politici..."}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "forniscono previsioni diverse per esempi di discorso d'odio e disinformazione in base alla loro categoria sociale. Nell'appendice sono presenti ulteriori esempi per evidenziare meglio questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che esiste un problema di equità molto pressante relativo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, se un modello linguistico con una tendenza di destra venisse affinato su discorsi d'odio, disinformazione o simili, e poi implementato su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che i discorsi d'odio rivolti ai gruppi minoritari potrebbero diffondersi impunemente senza alcun controllo."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ha suonato l'allarme per riconoscerci e affrontare i problemi di equità derivanti dalle inclinazioni politiche dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, una breve discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come trovarsi tra Scilla e Cariddi."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento del modello linguistico, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se proviamo in qualche modo a sanificare, rischiamo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba mantenere i dati sulla monotonia linguistica. È un po' come il problema del tram elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Bene, ottimo. Penso che questo sia più o meno tutto per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jenny, sono una studentessa del primo anno di dottorato presso la Carnegie Mellon University, e oggi presenterò il vostro lavoro, \"Anal Positionality: Caratterizzazione dei Pregiudizi Progettati di Insiemi di Dati e Modelli\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santee, Ronan Labrosse, Katarina Reinecke e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo quindi immaginando di lavorare per un giornale e di setacciare i commenti sotto un articolo di attualità, cercando di rimuovere i contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potresti orientarti verso un'API popolare come Perspective API per il rilevamento della tossicità. E questo funziona davvero bene se sei Carl Jones, dove Perspective API è in grado di rilevare correttamente gli esempi tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è davvero il caso di Aditya Sharma, dove l'API della prospettiva non è in realtà così sensibile ai termini offensivi che sono più comuni nei contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di pregiudizio nel design in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Preconcetti di progettazione come quello che abbiamo appena visto potrebbero verificarsi a causa della posizione degli studiosi di NLP e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno in virtù delle loro caratteristiche demografiche, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in quanto ricercatore, la posizione può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione o una prospettiva specifica?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo cercando di affermare che modelli, celle e set di dati abbiano di per sé identità demografiche ed esperienze di vita, ma essi aggrega giudizi e opinioni di persone reali e possono così rappresentare determinate posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, lavori precedenti hanno suggerito alcune evidenze aneddotiche della presenza di posizionamento, come ad esempio lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questi lavori non si concentrano effettivamente sul confronto tra gli utenti finali con i set di dati e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità dei modelli e dei set di dati diventa sempre più importante man mano che i compiti di NLP diventano più soggettivi e orientati al sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "E risulta impegnativo caratterizzare in che modo queste posizioni sono distorte, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Per studiare la posizione del dataset e del modello, confrontiamo effettivamente le annotazioni con gli utenti reali con i dataset e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "Lo facciamo attraverso il nostro framework di posizionamento NL (Natural Language)."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework funziona in due passaggi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo è quello di ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E scegliamo di fare ciò esaminando la demografia degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza e perché i dati demografici vengono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E così optiamo per ri-annotare i dati al fine di ottenere molti annotatori per istanza e un insieme ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Prendiamo quindi le annotazioni demografiche e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson's R."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così il nostro framework differisce effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con i modelli e i set di dati, le previsioni e le etichette, piuttosto che limitarsi a esaminare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro framework è in gran parte reso possibile grazie a Lab in the Wild, una piattaforma online di crowdsourcing fornita dal nostro collaboratore HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto a piattaforme come MTurk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Ospitiamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. Il funzionamento è il seguente: i partecipanti leggeranno una situazione dal dataset sulla chimica sociale e poi scriveranno quanto una situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'IA e con quelle di altri."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi confrontato queste annotazioni con la chimica sociale, il metodo Delphi e GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "Repliciamo poi un setup molto simile per il compito di rilevamento della tossicità e del discorso d'odio, in cui leggeranno un'istanza da DynaHate e scriveranno se ritengono che si tratti di un'istanza di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, abbiamo confrontato queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta e GPT-4. Il nostro studio ha infine accumulato oltre 16.000 annotazioni da oltre un migliaio di annotatori provenienti da 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "Ora siamo meglio attrezzati per rispondere a chi si allineano di più i dataset e i modelli di NLP? Scopriamo che esiste una posizione nell'elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, riscontriamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Pertanto, per l'analisi dell'accettabilità sociale di GPT-4, constatiamo che è più allineata ai paesi confuciani e di lingua inglese. Riscontriamo inoltre che il dyna-hate è anch'esso più allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo anche la maggior parte delle affinità con le persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nel compito di accettabilità sociale, constatiamo che è più allineato con le persone con un'istruzione universitaria o post-laurea."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "E lo stesso vale per Donahate, dove è più allineato alle persone con un'istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando i modelli e i set di dati sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente esclusi."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto ai corrispondenti uomini e donne. Riscontriamo questo nel compito di accettabilità sociale di GPT-4, così come nell'analisi del compito DynaHATE."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Dato che la positionalità è un aspetto presente nell'elaborazione del linguaggio naturale (NLP), cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo alcune raccomandazioni a riguardo. La prima è mantenere un registro di tutte le scelte di design rilevanti durante l'intero processo di ricerca. L'altra è condurre ricerche di NLP con un approccio perspectivista."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di costruire set di dati e modelli specializzati all'interno di quattro comunità specifiche. Un buon esempio di ciò è l'Iniziativa Masakane. Vogliamo sottolineare che l'NLP inclusivo non si limita a far funzionare tutte le tecnologie per chiunque."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "E questo conclude la nostra presentazione, ma se desiderate saperne di più, sentitevi liberi di consultare il nostro cruscotto per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro, \"Distillazione della Conoscenza di Script da Modelli Linguistici di Grandi Dimensioni per la Pianificazione del Linguaggio Vincolato\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo interazioni passo-passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Il lavoro precedente ha sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come preparare una torta, e ha dimostrato che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come preparare una torta al cioccolato, rimane ancora poco studiata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "che impongono vincoli diversi agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Poiché non esiste alcun insieme di dati di obiettivi specifici a supporto del nostro studio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati. Per l'acquisizione di dati con un essere umano nel ciclo, utilizzare InstructGPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Campioniamo 100 ragazze specifiche e valutiamo gli script generati da grandi modelli locali."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "Questa tabella riporta l'accuratezza complessiva dei risultati. Riscontriamo che tutti i modelli linguistici leggeri ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, conduciamo un'analisi dettagliata per indagare le ragioni per cui i modelli di apprendimento sequenziale falliscono."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati illustrati nella figura dimostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà alle restrizioni non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo analizzato categorie di argomenti più specifiche relative ai vincoli definiti su WikiHow. La mappa di calore nel grafico mostra che le prestazioni di pianificazione dei PD istruttivi variano considerevolmente per le ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di vento leggero presenta una elevata varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea del filtro Z sovragenerato per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Mostriamo inizialmente i tipi di vincoli con esempi per il CPT intrattabile, e otteniamo obiettivi specifici basati sugli obiettivi astratti iniziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, istruire GPT a sovra-generare script di casi per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, viene sviluppato un modello di filtro per selezionare gli script fattibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e obiettivi in embedding astratti GPT e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, evitiamo la sceneggiatura che contiene le parole chiave della restrizione bersaglio. Conserviamo la sceneggiatura solo se l'obiettivo bersaglio ottiene il punteggio più alto nell'insieme degli obiettivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, InstructZBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione, sia in termini di completezza semantica che di fedeltà al vincolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è fondamentale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passaggio essenziale per il raggiungimento di questo obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, gli studi precedenti non consentono di pianificare obiettivi specifici e l'annotazione manuale dei dataset è costosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare insiemi di dati di pianificazione linguistica vincolata da grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applica il nostro metodo per la costruzione di un dataset di pianificazione linguistica vincolata, denominato Codescript."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di convalida e di test, chiediamo ai lavoratori di fonti cloud di individuare campioni corretti rivisti."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle restrizioni della script di codice. Riscontriamo che la script di codice presenta un elevato grado di apprezzamento negli obiettivi specifici generati. Con la script di codice, possiamo tracciare modelli più piccoli ma specializzati per la pianificazione del linguaggio delle restrizioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che la funzione T-file sul tasso di costo può generare script di qualità superiore rispetto alla maggior parte dei modelli di linguaggio di grandi dimensioni, indicando che modelli più piccoli possono supportare modelli più grandi quando addestrati correttamente su dataset appropriati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo definito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di filtro sovragenerato per i grandi modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo grandi modelli linguistici per generare un dataset di script di alta qualità per la pianificazione linguistica vincolata. Ci auguriamo che il dataset CodeScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. Per ulteriori dettagli sullo script del codice, si prega di consultare il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Zhu Heng. Oggi presenterò il nostro articolo: \"I tagger di entità denominate Kernel 2003 funzionano ancora bene nel 2023?\" Iniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha esaminato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità denominate, o compito NER."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che i modelli hanno utilizzato CONO 2003 per sviluppare il NER da quasi 20 anni. E questo, naturalmente, solleva diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, se osserviamo una scarsa generalizzazione, quali sono le cause del calo delle prestazioni di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il dataset CONO++. Si tratta di un dataset che abbiamo raccolto dalle notizie Reuters del 2020 e poi annotato secondo le stesse linee guida di annotazione CONO 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo poi ottimizzato oltre 20 modelli su Kano 2003. Li abbiamo valutati sia sul set di test Kano 03 che sul set di test Kano++."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "Infine, ma non meno importante, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa è necessario per una buona generalizzazione? Dai nostri esperimenti, abbiamo scoperto che sono necessari tre ingredienti principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 182, "src_lang": "en", "tgt_lang": "it", "output": "La prima è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo riscontrato che i modelli basati su transformer generalmente generalizzano meglio su nuovi dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 183, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo ingrediente è la dimensione del modello. Abbiamo riscontrato che solitamente i modelli più grandi portano a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 184, "src_lang": "en", "tgt_lang": "it", "output": "E infine, ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influisce direttamente sulle prestazioni di un'attività a valle. Qui abbiamo scoperto anche che un maggior numero di esempi di ottimizzazione fine porta effettivamente a una migliore generalizzazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 185, "src_lang": "en", "tgt_lang": "it", "output": "Alla nostra prossima domanda, quali sono le cause del calo delle prestazioni di alcuni modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 186, "src_lang": "en", "tgt_lang": "it", "output": "Avevamo due ipotesi. La prima è l'overfitting adattivo, che è un overfitting causato dal riutilizzo dello stesso set di test ripetutamente. E questo si manifesta solitamente come un calo dei rendimenti su un nuovo set di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 187, "src_lang": "en", "tgt_lang": "it", "output": "La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test."}
{"dataset_id": "mcif_v1.0", "sample_id": 188, "src_lang": "en", "tgt_lang": "it", "output": "Per l'overfitting adattivo, come abbiamo visto dal grafico a destra, la linea di miglior adattamento rossa ha una pendenza superiore a 1."}
{"dataset_id": "mcif_v1.0", "sample_id": 189, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su Carnot 2003 si traduce in più di un'unità di miglioramento su Carnot++, il che implica che non ci sono rendimenti decrescenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 190, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci dimostra che in questo caso non si osserva il sovradattamento adattivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 191, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, che ne è della Deriva Temporale?"}
{"dataset_id": "mcif_v1.0", "sample_id": 192, "src_lang": "en", "tgt_lang": "it", "output": "Per la deriva temporale, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti, e abbiamo riscontrato che le prestazioni diminuiscono con un divario temporale più ampio."}
{"dataset_id": "mcif_v1.0", "sample_id": 193, "src_lang": "en", "tgt_lang": "it", "output": "E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è la deriva temporale."}
{"dataset_id": "mcif_v1.0", "sample_id": 194, "src_lang": "en", "tgt_lang": "it", "output": "La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande, nonché di più esempi di ottimizzazione. E questi vanno di pari passo. Non possiamo avere solo un ingrediente, ma tutti gli altri nel complesso."}
{"dataset_id": "mcif_v1.0", "sample_id": 195, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, abbiamo scoperto anche che il calo delle prestazioni qui è causato da una deriva temporale e, sorprendentemente, non è dovuto a un adattamento eccessivo, nonostante KONO 2003 sia in uso da oltre 20 anni."}
{"dataset_id": "mcif_v1.0", "sample_id": 196, "src_lang": "en", "tgt_lang": "it", "output": "Ritornando alla domanda che abbiamo posto nel titolo del nostro articolo, i tagger di Connell del 2003 funzionano ancora nel 2023? Abbiamo riscontrato che la risposta è in realtà un sonoro sì."}
{"dataset_id": "mcif_v1.0", "sample_id": 197, "src_lang": "en", "tgt_lang": "it", "output": "Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare le generalizzazioni dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 198, "src_lang": "en", "tgt_lang": "it", "output": "Infine, assicurati di consultare il nostro articolo, il nostro dataset e, se hai domande, non esitare a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 199, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione di espressioni di riferimento indirette per la selezione di entità, in cui introduciamo gli AltEntityScorers."}
{"dataset_id": "mcif_v1.0", "sample_id": 200, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinski, Sylvia Parity e Annie Lewis."}
{"dataset_id": "mcif_v1.0", "sample_id": 201, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro obiettivo è comprendere la lingua dell'utente quando desidera effettuare una scelta. Considera questa domanda alternativa. Intendevi \"facile per me\" o \"ho una sensazione\"? Qui un utente vuole selezionare una di queste due canzoni."}
{"dataset_id": "mcif_v1.0", "sample_id": 202, "src_lang": "en", "tgt_lang": "it", "output": "La cosa più ovvia è utilizzare un riferimento diretto. Per esempio, dicendo che il nome della canzone è \"Yami\" o la sua posizione, la prima."}
{"dataset_id": "mcif_v1.0", "sample_id": 203, "src_lang": "en", "tgt_lang": "it", "output": "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 204, "src_lang": "en", "tgt_lang": "it", "output": "o le pronunce sono troppo simili tra loro e difficili da distinguere."}
{"dataset_id": "mcif_v1.0", "sample_id": 205, "src_lang": "en", "tgt_lang": "it", "output": "oppure quando l'utente desidera specificare una preferenza. Ecco alcuni esempi di differenze dirette. Ad esempio, il più recente o la canzone che non è energica."}
{"dataset_id": "mcif_v1.0", "sample_id": 206, "src_lang": "en", "tgt_lang": "it", "output": "Si tratta di un problema importante nei sistemi conversazionali e anche per la valutazione della comprensione delle entità da parte dei LLM."}
{"dataset_id": "mcif_v1.0", "sample_id": 207, "src_lang": "en", "tgt_lang": "it", "output": "Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per un compito specifico. Pertanto, ne raccogliamo uno utilizzando l'annotazione collettiva. Il nostro set di dati copre tre diversi domini: musica, libri e ricette."}
{"dataset_id": "mcif_v1.0", "sample_id": 208, "src_lang": "en", "tgt_lang": "it", "output": "La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un insieme di completamento di cartoni animati."}
{"dataset_id": "mcif_v1.0", "sample_id": 209, "src_lang": "en", "tgt_lang": "it", "output": "Il fumetto ha tre nuvolette. Nella prima, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\" E con ciò, Bob stabilisce il contesto del dialogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 210, "src_lang": "en", "tgt_lang": "it", "output": "Nel secondo fumetto, Alice dice: \"Intendi 'facile per me' o 'ho una sensazione'?\""}
{"dataset_id": "mcif_v1.0", "sample_id": 211, "src_lang": "en", "tgt_lang": "it", "output": "qual è la domanda alternativa. E nella terza nuvoletta, Bob utilizza un riferimento indiretto per selezionare una di queste entità, per esempio, la nuova."}
{"dataset_id": "mcif_v1.0", "sample_id": 212, "src_lang": "en", "tgt_lang": "it", "output": "Forniamo automaticamente i primi e secondi balloon del discorso, ma il terzo viene compilato dall'annotatore. Il primo balloon viene scelto da alcuni prompt manuali per dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 213, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo, che è la domanda alternativa, viene generato come segue."}
{"dataset_id": "mcif_v1.0", "sample_id": 214, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo sempre un semplice modello. Ti riferisci al modello A o B? Dove A e B sono esempi tratti da Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 215, "src_lang": "en", "tgt_lang": "it", "output": "Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che si sale nella lista, le entità diventano più simili tra loro e, di solito, diventa più difficile effettuare la disambiguazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 216, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è uniforme a-caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 217, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso si verifica quando le entità hanno titoli simili, per esempio, due libri con il nome \"Il Ritorno\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 218, "src_lang": "en", "tgt_lang": "it", "output": "Il terzo caso si verifica quando hanno descrizioni simili su Wikipedia. Infine, quando presentano info box o attributi simili su Wikipedia, come ad esempio lo stesso genere o lo stesso artista."}
{"dataset_id": "mcif_v1.0", "sample_id": 219, "src_lang": "en", "tgt_lang": "it", "output": "Quando presentiamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente ne sanno qualcosa."}
{"dataset_id": "mcif_v1.0", "sample_id": 220, "src_lang": "en", "tgt_lang": "it", "output": "Quello che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, forniamo semplicemente un link di ricerca Google per ciascuna canzone."}
{"dataset_id": "mcif_v1.0", "sample_id": 221, "src_lang": "en", "tgt_lang": "it", "output": "e poi chiedere agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere le informazioni su ciascuna di esse. Ecco, per esempio, il risultato della ricerca Google per la canzone \"Easy Annotation\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 222, "src_lang": "en", "tgt_lang": "it", "output": "Per il dominio delle ricette e dei libri, mostriamo del testo di sfondo tratto da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono."}
{"dataset_id": "mcif_v1.0", "sample_id": 223, "src_lang": "en", "tgt_lang": "it", "output": "Chiediamo quindi agli annotatori di scegliere una di queste entità, per esempio, qui la prima, e descriverla utilizzando tre o cinque espressioni di riferimento indiretto."}
{"dataset_id": "mcif_v1.0", "sample_id": 224, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, quello con la musica al pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, quello senza parole, non quello con il ragazzo di 12 anni, o quello fittizio, o quello proveniente dall'Azerbaigian, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 225, "src_lang": "en", "tgt_lang": "it", "output": "Il corpus Altentities contiene 6000 domande alternative distribuite in tre domini e include 42.000 espressioni di riferimento indirette. I risultati ottenuti con il modello T5XLARGE sono riassunti di seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 226, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso allo stesso identico bagaglio di conoscenze degli annotatori, allora la precisione è davvero elevata. Si aggira tra il 92% e il 95%. Ma questa è una situazione non realistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 227, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza si attesta tra l'82% e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera la conoscenza di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 228, "src_lang": "en", "tgt_lang": "it", "output": "Se il modello linguistico ha accesso solo ai nomi di entità, allora l'accuratezza è solo del 60%. Quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili in diversi domini. Ecco un link al nostro dataset. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 229, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sara Pape dell'Università di Trento e della Fondazione Bruno Kessler e presenterò brevemente il lavoro intitolato \"L'attenzione come guida per la traduzione simultanea del parlato\", realizzato in collaborazione con Matteo Negri e Marco Turchi."}
{"dataset_id": "mcif_v1.0", "sample_id": 230, "src_lang": "en", "tgt_lang": "it", "output": "Che cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o simulST, è il processo di tradurre una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 231, "src_lang": "en", "tgt_lang": "it", "output": "E quali sono i problemi dei modelli SimulST attuali? Di solito vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare."}
{"dataset_id": "mcif_v1.0", "sample_id": 232, "src_lang": "en", "tgt_lang": "it", "output": "Procedure di addestramento lunghe e complesse, per esempio l'addestramento che coinvolge diversi obiettivi di ottimizzazione,"}
{"dataset_id": "mcif_v1.0", "sample_id": 233, "src_lang": "en", "tgt_lang": "it", "output": "e addestrando e mantenendo diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrando un modello con una latenza media di 1 secondo e un altro con una latenza di 2 secondi e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 234, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qual è la nostra soluzione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 235, "src_lang": "en", "tgt_lang": "it", "output": "Utilizza i primi due modelli SD offline già esistenti senza riaddestrarli o adottando architetture specifiche per ogni SD. Utilizza un solo modello per ogni regime di latenza e gestisci la latenza attraverso parametri specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 236, "src_lang": "en", "tgt_lang": "it", "output": "e sfruttare le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di cross-attenzione. Puoi vedere un esempio a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 237, "src_lang": "en", "tgt_lang": "it", "output": "La nostra soluzione è proporre l'ADAT (Attenzione Encoder-Decoder) o l'attenzione encoder-decoder, che è una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 238, "src_lang": "en", "tgt_lang": "it", "output": "Viene emesso una parola se la tensione non è concentrata, cioè, ... la sua somma è al di sotto di una certa soglia α, verso le ultime righe dei frame del discorso, il che significa che le informazioni ricevute sono ..."}
{"dataset_id": "mcif_v1.0", "sample_id": 239, "src_lang": "en", "tgt_lang": "it", "output": "Per esempio, se riceviamo un frammento di discorso contenente \"I'm going to talk about\" e il nostro modello prevede la traduzione in tedesco,"}
{"dataset_id": "mcif_v1.0", "sample_id": 240, "src_lang": "en", "tgt_lang": "it", "output": "E esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 241, "src_lang": "en", "tgt_lang": "it", "output": "Vedremo che le prime due parole indicano i primi schemi di linguaggio ricevuti, mentre l'ultima parola si riferisce agli ultimi schemi di linguaggio ricevuti come schemi di linguaggio lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 242, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che le prime due parole saranno omesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 243, "src_lang": "en", "tgt_lang": "it", "output": "Finché la somma dell'attenzione incrociata supera una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso."}
{"dataset_id": "mcif_v1.0", "sample_id": 244, "src_lang": "en", "tgt_lang": "it", "output": "Se continuiamo e riceviamo un altro frammento di discorso e il nostro modello prevede altri tre parole, esamineremo i pesi dell'attenzione incrociata."}
{"dataset_id": "mcif_v1.0", "sample_id": 245, "src_lang": "en", "tgt_lang": "it", "output": "vedremo che nessuna parola punta agli ultimi frame del discorso lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 246, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significa che queste tre parole verranno emesse."}
{"dataset_id": "mcif_v1.0", "sample_id": 247, "src_lang": "en", "tgt_lang": "it", "output": "Se esaminiamo i principali risultati di ciò,"}
{"dataset_id": "mcif_v1.0", "sample_id": 248, "src_lang": "en", "tgt_lang": "it", "output": "Rappresentiamo i risultati della traduzione simultanea del discorso in grafici in cui su un lato abbiamo il blu che misura la qualità della traduzione e il ritardo medio."}
{"dataset_id": "mcif_v1.0", "sample_id": 249, "src_lang": "en", "tgt_lang": "it", "output": "quella è la misura della latenza. E consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 250, "src_lang": "en", "tgt_lang": "it", "output": "Quindi vogliamo che le nostre curve siano il più in alto possibile in questo grafico."}
{"dataset_id": "mcif_v1.0", "sample_id": 251, "src_lang": "en", "tgt_lang": "it", "output": "ma vogliamo anche che siano spostati a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 252, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo con strategie appropriate che si applicano anche ai modelli offline, ovvero la strategia a chiave umida e l'accordo locale. Inoltre, confrontiamo con l'architettura allo stato dell'arte specificamente progettata per la pre-traduzione simultanea."}
{"dataset_id": "mcif_v1.0", "sample_id": 253, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono tutti i risultati della strategia di traduzione simultanea del discorso in tedesco."}
{"dataset_id": "mcif_v1.0", "sample_id": 254, "src_lang": "en", "tgt_lang": "it", "output": "E osserviamo che supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 255, "src_lang": "en", "tgt_lang": "it", "output": "E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo computazionale consapevole, quella è la strategia più rapida."}
{"dataset_id": "mcif_v1.0", "sample_id": 256, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri scoprire ulteriori risultati, leggi il nostro articolo. Inoltre, abbiamo rilasciato in open source il codice e i modelli, fornendo un'output simultaneo per agevolare la riproducibilità del nostro lavoro. Grazie per la tua attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 257, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Ying e, insieme al mio collega Zhiyang, presenteremo la nostra ricerca su \"Multi-Improvement: Migliorare l'Apprendimento Seriale Multimodale Breve tramite Tuning delle Istruzioni\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 258, "src_lang": "en", "tgt_lang": "it", "output": "Con i progressi nei modelli di linguaggio di grandi dimensioni, molti lavori hanno iniziato ad esplorare nuovi paradigmi di apprendimento che riutilizzano i modelli di linguaggio pre-addestrati per diversi compiti a valle in modo efficiente sia in termini di parametri che di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 259, "src_lang": "en", "tgt_lang": "it", "output": "Recentemente, numerosi studi hanno dimostrato che il tuning delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in precedenza in modo zero-shot, seguendo istruzioni naturali."}
{"dataset_id": "mcif_v1.0", "sample_id": 260, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si concentra sul miglioramento delle prestazioni del diagramma seriale su compiti esclusivamente linguistici, trascurando i compiti di visione artificiale e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 261, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo lavoro, desideriamo indagare se il tuning delle istruzioni su modelli pre-addestrati multimodali possa effettivamente migliorare la generalizzazione a compiti multimodali non visti."}
{"dataset_id": "mcif_v1.0", "sample_id": 262, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, al momento della nostra ricerca, abbiamo riscontrato una notevole discrepanza nella disponibilità di dataset di istruzione tra NLP e multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 263, "src_lang": "en", "tgt_lang": "it", "output": "Esistono più di 1600 compiti di istruzione basati solo sul linguaggio. Tuttavia, non esiste un compito di istruzione multimodale su larga scala pubblicamente accessibile. Pertanto, questo ci motiva a costruire un dataset di accordatura delle istruzioni multimodali."}
{"dataset_id": "mcif_v1.0", "sample_id": 264, "src_lang": "en", "tgt_lang": "it", "output": "Qui presentiamo Multi-Instruct, il primo dataset di benchmark per il tuning di istruzioni multimodali che consiste in 62 compiti multimodali diversificati che coprono 10 categorie generali."}
{"dataset_id": "mcif_v1.0", "sample_id": 265, "src_lang": "en", "tgt_lang": "it", "output": "Questi compiti sono derivati da 21 dataset open source esistenti e ogni compito è dotato di 5 istruzioni scritte da esperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 266, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare il tuning dell'istruzione multimodale sul nostro dataset proposto, utilizziamo OFA, un modello pre-addestrato multimodale unificato, come modello di base. OFA impiega un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una casella delimitatrice."}
{"dataset_id": "mcif_v1.0", "sample_id": 267, "src_lang": "en", "tgt_lang": "it", "output": "Qui mostriamo alcuni esempi tratti dal nostro dataset Multi-Instra."}
{"dataset_id": "mcif_v1.0", "sample_id": 268, "src_lang": "en", "tgt_lang": "it", "output": "Per unificare l'elaborazione di una varietà di tipi di dati di input e output,"}
{"dataset_id": "mcif_v1.0", "sample_id": 269, "src_lang": "en", "tgt_lang": "it", "output": "Seguiamo il metodo dell'OFA e formuliamo tutti i compiti in un formato unificato sequenza-sequenza, in cui il testo in ingresso, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio dei token."}
{"dataset_id": "mcif_v1.0", "sample_id": 270, "src_lang": "en", "tgt_lang": "it", "output": "Va bene, ora parlerò del tuning dell'istruzione multimodale."}
{"dataset_id": "mcif_v1.0", "sample_id": 271, "src_lang": "en", "tgt_lang": "it", "output": "Per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento di buon senso per il test e selezioniamo ulteriori 5 compiti dai gruppi VQA e vari."}
{"dataset_id": "mcif_v1.0", "sample_id": 272, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo tutte le istanze nella partizione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla partizione di test di istruzioni naturali come compiti non visti per l'elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 273, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi un modello OFA di grandi dimensioni pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con una delle sue cinque istruzioni di modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 274, "src_lang": "en", "tgt_lang": "it", "output": "Quindi durante il test, per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 275, "src_lang": "en", "tgt_lang": "it", "output": "Riportiamo la media e le prestazioni massime, nonché la deviazione standard delle prestazioni attraverso tutti e cinque gli esperimenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 276, "src_lang": "en", "tgt_lang": "it", "output": "Se il compito è un'attività di classificazione multimodale, riportiamo l'accuratezza. Se si tratta di un'attività di generazione multimodale, riportiamo il ROUGE-L. Per i compiti di NLP, riportiamo anch'essi il ROUGE-L."}
{"dataset_id": "mcif_v1.0", "sample_id": 277, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre in modo coerente gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nella formulazione dell'istruzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 278, "src_lang": "en", "tgt_lang": "it", "output": "Ecco il nostro principale risultato. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA nei compiti multimodali di scena."}
{"dataset_id": "mcif_v1.0", "sample_id": 279, "src_lang": "en", "tgt_lang": "it", "output": "Anche il transfer learning da insiemi di dati di istruzioni naturali può trarre vantaggio dal tuning delle istruzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 280, "src_lang": "en", "tgt_lang": "it", "output": "Qui possiamo osservare come, con l'aumentare del numero di compiti, il modello raggiunga prestazioni migliori e contemporaneamente una minore sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 281, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi condotto un esperimento utilizzando un'istruzione rispetto a cinque istruzioni. Come possiamo osservare, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 282, "src_lang": "en", "tgt_lang": "it", "output": "Questo illustra l'effetto di diverse strategie di ottimizzazione fine sulla sensibilità del modello. Come possiamo osservare, attraverso l'apprendimento trasferibile da un dataset di istruzioni naturali, il modello può raggiungere una sensibilità molto superiore rispetto al modello OFA originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 283, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo anche osservare che il transfer learning dal dataset di istruzioni Nitro può aiutare OFA a ottenere prestazioni molto migliori sul dataset di istruzioni Nitro."}
{"dataset_id": "mcif_v1.0", "sample_id": 284, "src_lang": "en", "tgt_lang": "it", "output": "Proponiamo il primo dataset di tuning delle istruzioni multi-modello su larga scala. Miglioriamo significativamente la capacità zero-shot di OFV e esploriamo diverse tecniche di transfer learning, dimostrando i loro vantaggi. Progettiamo una nuova metrica chiamata sensibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 285, "src_lang": "en", "tgt_lang": "it", "output": "Un'altra cosa, stiamo raccogliendo un dataset di tuning delle istruzioni multimodali molto più ampio, con circa 150 compiti linguistici varianti aggiuntivi, e li renderemo disponibili. Questo è un codice QR per i nostri dati e modelli. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 286, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto alla presentazione del nostro articolo per ACL 2023, \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 287, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con John Gauthier, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams."}
{"dataset_id": "mcif_v1.0", "sample_id": 288, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, rivalutiamo i paradigmi delle coppie minime."}
{"dataset_id": "mcif_v1.0", "sample_id": 289, "src_lang": "en", "tgt_lang": "it", "output": "Quindi il paradigma delle coppie minime valuta essenzialmente i modelli linguistici in base a giudizi di accettabilità, che possono includere anche la grammaticalità, come nel caso di BLIMP, la sintassi, le GEM, o l'accettabilità in termini di stereotipi, come le coppie incrociate."}
{"dataset_id": "mcif_v1.0", "sample_id": 290, "src_lang": "en", "tgt_lang": "it", "output": "E in questo paradigma di coppia minima, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticalmente corretta, e poi presentare una frase accettabile o una frase non grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 291, "src_lang": "en", "tgt_lang": "it", "output": "E poi si spera che il modello attribuisca essenzialmente una probabilità maggiore alla frase accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 292, "src_lang": "en", "tgt_lang": "it", "output": "L'attuale pipeline MPP non ci consente di valutare l'accettazione di un modello per frasi più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 293, "src_lang": "en", "tgt_lang": "it", "output": "Al giorno d'oggi, i grandi modelli linguistici stanno sviluppando finestre contestuali sempre più ampie. Pertanto, è cruciale valutare l'accettabilità del modello in tutta la finestra contestuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 294, "src_lang": "en", "tgt_lang": "it", "output": "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe."}
{"dataset_id": "mcif_v1.0", "sample_id": 295, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo è l'approccio. Ciò che facciamo è simulare queste sequenze più lunghe. Rivediamo gli stessi set di dati e poi ricreiamo le frasi scegliendo tra frasi accettabili o non accettabili da quei set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 296, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, qui abbiamo scelto una coppia tipica di grammaticalità dal dataset BLIMP, dal caso dell'isola degli elementi subordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 297, "src_lang": "en", "tgt_lang": "it", "output": "E ciò che facciamo è ricreare sequenze più lunghe e accettabili, con la stessa corrispondenza della struttura grammaticale, estraendo frasi grammaticalmente corrette dall'Isola Argent."}
{"dataset_id": "mcif_v1.0", "sample_id": 298, "src_lang": "en", "tgt_lang": "it", "output": "E poi lo aggiungiamo come prefisso sia alla query accettabile che a quella non accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 299, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento. E questo potrebbe essere utilizzato anche per testare l'accettabilità del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 300, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un diverso insieme di dati. Questo è ciò che definiamo uno scenario di mismatch."}
{"dataset_id": "mcif_v1.0", "sample_id": 301, "src_lang": "en", "tgt_lang": "it", "output": "Qui, le frasi provengono ancora da dataset pertinenti, ma non dallo stesso dataset che stai valutando. E possiamo fare lo stesso per il caso di inaccettabilità,"}
{"dataset_id": "mcif_v1.0", "sample_id": 302, "src_lang": "en", "tgt_lang": "it", "output": "Infine, possiamo scegliere frasi da un dominio completamente estraneo, come Wikipedia."}
{"dataset_id": "mcif_v1.0", "sample_id": 303, "src_lang": "en", "tgt_lang": "it", "output": "Quindi questo ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da qualsiasi contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 304, "src_lang": "en", "tgt_lang": "it", "output": "Se il contesto deriva da un sottoinsieme diverso del set di dati o se è completamente irrilevante rispetto alla frase che stiamo esaminando."}
{"dataset_id": "mcif_v1.0", "sample_id": 305, "src_lang": "en", "tgt_lang": "it", "output": "Allora, come si comporta il modello? Innanzitutto, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query corrente. E qui scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 306, "src_lang": "en", "tgt_lang": "it", "output": "Aumentiamo la lunghezza del contesto fino a 1024 per sfruttare al massimo i modelli OPT e GPT-2. Come possiamo vedere qui, nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 307, "src_lang": "en", "tgt_lang": "it", "output": "Ora cosa accade quando scegliamo frasi dallo stesso insieme di dati?"}
{"dataset_id": "mcif_v1.0", "sample_id": 308, "src_lang": "en", "tgt_lang": "it", "output": "Quindi ecco che scegliamo o creiamo frasi da domini accettabili e non accettabili dallo stesso dataset di blimp o gemma sintattica."}
{"dataset_id": "mcif_v1.0", "sample_id": 309, "src_lang": "en", "tgt_lang": "it", "output": "E poi osserviamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 310, "src_lang": "en", "tgt_lang": "it", "output": "Ma quando allineiamo la struttura, ovvero quando selezioniamo le frasi dallo stesso fenomeno nel testo che attribuisce la colpa, Jim,"}
{"dataset_id": "mcif_v1.0", "sample_id": 311, "src_lang": "en", "tgt_lang": "it", "output": "osserviamo un notevole aumento o una notevole diminuzione del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 312, "src_lang": "en", "tgt_lang": "it", "output": "Ora questo, e questo è molto significativo, come questo effetto aumenta lungo la lunghezza del contesto. E questo probabilmente influenzerebbe modelli linguistici più recenti, che hanno una finestra di contesto ampia."}
{"dataset_id": "mcif_v1.0", "sample_id": 313, "src_lang": "en", "tgt_lang": "it", "output": "Perché quindi il prefisso \"match\" influenza così tanto il giudizio del modello linguistico?"}
{"dataset_id": "mcif_v1.0", "sample_id": 314, "src_lang": "en", "tgt_lang": "it", "output": "Quindi abbiamo condotto una serie di analisi in cui abbiamo cercato di mantenere la struttura rilevante della frase di input, ma aggiungendo del \"rumore\" ad essa. E dopo aver eseguito diverse di queste perturbazioni,"}
{"dataset_id": "mcif_v1.0", "sample_id": 315, "src_lang": "en", "tgt_lang": "it", "output": "constatiamo che nessuno di questi rumori sta effettivamente modificando il modello, ehm, come cambiare il suo corso in termini di come ci mostra la tendenza del giudizio MPP."}
{"dataset_id": "mcif_v1.0", "sample_id": 316, "src_lang": "en", "tgt_lang": "it", "output": "In sostanza, riscontriamo che i modelli rispondono alle perturbazioni e alle frasi in modo simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 317, "src_lang": "en", "tgt_lang": "it", "output": "Ovvero, quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio inaccettabile, riscontriamo una diminuzione dei giudizi MPP in modo analogo."}
{"dataset_id": "mcif_v1.0", "sample_id": 318, "src_lang": "en", "tgt_lang": "it", "output": "Quindi i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti, che sono condivise tra le frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 319, "src_lang": "en", "tgt_lang": "it", "output": "E la valutazione MPP, nel modo in cui la effettuiamo attualmente con input brevi e a singola frase, potrebbe non catturare completamente la conoscenza astratta del modello linguistico attraverso la finestra di contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 320, "src_lang": "en", "tgt_lang": "it", "output": "Si prega di leggere il nostro articolo per maggiori dettagli sui nostri esperimenti. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 321, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Yusheng Zhang dell'Università di Penn State. Oggi presenterò il nostro lavoro, \"Parsing semantico cross-lingue in più lingue naturali e rappresentazioni minime\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 322, "src_lang": "en", "tgt_lang": "it", "output": "Il parsing semantico è un'attività volta a costruire rappresentazioni semantiche delle query degli utenti, come SQL e il calcolo lambda."}
{"dataset_id": "mcif_v1.0", "sample_id": 323, "src_lang": "en", "tgt_lang": "it", "output": "E il parsing semantico cross-lingue è il compito di tradurre le query in più lingue naturali in diverse rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 324, "src_lang": "en", "tgt_lang": "it", "output": "Hau-Tieng Wu, Ph.D.: Come mostrato nella sua figura, è necessario tradurre la query in più lingue naturali utilizzando modelli neurali per sequenziare lambda o fun QL e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 325, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di parsing semantico cross-lingue esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitate. Ad esempio,"}
{"dataset_id": "mcif_v1.0", "sample_id": 326, "src_lang": "en", "tgt_lang": "it", "output": "ci sono lacune nella copertura di alcune lingue naturali. Il cinese è assente e"}
{"dataset_id": "mcif_v1.0", "sample_id": 327, "src_lang": "en", "tgt_lang": "it", "output": "a causa della copertura su alcune mini rappresentazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 328, "src_lang": "en", "tgt_lang": "it", "output": "Il calcolo lambda è assente."}
{"dataset_id": "mcif_v1.0", "sample_id": 329, "src_lang": "en", "tgt_lang": "it", "output": "o vengono valutati solo su determinati modelli neurali. Ad esempio, esiste un solo modello per valutarli."}
{"dataset_id": "mcif_v1.0", "sample_id": 330, "src_lang": "en", "tgt_lang": "it", "output": "A tal fine, proponiamo un esemplare. Forniamo un esemplare di insieme di dati uniforme per l'incrocio della parsificazione semantica in più lingue naturali e rappresentazioni del significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 331, "src_lang": "en", "tgt_lang": "it", "output": "Contiene nove insiemi di dati in vari domini, cinque compiti di parsing semantico, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 332, "src_lang": "en", "tgt_lang": "it", "output": "E per valutare meglio il nostro punto di riferimento, consideriamo le sei impostazioni per l'addestramento e la valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 333, "src_lang": "en", "tgt_lang": "it", "output": "Il primo è un test di traduzione. Utilizziamo l'API di Google Translate per tradurre il testo sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per addestrare una valutazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 334, "src_lang": "en", "tgt_lang": "it", "output": "E, per esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza, traduciamo la query tedesca utilizzando un'API in inglese e poi utilizziamo il modello addestrato per prevedere l'SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 335, "src_lang": "en", "tgt_lang": "it", "output": "E testeremo anche il modulo monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 336, "src_lang": "en", "tgt_lang": "it", "output": "In questo contesto, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco in tedesco o inglese in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 337, "src_lang": "en", "tgt_lang": "it", "output": "Testiamo anche l'impostazione di riprese sul campo monolingue addestrando modelli monolingui con solo il 10% dei dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 338, "src_lang": "en", "tgt_lang": "it", "output": "e testiamo un modello multilingue, che addestriamo per una sola volta per tutte le lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 339, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, mettiamo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 340, "src_lang": "en", "tgt_lang": "it", "output": "per tradurre richieste in tedesco o in cinese o altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 341, "src_lang": "en", "tgt_lang": "it", "output": "Consideriamo anche il trasferimento cross-lingue zero-shot e few-shot. Addestriamo su una lingua sorgente e trasferiamo ad un'altra lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 342, "src_lang": "en", "tgt_lang": "it", "output": "Quindi durante l'addestramento, stiamo addestrando il modello su query in inglese o sulla combinazione di query few-shot in inglese e tedesco per prevedere l'output SQL."}
{"dataset_id": "mcif_v1.0", "sample_id": 343, "src_lang": "en", "tgt_lang": "it", "output": "E troviamo anche molti risultati interessanti. Quindi, per quanto riguarda l'analisi dei modelli monolinguali, valutiamo su due gruppi di modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 344, "src_lang": "en", "tgt_lang": "it", "output": "inclusi gli encoder PDR, che stanno per encoder pre-addestrati multilingue con decodificatori basati su puntatori, come XLMR più PDR e BERT più PDR."}
{"dataset_id": "mcif_v1.0", "sample_id": 345, "src_lang": "en", "tgt_lang": "it", "output": "Valutiamo anche modelli encoder-decoder, come i modelli encoder-decoder pre-addestrati multilingue mBART e MT5."}
{"dataset_id": "mcif_v1.0", "sample_id": 346, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che l'architettura Encoder-Decoder ottiene le prestazioni migliori su tutti e nove i dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 347, "src_lang": "en", "tgt_lang": "it", "output": "e valutiamo su MT5 e XLMR più PDR in un contesto multilingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 348, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che i modelli encoder-decoder o encoder-PDR possono essere migliorati addestrandoli su un insieme di dati misto di varie lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 349, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo riscontrato che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, ad eccezione dell'inglese, le cui prestazioni diminuiscono in sette set di dati e aumentano solo in tre set di dati."}
{"dataset_id": "mcif_v1.0", "sample_id": 350, "src_lang": "en", "tgt_lang": "it", "output": "Credo che questo sia noto come la maledizione della plurilinguismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 351, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche il divario nelle prestazioni tra lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 352, "src_lang": "en", "tgt_lang": "it", "output": "In questa figura, la linea blu rappresenta il trasferimento few-shot cross-lingue. La linea arancione indica il trasferimento zero-shot cross-lingue, mentre la linea verde rappresenta l'impostazione monolingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 353, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato che confrontando la linea verde e quella arancione, abbiamo scoperto che, in un contesto a zero-shot, il divario di prestazioni nel trasferimento cross-lingue è significativo. E confrontando la linea blu con quella arancione, abbiamo rilevato che, in un contesto a pochi-shot, il divario di trasferimento si riduce rapidamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 354, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo riscontrato anche altri risultati interessanti. Ad esempio, l'encoder-decoder supera i lavori precedenti o ha ottenuto risultati comparabili. La rappresentazione sulla lingua naturale inglese può migliorare significativamente le prestazioni del few-shot nelle lingue naturali target."}
{"dataset_id": "mcif_v1.0", "sample_id": 355, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo riscontrato che i modelli linguistici multilingue come CODIS e BLUE sono ancora inadeguati per i compiti di analisi semantica cross-lingue."}
{"dataset_id": "mcif_v1.0", "sample_id": 356, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo sviluppato Examplar, un benchmark unificato per il parsing semantico cross-lingue con più lingue naturali e rappresentazioni principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 357, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue, e i nostri risultati mostrano molte scoperte interessanti, ecc. Siete invitati a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 358, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti. Mi chiamo David Villar e fornirò una breve panoramica del documento intitolato \"Grunting Platform Translation, Assessing Strategies and Performance\". Si tratta di un lavoro realizzato in collaborazione con i miei colleghi di Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 359, "src_lang": "en", "tgt_lang": "it", "output": "PARM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato lo scorso anno nel 2022. È stato addestrato su una vasta raccolta di testi composta da 780 miliardi di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 360, "src_lang": "en", "tgt_lang": "it", "output": "Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 361, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, presentiamo il primo studio sistematico di un grande modello linguistico per la traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 362, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità AMT. Ciò comporta l'utilizzo dei più recenti set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico."}
{"dataset_id": "mcif_v1.0", "sample_id": 363, "src_lang": "en", "tgt_lang": "it", "output": "E confrontiamo due sistemi all'avanguardia. Quindi, i sistemi con le migliori prestazioni sono quelli della valutazione WMT."}
{"dataset_id": "mcif_v1.0", "sample_id": 364, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo metriche di traduzione automatica neurale all'avanguardia e, inoltre, presentiamo anche risultati di valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del PROM."}
{"dataset_id": "mcif_v1.0", "sample_id": 365, "src_lang": "en", "tgt_lang": "it", "output": "L'indicazione ha una grande influenza sulle prestazioni dei modelli linguistici di grandi dimensioni (LLM) per la traduzione. Come possiamo osservare in un semplice esperimento in cui utilizziamo l'indicazione one-shot e forniamo due indicazioni diverse per ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 366, "src_lang": "en", "tgt_lang": "it", "output": "La maggioranza delle frasi, 516 su 1000, la differenza osservata è di più di un punto di sfocatura."}
{"dataset_id": "mcif_v1.0", "sample_id": 367, "src_lang": "en", "tgt_lang": "it", "output": "E questo può arrivare, nei casi estremi, fino a 40 punti di sfocatura. Quindi è importante scegliere una buona strategia di prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 368, "src_lang": "en", "tgt_lang": "it", "output": "Nei nostri esperimenti, abbiamo optato per una strategia di prompt a cinque riprese, contrassegnando semplicemente ogni frase fornita al sistema con la lingua in cui è scritta."}
{"dataset_id": "mcif_v1.0", "sample_id": 369, "src_lang": "en", "tgt_lang": "it", "output": "Quindi in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con il due punti tedesco e le traduzioni inglesi con il due punti inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 370, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo osservato che la forma effettiva della sollecitazione non ha una grande influenza nel caso di diverse sollecitazioni brevi."}
{"dataset_id": "mcif_v1.0", "sample_id": 371, "src_lang": "en", "tgt_lang": "it", "output": "È cruciale per il prompt a zero e un colpo. E quando passiamo, come nel nostro caso, al prompt a cinque colpi, non c'è quasi alcuna differenza nella forma effettiva del prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 372, "src_lang": "en", "tgt_lang": "it", "output": "Sono gli esempi a portare il peso maggiore."}
{"dataset_id": "mcif_v1.0", "sample_id": 373, "src_lang": "en", "tgt_lang": "it", "output": "Una sintesi dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine."}
{"dataset_id": "mcif_v1.0", "sample_id": 374, "src_lang": "en", "tgt_lang": "it", "output": "È quindi importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo gli input di selezione dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 375, "src_lang": "en", "tgt_lang": "it", "output": "I dati di sviluppo (dev data) sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più omogenei e producono risultati migliori. Pertanto, si ottiene una prestazione migliore utilizzando i dati di sviluppo."}
{"dataset_id": "mcif_v1.0", "sample_id": 376, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni Palm. Ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate."}
{"dataset_id": "mcif_v1.0", "sample_id": 377, "src_lang": "en", "tgt_lang": "it", "output": "Le intuizioni che abbiamo acquisito dall'abilitazione umana che abbiamo condotto utilizzando il framework MQM sono che la fluidità di PALM è paragonabile a sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 378, "src_lang": "en", "tgt_lang": "it", "output": "in particolare, gli errori più comuni sono gli errori di omissione."}
{"dataset_id": "mcif_v1.0", "sample_id": 379, "src_lang": "en", "tgt_lang": "it", "output": "Quindi sembra che Palm scelga di produrre una traduzione che suona meglio a volte omettendo parti della frase di origine che risultano nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 380, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, la categoria \"stile-esterno\" per PAN è inferiore rispetto ai sistemi all'avanguardia, il che rappresenta un segnale aggiuntivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 381, "src_lang": "en", "tgt_lang": "it", "output": "che PARM produce un output davvero fluente, ma con ancora alcuni problemi di accuratezza."}
{"dataset_id": "mcif_v1.0", "sample_id": 382, "src_lang": "en", "tgt_lang": "it", "output": "E questo è tutto per questa breve panoramica. Per maggiori dettagli, vi prego di partecipare alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 383, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Dawei, sono uno studente di dottorato presso l'Università del Saarland in Germania. In questo video, vorrei presentarvi il nostro recente lavoro, \"Più debole di quanto pensiate\", un'analisi critica dell'apprendimento supervisionato settimanale."}
{"dataset_id": "mcif_v1.0", "sample_id": 384, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con Xiao Yusheng, Mario Smusbach e Gia Steffen e DT Schlaukel."}
{"dataset_id": "mcif_v1.0", "sample_id": 385, "src_lang": "en", "tgt_lang": "it", "output": "Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento debolmente supervisionato."}
{"dataset_id": "mcif_v1.0", "sample_id": 386, "src_lang": "en", "tgt_lang": "it", "output": "Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra."}
{"dataset_id": "mcif_v1.0", "sample_id": 387, "src_lang": "en", "tgt_lang": "it", "output": "Quando confrontate con le annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata."}
{"dataset_id": "mcif_v1.0", "sample_id": 388, "src_lang": "en", "tgt_lang": "it", "output": "Se addestriamo direttamente reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore etichettato e non generalizzano."}
{"dataset_id": "mcif_v1.0", "sample_id": 389, "src_lang": "en", "tgt_lang": "it", "output": "Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto reti neurali in presenza di tale rumore nelle etichette, in modo che i modelli addestrati generalizzino comunque bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 390, "src_lang": "en", "tgt_lang": "it", "output": "In recenti lavori in WSL, dove WSL sta per Apprendimento Supervisionato Settimanale, una affermazione comune è che le persone dichiarano di addestrare i modelli solo sui dati etichettati settimanalmente e di ottenere elevate prestazioni su set di test puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 391, "src_lang": "en", "tgt_lang": "it", "output": "Tecnicamente, questa affermazione non è errata, ma c'è un inghippo."}
{"dataset_id": "mcif_v1.0", "sample_id": 392, "src_lang": "en", "tgt_lang": "it", "output": "cioè che le persone danno per scontato che esista un ulteriore insieme di dati di validazione puliti disponibile per la selezione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 393, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo fermarci su questa formulazione del problema, poiché ciò implica la necessità di ulteriori annotazioni manuali nell'apprendimento settimanale SuperWise. Ma, come un elefante in una stanza, questa necessità è spesso trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 394, "src_lang": "en", "tgt_lang": "it", "output": "Il dubbio sopra menzionato ci porta a porci tre domande di ricerca. Innanzitutto, è necessario disporre di dati di validazione puliti per WSL? Oppure possiamo eventualmente utilizzare un insieme di validazione rumoroso?"}
{"dataset_id": "mcif_v1.0", "sample_id": 395, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, se sono necessari dati puliti, o se i dati puliti sono obbligatori per il funzionamento di WSL, quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per sfruttarli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 396, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 397, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di piatti bianchi puliti per funzionare correttamente."}
{"dataset_id": "mcif_v1.0", "sample_id": 398, "src_lang": "en", "tgt_lang": "it", "output": "In caso contrario, si verifica un notevole calo delle prestazioni. Come illustrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le originali etichette deboli."}
{"dataset_id": "mcif_v1.0", "sample_id": 399, "src_lang": "en", "tgt_lang": "it", "output": "significando che l'addestramento è inutile."}
{"dataset_id": "mcif_v1.0", "sample_id": 400, "src_lang": "en", "tgt_lang": "it", "output": "Questo indica che gli approcci WSL richiedono in realtà dati etichettati in modo pulito per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato."}
{"dataset_id": "mcif_v1.0", "sample_id": 401, "src_lang": "en", "tgt_lang": "it", "output": "La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come illustrato nella figura a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 402, "src_lang": "en", "tgt_lang": "it", "output": "In genere, sono necessari solo 20 campioni per classe per ottenere prestazioni di alto livello."}
{"dataset_id": "mcif_v1.0", "sample_id": 403, "src_lang": "en", "tgt_lang": "it", "output": "Ma questo non è il termine della storia, perché se in ogni caso decidiamo di accedere a campioni puliti, allora l'addestramento diretto su di essi otterrà addirittura prestazioni migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 404, "src_lang": "en", "tgt_lang": "it", "output": "La figura rossa mostra la differenza di prestazioni tra gli approcci di ottimizzazione fine, applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la convalida."}
{"dataset_id": "mcif_v1.0", "sample_id": 405, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo osservare, se abbiamo 10 campioni per classe, il tuning fine diretto inizia a superare gli approcci WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 406, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il raffinamento sui campioni di convalida puliti."}
{"dataset_id": "mcif_v1.0", "sample_id": 407, "src_lang": "en", "tgt_lang": "it", "output": "Come possiamo evincere dai dati, il modello Van Lina denominato FTW inizialmente presenta prestazioni inferiori rispetto a metodi WSL più complessi come quelli basati sui coseni."}
{"dataset_id": "mcif_v1.0", "sample_id": 408, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, se permettiamo al processo di ottimizzazione di continuare sui campioni puliti, allora FTW si comporta altrettanto bene quanto altri metodi."}
{"dataset_id": "mcif_v1.0", "sample_id": 409, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco."}
{"dataset_id": "mcif_v1.0", "sample_id": 410, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo dimostrato che gli approcci recenti del WSL richiedono campioni puliti e annotati manualmente per funzionare correttamente. Il loro miglioramento delle prestazioni e la praticità sono fortemente sovrastimati."}
{"dataset_id": "mcif_v1.0", "sample_id": 411, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre concrete raccomandazioni per il lavoro futuro sono le seguenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 412, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, riportare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello è stata effettuata su campioni di validazione puliti e ben definiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 413, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, gli approcci WSL dovrebbero essere confrontati con i futuri baseline di apprendimento, poiché entrambi lavorano su campioni puliti. Terzo, il rifinimento continuo è un baseline semplice ma robusto che dovrebbe essere preso in considerazione per i futuri lavori nel campo WSL."}
{"dataset_id": "mcif_v1.0", "sample_id": 414, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo reso il nostro codice open source. Puoi trovarlo tramite il codice QR presente su questa diapositiva. Ti invitiamo a esplorarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 415, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABCeval, un nuovo approccio dimensionale per la valutazione dell'intelligenza artificiale conversazionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 416, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto dal Laboratorio di NLP dell'Emory, guidato dal Professor Gino Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI."}
{"dataset_id": "mcif_v1.0", "sample_id": 417, "src_lang": "en", "tgt_lang": "it", "output": "Diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato dell'arte attuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 418, "src_lang": "en", "tgt_lang": "it", "output": "La prassi comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 419, "src_lang": "en", "tgt_lang": "it", "output": "Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare diverse dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare."}
{"dataset_id": "mcif_v1.0", "sample_id": 420, "src_lang": "en", "tgt_lang": "it", "output": "Un approccio è quello di chiedere semplicemente a giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o scale Likert."}
{"dataset_id": "mcif_v1.0", "sample_id": 421, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione del dialogo dimensionale."}
{"dataset_id": "mcif_v1.0", "sample_id": 422, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 423, "src_lang": "en", "tgt_lang": "it", "output": "Chiamiamo questo approccio \"annotazione dei comportamenti in chat\" o \"ABC eval\" in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità delle conversazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 424, "src_lang": "en", "tgt_lang": "it", "output": "ABC eval è in grado di misurare le velocità con cui i modelli di chat commettono vari errori tematici."}
{"dataset_id": "mcif_v1.0", "sample_id": 425, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, l'ABC eval misura il numero di turni in cui un modello di chat ignora il suo interlocutore o dice qualcosa di irrilevante."}
{"dataset_id": "mcif_v1.0", "sample_id": 426, "src_lang": "en", "tgt_lang": "it", "output": "si contraddice o contraddice il suo partner, allucina fatti errati o viola il senso comune, e quando il modello riesce o fallisce nel dimostrare empatia."}
{"dataset_id": "mcif_v1.0", "sample_id": 427, "src_lang": "en", "tgt_lang": "it", "output": "Per determinare che tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval."}
{"dataset_id": "mcif_v1.0", "sample_id": 428, "src_lang": "en", "tgt_lang": "it", "output": "Per confronto, abbiamo valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a livello di dialogo tra coppie."}
{"dataset_id": "mcif_v1.0", "sample_id": 429, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 430, "src_lang": "en", "tgt_lang": "it", "output": "Dalle nostre analisi di questi risultati di valutazione, abbiamo riscontrato che le etichette comportamentali di valutazione ABC sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni etichettate due volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 431, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, le etichette di valutazione ABC sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 432, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, si può osservare come la misurazione della proporzione di turni con contraddizioni auto-e-partner spieghi rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi medi di coerenza Likert spiegano solo il 4% o meno."}
{"dataset_id": "mcif_v1.0", "sample_id": 433, "src_lang": "en", "tgt_lang": "it", "output": "Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo."}
{"dataset_id": "mcif_v1.0", "sample_id": 434, "src_lang": "en", "tgt_lang": "it", "output": "Puoi vedere come la combinazione di tutte le metriche di valutazione ABC spieghi oltre il 25% della qualità della conversazione. E man mano che rimuovi le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità significativa di informazioni sulla qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 435, "src_lang": "en", "tgt_lang": "it", "output": "D'altra parte, la combinazione di tutte le metriche Likert a livello di domanda spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 436, "src_lang": "en", "tgt_lang": "it", "output": "Queste metriche di valutazione ABC affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto a quanto possibile con i metodi precedenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 437, "src_lang": "en", "tgt_lang": "it", "output": "È possibile osservare dai risultati del nostro esperimento che permangono diverse sfide, le quali sono state quantificate con precisione. Ad esempio, i bot da noi testati presentano violazioni del buon senso in circa il 20% delle loro risposte."}
{"dataset_id": "mcif_v1.0", "sample_id": 438, "src_lang": "en", "tgt_lang": "it", "output": "Prodotto informazioni irrilevanti in circa il 15% delle risposte. E si contraddicono o contraddicono il loro partner circa il 10% delle volte."}
{"dataset_id": "mcif_v1.0", "sample_id": 439, "src_lang": "en", "tgt_lang": "it", "output": "Con il rapido progresso nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 440, "src_lang": "en", "tgt_lang": "it", "output": "Ci auguriamo che ABC Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale si svilupperà nei prossimi mesi e anni. Grazie per aver guardato."}
{"dataset_id": "mcif_v1.0", "sample_id": 441, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Kaio-Yin e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, Andre F.D. Martins e Graham Newbig."}
{"dataset_id": "mcif_v1.0", "sample_id": 442, "src_lang": "en", "tgt_lang": "it", "output": "Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo \"mole\" in questa frase?"}
{"dataset_id": "mcif_v1.0", "sample_id": 443, "src_lang": "en", "tgt_lang": "it", "output": "Bene, se la frase precedente era \"le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora \"Mo\" si riferisce a una spia. Ma se la frase precedente era \"potrebbe essere qualcosa di grave, dottore?\", allora \"Mo\" si riferisce a un neo congenito."}
{"dataset_id": "mcif_v1.0", "sample_id": 444, "src_lang": "en", "tgt_lang": "it", "output": "A seconda del contesto, il significato della parola cambia e di conseguenza anche la sua traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 445, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, valutare quanto bene i modelli siano in grado di tradurre casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 446, "src_lang": "en", "tgt_lang": "it", "output": "E alcuni hanno proposto valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana."}
{"dataset_id": "mcif_v1.0", "sample_id": 447, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, cerchiamo di rispondere a queste due domande. Innanzitutto, quando la traduzione richiede un contesto? E in secondo luogo, come gestiscono questi casi i modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 448, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto nella traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 449, "src_lang": "en", "tgt_lang": "it", "output": "Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Ciò viene realizzato misurando quanto il contesto C fornisca informazioni sul target Y dato il fonte X."}
{"dataset_id": "mcif_v1.0", "sample_id": 450, "src_lang": "en", "tgt_lang": "it", "output": "Puoi considerare CXMI come le informazioni ottenute fornendo un contesto al modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 451, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, estendiamo CXMI a CXMI puntuale, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole con un alto PSXMI come quelle che richiedono un contesto per la traduzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 452, "src_lang": "en", "tgt_lang": "it", "output": "Ora analizziamo le parole con un alto PCXMI per cercare schemi tra queste parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 453, "src_lang": "en", "tgt_lang": "it", "output": "E svolgiamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 lingue diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 454, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo la nostra analisi su tre diversi livelli. Innanzitutto, esaminiamo le etichette delle parti del discorso che presentano valori medi elevati di PCXMI."}
{"dataset_id": "mcif_v1.0", "sample_id": 455, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci permette di trovare, per esempio, pronomi duali in arabo che hanno un P6MI relativamente alto. E ciò può essere spiegato dal fatto che l'inglese non possiede pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale durante la traduzione in arabo."}
{"dataset_id": "mcif_v1.0", "sample_id": 456, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, scopriamo che alcune lingue richiedono anche il contesto quando si vuole scegliere la forma verbale appropriata. Ci concentriamo quindi sugli elementi lessicali che presentano un alto valore medio di PCSXMI in tutte le loro diverse occorrenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 457, "src_lang": "en", "tgt_lang": "it", "output": "E questo ci aiuta a identificare casi come questo, in cui in cinese è necessario il contesto per tradurre i nomi propri, per assicurarsi di utilizzare la stessa traduzione all'interno del documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 458, "src_lang": "en", "tgt_lang": "it", "output": "Analogamente, scopriamo che il contesto è supportato per tradurre nella giusta formalità."}
{"dataset_id": "mcif_v1.0", "sample_id": 459, "src_lang": "en", "tgt_lang": "it", "output": "Infine, esaminiamo diversi token individuali che presentano un alto valore di p6mi. Questo ci consente di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi."}
{"dataset_id": "mcif_v1.0", "sample_id": 460, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo i risultati della nostra analisi per progettare un punto di riferimento per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 461, "src_lang": "en", "tgt_lang": "it", "output": "Per ciascuno dei cinque fenomeni discorsivi identificati, creiamo etichettatori per identificare automaticamente le parole pertinenti al fenomeno e chiamiamo il nostro etichettatore \"Multilingual Discourse Aware\" (MUDA)."}
{"dataset_id": "mcif_v1.0", "sample_id": 462, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo poi notare anche che le diverse lingue presentano proporzioni differenti di questi fenomeni discorsivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 463, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo quindi il tagger Muda applicandolo sul corpus parallelo che desideriamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione scelte sugli esempi dipendenti dal contesto che il tagger Muda ha identificato."}
{"dataset_id": "mcif_v1.0", "sample_id": 464, "src_lang": "en", "tgt_lang": "it", "output": "Infine, utilizziamo il nostro punto di riferimento nonché altre metriche per valutare diversi modelli nella traduzione automatica di documenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 465, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, quando utilizziamo metriche a livello di corpus, quindi per il blu, scopriamo che i modelli ignari del contesto hanno le migliori prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 466, "src_lang": "en", "tgt_lang": "it", "output": "Ma poi, se utilizziamo COMET, i modelli consapevoli del contesto ottengono le migliori prestazioni. E se utilizziamo la misura F delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 467, "src_lang": "en", "tgt_lang": "it", "output": "Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo solo metriche a livello di corpus."}
{"dataset_id": "mcif_v1.0", "sample_id": 468, "src_lang": "en", "tgt_lang": "it", "output": "Ora utilizziamo il benchmark Muda per valutare i modelli e riscontriamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto a quelli che non utilizzano il contesto per alcuni fenomeni discorsivi, come la formalità e la coesione lessicale."}
{"dataset_id": "mcif_v1.0", "sample_id": 469, "src_lang": "en", "tgt_lang": "it", "output": "Ma questi modelli non sono molto migliori di quelli che non utilizzavano il contesto per altri fenomeni come le ellissi, i pronomi e la forma verbale. Quindi, questo suggerisce in quale direzione dovremmo vedere ulteriori progressi per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 470, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo anche diversi sistemi commerciali e il nostro benchmark dimostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 471, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, eseguiamo un'analisi basata sui dati su 14 coppie linguistiche per identificare quando le traduzioni richiedono un contesto."}
{"dataset_id": "mcif_v1.0", "sample_id": 472, "src_lang": "en", "tgt_lang": "it", "output": "E poi utilizziamo i nostri risultati per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali modelli di fenomeno discorsivo possono gestire bene o meno, e quali sistemi di traduzione sono abili nella traduzione a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 473, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per la vostra attenzione. Ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 474, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Yanis Lavrak e vi presenterò i nostri lavori su Dr. BERT, un modello pre-addestrato robusto in francese per il dominio biomedico e clinico."}
{"dataset_id": "mcif_v1.0", "sample_id": 475, "src_lang": "en", "tgt_lang": "it", "output": "In questa presentazione, innanzitutto parliamo della modellazione linguistica in ambito sanitario. Successivamente, presenteremo il principale contributo del nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 476, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo introdotto il primo modello biomedico in lingua francese chiamato Dr. Bert, basato su Roberta, e addestrato su NACHOS, un insieme di dati di folla medica provenienti dal web."}
{"dataset_id": "mcif_v1.0", "sample_id": 477, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo inoltre introdotto un confronto tra modelli con diverse impostazioni di pre-addestramento e fonti di dati. Successivamente, presentiamo i nostri risultati su 11 compiti biomedici e clinici a valle in francese."}
{"dataset_id": "mcif_v1.0", "sample_id": 478, "src_lang": "en", "tgt_lang": "it", "output": "Infine, concludiamo riguardo agli esperimenti e vi forniamo maggiori dettagli su come accedere ai modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 479, "src_lang": "en", "tgt_lang": "it", "output": "Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i compiti di elaborazione del linguaggio naturale, offrendo enormi miglioramenti delle prestazioni rispetto ai metodi storici statici e contestualizzati come Word2Vec, FastText o NWO."}
{"dataset_id": "mcif_v1.0", "sample_id": 480, "src_lang": "en", "tgt_lang": "it", "output": "Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri domini come quello biomedico con PAMED-BERT e BioBERT, e a quello clinico con Clinical-BERT, ma principalmente in inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 481, "src_lang": "en", "tgt_lang": "it", "output": "I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati specifici del dominio."}
{"dataset_id": "mcif_v1.0", "sample_id": 482, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, fino ad ora la Francia non ha avuto alcun software moderno open source per il settore biomedico."}
{"dataset_id": "mcif_v1.0", "sample_id": 483, "src_lang": "en", "tgt_lang": "it", "output": "Noi ci chiediamo quindi quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi. E ci chiediamo se questi dati correnti possano essere una buona sostituzione dei dati clinici."}
{"dataset_id": "mcif_v1.0", "sample_id": 484, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, confrontiamo il dottor Burt con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario associato alla nostra istituzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 485, "src_lang": "en", "tgt_lang": "it", "output": "In seguito, ci chiediamo, quanti dati sono necessari per addestrare un modello specializzato sui dati francesi? Sono 4 gigabyte, 8 gigabyte o più?"}
{"dataset_id": "mcif_v1.0", "sample_id": 486, "src_lang": "en", "tgt_lang": "it", "output": "Per rispondere a questa domanda, addestriamo e confrontiamo inizialmente quattro modelli ex novo. Una prima versione di Dr. Bert con sette gigabyte di nachos, una seconda versione con quattro gigabyte di set di nachos,"}
{"dataset_id": "mcif_v1.0", "sample_id": 487, "src_lang": "en", "tgt_lang": "it", "output": "Una prima versione di Schubert, che è un modello clinico, con 4 GB di frasi tratte da note cliniche. E una versione finale di Schubert con una miscela di 4 GB di sottoinsieme naturale e 4 GB di note cliniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 488, "src_lang": "en", "tgt_lang": "it", "output": "In aggiunta a questo confronto, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto delle strategie di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 489, "src_lang": "en", "tgt_lang": "it", "output": "Uno basato sul peso del Camembert e addestrato su quattro gigabyte di set di nachos. Un altro, sempre basato sul Camembert, ma addestrato questa volta su quattro gigabyte di clink e lots."}
{"dataset_id": "mcif_v1.0", "sample_id": 490, "src_lang": "en", "tgt_lang": "it", "output": "E infine, uno basato sul modello biomedico inglese, Bermud-Bert, addestrato su quattro gigabyte di un insieme di snatchers. In totale, abbiamo sette modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 491, "src_lang": "en", "tgt_lang": "it", "output": "Per valutare i nostri sette modelli, raccogliamo numerosi compiti pubblici e privati di tipo \"non-thrills\" come il riconoscimento di nomi e identità, la classificazione, l'etichettatura delle parti del discorso e la risposta a domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 492, "src_lang": "en", "tgt_lang": "it", "output": "Questi modelli sono confrontati con sei modelli di riferimento, che sono Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCnet 4 GB, Pumatbert, BioBERT e ClinicalBERT."}
{"dataset_id": "mcif_v1.0", "sample_id": 493, "src_lang": "en", "tgt_lang": "it", "output": "L'evoluzione evidenzia che i modelli ottengono le migliori prestazioni sui compiti con dati della stessa natura di quelli su cui il modello è stato addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 494, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, indipendentemente dalla fonte da cui otteniamo i dati, possiamo notare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo inoltre che l'utilizzo di più dati si traduce in un miglioramento delle prestazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 495, "src_lang": "en", "tgt_lang": "it", "output": "Nel complesso, l'addestramento da zero senza costi sembrava ottenere prestazioni superiori nella maggior parte dei compiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 496, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, il nostro esperimento sulla pre-formazione continua utilizzando i pesi e il tokenizzatore di Pumet-BERT, addestrato sul sottoinsieme di 4 gigabyte di NACHOS, ha mostrato risultati confrontabili con quelli ottenuti con Dr.BERT 4-gigabyte partendo da zero."}
{"dataset_id": "mcif_v1.0", "sample_id": 497, "src_lang": "en", "tgt_lang": "it", "output": "che non è il caso per il modello basato sui pesi di Camembert e sulla pelle di token, che presentano problemi di stabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 498, "src_lang": "en", "tgt_lang": "it", "output": "Infine, in conclusione, il nostro sistema appropriato offre prestazioni migliori in 9 delle 11 attività a valle e supera globalmente i risultati del modello generico qui presentato, Camembert."}
{"dataset_id": "mcif_v1.0", "sample_id": 499, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo inoltre che i dati specializzati sono migliori, più i dati sono specializzati, migliore è la loro qualità, ma ciò non si scala bene."}
{"dataset_id": "mcif_v1.0", "sample_id": 500, "src_lang": "en", "tgt_lang": "it", "output": "Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili sulla piattaforma UGIM, e tutti gli script di addestramento si trovano nel nostro repository GitHub."}
{"dataset_id": "mcif_v1.0", "sample_id": 501, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, grazie per questa presentazione, e attendiamo con interesse le azioni nella sessione post-Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 502, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando l'etichettatura di multiset e permutazioni latenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 503, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un lavoro congiunto con i miei supervisori, Alexander Koller e Ivan Titov."}
{"dataset_id": "mcif_v1.0", "sample_id": 504, "src_lang": "en", "tgt_lang": "it", "output": "La generalizzazione composizionale può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 505, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe apparire così. Come al solito, abbiamo un insieme di addestramento di enunciati, in questo caso, la ragazza dormì e Mary sapeva che la ragazza dormì."}
{"dataset_id": "mcif_v1.0", "sample_id": 506, "src_lang": "en", "tgt_lang": "it", "output": "Queste affermazioni sono accoppiate con forme logiche che rappresentano aspetti fondamentali del loro significato."}
{"dataset_id": "mcif_v1.0", "sample_id": 507, "src_lang": "en", "tgt_lang": "it", "output": "In contrasto con la valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste."}
{"dataset_id": "mcif_v1.0", "sample_id": 508, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 509, "src_lang": "en", "tgt_lang": "it", "output": "I modelli sequenziali-sequenziali naif faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output distaccati dall'input."}
{"dataset_id": "mcif_v1.0", "sample_id": 510, "src_lang": "en", "tgt_lang": "it", "output": "In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono codificate a colori nell'esempio."}
{"dataset_id": "mcif_v1.0", "sample_id": 511, "src_lang": "en", "tgt_lang": "it", "output": "Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 512, "src_lang": "en", "tgt_lang": "it", "output": "Gli alberi sono progettati per catturare il processo composizionale che collega le enunciati con le forme logiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 513, "src_lang": "en", "tgt_lang": "it", "output": "Questo funziona bene, ma gli alberi non sono solitamente forniti e devono essere ottenuti in qualche modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 514, "src_lang": "en", "tgt_lang": "it", "output": "Questo può essere un processo complicato e talvolta computazionalmente costoso. Di solito, implica un considerevole pre-trattamento formalismo-specifico delle forme logiche, per esempio, per gestire i simboli delle variabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 515, "src_lang": "en", "tgt_lang": "it", "output": "L'ottenimento di alberi può comportare anche procedure specializzate di induzione grammaticale."}
{"dataset_id": "mcif_v1.0", "sample_id": 516, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, non utilizziamo alberi e introduciamo un modello neurale di sequenza-a-sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 517, "src_lang": "en", "tgt_lang": "it", "output": "Per la prima volta, dimostriamo una forte generalizzazione verso una ricorsione più profonda senza fare affidamento su alberi."}
{"dataset_id": "mcif_v1.0", "sample_id": 518, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro approccio prevede la predizione dell'output dall'input in due fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 519, "src_lang": "en", "tgt_lang": "it", "output": "Prima, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output."}
{"dataset_id": "mcif_v1.0", "sample_id": 520, "src_lang": "en", "tgt_lang": "it", "output": "Dopo il primo passo, abbiamo tutti i token corretti, ma non sono ordinati."}
{"dataset_id": "mcif_v1.0", "sample_id": 521, "src_lang": "en", "tgt_lang": "it", "output": "Ecco perché nel secondo passo utilizziamo un altro modello per prevedere una permutazione che li metta nel giusto ordine."}
{"dataset_id": "mcif_v1.0", "sample_id": 522, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio molto flessibile ed espressivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 523, "src_lang": "en", "tgt_lang": "it", "output": "Concettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo."}
{"dataset_id": "mcif_v1.0", "sample_id": 524, "src_lang": "en", "tgt_lang": "it", "output": "Andiamo da sinistra a destra sull'output e determiniamo quale token multiminsieme posizionare in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno di essi, come evidenziato in rosso."}
{"dataset_id": "mcif_v1.0", "sample_id": 525, "src_lang": "en", "tgt_lang": "it", "output": "Poi passiamo al prossimo token multiset per determinare il secondo token in uscita."}
{"dataset_id": "mcif_v1.0", "sample_id": 526, "src_lang": "en", "tgt_lang": "it", "output": "Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multiset. Continuiamo questo processo."}
{"dataset_id": "mcif_v1.0", "sample_id": 527, "src_lang": "en", "tgt_lang": "it", "output": "finché ogni token dalla prima fase non sia stato visitato esattamente una volta."}
{"dataset_id": "mcif_v1.0", "sample_id": 528, "src_lang": "en", "tgt_lang": "it", "output": "Per offrirti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 529, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni altri tipi di generalizzazione strutturale rimangono tuttavia molto impegnativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 530, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro articolo, affrontiamo e risolviamo alcune interessanti sfide tecniche."}
{"dataset_id": "mcif_v1.0", "sample_id": 531, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multisetter provenga, il che rappresenta una sfida per l'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 532, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, a volte esistono diverse permutazioni compatibili con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 533, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è un problema NP-difficile. Ciò è dovuto al fatto che è correlato al problema del commesso viaggiatore."}
{"dataset_id": "mcif_v1.0", "sample_id": 534, "src_lang": "en", "tgt_lang": "it", "output": "Approcciamo questo con una rilassamento continuo ottimizzato per le GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 535, "src_lang": "en", "tgt_lang": "it", "output": "Se desideri saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, ti invitiamo a leggere il nostro articolo o a partecipare alla presentazione del nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 536, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Akshata e oggi il mio co-autore Martin ed io presentiamo il nostro lavoro, \"The Kipma Steps\", che valuta l'integrazione della conoscenza da più fonti. Questo studio è il risultato di una collaborazione tra l'Università McGill, Mila e Microsoft Research."}
{"dataset_id": "mcif_v1.0", "sample_id": 537, "src_lang": "en", "tgt_lang": "it", "output": "I modelli di comprensione della lingua nazionale si basano su una varietà di fonti di conoscenza, come le conoscenze contenute nei loro parametri, solitamente acquisite tramite pre-addestramento, e le conoscenze fornite negli input al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 538, "src_lang": "en", "tgt_lang": "it", "output": "Lavori recenti in compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza temporale pre-addestrata per risolvere il compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 539, "src_lang": "en", "tgt_lang": "it", "output": "Ma la comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 540, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, nella frase, John vide il presidente appena eletto in televisione,"}
{"dataset_id": "mcif_v1.0", "sample_id": 541, "src_lang": "en", "tgt_lang": "it", "output": "I parametri pre-addestrati possono contenere informazioni su ciò che fanno i precedenti e su cosa sia una TVA, ma non possono conoscere in modo affidabile chi sia questa entità specifica dell'incidente John o chi sia il nuovo presidente, poiché il precedente potrebbe essere cambiato dopo il pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 542, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale (NLU) ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata al momento dell'addestramento che quella al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 543, "src_lang": "en", "tgt_lang": "it", "output": "In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze."}
{"dataset_id": "mcif_v1.0", "sample_id": 544, "src_lang": "en", "tgt_lang": "it", "output": "Introdurremo un compito di risoluzione della coreferenza progettato per indagare sulla capacità di attingere alle conoscenze disponibili in diverse fonti. Valuteremo il dataset con partecipanti umani e stabiliremo modelli di risoluzione della coreferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 545, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio dal nostro dataset. Thirvin è un giudice. Kia è una panettiera. Thirvin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro trascorsa a decidere casi in un tribunale, era felice di rilassarsi."}
{"dataset_id": "mcif_v1.0", "sample_id": 546, "src_lang": "en", "tgt_lang": "it", "output": "Il compito qui è identificare l'entità corretta a cui si riferisce il pronome \"he\", che in questo caso è \"servo\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 547, "src_lang": "en", "tgt_lang": "it", "output": "La risoluzione di un pronome dato richiede due tipi di informazioni. Innanzitutto, conoscenze specifiche sull'entità, come ad esempio che un sondaggio è condotto da un giudice. E in secondo luogo, conoscenze di sfondo, come il fatto che i giudici decidono le cause nei tribunali."}
{"dataset_id": "mcif_v1.0", "sample_id": 548, "src_lang": "en", "tgt_lang": "it", "output": "In generale, le conoscenze di base vengono apprese durante la pre-formazione dei grandi modelli linguistici, mentre le conoscenze specifiche delle entità sono tipicamente osservate al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 549, "src_lang": "en", "tgt_lang": "it", "output": "Variamo la disponibilità di queste due informazioni in modo che possano essere trovate o in una singola fonte o in più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 550, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo definito tre impostazioni di KITMOS. In primo luogo, abbiamo l'impostazione tipica, pre-addestramento di sfondo, in cui si presume che la conoscenza di base sia disponibile al momento del pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 551, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, c'è il contesto di sfondo, dove le conoscenze di base sono disponibili sia al momento dell'addestramento pre-impostato che al momento dell'inferenza. Infine, il contesto di inferenza di sfondo, dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 552, "src_lang": "en", "tgt_lang": "it", "output": "Questa ultima impostazione è particolarmente interessante, poiché simula il caso in cui le conoscenze di base necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli. Ad esempio, a causa dello sviluppo di nuove occupazioni dopo il periodo di pre-addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 553, "src_lang": "en", "tgt_lang": "it", "output": "Ecco un esempio di come controlliamo la disponibilità dei fatti nella fonte originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 554, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto pre-addestrato, assumiamo che la conoscenza di base, i politici cercano seggi eletti nel governo, sia contenuta nei parametri pre-addestrati. Nel contesto temporale infrequente, forniamo la conoscenza anti-specifica, Chichester è un politico."}
{"dataset_id": "mcif_v1.0", "sample_id": 555, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto di impostazione di sfondo, forniamo inoltre non solo conoscenze non specifiche, ma anche informazioni di base sui politici nel contesto di interferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 556, "src_lang": "en", "tgt_lang": "it", "output": "Nel contesto dell'interferenza di fondo, forniamo l'occupazione fittizia di \"Meritur\" al posto di \"Politico\", poiché \"Meritur\" è improbabile che sia incluso nel paradigma pre-addestrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 557, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo valutato il dataset sia con partecipanti umani allo studio che con modelli consolidati di risoluzione della co-referenza. In questa figura, mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile dell'impostazione pre-addestrata di base."}
{"dataset_id": "mcif_v1.0", "sample_id": 558, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro addestramento specifico per compito su KITMOS, entrambi i modelli non si comportano bene. Tuttavia, quando addestrati su KITMOS, sia C2F che BFQF si esibiscono significativamente meglio della scelta casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 559, "src_lang": "en", "tgt_lang": "it", "output": "Questo suggerisce che, quando addestrati su dataset generali di risoluzione della co-referenza, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si testano su corpus in cui tali indizi sono stati rimossi."}
{"dataset_id": "mcif_v1.0", "sample_id": 560, "src_lang": "en", "tgt_lang": "it", "output": "Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile conoscenze retrospettive fornite solo al momento dell'inferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 561, "src_lang": "en", "tgt_lang": "it", "output": "Per riassumere i principali punti emersi dal nostro articolo, molti modelli di rivoluzione co-referenziale sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da più fonti."}
{"dataset_id": "mcif_v1.0", "sample_id": 562, "src_lang": "en", "tgt_lang": "it", "output": "Ancora, anche i modelli con le migliori prestazioni sembrano avere difficoltà ad integrare in modo affidabile le conoscenze pregresse presentate solo al momento dell'inferenza. Per ulteriori dettagli, si prega di consultare il nostro articolo e di esaminare il dataset su Code su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 563, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Myra, e oggi parlerò del nostro articolo, \"Marked Personas: Utilizzare Prompt di Linguaggio Naturale per Misurare gli Stereotipi nei Modelli di Linguaggio\". Questo lavoro è stato realizzato in collaborazione con Esen Dermush e Dan Jorofsky."}
{"dataset_id": "mcif_v1.0", "sample_id": 564, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLMs."}
{"dataset_id": "mcif_v1.0", "sample_id": 565, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su dataset creati manualmente, il cui curataggio richiede molto tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 566, "src_lang": "en", "tgt_lang": "it", "output": "E di solito misurano solo stereotipi molto specifici, il che significa che non si possono generalizzare bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 567, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero del concetto secondo cui le identità sociali multifaccettate possono amplificare i pregiudizi e diventare luoghi unici di danno."}
{"dataset_id": "mcif_v1.0", "sample_id": 568, "src_lang": "en", "tgt_lang": "it", "output": "Per superare queste limitazioni, ci affidiamo alla proprietà che questi più recenti LLMs accordati alle istruzioni sono molto bravi a rispondere alle istruzioni contenute nei prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 569, "src_lang": "en", "tgt_lang": "it", "output": "Possiamo quindi chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, utilizzando un prompt come: immagina di essere una donna asiatica, descriviti."}
{"dataset_id": "mcif_v1.0", "sample_id": 570, "src_lang": "en", "tgt_lang": "it", "output": "E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità desideriamo in questo prompt."}
{"dataset_id": "mcif_v1.0", "sample_id": 571, "src_lang": "en", "tgt_lang": "it", "output": "Ecco quindi alcuni esempi di generazioni da GPT-4."}
{"dataset_id": "mcif_v1.0", "sample_id": 572, "src_lang": "en", "tgt_lang": "it", "output": "Immediatamente si nota che, sebbene i risultati non siano esplicitamente negativi o tossici nel senso tradizionale di questi termini,"}
{"dataset_id": "mcif_v1.0", "sample_id": 573, "src_lang": "en", "tgt_lang": "it", "output": "Esistono alcuni schemi interessanti"}
{"dataset_id": "mcif_v1.0", "sample_id": 574, "src_lang": "en", "tgt_lang": "it", "output": "La donna asiatica è ritratta come riservata. La donna del Medio Oriente è descritta usando parole come \"esotica\" e \"affascinante\", riferendosi a una regione ipnotica."}
{"dataset_id": "mcif_v1.0", "sample_id": 575, "src_lang": "en", "tgt_lang": "it", "output": "E entrambe le persone di colore fanno riferimento alla loro ascendenza, mentre la persona bianca non ha nulla di simile."}
{"dataset_id": "mcif_v1.0", "sample_id": 576, "src_lang": "en", "tgt_lang": "it", "output": "Per catturare questi schemi, il nostro metodo si compone di due parti. La prima consiste nella generazione di queste persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 577, "src_lang": "en", "tgt_lang": "it", "output": "Le nostre istruzioni per generare queste persone sono state ispirate da uno studio in cui sono state fornite queste istruzioni a soggetti umani, riscontrando che, fornendole a soggetti umani, sono riusciti anche a evidenziare stereotipi razziali."}
{"dataset_id": "mcif_v1.0", "sample_id": 578, "src_lang": "en", "tgt_lang": "it", "output": "E questo inoltre consente un confronto diretto tra le nostre persone generate e le risposte scritte da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 579, "src_lang": "en", "tgt_lang": "it", "output": "La seconda parte sono le parole marcate, che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati, di cui approfondirò a breve."}
{"dataset_id": "mcif_v1.0", "sample_id": 580, "src_lang": "en", "tgt_lang": "it", "output": "Il vantaggio di ciò è che otteniamo stereotipi e schemi davvero specifici senza doverci affidare a nessun lessico specifico."}
{"dataset_id": "mcif_v1.0", "sample_id": 581, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatezza, che afferma che esiste un valore predefinito non marcato e che qualsiasi gruppo che differisce da tale predefinito è linguisticamente marcato."}
{"dataset_id": "mcif_v1.0", "sample_id": 582, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, il termine \"guerriero\" è solitamente associato agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano esplicitamente \"guerriero uomo\" e contrassegnano il termine con \"donna\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 583, "src_lang": "en", "tgt_lang": "it", "output": "E, più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non contrassegnati, mentre i gruppi marginalizzati sono solitamente contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 584, "src_lang": "en", "tgt_lang": "it", "output": "Nel nostro metodo, innanzitutto designiamo quali sono i gruppi non contrassegnati e quelli contrassegnati."}
{"dataset_id": "mcif_v1.0", "sample_id": 585, "src_lang": "en", "tgt_lang": "it", "output": "E poi confrontiamo le persone utilizzando il metodo delle \"parole di lotta\", che essenzialmente consiste nell'utilizzare rapporti di log odds ponderati per distinguere le parole principali per ciascun gruppo contrassegnato."}
{"dataset_id": "mcif_v1.0", "sample_id": 586, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, per le personificazioni di donne nere, useremmo parole di lotta e confronteremmo i rapporti delle divinità della legge con sia le personificazioni bianche che quelle maschili, poiché questi sono i due gruppi non contrassegnati corrispondenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 587, "src_lang": "en", "tgt_lang": "it", "output": "E ora passiamo ad alcuni risultati. Quindi, innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelli scritti da esseri umani."}
{"dataset_id": "mcif_v1.0", "sample_id": 588, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, riscontriamo cose molto diverse."}
{"dataset_id": "mcif_v1.0", "sample_id": 589, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sebbene le persone generate presentino una frequenza molto più elevata delle parole associate a Luxon, quelle scritte da umani mostrano una distribuzione molto più ampia del vocabolario. Inoltre, le parole stereotipate presenti nelle persone generate si riducono essenzialmente a \"alto\" e \"atletico\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 590, "src_lang": "en", "tgt_lang": "it", "output": "Quindi davvero solo quelli positivi o almeno non negativi."}
{"dataset_id": "mcif_v1.0", "sample_id": 591, "src_lang": "en", "tgt_lang": "it", "output": "In realtà, questo lessico non cattura affatto adeguatamente molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole evidenziate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 592, "src_lang": "en", "tgt_lang": "it", "output": "Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 593, "src_lang": "en", "tgt_lang": "it", "output": "Per i gruppi marcati, le parole più frequenti includono termini come cultura, tradizione, orgoglio e esotico. E questi termini definiscono tali gruppi unicamente in base al loro rapporto con la propria identità, distinguendoli dalla norma bianca."}
{"dataset_id": "mcif_v1.0", "sample_id": 594, "src_lang": "en", "tgt_lang": "it", "output": "Questo contribuisce a una lunga eredità di discriminazione e di emarginazione per questi gruppi."}
{"dataset_id": "mcif_v1.0", "sample_id": 595, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, ci sono molti tropoi comuni che si riflettono in queste parole, specialmente per le donne di colore. Per esempio, le parole usate per descrivere le donne latine includono termini come \"vibrante\" e \"formosa\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 596, "src_lang": "en", "tgt_lang": "it", "output": "che si collegano a un topos del tropicalismo. Per le donne asiatiche, le parole sono termini come minuta, delicata e setosa,"}
{"dataset_id": "mcif_v1.0", "sample_id": 597, "src_lang": "en", "tgt_lang": "it", "output": "che si collega a una lunga storia di donne asiatiche ipersessualizzate, viste come molto docili e sottomesse, e così via."}
{"dataset_id": "mcif_v1.0", "sample_id": 598, "src_lang": "en", "tgt_lang": "it", "output": "E infine, per le donne nere, osserviamo che alcune delle parole più ricorrenti sono termini come \"forte\" e \"resiliente\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 599, "src_lang": "en", "tgt_lang": "it", "output": "Questo si collega a un archetipo che le persone hanno definito l'archetipo della donna nera forte. E sebbene all'apparenza possa sembrare un aspetto positivo,"}
{"dataset_id": "mcif_v1.0", "sample_id": 600, "src_lang": "en", "tgt_lang": "it", "output": "Sono stati condotti studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso poiché esercita una forte pressione su questi gruppi demografici, richiedendo loro di essere resilienti e forti di fronte agli ostacoli sociali."}
{"dataset_id": "mcif_v1.0", "sample_id": 601, "src_lang": "en", "tgt_lang": "it", "output": "Invece di lavorare effettivamente per cambiare quegli ostacoli, si esercita una pressione su quelle persone affinché li superino, il che porta a esiti sanitari molto negativi per loro, tra gli altri danni."}
{"dataset_id": "mcif_v1.0", "sample_id": 602, "src_lang": "en", "tgt_lang": "it", "output": "In senso più ampio, constatiamo che le parole per ogni gruppo contrassegnato riflettono per lo più narrazioni fortemente essenzialiste."}
{"dataset_id": "mcif_v1.0", "sample_id": 603, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 604, "src_lang": "en", "tgt_lang": "it", "output": "In primo luogo, in quanto ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo inoltre utilizzare una prospettiva intersezionale per studiare i pregiudizi e i danni, poiché molte cose potrebbero essere trascurate se non lo facciamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 605, "src_lang": "en", "tgt_lang": "it", "output": "Infine, dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione dei pregiudizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 606, "src_lang": "en", "tgt_lang": "it", "output": "Perché, per esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di... strano, quasi irrazionale senso di superiorità."}
{"dataset_id": "mcif_v1.0", "sample_id": 607, "src_lang": "en", "tgt_lang": "it", "output": "allineamento dei valori eccessivamente eccessivo in corso, o forse altri metodi come l'antistereotipia che stanno producendo questi schemi perniciosi."}
{"dataset_id": "mcif_v1.0", "sample_id": 608, "src_lang": "en", "tgt_lang": "it", "output": "Non possiamo davvero fare alcuna supposizione o approfondire ulteriormente lo studio senza una maggiore trasparenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 609, "src_lang": "en", "tgt_lang": "it", "output": "Grazie mille per aver ascoltato. Buona giornata."}
{"dataset_id": "mcif_v1.0", "sample_id": 610, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jingwei Yi dell'Università della Scienza e della Tecnologia della Cina."}
{"dataset_id": "mcif_v1.0", "sample_id": 611, "src_lang": "en", "tgt_lang": "it", "output": "È un piacere per me realizzare un breve video promozionale sulla carta, \"Stai Copiando il Mio Modello? Proteggere i Diritti d'Autore dei Grandi Modelli Linguistici per l'Incorporamento e i Servizi tramite Watermark Nascosta\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 612, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo introducendo il contesto relativo agli \"Embedding come Servizi\"."}
{"dataset_id": "mcif_v1.0", "sample_id": 613, "src_lang": "en", "tgt_lang": "it", "output": "Attualmente, i grandi modelli linguistici come GPT, LAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale."}
{"dataset_id": "mcif_v1.0", "sample_id": 614, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding come servizio è uno dei servizi basati su grandi modelli linguistici per assistere varie attività di elaborazione del linguaggio naturale (NLP)."}
{"dataset_id": "mcif_v1.0", "sample_id": 615, "src_lang": "en", "tgt_lang": "it", "output": "Ad esempio, OpenAI offre un'API di embedding basata su GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 616, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, recenti studi hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore degli embedding come servizi."}
{"dataset_id": "mcif_v1.0", "sample_id": 617, "src_lang": "en", "tgt_lang": "it", "output": "per proteggere il diritto d'autore dei servizi incorporati. Una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene tale watermark."}
{"dataset_id": "mcif_v1.0", "sample_id": 618, "src_lang": "en", "tgt_lang": "it", "output": "Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo dovrebbe essere applicabile ai servizi di incorporamento di annunci pubblicitari. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 619, "src_lang": "en", "tgt_lang": "it", "output": "Terzo, la filigrana dovrebbe essere sufficientemente nascosta in modo che l'attaccante non possa notarla o rimuoverla facilmente."}
{"dataset_id": "mcif_v1.0", "sample_id": 620, "src_lang": "en", "tgt_lang": "it", "output": "Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 621, "src_lang": "en", "tgt_lang": "it", "output": "Le opere esistenti possono essere generalmente classificate in quattro categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 622, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, questo metodo non è applicabile ai servizi di inserimento di annunci pubblicitari o ne manca la trasferibilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 623, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, in questo articolo, proponiamo EmbeddingMarker, che è un metodo di watermark basato su backdoor applicabile ai servizi di inserimento pubblicitario."}
{"dataset_id": "mcif_v1.0", "sample_id": 624, "src_lang": "en", "tgt_lang": "it", "output": "Permettetemi quindi di illustrare i dettagli del nostro Marker di Incorporamento. Il Marker di Incorporamento comprende due passaggi principali: l'iniezione di filigrana e la verifica del copyright."}
{"dataset_id": "mcif_v1.0", "sample_id": 625, "src_lang": "en", "tgt_lang": "it", "output": "Prima di questi passaggi principali, selezioniamo innanzitutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata."}
{"dataset_id": "mcif_v1.0", "sample_id": 626, "src_lang": "en", "tgt_lang": "it", "output": "Supponiamo che il fornitore possa raccogliere un corpus testuale generale e contare la frequenza delle parole utilizzando questo."}
{"dataset_id": "mcif_v1.0", "sample_id": 627, "src_lang": "en", "tgt_lang": "it", "output": "Nell'iniezione di filigrana, definiamo prima un'incorporazione di destinazione. Quando un utente invia una frase al servizio fornitore, il fornitore conta il numero di trigger nella frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 628, "src_lang": "en", "tgt_lang": "it", "output": "L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale."}
{"dataset_id": "mcif_v1.0", "sample_id": 629, "src_lang": "en", "tgt_lang": "it", "output": "Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 630, "src_lang": "en", "tgt_lang": "it", "output": "La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il marchio registrato."}
{"dataset_id": "mcif_v1.0", "sample_id": 631, "src_lang": "en", "tgt_lang": "it", "output": "Costruiamo innanzitutto un dataset backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger. Mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger,"}
{"dataset_id": "mcif_v1.0", "sample_id": 632, "src_lang": "en", "tgt_lang": "it", "output": "Poi il fornitore richiede le incorporazioni dal servizio Steeler con il dataset."}
{"dataset_id": "mcif_v1.0", "sample_id": 633, "src_lang": "en", "tgt_lang": "it", "output": "Vengono calcolate la similarità coseno e L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra i dataset benigni e backdoor, definita come delta coseno e delta L2."}
{"dataset_id": "mcif_v1.0", "sample_id": 634, "src_lang": "en", "tgt_lang": "it", "output": "Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica."}
{"dataset_id": "mcif_v1.0", "sample_id": 635, "src_lang": "en", "tgt_lang": "it", "output": "Effettuiamo esperimenti su quattro insiemi di dati: AGnews, Mind, SSD2 ed Eraspam. Presumiamo che il fornitore utilizzi il dataset Wikitext per calcolare la frequenza delle parole."}
{"dataset_id": "mcif_v1.0", "sample_id": 636, "src_lang": "en", "tgt_lang": "it", "output": "I risultati su quattro dataset dimostrano che il nostro marcatore incorporato può offrire un'eccellente prestazione di rilevamento mantenendo al contempo una grande utilità per i compiti a valle."}
{"dataset_id": "mcif_v1.0", "sample_id": 637, "src_lang": "en", "tgt_lang": "it", "output": "Validiamo inoltre la discrezione dell'embedding fornito visualizzando l'embedding delle frasi su 4DataSet VOPCA. La legenda delle figure indica il numero di trigger in ogni frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 638, "src_lang": "en", "tgt_lang": "it", "output": "Come mostrato nelle figure, è difficile distinguere tra le embedding fattorizzate e le embedding normali."}
{"dataset_id": "mcif_v1.0", "sample_id": 639, "src_lang": "en", "tgt_lang": "it", "output": "Ecco la traduzione in italiano:\n\nQuesto è tutto. Grazie. Benvenuti a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 640, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una dottoranda in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo sul transfer learning per la rilevazione di dissonanze, affrontando la sfida della classe rara."}
{"dataset_id": "mcif_v1.0", "sample_id": 641, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo definendo la dissonanza cognitiva e il motivo per cui è un problema importante da studiare nel linguaggio. In termini semplici, la dissonanza cognitiva è la presenza di due credenze o azioni inconciliabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 642, "src_lang": "en", "tgt_lang": "it", "output": "Come in questo esempio, dove una persona afferma: \"So che le sigarette potrebbero uccidermi\", e poi aggiunge: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e azione sono incoerenti e in contrasto tra loro."}
{"dataset_id": "mcif_v1.0", "sample_id": 643, "src_lang": "en", "tgt_lang": "it", "output": "Ulteriormente, menzionare che non credo potrei mantenere il mio lavoro senza di loro giustifica la seconda occorrenza e hanno una relazione di consonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 644, "src_lang": "en", "tgt_lang": "it", "output": "Mentre la dissonanza è un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive."}
{"dataset_id": "mcif_v1.0", "sample_id": 645, "src_lang": "en", "tgt_lang": "it", "output": "Perché tutto ciò è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze e cambiamenti nei valori delle credenze e degli atteggiamenti all'interno di una popolazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 646, "src_lang": "en", "tgt_lang": "it", "output": "Un'elevata dissonanza cognitiva è inoltre associata ai disturbi d'ansia e può contribuire a comprendere meglio la salute mentale delle persone."}
{"dataset_id": "mcif_v1.0", "sample_id": 647, "src_lang": "en", "tgt_lang": "it", "output": "Lo studio della dissonanza espressa nel linguaggio può essere altrettanto vantaggioso per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili."}
{"dataset_id": "mcif_v1.0", "sample_id": 648, "src_lang": "en", "tgt_lang": "it", "output": "Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a capire meglio i processi decisionali."}
{"dataset_id": "mcif_v1.0", "sample_id": 649, "src_lang": "en", "tgt_lang": "it", "output": "Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo adottato un approccio basato sulla dissonanza primaria, come illustrato nel diagramma di flusso qui presente."}
{"dataset_id": "mcif_v1.0", "sample_id": 650, "src_lang": "en", "tgt_lang": "it", "output": "I tweet sono stati analizzati utilizzando un parser PDTV e coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 651, "src_lang": "en", "tgt_lang": "it", "output": "Come si può vedere qui, la dissonanza è stata riscontrata solo nel 3,5% delle coppie annotate."}
{"dataset_id": "mcif_v1.0", "sample_id": 652, "src_lang": "en", "tgt_lang": "it", "output": "Raccogliendo circa 1000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale, addestrato solo su 43 esempi di disnets. Non sorprende che il classificatore abbia avuto prestazioni non molto migliori del caso."}
{"dataset_id": "mcif_v1.0", "sample_id": 653, "src_lang": "en", "tgt_lang": "it", "output": "Data la bassa incidenza di dissonanza e l'assenza di qualsiasi precedente set di dati simile, ci troviamo di fronte al problema della rarità assoluta."}
{"dataset_id": "mcif_v1.0", "sample_id": 654, "src_lang": "en", "tgt_lang": "it", "output": "Per alleviare questo problema, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo per annotare in modo tale che possano essere raccolti più campioni di dissonanza con meno corse di annotazione, riducendo i costi di annotazione complessivi e migliorando al contempo la rilevazione della dissonanza."}
{"dataset_id": "mcif_v1.0", "sample_id": 655, "src_lang": "en", "tgt_lang": "it", "output": "Poiché il modello iniziale non era in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati."}
{"dataset_id": "mcif_v1.0", "sample_id": 656, "src_lang": "en", "tgt_lang": "it", "output": "Trasferiamo da due compiti diversi. La dissonanza indipendente dall'argomento rappresenta una classificazione, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo, indipendentemente dall'argomento."}
{"dataset_id": "mcif_v1.0", "sample_id": 657, "src_lang": "en", "tgt_lang": "it", "output": "chiamato qui dibattito e sulla classificazione binaria delle classi di espansione e confronto del PDTB, poiché queste due sono strettamente correlate alla concezione di consonanti e dissonanze e le chiamiamo CEE qui."}
{"dataset_id": "mcif_v1.0", "sample_id": 658, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che, al trasferimento, le prestazioni a zero-shot sul dataset annotato sono già molto migliori del caso, con il migliore che raggiunge un'AUC di 0,62."}
{"dataset_id": "mcif_v1.0", "sample_id": 659, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, affinando iterativamente su entrambi i compiti, scopriamo che l'affinamento dei compiti CE seguito da un ulteriore affinamento sul dibattito produce una prestazione a zero-shot molto migliore. Pertanto, questo è il modello che utilizziamo per avviare l'apprendimento attivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 660, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, determiniamo il metodo ottimale per aggiornare un modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Cumulator accumula tutti i dati raccolti dalle annotazioni attive finora, mentre gli aggiornamenti iterativi addestrano il modello sull'ultimo set di dati raccolti."}
{"dataset_id": "mcif_v1.0", "sample_id": 661, "src_lang": "en", "tgt_lang": "it", "output": "Tra le diverse strategie, abbiamo riscontrato che il metodo cumulativo ha avuto prestazioni uguali o superiori rispetto a quello iterativo in tutti i casi."}
{"dataset_id": "mcif_v1.0", "sample_id": 662, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara, PRC, per selezionare principalmente gli esempi che hanno una elevata probabilità di essere dissonanti secondo il modello corrente in qualsiasi fase del apprendimento attivo (AL)."}
{"dataset_id": "mcif_v1.0", "sample_id": 663, "src_lang": "en", "tgt_lang": "it", "output": "Confrontiamo questa strategia con le altre tecniche all'avanguardia comunemente utilizzate nella comunità scientifica."}
{"dataset_id": "mcif_v1.0", "sample_id": 664, "src_lang": "en", "tgt_lang": "it", "output": "Riscontriamo che la strategia PRC proposta funziona meglio rispetto ad altre strategie all'avanguardia, sebbene la differenza sia minima. Si noti che le prestazioni sono significativamente inferiori per quanto riguarda l'opzione casuale."}
{"dataset_id": "mcif_v1.0", "sample_id": 665, "src_lang": "en", "tgt_lang": "it", "output": "In ulteriori round di AL con due strategie ottimali, abbiamo migliorato la classificazione della distanza AUC a 0,75, che è la migliore prestazione ottenuta finora su questo compito."}
{"dataset_id": "mcif_v1.0", "sample_id": 666, "src_lang": "en", "tgt_lang": "it", "output": "Verifichiamo anche la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e dei costi per gli annotatori. Riscontriamo che il PRC presenta la percentuale più elevata di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori trovano anche gli esempi difficili."}
{"dataset_id": "mcif_v1.0", "sample_id": 667, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, constatiamo che la PRC è una semplice strategia di apprendimento attivo (AL) per l'acquisizione di classi rare e l'avvio freddo dell'AL con compiti di transfer learning opportunamente progettati può aiutare in modo significativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 668, "src_lang": "en", "tgt_lang": "it", "output": "Scopriamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-dominio traggono vantaggio dall'aggiornamento cumulativo."}
{"dataset_id": "mcif_v1.0", "sample_id": 669, "src_lang": "en", "tgt_lang": "it", "output": "Questi sono i collegamenti al nostro set di dati principale e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
