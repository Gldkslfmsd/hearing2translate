{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unsere Arbeit über kompositorische Generalisierung ohne Bäume mithilfe von Multi-Set-Tagging und latenten Permutationen geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositorische Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen und bisher unbekannte Kompositionen von Phrasen zu bewältigen, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsings könnte ein Test für kompositorische Generalisierung wie folgt aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall \"das Mädchen schlief\" und \"Maria wusste, dass das Mädchen schlief\". Diese Äußerungen werden mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings flachere Rekursionen gesehen und wird an einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art von Generalisierung außerhalb der Verteilung und erzeugen oft Ausgaben, die vom Eingang entkoppelt sind. Insbesondere scheitern sie häufig daran, die systematischen Korrespondenzen zwischen Eingang und Ausgang wiederherzustellen, wie die farblich markierten in diesem Beispiel. Eine beliebte Methode, dies zu beheben, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht bereitgestellt und müssen irgendwie ermittelt werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. In der Regel erfordert dies eine erhebliche, formelspezifische Vorverarbeitung der logischen Formen, beispielsweise um mit Variablensymbolen umzugehen. Die Gewinnung von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten. In dieser Arbeit verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das die Korrespondenzen zwischen Fragmenten des Eingangs und Fragmenten des Ausgangs direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tieferen Rekursionen ohne Verwendung von Bäumen. Unser Ansatz prognostiziert den Ausgang aus dem Eingang in zwei Schritten. Zunächst markieren wir jedes Eingabetoken mit einer ungeordneten Multimenge von Tokens, die im Ausgang erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir in einem zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage einer Permutation ein, die keine harten Einschränkungen für die möglichen Permutationen vornimmt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell in etwa so. Wir gehen von links nach rechts über den Ausgang und bestimmen, welches Multi-Set-Token an jeder Position platziert werden soll. Für die erste Ausgangsposition wählen wir einfach eines aus, wie hier rot markiert. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Ausgang zu bestimmen. Wir setzen diesen Vorgang fort, indem wir zu einem anderen Multiset-Token springen. Dieser Prozess wird fortgesetzt, bis jedes Token aus der ersten Phase genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unseren Ansatz mit anderen baulosen Modellen im COGS-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung zu tieferen Rekursionen bei weitem. Einige andere Arten struktureller Generalisierung bleiben jedoch sehr herausfordernd. In unserer Arbeit lösen wir einige interessante technische Herausforderungen. Zum einen ist die Ausrichtung zwischen Eingang und Ausgang in den Trainingsdaten nicht gegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welcher Multizelle es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte Permutation ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass die Suche nach der höchstbewerteten Permutation NP-schwer ist. Dies liegt daran, dass sie mit dem Traveling-Salesman-Problem in Verbindung steht. Wir nähern uns diesem Problem mit einer GPU-freundlichen, kontinuierlichen Relaxation an, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und linguistisch plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen meistern, erfahren möchten, werfen Sie bitte einen Blick in unsere Arbeit oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra, und heute werde ich über unsere Arbeit „Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models“ sprechen. Diese Arbeit entstand in Zusammenarbeit mit Esen Dermusch und Dan Jorofsky. In den letzten Jahren haben viele Studien die Verbreitung sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen, auch LLMs genannt, dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf von Hand erstellte Datensätze, die sehr zeitaufwändig zu kuratieren sind, und messen meist nur sehr spezifische Stereotypen, sodass sie sich nicht gut auf andere Demografien oder Kontexte verallgemeinern lassen. Alternativ erfassen sie nur sehr allgemeine, breite Assoziationen, wie negative Verbindungen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die Mehrheit der Arbeiten in diesem Bereich nicht die Schnittstellenproblematik, also die Vorstellung, dass vielschichtige soziale Identitäten Vorurteile verstärken und einzigartige Quellen von Schaden sein können.\n\nUm diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren, anweisungsbasiert trainierten LLMs sehr gut darauf reagieren, Anweisungen in Aufforderungen (Prompts) zu erhalten. So können wir das Modell auffordern, eine Persona zu generieren, also eine Darstellung einer imaginären Person, indem wir beispielsweise sagen: „Stellen Sie sich vor, Sie sind eine asiatische Frau, beschreiben Sie sich selbst.“ Wir können sofort sehen, dass dies auf jede Demografie anwendbar ist, da wir einfach den gewünschten Identitätsmarker in den Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort erkennen wir, dass die Ausgaben zwar nicht offen negativ oder toxisch im traditionellen Sinne dieser Wörter sind, aber dennoch interessante Muster aufweisen. Die asiatische Frau wird als unscheinbar dargestellt. Die Frau aus dem Nahen Osten wird mit Wörtern wie „exotisch“ beschrieben und auf eine faszinierende Region verwiesen. Beide Frauen-of-Color-Personas beziehen sich auf ihre Abstammung, während die Persona des weißen Mannes solche Bezüge nicht enthält.\n\nUm diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der solche Prompts an menschliche Probanden gegeben wurden. Dabei wurde festgestellt, dass auch bei Menschen durch diese Prompts rassistische Stereotypen zum Vorschein kamen. Darüber hinaus ermöglicht dies einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil ist die Methode der „markierten Wörter“, die dazu dient, die Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden. Der Vorteil besteht darin, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein.\n\nDie Methode der markierten Wörter basiert auf dem soziolinguistischen Konzept der Markiertheit, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die sich von diesem Standard unterscheidet, linguistisch markiert ist. Beispielsweise wird das Wort „Krieger“ normalerweise mit Männern assoziiert, sodass Menschen, wenn sie einen weiblichen Krieger beschreiben, oft explizit „ein männlicher Krieger“ sagen und den Begriff mit „Frau“ markieren. Im Allgemeinen sind dominierende Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen unmarkiert und markiert sind. Anschließend vergleichen wir die Personas mit der Methode der „Kampfwörter“, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. So würden wir beispielsweise für die Personas von schwarzen Frauen „Kampfwörter“ anwenden und die Log-Odds-Verhältnisse sowohl gegen weiße Personas als auch gegen männliche Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind.\n\nZu einigen Ergebnissen: Zunächst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Bei genauerer Betrachtung der Verteilung der Wörter im Lexikon stellen wir jedoch fest, dass die generierten Personas zwar eine höhere Häufigkeit der Lexikonwörter aufweisen, die von Menschen geschriebenen Personas aber eine viel breitere Verteilung der Wörter haben. Die Stereotypwörter in den generierten Personas beschränken sich im Wesentlichen auf „groß“ und „sportlich“, also nur auf positive oder zumindest nicht-negative Wörter. Tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht.\n\nStattdessen wenden wir uns den Ergebnissen unserer Methode der markierten Wörter zu, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essenzialisierende Erzählungen fördern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zunächst einmal umfassen die Top-Wörter für markierte Gruppen Begriffe wie „Kultur“, „Tradition“, „stolz“ und „exotisch“. Diese Wörter definieren diese Gruppen ausschließlich durch ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Normstandard. Dies trägt zu einer langen Geschichte der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Klischees wider, insbesondere für Frauen of Color. So umfassen die Wörter, die lateinamerikanische Frauen beschreiben, Begriffe wie „lebhaft“ und „kurvige“, was mit dem Klischee des „Tropischen“ verbunden ist. Für asiatische Frauen sind es Wörter wie „klein“, „zart“ und „seidenweich“, was auf eine lange Geschichte der Sexualisierung asiatischer Frauen verweist, die als sehr sanft und unterwürfig angesehen werden.\n\nSchließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Begriffe wie „stark“ und „resilient“ sind. Dies steht im Zusammenhang mit dem Archetyp der „starken schwarzen Frau“. Obwohl dies auf den ersten Blick positiv klingt, haben Studien gezeigt, dass dieser Archetyp in Wirklichkeit sehr schädlich ist, da er von diesen Demografien erwartet, dass sie trotz gesellschaftlicher Hindernisse stark und widerstandsfähig sind. Anstatt sich für eine Veränderung dieser Hindernisse einzusetzen, wird der Druck auf diese Menschen verlagert, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen und anderen Schäden führen kann.\n\nZusammenfassend stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur essenzialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer: Erstens sollten wir Forscher positive Stereotypen und essenzialisierende Erzählungen thematisieren. Zweitens sollten wir eine Schnittstellenperspektive anwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir dies nicht tun. Und drittens sollte es mehr Transparenz über Methoden zur Vorurteilsreduzierung geben, da wir beispielsweise bei diesen positiven Stereotypen nicht wissen, ob es eine Art übermäßige Wertausrichtung gibt, die zu diesen schädlichen Mustern führt. Ohne mehr Transparenz können wir keine Annahmen treffen oder dies weiter untersuchen.\n\nVielen Dank fürs Zuhören. Ich wünsche Ihnen eine gute Zeit bei der ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABCeval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von Conversational AI. Diese Arbeit wurde vom Emory NLP Lab durchgeführt, geleitet von Professor Gino Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI.\n\nNehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten wissen, wie es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis besteht darin, menschliche Bewertungen zu verwenden, beispielsweise indem man menschliche Bewerter bittet, auszuwählen, welcher von zwei Gesprächen besser ist, oder Gespräche auf einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut für ganzheitliche Bewertungen der allgemeinen Dialogqualität, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer detaillierteren Ebene zu verstehen.\n\nEin Ansatz besteht darin, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalen-Methoden. Wir sind jedoch der Meinung, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu verringern, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen zeigt, wie z. B. das Bereitstellen irrelevanter Informationen oder das Widersprechen sich selbst. Wir nennen diesen Ansatz \"Annotierung von Verhaltensweisen im Chat\" oder kurz ABC-Eval.\n\nWir haben diese Methode entwickelt, um umfassend die Chat-Modellverhaltensweisen abzudecken, die in jüngster Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden. Dazu gehört, wenn ein Chat-Modell seinen Partner ignoriert oder irrelevante Aussagen macht, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder gegen allgemeines Wissen verstößt, und wenn das Modell es schafft oder nicht, Empathie zu zeigen.\n\nUm zu bestimmen, welche Art der Bewertung am effektivsten ist, wählten wir vier State-of-the-Art-Chat-Modelle aus und bewerteten sie anhand von 100 Mensch-Bot-Gesprächen pro Modell mit ABC-Eval. Zur Vergleichbarkeit bewerteten wir diese Gespräche auch mit drei bestehenden Methoden: Likert-Bewertungen auf der Dialogebene, Likert-Bewertungen auf der Dialogebene und dialogebene paarweise Vergleiche. Für jede der bestehenden Methoden sammelten wir Bewertungen zu acht der am häufigsten gemessenen Aspekte der Dialoge, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist.\n\nAus unserer Analyse dieser Bewertungsergebnisse geht hervor, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als Labels, die mit bestehenden Methoden gesammelt wurden, gemessen an der Inter-Annotator-Übereinstimmung bei 100 doppelt beschrifteten Gesprächen. Darüber hinaus sind ABC-Eval-Labels vorhersagender für die allgemeine Gesprächsqualität im Vergleich zu Metriken, die durch bestehende Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. Beispielsweise erklärt die Messung des Anteils der Züge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität, während die durchschnittlichen Likert-Konsistenz-Scores nur 4 % oder weniger erklären.\n\nSchließlich überprüften wir, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, unter Verwendung einer schrittweisen linearen Regression. Sie können sehen, dass die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, verlieren die meisten von ihnen eine beträchtliche Menge an Informationen über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf der Dialogebene viel weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen bei.\n\nDiese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten als dies mit vorherigen Methoden möglich war. Sie können in den Ergebnissen unseres Experiments sehen, dass noch mehrere Herausforderungen bestehen, die genau quantifiziert wurden. Beispielsweise weisen die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das gesunde Menschenverstand auf. Sie produzieren in etwa 15 % der Antworten irrelevante Informationen, und sie widersprechen sich selbst oder ihrem Partner etwa 10 % der Zeit.\n\nAngesichts des schnellen Fortschritts in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Modellvergleich anzustreben. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich Conversational AI in den kommenden Monaten und Jahren weiterentwickeln wird.\n\nVielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin Doktorandin im Fach Informatik an der Stony Brook University. Ich möchte unsere Arbeit, die beim ACL 2023 als Langpapier angenommen wurde, vorstellen: „Transfer Learning für Dissonanzerkennung“, eine Auseinandersetzung mit der Herausforderung seltener Klassen. Wir beginnen mit der Definition kognitiver Dissonanz und warum es wichtig ist, dieses Phänomen in der Sprache zu untersuchen. Kurz gesagt, kognitive Dissonanz liegt vor, wenn zwei Überzeugungen oder Handlungen inkonsistent sind. Ein Beispiel hierfür ist, wenn eine Person sagt: „Ich weiß, dass Zigaretten mich töten könnten“, und dann weiter ausführt: „Ich habe nach der Besprechung ein paar Zigaretten genommen.“ Diese Überzeugung und Handlung sind inkonsistent und stehen in Dissonanz. Die Aussage: „Ich denke nicht, dass ich meinen Job ohne sie behalten könnte“, rechtfertigt das zweite Ereignis und schafft eine Konsonanzbeziehung. Während Dissonanz ein sehr häufiges Phänomen in unseren täglichen Entscheidungen ist, kommt es in der Sprache im Vergleich zu anderen Diskursbeziehungen äußerst selten zum Ausdruck. Warum ist das relevant? Die Erforschung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends in Überzeugungen, Werten und Einstellungsänderungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz steht auch in Verbindung mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Die Untersuchung von in der Sprache ausgedrückter Dissonanz kann auch hilfreich sein, um Extremismus und Polarisierung anfälliger Gruppen zu begreifen. Schließlich ist kognitive Dissonanz wichtig, um die individuellen kognitiven Stile von Personen zu verstehen und hilft uns, Entscheidungsprozesse besser zu durchschauen.\n\nUm eine Ressource für kognitive Dissonanz zu erstellen, führten wir eine groß angelegte Annotation von Dissonanzbeziehungen durch. Wir verwendeten einen Dissonanz-zuerst-Ansatz, wie im Flussdiagramm dargestellt. Tweets wurden mit einem PDTV-Parser analysiert und Paare von Diskursseinheiten gemäß den in unserer Arbeit beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nach der Sammlung von etwa 1000 Beispielen von Diskursseinheitspaaren trainierten wir einen anfänglichen Klassifikator, der nur mit 43 Beispielen von Dissonanz trainiert wurde. Kein Wunder, dass der Klassifikator kaum besser als zufällig funktionierte. Angesichts der geringen Häufigkeit von Dissonanz und dem Fehlen vorheriger Datensätze zu diesem Thema stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transferlernen und aktivem Lernen, um mehr Dissonanzbeispiele durch weniger Annotierungsläufe zu sammeln, die Gesamtkosten zu senken und die Dissonanzerkennung zu verbessern. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir den aktiven Lernprozess, indem wir Gewichte aus eng verwandten Aufgaben übertragen. Wir übertragen aus zwei verschiedenen Aufgaben: Themenunabhängige Dissonanz-Standpunkt-Klassifizierung, eine Aufgabe, die bestimmt, ob zwei Debattenaussagen verschiedener Personen unabhängig vom Thema übereinstimmen oder nicht, genannt Debatte, und binäre Klassifizierung der Erweiterungs- und Vergleichsklassen des PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verwandt sind, und wir nennen sie hier CEE. Wir stellen fest, dass die Null-Shot-Leistung auf dem annotierten Datensatz nach dem Übertragen bereits deutlich besser als zufällig ist, mit der besten AUC von 0,62. Darüber hinaus verbessert sich die Leistung durch iterative Aktualisierungen des Modells durch aktives Lernen und Annotierungen. Kumulativ sammelt alle Daten, die bisher aus aktiven Annotierungen gesammelt wurden, während iterativ das Modell durch Training auf den neuesten Datensatz aktualisiert. Über die verschiedenen Strategien hinweg erwies sich kumulativ als gleich gut oder besser als iterativ.\n\nUm die Anzahl der Dissonanzbeispiele zu erhöhen, verwenden wir eine Strategie mit Wahrscheinlichkeit seltener Klassen, um hauptsächlich Beispiele auszuwählen, die nach dem aktuellen Modell in jeder Runde des aktiven Lernens mit hoher Wahrscheinlichkeit dissonant sind. Wir vergleichen dies mit anderen State-of-the-Art-Strategien, wobei der Unterschied gering ist. Beachten Sie, dass die Leistung für zufällige Auswahl deutlich niedriger ist. In weiteren Runden des aktiven Lernens mit den beiden besten Strategien verbesserten wir die Klassifikations-AUC für Dissonanz auf 0,75, was die beste Leistung ist, die wir bisher in dieser Aufgabe erreicht haben. Wir prüften auch die Machbarkeit jeder Strategie hinsichtlich Annotierungsqualität und Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanzbeispielen aufweist und sich am besten für die Akquise seltener Klassen eignet. Allerdings finden die Annotatoren die Beispiele auch schwieriger.\n\nZusammenfassend lässt sich sagen, dass PRC eine einfache Strategie des aktiven Lernens für die Akquise seltener Klassen und den Kaltstart des aktiven Lernens mit entsprechend gestalteten Transferlernaufgaben ist und signifikant helfen kann. Wir stellen auch fest, dass iterative Aktualisierung für Transferlernen aus einem anderen Bereich nützlich ist, während in-Domain-aktive Annotierungen von kumulativer Aktualisierung profitieren. Hier sind die Links zu unserem Code, unserem Datensatz und unserer Arbeit. Zögern Sie nicht, sich mit uns in Verbindung zu setzen, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Akshata und heute präsentiere ich gemeinsam mit meinem Mitautor Martin unsere Arbeit „The Kipma Steps“, in der wir die Integration von Wissen aus verschiedenen Quellen bewerten. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Nationale Sprachverstehensmodelle greifen auf verschiedene Wissensquellen zurück, wie beispielsweise das in ihren Parametern enthaltene Wissen, das in der Regel während des Prätrainings erworben wird, und das in den Eingaben bei der Inferenz bereitgestellte Wissen. Jüngste Arbeiten zu Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das im Voraus trainierte zeitliche Wissen nutzen können, um die Aufgabe zu lösen. Das natürliche Sprachverständnis erfordert jedoch oft auch Wissen, das bei der Inferenz bereitgestellt wird. Nehmen wir zum Beispiel den Satz: „John sah den neu gewählten Präsidenten im Fernsehen.“ Die prätrainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese ereignisspezifische Entität John ist oder wer der neue Präsident ist, da sich der Präsident seit dem Prätraining geändert haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl das im Voraus trainierte als auch das bei der Inferenz verfügbare Wissen zu integrieren und zu nutzen.\n\nIn dieser Arbeit schlagen wir einen Diagnosetest für die Wissensintegration vor. Wir führen eine Aufgabe zur Referenzauflösung ein, die darauf abzielt, die Fähigkeit zu untersuchen, auf Wissen aus verschiedenen Quellen zurückzugreifen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablieren Modelle zur Referenzauflösung. Hier ein Beispiel aus unserem Datensatz: „Servin ist Richter. Kia ist Bäcker. Servin und Kia trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er Fälle in einem Gerichtssaal entschied, war er froh, sich zu entspannen.“ Die Aufgabe besteht darin, die richtige Entität zu identifizieren, auf die das Pronomen „er“ verweist, in diesem Fall Servin. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: Erstens entitätsspezifisches Wissen, wie z. B. „Servin ist Richter“, und zweitens Hintergrundwissen, wie z. B. „Richter entscheiden Fälle in Gerichtssälen“. Im Allgemeinen wird Hintergrundwissen während des Prätrainings großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise bei der Inferenz beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen zu finden sein kann.\n\nWir haben drei Einstellungen für KITMOS definiert. Erstens haben wir die typische Einstellung „background-pretrain“, in der angenommen wird, dass das Hintergrundwissen bei der Prätrainingszeit verfügbar ist. Zweitens gibt es die Einstellung „background-both“, wo das Hintergrundwissen sowohl bei der Prätrainings- als auch bei der Inferenzzeit verfügbar ist. Und zuletzt die Einstellung „background-inference“, in der beide Wissensarten nur bei der Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das für die Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der prätrainierten Daten der Modelle ist, beispielsweise weil seit dem Prätraining neue Berufe entstanden sind.\n\nHier ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten und echten Quellen kontrollieren. In der Einstellung „background-pretrain“ gehen wir davon aus, dass das Hintergrundwissen „Politiker streben gewählte Sitze in der Regierung an“ in den prätrainierten Parametern enthalten ist. Und im 3-Zoll-Zeitkontext stellen wir die entitätsspezifische Information bereit, dass „Chichester ein Politiker ist“. In der Einstellung „background-both“ stellen wir zusätzlich sowohl entitätsspezifisches als auch Hintergrundwissen über Politiker im Inferenzzeitkontext bereit. In der Einstellung „background-inference“ verwenden wir stattdessen die fiktive Berufsangabe „Meritur“ anstelle von „Politiker“, da Meritur wahrscheinlich nicht in den prätrainierten Parametern enthalten ist.\n\nWir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Modellen zur Referenzauflösung. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle in der schwierigsten Variante der Einstellung „background-pretrain“. Ohne taskspezifisches Training auf KITMOS schneiden beide Modelle schlecht ab. Wenn sie jedoch auf KITMOS trainiert werden, performen sowohl C2F als auch BFQF signifikant besser als die zufällige Auswahl. Dies deutet darauf hin, dass Modelle, wenn sie auf allgemeinen Referenzauflösungs-Datensätzen trainiert werden, lernen, Oberflächenmerkmale auszunutzen, die beim Testen auf KITMOS, wo solche Merkmale entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle Schwierigkeiten haben, Hintergrundwissen, das nur bei der Inferenz bereitgestellt wird, zuverlässig zu integrieren.\n\nZusammenfassend lassen sich die wichtigsten Erkenntnisse unserer Arbeit wie folgt darstellen: Viele Referenzauflösungsmodelle scheinen ohne taskspezifisches Training nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu raisonieren. Mit taskspezifischem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Selbst die besten Modelle haben jedoch Schwierigkeiten, Hintergrundwissen, das nur bei der Inferenz präsentiert wird, zuverlässig zu integrieren. Wenn Sie weitere Details interessieren, finden Sie diese in unserer Arbeit und können den Datensatz auf GitHub im Code einsehen. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler, und ich werde kurz das Papier „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist.\n\nWas ist simultane Sprachübersetzung? Simultane Sprachübersetzung oder SimulST ist der Prozess der Übersetzung gesprochener Sprache in Echtzeit in einen Text in einer anderen Sprache, wodurch eine übersprachliche Kommunikation ermöglicht wird.\n\nUnd welche Probleme haben die aktuellen SimulST-Modelle? Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche zu optimierende Module eingeführt werden. Lange und komplizierte Trainingsverfahren, beispielsweise mit unterschiedlichen Optimierungsziele, und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen, z. B. das Training eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines anderen mit 2 Sekunden und so weiter.\n\nUnd was ist unsere Lösung? Erstens die Verwendung bereits vorhandener offline SD-Modelle ohne erneutes Training oder die Annahme einer spezifischen Architektur für SimulSD. Verwendung nur eines Modells für jedes Latenzregime und Handhabung der Latenz über spezifische Parameter. Und Nutzung des bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Textausgabe erworbenen Wissens, also des Cross-Aufmerksamkeitsmechanismus. Ein Beispiel sehen Sie auf der rechten Seite.\n\nUnsere Lösung besteht darin, EDAT (Encoder-Decoder-Aufmerksamkeit) vorzuschlagen, eine Strategie, bei der wir basierend darauf, wo die Aufmerksamkeit hinweist, entscheiden, ob wir eine partielle Übersetzung ausgeben oder nicht. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. ihre Summe unter einem bestimmten Schwellenwert Alpha liegt, auf weniger Lambda-Sprachrahmen, was bedeutet, dass die empfangenen Informationen ausreichend stabil sind.\n\nBeispielsweise, wenn wir einen Sprachabschnitt mit „Ich werde über“ erhalten und unser Modell die Übersetzung auf Deutsch vorhersagt und wir uns die Cross-Aufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen, d. h. Lambda-Sprachrahmen, verweist. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während wir, da die Summe der Cross-Aufmerksamkeit über einem bestimmten Schwellenwert Alpha liegt, das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten.\n\nWenn wir fortfahren und einen weiteren Sprachabschnitt erhalten und unser Modell drei weitere Wörter vorhersagt und wir uns die Cross-Aufmerksamkeitsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda-Sprachrahmen verweist. Das bedeutet, dass diese drei Wörter ausgegeben werden.\n\nWenn wir uns die Hauptresultate ansehen, plotten wir die Ergebnisse der simultanen Sprachübersetzung in Graphen, in denen Blau auf einer Seite die Übersetzungsqualität misst und der durchschnittliche Verzug, also die Latenzmessung, und wir berücksichtigen auch den rechenintensiv bewussten durchschnittlichen Verzug, der die Rechenzeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven in diesem Plot möglichst hoch sind, aber auch, dass sie nach links verschoben sind.\n\nWir vergleichen mit geeigneten Strategien, die auch auf Offline-Modelle angewendet werden, nämlich der Wet-Key-Strategie und der lokalen Übereinstimmung. Wir vergleichen auch mit der aktuellen Architektur, die speziell für die simultane Sprachübersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie auf Deutsch, und wir sehen, dass ADDOUT alle Strategien übertrifft, die auf Offline-Modelle angewendet werden, da die Kurven nach links verschoben sind.\n\nWir sehen auch, dass ADAT, wenn wir die tatsächliche verstrichene Zeit oder die rechenintensiv bewusste Zeit berücksichtigen, die schnellste Strategie ist. Um weitere Ergebnisse zu entdecken, lesen Sie bitte unsere Arbeit. Wir haben auch den Code und die Modelle als Open Source veröffentlicht und die simultane Ausgabe, um die Reproduzierbarkeit unserer Arbeit zu erleichtern.\n\nVielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Zhu Heng. Heute werde ich unseren Artikel mit dem Titel „Funktionieren Kernel-2003-Named-Entity-Tagger auch im Jahr 2023 noch gut?“ vorstellen. Los geht's. In unserem Artikel untersuchten wir das Problem der Verallgemeinerung unter Verwendung der Aufgabe der Named Entity Recognition (NER). Wir stellten fest, dass Modelle seit fast 20 Jahren Kano 2003 zur Entwicklung von NER nutzen, was natürlich mehrere Probleme aufwirft. Erstens, können diese Modelle auf moderne Daten verallgemeinern? Und wenn wir neue Tagger entwickeln, was ist für eine gute Verallgemeinerung erforderlich? Gleichzeitig, wenn wir eine schlechte Verallgemeinerung beobachten, was verursacht die Leistungsabnahme dieser Modelle? Um diese Probleme zu untersuchen, entwickelten wir das Kano++-Datenset. Dies ist ein Datensatz, den wir aus Reuters-Nachrichten von 2020 gesammelt und dann mit den gleichen Kano-2003-Anmerkungsrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf Kano 2003 feinabgestimmt. Wir bewerteten sie sowohl auf dem Cono-F1-Maß, um die Verallgemeinerungsfähigkeit jedes Modells zu beurteilen.\n\nWas ist also für eine gute Verallgemeinerung erforderlich? Durch unsere Experimente fanden wir heraus, dass drei Hauptkomponenten erforderlich sind. Die erste ist die Modellarchitektur. In unseren Experimenten stellten wir fest, dass Transformer-Modelle in der Regel besser auf neue Daten verallgemeinern. Die zweite Komponente ist die Modellgröße. Wir fanden heraus, dass in der Regel größere Modelle zu einer besseren Verallgemeinerung führen. Und nicht zu vergessen, wir wissen alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer Downstream-Aufgabe direkt beeinflusst. Hier stellten wir auch fest, dass mehr Feinabstimmungsbeispiele tatsächlich ebenfalls zu einer besseren Verallgemeinerung führen.\n\nZu unserer nächsten Frage, was verursacht die Leistungsabnahme einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptives Überanpassen, also Überanpassung durch wiederholtes Verwenden desselben Testsets, was sich in der Regel als abnehmende Rendite im neuen Testset manifestiert. Die zweite Hypothese ist zeitliche Drift, eine Leistungsverschlechterung durch die wachsende zeitliche Lücke zwischen Trainings- und Testdaten. Für adaptives Überanpassen sahen wir, dass die beste Anpassungsgerade (rote Linie) im Diagramm rechts eine Steigung von mehr als eins hat. Das bedeutet, dass jede Verbesserungseinheit, die wir bei CONO 2003 erzielten, zu mehr als einer Verbesserungseinheit bei Kano++ führt, was bedeutet, dass es keine abnehmende Rendite gibt. Dies zeigt, dass in diesem Fall kein adaptives Überanpassen beobachtet wurde.\n\nWas also die zeitliche Drift betrifft? ... Und dies bestätigt unsere Hypothese, dass die Hauptursache für die Leistungsabnahme die zeitliche Drift ist.\n\nUnser Fazit ist, dass für eine gute Verallgemeinerung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele erforderlich sind. Und diese gehen Hand in Hand. Wir können nicht nur eine Komponente haben und die anderen vernachlässigen. Gleichzeitig stellten wir auch fest, dass der Leistungsabfall hier durch zeitliche Drift verursacht wird, und überraschenderweise nicht durch adaptives Überanpassen, obwohl Kano 2003 seit über 20 Jahren verwendet wird.\n\nZurück zur Frage, die wir in der Überschrift unseres Artikels aufwarfen: Funktionieren Kano-2003-Tagger auch im Jahr 2023 noch? Und wir fanden heraus, dass die Antwort ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber aufruft, wie die Verallgemeinerungsfähigkeit von Modellen verbessert werden kann. Und zuletzt, vergessen Sie bitte nicht, unseren Artikel und unseren Datensatz zu prüfen, und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, herzlich willkommen zu unserer Präsentation von d.plain, einem neuen Korpus für die Vereinfachung deutscher Texte auf Dokument- und Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie beispielsweise Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, beispielsweise von Dokumenten oder Sätzen. Im hier gezeigten Beispiel sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Zur Vereinfachung des Satzes sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie beispielsweise lexikalische Substitution, Klaustulation, Klaustulationsreordnung oder das Einfügen von Wörtern.\n\nWir stellen nun unseren neuen Korpus d.plain vor. In den letzten Jahren gab es einige Probleme mit bestehenden Korpora. So sind beispielsweise diese Korpora hier zu klein, um ein Taxonifikationsmodell darauf zu trainieren. Die drei anderen in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass ihre Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus d.plain vor, der in zwei Teilkorpora unterteilt ist: d.plain-apa und d.plain-web. d.plain-apa basiert auf Nutzungstexten. In der einfachen APA haben wir 483 Dokumente alle manuell ausgerichtet. Das ergibt grob 30.000 bis 13.000 parallele Satzpaare. Für d.plain-web umfasst dieser Korpus verschiedene Domänen und wir haben auch alle 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergeben sich 30.450 Satzpaare.\n\nWir haben unsere Satzpaare etwas genauer analysiert, beispielsweise hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als beispielsweise die Nachrichtentexte oder die Sprachlernertexte auf allen Ebenen, was beispielsweise lexikalische Vereinfachung, strukturelle Vereinfachung oder das allgemeine Vereinfachungsniveau betrifft. Außerdem sehen Sie, dass unser d.plain-Korpus eine hohe Priorität für verschiedene Vereinfachungstransformationen aufweist. So haben wir beispielsweise im d.plain-API-Korpus viel mehr Reorderings und Wortzugänge als im d.plain-web-Korpus. Im Web-Korpus hingegen gibt es viel mehr Umschreibungen.\n\nLassen Sie uns nun sehen, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und werde nun über die Anwendungsfälle für unseren Datensatz D-plain sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der Maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in zwei parallelen Dokumenten mit derselben Sprache und gleichem Inhalt extrahieren möchten, die sich jedoch auf unterschiedlichen Komplexitätsniveaus befinden. Da wir nun unseren Datensatz d.plain mit manuell ausgerichteten Sätzen haben, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen sowie die Codes zum Durchführen unserer Experimente in der Arbeit veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für deutsche Textvereinfachung die Methode von math align ist. Den Code, um diese Methode auf Ihre eigenen Dokumente anzuwenden, finden Sie ebenfalls in der Arbeit.\n\nDer zweite Anwendungsfall, den wir in unserer Arbeit vorgestellt haben, ist die automatische Textvereinfachung durch Feinabstimmung von Sprachmodellen, um aus einem komplexen Eingabetext vereinfachten Text zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben finebased long-impart verwendet, um Satzebenen-Vereinfachungen zu erzeugen. Alle Checkpoints und detaillierte Informationen zu den Bewertungsmetriken unserer Experimente finden Sie in der Arbeit. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Basisscores erzielen konnte, und wir schlugen diese Ergebnisse als Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor.\n\nVielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie alle während der Konferenz zu treffen. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich stelle unsere Arbeit vor: „Distilling Script Knowledge from Large Language Models for Constraint Language Planning“. Im Alltag planen Menschen ihre Handlungen oft anhand schrittweiser Anweisungen in Form von garantierten Skripten. Bisherige Arbeiten konzentrierten sich jedoch hauptsächlich auf die Planung im Abstrakten. Ein guter Planer sollte Skripte schreiben, die erstmals die Ziele der Planung berücksichtigen. Ein abstraktes Ziel kann von verschiedenen spezifischen Zielen im wirklichen Leben mit vielschichtigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur geplanten Sprachverarbeitung unter Berücksichtigung von Einschränkungen. Da es keinen Datensatz spezifischer Ziele gibt, der unsere Studie unterstützt, müssen wir diese Ziele zunächst erwerben. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit vielschichtigen Einschränkungen für die Datenerhebung mit menschlicher Beteiligung unter Verwendung von InstructGPT. Wir wählen 100 spezifische Ziele aus und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle berichtet über die allgemeine Genauigkeit der Ergebnisse. Wir stellen fest, dass alle großen Sprachmodelle bei der Planung spezifischer Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Lernmodelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, die Treue zu den Einschränkungen jedoch nicht garantiert werden kann. Wir gingen detaillierteren, kornbasierten Themenkategorien der Einschränkungen nach, abhängig von der Arbeitsweise. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung instruktiver TPDs für Mädchen verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabegüte von leicht protokollierenden Modellen einen hohen Variationsumfang aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des übergenerierten Z-Filters, um die Generierungsqualität zu verbessern. Zuerst zeigen wir Constraint-Typen mit Beispielen für InstructGPT und erhalten spezifische Ziele auf der Grundlage der abstrakten Zielkeime. Dann übergeneriert InstructGPT Schlüsselskripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die machbaren Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Einbettungen und berechnen die kosinussimile Ähnlichkeit und Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur, wenn das Ziel in der Zielmenge die höchste Punktzahl erzielt. Mit unserer Methode kann InstructZBT Skripte höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit erheblich, sowohl in Bezug auf die semantische Vollständigkeit als auch auf die Treue zur Einschränkung. Da große Sprachmodelle kostspielig zu implementieren sind, ist es wichtig, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt dazu. Bisherige Studien ermöglichen jedoch keine Planung spezifischer Ziele, und die manuelle Datensatzanotierung ist teuer. Daher verfolgen wir die Idee der symbolischen Wissensdestillation, um Datensätze für die geplante Sprachverarbeitung mit Einschränkungen aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode zur Erstellung eines Datensatzes für die geplante Sprachverarbeitung an, der als CodeScript bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Teststellen zu gewährleisten, baten wir cloudbasierte Arbeiter, fehlerhafte Proben zu revidieren. Diese Abbildung zeigt die Constraint-Verteilung von CodeScript. Wir stellen fest, dass CodeScript in den generierten spezifischen Zielen eine hohe Applikabilität aufweist. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die Constraint-Sprachplanung einsetzen. Wir stellen fest, dass T-Dateienfunktionen auf CodeScript Skripte höherer Qualität generieren können als die meisten großen Sprachmodelle, was darauf hinweist, dass kleinere Modelle bei geeigneter Schulung auf geeigneten Datensätzen größere Modelle unterstützen können. Zusammenfassend haben wir das Problem der geplanten Sprachverarbeitung mit Einschränkungen aufgestellt. Wir bewerten die Fähigkeit großer Sprachmodelle zur geplanten Sprachverarbeitung mit Einschränkungen und entwickeln eine übergenerierende Filtermethode für die Sprachplanung. Vielen Dank für Ihre Aufmerksamkeit. Weitere Details zu CodeScript finden Sie in unserer Arbeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrac und ich werde Ihnen unsere Arbeiten zu Dr. BERT vorstellen, einem robusten vortrainierten Modell in Französisch für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir präsentieren das erste biomedizinische Modell in Französisch, genannt Dr. BERT, das auf Roberta basiert und mit NACHOS trainiert wurde, einem Datensatz medizinischer Web-Crawldaten. Wir haben auch einen Vergleich des Modells mit mehreren Vortrainings-Einstellungen und Datenquellen durchgeführt. Dann stellen wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch vor. Und schließlich ziehen wir Fazit aus den Experimenten und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können.\n\nSeit seiner Veröffentlichung im Jahr 2018 hat sich BERT als eine der effektivsten Methoden zur Lösung von Aufgaben der natürlichen Sprachverarbeitung erwiesen und bietet im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2Vec, FastText oder NWO erhebliche Leistungssteigerungen. Seitdem wurde dieses Modell an viele andere Sprachen angepasst, wie zum Beispiel in Französisch mit Camembert, und an andere Bereiche wie Biomedizin mit Perma-BERT und Bio-BERT sowie an den klinischen Bereich mit Clinical-BERT, hauptsächlich jedoch in Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten. Bisher gab es jedoch keine Open-Source-Modelle für die Biomedizin in Französisch.\n\nWir haben uns daher gefragt, welche Datenquellen für eine breite Palette von Anwendungen am geeignetsten sind und ob die aktuellen Daten eine gute Ersatzquelle für klinische Daten darstellen. Um diese Frage zu beantworten, haben wir Dr. BERT mit unserem Schubert-Modell verglichen, das auf anonymisierten Daten basiert, die aus dem nicht-universitären Krankenhaus unserer Einrichtung stammen. Eine weitere Frage, die wir uns gestellt haben, ist, wie viel Daten man benötigt, um ein spezialisiertes Modell auf Französisch zu trainieren. Benötigt man 4 GB, 8 GB oder 4 GB RAM?\n\nEine erste Version von Schubert, einem klinischen Modell, wurde mit 4 GB Sätzen aus klinischen Notizen trainiert. Die endgültige Version von Schubert enthält eine Mischung aus einem 4-GB-Subset von NACHOS und 4 GB klinischen Notizen. Zusätzlich zu diesem Vergleich haben wir drei Modelle vorgestellt, die auf kontinuierlichem Vortraining basieren, um den Einfluss der Vortrainingsstrategie zu analysieren. Eines dieser Modelle basiert auf den Gewichten von Camembert und wurde mit einem 4-GB-Subset von NACHOS trainiert. Ein weiteres Modell basiert ebenfalls auf Camembert, wurde jedoch mit 4 GB Perma-BERT, Bio-BERT und Clinical-BERT trainiert.\n\nDie Auswertung zeigt, dass die Modelle am besten auf Aufgaben abschneiden, für die Daten derselben Art verwendet wurden, wie die, auf denen das Modell trainiert wurde. Allerdings lässt sich beobachten, dass Daten aus heterogenen Quellen vielseitiger einsetzbar sind. Wir stellen auch fest, dass die Verwendung größerer Datenmengen zu besseren Leistungen führt. Insgesamt scheint das Training von Grund auf höhere Leistungen bei den meisten Aufgaben zu erzielen. Unsere Experimente zum kontinuierlichen Vortraining, die die Gewichte und den Tokenizer von Perma-BERT verwenden und auf dem 4-GB-Subset von NACHOS trainiert wurden, zeigen jedoch vergleichbare Ergebnisse zu denen von Dr. BERT 4 GB, das von Grund auf trainiert wurde. Dies gilt nicht für das Modell, das auf den Gewichten und dem Tokenizer von Camembert basiert und unter Stabilitätsproblemen leidet.\n\nAbschließend bietet unser eigenes System auf 9 der 11 nicht-getrimmten Aufgaben bessere Leistungen und übertrifft insgesamt die Ergebnisse des generischen Modells, hier Camembert. Wir stellen auch fest, dass spezialisiertere Daten besser sind, aber nicht gut skalierbar. Alle vortrainierten Modelle, die aus NACHOS abgeleitet wurden, sind frei auf UGIMFACE verfügbar, und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für Ihre Aufmerksamkeit und wir freuen uns auf Ihre Fragen während der Poster-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Xiangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit, die von der Vorabschulungsdaten bis zu Sprachmodellen und weiter zu nachgelagerten Aufgaben reicht, wobei wir die Spuren politischer Voreingenommenheit verfolgen, die zu ungerechten NLP-Modellen führen. Sprachmodelle werden also mit groß angelegten Web-Crawling-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vorabschulungsdaten gut abgedeckt. Laut einer Umfrage im C4-Korpus sind die New York Times, die Los Angeles Times, The Guardian, Huffington Post usw. in den Trainingsdaten von Sprachmodellen gut vertreten. Dies hat für Anwendungen von Sprachmodellen sowohl Segen als auch Fluch gebracht. Einerseits konnten sie aus unterschiedlichen Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairness-Problemen in nachgelagerten Aufgaben führen.\n\nUm dies zu beheben, schlagen wir vor, die Pipeline der politischen Voreingenommenheit zu untersuchen, die von den Vorabschulungsdaten über die Sprachmodelle bis zu den nachgelagerten Aufgaben reicht, indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielt das Fortbildungsdaten bei diesen politischen Voreingenommenheiten? Zweitens, wie schlagen sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in nachgelagerten Aufgaben und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen?\n\nKonkret haben wir zunächst vorgeschlagen, Sprachmodelle mit verschiedenen Prompt-Formaten zu befragen, indem wir politische Fragebögen wie den politischen Kompass-Test verwenden. Dies ermöglicht uns eine automatische Bewertung, die gut in der politischen Wissenschaft verankert ist. Vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie belegen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell aller ist und dass GPT-Theorien im Allgemeinen sozial liberaler sind als BERT-Theorie und ihre Varianten.\n\nZweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodell-Checkpoints weiter auf sechs verschiedenen parteiischen Korpora vorab trainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter nach ihrer politischen Ausrichtung differenziert werden. Durch das weitere Vorabtraining von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschoben werden. Beispielsweise verschiebt sich bei Roberta, weiter feinabgestimmt und auf den linksgerichteten Reddit-Korpus trainiert, die politische Voreingenommenheit deutlich in eine liberale Richtung.\n\nWir untersuchen auch, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Vorabtrainings-Korpora in einen Teil vor und nach dem 45. Präsidenten der Vereinigten Staaten auf und trainieren Sprachmodelle separat auf diesen beiden zeitlich unterschiedlichen Korpora. Wir stellen fest, dass Sprachmodelle nach 2017 im Allgemeinen eine politische Ausrichtung haben, die weiter vom Zentrum entfernt ist. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.\n\nAbschließend bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in den NLP-Anwendungen Hassrede-Erkennung und Falschinformationen-Erkennung, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben können. Wenn wir die Leistung pro Kategorie untersuchen, also die Leistung in verschiedene Demografien oder politische Bedeutungen von Nachrichtenmedien aufteilen, erkennen wir ein Muster: Zum Beispiel sind bei der Hassrede-Erkennung linksgerichtete Sprachmodelle besser darin, Hassreden gegen sozial benachteiligte Gruppen zu erkennen, aber schlechter darin, Hassreden gegen mächtigere Gruppen in unserer Gesellschaft zu erkennen. Umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassreden gegen Weiße und Männer zu erkennen, aber schlechter darin, Hassreden gegen Schwarze, LGBTQ+ und andere Minderheitengemeinschaften zu erkennen. Ähnliche Trends zeigen sich auch bei der Falschinformationen-Erkennung, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Falschinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen und umgekehrt.\n\nWir zeigen außerdem viele qualitative Beispiele, um zu veranschaulichen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen für Hassreden und Falschinformationen treffen, basierend auf ihrer sozialen Kategorie. Weitere Beispiele finden sich im Anhang, um dies zu verdeutlichen. Dies weist auf ein dringendes Fairness-Problem im Zusammenhang mit den politischen Voreingenommenheiten von Sprachmodellen hin. Beispielsweise könnte die Feinabstimmung rechtsgerichteter Sprachmodelle für Hassreden, Falschinformationen oder ähnliches und deren Einsatz auf einer beliebten Social-Media-Plattform dazu führen, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden. Dies könnte bedeuten, dass Hassreden gegen Minderheitengruppen unkontrolliert verbreitet werden.\n\nDies hat uns alarmiert, die Fairness-Probleme anzuerkennen und anzugehen, die durch die politische Ausrichtung von Sprachmodellen entstehen. Um die Diskussion abzuschließen, möchten wir auch das einzigartige Dilemma hervorheben, das mit den politischen Voreingenommenheiten von Sprachmodellen verbunden ist. Es ist wie zwischen Scylla und Charybdis. Wenn wir die politischen Meinungen in den Trainingsdaten von Sprachmodellen nicht bereinigen, wird die Voreingenommenheit von den Vorabschulungsdaten über die Sprachmodelle bis zu den nachgelagerten Aufgaben weitergegeben und schafft letztendlich Fairness-Probleme. Wenn wir versuchen, sie irgendwie zu bereinigen, riskieren wir Zensur oder Ausschluss, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten von Sprachmodellen beibehalten werden sollte. Es ist eine Art des elektrischen Trolley-Problems.\n\nDas war es für heute, danke für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Kostav Sinha und ich freue mich, Sie zu unserem Vortrag über unseren ACL 2023-Beitrag „Language Model Acceptability Judgements Are Not Always Robust to Context“ willkommen zu heißen. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy und Adina Williams. In dieser Arbeit überprüfen wir das Minimalpaar-Paradigma. Das Minimalpaar-Paradigma bewertet im Wesentlichen Sprachmodelle auf der Grundlage von Akzeptanzurteilen, die auch Grammatikalität umfassen können, wie z. B. BLIMP, Syntax-Gym oder Akzeptanz im Hinblick auf Stereotype, wie z. B. Krauss-Paare. Im Rahmen dieses Minimalpaar-Paradigmas wird Sprachmodellen typischerweise ein akzeptables oder grammatikalisches Satzbeispiel und anschließend ein inakzeptables oder ungrammatikalisches Satzbeispiel präsentiert. Das Modell weist dabei in der Regel der akzeptablen Äußerung eine höhere Wahrscheinlichkeit zu. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, die Akzeptanz von Modellen für längere Sätze zu bewerten. Heutzutage verfügen große Sprachmodelle über immer längere Kontextfenster. Daher überprüfen wir die Datensätze selbst und erstellen Sätze, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. So haben wir beispielsweise ein typisches Paar zur Grammatikalität aus dem BLIMP-Datensatz des Adjunct-Island-Falls ausgewählt. Um längere, akzeptable Sequenzen zu erstellen, die die gleiche grammatikalische Struktur aufweisen, extrahieren wir grammatikalische Sätze aus Adjunct Island und fügen sie als Präfix sowohl für die akzeptable als auch für die inakzeptable Abfrage hinzu. Wir können dasselbe tun, indem wir inakzeptable Sätze aus demselben Kontext auswählen, was ebenfalls zur Überprüfung der Akzeptanz des Modells verwendet werden kann. Außerdem können wir Sätze aus einem anderen Unterbereich oder einem anderen Datensatz auswählen. Dies nennen wir das Mismatch-Szenario. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir bewerten. Dasselbe gilt für einen Akzeptanzfall. Schließlich können wir Sätze aus einem völlig unerheblichen Bereich wie Wikipedia auswählen. Dies zeigt uns, ob die Akzeptanzurteile des Modells tatsächlich von einem Kontext beeinflusst werden, der für den betrachteten Satz völlig irrelevant ist. Wie schlägt sich das Modell? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind. Dabei stellen wir fest, dass die MPP-Urteile für eine willkürliche Kontextlänge weitgehend robust sind. Wir erhöhen die Kontextlänge bis zu 1024, um die Modelle OPT und GPT-2 auszureizen. Wie Sie an der orangefarbenen gestrichelten Linie sehen können, sind die MPP-Urteile relativ stabil. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier erstellen oder wählen wir Sätze aus akzeptablen und inakzeptablen Domänen aus demselben BLIMP- oder Syntax-Gem-Datensatz. Dabei stellen wir fest, dass die MPP-Urteile signifikant ansteigen oder abnehmen, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Wenn wir jedoch die Struktur übereinstimmen lassen, also Sätze aus demselben Phänomen in blame-person-text-gym auswählen, kommt es zu einem massiven Anstieg oder einem massiven Rückgang der MPP-Urteile des Modells, je nachdem, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Dieser Effekt nimmt mit der Kontextlänge zu und könnte wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum beeinflusst das übereinstimmende Präfix das Urteil des Sprachmodells so stark? Wir führten eine Reihe von Analysen durch, bei denen wir versuchten, den Eingangssatz zu manipulieren, indem wir die relevante Struktur beibehielten, aber Rauschen hinzufügten. Nach mehreren dieser Störungen stellten wir fest, dass keines dieser Rauschen das Modell dazu brachte, seinen Kurs in Bezug auf die MPP-Urteilstrend zu ändern. Im Grunde genommen stellen wir fest, dass die Modelle auf die gestörten Sätze in ähnlicher Weise reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, beobachten wir einen ähnlichen Anstieg bei allen Störungen. Und wenn wir die Sätze im inakzeptablen Bereich stören, beobachten wir einen ähnlichen Rückgang der MPP-Urteile. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in den Sätzen gemeinsam sind. Die aktuelle MPP-Bewertung mit kurzen, einzelnen Satzeingaben erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Weitere Details zu unseren Experimenten finden Sie in unserem Aufsatz. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Promotionsstudent an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen, „Schwächer als du denkst“, eine kritische Betrachtung des schwach überwachten Lernens. Dies ist eine gemeinsame Arbeit mit Xiao Yusheng, Mario Smusbach, Gia Steffen und Dietrich Klackow. Ich möchte mit einer kurzen Einführung in schwache Überwachung und schwach überwachtes Lernen beginnen. Bei der schwachen Überwachung beschriften wir die Daten nicht manuell. Stattdessen beschriften wir die Daten mithilfe schwacher Beschriftungsquellen, wie einfacher heuristischer Regeln, Wissensdatenbanken oder geringwertiger Crowdsourcing-Methoden, wie im rechten Abbild dargestellt. Im Vergleich zu menschlichen Anmerkungen sind die schwachen Anmerkungen viel kostengünstiger, aber auch rauschbehaftet, was bedeutet, dass eine bestimmte Anzahl der Anmerkungen falsch ist. Wenn wir neuronale Netze direkt auf schwach beschrifteten Daten trainieren, neigen die neuronalen Netze dazu, das beschriftete Rauschen zu memorieren und nicht zu verallgemeinern. Bei schwach überwachtem Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Beschriftungsrauschen zu trainieren, sodass die trainierten Modelle immer noch gut verallgemeinern können. In jüngsten Arbeiten im Bereich WSL (schwach überwachtes Lernen) wird häufig behauptet, dass man Modelle nur mit schwach beschrifteten Daten trainiert und eine hohe Leistung auf sauberen Testdatensätzen erzielt. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken, der in drei Forschungsfragen besteht. Erstens: Benötigen wir saubere Validierungsdaten? Und zweitens: Sollten wir nur die sauberen Beispiele für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass interessanteweise aktuelle WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Andernfalls kommt es zu einem starken Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle nicht über die ursprünglichen schwachen Beschriftungen hinaus verallgemeinern, wenn keine sauberen Validierungsbeispiele vorhanden sind, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Kosten für die Beschriftung zur Erlangung sauberer Validierungsbeispiele sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass eine Erhöhung der Anzahl sauberer Validierungsbeispiele dazu beiträgt, dass WSL-Ansätze eine bessere Leistung erbringen, wie in der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine hohe Leistung zu erzielen. Aber damit ist die Geschichte noch nicht zu Ende, denn wenn wir uns entscheiden, auf jeden Fall auf saubere Beispiele zuzugreifen, dann führt das direkte Training darauf sogar zu einer noch besseren Leistung. Die rote Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir sehen können, beginnt die direkte Feinabstimmung bei 10 Beispielen pro Klasse, die WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungssteigerung leicht erreicht werden, indem die kontinuierliche Feinabstimmung auf den sauberen Validierungsbeispielen erlaubt wird. Wie wir an den Abbildungen sehen können, unterperformiert das Van Linden-Modell, bezeichnet als W, zunächst komplexere WSL-Methoden wie Cosine. Wenn wir jedoch die kontinuierliche Feinabstimmung auf den sauberen Beispielen zulassen, erreicht FTW eine Leistung, die anderen Methoden ebenbürtig ist. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicher erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze sauber manuell annotierte Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt: Erstens: Berichten Sie über die Modellauswahlkriterien. Beispielsweise ob die Modellauswahl auf sauberen Validierungsdaten durchgeführt wird. Zweitens: WSL-Ansätze sollten mit wenigen kurzen Lern-Baselines verglichen werden, die auf sauberen Beispielen arbeiten. Drittens: Kontinuierliche Feinabstimmung ist eine einfache und dennoch starke Baseline, die in zukünftigen Arbeiten im Bereich WSL berücksichtigt werden sollte. Abschließend haben wir unseren Code offen gelegt. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte schauen Sie ihn sich gerne an. Vielen Dank und viel Spaß bei der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist David Vilar und ich werde einen kurzen Überblick über das Papier „Grunting Parm“ aus dem Bereich Übersetzung geben, in dem Strategien und Leistung bewertet werden. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Parm ist ein Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Textsammlung mit 780 Milliarden Token trainiert. Zum Zeitpunkt der Veröffentlichung erreichte es in Hunderten von NLP-Aufgaben den Stand der Technik. In dieser Arbeit präsentieren wir die erste systematische Studie zur Prompting-Technik für große Sprachmodelle in der maschinellen Übersetzung. Wir bewerteten die Übersetzungsfähigkeit dieser Modelle unter Verwendung der Best Practices der IMT-Community. Dazu gehört die Verwendung der neuesten Testdaten, um eine Überlappung mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir vergleichen zwei State-of-the-Art-Systeme, nämlich die besten Systeme der WMT-Bewertung. Wir verwenden moderne neuronale IM-Metriken und zeigen zusätzlich auch Ergebnisse der expertenbasierten menschlichen Bewertung. Abschließend geben wir Empfehlungen für Strategien zur Prompt-Auswahl.\n\nDas Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung. Dies zeigt sich in einem einfachen Experiment, bei dem wir One-Shot-Prompting verwenden und für jeden Satz zwei verschiedene Prompts bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt der beobachtete Unterschied mehr als einen Blur-Punkt. In extremen Fällen kann er bis zu 40 Blur-Punkte betragen. Es ist daher wichtig, eine gute Prompt-Auswahlstrategie zu wählen. In unseren Experimenten entschieden wir uns für eine Five-Shot-Prompting-Strategie, bei der wir jeden dem System bereitgestellten Satz einfach mit der Sprache markieren, in der er vorliegt. In diesem Beispiel hier, wo wir die Übersetzung von Deutsch ins Englische durchführen, werden die deutschen Sätze mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert.\n\nWir stellten fest, dass die tatsächliche Form des Promptings bei Multi-Shot-Prompting keinen großen Einfluss hat. Sie ist für Zero- und One-Shot-Prompting entscheidend, und wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, gibt es fast keinen Unterschied mehr zur Form des Promptings. Die Beispiele tragen das meiste Gewicht.\n\nZusammenfassend zeigen unsere experimentellen Ergebnisse, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit mit dem Quellsatz. Es ist daher wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Bewertungen mit den Dev-Daten. Die Dev-Daten sind viel besser kuratiert und von höherer Qualität als die Trainingsdaten, die rauschhafter sind. Die Ergebnisse zeigen eine bessere Leistung, wenn die Dev-Daten verwendet werden.\n\nDennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den Übersetzungen von Parm, aber Parm kommt einem kommerziellen System sehr nahe. In unserem Fall wählten wir Google Translate für die Bewertung.\n\nDie Erkenntnisse, die wir aus der E-Mail-Analyse gewannen, die wir mit dem MQM-Framework durchführten, sind, dass die Fließfähigkeit von Parm mit State-of-the-Art-Systemen vergleichbar ist, aber der Hauptunterschied in der Genauigkeit liegt. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass Parm manchmal Teile des Quellsatzes weglässt, die in der Übersetzung nicht relevant sind, um eine besser klingende Übersetzung zu erzeugen. Allerdings ist die Kategorie „stilistische Ungeschicklichkeit“ für Parm niedriger als für die State-of-the-Art-Systeme, was ein weiteres Signal dafür ist, dass Parm wirklich flüssige Ausgaben liefert, aber immer noch mit Genauigkeitsproblemen zu kämpfen hat.\n\nDas war es für diesen wirklich kurzen Überblick. Für weitere Details kommen Sie bitte zur vollständigen Präsentation des Papiers. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Jingwei Yi von der University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo über den Artikel „Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark“ zu präsentieren. Zuerst wollen wir den Hintergrund von Embedding und Services vorstellen. Aktuell sind große Sprachmodelle wie GPTT, LAMA und PALM herausragend in der natürlichen Sprachverstehens- und -generierung. Embedding as a Service ist einer der Dienstleistungen, die auf großen Sprachmodellen basieren und verschiedene NLP-Aufgaben unterstützen. Beispielsweise bietet OpenAI eine auf GPT basierende Embedding-API an. Allerdings haben aktuelle Arbeiten gezeigt, dass ein Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding as a Service zu schützen.\n\nUm das Urheberrecht von Embedding as a Service zu schützen, besteht eine der Lösungen darin, ein Wasserzeichen in den Anbieterdienst einzubetten und zu überprüfen, ob ein anderes Dienstleistungsangebot dieses Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen: Erstens sollte die Methode auf Embedding as a Service anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer heimlich genug sein oder der Angreifer sollte es nicht einfach entfernen können. Schließlich muss das Wasserzeichen während des Modellausziehungsprozesses auf die Dienste des Angreifers übertragbar sein.\n\nBestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht auf Embedding as a Service anwendbar oder weisen Mängel in Bezug auf die Übertragbarkeit auf.\n\nDetails zu unserem EmbeddingMarker: EmbeddingMarker besteht aus zwei Hauptstufen: Wasserzeicheneinbettung und Urheberrechtsüberprüfung. Vor diesen Hauptstufen wählen wir zunächst einen Auslösersatz aus. Der Auslösersatz ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeicheneinbettung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Das bereitgestellte Embedding ist die Gewichtssummation des Ziel-Embeddings und des Original-Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.\n\nDie Urheberrechtsüberprüfung dient dazu, festzustellen, ob ein Modell hinter einem anderen Dienst das Wortmarken-Wasserzeichen enthält. Wir konstruieren zunächst eine Backdoor und einen bösartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, bei denen alle Wörter zum Auslösersatz gehören, während alle Wörter in den Sätzen des bösartigen Datensatzes nicht zum Auslösersatz gehören. Dann fordert der Anbieter Embeddings vom Dieb-Dienst mit diesem Datensatz an. Es werden der Kosinus und die L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding berechnet. Wir berechnen den Ähnlichkeitsunterschied zwischen bösartigem und Backdoor-Datensatz, der als Delta-Kosinus und Delta-L2 definiert ist. Gleichzeitig wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritte Metrik.\n\nWir führen Experimente auf vier Datensätzen durch: AGnews, Mind, SSD2 und Eraspam. Wir gehen davon aus, dass der Anbieter den Wikitext-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser eingebettetes Marker eine hervorragende Erkennungsleistung erbringen kann, während es die Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Heimlichkeit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf den vier Datensätzen mithilfe von PCA visualisieren. Die Beschriftung der Grafiken gibt die Anzahl der Auslöser in jedem Satz an. Wie in den Grafiken gezeigt, ist es schwierig, zwischen den Backdoor-Embeddings und den normalen Embeddings zu unterscheiden.\n\nDas war's, vielen Dank. Wir freuen uns auf die Diskussion mit Ihnen."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Ying, und mein Kollege Zhiyang und ich werden unsere Forschung zu Multi-Improvement, der Verbesserung des Multi-Modell-Serien-Kurzlernens durch Instruktionsabstimmung, vorstellen. Mit den Fortschritten bei großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zu erkunden, indem sie vortrainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf parameter- und dateneffiziente Weise wiederverwenden. Kürzlich haben zahlreiche Studien gezeigt, dass Instruktionsabstimmung große Sprachmodelle dazu befähigt, unerwartete Aufgaben in einer Zero-Shot-Manier durch Befolgen natürlicher Anweisungen auszuführen. Allerdings konzentrieren sich die meisten bisherigen Arbeiten zur Instruktionsabstimmung auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während computer Vision- und multimodale Aufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Instruktionsabstimmung bei multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf unerwartete multimodale Aufgaben verbessern kann. Zusätzlich stellten wir bei unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instruktionsdatensätzen zwischen NLP und Multimodalen fest. Es existieren mehr als 1.600 sprachbasierte Instruktionsaufgaben, aber es gibt keine groß angelegte, öffentlich zugängliche multimodale Instruktionsaufgabe. Dies motivierte uns, einen multimodalen Instruktionsabstimmungs-Datensatz zu erstellen. Hier stellen wir MultiInstruct vor, den ersten multimodalen Instruktionsabstimmungs-Benchmark-Datensatz, der 62 vielfältige multimodale Aufgaben aus 10 breiten Kategorien umfasst. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit 5 von Experten verfassten Anweisungen ausgestattet. Zur Untersuchung der multimodalen Instruktionsabstimmung auf unserem vorgeschlagenen Datensatz verwenden wir OFA, ein vereinheitlichtes multimodales vortrainiertes Modell, als Basis. OFA verwendet einen vereinheitlichten Vokabular für Sprache, Bildtoken und die Koordinaten eines Bounding Box. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instruct-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabetypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem Eingabetext, Bilder, Anweisungen und Bounding Boxes im selben Token-Raum dargestellt werden.\n\nOkay, nun werde ich über die multimodale Instruktionsabstimmung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und sampeln 10.000 Instanzen pro Aufgabe. Für die Tests reservieren wir die gesamte Gruppe für den gesunden Menschenverstand und wählen zusätzlich fünf Aufgaben aus VQA und der Miscellaneous-Gruppe aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Darüber hinaus sampeln wir zufällig 20 Aufgaben aus dem Testsplit natürlicher Anweisungen als unerwartete Aufgaben für NLP. Wir verwenden ein vortrainiertes OFA-Großmodell als Basis. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während der Tests für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten. Wir berichten über den Mittelwert und das Maximum der Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente hinweg. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir über den Wurzel-L. Für eine RP-Aufgabe berichten wir ebenfalls über den Wurzel-JL. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt, die die Fähigkeit des Modells misst, für dieselbe Aufgabe konsistent dieselben Ausgaben zu erzeugen, unabhängig von leichten Variationen in der Formulierung der Anweisung.\n\nHier ist unser Hauptergebnis: Wie wir sehen können, kann Instruktionsabstimmung die Leistung von OFA bei multimodalen Szenenaufgaben erheblich verbessern. Auch Transfer Learning von natürlichen Instruktionsdatensätzen kann die Instruktionsabstimmung fördern. Wir können sehen, dass mit zunehmender Anzahl der Aufgaben das Modell eine bessere Leistung erzielt und gleichzeitig eine geringere Sensitivität aufweist. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung mit fünf Anweisungen verglichen haben. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt den Effekt unterschiedlicher Feinabstimmungsstrategien auf die Modellsensitivität. Wie wir sehen können, kann Transfer Learning von natürlichen Instruktionsdatensätzen dem Modell eine deutlich bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell ermöglichen. Wir können auch sehen, dass Transfer Learning von natürlichen Instruktionsdatensätzen OFA dabei helfen kann, eine viel bessere Leistung im Nitro-Instruct-Datensatz zu erzielen. Insgesamt schlagen wir den ersten groß angelegten multimodalen Instruktionsabstimmungs-Datensatz vor. Wir verbessern die Zero-Shot-Fähigkeit von OFA erheblich und erkunden verschiedene Transfer-Learning-Techniken und zeigen deren Vorteile. Wir entwerfen eine neue Metrik namens Sensitivität.\n\nEine letzte Sache: Wir sammeln derzeit einen viel größeren multimodalen Instruktionsabstimmungs-Datensatz mit etwa 150 zusätzlichen Varianten sprachbasierter Aufgaben und werden diese bald veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo alle zusammen, mein Name ist Yusheng Zhang von der Penn State University. Heute werde ich unsere Arbeit vorstellen: Cross-linguale semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Die semantische Analyse ist eine Aufgabe, bei der semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda-Kalkül erstellt werden. Die cross-linguale semantische Analyse besteht darin, Abfragen in mehreren natürlichen Sprachen in verschiedene Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir Abfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda, FunQL usw. übersetzen. Existierende cross-linguale semantische Analyse-Modelle wurden separat für begrenzte Aufgaben und Anwendungen vorgeschlagen und bewertet. Beispielsweise gibt es Lücken in der Abdeckung bestimmter natürlicher Sprachen. Chinesisch fehlt, und aufgrund der Abdeckung bestimmter Mini-Repräsentationen fehlt Lambda-Kalkül oder er wurde nur auf bestimmten neuronalen Modellen bewertet. Es gibt beispielsweise nur ein einzelnes Modell zur Bewertung. Daher schlagen wir Examplar vor. Wir stellen einen einheitlichen Datensatz, Examplar, für die cross-linguale semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält neun Datensätze aus verschiedenen Domänen, fünf semantische Analyse-Aufgaben, acht Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Bewertung. Die erste ist die Übersetzungs-Test-Einstellung. Wir verwenden die Google Translate API, um die Quellsprache in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell für Training und Bewertung. Beispielsweise trainieren wir das englische Modell mit englischen Abfragen. Während der Inferenz übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden das trainierte Modell, um das SQL vorherzusagen. Wir testen auch monolinguale Modelle. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache. Beispielsweise Deutsch-Deutsch oder Englisch-Englisch. Wir testen auch monolinguale Few-Shot-Einstellungen, indem wir monolinguale Modelle nur mit 10 % der Trainingsdaten trainieren. Und wir testen ein mehrsprachiges Modell, bei dem wir ein einziges mehrsprachiges Modell für alle Sprachen trainieren. Beispielsweise kombinieren wir deutsche, englische und chinesische Abfragen, um ein mehrsprachiges Modell zu trainieren. Während der Inferenz können wir dieses Modell verwenden, um deutsche, chinesische Abfragen usw. zu übersetzen. Wir berücksichtigen auch die cross-linguale Zero-Shot- und Few-Shot-Übertragung. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir also auf englischen Abfragen oder der Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein mehrsprachiges Modell zu trainieren und das SQL-Ausgabeformat vorherzusagen. Wir haben auch viele interessante Ergebnisse erzielt. Im Hinblick auf die Analyse monolingualer Modelle haben wir zwei Gruppen von Modellen bewertet, einschließlich Encoder PDR, was für mehrsprachige vorab trainierte Encoder mit zeigerbasierten Decodern steht, wie XLMR plus PDR und BERT plus PDR. Wir haben auch Encoder-Decoder-Modelle bewertet, die mehrsprachige vorab trainierte Encoder-Decoder-Modelle sind, wie MBART und MT5. Wir stellten fest, dass Encoder-Decoder auf allen neun Datensätzen die beste Leistung erbringt. Wir haben MT5 und XLMR plus PDR in der mehrsprachigen Einstellung bewertet. Wir stellten fest, dass Encoder-Decoder oder Encoder-PDR durch Training in einer Mischung verschiedener Sprachen verbessert werden können. Und wir stellten fest, dass dies daran liegt, dass die meisten wichtigen natürlichen Sprachen eine Leistungssteigerung erzielen, mit Ausnahme von Englisch, dessen Leistung in sieben Datensätzen abfällt und nur in drei Datensätzen zunimmt. Dies wird als Fluch der Mehrsprachigkeit bezeichnet. Wir vergleichen auch die cross-linguale Leistungsspanne. In dieser Abbildung stellt die blaue Linie die cross-linguale Few-Shot-Übertragung dar, die orangefarbene Linie die cross-linguale Zero-Shot-Übertragung und die grüne Linie die monolinguale Einstellung. Durch den Vergleich der grünen und orangefarbenen Linie stellten wir fest, dass die cross-linguale Übertragungsleistungsspanne im Zero-Shot-Modus signifikant ist. Und durch den Vergleich der blauen und orangefarbenen Linie stellten wir fest, dass die Übertragungsspanne im Few-Shot-Modus schnell verkürzt wird. Wir haben auch weitere interessante Erkenntnisse gewonnen. Beispielsweise übertrifft Encoder-Decoder frühere Arbeiten oder erreicht vergleichbare Ergebnisse. Die Darstellung auf Englisch als natürliche Sprache kann die Leistung im Few-Shot-Modus für Ziel-natürliche Sprachen erheblich steigern. Und wir stellten fest, dass mehrsprachige Sprachmodelle wie CODIS und BLUE immer noch für cross-linguale semantische Analyse-Aufgaben geeignet sind. Zusammenfassend haben wir Examplar entwickelt, einen einheitlichen Benchmark für die cross-linguale semantische Analyse mit mehreren natürlichen Sprachen und Hauptrepräsentationen. Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Besuchen Sie gerne unsere Publikation und den Code. Danke fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Szpilkowski und dieses Referat befasst sich mit der Abhängigkeitsstruktur von Koordinationsverbindungen. Wie Sie wissen könnten, gehen verschiedene Theorien und Korpusansätze von unterschiedlichen Abhängigkeitsstrukturen aus. So beispielsweise in den universellen Abhängigkeiten: Die Struktur der Koordinationsverbindung Lisa, Bart und Maggie ist so beschaffen, dass das erste Konjunkt der Kopf der gesamten Koordinationsstruktur ist, in diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igor Milchuks Bedeutungstexttheorie verfolgt, wo ebenfalls die gesamte Koordinationsstruktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind asymmetrisch, richtig? Sie heben eines der Konjunkte hervor.\n\nEs gibt jedoch auch symmetrische Ansätze für Koordinationsstrukturen, wie den Prager Ansatz oder den Konjunktionskopf-Ansatz, der in den Prager Abhängigkeitsbäumen verwendet wird. Hier werden Abhängigkeiten vom Regenten zu allen Konjunkten gebildet. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in Dick Hudsons Wortgrammatik verwendet wird, wo, sozusagen, alle Konjunkte Köpfe der Koordinationsstruktur sind.\n\nZiel dieses Artikels ist es, zwei Argumente gegen die asymmetrischen Strukturen von Koordinationsverbindungen wie die oben genannten zu liefern. Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das ich anhand dieser Beispiele erklären werde.\n\nIm Englischen, wie Sie wissen könnten, bevorzugen direkte Objekte, in der Nähe des Verbs zu stehen, während Adjunkte weiter entfernt sein können. So ist \"March read it yesterday\" in Ordnung, da das direkte Objekt in der Nähe des Verbs steht. Während \"March read yesterday it\" viel schlechter klingt, da hier zwischen Verb und direktem Objekt ein Adjunkt (\"yesterday\") steht. Dieser Effekt kann jedoch abgemildert werden, wenn das direkte Objekt sehr schwer und lang ist, da es dann an die Position nach dem Adjunkt verschoben werden kann. Dies wird in den folgenden Beispielen veranschaulicht.\n\nBeide Sätze sind akzeptabel: \"March read this absolutely fascinating book about the BCS today\" und \"March read yesterday this absolutely fascinating book about bees\". Die Begründung dafür ist, dass dieser Satzbau möglich ist, weil er trotz der Verletzung des allgemeinen grammatikalischen Prinzips, dass direkte Objekte neben dem Verb stehen sollten, das Prinzip der Abhängigkeitslängenminimierung erfüllt, welches besagt, dass kürzere Abhängigkeiten bevorzugt werden.\n\nDie beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also derjenigen, die sich zwischen diesen beiden Strukturen unterscheiden. Hier haben wir eine Abhängigkeit von \"read\" zum Adjunkt mit einer Länge von sieben Wörtern und eine Abhängigkeit von \"read\" zu \"book\" mit einer Länge von vier Wörtern. Zusammen ergeben sie elf. Wenn wir diese beiden Konjunkte austauschen, wird die Summe dieser beiden Abhängigkeiten zu sechs, also viel kürzer. Deshalb klingt dies recht gut. Es verletzt ein Prinzip, erfüllt aber ein anderes.\n\nWir haben verschiedene Statistiken über Koordinationsverbindungen aus der erweiterten Version des Penn Treebanks extrahiert und in unserem Artikel erläutert, warum wir keine universellen Abhängigkeiten verwendet haben. Diese Statistiken bestätigen die oft gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind, gemessen in Silben und auch die Beobachtung, dass diese Tendenz mit zunehmender Längendifferenz zwischen den Konjunkten stärker wird.\n\nWas jedoch neu an dieser Arbeit ist, ist die Feststellung, dass diese Tendenz nur dann auftritt, wenn der Regent links steht oder abwesend ist. In Beispielen wie \"I saw Bart and Lisa\" oder \"Homer came and sneezed\" – hier haben wir eine Koordinationsverbindung von zwei Verben ohne externen Regent – bevorzugt das linke Konjunkt, kürzer zu sein, je größer die Längendifferenz zu dem anderen Konjunkt ist.\n\nAllerdings verschwindet dieser Effekt, wenn der Regent rechts steht. Wir haben dies durch Messung der Länge in Zeichen (erste Spalte), Silben (mittlere Spalte) und Wörtern (rechte Spalte) gezeigt. Konzentrieren wir uns auf die rechte Spalte, sehen wir, dass die Tendenz des linken Konjunktes, kürzer zu sein, mit der absoluten Differenz in Wörtern zunimmt, wenn der Regent links steht. Das Gleiche beobachtet man, wenn kein Regent vorhanden ist, wie bei der Koordinationsverbindung von Sätzen. Aber wenn der Regent rechts steht, verschwindet diese Tendenz.\n\nIn unserem Artikel zeigen wir, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen wie die oben genannten und für symmetrische Strukturen liefert. Weitere Details und Argumente finden Sie in unserem Artikel, und wir freuen uns auf Ihre Fragen in der Poster-Session. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Mein Name ist Kaio Yin, und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengesteuerte mehrsprachige Erkundung“ präsentieren. Diese Arbeit entstand in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, Andre F.D. Martins und Graham Newbig. Viele Übersetzungen hängen vom Kontext ab. Nehmen wir zum Beispiel das Wort „Mole“ in diesem Satz: Wenn der vorherige Satz lautete: „Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden“, dann bezieht sich „Mole“ auf einen Spion. Wenn der vorherige Satz jedoch lautete: „Könnte es etwas Ernstes sein, Doktor?“, dann bezieht sich „Mole“ auf ein Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch recht schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was bedeutet, dass korpusbasierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Und obwohl einige eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen haben, unterstützen diese Ressourcen nur begrenzte Arten von kontextabhängigen Übersetzungen und eine begrenzte Anzahl von Sprachen, da sie üblicherweise auf Fachwissen und menschliche Kuratierung zurückgreifen.\n\nIn dieser Arbeit haben wir versucht, zwei Fragen zu beantworten: Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle solche Fälle? Um die erste Frage zu beantworten, begannen wir damit, zu messen, inwieweit ein Wort während der Übersetzung vom Kontext abhängt. In einer früheren Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodelle eingeführt. Dies geschieht durch die Messung, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann sich CXMI als die Information vorstellen, die durch die Bereitstellung von Kontext für das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu point-wise CXMI, das die Kontextnutzung auf Satz- oder Wortniveau messen kann. Wörter mit hohem P6MI können als solche angesehen werden, die für ihre Übersetzung Kontext erfordern.\n\nNun analysieren wir Wörter mit hohem P6MI, um Muster zwischen ihnen zu erkennen. Unsere Analyse basiert auf Transkripten von TED-Talks, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Erstens betrachten wir Wortart-Tags mit hohen PCXMI-Mittelwerten. Dies ermöglicht es uns, beispielsweise duale Pronomen im Arabischen zu finden, die relativ hohe PCXMI-Werte aufweisen. Dies lässt sich damit erklären, dass Englisch keine dualen Pronomen hat, sodass man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ähnlich stellen wir fest, dass bestimmte Sprachen auch Kontext erfordern, wenn man die passende Verbform wählen möchte.\n\nZweitens betrachten wir Vokabeln mit hohem PCSXMI, gemittelt über alle ihre verschiedenen Vorkommen. Dies hilft uns, Fälle wie diesen zu identifizieren, in denen im Chinesischen Kontext erforderlich ist, um Eigennamen richtig zu übersetzen und sicherzustellen, dass innerhalb des Dokuments dieselbe Übersetzung verwendet wird. Ähnlich stellen wir fest, dass Kontext erforderlich ist, um die richtige Formalität zu übersetzen.\n\nSchließlich betrachten wir einzelne Token mit hohem PCXMI. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur zum Ausdruck kommen, wie beispielsweise Ellipsenauflösung.\n\nNun nutzen wir unsere Analyseergebnisse, um einen Benchmark für die dokumentenebenen Übersetzung zu entwickeln. Für jedes der fünf identifizierten Diskursphänomene erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den mehrsprachigen Diskurs-bewussten, oder MUDA, Tagger. Wir können den Tagger dann verwenden, um Wörter zu identifizieren, die zu dem Phänomen gehören, und wir nennen unseren Tagger den"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich euch unsere Arbeit „Anal Positionality: Charakterisierung der entworfenen Verzerrungen von Datensätzen und Modellen“ vorstellen. Diese Arbeit entstand in Zusammenarbeit mit einigen Kollegen von der University of Washington und dem Allen Institute for AI, nämlich Sebastian Sante, Ronan Labrasse, Katarina Aranica und Martin Sapp.\n\nBeginnen wir mit einem Szenario: Sie arbeiten für eine Zeitung und durchsuchen die Kommentare unter einem Artikel, um toxische Inhalte zu entfernen. Sie könnten eine beliebte API wie Perspective API für die Toxizitätserkennung verwenden, die bei Carl Jones gut funktioniert. Bei Aditya Sharma jedoch ist die API nicht so empfindlich gegenüber abfälligen Ausdrücken, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für eine Design-Verzerrung, bei der wir systematische Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen beobachten.\n\nSolche Design-Verzerrungen können auf die Positionierung von NLP-Forschern und Modellentwicklern zurückzuführen sein. Positionierung beschreibt einfach die Perspektiven, die Menschen aufgrund ihrer demografischen Daten, Identität und Lebenserfahrungen einnehmen. Dies ist ein weit verbreitetes Konzept in kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen. Als Forscher kann die Positionierung den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändert.\n\nEine Frage, die sich stellt, ist: Haben Datensätze und Modelle eine Positionierung? Wir behaupten nicht, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen echter Menschen und können so bestimmte Positionierungen gegenüber anderen repräsentieren. Vorherige Arbeiten haben einige anekdotische Beweise für eine solche Positionierung geliefert, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modell-Positionierung. Diese Studien vergleichen jedoch nicht die Endnutzer mit den Datensätzen und Modellen selbst.\n\nDie Untersuchung der Positionierung von Modellen und Datensätzen gewinnt zunehmend an Bedeutung, da NLP-Aufgaben subjektiver und sozialer orientiert werden. Es ist schwierig, diese Verzerrungen zu charakterisieren, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.\n\nUm die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir die Anmerkungen echter Nutzer mit bestehenden Datensätzen und Modellen. Dies geschieht über unseren Rahmen „NL Positionality“. Unser Rahmen besteht aus zwei Hauptstufen: Erstens re-annotieren wir Datensätze mit verschiedenen Annotatoren. Wir entscheiden uns dafür, anstatt die demografischen Daten der ursprünglichen Annotatoren der Datensätze zu betrachten, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografiedaten selten gesammelt und geteilt werden. Wir entscheiden uns daher, Daten neu zu annotieren, um viele Annotatoren pro Instanz zu erhalten und einen reichhaltigen Satz demografischer Daten zu sammeln.\n\nAnschließend vergleichen wir die Annotationen nach Demografie mit Modellen und Datensätzen mithilfe eines Pearson-R-Korrelationskoeffizienten. Unser Rahmen unterscheidet sich damit von der Literatur über Annotator-Diskrepanzen, da wir Endnutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleichen, anstatt nur die Annotator-Übereinstimmung zu betrachten oder Annotator-Verteilungen zu modellieren.\n\nUnser Rahmen wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Crowdsourcing-Plattform unseres HCI-Kollaborateurs. Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können, im Gegensatz zu Plattformen wie MTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Darüber hinaus liefert Lab in the Wild immer noch hochwertige Daten.\n\nWir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Hier lesen die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz und schreiben dann, wie sozial akzeptabel die Situation ist. Um im Studium engagiert zu bleiben, können sie ihre Antworten anschließend mit einer KI und anderen vergleichen. Wir vergleichen diese Annotationen dann mit Social Chemistry, Delphi und GPT-4. Wir replizierten ein sehr ähnliches Setup für die Toxizitäts- und Hasssprachen-Erkennung.\n\nUnsere Studie sammelte letztendlich über 16.000 Annotationen von über tausend Annotatoren aus 87 Ländern. So sind wir nun besser gerüstet, um die Frage zu beantworten, mit wem NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir stellen fest, dass es Positionierung in NLP gibt. Beispielsweise sind Datensätze und Modelle am meisten mit englischsprachigen Ländern ausgerichtet. Für die GPT-4-Analyse der sozialen Akzeptanz stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir finden auch, dass Dyna-Hate am meisten mit englischsprachigen Ländern übereinstimmt. Zusätzlich stellen wir eine stärkere Übereinstimmung mit Menschen fest, die einen Hochschulabschluss haben.\n\nWenn Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen übereinstimmen, bleiben einige unvermeidlich zurück. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit ihren männlichen und weiblichen Gegenstücken. Dies finden wir sowohl in der GPT-4-Analyse der sozialen Akzeptanz als auch in der DynaHEAT-Aufgabenanalyse.\n\nAngesichts der Positionierung in NLP, was können wir dagegen tun? Wir haben einige Empfehlungen dafür. Die erste ist, alle relevanten Design-Entscheidungen im gesamten Forschungsprozess zu dokumentieren. Die zweite besteht darin, NLP-Forschung aus der Perspektive des Perspektivismus durchzuführen. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu entwickeln. Ein gutes Beispiel hierfür ist die Masakane-Initiative.\n\nWir möchten betonen, dass inklusives NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Das war es dann für unsere Präsentation. Wenn Sie jedoch mehr erfahren möchten, schauen Sie gerne auf unserem Dashboard nach den neuesten Analyseergebnissen und unserer Publikation. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit zur Auflösung indirekter Referenzausdrücke für die Entitätsauswahl sprechen, in der wir das Alt-Entities-Korporus einführen. Mein Name ist Jawad Hosseini, und dies ist eine gemeinsame Arbeit mit Philip Radlinski, Sylvia Parity und Annie Lewis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: Meinten Sie „Easy on Me“ oder „I Got a Feeling“? Hier möchte ein Benutzer zwischen zwei Songs auswählen. Das Offensichtlichste wäre die Verwendung einer direkten Referenz, z. B. durch Nennung des Songtitels „Easy on Me“ oder seiner Position („der erste“). Manchmal ist jedoch eine indirekte Referenz für ein natürlicheres Gespräch angemessener, z. B. wenn der Benutzer sich den Songtitel nicht merken kann, die Aussprachen zu ähnlich sind und eine Unterscheidung schwierig ist, oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen: „der neuere“ oder „der Song, der nicht energiegeladen ist“.\n\nDieses Problem ist in konversationsbasierten Systemen von Bedeutung und auch für die Bewertung der Entitätsverständnis-Fähigkeiten von LLMs. Uns ist kein öffentliches Datenset in großem Maßstab für diese Aufgabe bekannt, daher haben wir eines mit Hilfe von Crowdannotation erstellt. Unser Datenset umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensammlungsmethode betont die Informalität unter Verwendung eines Comic-Vervollständigungs-Setups. Der Comic enthält drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnerst du dich an den Song, den wir gestern gehört haben?“ Damit setzt Bob den Dialogkontext. In der zweiten Sprechblase fragt Alice: „Meinst du ‚Easy on Me‘ oder ‚I Got a Feeling‘?“ – die alternative Frage. In der dritten Blase verwendet Bob eine indirekte Referenz, um eine der Entitäten auszuwählen, z. B. „der neuere“.\n\nWir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird von den Annotatoren ausgefüllt. Die erste Blase wird aus einigen manuellen Eingaben pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert: Wir verwenden immer ein einfaches Template „Meinst du A oder B?“, wobei A und B Stichproben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben: Je höher man in der Liste geht, desto ähnlicher werden die Entitäten und die Unterscheidung wird in der Regel schwieriger. Die erste Methode ist eine gleichmäßige Zufallsstichprobe. Die zweite, wenn die Entitäten ähnliche Titel haben, z. B. zwei Bücher mit dem Namen „The Return“. Die dritte, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie z. B. das gleiche Genre oder den gleichen Künstler haben.\n\nWenn wir diese alternative Frage den Administratoren zeigen, kennen sie die Namen der Entitäten, aber unbedingt etwas über die Entitäten selbst. Daher zeigen wir einige Hintergrundinformationen zu den beiden Entitäten. Für Songs zeigen wir einfach einen Google-Suchlink für jeden Song und bitten die Annotatoren, sich zumindest Teile jedes Songs anzuhören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song „Easy on Me“. Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, z. B. die erste, und sie mit drei bis fünf indirekten Referenzausdrücken zu beschreiben, z. B. „der mit der Pianomusik“. Hier sind einige Beispiele aus unserem Datenset: „der ohne Worte“, „nicht der mit dem 12-jährigen Jungen“, „der fiktive“ oder „der aus Aserbaidschan stammt“ und so weiter.\n\nDas Identitäten-Korporus enthält 6.000 alternative Fragen in drei Domänen und 42.000 indirekte Referenzausdrücke. Die Ergebnisse mit dem T5XLARGE-Modell sind wie folgt zusammengefasst: Wenn das Sprachmodell auf genau die gleichen Hintergrundinformationen wie die Annotatoren zugreifen kann, ist die Genauigkeit sehr hoch, etwa 92 bis 95 Prozent. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell auf teilweise überschneidende Hintergrundinformationen zugreifen kann, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, z. B. wenn das Sprachmodell die Hintergrundinformationen abruft. Dann sinkt die Genauigkeit auf etwa 60 Prozent. Es besteht also viel Verbesserungspotenzial.\n\nWir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datenset. Vielen Dank."}
