{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是马蒂亚斯·林德曼（Matthias Lindemann），今天我将简要介绍我们关于使用多集标记和潜在置换进行树结构之外的组合泛化论文。这是我与导师亚历山大·科勒（Alexander Koller）和伊万·蒂托夫（Ivan Titov）的合作成果。组合泛化可以理解为学习者处理更深层递归和训练时单独见过的短语组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。与往常一样，我们有一个训练语句集，例如“女孩睡着了”和“玛丽知道女孩睡着了”。这些语句与代表其核心意义的逻辑形式配对。与标准机器学习评估不同，测试集不来自同一分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练期间见过更浅的递归，并在测试时遇到更深的递归例子。简单的序列到序列模型在这个超出分布的泛化方面遇到困难，经常产生与输入脱节的输出。特别是，它们经常无法再现输入和输出之间的系统对应关系，例如在例子中用颜色标注的对应关系。一种流行的解决方法是将树结构集成到模型中。这些树旨在捕捉与逻辑形式相关联的语句组合过程。这效果很好，但树结构通常不给定，需要以某种方式获取。这可能很复杂，有时是计算上昂贵的过程。通常，这需要对逻辑形式进行大量特定形式主义的预处理，例如处理变量符号。获取树结构也可能涉及专业的语法归纳程序。在本文中，我们不使用树结构，并引入一个神经序列到序列模型，直接建模输入片段和输出片段之间的对应关系。我们首次展示了不依赖树结构而对更深层递归进行强泛化。我们的方法从输入中预测输出，分为两个步骤。首先，我们为每个输入标记添加一个无序的多集标记，其中包含将出现在输出中的标记。在第一个步骤之后，我们有了所有正确的标记，但它们没有顺序。因此，在第二个步骤中，我们使用另一个模型来预测一个置换，将它们放入正确的顺序。我们引入了一种新方法来预测置换，对可能的置换不施加任何硬约束。这使我们的方法非常灵活且具有表达力。从概念上讲，我们的置换模型大致如下工作。我们从输出左侧到右侧移动，确定将每个多集标记放入哪个位置。对于第一个输出位置，我们简单地选择一个如红色高亮显示的标记。然后，我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式跳到另一个多集标记。我们继续这个过程，直到访问完第一个阶段中的每个标记。为了给您展示实验结果的一小部分，我们在这里将我们的方法与其他没有树结构的模型在COGS基准测试上的比较。我们的模型在对更深层递归的泛化方面比其他模型表现出色。然而，其他一些结构泛化仍然非常具有挑战性。在我们的文章中，我们解决了一些有趣的技术挑战。首先，输入和输出的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多集，这为训练带来了挑战。此外，有时有多个与数据一致的置换，但语言上正确的置换是潜在的。我们通过在训练过程中归纳对齐来解决这个问题。我们的置换方法非常灵活，但带来了找到最高得分置换是NP难的问题。这是因为它与旅行商问题相关。我们通过一个GPU友好的连续放松来近似这个问题，这还允许我们反向传播解决方案并学习语言上更合理的置换。如果您想了解更多关于我们实验和如何解决这些挑战的信息，请阅读我们的论文或参观我们的展板。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Myra，今天我将讨论我们的一篇论文《标记的人格：使用自然语言提示来衡量语言模型中的刻板印象》。这项工作是与Esen Dermusch和Dan Jorofsky合作完成的。近年来，许多研究已经记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍存在。然而，这些衡量方法存在各种局限性。它们通常依赖于手工构建的数据集，需要大量时间来整理，而且通常只衡量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景，或者它们只是捕捉到与特定群体相关的非常普遍的广泛关联。此外，这方面的大多数工作都没有考虑到交集性，即多层面的社会身份可以加剧偏见，并成为独特的伤害焦点。\n\n为了克服这些局限性，我们利用了这些新指令调优LLM的一个特性，即它们非常擅长响应提示中的指令。因此，我们可以要求模型生成一个人格，这是一个通过提示（例如，想象你是一个亚洲女性，描述你自己）来描绘想象中个人的描述。我们可以立即看到，这可以很好地应用于任何人口统计，因为我们可以将任何我们想要的身份标记指定到这个提示中。以下是GPT-4的一些生成示例。立即可以看到，虽然输出不是传统意义上的明显负面或有毒的，但有一些有趣的模式。亚洲女性被描绘为不显眼。中东女性被描述为异国情调和令人着迷的地区。有色人种女性的人格都提到了祖先，而白人男性的人格则没有。\n\n为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人格。我们生成这些人格的提示受到一项研究的启发，该研究向人类受试者提供了这些提示，发现通过将其提供给人类受试者，他们也能够揭示种族刻板印象。这还使我们能够直接比较我们生成的人物与人类撰写的响应。第二部分是标记词，这是一种方法，用于识别区分标记组和未标记组的词语，我很快会详细说明。这种方法的优点是，我们可以获得非常具体的刻板印象和模式，而不必依赖任何特定的词典。\n\n因此，标记词方法利用了社会语言学中的标记概念，该概念指出存在一个未标记的默认值，任何与该默认值不同的群体在语言上都被标记。例如，词语“战士”通常与男性相关联，所以当人们描述一个女性战士时，他们通常会实际指定一个男性战士，并在术语中标记为“女性”。更广泛地说，社会中的主导群体在语言和社会上都是未标记的，而边缘化群体通常被标记。因此，我们的方法首先指定了未标记和标记群体是什么。然后，我们使用战斗词方法比较这些人格，这基本上是使用加权对数几率比来区分每个标记群体的顶级词语。例如，对于黑人女性的人格，我们将使用战斗词方法，并将其对数几率比与白人人格和男性人格进行比较，因为它们是两个相应的未标记群体。\n\n现在来看一些结果。首先，我们使用了一个刻板印象词典，发现生成的人物包含比人类撰写的人物更多的刻板印象。然而，当我们实际查看词典中词语的分布时，发现情况完全不同。虽然生成的人物包含的词典词语比例更高，但人类撰写的人物包含的词语分布更广泛，而刻板印象词语在生成的人物中实际上只有“高”和“健壮”这两个词。因此，这些词典实际上并没有很好地捕捉到我们在之前的幻灯片中看到的许多有害模式。\n\n相反，为了展示这些看似积极的词语如何促进刻板印象和本质化叙事，我们将转向标记词方法的结果。在我们的分析中，我们揭示了这些看似积极的描述如何反映有害模式。首先，对于标记群体，顶级词语包括文化、传统、自豪和异国情调等词语。这些词语仅根据其与身份的关系来定义这些群体，并将其与白人规范区分开来。这为这些群体带来了长期的歧视和其他化历史。此外，这些词语中反映了许多共同的套路，尤其是对于有色人种女性。例如，描述拉美裔女性的词语包括充满活力和曲线玲珑，这与热带主义套路相关。对于亚洲女性，词语是小巧、精致和丝滑，这与亚洲女性被性化、被视为温顺和顺从的历史相关。\n\n最后，对于黑人女性，我们看到一些顶级词语是坚强和韧性。这与人们所说的黑人女性强人形象相关。虽然乍看之下似乎是积极的，但研究表明，这种形象实际上是非常有害的，因为它给这些人口统计群体带来了巨大的压力，要求他们在面对社会障碍时保持坚强和坚韧。因此，而不是真正努力改变这些障碍，它给这些人带来了压力，要求他们克服这些障碍，这导致这些人出现非常负面的健康结果，以及其他伤害。\n\n更广泛地说，我们发现每个标记群体的词语几乎完全反映了本质化叙事。基于这些模式，我们为模型所有者提出了三点建议。首先，作为研究人员，我们应该解决积极刻板印象和本质化叙事的问题。我们还应该使用交集性视角来研究偏见和伤害，因为如果不这样做，可能会忽略许多事情。最后，应该提高偏见缓解方法的透明度，因为例如，像这些积极刻板印象一样，我们不知道这是因为某种奇怪的、过度过度的价值观对齐在起作用，或者可能是其他反刻板印象方法导致这些有害模式。如果没有更多的透明度，我们真的不能做出任何假设或进一步研究这一点。\n\n非常感谢您的聆听。在ACL中玩得开心。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是詹姆斯·芬奇。我是莎拉·芬奇。今天我们将向您介绍 ABCeval，一种评估对话人工智能的新维度方法。这项工作由埃默里大学自然语言处理实验室完成，由埃默里大学的吉诺·崔教授领导，并与亚马逊 Alexa AI 合作。假设您刚开发了一个对话模型，想了解它与当前最先进技术相比表现如何。常见的做法是使用人工评估，例如请人类评判员选择两个对话中哪一个更好，或使用利克特量表对对话进行评分。这些方法在提供整体对话质量评估方面效果良好，但对话质量有多个方面。因此，您可能希望评估聊天质量的多个维度，以更细致地了解模型的优缺点。一种方法是简单地请人类评判员评估对话质量的多个维度，例如模型响应的相关性，使用现有的比较或利克特量表方法。然而，我们认为存在一种更精确、更可靠的维度对话评估策略。我们的方法试图通过明确标注每个模型响应是否表达某些行为来减少人工评估的主观性，例如提供无关信息或自相矛盾。我们将这种方法称为聊天行为标注，简称 ABC 评估。我们开发这种方法是为了全面覆盖最近文献中建议影响聊天质量聊天模型的行为。ABC 评估聊天模型是否忽略其对话伙伴或说出无关紧要的话，是否自相矛盾或与对话伙伴矛盾，是否编造不正确的事实或违反常识知识，以及模型在表现同理心方面是否成功或失败。为了确定最有效的评估类型，我们选择了四种最先进的聊天模型，并使用 ABC 评估对每个模型的 100 个人工聊天对话进行评估。为了进行比较，我们还使用三种现有方法对这些对话进行了评估：回合级利克特评分、对话级利克特评分和对话级配对比较。对于现有方法中的每一种，我们收集了八个最常见对话测量指标的评估，因为这是评估聊天模型多维度的标准做法。从我们对这些评估结果的分析中，我们发现 ABC 评估行为标签总体上比现有方法收集的标签更可靠，具体衡量标准是 100 个双重标注对话的标注者间一致性。此外，ABC 评估标签比现有方法产生的指标更能预测整体对话质量，如简单线性回归分析所示。例如，您可以看到，测量自相矛盾和对话伙伴矛盾的回合比例分别解释了 5% 和 10% 的对话质量，而平均利克特一致性得分仅解释了 4% 或更少。最后，我们检查了每个评估指标是否捕获了聊天质量的独特方面，使用逐步线性回归。您可以看到，所有 ABC 评估指标的组合解释了超过 25% 的对话质量，当您逐个删除指标时，大多数都会导致失去大量关于质量的信息。另一方面，所有回合级利克特指标的组合解释的质量远少，这些指标中较少的有独特信息。这些可靠、信息丰富且独特的 ABC 评估指标使我们能够以高于先前方法能达到的分辨率评估对话人工智能。您可以从我们实验的结果中看到，仍然存在几个挑战，并且这些挑战已被精确量化。例如，我们测试的机器人大约有 20% 的响应违反了常识。它们在大约 15% 的响应中产生无关信息，并且它们自相矛盾或与对话伙伴矛盾大约 10% 的时间。随着该领域的快速改进，这些错误率在新发布的模型中可能会降低，因为我们的评估进行。然而，这更说明了追求可靠且精确的评估指标以比较模型的重要性。我们希望 ABC 评估能被该领域的其他人士作为朝此方向迈出的有意义一步，我们期待看到未来几个月和几年对话人工智能的进步。谢谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是Vasudha，来自斯通尼布鲁克大学的计算机科学博士候选人。我想在ACL 2023会议上，以长论文的形式展示我们被接受的研究《转移学习用于不和谐检测》，解决稀有类别挑战。我们首先定义认知不和谐，并解释为什么它在语言研究中是一个重要问题。简单来说，认知不和谐是指两个不一致的信念或行为。例如，一个人说：“我知道香烟可能杀死我”，然后又说：“会议结束后，我拿了几支烟。”这里的信念和行为是不一致的，处于不和谐状态。此外，他接着说：“我认为没有它们我可能无法保住工作”，为第二次行为提供了理由，两者之间形成了和谐关系。虽然不和谐是我们在日常决策中非常常见的一种现象，但在语言中与其他类型的语篇关系相比，它们确实非常稀少。那么，这为什么重要？研究认知不和谐可以帮助我们理解人们之间不同意见的影响，追踪人口中信仰、价值观和态度变化的趋势。高度的认知不和谐也与焦虑障碍相关，有助于更好地理解人们的心理健康。研究语言中表达的不和谐也可以有利于理解易受伤害群体的极端主义和两极分化。最后，认知不和谐对于理解个人的认知风格至关重要，有助于我们更好地理解决策过程。为了创建认知不和谐资源，我们对不和谐关系进行了大规模标注。我们采用了一种不和谐优先的方法，如图所示。使用PDTV解析器解析推文，并根据论文中描述的指南对语篇单位对进行标注。如图所示，只有3.5%的标注对中发现了不和谐。在收集了大约1000个语篇单位对的示例后，我们对初始分类器进行了训练，仅使用43个不和谐示例进行训练。不令人意外的是，分类器的性能几乎没有超过随机猜测。由于不和谐的发生率很低，且缺乏任何先前的类似数据集，我们面临着绝对稀缺的问题。为了缓解这一问题，我们实验了转移学习和主动学习的组合，以便在更少的标注运行中收集更多的不和谐样本，降低整体标注成本的同时提高不和谐检测能力。由于初始模型完全无法捕捉不和谐类别，我们通过从紧密相关任务转移权重开始主动学习过程。我们从两个不同的任务转移：主题独立的不和谐立场分类，这个任务判断两个来自不同人的辩论陈述是否一致，无论主题如何，我们称之为辩论；以及对PDTB中的扩展和比较类别进行二元分类，因为这两个类别与和谐与不和谐的概念密切相关，我们称之为CEE。我们发现，在转移后，在标注数据集上的零次拍摄性能已经远远超过了随机猜测，最佳的AUC达到了0.62。此外，在主动学习和标注的迭代更新中，累积将所有来自主动标注的至今收集的数据累积起来，而迭代则通过在最新收集的数据集上训练来更新模型。在不同的策略中，我们发现累积在所有方面表现等于或优于迭代。接下来，为了增加不和谐示例的数量，我们使用稀有类别策略的概率来选择大部分很可能由当前模型在任何一轮主动学习中判断为不和谐的示例。我们将其与其他最先进的策略进行比较，尽管差异较小。请注意，随机策略的性能显著较低。在后续的主动学习轮次中，使用两种最佳策略，我们将分类AUC提高到0.75，这是我们在该任务中取得的最佳性能。我们还检查了每种策略的标注质量和对标注者的成本的可行性。我们发现，PRC具有最高的不和谐百分比，对于稀有类别最有效。然而，标注者也发现这些示例很难。总之，我们发现PRC是一种简单的AL策略，适用于稀有类别的获取和使用适当设计转移学习任务的AL的冷启动，可以提供显著帮助。我们还发现，迭代更新对于从不同领域转移学习有用，而域内主动标注则受益于累积更新。这是我们代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Akshata，今天我的合著者Martin和我一起展示我们的作品《Kipma步骤》，评估来自多个来源的知识集成。这项工作是麦吉尔大学、Mila和微软研究之间的合作。国家语言理解模型利用各种知识来源，例如包含在它们参数中的知识，通常通过预训练获得，以及推理时提供的输入中的知识。最近在问答等任务中，模型可以利用预训练时间知识来解决任务。但自然语言理解通常还需要在推理时提供的知识。例如，在句子“约翰在电视上看到了新当选的总统”中，预训练参数可能包含关于总统做什么和电视是什么的信息，但它们无法可靠地知道这个事件特定实体约翰是谁或新总统是谁，因为总统可能在预训练后已经改变了。因此，成功的知识密集型NLU任务模型需要能够集成和利用预训练时间和推理时间两种知识的能力。\n\n在这项工作中，我们提出了一个知识集成诊断测试套件。我们引入了一个代词指代解析任务，旨在探究从不同来源获取知识的能力。我们使用人类研究参与者和建立的代词指代解析模型对数据集进行评估。以下是我们数据集中的一个例子：\n\nServin是一个法官。Kia是一个面包师。Servin和Kia在公园里见面了。经过在法庭上裁决案件的长工作日，他很高兴放松一下。这里的任务是识别代词“他”指代正确的实体，在这个例子中是Servin。给定代词的解析需要两种信息。首先，实体特定知识，例如Servin是一个法官。其次，背景知识，例如法官在法庭上裁决案件。一般来说，背景知识是在大型语言模型的预训练期间学习的，而实体特定知识通常在推理时观察到。我们变化了这些两个信息的可用性，使其可能在单一来源或多个来源中找到。我们定义了KITMOS的三种设置。首先，我们有典型的背景预训练设置，假设背景知识在预训练时可用。其次，是背景两者设置，背景知识在预训练时间和推理时间都可用。最后，是背景推理设置，两种知识类型只在推理时可用。这个最后设置特别有趣，因为它模拟了背景知识不包含在模型的预训练数据中的情况，例如，因为自预训练以来出现了新的职业。\n\n以下是我们控制事实和真实来源可用性的示例。在背景预训练设置中，我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中。在3英寸时间上下文中，我们提供了实体特定知识“Chichester是一个政治家”。在背景两者设置中，我们不仅在推理时间上下文中提供了实体特定知识，还提供了关于政治家的背景知识。在背景推理设置中，我们提供了虚构的职业“Meritur”而不是政治家，因为Meritur不太可能包含在预训练参数中。\n\n我们使用人类研究参与者和建立的代词指代解析模型对数据集进行评估。在这个图中，我们展示了最难的变体背景预训练设置中表现最佳的模型的结果。在没有针对KITMOS进行任务特定训练的情况下，两个模型都没有表现良好。然而，当在KITMOS上进行训练时，C2F和BFQF两个模型都比随机选择表现显著更好。这表明，当在一般代词指代解析数据集上进行训练时，模型学会利用表面线索，而在测试KITMOS时，这些线索已被移除，因此这些线索没有用处。额外的虚构知识实验表明，即使是表现最佳的模型也无法可靠地集成只在推理时提供的背景知识。\n\n总结我们论文的主要发现，许多代词指代解析模型似乎无法在没有任务特定训练的情况下推理来自不同来源的知识。然而，在任务特定训练下，一些模型成功地集成来自多个来源的知识。尽管如此，即使是表现最佳的模型也似乎在可靠集成只在推理时呈现的背景知识方面存在困难。如果您对更多细节感兴趣，请参阅我们的论文并在GitHub Code上查看数据集。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是特伦托大学和布鲁诺·凯瑟尔基金会的萨拉·帕皮，我将简要介绍一下我们与马特奥·内格里和马可·图尔基合作的一篇论文《注意力作为同时语音翻译的指南》。\n\n什么是同时语音翻译？同时语音翻译（SimulST）是指将口语实时翻译成另一门语言的文本，实现跨语言交流。当前同时语音翻译模型存在哪些问题？通常需要训练特定的架构，引入额外的优化模块。例如，训练过程复杂而漫长，涉及不同的优化目标，并且需要训练和维护多个模型以达到不同的延迟水平，如训练一个平均延迟1秒的模型，另一个延迟2秒的模型，以此类推。\n\n那么我们的解决方案是什么？首先，我们使用现有的离线语音翻译模型，无需重新训练或采用特定架构。我们使用一个模型来处理所有延迟水平，并通过特定参数控制延迟。我们利用模型通过音频输入和文本输出之间的注意力机制（即交叉注意力机制）已经学到的知识。\n\n我们的解决方案是提出编码器-解码器注意力（EDAT），这是一种策略，我们根据注意力指向决定是否输出部分翻译。如果注意力不集中，即总和低于某个阈值α，指向较少的语音帧，意味着接收的信息足够稳定，则输出一个词。例如，如果我们接收一个包含“我要谈论”的语音片段，我们的模型预测德语翻译，我们观察交叉注意力权重，会看到前两个词指向最早接收的语音帧，而最后一个词指向最后接收的语音帧，即λ语音帧。这意味着前两个词将被输出，而由于交叉注意力总和高于阈值α，我们不会输出最后一个词，而是等待下一个语音片段。\n\n如果我们继续接收下一个语音片段，模型预测另外三个词，观察交叉注意力权重，会发现没有词指向最后的λ语音帧。这意味着这三个词将被输出。\n\n在主要结果中，我们将同时语音翻译结果绘制在图表上，蓝色一侧测量翻译质量，平均滞后是延迟的度量，我们还考虑计算感知平均滞后，包括模型预测输出的计算时间。我们希望我们的曲线在这个图表中尽可能高，同时也希望它们向左移动。我们与也适用于离线模型的适当策略（即湿键策略和局部一致性）进行比较，还与专门针对同时语音翻译设计的最新架构进行比较。\n\n这些是德国同时语音翻译策略的所有结果，我们可以看到ADAT超越了所有应用于离线模型的策略，因为曲线向左移动。我们还看到，如果考虑实际经过的时间或计算感知时间，ADAT是最快的策略。\n\n如果您想了解更多结果，请阅读我们的论文。我们还开源发布了代码、模型和同时输出，以促进我们工作的可复现性。谢谢大家的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是朱恒。今天我将展示我们的研究论文，题为《2003年卡诺命名实体标注器在2023年是否仍能良好运行？》让我们开始吧。我们的研究论文探讨了使用命名实体识别任务（NER任务）进行泛化能力的问题。我们观察到，模型已经使用了近20年的卡诺2003年数据集来开发NER，这自然地引发了几个问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么来实现良好的泛化能力？同时，如果我们发现泛化能力较差，导致这些模型性能下降的原因是什么？为了研究这些问题，我们开发了卡诺++数据集。这是一个我们从路透社2020年新闻中收集并根据卡诺2003年标注规则进行标注的数据集。然后，我们在卡诺2003年数据集上微调了20多个模型，并在两个数据集上使用F1分数评估了每个模型的泛化能力。那么，实现良好泛化需要什么？通过我们的实验，我们发现有三个主要因素：\n\n1. 模型架构。我们发现，变压器模型通常能更好地泛化到新数据。\n2. 模型大小。我们发现，通常较小的模型能带来更好的泛化能力。\n3. 微调示例数量。我们还发现，更多的微调示例也能导致更好的泛化能力。\n\n关于下一个问题，导致某些模型性能下降的原因是什么？我们有两个假设。第一个是适应性过拟合，即由于反复使用相同的测试集而引起的过拟合，通常表现为新测试集上的收益递减。第二个假设是时间偏移，即由于训练数据和测试数据之间的时间差距不断扩大而导致的性能下降。对于适应性过拟合，我们从右图中看到，最佳拟合线的梯度大于1，这意味着在卡诺2003年数据集上的每一次改进，在卡诺++数据集上都能得到超过一次的改进，即没有收益递减。这表明适应性过拟合在这种情况下没有观察到。那么，时间偏移呢？我们发现，随着时间差距的扩大，性能确实会下降，这证实了我们的时间偏移假设。\n\n我们的结论是，为了实现良好的泛化能力，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些因素相互关联，不能只关注其中一个。同时，我们还发现，性能下降是由时间偏移引起的，令人惊讶的是，并不是由适应性过拟合引起的，尽管卡诺2003年数据集已经使用超过20年。回到我们论文提出的问题，2003年卡诺标注器在2023年是否仍能运行？我们发现答案是肯定的。我们希望我们的论文能呼吁更多关于如何改进模型泛化能力的研究。最后，请务必查看我们的论文和数据集，如果有任何问题，欢迎与我联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "您好，欢迎参加我们关于 d.plain 的演讲，这是一个用于德国文本简化的新语料库，可以在文档和句子级别上使用。我叫 Regina Stodden，将引导您完成演讲的第一部分。首先，让我们定义一下文本简化。文本简化是一个适应文本的过程，旨在提高特定目标群体对文本的理解能力，例如阅读有困难的人或非母语使用者。为了训练文本简化模型，我们需要平行文本对，例如文档或句子对。在下面的例子中，您可以看到一个复杂德语句子和其简明语言翻译的平行对齐句子对。简化句子有不同的技术，如您在例子中看到的，例如词性替换、词语省略、词语省略重新排序或插入词语。\n\n我们现在提出新的语料库 d.plain。近年来，现有的语料库存在一些问题。例如，这些语料库太小，无法用于训练分类模型。近年来提出的其他三种模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了新的语料库 d.plain，分为两个子语料库：d.plain-apa 和 d.plain-web。d.plain-apa 基于使用文本，在简单 APA 中，我们手动对齐了 483 个文档，结果大约是 30,000-13,000 个平行句子对。对于 d.plainWeb，这个语料库涵盖了不同领域，我们也手动和自动对齐方法对齐了所有 750 个文档。总共有 30,450 个句子对。\n\n我们对句子对进行了更深入的分析，例如简化类型。正如您在这里所看到的，圣经文本的简化程度远高于新闻文本或语言学习者文本，在词性简化、结构简化或整体简化水平等所有方面都是如此。此外，您可以看到我们的 d.plain 语料库具有不同的简化转换优先级。例如，在 d.plain API 语料库中，我们有更多的重新排序和词语添加，而 d.plain web 语料库中则更多地进行改写。\n\n现在，让我们看看这个语料库能做什么。您好，我是 Omar，现在我将讨论我们的数据集 D-plain 的使用案例。对于第一个使用案例，我们可以评估自动对齐方法。近年来，出现了许多对齐方法，但在机器翻译的背景下，我们有两个平行文档，用不同语言编写，我们希望提取两个平行文档中句子的对齐，它们具有相同的语言和内容，但复杂程度不同。现在，由于我们有了手动对齐的 d.plain 数据集，我们可以将这些句子作为黄金标准对齐来评估一些提出的对齐方法，我们对这些方法进行了某些适应，并在论文中发布了所有这些适应和运行实验的代码。最后，我们得出结论，用于德国文本简化的最佳自动对齐方法是 math align 方法，您也可以在论文中找到运行此方法以对齐您自己文档的代码。\n\n我们在论文中展示的第二个使用案例是自动文本简化，通过微调语言模型从复杂输入文本生成简明文本。我们微调了两个不同的模型。我们基于 Long-impart 生成句子级别的简化。您还可以查看所有检查点，并在论文中详细了解实验的分数和评估指标。我们得出结论，这种基本的微调可以产生或获得比基线分数更好的分数，我们将这些结果作为未来自动文本简化问题的基准提出。\n\n非常感谢您的关注，希望在会议上能见到你们所有人。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是来自复旦大学的袁思宇。我今天要介绍我们的研究成果——《从大型语言模型中提炼脚本知识以用于约束语言规划》。在日常生活中，人类经常通过遵循保证脚本中的逐步指令来规划行动。然而，之前的研究主要集中在抽象规划上。一个好的规划器应该能够为第一次规划的目标编写脚本。一个抽象的目标可以由具有多方面约束的不同现实生活中的具体目标继承。一个好的规划器应该编写既合理又忠实于约束的脚本。在本文中，我们首先评估并改进大型语言模型的约束语言规划能力。由于没有具体目标的数据集来支持我们的研究，我们必须首先获取这些目标。如表所示，我们使用InstructGPT扩展了人类在环数据获取中的抽象目标，增加了多方面的约束。我们采样了100个具体目标并评估了大型语言模型生成的脚本。该表报告了结果的总体准确性。我们发现所有大型语言模型在具体目标规划上都取得了不令人满意的结果。然后，我们进行详细分析以调查模型学习失败的原因。图中的结果显示，生成的脚本在语义完整性方面是可以接受的，但对约束的忠实度无法保证。我们深入研究了约束的更细粒度主题类别，并根据工作方式进行分类。图中的热图显示，指示性TPDs的规划性能在不同类别的约束上差异显著。之前的研究表明，轻量级模型的输出质量具有高方差，导致性能不佳。因此，我们采用了过生成Z-filter的想法来提高生成质量。我们首先展示了InstructGPT中的约束类型及其示例，并基于种子抽象目标获得了具体目标。然后，InstructGPT为具体目标过生成关键脚本。接下来，我们开发了一个过滤模型来选择可行的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似性和相似度分数以衡量语义相似度。此外，我们对包含目标约束关键字的脚本给予奖励。我们仅在目标在目标集中得分最高时保留该脚本。使用我们的方法，InstructZBT可以生成更高质量的脚本。我们的方法在语义完整性和对约束的忠实度方面都大大提高了规划能力。由于部署大型语言模型的成本高昂，因此有必要使更小、更专业的模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。然而，之前的研究没有实现具体目标的规划，而手动数据集标注成本高昂。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏约束语言规划数据集。我们应用我们的方法构建了一个约束语言规划数据集，称为CodeScript。总共，我们生成了55,000个具体目标和脚本。为了确保验证集和测试集的质量，我们要求云端工人找到并修正错误样本。该图显示了CodeScript的约束分布。我们发现CodeScript在生成的具体目标中具有高多样性。使用CodeScript，我们可以处理更小但更专业的模型进行约束语言规划。我们发现Antune在成本率上的TFI可以生成0的平方根。使用CodeScript，我们可以处理更小但更专业的模型进行约束语言规划。我们发现T-file函数在CodeScript上可以生成比大多数大型语言模型更高质量的脚本，这表明当在合适的数据集上进行适当训练时，较小的模型可以支持较大的模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了一种过生成过滤方法用于语言规划研究。谢谢您的时间。请在我们的论文中了解CodeScript的更多细节。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是Yanis Lavrac，我将向您展示我们在Dr. BERT方面的工作，这是一个针对生物医学和临床领域的强大预训练法语模型。在这次演讲中，我们首先将讨论医疗领域的语言建模。然后，我们将介绍我们文章的主要贡献。我们推出了第一个法语生物医学模型，名为Dr. BERT，该模型基于Roberta，并在NACHOS上进行训练，这是一个从网络上抓取的医疗数据集。我们还引入了对模型在多种预训练设置和数据源的比较。接下来，我们将展示我们在11个法语生物医学和临床下游任务上的结果。最后，我们将总结实验并详细说明如何访问模型。\n\n自2018年发布以来，BERT已成为解决自然语言处理任务最有效的手段之一，与历史静态和上下文方法如Word2Vec、FastText或NWO相比，性能有了显著提高。此后，该模型已被适应到许多其他语言，如法语的Camembert，以及生物医学的Permt-BERT和Bio-BERT，临床的Clinical-BERT，但主要是在英语方面。专门针对其他语言的模型非常稀缺，通常由于缺乏域内数据而基于连续预训练。然而，法语在现在之前没有开源的生物医学模型。因此，我们问自己，对于广泛的使用范围，最合适的数据来源是什么，以及当前的数据是否能很好地替代临床数据。为了回答这个问题，我们将Dr. BERT与我们的Schubert模型进行比较，后者基于从我们医院获得的匿名数据。我们还问自己，训练一个法语专业模型需要多少数据？是4GB、8GB还是4GB的RAM？Schubert的第一个版本是一个临床模型，基于4GB来自临床笔记的句子。Schubert的最终版本则混合使用了4GB的NACHOS子集和4GB的临床笔记。\n\n除了这一比较，我们还引入了三个基于连续预训练的模型，以分析预训练策略的影响。一个基于Camembert的权重，并在4GB的NACHOS子集上进行训练。另一个也基于Camembert，但使用4GB的Permt-BERT、BioBERT和ClinicalBERT进行训练。评估结果表明，模型在模型训练数据与任务数据性质相同的情况下表现最佳。然而，我们可以观察到，来自异质来源的数据似乎更具多功能性。我们还观察到，使用更多数据会带来更好的性能。总体而言，从零开始的预训练在大多数任务上似乎获得了更高的性能。然而，我们使用Permt-BERT的权重和分词器，在4GB的NACHOS子集上进行的连续预训练实验，与从零开始训练的Dr. BERT 4GB获得了可比的结果，而基于Camembert权重和分词器的模型则存在稳定性问题。\n\n最后，作为结论，我们的专有系统在11个下游任务中的9个上表现更好，整体上超越了通用模型（这里是Camembert）的结果。我们还观察到，更专业的数据更好，但可扩展性差。所有从NACHOS获得的预训练模型都在UGIMFACE上免费提供，所有训练脚本都在我们的GitHub仓库中。因此，感谢您的聆听，我们期待在多伦多海报会议上与您交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是香斌，华盛顿大学博士生。今天我将介绍我们从预训练数据到语言模型再到下游任务的研究工作，追踪导致不公平自然语言处理（NLP）模型的政治偏见的轨迹。因此，语言模型是在大规模网络爬虫数据上训练的。政治新闻媒体在他们的预训练数据中得到了很好的覆盖。根据对C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。这为语言模型的应用带来了机遇和挑战。一方面，它们能够从多样的视角中学习，庆祝民主和思想的多元化。另一方面，这些不同的政治观点本身带有社会偏见，可能导致下游任务应用中的潜在公平问题。为此，我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体通过提出以下问题。首先，如何评估语言模型的政治倾向，以及相关数据在这些政治偏见中可能扮演什么角色？其次，具有不同政治倾向的语言模型在下游任务上的实际表现如何，这是否会在NLP应用中导致公平问题？具体来说，我们首先提出使用政治问卷（如政治罗盘测试）以不同的提示格式提示语言模型。这确保我们在政治科学文献的基础上进行自动评估。一些初步结果表明，语言模型确实具有不同的政治倾向，它们占据了政治罗盘上的所有四个象限。我们还可以看到，GPT-4是最自由的语言模型，GPT理论也普遍比BERT理论及其变体更自由。其次，我们旨在调查语言模型的政治偏见实际上在多大程度上来自训练数据。因此，我们进行了一个控制实验，进一步在六个不同的党派语料库上预训练语言模型检查点，这些语料库分为新闻和社交媒体，并根据其政治倾向进一步细分。通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，对于Roberta，进一步微调，在倾向左派的Reddit语料库上进行训练，我们可以看到它在政治偏见方面显着地向自由主义转变。我们还试图调查语言模型是否能捕捉到现代社会中普遍存在的极化现象。我们将预训练语料库分为美国第45任总统之前和之后，并在两个不同的时间语料库上分别预训练语言模型。我们可以看到，语言模型在2017年之后通常具有更偏离中心的政治倾向。这表明语言模型也能捕捉到社会中的极化现象。最后，我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等NLP应用中的表现，这些应用通常涉及语言模型，并且可能具有非常重要的影响。我们发现，如果调查每类的性能，即如果我们将性能分为不同的人口统计或新闻媒体的政治含义，我们可以看到一个模式，例如，在仇恨言论检测中，倾向左派的语言模型更好地检测到针对社会少数群体的仇恨言论，但更差地检测到针对社会更强大群体的仇恨言论。相反，倾向右派的语言模型更好地检测到针对白人男性的仇恨言论，但更差地检测到针对黑人、LGBTQ+和其他少数社区的仇恨言论。在假新闻检测中，也出现了类似的趋势，我们看到倾向左派的语言模型更好地检测到来自相反政治倾向的误导信息，反之亦然。我们进一步提供了许多定性示例，以展示具有不同政治倾向的语言模型根据其社会类别对仇恨言论和误导信息示例给出不同的预测。附录中还有更多示例，以进一步强调这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平问题。例如，如果倾向右派的语言模型被微调用于仇恨言论、误导信息等，并部署到一个流行的社交媒体平台，这意味着持有相反政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会肆意传播，毫无控制。因此，这为我们敲响了警钟，需要认识并解决语言模型政治倾向导致的公平问题。\n\n稍微讨论一下。我们也想强调我们揭露了语言模型政治偏见的独特困境。这就像在斯克拉和卡里布迪斯之间选择。如果我们不清理语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们尝试以某种方式进行清理，我们也将面临审查或排斥的风险，并且很难确定训练语言模型数据中真正中立的内容。这就像电动车问题。好了，我想今天就这些了。谢谢大家的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Kostav Sinha，很高兴欢迎大家参加我们关于ACL 2023论文的演讲，题目是《语言模型可接受性判断不总是对上下文稳健的》。这是一项与John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams的合作研究。在这项研究中，我们重新审视了最小对范式。最小对范式基本上是在可接受性判断的基础上评估语言模型，这些判断也可以包括语法性，比如BLIMP、语法GEM或在刻板印象方面的可接受性，例如Krauss对。在最小对范式中，评估语言模型的典型方法是展示一个可接受的句子或一个语法句，然后展示一个不可接受的句子或一个不语法句。整个模型的过程基本上给可接受的句子分配更高的概率。当前的MPP管道基本无法让我们评估模型对较长句子的接受程度。如今，大型语言模型出现了越来越长的上下文窗口。因此，我们重新审视了数据集本身，然后通过选择可接受或不可接受的句子从这些数据集中重新创建句子。例如，我们从BLIMP数据集的附加岛案例中选择了一个典型的语法性对。我们所做的是，为了重新创建更长的序列，我们从附加岛中提取语法句子，并将其作为前缀添加到可接受的查询和不可接受的查询中。我们也可以通过从相同的匹配中选择不可接受的句子来做到这一点，这也可以用于测试模型的可接受性。我们还可以通过从不同的子集或不同的数据集中选择句子来做到这一点。这就是我们所说的不匹配场景。在这里，句子仍然来自相关数据集，但不是您正在评估的相同数据集。我们也可以为可接受性案例做同样的事情。最后，我们可以从完全不相关的领域选择句子，例如维基百科。这将告诉我们模型的可接受性判断是否实际上受到任何与我们正在查看的句子完全无关的上下文的影响。\n\n模型表现如何？首先，我们看维基百科的句子，这些句子与当前的查询对完全无关。在那里，我们发现MPP判断在任意上下文长度下大多是稳健的。我们将上下文长度增加到1024以最大化OPT和GPT-2模型。正如我们在这里看到的橙色虚线，MPP判断相对稳定。当我们从相同数据集选择句子时会发生什么？在这里，我们从可接受和不可接受的领域创建句子，这些领域来自相同的BLIMP或语法GEM数据集。在那里，我们看到MPP判断在添加可接受前缀或不可接受前缀时显著增加或减少。但当我们匹配结构时，即当我们选择来自blame-person-text-gym相同现象的句子时，我们看到模型的MPP判断大幅增加或大幅减少，具体取决于所选择的前缀是否可接受。这个效果随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新语言模型。\n\n匹配前缀为什么会如此影响语言模型的判断？我们进行了一系列分析，试图通过保留相关结构但向输入添加噪声来调整输入句子。在进行了几次扰动后，我们发现这些噪声中没有一个实际上让模型改变其MPP判断趋势。基本上，我们发现模型对扰动句子的敏感度相似。即，当我们扰动可接受领域的句子时，所有扰动都会看到类似的增加；当我们扰动不可接受领域的句子时，MPP判断会以类似的方式减少。\n\n我们研究的关键结论是，语言模型对跨句子的潜在语法和语义特征敏感。当前，我们用短句或单句输入进行MPP评估，可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以了解更多实验细节。谢谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是德国萨尔兰大学博士生大威。在这个视频中，我想向大家介绍我们最近的研究成果《比你想象的更脆弱》，这是一项对弱监督学习的批判性研究。这是我与尤生肖、马里奥·斯穆斯巴赫、吉娅·斯特芬以及迪特里希·克拉科共同完成的工作。我想从简要介绍弱监督和弱监督学习开始。在弱监督中，我们不会手动标注数据。相反，我们使用弱标注源来标注数据，例如简单的启发式规则、知识库或低质量的众包标注，正如右图所示。与人工标注相比，弱标注成本更低，但同时也存在噪声，即一定数量的标注是不正确的。如果我们直接在弱标注数据上训练神经网络，神经网络倾向于记住标注噪声，而无法泛化。在弱监督学习中，提出了训练算法，以在这种标签噪声下稳健地训练神经网络，使训练后的模型仍能良好地泛化。在最近的弱监督学习研究中，一个常见的说法是，人们声称他们只在弱标签数据下训练模型，并在干净的测试集上实现了高性能。从技术上讲，这个说法并不错误，但存在一个问题，即三个研究问题。首先，我们是否需要干净的验证数据？最后，我们是否应该仅使用干净样本进行验证，还是有更好的利用方式？我们在工作中回答了这些研究问题，我们的发现如下。首先，有趣的是，我们发现最近的弱监督学习方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，那么训练后的模型无法超越原始的弱标签，这意味着训练是无意义的。这表明弱监督学习方法实际上需要干净的标注数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于弱监督学习方法实现更好的性能，如左图所示。通常，我们只需要每类20个样本就能达到高性能。但故事并未结束，因为如果我们决定使用干净样本进行训练，那么直接训练它们甚至能实现更好的性能。红图显示了直接应用在干净数据上的微调方法与仅将干净数据用于验证的弱监督学习方法之间的性能差异。正如我们所见，如果每类有10个样本，直接微调开始超越弱监督学习方法。最后，之前弱监督学习方法声称的性能提升可以通过允许继续在干净验证样本上进行微调轻松实现。从图中我们可以看到，范林登模型（称为W）最初在性能上落后于更复杂的弱监督学习方法，如余弦。然而，如果允许在干净样本上继续微调，那么FTW的表现与其它方法一样好。所以在实践中，没有理由选择需要更多计算时间和磁盘空间的更复杂弱监督学习方法。总之，我们表明最近的弱监督学习方法需要干净的人工标注样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否在干净验证样本上进行。第二，弱监督学习方法应与少样本学习基线进行比较，并在干净样本上进行工作。第三，连续微调是一个简单而强大的基线，应在未来的弱监督学习研究中得到考虑。最后，我们开源了我们的代码。你可以通过本幻灯片上的二维码找到它。请随时查看。谢谢，祝大家享受大会。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是大卫·维拉尔，我将简要介绍一下论文《Grunting Parm》中关于翻译的策略和表现的评估。这是我与谷歌翻译团队同事的合作研究。Parm是一个拥有5400亿参数的大型语言模型，于去年，即2022年发布。它训练了包含7800亿个标记的大量文本数据。在发布时，它在数百个自然语言处理任务中达到了最先进水平。在这项研究中，我们提出了对大型语言模型翻译提示的首次系统研究。我们使用国际机器翻译协会（IMT）的最佳实践来评估这些模型的翻译能力。这涉及到使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。我们比较了两个最先进的系统，即在WMT评估中表现最佳的系统。我们使用了最先进的神经机器翻译指标，同时还展示了基于专家的人类评估结果。最后，我们提供了关于提示选择策略的一些建议。\n\n提示对大型语言模型的翻译性能有很大影响。在我们的一个简单实验中，我们使用了单次提示，并为每句话提供了两个不同的提示，结果显示，在1000句话中，有516句话的差异超过了一个模糊点，在极端情况下，可达40个模糊点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了五次提示策略，即在提供给系统的每句话中标注其语言。例如，在从德语到英语的翻译中，我们用德语冒号标注德语句子，用英语冒号标注英语翻译。我们发现，在多次提示的情况下，提示的实际形式影响不大。它在零次和一次提示中至关重要，而当我们采用五次提示时，提示的实际形式几乎没有影响。例子承载了大部分权重。\n\n我们的实验结果总结是，例子质量比源句子的相似性更重要。因此，从高质量的翻译中选择例子非常重要。特别是，我们比较了从WMT评估的训练数据或开发数据中选择提示。开发数据比训练数据更精心整理，质量更高，训练数据更噪声，结果显示使用开发数据时表现更好。然而，最先进的专业系统在翻译质量上仍比Parm有显著优势，但Parm已经非常接近商业系统了。在我们的案例中，我们选择与谷歌翻译进行比较。\n\n我们使用MQM框架对电子邮件进行分析的洞察是，Parm的流利度可与最先进的系统相媲美，但准确度存在差异。具体来说，最常见的错误是遗漏错误。似乎Parm有时通过丢弃源句子中在翻译中没有表现出来的部分来生成听起来更好的翻译。然而，Parm的风格尴尬类别低于最先进的系统，这进一步表明Parm提供非常流利的输出，但准确度仍存在一些问题。\n\n以上就是这次简短的概述，更多细节请参阅论文的完整演讲。谢谢大家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的易精伟。很荣幸能通过这段简短的宣传视频向大家介绍我们的论文《你正在复制我的模型吗？通过后门水印保护大型语言模型的嵌入和服务版权》。首先，让我们介绍一下嵌入和服务相关的背景知识。目前，像GPTT、LAMA、PALM这样的大型语言模型在自然语言理解和生成方面表现卓越。嵌入作为服务是建立在大型语言模型基础上的服务之一，用于协助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入作为服务的版权显得尤为必要。\n\n为了保护嵌入作为服务的版权，一种解决方案是在提供服务中嵌入水印，并检测其他服务是否包含该水印。水印方法需要满足以下属性：首先，该方法应适用于嵌入作为服务；其次，水印不应降低所提供嵌入的实用性；第三，水印对攻击者来说应该足够隐蔽，或者攻击者无法轻易去除水印；最后，在模型提取过程中，水印需要转移到攻击者的服务中。\n\n现有研究可以大致分为四类。然而，这些方法要么不适用于嵌入作为服务，要么缺乏可转移性。下面详细介绍我们的EmbeddingMarker。EmbeddingMarker包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发词集。触发词集是一组频率区间适中的词语。我们假设提供者可以收集一个通用文本语料库并计算词频。在水印注入过程中，我们首先定义一个目标嵌入。当用户向提供服务发送句子时，提供者计算句子中的触发词数量。所提供的嵌入是目标嵌入和原始嵌入权重之和。目标嵌入的权重与句子中的触发词数量成正比。当句子中的触发词数量大于m时，所提供的嵌入与目标嵌入完全相同。\n\n版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词语都属于触发词集的句子，而良性数据集中的句子则不包含触发词集中的任何词语。然后，提供者使用这些数据集向窃取服务请求嵌入。计算请求的嵌入与目标嵌入之间的余弦和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，分别定义为delta余弦和delta L2。同时，我们还应用KS检验，并使用其p值作为第三个度量。\n\n我们在AGnews、Mind、SSD2和Eraspam四个数据集上进行了实验。我们假设提供者使用Wikitext数据集来计算词频。四个数据集的结果表明，我们的嵌入标记器在保持下游任务实用性的同时，可以具有出色的检测性能。我们还通过PCA可视化四个数据集句子的嵌入，验证了所提供嵌入的隐蔽性。图中的图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。\n\n谢谢大家。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Ying，我和我的同事 Zhiyang 将向大家展示我们关于“多改进：通过指令调优改进多模型串行短学习”的研究。随着大型语言模型的进步，许多研究开始探索新的学习范式，以高效的方式重用预训练语言模型来处理不同的下游任务。最近，许多研究表明，指令调优使大型语言模型能够通过遵循自然指令以零样本的方式执行未见过的任务。然而，之前关于指令调优的大部分工作都集中在改进语言仅限任务的零样本性能上，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想调查多模态预训练模型上的指令调优是否能实际提高对未见过的多模态任务的泛化能力。此外，在我们研究当时，我们发现 NLP 和多模态之间指令数据集的可用性存在显著差异。存在超过 1,600 个仅限语言的指令任务，然而，没有大规模公开可用的多模态指令任务。因此，这促使我们构建一个多模态指令调优数据集。这里我们展示 MultiInstruct，第一个多模态指令调优基准数据集，它包含 62 个多样化的多模态任务，涵盖 10 个广泛类别。这些任务来源于 21 个现有的开源数据集，每个任务配备 5 个专家编写的指令。为了在我们提出的数据集上研究多模态指令调优，我们采用 OFA，一个统一的多模态预训练模型，作为我们的基础模型。OFA 使用统一的词汇表来表示语言、图像令牌和边界框的坐标。这里我们展示一些来自我们多指令数据集的示例实例。为了统一处理各种输入和输出数据类型，我们遵循 OFA 的方法，将所有任务制定为统一的序列到序列格式，其中输入文本、图像、指令和边界框在相同的令牌空间中表示。好了，现在我将讨论多模态指令调优。对于训练数据集，我们使用 9 组中的 53 个任务进行训练，每个任务采样 10,000 个实例。在测试中，我们保留整个常识推理组用于测试，并从 VQA 和杂项组额外选择五个任务。我们使用每个任务的测试分割中的所有实例。此外，我们从自然指令的测试分割中随机采样 20 个任务作为 NLP 的未见任务。我们使用预训练的 OFA 大模型作为基础模型。在训练过程中，我们混合所有任务的所有实例。每个实例随机与五个指令模板中的一个组合。在每个任务的测试中，我们总共进行五个实验，通过使用每个实验中的五个指令之一来评估模型。我们报告所有五个实验中的平均值、最大性能和性能的标准偏差。如果任务是多模型分类任务，我们报告准确率。如果它是多模型生成任务，我们报告根 L。对于 RP 任务，我们也报告根 jl，并且我们还引入了一个额外的评估指标称为敏感度，它测量模型在指令措辞略有变化时是否能一致地为相同任务产生相同输出。这是我们的主要结果。正如我们所看到的，指令调优可以显著提高 OFA 在场景多模型任务上的性能。此外，从自然指令数据集进行迁移学习可以受益于指令调优。这里我们可以看到，随着任务数量的增加，模型达到更高的性能，同时敏感度降低。我们还进行了一个实验，使用一个指令与五个指令进行比较。正如我们所看到的，使用更多指令可以提高模型的总体性能并显著降低其敏感度。这展示了不同的微调策略对模型敏感度的影响。正如我们所看到的，通过从自然指令数据集进行迁移学习，模型可以达到比原始 OFA 模型更好的敏感度。我们还可以看到，从自然指令数据集进行迁移学习可以帮助 OFA 在 Nitro Instruct 数据集上达到更好的性能。总之，我们提出了第一个大规模多模型指令调优数据集。我们显著提高了 OFA 的零样本能力，并探索了不同的迁移学习技术并展示了它们的益处。我们设计了一个新的指标称为敏感度。再者，我们正在收集一个更庞大的多模态指令调优数据集，包含大约 150 个额外的变体语言任务，我们将很快发布。这是我们数据和模型的二维码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自宾夕法尼亚州立大学的张宇生。今天我将介绍我们的研究成果——《多种自然语言和语义表示的跨语言语义分析》。语义分析是一项构建用户查询的语义表示的任务，例如SQL和lambda演算。跨语言语义分析的任务是将多种自然语言的查询翻译成多种语义表示。如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义分析模型是分别在有限的任务和应用数据集上提出和评估的。例如，某些自然语言缺乏覆盖，中文缺失；由于某些微表示的覆盖范围，lambda演算缺失或仅在某种神经模型上进行评估。例如，只有一个单一的模型来评估它们。因此，我们提出了Examplar。我们提供了一个统一的数据集Examplar，用于多种自然语言和语义表示的跨语言语义分析。它包含来自各个领域的九个数据集、五个语义分析任务、八种语义表示以及22种自然语言，涵盖15个语言家族。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语查询上训练英语模型。在推理阶段，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试了单语模型。在这个设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试了单语少样本设置，通过仅使用10%的训练数据训练单语模型。我们测试了多语种模型，即训练一个多语种模型来处理所有语言。例如，我们将德语、英语、中文查询放在一起训练一个多语种模型。在推理阶段，我们可以使用该模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一种源语言上进行训练，然后转移到另一种语言。因此在训练阶段，我们在英语查询或英语和德语少样本查询的组合上训练一个多语种模型，并预测SQL输出。我们还发现了许多有趣的结果。关于单语模型的分析，我们评估了两组模型，包括编码器PDR（多语种预训练编码器与指针解码器，如XLMR加PDR和BERT加PDR）和编码器-解码器模型（多语种预训练编码器-解码器模型，如MBART和MT5）。我们发现编码器-解码器在所有九个数据集上取得了最佳性能。我们在多语种设置下评估了MT5和XLMR加PDR，发现编码器-解码器或编码器-PDR通过混合各种语言的训练可以得到改进。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集上的性能下降，仅在三个数据集上提升。我认为这被称为多语种的诅咒。我们还比较了跨语言性能差距。图中蓝线表示跨语言少样本迁移，橙线表示跨语言零样本迁移，绿线表示单语设置。通过比较绿线和橙线，我们发现在零样本设置下，跨语言迁移性能差距显著。通过比较蓝线和橙线，我们发现在少样本设置下，迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，编码器-解码器优于之前的工作或取得了可比的结果。在英语自然语言上的描绘可以显著提升目标自然语言的少样本性能。我们发现多语种语言模型如CODIS和BLUE对于跨语言语义分析任务仍然足够。总之，我们构建了Examplar，一个统一的基准平台，用于多种自然语言和主要语义表示的跨语言语义分析。我们对三种代表类型的多语种语言模型进行了全面的基准研究。我们的成果显示了许多有趣的发现等。欢迎访问我们的论文和代码。谢谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我的名字是亚当·什皮尔科夫斯基，这次演讲的主题是并列结构的依赖关系。如您所知，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依赖中，并列结构“丽莎、巴特和玛吉”的结构是这样的：第一个并列词是整个并列结构的头部，在这种情况下是丽莎。伊戈尔·米尔丘克的意义文本理论也采取了类似的观点，整个并列结构再次由第一个并列词主导。这两种方法是不对称的，对吗？它们突出了其中一个并列词。现在，也有像布拉格方法那样对并列结构采取对称方法，在布拉格依赖语树中假设的并列词主导方法，其中并列结构由并列词主导。因此，我们从治理词到所有并列词得到依赖关系。最后，还有例如在迪克·哈德森的词法语法中使用的多头部方法，可以说，所有并列词都是并列结构的头部，因此，我们从治理词这里爱到所有并列词分别得到依赖关系。\n\n本文的目的是提出两种反对并列结构不对称的方法，如上述两种。论证基于依赖长度最小化原则，我将通过这些例子来解释。所以，在英语中，如您所知，直接宾语倾向于靠近动词，而状语可以更远，对吗？例如，“丽莎昨天读了它”是可以接受的，因为直接宾语“它”靠近动词。而“丽莎读了昨天它”则不那么自然，因为在这里，动词和直接宾语之间有一个状语“昨天”。然而，当直接宾语非常沉重和长时，这种影响可能会减轻，因为它可以移动到状语之后的位置。这在下面例子中得到说明。所以，这两个句子都是可以接受的：“丽莎读了这个绝对迷人的关于BCS的书。”和“丽莎昨天读了这个绝对迷人的关于蜜蜂的书。”这里的推理是，这是可能的，因为尽管这个句子违反了一般语法原则，即直接宾语应该在动词旁边，但它满足了依赖长度最小化原则，该原则表明较短的依赖关系更可取。\n\n这两个树只显示了关键依赖关系的长度，即在两种结构中不是常数的依赖关系。在这里，我们有从“读”到状语的长度为七的依赖关系，从“读”到“书”的长度为四的依赖关系。加起来是11。当你交换这两个成分的位置时，这两个依赖关系的总和变成六，对吗？从11变成6，短得多。这就是为什么这听起来相当不错。它违反了一个原则，但满足了另一个原则。\n\n所以，我们做了什么，我们从增强版的Penn语树库中提取了关于并列的各种统计数据，并解释了我们为什么没有使用通用依赖。这些统计数据证实了之前多次观察到的现象：左并列词倾向于更短。我们没有使用通用依赖，这些统计数据证实了之前多次观察到的现象：左并列词倾向于更短，如“盐和胡椒”而不是“胡椒和盐”，以音节来衡量，以及之前提到的观察结果，这种倾向随着长度差异的增加而增强。当两个并列词的长度差异增大时，较短的并列词更倾向于成为第一个。\n\n然而，本文的新颖之处在于，我们观察到这种倾向仅在治理词在左侧或缺失时发生，如在例子“我看到了巴特和丽莎”中，治理词在左侧；而在“霍默来并打喷嚏”中，治理词缺失，这里有两个并列的动词，没有外部治理词。在这种情况下，左并列词倾向于更短，尤其当两个并列词的长度差异较大时。但是，当治理词在右侧时，这种效果消失了。\n\n我们通过测量长度（以字符为单位，如第一列，以音节为单位，如中间列，以词为单位，如右列）来证明这一点。我将专注于右列。我们在这里看到的是，当治理词在左侧时，左并列词更短的倾向随着绝对词数差异的增加而稳步增长。当治理词缺失时，如在句子并列中，观察到相同的现象。但当治理词在右侧时，这种倾向消失了。我们在论文中展示了这如何为反对如上述两种不对称并列结构和支持如上述两种对称并列结构提供论据。有关完整论证和讨论，请参阅论文，并在海报环节与我们交流。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "你好！我叫Kaio Yin，将向大家展示我们题为《何时翻译需要上下文？基于数据的多语探索》的研究。这项工作是与Patrick Fernandes、Emmy Liu、Andre F.D. Martins和Graham Newbig合作完成的。许多翻译依赖于上下文。例如，在句子中如何翻译“mole”？如果前一句是“如果部长们发现，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，会是什么严重的问题吗？”那么“mole”指的是胎记。根据上下文，词的意义会改变，因此其翻译也会相应地改变。然而，评估模型能多好地翻译这类案例是相当困难的。首先，因为只有少量翻译依赖于上下文，这导致语料库级别的指标如BLEU无法捕捉到这些翻译。有些人建议对上下文依赖的翻译进行定向评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工整理。\n\n在这项工作中，我们尝试回答两个问题。首先，何时翻译需要上下文？其次，模型能多好地处理这些案例？为了回答第一个问题，我们开始测量词在翻译过程中对上下文的依赖程度。在先前的工作中，我们引入了CXMI作为机器翻译模型上下文使用量的度量。这通过测量上下文C在给定源X时对目标Y提供的信息量来实现。可以将CXMI视为给模型提供上下文获得的信息量。在本工作中，我们将CXMI扩展为点CXMI，它可以在句子级别或词级别测量上下文使用量。我们可以将具有高P6MI的词视为需要上下文进行翻译的词。\n\n现在我们分析具有高P6MI的词，以寻找这些词之间的模式。我们在从英语翻译成14种不同语言的TED演讲文本上进行分析。我们在三个不同级别进行分析。首先，我们查看具有高平均PCXMI的词性标签。这使我们能够发现，例如，阿拉伯语中具有相对高PCXMI的双数代词。这可以解释为，因为英语没有双数代词，所以在翻译成阿拉伯语时，需要上下文来确定代词是否为双数。类似地，我们发现某些语言在选择适当的动词形式时也需要上下文。\n\n然后，我们查看在所有不同出现中具有高平均PCSXMI的词汇项。这有助于我们识别像这里这样的案例，在中文中，你需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。类似地，我们发现上下文有助于翻译正确的语气。最后，我们查看具有高PCXMI的不同单个词。这使我们能够识别那些不能由词本身捕捉到的现象，而是表达在句子结构中，如省略号解析。\n\n现在我们利用分析结果来设计一个文档级翻译的基准。对于我们识别的五个话语现象中的每一个，我们创建自动识别与现象相关的词的标记器，我们称之为多语话语感知（MUDA）标记器。我们可以使用标记器识别与现象相关的词，我们称之为多语话语感知（MUDA）标记器。我们可以使用标记器识别与现象相关的词，我们称之为多语话语感知（MUDA）标记器。我们可以使用标记器识别与现象相关的词，我们称之为多语话语感知（MUDA）标记器。我们可以使用标记器识别与现象相关的词，我们称之为多语话语感知（MUDA）标记器。我们可以使用标记器识别与现象相关的词，我们称之为多语话语感知（MUDA）标记器。我们可以使用标记器识别与现象相关的词，我们称之为多语话语感知（MUDA）标记器。\n\n我们可以使用标记器识别与现象相关的词，并使用标记器在我们要用于评估的平行语料库上应用标记器。我们在MUDA标记器识别出的上下文依赖示例上应用我们选择的翻译指标。最后，我们使用我们的基准和其他指标来评估文档级机器翻译的不同模型。首先，当我们使用语料库级别的指标时，对于BLEU，无上下文感知的模型表现最佳，但如果使用COMET，则上下文感知的模型表现最佳。如果使用词F度量，则具有或不具有上下文的模型具有可比的性能。这再次表明，如果仅使用语料库级别的指标，很难确定最佳的文档级翻译系统。\n\n现在我们使用MUDA基准来评估模型，我们发现在语气和词汇连贯性等某些话语现象上，上下文感知的模型准确性显著高于不使用上下文的模型。但这些模型在其他现象如省略号、代词和动词形式上与未使用上下文的模型没有太大差异。这在一定程度上表明了文档级翻译需要更多进步的领域。我们还比较了不同的商业系统，我们的基准显示，DeepL通常比Google Translate更适合文档级翻译。\n\n总之，我们在14个语言对上进行数据驱动的分析，以识别何时翻译需要上下文。然后，我们利用分析结果建立一个文档级机器翻译的基准，它可以帮助我们识别哪些话语现象模型能很好地处理，哪些翻译系统擅长文档级翻译。非常感谢大家的关注。多伦多见！"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基·梅隆大学一年级博士生，今天我将向大家介绍你们的作品《肛位性：描述数据集和模型的设计偏见》。这项工作是与华盛顿大学和艾伦人工智能研究所的一些同事合作完成的，他们是塞巴斯蒂安·桑特、罗南·拉布拉塞、卡塔琳娜·阿兰尼卡和马丁·萨普。让我们从一个场景开始：想象一下，你为一家报纸工作，正在筛选新闻文章下的评论，试图删除有毒内容。你可能会使用像Perspective API这样的流行API来检测有毒内容，如果你是卡尔·琼斯，Perspective API就能正确地检测到有毒实例。但对阿迪提亚·沙尔马来说，情况就不同了，Perspective API对印度背景中更常见的冒犯性术语不够敏感。这就是设计偏见的一个例子，我们看到技术在不同人群之间的系统性性能差异。\n\n像我们刚才看到的这种设计偏见可能发生在自然语言处理（NLP）研究人员和模型开发者的位置性上。位置性是指人们由于人口统计、身份和生活经历而持有的观点。这是一个在批判研究中广泛使用的概念，特别是在女权主义和酷儿学术领域。作为研究人员，位置性会影响研究过程及其结果和结论，因为它会改变研究人员做出的决定。因此，人们可能会问，数据集和模型有位置性吗？我们不是说模型和数据集本身具有人口统计身份和生活经历，但它们确实汇集了真实人士的判断和意见，因此可以代表某些位置性而非其他位置性。\n\n之前的研究提供了位置性的某些轶事证据，例如文化与模型和数据集之间的差距，以及对模型位置性的理论定义。然而，这些工作并没有真正比较最终用户与数据集和模型本身。研究模型和数据集的位置性在NLP任务变得更加主观和社会化时越来越重要。\n\n描述这些位置性如何偏斜具有挑战性，因为并非所有决定都有记录，许多模型隐藏在API后面。为了研究数据集和模型的位置性，我们实际上将真实用户的标注与现有数据集和模型进行比较。我们通过NLP位置性框架来实现这一点。\n\n我们的框架主要分为两个步骤。第一步是使用多样化的标注员重新标注数据集。我们选择这样做，而不是查看原始数据集标注员的人口统计数据，因为通常只有少数标注员标注每个实例，而且人口统计数据很少被收集和共享。因此，我们选择重新标注数据，为每个实例获得大量标注员，并收集到丰富的人口统计数据。\n\n然后，我们根据人口统计数据进行标注，并使用皮尔逊R相关系数与模型和数据集进行比较。因此，我们的框架与标注员分歧文献不同，通过将最终用户与模型和数据集、预测和标签进行比较，而不是仅仅看标注员一致性或建模标注员分布。\n\n我们的框架很大程度上得益于来自人机交互合作者的在线众包平台“Lab in the Wild”。与像MTurk这样的平台相比，主要参与者来自美国或印度，“Lab in the Wild”仍然能够获得高质量的数据。我们在“Lab in the Wild”上托管了两个任务，其中之一是社会可接受性任务。\n\n该任务的运作方式是参与者阅读来自社会化学数据集的场景，然后写下该场景的社会可接受性程度。为了保持参与者的参与度，他们可以将自己的回答与AI和其他人进行比较。然后，我们将这些标注与社会化学、Delphi和GPT-4进行比较。我们为有毒言论和仇恨言论检测任务复制了非常类似的设置。\n\n我们将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。最终，我们的研究收集了来自87个国家的1000多名标注员的16000多条标注。因此，我们现在更好地装备来回答NLP数据集和模型最一致的人群是谁。\n\n我们发现NLP中存在位置性。例如，我们发现数据集与英语国家最一致。在GPT-4社会可接受性分析中，我们发现它最符合儒家思想和英语国家。DynaHate也是最符合英语国家的。我们还发现与具有大学教育的人群有额外的一致性。\n\n在GPT-4的社会可接受性任务中，我们发现它最符合具有大学教育或研究生教育的人群。对于DynaHate，我们发现情况也是如此，它最符合具有大学教育的人群。然而，当模型和数据集与特定人群一致时，有些人不可避免地被落在后面。\n\n例如，数据集和模型与非二元性别人群相比，与男性和女性同行一致性更低。我们在GPT-4社会可接受性任务以及DynaHEAT任务分析中都发现了这一点。\n\n既然NLP中存在位置性，我们能做些什么呢？我们为此提出了一些建议。第一个是记录整个研究过程中所有相关的设计选择。另一个是通过观点主义的视角进行NLP研究。我们的第三个建议是在四个特定社区内构建专业的数据集和模型。一个很好的例子是Masakane倡议。\n\n我们想强调，包容性NLP不仅仅是让所有技术为每个人服务。这就结束了我们的演讲。但如果您想了解更多，请随时查看我们的仪表板以获取最新的分析结果，并阅读我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我将讨论我们在解决实体选择中间接引用表达方面的工作，其中我们引入了替代实体语料库。我的名字是Jawad Hosseini，这是与Philip Radlinski、Sylvia Parity和Annie Lewis的合作项目。我们的目标是理解用户在做出选择时使用的语言。请考虑这个替代问题：你是不是指“轻点对我”还是“我有感觉”？这里，用户想在这两首歌中选择一个，最明显的方法是使用直接引用，例如说出歌曲“轻点对我”的名字或它的位置“第一个”。但有时间接引用更合适，可以让对话更自然。这可能发生在用户记不住歌曲的名字、发音太相似难以区分，或用户想表达偏好时。以下是某些间接差异的例子，例如“较新的那首”或“不带活力歌曲”。\n\n这在对话系统中是一个重要问题，也是评估大型语言模型（LLM）实体理解能力的重要基准。我们不知道有没有一个公开的大规模数据集来完成这个任务，所以我们使用众包标注收集了一个。我们的数据集涵盖了三个不同领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用卡通完成设置。卡通有三个对话框。在第一个对话框中，鲍勃说：“记得我们昨天听的那首歌吗？”从而设定了对话背景。在第二个对话框中，爱丽丝说：“你是不是指‘轻点对我’还是‘我有感觉’？”这是替代问题。在第三个对话框中，鲍勃使用间接引用来选择这两个实体之一，例如“较新的那首”。我们自动提供第一个和第二个对话框，但第三个由标注者填写。第一个对话框从每个领域的几个手动提示中选择。第二个，即替代问题，是这样生成的：我们总是使用一个简单的模板“你是不是指A还是B”，其中A和B是来自维基百科的样本。以下是我们使用的不同采样方法：当我们在列表中向上移动时，实体变得越来越相似，通常更难进行歧义消除。第一个是均匀随机采样，第二个是当实体有相似标题时，例如两本书的名字都是《归来》。第三个是当它们在维基百科上有相似描述，最后，例如，当它们属于同一类型或同一艺术家时。\n\n当我们向管理员展示这个替代问题时，他们知道这些实体的名字，但并不一定了解这些实体，所以我们向他们展示了一些关于两个实体的背景知识。对于歌曲，我们简单地向每个歌曲提供一个谷歌搜索链接，然后要求标注者至少听一些每首歌，并阅读关于每首歌的内容。这是歌曲“轻点对我”的谷歌搜索结果示例。对于食谱和书籍领域，我们显示维基百科上的某些背景文本。对于食谱，我们还显示它们的图片，以便标注者知道它们的样子。然后，我们要求标注者选择这些实体之一，例如这里的第一个，并使用三个到五个间接引用表达来描述它们，例如“有钢琴音乐的那首”。以下是我们数据集的一些示例，例如“没有字的那首，不是那首有12岁男孩的，虚构的，或来自阿塞拜疆的”等。\n\n替代实体语料库包含三个领域的6,000个替代问题，并包含42,000个间接引用表达结果。以下是使用T5XLARGE模型的结果摘要。如果语言模型拥有与标注者完全相同的背景知识，准确率非常高，约为92%到95%。但这不现实。如果语言模型拥有部分重叠的背景知识，准确率在82%到87%之间，这更现实，例如，当语言模型检索背景知识时，准确率仅为60%，因此还有很大的改进空间。我们还证明了模型具有领域泛化能力。这是我们数据集的链接。谢谢。"}
