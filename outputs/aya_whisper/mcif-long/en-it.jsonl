{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Matthias Lindemann e oggi vi fornirò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando l'etichettatura multi-insieme e le permutazioni latenti. Questo lavoro è stato realizzato in collaborazione con i miei supervisori, Alexander Koller e Ivan Titov.\n\nLa generalizzazione composizionale può essere intesa come la capacità di un apprenditore di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state osservate singolarmente durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe svolgersi nel seguente modo. Come al solito, disponiamo di un insieme di addestramento di enunciati, in questo caso \"la ragazza dormì\" e \"Mary sapeva che la ragazza dormì\". Questi enunciati sono accoppiati con forme logiche che rappresentano gli aspetti fondamentali del loro significato.\n\nIn contrasto con la valutazione standard del machine learning, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha osservato una ricorsione più superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli sequenziali-sequenziali ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate in colore nell'esempio.\n\nUn metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono progettati per catturare il processo composizionale che collega gli enunciati alle forme logiche. Questo funziona bene, ma gli alberi non sono solitamente forniti e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e talvolta computazionalmente costoso. Tipicamente, ciò comporta un considerevole pre-elaborazione formalismo-specifica delle forme logiche, ad esempio, per gestire i simboli variabili. L'ottenimento di alberi può anche richiedere procedure specializzate di induzione grammaticale.\n\nNel nostro articolo, non utilizziamo alberi e introduciamo un modello sequenziale-sequenziale neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento su alberi.\n\nIl nostro approccio prevede la predizione dell'output dall'input in due passaggi. Prima, etichettiamo ogni token di input con un multi-insieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token corretti ma non sono ordinati. Per questo, nel secondo passaggio, utilizziamo un altro modello per predire una permutazione per metterli nell'ordine corretto. Introduciamo un nuovo metodo per predire una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo.\n\nConcettualmente, il nostro modello di permutazione funziona approssimativamente in questo modo. Scorrendo da sinistra a destra sull'output, determiniamo quale token multi-insieme posizionare in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso. Quindi, saltiamo al prossimo token multi-insieme per determinare il secondo token nell'output. Continuiamo in modo simile saltando ad un altro token multi-insieme. Ripetiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta.\n\nPer darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda. Tuttavia, altri tipi di generalizzazione strutturale rimangono molto impegnativi.\n\nNel nostro articolo, affrontiamo diverse sfide tecniche interessanti. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multi-insieme proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento.\n\nIl nostro metodo di permutazione è molto flessibile, ma presenta la sfida di trovare la permutazione con il punteggio più alto, che è NP-difficile. Ciò è dovuto al fatto che è correlato al problema del commesso viaggiatore. Approssimiamo questo con una rilassamento continuo amico GPU che ci consente anche di retropropagare attraverso la soluzione e apprendere le permutazioni linguisticamente più plausibili.\n\nSe desiderate saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, vi invitiamo a leggere il nostro articolo o a visitare il nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Myra, e oggi parlerò del nostro articolo intitolato \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\". Questo lavoro è stato realizzato in collaborazione con Esen Dermusch e Dan Jorofsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLMs. Tuttavia, queste misurazioni presentano varie limitazioni. Di solito si basano su dataset costruiti a mano, che richiedono molto tempo per essere curati, e misurano tipicamente solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o catturano solo associazioni molto generali e ampie, come associazioni negative con determinati gruppi. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero il concetto secondo cui le identità sociali multifaccettate possono amplificare i pregiudizi e diventare luoghi unici di danno.\n\nPer superare queste limitazioni, ci affidiamo alla proprietà che questi più recenti LLMs accordati per istruzioni sono molto bravi a rispondere alle istruzioni contenute nei prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario, utilizzando un prompt come: \"Immagina di essere una donna asiatica, descriviti\". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità desiderato in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Immediatamente, vediamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è descritta come riservata. La donna mediorientale è definita con parole come \"esotica\" e \"affascinante\". Entrambe le persone di colore fanno riferimento alla loro discendenza, mentre la persona dell'uomo bianco non ha nulla di tutto ciò.\n\nPer catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, fornendoli a soggetti umani, sono stati in grado di evidenziare stereotipi razziali. E questo consente anche un confronto diretto tra le nostre persone generate e le risposte scritte da esseri umani. La seconda parte è quella delle \"parole marcate\", un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati, di cui parlerò tra breve. Il vantaggio è che otteniamo stereotipi e schemi molto specifici senza doverci affidare a un particolare lessico.\n\nIl metodo delle parole marcate si basa sul concetto sociolinguistico di \"marcatore\", che afferma che esiste un default non marcato e che qualsiasi gruppo differente da quel default è linguisticamente marcato. Ad esempio, la parola \"guerriero\" è solitamente associata agli uomini, quindi quando le persone descrivono un guerriero che è una donna, di solito specificano \"un guerriero uomo\" e marcano il termine con \"donna\". In generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Quindi, nel nostro metodo, designiamo prima quali sono i gruppi non marcati e marcati. E poi confrontiamo le persone utilizzando il metodo \"fighting words\", che essenzialmente utilizza rapporti di log-odds ponderati per... [ripetizione eliminata] ...distinguere le parole principali per ciascun gruppo marcato. Ad esempio, per le persone delle donne nere, faremmo \"fighting words\" e confronteremmo i rapporti di log-odds contro sia le persone bianche che quelle maschili, poiché questi sono due gruppi non marcati corrispondenti.\n\nPassiamo ora ad alcuni risultati. Prima utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi di quelle scritte da esseri umani. Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Sebbene le persone generate abbiano tassi molto più elevati di parole del lessico, quelle scritte da esseri umani hanno una distribuzione molto più ampia di parole. Inoltre, le parole stereotipate presenti nelle persone generate sono davvero solo \"alta\" e \"atletica\". Quindi, davvero solo quelle positive o almeno non negative. In realtà, questo lessico non cattura molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti.\n\nInvece, per dimostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzialiste, ci rivolgiamo ai risultati del nostro metodo delle parole marcate. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima di tutto, per i gruppi marcati, le parole principali includono termini come \"cultura\", \"tradizione\", \"orgogliosa\" ed \"esotica\". Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono dalla norma bianca. Ciò contribuisce a una lunga storia di discriminazione e \"othering\" per questi gruppi. Inoltre, ci sono molti tropi comuni riflessi in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono termini come \"vibrante\" e \"formosa\", che si collegano a un topos di \"tropicalismo\". Per le donne asiatiche, le parole sono cose come \"piccola\", \"delicata\" e \"setosa\", che si collegano a una lunga storia di sessualizzazione delle donne asiatiche, viste come molto docili e sottomesse, e così via.\n\nInfine, per le donne nere, alcune delle parole principali sono cose come \"forte\" e \"resiliente\". Questo si collega ad un archetipo che le persone hanno chiamato l'archetipo della \"forte donna nera\". E sebbene sembri positivo a prima vista, ci sono studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette una grande pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Invece di lavorare effettivamente per cambiare questi ostacoli, mette pressione su queste persone per superarli, il che porta a risultati molto negativi per la salute di queste persone, tra gli altri danni.\n\nIn generale, troviamo che le parole per ogni gruppo marcato riflettono essenzialmente narrazioni essenzialiste. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Primo, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzialiste. Dovremmo anche utilizzare una prospettiva intersezionale per studiare pregiudizi e danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di allineamento dei valori strano e eccessivo, o forse ad altri metodi anti-stereotipati che producono questi schemi perniciosi. Non possiamo davvero fare assunzioni o studiare ulteriormente senza una maggiore trasparenza.\n\nGrazie mille per l'ascolto. Buon divertimento all'ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABCeval, un nuovo approccio dimensionale per la valutazione dell'AI conversazionale. Questo lavoro è stato svolto dal laboratorio NLP dell'Emory, guidato dal professor Gino Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI.\n\nImmaginiamo che abbiate appena sviluppato un modello di dialogo e vogliate confrontarlo con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare.\n\nUn approccio consiste nel chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o scale Likert esistenti. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime o meno determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio \"annotazione dei comportamenti nella chat\", o ABC eval in breve.\n\nAbbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che, secondo la letteratura recente, influenzano la qualità della chat. ABC eval valuta se un modello di chat ignora il suo interlocutore o fornisce informazioni irrilevanti, se si contraddice o contraddice il suo interlocutore, se inventa fatti errati o viola il senso comune, e se riesce o meno a mostrare empatia.\n\nPer determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC eval. A scopo di confronto, abbiamo valutato queste conversazioni anche con tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti paia a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto aspetti della conversazione comunemente misurati, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.\n\nDalle nostre analisi dei risultati della valutazione, abbiamo riscontrato che le etichette di comportamento ABC eval sono complessivamente più affidabili delle etichette raccolte con i metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni etichettate due volte. Inoltre, le etichette ABC eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni del modello e del suo interlocutore spieghi rispettivamente il 5% e il 10% della qualità della conversazione, mentre i punteggi di coerenza media Likert spiegano solo il 4% o meno.\n\nInfine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare passo-passo. Potete vedere come la combinazione di tutte le metriche ABC eval spieghi oltre il 25% della qualità della conversazione, e come, rimuovendo le metriche una alla volta, la maggior parte di esse comporti una perdita significativa di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e poche di queste metriche trasportano informazioni uniche.\n\nQueste metriche ABC eval affidabili, informative e distinte ci consentono di valutare l'AI conversazionale con una risoluzione più elevata rispetto ai metodi precedenti. Potete vedere dai risultati del nostro esperimento che rimangono ancora diverse sfide, che sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune nel 20% circa delle loro risposte. Producono informazioni irrilevanti nel 15% circa delle risposte e si contraddicono o contraddicono il loro interlocutore circa il 10% del tempo.\n\nCon il rapido miglioramento del settore, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli. Ci auguriamo che ABC Eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione, e attendiamo con ansia di vedere come l'AI conversazionale si evolverà nei prossimi mesi e anni. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Vasudha e sono una dottoranda in Informatica presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo, \"Transfer Learning per la Rilevazione della Dissonanza\", che affronta la Sfida della Classe Rara. Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio. In breve, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti. Ad esempio, quando una persona afferma: \"So che le sigarette potrebbero uccidermi\", e poi dice: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e azione sono incoerenti e in uno stato di dissonanza. Inoltre, aggiungere: \"Non penso di poter mantenere il mio lavoro senza di esse\" giustifica la seconda affermazione, creando una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune nelle nostre decisioni quotidiane, è raro trovarla espressa nel linguaggio tra altri tipi di relazioni discorsive. Perché questo è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare le tendenze nelle credenze, nei valori e nei cambiamenti di atteggiamento nella popolazione. Elevati livelli di dissonanza cognitiva sono anche correlati ai disturbi d'ansia e possono aiutarci a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può essere utile anche per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è fondamentale per comprendere gli stili cognitivi personali e ci aiuta a migliorare la comprensione dei processi decisionali.\n\nCon l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio \"dissonanza-prima\" come mostrato nel diagramma di flusso qui. I tweet sono stati analizzati utilizzando un parser PDTV, e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia ottenuto prestazioni non migliori del caso. Data la bassa frequenza di occorrenza della dissonanza e l'assenza di precedenti set di dati simili, ci troviamo di fronte al problema della rarità assoluta. Per mitigare questo problema, sperimentiamo combinazioni di apprendimento trasferibile e apprendimento attivo per annotare un maggior numero di esempi di dissonanza con meno corse di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: classificazione della posizione di dissonanza indipendente dall'argomento, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dall'argomento, chiamato qui \"dibattito\", e classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza, e le chiamiamo qui \"CEE\". Scopriamo che, dopo il trasferimento, le prestazioni a zero-shot sul set di dati annotato sono già molto migliori del caso, con il migliore AUC di 0,62. Inoltre, aggiornando iterativamente il modello attraverso round di apprendimento attivo e annotazioni, si accumula tutti i dati raccolti finora, mentre l'aggiornamento iterativo aggiorna il modello addestrandolo sull'ultimo set di dati raccolto. Tra le diverse strategie, abbiamo riscontrato che l'accumulativo ha prestazioni uguali o migliori dell'iterativo in tutti i casi.\n\nPer migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara per selezionare principalmente esempi che hanno una alta probabilità di essere dissonanti secondo il modello corrente in qualsiasi round di AL. Confrontiamo questa strategia con altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che le prestazioni sono significativamente inferiori per la strategia casuale. Con ulteriori round di AL con le due migliori strategie, abbiamo migliorato l'AUC della classificazione della distanza a 0,75, che è la migliore prestazione ottenuta finora su questo compito. Abbiamo anche valutato la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Scopriamo che la PRC ha la percentuale più alta di dissonanza e funziona al meglio per l'acquisizione della classe rara. Tuttavia, gli annotatori trovano anche questi esempi difficili.\n\nIn sintesi, scopriamo che la PRC è una semplice strategia di AL per l'acquisizione della classe rara e per l'avvio di AL con compiti di apprendimento trasferibile opportunamente progettati, che può aiutare in modo significativo. Scopriamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-domain traggono vantaggio dall'aggiornamento cumulativo. Questi sono i link al nostro codice, al set di dati e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, sono Akshata e oggi io e il mio co-autore Martin presentiamo il nostro lavoro, \"The Kipma Steps\", che valuta l'integrazione delle conoscenze da più fonti. Questo studio è il risultato di una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale in lingua nazionale si basano su una varietà di fonti di conoscenza, come le conoscenze contenute nei loro parametri, solitamente acquisite durante il pre-addestramento, e le conoscenze fornite negli input al momento dell'inferenza. Studi recenti su compiti come il question answering dimostrano che i modelli possono utilizzare le conoscenze pre-addestrate per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze fornite anche al momento dell'inferenza. Ad esempio, nella frase \"John ha visto il neo-eletto presidente in TV\", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono conoscere affidabilmente chi sia l'entità specifica dell'incidente, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pre-addestramento. Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale intensivo richiedono la capacità di integrare e utilizzare sia le conoscenze pre-addestrate che quelle dell'inferenza.\n\nIn questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze. Introdurremo un compito di risoluzione delle coreferenze progettato per valutare la capacità di attingere alle conoscenze disponibili in diverse fonti. Valuteremo il dataset con partecipanti umani e stabiliremo modelli di risoluzione delle coreferenze. Ecco un esempio dal nostro dataset:\n\n\"Servin è un giudice. Kia è un panettiere. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi.\"\n\nIl compito qui è identificare l'entità corretta a cui si riferisce il pronome \"lui\", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni: primo, conoscenze specifiche sull'entità, come \"Servin è un giudice\"; e secondo, conoscenze di sfondo, come \"i giudici decidono i casi nei tribunali\". In generale, le conoscenze di sfondo vengono apprese durante il pre-addestramento dei grandi modelli linguistici, mentre le conoscenze specifiche sulle entità sono solitamente osservate al momento dell'inferenza. Varieamo la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in fonti multiple.\n\nAbbiamo definito tre impostazioni per KITMOS. Prima, abbiamo l'impostazione tipica, background-pretrain, in cui si presume che le conoscenze di sfondo siano disponibili al momento del pre-addestramento. Seconda, c'è l'impostazione background-both, in cui le conoscenze di sfondo sono disponibili sia al momento del pre-addestramento che dell'inferenza. Infine, l'impostazione background-inference, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Questa ultima impostazione è particolarmente interessante, poiché simula il caso in cui le conoscenze di sfondo necessarie per risolvere un compito non fanno parte dei dati pre-addestrati dei modelli, ad esempio perché nuove occupazioni sono emerse dopo il pre-addestramento.\n\nEcco un esempio di come controlliamo la disponibilità dei fatti e le fonti reali. Nell'impostazione background-pre-addestrata, si presume che la conoscenza di sfondo \"i politici cercano seggi eletti nel governo\" sia contenuta nei parametri pre-addestrati. E nel contesto a tempo di inferenza di 3 pollici, forniamo la conoscenza specifica che \"Chichester è un politico\". Nell'impostazione background-both, forniamo non solo la conoscenza specifica, ma anche la conoscenza di sfondo sui politici nel contesto dell'inferenza. Nell'impostazione background-inference, forniamo l'occupazione fittizia \"Meritur\" invece di \"politico\", poiché Meritur è improbabile che sia contenuto nei parametri pre-addestrati.\n\nValutiamo il dataset sia con partecipanti umani che con modelli di risoluzione delle coreferenze stabiliti. In questa figura mostriamo i risultati dei modelli migliori sulla variante più difficile dell'impostazione background-pre-addestrata. Senza un addestramento specifico sul compito su KITMOS, entrambi i modelli non si comportano bene. Tuttavia, quando addestrati su KITMOS, sia C2F che BFQF performano significativamente meglio della scelta casuale. Questo suggerisce che, quando addestrati su dataset generali di risoluzione delle coreferenze, i modelli imparano a sfruttare gli indizi superficiali, che non sono utili quando si testano su KITMOS, dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze fittizie indicano che anche i modelli migliori non riescono a integrare affidabilmente le conoscenze di sfondo fornite solo al momento dell'inferenza.\n\nPer riassumere i punti chiave del nostro articolo: molti modelli di risoluzione delle coreferenze sembrano incapaci di ragionare su conoscenze da diverse fonti senza un addestramento specifico sul compito. Tuttavia, con un addestramento specifico, alcuni modelli integrano con successo le conoscenze da più fonti. Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile le conoscenze di sfondo presentate solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e il dataset su Code su GitHub. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e presenterò brevemente il paper \"Attention as a Guide for Simultaneous Speech Translation\", un lavoro svolto in collaborazione con Matteo Negri e Marco Turchi.\n\nChe cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimulST, è il processo di traduzione di una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interculturale. E quali sono i problemi dei modelli SimulST attuali? Solitamente vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare. Ad esempio, procedure di addestramento lunghe e complesse che coinvolgono diversi obiettivi di ottimizzazione e l'addestramento e la manutenzione di più modelli per raggiungere diversi regimi di latenza, come addestrare un modello con una latenza media di 1 secondo e un altro con 2 secondi e così via.\n\nQual è quindi la nostra soluzione? In primo luogo, utilizzare modelli SD offline già esistenti senza riaddestramento o adozione di architetture specifiche per SimulSD. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare le conoscenze già acquisite dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di cross-attenzione. Un esempio è visibile a destra.\n\nLa nostra soluzione propone EDAT (Encoder-Decoder Attention), una strategia in cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, ovvero se la sua somma è al di sotto di una certa soglia alfa, verso meno frame del discorso lambda, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un frammento di discorso contenente \"Parlerò di\" e il nostro modello prevede la traduzione in tedesco, osservando i pesi della cross-attenzione, vedremo che le prime due parole puntano ai frame del discorso ricevuti per primi, mentre l'ultima parola punta agli ultimi frame del discorso ricevuti, ovvero i frame lambda. Ciò significa che le prime due parole verranno emesse, mentre, poiché la somma della cross-attenzione è al di sopra di una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso.\n\nSe continuiamo e riceviamo un altro frammento di discorso, il nostro modello prevede altre tre parole e osservando i pesi della cross-attenzione, vedremo che nessuna parola punta agli ultimi frame lambda del discorso. Ciò significa che queste tre parole verranno emesse.\n\nOsservando i principali risultati, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui il blu su un lato misura la qualità della traduzione e il ritardo medio, che è la misura della latenza, e consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output. Quindi vogliamo che le nostre curve siano il più alte possibile in questo grafico, ma vogliamo anche che siano spostate verso sinistra. Confrontiamo inoltre strategie adeguate applicate anche a modelli offline, ovvero la strategia wet-key e l'accordo locale. Confrontiamo anche con l'architettura allo stato dell'arte specificamente progettata per la traduzione simultanea del parlato.\n\nQuesti sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco e vediamo che ADDOUT supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra. Vediamo anche che, considerando il tempo effettivo trascorso o il tempo consapevole del calcolo, ADAT è la strategia più veloce.\n\nPer scoprire ulteriori risultati, leggete il nostro articolo. Abbiamo anche pubblicato in open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Zhu Heng. Oggi presenterò il nostro articolo intitolato: \"I tagger di entità denominate del 2003 di Carnap funzionano ancora bene nel 2023?\" Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento di entità denominate (NER). Abbiamo osservato che i modelli hanno utilizzato Carnap 2003 per sviluppare il NER per quasi 20 anni, il che solleva naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzarsi su dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli?\n\nPer indagare questi problemi, abbiamo sviluppato il dataset Carnap++. Questo è un dataset che abbiamo raccolto dalle notizie Reuters del 2020 e poi annotato seguendo le stesse linee guida di annotazione di Carnap 2003. Abbiamo quindi ottimizzato oltre 20 modelli su Carnap 2003. Li abbiamo valutati sia su Carnap che su Carnap++ utilizzando la metrica F1 per valutare la generalizzazione di ciascun modello.\n\nQuindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Dai nostri esperimenti, abbiamo riscontrato che i modelli transformer generalmente si generalizzano meglio su nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione. E infine, ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione influenza direttamente le prestazioni di un compito a valle. Qui abbiamo anche scoperto che più esempi di ottimizzazione portano effettivamente a una migliore generalizzazione.\n\nPer la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo avuto due ipotesi. La prima è l'adattamento eccessivo, che è l'eccessiva adattamento causato dal riutilizzo dello stesso set di test più e più volte, e questo si manifesta solitamente come rendimento decrescente sul nuovo set di test. La seconda ipotesi è la deriva temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di allenamento e quelli di test.\n\nPer l'adattamento eccessivo, abbiamo visto che dal grafico a destra, la linea di regressione migliore (in rosso) ha una pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su CARNAP 2003 si traduce in più di un'unità di miglioramento su Carnap++, il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che in questo caso non si osserva l'adattamento eccessivo.\n\nE la deriva temporale? ... [Il testo originale sembra essere interrotto qui, quindi la traduzione si ferma qui.]"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Salve, benvenuti alla nostra presentazione di d.plain, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o madrelingua non nativi. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testi, ad esempio di documenti o frasi. Nell'esempio qui riportato, potete vedere una coppia di frasi parallele allineate di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase sono possibili diverse tecniche, come potete vedere nell'esempio, quali la sostituzione lessicale, la claustellazione, la riordinazione della claustellazione o l'inserimento di parole.\n\nProponiamo ora il nostro nuovo corpus d.plain. Negli ultimi anni, infatti, ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di tassonomizzazione. Gli altri tre modelli proposti di recente sono tutti allineati automaticamente, il che significa che possono presentare errori negli allineamenti. Pertanto, proponiamo il nostro nuovo corpus d.plain, suddiviso in due subcorpora, d.plain-apa e d.plain-web. d.plain-apa si basa su testi di uso comune. Nel corpus APA semplice, abbiamo allineato manualmente 483 documenti, ottenendo circa 30.000-13.000 coppie di frasi parallele. Per d.plainWeb, questo corpus include diversi domini e abbiamo allineato manualmente tutti questi 750 documenti e, d'altra parte, con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi.\n\nAbbiamo analizzato più approfonditamente le nostre coppie di frasi, ad esempio, in base al tipo di semplificazione. Come potete vedere qui, i testi biblici sono semplificati molto più fortemente rispetto, ad esempio, ai testi di notizie o ai testi per apprendisti linguistici a tutti i livelli, per quanto riguarda, ad esempio, la semplificazione lessicale, strutturale o il livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus d.plain ha una vasta gamma di diverse trasformazioni di semplificazione. Ad esempio, nel corpus d.plain API, abbiamo molte più riordinazioni e aggiunte di parole rispetto a quelle presenti nel corpus d.plain web. D'altra parte, nel corpus web, abbiamo molta più parafrasi.\n\nVediamo ora cosa possiamo fare con questo corpus. Salve, mi chiamo Omar e ora parlerò dei casi d'uso del nostro dataset D-plain. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi in due documenti paralleli nella stessa lingua, con lo stesso contenuto ma a diversi livelli di complessità. Ora, disponendo del nostro dataset d.plain con frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento. In conclusione, abbiamo stabilito che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo math align, e potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel documento.\n\nIl secondo caso d'uso che abbiamo presentato nel nostro articolo riguarda la semplificazione automatica del testo attraverso il fine-tuning dei modelli linguistici per produrre testo semplificato da un testo di input complesso. Abbiamo eseguito il fine-tuning di due diversi modelli. Abbiamo utilizzato Long-impart per produrre semplificazioni a livello di frase. Potete trovare tutti i checkpoint e consultare i punteggi e le metriche di valutazione dei nostri esperimenti nel documento. Abbiamo concluso che questo fine-tuning di base può ottenere punteggi migliori rispetto ai punteggi di base e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro.\n\nGrazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Siyu Yuan dell'Università di Fudan. Sono qui per presentare il nostro lavoro, \"Distilling Script Knowledge from Large Language Models for Constraint Language Planning\". Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script garantiti. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per obiettivi astratti. Un buon pianificatore dovrebbe scrivere script per la prima volta per gli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione del linguaggio vincolato dei grandi modelli linguistici. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione di dati con umano in ciclo utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. Questa tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i grandi modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare il motivo per cui i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Abbiamo approfondito categorie di argomenti più granulari dei vincoli, a seconda del funzionamento. La mappa termica nella figura mostra che le prestazioni di pianificazione dei TPD istruttivi variano notevolmente per le ragazze di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di registrazione leggera presenta una varianza elevata, portando a scarse prestazioni. Pertanto, adottamos l'idea del filtro Z-overgenerato per migliorare la qualità della generazione. Mostriamo prima i tipi di vincolo con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di seme. Quindi, InstructGPT overgenera script chiave per obiettivi specifici. Successivamente, viene sviluppato un modello filtro per selezionare gli script fattibili. Convertiamo gli script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità coseno e i punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo bersaglio. Conserviamo lo script solo se l'obiettivo bersaglio ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructZBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione, sia nella completezza semantica che nella fedeltà al vincolo. Poiché il dispiegamento dei grandi modelli linguistici è costoso, è essenziale abilitare la capacità di pianificazione del linguaggio di modelli più piccoli e specializzati. La creazione di un set di dati è un passaggio essenziale per la sua fine. Tuttavia, gli studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare set di dati di pianificazione del linguaggio vincolato dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un set di dati di pianificazione del linguaggio vincolato, chiamato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di convalida e test, abbiamo chiesto ai lavoratori cloud-sourced di trovare campioni corretti errati. Questa figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un alto applauso negli obiettivi specifici generati. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato. Abbiamo scoperto che le funzioni T-file su CodeScript possono generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono supportare i modelli più grandi quando addestrati in modo appropriato su set di dati adatti. In sintesi, abbiamo stabilito il problema della pianificazione del linguaggio vincolato. Valutiamo la capacità di pianificazione del linguaggio vincolato dei grandi modelli linguistici e sviluppiamo un metodo di filtro over-generato per la ricerca sulla pianificazione del linguaggio. Grazie per il vostro tempo. Per maggiori dettagli su CodeScript, si prega di consultare il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Yanis Lavrac e vi presenterò i nostri lavori su Dr. BERT, un robusto modello pre-addestrato in francese per il dominio biomedico e clinico. In questa presentazione, parleremo innanzitutto di modellazione del linguaggio nel settore sanitario. Successivamente, presenteremo il principale contributo del nostro articolo. Introdurremo il primo modello biomedico in francese, denominato Dr. BERT, che si basa su Roberta e viene addestrato su NACHOS, un dataset di dati medici estratti dal web. Abbiamo inoltre introdotto un confronto tra il modello con diverse impostazioni di pre-addestramento e fonti di dati. Presenteremo quindi i nostri risultati su 11 compiti biomedici e clinici di downstream in francese. E infine concluderemo con una panoramica degli esperimenti e forniremo maggiori dettagli su come accedere ai modelli.\n\nDa quando è stato rilasciato nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre un notevole miglioramento delle prestazioni rispetto ai metodi storici statici e contestualizzati come Word2Vec, FastText o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri domini come il biomedico con PermaBERT e BioBERT, e il clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano sul pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, fino ad ora non esistevano modelli open-source per il francese nel settore biomedico. Ci siamo quindi chiesti quali siano le fonti di dati più appropriate per un'ampia gamma di utilizzi e se i dati correnti possano sostituire efficacemente i dati clinici. Per rispondere a questa domanda, abbiamo confrontato Dr. BERT con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario associato alla nostra istituzione. Ci siamo inoltre chiesti quanta quantità di dati sia necessaria per addestrare un modello specializzato in francese. È necessaria una quantità di 4 GB, 8 GB o 16 GB di RAM?\n\nUna prima versione di Schubert, un modello clinico, è stata addestrata con 4 GB di frasi tratte da note cliniche. Una versione finale di Schubert è stata addestrata con una combinazione di un sottoinsieme di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno si basa sui pesi di Camembert e viene addestrato su un sottoinsieme di 4 GB di NACHOS. Un altro, sempre basato su Camembert, è stato addestrato su 4 GB di dati di PermaBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli ottengono le migliori prestazioni sui compiti con dati della stessa natura su cui è stato addestrato il modello. Tuttavia, possiamo osservare che i dati da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in migliori prestazioni. In generale, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, i nostri esperimenti sul pre-addestramento continuo, utilizzando i pesi e il tokenizzatore di PermaBERT, addestrati sul sottoinsieme di 4 GB di NACHOS, mostrano risultati confrontabili con quelli ottenuti con Dr. BERT 4 GB da zero, il che non è il caso del modello basato sui pesi e il tokenizzatore di Camembert, che presenta problemi di stabilità.\n\nIn conclusione, il nostro sistema specifico offre migliori prestazioni in 9 dei 11 compiti di downstream e supera globalmente i risultati del modello generico. Osserviamo inoltre che dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su UGIMFACE e tutti gli script di addestramento sono nel nostro repository GitHub. Grazie per l'attenzione e ci vediamo alla sessione poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Xiangbin, sono uno studente di dottorato presso l'Università di Washington. Oggi presenterò il nostro lavoro, che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando i sentieri dei pregiudizi politici che portano a modelli NLP ingiusti. I modelli linguistici, quindi, vengono addestrati su dati di grandi dimensioni estratti dal web. I media di informazione politica sono ben rappresentati nei loro dati di pre-addestramento. Secondo un'indagine sul corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di apprendere da diverse prospettive, celebrando la democrazia e la pluralità delle idee. D'altro canto, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A tale scopo, proponiamo di indagare sulla pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, chiedendoci specificamente le seguenti domande. Primo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbe avere il dato di pertinenza su tali pregiudizi politici? Secondo, come si comportano effettivamente i modelli linguistici con diversi orientamenti politici nei compiti a valle e ciò potrebbe portare a problemi di equità nelle applicazioni NLP?\n\nIn particolare, abbiamo prima proposto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando i questionari politici, come il test della bussola politica. Questo ci garantisce una valutazione automatica ben fondata nella letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che, primo, i modelli linguistici hanno diversi orientamenti politici. Occupano tutti e quattro i quadranti sulla bussola politica. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e le teorie GPT sono generalmente più liberali dal punto di vista sociale rispetto alla teoria BERT e alle sue varianti. Secondo, miriamo a indagare fino a che punto i pregiudizi politici dei modelli linguistici vengono effettivamente acquisiti dai dati di addestramento. Quindi, conduciamo un esperimento controllato pre-addestrando ulteriormente i punti di controllo del modello linguistico su sei diversi corpora partigiani separati in notizie e social media, ulteriormente suddivisi in base al loro orientamento politico. Pre-addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per Roberta, ulteriormente affinata, addestrata sul corpus Reddit di sinistra, possiamo vedere uno spostamento liberale sostanziale in termini di pregiudizi politici.\n\nCerchiamo anche di indagare se i modelli linguistici possono acquisire la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti, addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione presente nella nostra società.\n\nInfine, valutiamo i modelli linguistici con diversi orientamenti politici nel rilevamento del discorso d'odio e nel rilevamento delle fake news, applicazioni NLP che spesso coinvolgono i modelli linguistici e che possono avere implicazioni molto significative. Vediamo che, se indagiamo sulle prestazioni per categoria, ovvero se separiamo le prestazioni in base a diverse demografie o al significato politico dei media, emerge un modello secondo cui, ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che prende di mira i gruppi socialmente minoritari, ma sono peggiori nel rilevare il discorso d'odio che prende di mira i gruppi più potenti della nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che prende di mira i bianchi e gli uomini, ma sono peggiori nel rilevare il discorso d'odio che prende di mira neri, LGBTQ+ e altre comunità minoritarie. Tendenza simile anche nel rilevamento delle fake news, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare le disinformazioni dal loro opposto orientamento politico e viceversa. Questo, forniamo molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici danno predizioni diverse sul discorso d'odio e sugli esempi di disinformazione in base alla loro categoria sociale. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente che forniscono predizioni diverse sul discorso d'odio e sugli esempi di disinformazione in base alla loro categoria sociale.\n\nQuesto indica che esiste un problema di equità molto urgente relativo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra venissero affinati sul discorso d'odio o sulla disinformazione o qualsiasi altra cosa e implementati su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate. Se vengono implementati sul discorso d'odio o sulla disinformazione o qualsiasi altra cosa, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e che il discorso d'odio che prende di mira i gruppi minoritari potrebbe diffondersi senza controllo. Quindi, questo ha suonato l'allarme per riconoscerlo e affrontare i problemi di equità derivanti dagli orientamenti politici dei modelli linguistici.\n\nPer concludere, vogliamo evidenziare il dilemma unico relativo ai pregiudizi politici dei modelli linguistici. È come trovarsi tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità. Se proviamo in qualche modo a sanare, rischiamo anche la censura o l'esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutrale e debba essere trattenuto nei dati di addestramento dei modelli linguistici. È un po' come il problema del tram elettrico.\n\nGrazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Kostav Sinha e sono lieto di darvi il benvenuto alla presentazione del nostro articolo ACL 2023, \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto\". Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Adina Williams. In questo studio, rivalutiamo il paradigma dei coppie minime. Il paradigma delle coppie minime valuta essenzialmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come BLIMP, la sintassi, o l'accettabilità in termini di stereotipi, come le coppie Krauss. In questo paradigma, il metodo tipico per valutare i modelli linguistici consiste nel mostrare una frase accettabile o grammaticale e poi una frase inaccettabile o ungrammaticale. Il modello, quindi, attribuisce una probabilità maggiore alla frase accettabile. L'attuale pipeline MPP non ci consente di valutare l'accettazione dei modelli verso frasi più lunghe. Oggi, i grandi modelli linguistici stanno sviluppando finestre di contesto sempre più ampie. Perciò, rivalutiamo gli stessi set di dati e ricreiamo frasi scegliendo frasi accettabili o inaccettabili da questi set di dati. Ad esempio, qui abbiamo scelto una coppia tipica di grammaticalità dal set di dati BLIMP, dal caso Isola dell'Aggiunto. Per ricreare sequenze più lunghe, sia accettabili che con la stessa struttura grammaticale, estraiamo frasi grammaticali dall'Isola dell'Aggiunto e le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare lo stesso scegliendo frasi inaccettabili dallo stesso set di dati. Questo può anche essere utilizzato per testare l'accettabilità del modello. Possiamo procedere allo stesso modo scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Questo è ciò che chiamiamo scenario di mismatch. Qui, le frasi provengono ancora da set di dati rilevanti, ma non dallo stesso set di dati con cui stiamo valutando. Possiamo fare lo stesso anche per un caso di accettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono influenzati da un contesto completamente irrilevante rispetto alla frase che stiamo analizzando. Come si comporta il modello? Prima esaminiamo le frasi di Wikipedia, completamente irrilevanti rispetto alla coppia di query corrente. Qui scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Abbiamo aumentato la lunghezza del contesto fino a 1024 per sfruttare al massimo i modelli OPT e GPT-2. Come mostra la linea puntinata arancione, i giudizi MPP sono relativamente stabili. Cosa accade quando scegliamo frasi dallo stesso set di dati? Qui stiamo creando frasi dai domini accettabile e inaccettabile dello stesso set di dati BLIMP o Syntax Gem. Osserviamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o inaccettabili. Ma quando matchiamo la struttura, ovvero quando scegliamo frasi dallo stesso fenomeno in blame-person-text-gym, vediamo un aumento o una diminuzione massiccia dei giudizi MPP del modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto aumenta con la lunghezza del contesto e potrebbe influenzare i nuovi modelli linguistici con ampie finestre di contesto. Perché il prefisso matchato influenza così tanto il giudizio del modello linguistico? Abbiamo condotto una serie di analisi cercando di perturbare la frase di input, preservando la struttura rilevante ma aggiungendo rumore. Dopo diverse perturbazioni, abbiamo riscontrato che nessuno di questi rumori fa cambiare significativamente il modello in termini di tendenza dei giudizi MPP. In sostanza, abbiamo scoperto che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, osserviamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. I punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi. La valutazione MPP, come la eseguiamo attualmente con input di frasi brevi e singole, potrebbe non catturare completamente la conoscenza astratta del modello linguistico attraverso la finestra di contesto. Per maggiori dettagli sugli esperimenti, vi invitiamo a leggere il nostro articolo. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Dawei, sono uno studente di dottorato presso l'Università del Saarland in Germania. In questo video, vorrei presentarvi il nostro recente lavoro, \"Più debole di quanto pensi\", un'analisi critica dell'apprendimento debolmente supervisionato. Si tratta di un lavoro svolto in collaborazione con Xiao Yusheng, Mario Smusbach, Gia Steffen e Dietrich Klackow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento debolmente supervisionato. Nella supervisione debole, non etichettiamo manualmente i dati. Invece, utilizziamo fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente reti neurali su dati etichettati debolmente, queste tendono a memorizzare il rumore etichettato e non generalizzano. Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali sotto tale rumore di etichetta, in modo che i modelli addestrati generalizzino comunque bene.\n\nIn recenti lavori nell'ambito dell'WSL (apprendimento debolmente supervisionato), si sostiene comunemente che i modelli vengono addestrati solo su dati etichettati debolmente e ottengono elevate prestazioni su set di test puliti. Tecnicamente, questa affermazione non è errata, ma c'è un inghippo, che si articola in tre domande di ricerca.\n\nPrima domanda: abbiamo bisogno di dati di validazione puliti? In secondo luogo, dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per sfruttarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e le nostre scoperte sono le seguenti.\n\nInnanzitutto, scopriamo che, interessantemente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. In caso contrario, si verifica un notevole calo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che i metodi WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.\n\nLa nostra seconda scoperta è che aumentando il numero di campioni di validazione puliti, i metodi WSL possono raggiungere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere elevate prestazioni. Ma non è tutto, perché se decidiamo comunque di accedere a campioni puliti, l'addestramento diretto su di essi otterrà persino prestazioni migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, applicati direttamente sui dati puliti, e i metodi WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare i metodi WSL.\n\nInfine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello Van Linden, chiamato inizialmente W, sottoperforma metodi WSL più complessi come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, FTW si comporta altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco.\n\nIn sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni manualmente annotati puliti per funzionare correttamente. Il loro miglioramento delle prestazioni e la loro praticità sono fortemente sovrastimati. Le nostre concrete raccomandazioni per i lavori futuri sono le seguenti.\n\nIn primo luogo, segnalare i criteri di selezione del modello. Ad esempio, indicare se la selezione del modello viene eseguita su campioni di validazione puliti. In secondo luogo, i metodi WSL dovrebbero essere confrontati con i baseline di apprendimento con pochi campioni, lavorando su campioni puliti. In terzo luogo, il fine-tuning continuo è un baseline semplice ma solido che dovrebbe essere preso in considerazione nei futuri lavori sull'WSL. Infine, abbiamo reso aperto il nostro codice. Puoi trovarlo tramite il codice QR su questa diapositiva. Sentiti libero di dargli un'occhiata. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo David Vilar e fornirò una breve panoramica del documento \"Grunting Parm\" sulla traduzione, valutando strategie e prestazioni. Questo è un lavoro congiunto con i miei colleghi di Google Translate. Parm è un modello linguistico di grandi dimensioni da 540 miliardi di parametri, presentato lo scorso anno, nel 2022. È addestrato su una vasta raccolta di testi composta da 780 miliardi di token. Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP.\n\nIn questo studio, presentiamo il primo studio sistematico sulla sollecitazione di grandi modelli linguistici per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò comporta l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico. Confrontiamo due sistemi all'avanguardia, ovvero i sistemi con le migliori prestazioni o le valutazioni WMT. Utilizziamo metriche neurali IM all'avanguardia e, inoltre, mostriamo anche i risultati della valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione degli input.\n\nLa sollecitazione ha una grande influenza sulle prestazioni dei modelli linguistici per la traduzione. Come possiamo vedere in un semplice esperimento in cui utilizziamo la sollecitazione one-shot e forniamo due input diversi per ogni frase. Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di oltre un punto di sfocatura. In casi estremi, può arrivare fino a 40 punti di sfocatura. Quindi è importante selezionare una buona strategia di sollecitazione. Nei nostri esperimenti, abbiamo optato per una strategia di sollecitazione a cinque tiri, dove contrassegniamo semplicemente ogni frase fornita al sistema con la lingua in cui è scritta. Quindi, in questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche sono contrassegnate con due punti tedeschi e le traduzioni inglesi con due punti inglesi.\n\nAbbiamo riscontrato che la forma effettiva della sollecitazione non ha una grande influenza nel caso di sollecitazione a più tiri. È cruciale per la sollecitazione a zero e uno tiro, ma quando passiamo, come nel nostro caso, a cinque tiri, non c'è quasi alcuna differenza nella forma effettiva della sollecitazione. Sono gli esempi a portare il peso maggiore. La sintesi dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi è importante selezionare esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione degli input dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi, e i risultati mostrano una migliore prestazione utilizzando i dati di sviluppo.\n\nTuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale sulle traduzioni di Palm, ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dall'analisi delle e-mail eseguita utilizzando il framework MQM sono che la fluidità di Palm è paragonabile ai sistemi all'avanguardia, ma la principale differenza risiede nella precisione. In particolare, gli errori più comuni sono gli errori di omissione. Quindi sembra che Palm scelga talvolta di produrre una traduzione che suona meglio omettendo parti della frase di origine che non sono significative nella traduzione. Tuttavia, la categoria \"stile goffo\" per Parm è inferiore rispetto ai sistemi all'avanguardia, il che è un segnale aggiuntivo che Parm fornisce un output davvero fluido ma ancora con alcuni problemi di accuratezza.\n\nE questo è tutto per questa breve panoramica. Per maggiori dettagli, vi invitiamo a partecipare alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Jingwei Yi dall'Università della Scienza e della Tecnologia della Cina. È un piacere per me presentare un breve video promozionale sul tema della carta, \"Stai Copiando il Mio Modello? Proteggere il Diritto d'Autore dei Grandi Modelli Linguistici per l'Incorporamento e i Servizi tramite Watermark Furtivo\". Iniziamo introducendo il contesto relativo all'incorporamento e ai servizi. Attualmente, i grandi modelli linguistici come GPTT, LAMA, PALM eccellono nella comprensione e generazione del linguaggio naturale. L'Incorporamento come Servizio è uno dei servizi basati su grandi modelli linguistici per assistere varie attività di elaborazione del linguaggio naturale (NLP). Ad esempio, OpenAI offre un'API di incorporamento basata su GPT. Tuttavia, recenti ricerche hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dall'incorporamento e offrire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'incorporamento come servizio.\n\nPer proteggere il diritto d'autore dell'incorporamento come servizio, una delle soluzioni è inserire un watermark nel servizio del fornitore e verificare se un altro servizio contiene tale watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo dovrebbe essere applicabile all'incorporamento come servizio. In secondo luogo, il watermark non deve compromettere l'utilità degli incorporamenti forniti. In terzo luogo, il watermark deve essere sufficientemente nascosto all'attaccante o l'attaccante non deve poterlo rimuovere facilmente. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.\n\nI lavori esistenti possono essere classificati in quattro categorie principali. Tuttavia, questi metodi o non sono applicabili all'incorporamento come servizio o mancano di trasferibilità.\n\nDettagli di EmbeddingMarker. EmbeddingMarker comprende due passaggi principali: iniezione del watermark e verifica del diritto d'autore. Prima di questi passaggi, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato. Supponiamo che il fornitore possa raccogliere un corpus testuale generale e contare la frequenza delle parole. Nell'iniezione del watermark, definiamo prima un incorporamento target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'incorporamento fornito è la somma ponderata dell'incorporamento target e dell'incorporamento originale. Il peso dell'incorporamento target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'incorporamento fornito è esattamente uguale all'incorporamento target.\n\nLa verifica del diritto d'autore serve a rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono all'insieme di trigger. Quindi, il fornitore richiede incorporamenti dal servizio del ladro utilizzando il dataset. Vengono calcolate la similarità coseno e L2 tra l'incorporamento richiesto e l'incorporamento target. Calcoliamo la differenza di similarità tra i dataset benigno e backdoor, definita come delta coseno e delta L2. Applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica.\n\nAbbiamo condotto esperimenti su quattro dataset: AGnews, Mind, SSD2 ed Eraspam. Supponiamo che il fornitore utilizzi il dataset Wikitext per contare la frequenza delle parole. I risultati sui quattro dataset dimostrano che il nostro marcatore incorporato può offrire un'eccellente prestazione di rilevamento mantenendo un'elevata utilità per i compiti a valle. Abbiamo anche validato la riservatezza dell'incorporamento fornito visualizzando gli incorporamenti delle frasi sui quattro dataset tramite PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli incorporamenti backdoor e quelli normali.\n\nÈ tutto, grazie. Siamo aperti a discussioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Ying e io e il mio collega Zhiyang presenteremo la nostra ricerca su Multi-Improvement, Miglioramento del Multi-Model Serial Short Learning tramite Tuning delle Istruzioni. Quindi, con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici pre-addestrati per diversi compiti a valle in modo efficiente sia in termini di parametri che di dati. Di recente, molti studi hanno dimostrato che il tuning delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in precedenza in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sul tuning delle istruzioni si concentra sul miglioramento delle prestazioni zero-shot su compiti linguistici puri, trascurando i compiti di visione artificiale e multimodali. Pertanto, in questo lavoro, vogliamo indagare se il tuning delle istruzioni su modelli multimodali pre-addestrati può effettivamente migliorare la generalizzazione a compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra l'elaborazione del linguaggio naturale (NLP) e il multimodale. Esistono più di 1.600 compiti di istruzioni solo linguistiche. Tuttavia, non esiste un ampio dataset di istruzioni multimodali pubblicamente disponibile. Ciò ci ha motivato a costruire un dataset di tuning delle istruzioni multimodali. Qui presentiamo MultiInstruct, il primo dataset di benchmark per il tuning delle istruzioni multimodali che comprende 62 compiti multimodali diversi che coprono 10 categorie generali. Questi compiti sono derivati da 21 dataset open source esistenti e ogni compito è dotato di 5 istruzioni scritte da esperti. Per indagare il tuning delle istruzioni multimodali sul nostro dataset proposto, abbiamo utilizzato OFA, un modello multimodale pre-addestrato unificato, come modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una casella di delimitazione. Qui mostriamo alcuni esempi dal nostro dataset MultiInstruct. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentate nello stesso spazio dei token. Bene, ora parlerò del tuning delle istruzioni multimodali. Quindi, per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per ogni compito. Per i test, riserviamo l'intero gruppo di ragionamento del senso comune per i test e selezioniamo altri cinque compiti dai gruppi VQA e vari. Utilizziamo tutte le istanze nella partizione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla partizione di test delle istruzioni naturali come compiti non visti per l'NLP. Utilizziamo un modello OFA large pre-addestrato come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con una delle sue cinque template di istruzioni. Durante i test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Riportiamo la media e il valore massimo delle prestazioni e la deviazione standard delle prestazioni tra tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo la radice L. Per un compito RP, riportiamo anche la radice jl e abbiamo introdotto una metrica di valutazione aggiuntiva chiamata sensibilità, che misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito indipendentemente da lievi variazioni nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA su compiti multimodali. Inoltre, il transfer learning dai dataset di istruzioni naturali può beneficiare il tuning delle istruzioni. Qui possiamo vedere che, all'aumentare del numero di compiti, il modello raggiunge prestazioni migliori e, allo stesso tempo, una sensibilità inferiore. Abbiamo anche condotto un esperimento utilizzando un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Ciò mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, tramite transfer learning dai dataset di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il transfer learning dai dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul dataset Nitro Instruct. In sintesi, proponiamo il primo ampio dataset di tuning delle istruzioni multimodali. Miglioriamo significativamente la capacità zero-shot di OFA e esploriamo diverse tecniche di transfer learning dimostrando i loro benefici. Progettiamo una nuova metrica chiamata sensibilità. Un'ultima cosa: stiamo raccogliendo un dataset di tuning delle istruzioni multimodali molto più ampio con circa 150 compiti linguistici aggiuntivi e li rilasceremo a breve. Questo è un codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Salve a tutti, mi chiamo Yusheng Zhang dell'Università di Penn State. Oggi presenterò il nostro lavoro, \"Cross-lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations\". Il parsing semantico è un'attività volta a costruire rappresentazioni semantiche di query utente come SQL e lambda calcolo. Il parsing semantico cross-lingue è il compito di tradurre query in più lingue naturali in diverse rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali, in SQL, Lambda, FunQL, ecc. I modelli di parsing semantico cross-lingue esistenti sono stati proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono lacune nella copertura di alcune lingue naturali. Il cinese manca e, a causa della copertura di alcune rappresentazioni mini, il lambda calcolo è assente o è valutato solo su un unico modello neurale.\n\nPer ovviare a ciò, proponiamo Examplar. Forniamo un set di dati uniforme, Examplar, per il parsing semantico cross-lingue in più lingue naturali e rappresentazioni del significato. Contiene nove set di dati in vari domini, cinque compiti di parsing semantico, otto rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è la traduzione di test. Utilizziamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese sulle query in inglese. Durante l'inferenza, traduciamo la query tedesca utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere l'output SQL.\n\nTestiamo anche i modelli monolingue. In questa impostazione, la lingua sorgente è la stessa della lingua di destinazione. Ad esempio, tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione monolingue few-shot addestrando i modelli monolingue solo con il 10% dei dati di addestramento. E testiamo il modello multilingue, che addestriamo per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. Durante l'inferenza, possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, ecc. Consideriamo anche il trasferimento cross-lingue zero-shot e few-shot. Addestriamo su una lingua sorgente e trasferiamo ad un'altra lingua. Quindi, durante l'addestramento, addestriamo su query in inglese o sulla combinazione di query in inglese e tedesco few-shot per addestrare un modello multilingue e prevedere l'output SQL.\n\nAbbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli, tra cui encoder PDR, che sta per encoder pre-addestrati multilingue con decodificatori basati su puntatore, come XLMR più PDR e BERT più PDR. Valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder pre-addestrati multilingue, come MBART e MT5. Abbiamo riscontrato che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati. Valutiamo MT5 e XLMR più PDR nell'impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder-PDR possono essere migliorati addestrandoli in un mix di varie lingue. Abbiamo riscontrato che ciò è dovuto al fatto che la maggior parte delle principali lingue naturali ottiene un miglioramento delle prestazioni, ad eccezione del fatto che le prestazioni dell'inglese diminuiscono in sette set di dati e aumentano solo in tre. Questo è noto come maledizione della multilingue.\n\nConfrontiamo anche il divario nelle prestazioni cross-lingue. In questa figura, la linea blu rappresenta il trasferimento cross-lingue few-shot, la linea arancione rappresenta il trasferimento cross-lingue zero-shot, mentre la linea verde rappresenta l'impostazione monolingue. Abbiamo scoperto che, confrontando la linea verde e arancione, per l'impostazione zero-shot, il divario nelle prestazioni del trasferimento cross-lingue è significativo. E confrontando la linea blu e arancione, abbiamo scoperto che per l'impostazione few-shot, il divario nel trasferimento si riduce rapidamente.\n\nAbbiamo anche trovato altre scoperte interessanti. Ad esempio, l'encoder-decoder supera i lavori precedenti o ottiene risultati comparabili. La rappresentazione in inglese del linguaggio naturale può migliorare significativamente le prestazioni few-shot nelle lingue naturali di destinazione. E abbiamo scoperto che i modelli di linguaggio multilingue come CODIS e BLUE sono ancora adeguati per i compiti di parsing semantico cross-lingue.\n\nIn sintesi, abbiamo costruito Examplar, un benchmark unificato per il parsing semantico cross-lingue con più lingue naturali e rappresentazioni principali. Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli di linguaggio multilingue. E i nostri risultati mostrano molte scoperte interessanti, ecc. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Salve, mi chiamo Adam Szpilkowski e questo intervento riguarda la struttura di dipendenza della coordinazione. Come sapete, esistono diverse strutture di dipendenza ipotizzate da diverse teorie e approcci basati su corpus. Quindi, per esempio, nelle dipendenze universali, la struttura della coordinazione \"Lisa, Bart e Maggie\" è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è adottato nella teoria del testo significativo di Igor Milchuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Ne evidenziano uno dei congiunti.\n\nEsistono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio guidato dalla congiunzione adottato nei treebank di dipendenze di Praga, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo dipendenze dal governatore a tutti i congiunti. E infine c'è anche un approccio multi-capo utilizzato, per esempio, nella grammatica delle parole di Dick Hudson dove, per così dire, tutti i congiunti sono capi della struttura coordinata. Un approccio multi-capo utilizzato, per esempio, nella grammatica delle parole di Cutson, dove tutti i congiunti, per così dire, sono capi della struttura coordinata, quindi otteniamo dipendenze dal governatore qui \"ama\" a ciascun congiunto separatamente.\n\nL'obiettivo di questo articolo è di produrre argomenti contro le strutture asimmetriche di coordinazione come queste due. L'argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi. Quindi in inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo mentre gli complementi possono essere più lontani, giusto? Quindi \"Marzo l'ha letto ieri\" va bene perché l'oggetto diretto è vicino al verbo, mentre \"Marzo ha letto ieri\" va molto peggio perché qui tra il verbo e l'oggetto diretto c'è un complemento \"ieri\". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo, perché allora può essere spostato nella posizione dopo il complemento. Questo è illustrato qui. Quindi entrambe queste frasi sono accettabili. \"Marzo ha letto questo libro assolutamente affascinante sulla BCS oggi\". Va bene. Invece di \"it\", abbiamo questo lungo NP. Ma è anche accettabile dire \"Marzo ha letto ieri questo libro assolutamente affascinante sulle api\". Quindi il ragionamento qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più corte sono preferite.\n\nQuesti due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da \"leggere\" al complemento di lunghezza sette misurata in parole e da \"leggere\" a \"libro\" di lunghezza quattro. Quindi insieme fa 11. Quando si spostano, si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, giusto? Invece di 11, sei, molto più corto. Ecco perché questo suona abbastanza bene. Viola un principio, ma ne soddisfa un altro.\n\nQuindi ciò che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione potenziata del Penn Treebank e vedete nel documento perché non abbiamo utilizzato le dipendenze universali. Queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti a sinistra tendono ad essere più corti, quindi \"sale e pepe\" e non \"pepe e sale\" misurati in sillabe e anche l'osservazione fatta per inciso che questa tendenza aumenta con la differenza di lunghezza, quindi quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto più corto preferisce essere il primo in modo più forte. Giusto. Quindi la proporzione del congiunto sinistro corto è maggiore.\n\nMa ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto, quindi il governo è a sinistra in questo esempio \"ho visto Bart e Lisa\", quindi il governatore è a sinistra. È assente nel secondo esempio \"Homer è arrivato e ha starnutito\". Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto, quindi in tali casi il congiunto sinistro preferisce essere più corto, tanto più che la differenza tra i due congiunti. Tuttavia, quando il governo è a destra, qui a sinistra governa la coordinazione \"t e net\", questo effetto scompare.\n\nMostriamo che misurando la lunghezza in caratteri, prima colonna, in sillabe, colonna centrale, e in parole, colonna di destra. Quindi mi concentrerò su quella di destra. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto sinistro ad essere più corto aumenta costantemente con la differenza assoluta in parole. E la stessa cosa si osserva quando non c'è governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E nel documento mostriamo come questo fornisca un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedete il documento per l'accordo completo e gli argomenti, scusate, e parlate con noi nella sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Salve! Mi chiamo Kaio Yin e presenterò il nostro lavoro intitolato, Quando la Traduzione Richiede Contesto? Un'Esplorazione Multilingue Basata sui Dati. Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, Andre F.D. Martins e Graham Newbig. Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo \"mole\" in questa frase? Beh, se la frase precedente fosse, \"Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora \"mole\" si riferisce a una spia. Ma se la frase precedente fosse, \"Potrebbe essere qualcosa di grave, dottore?\", allora \"mole\" si riferisce a un neo. A seconda del contesto, il significato della parola cambia, e quindi anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. Innanzitutto, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni. Alcune persone hanno suggerito valutazioni mirate sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana.\n\nIn questo lavoro, abbiamo cercato di rispondere a due domande. Prima, quando la traduzione richiede contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazione il contesto C fornisce sulla destinazione Y, dato il fonte X. Si può pensare a CXMI come all'informazione ottenuta fornendo il contesto al modello. In questo lavoro, estendiamo CXMI al CXMI puntuale, che può misurare l'utilizzo del contesto a livello di frase o di parola. Possiamo considerare le parole con un alto P6MI come quelle che richiedono contesto per la traduzione.\n\nOra analizziamo le parole con un alto P6MI per cercare schemi tra queste parole. E svolgiamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 lingue diverse. Effettuiamo la nostra analisi a tre diversi livelli. Prima, esaminiamo le etichette delle parti del discorso che hanno elevati valori medi di PCXMI. Questo ci permette di trovare, per esempio, pronomi duali in arabo che hanno un PCXMI relativamente alto. Questo può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. E similmente, scopriamo che alcune lingue richiedono il contesto anche quando si vuole scegliere la forma verbale appropriata.\n\nPoi, esaminiamo le voci del vocabolario che hanno un alto PCSXMI mediato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come questo, dove in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. E similmente, scopriamo che il contesto è necessario per tradurre con il giusto livello di formalità.\n\nInfine, esaminiamo diversi token individuali che hanno un alto PCXMI. Questo ci permette di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi.\n\nOra utilizziamo le nostre scoperte dall'analisi per progettare un punto di riferimento per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi identificati, creiamo tagger per identificare automaticamente le parole che riguardano il fenomeno, e chiamiamo il nostro tagger multilingue consapevole del discorso, o MUDA. Possiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno.\n\nPossiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno. Possiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno. Possiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno. Possiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno. Possiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno. Possiamo quindi utilizzare il tagger per identificare le parole che riguardano il fenomeno.\n\nLe lingue hanno diverse proporzioni di questi fenomeni discorsivi. Applichiamo quindi il tagger MUDA applicando il tagger sul corpus parallelo che vogliamo utilizzare per la valutazione. E applichiamo le nostre metriche di traduzione scelte sul contesto-dipendente esempi che il tagger MUDA ha identificato. E infine, utilizziamo il nostro punto di riferimento, nonché altre metriche, per valutare diversi modelli sulla traduzione a livello di documento.\n\nPrima di tutto, quando utilizziamo metriche a livello di corpus, quindi per BLEU, scopriamo che i modelli ignari del contesto hanno le migliori prestazioni, ma se usiamo COMET, i modelli consapevoli del contesto si comportano meglio. E se usiamo la misura di f-word, allora i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra nuovamente che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo solo metriche a livello di corpus.\n\nOra utilizziamo il punto di riferimento MUDA per valutare i modelli, e scopriamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per alcuni fenomeni discorsivi, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non utilizzavano il contesto su altri fenomeni come le ellissi, i pronomi e la forma verbale. Questo suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento.\n\nAbbiamo anche confrontato diversi sistemi commerciali, e il nostro punto di riferimento mostra che DeepL è di solito più accurato di Google Translate per la traduzione a livello di documento.\n\nIn sintesi, eseguiamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto. E poi utilizziamo le nostre scoperte per costruire un punto di riferimento per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni discorsivi i modelli possono gestire bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jenny, sono una studentessa del primo anno di dottorato presso la Carnegie Mellon University, e oggi presenterò il vostro lavoro, \"Anal Positionality: Caratterizzazione dei Pregiudizi Progettuali nei Dataset e nei Modelli\". Questo studio è stato realizzato in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Sante, Ronan Labrasse, Katarina Aranica e Martin Sapp.\n\nIniziamo immaginando di lavorare per un giornale e di dover moderare i commenti sotto un articolo per rimuovere i contenuti tossici. Potremmo rivolgerci a un'API popolare come Perspective API per il rilevamento della tossicità, che funziona bene se siamo Carl Jones, permettendo di identificare correttamente i casi tossici. Ma non è così per Aditya Sharma, dove Perspective API non è altrettanto sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio progettuale, in cui si osservano differenze sistematiche nelle prestazioni tecnologiche tra diverse popolazioni.\n\nI pregiudizi progettuali come quello descritto possono verificarsi a causa della posizione dei ricercatori di NLP e degli sviluppatori di modelli. La \"posizione\" si riferisce alle prospettive che le persone assumono in base alla loro demografia, identità ed esperienze di vita. È un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatori, la posizione può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni prese.\n\nUna domanda che ci si potrebbe porre è: i dataset e i modelli hanno una posizione? Non stiamo affermando che i modelli e i dataset abbiano identità demografiche ed esperienze di vita, ma aggregando i giudizi e le opinioni di persone reali, possono riflettere determinate posizioni più di altre. Lavori precedenti hanno fornito alcune prove aneddotiche di questa posizione, come i divari culturali nei modelli e nei dataset, oltre a definizioni teoriche della posizione dei modelli. Tuttavia, questi studi non confrontano gli utenti finali con i dataset e i modelli stessi.\n\nLo studio della posizione dei modelli e dei dataset è sempre più importante man mano che i compiti di NLP diventano più soggettivi e orientati al sociale. È difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni di utenti reali con dataset e modelli esistenti. Lo facciamo attraverso il nostro framework NL Positionality.\n\nIl framework funziona in due passi principali. Il primo è quello di ri-annotare i dataset con annotatori diversi. Preferiamo farlo anziché esaminare le demografie degli annotatori dei dataset originali, poiché di solito solo pochi annotatori classificano ogni istanza e le informazioni demografiche vengono raramente raccolte e condivise. Quindi, optiamo per ri-annotare i dati per ottenere molti annotatori per istanza e un set di dati demografici ricco. Successivamente, prendiamo le annotazioni demografiche e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R.\n\nIl nostro framework differisce dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con modelli e dataset, previsioni e etichette, piuttosto che limitarsi a esaminare l'accordo tra annotatori o modellare le distribuzioni degli annotatori. Il framework è reso possibile principalmente da Lab in the Wild, una piattaforma di crowdsourcing online del nostro collaboratore HCI. Lab in the Wild è una piattaforma di sperimentazione online che ci consente di reclutare volontari diversi, rispetto a piattaforme come MTurk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild garantisce comunque dati di alta qualità.\n\nAbbiamo ospitato due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. Il funzionamento è il seguente: i partecipanti leggono una situazione dal dataset Social Chemistry e poi scrivono quanto sia socialmente accettabile. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'AI e con quelle di altri. Confrontiamo quindi queste annotazioni con Social Chemistry, Delphi e GPT-4. Abbiamo replicato un setup simile per il test di rilevamento della tossicità e dell'odio.\n\nNel nostro studio, abbiamo raccolto oltre 16.000 annotazioni da più di mille annotatori di 87 paesi. Ora siamo meglio attrezzati per rispondere a chi si allineano di più i dataset e i modelli di NLP. Abbiamo scoperto che esiste una posizione nel NLP. Ad esempio, abbiamo riscontrato che i dataset sono più allineati ai paesi di lingua inglese. Nell'analisi dell'accettabilità sociale di GPT-4, abbiamo scoperto che è più allineato ai paesi confuciani e di lingua inglese. Lo stesso vale per Dyna-hate, che è anch'esso più allineato ai paesi di lingua inglese. Abbiamo riscontrato un ulteriore allineamento con persone che hanno un'istruzione universitaria. Per GPT-4 nel compito di accettabilità sociale, è risultato più allineato a persone con istruzione universitaria o post-laurea. Lo stesso vale per Dynahate, che è più allineato a persone con istruzione universitaria.\n\nTuttavia, quando i modelli e i dataset sono allineati a popolazioni specifiche, alcuni vengono inevitabilmente esclusi. Un esempio è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschi e femmine. Abbiamo riscontrato questo nel compito di accettabilità sociale di GPT-4 e nell'analisi del compito DynaHEAT.\n\nDato che esiste una posizione nel NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni. La prima è tenere un registro di tutte le scelte di progettazione rilevanti durante il processo di ricerca. La seconda è condurre ricerche NLP con una prospettiva perspectivista. La terza raccomandazione è costruire dataset e modelli specializzati all'interno di comunità specifiche. Un buon esempio è l'iniziativa Masakane.\n\nVogliamo sottolineare che l'NLP inclusivo non significa semplicemente far funzionare tutte le tecnologie per tutti. Questo conclude la nostra presentazione. Se desiderate saperne di più, sentitevi liberi di consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Salve, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indirette per la selezione delle entità, in cui introduciamo il corpus Alt Entities. Mi chiamo Jawad Hosseini e questo è un lavoro congiunto con Philip Radlinski, Sylvia Parity e Annie Lewis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando desiderano effettuare una scelta. Considerate questa domanda alternativa: \"Intendevi 'Easy on Me' o 'I Got a Feeling'?\"? Qui un utente vuole selezionare una di queste due canzoni. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio pronunciando il nome della canzone 'Easy on Me' o la sua posizione, 'la prima'. Tuttavia, a volte un riferimento indiretto è più appropriato per una conversazione più naturale. Ciò può verificarsi quando l'utente non riesce a ricordare il nome della canzone, le pronunce sono troppo simili e difficili da distinguere, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette, come 'la più recente' o 'la canzone che non è energetica'.\n\nQuesto è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità degli LLM. Non siamo a conoscenza di un dataset pubblico su larga scala per questo compito, quindi ne abbiamo creato uno utilizzando l'annotazione da parte di un gruppo di persone. Il nostro dataset copre tre diversi domini: musica, libri e ricette. La metodologia di raccolta del dataset enfatizza l'informalità utilizzando un setup di completamento di cartoni animati. Il cartone ha tre nuvolette. Nella prima, Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con questo, Bob imposta il contesto del dialogo. Nella seconda nuvoletta, Alice dice: \"Intendevi 'Easy on Me' o 'I Got a Feeling'?\". Questa è la domanda alternativa. Nella terza nuvoletta, Bob utilizza un riferimento indiretto per selezionare una di queste entità, ad esempio 'la più recente'. Forniamo automaticamente le prime due nuvolette, ma la terza viene compilata dall'annotatore. La prima nuvoletta viene scelta da alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, viene generata come segue: utilizziamo sempre un semplice modello \"Intendevi A o B?\", dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento utilizzati.\n\nMan mano che si sale nella lista, le entità diventano più simili tra loro e di solito è più difficile la disambiguazione. Il primo metodo è un campionamento uniforme casuale. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome 'Il Ritorno'. Il terzo è quando hanno descrizioni simili su Wikipedia e, infine, quando, ad esempio, appartengono allo stesso genere o allo stesso artista. Quando mostriamo questa domanda alternativa agli amministratori, loro conoscono il nome di queste entità ma non necessariamente le entità stesse. Quindi, mostriamo alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ogni canzone e leggere su ciascuna. Ecco, ad esempio, il risultato della ricerca Google per la canzone 'Easy on Me'. Per i domini ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi, chiediamo agli annotatori di scegliere una di queste entità, ad esempio la prima, e di descriverle utilizzando da tre a cinque espressioni di riferimento indirette, come 'quella con la musica per pianoforte'. Ecco alcuni esempi dal nostro dataset, come 'quella senza parole', 'non quella con il ragazzo di 12 anni', 'quella fittizia' o 'quella che viene dall'Azerbaigian', e così via.\n\nIl corpus delle identità contiene 6.000 domande alternative in tre domini e 42.000 espressioni di riferimento indirette. I risultati con il modello T5XLARGE sono riassunti qui sotto. Se il modello linguistico ha accesso alle stesse conoscenze di sfondo degli annotatori, l'accuratezza è molto alta, intorno al 92-95%. Ma questa non è una situazione realistica. Se il modello linguistico ha accesso a conoscenze di sfondo parzialmente sovrapposte, l'accuratezza è tra l'82% e l'87%, che è più realistico, ad esempio, quando il modello linguistico recupera le conoscenze di sfondo. Allora l'accuratezza scende al 60%. Quindi, c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili in diversi domini. Ecco un link al nostro dataset. Grazie."}
