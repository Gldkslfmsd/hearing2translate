{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Asaf Harari und ich werde unseren Artikel präsentieren: „Few-Shot Tabular Data Enrichment Using Fine-Tuning Transformers Architectures“. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der vorhandenen Merkmale. Doch manchmal sind diese Merkmale begrenzt. Die Merkmalsgenerierung unter Verwendung einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische tabellarische Datenanreicherung unter Verwendung von externen Textquellen.\n\nAngenommen, wir haben einen tabellarischen Datensatz und eine Wissensbasis. Wir benötigen einen automatischen Prozess, der Entity Linking und Textanalyse umfasst, um neue Merkmale aus dem freien Text der Wissensbasis zu extrahieren. Unser Framework, FAST, ist genau dieser automatische Prozess. Sehen wir uns ein Beispiel an.\n\nIn einem Datensatz, der in FAST eingespeist wird, ist in diesem Beispiel der Datensatz ein Universitätsdatensatz, dessen Ziel es ist, Universitäten in schlecht platzierte und gut platzierte Universitäten zu klassifizieren. Als Wissensbasis verwenden wir Wikipedia.\n\nDie erste Phase von FAST ist das Entity Linking. Dabei wird jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensbasis verknüpft. Und der Text der Wissensbasis-Entitäten wird extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text der Abstract der Wikipedia-Seite.\n\nNun müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Dazu benötigen wir eine Merkmalsextraktionsphase, die Textanalyse umfasst. Und dies ist die Hauptneuheit dieses Artikels, auf die ich in den nächsten Folien eingehen werde.\n\nNach der Merkmalsextraktionsphase folgt eine Merkmalsgenerierungsphase, in der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst generieren wir Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, also generiert FAST zwei neue Merkmale. Wenn der Datensatz jedoch fünf Klassen hat, generiert FAST fünf neue Merkmale. Jedes Merkmal stellt die Wahrscheinlichkeit für jede Klasse dar.\n\nZur Analyse des Textes verwenden wir den aktuellen Stand der Technik bei der Textanalyse, nämlich transformerbasierte Sprachmodelle wie BERT, GPT, XNL und andere. Es ist jedoch unwahrscheinlich, dass wir ein Sprachmodell mit den Eingabedatensätzen trainieren können. Ein naiver Ansatz wäre daher eine Zielaufgaben-Feinabstimmung. In der Merkmalsextraktionsphase können wir also ein vortrainiertes Sprachmodell herunterladen und es mit dem Ziel-Datensatz feinabstimmen. In diesem Beispiel wird das Sprachmodell feinabgestimmt, um Text in Klassen einzuteilen, Abstracts in Klassen, niedrig oder hoch, und liefert als Ausgabe die Wahrscheinlichkeit für jede Klasse, die als neue Merkmale verwendet wird.\n\nDas Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige eindeutige Entitätsschlagwörter haben. In unserem Experiment enthielten fast die Hälfte der Datensätze weniger als 400 Beispiele, und der kleinste Datensatz hatte in seinem Trainingsdatensatz nur 35 Beispiele. Eine Feinabstimmung eines Sprachmodells mit diesem Datensatz wäre daher ineffektiv.\n\nWir können jedoch auf vorhandenes Wissen über zuvor analysierte Datensätze zurückgreifen und diese Informationen nutzen, wenn wir den N-ten Datensatz analysieren. Wir schlagen vor, eine weitere Feinabstimmungsphase hinzuzufügen, eine vorläufige mehrstufige Feinabstimmungsphase, in der das Sprachmodell mit N-1 Datensätzen feinabgestimmt wird, gefolgt von einer weiteren Feinabstimmungsphase, der Zielaufgaben-Feinabstimmung, in der das Sprachmodell mit dem N-ten Ziel-Datensatz feinabgestimmt wird.\n\nDer Stand der Technik bei der mehrstufigen Feinabstimmung ist mtDNN. mtDNN verfügt über Köpfe in der Anzahl der Aufgaben im Trainingsdatensatz. In diesem Beispiel gibt es vier Aufgaben im Trainingsdatensatz, also hat mtDNN vier Köpfe, wie Sie im Bild sehen können. Es wählt zufällig einen Batch aus dem Trainingsdatensatz aus. Wenn der zufällige Batch beispielsweise zu einer einzelnen Satzklassifizierungsaufgabe gehört, führt es eine Vorwärts- und Rückwärtsübermittlung durch den ersten Kopf aus. Gehört der zufällige Batch zu einer paarweisen Rangfolgeaufgabe, führt es eine Vorwärts- und Rückwärtsübermittlung durch den letzten Kopf aus.\n\nIn unserem Szenario variieren tabellarische Datensätze in der Anzahl der Klassen. Es gibt also viele Aufgaben. mtDNN verfügt über eine Anzahl von Klassen-Köpfen und Ausgabeschichten, und zusätzlich muss mtDNN für einen neuen Datensatz mit einer neuen Aufgabe neue Köpfe initialisieren.\n\nUnser Ansatz, die Aufgaben-Umformulierungs-Feinabstimmung, besteht darin, dass wir anstelle der Verwaltung mehrerer Köpfe jeden Datensatz in einen Satz pro Klassifizierungsaufgabe umformulieren, was eine Aufgabe mit zwei Klassen ist. Sehen wir uns ein Beispiel an. Hier ist unser Eingabedatensatz, der aus Entitäten, Merkmalen, Text und Klassen besteht. Und unsere Aufgabe besteht darin, den Text in niedrig und hoch zu klassifizieren, also den Abstract zu klassifizieren, ob er zur Klasse gehört oder nicht. In diesem Fall besteht der Label-Vektor immer aus zwei Klassen.\n\nDann umformuliert es die Aufgabe in Aufgaben mit einem Satz pro Klassifizierung. Das Sprachmodell wird auf die neue Aufgabe angewendet und liefert die Wahrscheinlichkeit für jede Klasse als neu generiertes Merkmal in der Anzahl der Klassen.\n\nUm unser Framework zu bewerten, verwenden wir 17 tabellarische Klassifikationsdatensätze, die Größe, Merkmale, Ausgewogenheit, Domäne und anfängliche Leistung variieren. Als Wissensbasis verwenden wir Wikipedia. Wir entwerfen unser Experiment als Live-One-Out-Bewertung, bei der wir FAST mit 16 Datensätzen trainieren und es auf den 17. Datensatz anwenden. Wir teilen jeden Datensatz auch in vier Faltungen auf und führen eine vierfache Kreuzvalidierung durch. Dann generieren wir die neuen Merkmale und bewerten sie mit fünf Evaluierungs-Klassifikatoren.\n\nIn unserem Experiment verwenden wir eine auf Baukästen basierende Architektur. Hier sind die Ergebnisse unseres Experiments. Sie können sehen, dass wir unseren Ansatz mit der Ziel-Datensatz-Feinabstimmung, der vorläufigen mtDNN-Feinabstimmung und unserer umformulierten Feinabstimmung vergleichen, und unser umformulierter Feinabstimmungsansatz erzielt das beste Ergebnis und die beste Leistung. Während mtDNN eine Verbesserung um zwei Prozent gegenüber der Ziel-Datensatz-Feinabstimmung erzielte, verbesserte unser Ansatz die Leistung um sechs Prozent.\n\nWenn wir uns die kleinen Datensätze ansehen, können wir sehen, dass die Leistung des leeren DNN abnimmt und die Verbesserung der alleinigen Zielaufgaben-Feinabstimmung.\n\nZusammenfassend ermöglicht FAST die Few-Shot-Anreicherung ab 35 Beispielen in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben-Datensätze und behält den Kopf des Modells bei. Es fügt jedoch eine Umformulierungsphase hinzu. Es erweitert den Trainingsdatensatz und benötigt einen Zielwert mit semantischer Bedeutung, der in das Sprachmodell eingespeist und in der Satz-pro-Klassifizierungsaufgabe verwendet werden kann. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, heute möchte ich unsere Forschungsarbeit vorstellen: „Learning to Reason Detectively, Metabolic Problem Solving as Complex Reason Extraction“. Ich bin Alan vom ByteDance AI Lab, und dies ist eine gemeinsame Arbeit mit Jerry von der University of Texas at Austin und Weilu von der SUTD. Zunächst möchte ich über unsere Motivation für das Schlussfolgern sprechen. Hier zeigen wir ein Beispiel, in dem mehrschrittiges Schlussfolgern hilfreich ist. Diese Abbildung stammt aus einer Studie, in der mithilfe von Prompting ein mathematisches Problem in einem Few-Shot-Lernszenario gelöst wird. Auf der linken Seite sehen wir, dass wir bei der Bereitstellung von Beispielen mit lediglich Fragen und Antworten möglicherweise nicht die korrekten Antworten erhalten. Aber wenn wir eine detailliertere Schlussfolgerungsbeschreibung geben, kann das Modell diese vorhersagen und auch hier eine korrekte Vorhersage treffen. Es ist also vorteilhaft, interpretierbare, mehrschrittige Schlussfolgerungen als Ausgabe zu haben. Wir sind der Meinung, dass das Method-Problem eine direkte Anwendung zur Bewertung solcher Schlussfolgerungsfähigkeiten darstellt.\n\nIn unserem Problemsetup erhalten wir Fragen und müssen diese lösen, um numerische Antworten zu erhalten. In unseren Datensätzen wird uns auch die mathematische Expression bereitgestellt, die zu dieser bestimmten Antwort führt. Bestimmte Annahmen gelten auch wie in früheren Arbeiten. Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist und wir nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentiation berücksichtigen. Komplexere Operatoren können tatsächlich in diese grundlegenden Operatoren zerlegt werden.\n\nFrühere Arbeiten zur Lösung von Method-Problemen lassen sich in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle einteilen. Traditionelle Sequenz-zu-Sequenz-Modelle wandeln die Expression in eine spezifische Sequenz für die Generierung um, was recht einfach zu implementieren ist und sich auf viele verschiedene komplexe Probleme verallgemeinern lässt. Allerdings ist die Leistung dieser Modelle im Allgemeinen nicht besser als die von strukturierten Modellen, und sie weisen einen Mangel an Interpretierbarkeit der Vorhersagen auf. Aufgrund des Transformer-Modells ist diese Richtung jedoch nach wie vor sehr beliebt.\n\nBei baumbasierten Modellen strukturieren wir diese Ausdrücke in Form eines Baums und folgen bei der Generierung einem Pre-Order-Traverse. Hier generieren wir Operatoren, bis wir die Blätter erreichen, die die Mengen darstellen. Der Vorteil ist, dass wir so eine binäre Baumstruktur erhalten. Es ist jedoch kontraintuitiv, da wir zunächst den Operator und dann erst am Ende die Mengen generieren. Ein weiterer Nachteil ist, dass es zu wiederholten Berechnungen kommt. In diesem Ausdruck wird beispielsweise „a mal 3 plus 3“ zweimal generiert. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Im zweiten Schritt können wir beispielsweise diesen Divisor, der 27 ist, erhalten und uns auf die ursprüngliche Frage beziehen, um relevante Inhalte zu finden. In diesen Schritten erhalten wir die Divisoren, und im dritten Schritt erhalten wir dann den Quotienten. Nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt wiederverwenden und die Ergebnisse des vierten Schritts erhalten. Schließlich können wir die Dividenden erhalten. Hier generieren wir den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Mengen zu generieren, was den Prozess genauer macht.\n\nIn unserem deduktiven System beginnen wir mit einer Reihe von Mengen, die in den Fragen präsentiert werden, und schließen auch einige Konstanten als anfängliche Zustände ein. Die Expression wird durch EIJOP dargestellt, wobei wir den Operator zwischen QI und QJ anwenden, und diese Expression ist gerichtet. Wir haben auch eine Subtraktionsumkehrung, um die entgegengesetzte Richtung darzustellen. Dies ähnelt der Relationsextraktion. In einem formalen deduktiven System wenden wir beim Zeit Schritt t den Operator zwischen dem QI- und QJ-Paar an und erhalten so eine neue Expression. Diese fügen wir zum nächsten Zustand hinzu, um eine neue Menge zu erhalten. Diese Folie visualisiert die Evolution der Zustände, in der wir kontinuierlich Expressions zum aktuellen Zustand hinzufügen.\n\nBei der Implementierung unserer Modelle verwenden wir zunächst ein vorab trainiertes Sprachmodell, das BERT oder RoBERTa sein kann, und kodieren dann einen Satz, um die Mengenrepräsentationen zu erhalten. Sobald wir die Mengenrepräsentationen haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel, in dem wir die Repräsentation für Q1 dividiert durch Q2 und dann mal Q3 erhalten. Zuerst erhalten wir die Paarrepräsentation, die im Wesentlichen nur die Verkettung von Q1 und Q2 ist. Dann wenden wir ein Feedforward-Netzwerk an, das durch den Operator parametrisiert wird, und erhalten schließlich die Expression-Repräsentation Q1 dividiert durch Q2. In der Praxis können wir während der Inferenzphase jedoch auch falsche Expressions erhalten. Hier ist die Anzahl aller möglichen Expressions gleich der dreifachen Anzahl der Operatoren. Wir können jedoch leicht Einschränkungen hinzufügen, um diesen Suchraum zu kontrollieren. Wenn beispielsweise eine bestimmte Expression nicht zulässig ist, können wir sie einfach aus unserem Suchraum entfernen. Im zweiten Schritt machen wir dasselbe, aber der einzige Unterschied besteht darin, dass eine weitere Menge hinzukommt. Diese Menge stammt aus der zuvor berechneten Expression. Schließlich erhalten wir die endgültige Expression Q3 mal Q4, und wir können auch sehen, dass die Anzahl aller möglichen Expressions sich von dem vorherigen Schritt unterscheidet. Diese Unterschiede machen es schwierig, Beam Search anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgeglichen ist.\n\nDas Trainingverfahren ähnelt dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeit Schritt optimieren. Hier verwenden wir auch Tau, um anzugeben, wann wir den Generierungsprozess beenden sollten. Der Suchraum unterscheidet sich von dem in Sequenz-zu-Sequenz-Modellen, da er sich bei jedem Zeit Schritt unterscheidet. In traditionellen Sequenz-zu-Sequenz-Modellen entspricht er der Anzahl des Vokabulars, und es ermöglicht uns auch, bestimmte Einschränkungen aus vorherigem Wissen zu übernehmen.\n\nWir führen Experimente mit den häufig verwendeten Method-Problem-Datensätzen MAWPS, Math23k, MathQA und SWAMP durch und zeigen hier kurz die Ergebnisse im Vergleich zu den vorherigen besten Ansätzen. Unsere beste Variante ist der RoBERTa-deduktive-Reasoner, und im Gegensatz zu den besten Ansätzen verwenden wir keinen Beam Search. Die besten Ansätze sind oft baumbasierte Modelle. Insgesamt kann unser Reasoner diese baumbasierten Modelle deutlich übertreffen, aber die absoluten Werte bei MathQA oder SWAMP sind nicht besonders hoch.\n\nWir untersuchen die Ergebnisse auf SWAMP genauer, da diese Datenmenge anspruchsvoll ist. Der Autor hat versucht, das NLP-Modell zu verwirren, indem er irrelevante Informationen und zusätzliche Mengen hinzugefügt hat. In unseren Vorhersagen stellen wir fest, dass einige Zwischenwerte negativ sind. Beispielsweise fragt diese Frage, wie viele Äpfel Jake hat, enthält aber zusätzliche Informationen wie „17 weniger Pfirsiche“ und „Steven hat 8 Pfirsiche“, die völlig irrelevant sind. Unser Modell macht Vorhersagen, die negative Werte erzeugen. Wir beobachten, dass diese beiden Expressions ähnliche Scores haben. Wir können diesen Suchraum einschränken, indem wir diese negativen Ergebnisse entfernen, um die Antwort korrekt zu machen. Diese Einschränkung verbessert die Leistung einiger Modelle erheblich. Beispielsweise verbessert sich BERT um sieben Punkte, und der RoBERTa-basierte Modell um zwei Punkte. Bessere Sprachmodelle verfügen über eine bessere Sprachverständnisfähigkeit, was zu höheren Werten bei RoBERTa und niedrigeren Werten bei BERT führt.\n\nWir analysieren auch die Schwierigkeit hinter diesen Datensätzen und gehen davon aus, dass die Anzahl der nicht verwendeten Mengen als irrelevante Informationen angesehen werden kann. Hier sehen wir den Prozentsatz der Beispiele mit nicht verwendeten Mengen, und die SWAMP-Datenmenge hat den höchsten Anteil. Wir zeigen auch die Gesamtleistung für Beispiele ohne nicht verwendete Mengen, die höher ist als die Gesamtleistung. Bei Beispielen mit nicht verwendeten Mengen ist die Leistung jedoch deutlich schlechter als die Gesamtleistung. Bei MAWPS haben wir nicht viele Todesfälle, daher ignoriere ich diesen Teil.\n\nAbschließend möchten wir die Interpretierbarkeit durch ein Beispiel veranschaulichen. Hier macht unser Modell in dem ersten Schritt eine falsche Vorhersage. Wir können diese Expression mit dem Satz hier korrelieren. Wir vermuten, dass dieser Satz das Modell zu einer falschen Vorhersage verleitet. Durch das Drucken von „weitere 35“ geht das Modell davon aus, dass es sich um einen Additionsoperator handeln sollte. Wir versuchen, den Satz zu ändern, indem wir schreiben: „Die Anzahl der Birnbäume ist 55 geringer als die der Apfelbäume“. So vermitteln wir genauere Semantik, sodass das Modell die Vorhersage korrigieren kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Verhalten des Modells zu verstehen.\n\nZusammenfassend ist unser Modell sehr effizient und liefert interpretierbare Lösungsverfahren. Wir können leicht vorheriges Wissen als Einschränkung einbeziehen, was die Leistung verbessert. Die zugrunde liegende Mechanik gilt nicht nur für mathematische Problem-Lösungs-Aufgaben, sondern auch für andere Aufgaben, die mehrschrittiges Schlussfolgern erfordern. Es gibt jedoch auch Einschränkungen. Bei einer großen Anzahl von Operatoren oder Konstanten kann der Speicherverbrauch sehr hoch sein. Außerdem ist es, wie bereits erwähnt, aufgrund der unausgeglichenen Wahrscheinlichkeitsverteilung an verschiedenen Zeit Schritten schwierig, eine Beam-Search-Strategie anzuwenden. Das war es mit meinem Vortrag, und ich stehe gerne für Fragen zur Verfügung. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry präsentieren, die sich mit einem neuen Datensatz für die Retrieval von Gesetzestexten befasst. Rechtliche Fragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat wenig bis gar kein Wissen über ihre Rechte und grundlegenden rechtlichen Prozesse. Als Folge bleiben viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, ungeschützt oder werden im schlimmsten Fall ausgenutzt. Unsere Arbeit zielt darauf ab, die Lücke zwischen Menschen und dem Recht zu schließen, indem wir effektive Retrieval-Systeme für Gesetzestexte entwickeln. Ein solches System könnte einen kostenlosen Rechtsbeistand für ungeübte Personen bereitstellen.\n\nBevor wir zur Hauptbeitrag dieser Arbeit kommen, beschreiben wir zunächst das Problem der Retrieval von Gesetzestexten. Bei einer einfachen Frage zu einem Rechtsgebiet, wie z.B. \"Welche Risiken drohen mir, wenn ich die berufliche Schweigepflicht verletze?\", ist ein Modell erforderlich, das alle relevanten Gesetzestexte aus einem großen Rechtsbestand extrahiert. Diese Informationsretrieval-Aufgabe bringt ihre eigenen Herausforderungen mit sich. Erstens befasst sie sich mit zwei Arten von Sprache: Alltagssprache für die Fragen und komplexe juristische Sprache für die Gesetze. Dieser Unterschied in der Sprachverteilung erschwert es einem System, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine juristische Frage übersetzen kann, die der Terminologie der Gesetze entspricht. Darüber hinaus ist das Gesetzbuch keine Sammlung unabhängiger Artikel, die als vollständige Informationsquelle für sich stehen können, wie z.B. Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung rechtlicher Bestimmungen, die nur in ihrem Gesamtkonsens eine Bedeutung haben, also zusammen mit den ergänzenden Informationen aus den benachbarten Artikeln, den Feldern und Unterfeldern, zu denen sie gehören, und ihrer Position in der Struktur des Gesetzes. Zuletzt sind die Gesetzestexte in kleine Absätze unterteilt, die in den meisten Retrieval-Arbeiten typischerweise die übliche Retrieval-Einheit darstellen. Hier haben wir es mit langen Dokumenten zu tun, die bis zu 6.000 Wörter umfassen können.\n\nDie jüngsten Fortschritte im Bereich der NLP haben ein großes Interesse an vielen rechtlichen Aufgaben geweckt, wie z.B. der Vorhersage rechtlicher Urteile oder der automatisierten Vertragsprüfung, aber die Retrieval von Gesetzestexten ist aufgrund des Mangels an großen und hochwertigen labelten Datensätzen weitgehend unberührt geblieben. In dieser Arbeit stellen wir einen neuen, auf französische Bürger ausgerichteten Datensatz vor, um zu untersuchen, ob Retrieval-Modelle die Effizienz und Zuverlässigkeit von Rechtsexperten bei der Aufgabe der Retrieval von Gesetzestexten annähern können. Unser belgischer Datensatz für die Retrieval von Gesetzestexten, PSART, besteht aus mehr als 1.100 rechtlichen Fragen belgischer Bürger. Diese Fragen decken ein breites Themenspektrum ab, von Familie, Wohnen, Geld bis hin zu Arbeit und Sozialversicherung. Jede Frage wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als 22.600 Gesetzestexten aus belgischen Gesetzbüchern labelt.\n\nLassen Sie uns nun darüber sprechen, wie wir diesen Datensatz erstellt haben. Zunächst kompilierten wir einen großen Korpus von Gesetzestexten. Wir berücksichtigten 32 öffentlich zugängliche belgische Gesetzbücher und extrahierten alle Artikel sowie die entsprechenden Abschnittüberschriften. Dann sammelten wir rechtliche Fragen mit Verweisen auf relevante Gesetze. Dazu arbeiteten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr rund 4.000 E-Mails von belgischen Bürgern erhält, die Rat zu einer persönlichen Rechtsangelegenheit suchen. Wir hatten das Glück, Zugang zu ihrer Website zu erhalten, auf der ihr Team erfahrener Juristen die häufigsten rechtlichen Probleme in Belgien behandelt. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Gesetze annotiert waren. Abschließend analysierten wir die rechtlichen Verweise und filterten die Fragen heraus, deren Verweise keine Artikel aus einem der von uns berücksichtigten Gesetzbücher waren. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Am Ende hatten wir 1.108 Fragen, jede sorgfältig labelt mit den IDs der relevanten Artikel aus unserem großen Korpus von 22.633 Gesetzestexten. Zusätzlich verfügt jede Frage über eine Hauptkategorie und eine Verkettung von Unterkategorien, und jeder Artikel über eine Verkettung der nachfolgenden Überschriften in der Struktur des Gesetzes. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschungen im Bereich der rechtlichen Informationsretrieval oder der Klassifizierung rechtlicher Texte von Interesse sein.\n\nBetrachten wir nun einige Merkmale unseres Datensatzes. Die Fragen sind zwischen 5 und 44 Wörter lang, mit einem Median von 40 Wörtern. Die Artikel sind viel länger, mit einem Median von 77 Wörtern, und 142 von ihnen überschreiten 1.000 Wörter, wobei der längste Artikel bis zu 5.790 Wörter umfasst. Wie bereits erwähnt, decken die Fragen ein breites Themenspektrum ab, wobei etwa 85 % von ihnen sich mit Familie, Wohnen, Geld oder Justiz befassen, während die restlichen 15 % Sozialversicherung, Ausländer oder Arbeit betreffen. Auch die Artikel sind sehr vielfältig, da sie aus 32 verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl rechtlicher Themen abdecken. Hier ist die Gesamtzahl der aus jedem dieser belgischen Gesetzbücher gesammelten Artikel. Von den 22.633 Artikeln werden nur 1.612 als für mindestens eine Frage im Datensatz relevant angegeben. Und etwa 80 % dieser zitierten Artikel stammen entweder aus dem Bürgerlichen Gesetzbuch, dem Gerichtsverfassungsgesetz, dem Strafprozessgesetzbuch oder dem Strafgesetzbuch. Währenddessen haben 18 der 32 Gesetzbücher weniger als 5 Artikel, die als für mindestens eine Frage relevant angegeben werden, was darauf zurückzuführen sein kann, dass diese Gesetzbücher weniger auf Einzelpersonen und ihre Anliegen ausgerichtet sind. Insgesamt beträgt die mittlere Zitierzahl für diese zitierten Artikel 2, und weniger als 25 % von ihnen werden mehr als 5 Mal zitiert.\n\nMit unseren Datensätzen benchmarken wir mehrere Retrieval-Ansätze, einschließlich lexikalischer und dichter Architekturen. Bei einer Abfrage und einem Artikel weist ein lexikalisches Modell eine Bewertung für das Abfrage-Artikel-Paar zu, indem es die Summe der Gewichte jedes in der Abfrage enthaltenen Terms in diesem Artikel berechnet. Wir experimentieren mit den Standard-TF-IDF- und BM25-Ranking-Funktionen. Das Hauptproblem dieser Ansätze besteht darin, dass sie nur Artikel zurückrufen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer auf neuronalen Netzen basierenden Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein b-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet und eine Relevanzbewertung zwischen einem Abfrage-Artikel-Paar durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen ergeben sich typischerweise aus einer Pooling-Operation auf der Ausgabe eines Wort-Einbettungsmodells.\n\nZunächst untersuchen wir die Effektivität von Siamese b-Encodern in einem Zero-Shot-Evaluierungs-Setup, was bedeutet, dass vorab trainierte Wort-Einbettungsmodelle direkt ohne zusätzliche Feinabstimmung angewendet werden. Wir experimentieren mit kontextunabhängigen Text-Encodern, nämlich Word2Vec und FastText, und kontextabhängigen Einbettungsmodellen, nämlich Robota und spezifischer Camembert, einem französischen Robota-Modell. Zusätzlich trainieren wir eigene Camembert-basierte b-Encoder-Modelle auf unseren Datensätzen. Es sei darauf hingewiesen, dass wir bei der Trainingsdurchführung zwei Varianten der b-Encoder-Architektur untersuchen. Siamese, das ein einziges Wort-Einbettungsmodell verwendet, das die Abfrage und den Artikel gemeinsam in einen dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Wort-Einbettungsmodelle verwendet, die die Abfrage und den Artikel separat in unterschiedliche Einbettungsräume abbilden. Wir experimentieren mit Mean-, Max- und CLS-Pooling sowie Dot-Product und Cosine für die Berechnung von Ähnlichkeiten.\n\nHier sind die Ergebnisse unserer Baseline-Methoden, der oben genannten lexikalischen Methoden, der Siamese b-Encoder, die in einem Zero-Shot-Setup evaluiert wurden, in der Mitte, und der feinabgestimmten b-Encoder darunter. Insgesamt übertreffen die feinabgestimmten b-Encoder alle anderen Baseline-Methoden deutlich. Das Two-Tower-Modell verbessert seine Siamese-Variante in Bezug auf die Recall-Rate bei 100, zeigt aber bei den anderen Metriken ähnliche Leistungen. Obwohl BM25 im Vergleich zum trainierten B-Encoder schlechter abschnitt, deutet seine Leistung darauf hin, dass es immer noch eine starke Baseline für domänenspezifische Retrieval-Aufgaben ist.\n\nBezüglich der Zero-Shot-Evaluierung der Siamese B-Encoder stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorab trainierten Camembert-Modells ohne Optimierung für die Informationsretrieval-Aufgabe zu schlechten Ergebnissen führt, was mit früheren Erkenntnissen übereinstimmt. Darüber hinaus beobachten wir, dass der auf Word2Vec basierende b-Encoder die auf FastText und BERT basierenden Modelle deutlich übertrifft, was darauf hindeutet, dass vorab trainierte wortbasierte Einbettungen für diese Aufgabe möglicherweise besser geeignet sind als charakterbasierte oder subwortbasierte Einbettungen.\n\nObwohl vielversprechend, deuten diese Ergebnisse auf erhebliche Verbesserungsmöglichkeiten hin, verglichen mit einem geschulten Rechtsexperten, der letztendlich alle relevanten Artikel zu jeder Frage abrufen und damit perfekte Ergebnisse erzielen kann.\n\nAbschließend wollen wir auf zwei Einschränkungen unserer Datensätze eingehen. Erstens ist der Korpus der Artikel auf die aus den 32 berücksichtigten belgischen Gesetzbüchern gesammelten beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Verordnungen, Richtlinien und Erlassen fehlen. Während der Erstellung des Datensatzes wurden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen nur einen Bruchteil der ursprünglichen Anzahl relevanter Artikel haben. Dieser Informationsverlust bedeutet, dass die Antwort, die in den verbleibenden relevanten Artikeln enthalten ist, unvollständig sein könnte, obwohl sie immer noch vollständig angemessen ist. Zweitens sollten wir beachten, dass nicht alle rechtlichen Fragen allein mit Gesetzestexten beantwortet werden können. Beispielsweise könnte die Frage \"Kann ich meine Mieter rauswerfen, wenn sie zu viel Lärm machen?\" innerhalb des Gesetzestextes keine detaillierte Antwort enthalten, die einen spezifischen Lärmschwellenwert für eine Räumung quantifiziert. Stattdessen sollte sich der Vermieter wahrscheinlich eher auf die Rechtsprechung stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind. Beispielsweise macht der Mieter zweimal pro Woche bis 2 Uhr morgens eine Party. Daher eignen sich einige Fragen besser für die Retrieval von Gesetzestexten als andere, und der Bereich der weniger geeigneten Fragen bleibt zu bestimmen.\n\nWir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Modelle für die Retrieval von Gesetzestexten weckt, die dazu beitragen können, den Zugang zur Gerechtigkeit für alle zu verbessern. Sie können unsere Arbeit, den Datensatz und den Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Wir freuen uns, unsere Arbeit an VALS vorzustellen, einem task-unabhängigen Benchmark, der dazu dient, Vision- und Sprachmodelle mit spezifischen linguistischen Phänomenen zu testen. Warum haben wir uns die Mühe gemacht, diesen Benchmark zu erstellen? In den letzten Jahren haben wir eine Explosion von transformer-basierten Vision- und Sprachmodellen erlebt, die auf großen Mengen von Bild-Text-Paaren vorabtrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei Vision- und Sprachaufgaben wie visueller Fragebeantwortung, visuellem gesundem Menschenverstand, Bildabruf und Phrasenverankerung. Wir haben jedoch eine Botschaft – die Genauigkeiten bei diesen task-spezifischen Benchmarks steigen stetig. Aber wissen wir, was die Modelle tatsächlich gelernt haben? Was versteht ein Vision- und Sprachtransformer, wenn er für dieses Bild und diesen Satz eine hohe Übereinstimmung bewertet und für diesen eine niedrige Bewertung? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige, oder konzentrieren sie sich auf Verzerrungen, wie vorherige Arbeiten gezeigt haben? Um mehr Licht auf diesen Aspekt zu werfen, schlagen wir eine eher task-agnostische Richtung vor und führen VALVes ein, die die Sensitivität von Vision- und Sprachmodellen für spezifische linguistische Phänomene testen, die sowohl die sprachliche als auch die visuelle Modalität beeinflussen. Wir zielen auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz ab. Aber wie testen wir, ob Vision- und Sprachmodelle diese Phänomene erfasst haben? Durch FOILing, eine Methode, die zuvor für Vision- und Sprachmodelle angewendet wurde, nur für Nominalphrasen von Ravi Shekhar und Mitarbeitern, auf Zählen von uns in vorheriger Arbeit. FOILing bedeutet im Grunde, dass wir die Bildunterschrift eines Bildes nehmen und eine Fälschung erstellen, indem wir die Unterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Und wir führen diese Phrasenänderungen durch, indem wir uns auf sechs spezifische Bereiche konzentrieren, wie Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz, wobei jeder Bereich aus einem oder mehreren Instrumenten bestehen kann, falls wir mehr als eine interessante Möglichkeit gefunden haben, FOIL-Instanzen zu erstellen. Beispielsweise haben wir im Fall des Handlungsbereichs zwei Instrumente, eines, bei dem das Handlungsverb durch eine andere Handlung ersetzt wird, und eines, bei dem die Actanten getauscht werden. Zählen und Coreferenz sind ebenfalls Bereiche mit mehr als einem Instrument. Und wir erstellen diese Fälschungen, indem wir sicherstellen, dass sie das Bild nicht beschreiben, dass sie grammatikalisch und ansonsten gültige Sätze sind. Dies ist nicht einfach zu tun, da eine gefälschte Bildunterschrift weniger wahrscheinlich sein kann als die ursprüngliche Bildunterschrift. Zum Beispiel ist es zwar nicht unmöglich, aber statistisch gesehen weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten diese Verzerrung aufgreifen. Daher müssen wir, um gültige Fälschungen zu erhalten, Maßnahmen ergreifen. Erstens nutzen wir starke Sprachmodelle, um Fälschungen vorzuschlagen. Zweitens verwenden wir natürliche Sprachinferenz (NLI), um Fälschungen auszufiltern, die das Bild möglicherweise immer noch beschreiben könnten, da wir beim Erstellen von Fälschungen sicherstellen müssen, dass sie das Bild nicht beschreiben. Um dies automatisch zu testen, wenden wir natürliche Sprachinferenz mit der folgenden Logik an. Wir betrachten ein Bild als Prämissen und seine Bildunterschrift als impliziert. Darüber hinaus betrachten wir die Bildunterschrift als Prämissen und die FOIL als Hypothese. Wenn ein NLI-Modell vorhersagt, dass die FOIL der Bildunterschrift widerspricht oder neutral gegenüber ihr ist, nehmen wir dies als Indikator für eine gültige FOIL. Wenn ein NLI die FOIL als durch die Bildunterschrift impliziert vorhersagt, kann es keine gute FOIL sein, da es durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefern würde, und wir filtern diese FOILs aus. Aber dieses Verfahren ist nicht perfekt. Es ist nur ein Indikator für gültige FOILs, daher setzen wir als dritte Maßnahme zur Erstellung gültiger FOILs menschliche Annotatoren ein, um die in VALS verwendeten Daten zu validieren. So haben wir nach dem Filtern und der menschlichen Bewertung so viele Testinstanzen wie in dieser Tabelle beschrieben. Beachten Sie, dass VALS keine Trainingsdaten liefert, sondern nur Testdaten, da es sich um einen Zero-Shot-Test-Benchmark handelt. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vorabtraining zu nutzen. Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Und wie gesagt, wir sind daran interessiert, zu bewerten, welche Fähigkeiten Vision- und Sprachmodelle nach dem Vorabtraining haben. Wir experimentieren mit fünf Vision- und Sprachmodellen auf VALS, nämlich CLIP, LXMERT, Wil VILBERT, VILBERT 12-in-1 und VISUALBERT. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren in Bildunterschriften und Fälschungen. Vielleicht relevanter für dieses Video werden wir unsere permissivere Metrik, die paarweise Genauigkeit, vorstellen, die misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein gefälschtes Paar. Für weitere Metriken und Ergebnisse siehe unsere Arbeit. Die Ergebnisse mit paarweiser Genauigkeit sind hier gezeigt und stimmen mit den Ergebnissen überein, die wir mit den anderen Metriken erhalten haben, dass die beste Zero-Shot-Leistung von Wilbert 12-in-1 erreicht wird, gefolgt von Wilbert, Alex Mert, Clip und schließlich Visual Bird. Es ist bemerkenswert, wie Instrumente, die sich auf einzelne Objekte wie Existenz und Nominalphrasen konzentrieren, von Wilbert 12-in-1 fast gelöst werden, was zeigt, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Keiner der verbleibenden Bereiche kann jedoch in unseren adversären Fälschungseinstellungen zuverlässig gelöst werden. Wir sehen aus den Pluralitäts- und Zählungsinstrumenten, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne gegenüber mehreren Objekten zu unterscheiden oder sie in einem Bild zu zählen. Der Relationsbereich zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, auch wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir im Handlungsbereich sehen. Aus dem Coreferenzbereich geht hervor, dass das Nachverfolgen mehrerer Referenzen auf dasselbe Objekt in einem Bild unter Verwendung von Pronomen für Vision- und Sprachmodelle ebenfalls schwierig ist. Als Plausibilitätsprüfung und weil es ein interessantes Experiment ist, benchmarken wir auch zwei Text-only-Modelle, GPT-1 und GPT-2, um zu bewerten, ob VALS durch diese unimodalen Modelle gelöst werden kann, indem wir die Perplexität der korrekten und der gefälschten Bildunterschrift berechnen und den Eintrag mit der niedrigsten Perplexität vorhersagen. Wenn die Perplexität für die Fälschung höher ist, nehmen wir dies als Hinweis darauf, dass die gefälschte Bildunterschrift möglicherweise unter Plausibilitätsverzerrung oder anderen linguistischen Verzerrungen leidet. Und es ist interessant zu sehen, dass die Text-only-GPT-Modelle in einigen Fällen die Plausibilität der Welt besser erfasst haben als die Vision- und Sprachmodelle. Zusammenfassend ist VALS ein Benchmark, der die linguistischen Konstrukte als Linse verwendet, um der Gemeinschaft zu helfen, Vision- und Sprachmodelle durch hartes Testen ihrer visuellen Verankerungsfähigkeiten zu verbessern. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte und ihre Anwesenheit in Bildern gut identifizieren können, wie der Existenzbereich zeigt, aber Schwierigkeiten haben, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen sind, linguistische Indikatoren zu respektieren. Wir möchten die Gemeinschaft wirklich ermutigen, VALS für die Messung des Fortschritts bei der Sprachverankerung mit Vision- und Sprachmodellen zu verwenden. Und noch mehr könnte VALS als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich in einem der von VALS getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, sehen Sie sich die VALS-Daten auf GitHub an, und wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamizawa von der Universität Tokio. Ich werde einen Artikel mit dem Titel RNSUN vorstellen, ein umfangreiches Datenset für die automatische Erstellung von Release-Notizen durch Zusammenfassung von Commit-Protokollen. Ich werde wie folgt vorgehen. Zunächst werde ich die automatische Erstellung von Release-Notizen vorstellen, an der wir in dieser Forschung arbeiten. Eine Release-Notiz ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts verteilt werden. Das Bild zeigt die Release-Notizen für die Version 2.6.4 der Vue.js-Bibliothek. Release-Notizen spielen eine wichtige Rolle in der Open-Source-Entwicklung, aber ihre manuelle Erstellung ist zeitaufwändig. Daher wäre es sehr nützlich, hochwertige Release-Notizen automatisch generieren zu können. Ich werde auf zwei frühere Forschungen zur automatischen Erstellung von Release-Notizen eingehen. Die erste ist ein System namens Arena, das 2014 veröffentlicht wurde. Es verfolgt einen regelbasierten Ansatz, beispielsweise durch die Verwendung eines Änderungsextraktors, um Kernunterschiede, Bibliotheksänderungen und Dokumentänderungen aus den Unterschieden zwischen den Versionen zu extrahieren und diese schließlich zu kombinieren. Das auffälligste Merkmal dieses Systems ist der Issue-Extraktor in der oberen rechten Ecke, der mit Jira, dem Issue-Tracker-System, verknüpft sein muss und nur auf Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Das zweite System, Glyph, wurde kürzlich im Jahr 2020 angekündigt. Es ist im Internet verfügbar und kann über PIP installiert werden. Dieses System verfügt über ein einfaches, auf dem Lernen basierendes Textklassifizierungsmodell und gibt für jede eingegebene Commit-Nachricht eine von fünf Stufen aus, wie beispielsweise Funktionen oder Fehlerbehebungen. Das Bild zeigt ein Beispiel für die Verwendung, das ein Korrektur- oder Fehlerbehebungs-Label zurückgibt. Glyphs Trainingsdaten sind relativ klein, etwa 5000, und in den unten beschriebenen Experimenten ist die Leistung des Textklassifizierungsmodells nicht hoch. Ich habe zwei verwandte Forschungen vorgestellt, aber es gibt Probleme mit der eingeschränkten Anwendbarkeit und knappen Datenressourcen. Unser Artikel löst diese beiden Probleme und generiert automatisch hochwertige Release-Notizen. Für das Problem der eingeschränkten Anwendbarkeit schlagen wir eine hochwertige Klassifizierungs-Zusammenfassungs-Methode vor, die nur Commit-Nachrichten als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem der knappen Datenressourcen haben wir ein RNSUM-Datenset bestehend aus etwa 82.000 Datensätzen erstellt, indem wir Daten aus öffentlichen GitHub-Repositories mithilfe der GitHub-API gesammelt haben. Als Nächstes beschreibe ich unser Datenset. Hier ist ein Beispiel für die Daten. Auf der linken Seite befindet sich die Commit-Nachricht, auf der rechten Seite die Release-Notizen. Die Release-Notizen sind als Verbesserungen von Gesichtern usw. gekennzeichnet. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die gekennzeichneten Release-Notizen ausgibt. Dies kann als Zusammenfassungsaufgabe betrachtet werden. Wir haben vier Labels vorgegeben: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecations, Entfernungen und Breaking Changes. Diese wurden auf der Grundlage früherer Forschungen und anderer Faktoren festgelegt. Die unteren Knoten in der unteren rechten Ecke werden aus den unteren Knoten in der unteren linken Ecke extrahiert. Zu diesem Zeitpunkt ist es notwendig, die vier vorab festgelegten Labels zu erkennen. Die Labels sind jedoch nicht immer konsistent mit jedem Repository. Beispielsweise umfasst das Verbesserungen-Label Verbesserungen, Erweiterungen, Optimierungen usw. Wir haben für jedes dieser notationsbedingten Variationen eine Vokabelliste unserer Studienlabels erstellt, sie verwendet, um die Klasse der Release-Notiz zu erkennen, und den Text der Liste korrigiert, der als Satz der Release-Notiz für die Klasse folgt, müssen die vorherige Release-Version von 2.5 bis 18 identifizieren und ihren Diff erhalten. Dies ist etwas mühsam und es reicht nicht aus, einfach eine Liste der Releases zu erhalten und die vorherigen und nachfolgenden Versionen zu betrachten. Wir haben eine heuristische Matching-Regel erstellt, um die vorherigen und nachfolgenden Versionen zu erhalten. Nach der Datensammlung. Letztendlich wurden 7.200 Repositories und 82.000 Datensätze gesammelt. Außerdem beträgt die durchschnittliche Anzahl der Release-Notizen-Token 63, was für eine Zusammenfassungsaufgabe recht hoch ist. Auch die Anzahl der eindeutigen Token ist mit 8.830.000 recht groß. Dies ist auf die große Anzahl eindeutiger Klassen- und Methodenbezeichnungen zurückzuführen, die im Repository gefunden wurden. Als Nächstes werde ich die vorgeschlagene Methode erklären. Die klassenweise extraktive und dann abstrakte Zusammenfassungs-Methode besteht aus zwei neuronalen Netzen. Das erste Netz klassifiziert jede Commit-Nachricht in fünf Release-Notizen-Klassen: Implementierungen, Fehlerbehebungen, Deprecations plus und andere. Die als andere klassifizierten Commit-Nachrichten werden verworfen. Anschließend wendet CEAS den Generator auf die vier Label-Dokumente unabhängig an und generiert für jede Klasse Release-Notizen. Bei dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Release-Notizen nicht bekannt. Daher weisen wir zur Schulung des Klassifikators jedem Eingabe-Commit-Nachricht mithilfe der ersten 10 Zeichen jeder Commit-Nachricht Pseudo-Labels zu. Wir modellieren die klassenweise abstrakte Zusammenfassung durch unseren Ansatz mit zwei verschiedenen Methoden. Das erste Modell, das wir CSSingle nennen, besteht aus einem einzelnen Set-to-Set-Netzwerk und generiert einen einzelnen langen Stück-Text, indem es die Eingabe-Commit-Nachrichten zusammenfügt. Das zweite Modell, CAS, besteht aus vier separaten Set-to-Set-Netzwerken, von denen jedes einem der am wenigsten bekannten Klassen entspricht. Okay, lassen Sie mich das Experiment erklären. Es wurden fünf Methoden verglichen: CAS, CASSingle, CASMatch, PlusSelling und eine frühere Studie, GRIF. Bezüglich der Abweichung wichen in einigen Fällen CASMatch, Blustering und die vorherige Studie Glyph ab. Bei der Bewertung werden die Release-Notizen manchmal in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze direkt zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Die blaue Farbe wird bestraft, wenn das System einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren Blau-Wert in den nächsten Experimentergebnissen. Schließlich berechnen wir auch die Spezifität, da Rouge und Blau nicht berechnet werden können, wenn die Release-Notizen leer sind. Eine hohe Spezifität bedeutet, dass das Modell in Fällen, in denen die Release-Notizen leer sein sollten, korrekt leeren Text ausgibt. Hier sind die Ergebnisse. Da das Datenset E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch das gereinigte Datenset bewertet, das diese ausschließt. CES und CAS erreichten Rouge-L-Scores, die mehr als 10 Punkte über den Baselines lagen. Insbesondere sprang der Punktabstand zwischen dem vorgeschlagenen Verfahren und den Baselines im sauberen Testset auf mehr als 20 Punkte. Diese Ergebnisse zeigen, dass CAS und CAS signifikant wirksam sind. CAS erzielte einen besseren ROUGE-A-Score als CAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators bei der Schulung des Klassifikators mithilfe von Zwei-Doubles effektiv ist. Eine hohe Abdeckung von CAS kann erreicht werden, wahrscheinlich weil der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann. CASMatch neigte dazu, höhere ROUGE-L-Werte als CASSingle zu erzielen, was darauf hindeutet, dass es auch effektiv ist, für jede Release-Notizen-Klasse unabhängig voneinander unterschiedlich abstrakte Zusammenfassungsmodelle zu entwickeln. Hier ist eine Fehleranalyse. CAS-Methoden neigen dazu, kürzere Sätze als menschliche Referenzsätze auszugeben. Im rechten Bild hat der Referenzsatz drei oder vier Sätze, während CAS nur einen hat. Der Grund für diese Zurückhaltung des Modells liegt darin, dass in den Trainingsdaten nur 33 % der Sätze im Funktionen-Label und 40 % im Verbesserungen-Label vorhanden sind. Darüber hinaus können CES-Methoden ohne zusätzliche Informationen keine genauen Release-Notizen generieren. Das obere Beispiel rechts zeigt eine sehr unordentliche Commit-Nachricht, und der vollständige Satz kann nicht generiert werden, ohne auf die entsprechende Pro-Anfrage oder das Issue zu verweisen. Das untere Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe miteinander in Verbindung stehen und zu einem Satz kombiniert werden sollten, was jedoch misslingt. Abschließend ein Fazit. Wir haben ein neues Datenset für die automatische Erstellung von Listen erstellt. Wir haben auch die Aufgabe definiert, Commit-Nachrichten einzugeben und sie zusammenzufassen, sodass sie auf alle in Englisch geschriebenen Projekte anwendbar ist. Unser Experiment zeigt, dass die vorgeschlagene Methode im Vergleich zu den Baselines Release-Notizen mit geringerem Rauschen und höherer Abdeckung generiert. Bitte besuchen Sie unsere Abstieg-nur-Registerkarte. Vielen Dank."}
