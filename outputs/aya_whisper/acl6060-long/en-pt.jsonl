{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Asaf Harari e eu apresentarei nosso artigo, \"Enriquecimento de Dados Tabulares com Poucos Exemplos Usando Arquiteturas de Transformadores Ajustados Finamente\". Os cientistas de dados analisam dados e se concentram principalmente na manipulação dos recursos existentes dos dados. Mas, às vezes, esses recursos são limitados. A geração de recursos usando outra fonte de dados pode adicionar informações significativas. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando texto livre de fontes externas. A geração de recursos usando outra fonte de dados pode adicionar informações significativas. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando texto livre de fontes externas.\n\nSuponha que tenhamos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a ligação de entidades e a análise de texto para extrair novos recursos do texto livre da base de conhecimento. Nosso framework, FAST, é exatamente esse processo automático. Vamos ver um exemplo. Em um conjunto de dados alimentado no FAST, neste exemplo, o conjunto de dados é um conjunto de dados de universidades, cujo objetivo é classificar universidades em universidades de baixo e alto escalão. Como base de conhecimento, usamos a Wikipédia.\n\nA primeira fase do FAST é a ligação de entidades. Quando cada entidade, neste exemplo, o nome da universidade, é ligada a uma entidade dentro da base de conhecimento. E o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipédia. Agora, precisamos gerar ou extrair recursos do texto recuperado. Então, precisamos de uma fase de extração de recursos, que inclui análise de texto. E esta é a principal novidade deste artigo, e eu aprofundarei isso nas próximas slides.\n\nApós a fase de extração de recursos, há uma fase de geração de recursos, na qual usamos os recursos extraídos para gerar um pequeno número de novos recursos. Primeiro, geramos recursos no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então o FAST gera dois novos recursos. Mas se o conjunto de dados tiver cinco classes, o FAST gera cinco novos recursos. Cada recurso representa a probabilidade para cada classe.\n\nPara analisar o texto, usamos o estado da arte atual da análise de texto, que são modelos de linguagem baseados em transformadores, como BERT, GPT, XNL, entre outros. No entanto, não é provável que possamos treinar um modelo de linguagem usando os conjuntos de dados de entrada. Então, uma abordagem ingênua seria o ajuste fino para a tarefa-alvo. Assim, na fase de extração de recursos, podemos baixar um modelo de linguagem pré-treinado, ajustá-lo finamente ao conjunto de dados-alvo. Neste exemplo, para ajustar finamente o modelo de linguagem, classificamos o texto em classes, resumos em classes, baixo ou alto, recebemos a saída do modelo de linguagem, que é a probabilidade para cada classe, e a usamos como novos recursos.\n\nO problema com essa abordagem é que os conjuntos de dados podem ter poucas etiquetas de entidades distintas. Em nossa experiência, quase metade dos conjuntos de dados contém menos de 400 amostras, e o menor conjunto de dados continha 35 amostras em seu conjunto de treinamento. Então, ajustar finamente um modelo de linguagem sobre esse conjunto de dados seria ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados e usar essa informação quando analisamos o N-ésimo conjunto de dados.\n\nO que sugerimos é adicionar outra fase de ajuste fino, uma fase de ajuste fino multitarefa preliminar, quando ajustamos finamente o modelo de linguagem em N-1 conjuntos de dados, e então executamos outra fase de ajuste fino, que é o ajuste fino da tarefa-alvo, quando ajustamos finamente o modelo de linguagem no N-ésimo conjunto de dados-alvo. O estado da arte no ajuste fino multitarefa é chamado de mtDNN. No mtDNN, o mtDNN mantém cabeças no número de tarefas no conjunto de treinamento. Então, neste exemplo, há quatro tarefas no conjunto de treinamento, então o mtDNN mantém quatro cabeças, como você pode ver na imagem, e ele amostra um lote aleatório do conjunto de treinamento. E se o lote aleatório pertencer a, por exemplo, uma tarefa de classificação de sentença única, ele executa o passe para frente e para trás através da primeira cabeça. E se o lote aleatório pertencer a uma tarefa de classificação par a par, ele executa o passe para frente e para trás através da última cabeça.\n\nEm nosso cenário, os conjuntos de dados tabulares variam no número de classes. Então, em nosso cenário, um conjunto de dados tabular verifica o número de classes. Então, há muitas tarefas. O MTDNN mantém cabeças no número de classes, camadas de saída, e além disso, o MTDNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem, chamada de ajuste fino de reformulação de tarefas, reformula cada conjunto de dados em uma sentença por problema de classificação, que é uma tarefa de duas classes. Em vez de manter várias cabeças, reformulamos cada conjunto de dados em uma sentença por problema de classificação, que são tarefas de duas classes.\n\nVamos ver um exemplo. Aqui está nosso conjunto de dados de entrada, que consiste em entidades, recursos, texto e classes. E nossa tarefa é classificar o texto em baixo e alto, classificar o resumo e a classe, se o resumo pertence à classe ou não. Então, o vetor de rótulo neste caso permanece sempre com duas classes. E isso é. Então, reformula a tarefa em tarefas de classificação de sentença única. Aplique o modelo de linguagem à nova tarefa e a saída de probabilidade para cada classe. E note que o modelo de linguagem já foi ajustado finamente em N-1 conjuntos de dados usando um ajuste fino multitarefa preliminar. Então, usamos o vetor de saída do modelo de linguagem como um recurso gerado recentemente no número de classes.\n\nPara avaliar nosso framework, usamos 17 conjuntos de dados de classificação tabular, que verificam tamanho, recursos, equilíbrio, domínio e desempenho inicial. E, como base de conhecimento, usamos a Wikipédia. Projetamos nossa experiência como uma avaliação de k-folds, quando treinamos o FAST em 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro folds e aplicamos uma validação cruzada de quatro folds. Então, geramos o novo recurso e os avaliamos usando cinco classificadores de avaliação.\n\nUsamos uma arquitetura baseada em transformadores em nossa experiência. Aqui estão os resultados de nossa experiência. Você pode ver que comparamos nosso framework com o ajuste fino do conjunto de dados-alvo, o ajuste fino da tarefa-alvo e o ajuste fino preliminar do mtDNN, e nosso ajuste fino reformulado alcançou o melhor resultado, o melhor desempenho. Enquanto o MTDNN alcançou uma melhoria de dois por cento em relação ao ajuste fino do conjunto de dados-alvo, nossa abordagem alcançou uma melhoria de seis por cento. Quando olhamos para os pequenos conjuntos de dados, podemos ver que o desempenho do DNN vazio diminui e a melhoria do ajuste fino da tarefa-alvo sozinho.\n\nEm resumo, o FAST permite o enriquecimento com poucos exemplos a partir de 35 amostras em nossa experiência. Ele usa uma única arquitetura para todos os conjuntos de dados de tarefas e mantém a cabeça do modelo. Mas adiciona uma fase de reformulação. Ele aumenta o conjunto de treinamento e precisa de um valor-alvo com significado semântico, para que possamos alimentá-lo no modelo de linguagem e usá-lo no problema de classificação de sentença única. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos, hoje vou apresentar nosso trabalho de pesquisa, Aprendendo a Raciocinar Detetivamente: Resolução de Problemas Metabólicos como Extração de Razão Complexa. Sou Alan do ByteDance AI Lab, e este é um trabalho conjunto com Jerry da Universidade do Texas em Austin e Weilu do SUTD. Primeiro, gostaria de falar sobre nossa motivação para o raciocínio. Aqui, apresentamos um exemplo no qual o raciocínio multi-passo é útil. Esta figura é retirada de um cenário de aprendizagem com poucos exemplos, onde se realiza o prompt para resolver um problema matemático no papel. No lado esquerdo, podemos ver que, se fornecermos apenas exemplos com perguntas e respostas, talvez não consigamos obter as respostas corretas. Mas, se fornecermos uma descrição mais detalhada do raciocínio, o modelo consegue prever a descrição do raciocínio e também fazer uma previsão correta. Portanto, é benéfico ter um raciocínio multi-passo interpretável como saída. Também consideramos que o problema de método é uma aplicação direta para avaliar tais habilidades de raciocínio.\n\nEm nossa configuração de problema, dadas as perguntas, precisamos resolver e obter as respostas numéricas. Em nossos conjuntos de dados, também recebemos a expressão matemática que leva a essa resposta específica. Certas suposições também se aplicam, como em trabalhos anteriores. Supomos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtração, multiplicação, divisão e exponenciação. Além disso, operadores complicados podem ser decompostos nesses operadores básicos.\n\nTrabalho anterior na resolução de problemas de método pode ser categorizado em modelos sequência-para-sequência e sequência-para-árvore. O modelo sequência-para-sequência tradicional converte a expressão em uma sequência específica para geração, é fácil de implementar e pode generalizar para muitos problemas complicados. No entanto, a desvantagem é que o desempenho geralmente não é melhor que o modelo estrutural e falta interpretabilidade na previsão. Mas essa abordagem ainda é popular devido ao modelo transformador.\n\nEm modelos baseados em árvore, estruturamos as expressões na forma de árvore e seguimos uma travessia em pré-ordem na geração. Aqui, continuamos gerando operadores até alcançarmos as folhas, que são as quantidades. A vantagem é que obtemos uma estrutura de árvore binária, mas é contra-intuitivo porque geramos o operador primeiro e só no final geramos as quantidades. Além disso, há computações repetitivas. Aqui, se observarmos a expressão \"a vezes 3 mais 3\", ela é gerada duas vezes. Devemos reutilizar os resultados, então, em nossa abordagem proposta, queremos resolver esses problemas de forma passo a passo e interpretável. Por exemplo, no segundo passo, podemos obter o divisor, que é 27, e também podemos voltar à pergunta original para encontrar o conteúdo relevante.\n\nNo terceiro passo, obtemos o quociente. Após esses três passos, podemos reutilizar os resultados do segundo passo e obter os resultados do quarto passo. Finalmente, podemos obter os dividendos. Aqui, geramos a expressão completa diretamente, em vez de gerar operadores ou quantidades individuais. Isso torna o processo mais preciso.\n\nEm nosso sistema dedutivo, começamos com um conjunto de quantidades apresentadas nas perguntas, incluindo algumas constantes como estados iniciais. A expressão é representada por EIJOP, onde realizamos o operador de QI para QJ, e essa expressão é direcionada. Também temos a subtração inversa para representar a direção oposta, o que é semelhante à extração de relação. Em um sistema dedutivo formal, no passo de tempo t, aplicamos o operador entre o par QI e QJ, e obtemos novas expressões. Adicionamos isso ao próximo estado para se tornar uma nova quantidade. Esta apresentação visualiza a evolução dos estados, onde continuamos adicionando expressões aos estados atuais.\n\nEm nossas implementações de modelo, primeiro usamos um modelo de linguagem pré-treinado, que pode ser BERT ou RoBERTa, e então codificamos uma frase para obter as representações das quantidades. Uma vez obtidas as representações das quantidades, podemos começar a inferir. Aqui, mostramos um exemplo de Q1 para obter a representação de Q1 dividido por Q2 e vezes Q3. Primeiro, obtemos a representação do par, que é basicamente a concatenação de Q1 e Q2. Em seguida, aplicamos uma rede feedforward, parametrizada pelo operador, e finalmente obtemos a representação da expressão Q1 dividido por Q2.\n\nNa prática, na etapa de inferência, também podemos obter expressões incorretas. Aqui, todas as expressões possíveis são iguais a três vezes o número de operadores. A vantagem é que podemos adicionar facilmente restrições para controlar esse espaço de pesquisa. Por exemplo, se essa expressão não for permitida, podemos removê-la do espaço de pesquisa. No segundo passo, fazemos o mesmo, mas a diferença é que há mais uma quantidade, que vem da expressão calculada anteriormente. Finalmente, obtemos a expressão final, Q3 vezes Q4, e podemos ver que o número de expressões possíveis é diferente do passo anterior.\n\nEssas diferenças tornam difícil aplicar a busca de feixe (beam search) porque a distribuição de probabilidade entre esses dois passos é desequilibrada. O procedimento de treinamento é semelhante ao de um modelo sequência-para-sequência, onde otimizamos a perda em cada passo de tempo. Aqui, também usamos τ para representar quando devemos terminar o processo de geração. O espaço é diferente do modelo sequência-para-sequência tradicional, pois muda em cada passo de tempo, permitindo também impor restrições de conhecimento prévio.\n\nRealizamos experimentos em conjuntos de dados de problemas de método comumente usados: MAWPS, Math23k, MathQA e SWAM. Aqui, mostramos brevemente os resultados em comparação com as abordagens anteriores. Nossa variante de melhor desempenho é o Robeta Deductive Reasoner. Na verdade, não usamos busca de feixe, ao contrário das abordagens anteriores, que geralmente são modelos baseados em árvore. Em geral, nosso raciocínio supera significativamente esses modelos baseados em árvore, mas podemos ver que os números absolutos em MathQA ou SWAM não são muito altos.\n\nInvestigamos mais a fundo os resultados em SWAM, que é um conjunto de dados desafiador porque o autor tentou adicionar manualmente informações para confundir o modelo de NLP, como informações irrelevantes e quantidades extras. Em nossas previsões, encontramos alguns valores intermediários negativos. Por exemplo, na pergunta \"Quantas maçãs Jake tem?\", há informações extras como \"17 pêssegos a menos\" e \"Steven tem 8 pêssegos\", que são totalmente irrelevantes. Nosso modelo faz previsões como essa, produzindo valores negativos. Observamos que essas duas expressões têm pontuações semelhantes.\n\nPodemos limitar o espaço de pesquisa removendo esses resultados negativos para obter uma resposta correta. Descobrimos que essa restrição melhora significativamente para alguns modelos. Por exemplo, para BERT, melhoramos em sete pontos, e para o modelo baseado em RoBERTa, melhoramos em dois pontos. Um melhor modelo de linguagem tem uma melhor capacidade de compreensão, então o número aqui é maior para RoBERTa e menor para BERT.\n\nTambém tentamos analisar a dificuldade por trás desses conjuntos de dados. Supomos que a quantidade de quantidades não utilizadas pode ser considerada informação irrelevante. Aqui, vemos a porcentagem de amostras com quantidades não utilizadas, e o conjunto de dados SWAM tem a maior proporção. Também mostramos o desempenho geral para amostras sem quantidades não utilizadas, que é maior que o desempenho geral. Mas, com amostras de quantidade não utilizada, é muito pior que o desempenho geral. Para MAWPS, não temos muitos casos de morte, então ignorei essa parte.\n\nFinalmente, queremos mostrar a interpretabilidade através de um exemplo de erro e correção. Aqui, nosso modelo faz uma previsão errada no primeiro passo. Podemos correlacionar essa expressão com a frase aqui. Acreditamos que essa frase pode estar levando o modelo a uma previsão incorreta. Imprimir \"outros 35\" faz o modelo pensar que deve ser um operador de adição. Tentamos revisar a frase para algo como \"o número de pereiras é 55 a menos que o de macieiras\". Fazemos isso para transmitir semântica mais precisa, de modo que o modelo consiga fazer a previsão correta.\n\nEste estudo demonstra como previsões interpretáveis nos ajudam a entender o comportamento do modelo. Para concluir nosso trabalho, primeiro, nosso modelo é bastante eficiente e fornece um procedimento de solução interpretável. Podemos incorporar facilmente conhecimento prévio como restrição, o que ajuda a melhorar o desempenho. Por fim, o mecanismo subjacente não se aplica apenas a tarefas de resolução de problemas de rede, mas também a outras tarefas que envolvem raciocínio multi-passo.\n\nNo entanto, também temos limitações. Se houver um grande número de operadores ou constantes, o consumo de memória pode ser alto. Além disso, como mencionado, devido ao desequilíbrio na distribuição de probabilidade em diferentes passos de tempo, é desafiador aplicar a estratégia de busca de feixe. Obrigado pela atenção. Estou aberto a perguntas."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Apresentarei meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para recuperação de artigos estatutários. Questões legais fazem parte da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem arcar com o custooso auxílio de um especialista jurídico ficam desprotegidos ou, pior, são explorados. Nosso trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo sistemas eficazes de recuperação de artigos estatutários. Tal sistema poderia fornecer um serviço de ajuda jurídica gratuito para pessoas não especializadas. Antes de mergulharmos na principal contribuição deste trabalho, vamos descrever primeiro o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre um assunto legal, como \"O que eu risco se violar o sigilo profissional?\", é necessário um modelo para recuperar todos os artigos estatutários relevantes de um grande corpo de legislação. Essa tarefa de recuperação de informações traz seus próprios desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem jurídica complexa para os estatutos. Essa diferença na distribuição da linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois indiretamente requer um sistema de interpretação inerente que possa traduzir uma pergunta natural para uma pergunta jurídica que corresponda à terminologia dos estatutos. Além disso, o direito estatutário não é uma pilha de artigos independentes que podem ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas em seu contexto geral, ou seja, juntamente com as informações suplementares de seus artigos vizinhos, os campos e subcampos aos quais pertencem e seu lugar na estrutura da lei. Por fim, os artigos estatutários estão em pequenos parágrafos, que geralmente são a unidade típica de recuperação na maioria dos trabalhos de recuperação. Aqui, temos documentos longos que podem ter até 6.000 palavras. Os recentes avanços no Processamento de Linguagem Natural (PLN) geraram grande interesse em muitas tarefas legais, como a previsão de julgamentos legais ou a revisão automatizada de contratos, mas a recuperação de artigos estatutários permaneceu em grande parte intocada devido à falta de grandes conjuntos de dados rotulados de alta qualidade. Neste trabalho, apresentamos um novo conjunto de dados nativo em francês e centrado no cidadão para verificar se os modelos de recuperação podem aproximar a eficiência e a confiabilidade dos especialistas jurídicos na tarefa de recuperação de artigos estatutários. Nosso conjunto de dados belga de recuperação de artigos estatutários, PSART, consiste em mais de 1.100 perguntas legais feitas por cidadãos belgas. Essas perguntas abrangem uma ampla gama de tópicos, desde família, habitação, dinheiro, até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de 22.600 artigos legais de códigos legais belgas.\n\nVamos agora falar sobre como coletamos esse conjunto de dados. Primeiro, compilamos um grande corpus de artigos legais. Consideramos 32 códigos belgas públicos e extraímos todos os seus artigos, bem como os títulos das seções correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, estabelecemos uma parceria com um escritório de advocacia belga que recebe cerca de 4.000 e-mails por ano de cidadãos belgas que solicitam aconselhamento sobre um problema legal pessoal. Tivemos a sorte de obter acesso aos seus sites, onde sua equipe de juristas experientes aborda as questões legais mais comuns na Bélgica. Coletamos milhares de perguntas, anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por fim, analisamos as referências legais e excluímos as perguntas cujas referências não eram artigos em um dos códigos de lei que consideramos. As referências restantes foram correspondidas e convertidas para os IDs de artigo correspondentes do nosso corpus. Acabamos com 1.108 perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso grande corpus de 22.633 artigos estatutários. Além disso, cada pergunta vem com uma categoria principal e uma concatenação de subcategorias, e cada artigo vem com uma concatenação de seus títulos subsequentes na estrutura da lei. Essas informações extras não são usadas no presente trabalho, mas podem ser de interesse para futuras pesquisas sobre recuperação de informações legais ou classificação de textos legais.\n\nVamos analisar algumas características do nosso conjunto de dados. As perguntas têm entre 5 e 44 palavras, com uma mediana de 40 palavras. Os artigos são muito mais longos, com uma mediana de 77 palavras, sendo que 142 deles excedem 1.000 palavras, sendo o mais longo de até 5.790 palavras. Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de 85% delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os 15% restantes dizem respeito à segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois são provenientes de 32 códigos belgas diferentes que abrangem um grande número de tópicos legais. Aqui está o número total de artigos coletados de cada um desses códigos belgas. Dos 22.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta no conjunto de dados. E cerca de 80% desses artigos citados vêm do Código Civil, Código Judiciário, Código de Investigação Criminal ou Códigos Penais. Enquanto isso, 18 dos 32 códigos têm menos de 5 artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esses códigos se concentram menos em indivíduos e suas preocupações. No geral, o número médio de citações para esses artigos citados é 2, e menos de 25% deles são citados mais de 5 vezes.\n\nUsando nossos conjuntos de dados, avaliamos várias abordagens de recuperação, incluindo arquitetura lexical e densa. Dada uma consulta e um artigo, um modelo lexical atribui uma pontuação à dupla consulta-artigo calculando a soma sobre os termos da consulta dos pesos de cada um desses termos naquele artigo. Experimentamos com as funções de classificação TF-IDF e BM25. O principal problema com essas abordagens é que elas só podem recuperar artigos que contenham palavras-chave presentes na consulta. Para superar essa limitação, experimentamos com uma arquitetura baseada em rede neural que pode capturar relações semânticas entre consultas e artigos. Usamos um modelo b-encoder que mapeia consultas e artigos em representações vetoriais densas e calcula uma pontuação relevante entre uma dupla consulta-artigo pela similaridade de suas incorporações. Essas incorporações geralmente resultam de uma operação de agrupamento na saída de um modelo de incorporação de palavras. Primeiro, estudamos a eficácia dos b-encoders Siamese em um setup de avaliação zero-shot, o que significa que os modelos de incorporação de palavras pré-treinados são aplicados diretamente, sem qualquer ajuste fino adicional. Experimentamos com codificadores de texto independentes do contexto, nomeadamente Word2Vec e FastText, e modelos de incorporação dependentes do contexto, nomeadamente Robota e, mais especificamente, Camembert, que é um modelo Robota em francês. Além disso, treinamos nossos próprios b-encoders baseados em Camembert em nossos conjuntos de dados. Note que, para o treinamento, experimentamos com as duas variantes da arquitetura b-encoder. Siamese, que usa um único modelo de incorporação de palavras que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado, e Two-Tower, que usa dois modelos de incorporação de palavras independentes que codificam a consulta e o artigo separadamente em diferentes espaços de incorporação. Experimentamos com agrupamento médio, máximo e CLS, bem como produto escalar e cosseno para calcular similaridades. Aqui estão os resultados do nosso modelo de referência no conjunto de dados, com os métodos lexicais acima, os b-encoders Siamese avaliados em um setup zero-shot no meio e os b-encoders ajustados abaixo. No geral, os b-encoders ajustados superam significativamente todos os outros modelos de referência. O modelo Two-Tower melhora em relação à sua variante Siamese no recall em 100, mas apresenta desempenho semelhante nos outros indicadores. Embora o BM25 tenha apresentado desempenho inferior ao B-Encoder treinado, significativamente, seu desempenho indica que ainda é um modelo de referência forte para recuperação específica de domínio. Em relação à avaliação zero-shot dos b-encoders Siamese, descobrimos que o uso direto das incorporações de um modelo Camembert pré-treinado, sem otimização para a tarefa de recuperação de informações, resulta em resultados pobres, o que é consistente com descobertas anteriores. Além disso, observamos que o b-encoder baseado em Word2Vec superou significativamente os modelos baseados em FastText e BERT, sugerindo que, talvez, as incorporações de palavras pré-treinadas em nível de palavra sejam mais adequadas para a tarefa do que as de nível de caractere ou subpalavra. Embora promissores, esses resultados sugerem uma ampla oportunidade de melhoria em comparação com um especialista jurídico habilidoso que pode, eventualmente, recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas.\n\nVamos concluir discutindo duas limitações de todos os conjuntos de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos 32 códigos belgas considerados, o que não abrange toda a lei belga, pois os artigos de decretos, diretivas e ordenanças estão ausentes. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes. Essa perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja completamente apropriada. Segundo, devemos observar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso despejar meus inquilinos se eles fizerem muito barulho?\" pode não ter uma resposta detalhada dentro do direito estatutário que quantifique um limite específico de ruído no qual o despejo é permitido. Em vez disso, o proprietário provavelmente deve confiar mais na jurisprudência e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz duas festas por semana até as 2h da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas ainda precisa ser determinado. Esperamos que nosso trabalho gere interesse no desenvolvimento de modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso artigo, conjunto de dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá! Temos o prazer de apresentar nosso trabalho sobre VALS, um marco de referência independente da tarefa destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de criar este marco de referência? Bem, nos últimos anos, testemunhamos uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados em grandes quantidades de pares de texto de imagem. Cada um desses modelos impulsiona o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, ancoragem de frases. Então, recebemos uma mensagem - as precisões nesses marcos de referência específicos de tarefa estão aumentando constantemente. Mas sabemos realmente o que os modelos aprenderam de fato? O que um transformador de visão e linguagem entendeu ao atribuir uma pontuação alta para esta imagem e esta frase para combinar, e uma pontuação baixa para esta outra? Os modelos de visão e linguagem se concentram no aspecto correto, ou se concentram em vieses, como mostrado por trabalhos anteriores? Para lançar mais luz sobre este aspecto, propomos uma direção mais agnóstica em relação à tarefa e introduzimos válvulas que testam a sensibilidade dos modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguística quanto visual. Nosso objetivo é a existência, pluralidade, contagem, relações espaciais, ações e coreferência de entidades. Mas como testamos se os modelos de visão e linguagem capturaram esses fenômenos? Por FOILing, um método anteriormente aplicado para modelos de visão e linguagem, apenas para frases nominais por Ravi Shekhar e colaboradores, na contagem por nós em trabalho anterior. FOILing basicamente significa que pegamos a legenda de uma imagem e produzimos um FOIL alterando a legenda de forma que ela não descreva mais a imagem. E realizamos essas alterações de frases focando em seis peças específicas, como existência, pluralidade, contagem, relações espaciais, ações e coreferência de entidades, onde cada peça pode consistir em um ou mais instrumentos, caso tenhamos encontrado mais de uma maneira interessante de criar instâncias FOIL. Por exemplo, no caso da peça ações, temos dois instrumentos, um no qual o verbo da ação é alterado por uma ação diferente, e outro no qual os agentes são trocados. Contagem e coreferência também são peças que têm mais de um instrumento. E criamos esses FOILs garantindo que falhem em descrever a imagem, que sejam gramaticalmente corretos e sentenças válidas. Isso não é fácil de fazer, porque uma legenda frustrada pode ser menos provável do que a legenda original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem corte plantas, e grandes modelos de visão e linguagem poderiam perceber isso. Portanto, para obter foils válidos, devemos tomar medidas. Primeiro, utilizamos modelos de linguagem fortes para propor foils. Em segundo lugar, utilizamos a inferência de linguagem natural, ou NLI, para filtrar foils que ainda poderiam descrever a imagem, já que, ao construir os foils, precisamos garantir que falhem em descrever a imagem. Para testar isso automaticamente, aplicamos a inferência de linguagem natural com a seguinte lógica. Consideramos uma imagem como a premissa e sua legenda como implicada. Além disso, consideramos a legenda como a premissa e o FOIL como sua hipótese. Se um modelo NLI prever que o FOIL contradiz ou é neutro em relação à legenda, tomamos isso como um indicador de um FOIL válido. Se um NLI prever que o FOIL é implicado pela legenda, não pode ser um bom FOIL, pois, por transitividade, fornecerá uma descrição verdadeira da imagem e filtramos esses FOILs. Mas este procedimento não é perfeito. É apenas um indicador para FOIs válidos, portanto, como terceira medida para gerar FOIs válidos, empregamos anotadores humanos para validar os dados usados no VALS. Então, após a filtragem e a avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Observe que o VALS não fornece nenhum dado de treinamento, apenas dados de teste, pois é um marco de referência de teste zero-shot. Ele é projetado para aproveitar as capacidades existentes dos modelos de visão e linguagem após o pré-treinamento. O ajuste fino apenas permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados. E todos sabemos que esses modelos gostam de trapacear e pegar atalhos. E como dissemos, estamos interessados em avaliar quais capacidades os modelos de visão e linguagem têm após o pré-treinamento. Experimentamos com cinco modelos de visão e linguagem no VALS, a saber, CLIP, LXMERT, Wil VILBERT, VILBERT 12 em 1 e VISUALBERT. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos na classificação de pares imagem-sentença em legendas e foils. Talvez mais relevante para este vídeo, apresentaremos nossa métrica mais permissiva, a precisão par a par, que mede se a pontuação de alinhamento imagem-sentença é maior para o par imagem-texto correto do que para seu par frustrado. Para mais métricas e resultados sobre elas, confira nosso artigo. Os resultados com precisão par a par são mostrados aqui e são consistentes com os resultados obtidos das outras métricas. Isso significa que o melhor desempenho zero-shot é alcançado pelo Wilbert 12 em 1, seguido pelo Wilbert. É notável como os instrumentos centrados em objetos individuais, como existência e frases nominais, são quase resolvidos pelo Wilbert 12 em 1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhuma das peças restantes pode ser resolvida de forma confiável em nossas configurações de frustração adversária. Vemos a partir dos instrumentos de pluralidade e contagem que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos em comparação com múltiplos objetos ou contá-los em uma imagem. A peça de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm dificuldade em distinguir ações e identificar seus participantes, mesmo quando apoiados por vieses de plausibilidade, como vemos na peça de ações. A partir da peça de coreferência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem. Como um controle de qualidade e porque é um experimento interessante, também benchmarking dois modelos de texto apenas, GPT-1 e GPT-2, para avaliar se o VALS é solucionável por esses modelos unimodais calculando a perplexidade da legenda correta e frustrada e prevendo a entrada com a menor perplexidade. Se a perplexidade for maior para o FOIL, tomamos isso como uma indicação de que a legenda frustrada pode sofrer de viés de plausibilidade ou outros vieses linguísticos. E é interessante ver que, em alguns casos, os modelos GPT de texto apenas capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem. Para resumir. O VALS é um marco de referência que utiliza a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem testando rigorosamente suas capacidades de ancoragem visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam bem objetos nomeados e sua presença em imagens, como mostrado pela peça de existência, mas lutam para ancorar sua interdependência e relacionamentos em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de incentivar a comunidade a usar o VALS para medir o progresso em direção à ancoragem de linguagem com modelos de visão e linguagem. E ainda mais, o VALS poderia ser usado como uma avaliação indireta de conjuntos de dados, já que os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo VALS. Se você estiver interessado, confira os dados do VALS no GitHub e, se tiver alguma dúvida, não hesite em entrar em contato conosco."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamizawa da Universidade de Tóquio. Apresentarei um artigo intitulado RNSUN, um grande conjunto de dados para a geração automática de notas de lançamento por meio da resumificação de logs de commits. Vou explicar na seguinte ordem. Primeiro, apresentarei a geração automática de notas de lançamento, na qual estamos trabalhando nesta pesquisa. A nota de lançamento é um documento técnico que resume as alterações distribuídas com cada versão de um produto de software. A imagem mostra as notas de lançamento para a versão 2.6.4 da biblioteca Vue.js. As notas de lançamento desempenham um papel importante no desenvolvimento de código aberto, mas são demoradas para serem preparadas manualmente. Portanto, seria muito útil poder gerar automaticamente notas de lançamento de alta qualidade. Vou me referir a duas pesquisas anteriores sobre a geração automática de notas de lançamento. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando o extrator de alterações para extrair diferenças principais, alterações de biblioteca e alterações de documentos das diferenças entre versões, e finalmente combinando-as. A característica mais notável deste sistema é o extrator de questões no canto superior direito, que deve ser vinculado ao Jira, o sistema de rastreamento de questões, e pode ser aplicado apenas a projetos que usam o Jira. Em outras palavras, não pode ser usado para muitos projetos no GitHub. O segundo é o Glyph, anunciado recentemente em 2020. Está disponível na internet e pode ser instalado via PIP. Este sistema possui um modelo simples de classificação de texto baseado em aprendizado e produz um dos cinco níveis, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna um rótulo de correções ou correções de bugs. Os dados de treinamento do Glyph são relativamente pequenos, cerca de 5000, e na revisão dos experimentos descritos abaixo, o desempenho do modelo de classificação de texto não é alto. Apresento duas pesquisas relacionadas, mas existem problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolve esses dois problemas e gera automaticamente notas de lançamento de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de resumificação de classificação de alta qualidade usando apenas a mensagem de commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês. Para o segundo problema de recursos de dados escassos, construímos um conjunto de dados R e sum composto por cerca de 82.000 dados, coletando dados de repositórios públicos do GitHub usando a API do GitHub e alguns conjuntos de dados. Nosso conjunto de dados Rnsum, composto por cerca de 82.000 dados, foi coletado a partir de repositórios públicos do GitHub usando a API do GitHub. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dado. O lado esquerdo é a mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são rotuladas como melhorias de faces, etc. Configuramos uma tarefa que recebe as mensagens de commit como entrada e produz as notas de lançamento rotuladas. Isso pode ser considerado uma tarefa de resumificação. Predefinimos quatro rótulos: recursos, melhorias, correções de bugs, depreciações, remoções e alterações de quebra. Esses rótulos foram definidos com base em pesquisas anteriores e outros fatores. As notas de lançamento na parte inferior direita são extraídas das notas de lançamento mostradas na parte inferior esquerda. Neste momento, é necessário detectar os quatro rótulos definidos previamente. Mas os rótulos nem sempre são consistentes com cada repositório. Por exemplo, o rótulo de melhorias inclui melhorias, aprimoramentos, otimizações, etc. Preparamos uma lista de vocabulário dos rótulos do nosso estudo para cada uma dessas variações de notação, usamos-a para detectar a classe da nota de lançamento e corrigimos o texto da lista que se segue como a frase da nota de lançamento para a classe. É necessário identificar a versão anterior do lançamento, de 2.5 a 18, e obter o diff. Isso é um pouco tedioso e não é suficiente apenas obter uma lista de lançamentos e verificar o antes e o depois. Criamos uma regra de correspondência heurística para obter as versões anterior e posterior. Na análise do conjunto de dados, coletamos 7.200 repositórios e 82.000 dados. Além disso, o número médio de tokens das notas de lançamento é 63, o que é bastante alto para uma tarefa de resumificação. O número de tokens únicos também é bastante grande, com 8.830.000. Isso se deve ao grande número de nomes de classes e métodos únicos encontrados no repositório. Em seguida, explicarei o método proposto. O modelo de resumificação extrativa e, em seguida, abstrativa por classe usa um classificador para classificar cada mensagem de commit em cinco classes de notas de lançamento. Escolhemos implementar, correções de bugs, depreciações e outros. As mensagens de commit classificadas como outros são descartadas. Em seguida, o CEAS aplica o gerador aos quatro documentos rotulados de forma independente e gera notas de lançamento para cada classe. Nesta tarefa, as correspondências diretas entre as mensagens de commit e as notas de lançamento não são conhecidas. Portanto, para treinar o classificador, atribuímos rótulos pseudo a cada mensagem de commit de entrada usando os primeiros 10 caracteres de cada mensagem de commit. Modelamos a resumificação abstrativa por classe através da nossa abordagem por dois métodos diferentes. O primeiro modelo, que chamamos de cssingle, consiste em uma única rede de conjunto para conjunto e gera um único texto longo de nota, dando uma concatenação das mensagens de commit de entrada. Os modelos de rede de saída, cada um correspondente a uma das classes menos conhecidas. Bem, vou explicar a experiência. Cinco métodos foram comparados: CAS, CASSingle, CASMatch, PlusSelling e o estudo anterior, GRIF. Em relação à aberração, em alguns casos, CSMatch, Blustering e o Estudo Anterior Glyph apresentaram resultados inferiores. Em relação à avaliação, em alguns casos, as notas de lançamento são saída em várias frases. Como é difícil calcular o número de frases como estão, elas são combinadas com espaços e tratadas como uma única frase longa. O azul é penalizado quando o sistema produz uma frase curta. Essa penalidade resulta em um valor azul mais baixo nos resultados da experiência descritos a seguir. Finalmente, também calculamos a especificidade, pois o rouge e o azul não podem ser calculados se as notas de lançamento estiverem vazias. Uma alta especificidade significa que o modelo produz corretamente um texto vazio em casos em que as notas de lançamento são assumidas como vazias. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores hash, etc., também avaliamos o conjunto de dados limpo, que os exclui. CES e CAS alcançaram pontuações rouge L mais de 10 pontos superiores às dos baselines. Em particular, no conjunto de teste limpo, a lacuna de pontuação entre o método proposto e os baselines saltou para mais de 20 pontos. Esses resultados indicam que CAS e CAS são significativamente eficazes. CAS obteve uma pontuação root-A melhor do que CAS, sugerindo que combinar um classificador e um gerador é eficaz no treinamento do classificador usando dois-duplos. Uma alta cobertura do CAS pode ser alcançada, provavelmente, porque o classificador pode se concentrar na seleção de mensagens de commit relevantes para cada classe. CAS match tende a produzir pontuações rouge L mais altas do que CAS single, sugerindo que também é eficaz desenvolver modelos de resumificação abstrativa diferentes para cada classe de nota de lançamento. Aqui está a análise de erros. Os métodos CAS tendem a produzir frases mais curtas do que as frases de referência humana. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto o CAS tem apenas uma. A razão para essa relutância do modelo é que, nos dados de treinamento, apenas 33% das frases estão presentes no rótulo de recursos e 40% no rótulo de melhorias. Além disso, os métodos CES não conseguem gerar notas de lançamento precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa. Os métodos CS não conseguem gerar uma nota de lançamento precisa sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa, e a frase completa não pode ser gerada sem referência ao pedido ou questão correspondente. O exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma única frase, mas falha em fazê-lo. Finalmente, uma conclusão. Construímos um novo conjunto de dados para a geração automática de listas de notas. Também formamos a tarefa de inserir mensagens de commit e resumizá-las para que seja aplicável a todos os projetos escritos em inglês. Nossa experiência mostra que o método proposto gera notas de lançamento menos ruidosas com maior cobertura do que os baselines. Confira nossa guia de descida apenas. Obrigado."}
