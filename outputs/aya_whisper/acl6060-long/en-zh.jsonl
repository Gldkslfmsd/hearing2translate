{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是阿萨夫·哈拉里，我将介绍我们的研究论文《使用微调变压器架构的少样本表数据丰富》。数据科学家分析数据，主要关注操作现有特征。但有时这些特征是有限的。使用另一个数据源生成特征可以添加大量信息。我们的研究目标是使用外部来源的文本自动丰富表数据。假设我们有一个表数据集和一个知识库。我们需要一个自动过程，涉及实体链接和文本分析，从知识库的文本中提取新特征。我们提出的框架FAST正是这个自动过程。让我们看一个例子。在输入到FAST的数据集中，在这个例子中，数据集是大学数据集，其目标是将大学分类为低排名大学和高排名大学。作为知识库，我们使用维基百科。FAST的第一阶段是实体链接。当每个实体，在这个例子中是大学名称，链接到知识库中的实体时。知识库实体的文本被提取并添加到数据集中。在这个例子中，文本是维基百科页面摘要。现在我们需要从检索的文本中生成或提取特征。因此我们需要一个特征提取阶段，包括文本分析。这是本文的主要创新点，我将在接下来的幻灯片中深入探讨。在特征提取阶段之后，有一个特征生成阶段，我们使用提取的特征来生成少量新特征。首先，根据原始数据集的类别数量生成特征。在这个例子中，原始数据集有两个类别，所以FAST生成两个新特征。但如果数据集有五个类别，FAST将生成五个新特征。每个特征表示每个类别的可能性。为了分析文本，我们使用当前文本分析的最新技术，即基于变压器的语言模型，如BERT、GPT、XNL等。但不可能使用输入数据集训练语言模型。因此，一个简单的方法是目标任务微调。所以在特征提取阶段，我们可以下载预训练语言模型，并在目标数据集上微调语言模型。在这个例子中，为了微调语言模型，将文本分类为低或高，将摘要分类为类别，我们接收语言模型的输出，即每个类别的可能性，并将其用作新特征。这个方法的问题是数据集可能只有少数不同的实体标签。在我们的实验中，近一半的数据集包含不到400个样本，最小的数据集训练集只有35个样本。因此，在这样的数据集上微调语言模型将是无效的。但我们可以使用对之前分析的数据集的先验知识，因为我们已经分析了N-1个数据集，并在分析第N个数据集时使用这些信息。我们建议添加另一个微调阶段，一个初步的多任务微调阶段，在N-1个数据集上微调语言模型，然后执行另一个微调阶段，即目标任务微调，在目标数据集上微调语言模型。多任务微调的最新技术称为mtDNN。在mtDNN中，mtDNN维护的头数量与训练集中任务的数量相同。所以在这个例子中，训练集中有四个任务，mtDNN维护四个头，如图所示，并从训练集中采样一个随机批次。如果随机批次属于单句分类任务，则通过第一个头执行前向和反向传播。如果随机批次属于配对排名任务，则通过最后一个头执行前向和反向传播。在我们的场景中，表数据集在类别数量上有所不同。因此，在我们的场景中，一个表数据集验证了类别数量。所以有多个任务。MTDNN维护类别数量的头，输出层，此外，MTDNN需要为新的数据集和新的任务初始化新的头。我们的方法称为任务重构微调，在我们的方法中，任务重构功能，而不是维护多个头，我们将每个数据集重构为一个句子，每个分类问题，这是一个二元分类任务。让我们看一个例子。这是我们的输入数据集，由实体、特征、文本和类别组成。我们的任务是从低到高分类文本，将摘要分类为类别，如果摘要属于类别。所以在这种情况下，标签向量总是包含两个类别。然后它将任务重构为每个分类问题的一个句子。将语言模型应用于新任务，并输出每个类别的可能性。请注意，语言模型已经在N-1个数据集上进行了初步的多任务微调。然后我们使用语言模型的输出向量作为新生成的特征，数量等于类别。为了评估我们的框架，我们使用17个表分类数据集，验证大小、特征、平衡、领域和初始性能。作为知识库，我们使用维基百科。我们设计实验为留一交叉验证，我们在16个数据集上训练FAST，并将其应用于第17个数据集。我们还将每个数据集划分为四个折，并应用四折交叉验证。然后我们生成新特征，并使用五个评估分类器对其进行评估。我们在实验中使用基于构建的建筑。以下是我们实验的结果。您可以看到，我们将我们的框架与目标数据集微调、目标任务微调和MTDNN初步微调进行了比较，我们的改革微调取得了最佳结果和最佳性能。而空DNN的改革微调取得了最佳结果和最佳性能。MTDNN在目标数据集微调基础上提高了2%，而我们的方法提高了6%。当我们观察小数据集时，可以看到空DNN的性能下降，而仅目标任务微调的改进。总之，FAST实现了从35个样本开始的少样本丰富，在我们的实验中。它使用一个架构适用于所有任务的数据集，并保持模型的头。但它添加了一个重构阶段。它增强了训练集，并需要一个具有语义意义的目标值，以便我们可以将其输入到语言模型中，并将其用于每个分类问题的一个句子。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，今天我将介绍我们的研究工作——《学习推理：侦探式推理与代谢问题求解作为复杂推理提取》。我是来自字节跳动AI实验室的Alan，这是与德克萨斯大学奥斯汀分校的Jerry和新加坡科技设计大学的Weilu的合作项目。首先，我想谈谈我们进行推理的动机。在这里，我们展示了一个多步推理有帮助的例子。这个图示来自纸笔上的促导，用于在少样本学习场景下解决数学问题。在图的左侧，我们可以看到，如果我们只提供问题和答案，可能无法得到正确答案。但如果我们提供更多的推理描述，模型能够预测推理描述并给出正确预测。因此，输出可解释的多步推理是非常有益的。我们也认为数学问题是评估此类推理能力的一个直接应用。\n\n在我们的问题设置中，给定问题，我们需要解决这个问题并得到数值答案。在我们的数据集中，我们还给出了导致特定答案的数学表达式。某些假设也适用于之前的工作。我们假设量的精度是已知的，我们只考虑基本运算符，如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以分解成这些基本运算符。\n\n之前在数学问题求解方面的工作实际上可以分为序列到序列和序列到树模型。传统的序列到序列模型将表达式转换为特定的序列进行生成，实现起来非常简单，并且可以推广到许多不同的复杂问题。但性能的缺点是通常不如结构模型好，并且缺乏预测的可解释性。但由于变压器模型的原因，这个方向仍然非常流行。\n\n在基于树的模型中，我们实际上将表达式结构化为树的形式，并遵循前序遍历进行三次生成。在这里，我们不断生成运算符，直到到达叶节点，即量。这里的好处是它实际上给我们提供了二叉树结构，但它实际上相当直观，因为我们首先生成运算符，然后在最后生成量。其次，它也包含一些重复计算。在这里，如果我们看这个表达式，a乘以3加3实际上被生成了两次。但实际上，我们应该重用结果。在我们提出的方法中，我们希望以逐步和可解释的方式解决这些问题。例如，在这里的第二步，我们可以得到这个除数27，我们也可以回顾原始问题以找到相关内容。在这些步骤中，我们得到除数。然后在第三步，我们实际上得到了商。好了，在经过这三步后，我们实际上可以重用第二步的结果，然后得到第四步的结果。最后，我们可以得到被除数。在这里，我们实际上直接生成整个表达式，而不是生成单个运算符或量。这使过程更加准确。\n\n在我们的演绎系统中，我们首先从问题中呈现的一组量开始，并包括一些常量作为初始状态。表达式表示为EIJOP，其中我们在QI到QJ之间执行运算符，这样的表达式是有方向的。我们也这里有反减法来表示相反的方向。这与关系提取非常相似。在正式的演绎系统中，在时间步t，我们在QI和QJ之间应用运算符，然后得到新的表达式。我们将其添加到下一个状态以成为新的量。这个幻灯片实际上可视化了状态的演变，我们不断将表达式添加到当前状态中。\n\n在我们的模型实现中，我们首先使用预训练的语言模型，可以是BERT或RoBERTa，然后编码句子，并得到量表示。一旦我们得到量表示，我们就可以开始进行推理。这里我们展示了一个从Q1开始得到Q1除以Q2再乘以Q3的表示的例子。首先，我们得到基本的配对表示，即Q1和Q2的基本连接。然后，我们应用一个由运算符参数化的前馈网络。最后，我们得到Q1除以Q2的表达式表示。但在实践中，在推理阶段，我们也可能得到不正确的表达式。这里所有可能的表达式等于运算符数量的三倍。好处是我们可以轻松添加约束来控制这个搜索空间。例如，如果这个表达式不允许，我们可以简单地将其从搜索空间中删除。在第二步，我们做同样的事情，但唯一的区别是多了一个量。这个量来自之前计算的表达式。最后，我们可以得到最终表达式Q3乘以Q4。我们也可以看到所有可能表达式的数量与之前步骤不同。这些差异使得很难应用束搜索，因为这两个步骤之间的概率分布是不平衡的。\n\n训练过程类似于训练序列到序列模型，我们优化每个时间步的损失。这里我们还使用τ来表示我们应该何时终止生成过程。这里空间与序列到序列不同，因为每个时间步的空间不同。在传统的序列到序列模型中，它是词汇表的大小，它还允许我们从先验知识中施加某些约束。\n\n我们在常用的数学问题数据集MAWPS、Math23k、MathQA和SWAM上进行了实验。这里我们简要展示了与之前最佳方法的比较结果。我们表现最佳的变体是RoBERTa演绎推理器。事实上，我们没有使用束搜索，而之前最佳的方法通常是基于树的模型。总体而言，我们的推理器能够显著超越基于树的模型，但我们可以看到MathQA或SWAM上的绝对数字并不真的很高。\n\n我们进一步调查了SWAM上的结果，这个数据集具有挑战性，因为作者试图手动添加一些内容来混淆NLP模型，例如添加无关信息和额外量。在我们的预测中，我们发现一些中间值实际上是负数。例如，在这个问题中，我们询问Jake有多少个苹果，但我们有一些额外的信息，如少了17个桃子，Steven有8个桃子，这完全无关。我们的模型做出像这样的预测，产生负值。我们观察到这两个表达式实际上具有相似的分数。我们实际上可以通过删除这些负结果来限制搜索空间，从而使答案正确。我们进一步发现，这种约束对某些模型的改进非常大。例如，对于BERT，我们提高了7个点。对于基于RoBERTa的模型，我们实际上提高了2个点。更好的语言模型具有更好的语言理解能力，因此RoBERTa的数字更高，BERT的数字更低。\n\n我们还尝试分析所有这些数据集背后的难度。我们假设未使用的量的数量可以被视为无关信息。在这里，我们可以看到具有未使用量的样本的百分比，SWAM数据集具有最大的比例。我们还展示了总体性能。对于没有未使用量的样本，总体性能实际上高于总体性能。但对于具有未使用量的样本，它实际上远低于总体性能。对于MAWPS，我们没有太多的死亡案例，所以我忽略了这一部分。\n\n最后，我们想通过一个错误和修正的例子来展示可解释性。在这里，我们的模型在第一步做出错误预测。我们可以将这个表达式与这里的句子相关联。我们认为这个句子可能在误导模型做出错误预测。在这里，打印另一个35使模型认为应该是加法运算符。我们尝试将句子修订为类似于“梨树的数量比苹果树少55”，使其传达更准确的语义，使模型能够正确预测。这项研究展示了可解释预测如何帮助我们理解模型行为。\n\n总结我们的工作，首先，我们的模型非常高效，能够提供可解释的求解过程。我们可以轻松地将一些先验知识纳入约束，这可以帮助提高性能。最后，潜在机制不仅适用于网络问题求解任务，还适用于涉及多步推理的其他任务。但我们也有某些局限性。如果我们有大量运算符或常量，内存消耗可能会非常高。其次，正如提到的，由于不同时间步的概率分布不平衡，应用束搜索策略也非常具有挑战性。这是演讲的结束，欢迎提问，谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我叫安托万，来自马斯特里赫特大学。我将与杰里共同展示我们关于法定条款检索新数据集的工作。法律问题是许多人生活中不可或缺的一部分，但大多数公民对自己的权利和基本法律程序知之甚少。因此，许多无法负担法律专家昂贵协助的弱势公民得不到保护，甚至遭到利用。我们的工作旨在通过开发有效的法定条款检索系统，缩小人民与法律之间的差距。这样一个系统可以为普通人提供免费的法律帮助服务。在深入探讨这项工作的核心贡献之前，让我们先描述一下法定条款检索的问题。给定一个关于法律事务的简单问题，例如我违反职业保密会面临什么风险，需要一个模型从大量法律文本中检索出所有相关法定条款。这项信息检索任务本身面临诸多挑战。首先，它涉及两种语言：问题使用通用的自然语言，而法规则采用复杂的法律语言。这种语言分布的差异使得系统更难检索到相关候选项，因为它间接需要一个内在的解释系统，将自然语言问题转化为与法规术语相匹配的法律问题。此外，法定法不是可以独立处理的条款集合，像新闻或食谱那样可以单独作为完整信息来源。相反，它是一个结构化的法律条款集合，只有在考虑其整体背景时才有完整的意义，即结合其相邻条款的补充信息、它们所属的领域和子领域，以及它们在法律结构中的位置。最后，法定条款通常以小段落的形式出现，而大多数检索工作中的典型检索单位也是小段落。在这里，我们面对的是长达6000字的长文档。自然语言处理的最新进展在许多法律任务中激发了巨大兴趣，如法律判决预测或自动化合同审查，但法定条款检索由于缺乏大量高质量标注数据集而基本未被触及。在这项工作中，我们提出了一个以法国公民为中心的比利时法定条款检索数据集，以检验检索模型是否能达到法律专家的效率和可靠性。我们的比利时法定条款检索数据集PSART由超过1100个比利时公民提出的法律问题组成。这些问题涵盖了从家庭、住房、金钱到工作和社会保障等广泛主题。每个问题都由经验丰富的法官标注了相关条款的引用，引用来自超过22,600个比利时法律条款的语料库。\n\n现在让我们谈谈如何收集这个数据集。首先，我们编译了一个大量法律条款的语料库。我们考虑了32个公开可用的比利时法典，提取了所有条款以及相应的章节标题。然后，我们收集了带有相关法规引用的法律问题。为此，我们与一家比利时律师事务所合作，该事务所有每年收到约4000封来自比利时公民的电子邮件，寻求关于个人法律问题的建议。我们有幸获得了他们网站的访问权限，他们的经验丰富的法官团队在那里解决比利时最常见的法律问题。我们收集了数千个问题，并附有类别、子类别和相关法规的标注。最后，我们解析了法规引用，筛选出引用不是我们考虑的法典中条款的问题。剩余的引用与我们语料库中的相应条款ID进行匹配和转换。最终，我们得到了1108个问题，每个问题都仔细标注了来自我们22,633个法定条款大语料库的相关条款ID。此外，每个问题都有一个主类别和子类别的连接，每个条款都有其法律结构中后续标题的连接。这些额外信息在本工作中没有使用，但可能对未来法律信息检索或法律文本分类研究有兴趣。\n\n让我们看看数据集的某些特征。问题长度在5到44个字之间，中位数是40个字。条款要长得多，中位数是77个字，其中142个超过1000字，最长的达到5790字。如前所述，问题涵盖了广泛的主题，大约85%的问题涉及家庭、住房、金钱或司法，而剩余的15%涉及社会保障、外国人或工作。条款也非常多样化，因为它们来自32个不同的比利时法典，涵盖了大量法律主题。以下是每个比利时法典收集的条款总数。在22,633个条款中，只有1,612个被引用为与数据集中的至少一个问题相关。这些被引用的条款中大约80%来自民法、司法法、刑事调查法或刑法典。同时，32个法典中的18个被引用的条款少于5个，这可以解释为这些法典不太关注个人及其关切的问题。总体而言，这些被引用的条款的中位数引用次数是2，不到25%的条款被引用超过5次。\n\n使用我们的数据集，我们对包括词法和密集架构在内的多种检索方法进行了基准测试。给定一个查询和一条条款，词法模型通过计算查询项在该条款中的权重之和来为查询-条款对分配一个分数。我们实验了标准的TF-IDF和BM25排名函数。这些方法的主要问题是它们只能检索包含查询中关键字的条款。为了克服这一局限性，我们实验了一个基于神经网络的架构，可以捕捉查询和条款之间的语义关系。我们使用一个b-encoder模型，将查询和条款映射为密集向量表示，并通过计算它们嵌入之间的相似性来计算查询-条款对的相关分数。这些嵌入通常来自词嵌入模型对输入的池化操作。首先，我们研究了Siamese b-encoders在零样本评估设置中的有效性，这意味着预训练的词嵌入模型直接应用，没有额外的微调。我们实验了上下文独立的文本编码器，即Word2Vec和FastText，以及上下文依赖的嵌入模型，即Robota，更具体地说，Camembert，这是一个法语Robota模型。此外，我们在我们的数据集上训练了基于Camembert的b-encoder模型。请注意，在训练时，我们实验了b-encoder架构两种变体。Siamese使用一个唯一的词嵌入模型，将查询和条款一起映射到一个共享的密集向量空间中，而Two-Tower则使用两个独立的词嵌入模型，分别将查询和条款编码为不同的嵌入空间。我们实验了均值、最大值和CLS池化，以及点积和余弦计算相似度。以下是我们基线模型的结果，上面是上述词法方法，中间是Siamese b-encoders在零样本设置中的评估，下面是微调的b-encoders。总体而言，微调的b-encoders显著优于所有其他基线。Two-Tower模型在100的召回率上优于其Siamese变体，但在其他指标上表现相似。虽然BM25明显低于训练后的B-Encoder，但其表现表明它仍然是特定领域检索的强基线。关于Siamese B-Encoder的零样本评估，我们发现直接使用预训练的Camembert模型的嵌入，而没有针对信息检索任务进行优化，会产生糟糕的结果，这与先前的发现一致。此外，我们观察到基于Word2Vec的b-encoder显著优于FastText和BERT模型，这表明预训练的词级嵌入可能比字符级或子词级嵌入更适合这项任务，当直接使用时尤为如此。尽管这些结果很有前景，但与熟练的法律专家相比，他们最终可以检索出任何问题的全部相关条款，从而获得满分，因此仍有大量改进的空间。\n\n最后，让我们讨论数据集的两个局限性。首先，条款语料库仅限于从32个比利时法典中收集的条款，这并未涵盖整个比利时法律，因为来自法令、指令和条例的条款缺失。在数据集构建期间，对这些未收集条款的引用被忽略，导致一些问题只能保留最初相关条款的一小部分。这种信息丢失意味着剩余相关条款中的答案可能不完整，尽管仍然完全适用。其次，我们应该注意到并非所有法律问题都能仅用法规来回答。例如，如果房客制造过多噪音，我能驱逐他们吗？这个问题可能没有在法定法中找到详细答案，量化允许驱逐的特定噪音阈值。相反，房东可能更应该依赖判例法，找到与当前情况类似的先例。例如，房客每周举办两次派对，直到凌晨2点。因此，一些问题比其他问题更适合法定条款检索任务，而最不适合的领域仍有待确定。我们希望我们的工作能激发兴趣，开发实用且可靠的法定条款检索模型，以帮助改善所有人的司法公正。您可以在以下链接查看我们的论文、数据集和代码。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "你好！我们很高兴地介绍我们关于VALS的工作，这是一个任务独立的基准测试，旨在测试视觉和语言模型对特定语言现象的敏感度。为什么我们要花精力建立这个基准测试呢？在过去几年里，我们见证了基于转换器的视觉和语言模型的爆发式增长，这些模型在大量图像文本对上进行了预训练。这些模型在视觉和语言任务上不断突破极限，例如视觉问答、视觉常识推理、图像检索、短语定位等。我们得到的消息是——这些任务特定基准测试上的准确率正在稳步提高。但是，我们真的了解这些模型实际上学到了什么吗？当一个视觉和语言模型为这张图片和这句话的匹配给出高分，为另一对给出低分时，它理解了什么？视觉和语言模型是否关注正确的内容，还是像之前的研究所示，关注偏见？为了更好地了解这一点，我们提出了一个更任务无关的方向，并引入了VALS，用于测试视觉和语言模型对影响语言和视觉模式的特定语言现象的敏感度。我们关注的存在、复数、计数、空间关系、动作和实体共指等现象。\n\n但是，如何测试视觉和语言模型是否捕捉到了这些现象？我们使用FOILing方法，这是一种之前应用于视觉和语言模型的技术，由Ravi Shekhar等人在名词短语上应用，我们在计数上进行了应用。FOILing的基本原理是取一个图像的标题，并通过更改标题使其不再描述图像来生成FOIL（反例）。我们通过关注六个特定的部分来进行短语更改，即存在、复数、计数、空间关系、动作和实体共指，每个部分可以由一个或多个工具组成，如果我们发现了创建FOIL实例的多种有趣方法。例如，在动作部分，我们有两个工具，一个是改变动作动词，另一个是交换行为者。计数和共指也有多个工具。我们创建这些FOILs，确保它们无法描述图像，同时是语法上正确且有效的句子。这并不容易，因为一个被篡改的标题可能比原始标题更不符合逻辑。例如，虽然不是不可能的，但从统计学上讲，植物切割男人比男人切割植物更不常见，大型视觉和语言模型可能会捕捉到这一点。因此，为了获得有效的FOILs，我们必须采取行动。首先，我们使用强大的语言模型来提出FOILs。其次，我们使用自然语言推理（NLI）来过滤掉仍然可能描述图像的FOILs，因为在构建FOILs时，我们需要确保它们无法描述图像。\n\n为了自动测试这一点，我们应用自然语言推理，其逻辑如下：我们将图像视为前提，其标题视为其隐含的结论。此外，我们将标题视为前提，FOIL视为其假设。如果NLI模型预测FOIL与标题相矛盾或中立，我们将其视为有效FOIL的指标。如果NLI预测FOIL由标题隐含，那么它不能是一个好的FOIL，因为根据传递性，它将给出图像的真实描述，我们会过滤掉这些FOILs。但这个过程并不完美。它只是有效FOILs的指标，因此，作为生成有效FOILs的第三个措施，我们雇用了人类标注员来验证VALS中使用数据。\n\n经过过滤和人类评估，我们拥有了本表中描述的那样多的测试实例。请注意，VALS没有提供任何训练数据，只有测试数据，因为它是一个零样本测试基准测试。它旨在利用视觉和语言模型在预训练后的现有能力。微调只会使模型利用数据中的人工产物或统计偏见。我们都知道这些模型喜欢作弊和走捷径。正如我们所说，我们有兴趣评估视觉和语言模型在预训练后的能力。\n\n我们在VALS上对五个视觉和语言模型进行了实验，即CLIP、LXMERT、Wil VILBERT、VILBERT 12 in 1和VISUALBERT。我们两个最重要的评估指标是模型在将图像句子对分类为标题和FOIL方面的准确率。对于本视频来说，我们将展示我们更宽松的指标——配对准确率，它测量正确的图像文本对的对齐分数是否大于其FOIL对。有关更多指标和结果，请参阅我们的论文。这些结果与我们从其他指标获得的结果一致。\n\n从配对准确率的结果来看，Wilbert 12-in-1在零样本性能方面表现最佳，其次是Wilbert，这些结果与我们使用其他指标获得的结果一致。值得注意的是，以个体物体为中心的工具，如存在和名词短语，几乎被Wilbert 12 in 1解决，这表明模型能够识别图像中命名的物体及其存在。然而，在我们的对抗性FOIL设置中，没有一个剩余的部分可以被可靠地解决。\n\n从复数和计数工具中，我们可以看到视觉和语言模型在区分单个物体和多个物体的引用或在图像中对它们进行计数方面存在问题。关系部分显示，它们在正确分类图像中物体之间的命名空间关系方面存在困难。它们也难以区分动作并识别其参与者，即使有可信度偏见的支持，正如我们在动作部分所看到的。从共指部分中，我们发现使用代词跟踪图像中同一物体的多个引用对视觉和语言模型来说也很困难。\n\n作为合理性检查，也是因为这是一个有趣的实验，我们还对两个文本仅模型GPT-1和GPT-2进行了基准测试，以评估VALS是否能被这些单模态模型解决，我们计算了正确标题和FOIL标题的困惑度，并预测困惑度较低的条目。如果FOIL的困惑度更高，我们将其视为FOIL标题可能受到可信度偏见或其他语言偏见的影响的迹象。有趣的是，在某些情况下，仅文本的GPT模型比视觉和语言模型更好地捕捉了世界的可信度。\n\n总之，VALS是一个使用语言构造的视角来帮助社区通过严格测试其视觉定位能力来改进视觉和语言模型的基准测试。我们的实验表明，视觉和语言模型能够很好地识别图像中命名的物体及其存在，正如存在部分所示，但在被迫尊重语言指标时，它们在视觉场景中难以定位它们之间的相互依存关系和关系。我们真诚地鼓励社区使用VALS来衡量在语言与视觉模型的语言定位方面取得的进展。更进一步，VALS还可以作为对数据集的间接评估，因为模型可以在训练或微调前后的不同阶段进行评估，以了解数据集是否有助于模型在VALS测试的任何方面取得改进。如果您感兴趣，请在GitHub上查看VALS数据，如果您有任何问题，请随时与我们联系。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是来自东京大学的Kamizawa。我将展示一篇题为RNSUN的论文，这是一个大规模的数据集，用于通过提交日志总结自动生成发布说明。我将按以下顺序进行说明。首先，我将介绍我们在此研究中正在进行的自动发布说明生成工作。发布说明是一份技术文档，总结了随每个软件产品版本一起分发的更改。图中显示的是Vue.js库2.6.4版本的发布说明。在开源开发中，发布说明发挥着重要作用，但手动准备起来非常耗时。因此，能够自动生成高质量的发布说明将非常有用。我将参考两项关于自动发布说明生成的早期研究。第一项是名为Arena的系统，于2014年发布。它采用基于规则的方法，例如，使用更改提取器从发布之间的差异中提取核心差异、库更改和文档更改，最后将它们组合起来。该系统的最显著特征是右上角的议题提取器，它必须与Jira问题跟踪系统链接，并且只能应用于使用Jira的项目。换句话说，它不能用于GitHub上的许多项目。第二项是Glyph，最近于2020年宣布。它在网上提供，可以通过PIP存储。该系统拥有一个简单的基于学习的文本分类模型，并为每个输入的提交消息输出五个级别之一，如功能或错误修复。图中是一个示例用法，返回更正或错误修复标签。Glyph的训练数据相当小，大约5000个，在下面的实验中，文本分类模型的性能并不高。我介绍了两项相关研究，但它们存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成高质量的发布节点。对于适用性有限的问题，我们提出了一种仅使用提交消息作为输入的高质量分类器总结方法。这种拟议的方法可以用于所有英语仓库。对于数据资源稀缺的第二个问题，我们通过收集来自公共GitHub仓库的数据，使用GitHub API构建了一个R和sum数据集，大约包含82,000条数据。我们的Rnsum数据集由从公共GitHub仓库收集的大约82,000条数据组成。接下来，我将描述我们的数据集。这里是一个数据示例。左侧是提交消息，右侧是发布说明。发布说明被标记为面部改进等。我们设置了一个任务，将提交消息作为输入，并输出标记的发布节点。这可以被视为一个总结任务。我们预定义了四个标签：功能、改进、错误修复、弃用和删除，以及破坏性更改。这些是基于早期研究和其他因素设定的。底右角的最小节点是从底左角显示的最小节点中提取的。此时，需要检测四个预设标签。但标签并不总是与每个仓库一致。例如，改进标签包括改进、增强、优化等。我们为我们研究中的每个记号变化准备了一个词汇表列表，并将其用于检测发布说明类，并更正后续列表中的文本作为该类的发布说明句子。需要识别前一版本从2.5到18并获取其diff。这有点麻烦，仅仅获取发布列表并查看前后并不足够。我们创建了一个启发式匹配规则来获取前一版本和后一版本。数据集分析。最终，收集了7,200个仓库和82,000条数据。此外，发布节点令牌的平均数量为63，这对于总结任务来说相当高。唯一令牌的数量也相当大，达到8,830,000。这是因为仓库中发现了大量唯一的类和方法名称。接下来，我将解释我们提出的方案。类智慧提取式然后是抽象式总结模型由两个神经网络组成：一个分类器和一个生成器。分类器将每个提交消息分类为五个发布说明类。我们选择实现、错误修复、弃用加和其他。分类为其他的消息被丢弃。然后，CEAS独立地将生成器应用于四个标签文档，并为每个类生成发布说明。在这个任务中，提交消息和发布说明之间的直接对应关系并不已知。因此，为了训练分类器，我们使用每个提交消息的前10个字符为每个输入提交消息分配伪标签。我们通过两种不同方法对类智慧抽象总结进行建模。第一个模型，我们称之为cssingle，由一个集到集网络组成，并生成一个长段落，给定输入提交消息的连接。第二个模型，我们称之为cas，由四个独立的集到集网络组成，每个网络对应于最不了解的类之一。好，让我解释实验。我们比较了五种方法：CAS、CASSingle、CASMatch、PlusSelling和早期研究GRIF。关于偏差，在某些情况下，CSMatch、Blustering和早期研究Glyph表现不佳。关于评估，在某些情况下，发布说明以多句形式输出。由于它们难以按原样计算句子数量，因此它们被空格连接并视为一个长句。蓝色在系统输出短句时受到惩罚。这种惩罚导致实验中描述的下一个结果的蓝色值降低。最后，我们还计算了特异度，因为如果发布说明为空，则无法计算rouge和蓝色。高特异度意味着模型在假设发布说明为空的情况下正确输出空文本。以下是结果。由于数据集包含电子邮件地址、哈希值等，我们还评估了排除它们的清理数据集。CES和CAS在rouge L分数上比基线高出10多分。特别地，在清理测试集中，拟议方法与基线之间的分数差距跃升至20多分。这些结果表明CAS和CAS非常有效。CAS比CAS获得了更好的root-A分数，这表明在使用两倍数据训练分类器时，结合分类器和生成器是有效的。CAS的高覆盖率可能是因为分类器能够专注于为每个类选择相关的提交消息。CASmatch倾向于比CASsingle产生更高的rouge L，这表明为每个发布说明类独立开发不同的抽象总结模型也是有效的。这里是错误分析。CAS方法倾向于输出比人类参考句子短的句子。在右边的图中，参考句子有三个或四个句子，而CAS只有一个。这种模型不愿意的原因是在训练数据中，只有33%的句子存在于功能标签中，40%在改进标签中。此外，CES方法在没有额外信息的情况下无法生成准确的发布说明。右边最上面的示例是一个非常混乱的提交消息的示例，而完整的句子无法在不参考相应的拉取请求或问题的情况下生成。下面的示例表明输入的两个提交消息相关，应该组合为一个句子，但它未能做到。最后，一个结论。我们为自动列表生成构建了一个新数据集。我们还形成了输入提交消息并总结它们以适用于所有用英语编写的项目的任务。我们的实验表明，拟议的方法比基线生成覆盖率更高的低噪声发布说明。请查看我们的下降仅选项卡。谢谢。"}
