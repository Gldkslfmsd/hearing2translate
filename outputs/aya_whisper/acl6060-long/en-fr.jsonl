{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Asaf Harari et je vais présenter notre article, « Enrichissement de données tabulaires en quelques tirs en utilisant des architectures de transformateurs affinés ». Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données. Mais parfois, ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter une information substantielle. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à l'aide de textes libres provenant de sources externes. La génération de caractéristiques à partir d'une autre source de données peut ajouter une information substantielle. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à l'aide de textes libres provenant de sources externes.\n\nSupposons que nous disposions d'un jeu de données tabulaires et d'une base de connaissances. Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre, FAST, est exactement ce processus automatique. Examinons donc un exemple. Dans un jeu de données introduit dans FAST, prenons le cas d'un jeu de données d'universités dont l'objectif est de classer les universités en universités mal classées et bien classées. Comme base de connaissances, nous utilisons Wikipédia.\n\nLa première phase de FAST est la liaison d'entités. Chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances. Et le texte des entités de la base de connaissances est extrait et ajouté au jeu de données. Dans cet exemple, le texte est l'abstrait de la page Wikipédia. Nous devons maintenant générer ou extraire des caractéristiques à partir du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques, qui inclut l'analyse de texte. Et c'est la principale nouveauté de cet article, sur laquelle je reviendrai en détail dans les diapositives suivantes.\n\nAprès la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques où nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Premièrement, nous générons des caractéristiques au nombre de classes du jeu de données original. Dans cet exemple, le jeu de données original a deux classes, donc FAST génère deux nouvelles caractéristiques. Mais si le jeu de données a cinq classes, FAST génère cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe.\n\nPour analyser le texte, nous utilisons l'état actuel de l'art en matière d'analyse de texte, qui sont les modèles de langage basés sur les transformateurs tels que BERT, GPT, XNL, etc. Cependant, il est peu probable que nous puissions entraîner un modèle de langage avec les jeux de données d'entrée. Une approche naïve serait donc un réglage fin sur la tâche cible. Ainsi, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, puis affiner le modèle de langage sur le jeu de données cible. Dans cet exemple, pour affiner le modèle de langage, nous classons le texte en classes, c'est-à-dire en abstraits de classes, bas ou élevés, nous recevons la sortie du modèle de langage, qui est la probabilité pour chaque classe, et nous l'utilisons comme nouvelles caractéristiques.\n\nLe problème avec cette approche est que les jeux de données peuvent avoir peu d'étiquettes d'entités distinctes. Dans nos expériences, près de la moitié des jeux de données contiennent moins de 400 échantillons, et le plus petit jeu de données contenait 35 échantillons dans son ensemble d'entraînement. Affiner un modèle de langage sur un tel jeu de données serait donc inefficace. Mais nous pouvons utiliser des connaissances préalables sur des jeux de données pré-analysés et utiliser ces informations lors de l'analyse du Nème jeu de données.\n\nCe que nous suggérons, c'est d'ajouter une autre phase de réglage fin, une phase de réglage fin multitâche préliminaire, où nous affinons le modèle de langage sur N-1 jeux de données, puis nous exécutons une autre phase de réglage fin, qui est un réglage fin sur la tâche cible, lorsque nous affinons le modèle de langage sur le Nème jeu de données cible. L'état actuel de l'art en matière de réglage fin multitâche est appelé mtDNN. Dans mtDNN, mtDNN maintient des têtes au nombre de tâches dans l'ensemble d'entraînement. Donc, dans cet exemple, il y a quatre tâches dans l'ensemble d'entraînement, mtDNN maintient donc quatre têtes, comme vous pouvez le voir sur l'image, et il échantillonne un lot aléatoire à partir de l'ensemble d'entraînement. Si le lot aléatoire appartient, par exemple, à une tâche de classification de phrases uniques, il exécute une passe avant et arrière à travers la première tête. Si le lot aléatoire appartient à une tâche de classement par paires, il exécute une passe avant et arrière à travers la dernière tête.\n\nDans notre scénario, les jeux de données tabulaires varient en nombre de classes. Donc, dans notre scénario, un jeu de données tabulaire vérifie le nombre de classes, il y a donc de nombreuses tâches. MTDNN maintient des têtes au nombre de classes, des couches de sortie, et de plus, MTDNN doit initialiser de nouvelles têtes pour un nouveau jeu de données avec une nouvelle tâche. Notre approche, appelée réglage fin de reformulation de tâche, consiste à reformuler chaque jeu de données en une phrase par problème de classification, qui est une tâche à deux classes. Au lieu de maintenir plusieurs têtes, nous reformulons chaque jeu de données en une phrase par problème de classification, qui est une tâche à deux classes.\n\nExaminons un exemple. Voici notre jeu de données d'entrée, qui se compose d'entités, de caractéristiques, de texte et de classes. Et nous passons de la classification du texte en bas et haut à la classification de l'abstrait en classe, c'est-à-dire à déterminer si l'abstrait appartient à la classe ou non. Donc, dans ce cas, le vecteur d'étiquettes est toujours composé de deux classes. Et cela reformule la tâche en tâches de classification par phrase. On applique ensuite le modèle de langage à la nouvelle tâche et on obtient la probabilité de sortie pour chaque classe. Notons que le modèle de langage est déjà affiné sur N-1 jeux de données à l'aide d'un réglage fin multitâche préliminaire. Nous utilisons ensuite le vecteur de sortie du modèle de langage comme nouvelle caractéristique générée au nombre de classes.\n\nPour évaluer notre cadre, nous utilisons 17 jeux de données de classification tabulaire, qui vérifient la taille, les caractéristiques, l'équilibre, le domaine et les performances initiales. Et comme base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation en laissant de côté un jeu de données, où nous entraînons FAST sur 16 jeux de données et l'appliquons au 17ème jeu de données. Nous divisons également chaque jeu de données en quatre plis et appliquons une validation croisée à quatre plis. Nous générons ensuite les nouvelles caractéristiques et les évaluons à l'aide de cinq classifieurs d'évaluation.\n\nNous utilisons dans notre expérience une architecture basée sur les transformateurs. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre cadre à un réglage fin sur le jeu de données cible, à un réglage fin sur la tâche cible et à un réglage fin préliminaire mtDNN, et notre réglage fin reformulé obtient le meilleur résultat, les meilleures performances. Alors que MTDNN a obtenu une amélioration de deux pour cent par rapport au réglage fin sur le jeu de données cible, notre approche a atteint une amélioration de six pour cent. Lorsque nous examinons les petits jeux de données, nous pouvons voir que les performances du DNN vide diminuent, tandis que l'amélioration du réglage fin sur la tâche cible seule est plus faible.\n\nEn résumé, FAST permet un enrichissement en quelques tirs à partir de 35 échantillons dans notre expérience. Il utilise une seule architecture pour tous les jeux de données de tâches et maintient la tête du modèle. Mais il ajoute une phase de reformulation. Il augmente l'ensemble d'entraînement et nécessite une valeur cible avec une signification sémantique, que nous pouvons introduire dans le modèle de langage et utiliser dans le problème de classification par phrase. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous, aujourd'hui je vais présenter notre travail de recherche, Apprendre à Raisonner de Manière Déductive, Résolution de Problèmes Métabolisant l'Extraction de Raisons Complexes. Je suis Alan du ByteDance AI Lab, et c'est un travail en collaboration avec Jerry de l'Université du Texas à Austin et Weilu du SUTD.\n\nTout d'abord, j'aimerais parler de notre motivation pour la déduction. Voici un exemple où la déduction en plusieurs étapes est bénéfique. Cette figure est extraite d'un travail sur papier où l'on utilise des invites pour résoudre un problème mathématique dans un scénario d'apprentissage à quelques tirs. À gauche, on voit que si nous fournissons seulement des exemples avec des questions et des réponses, nous ne parvenons pas à obtenir les réponses correctes. Mais si nous ajoutons une description de la déduction, le modèle peut prédire cette description et donner une prédiction correcte. Il est donc avantageux d'avoir une déduction interprétable en plusieurs étapes comme sortie. Nous pensons également que le problème de méthode est une application directe pour évaluer de telles capacités de déduction.\n\nDans notre configuration de problème, étant donné les questions, nous devons résoudre celle-ci et obtenir des réponses numériques. Dans nos ensembles de données, nous avons également l'expression mathématique qui mène à cette réponse spécifique. Certaines hypothèses s'appliquent également comme dans les travaux précédents. Nous supposons que la précision des quantités est connue et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, les opérateurs compliqués peuvent être décomposés en ces opérateurs de base.\n\nLes travaux précédents sur la résolution de problèmes de méthode peuvent être catégorisés en modèles séquence-à-séquence et séquence-à-arbre. Le modèle séquence-à-séquence traditionnel convertit l'expression en une séquence spécifique pour la génération, ce qui est facile à mettre en œuvre et peut généraliser à de nombreux problèmes compliqués. Cependant, les performances ne sont généralement pas meilleures que celles des modèles structurés et il manque d'interprétabilité dans les prédictions. Mais cette approche reste populaire en raison du modèle transformateur.\n\nDans les modèles basés sur des arbres, nous structurons ces expressions sous forme d'arbre et suivons une traversée pré-ordonnée lors de la génération. Nous générons les opérateurs jusqu'à atteindre les feuilles, qui sont les quantités. Cela nous donne une structure d'arbre binaire, mais c'est contre-intuitif car nous générons d'abord l'opérateur puis les quantités à la fin. De plus, il y a des calculs redondants, comme l'expression « a fois 3 plus 3 » générée deux fois.\n\nDans notre approche proposée, nous voulons résoudre ces problèmes de manière étape par étape et interprétable. Par exemple, à la deuxième étape, nous obtenons ce diviseur qui est 27 et nous pouvons également revenir aux questions originales pour trouver les contenus pertinents. Aux étapes suivantes, nous obtenons les diviseurs, puis à la troisième étape, nous obtenons le quotient. Après ces trois étapes, nous pouvons réutiliser les résultats de la deuxième étape pour obtenir les résultats de la quatrième étape, et enfin, nous obtenons les dividendes. Nous générons ainsi l'expression complète directement plutôt que des opérateurs ou quantités individuels, ce qui rend le processus plus précis.\n\nDans notre système déductif, nous commençons par un ensemble de quantités présentées dans les questions, incluant également des constantes comme états initiaux. L'expression est représentée par EIJOP, où nous appliquons l'opérateur de QI à QJ, et cette expression est dirigée. Nous avons également une soustraction inverse pour représenter la direction opposée, ce qui est similaire à l'extraction de relations.\n\nDans un système déductif formel, à l'étape temporelle t, nous appliquons l'opérateur entre la paire QI et QJ, obtenant ainsi de nouvelles expressions. Nous l'ajoutons à l'état suivant pour devenir une nouvelle quantité. Cette diapositive visualise l'évolution des états où nous ajoutons continuellement des expressions.\n\nDans la mise en œuvre de notre modèle, nous utilisons d'abord un modèle de langage pré-entraîné, comme BERT ou RoBERTa, puis nous encodons une phrase pour obtenir des représentations de quantités. Une fois ces représentations obtenues, nous pouvons commencer l'inférence. Nous montrons un exemple pour Q1, où nous obtenons la représentation de Q1 divisé par Q2 puis multiplié par Q3. Premièrement, nous obtenons la représentation de paire, qui est la concaténation de Q1 et Q2. Ensuite, nous appliquons un réseau de neurones à propagation directe, paramétré par l'opérateur, et enfin, nous obtenons la représentation de l'expression Q1 divisé par Q2.\n\nEn pratique, lors de l'étape d'inférence, nous pouvons également obtenir des expressions incorrectes. Le nombre de toutes les expressions possibles est égal à trois fois le nombre d'opérateurs. Nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si une expression n'est pas autorisée, nous l'éliminons de l'espace de recherche.\n\nLa procédure d'entraînement est similaire à celle d'un modèle séquence-à-séquence, où nous optimisons la perte à chaque étape temporelle. Nous utilisons également τ pour représenter le moment où nous devrions terminer le processus de génération. L'espace est différent d'un modèle séquence-à-séquence car il varie à chaque étape temporelle.\n\nNous avons mené des expériences sur les ensembles de données de problèmes de méthode couramment utilisés : MAWPS, Math23k, MathQA et SWAM. Notre meilleur modèle, RoBERTa Deductive Reasoner, surpasse significativement les modèles basés sur des arbres, mais les scores absolus sur MathQA ou SWAM ne sont pas très élevés.\n\nEn investigant plus avant sur SWAM, nous constatons que ce jeu de données est difficile car l'auteur a tenté de confondre les modèles NLP en ajoutant des informations irrelevantes et des quantités supplémentaires. Dans nos prédictions, nous trouvons des valeurs intermédiaires négatives. Par exemple, pour la question « Combien de pommes Jake a-t-il ? », nous avons des informations supplémentaires comme « 17 pêches en moins » et « Steven a 8 pêches », qui sont irrelevantes. Notre modèle prédit des valeurs négatives.\n\nNous observons que deux expressions ont des scores similaires, nous pouvons donc limiter l'espace de recherche en éliminant les résultats négatifs pour obtenir une réponse correcte. Cette contrainte améliore considérablement les performances pour certains modèles, comme BERT avec une amélioration de 7 points, et RoBERTa avec une amélioration de 2 points. Un meilleur modèle de langage a une meilleure capacité de compréhension du langage, d'où des scores plus élevés pour RoBERTa et plus faibles pour BERT.\n\nNous avons également analysé la difficulté de ces ensembles de données en supposant que le nombre de quantités non utilisées peut être considéré comme des informations irrelevantes. Le pourcentage d'échantillons avec des quantités non utilisées est le plus élevé dans SWAM. La performance globale sur les échantillons sans quantités non utilisées est plus élevée que la performance globale, mais elle est bien plus faible pour les échantillons avec quantités non utilisées.\n\nPour MAWPS, nous n'avons pas beaucoup de cas difficiles, donc j'ai ignoré cette partie. Enfin, nous voulons montrer l'interprétabilité à travers un exemple de crash et de participation. Notre modèle fait une prédiction incorrecte à la première étape, et nous pouvons corréler cette expression avec la phrase. Nous pensons que cette phrase induit le modèle en erreur. En imprimant « 35 autres », le modèle pense qu'il s'agit d'un opérateur d'addition.\n\nEn révisant la phrase pour dire « Le nombre de poiriers est 55 de moins que celui des pommiers », nous transmettons une sémantique plus précise, permettant au modèle de faire une prédiction correcte. Cette étude démontre comment les prédictions interprétables nous aident à comprendre le comportement du modèle.\n\nPour conclure, notre modèle est efficace et fournit une procédure de résolution interprétable. Nous pouvons facilement intégrer des connaissances préalables comme contraintes pour améliorer les performances. Le mécanisme sous-jacent s'applique non seulement aux tâches de résolution de problèmes mathématiques, mais aussi à d'autres tâches impliquant une déduction en plusieurs étapes. Cependant, nous avons des limitations : une grande quantité d'opérateurs ou de constantes peut entraîner une consommation de mémoire élevée, et l'application d'une stratégie de recherche par faisceau est difficile en raison de la distribution de probabilité déséquilibrée à différentes étapes temporelles.\n\nMerci pour votre attention, et n'hésitez pas à poser des questions."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je viens de l'Université de Maastricht. Je vais présenter mon travail collaboratif avec Jerry, qui porte sur un nouveau jeu de données pour la récupération d'articles de loi. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus légaux fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique restent sans protection ou, pire, sont exploités. Notre travail vise à combler le fossé entre les individus et la loi en développant des systèmes de récupération efficaces pour les articles de loi. Un tel système pourrait offrir un service d'aide juridique gratuit aux personnes non qualifiées.\n\nAvant d'aborder la principale contribution de ce travail, décrivons le problème de la récupération d'articles de loi. Étant donné une question simple sur un sujet juridique, comme « Quels risques encours-je si je viole le secret professionnel ? », un modèle est requis pour récupérer tous les articles de loi pertinents à partir d'un vaste corpus de législation. Cette tâche de récupération d'information présente ses propres défis. Premièrement, elle implique deux types de langage : le langage naturel courant pour les questions et le langage juridique complexe pour les statuts. Cette différence dans la distribution des langues rend plus difficile pour un système la récupération de candidats pertinents, car elle nécessite indirectement un système d'interprétation inhérent capable de traduire une question naturelle en une question juridique correspondant à la terminologie des statuts.\n\nDe plus, le droit statutaire n'est pas une simple collection d'articles indépendants qui peuvent être traités comme une source d'information complète en soi, comme les actualités ou les recettes, par exemple. Au contraire, il s'agit d'une collection structurée de dispositions légales qui n'acquièrent un sens complet que lorsqu'elles sont considérées dans leur contexte global, c'est-à-dire avec les informations complémentaires de leurs articles voisins, des domaines et sous-domaines auxquels ils appartiennent, et leur place dans la structure de la loi. Enfin, les articles de loi sont généralement de petits paragraphes, qui constituent habituellement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, nous avons affaire à des documents longs pouvant atteindre 6 000 mots.\n\nLes récentes avancées en traitement automatique du langage naturel (TALN) ont suscité un grand intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements ou la révision automatisée de contrats, mais la récupération d'articles de loi est restée largement inchangée en raison du manque de jeux de données étiquetés de grande taille et de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur les citoyens francophones pour évaluer si les modèles de récupération peuvent approcher l'efficacité et la fiabilité des experts juridiques dans la tâche de récupération d'articles de loi.\n\nNotre jeu de données belge de récupération d'articles de loi, PSART, comprend plus de 1 100 questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, allant de la famille, du logement, de l'argent, au travail et à la sécurité sociale. Chacune d'elles a été étiquetée par des juristes expérimentés avec des références aux articles pertinents d'un corpus de plus de 22 600 articles juridiques issus des codes de loi belges.\n\nPassons maintenant à la description de la collecte de ce jeu de données. Tout d'abord, nous avons constitué un grand corpus d'articles juridiques. Nous avons pris en compte 32 codes belges publics et extrait tous leurs articles ainsi que les titres de sections correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ 4 000 courriels de citoyens belges demandant des conseils sur un problème juridique personnel. Nous avons eu la chance d'obtenir accès à leur site web, où leur équipe de juristes expérimentés aborde les problèmes juridiques les plus courants en Belgique. Nous avons collecté des milliers de questions, annotées avec des catégories, sous-catégories et références juridiques aux statuts pertinents. Enfin, nous avons analysé les références juridiques et exclu les questions dont les références n'étaient pas des articles de l'un des codes de loi considérés. Les références restantes ont été mises en correspondance et converties en identifiants d'articles correspondants dans notre corpus. Nous avons ainsi obtenu 1 108 questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre vaste corpus de 22 633 articles de loi.\n\nEn outre, chaque question est accompagnée d'une catégorie principale et d'une concaténation de sous-catégories, et chaque article est accompagné d'une concaténation de ses titres successifs dans la structure de la loi. Ces informations supplémentaires ne sont pas utilisées dans le présent travail, mais pourraient présenter un intérêt pour les futures recherches sur la récupération d'informations juridiques ou la classification de textes juridiques.\n\nExaminons maintenant certaines caractéristiques de notre jeu de données. Les questions varient de 5 à 44 mots, avec une médiane de 40 mots. Les articles sont beaucoup plus longs, avec une médiane de 77 mots, et 142 d'entre eux dépassent 1 000 mots, le plus long atteignant 5 790 mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, avec environ 85 % d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les 15 % restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très divers, car ils proviennent de 32 codes belges différents couvrant un grand nombre de sujets juridiques. Voici le nombre total d'articles collectés à partir de chacun de ces codes belges.\n\nSur les 22 633 articles, seuls 1 612 sont cités comme pertinents pour au moins une question du jeu de données. Et environ 80 % de ces articles cités proviennent du code civil, du code judiciaire, du code d'instruction criminelle ou du code pénal. Pendant ce temps, 18 des 32 codes ont moins de 5 articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de 2, et moins de 25 % d'entre eux sont cités plus de 5 fois.\n\nEn utilisant nos jeux de données, nous avons évalué plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête et un article, un modèle lexical attribue un score à la paire requête-article en calculant la somme des poids de chaque terme de la requête dans cet article. Nous expérimentons avec les fonctions de classement TF-IDF et BM25 standard. Le principal problème avec ces approches est qu'elles ne peuvent récupérer que les articles contenant des mots-clés présents dans la requête.\n\nPour surmonter cette limitation, nous expérimentons avec une architecture basée sur les réseaux neuronaux qui peut capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle b-encodeur qui mappe les requêtes et les articles en représentations vectorielles denses et calcule un score de pertinence entre une paire requête-article en fonction de la similarité de leurs embeddings. Ces embeddings résultent généralement d'une opération de regroupement sur la sortie d'un modèle d'embedding de mots.\n\nTout d'abord, nous étudions l'efficacité des b-encodeurs Siamese dans un contexte d'évaluation zéro-shot, ce qui signifie que les modèles d'embedding de mots pré-entraînés sont utilisés tels quels, sans aucun réglage supplémentaire. Nous expérimentons avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et FastText, et des modèles d'embedding dépendants du contexte, à savoir Robota et, plus spécifiquement, Camembert, qui est un modèle Robota en français. De plus, nous entraînons nos propres b-encodeurs basés sur Camembert sur nos jeux de données.\n\nIl convient de noter que, pour l'entraînement, nous expérimentons avec les deux variantes de l'architecture b-encodeur : Siamese, qui utilise un unique modèle d'embedding de mots pour mapper la requête et l'article ensemble dans un espace vectoriel dense partagé, et Two-Tower, qui utilise deux modèles d'embedding de mots indépendants pour encoder séparément la requête et l'article dans des espaces d'embedding différents. Nous expérimentons avec le regroupement moyen, max et CLS, ainsi qu'avec le produit scalaire et la cosinus pour calculer les similarités.\n\nVoici les résultats de nos baselines, avec les méthodes lexicales ci-dessus, les b-encodeurs Siamese évalués dans un contexte zéro-shot au milieu, et les b-encodeurs réglés en bas. Dans l'ensemble, les b-encodeurs réglés surpassent significativement toutes les autres baselines. Le modèle Two-Tower améliore son variant Siamese en termes de rappel à 100, mais performe de manière similaire sur les autres métriques. Bien que BM25 ait sous-performé le b-encodeur réglé de manière significative, sa performance indique qu'il reste une baseline solide pour la récupération spécifique au domaine.\n\nEn ce qui concerne l'évaluation zéro-shot des b-encodeurs Siamese, nous constatons que l'utilisation directe des embeddings d'un modèle Camembert pré-entraîné, sans optimisation pour la tâche de récupération d'informations, donne des résultats médiocres, ce qui est cohérent avec les découvertes précédentes. De plus, nous avons observé que le b-encodeur basé sur Word2Vec a significativement surpassé les modèles basés sur FastText et BERT, suggérant que les embeddings de mots pré-entraînés au niveau du mot pourraient être plus adaptés à la tâche que les embeddings au niveau des caractères ou des sous-mots lorsqu'ils sont utilisés tels quels.\n\nBien que prometteurs, ces résultats suggèrent une marge d'amélioration par rapport à un expert juridique qualifié qui peut éventuellement récupérer tous les articles pertinents pour n'importe quelle question et obtenir ainsi des scores parfaits.\n\nPour conclure, discutons des deux limitations de tous les jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des 32 codes belges considérés, ce qui ne couvre pas l'ensemble de la loi belge, car les articles des décrets, directives et ordonnances manquent. Pendant la construction du jeu de données, toutes les références à ces articles non collectés sont ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction du nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\n\nDeuxièmement, il est important de noter que toutes les questions juridiques ne peuvent pas être répondues uniquement avec des statuts. Par exemple, la question « Puis-je expulser mes locataires s'ils font trop de bruit ? » pourrait ne pas avoir de réponse détaillée dans la loi statutaire quantifiant un seuil de bruit spécifique au-delà duquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement se fier davantage à la jurisprudence et trouver des précédents similaires à sa situation actuelle. Par exemple, le locataire organise deux fêtes par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles de loi, et le domaine de celles moins adaptées reste à déterminer.\n\nNous espérons que notre travail suscitera un intérêt pour le développement de modèles de récupération d'articles de loi pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article, jeu de données et code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour ! Nous sommes ravis de vous présenter notre travail sur VALS, une référence d'évaluation indépendante de la tâche destinée à tester les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous entrepris la création de cette référence ? Ces dernières années, nous avons assisté à une explosion des modèles de vision et de langage basés sur les transformateurs, pré-entraînés sur de grandes quantités de paires image-texte. Chacun de ces modèles améliore les performances sur des tâches spécifiques de vision et de langage, telles que la réponse à des questions visuelles, la raisonnement de bon sens visuel, la récupération d'images, l'ancrage de phrases. Cependant, nous avons reçu un message clair : les précisions sur ces références spécifiques à une tâche augmentent régulièrement. Mais savons-nous réellement ce que les modèles ont appris ? Qu'est-ce qu'un transformateur de vision et de langage comprend lorsqu'il attribue une note élevée à l'appariement de cette image et de cette phrase, et une note faible à un autre appariement ? Les modèles de vision et de langage se concentrent-ils sur les bons éléments, ou sont-ils influencés par des biais comme l'ont montré des travaux antérieurs ?\n\nPour apporter plus de clarté à cet aspect, nous proposons une approche plus agnostique par rapport à la tâche et introduisons des « valves » qui testent la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistique et visuelle. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence d'entités.\n\nMais comment tester si les modèles de vision et de langage ont saisi ces phénomènes ? Nous utilisons la méthode FOILing, précédemment appliquée aux modèles de vision et de langage uniquement pour les groupes nominaux par Ravi Shekhar et collaborateurs, et pour le comptage par nos soins dans un travail antérieur. FOILing consiste à prendre la légende d'une image et à produire une « valve » en modifiant la légende de manière à ce qu'elle ne décrive plus l'image. Nous effectuons ces altérations de phrases en nous concentrant sur six éléments spécifiques : l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence d'entités. Chaque élément peut comporter plusieurs « instruments », car nous avons identifié plusieurs façons intéressantes de créer des instances FOIL. Par exemple, dans le cas des actions, nous avons deux instruments : l'un dans lequel le verbe d'action est remplacé par un autre verbe, et l'autre dans lequel les actants sont échangés. Le comptage et la coréférence sont également des éléments comportant plusieurs instruments.\n\nNous créons ces valves en veillant à ce qu'elles ne décrivent plus l'image, tout en étant grammaticalement correctes et valides. Cette tâche n'est pas aisée, car une légende altérée peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme que l'inverse, et les grands modèles de vision et de langage pourraient détecter ce biais.\n\nPour obtenir des valves valides, nous devons donc prendre des mesures. Premièrement, nous utilisons des modèles linguistiques puissants pour proposer des valves. Deuxièmement, nous employons l'inférence en langage naturel (NLI) pour éliminer les valves qui pourraient encore décrire l'image. En effet, lors de la création des valves, nous devons nous assurer qu'elles ne décrivent plus l'image. Pour tester cela automatiquement, nous appliquons l'inférence en langage naturel avec la logique suivante : nous considérons l'image comme la prémisse et sa légende comme l'entaillement. De plus, nous considérons la légende comme la prémisse et la valve comme l'hypothèse. Si un modèle NLI prédit que la valve contredit ou est neutre par rapport à la légende, nous l'interprétons comme une valve valide. Si le modèle NLI prédit que la valve est entailée par la légende, elle ne peut pas être une bonne valve, car par transitivité, elle fournirait une description véridique de l'image, et nous éliminons ces valves.\n\nCependant, cette procédure n'est pas parfaite. Il s'agit simplement d'un indicateur de valves valides. Par conséquent, en tant que troisième mesure pour générer des valves valides, nous faisons appel à des annotateurs humains pour valider les données utilisées dans VALS.\n\nAinsi, après filtrage et évaluation humaine, nous obtenons autant d'instances de test que décrites dans ce tableau. Notez que VALS ne fournit aucune donnée d'entraînement, mais uniquement des données de test, car il s'agit d'une référence d'évaluation à zéro coup seulement. Il est conçu pour exploiter les capacités existantes des modèles de vision et de langage après pré-entraînement. Un réglage fin n'aurait pour effet que de permettre aux modèles d'exploiter des artefacts ou des biais statistiques dans les données, et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Comme nous l'avons mentionné, nous souhaitons évaluer les capacités des modèles de vision et de langage après pré-entraînement.\n\nNous expérimentons avec cinq modèles de vision et de langage sur VALS : CLIP, LXMERT, Wil VILBERT, VILBERT 12 en 1, et VISUALBERT. Deux de nos métriques d'évaluation les plus importantes sont la précision des modèles dans la classification des paires image-phrase en légendes et valves. Dans cette vidéo, nous mettrons plutôt en avant notre métrique plus permissive, la précision paire à paire, qui mesure si le score d'alignement image-phrase est plus élevé pour la paire image-texte correcte que pour sa paire altérée. Pour plus de métriques et de résultats, veuillez consulter notre article. Les résultats obtenus avec la précision paire à paire sont cohérents avec ceux des autres métriques.\n\nIl est notable que la meilleure performance à zéro coup est obtenue par Wilbert 12 en 1, suivi de Wilbert. Les instruments centrés sur les objets individuels, comme l'existence et les groupes nominaux, sont presque résolus par Wilbert 12 en 1, ce qui met en évidence la capacité des modèles à identifier les objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos paramètres de FOILing adversarial.\n\nNous constatons, à partir des instruments de pluralité et de comptage, que les modèles de vision et de langage ont du mal à distinguer les références à un objet unique de celles à plusieurs objets, ou à compter ces derniers dans une image. L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image. Ils peinent également à distinguer les actions et à identifier leurs participants, même lorsqu'ils sont soutenus par des biais de plausibilité, comme nous le voyons dans l'élément actions. À partir de l'élément coréférence, nous découvrons qu'il est également difficile pour les modèles de vision et de langage de suivre les multiples références au même objet dans une image en utilisant des pronoms.\n\nÀ titre de contrôle et parce que c'est une expérience intéressante, nous avons également évalué deux modèles de texte uniquement, GPT-1 et GPT-2, pour déterminer si VALS peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et de la légende altérée, puis en prédisant l'entrée avec la perplexité la plus faible. Si la perplexité est plus élevée pour la légende altérée, nous interprétons cela comme une indication que la légende altérée peut souffrir de biais de plausibilité ou d'autres biais linguistiques. Il est intéressant de noter que, dans certains cas, les modèles GPT uniquement textuels ont mieux saisi la plausibilité du monde que les modèles de vision et de langage.\n\nPour résumer, VALS est une référence d'évaluation qui utilise le prisme des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient bien les objets nommés et leur présence dans les images, comme le démontre l'élément existence, mais qu'ils peinent à ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont contraints de respecter des indicateurs linguistiques. Nous encourageons vivement la communauté à utiliser VALS pour mesurer les progrès vers l'ancrage linguistique avec les modèles de vision et de langage. De plus, VALS pourrait servir d'évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après l'entraînement ou le réglage fin pour déterminer si un ensemble de données aide les modèles à s'améliorer dans les aspects testés par VALS.\n\nSi vous êtes intéressé, consultez les données VALS sur GitHub, et n'hésitez pas à nous contacter si vous avez des questions."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamizawa de l'Université de Tokyo. Je vais présenter un article intitulé RNSUN, un grand ensemble de données pour la génération automatique de notes de version via la synthèse de journaux de validation. Je vais m'exprimer dans l'ordre suivant. Premièrement, je vais introduire la génération automatique de notes de version sur laquelle porte notre recherche. Une note de version est un document technique qui résume les modifications distribuées avec chaque version d'un produit logiciel. L'image présente les notes de version pour la version 2.6.4 de la bibliothèque Vue.js. Les notes de version jouent un rôle important dans le développement open source, mais leur préparation manuelle est chronophage. Il serait donc très utile de pouvoir générer automatiquement des notes de version de haute qualité.\n\nJe vais me référer à deux recherches antérieures sur la génération automatique de notes de version. La première est un système appelé Arena, publié en 2014. Il adopte une approche basée sur des règles, par exemple, en utilisant un extracteur de changements pour extraire les différences fondamentales, les modifications de bibliothèque et les changements de documents à partir des différences entre les versions, et en les combinant finalement. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, le système de suivi de problèmes, et ne peut être appliqué qu'aux projets utilisant Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\n\nLe second, Glyph, a été annoncé récemment en 2020. Il est disponible sur Internet et peut être installé via PIP. Ce système possède un modèle de classification de texte simple basé sur l'apprentissage et produit l'une des cinq catégories, telles que les fonctionnalités ou les corrections de bogues, pour chaque message de validation en entrée. L'image est un exemple d'utilisation qui retourne une étiquette de correction ou de correction de bogue. Les données d'entraînement de Glyph sont plutôt petites, environ 5 000, et après révision dans les expériences décrites ci-dessous, la performance du modèle de classification de texte n'est pas élevée.\n\nJe présente deux recherches connexes, mais elles présentent des problèmes de faible applicabilité et de ressources de données limitées. Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité. Pour le problème de faible applicabilité, nous proposons une méthode de synthèse de classificateur de haute qualité utilisant uniquement les messages de validation comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais.\n\nPour le second problème de ressources de données limitées, nous avons constitué un ensemble de données R et sum composé d'environ 82 000 données en collectant des données à partir de dépôts GitHub publics en utilisant l'API GitHub. Notre ensemble de données Rnsum se compose d'environ 82 000 données collectées à partir de dépôts GitHub publics en utilisant l'API GitHub.\n\nJe vais maintenant décrire notre ensemble de données. Voici un exemple de données. À gauche se trouve le message de validation et à droite les notes de version. Les notes de version sont étiquetées comme des améliorations de visages, etc. Nous avons défini une tâche qui prend les messages de validation comme entrée et produit les notes de version étiquetées. Cela peut être considéré comme une tâche de synthèse. Nous avons prédéfini quatre étiquettes : fonctionnalités, améliorations, corrections de bogues, dépréciations, suppressions et changements rompus. Ces étiquettes ont été définies sur la base de recherches antérieures et d'autres facteurs.\n\nLes nœuds les moins importants en bas à droite sont extraits des nœuds les moins importants présentés en bas à gauche. À ce moment-là, il est nécessaire de détecter les quatre étiquettes prédéfinies. Cependant, les étiquettes ne sont pas toujours cohérentes avec chaque dépôt. Par exemple, l'étiquette améliorations inclut les améliorations, les améliorations, les optimisations, etc. Nous avons préparé une liste de vocabulaire de nos étiquettes d'étude pour chacune de ces variations de notation, l'avons utilisée pour détecter la classe de la note de version, et avons corrigé le texte de la liste qui suit en tant que phrase de note de version pour la classe. Il est nécessaire d'identifier la version précédente de la note de version 2.5 à 18 et d'obtenir sa différence.\n\nCela est un peu fastidieux et il ne suffit pas de simplement obtenir une liste de versions et de regarder les avant et après. Nous avons créé une règle de correspondance heuristique pour obtenir les versions précédente et suivante. Après l'analyse, 7 200 dépôts et 82 000 données ont été collectés. De plus, le nombre moyen de jetons de notes de version est de 63, ce qui est assez élevé pour une tâche de synthèse. Le nombre de jetons uniques est également très élevé, soit 8 830 000. Cela est dû au grand nombre de noms de classes et de méthodes uniques trouvés dans le dépôt.\n\nJe vais maintenant expliquer la méthode proposée. Le modèle de synthèse extractive puis abstractive par classe utilise un classificateur pour classer chaque message de validation dans l'une des cinq classes de notes de version. Nous choisissons les implémentations, les corrections de bogues, les dépréciations plus et autres. Les messages de validation classés comme autres sont ignorés. Ensuite, CEAS applique le générateur aux quatre documents étiquetés indépendamment et génère des notes de version pour chaque classe.\n\nDans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues. Par conséquent, pour entraîner le classificateur, nous attribuons des étiquettes pseudo à chaque message de validation en entrée en utilisant les 10 premiers caractères de chaque message de validation. Nous modélisons la synthèse abstractive par classe à travers notre approche en utilisant deux méthodes différentes. Le premier modèle, que nous appelons cssingle, se compose d'un seul réseau de ensemble à ensemble et génère un seul texte long de note, en donnant une concaténation des messages de validation en entrée.\n\nLe deuxième modèle, CAS, utilise un classificateur pour classer chaque message de validation dans l'une des cinq classes de notes de version. Ensuite, il applique un générateur à chaque classe indépendamment pour produire des notes de version. CAS obtient de meilleurs scores ROUGE-L que les méthodes de base, en particulier sur l'ensemble de test propre où l'écart de score dépasse les 20 points. Ces résultats démontrent l'efficacité significative de CAS.\n\nCAS a obtenu un meilleur score ROUGE-A que CAS, suggérant qu'une combinaison d'un classificateur et d'un générateur est bénéfique. La couverture élevée de CAS peut être attribuée au fait que le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe. CAS match a tendance à produire des scores ROUGE-L plus élevés que CAS single, indiquant qu'il est également efficace de développer indépendamment des modèles de synthèse abstractive différents pour chaque classe de notes de version.\n\nEn ce qui concerne l'analyse des erreurs, les méthodes CAS ont tendance à produire des phrases plus courtes que les phrases de référence humaines. Dans la figure de droite, la phrase de référence comporte trois ou quatre phrases, tandis que CAS n'en a qu'une. La réticence du modèle à produire des phrases plus longues s'explique par le fait que, dans les données d'entraînement, seulement 33 % des phrases sont présentes dans l'étiquette fonctionnalités et 40 % dans l'étiquette améliorations.\n\nDe plus, les méthodes CES ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple du haut à droite est un exemple de message de validation très confus. La phrase complète ne peut pas être générée sans référence à la demande ou au problème correspondant. L'exemple du bas montre que les deux messages de validation en entrée sont liés et devraient être combinés en une seule phrase, mais cela échoue.\n\nEn conclusion, nous avons constitué un nouvel ensemble de données pour la génération automatique de notes de version. Nous avons également formulé la tâche consistant à entrer des messages de validation et à les résumer de manière à ce qu'elle soit applicable à tous les projets rédigés en anglais. Notre expérience montre que la méthode proposée génère des notes de version moins bruyantes avec une meilleure couverture que les méthodes de base. Veuillez consulter notre onglet Descente seulement. Merci."}
