{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unser Papier über compositionselle Verallgemeinerung ohne Bäume mithilfe von Multisets-Tagging und latenten Permutationen geben. Dies ist eine Gemeinschaftsarbeit mit meinen Doktoranden Alexander Koller und Ivan Titov. Compositionselle Verallgemeinerung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursionen zu bewältigen und ungekannte Kombinationen von Phrasen zu handhaben, die während des Trainings einzeln vorgekommen sind. Im Kontext des semantischen Parsings könnte das Testen auf compositionselle Verallgemeinerung wie gewohnt aussehen: Wir haben einen Trainingsdatensatz mit Äußerungen, in diesem Fall \"das Mädchen schlief\" und \"Maria wusste, dass das Mädchen schlief\". Diese Äußerungen sind mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung repräsentieren. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursionen gesehen und wird nun mit einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art der out-of-distribution-Verallgemeinerung und erzeugen oft Ausgaben, die vom Input abgekoppelt sind. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie beispielsweise farblich im Beispiel hervorgehoben sind. Eine beliebte Methode, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den compositionsellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht direkt vorgegeben und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein, der typischerweise eine beträchtliche formalismus-spezifische Vorverarbeitung der logischen Formen erfordert, beispielsweise zur Behandlung von Variablen-Symbolen. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten. In diesem Papier verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir eine starke Verallgemeinerung auf tiefere Rekursionen, ohne auf Bäume angewiesen zu sein. Unser Ansatz sagt die Ausgabe aus dem Input in zwei Schritten voraus. Zunächst taggen wir jedes Input-Token mit einem ungeordneten Multiset von Tokens, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Aus diesem Grund verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation zu vorhersagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage einer Permutation ein, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell grob wie folgt: Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jede Position gehört. Für die erste Ausgabeposition wählen wir einfach eines aus, wie rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token in der Ausgabe zu bestimmen. Wir bestimmen das dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumfreien Modellen am COGS-Benchmark. Unser Modell übertrifft die anderen um einen großen Abstand bei der Verallgemeinerung auf tiefere Rekursionen. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr anspruchsvoll. In unserem Papier lösen wir einige interessante technische Herausforderungen. Erstens wird die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht angegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte latent ist. Wir beheben dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der Permutation mit der höchsten Punktzahl NP-schwer ist. Das liegt daran, dass dies mit dem Traveling-Salesman-Problem verwandt ist. Wir nähern dies mit einer GPU-freundlichen kontinuierlichen Relaxation an, die es uns auch ermöglicht, durch die Lösung zu backpropagieren und die linguistisch plausibleren Permutationen zu erlernen. Wenn Sie mehr über unsere Experimente und darüber erfahren möchten, wie wir diese Herausforderungen bewältigen, werfen Sie bitte einen Blick auf unser Papier oder besuchen Sie unser Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra und heute werde ich über unsere Arbeit zu „markierten Personas“ sprechen, wobei wir natürliche Sprachaufforderungen nutzen, um Stereotypen in Sprachmodellen zu messen. Diese Arbeit wird in Zusammenarbeit mit Essenndermush und Danjorovsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Diese Messmethoden weisen jedoch verschiedene Einschränkungen auf: Sie basieren in der Regel auf manuell erstellten Datensätzen, die sehr zeitaufwendig zu erstellen sind, oder sie erfassen lediglich sehr spezifische Stereotypen, sodass sie nicht gut auf andere demografische Gruppen oder Kontexte generalisieren. Oft erfassen sie auch nur sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten auf diesem Gebiet keine Intersektionalität, d. h. die Vorstellung, dass facettenreiche soziale Identitäten Vorurteile verstärken und einzigartige Schadensquellen darstellen können. Um diese Einschränkungen zu überwinden, stützen wir uns auf die Eigenschaft, dass diese neueren, instrukturneural trainierten LLMs sehr gut darin sind, Anweisungen in Aufforderungen zu befolgen. So können wir das Modell bitten, eine Persona zu generieren, d. h. eine Darstellung einer fiktiven Person, mithilfe einer Aufforderung wie „Stell dir vor, du bist eine asiatische Frau, beschreibe dich selbst“. Wir können sofort erkennen, dass dies sehr gut auf jede demografische Gruppe generalisiert werden kann, da wir einfach die gewünschte Identitätsmarkierung in diese Aufforderung einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Wir sehen sofort, dass die Ausgaben zwar nicht offen negativ oder toxisch im traditionellen Sinne sind, es aber einige interessante Muster gibt. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Begriffen wie exotisch, ähm, und mit Bezugnahme auf eine faszinierende Region bezeichnet, und beide Frauen mit Farbhaut-Hintergründen machen Bezüge zu ihrer Abstammung, während die Persona des weißen Mannes keine derartige Angaben macht. Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste Teil ist die Generierung dieser Personas. Unsere Aufforderungen zur Generierung dieser Personas wurden von einer Studie inspiriert, in der sie diese Aufforderungen an menschliche Probanden gaben und feststellten, dass sie dadurch auch rassische Stereotypen aufdecken konnten. Dies ermöglicht einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil ist „Markierte Wörter“, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unseren referenziellen Gruppen unterscheiden, auf die ich gleich näher eingehen werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Methode der markierten Wörter stützt sich auf das soziolinguistische Konzept der Markierung, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. So wird beispielsweise das Wort Mann – oder Entschuldigung, das Wort Krieger – üblicherweise mit Männern assoziiert. Wenn also Menschen einen Krieger beschreiben, der eine Frau ist, werden sie in der Regel explizit angeben, dass es sich um eine „Frau-Krieger“ handelt, und den Begriff mit „Frau“ markieren. Im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen als unmarkiert und welche als markiert gelten, und vergleichen dann die Personas mithilfe der Methode der „Kampfwörter“, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen „Kampfwörter“ verwenden und die Log-Odds-Verhältnisse sowohl mit den weißen Personas als auch mit den männlichen Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir das Lexikon der Stereotypen und stellen fest, dass die generierten Personas deutlich mehr Stereotypen enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter im Lexikon betrachten, stellen wir sehr unterschiedliche Dinge fest. Während die generierten Personas einen viel höheren Anteil an Wörtern aus dem Lexikon aufweisen, haben die von Menschen verfassten eine viel größere Wortvielfalt. Die Stereotyp-Wörter, die in den generierten Personas vorkommen, sind tatsächlich nur die Wörter „groß“ und „sportlich“, also wirklich nur die positiven oder zumindest nicht-negativen. Tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht. Stattdessen werden wir uns daher auf die Ergebnisse unserer Methode der markierten Wörter stützen, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essentialisierende Erzählungen erleichtern. In unserer Analyse zeigen wir auf, wie diese vermeintlich positiven Darstellungen schädliche Muster widerspiegeln. Zunächst umfassen die Top-Wörter für die markierten Gruppen Dinge wie Kultur, Tradition, Stolz und exotisch. Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und Ausgrenzung für diese Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Tropen wider, insbesondere für Frauen mit Farbhaut. So umfassen die Wörter, die eine lateinamerikanische Frau beschreiben, beispielsweise Begriffe wie lebendig und kurvig, die auf einen Tropikalismus verweisen. Für asiatische Frauen sind die Wörter beispielsweise zierlich, fein und seidig, die auf eine lange Geschichte der Hypersexualisierung asiatischer Frauen als sehr scheu und unterwürfig zurückweisen. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter „stark“ und „widerstandsfähig“ sind. Dies verweist auf das sogenannte „Strong Black Woman“-Archetyp. Obwohl es auf den ersten Blick positiv klingt, hat es sich gezeigt, dass dieser Archetyp sehr schädlich ist, da er diese demografischen Gruppen unter enormen Druck setzt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt daran zu arbeiten, diese Hindernisse zu beseitigen, setzt er diese Menschen unter Druck, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen und anderen Schäden führt. Im weiteren Sinne spiegeln die Wörter für jede markierte Gruppe im Wesentlichen nur essentialisierende Erzählungen wider. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbetreiber. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Erzählungen angehen. Wir sollten auch intersektionale Perspektiven verwenden, um Vorurteile und Schäden zu untersuchen, da es viele Dinge geben könnte, die übersehen würden, wenn wir dies nicht tun. Und schließlich sollte es wirklich eine erhöhte Transparenz bei Methoden zur Voreingenommenheitsminderung geben, da wir beispielsweise bei positiven Stereotypen nicht wissen, ob es daran liegt, dass es eine Art von seltsam übertriebenem Werteausgleich gibt oder ob es möglicherweise einige andere Anti-Stereotyp-Methoden gibt, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank für Ihre Aufmerksamkeit. Ich wünsche Ihnen einen schönen Aufenthalt auf der ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch.\n\nUnd heute werden wir Ihnen alles über ABCEV erzählen, einen neuen dimensionsbezogenen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde im Emory NLP-Labor durchgeführt, unter der Leitung von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI.\n\nNehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die übliche Vorgehensweise ist die Verwendung einer menschlichen Bewertung, beispielsweise durch die Bitte von menschlichen Gutachtern, auszuwählen, welche von zwei Konversationen besser ist, oder Konversationen anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der allgemeinen Dialogqualität zu liefern. Dialogqualität hat jedoch viele Aspekte, daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz ist es, menschliche Gutachter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz der Modellantworten, wobei bestehende Vergleichs- oder Likert-Skalenmethoden verwendet werden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. das Antworten mit irrelevanten Informationen oder das Widersprechen zu sich selbst. Wir bezeichnen diesen Ansatz als Annotation von Verhaltensweisen im Chat oder kurz ABCEval. Wir haben diese Methode entwickelt, um Chat-Modell-Verhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als wirksam für die Chat-Qualität befunden wurden. ABCEval kann die Raten messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Beispielsweise misst ABCEval die Anzahl der Züge, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Gesprächspartner widerspricht, falsche Fakten halluziniert oder gesundem Menschenverstand widerspricht, und wann das Modell Empathie zeigt oder nicht. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie mit ABCEval auf 100 menschlichen Bot-Konversationen pro Modell bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Zugstufe, Likert-Bewertungen auf der Dialogebene und Dialogebenen-Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen der acht am häufigsten gemessenen Aspekte des Dialogs erhoben, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unseren Analysen dieser Bewertungsergebnisse stellten wir fest, dass ABCEval-Verhaltensbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden erhoben wurden, gemessen am Inter-Annotator-Agreement bei hundert doppelt beschrifteten Konversationen. Darüber hinaus sind ABCEval-Bezeichnungen prädiktiver für die gesamte Konversationsqualität im Vergleich zu Metriken, die von bestehenden Methoden erzeugt werden, wie diese einfache lineare Regressionsanalyse zeigt. So können Sie beispielsweise sehen, wie die Messung des Anteils der Züge mit Selbst- und Partnerwidersprüchen  Prozent und zehn Prozent der Konversationsqualität erklären, während die durchschnittlichen Likert-Konsistenzwerte nur vier Prozent oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungskennzahl einen einzigartigen Aspekt der Chat-Qualität erfasst, wobei eine schrittweise lineare Regression verwendet wurde. Sie können sehen, wie die Kombination aller ABCEval-Metriken über 25 Prozent der Konversationsqualität erklären. Und wenn Sie die Metriken einzeln entfernen, verliert man dabei meistens eine beträchtliche Menge an Informationen über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf Zugstufe viel weniger der Qualität, und weniger dieser Metriken enthalten einzigartige Informationen. Diese zuverlässigen, informativen und eindeutigen ABCEval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden vermochten. Sie können dies an den Ergebnissen unseres Experiments sehen, die zeigen, dass noch mehrere Herausforderungen bestehen und präzise quantifiziert wurden. Beispielsweise weisen die von uns getesteten Bots in etwa 20 Prozent ihrer Antworten Verfehlungen des gesunden Menschenverstands auf, erzeugen in etwa 15 Prozent der Fälle irrelevante Informationen und widersprechen sich selbst oder ihrem Partner in etwa 10 Prozent der Fälle. Angesichts des rasanten Fortschritts in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Es gibt jedoch allen Grund, zuverlässige und präzise Bewertungskennzahlen für den Vergleich von Modellen zu entwickeln. Wir hoffen, dass ABCEval von anderen in diesem Bereich genutzt werden kann, als sinnvoller Schritt in diese Richtung, und wir freuen uns darauf zu sehen, wie sich konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vauddha und ich bin Doktorand der Informatik an der Stony Brook University. Ich möchte Ihnen unsere Arbeit vorstellen, die als Long Paper auf der ACL 2023 angenommen wurde: Transferlernen zur Erkennung von kognitiver Dissonanz, wobei die Herausforderung seltener Klassen adressiert wird. Wir beginnen mit der Definition kognitiver Dissonanz und warum es ein wichtiges Problem ist, in der Sprache zu untersuchen. Kognitive Dissonanz ist vereinfacht gesagt die Inkonsistenz zwischen zwei Überzeugungen oder Handlungen, wie beispielsweise in diesem Beispiel, wo eine Person sagt: „Ich weiß, dass Zigaretten mich töten können“ und dann hinzufügt: „Ich habe nach dem Meeting ein paar Zigaretten geraucht“. Diese Überzeugung und Handlung sind inkonsistent und stehen in Dissonanz. Die weitere Erwähnung, dass sie ohne sie ihre Arbeit nicht halten könne, rechtfertigt das zweite Auftreten und schafft ein Konsonanzverhältnis. Obwohl Dissonanz ein sehr häufiges Phänomen ist, das wir bei täglichen Entscheidungen erleben, findet es sich nur selten in der Sprache, im Gegensatz zu anderen Diskursrelationen, wieder. Warum ist das wichtig? Das Studium kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen, Trendänderungen und Wertvorstellungen sowie Verhaltensänderungen in der Bevölkerung zu verstehen. Kognitive Dissonanz steht außerdem in Verbindung mit Angststörungen und kann uns helfen, die psychische Gesundheit von Menschen besser zu verstehen. Das Studium der in der Sprache ausgedrückten Dissonanz kann auch für das Verständnis von Extremismus und der Polarisierung gefährdeter Gruppen von Vorteil sein. Schließlich ist es wichtig, kognitive Stile von Individuen zu verstehen und Entscheidungsprozesse besser zu analysieren. Im Ziel, eine Ressource zur kognitiven Dissonanz zu erstellen, führten wir eine groß angelegte Annotation von Dissonanzrelationen durch. Wir verwendeten einen Dissonanz-zuerst-Ansatz, wie ihn das folgende Flussdiagramm zeigt. Tweets wurden mithilfe eines PDTV-Parsers verarbeitet und Paare von Diskurs-Einheiten gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie man sehen kann, wurde Dissonanz nur in 3,5 Prozent der annotierten Paare gefunden. Nachdem wir etwa tausend Beispiele von Diskurs-Einheiten-Paaren gesammelt hatten, führten wir ein Training für einen anfänglichen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Nicht überraschenderweise schnitt der Klassifikator angesichts der geringen Häufigkeit von Dissonanz und des Fehlens eines vorherigen Datensatzes nicht wesentlich besser als zufällig ab. Wir stehen vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transferlernen und aktivem Lernen, um solche Daten zu annotieren, sodass mehr Dissonanzbeispiele mit weniger Annotationsrunden gesammelt werden können, wodurch die Gesamtkosten der Annotation gesenkt und die Dissonanzerkennung verbessert werden. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir den Active-Learning-Prozess mit der Übertragung von Gewichten von eng verwandten Aufgaben. Wir übertragen von zwei verschiedenen Aufgaben: unabhängiger Klassifizierung von Dissonanz, einer Aufgabe, die feststellt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema (als Debatte bezeichnet), und der binären Klassifizierung von Expansion und Vergleichsklassen von PDTV. Da diese beiden eng mit dem Begriff der Konsonanz und Dissonanz verbunden sind, bezeichnen wir sie als ceE. Wir stellen fest, dass die Übertragung bereits eine deutlich bessere Leistung als der Zufall auf dem annotierten Datensatz erzielt, mit der besten Leistung einen Auc-Wert von 0,62. Durch iteratives Feintuning auf beiden Aufgaben stellen wir fest, dass das Feintuning der ceE-Aufgaben, gefolgt von weiterem Feintuning auf Debatten, eine deutlich bessere Leistung bei Zero-Shot erzielt. Dies ist das Modell, das wir zur Initialisierung des aktiven Lernens verwenden. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des aktiven Lernens zu aktualisieren. Kumulativ akkumuliert alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während iterative Aktualisierungen das Modell anhand des neuesten Datensatzes trainieren, der aus den verschiedenen Strategien gesammelt wurde. Wir stellen fest, dass kumulativ bei allen Strategien gleich gut oder besser abschneidet. Um die Anzahl der Dissonanzbeispiele zu erhöhen, verwenden wir eine Probability of Rare Class-Strategie (PRC), um hauptsächlich die Beispiele auszuwählen, die vom aktuellen Modell mit großer Wahrscheinlichkeit dissonant sind. In jeder Runde des aktiven Lernens vergleichen wir dies mit anderen State-of-the-Art-AL-Strategien, die in der Community üblicherweise verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Es ist zu beachten, dass die Leistung bei weiteren Runden des AL mit den beiden besten Strategien bei der Dissonanzklassifizierung einen Auc-Wert von 0,75 erreicht, was die beste Leistung ist, die wir bisher in dieser Aufgabe erzielt haben. Wir überprüfen auch die Durchführbarkeit jeder Strategie hinsichtlich der Annotationsqualität und der Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für seltene Klassen geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Akquisition seltener Klassen ist und Cold-Starting-AL mit sorgfältig gestalteten Transferlernaufgaben erheblich helfen kann. Wir stellen auch fest, dass iterative Aktualisierungen für das Transferlernen aus einer anderen Domäne nützlich sind, während in-domain-aktive Annotationen von kumulativen Aktualisierungen profitieren. Dies sind die Links zu unserem Code, unserem Datensatz und unserem Paper. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Ich bin Akshata und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit mit dem Titel „Kit Must: Evaluierung der Wissensintegration aus Mehrfachquellen“. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. National Language Understanding Modelle (NLU) stützen sich auf eine Vielzahl von Wissensquellen, wie z. B. Wissen, das in ihren Parametern enthalten ist und üblicherweise durch Vortraining erworben wird, und Wissen, das zur Inferenzzeit in den Eingaben bereitgestellt wird. Jüngste Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen. Natürliches Sprachverständnis erfordert jedoch oft Wissen, das ebenfalls zur Inferenzzeit bereitgestellt wird, beispielsweise im Satz „John sah den frisch gewählten Präsidenten im Fernsehen“. Vortrainierte Parameter können zwar Informationen darüber enthalten, was Präsidenten tun und was ein Fernsehen ist, aber sie können zuverlässig nicht wissen, wer diese instanzspezifische Entität John ist oder wer der neue Präsident ist, da sich der Präsident seit dem Vortraining geändert haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch zur Inferenzzeit bereitgestelltes Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Kern-Referenz-Auflösungsaufgabe ein, die darauf abzielt, die Fähigkeit zu testen, Wissen aus verschiedenen Quellen zu nutzen. Wir evaluieren den Datensatz mit menschlichen Studienanpartizipanten und etablieren Kern-Referenz-Auflösung Modelle. Hier ist ein Beispiel aus unserem Datensatz: Servile ist ein Richter, Kia ist ein Bäcker, Termin und Kia metadaten parken nach einem langen Tag bei der Arbeit, in dem er Fälle in einem Gesetzbuch entscheidete, war er froh, sich zu entspannen. Die Aufgabe hier ist es, die korrekte Entität zu identifizieren, auf die das Pronomen „er“ sich bezieht, was in diesem Fall. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: Erstens, entitätsspezifisches Wissen, wie z. B. Servile ist ein Richter, und zweitens, allgemeines Wissen, wie z. B. Richter entscheiden in Gerichten grundsätzlich. Allgemeines Wissen wird typischerweise während des Vortrainings großer Sprachmodelle erlernt, während entitätsspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationsarten, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von Kitdmos definiert. Erstens haben wir die typische Einstellung Back-Pre-Train, bei der angenommen wird, dass allgemeines Wissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es die Background-Both-Einstellung, bei der allgemeines Wissen sowohl zur Vortrainingszeit als auch zur Inferenzzeit verfügbar ist. Schließlich die Background-Inference-Einstellung, bei der beide Wissensarten nur zur Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das zur Lösung einer Aufgabe notwendige allgemeine Wissen nicht Teil der Vortrainingsdaten von Modellen ist, beispielsweise weil sich seit der Vortrainingszeit neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen steuern. In der Background-Pre-Train-Einstellung gehen wir davon aus, dass das allgemeine Wissen, Politiker streben nach gewählten Sitzen in der Regierung, in den vortrainierten Parametern enthalten ist. Im Kontext der Interferenzzeit stellen wir das instanzspezifische Wissen, Chichester ist ein Politiker, bereit. In der Background-Both-Einstellung stellen wir zusätzlich zum instanzspezifischen Wissen auch allgemeines Wissen über Politiker im Kontext der Interferenzzeit bereit. In der Background-Inference-Einstellung stellen wir die Berufungsbezeichnung merely tour anstelle von Politiker bereit, da merelytour wahrscheinlich nicht in den vortrainierten Parametern enthalten ist. Wir evaluieren den Datensatz sowohl mit menschlichen Studienanpartizipanten als auch mit etablierten Präferenz-Auflösung Modellen. In dieser Abbildung zeigen wir die Ergebnisse der am besten abschneidenden Modelle auf der schwierigsten Variante der Background-Pre-Train-Einstellung ohne aufgabenspezifisches Training auf Kitdmos. Beide Modelle schneiden ohne aufgabenspezifisches Training auf Kitdmos nicht gut ab. Allerdings schneiden sowohl C2F als auch Built for Coref signifikant besser ab als die zufällige Wahl. Dies deutet darauf hin, dass Modelle beim Training auf generischen Referenz-Auflösung Datensätzen lernen, Oberflächenhinweise auszunutzen, die beim Testen auf Kitdmus, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen deuten darauf hin, dass selbst die am besten abschneidenden Modelle nicht zuverlässig allgemeines Wissen nur zur Inferenzzeit integrieren können. Um die wichtigsten Erkenntnisse unseres Papiers zusammenzufassen: Viele Co-Referenz-Auflösungsmodelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen ohne aufgabenspezifisches Training zu resonieren. Mit aufgabenspezifischem Training integrieren jedoch einige Modelle Wissen aus verschiedenen Quellen. Selbst die am besten abschneidenden Modelle haben jedoch Schwierigkeiten, allgemeines Wissen, das nur zur Inferenzzeit präsentiert wird, zuverlässig zu integrieren. Wenn Sie an weiteren Details interessiert sind, werfen Sie bitte einen Blick in unser Papier und schauen Sie sich den Datensatz und den Code auf GitHub an. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Sarah Pai von der Universität Trient und dem Forschungsinstitut Bruno Kessler, und ich werde kurz das Paper \"Attention as a Guide for Simultaneous Speech Translation\" vorstellen, eine Gemeinschaftsarbeit mit Matteo Negri und Marco Durchi. Was ist simultane Sprechübersetzung? Simultane Sprechübersetzung oder SimST ist der Prozess, gesprochene Sprache in Echtzeit in Text einer anderen Sprache zu übersetzen, um sprachübergreifende Kommunikation zu ermöglichen. Was sind die Probleme aktueller SimST-Modelle? Spezifische Architekturen werden in der Regel trainiert, wobei zusätzliche Module eingeführt werden, die optimiert werden müssen, was zu langen und komplizierten Trainingsprozeduren führt. Beispielsweise Training mit unterschiedlichen Optimierungszielen und das Trainieren und Warten mehrerer Modelle, um unterschiedliche Latenzregime zu erreichen. Beispielsweise das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines weiteren mit zwei Sekunden und so weiter. Was ist also unsere Lösung? Erstens, bereits existierende Offline-SD-Modelle zu verwenden, ohne sie neu zu trainieren oder spezifische Architekturen für die Verwendung in SimST anzupassen. Nur ein Modell pro Latenzregime zu verwenden und die Latenz über spezifische Parameter zu steuern. Zweitens, das bereits vorhandene Wissen eines Modells über den Spannungsmechanismus zwischen Audioeingabe und Texteausgabe zu nutzen – den Cross-Attention-Mechanismus. Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung besteht darin, eine Dot-Product-Encoder-Decoder-Attention vorzuschlagen, eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben oder nicht, basierend darauf, wohin die Attention zeigt. Ein Wort wird ausgegeben, wenn die Attention nicht konzentriert ist, d.h. diese Summe unterhalb eines bestimmten Schwellenwerts alpha in Bezug auf die letzten Lambda Sprachrahmen liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist. Beispielsweise, wenn wir einen Sprachabschnitt erhalten, der mit „Ich werde über… sprechen beginnt“ und unser Modell die Übersetzung ins Deutsche vorhersagt, schauen wir uns die Cross-Attention-Gewichte an und sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen zeigen, während das letzte Wort auf die letzten empfangenen Sprachrahmen, die Lambda Sprachrahmen sind, zeigt. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, während das letzte Wort nicht ausgegeben wird, da die Summe der Cross-Attention über einem bestimmten Schwellenwert alpha liegt, und wir auf einen weiteren Sprachabschnitt warten. Wenn wir fortfahren und einen weiteren Sprachabschnitt erhalten, der drei weitere Wörter vorhersagt, schauen wir uns die Cross-Attention-Gewichte an und sehen, dass kein Wort auf die letzten Lambda Sprachrahmen zeigt. Dies bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von Dot anschauen, stellen wir die Ergebnisse der simultanen Sprechübersetzung in Diagrammen dar, bei denen eine Seite blau ist und Übersetzungsqualität und durchschnittlicher Rückstand – das Latenzmaß – misst. Wir berücksichtigen auch den rechnerisch bedingten durchschnittlichen Rückstand, der die Rechenzeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen, dass unsere Kurven auf diesem Diagramm so hoch wie möglich sind, aber auch so weit wie möglich nach links verschoben sind. Wir vergleichen sie mit Plepara-Strategien, die ebenfalls auf Offline-Modelle angewendet werden, wie die Whitkey-Strategie und die Local Agreement Strategie. Wir vergleichen sie auch mit dem aktuellen Stand der Technik, Architekturen, die speziell für die simultane Sprachübersetzung entwickelt wurden. Dies sind alle Ergebnisse der simultanen Sprechübersetzungsstrategie auf Deutsch, und wir sehen, dass Dot alle Strategien, die auf Offline-Modelle angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Wir sehen auch, dass Dot, wenn wir die tatsächliche verstrichene Zeit oder die rechnerisch bedingte Zeit betrachten, die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Paper. Wir haben auch den Code und die Modelle sowie die simultanen Ausgaben Open Source freigegeben, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag zusammen. Mein Name ist Shu Ha. Heute werde ich unseren Beitrag „Do Connel 2003 Named Entity Tagging noch 2023 gut funktionieren?“ vorstellen. Lassen Sie uns beginnen. Unsere Arbeit untersuchte das Problem der Generalisierung anhand der Aufgabe der Erkennung benannter Entitäten, oder kurz NER. Wir stellen fest, dass Modelle seit fast 20 Jahren Con 2003 zur Entwicklung von NER verwenden, was natürlich mehrere Probleme aufwirft. Erstens: Können diese Modelle auf moderne Daten generalisieren? Und was ist für eine gute Generalisierung erforderlich, wenn wir neue Tagger entwickeln? Gleichzeitig: Wenn wir eine schlechte Generalisierung beobachten, was verursacht dann den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Con plus+ Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News ab 2020 erhoben und mit den gleichen Con 2003 Annotationsrichtlinien versehen haben. Anschließend haben wir über 20 Modelle mit Con 2003 feinabgestimmt und sie sowohl auf dem Con 3 Testdatensatz als auch auf dem Con plus plus Testdatensatz evaluiert. Und zu guter Letzt berechneten wir die prozentuale Veränderung des F1-Werts, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Durch unsere Experimente haben wir festgestellt, dass drei Hauptfaktoren entscheidend sind. Der erste ist die Modellarchitektur. Unsere Experimente zeigten, dass Transformer-Modelle in der Regel besser auf neue Daten generalisieren. Der zweite Faktor ist die Modellgröße. Wir haben festgestellt, dass größere Modelle üblicherweise zu einer besseren Generalisierung führen. Und zu guter Letzt wissen wir alle, dass die Anzahl der Feinabstimmungsexemplare die Leistung einer nachgelagerten Aufgabe direkt beeinflusst. Auch hier haben wir festgestellt, dass mehr Feinabstimmungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Bezüglich unserer nächsten Frage, was den Leistungsabfall einiger Modelle verursacht: Wir hatten zwei Hypothesen. Die erste ist adaptives Overfitting, also ein Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes entsteht, was sich typischerweise als abnehmende Grenzerträge auf einem neuen Testdatensatz äußert. Die zweite Hypothese ist der zeitliche Drift, d. h. die Leistungsminderung, die durch die zunehmende zeitliche Kluft zwischen den Trainings- und Testdaten verursacht wird. Bezüglich adaptiven Overfittings konnten wir aus dem Diagramm rechts sehen, dass die rote Best-Fit-Linie eine Steigung aufweist, die größer als eins ist. Das bedeutet, dass jede Einheit an Verbesserung, die wir auf Con 2003 erzielen, zu mehr als einer Einheit an Verbesserung auf Con plus plus führt, was bedeutet, dass es keine abnehmenden Grenzerträge gibt. Dies zeigt uns, dass adaptives Overfitting in diesem Fall nicht beobachtet wird. Was ist aber mit dem zeitlichen Drift? Um den zeitlichen Drift zu untersuchen, führten wir ein Experiment durch, bei dem wir einige Modelle mit aktuelleren Daten erneut trainierten oder weiter vortrainierten und feststellten, dass die Leistung mit einer größeren zeitlichen Kluft abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsexemplare benötigen. Diese Aspekte stehen im Zusammenhang miteinander. Wir können nicht nur einen Faktor verbessern, sondern müssen auch die anderen berücksichtigen. Wir haben auch festgestellt, dass der Leistungsabfall durch zeitlichen Drift verursacht wird und, überraschenderweise, nicht durch adaptives Overfitting, obwohl Con 2003 seit über 20 Jahren verwendet wird. Bezogen auf die Frage, die wir in der Überschrift unseres Beitrags aufgeworfen haben: Funktionieren Connel 2003 Tagger noch 2023? Haben wir festgestellt, dass die Antwort ein klares Ja ist. Wir hoffen, dass unser Beitrag mehr Forschung dazu anregt, wie man die Generalisierbarkeit von Modellen verbessern kann. Und abschließend möchten wir Sie bitten, unseren Beitrag und unseren Datensatz anzusehen. Wenn Sie Fragen haben, können Sie mich gerne kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo! Herzlich willkommen zu unserer Präsentation von De plain, einem neuen Korpus für die deutsche Textidentifikation sowohl auf Dokumenten- als auch auf Satzebene. Mein Name ist Regina Stoden und ich werde Sie durch den ersten Teil der Präsentation führen. Definieren wir zunächst Textvereinfachung. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um dessen Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, beispielsweise für Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, beispielsweise von Dokumenten oder Sätzen. Im folgenden Beispiel können Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in vereinfachter Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, z. B. lexikalische Substitution, Klauselerweiterung, Kreuz-Ellision, Umstellung oder Einfügung von Wörtern. Wir schlagen nun unseren neuen Korpus D plane vor, da es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Diese Korpora sind beispielsweise zu klein, um ein Taxonomiemodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher schlagen wir unseren neuen Korpus Dplane vor, der in zwei Subkorpora aufgeteilt ist: Deplane APA und Deplane web. Deplane APA basiert auf Nutzungstexten. In Deplane APA haben wir 483 Dokumente alle manuell ausgerichtet. Das ergibt etwa dreißigtausend dreizehntausend parallele Satzpaare. Für deepplae web umfasst dieser Korpus verschiedene Bereiche, und wir richten auch alle siebenhundertfünfzig Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden aus. Insgesamt erhalten wir 30 450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, z. B. hinsichtlich der Art der Vereinfachung, wie Sie hier sehen können. Bibeltexte sind viel stärker vereinfacht als beispielsweise Nachrichtentexte oder sprachlernende Texte auf allen Ebenen, beispielsweise hinsichtlich lexikalischer Vereinfachung, strukturierter Vereinfachung und insgesamt des Vereinfachungsgrades. Darüber hinaus können Sie sehen, dass unser Deep-Plaining-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir beispielsweise im Deplane API-Korpus viel mehr Umstellungen und Wortzusätze als im Deep Plane Web-Korpus. Im Web-Korpus haben wir hingegen viel mehr Umschreibungen. Sehen wir uns nun an, was wir mit diesem Korpus anfangen können. Hallo, ich bin Omar, und ich werde nun über die Anwendungsfälle für unser Datenset d plane sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden evaluieren. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in Post-Dokumenten extrahieren möchten, versuchen wir in unserem Anwendungsfall, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache und denselben Inhalt haben, sich aber auf unterschiedlichen Komplexitätsstufen befinden. Und da wir nun unser Datenset d plane haben, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu evaluieren. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen und den Code zur Durchführung unserer Experimente im Artikel veröffentlicht. Wir sind zu dem Schluss gekommen, dass die beste automatische Ausrichtungsmethode für deutsche Textvereinfachung die Methode von mass align ist. Sie finden den Code auch, um diese Methode auf Ihren eigenen Dokumenten auszuführen, im Artikel. Der zweite Anwendungsfall, den wir in unserem Artikel vorgestellt haben, ist der der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell von long part feinabgestimmt, um dokumentenbezogene Vereinfachungen zu erzeugen, und wir haben auch das normale base long, das normale base part feinabgestimmt, um satzbezogene Vereinfachungen zu erzeugen. Sie finden alle Checkpoints und können sich im Artikel die Details zu den Scores und den Auswertungskennzahlen unserer Experimente ansehen. Wir sind zu dem Schluss gekommen, dass diese grundlegende Feinabstimmung bessere Scores erzielen oder bessere als die Baseline-Scores erreichen kann, und wir haben diese Ergebnisse als Benchmark, als Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir freuen uns darauf, Sie alle während der Konferenz zu treffen. Danke."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist X Yuan von der FNAi Universität. Ich möchte Ihnen unsere Arbeit vorstellen, „Unterscheidung von Skriptwissen und Sprachmodellen für eingeschränkte Sprachplanung“. Im Alltag müssen Menschen oft ihre Handlungen planen, indem sie schrittweisen Anweisungen in Form von garantierten Skripten folgen. Frühere Arbeiten nutzten Sprachmodelle, um für abstrakte Ziele stereotypischer Aktivitäten zu planen, wie z. B. einen Kuchen backen, und zeigten, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Die bisherige Forschung konzentrierte sich jedoch hauptsächlich auf die Planung für die abstrakten Ziele stereotypischer Aktivitäten; die Planung für Ziele mit spezifischen Einschränkungen, wie z. B. das Backen eines Schokoladenkuchens, bleibt unterrepräsentiert. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das unterschiedliche Einschränkungen für die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen, realen, spezifischen Zielen mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte verfassen, die vernünftig sind und den Einschränkungen entsprechen. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit von Sprachmodellen zur eingeschränkten Sprachplanung. Da keine Datensätze für spezifische Ziele vorliegen, um unsere Studie zu unterstützen, müssen wir diese Ziele zunächst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielfältigen Einschränkungen für die menschliche Datengewinnung im Loop. Wir verwenden instruct GPT, um 100 spezifische Ziele zu entnehmen und die Skripte zu bewerten, die von Bibliotheksmodellen generiert werden. Diese Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Lernmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, was die Lernmodelle fehlerhaft macht. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, die Einhaltung der Einschränkungen jedoch nicht garantiert werden kann. Wir untersuchen detailliertere Themenkategorien von Einschränkungen, die in WiH definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von instruct GPTs für Ziele unterschiedlicher Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Lernmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee der übergenerierten Filterung, um die Erzeugungsqualität zu verbessern. Zunächst zeigen wir Einschränkungstypen mit Beispielen für instruct CPT und erhalten auf der Grundlage der Start- abstrakten Ziele spezifische Ziele. Anschließend generiert instruct GPT übermäßig Schlüssel-Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in instruct GPT-Einbettungen und berechnen Cosinus-Ähnlichkeiten und Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus zeichnen wir das Skript aus, das die Schlüsselwörter der Ziel-Einschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel am höchsten bewertet wird. Mit unserer Methode kann instructGbt Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planbarkeit sowohl in Bezug auf Semantik, Vollständigkeit als auch auf die Einhaltung der Einschränkung erheblich. Da große Sprachmodelle teuer zu implementieren sind, ist es unerlässlich, die Sprachplanungsfähigkeit kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt zu diesem Zweck. Bisherige Studien haben jedoch keine Planung für spezifische Ziele ermöglicht und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um einen Datensatz für die eingeschränkte Sprachplanung aus Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für die eingeschränkte Sprachplanung namens CodeScript zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testseiten zu gewährleisten, bitten wir Crowd-Sourced-Mitarbeiter, die falschen Stichproben Korrekturen vorzunehmen. Diese Abbildung zeigt die Einschränkungsverteilung von CodeScript. Wir stellen fest, dass CodeScript eine hohe Vielfalt in den generierten spezifischen Zielen aufweist. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung einsetzen. Wir stellen fest, dass t5, das auf die Scoringsrate abgestimmt ist, Skripte von höherer Qualität generiert als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle große Modelle unterstützen können, wenn sie ordnungsgemäß auf geeigneten Datensätzen trainiert werden. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert, die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle bewertet und eine Übergenerierungsfiltermethode für Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen qualitativ hochwertigen Datensatz CodeScript für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der Code-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung voranzutreiben. Vielen Dank für Ihre Zeit. Detailliertere Informationen zu CodeScript finden Sie in unserem Paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Jannislavak und ich werde Ihnen unsere Arbeit zu Dr. Bert vorstellen, einem robusten, vortrainierten Modell für Französisch im biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend präsentieren wir den Hauptbeitrag unseres Artikels. Wir stellen das erste biomedizinische Modell in Französisch namens Dr. Bert vor, das auf Roberta basiert und mit nachtchos trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web gecrawlt wurden. Wir stellen außerdem einen Vergleich des Modells mit verschiedenen kryonischen Einstellungen und Datenquellen vor. Danach präsentieren wir unsere Ergebnisse bei 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch und schließen schließlich über die Experimente ab und geben Ihnen weitere Details darüber, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist Bert zu einem der effektivsten Ansätze geworden, um Aufgaben der natürlichen Sprachverarbeitung zu lösen und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec, FastText und anderen eine enorme Leistungssteigerung. Seitdem wurde dieses Modell in viele andere Sprachen adaptiert, wie in Französisch mit CamemBERT und in andere Bereiche wie Biomedizin mit PubMedBERT und BioBERT und im klinischen Bereich mit ClinicalBERT, wobei spezialisierte Modelle für andere Sprachen jedoch meist rar sind und oft auf kontinuierlichem Vortraining basieren aufgrund des Mangels an domänenspezifischen Daten. Französisch hatte jedoch bis jetzt kein Open-Source-Modell für den biomedizinischen Bereich. Wir fragten uns also, welche die am besten geeigneten Datenquellen für eine breite Palette von Anwendungen sind und ob rohe Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die von einem nicht-universitären Krankenhaus in unserer Einrichtung gewonnen wurden. Anschließend fragen wir uns, wie viele Daten wir benötigen, um ein spezialisiertes Modell auf Französisch zu trainieren: Vier Gigabyte, acht Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier Modelle von Grund auf: eine erste Version von Dr. Bert mit sieben Gigabyte von Nachos, eine zweite Version mit vier Gigabyte von Set of Natureos, eine erste Version von Schubert, einem klinischen Modell mit vier Gigabyte Sätzen aus klinischen Notizen, und eine abschließende Version von Schubert mit einer Mischung aus vier Gigabyte von Set of Natureos und vier Gigabyte klinischen Notizen. Ergänzend zu diesem Vergleich haben wir drei Modelle auf Basis von kontrolliertem Vortraining eingeführt, um die Auswirkungen der Vortrainingsstrategie zu analysieren: eines basiert auf den Gewichten von CamemBERT und wurde mit vier Gigabyte von Set of Natureos trainiert, ein weiteres ebenfalls auf CamemBERT basiert, wurde aber dieses Mal mit vier Gigabyte klinischer Notizen trainiert, und schließlich eines auf einem englischen biomedizinischen Modell, bermedBERT, das auf vier Gigabyte von Set of Snatchtrainiert wurde. Insgesamt haben wir sieben Modelle zur Evaluierung. Unsere sieben Modelle werden mit öffentlichen und privaten Downstream-Aufgaben wie Named-Entity-Recognition, Klassifikation, Part-of-Speech-Tagging und Question Answering verglichen. Diese Modelle werden mit sechs Basislinienmodellen verglichen: CamemBERT, Oscar (138 Gigabyte), CamemBERT (4 Gigabyte), CamemBERT CCNet (4 Gigabyte), PubMedBERT, BioBERT und ClinicalBERT. Die Entwicklung zeigt, dass die Modelle am besten bei Aufgaben funktionieren, deren Daten der gleichen Art sind wie die, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir beobachten auch, dass die Verwendung von mehr Daten zu einer besseren Leistung führt. Insgesamt scheinen Modelle, die von Grund auf neu trainiert wurden, bei den meisten Aufgaben eine höhere Leistung zu erzielen. Unsere Experimente mit kontrolliertem Vortraining unter Verwendung der Gewichte und des Tokenizers von PubMedBERT, das auf einem vier Gigabyte großen Subset von Nachos trainiert wurde, zeigten jedoch vergleichbare Ergebnisse wie die von Dr. Bert mit vier Gigabyte von Grund auf neu trainiert, was nicht für Modelle gilt, die auf CamemBERT-Gewichten und -Tokenizern basieren, die unter Stabilitätsproblemen leiden. Abschließend bietet unser proprietäres System eine bessere Leistung bei neun von 11 Downstream-Aufgaben und übertrifft global das Ergebnis des generischen Modells CamemBERT. Wir beobachten auch, dass spezialisierte Daten besser sind; mehr spezialisierte Daten sind besser, skalieren aber nicht gut. Alle vortrainierten Modelle von Nachos sind auf einer You-Interface verfügbar, und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf die Diskussionen auf der Poster-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Xhang Bing, ich bin Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit von der Vorabtrainierung von Daten zu Sprachmodellen bis hin zu nachgelagerten Aufgaben, die die Spur politischer Voreingenommenheit verfolgt, die zu unfairen NLP-Modellen führt. Sprachmodelle werden also auf großen Web-Crawling-Datensätzen trainiert. Politische Nachrichtenmedien sind in ihren Vorabtrainingsdaten gut abgedeckt. Laut einer Umfrage des c4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, der Guardian, die Huffington Post usw. gut in den Trainingsdaten für Sprachmodelle abgedeckt sind. Dies hat ein zweischneidiges Schwert für Sprachmodellanwendungen geschaffen. Einerseits waren sie in der Lage, aus unterschiedlichen Perspektiven zu lernen, was die Demokratie und eine Vielfalt an Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können zu potenziellen Fairnessproblemen bei nachgelagerten Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von der Vorabtrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben zu untersuchen, insbesondere indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen die Auswahl der Trainingsdaten in Bezug auf solche politischen Voreingenommenheiten? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlicher politischer Ausrichtung tatsächlich bei nachgelagerten Aufgaben und kann dies zu Fairnessproblemen in NLP-Anwendungen führen? Konkret schlagen wir zunächst vor, Sprachmodelle mit unterschiedlichen Prompt-Formaten unter Verwendung politischer Fragebögen wie dem Political Compass Test zu versehen. Dies ermöglicht uns eine automatische Bewertung, die gut in der Literatur der politischen Wissenschaft verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben, sie besetzen alle vier Quadranten des Political Compass. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und GPT-Serien im Allgemeinen sozial liberaler sind als Bird-Serien und deren Varianten. Zweitens wollen wir untersuchen, in welchem Umfang die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Daher könnten wir ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints zusätzlich auf sechs verschiedene parteiische Korpora vorabtrainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter nach ihrer politischen Ausrichtung unterteilt sind. Indem wir Sprachmodelle auf solche parteiische Korpora vorabtrainieren, können wir feststellen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Beispielsweise verschiebt sich Roberta, das weiter auf dem linken Reddit-Korpus feinabgestimmt wurde, in Bezug auf seine politischen Voreigungen erheblich nach links. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung erfassen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Vorabtrainingskorpora in die Zeit vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten auf und vorabtrainieren die Sprachmodelle separat auf die beiden verschiedenen zeitlichen Korpora. Wir können feststellen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft erfassen können. Zuletzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei der Erkennung von Hassreden und der Erkennung von Falschmeldungen, zwei NLP-Anwendungen, die häufig Sprachmodelle beinhalten und sehr weitreichende Auswirkungen haben könnten. Wir sehen, dass wenn wir die Leistung pro Kategorie untersuchen, d. h. wenn wir die Leistung in verschiedene demografische Gruppen oder politische Medien, Nachrichtenmedien, unterteilen, wir ein Muster feststellen, dass beispielsweise bei der Erkennung von Hassreden linksläufige Sprachmodelle besser darin sind, Hassreden zu erkennen, die sich gegen sozial marginalisierte Gruppen richten, aber schlechter darin sind, Hassreden zu erkennen, die sich gegen mächtigere Gruppen in unserer Gesellschaft richten. Umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassreden zu erkennen, die sich gegen Weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen Schwarze, LGBTQ+-Personen und andere Minderheitengruppen richten. Ähnliche Trends treten auch bei der Erkennung von Falschmeldungen auf, wo wir sehen, dass linksläufige Sprachmodelle besser darin sind, Fehlinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen, und umgekehrt. Wir zeigen weiterhin viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen zu Hassreden und Falschmeldungen treffen, basierend auf ihren sozialen Kategorien. Es gibt eine Reihe weiterer Beispiele im Anhang, um hervorzuheben, dass dies ein dringendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigt. Wenn beispielsweise ein rechtsgerichtetes Sprachmodell auf Hassreden oder Falschmeldungen oder was auch immer feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würde, würde dies bedeuten, dass Personen mit gegensätzlichen politischen Meinungen marginalisiert werden und Hassreden, die sich gegen Minderheitengruppen richten, unkontrolliert grassieren könnten. Dies schlägt Alarm an, um die durch die politischen Ausrichtungen von Sprachmodellen verursachten Fairnessprobleme anzuerkennen und anzugehen. Etwas Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen: Es ist wie zwischen Schizylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten für Sprachmodelle nicht reinigen, würde die Voreingenommenheit von den Vorabtrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben fortbestehen und letztendlich zu Fairnessproblemen führen. Wenn wir jedoch versuchen, sie irgendwie zu reinigen, riskieren wir auch Zensur oder Ausgrenzung, und es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Sprachmodell-Daten erhalten bleiben sollte. Es ist also wie das Trolley-Problem. Gut, ich denke, das ist alles, was ich heute habe. Vielen Dank für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Koov Sinna und ich freue mich, Sie zu unserem Vortrag über unser ACL 23 Paper begrüßen zu dürfen. Die Beurteilungen der Akzeptabilität von Sprachmodellen sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Waqui, Aaron Mueller, Kanishka Mishra, Karen Fs, Roger Levy und Atina Williams. In dieser Arbeit betrachten wir das Minimal Pair Paradigma erneut. Das Minimal Pair Paradigma bewertet im Wesentlichen Sprachmodelle anhand von Akzeptabilitätsurteilen, die auch die Grammatikalität umfassen können, wie z. B. bei Blimp Syntax Gym, oder die Akzeptanz in Bezug auf Stereotypen, wie z. B. bei Crowds Pairs. Und im Minimal Pair Paradigma ist der typische Weg, Sprachmodelle zu bewerten, dass man ein akzeptables oder grammatikalisches Satzgefüge zeigt und dann ein inakzeptables oder ungrammatisches Satzgefüge präsentiert. Die Hoffnung ist, dass das Modell dem akzeptablen Satzgefüge eine höhere Wahrscheinlichkeit zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns jedoch nicht, die Akzeptanz der Modelle gegenüber längeren Sätzen zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern, sodass es entscheidend ist, die Akzeptanz der Modelle über das gesamte Kontextfenster hinweg zu bewerten. Und genau das versuchen wir hier: Wir wollen die MPP-Pipeline wieder aufleben lassen, indem wir das Modell bitten, die Akzeptanz längerer und längerer Sequenzen zu bewerten. Das ist unser Ansatz. Was wir tun, ist, diese längeren Sequenzen zu simulieren, indem wir selbst die Datensätze erneut betrachten und Sätze neu erstellen, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. So haben wir beispielsweise ein typisches grammatikalisches Paar aus dem Blimp-Datensatz aus dem Adjunct Island-Fall ausgewählt, und was wir tun, ist, längere Sequenzen zu erstellen, die akzeptabel sind und die die gleiche grammatikalische Struktur aufweisen. Wir extrahieren grammatikalische Sätze aus Adjunct Island und fügen sie sowohl dem akzeptablen als auch dem inakzeptablen Query als Präfix hinzu. Wir können das Gleiche tun, indem wir inakzeptable Sätze aus derselben Struktur auswählen, was ebenfalls dazu verwendet werden kann, die Akzeptanz des Modells zu testen. Wir können auch das Gleiche tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Das bezeichnen wir als das Mismatch-Szenario. Hier stammen die Sätze zwar noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir sie bewerten. Wir können das Gleiche für den Fall der Inakzeptanz tun. Schließlich können wir Sätze aus einer völlig anderen Domäne wie Wikipedia auswählen. Dies wird uns zeigen, ob die Akzeptanzurteile der Modelle tatsächlich von irgendeinem Kontext beeinflusst werden, sei es, dass der Kontext aus einem anderen Subset des Datensatzes stammt oder ob er völlig irrelevant für den Satz ist, den wir betrachten. Wie verhält sich das Modell also? Zuerst betrachten wir die Sätze aus Wikipedia, die völlig irrelevant für das aktuelle Query-Paar sind. Dort stellen wir fest, dass die MPP-Urteile weitgehend robust für beliebige Kontextlängen sind. Wir erhöhen die Kontextlänge auf bis zu tausend und vierundzwanzig, um die Modelle von Ot und GPT-2 zu maximieren, und wir sehen hier in der orangefarbenen gepunkteten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir oder erstellen Sätze aus akzeptablen und inakzeptablen Bereichen aus demselben Blimp Syntax Gym Datensatz aus. Dort stellen wir fest, dass sich die MPP-Urteile entweder deutlich erhöhen oder verringern, wenn wir entweder akzeptable Präfixe oder inakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, d. h. wenn wir Sätze aus demselben Phänomen in Blimp Syntax Gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, je nachdem, ob das ausgewählte Präfix akzeptabel oder inakzeptabel ist. Dies ist sehr groß, dieser Effekt verstärkt sich über die Kontextlänge hinweg, und er würde wahrscheinlich neuere Sprachmodelle beeinflussen, die große Kontextfenster haben. Warum beeinflusst ein passendes Präfix die Sprachmodell-Urteile so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingangsatz zu stören, indem wir die relevante Struktur beibehalten, aber dem Eingangs-satz Rauschen hinzufügen. Nach Durchführung mehrerer dieser Störungen stellen wir fest, dass keines dieser Geräusche dazu führt, dass sich das Modell in Bezug auf die Anzeige der MPP-Urteilstrend ändert. Wir stellen fest, dass die Modelle auf ähnliche Weise auf die gestörten Sätze reagieren, d. h. wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir die Sätze im akzeptablen Genehmigungsbereich stören, sehen wir eine Abnahme der MPP-Urteile. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über die Sätze hinweg gemeinsam genutzt werden. Die MPP-Evaluierung, wie wir sie derzeit mit kurzen, einzelnen Satz-Eingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells über das gesamte Kontextfenster hinweg. Bitte lesen Sie unser Paper für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Doktorand an der Staland Universität in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit vorstellen, „Weaker than you think“, einen kritischen Blick auf schwaches Lernen. Dies ist eine Gemeinschaftsarbeit mit X, Myos Mosbach und Ge Steffen sowie Dirich Klako. Ich möchte mit einer kurzen Einführung in schwache Supervision und schwach überwachtes Lernen beginnen. Bei schwacher Supervision labeln wir die Daten nicht manuell. Stattdessen labeln wir die Daten mithilfe schwacher Labelquellen wie einfachen heuristischen Regeln, Wissensdatenbanken oder niedrigwertiger Crowdsourcing-Methoden, wie in der rechts abgebildeten Grafik dargestellt. Im Vergleich zu menschlichen Annotationen sind schwache Annotationen deutlich kostengünstiger, aber auch verrauscht, was bedeutet, dass ein gewisser Teil der Annotationen fehlerhaft ist. Wenn wir neuronale Netze direkt mit schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, die Labelrauschigkeit zu merken und generalisieren nicht. Beim schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solch einer Labelrauschigkeit zu trainieren, sodass die trainierten Modelle dennoch gut generalisieren. In aktuellen Arbeiten in wSL – wobei wSL für weakly supervised learning steht – ist eine gängige Behauptung, dass Modelle lediglich auf den schwach gelabelten Daten trainiert werden und dennoch eine hohe Leistung auf sauberen Testdatensätzen erzielen. Diese Behauptung ist technisch nicht falsch, aber es gibt einen Haken: Es wird angenommen, dass ein zusätzlicher sauberer Validierungsdatensatz vorhanden ist, der für die Modellauswahl verwendet wird. Wir haben uns auf dieses Problem eingestellt, was jedoch impliziert, dass zusätzliche manuelle Annotationen erforderlich sind, um in schwach überwachtem Lernen zu unterstützen. Doch wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannten Arbeiten adressieren drei Forschungsfragen. Erstens: Ist ein sauberer Validierungsdatensatz für wSL notwendig, oder können wir stattdessen einen unsauberen Validierungsdatensatz verwenden? Zweitens: Wenn saubere Daten erforderlich sind, oder wenn saubere Daten für die Funktionsweise von WSL obligatorisch sind, wie viele saubere Samples benötigen wir dann? Und drittens: Sollten wir die sauberen Samples nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit adressiert, und unsere Ergebnisse sind wie folgt: Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen deutlichen Leistungsabfall, wie in der Grafik dargestellt. Wenn keine sauberen Validierungsbeispiele vorhanden sind, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was den Trainingsprozess sinnlos macht. Dies deutet darauf hin, dass wsSL-Ansätze tatsächlich sauber gelabelte Daten benötigen, um ordnungsgemäß zu funktionieren, und dass die Annotationskosten für die Beschaffung sauberer Validierungsbeispiele nicht übersehen werden sollten. Zweitens stellen wir fest, dass die Erhöhung der Anzahl sauberer Validierungsbeispiele dazu beiträgt, dass WSL-Ansätze eine bessere Leistung erzielen, wie in der Grafik auf der linken Seite dargestellt. Typischerweise benötigen wir nur 20 Samples pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns dennoch entscheiden, saubere Samples zu verwenden, erzielt das Training mit diesen sogar eine bessere Leistung. Die rote Grafik zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt unter sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie man sehen kann, beginnt das Fine-Tuning, wenn wir zehn Samples pro Klasse haben, WSL-Ansätze zu übertreffen. Schließlich kann die Leistungssteigerung, die in früheren WSL-Ansätzen beansprucht wurde, leicht erzielt werden, indem man es erlaubt, das Fine-Tuning auf sauberen Validierungsbeispielen fortzusetzen. Wie aus den Grafiken ersichtlich ist, erzielt das Validierungsmodell FTW zunächst schlechtere Ergebnisse als komplexere WSL-Methoden wie Cosine. Wenn wir es jedoch erlauben, das Fine-Tuning auf sauberen Samples fortzusetzen, erzielt FTW die gleiche Leistung wie andere Methoden. Daher gibt es in der Praxis keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle wSL-Ansätze saubere, manuell annotierte Samples benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt: Berichten Sie zunächst über die Kriterien für die Modellauswahl, beispielsweise ob die Modellauswahl mithilfe sauberer Validierungsbeispiele durchgeführt wird. Zweitens sollten WSL-Ansätze mit wenigen Few-Shot-Learning-Baselines verglichen werden, wie z. B. Arbeiten mit konkreten Samples. Drittens sollte das kontinuierliche Fine-Tuning als einfacher, aber starker Basismesswert in zukünftigen Arbeiten in WSL betrachtet werden. Schließlich haben wir unseren Code Open Source gestellt. Sie finden ihn über den QR-Code auf dieser Folie. Bitte fühlen Sie sich frei, ihn zu prüfen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Al Villaard und ich werde Ihnen einen kurzen Überblick über die Arbeit „Prompting P from translation: Assessing Strategies and Performance“ geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein Sprachmodell mit 540 Milliarden Parametern, das im Jahr 2022 vorgestellt wurde. Es wurde auf einer umfangreichen Textsammlung trainiert, die zum Zeitpunkt der Veröffentlichung 780 Milliarden Token umfasst. Es erzielt in Hunderten von NLP-Aufgaben den neuesten Stand der Technik. In dieser Arbeit präsentieren wir eine systematische Studie zum Prompting von Large Language Models für die maschinelle Übersetzung. Wir evaluieren die Übersetzungsfähigkeiten solcher Modelle unter Verwendung der Best Practices der IMT-Community. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir vergleichen zwei State-of-the-Art-Systeme, die leistungsstärksten Systeme der WMT-Evaluation. Wir verwenden State-of-the-Art-Neural-MT-Metriken und zeigen zusätzlich Ergebnisse einer durch Experten bewerteten menschlichen Evaluation. Abschließend geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, in dem wir ein kurzes Prompt verwenden und für verschiedene Sätze zwei unterschiedliche Prompts bereitstellen. Bei der Mehrheit der Sätze, 516 von 1000, beträgt die beobachtete Differenz mehr als ein Blurred-Punkt. In extremen Fällen kann dies bis zu 40 Blurred-Punkte erreichen. Daher ist es wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine Five-Shot-Prompting-Strategie entschieden, bei der wir lediglich den Satz, den wir dem System bereitstellen, mit der jeweiligen Sprache markieren. In diesem Beispiel, in dem wir eine Übersetzung von Deutsch ins Englische durchführen, werden die deutschen Ausgangssätze mit „deutsch:“ und die englischen Übersetzungen mit „englisch:“ markiert. Wir haben festgestellt, dass die tatsächliche Form des Prompts keinen großen Einfluss hat, insbesondere bei wenigen kurzen Prompts. Es ist entscheidend für Zero- und One-Shot-Prompting, aber wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Prompts. Die Beispiele tragen den Großteil des Gewichts. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit mit dem Ausgangssatz. Daher ist es wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Evaluationen oder den Dev-Daten. Die Dev-Daten sind wesentlich kuratierter und von höherer Qualität als die Trainingsdaten, und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte State-of-the-Art-Systeme einen deutlichen Vorteil gegenüber den PaLM-Übersetzungen, aber PaLM kommt einem kommerziellen System recht nahe. In unserem Fall haben wir Google Translate zur Evaluation verwendet. Die Erkenntnisse, die wir aus der Evaluation mit dem NpN-Framework gewonnen haben, sind, dass die Flüssigkeit von PaLM mit State-of-the-Art-Systemen vergleichbar ist, der Hauptunterschied ergibt sich jedoch aus der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass PaLM manchmal eine besser klingende Übersetzung erzeugen möchte, indem es Teile des Ausgangssatzes weglässt. Die stilistische Qualität von PaLM ist jedoch niedriger als bei den State-of-the-Art-Systemen, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssigen Output liefert, aber dennoch einige Probleme mit der Genauigkeit hat. Das war’s für diesen kurzen Überblick. Für weitere Details laden wir Sie ein, sich die vollständige Präsentation des Papiers anzusehen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Jin Wei Y von der Universität für Wissenschaft und Technologie Chinas. Es ist mir eine Freude, Ihnen ein kurzes Werbevideo für unser Papier vorzustellen. Kopieren Sie mein Modell, Schutz des Urheberrechts großer Sprachmodelle für Einbettungen und Dienste? Wir setzen auf eine Wasserzeichentechnologie. Lassen Sie uns zunächst den Hintergrund von Einbettungen und Diensten vorstellen. Derzeit sind große Sprachmodelle wie Gbt, La, PLm außergewöhnlich im Bereich des natürlichen Sprachverständnisses und der Textgenerierung. Einbettungen und Dienste sind eine der auf großen Sprachmodellen basierenden Dienste, die verschiedene NLP-Aufgaben unterstützen. Beispielsweise bietet OpenI eine auf Gbt basierende Einbettungs-API. Jüngste Arbeiten haben jedoch gezeigt, dass ein Angreifer das Modell durch Lernen aus den Einbettungen stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Einbettungen als Dienste zu schützen. Um das Urheberrecht von Einbettungsdiensten zu schützen, schlagen wir vor, ein Wasserzeichen in den vom Anbieter bereitgestellten Dienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode auf Einbettungen als Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Einbettung nicht beeinträchtigen. Drittens sollte das Wasserzeichen ausreichend robust sein, sodass ein Angreifer es entweder nicht erkennen oder leicht entfernen kann. Viertens muss das Wasserzeichen während des Modellausleseprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht auf Einbettungen als Dienste anwendbar oder es fehlt ihnen an Übertragbarkeit. In diesem Papier schlagen wir Embedding Marker vor, eine Backdoor-basierte Wasserzeichenmethode, die auf Einbettungen als Dienste anwendbar ist. Lassen Sie mich nun die Details unseres Embedding Markers vorstellen. Embedding Marker umfasst zwei Hauptschritte: Wasserzeicheneinschub und Urheberrechtsverifikation. Vor diesen Hauptschritten wählen wir zunächst einen Auslösesatz. Der Auslösesatz ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Häufigkeit der Wörter damit zählen kann. Beim Wasserzeicheneinschub definieren wir zunächst eine Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Auslöser in dem Satz. Die bereitgestellte Einbettung ist eine gewichtete Summe der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Bei der Urheberrechtsverifikation wird festgestellt, ob das Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zunächst einen Backdoor-Datensatz und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Auslösesatz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Auslösesatz gehören. Anschließend fordert der Anbieter Einbettungen vom stiller Dienst mit dem Datensatz an. Die Kosinus- und L2-Ähnlichkeit zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem gutartigen und dem Backdoor-Datensatz, die als Delta Kosinus und Delta L2 definiert ist. Wir wenden außerdem einen KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente an vier Datensätzen durch: Aging News, Mind, SD2 und A Spam. Wir nehmen an, dass der Anbieter den Wiki-Text-Datensatz verwendet, um die Häufigkeit der Wörter zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielen kann und gleichzeitig eine hohe Nutzbarkeit für nachgelagerte Aufgaben aufrechterhält. Wir validieren außerdem die Robustheit der bereitgestellten Einbettung durch Visualisierung der Einbettungen von Sätzen auf vier Datensätzen mit PCA. Die Legende der Abbildungen bedeutet die Anzahl der Auslöser in jedem Satz, wie in den Abbildungen gezeigt. Es ist schwierig, zwischen den modifizierten Einbettungen und normalen Einbettungen zu unterscheiden. Das war alles. Vielen Dank. Wir laden Sie herzlich ein, sich mit uns auszutauschen."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Ian und meine Kollegin Jiian und ich werden unsere Forschung zur Multi-Instruct verbessern der multimodalen seriellen Lernverfahren durch Instruction Tuning vorstellen. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, bei denen vortrainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf eine parameter- und dateneffiziente Weise eingesetzt werden. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning großen Sprachmodellen ermöglicht, sich auf unbekannten Aufgaben in einem Serial-Shot-Verfahren zu verhalten, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Instruction Tuning konzentrieren sich jedoch auf die Verbesserung der Serial-Shot-Leistung bei sprachbasierten Aufgaben, wobei Computer Vision und multimodale Aufgaben vernachlässigt wurden. In dieser Arbeit möchten wir untersuchen, ob Instruction Tuning auf multimodalen Proteinmodellen tatsächlich die Verallgemeinerung auf unbekannte multimodale Aufgaben verbessern kann. Darüber hinaus haben wir während unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Instruction-Datensätzen zwischen sprachbasierten und multimodalen Aufgaben festgestellt. Es gibt mehr als 1600 sprachbasierte Instruction-Aufgaben, jedoch keinen großflächig öffentlich verfügbaren multimodalen Instruction-Aufgabendatensatz. Dies motivierte uns, einen multimodalen Instruction-Tuning-Datensatz zu erstellen. Hier präsentieren wir Multi-Instru, den ersten multimodalen Instruction-Tuning-Benchmark-Datensatz, der aus 62 verschiedenen multimodalen Aufgaben mit 10 Hauptkategorien besteht. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet, um das multimodale Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen. Wir verwenden Offa, ein einheitliches multimodales Trainingsmodell, als unser Basismodell. Offa verwendet ein einheitliches Vokabular für Sprach-, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instru-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabetypen zu vereinheitlichen, folgen wir der Methode von Offa und formulieren alle Aufgaben in einem einheitlichen Sequence-to-Sequence-Format, wobei der Eingabetext, die Bilder, die Anweisungen und die Begrenzungsrahmen im selben Token-Raum dargestellt werden. Nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus N Group für das Training und wählen 10.000 Instanzen pro Aufgabe für das Testen aus. Wir reservieren die gesamte Common Sense Reading Group für das Testen und wählen zusätzlich fünf Aufgaben aus Wiki und der Miscellaneous Group aus. Wir verwenden alle Instanzen im Test-Set für jede Aufgabe. Zusätzlich wählen wir zufällig 20 Aufgaben aus dem Test-Set von Natural Instruction als Same-Task-Aufgaben für NP aus. Während des Trainings verwenden wir ein vortrainiertes großes Modell als Basismodell. Wir mischen alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer von fünf Instruction-Templates kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer von fünf Anweisungen bewerten. In jedem Experiment melden wir die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine multimodale Klassifikationsaufgabe ist, melden wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, melden wir den Root L. Für eine LP-Aufgabe berichten wir ebenfalls den Root L. Wir haben außerdem eine zusätzliche Evaluationsmetrik eingeführt, die Sensitivität. Diese Metrik misst die Fähigkeit des Modells, für dieselbe Aufgabe konsistent dieselben Ausgaben zu erzeugen, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann Instruction Tuning die Leistung von Offa auf multimodalen Aufgaben signifikant verbessern. Auch das Transferlernen aus dem Natural Instruction-Datensatz kann dem Instruction Tuning zugute kommen. Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig die Sensitivität verringert. Wir haben außerdem ein Experiment durchgeführt, in dem wir eine Anweisung gegenüber fünf Anweisungen verwendet haben. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität deutlich reduzieren. Dies zeigt den Effekt verschiedener Finetuning-Strategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transferlernen aus Natural Instruction-Datensätzen eine viel bessere Sensitivität erzielen als das ursprüngliche Offa-Modell. Wir können auch sehen, dass das Transferlernen aus dem Natural Instruction-Datensatz Offa helfen kann, auf dem Nitrogen-Instruct-Datensatz eine viel bessere Leistung zu erzielen. Insgesamt schlagen wir den ersten großflächigen multimodalen Instruction-Tuning-Datensatz vor, verbessern die neurale Fähigkeit von Offa signifikant und untersuchen verschiedene Transferlerntechniken und zeigen deren Vorteile. Wir haben eine neue Metrik namens Sensitivität entwickelt. Als zusätzlichen Punkt sammeln wir einen noch größeren multimodalen Instruction-Tuning-Datensatz mit rund 150 zusätzlichen varianten Sprachaufgaben und werden ihn veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Just John von der Pennsylvania State University. Heute werde ich unsere Arbeit vorstellen, ExAmpler: Cross-linguale semantische Analyse in mehreren natürlichen Sprachen und manuellen Repräsentationen. Die semantische Analyse ist eine Aufgabe, um semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda-Kalkül zu erstellen. Und die cross-linguale semantische Analyse ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen, wie in dieser Abbildung dargestellt. Wir müssen die Abfrage in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende cross-linguale semantische Analysemodelle werden separat vorgeschlagen und auf Datensätzen mit begrenzter Anzahl von Fällen und Anwendungen bewertet. Beispielsweise gibt es Lücken in der Abdeckung bestimmter natürlicher Sprachen, Chinesisch fehlt, oder Lücken in der Abdeckung bestimmter Bedeutungsrepräsentationen, der Lambda-Kalkül fehlt, oder sie werden nur für bestimmte neuronale Modelle bewertet. Beispielsweise gibt es nur ein einzelnes Modell zur Bewertung. Zu diesem Zweck schlagen wir ExAmpler vor, einen einheitlichen Datensatz ExAmpler für die cross-linguale semantische Analyse in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Er enthält neun Datensätze in verschiedenen Domänen, fünf semantische Analysetypen, acht Millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser bewerten zu können, betrachten wir sechs Einstellungen für Training und Evaluierung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell für Training und Evaluierung. Beispielsweise trainieren wir das englische Modell mit englischen Abfragen und übersetzen während der Inferenz die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um die SQL-Ausgabe vorherzusagen. Wir testen außerdem monolinguale Modelle. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache, beispielsweise Deutsch-Deutsch oder Englisch-Englisch. Wir testen außerdem die monolinguale Fusions-Einstellung, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren und ein multilinguales Modell testen, das wir für alle Sprachen trainieren. Beispielsweise kombinieren wir deutsche, englische und chinesische Abfragen, um ein multilinguales Modell zu trainieren, und können dieses Modell während der Inferenz verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Wir betrachten außerdem cross-linguale Zero-Shot- und Few-Shot-Übertragungen. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir es mit englischen Abfragen oder einer Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein multilinguales Modell zu trainieren und die seQL-Ausgabe vorherzusagen. Wir haben auch viele interessante Ergebnisse gefunden. Bezüglich der Analyse monolingualer Modelle bewerten wir zwei Gruppen von Modellen, einschließlich Encoder-Decoder, was für mehrsprachige vortrainierte Encoder mit Pointer-basierten Decodern wie XLMR-PDdR und BART-PDdR steht, und wir bewerten außerdem Encoder-Decoder-Modelle, was für mehrsprachige vortrainierte Encoder-Decoder-Modelle wie MBT und Mt5 steht. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung in allen neun Datensätzen erzielen. Wir evaluieren MT5 und XLMR-PDdR in einem mehrsprachigen Setting. Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PDdR verbessert werden können, indem sie mit einer Mischung aus verschiedenen Sprachen trainiert werden. Wir haben festgestellt, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, außer dass die englische Leistung in sieben Datensätzen abnimmt und in nur drei Datensätzen zunimmt. Ich denke, dies ist als Kurve der Mehrsprachigkeit bekannt. Wir haben auch die cross-linguale Leistungsdifferenz verglichen. In dieser Abbildung ist die blaue Linie die cross-linguale Few-Shot-Übertragung, die orange Linie ist die cross-linguale Zero-Shot-Übertragung, während die grüne Linie die monolinguale Einstellung ist. Wir haben festgestellt, dass wir beim Vergleich der grünen und orangefarbenen Linie festgestellt haben, dass die cross-linguale Übertragungsleistungsdifferenz im Zero-Shot-Setting erheblich ist, und beim Vergleich der blauen und orangefarbenen Linie festgestellt haben, dass die Übertragungslücke im Few-Shot-Setting schnell verkürzt wird. Wir haben auch einige andere interessante Ergebnisse gefunden, beispielsweise, dass Encoder-Decoder die bestehenden Arbeiten übertreffen oder vergleichbare Ergebnisse erzielen, und das Training unseres englischen Natural Language die Leistung des Few-Shot auf Ziel-Natural Language deutlich steigern kann. Wir haben festgestellt, dass multilinguale Sprachmodelle wie Encoder und BART immer noch unzureichend für die cross-linguale semantische Analyse sind. Zusammenfassend haben wir ExAmpler, einen einheitlichen Benchmark für die cross-linguale semantische Analyse mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen, entwickelt. Wir haben eine umfassende Benchmark-Studie zu drei repräsentativen Typen von multilinguellen Sprachmodellen durchgeführt, und unsere Ergebnisse zeigen viele interessante Ergebnisse usw. Besuchen Sie gerne unser Paper und den zugehörigen Code. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Shrikkovski und dieser Vortrag handelt von der Dependenzstruktur der Koordination. Wie Sie möglicherweise wissen, gehen verschiedene Theorien und Korpusansätze unterschiedliche Dependenzstrukturen an. So zum Beispiel ist die Struktur der Koordination „Lisa, Bart und Maggie“ in den Universal Dependencies so, dass das erste Konjunkt der Kopf der gesamten koordinierten Struktur ist. In diesem Fall ist „Lisa“ ein ähnlicher Ansatz wird in Igors Miltschuks Meaning-Text-Theorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind asymmetrisch, sie heben ein Konjunkt hervor. Es gibt aber auch symmetrische Ansätze für Koordinationsstrukturen, wie beispielsweise der pragmatische Ansatz, der in den pragmatischen Dependency-Baumbanken verwendete Konjunktionskopf-Ansatz, bei dem koordinierte Strukturen vom Konjunktion geleitet werden. Wir erhalten so Abhängigkeiten von der Konjunktion zu allen Konjunkten. Schließlich gibt es auch einen Mehrkopf-Ansatz, der beispielsweise in CUSF’s Wortgrammatik verwendet wird, wo sozusagen alle Konjunkte Köpfe der Koordinationsstruktur sind. Wir erhalten also Abhängigkeiten vom Regenten zu allen Konjunkten getrennt. Dies sind die Grundlagen. Das Ziel dieses Papiers ist es, ein neuartiges Argument für die symmetrischen Strukturen der Koordination, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordination, wie diese beiden, zu liefern. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erläutern werde. Im Englischen, wie Sie vielleicht wissen, bevorzugen direkte Objekte, nahe am Verb zu sein, während Adverbiale weiter entfernt stehen können. „March read it yesterday“ ist in Ordnung, weil das direkte Objekt nahe am Verb ist, während „March read yesterday it“ viel schlechter klingt, weil sich zwischen dem Verb und dem direkten Objekt das Adverbial „yesterday“ befindet. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und lang ist, da es dann hinter das Adverbial verschoben werden kann. Dies wird hier veranschaulicht. Beide Sätze sind in Ordnung: „March read this absolutely fascinating book about bees yesterday“ ist in Ordnung, oder „March read yesterday this absolutely fascinating book about bees“. Die Argumentation dahinter ist, dass es möglich ist, weil, obwohl dieser Satz das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte direkt neben dem Verb stehen sollten, er das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also die, die in diesen beiden Strukturen nicht konstant sind. Hier haben wir die Abhängigkeit von „read“ zum Adverbial der Länge sieben, gemessen in Wörtern, und von „read“ zu „book“ der Länge vier. Insgesamt sind es also 11. Wenn man diese beiden Konstituenten vertauscht, wird die Summe dieser beiden Abhängigkeiten sechs. Das ist also viel kürzer, deshalb klingt es ganz in Ordnung. Es verletzt ein Prinzip, erfüllt aber ein anderes. Wir haben verschiedene Statistiken aus der Koordination extrahiert, aus der erweiterten Version von Penn Treebank und sehen im Paper, warum wir die Universal Dependencies nicht verwendet haben. Diese Statistiken bestätigen die mehrfach zuvor getroffene Beobachtung, dass linke Konjunkte tendenziell kürzer sind, beispielsweise „salt and pepper“ und nicht „pepper and salt“, gemessen in Silben. Auch die Beobachtung, dass diese Tendenz mit der Länge wächst, also dass, wenn der Unterschied zwischen den Längen der beiden Konjunkte wächst, die kürzere Konjunkt eher vorne steht, ist stärker. Der Anteil der linken, kurzen Konjunkte ist also größer. Was in diesem Paper neu ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regent links fehlt. Der Regent befindet sich in diesem Beispiel links: „I saw Bart and Lisa“. Er fehlt im zweiten Beispiel: „Homer came and sneezed“. Hier haben wir eine Koordination von zwei Verben, und es gibt keinen externen, übergeordneten Regenten. In solchen Fällen bevorzugt die linke Konjunkt, kürzer zu sein, je größer der Unterschied zwischen den beiden Konjunkten ist. Wenn der Regent jedoch rechts ist, wie hier: „Left governs the coordination tail and net“, verschwindet dieser Effekt. Wir zeigen dies, indem wir die Länge in Zeichen messen, die erste Spalte in Silben, die mittlere Spalte und die rechte Spalte in Wörtern. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass, wenn der Regent links steht, die Tendenz, dass die linke Konjunkt kürzer ist, stetig mit dem absoluten Unterschied in Wörtern wächst. Das Gleiche wird beobachtet, wenn kein Regent vorhanden ist, wie bei der Koordination von Sätzen. Wenn der Regent jedoch rechts steht, verschwindet diese Tendenz. Wir zeigen im Paper, wie dies ein Argument gegen asymmetrische Strukturen der Koordination liefert, die ein Konjunkt hervorheben, im Gegensatz zu den symmetrischen Strukturen. Sehen Sie im Paper die vollständige Argumentation und die Argumente. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Guten Tag, mein Name ist Kyo Yin und ich werde unsere Arbeit mit dem Titel \"Wann erfordert Übersetzung Kontext? Eine datengesteuerte, mehrsprachige Untersuchung\" vorstellen. Diese Arbeit entstand in Zusammenarbeit mit Patrick Ferange, Emiliu, Andre F.D. Martins und Graham Newbiig. Es stellt sich also heraus, dass viele Übersetzungen vom Kontext abhängen. Zum Beispiel: Wie würden wir „mole“ in diesem Satz übersetzen, wenn der vorherige Satz lautet: „Die Dinge könnten gefährlich werden, wenn die Minister davon erfahren“? Dann bezieht sich „mole“ auf einen Spion. Aber wenn der vorherige Satz lautet: „Könnte es etwas Ernstes sein, Doktor?“, dann bezieht sich „mole“ auf eine Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes, und folglich auch seine Übersetzung. Die Bewertung, wie gut Modelle Fälle wie diesen übersetzen können, ist jedoch ziemlich schwierig. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was dazu führt, dass metrische Werte auf Korpus-Ebene wie BLEU diese Übersetzungen nicht erfassen können. Einige haben zwar gezielte Bewertungen für kontextabhängige Übersetzungen vorgeschlagen, diese Ressourcen unterstützen jedoch nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie in der Regel auf Domänenwissen und menschliche Kuratierung angewiesen sind. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten: Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut gehen Modelle mit diesen Fällen um? Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie stark eine Übersetzung vom Kontext abhängt. In unserer vorherigen Arbeit haben wir cxmi als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht durch die Messung, wie viele Informationen der Kontext C über die Zielsprache y gibt, gegeben die Ausgangssprache x. Man kann sich cxmi als die Information vorstellen, die man erhält, wenn man dem Modell Kontext liefert. In dieser Arbeit erweitern wir cxmi auf Punkt y, cxmi, das die Kontextnutzung auf Wortebene oder Satzebene messen kann. Wir können uns Wörter mit einem hohen p6mi als solche vorstellen, die für die Übersetzung einen Kontext erfordern. Nun analysieren wir Wörter mit einem hohen p6mi, um nach Mustern zwischen diesen Wörtern zu suchen. Unsere Analyse führen wir an Transkripten von TED Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Unsere Analyse führen wir auf drei verschiedenen Ebenen durch. Zunächst betrachten wir Wortarten, die hohe Mittelwerte für pxmi aufweisen. Dies ermöglicht uns beispielsweise, duale Pronomen im Arabischen zu finden, die relativ hohe pxMmi-Werte aufweisen. Dies lässt sich erklären, da Englisch keine dualen Pronomen hat, sodass man einen Kontext benötigt, um festzustellen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ebenso stellen wir fest, dass bestimmte Sprachen ebenfalls einen Kontext erfordern, wenn wir die passende Verbform auswählen möchten. Als Nächstes betrachten wir Wortelemente, die über alle ihre verschiedenen Vorkommnisse hinweg einen hohen p6I aufweisen. Dies hilft uns, Fälle wie diesen zu identifizieren, bei denen man im Chinesischen einen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man dieselbe Übersetzung innerhalb des Dokuments verwendet. Ebenso stellen wir fest, dass der Kontext erforderlich ist, um die richtige Formalität bei der Übersetzung zu gewährleisten. Schließlich betrachten wir einzelne Token mit einem hohen pxmi, was es uns ermöglicht, Phänomene zu identifizieren, die nicht wirklich vom Wort selbst erfasst werden können, sondern eher in der Satzstruktur zum Ausdruck kommen, wie z. B. Ellipsenauflösung. Nun nutzen wir unsere Ergebnisse aus unserer Analyse, um einen Benchmark für Übersetzungen auf Dokumentenebene zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, haben wir Tagger erstellt, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Wir nennen unseren Tagger Multilingual Discourse Aware oder MUDA Tagger. Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben. Dann verwenden wir den MUDA Tagger, indem wir den Tagger auf einem parallelen Korpus anwenden, den wir für die Bewertung verwenden möchten, und wenden unsere Übersetzungsmetriken der Wahl auf die kontextabhängigen Beispiele an, die der M Tagger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf Dokumentenebene maschinell zu übersetzen. Zunächst stellen wir fest, dass bei der Verwendung von metrischen Werten auf Korpus-Ebene, wie z. B. BLEU, kontextunabhängige Modelle die beste Leistung erbringen. Wenn wir jedoch Comment verwenden, erzielen kontextbewusste Modelle die besten Ergebnisse. Und wenn wir die Wordf-Maßnahme verwenden, erzielen Modelle mit oder ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste Dokumentübersetzungssystem zu bestimmen, wenn wir allein metrische Werte auf Korpus-Ebene verwenden. Nun verwenden wir den M-Benchmark, um Modelle zu bewerten, und wir stellen fest, dass kontextbewusste Modelle bei bestimmten Diskursphänomenen wie Formalität und lexikalischer Kohärenz deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen wie Ellipsenpronomen und Verbformen nicht wesentlich besser als Modelle, die keinen Kontext verwenden. Dies deutet also darauf hin, wo wir mehr Fortschritte für die Übersetzung auf Dokumentenebene erzielen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass dL in der Regel genauer ist als Google Trans für Übersetzungen auf Dokumentenebene. Zusammenfassend haben wir eine datengesteuerte Analyse über 14 Sprachpaare hinweg durchgeführt, um festzustellen, wann Übersetzungen Kontext erfordern, und dann unsere Erkenntnisse genutzt, um einen Benchmark für die maschinelle Übersetzung auf Dokumentenebene zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder nicht gut verarbeiten können, und welche Übersetzungssysteme gut für Übersetzungen auf Dokumentenebene geeignet sind. Vielen Dank für Ihre Aufmerksamkeit, sehen Sie Sie in Toronto!"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich Ihnen unsere Arbeit „AnL Positionality“ vorstellen, die Design-Bias und Beta-Datensätze von Modellen charakterisiert. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen an der University of Washington und dem Allen Institute for AI, insbesondere Sebastian Santi, Ronin Labrasse, Katharina Reinika und Martin Sapp, durchgeführt. Lassen Sie uns zunächst annehmen, dass Sie für eine Zeitung arbeiten und durch Kommentare unter Ihren Nachrichtenartikeln suchen, um toxische Inhalte zu entfernen. Möglicherweise greifen Sie auf eine beliebte API wie die Perspective API zur Toxizitätserkennung zurück. Dies funktioniert gut, wenn Sie Carl Jones sind, da die Perspective API toxische Instanzen korrekt erkennen kann. Das ist jedoch nicht der Fall bei Didtha Sharma, wo die Perspective API deutlich weniger empfindlich auf beleidigende Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Bevölkerungsgruppen feststellen. Design-Bias wie der, den wir gerade gesehen haben, können auf die Positionality der NLP-Forscher und Modellentwickler zurückzuführen sein. Positionality bezeichnet einfach die Perspektiven, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen, weit verbreitet ist. Als Forscher kann Positionality den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann. Eine Frage, die sich daher stellt, ist, ob Datensätze und Modelle eine Positionality haben. Wir wollen damit nicht sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können somit bestimmte Positionalities stärker repräsentieren als andere. Vorherige Arbeiten haben einige anekdotische Belege für Positionality geliefert, wie z. B. kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen von Modell-Positionality. Diese Arbeiten betrachten jedoch nicht, wie Endbenutzer mit den Datensätzen und Modellen selbst verglichen werden. Die Untersuchung von Modell- und Datensatz-Positionality wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozialer werden, und es ist eine Herausforderung, zu charakterisieren, wie diese Positionalities verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind. Um die Positionality von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen mit echten Nutzern mit bestehenden Datensätzen und Modellen. Dies tun wir durch unseren Rahmen Nl Positionality. Unser Rahmenwerk umfasst zwei Hauptschritte. Der erste Schritt ist die erneute Annotation von Datensätzen mit einer vielfältigen Gruppe von Annotatoren. Wir bevorzugen dies gegenüber der Betrachtung der demografischen Merkmale der ursprünglichen Datensätze, da in der Regel nur wenige Annotatoren jede Instanz annotieren und da demografische Daten selten erfasst und geteilt werden. Wir bevorzugen daher die erneute Annotation von Daten, um viele Annotationen pro Instanz zu erhalten und gleichzeitig einen reichen Satz an demografischen Daten zu erhalten. Wir analysieren dann die Annotationen nach demografischen Merkmalen und vergleichen sie mit den Modellen und Datensätzen mithilfe des Pearson-Korrelationskoeffizienten. Unser Rahmenwerk unterscheidet sich somit von der Literatur über Annotatoreinigkeit, indem wir Endbenutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleichen, anstatt nur die Einigkeit zwischen Annotatoren zu betrachten oder Annotatorendistributionen zu modellieren. Unser Rahmenwerk wird größtenteils durch Lab in the Wild ermöglicht, eine Online-Crowdsourcing-Plattform. Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Gegensatz zu Plattformen wie Turk, die größtenteils Teilnehmer aus den USA oder Indien haben, ermöglicht Lab in the Wild zudem die Erfassung hochwertiger Daten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Bei dieser Aufgabe lesen die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz und schreiben dann auf, wie sozial akzeptabel diese Situation ist. Um am Studium teilzunehmen, können sie ihre Antworten mit denen einer KI und anderer vergleichen. Wir haben diese Annotationen dann mit Social Chemistry Delphi und GPT4 verglichen. Wir haben einen sehr ähnlichen Ansatz für die Aufgabe der Toxizitäts- und Hassreden-Erkennung repliziert, bei der die Teilnehmer eine Instanz aus Dyna Hate lesen und angeben, ob sie der Ansicht sind, dass es sich um einen Fall von Hassrede handelt. Diese Annotationen haben wir dann mit Dyna Hate, Perspective API, Rewire API, Hate Roberta und GPT4 verglichen. Unsere Studie umfasste insgesamt über 16.000 Annotationen von über tausend Annotatoren aus achtzig sieben Ländern. Damit sind wir nun besser in der Lage, die Frage zu beantworten: Mit wem stimmen NLP-Datensätze und -Modelle am ehesten überein? Wir stellen fest, dass es Positionality in NLP gibt. So stellen wir fest, dass Datensätze und Modelle am ehesten mit englischsprachigen Ländern übereinstimmen. Bei der Analyse der sozialen Akzeptanz von GPT-4 stellen wir fest, dass es am ehesten mit konfuzianisch geprägten und englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass Dyna Hate ebenfalls am ehesten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch eine weitere Übereinstimmung mit Personen mit einem Hochschulabschluss fest. Bei der Analyse der sozialen Akzeptanz von GPT-4 stellen wir fest, dass es am ehesten mit Personen mit einem Hochschul- oder Hochschulabschluss übereinstimmt. Dasselbe gilt für Dyna Hate. Allerdings werden einige Bevölkerungsgruppen unweigerlich zurückgelassen, wenn Modelle und Datensätze auf bestimmte Bevölkerungsgruppen ausgerichtet sind. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger gut mit nicht-binären Personen übereinstimmen als mit männlichen und weiblichen Personen. Dies finden wir sowohl in der Analyse der sozialen Akzeptanz von GPT-4 als auch in der Analyse von Dyna Hate. Angesichts der Tatsache, dass es Positionality in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen in diesem Zusammenhang. Die erste Empfehlung ist, alle relevanten Designentscheidungen während des gesamten Forschungsprozesses zu dokumentieren. Die zweite Empfehlung ist, NLP-Forschung unter dem Gesichtspunkt des Perspektivenismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu erstellen. Ein gutes Beispiel hierfür ist die Masakanne Initiative. Wir möchten betonen, dass inklusives NLP nicht nur darin besteht, alle Technologien für jeden nutzbar zu machen. Damit schließt unsere Präsentation ab. Wenn Sie mehr erfahren möchten, können Sie gerne unseren Dashboard mit den neuesten Analyseergebnissen und unserem Paper einsehen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo und ich werde über unsere Arbeit zur Lösung indirekter Differentialausdrücke für die Entitätselektion sprechen, wobei wir den Alt-Entity-Korpus vorstellen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Philip Radlinsky, Sylvia Parity und Annie Luis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Ich betrachte hier folgende alternative Frage: Meinten Sie „Easy on Me“ oder „I got a feeling“? Hier möchte der Benutzer zwischen einem dieser beiden Songs wählen. Das Offensichtlichste wäre, eine direkte Referenz zu verwenden, zum Beispiel indem man den Namen des Songs „Easy on Me“ oder seine Position, die erste, nennt. Manchmal ist eine indirekte Referenz jedoch angemessener, um eine natürlichere Konversation zu führen. Dies kann passieren, wenn der Benutzer sich den Namen des Songs nicht erinnert oder die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind, oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Differenzen. Zum Beispiel das neuere Stück oder das Genre, das nicht energiegeladen ist. Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Bewertung des Entitätsverständnisses von LLMs. Uns ist kein öffentlicher Datensatz, ein größerer, öffentlicher Datensatz für diese Aufgabe bekannt, daher haben wir einen mithilfe von Crowd-Annotationen erstellt. Unser Datensatz umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unsere Datensatzzusammenstellungsmethodik betont die Informalität mithilfe eines Cartoon-Vervollständigungsaufbaus. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ Damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: „Meinten Sie 'Easy on me' oder 'I got a feeling'?“, was die alternative Frage ist. Und in der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel „das neuere“. Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts pro Bereich ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage: „Meinten Sie A oder B“, wobei A und B Proben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir in der Liste höher gehen, werden die Entitäten ähnlicher zueinander, und es ist in der Regel schwieriger, die Disambiguierung vorzunehmen. Die erste Methode ist zufällig und gleichverteilt. Die zweite Methode ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen „Return“. Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie ähnliche Info Boxes oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler für Songs. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen diese die Namen dieser Entitäten, aber nicht unbedingt etwas über die Entitäten. Was wir tun, ist, dass wir einige Hintergrundinformationen über die beiden Entitäten anzeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song und bitten die Annotatoren, mindestens einige Teile von jedem Song anzuhören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song „Easy on me“. Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste und sie mithilfe von drei bis fünf indirekten Bezugsausdrücken zu beschreiben, zum Beispiel „das mit der Klavier-Musik“. Hier sind einige Beispiele aus unserem Datensatz: zum Beispiel „das ohne Wörter“, „nicht das mit dem 12-jährigen Jungen“ oder „das fiktive“ oder „das aus Aserbaidschan stammt“ usw. Der Alternative-Korpus hat 6.000 alternative Fragen in drei Bereichen und 422.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5x Large Model sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf die gleichen Hintergrundinformationen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92 bis 95 Prozent. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf einige teilweise überlappende Hintergrundinformationen hat, liegt die Genauigkeit zwischen 82 und 877 Prozent, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur auf Entitätsnamen Zugriff hat, liegt die Genauigkeit nur bei 6 Prozent. Es gibt also viel Spielraum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank für Ihre Aufmerksamkeit."}
