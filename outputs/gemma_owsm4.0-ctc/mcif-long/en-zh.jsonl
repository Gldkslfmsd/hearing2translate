{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是马蒂亚斯·林德曼，今天我将为大家简要介绍我们关于在没有树结构的情况下，使用多重集标记和潜在置换实现组合泛化的论文。 这项工作是我与我的研究生亚历山大·科勒和伊万·蒂托夫共同完成的。 组合泛化可以理解为学习者处理更深层递归以及在训练过程中单独看到的短语的未见组合的能力。 在语义解析的测试背景下，组合泛化可能如下所示：通常，我们有一个训练集，其中包含语句，例如“女孩睡觉了”和“玛丽知道女孩睡觉了”。 这些语句与逻辑形式配对，这些逻辑形式代表其含义的核心方面。 与标准的机器学习评估相比，测试集不来自相同的分布，而是包含结构上未见的逻辑形式。 在这个例子中，模型在训练过程中看到了浅层递归，然后被测试在具有更深层递归的例子上。 朴素的序列到序列模型难以处理这种分布外泛化，并且通常会产生与输入无关的输出。 尤其是，它们常常无法再现输入和输出之间的系统性对应关系，例如在示例中用颜色编码显示的那种对应关系。 一种流行的解决这个问题的方法是在模型中集成树结构。 这些树结构旨在捕捉将语句与逻辑形式相关的组合过程。 这通常效果良好，但树结构通常不提供，需要以某种方式获得。 这可能很复杂，有时也可能是计算量很大的过程。 通常，这涉及到对逻辑形式进行相当程度的特定形式的预处理，例如，用于处理变量符号。 获取树结构可能还涉及专门的语法归纳程序。 在本文中，我们不使用树结构，而是引入了一个神经序列到序列模型，该模型直接模拟输入片段与输出片段之间的对应关系。 我们首次展示了在没有依赖树结构的情况下，对更深层递归实现强大的泛化能力。 我们的方法分两个步骤从输入预测输出：首先，我们使用多重集标记对每个输入标记进行标记，该多重集包含将在输出中出现的标记。 在第一步之后，我们拥有了所有正确的标记，但它们没有被排序。 因此，在第二步中，我们使用另一个模型来预测一个置换，将它们排列成正确的顺序。 我们介绍了一种新的预测置换的方法，该方法不会对可能的置换施加任何硬性约束，这使得我们的方法非常灵活和富有表现力。 概念上，我们的置换模型的工作方式大致如下：我们从左向右遍历输出，确定将哪个多重集标记放入每个位置。 对于第一个输出位置，我们只需选择一个，如图中红色突出显示的那样。 然后，我们跳到下一个多重集标记以确定输出中的第二个标记。 我们以类似的方式确定输出中的第三个标记，通过跳到另一个多重集标记。 我们继续这个过程，直到每个标记都从第一阶段访问过一次。 为了让大家对实验结果有所了解，这里我们与其他的无树模型在COgs基准测试中进行比较。 我们的模型在更深层递归的泛化方面，明显优于其他模型。 然而，其他一些类型的结构泛化仍然非常具有挑战性。 在我们的论文中，我们解决了几个有趣的难题。 首先，输入和输出之间的对齐方式在训练数据中没有给出。 因此，对于给定的标记，我们不知道它来自哪个多重集，这给训练带来了挑战。 此外，有时存在多个与数据一致的置换，但语言上正确的置换是潜在的。 我们通过将对齐方式作为训练的一部分进行归纳来解决这个问题。 我们的置换方法非常灵活，但带来了寻找最高分置换的挑战，这属于NP难问题。 这是因为这与旅行商问题相关。 我们使用 GPU 友好的连续松弛法来近似它，这也允许我们反向传播解决方案并学习更符合语言习惯的置换。 如果您想了解更多关于我们的实验以及我们如何应对这些挑战的信息，请阅读我们的论文或访问我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是Myra，今天我将介绍我们的论文“标记人格”，利用自然语言提示词来衡量语言模型中的刻板印象。这项工作是与Essenndermush和Danjorovsky合作完成的。近年来，许多研究已经记录了大语言模型或LLM中社会偏见和刻板印象的存在。然而，这些衡量方法存在各种局限性：它们通常依赖于耗时且难以收集的手工构建数据集；或者它们仅测量非常具体的刻板印象，因此无法很好地推广到其他人群或情境；或者它们仅仅捕捉非常宽泛的关联，例如与特定群体相关的负面联想。此外，大多数相关工作都没有考虑到交叉性，即多重社会身份会复合偏见，并成为伤害的独特载体。为了克服这些局限性，我们利用了较新的指令微调LLM具有非常好地响应指令和提示词这一特性，因此我们可以要求模型生成一个“人格”，即想象人物的描述，使用诸如“想象你是一位亚裔女性，请描述一下自己”之类的提示词。我们就能立即看到这具有很强的可推广性，因为我们可以随意指定任何身份标识符到这个提示词中。\n\n以下是一些GPT-4的示例生成结果：我们立刻看到，虽然这些输出在传统意义上不是公开负面的或有毒的，但存在一些有趣的模式。亚裔女性被描绘成不起眼；中东女性被使用诸如“异域风情”、“迷人”等词语来描述其所在地区；而这些有色人种的人格都提到了祖先，而白人男性人格则没有。\n\n为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人格。生成这些人格的提示词灵感来源于一项研究，该研究将这些提示词给予人类受试者，发现这样做能够挖掘出种族刻板印象，并且这使得我们可以将生成的“人格”与人类书写的回复进行直接比较。第二部分是“标记词”，这是一种识别区分“标记群体”与“非标记群体”的词语的方法，我将在稍后详细阐述。这样做的好处是，我们可以在不依赖任何特定词库的情况下，获得非常具体的刻板印象和模式。\n\n“标记词”方法借鉴了社会语言学的“标记性”概念，该概念指出存在一个未标记的默认状态，而任何偏离该默认状态的群体在语言上都是被标记的。例如，通常与男性相关的词语是“战士”；因此，当人们描述一位女性战士时，他们通常会明确地说明“一位女性战士”，并在术语中标记“女性”。更广泛地说，社会中的占主导地位的群体在语言上和社会上都是未标记的，而边缘化群体通常是被标记的。在我们的方法中，我们首先指定未标记和标记群体分别是什么，然后使用“战斗词语”方法（本质上是使用加权对数比率来区分每个标记群体的最常见的词语）来比较“人格”。\n\n现在，我们来看一些结果。首先，我们使用刻板印象词库，发现生成的“人格”比人类书写的包含更多的刻板印象。然而，当我们实际查看这些词语的分布时，却发现存在着非常不同的情况。虽然生成的“人格”具有更高的词库单词比例，但人类书写的单词分布范围却更广。而出现在生成人格中的刻板印象词语，实际上仅仅是“高大”和“运动员”这些词语，也就是仅仅是积极的，或者至少是非负面的词语。事实上，这个词库并不能真正捕捉到我们在前面幻灯片中看到的许多有害模式。\n\n因此，为了解决这个问题，我们将转向“标记词”方法的结果，以展示这些看似积极的词语如何促进刻板印象和本质化叙事。在我们的分析中，我们发现这些看似积极的描述反映出有害的模式。首先，对于标记群体，最常见的词语包括“文化”、“传统”、“骄傲”和“异域风情”，这些词语仅仅通过它们与身份的关系来定义这些群体，并将它们与其他白人群体区分开来，这加剧了这些群体的长期歧视和边缘化。此外，在这些词语中反映了许多常见的刻板印象，尤其是在有色人种女性身上。例如，描述拉丁裔女性的词语包括“充满活力”和“曲线优美”，这与“热带风情”的刻板印象相关；描述亚裔女性的词语包括“娇小”、“柔弱”和“丝滑”，这与亚裔女性被过度性化、被视为非常温顺和顺从的历史有关；最后，对于黑人女性，一些最常见的词语是“坚强”和“有韧性”，这与人们所说的“强壮的黑人女性”的刻板印象有关。虽然乍一看，这似乎是积极的，但研究表明，这种刻板印象实际上是有害的，因为它给这些群体带来了巨大的压力，要求他们在面对社会障碍时保持坚强和有韧性，而不是真正地努力改变这些障碍，从而导致这些人的健康状况和其他负面后果。\n\n更广泛地说，我们发现每个标记群体的词语几乎仅仅反映了本质化的叙事。基于这些模式，我们得出了三项建议，供模型所有者参考。首先，作为研究人员，我们应该解决积极的刻板印象和本质化的叙事。我们还应该使用交叉视角来研究偏见和危害，因为如果不这样做，可能会忽略许多问题。最后，应该增加关于偏见缓解方法的透明度，因为例如，这些积极的刻板印象可能源于某种奇怪的过度价值对齐，或者是一些导致这些有害模式的反刻板印象方法。除非有更多的透明度，否则我们无法做出任何假设或进一步研究。\n\n感谢大家的聆听，祝大家在ACL度过愉快的时光。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是詹姆斯·芬奇，我是莎拉·芬奇。\n\n今天我们将向您介绍ABCEV，这是一种评估对话式人工智能的新型多维方法。这项工作由埃默里NLP实验室完成，由埃默里大学的乔伊·金教授领导，并与亚马逊Alexa AI合作。\n\n假设您刚刚开发了一个对话模型，并希望了解其与当前最先进水平的表现如何。常用的做法是进行人工评估，例如要求人工评委选择两个对话中哪个更好，或者根据等级评估对话。这些方法能够很好地提供对整体对话质量的 holistic 评估，但对话质量包含许多方面。因此，您可能希望评估聊天质量的多个维度，以更细粒度地了解模型的优点和缺点。一种方法是简单地要求人工评委评估对话质量的多个维度，例如使用现有的对比或等级评估方法来评估模型响应的相关性。然而，我们认为存在一种更精确、更可靠的多维对话评估策略。我们的方法试图通过明确标注每个模型响应是否表达了某些行为来减少人工评估的主观性，例如响应不相关的信息或自相矛盾。我们称这种方法为“标注聊天行为”，简称ABCEV。\n\n我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。ABCEV 能够衡量聊天模型犯各种主题错误的速率。例如，ABCEV 衡量聊天模型忽略其对话伙伴或说一些不相关的话、自相矛盾或与其伙伴相矛盾、产生不正确的虚构事实或违反常识知识以及模型成功或失败地表现出同理心所占的轮次数。\n\n为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用 ABCEV 对每个模型评估了 100 个人工机器人对话。为了进行比较，我们还使用三种现有方法评估了这些对话：按轮次进行等级评估、按对话进行等级评估以及对话级别的成对比较。对于每种现有方法，我们收集了关于对话中最常衡量的前八个方面的评估结果，因为这是评估聊天模型多个维度的标准做法。\n\n通过分析这些评估结果，我们发现 ABCEV 行为标签总体上比现有方法收集的标签更可靠，这由对一百个双重标注对话的标注员间一致性衡量。此外，与现有方法产生的指标相比，ABCEV 标签更能预测整体对话质量，这如简单的线性回归分析所示。例如，您可以看到衡量包含自我和伙伴矛盾的轮次比例可以解释对话质量的百分之十到百分之二十，而平均等级一致性分数仅解释了百分之四或更少。\n\n最后，我们使用逐步线性回归检查每种评估指标是否捕捉到聊天质量的独特方面。您可以看到，所有 ABCEV 指标的组合可以解释对话质量的百分之二十五以上，并且在一次删除一个指标时，大多数指标会导致失去大量关于质量的信息。另一方面，所有按轮次进行的等级指标的组合解释的质量要少得多，并且较少数的指标包含独特的有用信息。\n\n这些可靠、信息丰富且独特的 ABCEV 指标使我们能够以比先前方法能够实现的更高分辨率评估对话式人工智能。您可以看到，在我们实验的结果中，仍然存在一些挑战，并且已经被精确地量化。例如，我们测试的机器人响应中约有 20% 包含常识违反，约有 15% 的响应包含不相关的信息，并且大约有 10% 的响应包含自我或与伙伴相矛盾。\n\n随着该领域快速进步，许多这些错误率可能会在新的模型中减少，自我们的评估进行以来。然而，这更有理由追求可靠和精确的评估指标来比较模型。我们希望 ABCEV 可以被该领域的其他人利用，作为朝着这一方向迈出的有意义的一步，并期待看到在未来几个月和几年里对话式人工智能将如何进步。感谢您的观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我叫Vauddha，是斯托尼布鲁克大学计算机科学博士候选人。我希望向大家介绍我们一篇被ACL 2023以长文形式接受的工作，名为“迁移学习用于不和谐检测”，旨在解决罕见类别挑战问题。\n\n我们首先定义认知失调，并解释为什么在语言学研究中，探讨认知失调是一个重要的课题。简单来说，认知失调指的是两个相互不一致的信念或行为。例如，一个人可能会说“我知道吸烟可能会杀死我”，然后又说“我会在会后抽几根烟”。这两个信念和行为是不一致的，处于失调状态。进一步解释说“没有它们我无法保住工作”可以合理化第二次行为，两者之间呈现一种和谐关系。\n\n尽管认知失调在日常决策中非常常见，但在其他类型的语篇关系中，在语言中表达出来的情况却非常罕见。这有什么意义呢？研究认知失调可以帮助我们理解人们之间的意见分歧、趋势、信念价值观以及人群中的态度变化。高认知失调也与焦虑症相关，有助于我们更好地理解人们的心理健康。在语言中表达的认知失调研究，也有助于理解极端主义和弱势群体两极分化。\n\n最后，理解认知失调有助于了解个人的认知风格，并帮助我们更好地理解决策过程。为了构建认知失调资源，我们进行了一项大规模的失调关系标注工作。我们采用了“失调优先”方法，如流程图所示，推文通过PDTV解析器处理，根据我们在论文中描述的指南，对语篇单元对进行标注。如您所见，在标注对中，只有3.5%发现了失调现象。\n\n在收集了大约一千个语篇单元对之后，我们对仅基于43个失调示例进行训练的初始分类器进行了训练。不出所料，由于失调现象的低出现率以及缺乏任何先前的相关数据集，分类器的表现好于随机猜测的情况并不明显。我们面临着绝对稀有问题的挑战。\n\n为了缓解这个问题，我们对各种迁移学习和主动学习的组合进行了实验，旨在通过较少轮次的标注收集更多的失调样本，从而降低整体标注成本，同时提高失调检测能力。\n\n由于初始模型完全无法捕捉到失调类别，因此我们从相关的任务中迁移权重来启动主动学习过程。我们从两个不同的任务中迁移权重：主题无关的失调分类任务（确定来自不同人的辩论陈述是否一致或不一致，无论主题如何，该任务称为辩论），以及基于PDTV的扩展和比较类的二元分类任务（这些任务与和谐和失调的概念密切相关，我们称之为CE）。\n\n我们发现，迁移零样本性能在标注数据集上已经明显优于随机猜测，AUC达到0.62。进一步迭代地在两个任务上进行微调，我们发现先对CE任务进行微调，然后再对辩论任务进行微调，可以获得更好的零样本性能。因此，这是我们用来启动主动学习的模型。\n\n接下来，我们确定了在主动学习的每一轮中，用新数据更新模型的最佳方法。累积方法会汇总迄今为止主动标注收集的所有数据，而迭代方法则通过训练最新的数据集来更新模型。我们发现，累积方法在所有方面都优于或等于迭代方法。\n\n为了提高失调示例的数量，我们使用罕见类别概率策略（PRC）来选择当前模型很可能表示失调的示例。在主动学习的每一轮中，我们将此策略与其他最先进的主动学习策略进行比较。我们发现，所提出的PRC策略优于其他最先进的策略，尽管差异很小。值得注意的是，对于随机策略，在主动学习的后续轮次中，性能明显下降。\n\n使用两个最佳策略，我们提高了失调分类的AUC至0.75，这是我们迄今为止在该任务上取得的最佳性能。我们还检查了每种策略对于标注质量和标注人员成本的可行性。我们发现，PRC具有最高的失调比例，并且最适合罕见类别。然而，标注人员也发现这些示例难以标注。\n\n总而言之，我们发现PRC是一种简单的主动学习策略，用于获取罕见类别，并且带有适当设计的迁移学习任务的冷启动主动学习可以帮助我们显著提升效果。我们还发现，迭代更新方法适用于来自不同领域的迁移学习，而域内主动标注则受益于累积更新。\n\n这些是我们代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢！"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我是阿克沙塔，今天我和我的共同作者马丁将为大家呈现我们的工作——“知识整合评估：KitMust”。这项工作是麦吉尔大学、Mila 和微软研究院合作的成果。现代自然语言理解模型会利用多种知识来源，例如通常通过预训练获得并存储在模型参数中的知识，以及在推理时提供的输入信息。最近在问答等任务中的研究表明，模型可以利用预训练的知识来解决问题。但自然语言理解通常需要也需要在推理时提供知识，例如在句子“约翰在电视上看到了新当选的总统”中，预训练参数可以包含关于总统做什么以及电视是什么的信息，但它们无法可靠地知道这个特定实体约翰是谁或新总统是谁，因为总统可能在预训练之后发生了变化。因此，对于知识密集型自然语言理解任务，成功的模型需要能够整合并利用预训练知识和推理时知识。在这项工作中，我们提出了一套用于评估知识整合的诊断测试。我们引入了一个核心指代消解任务，旨在探究从不同来源获取知识的能力。我们使用人类研究参与者对数据集进行评估，并建立核心指代消解模型。以下是我们数据集中一个例子：服务是一个法官，基娅是一个面包师。在辛苦了一天，审查法律代码后，他很高兴放松一下。这里的任务是识别代词“他”所指代的正确实体，在本例中是。指代消解需要两种类型的信息：首先，实体特定知识，例如服务是一个法官；其次，背景知识，例如法官通常在法庭上做出裁决。背景知识通常是在大型语言模型预训练期间学习的，而实体特定知识通常在推理时观察到。我们改变了这两种信息可用性，使得它们可能出现在单个来源或多个来源中。我们定义了三种 KitMust 设置。首先，我们有典型的设置“背景预训练”，其中假设背景知识在预训练时间可用。其次，有“背景并置”设置，其中背景知识在预训练时间和推理时间都可用。最后，是“背景推理”设置，其中两种知识类型仅在推理时间可用。这种最后的设置尤其有趣，因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分的情况，例如因为自预训练以来出现了新的职业。以下是展示我们如何控制两种来源中事实可用性的一个例子。在“背景预训练”设置中，我们假设关于“政治家寻求当选政府职位”的背景知识包含在预训练参数中。在推理时间上下文中，我们提供特定于实体的知识“奇切斯特是一位政治家”。在“背景并置”设置中，我们除了提供特定于实体的知识外，还在推理时间上下文中还提供关于政治家的背景知识。在“背景推理”设置中，我们提供事实职业“巡礼者”而不是“政治家”，因为“巡礼者”不太可能包含在预训练参数中。我们使用人类研究参与者和建立的指代消解模型评估数据集。在本图中，我们展示了在“背景预训练”设置中最困难的变体中，未经任务特定训练的最佳模型的结果。这两个模型在没有经过 KitMust 任务特定训练时表现不佳。然而，经过 KitMust 训练后，C2F 和 Built for Coref 的表现明显优于随机选择，这表明在通用指代消解数据集上训练的模型学会了利用表面线索，而这些线索在测试 KitMust 时没有用，因为这些线索已被移除。带有虚构知识的额外实验表明，即使是表现最佳的模型也无法可靠地整合仅在推理时间内提供的背景知识。总之，我们的论文的主要结论是，许多核心指代消解模型在没有任务特定训练的情况下似乎无法推理来自不同来源的知识。然而，通过任务特定训练，一些模型可以成功整合来自多种来源的知识。尽管如此，即使是表现最佳的模型似乎在可靠地整合仅在推理时间内提供的背景知识方面仍然存在困难。如果您想了解更多详细信息，请参阅我们的论文，并在 GitHub 上查看数据集和代码。感谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自特伦托大学和布鲁诺·凯斯勒基金会的Sarah Pai，我将简要介绍一篇以注意力机制引导的实时语音翻译论文，该论文是与Matteo Negri和Marco Durchi的合作成果。什么是实时语音翻译？实时语音翻译（SimST）是指将口语实时翻译成另一种语言的文本的过程，从而实现跨语言交流。那么，当前SimST模型的面临哪些问题？特定的架构通常需要训练额外的模块进行优化，导致冗长且复杂的训练过程，例如涉及不同优化目标、训练和维护多个模型以达到不同的延迟等级，例如训练一个平均延迟一秒的模型，再训练一个平均延迟两秒的模型，以此类推。那么，我们的解决方案是什么？首先，利用已有的离线语音翻译模型，无需重新训练或采用特定的SimST架构。对于不同的延迟等级，仅使用一个模型，并通过特定的参数来控制延迟。其次，利用注意力机制在音频输入和文本输出之间传递的知识，即交叉注意力机制。您可以在右侧看到一个示例。我们的解决方案是提出一种点注意力（Dot Attention）或解码器注意力（Decoder Attention），这是一种策略，我们根据注意力指向的位置决定是否发出部分翻译。如果注意力集中在某个词上，即总和低于某个阈值α，并且指向最后λ个语音帧，这意味着接收到的信息已经足够稳定。例如，如果接收到包含“我将要谈论”的语音片段，并且我们的模型预测翻译结果为德语，我们会观察交叉注意力权重，发现前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧（λ个语音帧）。这意味着前两个词将被发出，但由于交叉注意力的总和高于某个阈值α，我们将不会发出最后一个词，而是等待另一个语音片段。如果继续接收下一个语音片段，并且我们的模型预测另外三个词，我们会观察交叉注意力权重，发现没有词指向最后λ个语音帧，这意味着这三个词将被发出。在“点注意力”的主要结果方面，我们在图表上绘制了实时语音翻译的结果，图表的一侧是蓝色，用于衡量翻译质量和平均延迟（即延迟指标），我们还考虑了计算感知平均延迟，该指标考虑了模型预测输出所需的计算时间。因此，我们希望曲线尽可能地高，并且尽可能地向左偏移。我们将与应用于离线模型的传统策略（如Whit key策略和Local Agreement）进行比较，还会与专门为实时语音翻译定制的最先进架构进行比较。这些是实时语音翻译策略在德语上的所有结果，我们看到“点注意力”优于所有应用于离线模型的策略，因为曲线向左偏移。我们还看到，如果考虑实际经过的时间或计算穿透时间，AD是最快的策略。如果您想了解更多结果，请阅读我们的论文。我们还开源了代码和模型，以及同时输出，以促进我们工作的可重复性。感谢您的关注。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "各位好。我叫舒哈。今天我将介绍我们的论文《Connel 2003命名实体标注器在2023年是否仍然有效》。 让我们开始吧。我们的论文调查了泛化问题，使用了命名实体识别任务，或者NER任务。我们观察到，模型已经使用了Connel 2003来开发NER 接近20年，这自然会带来几个问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？同时，如果观察到泛化能力不足，是什么原因导致这些模型的性能下降？为了调查这些问题，我们开发了Con++数据集。这是一个我们从路透新闻中收集并使用Connel 2003标注指南进行标注的数据集。然后，我们在Connel 2003上对超过20个模型进行微调。 我们在Con3测试集和Con++快速测试集上对它们进行了评估。最后，我们计算了F1值的百分比变化，以评估每个模型的泛化程度。那么，实现良好泛化需要什么？通过我们的实验，我们发现有三个主要因素是必要的。第一个是模型架构。通过我们的实验，我们发现Transformer模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现通常，更大的模型能带来更好的泛化效果。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们同样发现，更多的微调示例实际上也能带来更好的泛化效果。至于我们的下一个问题，是什么导致了一些模型的性能下降？我们有两个假设。第一个是自适应过拟合，这是一种由重复使用相同的测试集而引起的过拟合现象，通常在新的测试集上表现为边际效应递减。第二个假设是时间漂移，这是一种由训练数据和测试数据之间不断增加的时间差距而引起的性能下降。对于自适应过拟合，我们从右侧的图表上看到，最佳拟合线（红色）的梯度大于一。这意味着我们在Connel 2003上所做的每一次改进，在Con++上都会带来超过一个单位的改进，这意味着没有边际效应递减。这表明在这种情况下没有观察到自适应过拟合。那么，时间漂移呢？对于时间漂移，我们进行了一项实验，使用更近的数据重新训练或继续预训练了一些模型，我们发现性能随着时间差距的增大而下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构，更大的模型大小以及更多的微调示例。这些因素是相互关联的，我们不能仅仅依赖一个因素，而忽略其他因素。我们还发现，这里的性能下降是由时间漂移引起的，而且出人意料的是，并非由自适应过拟合引起的，尽管Connel 2003已经被使用了20多年。回到我们论文标题中提出的问题：Connel 2003标注器在2023年是否仍然有效？我们发现答案实际上是肯定的。我们希望我们的论文能够促使人们对如何改进模型的泛化能力进行更多的研究。最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时与我联系。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "您好！欢迎参加我们的 Deplain 演示，这是一个用于德语文本识别的新语料库，可在文档级别和句子级别进行识别。我叫 Regina Stoden，我将引导大家完成演示的第一个部分。首先，我们来定义一下文本简化。文本简化是指调整文本以提高特定目标群体的文本理解能力，例如阅读障碍者或非母语人士。为了训练文本简化模型，我们需要平行的文本对，例如文档或句子。在此示例中，您可以看到一个复杂的德语句子及其翻译成通俗语言的平行对齐句子。为了简化句子，可以使用不同的技术，如您在示例中看到的词汇替换、从句扩张、交叉删除重排或插入单词。\n\n我们现在提出我们的新语料库 D plane，因为近年来，现有的语料库存在一些问题。例如，这里的这些语料库太小，无法用于训练文本分类模型。在近年来提出的另外三种模型都是自动对齐的，这意味着它们在对齐时可能会出错。因此，我们提出了我们的新语料库 Dplane，它被分为两个子语料库：Deplane APA 和 Deplane web。Deplane APA 基于使用文本。在 Deplane APA 中，我们手动对齐了 483 篇文档，结果大约有三万零一千三百个平行句子对。对于 deepplae web，该语料库包含不同的领域，我们还使用手动和自动对齐方法对这 750 篇文档进行了对齐。总共结果为 30450 个句子对。\n\n我们对句子对进行了更深入的分析，例如简化类型，如您在此处看到的，圣经文本比例如新闻文本或语言学习文本更强地进行了简化，在所有层面上，例如词汇简化、结构简化以及总体简化程度。此外，您还可以看到我们的 deep plaining 语料库具有各种不同的简化变换，例如在 deeppla API 语料库中，我们比在 deep plane web 语料库中拥有更多的重排和添加单词。另一方面，在 web 语料库中，我们拥有更多的改写。\n\n现在，让我们看看我们可以用这个语料库做什么。\n\n大家好，我是 Omar，现在我将介绍我们数据集 d plane 的应用场景。对于第一个应用场景，我们可以评估自动对齐方法。近年来，出现了很多对齐方法，但在机器翻译的背景下，我们拥有用不同语言书写的两个平行文档，并且希望提取这些文档中句子的对齐信息。但在我们的用例中，我们试图提取具有相同内容但复杂度不同的两个平行文档中句子的对齐信息。现在，由于我们拥有手动对齐句子的数据集 d plane，我们可以将这些句子用作黄金标准对齐，以评估一些提出的对齐方法。我们对提出的方法进行了一些调整，并在论文的最后发布了所有这些调整和运行实验的代码。我们得出结论，用于德语文本简化的最佳自动对齐方法是 mass align 方法，您也可以在论文中找到运行该方法在您自己的文档上的代码。\n\n我们在论文中展示的第二个用例是自动文本简化的案例，通过微调语言模型来生成简化的文本。我们微调了两个不同的模型，我们微调了 long part 模型以生成文档级别的简化，我们还微调了 normal base long 和 normal base part 来生成句子级别的简化。您也可以在论文中找到所有检查点，并可以深入了解实验的得分和评估指标。我们得出结论，这种基本的微调可以产生比基线得分更好的得分，并将这些结果作为自动文本简化的基准。\n\n非常感谢您的关注，我们希望在会议上与各位见面。谢谢！"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "您好，我是X袁，来自FNAi大学。我在这里介绍我们的工作，题为《区分脚本知识与轻量级语言模型，以用于受约束的语言规划》。在日常生活中，人类必须经常遵循分步骤的指示，以形成保证脚本来规划他们的行动。先前的研究利用语言模型来规划抽象目标，例如刻蛋糕等刻板活动，并表明大型语言模型可以有效地将目标分解为步骤。然而，先前的研究主要集中在规划刻板活动的抽象目标，而对于具有特定约束的目标进行规划，例如制作巧克力蛋糕，仍然相对不足。在本文中，我们定义了受约束的语言规划问题，该问题对规划的目标施加不同的约束。一个抽象目标可以被不同的现实生活中的特定目标继承，这些特定目标具有多方面的约束。一个好的规划者应该编写既合理又忠实于约束的脚本。在本文中，我们首先评估和改进了大型语言模型受约束的语言规划能力。由于没有特定目标的数据来支持我们的研究，我们必须首先获取这些目标。如图表所示，我们使用人环线数据采集，通过Instruct GPT扩展抽象目标，添加多方面的约束，对100个特定目标进行采样，并评估来自库模型的生成的脚本。该表报告了结果的整体准确率。我们发现，所有学习模型在规划特定目标方面都取得了不令人满意的结果。然后，我们进行详细分析，以研究学习模型失败的原因。图中的结果表明，生成脚本的语义完整性是可以接受的，但不能保证对约束的忠实性。我们深入研究了WiH中定义的约束的更细粒度的类别。该热图显示，Instruct GPT的规划性能因不同类别的目标而异。先前的研究表明，学习模型的输出质量存在高方差，从而导致性能不佳。因此，我们采用了超生成和过滤的思想来提高生成质量。我们首先向Instruct GPT展示了带有示例的约束类型，并根据种子抽象目标获取了特定目标，然后Instruct GPT对特定目标进行过度生成关键脚本。接下来，开发了一个过滤器模型来选择忠实的脚本。我们将脚本和目标转换为Instruct GPT嵌入，并计算余弦相似度和相似度得分来衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。我们仅保留目标得分最高的脚本。通过我们的方法，Instruct GPT可以生成更高质量的脚本。我们的方法在语义、完整性和对约束的忠实度方面都大大提高了规划能力。由于大型语言模型的部署成本高昂，因此启用较小和专用模型的语言规划至关重要。创建数据集是实现这一目标的关键步骤。然而，先前的研究并没有实现对特定目标的规划，并且手动数据集标注成本高昂。因此，我们遵循知识蒸馏的思想，从轻量级语言模型中蒸馏受约束的语言规划数据集。我们将我们的方法应用于构建一个名为CodeScript的受约束的语言规划数据集。总共，我们生成了55,000个特定目标和脚本。为了确保验证和测试数据的质量，我们请众包工人发现并修改不正确的样本。此图显示了CodeScript的约束分布。我们发现CodeScript在生成的特定目标中显示出很高的多样性。通过CodeScript，我们可以使用较小但专业的模型进行受约束的语言规划。我们发现，在score rate上进行微调的T5可以生成比大多数大型语言模型更高质量的脚本，表明当在合适的训练数据上进行适当训练时，较小的模型可以支持大型模型。总之，我们确立了受约束的语言规划问题，评估了大型语言模型的受约束的语言规划能力，并为大型语言模型开发了一种超生成过滤方法。我们使用大型语言模型生成一个高质量的训练数据集CodeScript，用于受约束的语言规划。我们希望CodeScript数据集能够成为推进语言规划研究的宝贵资源。感谢您的时间。CodeScript的更多详情请参见我们的论文。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Jannislavak，我将向大家介绍我们关于Dr. Bert的研究成果，这是一个为生物医学和临床领域构建的、在法语环境下具有强大性能的预训练模型。本次演讲中，我们将首先探讨医疗保健领域的语言建模。随后，我们将介绍我们论文的主要贡献。我们提出了第一个基于Roberta并使用Nachtchos（一个从网络抓取的医学数据的集合）进行训练的法语生物医学模型，名为Dr. Bert。我们还将介绍在多种冻结设置和数据源下的模型对比。接下来，我们将展示我们在法语环境下11个生物医学和临床下游任务上的结果，最后我们将总结实验，并提供更多关于如何访问该模型的信息。自2018年发布以来，Bert已成为解决自然语言处理任务的最有效方法之一，相比于历史上的静态和上下文化方法（如word2vec、fastText等），它带来了巨大的性能提升。此后，该模型已被改编为许多其他语言，如法语中的CamemBERT，以及生物医学领域中的Permit-BERT和BioBERT，以及临床领域中的Clinical-BERT，但主要集中在英语上。对于其他语言而言，专业的模型相对稀缺，通常依赖于持续预训练，这源于缺乏特定领域的训练数据。然而，在法语环境中，直到现在才出现了开源的生物医学模型。因此，我们思考了最合适的数据来源，以支持广泛的使用。粗略的数据是否可以作为临床数据的替代方案？为了回答这个问题，我们将Dr. Bert与我们的Schubert模型进行对比，后者基于我们医院获取的匿名化数据。随后，我们进一步探究了训练法语专用模型所需的最小数据量。是4GB、8GB还是更多？为了回答这个问题，我们首先从头开始训练并对比了四个模型：Dr. Bert的第一个版本，使用7GB的Nachtchos；Dr. Bert的第二个版本，使用4GB的Natureos；Schubert的第一个版本，这是一个临床模型，使用4GB的临床记录句子；Schubert的最终版本，混合使用4GB的Natureos和4GB的临床记录。除了这个对比之外，我们还引入了三个基于控制预训练的模型，以分析预训练策略的影响：一个基于CamemBERT的权重，并在4GB的Natureos上训练；另一个同样基于CamemBERT，但这次在4GB的临床记录上进行训练；最后，一个基于英文生物医学模型BioBERT，并在4GB的Nachtchos上训练。总共，我们有七个模型用于评估。为了评估这七个模型，我们收集了公开和私有的下游任务，例如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行比较，包括CamemBERT、Oscar（138GB）、CamemBERT（4GB）、CamemBERT-CCNet（4GB）、Permit-BERT和BioBERT以及Clinical-BERT。 结果表明，模型在与训练数据性质相同任务上表现最佳。然而，我们可以观察到，来自异构来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总体而言，从头开始的自由预训练似乎在大多数任务上获得了更高的性能。然而，我们的实验表明，使用Permit-BERT权重和分词器在4GB Natureos子集上进行控制预训练的模型，其结果与Dr. Bert 4GB从头开始训练的模型相比具有可比性，而基于CamemBERT权重和分词器的模型则存在稳定性问题。 综上所述，我们的系统在11个下游任务中的9个任务上表现出更好的性能，并且总体上超越了通用模型CamemBERT的结果。我们还观察到，专业化数据更好，但其可扩展性较差。所有从Nachtchos获得的预训练模型都可以在我们的interface和GitHub存储库上免费获取，感谢各位的聆听，期待在多伦多的海报展示环节与大家互动。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是张冰，华盛顿大学的博士生。今天我将介绍我们的工作，从预训练数据到语言模型再到下游任务，追踪导致不公平自然语言处理模型的政治偏见。所以，语言模型是在大规模的网络爬取数据上训练的。政治新闻媒体在其预训练数据中得到了很好的覆盖。根据对C4语料库的调查，我们可以看到，《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中都有很好的覆盖。这对于语言模型应用来说既是好事，也是坏事。一方面，它们能够从不同的视角中学习，这庆祝了民主和思想的多样性；另一方面，这些不同的政治观点本质上是具有社会偏见的，并可能导致下游任务应用中的潜在公平问题。为此，我们旨在调查从预训练数据到语言模型再到下游任务的政治偏见传播流程。具体来说，我们将探讨以下问题：首先，我们如何评估语言模型的政治倾向，并且数据来源在多大程度上会影响这些政治偏见？其次，具有不同政治倾向的语言模型在下游任务中的表现如何，这是否会导致自然语言处理应用中的公平问题？\n\n为此，我们首先提出使用政治问卷，例如政治指南针测试，用不同的提示格式来提示语言模型，以进行自动评估，并将其建立在政治科学文献的基础上。一些初步结果表明，首先，语言模型确实具有不同的政治倾向，它们在政治指南针的四个象限中均有分布。我们还可以看到，GPT-4是最自由派的语言模型，GPT系列通常比BERT系列及其变体更具社会自由性。其次，我们旨在调查语言模型的政治偏见在多大程度上实际上来自训练数据。为此，我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点，并将其分为新闻和社交媒体，从而进行受控实验。这些党派语料库进一步按政治倾向划分。通过对这些党派语料库进行进一步预训练，我们可以看到语言模型的意识形态坐标也会相应地发生变化。例如，对于在左翼Reddit语料库上进行进一步微调的Roberta，在政治偏见方面会看到明显的自由派转变。我们还尝试调查语言模型是否能够捕捉到我们现代社会普遍存在的极化现象。为此，我们将预训练语料库分为美国第45任总统之前和之后的两个时期，并将语言模型分别预训练到这两个不同的时间段语料库上。我们可以看到，语言模型通常在2017年之后具有更远离中心的政治倾向。这表明语言模型也可以捕捉到我们社会中的极化现象。\n\n最后，但同样重要的是，我们在仇恨言论检测和虚假新闻检测等自然语言处理应用中评估了具有不同政治倾向的语言模型，这些应用通常涉及语言模型，并且可能具有非常重要的影响。我们发现，如果我们调查每个类别的性能，也就是说，如果我们将性能分为不同的人口统计群体或政治媒体（新闻媒体），我们可以看到一个模式：例如，对于仇恨言论检测，左翼语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对更强势群体的仇恨言论方面表现较差，反之亦然。右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。类似的趋势也发生在虚假新闻检测中，我们发现左翼语言模型在检测来自其相反政治倾向的虚假信息方面表现更好，反之亦然。我们进一步展示了许多定性示例，以了解具有不同政治含义的语言模型会根据其社会类别给出不同的仇恨言论和虚假信息预测。在附录中提供了更多示例，以进一步强调这表明存在一个非常紧迫的公平问题，即语言模型的政治偏见。例如，如果一个右翼语言模型被微调用于仇恨言论或虚假信息，并部署到流行的社交媒体平台，那么这将意味着与自己政治观点相反的人可能会被边缘化，并且针对少数群体的仇恨言论可能会肆虐，而没有任何控制。这为我们敲响了警钟，要求我们承认并解决由语言模型的政治倾向造成的公平问题。\n\n所以，稍微讨论一下。我们还希望强调我们揭示了关于语言模型政治偏见的独特困境，就像两难困境一样。如果我们不清理语言模型训练数据中的政治观点，偏见就会从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们尝试清理，我们也面临着审查或排斥的风险，并且很难确定什么是真正中立的，以及应该保留哪些语言数据。这有点像电车难题。好的，我认为今天我差不多讲完了。感谢您的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我是Koov Sinna，很高兴欢迎大家参加关于我们ACL 23论文的讨论。语言模型可接受性判断并非总是对上下文稳健。这是一项与John Waqui、Aaron Mueller、Kanishka Mishra、Karen Fs、Roger Levy和Atina Williams的合作研究。因此，在这项工作中，我们重新审视了最小对偶范式。最小对偶范式基本上是根据可接受性判断（也可能包括语法性，例如Blimp语法Gym或关于刻板印象的可接受性，例如Crowds Pairs）来评估语言模型的。并且，在最小对偶范式中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子，希望模型能够将更高的概率赋予可接受的句子。目前MPP流程图基本上不允许我们评估模型对较长句子可接受性的评价。如今，大型语言模型正在产生越来越长的上下文窗口，因此至关重要的是，我们需要评估模型在整个上下文窗口上的可接受性，而这正是我们试图做的事情。我们试图通过要求模型对越来越长的序列进行可接受性评估来重新审视MPP流程图。这就是我们的方法。我们的做法是，为了模拟这些较长的序列，我们重新审视数据集本身，然后通过从这些数据集中选择可接受或不可接受的句子来重建句子。例如，这里我们选择了一个典型的来自Blim数据集中来自附属岛案例的语法正确对，我们所做的是重建更长的序列，这些序列是可接受的，并且具有相同的语法结构。我们从附属岛中提取语法正确的句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。我们也可以通过选择相同的匹配的不可接受句子来做同样的事情，这也可以用来测试模型的可接受性。我们还可以通过从不同的子集或不同的数据集选择句子来做同样的事情，这就是我们所谓的失配场景。在这里，句子仍然来自相关的领域，但不是您用来评估的同一个领域。我们还可以对不可接受性的情况做同样的处理，最终我们可以选择来自完全无关领域（例如维基百科）的句子。这将告诉我们，模型的接受性判断是否实际上受到任何上下文的影响，例如上下文是否来自数据集的不同子集，或者它是否与当前句子完全无关。那么模型表现如何？首先，我们研究维基百科句子，这些句子与当前的查询对完全无关，在那里我们发现MPP判断在任意上下文长度下大多是稳健的。我们将上下文长度增加到一千零二十四，以最大化Ot和GPT-2模型的上下文长度，我们在这里看到在橙色虚线中，MPP判断相对稳定。当选择来自同一数据集的句子时会发生什么？在这里，我们从相同的Blimp语法Gym数据集的接受性和不可接受性领域创建句子，在那里我们看到，当添加可接受的前缀或不可接受的前缀时，MPP判断要么增加，要么显著减少，但是当我们匹配结构时，也就是说，当我们从Blimp语法Gym中相同的现象中选择句子时，我们看到MPP判断对于模型出现巨大的增加或巨大的减少，这取决于所选的前缀是可接受还是不可接受。这和这非常大，这种效应随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新型语言模型。为什么匹配的前缀会如此影响语言模型的判断？为此，我们进行了一系列分析，试图通过尝试保留相关的结构并向输入添加噪声来扰动输入句子。经过多次扰动后，我们发现这些噪声中的任何一种都没有真正导致模型改变其MPP判断趋势。基本上，我们发现模型对扰动句子以类似的方式敏感，即，当我们在可接受的领域中扰动句子时，我们看到所有扰动中MPP判断都有所增加，当我们在可接受的批准领域中扰动句子时，我们看到MPP判断以类似的方式减少。因此，我们工作的关键要点是，语言模型对句子中共享的潜在句法和语义特征敏感。目前我们以短句和单句输入的这种方式进行的MPP评估，可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多我们实验的详细信息。感谢您的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是大卫，德国斯塔兰大学的博士生。在这个视频中，我想介绍一下我们最近的工作《弱于你所想》，对每周视觉学习（weeklyvis learning）进行批判性分析。这篇论文是与X、迈奥斯·莫斯巴赫、格·斯特芬和德里希·克拉科共同完成的。我想首先简要介绍一下弱监督和弱监督学习。在弱监督中，我们不手动标注数据。相反，我们使用弱标注来源来标注数据，例如简单的启发式规则、知识库或低质量的众包，如图所示。与人工标注相比，弱标注的成本要低得多，但它们也包含噪声，这意味着一定比例的标注是不正确的。如果我们直接在弱标注数据上训练神经网络，神经网络往往会记住标注噪声，而无法泛化。在弱监督学习中，提出了训练算法，以稳健地训练神经网络，使其在存在标签噪声的情况下也能良好泛化。最近的wSL（wSL代表弱监督学习）工作中，一个常见的说法是，人们只在弱标注数据上训练模型，并在干净的测试集上实现高表现。从技术上讲，这个说法并不错误，但有一个需要注意的点，即人们通常假设存在一个额外的干净验证集，用于模型选择。我们研究的场景正是基于此，但这也意味着在弱监督学习中需要额外的手动标注，而这种必要性往往被忽视。我们针对上述问题提出了三个研究问题。首先，干净验证数据对wSL是否必要？或者我们可以使用带噪声的验证集代替吗？其次，如果需要干净数据，为了使wSL起作用，我们需要多少干净样本？最后，我们是否应该只使用干净样本进行验证，还是有更好的利用方法？我们在本研究中解决了这些研究问题，并得出了以下发现：首先，我们发现有趣的现象是，最近的wSL方法确实需要干净验证样本才能正常工作。否则，性能会大幅下降，如图所示。如果没有干净验证样本，训练的模型无法泛化到原始的弱标签之外，这意味着训练毫无意义。这表明wsSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于wSL方法实现更好的性能，如图所示。通常，每个类别只需要20个样本就能获得高表现。但这并非故事的全部，因为如果无论如何决定获取干净样本，直接在这些样本上训练将获得更好的性能。红色的图表显示了直接应用于干净数据（微调方法）和仅将干净数据用于验证（wSL方法）之间的性能差异。我们可以看到，如果有每个类别十个样本，直接微调就开始超越wSL方法。最后，之前wSL方法中声称的性能提升可以通过允许在干净验证样本上继续微调来实现。从图表可以看出，最初性能低于更复杂的wSL方法（如余弦相似度）的FTW模型，如果允许在干净样本上继续微调，那么FTW的性能就能与其它方法相媲美。因此，在实践中，没有理由选择更复杂的wSL方法，因为它们需要更多的计算时间和磁盘空间。总而言之，我们表明，最近的wSL方法需要干净的手动标注样本才能正常工作，它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下：首先，报告模型选择标准，例如报告模型选择是否在干净验证样本上进行。其次，wSL方法应该与少样本学习（few-shot learning）基线进行比较，假设研究人员在少量样本上进行工作。第三，持续微调是一种简单而强大的基线，应该在未来的wSL工作中考虑。最后，我们开源了我们的代码。您可以通过幻灯片上的二维码找到它。请随时查阅。谢谢，祝您会议愉快！"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫阿尔·维拉德，我将简要介绍一篇关于提示大型语言模型进行机器翻译，评估策略和性能的论文。 这项工作是我与谷歌翻译同事的合作成果。Pm 是一个拥有 5400 亿参数的语言模型，于去年 2022 年发布。它在包含 7800 亿个 token 的大型文本集合上进行训练，并在当时实现了数百项 NLP 任务的领先水平。 在本工作中，我们提出了一项对大型语言模型提示进行机器翻译的免费、系统的研究。 我们使用 IMT 社区的最佳实践来评估这些模型的翻译能力，这涉及到使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。 我们比较了两个最先进的系统，即 WMT 评估中表现最佳的系统。 我们使用最先进的神经机器翻译指标，并额外展示了基于专家的人工评估结果。 最后，我们提供了一些提示选择策略的建议。 提示对 LLM 在翻译方面的性能有很大影响，正如我们在一个简单的实验中看到的那样，我们使用了简短的提示，并为不同的句子提供了两个不同的提示。 在 1000 个句子中，有 516 个句子的差异超过 1 BLEU 分值。 在极端情况下，这个差异甚至可能高达 40 BLEU 分值。 因此，选择好的提示策略非常重要。 在我们的实验中，我们选择了五次提示策略，即我们只是标记提供给系统的句子及其所使用的语言。 在这里，我们从德语翻译成英语的示例中，德语源句标记为“德语：”，英语译文标记为“英语：”。 我们发现，提示的实际形式在几段简短提示的情况下没有太大影响。 对于零次提示和一次提示来说，这至关重要，但当我们像我们一样采用五段短提示时，提示的实际形式几乎没有差异，关键在于示例的重要性。 我们实验结果的总结是，示例质量比与源句的相似度更重要。 因此，重要的是从高质量的翻译中选择示例。 特别是，我们将从 WMT 评估的训练数据或开发数据中选择提示进行比较，开发数据质量更高，更规范，结果表明使用开发数据可以获得更好的性能。 然而，专门的、最先进的系统在 Palm 翻译方面具有显著的优势，但 Palm 已经非常接近商业系统。 在我们的案例中，我们选择使用 Google Translate 进行评估。 我们通过 NpN 框架进行的评估中获得的见解是，Palm 的流畅度与最先进的系统相当，但主要差异在于准确性。 尤其，最常见的错误是遗漏错误。 似乎 Palm 有时会选择产生听起来更好的翻译，方法是删除源句中的部分内容。 然而，Palm 的风格外发类别低于最先进的系统，这是一个额外的信号，表明 Palm 提供了非常流畅的输出，但仍然存在准确性问题。 以上就是本篇简短介绍，如需更多详情，请参阅论文的完整演示。 非常感谢大家。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我是来自中国科学技术大学的晋维业。很高兴能为大家呈现我们的论文的简短广告视频。你们是否在抄袭我的模型？“保护大型语言模型在嵌入和服务的版权”？我们通过水印技术来保护。首先，让我们介绍一下嵌入和服务的背景。目前，像GPT、LLaMA、PLM这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入和服务的构建是建立在大语言模型之上的服务之一，旨在辅助各种NLP任务。例如，OpenAI 提供基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入和服务的版权是必要的。为了保护嵌入服务的版权，一种解决方案是在服务提供商的服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下几个特性：首先，该方法应适用于嵌入和服务的场景；其次，水印不应降低所提供嵌入的效用；第三，水印应足够难以被攻击者移除或轻易被攻击者删除；最后，水印需要在模型提取过程中能够传递给攻击者服务。现有工作可以大致分为四类，但这些方法要么不适用于嵌入和服务的场景，要么缺乏可传递性。因此，在本文中，我们提出了嵌入标记（Embedding Marker），这是一种基于后门的水印方法，适用于嵌入和服务的场景。接下来，我将介绍我们的嵌入标记的细节。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组频率在适中范围内的词语。我们假设服务提供商可以收集一个通用的文本语料库并统计词语频率。在水印注入过程中，我们首先定义一个目标嵌入。当用户将句子发送到服务提供商时，提供商会统计句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。当句子中的触发词数量大于m时，提供的嵌入与目标嵌入完全相等。版权验证的目的是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。然后，提供商向盗版服务请求嵌入，使用该数据集。计算请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，定义为Delta Cosine和Delta L2。同时，我们还应用KS检验，并使用其p值作为第三指标。我们对四个数据集（Aging News、MindSD2和A-Spam）进行了实验。我们假设提供商使用WikiText数据集来统计词语频率。在四个数据集上的结果表明，我们的嵌入标记可以实现出色的检测性能，同时保持对下游任务的卓越效用。我们还通过在四个数据集上对句子进行嵌入可视化来验证所提供的嵌入的不可检测性（convertness），采用PCA进行降维。图中的图例表示每个句子中的触发词数量，如图所示。很难区分加水印的嵌入和正常的嵌入。以上就是全部内容。谢谢大家。欢迎大家来和我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "各位好。我叫Ian，我的同事Jiian和我将为大家介绍我们关于多指令调优的研究，旨在通过指令调优提升多模态序列学习效果。随着大型语言模型的快速发展，许多研究开始探索利用预训练语言模型以参数和数据高效的方式，针对不同的下游任务进行学习的新范式。最近，许多研究表明，指令调优可以使大型语言模型遵循自然指令，在零样本条件下完成未见过的任务。然而，以往的大部分指令调优工作都集中于提升语言任务的零样本性能，而忽略了计算机视觉和多模态任务。因此，在本工作中，我们旨在研究在多模态蛋白模型上进行指令调优，是否能够真正提升对未见过的多模态任务的泛化能力。此外，在我们的研究过程中，我们发现P模态和多模态指令数据集的可获得性存在显著差异。仅语言指令任务数量超过1600，但缺乏大规模公开的多模态指令任务数据集。这促使我们构建了一个多模态指令调优数据集。我们在此呈现Multi-InstInstruct，这是第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，覆盖10个类别。这些任务来源于21个现有的开源数据集，并且每个任务都配备了五个专家撰写的指令，用于研究我们在提出的数据集上进行多模态指令调优。我们以OFA作为基础模型，OFA采用统一的多模态训练模型，并使用统一的词汇表来处理语言、图像和边界框的坐标。这里展示了我们Multi-InstInstruct数据集中的一些示例。为了统一处理各种输入和输出数据类型，我们遵循OFA的方法，将所有任务都格式化为统一的序列到序列格式，其中文本、图像、指令和边界框都表示在相同的token空间中。现在，我将介绍多模态指令调优。对于训练数据集，我们从N组中选取了53个任务进行训练，并对每个任务采样10,000个实例用于测试。我们将Common Sense Reading组完整保留用于测试，并从Wiki和Miscellaneous组中额外选择了5个任务。我们对每个任务使用所有测试实例，此外，我们还从自然指令测试集中随机采样20个任务作为NP任务。在训练过程中，我们使用预训练的大型模型作为基础模型，并将所有任务的实例混合在一起，每个实例会随机与它的五个指令模板中的一个进行组合。在测试过程中，对于每个任务，我们进行总共五个实验，通过使用五个指令中的一个来评估模型。在每个实验中，我们报告所有五个实验的平均值、最大值和标准差。如果任务是多模态分类任务，我们报告准确率；如果任务是多模态生成任务，我们报告root l；对于LP任务，我们同样报告root l。我们还引入了一个额外的评估指标，称为敏感性，它衡量模型在指令措辞略有变化的情况下，是否能够始终如一地产生相同输出的能力。这是我们的主要结果。正如我们所见，指令调优可以显著提升OFA在多模态任务上的性能。此外，从自然指令数据集进行迁移学习也有利于指令调优。在这里我们可以看到，随着任务数量的增加，模型能够取得更好的性能，并且同时降低敏感性。我们还进行了一项实验，比较使用一个指令和五个指令的效果。如您所见，使用更多的指令可以显著提升模型的整体性能并降低其敏感性。这表明不同的微调策略对模型敏感性的影响。我们可以看到，通过从自然指令数据集进行迁移学习，模型可以实现比原始OFA模型更好的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助OFA在Nitrogen Instruct数据集上取得更好的性能。总而言之，我们提出了第一个大规模的多模态指令调优数据集，显著提升了OFA的神经能力，并探索了不同的迁移学习技术，展示了它们的优势。我们设计了一个新的指标，称为敏感性。最后，我们正在收集一个更大的多模态指令调优数据集，包含约150个额外的变体语言任务，并将公开它们。这是我们数据集和模型的二维码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。我叫Just John，来自宾夕法尼亚州立大学。今天我将介绍我们的工作，即Exampler：多自然语言和人工表示的跨语言语义解析。语义解析的任务是构建用户查询的语义表示，例如SQL和Lambda演算。跨语言语义解析的任务是将多种自然语言的查询翻译成多种语义表示，如图所示，我们需要使用神经网络模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型是独立提出的，并在有限的数据集和应用场景下进行评估。例如，在某些自然语言的覆盖率存在不足，中文缺失的情况；或在某些语义表示的覆盖率存在不足，Lambda演算缺失的情况；或者它们仅在特定的神经网络模型上进行评估，例如只有一个模型用于评估。为此，我们提出了Exampler，提供了一个统一的数据集，用于在多自然语言和语义表示中的跨语言语义解析。它包含九个数据集，涵盖五个病毒领域，五种语义解析任务，八百万个表示和22种自然语言，属于15个语系。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用Google翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语数据集上训练英语模型，在推理时，我们使用API将德语查询翻译成英语，然后使用训练好的模型来预测SQL。我们还测试了单语模型。在这种设置下，源语言和目标语言相同，例如德语到德语或英语到英语。我们还测试了单语融合设置，通过仅使用10%的训练数据训练单语模型。我们还测试了多语言模型，我们为所有语言训练一个多语言模型，例如，我们将德语、英语、中文查询组合起来训练一个多语言模型，在推理时，我们可以使用该模型来翻译德语查询或中文查询等等。我们还考虑了跨语言零样本和少样本迁移。我们在一种源语言上进行训练，然后迁移到另一种语言。因此，在训练过程中，我们使用英语查询或英语和德语少样本查询训练一个多语言模型，并预测SQL输出。我们还发现了很多有趣的成果。关于单语模型的分析，我们在两组模型上进行评估，包括编码器指针译码器 (Encoder-Decoder with Pointer-based Decoder)，例如XLMR-PDDR和BIRD-PDDR，我们还评估了编码器译码器模型，即多语言预训练编码器译码器模型，例如MBT和MT5。我们发现，编码器译码器模型在所有九个数据集上都获得了最佳性能。我们还在MT5和XLMR-PDDR的多语言设置下进行评估。我们发现，通过在各种语言的混合中进行训练，可以改进编码器译码器模型或编码器指针译码器模型。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集上的性能下降，仅在三个数据集上获得提升。我认为这被称为多语言曲线。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，而绿线是单语设置。我们发现，通过比较绿线和橙线，我们发现零样本设置中的跨语言迁移性能差距很大，通过比较蓝线和橙线，我们发现少样本设置中的迁移差距迅速缩短。我们还发现了一些其他的有趣的发现，例如编码器译码器模型优于以往的工作，或取得了可比的结果。训练英语自然语言可以显著提高目标自然语言的少样本性能。我们发现像Coders和Blue这样的多语言语言模型对于跨语言语义解析任务仍然不足。总而言之，我们构建了Exampler，一个用于跨语言语义解析的统一基准，该基准具有多种自然语言和语义表示。我们对三种类型的多语言语言模型进行了全面的基准研究，我们的结果表明了很多有趣的发现等等。欢迎访问我们的论文和代码。感谢聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫亚当·施里克科夫斯基，本次演讲是关于并列结构的依存关系。正如你们可能知道的，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存关系中，并列结构“丽莎、巴特和玛姬”的结构是，第一个连词是整个并列结构的中心词，在本例中，丽莎。伊戈尔·米尔丘克的语义文本理论也假设了类似的结构，同样，整个并列结构的中心词是第一个连词。这两种方法都是不对称的，它们突出显示了其中一个连词。当然，也有对称的并列结构方法，例如 prag 方法，prag 依存树库中假定的连词中心化方法，其中并列结构的中心词是连词，因此我们从连词到所有连词之间建立依存关系。最后，还有一种多中心词方法，例如在卡森的词法语法中使用的，可以这么说，所有连词都是并列结构的中心词，因此我们从控制词开始，分别建立到所有连词的依存关系。这些就是目前的几种情况。\n\n本文的目标是为这些对称的并列结构，以及反对这些不对称的并列结构，提出一种新的论据。这个论据是基于依存长度最小化的原则，我将通过这些例子来解释。在英语中，正如你们可能知道的，直接宾语倾向于靠近动词，而状语可以离动词更远。例如，“马车昨天读了书”是可以接受的，因为直接宾语靠近动词，而“马车昨天读了书”则不太好，因为在动词和直接宾语之间有一个状语“昨天”。但是，当直接宾语非常长且复杂时，这种效果可能会得到缓解，因为它可以移动到状语之后。这一点在这里得到了说明，这两个句子都是可以接受的：“马车读了这本绝对迷人的关于蜜蜂的书昨天”和“马车昨天读了这本绝对迷人的关于蜜蜂的书”。这里的理由是，即使这个句子违反了直接宾语应该紧邻动词的语法原则，它也满足了依存长度最小化的原则，即更短的依存关系更受欢迎。这些树只显示了这些关键依存关系的长度，即在这些结构中不恒定的依存关系。在这里，我们看到从动词“读”到状语的依存关系长度为七个单词，从动词“读”到宾语“书”的依存关系长度为四个单词，总计为 11。当交换这两个成分时，这两个依存关系的长度之和变为六，比 11 短得多，这就是为什么它听起来还可以的原因，它违反了一个原则，但满足了另一个原则。\n\n我们从增强版的 PenTreeBank 中提取了各种关于并列结构的统计数据，请参阅本文，了解我们为什么没有使用通用依存关系。这些统计数据证实了之前多次观察到的现象，即左侧连词往往更短，例如“盐和胡椒”而不是“胡椒和盐”，以音节为单位测量。此外，还观察到，这种趋势随着长度的增加而增强，即当两个连词的长度差异增大时，较短的连词更倾向于出现在第一个位置。因此，左侧短连词的比例更大。本文的新颖之处在于，我们观察到这种趋势仅在控制词位于左侧时才会发生。例如，在这个例子中，“我看见巴特和丽莎”，\"我\" 是控制词，位于左侧。在第二个例子中，“荷马来了并打了个喷嚏”，这里是两个动词的并列，没有外部控制词。在这种情况下，左侧连词更短，连词长度差异越大，这种趋势越明显。然而，当控制词位于右侧，例如“左边控制着尾巴和网”时，这种效果消失了。我们通过测量字符数、音节数和单词数（分别对应于第一列、中间列和最后一列）来证明这一点。我将主要关注最后一列。我们看到，当控制词位于左侧时，左侧连词更短的趋势随着单词数的绝对差异的增加而稳定增长。当没有控制词时，例如在句子并列中，也观察到相同的情况。但是，当控制词位于右侧时，这种趋势消失了。我们在本文中展示了，这为反对并列结构的非对称性提供了一种论据，正如这两种非对称结构那样，反对这两种对称结构。请参阅本文以获取完整的证明和论据。稍后与我们讨论海报环节。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "- 您好，我叫Kyo Yin，我将为大家介绍我们的工作，题为《翻译何时需要语境：基于数据的多语言探索》。这项工作是我们与Patrick Ferange、Emiliu、Andre F.D. Martins和Graham Newbiig合作完成的。因此，很多翻译都依赖于语境。例如，如果前一句是“如果部长们发现了，事情可能会变得危险”，那么“mole”指的是间谍，但如果前一句是“医生，这可能是什么严重的事情吗”，那么“mole”指的是胎记。所以，根据语境，单词的含义会发生变化，因此其翻译也会随之改变。然而，评估模型在处理这类情况时表现如何，相当困难。首先，只有一小部分翻译依赖于语境，这使得诸如BLEU之类的语料库级别指标无法捕捉到这些翻译。有些人建议对语境相关的翻译进行有针对性的评估，但这些资源仅支持有限类型的语境相关翻译和有限的语言集，因为它们通常依赖于领域知识和人工策展。在这项工作中，我们试图回答这两个问题：第一，翻译何时需要语境？第二，模型如何处理这些情况？为了回答第一个问题，我们首先通过测量翻译中语境依赖的程度，我们在之前的研究中引入了cxmi作为衡量机器翻译模型语境使用的指标。这通过测量语境C提供的关于目标y的信息量来完成，给定源x。您可以将cxmi视为向模型提供语境后获得的信息。在这项工作中，我们将cxmi扩展到点y，cxmi (p6mi)可以测量句子级别或单词级别的语境使用情况。我们可以将具有高p6mi的单词视为需要语境进行翻译的单词。现在，我们分析具有高p6mi的单词，以寻找这些单词之间的模式。我们对英语到14种不同语言的ted talks的文本进行分析。我们在三个不同的层面进行分析。首先，我们观察词性标签，这些词性标签具有较高的平均pxmi，这使我们能够发现阿拉伯语中的双数代词，它们的pxMmi相对较高。这可以解释为，英语没有双数代词，因此在翻译成阿拉伯语时需要语境来确定代词是否为双数。类似地，我们发现某些语言在选择适当的动词形式时也需要语境。然后，我们观察所有不同出现情况的平均p6I的词汇项，这有助于我们识别像这种情况下的案例：在中文中，您需要语境来翻译专有名词，以确保在文档中使用相同的翻译。类似地，我们发现语境对于翻译成适当的正式程度也是必要的。最后，我们观察不同的单个token，这些token具有较高的pxmi，这使我们能够识别无法真正通过单词本身捕捉到的现象，而是体现在句子结构中，例如省略现象的解决。现在，我们使用我们的分析结果来设计用于文档级别翻译的基准。对于我们识别的五种话语现象中的每一种，我们都创建了标签器，以自动识别属于该现象的单词。我们称我们的标签器为“多语言话语感知”或Muda标签器。然后，我们还可以注意不同语言具有不同比例的这些话语现象。然后，我们使用Muda标签器，将标签器应用于我们希望用于评估的平行语料库，并在Muda标签器识别出的语境相关示例上应用我们选择的翻译指标。最后，我们还将我们的基准和其他指标用于评估不同模型在文档级别的机器翻译中的表现。首先，当我们使用语料库级别的指标时，例如BLEU，我们发现无语境模型表现最佳。但是，如果使用commentt，那么具有语境感知的模型表现最佳。如果使用wordf度量，那么具有或不具有语境的模型表现可比。这再次表明，如果仅使用语料库级别的指标，很难确定最佳的文档级别翻译系统。现在，我们使用M基准来评估模型，我们发现对于某些话语现象（如正式程度和词汇连贯性）而言，具有语境感知的模型比不使用语境的模型更准确。但是，对于其他现象（如省略代词和动词形式）来说，这些模型的表现并不比不使用语境的模型好太多。这表明我们需要在文档级别翻译方面看到更多的进步。我们还比较了不同的商业系统，我们的基准表明，dL通常比Google Trans更准确，可以进行文档级别的翻译。总而言之，我们对14种语言对进行了基于数据的分析，以确定翻译何时需要语境，然后我们利用这些发现来构建文档级别机器翻译的基准，这可以帮助我们识别模型能够很好地处理或不能处理哪些话语现象，以及哪些翻译系统擅长文档级别的翻译。感谢大家的关注，我们将在多伦多再见。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是 Jenny，卡耐基梅隆大学一年级的博士生，今天我将为大家介绍我们的工作“AnL 位置性”，它旨在描述设计偏差和模型集合。这项工作是在与华盛顿大学和人工智能艾伦研究所的一些同事合作完成的，具体包括 Sebastian Santi、Ronin Labrasse、Katharina Reinika 和 Martin Sapp。\n\n那么，让我们先想象一下，你正在为一家报纸工作，需要筛选文章下面的评论，试图去除有毒内容。你可能会使用一种流行的 API，比如 Perspective API，用于检测毒性。如果你的用户是 Carl Jones，Perspective API 通常能正确检测出有毒内容。但如果你的用户是 Didtha Sharma，Perspective API 对在印度语境中更常见的冒犯性词语的敏感度就明显降低。这是一种设计偏差，我们观察到技术在不同人群中的表现存在系统性差异。\n\n这种设计偏差，就像我们刚才看到的偏差一样，可能源于 NLP 研究人员和模型开发者自身的位置性。位置性指的是人们由于其人口统计特征、身份和人生经历而持有的一些特定视角。这是一个在批判性研究中广泛使用的概念，尤其是在女权主义和酷儿学术领域。作为一名研究人员，位置性会影响研究过程及其结果，因为它可以改变研究人员所做的决策。\n\n那么，一个可能的问题是：数据集和模型是否具有位置性？我们并不是说数据集和模型本身具有人口统计特征和人生经历，但它们确实会聚合真实人们的判断和观点，因此可能代表其他位置性。\n\n先期的研究已经提出了一些关于位置性的轶事证据，例如模型和数据集中存在文化差异，以及对模型位置性的理论定义。然而，这些研究并没有将最终用户与数据集和模型本身进行比较。\n\n随着 NLP 任务变得更加主观和具有社会导向性，研究数据和模型的位置性变得越来越重要。由于并非所有决策都已记录，并且许多模型隐藏在 API 背后，因此很难阐明这些位置性是如何产生的偏差。\n\n为了研究数据集和模型的位置性，我们实际上将真实用户的标注与现有数据集和模型进行比较。我们通过我们的框架 “Nl Positionality” 来实现这一点。\n\n我们的框架主要分为两个步骤。第一步是使用多样化的标注者对数据集进行重新标注。我们选择这样做，而不是关注原始数据集的标注者的统计特征，因为通常每个实例只有一个或几个标注者，而且很少收集和分享统计数据。因此，我们选择重新标注数据，以获得大量的标注，并获得一组丰富的统计数据。\n\n然后，我们按统计特征对标注进行分组，并将它们与模型和数据集进行比较，使用 Pearson 相关系数。因此，我们的框架不同于标注者意见不一致的文献，它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅关注标注者之间的意见一致性或建模标注者分布。\n\n我们的框架很大程度上得益于“Lab in the Wild”，这是一个在线众包平台，它是一个前 HCI 合作实验室。Lab in the Wild 是一个在线实验平台，我们可以招募到各种各样的志愿者。与 Turker 平台相比，Turker 平台主要有来自美国或印度的参与者，Lab in the Wild 仍然能够获得高质量的数据。\n\n我们在 Lab in the Wild 上托管了两个任务，其中一个任务是“社会接受度”。它的工作方式是，参与者会阅读来自 Social Chemistry 数据集的情境，然后写下该情境的社会接受程度。为了保持对研究的参与，他们可以将自己的答案与 AI 和其他人进行比较。然后我们将这些标注与 Social Chemistry Delphi 和 GPT4 进行比较。\n\n我们还针对毒性和仇恨言论检测任务，复制了非常相似的设置，参与者会阅读来自 Dynah Hate 数据集的一个实例，并写下他们是否认为该实例是仇恨言论。然后我们将这些标注与 Dynah Hate、Perspective API、Rewire API、Hate Roberta 和 GPT4 进行比较。\n\n我们的研究最终收集了来自八十七个国家/地区的超过一千名标注者的 16,000 多份标注。\n\n因此，我们现在更有能力回答“NLP 数据集和模型与哪些人群最一致？”\n\n我们发现 NLP 中存在位置性。例如，我们发现数据集和模型与英语国家最为一致。\n\n对于 GPT4 社会接受度分析，我们发现它与儒家文化和英语国家最为一致。我们也发现 Dynah Hate 与英语国家最为一致。我们还发现与受过大学教育的人群存在额外的相关性。\n\n对于 GPT4 在社会接受度任务中，我们发现它与受过大学或研究生教育的人群最为一致。我们还在 Dynah Hate 中发现了同样的现象，它与受过大学教育的人群最为一致。\n\n然而，当数据集和模型与特定人群一致时，一些人群不可避免地会被遗忘。一个例子是，数据集和模型与非二元性别群体相比，与男性和女性群体不太一致。我们也在 GPT4 社会接受度任务和 Dynah Hate 任务分析中发现了这一点。\n\n既然 NLP 中存在位置性，我们该如何解决这个问题呢？\n\n我们有一些建议：首先，记录研究过程中的所有相关设计选择；其次，从前瞻性的角度进行 NLP 研究；第三，为特定社区构建专业的数据集和模型。 Masakanne 倡议是一个很好的例子。\n\n我们想强调的是，包容性的 NLP 不仅仅是让所有技术为所有人服务。\n\n至此，我们的演讲就告一段落了。如果您想了解更多信息，请随时查看我们的仪表板，以获取最新分析结果和我们的论文。谢谢！"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我将介绍我们关于解决实体选择中的间接指代表达式的研究，其中我们引入了替代实体语料库。我的名字是Javad Hosseini，这项工作是与Philip Radlinsky、Sylvia Parity和Annie Luis共同完成的。我们的目标是理解用户在做出选择时使用的语言。我考虑一个替代性的问题：你是想说“Easy on Me”还是“I got a feeling”？ 在这里，用户想在两首歌曲中进行选择。最明显的方法是使用直接引用，例如说歌曲的名字“Easy on Me”或它的位置，比如第一首，但有时间接引用更合适，以便进行更自然的对话。这可能发生在用户记不起歌曲的名字时，或者发音太相似难以区分，或者当用户想表达偏好时。以下是一些间接指代的例子。例如，“较新的那首”或“不具活力的那首”。 这在对话系统和用于基准测试LLM的实体理解方面是一个重要的问题。我们没有发现任何公开的数据集，特别是更大规模的公开数据集，因此我们使用众包标注收集了一个数据集。我们的数据集涵盖音乐、书籍和食谱三个不同的领域。我们的数据集收集方法强调非正式性，采用了卡通补全设置。卡通中有三个语音气泡。在第一个气泡中，Bob说：“还记得我们昨天听的那首歌吗？” 通过这个，Bob设置了对话语境。在第二个语音气泡中，Alice说：“你是想说‘Easy on Me’还是‘I got a feeling’？”，这是替代问题。在第三个语音气泡中，Bob使用间接引用来选择这两个实体之一，例如“较新的那首”。 我们自动提供第一个和第二个语音气泡，但第三个由标注者填充。第一个语音气泡的选择来自每个领域的一些手动提示。第二个语音气泡，即替代问题，按照以下方式生成：我们总是使用一个简单的模板：“你是想说A还是B？” 其中A和B是来自维基百科的样本。以下是我们在使用的不同采样方法。当我们向上移动列表时，实体之间的相似度会越来越高，通常更难进行消除歧义。第一种方法是在随机均匀采样。第二种方法是当实体具有相似的标题时，例如两本以“Return”命名的书籍。第三种方法是当它们在维基百科上有相似的描述时，最后一种是当它们在维基百科上有相似的信息框或属性时，例如歌曲的同一类型或同一艺术家。当我们向标注者展示这个替代问题时，他们知道这些实体的名字，但他们可能不了解这些实体。所以我们所做的就是展示关于这两个实体的背景知识。对于歌曲，我们只需向每个歌曲提供一个Google搜索链接，然后要求标注者至少听一些每首歌曲并了解每首歌曲。以下是歌曲“Easy on me”的Google搜索结果。 对于食谱和书籍领域，我们展示了来自维基百科的一些背景文本。对于食谱，我们还展示了它们的图片，同样来自维基百科，以便标注者了解它们的样貌。然后，我们要求标注者选择其中一个实体，例如这里的第一首，并使用三到五个间接引用来描述它们。例如，“有钢琴音乐的那首” 。以下是一些来自我们数据集的例子：例如，“没有歌词的那首”、“12岁的男孩那首”或“虚构的那首”或“来自亚美尼亚的那首”等等。 替代语料库包含来自三个领域（音乐、书籍和食谱）的6000个替代问题，并包含422,000个间接引用表达结果。使用T5x Large模型的结果总结如下。如果语言模型可以访问与标注者完全相同的背景知识，那么准确率会非常高，大约在92%到95%之间，但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87.7%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率仅为6%，因此有很大的改进空间。我们还表明，模型具有领域泛化能力。这里有一个指向我们数据集的链接。感谢您的收听。"}
