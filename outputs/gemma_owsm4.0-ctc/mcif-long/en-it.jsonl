{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi, utilizzando il multiset tagging e le permutazioni latenti. Si tratta di un lavoro congiunto con i miei studenti Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere intesa come l'abilità di un apprenditore di gestire una ricorsione più profonda e combinazioni inedite di frasi che sono state viste individualmente durante l'addestramento. Nel contesto dell'analisi semantica, il test per la generalizzazione composizionale potrebbe apparire come al solito abbiamo un insieme di addestramento di enunciati, in questo caso “la ragazza dormì” e “Mary sapeva che la ragazza dormì”. Questi enunciati sono accoppiati con forme logiche che rappresentano gli aspetti fondamentali del loro significato. In contrasto con la valutazione standard di machine learning, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente inedite. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli sequenza-a-sequenza naïf faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output scollegati dall'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate in modo colorato nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi hanno lo scopo di catturare il processo composizionale che mette in relazione gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi non sono di solito forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente costoso. Tipicamente, questo comporta una pre-elaborazione formalmente specifica delle forme logiche, ad esempio per gestire i simboli variabili. L'ottenimento degli alberi può anche comportare procedure specializzate di induzione della grammatica. In questo articolo non utilizziamo alberi e introduciamo un modello sequenza-a-sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la previsione dell'output dall'input in due passaggi. In primo luogo, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token corretti, ma non sono ordinati. Ecco perché, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona approssimativamente così: ci muoviamo da sinistra a destra sull'output e determiniamo quale token del multiset inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno come evidenziato in rosso. Quindi saltiamo al successivo token del multiset per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token del multiset. Continuiamo questo processo finché ogni token del primo passaggio non è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri con un ampio margine nella generalizzazione a una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo, risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multiset proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida di trovare la permutazione con il punteggio più alto, che è N-hard. Questo perché è correlato al problema del commesso viaggiatore. Lo approssimiamo con una rilassazione continua adatta alla GPU che ci permette anche di retropropagare attraverso la soluzione e apprendere le permutazioni più plausibili dal punto di vista linguistico. Se volete saperne di più sui nostri esperimenti e su come abbiamo affrontato queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, io sono Myra e oggi parlerò dei nostri \"persona marcate\", utilizzando prompt in linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato realizzato in collaborazione con Essenndermush e Danjorovsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure presentano varie limitazioni: di solito si basano su dataset costruiti a mano, che richiedono molto tempo per essere curati, e spesso misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, oppure semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero la nozione che le identità sociali sfaccettate possono amplificare i pregiudizi e rappresentare luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi più recenti LLM ottimizzati tramite istruzioni sono molto bravi a rispondere a istruzioni nei prompt, quindi possiamo chiedere al modello di generare una \"persona\", che è una rappresentazione di un individuo immaginario, utilizzando un prompt come \"Immagina di essere una donna asiatica, descriviti\". Possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marcatore di identità in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Immediatamente vediamo che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale del termine, ci sono alcuni schemi interessanti. La donna asiatica è descritta come poco appariscente, la donna mediorientale è definita usando parole come \"esotica\", \"affascinante\", riferendosi a una regione \"mesmerizing\", e entrambe le \"persona\" di donne di colore fanno riferimento alla discendenza, mentre la \"persona\" dell'uomo bianco non presenta nulla di tutto ciò. Per catturare questi schemi, il nostro metodo ha due parti. La prima è quella di generare queste \"persona\". I nostri prompt per generare queste \"persona\" sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che, fornendoli a soggetti umani, erano anche in grado di far emergere stereotipi razziali e questo consente un confronto diretto tra le nostre \"persona\" generate e le risposte scritte dagli umani. La seconda parte è \"parole marcate\", un metodo per identificare le parole che distinguono i gruppi \"marcati\" dai nostri \"marcati\", su cui elaborerò tra poco. Il vantaggio è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico. Quindi, il metodo delle \"parole marcate\" attinge al concetto sociolinguistico di \"marcatura\", secondo il quale esiste un default non marcato e qualsiasi gruppo che differisce da tale default è linguisticamente marcato. Ad esempio, la parola \"uomo\" o, scusate, la parola \"guerriero\" è solitamente associata agli uomini, quindi quando si descrive un guerriero che è una donna, di solito specificano \"un guerriero uomo\" e marcano il termine con \"donna\". Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Nel nostro metodo, designiamo prima quali sono i gruppi non marcati e marcati, e poi confrontiamo le \"persona\" utilizzando il metodo delle \"parole di combattimento\", che consiste fondamentalmente nell'utilizzare rapporti logaritmici ponderati per distinguere le parole principali per ciascun gruppo \"marcato\". Ad esempio, per le \"persona\" di donne nere, faremmo \"parole di combattimento\" e confronteremmo i rapporti log-odds contro sia le \"persona\" bianche che quelle maschili, perché questi sono i due gruppi non marcati corrispondenti. Ora, per alcuni risultati, per prima cosa utilizziamo il lessico degli stereotipi e scopriamo che le \"persona\" generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando esaminiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Sebbene le \"persona\" generate abbiano tassi molto più alti di parole nel lessico, quelle scritte dagli umani hanno una distribuzione di parole molto più ampia, mentre le parole che costituiscono gli stereotipi nelle \"persona\" generate sono solo le parole \"alto\" e \"atletico\", quindi solo quelle positive o almeno non negative. Infatti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Invece, per farlo, ci rivolgeremo ai risultati del nostro metodo delle \"parole marcate\" per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi. In primo luogo, per i gruppi \"marcati\", le parole principali includono cose come \"cultura\", \"tradizione\", \"orgoglioso\" ed \"esotico\", e queste parole definiscono questi gruppi solo in relazione al loro rapporto con la loro identità e li distinguono dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti tropi comuni che sono riflessi in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono la donna latina includono cose come \"vibrante\" e \"formosa\", che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come \"piccola\", \"delicata\" e \"setosa\", che si collegano a una lunga storia di iper-sessualizzazione delle donne asiatiche, viste come molto docili e sottomesse e così via. Infine, per le donne nere, vediamo che alcune delle parole principali sono cose come \"forte\" e \"resiliente\", che si collega a un archetipo che le persone hanno chiamato \"la donna nera forte\". E sebbene a prima vista possa sembrare positivo, ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie affinché siano resilienti e forti di fronte agli ostacoli sociali, piuttosto che lavorare effettivamente per cambiare tali ostacoli, mette pressione su queste persone per superarli, il che porta a esiti sanitari molto negativi per queste persone, tra le altre cose. Più in generale, scopriamo che le parole per ciascun gruppo \"marcato\" riflettono fondamentalmente solo narrazioni molto essenzializzanti. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari di modelli. Innanzitutto, noi ricercatori dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare lenti intersezionali per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. Infine, ci dovrebbe essere una maggiore trasparenza sui metodi di mitigazione del bias, perché, ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di valore di allineamento eccessivo e bizzarro, o forse ad alcuni altri metodi anti-stereotipo che stanno generando questi schemi dannosi. Non possiamo davvero fare assunzioni o studiare ulteriormente senza maggiore trasparenza. Grazie mille per aver ascoltato, divertitevi all'ACL!"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E io sono Sarah Finch.\nE oggi vi parleremo di ABCEV, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato svolto dal laboratorio di NLP dell'Emory, guidato dal Professor Gino Choi all'Emory University e in collaborazione con Amazon Alexa AI.\nQuindi, supponiamo che abbiate appena sviluppato un modello di dialogo e vogliate vedere quanto bene si confronta con lo stato dell'arte. La pratica comune è quella di utilizzare valutazioni umane, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni è migliore o di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti, quindi potreste voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare. Un approccio consiste semplicemente nel chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o Likert esistenti. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddicendosi. Chiamiamo questo approccio l'annotazione di comportamenti nella chat, o ABCEV, in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABCEV è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABCEV misura il numero di turni in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di irrilevante, si contraddice o contraddice il proprio interlocutore, allucina fatti errati o viola la conoscenza del senso comune e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni bot umane per modello utilizzando ABCEV. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni. Dalle nostre analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette comportamentali di ABCEV sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su cento conversazioni etichettate in modo doppio. Inoltre, le etichette ABCEV sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con auto-contraddizioni e contraddizioni con il partner spieghi rispettivamente il dieci e il dieci percento della qualità della conversazione, mentre le valutazioni di coerenza Likert medie spiegano solo il quattro percento o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione catturi un aspetto unico della qualità della chat utilizzando una regressione lineare a stepwise. Potete vedere come la combinazione di tutte le metriche ABCEV spieghi oltre il 25 percento della qualità della conversazione e come, rimuovendo le metriche una alla volta, la maggior parte di esse comporti la perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molta meno della qualità e meno di queste metriche trasportano informazioni uniche. Queste metriche ABCEV affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto ai metodi precedenti. Potete vedere questo nei risultati del nostro esperimento che diversi problemi rimangono e sono stati quantificati con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni del senso comune in circa il 20 percento delle loro risposte, producono informazioni irrilevanti in circa il 15 percento delle risposte e si contraddicono o contraddicono il proprio interlocutore circa il 10 percento delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando abbiamo condotto la nostra valutazione. Tuttavia, questo è tutto il più grande motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABCEV possa essere sfruttato da altri nel settore come un passo significativo in questa direzione e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale progredirà nei prossimi mesi e anni. Grazie per averci seguito."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "– Buongiorno, mi chiamo Vauddha e sono una candidata al dottorato di ricerca in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per ACL 2023 come articolo esteso, riguardante l’apprendimento per trasferimento per la rilevazione della dissonanza cognitiva, affrontando la sfida delle classi rare. Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio; in sostanza, la dissonanza cognitiva si verifica quando due credenze o azioni sono inconsistenti, come in questo esempio in cui una persona afferma \"So che le sigarette potrebbero uccidermi\" e poi prosegue dicendo \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e questa azione sono inconsistenti e sono in dissonanza. Ulteriori dichiarazioni del tipo \"Non penso che potrei mantenere il mio lavoro senza di loro\" giustificano la seconda occorrenza e presentano una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è molto rara da trovare espressa nel linguaggio, tra le altre relazioni discorsive. Ma perché questo è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, le tendenze e i valori di credenze e i cambiamenti di atteggiamento nella popolazione. L'alta dissonanza cognitiva è inoltre legata ai disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi individuali e ci aiuta a comprendere meglio i processi decisionali. Con l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo adottato un approccio \"dissonanza first\", come si può vedere nel diagramma di flusso qui presentato. I tweet sono stati analizzati utilizzando un parser PDTV e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può notare, la dissonanza è stata rilevata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa mille esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia funzionato non molto meglio del caso fortuito, data la bassa occorrenza della dissonanza e l'assenza di qualsiasi set di dati precedente. Ci troviamo di fronte al problema della rarità assoluta. Per attenuare questo problema, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo al fine di annotare in modo da poter raccogliere più esempi di dissonanza con un minor numero di cicli di annotazione, riducendo così i costi complessivi di annotazione e migliorando al contempo la rilevazione della dissonanza. Poiché il modello iniziale non era in grado di cogliere affatto la classe dissonanza, abbiamo avviato il processo di apprendimento attivo trasferendo i pesi da attività strettamente correlate. Trasferiamo da due diverse attività: la classificazione della dissonanza indipendente dall’argomento, un compito che determina se due affermazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo indipendentemente dall'argomento (chiamato \"dibattito\" qui) e la classificazione binaria delle classi di espansione e confronto di PurityTB. Poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza, le chiamiamo ceE. Scopriamo che trasferendo i pesi, le prestazioni iniziali sul set di dati annotato sono già molto migliori del caso fortuito, con l'AUC migliore a 0,62. Inoltre, ottimizzando iterativamente su entrambe le attività, scopriamo che l'ottimizzazione di ceE seguita da un'ulteriore ottimizzazione su \"dibattito\" produce prestazioni iniziali molto migliori. Questo è il modello che utilizziamo per l'avvio dell'apprendimento attivo. Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ciascun ciclo di apprendimento attivo. L'approccio cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre l'approccio iterativo aggiorna il modello addestrandolo sul set di dati più recente raccolto attraverso le diverse strategie. Abbiamo scoperto che l'approccio cumulativo ha ottenuto prestazioni migliori o equivalenti all'approccio iterativo in tutti i casi. Per aumentare il numero di esempi di dissonanza, utilizziamo una strategia PRC (Probability of Rare Class) per selezionare principalmente gli esempi che hanno un'alta probabilità di essere dissonanti secondo il modello corrente in ogni round di apprendimento attivo. Confrontiamo questa strategia con le altre strategie all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio rispetto alle altre strategie all'avanguardia, sebbene la differenza sia minima. Va notato che le prestazioni sono significativamente inferiori per il metodo casuale nei successivi round di apprendimento attivo. Con le due strategie migliori, miglioriamo la classificazione della dissonanza all'AUC 0,75, che è la migliore prestazione che abbiamo ottenuto su questo compito finora. Verifichiamo inoltre la fattibilità di ciascuna strategia in termini di qualità dell'annotazione e costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per le classi rare. Tuttavia, gli annotatori hanno anche riscontrato che gli esempi sono difficili. In sintesi, scopriamo che PRC è una strategia semplice per l'acquisizione di classi rare e che l'avvio dell'apprendimento attivo con attività di apprendimento per trasferimento opportunamente progettate può aiutare in modo significativo. Scopriamo inoltre che l'aggiornamento iterativo è utile per l'apprendimento per trasferimento da un dominio diverso, mentre le annotazioni attive all'interno del dominio beneficiano di un aggiornamento cumulativo. Questi sono i link al nostro codice, al set di dati e al nostro articolo. Non esitate a contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Sono Akshata e oggi, insieme al mio co-autore Martin, presentiamo il nostro lavoro \"Kit Must: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple\". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-training, e la conoscenza fornita in input al momento dell'inferenza. Recenti studi su compiti come la risposta a domande dimostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede una conoscenza che viene fornita anche al momento dell'inferenza, ad esempio, nella frase \"John ha visto il presidente appena eletto in TV; i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono sapere in modo affidabile chi sia questa entità specifica, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato rispetto al periodo di pre-training. Pertanto, modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza disponibile al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di coreferenza progettato per sondare l'abilità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il data set con partecipanti a uno studio umano e sviluppiamo modelli di coreferenza. Ecco un esempio dal nostro data set: Servile è un giudice, Kia è un panettiere, Termin e Kia mettono in scena un parco dopo una lunga giornata di lavoro a decidere casi in un codice legale; era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui il pronome \"egli\" si riferisce, che in questo caso è. La risoluzione di un pronome dato richiede due tipi di informazioni: in primo luogo, la conoscenza specifica dell'entità, come \"Servile è un giudice\", e in secondo luogo, la conoscenza di fondo, come \"i giudici decidono casi nei tribunali\"; la conoscenza di fondo viene generalmente appresa durante il pre-training dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità viene solitamente osservata al momento dell'inferenza. Variaamo la disponibilità di questi due tipi di informazioni in modo che possano essere trovati in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di Kitdmos. Innanzitutto, abbiamo l'impostazione tipica, background pre-train, in cui la conoscenza di fondo è considerata disponibile al momento del pre-training. In secondo luogo, c'è l'impostazione background both, in cui la conoscenza di fondo è disponibile sia al momento del pre-training che dell'inferenza. Infine, l'impostazione background inference, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non fa parte dei dati di pre-training dei modelli, ad esempio perché sono sviluppate nuove professioni da quando è avvenuto il pre-training. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione background pre-train, supponiamo che la conoscenza di fondo \"i politici cercano seggi eletti nel governo\" sia contenuta nei parametri pre-addestrati. Nel contesto del tempo di inferenza, forniamo la conoscenza specifica dell'entità, \"Chichester è un politico\". Nell'impostazione background both, forniamo non solo la conoscenza specifica dell'entità, ma anche la conoscenza di fondo sui politici nel contesto del tempo di inferenza. Nell'impostazione background inference, forniamo la professione \"merely tour\" invece di \"politico\", perché \"merely tour\" è improbabile che sia contenuto nei parametri pre-addestrati. Valutiamo il data set sia con partecipanti a uno studio umano che sviluppiamo modelli di risoluzione della preferenza. In questa figura, mostriamo i risultati dei modelli con le migliori prestazioni sulla variante più difficile dell'impostazione background pre-train senza addestramento specifico per il compito su Kitdmos. Entrambi i modelli non funzionano bene quando vengono addestrati su Kitdmos; tuttavia, sia C2F che BuiltForCoref funzionano significativamente meglio rispetto alla scelta casuale. Ciò suggerisce che, quando vengono addestrati su data set generici di risoluzione della coreferenza, i modelli imparano a sfruttare gli indizi superficiali, che non sono utili quando si testa su Kitdmos, dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenza fittizia indicano che anche i modelli con le migliori prestazioni non possono integrare in modo affidabile la conoscenza di fondo solo al momento dell'inferenza. Per riassumere i punti chiave del nostro articolo, molti modelli di evoluzione della coreferenza sembrano impossibilitati a ragionare sulla conoscenza da diverse fonti senza un addestramento specifico per il compito; tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli con le migliori prestazioni sembrano avere difficoltà a integrare in modo affidabile la conoscenza di fondo presentata solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e il data set in codice su GitHub. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Pai dell'Università di Trento e dell'Istituto Bruno Kessler, e introdurrò brevemente l'attenzione come guida per un articolo sulla traduzione simultanea del parlato, che è un lavoro congiunto con Matteo Negri e Marco Durchi. Cosa è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica. E quali sono i problemi dei modelli SimST attuali? Architetture specifiche vengono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, con procedure di addestramento lunghe e complesse, ad esempio addestramento che coinvolge obiettivi di ottimizzazione differenti e addestramento e mantenimento di diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrando un modello con una latenza media di un secondo e un altro con due secondi, e così via. Quindi, qual è la nostra soluzione? Innanzitutto, utilizzare modelli SD offline già esistenti senza riaddestramento o adottare architetture specifiche per l'uso SimST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici, sfruttando la conoscenza già acquisita da un modello attraverso il meccanismo di attenzione tra input audio e output testuale, ovvero il meccanismo di cross-attenzione. Potete vedere un esempio a destra. La nostra soluzione è proporre un dot product o encoder-decoder attention, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè questa somma è inferiore a una certa soglia alfa verso gli ultimi lambda frame del discorso, il che significa che l'informazione ricevuta è sufficientemente stabile. Ad esempio, se riceviamo un blocco di discorso contenente \"Sto per parlare\" e il nostro modello prevede la traduzione in tedesco, guardando i pesi della cross-attenzione vedremo che le prime due parole puntano ai frame del discorso ricevuti più presto, mentre l'ultima parola punta agli ultimi frame del discorso ricevuti, ovvero ai frame lambda del discorso. Ciò significa che le prime due parole verranno emesse, mentre poiché la somma della cross-attenzione è superiore a una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro blocco di discorso. Se continuiamo e riceviamo un altro blocco di discorso e il nostro modello prevede altre tre parole, guardando i pesi della cross-attenzione vedremo che nessuna parola punta agli ultimi lambda frame lambda del discorso. Ciò significa che queste tre parole verranno emesse. Se guardiamo ai risultati principali di AD, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo il blu su un lato che misura la qualità della traduzione e il lagging medio, ovvero la misura della latenza, e consideriamo anche il lagging medio consapevole del calcolo, che tiene conto del tempo di calcolo del modello per prevedere l'output. Vogliamo quindi che le nostre curve siano il più alte possibile su questo grafico, ma anche che siano spostate a sinistra. Confrontiamo con strategie plepara che vengono applicate anche ai modelli offline, come la strategia Whit key e l'accordo locale, e confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la traduzione simultanea dello spazio. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco e vediamo che AD supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. Vediamo anche che se consideriamo il tempo trascorso effettivo o il tempo di calcolo, AD è la strategia più veloce. Se desiderate scoprire altri risultati, leggete il nostro articolo e abbiamo anche rilasciato il codice e i modelli open source e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Shu Ha. Oggi presenterò il nostro articolo dal titolo “Do Connel 2003 Named Entity Tagging still work well in 2023?”. Iniziamo. Il nostro articolo ha investigato il problema della generalizzazione utilizzando il compito del riconoscimento di entità nominate, o NER task. Abbiamo osservato che i modelli utilizzano Con 2003 per sviluppare il NER da quasi 20 anni e questo solleva naturalmente diversi problemi. Innanzi tutto, questi modelli possono generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per investigare questi problemi, abbiamo sviluppato il dataset Con plus+. Questo è un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e poi annotato seguendo le stesse linee guida di annotazione di Car 2003. Successivamente, abbiamo ottimizzato più di 20 modelli su Car 2003 e li abbiamo valutati sia sul test set di Con 3 che sul test set di Con plus+. Infine, abbiamo calcolato la variazione percentuale dell'f1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalizzano normalmente meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che, solitamente, modelli più grandi portano a una migliore generalizzazione. E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione influisce direttamente sulle prestazioni di un task downstream. Anche qui, abbiamo scoperto che un numero maggiore di esempi di ottimizzazione porta effettivamente a una migliore generalizzazione. Per quanto riguarda la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo avuto due ipotesi. La prima è l'overfitting adattivo, ovvero un overfitting causato dal riutilizzo dello stesso test set più e più volte, e questo si manifesta solitamente come rendimenti decrescenti su un nuovo test set. La seconda ipotesi è il temporal drift, ovvero il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting adattivo, abbiamo visto che dal grafico a destra la retta di migliore adattamento rossa ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su Con 2003 si traduce in più di un'unità di miglioramento su Con plus plus, il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che l'overfitting adattivo in questo caso non viene osservato. E per quanto riguarda il temporal drift? Per il temporal drift, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni diminuiscono con un divario temporale maggiore. Questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il temporal drift. La nostra conclusione è che per una buona generalizzazione avremmo bisogno di una migliore architettura del modello, una dimensione del modello maggiore e anche di più esempi di ottimizzazione. Questi elementi vanno di pari passo; non possiamo avere un ingrediente senza considerare gli altri. Abbiamo anche scoperto che il calo delle prestazioni è causato dal temporal drift e, in modo un po’ sorprendente, non dall'overfitting adattivo, anche se Connel 2003 viene utilizzato da oltre 20 anni. Ritornando alla domanda che abbiamo posto nel titolo del nostro articolo, \"i tagger Connel 2003 funzionano ancora nel 2023?\", abbiamo scoperto che la risposta è in realtà un deciso sì. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. Infine, vi invitiamo a consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di Deplain, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Mi chiamo Regina Stoden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con difficoltà di lettura o non madrelingua. Per addestrare un modello di semplificazione del testo, richiediamo coppie parallele di testi, ad esempio documenti o frasi. Nell'esempio qui mostrato, potete vedere una coppia di frasi parallele allineate di una frase tedesca complessa e della sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, la dilatazione delle clausole, la cancellazione incrociata, la riordinazione o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus Dplain perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di tassificazione. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono presentare errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus Dplane, che è suddiviso in due sottocorpora, Deplane APA e Deplane web. Deplane APA si basa su testi di utilizzo. In Deplane APA, abbiamo allineato manualmente 483 documenti, ottenendo circa trenta mila coppie parallele di frasi, tredicimila. Per deepplae web, questo corpus include diversi domini e allineiamo anche tutti questi settecentosettantacinque documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio sul tipo di semplificazione, come potete vedere qui, i testi biblici sono molto più fortemente semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue, a tutti i livelli, per quanto riguarda, ad esempio, la semplificazione lessicale, la semplificazione strutturale, anche il livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus di semplificazione profonda ha un'elevata varietà di diverse trasformazioni di semplificazione, ad esempio, nel corpus di deeppla API abbiamo molte più riordinamenti e aggiunte di parole rispetto al corpus di deep plane web. D'altra parte, nel corpus web abbiamo molte più parafrasi. Ora vediamo cosa possiamo fare con questo corpus. Salve, sono Omar e ora parlerò dei casi d'uso del nostro set di dati dplain. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati sviluppati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi nei documenti post-traduzione, nel nostro caso stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli che hanno la stessa lingua, lo stesso contenuto, ma sono a diversi livelli di complessità. E ora che abbiamo il nostro set di dati d plane che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti e abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nell'articolo. Abbiamo concluso che il metodo di allineamento automatico migliore da utilizzare per i testi di semplificazione del testo in tedesco è il metodo di mass align e potete trovare anche il codice per eseguire questo metodo sui vostri documenti nell'articolo. Il secondo caso d'uso che abbiamo mostrato nel nostro articolo è il caso della semplificazione automatica del testo mediante l'affinamento dei modelli linguistici per produrre un testo semplificato dal testo di input complesso. Abbiamo affinato due modelli diversi: abbiamo affinato il modello di long part per produrre semplificazioni a livello di documento e abbiamo anche affinato il normal base long, il normal base part per produrre semplificazioni a livello di frase. Potete trovare tutti i checkpoint e potete consultare ulteriori dettagli sui punteggi e sulle metriche di valutazione dei nostri esperimenti nell'articolo. Abbiamo concluso che questo affinamento di base poteva produrre o ottenere punteggi migliori rispetto ai punteggi di riferimento e abbiamo proposto questi risultati come benchmark, benchmark di base, per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Salve, sono X Yuan dell'Università FNAi. Sono qui per presentare il nostro lavoro, \"Distinguere la Conoscenza degli Script dai Modelli Linguistici Leggeri per la Pianificazione Linguistica Vincolata\". Nella vita di tutti i giorni, gli esseri umani devono spesso pianificare le proprie azioni seguendo istruzioni passo-passo sotto forma di script garantiti. Lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come preparare una torta, dimostrando che i modelli linguistici di grandi dimensioni possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per obiettivi astratti di attività stereotipate; la pianificazione per obiettivi con vincoli specifici, come preparare una torta al cioccolato, rimane ancora inesplorata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone vincoli diversi agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifunzionali. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei modelli linguistici leggeri. Poiché non esistono dati specifici per gli obiettivi da sfruttare, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifunzionali per l'acquisizione di dati con l'intervento umano. Usiamo Instruct GPT per campionare 100 obiettivi specifici e valutare gli script generati dai modelli di libreria. La tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli di apprendimento ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi, conduciamo un'analisi dettagliata per indagare su cosa falliscono i modelli di apprendimento. I risultati nella figura mostrano che la completezza semantica degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo argomenti più specifici di categorie di vincoli definiti in WiH. La mappa di calore nella figura mostra che le prestazioni di pianificazione di Instruct GPT variano considerevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di apprendimento cade in un'alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di generazione eccessiva con filtro per migliorare la qualità della generazione. Mostriamo innanzitutto tipi di vincoli con esempi per Instruct GPT e otteniamo obiettivi specifici basati sugli obiettivi astratti iniziali. Successivamente, Instruct GPT genera eccessivamente script chiave per obiettivi specifici. Quindi, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo script e obiettivi in embedding di Instruct GPT e calcoliamo similarità del coseno e punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione. Conserviamo solo lo script se l'obiettivo di destinazione ottiene il punteggio più alto nella dimensione dell'obiettivo. Con il nostro metodo, Instruct GPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di semantica, completezza che fedeltà al vincolo. Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale abilitare la pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passo essenziale a questo scopo. Tuttavia, studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale dei dataset è costosa. Per questo motivo, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un dataset di pianificazione linguistica vincolata da modelli linguistici leggeri. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata denominato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei siti di validazione e test, chiediamo a lavoratori provenienti da fonti esterne di trovare e rivedere i risultati errati. La figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un'elevata pluralità negli obiettivi specifici generati. Con CodeScript, possiamo trattare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che t5, messo a punto sul tasso di punteggio, può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che modelli più piccoli possono supportare modelli più grandi quando vengono adeguatamente addestrati su dataset adatti. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e abbiamo sviluppato un metodo di filtraggio della generazione eccessiva per modelli linguistici leggeri. Abbiamo utilizzato modelli linguistici di grandi dimensioni per generare un dataset di alta qualità, CodeScript, per la pianificazione linguistica vincolata. Speriamo che questo dataset possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Potete trovare maggiori dettagli su CodeScript nel nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Jannislavak e vi presenterò il nostro lavoro su Dr. Bert, un modello pre-addestrato robusto in francese per il dominio biomedico e clinico. In questa presentazione, inizieremo discutendo del language modeling nel settore sanitario. Successivamente, presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese denominato Dr. Bert, basato su Roberta e addestrato su nachtchos, un dataset di dati medici estratti dal web. Presentiamo anche un confronto del modello con diverse impostazioni criogeniche e fonti di dati. Quindi, presentiamo i nostri risultati su 11 task biomedici e clinici downstream in francese e, infine, concludiamo con le nostre osservazioni sugli esperimenti e vi forniremo maggiori dettagli su come accedere ai modelli, dal momento che è stato rilasciato nel 2018 Bert è diventato uno degli approcci più efficaci per risolvere i task di natural language processing e offre un enorme incremento di prestazioni rispetto ai metodi statici e contestualizzati storici come word2vec, fastText o simili. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT e altri domini come biomedico con PubMedBERT e BioBERT e in ambito clinico con ClinicalBERT, ma per le altre lingue i modelli specializzati sono scarsi e spesso basati su pre-training continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva alcun modello open source per il settore biomedico fino ad ora. Quindi, ci siamo posti la domanda: quali sono le fonti di dati più appropriate per un'ampia gamma di utilizzi? I dati grezzi sono una buona sostituzione per i dati clinici? Per rispondere a questa domanda, confrontiamo Dr. Bert con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale non universitario della nostra città. In seguito, ci siamo chiesti quanta dati siano necessarie per addestrare un modello specializzato in francese: quattro gigabyte, otto gigabyte o più? Per rispondere a questa domanda, abbiamo inizialmente addestrato e confrontato quattro modelli da zero: una prima versione di Dr. Bert con sette gigabyte di nachtchos, una seconda versione con quattro gigabyte di set di natureos, una prima versione di Schubert, un modello clinico con quattro gigabyte di frasi tratte da note cliniche e una versione finale di Schubert con una combinazione di quattro gigabyte di set di natureos e quattro gigabyte di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-training controllato per analizzare l'impatto della strategia di pre-training, uno basato sui pesi di CamemBERT e addestrato su quattro gigabyte di set di natureos, un altro anch'esso basato su CamemBERT ma addestrato questa volta su quattro gigabyte di note cliniche e infine uno basato su un modello biomedico inglese, BioBERT, e addestrato su quattro gigabyte di nachtchos. In totale, abbiamo sette modelli da valutare. Per i nostri sette modelli, raccogliamo task downstream pubblici e privati come named entity recognition, classificazione, part-of-speech tagging e question answering. Questi modelli sono confrontati con sei modelli di riferimento, tra cui CamemBERT, OSCAR 138 gigabyte, CamemBERT 4 gigabyte, CamemBERT cc net 4 gigabyte, PubMedBERT, BioBERT e ClinicalBERT. L’evoluzione evidenzia che i modelli funzionano meglio sui task con dati della stessa natura di quelli su cui sono stati addestrati; tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori. In generale, il pre-training da zero sembra ottenere prestazioni più elevate nella maggior parte dei task. Tuttavia, il nostro esperimento sul continuous pre-training utilizzando i pesi e il tokenizer di PubMedBERT addestrato su un sottoinsieme di quattro gigabyte di natureos mostra risultati comparabili a quelli ottenuti con Dr. Bert 4 gigabyte da zero, il che non è il caso dei modelli basati sui pesi e sul tokenizer di CamemBERT, che presentano problemi di stabilità. In conclusione, il nostro sistema specializzato offre prestazioni migliori su nove degli 11 task downstream e supera globalmente i risultati del modello generico CamemBERT. Osserviamo anche che i dati specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da nachtchos sono liberamente disponibili su una interfaccia You.com e tutti gli script di training sono presenti nel nostro repository GitHub. Quindi, grazie per questa presentazione e non vediamo l'ora di incontrarvi alla sessione poster a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "– Ciao, sono Xhang Bing, dottorando all'Università di Washington. Oggi vi presento il nostro lavoro, che parte dai dati di pre-addestramento ai modelli linguistici fino alle attività successive, tracciando le tracce di pregiudizi politici che portano a modelli di NLP ingiusti. Quindi, i modelli linguistici vengono addestrati su dati di web crawling su larga scala. Le notizie dei media politici sono ben rappresentate nei loro dati di pre-addestramento. Secondo un'indagine sul corpus c4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato un compromesso per le applicazioni dei modelli linguistici: da un lato, sono stati in grado di imparare da prospettive diverse, il che celebra la democrazia e una pluralità di idee; dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di attività successive. A tal fine, proponiamo di indagare sulla pipeline di propagazione del pregiudizio politico dai dati di pre-addestramento ai modelli linguistici alle attività successive, chiedendo specificamente le seguenti domande: in primo luogo, come valutiamo la propensione politica dei modelli linguistici e quale ruolo potrebbe avere il cruing data in tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse propensioni politiche sulle attività successive e se ciò potrebbe comportare problemi di equità nelle applicazioni NLP? Nello specifico, proponiamo innanzitutto di sollecitare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test della bussola politica. Ciò ci consente di eseguire una valutazione automatica ben fondata sulla letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che innanzitutto i modelli linguistici hanno effettivamente diverse propensioni politiche; occupano tutti e quattro i quadranti sulla bussola politica. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale tra tutti e la serie GPT è generalmente più socialmente liberale rispetto alla serie BERT e alle sue varianti. In secondo luogo, ci proponiamo di indagare in quale misura i pregiudizi politici dei modelli linguistici provengano effettivamente dai dati di addestramento. Pertanto, potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint dei modelli linguistici su sei diversi corpora partigiani separati in notizie e social media, ulteriormente suddivisi nelle loro propensioni politiche pre-addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico corrispondono anch'esse di conseguenza. Ad esempio, per Roberta ulteriormente ottimizzato e riaddestrato sul corpus reddit di sinistra, possiamo vedere un notevole spostamento liberale in termini dei suoi pregiudizi politici e proviamo anche a indagare se i modelli linguistici possono rilevare la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pre-addestramento nel periodo precedente al 45° presidente degli Stati Uniti e successivo al 45° presidente degli Stati Uniti e pre-addestriamo separatamente i modelli linguistici su entrambi i diversi corpora temporali. Possiamo vedere che i modelli linguistici hanno generalmente avuto una propensione politica più lontana dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche rilevare la polarizzazione nella nostra società. Ultimo ma non meno importante, valutiamo i modelli linguistici con diverse propensioni politiche sulla rilevazione di discorsi d'odio e fake news, applicazioni NLP che spesso coinvolgono modelli linguistici e che potrebbero avere implicazioni molto significative. Vediamo che se esaminiamo le prestazioni per categoria, ovvero se separiamo le prestazioni in diverse demografie o mezzi di comunicazione politici, possiamo vedere un modello per cui, ad esempio, per il rilevamento di discorsi d'odio, i modelli linguistici di sinistra sono migliori nel rilevare discorsi d'odio che prendono di mira gruppi socialmente minoritari, ma sono peggiori nel rilevare discorsi d'odio che prendono di mira gruppi più potenti nella nostra società e viceversa, i modelli linguistici di destra sono migliori nel rilevare discorsi d'odio che prendono di mira bianchi e uomini, ma peggiori nel rilevare discorsi d'odio rivolti a neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento di fake news, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dei loro avversari politici e viceversa. In questo mostriamo inoltre molti esempi qualitativi per vedere che i modelli linguistici con diversi significati politici forniscono effettivamente previsioni diverse su discorsi d'odio e esempi di disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che esiste un problema di equità molto urgente riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere ottimizzato su discorsi d'odio o disinformazione o qualunque cosa e distribuito su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso d'odio che prende di mira i gruppi minoritari potrebbe semplicemente diventare incontrollato. Questo suona l'allarme affinché riconosciamo e affrontiamo i problemi di equità derivanti dalle propensioni politiche dei modelli linguistici. Quindi, un po' di discussione. Vorremmo anche evidenziare che esaminiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici; è come tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici alle attività successive, creando in definitiva problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere conservato nei dati di addestramento del modello linguistico. È un po' come il problema del carrello elettrico. Ottimo, penso che sia abbastanza tutto per oggi. Cinque per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Koov Sinna e sono lieto di darvi il benvenuto alla presentazione del nostro articolo per ACL 23. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Waqui, Aaron Mueller, Kanishka Mishra, Karen Fs, Roger Levy e Atina Williams. Quindi, in questo lavoro, riprendiamo il paradigma delle coppie minime. Il paradigma delle coppie minime valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come in Blimp Syntax Gym, o l’accettabilità in termini di stereotipi, come in Crowds Pairs. E in questo paradigma delle coppie minime, il modo tipico per valutare i modelli linguistici è quello di mostrare una frase accettabile o una frase grammaticale e poi mostrare una frase inaccettabile o una frase non grammaticale e la speranza è che il modello assegni una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe. Al giorno d'oggi, i grandi modelli linguistici stanno producendo finestre di contesto sempre più lunghe, quindi è fondamentale che valutiamo l'accettabilità dei modelli attraverso l'intera finestra di contesto e questo è ciò che stiamo cercando di fare qui: stiamo cercando di riprendere la pipeline MPV chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi, questo è l'approccio. Ciò che facciamo è simulare queste sequenze più lunghe riprendendo i set di dati stessi e poi ricreiamo le frasi scegliendo, ad esempio, frasi accettabili o inaccettabili da tali set di dati. Ad esempio, qui abbiamo scelto una tipica coppia grammaticale di frasi dal set di dati Blim, dal caso dell'Isola Adjuntiva e quello che facciamo è ricreare sequenze più lunghe che siano accettabili e che abbiano la stessa struttura grammaticale estraendo frasi grammaticali dall'Isola Adjuntiva e aggiungendole come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dalla stessa struttura e questo potrebbe anche essere utilizzato per testare l'accettabilità del modello. Possiamo anche fare la stessa cosa scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso, in modo da definire quello che chiamiamo scenario di mismatch. Quindi, in questo caso, le frasi provengono ancora da set di dati pertinenti, ma non dallo stesso set di dati con cui si sta effettuando la valutazione. Possiamo fare la stessa cosa per il caso di inaccettabilità, scegliendo frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, ad esempio, se il contesto proviene da un sottoinsieme diverso del set di dati o se è completamente irrilevante rispetto alla frase che stiamo esaminando. Come si comporta il modello? Innanzitutto, osserviamo le frasi di Wikipedia, che sono completamente irrilevanti per la query pair corrente, e lì troviamo che i giudizi MPP sono per lo più robusti per una lunghezza del contesto arbitraria. Aumentiamo la lunghezza del contesto fino a mille e ventiquattro, per massimizzare le capacità di Ot e GPT-2. E qui vediamo, nella linea tratteggiata arancione, che i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso set di dati? Qui, stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso set di dati Blim Syntax Gym e lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili, ma quando si corrisponde la struttura, ovvero quando si scelgono frasi dallo stesso fenomeno in Blimp Syntax Gym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo e questo è molto grande, questo effetto aumenta con la lunghezza del contesto e questo probabilmente influenzerebbe i modelli linguistici più recenti, che hanno finestre di contesto ampie. Perché il prefisso corrispondente influenza così tanto il giudizio del modello linguistico? Abbiamo eseguito una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante ma aggiungendo rumore all'input e, dopo aver eseguito diverse di queste perturbazioni, scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare al modello il suo corso in termini di come mostra la tendenza del giudizio MPP. Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modo simile: quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio di approvazione accettabile, vediamo una diminuzione dei giudizi MPP, in modo simile. Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi e che la valutazione MPP, nel modo in cui la stiamo eseguendo attualmente con input brevi e di singola frase, potrebbe non catturare appieno la conoscenza astratta del modello linguistico attraverso l'intera finestra di contesto. Si prega di leggere il nostro articolo per ulteriori dettagli sui nostri esperimenti. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, sono Dawei, dottorando all'Università di Staland in Germania. In questo video vorrei presentare il nostro recente lavoro, \"Weaker than you think\", un'analisi critica dell'apprendimento con supervisione debole. Questo è un lavoro congiunto con X, Myos Mosbach e Ge Steffen e Dirich Klako. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole. Nella supervisione debole non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole come regole euristiche semplici, basi di conoscenza o crowd sourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni più deboli sono molto più economiche, ma anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente reti neurali su dati di etichettatura debole, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento con supervisione debole vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali in presenza di tale rumore dell'etichetta in modo che i modelli addestrati generalizzino comunque bene. In recenti lavori in wSL – dove wSL sta per weakly supervised learning – un'affermazione comune è che i modelli addestrati si basano esclusivamente sui dati di etichettatura debole e ottengono prestazioni elevate su set di test puliti. Tecnicamente, questa affermazione non è errata, ma c'è un aspetto da considerare: si presume che esista un set di validazione pulito aggiuntivo, ben scelto per la selezione del modello. Abbiamo esaminato questo problema, ma ciò implica la necessità di annotazioni manuali aggiuntive nell'apprendimento con supervisione debole, ma come un elefante nella stanza, questa necessità viene spesso trascurata. L'approccio menzionato sopra si pone tre domande di ricerca: innanzitutto, i dati di validazione puliti sono necessari per wSL o possiamo utilizzare un set di validazione annu invece? In secondo luogo, se i dati puliti sono necessari, ovvero se i dati puliti sono obbligatori affinché wSL funzioni, quanti campioni puliti ci servono? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti: innanzitutto, scopriamo che i recenti metodi wSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente, altrimenti si verifica un calo significativo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli addestrati non riescono a generalizzare oltre le etichette deboli originali, il che rende l'addestramento inutile. Ciò indica che gli approcci wsSL richiedono effettivamente dati etichettati puliti per funzionare correttamente e non dovrebbero essere trascurati i costi di annotazione per l'ottenimento di campioni di validazione puliti. La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti migliorerà le prestazioni degli approcci wSL, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere prestazioni elevate. Ma non è finita qui, perché se decidiamo in ogni caso di accedere a campioni puliti, addestrarli direttamente otterrà prestazioni ancora migliori. La figura rossa mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente ai dati puliti, e gli approcci wSL, che utilizzano i dati puliti solo per la validazione. Come si può vedere, se abbiamo dieci campioni per classe, il fine-tuning inizia a superare gli approcci wSL. Infine, il miglioramento delle prestazioni rivendicato negli approcci wSL precedenti può essere facilmente raggiunto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come si può vedere dalle figure, il modello di validazione denominato FTW inizialmente sottoperforma rispetto ai metodi wSL più complessi come cosine. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, FTW ottiene prestazioni equivalenti a quelle degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi wSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci wSL richiedono campioni annotati manualmente e puliti per funzionare correttamente; il loro guadagno di prestazioni e la loro praticità sono fortemente sovrastimati. Le nostre raccomandazioni concrete per il futuro lavoro sono le seguenti: innanzitutto, indicare i criteri di selezione del modello, ad esempio, indicare se la selezione del modello viene eseguita con campioni di validazione puliti. In secondo luogo, gli approcci wSL dovrebbero essere confrontati con linee di base few-shot learning, come ad esempio il lavoro su campioni concreti. In terzo luogo, il fine-tuning continuo è una linea di base semplice ma efficace che dovrebbe essere presa in considerazione in futuro nel wSL. Infine, abbiamo reso il nostro codice open source. Potete trovarlo tramite il codice QR in questa diapositiva. Sentitevi liberi di consultarlo. Grazie e buona conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Al Villaard e vi fornirò una breve panoramica dell'articolo riguardante il prompting di P dalla traduzione, valutando strategie e prestazioni. Si tratta di un lavoro congiunto con i miei colleghi di Google Translate. PM è un modello linguistico con 540 miliardi di parametri, presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testi comprendente 780 miliardi di token. Al momento della pubblicazione, raggiunge lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo uno studio sistematico e gratuito del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica. Valutiamo la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità IMT. Ciò comporta l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico e confrontiamo due sistemi all'avanguardia, ovvero i sistemi con le migliori prestazioni nella valutazione WMT. Utilizziamo metriche neurali di traduzione automatica all'avanguardia e mostriamo anche i risultati della valutazione umana basata sull'esperienza. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha una grande influenza sulle prestazioni degli LLM per la traduzione, come si può vedere in un semplice esperimento in cui utilizziamo un prompting breve e forniamo due prompt diversi per frasi diverse. Nella maggior parte delle frasi, 516 su 1000, la differenza osservata è di più di un punto BLEU. E in casi estremi, può arrivare fino a 40 punti BLEU. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a cinque colpi, in cui indichiamo semplicemente la lingua della frase che forniamo al sistema. In questo esempio, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con \"tedesco:\" e le traduzioni in inglese con \"inglese:\". Abbiamo riscontrato che la forma effettiva del prompting non ha una grande influenza nel caso di prompting breve. È fondamentale per il prompting a zero e a uno colpo, ma quando si passa, come nel nostro caso, a un prompting a cinque colpi, c'è quasi nessuna differenza rispetto alla forma effettiva del prompting; sono gli esempi a portare la maggior parte del peso. Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Pertanto, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompting dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, e i risultati mostrano una migliore performance nell'utilizzo dei dati di sviluppo. Ciononostante, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di Palm, ma Palm si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le informazioni che abbiamo ottenuto dalla valutazione eseguita utilizzando il framework NpN indicano che la fluidità di Palm è paragonabile a quella dei sistemi all'avanguardia, ma la differenza principale risiede nell'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembra che Palm scelga di produrre una traduzione dal suono migliore a volte omettendo parti della frase di origine. Tuttavia, lo score di stile di Palm è inferiore rispetto a quello dei sistemi all'avanguardia, il che è un segnale aggiuntivo che Palm fornisce un output davvero fluido, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, vi invitiamo alla presentazione completa dell'articolo. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Jin Wei Y, sono dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video promozionale del nostro articolo. State copiando il mio modello, \"Protecting the copyright of large Lang models for embedding and services\"? Vi proteggiamo con il watermark. Introdurremo innanzitutto il contesto relativo agli embedding e ai servizi. Attualmente, i grandi modelli linguistici come Gbt, La, PLm eccellono nella comprensione e nella generazione del linguaggio naturale. Gli embedding e i servizi correlati sono uno dei servizi costruiti su grandi modelli linguistici per assistere varie attività di NLP. Ad esempio, OpenI offre un’API di embedding basata su Gbt, ma studi recenti hanno dimostrato che un attaccante può rubare il modello apprendendo dagli embedding e fornendo servizi simili. Pertanto, è necessario proteggere il copyright degli embedding come servizi. Per proteggere il copyright degli embedding services, una delle soluzioni è incorporare un watermark nel servizio del provider e rilevare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà: in primo luogo, il metodo deve essere applicabile agli embedding come servizi; in secondo luogo, il watermark non deve degradare l’utilità degli embedding forniti; in terzo luogo, il watermark deve essere sufficientemente resistente, oppure l’attaccante può rimuoverlo facilmente; infine, il watermark deve essere trasferibile ai servizi dell’attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie, ma questi metodi o non sono applicabili agli embedding come servizi, o mancano di trasferibilità. In questo articolo, proponiamo Embedding Marker, un metodo di watermark basato su backdoor applicabile agli embedding come servizi. Permettetemi ora di introdurre i dettagli del nostro Embedding Marker. Embedding Marker contiene due passaggi principali: l'iniezione del watermark e la verifica del copyright. Prima di questi passaggi principali, selezioniamo innanzitutto un trigger set. Il trigger set è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il provider possa raccogliere un corpus di testo generale e contarne la frequenza delle parole. Durante l'iniezione del watermark, definiamo innanzitutto un embedding target. Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se il modello alla base di un altro servizio contiene il watermark. Costruiamo innanzitutto un backdoor e un dataset benigno. Il dataset del backdoor contiene frasi di cui tutte le parole appartengono al trigger set, mentre tutte le parole nelle frasi del dataset benigno non appartengono al trigger set. Quindi, il provider richiede embedding dal servizio dell'attaccante con il dataset. Vengono calcolate la similarità del coseno e la similarità L2 tra l'embedding richiesto e l'embedding target. Calcoliamo la differenza di similarità tra il dataset benigno e il dataset del backdoor, definita come Delta coseno e delta L2. Applichiamo inoltre il test KS e utilizziamo il suo p-value come terza metrica. Abbiamo condotto esperimenti su quattro dataset: Aaging news, Mind SD2 e A spam. Assumiamo che il provider applichi il dataset Wiki Text per contare la frequenza delle parole. I risultati sui quattro dataset dimostrano che il nostro Embedding Marker può ottenere ottime prestazioni di rilevamento mantenendo un'ottima utilità per le attività downstream. Abbiamo inoltre convalidato la resistenza degli embedding forniti visualizzando gli embedding delle frasi sui quattro dataset B PCA. La legenda delle figure indica il numero di trigger in ogni frase, come mostrato nelle figure. È difficile distinguere tra gli embedding modificati e quelli normali. Questo è tutto. Grazie. Siamo a disposizione per discuterne."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Ian e io e la mia collega Jiian presenteremo la nostra ricerca sul multi-instruct, ovvero il miglioramento dell'apprendimento seriale multimodale tramite instruction tuning. Con i progressi nei modelli linguistici di grandi dimensioni, molti studi hanno iniziato a esplorare nuovi paradigmi di apprendimento che utilizzano modelli linguistici pre-addestrati per diversi task downstream in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'instruction tuning permette ai modelli linguistici di grandi dimensioni di eseguire task inediti in modalità serialshot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si concentra sul miglioramento delle prestazioni serialshot in task esclusivamente linguistici, trascurando la computer vision e i task multimodali. Pertanto, in questo lavoro, vogliamo investigare se l'instruction tuning su modelli proteici multimodali possa effettivamente migliorare la generalizzazione a task multimodali inediti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra P e multimodali: esistono più di 1600 task esclusivamente linguistici, mentre non esiste un dataset multimodale di istruzioni su larga scala disponibile pubblicamente. Ciò ci ha motivati a creare un dataset di instruction tuning multimodale. Qui presentiamo multi-ins instructct, il primo benchmark dataset di instruction tuning multimodale che consiste di 62 diversi task multimodali che coprono 10 ampie categorie. Questi task sono derivati da 21 dataset open source esistenti e ogni task è dotato di cinque istruzioni scritte da esperti per investigare l'instruction tuning sul nostro dataset proposto. Prendiamo offa, un modello di training multi-modale unificato, come modello base. offa utilizza un vocabolario unificato per token linguistici, immagini e le coordinate di un bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset multi-instra. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di offa e formuliamo tutti i task in un formato unificato sequence-to-sequence, in cui il testo, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Ora parlerò dell'instruction tuning multimodale. Per il dataset di training, utilizziamo 53 task del gruppo N per l'addestramento e campioniamo 10.000 istanze per task per il testing. Riserviamo l'intero gruppo di lettura del senso comune per il testing e selezioniamo ulteriori cinque task da Wiki e dal gruppo miscellaneous. Utilizziamo tutte le istanze nella test set per ogni task, in aggiunta, campioniamo casualmente 20 task dalla test set di natural instruction come task NP. Utilizziamo un modello di grandi dimensioni pre-addestrato come modello base durante l'addestramento. Mescoliamo tutte le istanze per tutti i task: ogni istanza viene combinata casualmente con uno dei suoi cinque template di istruzione. Durante il test, per ogni task, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Segnaliamo la media e il massimo delle prestazioni e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti. Se il task è un task di classificazione multimodale, segnaliamo l'accuratezza. Se è un task di generazione multimodale, segnaliamo la radice quadrata dell'errore quadratico medio (RMSE). Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensitivity, che misura la capacità del modello di produrre costantemente gli stessi output per lo stesso task, indipendentemente da lievi variazioni nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'instruction tuning può migliorare significativamente le prestazioni di offa su task multimodali. Inoltre, il transfer learning dal dataset di istruzioni naturali può avvantaggiare l'instruction tuning. Qui possiamo vedere come l'aumento del numero di task porta a prestazioni migliori e, allo stesso tempo, a una sensitivity inferiore. Abbiamo anche condotto un esperimento utilizzando un'istruzione rispetto a cinque istruzioni: come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensitivity. Questo dimostra l'effetto di diverse strategie di fine-tuning sulla sensitivity del modello. Possiamo anche vedere che il transfer learning dal dataset di istruzioni naturali permette a offa di ottenere una sensitivity molto migliore rispetto al modello offa originale. Possiamo anche vedere che il transfer learning dal dataset di istruzioni naturali può aiutare offa a raggiungere prestazioni migliori sul dataset di nitrogen instruct. In definitiva, proponiamo il primo dataset di instruction tuning multimodale su larga scala, miglioriamo significativamente le capacità neurali di offa, esploriamo diverse tecniche di transfer learning e mostriamo i loro vantaggi. Abbiamo progettato una nuova metrica chiamata sensitivity. Un'ultima cosa: stiamo raccogliendo un dataset di instruction tuning multimodale molto più grande con circa 150 task aggiuntivi in variante linguistica, che rilasceremo. Questo è un codice QR per il nostro dataset e il nostro modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti. Mi chiamo Just John, dell'Università della Pennsylvania (Penn State University). Oggi vi presenterò il nostro lavoro, ExAmplEr: Semantic parsing cross-linguale in molteplici lingue naturali e rappresentazioni manuali. La semantic parsing è un compito che consiste nel costruire rappresentazioni semantiche di query utente, come SQL e calcolo lambda. E la semantic parsing cross-linguale è il compito di tradurre query in molteplici lingue naturali in molteplici rappresentazioni del significato, come illustrato in questa figura. Abbiamo bisogno di tradurre la query in molteplici lingue naturali utilizzando modelli neurali in SQL, lambda, funql, ecc. Gli attuali modelli di semantic parsing cross-linguale sono proposti e valutati separatamente su dataset di dimensioni limitate e con applicazioni ristrette. Ad esempio, si verificano lacune di copertura in certe lingue naturali – il cinese è assente – oppure lacune di copertura in certe rappresentazioni, come il calcolo lambda, oppure vengono valutati solo su certi modelli neurali, ad esempio, solo un singolo modello per la valutazione. A tal fine, proponiamo ExAmplEr, un dataset uniforme per la semantic parsing cross-linguale in molteplici lingue naturali e rappresentazioni del significato. Esso contiene nove dataset in diversi domini, cinque task di semantic parsing, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è “translate-test”. Utilizziamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo un modello inglese su query in inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere il SQL. Testiamo anche modelli monolingui. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione di fusione monolingue, addestrando modelli monolingui con solo il 10% dei dati di addestramento e testiamo un modello multilingue, che addestriamo per tutte le lingue, ad esempio, mettiamo insieme query in tedesco, inglese e cinese per addestrare un modello multilingue e, durante l'inferenza, possiamo utilizzare questo modello per tradurre query in tedesco, cinese, ecc. Consideriamo anche il trasferimento zero-shot e few-shot cross-linguale. Addestriamo su una lingua di origine e trasferiamo un'altra lingua, quindi, durante l'addestramento, lo addestriamo su query in inglese o sulla combinazione di query in inglese e tedesco per few-shot, per addestrare un modello multilingue e prevedere l'output SQL. Abbiamo anche riscontrato molti risultati interessanti. Riguardo all'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, tra cui encoder-decoder con puntatore (Pointer-based decoders), come XLM-RoBERTa-PDDR e BART-PDDR, e valutiamo modelli encoder-decoder, che sono modelli encoder-decoder multilingue, come MBart e MT5. Abbiamo scoperto che gli encoder-decoder ottengono le migliori prestazioni su tutti e nove i dataset. Valutiamo MT5 e XLM-RoBERTa-PDDR in un'impostazione multilingue. Abbiamo scoperto che gli encoder-decoder o gli encoder-decoder con puntatore possono essere migliorati addestrandoli in una miscela di varie lingue. Questo perché la maggior parte delle principali lingue naturali possono ottenere un miglioramento delle prestazioni, ad eccezione del fatto che le prestazioni in inglese diminuiscono in sette dataset e ottengono miglioramenti solo in tre dataset. Credo che questo sia noto come curve della multilinguità. Abbiamo anche confrontato il gap di performance cross-linguale. In questa figura, la linea blu è il trasferimento few-shot cross-linguale, la linea arancione è il trasferimento zero-shot cross-linguale, mentre la linea verde è nell'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e la linea arancione, il gap di performance del trasferimento zero-shot è significativo e, confrontando la linea blu e la linea arancione, abbiamo scoperto che con il trasferimento few-shot, il gap di trasferimento si riduce rapidamente. Abbiamo anche riscontrato alcune altre scoperte interessanti, ad esempio gli encoder-decoder superano i lavori precedenti o raggiungono risultati comparabili e l'addestramento del nostro modello in lingua naturale inglese può migliorare significativamente le prestazioni few-shot sulle lingue naturali target. Abbiamo scoperto che modelli linguistici multilingue come coder e blue sono ancora inadeguati per la semantic parsing cross-linguale. In sintesi, abbiamo costruito ExAmplEr, un benchmark unificato per la semantic parsing cross-linguale con molteplici lingue naturali e rappresentazioni del significato. Abbiamo condotto uno studio benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue e i nostri risultati mostrano molte scoperte interessanti, ecc. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Shrikkovski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come sapete, diverse teorie e approcci basati su corpora presuppongono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali, la struttura della coordinazione “Lisa, Bart e Maggie” è tale che il primo congiunto è la testa dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del significato testuale di Igor Milchuk, dove, ancora una volta, l’intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici, giusto? Singolano uno dei congiunti. Esistono anche approcci simmetrici alle strutture di coordinazione, come l’approccio pragmatico, l’approccio alla testa della congiunzione assunto nei dependency treebank pragmatici, dove le strutture coordinate sono guidate dalla congiunzione. In questo modo, otteniamo dipendenze dall'elemento di collegamento a tutti i congiunti. Infine, c'è anche un approccio multi-testa, utilizzato ad esempio nella word grammar di Cutson, dove, diciamo, tutti i congiunti sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore che ama a tutti i congiunti separatamente. Queste sono le premesse. L'obiettivo di questo articolo è produrre un argomento nuovo a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. L'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza, che spiegherò sulla base di questi esempi. In inglese, come potreste sapere, i complementi oggetto diretto preferiscono essere vicini al verbo, mentre i complementi avverbiali possono essere più lontani. Ad esempio, \"March lesse il giornale ieri\" va bene, perché il complemento oggetto diretto è vicino al verbo, mentre \"March lesse ieri il giornale\" è molto peggiore. Qui, tra il verbo e il complemento oggetto diretto, c'è un complemento avverbiale, “ieri”. Tuttavia, questo effetto può essere mitigato quando il complemento oggetto diretto è molto pesante e lungo, perché può essere spostato dopo il complemento avverbiale. Questo è illustrato qui. Entrambe queste frasi vanno bene: “March lesse questo libro assolutamente affascinante su api ieri” è okay, così come “March lesse ieri questo libro assolutamente affascinante su api”. Il ragionamento è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che i complementi oggetto diretto debbano essere vicini al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che sono preferite dipendenze più corte. Quindi, queste due alberi mostrano solo la lunghezza delle dipendenze cruciali, cioè quelle che non sono costanti tra queste due strutture. Qui abbiamo la dipendenza da \"lesse\" al complemento avverbiale di lunghezza sette, misurata in parole, e da \"lesse\" a \"libro\" di lunghezza quattro. In totale sono undici. Quando si sposta, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa sei, quindi molto più corta. Ecco perché suona abbastanza bene, viola un principio ma ne soddisfa un altro. Okay, quindi abbiamo estratto varie statistiche dalla coordinazione, dalla versione migliorata di Pen di PenTreeBank e spiegheremo perché non abbiamo usato le dipendenze universali. Queste statistiche confermano l’osservazione fatta molte volte prima che i congiunti sinistri tendano a essere più corti, salt and pepper, non pepper and salt misurato in sillabe, e anche l’osservazione che questa tendenza aumenta con la lunghezza, la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti aumenta, il congiunto sinistro preferisce essere il più corto, più forte, quindi la proporzione dei congiunti sinistri corti è maggiore. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è assente a sinistra. Il governatore è a sinistra in questo esempio: “Ho visto Bar e Lisa”, quindi “ho visto” è il governatore ed è assente nel secondo esempio: \"Homer venne e starnutì\". Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. In tali casi, il congiunto sinistro preferisce essere più corto, tanto più grande è la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come in questo caso, \"Governa la coda della coordinazione e la rete\", questo effetto scompare. Lo dimostriamo misurando la lunghezza in caratteri, la prima colonna in sillabe, la colonna centrale e in parole la colonna di destra. Mi concentrerò sulla colonna di destra. Ciò che vediamo è che quando il governatore è a sinistra, la tendenza a rendere il congiunto sinistro più corto cresce costantemente con la differenza assoluta in parole e lo stesso viene osservato quando non c'è nessun governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare e dimostriamo nel documento come questo fornisca un argomento contro le strutture di coordinazione asimmetriche, come queste due, a favore delle strutture simmetriche come queste due. Consultate il documento per l'accordo e gli argomenti completi, e parliamo della sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno, mi chiamo Kyo Yin e presenterò il nostro lavoro intitolato \"Quando la Traduzione Richiede il Contesto: Un'Esplorazione Multilingue Basata sui Dati?\". Questo lavoro è stato realizzato in collaborazione con Patrick Ferange, Emiliu, Andre F.D. Martins e Graham Newbigging. Quindi, molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase? Se la frase precedente fosse \"le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono\", allora \"mole\" si riferisce a una spia, ma se la frase precedente fosse \"potrebbe essere qualcosa di serio, dottore?\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione. Tuttavia, valutare quanto bene i modelli riescano a tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola porzione di traduzioni dipende dal contesto, il che rende le metriche a livello di corpus, come BLEU, incapaci di catturare queste traduzioni. Alcuni hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni con dipendenza dal contesto e un numero limitato di lingue, poiché in genere si basano sulla conoscenza del dominio e sulla curatela umana. In questo lavoro, cerchiamo di rispondere a queste due domande: prima, quando la traduzione richiede il contesto? e seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto un testo dipenda dal contesto nella traduzione. Nel nostro lavoro precedente abbiamo introdotto cxmi come misura dell'uso del contesto da parte dei modelli di traduzione automatica, misurando quanto informazioni il contesto C fornisce sulla traduzione target y, dato il testo sorgente x. Potete pensare a cxmi come all'informazione acquisita fornendo un contesto al modello. In questo lavoro, estendiamo cxmi a point y, cxmi, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un cxmi elevato come quelle che richiedono un contesto per la traduzione. Ora analizziamo le parole con un cxmi elevato per individuare modelli tra queste parole e conduciamo la nostra analisi su trascrizioni di TED Talks che sono state tradotte dall'inglese in 14 diverse lingue. Conduciamo la nostra analisi a tre diversi livelli. Innanzitutto, osserviamo le etichette delle parti del discorso che hanno un cxmi medio elevato, il che ci consente di trovare, ad esempio, pronomi duali in arabo che hanno un cxmi relativamente elevato. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario un contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che anche alcune lingue richiedono un contesto quando vogliamo scegliere la forma verbale appropriata. Quindi, esaminiamo gli elementi del vocabolario che hanno un cxmi elevato, in media su tutte le sue diverse occorrenze, il che ci aiuta a identificare casi come quello qui, in cui in cinese è necessario un contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è utile per tradurre con la forma di cortesia appropriata. Infine, esaminiamo diversi token individuali che hanno un cxmi elevato, il che ci consente di identificare fenomeni che non possono essere realmente catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi. Ora utilizziamo i nostri risultati dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, abbiamo creato dei tagger per identificare automaticamente le parole che appartengono al fenomeno. Chiamiamo il nostro tagger Multilingual Discourse Aware, o MUDA tagger. Possiamo anche notare che le diverse lingue hanno proporzioni diverse di questi fenomeni del discorso. Quindi, utilizziamo il tagger MUDA applicando il tagger su un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MUDA ha identificato. Infine, utilizziamo il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento. Innanzitutto, quando utilizziamo metriche a livello di corpus, quindi BLEU, troviamo che i modelli senza contesto hanno le migliori prestazioni. Ma poi, se utilizziamo comment, i modelli con contesto performano meglio. E se utilizziamo la misura Wordf, allora i modelli con o senza contesto hanno prestazioni comparabili. Ciò dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus. Ora utilizziamo il benchmark MUDA per valutare i modelli e scopriamo che i modelli con contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per determinati fenomeni del discorso, come la formalità e la coesione lessicale, ma questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni, come i pronomi dell'ellissi e le forme verbali. Ciò suggerisce in qualche modo dove dovremmo vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che dl è solitamente più accurato di Google Translate per la traduzione a livello di documento. In sintesi, conduciamo un'analisi basata sui dati su 14 coppie linguistiche per identificare quando le traduzioni richiedono il contesto e poi utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli riescono a gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione, vi vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa di dottorato del primo anno alla Carnegie Mel University, e oggi vi presenterò il nostro lavoro su AnL positionality, caratterizzando i bias di progettazione e i set beta dei modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronin Labrasse, Katharina Reinika e Martin Sapp. Quindi, cominciamo immaginando che stiate lavorando per un giornale e state esaminando i commenti sotto il vostro articolo cercando di rimuovere contenuti tossici. Potreste ricorrere a un'API popolare come Prospective API per il rilevamento della tossicità. Questo funziona davvero bene se siete Carl Jones, perché Prospective API è in grado di rilevare correttamente le istanze tossiche, ma non è lo stesso per didtha Sharma, dove Perspective API non è particolarmente sensibile ai termini offensivi più comuni in contesti indiani. Questo è un esempio di bias di progettazione, dove osserviamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni. Bias di progettazione come quello che abbiamo appena visto potrebbero derivare dalla positionality dei ricercatori NLP e degli sviluppatori di modelli. La positionality è semplicemente l'insieme delle prospettive che le persone detengono a seguito della loro demografia, identità ed esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E in quanto ricercatori, la positionality può influenzare il processo di ricerca e i suoi risultati perché può modificare le decisioni che i ricercatori prendono. Quindi, una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno una positionality? Non stiamo cercando di dire che i modelli stessi e i set di dati stessi hanno identità demografiche ed esperienze di vita, ma essi sì, aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate positionality rispetto ad altre. Lavori precedenti hanno suggerito alcune prove aneddotiche di positionality, come lacune culturali nei modelli e nei set di dati, nonché definizioni teoriche di positionality del modello. Tuttavia, questi lavori non considerano realmente il confronto tra utenti finali, set di dati e modelli stessi e lo studio della positionality dei modelli e dei set di dati sta diventando sempre più importante man mano che le attività NLP diventano più soggettive e socialmente orientate. Ed è impegnativo caratterizzare come questi positionality siano distorti perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la positionality dei set di dati e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con set di dati e modelli esistenti. Lo facciamo attraverso il nostro framework Nl positionality. Il nostro framework funziona in due passaggi principali: il primo passaggio è quello di ri-annotare i set di dati con annotatori diversi e scegliamo di farlo invece di esaminare la demografia degli annotatori dei set di dati originali perché di solito solo pochi annotatori annotano ogni istanza e perché la demografia è raramente raccolta e condivisa. Quindi, scegliamo di ri-annotare i dati per ottenere molte annotazioni per istanza e per ottenere un ricco set di dati demografici. Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il punteggio di correlazione di Pearson. Quindi, il nostro framework differisce dalla letteratura sulla discordia degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, anziché guardare solo all'accordo tra gli annotatori o alla modellazione delle distribuzioni degli annotatori. Il nostro framework è ampiamente abilitato da lab in the wild, una piattaforma online di crowdsourcing. Un collaboratore precedente in HCI e lab in the wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come Turk, che hanno in gran parte partecipanti provenienti dagli Stati Uniti o dall'India, e inoltre lab in the wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo due task su lab in the wild, uno dei quali è l'accettabilità sociale e il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati di social chemistry e poi scriveranno quanto una situazione sia socialmente accettabile. Per rimanere impegnati nello studio, possono confrontare le loro risposte con un'IA e con altri. Confrontiamo quindi queste annotazioni con social chemistry Delphi e gPT4. Abbiamo quindi replicato una configurazione molto simile per il compito di rilevamento della tossicità e dell'odio, dove leggeranno un'istanza da Dynah Hate e scriveranno se pensano che sia un'istanza di odio. Confrontiamo quindi queste annotazioni con Dyna Hate Perspective API, Rewire API, Hate Roberta e GPT4. Il nostro studio, alla fine, ha accumulato oltre 16.000 annotazioni da oltre mille annotatori provenienti da ottantasei paesi. Quindi, siamo ora meglio equipaggiati per rispondere a chi si allineano di più i set di dati e i modelli NLP? Scopriamo che c'è positionality in NLP. Ad esempio, scopriamo che i set di dati e i modelli si allineano maggiormente ai paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di gpd four, scopriamo che si allinea maggiormente ai paesi di lingua confuciana e inglese. Scopriamo anche che Dinah hate si allinea maggiormente ai paesi di lingua inglese. Scopriamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi, per gpd4 nel compito di accettabilità sociale, scopriamo che si allinea maggiormente con le persone con un'istruzione universitaria o un'istruzione post-laurea e troviamo la stessa cosa per Danny hate dove si allinea maggiormente con le persone con un'istruzione universitaria. Tuttavia, quando modelli e set di dati si allineano a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di questo è che i set di dati e i modelli si allineano meno con le persone non binarie rispetto alle controparti maschili e femminili. Lo scopriamo nel compito di accettabilità sociale di gPDd4 e nell'analisi del compito di dina hate. Quindi, dato che c'è physician anality in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni al riguardo. La prima è tenere un registro di tutte le scelte di progettazione rilevanti durante l'intero processo di ricerca e la seconda è di condurre ricerche NLP con la lente del prospectivismo. La nostra terza raccomandazione è di costruire set di dati e modelli specializzati all'interno di specifiche comunità e un buon esempio di questo è l'iniziativa masakanne. E vogliamo sottolineare che NLP inclusivo non significa solo fare in modo che tutte le tecnologie funzionino per tutti. Quindi, questo conclude la nostra presentazione, ma se desiderate saperne di più, sentitevi liberi di visitare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ciao e parlerò del nostro lavoro sulla risoluzione di espressioni differenziali indirette per la selezione di entità, in cui introduciamo il corpus di entità alternative. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Philip Radlinsky, Sylvia Parity e Annie Luis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. E considero questa domanda alternativa: Intendevi \"Easy on Me\" o \"I got a feeling\"? Qui, un utente vuole selezionare una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone \"Easy on Me\" o la sua posizione, la prima, ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone o quando le pronunce sono troppo simili l'una all'altra e difficili da disambiguare, o quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette. Ad esempio, quella più recente o il genere che non è energico. Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking della comprensione delle entità da parte dei modelli linguistici di grandi dimensioni (LLM). Non siamo a conoscenza di un set di dati pubblico, un set di dati pubblico su larga scala per questo compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione tramite crowdsourcing. Il nostro set di dati copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un setup di completamento di cartoni animati. Il cartone animato ha tre fumetti: nella prima bolla Bob dice: \"Ricordi quella canzone che stavamo ascoltando ieri?\". Con questo, Bob imposta il contesto del dialogo. Nella seconda bolla, Alice dice: \"Intendevi 'Easy on Me' o 'I got a feeling'?\", che è la domanda alternativa. Nella terza bolla, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio \"la più recente\". Forniamo la prima e la seconda bolla automaticamente, ma la terza viene compilata dall'annotatore. La prima bolla è scelta da alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, viene generata come segue: usiamo sempre un semplice modello: \"Intendevi a o b\", dove a e b sono campioni tratti da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Man mano che ci spostiamo più in alto nella lista, le entità diventano più simili l'una all'altra ed è solitamente più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome \"Return\". Il terzo è quando hanno descrizioni simili su Wikipedia e, infine, quando hanno informazioni simili (info boxes o attributi) su Wikipedia, ad esempio lo stesso genere o lo stesso artista per le canzoni. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente informazioni su di esse. Quindi, quello che facciamo è mostrare alcune conoscenze di base su entrambe le entità. Per le canzoni, mostriamo semplicemente un collegamento alla ricerca di Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ciascuna canzone e di informarsi su ciascuna. Ecco, ad esempio, il risultato della ricerca di Google per la canzone \"Easy on Me\". Per i domini di ricette e libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e di descriverla usando da tre a cinque espressioni di riferimento indirette, ad esempio \"quella con la musica del pianoforte\". Ecco alcuni esempi dal nostro set di dati: ad esempio, quella senza parole, non quella con il dodicenne, quella fittizia o quella proveniente dall'Azerbaigian e così via. Il corpus di entità alternative ha 6.000 domande alternative in tre domini e 422.000 espressioni di riferimento indirette. I risultati con il modello T5x Large sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse identiche conoscenze di base degli annotatori, l'accuratezza è molto alta, intorno al 92-95 percento, ma questo non è realistico. Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, l'accuratezza è compresa tra l'82 e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, l'accuratezza è solo del 6 percento, quindi c'è ancora molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili a diversi domini. Ecco un collegamento al nostro set di dati. Grazie per l'attenzione."}
