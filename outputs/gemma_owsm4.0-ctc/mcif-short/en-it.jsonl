{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao! Benvenuti alla nostra presentazione di Deplane, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Mi chiamo Regina Stoden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione dei testi."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "La ramificazione è un processo di adattamento di un testo volto a migliorarne la comprensibilità per un gruppo di riferimento specifico, come persone con difficoltà di lettura o non madrelingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Per addestrare un modello di textificazione, è necessario disporre di coppie parallele di testo, ad esempio documenti o frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "In questo esempio, è possibile notare una frase allineata in parallelo, costituita da una complessa frase tedesca e dalla sua traduzione odierna in linguaggio semplice."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "semplificare la frase sono possibili diverse tecniche, come si può notare nell'esempio: sostituzione lessicale, ampliamento della clausola, cancellazione, riordino o inserimento di costrutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "ora proponiamo il nostro nuovo corpus di dati, poiché negli ultimi anni sono emersi alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di tassonomia."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nell'allineamento."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "proponiamo il nostro nuovo corpus D planee, suddiviso in due sottocorpora: Dplane APA e Dplane web. D planee APA si basa su testi d'uso."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "In Depla APA, abbiamo allineato manualmente 483 documenti. Il risultato è di circa 30.000 coppie di frasi parallele, ovvero 13.000."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "deep plane web. Questo corpus include diversi domini e abbiamo inoltre allineato tutti questi 750 documenti, da un lato manualmente e, dall’altro, con metodi di allineamento automatici."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "In totale, otteniamo 30.450 coppie di frasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo analizzato le nostre coppie di frasi in modo leggermente più approfondito, ad esempio per quanto riguarda il tipo di notifiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "qui si può notare come i testi biblici siano notevolmente più semplici e accessibili rispetto, ad esempio, ai testi di cronaca o ai testi per studenti di lingua."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "tutti i livelli, per esempio la semplificazione lessicale, la semplificazione strutturale e anche la semplificazione a livello generale."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "si può notare che il nostro corpus di pianificazione profonda presenta un'ampia varietà di diverse trasformazioni di semplificazione; ad esempio, nel corpus API di pianificazione profonda abbiamo molti più riordinamenti e aggiunte di radici rispetto a quanto presente nel corpus web di pianificazione profonda."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "d'altro canto nel corpus web abbiamo molte più riformulazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Vediamo ora cosa possiamo fare con questo corpus. Salve, sono Omar e ora parlerò dei casi d'uso per il nostro dataset dLAN. Per il primo caso d'uso, possiamo valutare metodi di allineamento automatico."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Negli ultimi anni, sono stati sviluppati numerosi metodi di allineamento, ma nel contesto della traduzione automatica."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "dove abbiamo due documenti paralleli scritti in lingue diverse e desideriamo estrarre gli allineamenti di frasi nei documenti post-editati."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ma, nel nostro caso d'uso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, nella stessa lingua, con lo stesso contenuto, ma con un diverso livello di complessità."}
{"dataset_id": "mcif_v1.0", "sample_id": 21, "src_lang": "en", "tgt_lang": "it", "output": "E ora che abbiamo a disposizione il dataset deepplan, contenente frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento (gold standard) per valutare alcuni dei metodi di allineamento proposti."}
{"dataset_id": "mcif_v1.0", "sample_id": 22, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 23, "src_lang": "en", "tgt_lang": "it", "output": "alla fine, siamo giunti alla conclusione che il metodo di allineamento automatico più efficace da utilizzare per la semplificazione del testo in tedesco è il metodo di allineamento di massa."}
{"dataset_id": "mcif_v1.0", "sample_id": 24, "src_lang": "en", "tgt_lang": "it", "output": "È inoltre possibile trovare il codice per eseguire questo metodo sui propri documenti nell'articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 25, "src_lang": "en", "tgt_lang": "it", "output": "Il secondo caso d'uso che abbiamo presentato nel nostro articolo è un caso di semplificazione automatica del testo."}
{"dataset_id": "mcif_v1.0", "sample_id": 26, "src_lang": "en", "tgt_lang": "it", "output": "affinando i modelli linguistici per produrre versioni semplificate di quel testo a partire da testi di input complessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 27, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo ottimizzato due modelli differenti. Abbiamo ottimizzato il modello di parti lunghe per produrre semplificazioni a livello di documento."}
{"dataset_id": "mcif_v1.0", "sample_id": 28, "src_lang": "en", "tgt_lang": "it", "output": "E abbiamo inoltre ottimizzato la base standard, la base standard in parte, per produrre semplificazioni a livello di frase."}
{"dataset_id": "mcif_v1.0", "sample_id": 29, "src_lang": "en", "tgt_lang": "it", "output": "potete trovare anche tutti i checkpoint e potete approfondire i dettagli relativi ai punteggi e alle metriche di valutazione dei nostri esperimenti nel paper."}
{"dataset_id": "mcif_v1.0", "sample_id": 30, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo concluso che questo semplice affinamento poteva produrre o ottenere punteggi superiori a quelli di riferimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 31, "src_lang": "en", "tgt_lang": "it", "output": "E proponiamo tali risultati come punto di riferimento, un benchmark di base per il problema della semplificazione automatica del testo in futuro."}
{"dataset_id": "mcif_v1.0", "sample_id": 32, "src_lang": "en", "tgt_lang": "it", "output": "La ringraziamo vivamente per la sua attenzione e speriamo di avere l'opportunità di incontrare tutti voi durante la conferenza.\nGrazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 33, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Skirkovsky e questa presentazione riguarda la struttura di dipendenza della coordinazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 34, "src_lang": "en", "tgt_lang": "it", "output": "Come saprete, diverse teorie e approcci basati su corpora presuppongono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali è prevista la struttura di coordinazione coordinata di Lisa, Bart e Maggie."}
{"dataset_id": "mcif_v1.0", "sample_id": 35, "src_lang": "en", "tgt_lang": "it", "output": "è tale che il primo congiunto è il capo dell'intera struttura coordinata, quindi in questo caso, Lisa"}
{"dataset_id": "mcif_v1.0", "sample_id": 36, "src_lang": "en", "tgt_lang": "it", "output": "gli approcci assunti nella teoria del significato di Igor Milchuk dove, ancora una volta, l'intera struttura coordinata è governata dal primo connettivo; questi due approcci sono asimmetrici, quindi, essi singolano uno dei congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 37, "src_lang": "en", "tgt_lang": "it", "output": "esistono anche approcci simmetrici alle strutture coordinate, come l'approccio pragmatico, l'approccio \"conjunction-headed\", assunto nei dependency treebank di Plugg, dove le strutture coordinate sono governate dalla congiunzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 38, "src_lang": "en", "tgt_lang": "it", "output": "Quindi otteniamo dipendenze dall'elemento principale a tutti i congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 39, "src_lang": "en", "tgt_lang": "it", "output": "E infine, esiste anche un approccio multi-testa, utilizzato ad esempio nella grammatica lessicale di Dekatson."}
{"dataset_id": "mcif_v1.0", "sample_id": 40, "src_lang": "en", "tgt_lang": "it", "output": "dove si dice che tutti i condotti sono teste della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ama tutti i condotti separatamente, questi sono pulsanti che creano."}
{"dataset_id": "mcif_v1.0", "sample_id": 41, "src_lang": "en", "tgt_lang": "it", "output": "L'obiettivo di questo articolo è quello di presentare una nuova argomentazione a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 42, "src_lang": "en", "tgt_lang": "it", "output": "Bene, l'argomentazione si basa sul principio di minimizzazione della lunghezza delle dipendenze, che illustrerò sulla base di questi esempi."}
{"dataset_id": "mcif_v1.0", "sample_id": 43, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in inglese, come potreste sapere, i complementi oggetto diretto preferiscono stare vicini al verbo, mentre gli elementi aggiuntivi possono essere più distanti, giusto? Quindi, “March l'ha letto ieri” va bene, perché il complemento oggetto diretto è vicino al verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 44, "src_lang": "en", "tgt_lang": "it", "output": "March ha letto ieri, ed è molto peggiore, proprio perché qui, tra il verbo e il complemento oggetto, si interpone un avverbio di tempo, ovvero ieri."}
{"dataset_id": "mcif_v1.0", "sample_id": 45, "src_lang": "en", "tgt_lang": "it", "output": "Questo effetto può essere attenuato quando l'oggetto diretto è particolarmente pesante e lungo, poiché in tal caso può essere spostato in posizione successiva al complemento."}
{"dataset_id": "mcif_v1.0", "sample_id": 46, "src_lang": "en", "tgt_lang": "it", "output": "è illustrato qui. Quindi entrambe queste frasi sono corrette. Marzo ha letto questo libro assolutamente affascinante sulle bestie ieri, io va bene in un certo senso invece di esso abbiamo questa lunga andp."}
{"dataset_id": "mcif_v1.0", "sample_id": 47, "src_lang": "en", "tgt_lang": "it", "output": "È anche accettabile dire che Marco ha letto ieri questo libro assolutamente affascinante sulle api."}
{"dataset_id": "mcif_v1.0", "sample_id": 48, "src_lang": "en", "tgt_lang": "it", "output": "la ragione qui è che ciò è possibile perché, sebbene questa frase violi il principio grammaticale generale che gli oggetti diretti debbano seguire immediatamente il verbo."}
{"dataset_id": "mcif_v1.0", "sample_id": 49, "src_lang": "en", "tgt_lang": "it", "output": "Soddisfa il principio della minimizzazione della lunghezza delle dipendenze, che afferma che le dipendenze più brevi sono preferite."}
{"dataset_id": "mcif_v1.0", "sample_id": 50, "src_lang": "en", "tgt_lang": "it", "output": "Questi due alberi mostrano quindi solo la lunghezza delle dipendenze cruciali, ovvero quelle che non sono costanti tra queste due strutture."}
{"dataset_id": "mcif_v1.0", "sample_id": 51, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, qui abbiamo la dipendenza da \"red\" all'aggiunta di lunghezza 7 misurata in parole e da \"red\" a \"book\" di lunghezza 4. Quindi, complessivamente, sono 11."}
{"dataset_id": "mcif_v1.0", "sample_id": 52, "src_lang": "en", "tgt_lang": "it", "output": "ci si muove quando si scambiano; questi due costituenti sono la somma di queste due dipendenze che diventa sei, giusto? Quindi, invece di 11, 6, decisamente più corto; ecco perché suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro."}
{"dataset_id": "mcif_v1.0", "sample_id": 53, "src_lang": "en", "tgt_lang": "it", "output": "Bene, dunque, ciò che abbiamo fatto è estrarre varie statistiche sulla coordinazione dalla versione migliorata del pentry bank; si veda l'articolo per capire perché non abbiamo utilizzato le dipendenze universitarie."}
{"dataset_id": "mcif_v1.0", "sample_id": 54, "src_lang": "en", "tgt_lang": "it", "output": "Queste statistiche confermano l'osservazione, già fatta innumerevoli volte, che i congiunti sinistri tendono ad essere più brevi, quindi \"sale e pepe\" e non \"pepe e sale\", misurati in sillabe."}
{"dataset_id": "mcif_v1.0", "sample_id": 55, "src_lang": "en", "tgt_lang": "it", "output": "E anche l'osservazione, fatta di passaggio, che questa tendenza cresce con la lunghezza in Francia."}
{"dataset_id": "mcif_v1.0", "sample_id": 56, "src_lang": "en", "tgt_lang": "it", "output": "Quando la differenza tra la lunghezza dei due congiunti aumenta, il congiunto più corto preferisce essere il primo, più forte, quindi la proporzione è maggiore dei congiunti corti a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 57, "src_lang": "en", "tgt_lang": "it", "output": "Ciò di nuovo in questo articolo è che abbiamo osservato che tale tendenza si verifica solo in assenza dei governatori a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 58, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in questo esempio, il governatore si trova a sinistra; ho visto Baton Lisa, perciò il governatore è a sinistra."}
{"dataset_id": "mcif_v1.0", "sample_id": 59, "src_lang": "en", "tgt_lang": "it", "output": "assente nel secondo esempio. Omero è venuto e ha starnutito. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. Pertanto, in tali casi, il congiunto sinistro preferisce essere più breve, tanto più grande è la differenza tra i due congiunti."}
{"dataset_id": "mcif_v1.0", "sample_id": 60, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando la governance è a destra, come nel caso presente, la sinistra gestisce la coda e la rete di coordinamento, e questo effetto scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 61, "src_lang": "en", "tgt_lang": "it", "output": "abbiamo dimostrato che misurando la lunghezza in caratteri, nella prima colonna troviamo le sillabe, nella colonna centrale i termini e nella colonna di destra le parole; pertanto, mi concentrerò su quest'ultima."}
{"dataset_id": "mcif_v1.0", "sample_id": 62, "src_lang": "en", "tgt_lang": "it", "output": "Ciò che osserviamo qui è che quando il governatore si trova a sinistra,"}
{"dataset_id": "mcif_v1.0", "sample_id": 63, "src_lang": "en", "tgt_lang": "it", "output": "la tendenza della congiunzione sinistra ad essere più breve aumenta costantemente con la differenza assoluta nel numero di parole e lo stesso si osserva in assenza di un governatore, come nel caso del coordinamento di frasi, ma quando il governatore si trova a destra tale tendenza scompare."}
{"dataset_id": "mcif_v1.0", "sample_id": 64, "src_lang": "en", "tgt_lang": "it", "output": "nel presente articolo dimostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche, in quanto queste raddoppiano le strutture simmetriche come queste due."}
{"dataset_id": "mcif_v1.0", "sample_id": 65, "src_lang": "en", "tgt_lang": "it", "output": "consultare l'articolo per l'accordo e gli argomenti completi. Ci scusiamo e saremo lieti di discuterne con voi durante la sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 66, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Shahang B, dottorando all'Università di Washington.\nOggi vi presento il nostro lavoro, che parte dai dati di pre-training fino ai modelli linguistici e alle applicazioni successive, tracciando le tracce di pregiudizi politici che portano a modelli NLB ingiusti."}
{"dataset_id": "mcif_v1.0", "sample_id": 67, "src_lang": "en", "tgt_lang": "it", "output": "i modelli linguistici vengono addestrati su dati di acquisizione web su larga scala."}
{"dataset_id": "mcif_v1.0", "sample_id": 68, "src_lang": "en", "tgt_lang": "it", "output": "I media d'informazione sono ampiamente rappresentati nei loro dati di pre-addestramento, come possiamo constatare da un'analisi del corpus C4: New York Times, Los Angeles Times, The Guardian, Huffington Post e altri sono ben rappresentati nei dati di addestramento dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 69, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ha creato una circostanza a doppio taglio per le applicazioni dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 70, "src_lang": "en", "tgt_lang": "it", "output": "Da un lato, sono stati in grado di apprendere da prospettive diverse, cosa che celebra la democrazia e la pluralità di idee.\nDall'altro lato, queste differenti opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di compiti successivi."}
{"dataset_id": "mcif_v1.0", "sample_id": 71, "src_lang": "en", "tgt_lang": "it", "output": "A questo scopo, intendiamo analizzare il processo di propagazione del bias politico, a partire dai dati di pre-addestramento, fino ai modelli linguistici e ai task successivi, ponendo in particolare le seguenti domande."}
{"dataset_id": "mcif_v1.0", "sample_id": 72, "src_lang": "en", "tgt_lang": "it", "output": "Innanzitutto, come valutiamo il significato politico dei modelli linguistici e quale ruolo potrebbe avere l'addestramento sui dati in tali pregiudizi politici?"}
{"dataset_id": "mcif_v1.0", "sample_id": 73, "src_lang": "en", "tgt_lang": "it", "output": "In secondo luogo, come si comportano effettivamente i modelli linguistici dotati di diverse plutonine in compiti a valle e se ciò potrebbe comportare problemi di equità nelle applicazioni di NLP?"}
{"dataset_id": "mcif_v1.0", "sample_id": 74, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in modo specifico, proponiamo inizialmente di sollecitare i modelli linguistici con diversi formati di prompt, utilizzando questionari politici come il test della bussola politica. Questo ci consente di effettuare una valutazione automatica ben fondata nella letteratura scientifica di scienze politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 75, "src_lang": "en", "tgt_lang": "it", "output": "Alcuni risultati preliminari dimostrano che i modelli linguistici di prima lingua presentano effettivamente inclinazioni politiche diverse. Occupano tutti e quattro i quadranti della bussola politica."}
{"dataset_id": "mcif_v1.0", "sample_id": 76, "src_lang": "en", "tgt_lang": "it", "output": "Si può notare anche che GPT4 è il modello linguistico più orientato a sinistra di tutti e che la serie GPT è generalmente più socialmente progressista rispetto alla serie BER e alle sue varianti."}
{"dataset_id": "mcif_v1.0", "sample_id": 77, "src_lang": "en", "tgt_lang": "it", "output": "secondariamente, ci proponiamo di indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente assorbiti dai dati di addestramento."}
{"dataset_id": "mcif_v1.0", "sample_id": 78, "src_lang": "en", "tgt_lang": "it", "output": "potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint di modelli linguistici su sei diversi corpora partigiani, suddivisi in notizie e social media, a loro volta ulteriormente divisi in base alle loro inclinazioni politiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 79, "src_lang": "en", "tgt_lang": "it", "output": "ulteriori pre-addestramenti dei modelli linguistici su tali partiti e corpora rivelano che le coordinate ideologiche del modello linguistico si spostano di conseguenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 80, "src_lang": "en", "tgt_lang": "it", "output": "Per Roberta, ulteriormente affinata e addestrata su un corpus di Reddit orientato a sinistra, possiamo notare un notevole spostamento verso posizioni liberali in termini di quanto essa riflette."}
{"dataset_id": "mcif_v1.0", "sample_id": 81, "src_lang": "en", "tgt_lang": "it", "output": "In termini dei suoi pregiudizi politici."}
{"dataset_id": "mcif_v1.0", "sample_id": 82, "src_lang": "en", "tgt_lang": "it", "output": "E cerchiamo anche di indagare se i modelli linguistici siano in grado di cogliere la polarizzazione diffusa nella nostra società moderna."}
{"dataset_id": "mcif_v1.0", "sample_id": 83, "src_lang": "en", "tgt_lang": "it", "output": "dividiamo i corpora di pre-training in corpora precedenti al 45° presidente degli Stati Uniti e corpora successivi al 45° presidente degli Stati Uniti; pre-addestriamo separatamente i modelli linguistici sui due diversi corpora temporali."}
{"dataset_id": "mcif_v1.0", "sample_id": 84, "src_lang": "en", "tgt_lang": "it", "output": "Si può osservare che i modelli linguistici mostravano generalmente una tendenza politica più distante dal centro a partire dal 2017. Ciò indica che anche i modelli linguistici possono rispecchiare la polarizzazione presente nella nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 85, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, in ultimo ma non meno importante, valutiamo modelli linguistici con diverse inclinazioni politiche in merito alla rilevazione di discorsi d'odio e alla rilevazione di notizie false, in applicazioni di NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative."}
{"dataset_id": "mcif_v1.0", "sample_id": 86, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, vediamo che se analizziamo le prestazioni per categoria, ovvero se suddividiamo le prestazioni in…"}
{"dataset_id": "mcif_v1.0", "sample_id": 87, "src_lang": "en", "tgt_lang": "it", "output": "In diverse fasce demografiche o contesti mediatici politici, analizzando i media, possiamo osservare un andamento per cui, ad esempio, nell'ambito del rilevamento di discorsi d'odio, i modelli linguistici orientati a sinistra ottengono risultati migliori."}
{"dataset_id": "mcif_v1.0", "sample_id": 88, "src_lang": "en", "tgt_lang": "it", "output": "Nel rilevamento di discorsi d'odio rivolti a gruppi socialmente minoritari."}
{"dataset_id": "mcif_v1.0", "sample_id": 89, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, sono meno efficaci nel rilevare discorsi d'odio diretti a gruppi più potenti della nostra società."}
{"dataset_id": "mcif_v1.0", "sample_id": 90, "src_lang": "en", "tgt_lang": "it", "output": "Viceversa, i modelli linguistici correttivi sono migliori nel rilevare discorsi d'odio che colpiscono persone bianche e uomini, tuttavia risultano meno efficaci nel rilevare discorsi d'odio rivolti a comunità nere LGBTQ+ e ad altre minoranze."}
{"dataset_id": "mcif_v1.0", "sample_id": 91, "src_lang": "en", "tgt_lang": "it", "output": "le tendenze si riscontrano anche nella rilevazione di notizie false, dove si osserva che i modelli linguistici di sinistra sono più efficaci nell'individuare disinformazione proveniente dalla loro opposizione politica e viceversa."}
{"dataset_id": "mcif_v1.0", "sample_id": 92, "src_lang": "en", "tgt_lang": "it", "output": "in questo lavoro dimostriamo ulteriormente numerosi esempi qualitativi per evidenziare che i modelli linguistici con diverse connotazioni politiche,"}
{"dataset_id": "mcif_v1.0", "sample_id": 93, "src_lang": "en", "tgt_lang": "it", "output": "fornire previsioni differenti per esempi di discorsi d'odio e disinformazione in base alle loro categorie sociali. Nell'Appendice sono presenti numerosi esempi aggiuntivi per evidenziare ulteriormente questo aspetto."}
{"dataset_id": "mcif_v1.0", "sample_id": 94, "src_lang": "en", "tgt_lang": "it", "output": "Ciò indica che sussiste una problematica di equità estremamente urgente riguardo ai pregiudizi politici dei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 95, "src_lang": "en", "tgt_lang": "it", "output": "Se modelli linguistici di correzione, opportunamente ottimizzati su discorsi d'odio o disinformazione o simili, venissero implementati su una popolare piattaforma di social media,"}
{"dataset_id": "mcif_v1.0", "sample_id": 96, "src_lang": "en", "tgt_lang": "it", "output": "Ciò significherebbe che persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti di gruppi minoritari potrebbe dilagare incontrollato."}
{"dataset_id": "mcif_v1.0", "sample_id": 97, "src_lang": "en", "tgt_lang": "it", "output": "Ciò ha suonato l'allarme per spingerci a riconoscere e affrontare i problemi di equità derivanti dai significati politici incorporati nei modelli linguistici."}
{"dataset_id": "mcif_v1.0", "sample_id": 98, "src_lang": "en", "tgt_lang": "it", "output": "un breve approfondimento vorremmo inoltre sottolineare l'esclusiva difficoltà che affrontiamo per quanto riguarda i pregiudizi politici nei modelli linguistici: è come trovarsi tra Scilla e Cariddi."}
{"dataset_id": "mcif_v1.0", "sample_id": 99, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il bias si propagherebbe dai dati di pre-addestramento ai modelli linguistici e alle attività successive, creando, in ultima analisi, problemi di equità."}
{"dataset_id": "mcif_v1.0", "sample_id": 100, "src_lang": "en", "tgt_lang": "it", "output": "Se provassimo a sanificare in qualche modo, rischieremmo anche censura o esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutro e debba mantenere il linguaggio che accompagna i dati. È un po’ come il problema del carrello ferroviario elettrico."}
{"dataset_id": "mcif_v1.0", "sample_id": 101, "src_lang": "en", "tgt_lang": "it", "output": "Ottimo. Penso che per oggi sia sostanzialmente tutto. F5 per oggi. Grazie per la vostra attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 102, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, studentessa dottoranda del primo anno alla Carnegie Mellon University, e oggi presenterò il mio lavoro su posizioni anali, caratterizzazione dei bias progettuali e dei dataset e modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 103, "src_lang": "en", "tgt_lang": "it", "output": "Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santi, Ronan Labrasse, Katarina Reinika e Martin Sapp."}
{"dataset_id": "mcif_v1.0", "sample_id": 104, "src_lang": "en", "tgt_lang": "it", "output": "Cominciamo dunque immaginando che stiate lavorando per un giornale e state esaminando i commenti sotto il vostro articolo di cronaca, cercando di rimuovere contenuti tossici."}
{"dataset_id": "mcif_v1.0", "sample_id": 105, "src_lang": "en", "tgt_lang": "it", "output": "Potreste orientarvi verso un'API popolare come Perspective API per il rilevamento della tossicità, e questo funziona molto bene se siete Carl Jones, poiché Perspective API è in grado di rilevare correttamente istanze tossiche."}
{"dataset_id": "mcif_v1.0", "sample_id": 106, "src_lang": "en", "tgt_lang": "it", "output": "Ma non è proprio così per Aditya Sharma, dove un potenziale API non è particolarmente sensibile a termini offensivi più comuni in contesti indiani."}
{"dataset_id": "mcif_v1.0", "sample_id": 107, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un esempio di distorsione progettuale in cui si riscontrano differenze sistematiche nelle prestazioni della tecnologia tra le diverse popolazioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 108, "src_lang": "en", "tgt_lang": "it", "output": "Pregiudizi progettuali come quello che abbiamo appena visto potrebbero indurti ad assumere la prospettiva dei ricercatori NLP e degli sviluppatori di modelli. La positionality è semplicemente l'insieme delle prospettive che le persone detengono in virtù della loro demografia, identità ed esperienze di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 109, "src_lang": "en", "tgt_lang": "it", "output": "Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli ambiti accademici femministi e queer."}
{"dataset_id": "mcif_v1.0", "sample_id": 110, "src_lang": "en", "tgt_lang": "it", "output": "E in quanto ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, poiché può modificare le decisioni che i ricercatori prendono."}
{"dataset_id": "mcif_v1.0", "sample_id": 111, "src_lang": "en", "tgt_lang": "it", "output": "E quindi, una domanda che le persone potrebbero porsi è: dataset e modelli presentano una posizione specifica?"}
{"dataset_id": "mcif_v1.0", "sample_id": 112, "src_lang": "en", "tgt_lang": "it", "output": "E non stiamo affermando che i modelli in cellule e i dataset stessi possiedano identità demografiche e esperienze di vita, ma essi aggregato giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre."}
{"dataset_id": "mcif_v1.0", "sample_id": 113, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, lavori precedenti hanno suggerito alcune evidenze aneddotiche dell'esistenza di una posizione specifica, come lacune culturali nei modelli e nei set di dati, nonché definizioni teoriche della posizionalità dei modelli."}
{"dataset_id": "mcif_v1.0", "sample_id": 114, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, queste opere non si concentrano affatto sul confronto tra gli utenti finali e i dataset e i modelli stessi."}
{"dataset_id": "mcif_v1.0", "sample_id": 115, "src_lang": "en", "tgt_lang": "it", "output": "E lo studio della posizionalità dei modelli e dei dataset sta diventando sempre più importante, poiché i test di NLP diventano più soggettivi e orientati alla dimensione sociale."}
{"dataset_id": "mcif_v1.0", "sample_id": 116, "src_lang": "en", "tgt_lang": "it", "output": "Ed è difficile caratterizzare il modo in cui tali posizionamenti sono distorti, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API."}
{"dataset_id": "mcif_v1.0", "sample_id": 117, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, per studiare la posizionalità dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con il dataset e i modelli esistenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 118, "src_lang": "en", "tgt_lang": "it", "output": "realizzare ciò attraverso il nostro framework di posizionamento NL."}
{"dataset_id": "mcif_v1.0", "sample_id": 119, "src_lang": "en", "tgt_lang": "it", "output": "il framework opera in due fasi principali."}
{"dataset_id": "mcif_v1.0", "sample_id": 120, "src_lang": "en", "tgt_lang": "it", "output": "Il primo passo consiste nel ri-annotare i dataset con annotatori diversi."}
{"dataset_id": "mcif_v1.0", "sample_id": 121, "src_lang": "en", "tgt_lang": "it", "output": "E dovremmo farlo esaminando i dati demografici degli annotatori del dataset originale, poiché di solito solo pochi annotatori annotano ciascuna istanza, e perché i dati demografici vengono raramente raccolti e condivisi."}
{"dataset_id": "mcif_v1.0", "sample_id": 122, "src_lang": "en", "tgt_lang": "it", "output": "E quindi scegliamo di ri-annotare i dati per ottenere numerose annotazioni, ad esempio, e per acquisire un insieme ricco di dati demografici."}
{"dataset_id": "mcif_v1.0", "sample_id": 123, "src_lang": "en", "tgt_lang": "it", "output": "Procediamo quindi ad analizzare le annotazioni in base ai dati demografici, confrontandole con i modelli e il dataset utilizzando il punteggio di correlazione R di comparisonar."}
{"dataset_id": "mcif_v1.0", "sample_id": 124, "src_lang": "en", "tgt_lang": "it", "output": "E così il nostro framework differisce effettivamente dalla letteratura sulla discordanza tra annotatori confrontando gli utenti finali con modelli e set di dati, previsioni ed etichette, anziché limitarsi a considerare la concordanza tra annotatori o la modellazione delle loro distribuzioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 125, "src_lang": "en", "tgt_lang": "it", "output": "framer è reso possibile in larga misura attraverso Lab in the wild, una piattaforma di crowdsourcing online sviluppata da un ex collaboratore di HCI."}
{"dataset_id": "mcif_v1.0", "sample_id": 126, "src_lang": "en", "tgt_lang": "it", "output": "And Lab in the Wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari più diversificati rispetto alle piattaforme come MTERk, che hanno partecipanti prevalentemente provenienti da Stati Uniti o India. E inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità."}
{"dataset_id": "mcif_v1.0", "sample_id": 127, "src_lang": "en", "tgt_lang": "it", "output": "Organizziamo due compiti in laboratorio sul campo, uno dei quali riguarda l'accettabilità sociale, e il loro funzionamento consiste nel fatto che i partecipanti leggeranno una situazione tratta dal dataset di social chemistry e poi scriveranno quanto una determinata situazione sia socialmente accettabile."}
{"dataset_id": "mcif_v1.0", "sample_id": 128, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, per rimanere coinvolti nella città, possono confrontare le proprie risposte con quelle di un'IA e di altri utenti."}
{"dataset_id": "mcif_v1.0", "sample_id": 129, "src_lang": "en", "tgt_lang": "it", "output": "Confrontammo quindi queste annotazioni con social chemistry, Delphi e GPT4."}
{"dataset_id": "mcif_v1.0", "sample_id": 130, "src_lang": "en", "tgt_lang": "it", "output": "dopodiché replicare una configurazione molto simile per il compito di rilevamento della tossicità e del discorso d'odio, dove leggeranno un esempio da Dinah hatete e indicheranno se ritengono che si tratti di un'istanza di discorso d'odio."}
{"dataset_id": "mcif_v1.0", "sample_id": 131, "src_lang": "en", "tgt_lang": "it", "output": "Abbiamo quindi confrontato queste annotazioni con Dynah Hate, Perspective API, Rewire API, Hate Roberta e GPT4. Il nostro studio, al termine, ha raccolto più di 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi."}
{"dataset_id": "mcif_v1.0", "sample_id": 132, "src_lang": "en", "tgt_lang": "it", "output": "ora siamo meglio attrezzati per rispondere alla domanda: con quali posizioni si allineano i dataset e i modelli di NLP? Scopriamo che esiste una posizione nel campo dell'NLP."}
{"dataset_id": "mcif_v1.0", "sample_id": 133, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che dataset e modelli sono maggiormente allineati ai paesi di lingua inglese. Pertanto, per l'analisi di accettabilità sociale di GPD4, riscontriamo un allineamento predominante con i paesi di cultura confuciana e di lingua inglese. Anche il fenomeno del \"dyna hate\" risulta essere maggiormente allineato ai paesi di lingua inglese."}
{"dataset_id": "mcif_v1.0", "sample_id": 134, "src_lang": "en", "tgt_lang": "it", "output": "Troviamo inoltre una maggiore corrispondenza con persone che hanno conseguito un'istruzione universitaria. Quindi, per GPD4 nel compito di Accettabilità Sociale, riscontriamo che si allinea maggiormente con persone con un'istruzione universitaria o post-universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 135, "src_lang": "en", "tgt_lang": "it", "output": "Lo stesso vale per Diny Haight, dove si riscontra la maggiore correlazione con persone in possesso di istruzione universitaria."}
{"dataset_id": "mcif_v1.0", "sample_id": 136, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, quando modelli e set di dati sono allineati a specifiche popolazioni, alcune vengono inevitabilmente escluse."}
{"dataset_id": "mcif_v1.0", "sample_id": 137, "src_lang": "en", "tgt_lang": "it", "output": "Un esempio di ciò è che dataset e modelli sono meno allineati alle persone non binarie rispetto ai rispettivi omologhi maschili e femminili. Lo riscontriamo sia nel compito di accettabilità sociale GPG4 sia nell'analisi del compito Diny hatete."}
{"dataset_id": "mcif_v1.0", "sample_id": 138, "src_lang": "en", "tgt_lang": "it", "output": "Dato che esiste una posizione in LD in LP, cosa possiamo fare al riguardo?"}
{"dataset_id": "mcif_v1.0", "sample_id": 139, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, abbiamo alcune raccomandazioni a riguardo. La prima è tenere traccia di tutte le scelte progettuali rilevanti durante l'intero processo di ricerca, e la seconda è condurre ricerche di NLP con l'ottica del prospettivismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 140, "src_lang": "en", "tgt_lang": "it", "output": "La nostra terza raccomandazione è quella di sviluppare dataset e modelli specializzati all'interno di quattro comunità specifiche, e un buon esempio di ciò è l'iniziativa Masakanne. Vogliamo sottolineare che l'elaborazione del linguaggio naturale inclusiva non si limita a garantire che tutte le tecnologie funzionino per tutti."}
{"dataset_id": "mcif_v1.0", "sample_id": 141, "src_lang": "en", "tgt_lang": "it", "output": "quindi, per concludere, questa è la fine della nostra presentazione. Ma se desiderate approfondire, sentitevi liberi di consultare la nostra dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 142, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono X Yuan dell'Università Faii. Sono qui per presentare il nostro lavoro: Distinzione tra la Conoscenza degli Script e i Modelli Linguistici Leggeri per la Pianificazione del Linguaggio Vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 143, "src_lang": "en", "tgt_lang": "it", "output": "Nella vita quotidiana, chi deve spesso pianificare le proprie azioni seguendo istruzioni passo-passo sotto forma di script garantiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 144, "src_lang": "en", "tgt_lang": "it", "output": "Modelli linguistici precedenti hanno esplorato la pianificazione per obiettivi astratti di attività stereotipate, come preparare una torta, dimostrando che i modelli linguistici di grandi dimensioni possono efficacemente suddividere gli obiettivi in fasi."}
{"dataset_id": "mcif_v1.0", "sample_id": 145, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti per attività stereotipate. La pianificazione per obiettivi specifici, con vincoli specifici, come preparare una torta al cioccolato, rimane ancora in gran parte trascurata."}
{"dataset_id": "mcif_v1.0", "sample_id": 146, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, definiamo il problema della pianificazione linguistica vincolata."}
{"dataset_id": "mcif_v1.0", "sample_id": 147, "src_lang": "en", "tgt_lang": "it", "output": "Le quali impongono vincoli diversi agli obiettivi della pianificazione; un obiettivo astratto può essere ereditato da diversi obiettivi specifici del mondo reale con vincoli multifacciali. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 148, "src_lang": "en", "tgt_lang": "it", "output": "In questo articolo, valutiamo e miglioriamo inizialmente la capacità di pianificazione del linguaggio vincolato dei modelli linguistici di vita."}
{"dataset_id": "mcif_v1.0", "sample_id": 149, "src_lang": "en", "tgt_lang": "it", "output": "Non esistono dati al di fuori di obiettivi specifici per individuare il nostro giorno fortunato."}
{"dataset_id": "mcif_v1.0", "sample_id": 150, "src_lang": "en", "tgt_lang": "it", "output": "Dobbiamo innanzitutto acquisire questi obiettivi, come illustrato nella tabella; estendiamo gli obiettivi astratti con vincoli molteplici per l'acquisizione di dati con l'intervento umano, utilizzando Instruct GPT."}
{"dataset_id": "mcif_v1.0", "sample_id": 151, "src_lang": "en", "tgt_lang": "it", "output": "Analizziamo centinaia di obiettivi specifici e valutiamo gli script generati dai modelli logici."}
{"dataset_id": "mcif_v1.0", "sample_id": 152, "src_lang": "en", "tgt_lang": "it", "output": "La presente tabella riporta l'accuratezza complessiva dei risultati. Si rileva che tutti i modelli Lilong ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 153, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, conduciamo un'analisi dettagliata per investigare a cosa servano i modelli di apprendimento."}
{"dataset_id": "mcif_v1.0", "sample_id": 154, "src_lang": "en", "tgt_lang": "it", "output": "I risultati presentati nella figura mostrano che la completezza settimanale degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita."}
{"dataset_id": "mcif_v1.0", "sample_id": 155, "src_lang": "en", "tgt_lang": "it", "output": "Esaminiamo categorie di vincoli più specifiche definite nel Wi home. La mappa di calore nella figura mostra che le prestazioni di pianificazione di instructiv variano notevolmente per ragazze di diverse categorie."}
{"dataset_id": "mcif_v1.0", "sample_id": 156, "src_lang": "en", "tgt_lang": "it", "output": "Studi precedenti hanno dimostrato che la qualità dell'output dei modelli live presenta un'elevata varianza, con conseguente scarsa performance. Pertanto, abbiamo adottato l'idea di sovra-generare il filtro per migliorare la qualità della generazione."}
{"dataset_id": "mcif_v1.0", "sample_id": 157, "src_lang": "en", "tgt_lang": "it", "output": "Iniziamo mostrando i tipi vincolati con esempi per istruire CPT e ottenere obiettivi specifici basati sugli obiettivi astratti definiti."}
{"dataset_id": "mcif_v1.0", "sample_id": 158, "src_lang": "en", "tgt_lang": "it", "output": "Istruire l’GPT attraverso script chiave generali per obiettivi specifici."}
{"dataset_id": "mcif_v1.0", "sample_id": 159, "src_lang": "en", "tgt_lang": "it", "output": "Successivamente, si deriva un modello di filtro per selezionare gli script fisici."}
{"dataset_id": "mcif_v1.0", "sample_id": 160, "src_lang": "en", "tgt_lang": "it", "output": "Convertiamo script e immagini in embedding GPT e calcoliamo la similarità del coseno come punteggio di similarità semantica."}
{"dataset_id": "mcif_v1.0", "sample_id": 161, "src_lang": "en", "tgt_lang": "it", "output": "Inoltre, assegniamo lo script che contiene le parole chiave del vincolo di destinazione. Conserviamo lo script solo se l'obiettivo di destinazione ottiene il punteggio più alto nel sito obiettivo."}
{"dataset_id": "mcif_v1.0", "sample_id": 162, "src_lang": "en", "tgt_lang": "it", "output": "Con il nostro metodo, l'istruibilità può generare viti di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità sia in termini semantici, di completezza che di fedeltà ai vincoli."}
{"dataset_id": "mcif_v1.0", "sample_id": 163, "src_lang": "en", "tgt_lang": "it", "output": "Dato che i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale consentire alle versioni più piccole e specializzate di possedere capacità di pianificazione linguistica. La creazione di dataset rappresenta un passaggio fondamentale per"}
{"dataset_id": "mcif_v1.0", "sample_id": 164, "src_lang": "en", "tgt_lang": "it", "output": "Tuttavia, studi precedenti non consentono di pianificare obiettivi specifici, e l'annotazione manuale dei dati richiede costi elevati."}
{"dataset_id": "mcif_v1.0", "sample_id": 165, "src_lang": "en", "tgt_lang": "it", "output": "Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare un dataset di pianificazione linguistica vincolata da modelli linguistici di vita reale."}
{"dataset_id": "mcif_v1.0", "sample_id": 166, "src_lang": "en", "tgt_lang": "it", "output": "Applicheremo il nostro metodo per la costruzione di un dataset di pianificazione del linguaggio vincolato denominato CodeScri."}
{"dataset_id": "mcif_v1.0", "sample_id": 167, "src_lang": "en", "tgt_lang": "it", "output": "In totale, abbiamo generato cinquantacinquemila obiettivi specifici con script per garantire la qualità dei siti di validazione e di test. Chiediamo ai collaboratori di crowdsourcing di rivedere infine i redditi nei campioni errati."}
{"dataset_id": "mcif_v1.0", "sample_id": 168, "src_lang": "en", "tgt_lang": "it", "output": "Questa figura mostra la distribuzione delle limitazioni di CodeScript. Abbiamo riscontrato che Coscript manifesta un elevato pluralismo negli obiettivi specifici generati. Con Coscript, possiamo gestire modelli più piccoli ma specializzati per la pianificazione del linguaggio vincolato."}
{"dataset_id": "mcif_v1.0", "sample_id": 169, "src_lang": "en", "tgt_lang": "it", "output": "Con l'aumentare delle dimensioni, il punteggio t-five ottenuto dopo il fine-tuning può generare script di qualità variabile e, nei modelli di dimensioni maggiori, suggerire che modelli più piccoli possono sopprimere modelli più grandi, qualora adeguatamente addestrati su dataset appropriati."}
{"dataset_id": "mcif_v1.0", "sample_id": 170, "src_lang": "en", "tgt_lang": "it", "output": "In sintesi, abbiamo definito il problema della pianificazione linguistica vincolata. Abbiamo sviluppato un'abilità di pianificazione linguistica vincolata per i modelli linguistici di grandi dimensioni e abbiamo elaborato un metodo di filtraggio sovra generativo per i modelli linguistici di grandi dimensioni."}
{"dataset_id": "mcif_v1.0", "sample_id": 171, "src_lang": "en", "tgt_lang": "it", "output": "Utilizziamo modelli linguistici di grandi dimensioni per generare un dataset quadratico di alta qualità, Codecri, per la pianificazione linguistica vincolata. Speriamo che il dataset CodeSscript possa essere una risorsa preziosa per promuovere la ricerca sulla pianificazione linguistica."}
{"dataset_id": "mcif_v1.0", "sample_id": 172, "src_lang": "en", "tgt_lang": "it", "output": "Grazie per il suo tempo. \nPer maggiori dettagli su Codecri, si veda il nostro articolo."}
{"dataset_id": "mcif_v1.0", "sample_id": 173, "src_lang": "en", "tgt_lang": "it", "output": "Buongiorno a tutti, mi chiamo Shu H.\nOggi presenterò il nostro articolo dal titolo \"Do Named Entity Taggers still work well in 2023?\", basato sul lavoro di Connell del 2003.\nIniziamo."}
{"dataset_id": "mcif_v1.0", "sample_id": 174, "src_lang": "en", "tgt_lang": "it", "output": "Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate, o il NER task."}
{"dataset_id": "mcif_v1.0", "sample_id": 175, "src_lang": "en", "tgt_lang": "it", "output": "Osserviamo che i modelli utilizzano ConONO 2003 per sviluppare NER da quasi 20 anni, e ciò pone naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzare a dati moderni?"}
{"dataset_id": "mcif_v1.0", "sample_id": 176, "src_lang": "en", "tgt_lang": "it", "output": "E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?"}
{"dataset_id": "mcif_v1.0", "sample_id": 177, "src_lang": "en", "tgt_lang": "it", "output": "Contemporaneamente, se osserviamo una scarsa capacità di generalizzazione, quali sono le cause del calo di performance di questi modelli?"}
{"dataset_id": "mcif_v1.0", "sample_id": 178, "src_lang": "en", "tgt_lang": "it", "output": "Per indagare questi problemi, abbiamo sviluppato il dataset Connell++. Si tratta di un dataset che abbiamo raccolto da Reuters News a partire dal 2020 e successivamente annotato secondo le stesse linee guida di annotazione di Connell del 2003."}
{"dataset_id": "mcif_v1.0", "sample_id": 179, "src_lang": "en", "tgt_lang": "it", "output": "quindi ottimizzate ulteriormente oltre 20 modelli su Conal 2003. Li abbiamo valutati sia sul set di test Con O3 che sul set di test Cono plus first."}
{"dataset_id": "mcif_v1.0", "sample_id": 180, "src_lang": "en", "tgt_lang": "it", "output": "E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la capacità di generalizzazione di ciascun modello."}
{"dataset_id": "mcif_v1.0", "sample_id": 181, "src_lang": "en", "tgt_lang": "it", "output": "Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo riscontrato che sono necessari tre ingredienti principali."}
