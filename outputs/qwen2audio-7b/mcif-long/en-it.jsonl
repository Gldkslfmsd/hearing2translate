{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Il mio nome è Matthias Lendermann e oggi vi parlerò brevemente del nostro lavoro sul generaleizzazione compositiva senza alberi utilizzando la taggatura multietichettata e le permutazioni latenti. Questo è un lavoro in collaborazione con i miei consiglieri Alexander Kolah e Ivan Titev. La generaleizzazione compositiva può essere compresa come la capacità di un apprendista di gestire ricorsione più profonda e composizioni non visibili."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "\"Mi chiamo Maria e oggi parleremo del nostro lavoro sulle persone marcate dallo spazio utilizzando promemoria naturali per misurare i stereotipi nei modelli di lingua. Questo lavoro è fatto in collaborazione con Esmond D'Arcy e Dan Jaroszky. In recenti anni, molti hanno documentato la prevalenza di bias sociali e stereotipi nei grandi modelli di lingua o LLMs. Tuttavia, queste misure hanno diverse limitazioni. Di solito si affidano a set di dati costruiti a mano che sono molto impegnativi da curare e utilizzano anche."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Gentile ascoltatore,\nSono James Finch e sono Sarah Finch. Oggi vi parleremo di ABC EVALE, un nuovo approccio multidimensionale all'valutazione dell'intelligenza conversazionale. Questo lavoro è stato svolto dal laboratorio NLP dell'Emory University, guidato dal professor Gino Choi. In collaborazione con Amazon Alexa AI.\nPer dire che hai appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato attuale della arte, la pratica comune consiste nell'utilizzare valutazioni umane."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Hello, my name is Vasudha e sono un candidato al dottorato di ricerca in informatica presso l'Università di Stony Brook. Vorrei presentare il mio lavoro accettato all'Acl 2023 intitolato 'Transfer Learning per la rilevazione del dissonanza cognitiva', che affronta il problema della classe rara dei disturbi cognitivi. Iniziamo definendo cosa sia la dissonanza cognitiva e perché sia importante studiarla nella lingua. La dissonanza cognitiva è semplicemente due credenze o azioni contrastanti."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "L'audio dice: 'Ciao a tutti, sono Akshata e oggi il mio coautore Martin e io presentiamo il nostro lavoro, chiamato Kit Musta, che valuta l'integrazione del sapere da fonti multiple. Questo lavoro è una collaborazione tra l'università di McGill, Mila e Microsoft Research. I modelli di comprensione della lingua nazionale si basano su una varietà di fonti di conoscenza, come quella contenuta nei loro parametri, generalmente acquisita attraverso un addestramento pre-trainato e la conoscenza dell'ambiente.'"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "L'attenzione è una guida per il paper di traduzione simultanea del discorso, un lavoro congiunto con Matteo Negrini e Marco Turco."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Il discorso inglese dice: 'Ciao a tutti, il mio nome è Shu Han. Oggi vi presenterò il nostro lavoro intitolato \"Do Conal 2003 chiamato entità taggeri funzionano ancora nel 2023?\". Andiamo avanti.'"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "L'audio dice: 'Ecco la presentazione di DeepL, un nuovo corpus per la classificazione dei testi in Germano al livello del documento e al livello della frase.'"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "\"Salve, sono Si Yuanyuan dell'Università di Fudan. Sono qui per introdurre il nostro lavoro, distingueremo la conoscenza del script dal modello linguistico grande per pianificare le azioni in una lingua con restrizioni.\""}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yannick Lavaud e presenterò i nostri lavori sul modello Dr. Bert, un modello pre-trainato robusto in francese per il dominio biomedico e clinico. In questa presentazione, parleremo prima di modellizzazione del linguaggio nella sanità, quindi presenteremo la principale contribuzione del nostro articolo. Introdurremo il primo modello biomedico in francese chiamato Dr. Bert, che si basa su Roberta e addestrato sui dati di prova Natsos, che è un set di dati di dati medici raccolti dal National Cancer Institute degli Stati Uniti."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Jang Bin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro, dal data pre-training ai modelli linguistici, alle attività di down stream, tracciare le tracce delle tendenze politiche che portano a modelli NLP inesatti. I modelli linguistici vengono addestrati sui grandi dati web raccolti. I media di notizie politiche sono ben coperti nei loro dati pre-train. Secondo una survey del C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Costruttore Senna e sono lieto di accogliervi nel nostro talk sul nostro articolo ACER 2023 intitolato \"Judgmenti di adattabilità linguistica non sempre robusti al contesto\". È un lavoro congiunto con John Goughere, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Ada Williams. Quindi, in questo lavoro, ripercorriamo il paradigma del minimo paio. Il paradigma del minimo paio valuta i modelli della lingua o su base dei giudizi di adattabilità."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "In questo video vorrei presentare il nostro lavoro più recente, intitolato \"Miglior di quanto pensate: una valutazione critica della formazione periodica\". È un lavoro in collaborazione con Xiao Yunshen, Maios Musba, Gia Stephen e Dieter Klackow. Vorrei iniziare con una breve introduzione alla supervisione settimanale e alla formazione periodica. In supervisione settimanale non si fa manovra."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Il paper 'Gruntling: Building a Large-Scale Parameterized Language Model from a Multilingual Corpus' è un lavoro congiunto con i miei colleghi di Google Translate. Il modello Gruntling è un modello di lingua parametrizzato di 540 miliardi di parametri che è stato presentato l'anno scorso (2022). È stato addestrato su una grande raccolta di testi che comprende 180 miliardi di token. Durante la produzione del tama, ha raggiunto uno stato di arte in centinaia di attività di elaborazione parallele."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Gentilezza, tutti, il mio nome è Qin Weiyi dall'Università della Scienza e della Tecnologia di China. È un piacere per me fare una breve pubblicità sul video su carta. Stai copiando il mio modello? Proteggiamo la proprietà intellettuale dei grandi modelli di lingua per l'integrazione e i servizi. Cerchiamo un watermark backdoor. Prima introdurremo il background sull'integrazione dei servizi. Attualmente, i grandi modelli di lingua, come TPT, Lama, Pangu..."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Hello everyone, my name is Yin and my colleague Zhiyang, and we will be presenting our research on Multi-instruct, improving multi-model serial learning via instruction tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter-efficient way. Recently, many studies have shown that instruction tuning enables large language models."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "L'audio dice: 'Ciao a tutti, mi chiamo Liu Xinjohn dell'università di Peking. Oggi sono qui per presentare il mio lavoro: esempio di sintesi semantica in più lingue naturali e rappresentazioni mentali. Quindi, la sintesi semantica è un compito che consiste nel costruire rappresentazioni semantiche degli query degli utenti, come SQL e calcoli matematici in lambda. E la sintesi semantica crosslinguistica è il compito di tradurre le query in diverse lingue naturali in rappresentazioni mentali multiple.'"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "L'audio tratta della struttura di dipendenza della coordinazione e presenta diverse teorie e approcci per rappresentarla. In particolare, si fa riferimento alla struttura delle coordinate proposta da Lisa Bart e Maggie, in cui il primo congiuntivo è la testa dell'intera struttura di coordinamento. Un approccio simile viene adottato in ambito di egocentrismo."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Il lavoro si intitola 'Quando richiede la traduzione di contesto? Una esplorazione multilingue basata sui dati'. È stato realizzato in collaborazione con Patrick Farnsworth, MEYU, André F. D. Martinez e Graham Newbigging. In molti casi, le traduzioni dipendono dal contesto. Ad esempio, come tradurremo 'molle' in questa frase? Se la frase precedente era 'Se i ministri lo trovassero, le cose potrebbero cominciare a diventare pericolose', allora 'molle' si riferisce a un aggegno."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Il lavoro è stato fatto in collaborazione con alcuni folks al University of Washington e il team AI è composto da Sebastian Santi, Ronan Le Bras, Katerina Rynika e Martin SAP."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Il discorso riguarda il nostro lavoro sulla risoluzione di espressioni indirette per l'elenco delle entità, in cui introduciamo gli spazi degli identificatori d'entità. Il mio nome è Javot Hosaini e questo è un lavoro condiviso con Philip Radoszyk, Silvia Parati e Anna Lewis. Il nostro obiettivo è capire la lingua degli utenti quando vogliono fare una scelta e considerare questa domanda alternativa: \"Hai inteso facilmente o ho avuto un senso?\" Qui un utente ha detto..."}
