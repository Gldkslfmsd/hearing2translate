{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Matthias Lendemann und heute werde ich Ihnen eine kurze Einführung in unseren Artikel über Kompositionelle Generalisierung ohne Bäume unter Verwendung von Multi-Set-Tagging und latenten Permutationen geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und ungesichtete Kompositionen von Phrasen zu bewältigen, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsings könnte das Testen auf Kompositionelle Generalisierung so aussehen. Wie üblich haben wir einen Trainingssatz von Äußerungen, in diesem Fall \"das Mädchen schlief\" und \"Mary wusste, dass das Mädchen schlief\". Diese Äußerungen sind mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Maschinenlern-Bewertung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungesichtete logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursion gesehen und wird an einem Beispiel mit tieferer Rekursion getestet. Naive Sequenz-zu-Sequenz-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Generalisierung und produzieren oft Ausgaben, die vom Eingang getrennt sind. Insbesondere scheitern sie oft daran, die systematischen Korrespondenzen zwischen Eingang und Ausgang zu reproduzieren, wie die farblich kodierten im Beispiel. Eine beliebte Methode, dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den Kompositionellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann kompliziert und manchmal ein rechnerisch aufwändiger Prozess sein. Typischerweise beinhaltet dies eine erhebliche formalismusspezifische Vorverarbeitung der logischen Formen, zum Beispiel um Variablensymbole zu behandeln. Das Erhalten von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren beinhalten. In diesem Artikel verwenden wir keine Bäume und stellen ein neuronales Sequenz-zu-Sequenz-Modell vor, das die Korrespondenzen zwischen Fragmenten des Eingangs und Fragmenten des Ausgangs direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tieferer Rekursion, ohne auf Bäume zu setzen. Unser Ansatz prognostiziert den Ausgang vom Eingang in zwei Schritten. Zuerst taggen wir jedes Eingabetoken mit einem ungeordneten Multiset von Token, die im Ausgang erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir stellen eine neue Methode vor, um eine Permutation vorherzusagen, die keine harten Einschränkungen an die möglichen Permutationen stellt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptuell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Ausgang und bestimmen, welches Multiset-Token in jede Position gesetzt wird. Für die erste Ausgangsposition wählen wir einfach eines aus, wie in Rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Ausgang zu bestimmen. Wir bestimmen das dritte Token im Ausgang auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf der Cogs-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung zu tieferer Rekursion um einen großen Abstand. Andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Artikel lösen wir einige interessante technische Herausforderungen. Erstens wird die Ausrichtung zwischen Eingang und Ausgang in den Trainingsdaten nicht gegeben. Als Folge wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Außerdem gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir gehen darauf ein, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, aber sie bringt die Herausforderung mit sich, dass das Finden der höchstbewerteten Permutation NP-schwer ist. Das liegt daran, dass dies mit dem Problem des Handlungsreisenden verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzuverfolgen und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unseren Artikel oder kommen Sie zu unserem Poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Myra und heute werde ich über unsere Arbeit \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Essin Darmush und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Verbreitung von sozialen Vorurteilen und Stereotypen in großen Sprachmodellen oder LLMs dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handgefertigte Datensätze, die sehr zeitaufwendig zu erstellen sind, und messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere demografische Gruppen oder Kontexte übertragen lassen, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich nicht die Intersektionalität, die die Vorstellung ist, dass vielschichtige soziale Identitäten Vorurteile verstärken und einzigartige Schadensherde darstellen können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren, anweisungsorientierten LLMs sehr gut auf Anweisungen in Prompts reagieren. Wir können das Modell also bitten, eine Persona zu generieren, die eine Darstellung einer imaginären Person ist, indem wir einen Prompt wie \"Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst\" verwenden. Und wir können sofort sehen, dass dies sehr gut auf jede demografische Gruppe übertragbar ist, weil wir einfach den gewünschten Identitätsmarker in diesen Prompt eingeben können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass die Ausgaben nicht offensichtlich negativ oder toxisch im traditionellen Sinne dieser Wörter sind, aber es gibt einige interessante Muster. Die asiatische Frau wird als unauffällig dargestellt. Die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und wie einer faszinierenden Region beschrieben. Und beide Personas der Frauen of Color beziehen sich auf ihre Abstammung, während die Persona des weißen Mannes nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil besteht darin, diese Personas zu generieren. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Prompts menschlichen Probanden gegeben wurden, wobei festgestellt wurde, dass durch die Vergabe an menschliche Probanden auch rassistische Stereotypen aufgedeckt werden konnten. Dies ermöglicht auch einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Methode der markierten Wörter greift auf das soziolinguistische Konzept der Markiertheit zurück, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. So ist das Wort Kriegerin normalerweise mit Männern assoziiert. Wenn also Menschen eine Kriegerin beschreiben, die eine Frau ist, spezifizieren sie normalerweise eine männliche Kriegerin und markieren den Begriff mit Frau. Und allgemeiner gesagt, sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen normalerweise markiert sind. In unserer Methode bestimmen wir zunächst, was die unmarkierten und markierten Gruppen sind. Und dann vergleichen wir die Personas mit der Methode der kämpfenden Wörter, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. So würden wir beispielsweise für die Personas schwarzer Frauen die Methode der kämpfenden Wörter anwenden und die Log-Odds-Verhältnisse sowohl mit weißen Personas als auch mit männlichen Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon der Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter im Lexikon betrachten, stellen wir sehr unterschiedliche Dinge fest. Während die generierten Personas viel höhere Raten der Lexikon-Wörter aufweisen, haben die von Menschen geschriebenen eine viel breitere Verteilung der Wörter, während die Stereotypen-Wörter, die in den generierten Personas vorkommen, wirklich nur die Wörter groß und athletisch sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den früheren Folien gesehen haben, überhaupt nicht gut. Stattdessen werden wir uns die Ergebnisse unserer Methode der markierten Wörter ansehen, um zu zeigen, wie diese positiv erscheinenden Wörter Stereotypen und essentialisierende Narrative fördern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens gehören zu den Top-Wörtern für markierte Gruppen Dinge wie Kultur, Tradition, stolz und exotisch. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders als die weiße Norm. Dies trägt zu einem langen Erbe der Diskriminierung und des Andersmachens für diese Gruppen bei. Darüber hinaus gibt es viele gemeinsame Klischees, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen of Color. So beinhalten beispielsweise die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie lebhaft und kurvenreich, was mit einem Klischee des Tropikalismus verbunden ist. Für asiatische Frauen sind die Wörter Dinge wie zierlich und zart und seidig, was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen verbunden ist, die als sehr zahm und unterwürfig angesehen werden und so weiter. Und schließlich sehen wir, dass einige der Top-Wörter für schwarze Frauen Dinge wie stark und widerstandsfähig sind. Dies ist mit einem Archetyp verbunden, den Menschen das Archetyp der starken schwarzen Frau genannt haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass dieser Archetyp tatsächlich sehr schädlich ist, weil er einen großen Druck auf diese demografischen Gruppen ausübt, widerstandsfähig und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt also tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, übt es Druck auf diese Menschen aus, sie zu überwinden, was zu sehr negativen gesundheitlichen Auswirkungen für diese Menschen unter anderem führt. Allgemeiner gesagt stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Narrative widerspiegeln. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch intersektionale Linsen verwenden, um Vorurteile und Schäden zu untersuchen, denn es gibt viele Dinge, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich eine erhöhte Transparenz über Methoden zur Vorurteilsminderung geben. Denn zum Beispiel wissen wir nicht, ob es an einer Art seltsamen, übermäßig exzessiven Wertausrichtung liegt oder vielleicht an anderen Anti-Stereotypen-Methoden, die zu diesen schädlichen Mustern führen. Wir können wirklich keine Annahmen machen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank, dass Sie zugehört haben. Ich wünsche Ihnen viel Spaß bei der ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab durchgeführt, geleitet von Professor Gino Choi an der Emory University, und in Zusammenarbeit mit Amazon Alexa AI. Angenommen, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis besteht darin, menschliche Bewertungen zu verwenden, indem man menschliche Richter bittet, auszuwählen, welche von zwei Gesprächen besser ist, oder Gespräche auf einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um ganzheitliche Bewertungen der Gesamtqualität des Dialogs zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z.B. die Relevanz der Modellantworten, unter Verwendung bestehender vergleichender oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z.B. das Reagieren mit irrelevanten Informationen oder das Widersprechen. Wir nennen diesen Ansatz Annotating Behaviors in Chat oder ABC-Eval in Kurzform. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chat-Modellen umfassend abzudecken, die in der jüngsten Literatur als die Chat-Qualität beeinflussend vorgeschlagen wurden. ABC-Eval ist in der Lage, die Häufigkeit zu messen, mit der Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Züge, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinem Partner widerspricht, falsche Fakten halluziniert oder gesunden Menschenverstand verletzt, und wenn das Modell erfolgreich oder fehlschlägt, Mitgefühl zu zeigen. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, wählten wir vier State-of-the-Art-Chat-Modelle aus und bewerteten sie anhand von 100 menschlichen Bot-Gesprächen pro Modell unter Verwendung von ABC-Eval. Zum Vergleich bewerteten wir diese Gespräche auch mit drei bestehenden Methoden: Likert-Bewertungen auf der Zug-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene paarweise Vergleiche. Für jede der bestehenden Methoden sammelten wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unseren Analysen dieser Bewertungsergebnisse stellten wir fest, dass ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die von bestehenden Methoden gesammelten Labels, gemessen an der Übereinstimmung zwischen den Annotatoren bei 100 doppelt annotierten Gesprächen. Darüber hinaus sind ABC-Eval-Labels besser vorhersagbar für die Gesamtqualität des Gesprächs im Vergleich zu den von bestehenden Methoden erzeugten Metriken, wie durch diese einfache lineare Regressionsanalyse gezeigt wird. Zum Beispiel können Sie sehen, wie die Messung des Anteils der Züge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich überprüften wir, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, unter Verwendung einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken eine nach der anderen entfernen, verlieren die meisten von ihnen eine anständige Menge an Informationen über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf der Zug-Ebene weit weniger von der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und einzigartigen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es vorherige Methoden erreichen können. Sie können in den Ergebnissen unseres Experiments sehen, dass mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, Verletzungen des gesunden Menschenverstands in etwa 20 % ihrer Antworten. Sie produzieren irrelevante Informationen in etwa 15 % der Antworten. Und sie widersprechen sich selbst oder ihrem Partner etwa 10 % der Zeit. Mit dem schnellen Fortschritt auf diesem Gebiet könnten viele dieser Fehlerraten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen auf dem Gebiet als ein bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Vasudha und ich bin Kandidatin für einen Master in Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die als Langpapier in die ACL 2023 aufgenommen wurde: Transfer Learning für die Erkennung von kognitiver Dissonanz, die die Herausforderung der seltenen Klasse angeht. Wir beginnen mit der Definition der kognitiven Dissonanz und warum es ein wichtiges Problem ist, das in der Sprache untersucht werden sollte. Einfach ausgedrückt, ist kognitive Dissonanz zwei Überzeugungen oder Handlungen, die inkonsistent sind. Wie in diesem Beispiel, in dem eine Person sagt: „Ich weiß, dass Zigaretten mich umbringen könnten“, und dann sagt: „Ich habe mir nach dem Meeting ein paar Zigaretten geschnappt“. Diese Überzeugung und Handlung sind inkonsistent und stehen in Dissonanz. Wenn man weiter sagt: „Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte“, rechtfertigt dies das zweite Auftreten und sie haben eine konsonante Beziehung. Während Dissonanz ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsprozess erleben, sind sie in der Sprache selten zu finden, unter anderen Arten von Diskursbeziehungen. Warum ist das wichtig? Die Untersuchung der kognitiven Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Veränderungen in Überzeugungen, Werten und Einstellungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz steht auch in Zusammenhang mit Angststörungen und kann helfen, die psychische Gesundheit von Menschen besser zu verstehen. Die Untersuchung von Dissonanz, die in der Sprache ausgedrückt wird, kann auch beim Verständnis von Extremismus und Polarisierung von gefährdeten Gruppen hilfreich sein. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um eine Ressource für kognitive Dissonanz zu schaffen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir haben einen dissonanzbasierten Ansatz verwendet, wie er im Flussdiagramm hier zu sehen ist. Tweets wurden mit einem PDTV-Parser verarbeitet und Paare von Diskurseinheiten wurden gemäß den in unserem Papier beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa tausend Beispiele von Diskurseinheitenpaaren gesammelt hatten, führten wir ein Training für einen anfänglichen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Keine Überraschung, der Klassifikator leistete nicht viel besser als zufällig. Angesichts des geringen Auftretens von Dissonanz und des Fehlens eines solchen Datensatzes stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen von Transfer Learning und Active Learning, um so zu annotieren, dass mehr dissonante Proben über weniger Annotationsrunden gesammelt werden können, wodurch die Gesamtkosten der Annotation gesenkt und die Erkennung von Dissonanz verbessert werden. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir den Active Learning-Prozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben. Themaunabhängige Dissonanzstandklassifikation, eine Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema, genannt Debatte hier, und auf binärer Klassifikation der Erweiterung und Vergleichsklassen von PDTB, da diese beiden eng mit der Vorstellung von Konsonanz und Dissonanz verwandt sind, und wir nennen sie CEE hier. Wir stellen fest, dass beim Übertragen die Null-Schuss-Leistung auf dem annotierten Datensatz bereits viel besser ist als zufällig, mit der besten mit AUC 0,62. Darüber hinaus finden wir, dass das Feintuning der CE-Aufgabe gefolgt von weiterem Feintuning auf Debatte eine viel bessere Null-Schuss-Leistung ergibt. Dies ist das Modell, das wir verwenden, um den Active Learning zu starten. Als nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des Active Learning und der Annotationen zu aktualisieren. Kumulativ sammelt alle Daten, die bisher aus der aktiven Annotation gesammelt wurden, während iterativ das Modell durch Training auf dem neuesten Satz gesammelter Daten aktualisiert. Über die verschiedenen Strategien stellten wir fest, dass kumulativ gleich oder besser als iterativ durchweg abschneidet. Als nächstes, um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeit der seltenen Klassenstrategie, PRC, um hauptsächlich die Beispiele auszuwählen, die vom aktuellen Modell in jeder Runde des AL wahrscheinlich dissonant sind. Wir vergleichen dies mit den anderen State-of-the-Art-AL-Strategien, die in der Community häufig verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für zufällig signifikant niedriger ist. In weiteren Runden des AL mit den beiden besten Strategien verbessern wir die Dissonanzklassifikation auf AUC 0,75, was die beste Leistung ist, die wir bisher bei der Aufgabe haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Annotationsqualität und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz hat und am besten für die seltene Klasse funktioniert. Die Annotatoren finden jedoch auch die Beispiele schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Akquisition seltener Klassen ist und das Starten von AL mit entsprechend gestalteten Transfer Learning-Aufgaben erheblich hilft. Wir stellen auch fest, dass die iterative Aktualisierung nützlich für das Transfer Learning aus einem anderen Bereich ist, während die aktive Annotation im Bereich von der kumulativen Aktualisierung profitiert. Dies sind die Links zu unserem Code, Datensatz und unserem Papier. Zögern Sie nicht, sich mit uns in Verbindung zu setzen, wenn Sie Fragen haben. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Akshata und heute stellen mein Co-Autor Martin und ich unsere Arbeit vor, die Kitmas-Tests, die die Integration von Wissen aus mehreren Quellen bewerten. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Modelle für das Verständnis natürlicher Sprache greifen auf eine Vielzahl von Wissensquellen zurück, wie z.B. Wissen, das in ihren Parametern enthalten ist, das in der Regel durch Vortraining erworben wird, und Wissen, das zu Inferenzzeiten in den Eingaben gegeben wird. Neuere Arbeiten zu Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen. Das Verständnis natürlicher Sprache erfordert jedoch oft Wissen, das auch zu Inferenzzeiten geliefert wird. Zum Beispiel kann in dem Satz \"John sah den neu gewählten Präsidenten im Fernsehen\" das vortrainierte Wissen Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer die instanzspezifische Entität John ist oder wer der neue Präsident ist, weil sich der Präsident seit dem Vortraining geändert haben könnte. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch inferenzzeitliches Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreference-Auflösungsaufgabe ein, die darauf abzielt, die Fähigkeit zu testen, auf Wissen zurückzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten den Datensatz mit menschlichen Studienpartnern und etablierten Coreference-Auflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kia ist eine Bäckerin. Servin und Kia trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er Fälle in einem Gerichtssaal entschied, war er glücklich, sich zu entspannen. Die Aufgabe hier ist es, die richtige Entität zu identifizieren, auf die das Pronomen \"er\" verweist, in diesem Fall Servin. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens entitätsspezifisches Wissen, wie z.B. Servin ist ein Richter. Und zweitens Hintergrundwissen, wie z.B. Richter entscheiden Fälle in Gerichtssälen. Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle erlernt, während entitätsspezifisches Wissen in der Regel zu Inferenzzeiten beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von Kitmos definiert. Erstens haben wir die Tobika-Einstellung, Hintergrund-Vortraining, bei der angenommen wird, dass Hintergrundwissen zu Vortrainingszeiten verfügbar ist. Zweitens gibt es die Hintergrund-beide-Einstellung, bei der Hintergrundwissen sowohl zu Vortrainings- als auch zu Inferenzzeiten verfügbar ist. Schließlich die Hintergrund-Inferenz-Einstellung, bei der beide Wissensarten nur zu Inferenzzeiten verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das Hintergrundwissen, das zur Lösung einer Aufgabe erforderlich ist, nicht Teil der vortrainierten Daten der Modelle ist, z.B. weil sich neue Berufe seit der Zeit des Vortrainings entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den beiden Quellen kontrollieren. In der Hintergrund-Vortrainings-Einstellung nehmen wir an, dass das Hintergrundwissen, dass Politiker gewählte Sitze in der Regierung anstreben, in den vortrainierten Parametern enthalten ist. Im Inferenzzeit-Kontext liefern wir das entitätsspezifische Wissen, dass Chichester ein Politiker ist. In der Hintergrund-beide-Einstellung liefern wir zusätzlich nicht nur entitätsspezifisches, sondern auch Hintergrundwissen über Politiker im Inferenzzeit-Kontext. In der Hintergrund-Inferenz-Einstellung liefern wir den fiktiven Beruf Meretua anstelle von Politiker, da Meretua unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir bewerten den Datensatz sowohl mit menschlichen Studienpartnern als auch mit etablierten Coreference-Auflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der bestplatzierten Modelle bei der schwierigsten Variante der Hintergrund-Vortrainings-Einstellung. Ohne spezifisches Training auf Kitmos leisten beide Modelle keine gute Arbeit. Wenn jedoch auf Kitmos trainiert wird, leisten sowohl C2F als auch Built4Coref signifikant besser als die zufällige Wahl. Dies deutet darauf hin, dass Modelle, wenn sie auf allgemeinen Coreference-Auflösungsdatensätzen trainiert werden, lernen, Oberflächenhints auszunutzen, die beim Testen auf Kitmos, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen deuten darauf hin, dass selbst die bestplatzierten Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur zu Inferenzzeiten geliefert wird. Zusammenfassend lässt sich sagen, dass viele Coreference-Auflösungsmodelle ohne spezifisches Training nicht in der Lage zu sein scheinen, über Wissen aus verschiedenen Quellen zu argumentieren. Mit spezifischem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Dennoch scheinen selbst die bestplatzierten Modelle Schwierigkeiten zu haben, Hintergrundwissen zuverlässig zu integrieren, das nur zu Inferenzzeiten geliefert wird. Wenn Sie an weiteren Details interessiert sind, sehen Sie sich bitte unseren Artikel an und werfen Sie einen Blick auf den Datensatz und den Code auf GitHub. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sarah Papi von der Universität von Trento und der Fondazione Bruno Kessler, und ich werde kurz das Papier \"Attention as a Guide for Simultaneous Speech Translation\" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist Simultansprachübersetzung? Simultansprachübersetzung oder SimulST ist der Prozess der Übersetzung von gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, der eine Kommunikation über Sprachgrenzen hinweg ermöglicht. Und was sind die Probleme der derzeitigen SimulST-Modelle? Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche Module eingeführt werden, die optimiert werden müssen. Lange und komplizierte Trainingsverfahren, zum Beispiel Training, das verschiedene Optimierungsziele beinhaltet, und Training und Pflege mehrerer Modelle, um verschiedene Latenzregime zu erreichen, zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Was ist also unsere Lösung? Erstens, die Verwendung bereits existierender Offline-ST-Modelle ohne erneutes Training oder die Adoption einer spezifischen Architektur für SimulST. Verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz durch spezifische Parameter. Und nutzen Sie das bereits erworbene Wissen des Modells durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Textausgabe, das heißt, den Quer-Attentionsmechanismus. Und Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, ein DOT oder Encoder-Decoder-Attention vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben wird oder nicht, basierend darauf, worauf die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, diese Summe liegt unter einem bestimmten Schwellenwert Alpha, auf die letzten Lambda-Sprachrahmen, was bedeutet, dass die empfangenen Informationen stabil genug sind. Zum Beispiel, wenn wir einen Sprachabschnitt empfangen, der \"I'm going to talk about\" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Kreuzattentionsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen zeigen, während das letzte Wort auf die letzten empfangenen Sprachrahmen zeigt, also Lambda-Sprachrahmen. Dies bedeutet, dass die ersten beiden Wörter ausgegeben werden, während, da die Summe der Kreuzattention über einem bestimmten Schwellenwert Alpha liegt, wir das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten. Wenn wir weitermachen und einen weiteren Sprachabschnitt empfangen und unser Modell drei weitere Wörter vorhersagt und wir uns die Kreuzattentionsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda-Sprachrahmen zeigt. Dies bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse eines DOT ansehen, werden wir die Ergebnisse der Simultansprachübersetzung in Diagrammen darstellen, in denen wir auf der einen Seite Blau haben, das die Übersetzungsqualität misst, und die durchschnittliche Verzögerung, das heißt, das Latenzmaß, und wir berücksichtigen auch die berechnungsbewusste durchschnittliche Verzögerung, die die Berechnungszeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen also, dass unsere Kurven in diesem Diagramm so hoch wie möglich sind, aber auch, dass sie nach links verschoben sind. Und wir vergleichen mit vorherigen Strategien, die auch auf Offline-Modelle angewendet werden, nämlich der Wait-K-Strategie und der lokalen Übereinstimmung. Und wir vergleichen auch mit der State-of-the-Art-Architektur, die speziell für die Simultansprachübersetzung zugeschnitten ist. Dies sind alle Ergebnisse der Simultansprachübersetzungsstrategie auf Deutsch. Und wir sehen, dass ein DOT alle Strategien übertrifft, die auf Offline-Modelle angewendet werden, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die berechnungsbewusste Zeit berücksichtigen, ein DOT die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier, und wir haben auch den Quellcode und die Modelle und die Simultaneous-Outputs als Open Source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Zhu Hang. Heute werde ich unseren Artikel vorstellen: Funktionieren die NER-Tagger von Connell 2003 im Jahr 2023 noch gut? Fangen wir an. Unser Artikel untersuchte das Problem der Generalisierung anhand der Aufgabe der Erkennung benannter Entitäten oder der NER-Aufgabe. Wir stellten fest, dass Modelle seit fast 20 Jahren Connell 2003 verwenden, um NER zu entwickeln. Dies wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Connell++-Datensatz entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und dann mit denselben Connell-2003-Annotationsrichtlinien annotiert haben. Wir haben dann über 20 Modelle auf Connell 2003 feinabgestimmt. Wir haben sie sowohl auf dem Connell 03-Testdatensatz als auch auf dem Connell++-Testdatensatz bewertet. Und last but not least haben wir die prozentuale Änderung des F1-Werts berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? In unseren Experimenten haben wir festgestellt, dass es drei Hauptbestandteile gibt, die erforderlich sind. Der erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformermodelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle in der Regel zu einer besseren Generalisierung führen. Und last but not least wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer nachgelagerten Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist das adaptive Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird, und dies manifestiert sich normalerweise als abnehmende Renditen auf dem neuen Testdatensatz. Die zweite Hypothese ist der temporale Drift, der Leistungsabfall, der durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird. Beim adaptiven Overfitting sahen wir, dass die rote Best-Fit-Linie in der rechten Grafik einen Gradienten hat, der größer als eins ist. Dies bedeutet, dass jede Einheit der Verbesserung, die wir bei Connell 2003 erzielt haben, zu mehr als einer Einheit der Verbesserung bei Connell++ führt, was bedeutet, dass es keine abnehmenden Renditen gibt. Und das zeigt uns, dass in diesem Fall kein adaptives Overfitting beobachtet wird. Was ist also mit dem temporalen Drift? Beim temporalen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten neu zu trainieren oder weiter vorzutrainieren, und wir haben festgestellt, dass die Leistung mit einer größeren zeitlichen Lücke abnimmt. Und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der temporale Drift ist. Unser Schluss ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigen. Und diese gehen Hand in Hand. Wir können nicht nur einen Bestandteil haben, sondern auch die anderen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch den temporalen Drift verursacht wird, und überraschenderweise nicht durch adaptives Overfitting, obwohl Connell 2003 seit über 20 Jahren verwendet wird. Gehen wir also zurück zu der Frage, die wir im Titel unseres Artikels gestellt haben: Funktionieren die Connell-2003-Tagger im Jahr 2023 noch? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber anregt, wie die Generalisierung der Modelle verbessert werden kann. Und schließlich stellen Sie bitte sicher, dass Sie unseren Artikel und unseren Datensatz überprüfen, und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, willkommen zu unserer Präsentation von DeepLain, einem neuen Korpus für die Vereinfachung deutscher Texte auf Dokumenten- und Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess, bei dem ein Text angepasst wird, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie z. B. Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, z. B. von Dokumenten oder Sätzen. Im Beispiel hier sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z. B. lexikalische Substitution, Klauseltilgung, Klauseltilgung, Umordnung oder Einfügung von Wörtern. Wir schlagen nun unseren neuen Korpus DeepLain vor, weil es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihrer Ausrichtung sein können. Daher schlagen wir unseren neuen Korpus DeepLain vor, der in zwei Subkorpora unterteilt ist, DeepLain APA und DeepLain Web. DeepLain APA basiert auf Nachrichtentexten. In DeepLain APA haben wir 483 Dokumente manuell ausgerichtet. Das ergibt etwa 30.000, 13.000 parallele Satzpaare. Für DeepLain Web umfasst dieser Korpus verschiedene Domänen und wir haben auch alle diese 750 Dokumente manuell und mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergeben sich 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, z. B. nach dem Typ der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als z. B. die Nachrichtentexte oder die Sprachlerntexte. Auf allen Ebenen, z. B. lexikalische Vereinfachung, strukturelle Vereinfachung oder das allgemeine Vereinfachungsniveau. Darüber hinaus sehen Sie, dass unser DeepLain-Korpus eine hohe Vielfalt verschiedener Vereinfachungstransformationen aufweist. Zum Beispiel haben wir im DeepLain-API-Korpus viel mehr Umordnungen und Wortergänzungen als im DeepLain-Web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Schauen wir uns nun an, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz DeepLain sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext von maschinellen Übersetzungen, bei denen wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache, denselben Inhalt haben, aber auf unterschiedlichen Komplexitätsstufen liegen. Und jetzt, da wir unseren Datensatz DeepLain haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und den Code, um unsere Experimente durchzuführen, im Papier veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die Vereinfachung deutscher Texte die Methode von MassAlign ist. Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten im Papier auszuführen. Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell von Long Impart feinabgestimmt, um Vereinfachungen auf Dokumentenebene zu erzeugen, und wir haben auch das normale Basis-Impart feinabgestimmt, um Vereinfachungen auf Satzebene zu erzeugen. Sie können auch alle Checkpoints finden und Sie können sich mehr Details zu den Punktzahlen und den Bewertungsmetriken unserer Experimente im Papier ansehen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Punktzahlen als die Basislinien-Punktzahlen erzielen konnte, und wir schlagen diese Ergebnisse als Benchmark, einen Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Si Yuan von der Fudan University. Ich bin hier, um unsere Arbeit vorzustellen, die sich mit der Unterscheidung von Skriptwissen in großen Sprachmodellen für die eingeschränkte Sprachplanung befasst. Im täglichen Leben planen Menschen ihre Handlungen oft, indem sie schrittweise Anweisungen in Form von vorgegebenen Skripten befolgen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten zu planen, wie z.B. einen Kuchen zu backen, und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Allerdings konzentrieren sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie z.B. einen Schokoladenkuchen zu backen, bleibt weitgehend unerforscht. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit multifaktoriellen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In diesem Papier bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da keine Datensätze für spezifische Ziele existieren, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit multifaktoriellen Einschränkungen für die menschliche Schleife der Datenerfassung mit InstructGPT. Wir haben 100 spezifische Ziele beprobt und die von den Sprachmodellen generierten Skripte bewertet. Diese Tabelle berichtet über die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Sprachmodelle für Ergebnisse sorgen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir gehen in feiner granulierte Themenkategorien von Einschränkungen ein, die in Wikihow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPT für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen stark variiert, was zu schlechten Leistungen führt. Daher übernehmen wir die Idee der übermäßigen Generierung und Filterung, um die Generierungsqualität zu verbessern. Wir zeigen zunächst Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den angegebenen abstrakten Zielen. Dann generiert InstructGPT K-Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Einbettungen und berechnen die Cosinus-Ähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich bewerten wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir kopieren das Skript nur, wenn das Ziel die höchste Punktzahl im Zielsatz erzielt. Mit unserer Methode kann InstructGPT Skripte von hoher Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in semantischer Vollständigkeit als auch in der Treue zu den Einschränkungen erheblich. Da Sprachmodelle teuer im Einsatz sind, ist es wichtig, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wesentlicher Schritt zu diesem Zweck. Frühere Studien ermöglichen jedoch keine Planung für spezifische Ziele, und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode zur Erstellung eines Datensatzes für die eingeschränkte Sprachplanung an, der als CoScript bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testdatensätze zu gewährleisten, bitten wir Crowd-Sourced-Arbeiter, falsche Proben zu finden und zu überarbeiten. Diese Abbildung zeigt die Einschränkungsverteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Plausibilität in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung trainieren. Wir stellen fest, dass T5 auf CoScript Skripte von hoher Qualität generieren kann, was darauf hindeutet, dass kleinere Modelle größere Modelle unterstützen können, wenn sie auf geeigneten Datensätzen trainiert werden. Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert, die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle bewertet und eine übermäßige Generierungs- und Filtermethode für große Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung voranzutreiben. Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details zu CoScript in unserem Papier."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Yanis Lavrac und ich werde Ihnen unsere Arbeiten zu Dr. BERT vorstellen, einem robusten vorab trainierten Modell auf Französisch für den biomedizinischen und klinischen Bereich. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann stellen wir den Hauptbeitrag unseres Artikels vor. Wir stellen das erste biomedizinische Modell auf Französisch vor, das Dr. BERT genannt wird und auf Roberta basiert und auf Nachos trainiert wurde, einem Datensatz medizinischer Crowd-Daten aus dem Web. Wir stellen auch einen Vergleich von Modellen mit mehreren Vorab-Trainings-Einstellungen und Datenquellen vor. Dann präsentieren wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Downstream-Aufgaben auf Französisch. Und schließlich ziehen wir ein Fazit aus den Experimenten und geben Ihnen weitere Details, wie Sie auf die Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze geworden, um Aufgaben der natürlichen Sprachverarbeitung zu lösen, und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec, FastText oder NWO enorme Leistungsgewinne. Seitdem wurde dieses Modell an viele andere Sprachen angepasst, wie Französisch mit Camembert und andere Bereiche wie biomedizinisch mit PermitBERT und BioBERT und klinisch mit ClinicalBERT, aber hauptsächlich auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vorab-Training aufgrund des Mangels an In-Domain-Daten. Französisch hatte jedoch bisher kein Open-Source-Modell für biomedizinische Anwendungen. Wir fragten uns also, welche Datenquellen für eine breite Palette von Anwendungen am besten geeignet sind und ob diese Crowd-Daten einen guten Ersatz für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir Dr. BERT mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die aus dem Non-University Hospital Data Warehouse stammen. Danach fragten wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir zunächst vier Modelle von Grund auf. Eine erste Version von Dr. BERT mit 7 Gigabyte Nachos, eine zweite Version mit 4 Gigabyte Nachos, eine erste Version von Schubert, einem klinischen Modell, mit 4 Gigabyte Sätzen aus klinischen Notizen und eine letzte Version von Schubert mit einer Mischung aus 4 Gigabyte Nachos und 4 Gigabyte klinischen Notizen. Zusätzlich zu diesem Vergleich stellen wir drei Modelle vor, die auf kontinuierlichem Vorab-Training basieren, um den Einfluss der Vorab-Trainingsstrategie zu analysieren. Eines basiert auf den Gewichten von Camembert und wird auf 4 Gigabyte Nachos trainiert, ein anderes ebenfalls auf Camembert, aber dieses Mal auf 4 Gigabyte klinischen Notizen, und schließlich eines basierend auf dem englischen biomedizinischen Modell PermitBERT und trainiert auf 4 Gigabyte Nachos. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir mehrere öffentliche und private Downstream-Aufgaben wie Namens- und Erkennungsklassifizierung, Teil der Sprachmarkierung und Beantwortung von Fragen. Diese Modelle werden mit sechs Baseline-Modellen verglichen, die Camembert Oscar 138 Gigabyte, Camembert Oscar 4 Gigabyte, Camembert CCNet 4 Gigabyte, PermitBERT, BioBERT und ClinicalBERT sind. Die Bewertung zeigt, dass das Modell die Aufgabe am besten mit Daten derselben Art wie denen, auf denen das Modell trainiert wurde, ausführt. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir stellen auch fest, dass die Verwendung von mehr Daten zu besseren Leistungen führt. Insgesamt scheint das Vorab-Training von Grund auf höhere Leistungen bei den meisten Aufgaben zu erzielen. Unsere Experimente zum kontinuierlichen Vorab-Training unter Verwendung der Gewichte und des Tokenizers von PermitBERT, trainiert auf 4 Gigabyte Nachos, zeigen jedoch vergleichbare Ergebnisse wie die mit Dr. BERT 4 Gigabyte von Grund auf erzielten, was bei dem Modell auf der Grundlage der Gewichte und des Tokenizers von Camembert nicht der Fall ist, das unter Stabilitätsproblemen leidet. Schließlich, als Fazit, bietet unser eigenes System bessere Leistungen bei neun der 11 Downstream-Aufgaben und übertrifft insgesamt die Ergebnisse des generischen Modells hier, Camembert. Wir stellen auch fest, dass spezialisierte Daten besser sind, mehr spezialisierte Daten besser sind, aber sie skalieren nicht gut. Alle vorab trainierten Modelle, die aus Nachos stammen, sind kostenlos auf Hugging Face verfügbar und alle Trainings-Skripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf den Austausch in der Poster-Session in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Sean Bing, Doktorand an der University of Washington. Heute stelle ich unsere Arbeit vor, die sich mit der Vorverarbeitung von Daten, Sprachmodellen und nachgelagerten Aufgaben beschäftigt und die Spuren politischer Voreingenommenheiten verfolgt, die zu unfairen NLP-Modellen führen. Sprachmodelle werden auf großen Web-Crawling-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vorverarbeitungsdaten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, der Huffington Post usw. gut in den Trainingsdaten für Sprachmodelle vertreten sind. Dies hat sowohl Vor- als auch Nachteile für die Anwendung von Sprachmodellen. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen inhärent sozial voreingenommen und könnten zu potenziellen Fairnessproblemen bei der Anwendung nachgelagerter Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Propagationspipeline politischer Voreingenommenheiten von der Vorverarbeitung von Daten zu Sprachmodellen zu nachgelagerten Aufgaben zu untersuchen, indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Linie von Sprachmodellen und welche Rolle könnte die Vorverarbeitung von Daten bei solchen politischen Voreingenommenheiten spielen? Zweitens, wie performen Sprachmodelle mit unterschiedlichen politischen Linien tatsächlich bei nachgelagerten Aufgaben und ob dies zu Fairnessproblemen bei NLP-Anwendungen führen könnte? Konkret schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten unter Verwendung politischer Fragebögen wie dem politischen Kompass-Test zu befragen. Dies stellt sicher, dass die automatische Bewertung gut in der politischen Wissenschaftsliteratur verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie belegen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und dass GPT-Theorien im Allgemeinen sozial liberaler sind als BERT-Theorien und deren Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodell-Checkpoints auf sechs verschiedenen parteiischen Korpora weiter trainieren, die in Nachrichten und soziale Medien unterteilt sind, die weiter in ihre politische Neigung unterteilt sind. Durch das weitere Training von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel können wir bei Roberta, das weiter auf dem linksgerichteten Reddit-Korpus trainiert wurde, eine erhebliche Verschiebung in Richtung Liberalismus in Bezug auf seine politischen Voreingenommenheiten feststellen. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Vorverarbeitungskorpora in die Zeit vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trainieren Sprachmodelle separat auf den beiden unterschiedlichen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Schließlich bewerten wir Sprachmodelle mit unterschiedlichen politischen Neigungen bei der Erkennung von Hassreden und Fake News, zwei NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben können. Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Neigungen von Nachrichtenmedien unterteilen, ein Muster erkennbar ist, dass zum Beispiel linksgerichtete Sprachmodelle bei der Erkennung von Hassreden, die sich gegen sozial benachteiligte Gruppen richten, besser abschneiden, jedoch schlechter bei der Erkennung von Hassreden, die sich gegen mächtigere Gruppen in unserer Gesellschaft richten. Und umgekehrt sind rechtsgerichtete Sprachmodelle besser bei der Erkennung von Hassreden, die sich gegen Weiße und Männer richten, jedoch schlechter bei der Erkennung von Hassreden, die sich gegen schwarze LGBTQ+- und andere Minderheitsgemeinschaften richten. Ähnliche Trends treten auch bei der Erkennung von Fake News auf, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Fehlinformationen aus ihrer gegensätzlichen politischen Neigung zu erkennen, und umgekehrt. Wir zeigen viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorhersagen zu Hassreden und Fehlinformationsbeispielen basierend auf ihrer sozialen Kategorie treffen. Es gibt eine Menge weiterer Beispiele im Anhang, um dies weiter zu verdeutlichen. Dies deutet darauf hin, dass es ein sehr dringendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen gibt. Zum Beispiel, wenn rechtsgerichtete Sprachmodelle auf Hassreden oder Fehlinformationen oder was auch immer feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würden, würde dies bedeuten, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert werden könnten und Hassreden, die sich gegen Minderheitsgruppen richten, unkontrolliert um sich greifen könnten. Dies hat den Alarm ausgelöst, dass wir die Fairnessprobleme, die durch die politischen Neigungen von Sprachmodellen entstehen, anerkennen und angehen müssen. Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufdecken. Es ist wie zwischen Scylla und Charybdis. Wenn wir die politischen Meinungen in den Trainingsdaten für Sprachmodelle nicht bereinigen, wird sich die Voreingenommenheit von den Vorverarbeitungsdaten zu den Sprachmodellen zu den nachgelagerten Aufgaben ausbreiten und letztlich Fairnessprobleme schaffen. Wenn wir versuchen, sie irgendwie zu bereinigen, riskieren wir Zensur oder Ausschluss, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten für Sprachmodelle beibehalten werden sollte. Es ist also ein bisschen wie das elektrische Charlie-Problem. Okay, großartig. Ich denke, das war's für heute. Danke für Ihre Zeit."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Ich bin Kostov Sina und freue mich, Sie zu unserem Vortrag über unsere ACL 2023-Publikation \"Language Model Acceptability Judgments Are Not Always Robust to Context\" begrüßen zu dürfen. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy und Athena Williams. In dieser Arbeit überdenken wir das Minimal-Pair-Paradigma. Das Minimal-Pair-Paradigma bewertet Sprachmodelle anhand von Akzeptabilitätsurteilen, die auch Grammatikalität wie BLIMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotypen wie Crowspairs umfassen können. Im Minimal-Pair-Paradigma wird ein Sprachmodell typischerweise so bewertet, dass ein akzeptabler Satz oder ein grammatikalischer Satz gezeigt wird und dann ein unakzeptabler Satz oder ein ungrammatischer Satz. Die Hoffnung ist, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, die Akzeptanz von Modellen für längere Sätze zu bewerten. Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern auf den Markt. Es ist daher entscheidend, dass wir die Akzeptanz der Modelle im gesamten Kontextfenster bewerten. Und genau das versuchen wir hier zu tun. Wir versuchen, die MPP-Pipeline zu überdenken, indem wir das Modell auffordern, die Akzeptanz längerer und längerer Sequenzen zu bewerten. Das ist der Ansatz. Was wir tun, ist, dass wir, um diese längeren Sequenzen zu simulieren, die Datensätze selbst überdenken und dann Sätze neu erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar von Grammatikalität aus dem BLIMP-Datensatz aus dem Adjunct Island-Fall ausgewählt. Und was wir tun, ist, dass wir, um längere Sequenzen zu erstellen, die akzeptabel sind und die gleiche Übereinstimmung der grammatikalischen Struktur haben, grammatikalische Sätze aus Adjunct Island extrahieren und diese dann als Präfix sowohl zur akzeptablen Abfrage als auch zur unakzeptablen Abfrage hinzufügen. Wir können dasselbe tun, indem wir unakzeptable Sätze aus derselben Übereinstimmung auswählen, und das könnte auch verwendet werden, um die Akzeptanz des Modells zu testen. Und wir könnten dasselbe tun, indem wir Sätze aus einem anderen Teilsatz oder einem anderen Datensatz auswählen. Das nennen wir das Mismatch-Szenario. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, den wir bewerten. Und wir können dasselbe für den Unakzeptabilitätsfall tun. Schließlich können wir Sätze aus einem völlig unrelateden Bereich wie Wikipedia auswählen. Das wird uns zeigen, ob die Akzeptabilitätsurteile des Modells tatsächlich durch einen Kontext beeinflusst werden, sei es, dass der Kontext aus einem anderen Teilsatz des Datensatzes stammt oder völlig irrelevant für den aktuellen Satz ist, den wir betrachten. Wie schneidet das Modell ab? Zunächst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind. Und dort stellen wir fest, dass die MPP-Urteile für eine beliebige Kontextlänge größtenteils robust sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um OPT- und GPT2-Modelle zu maximieren. Und wir sahen hier in der orangefarbenen Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier erstellen wir Sätze aus akzeptablen und unakzeptablen Bereichen aus demselben BLIMP- oder SyntaxGym-Datensatz. Und dort sehen wir, dass die MPP-Urteile entweder signifikant steigen oder sinken, wenn wir entweder akzeptable Präfixe oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, das heißt, wenn wir die Sätze aus demselben Phänomen in BLIMP oder SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Und dieser Effekt ist sehr groß, er nimmt im gesamten Kontext zu und würde wahrscheinlich neuere Sprachmodelle mit großem Kontextfenster beeinflussen. Warum beeinflusst das Match-Präfix das Sprachmodellurteil so sehr? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabesatz zu stören, indem wir die relevante Struktur beibehalten, aber Rauschen zum Input hinzufügen. Und nach mehreren dieser Störungen stellen wir fest, dass keines dieser Rauschen das Modell tatsächlich dazu bringt, seinen Kurs in Bezug auf den MPP-Urteilstrend zu ändern. Im Grunde genommen stellen wir fest, dass die Modelle auf die gestörten Sätze in ähnlicher Weise reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen. Und wenn wir die Sätze im unakzeptablen Bereich stören, sehen wir einen ähnlichen Rückgang der MPP-Urteile. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über die Sätze hinweg geteilt werden. Und die MPP-Bewertung, so wie wir sie derzeit mit kurzen und einzelnen Satzinputs durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Lesen Sie bitte unseren Artikel für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich bin Dawei, ein Doktorand an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit vorstellen, \"Weaker Than You Think\", ein kritischer Blick auf schwach überwachtes Lernen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Mario Lučić, Gijs van Steenkiste und Dieter Fox. Ich möchte mit einer kurzen Einführung in schwache Überwachung und schwach überwachtes Lernen beginnen. Bei schwacher Überwachung werden die Daten nicht manuell beschriftet. Stattdessen beschriften wir die Daten mit schwachen Beschriftungsquellen, wie einfachen heuristischen Regeln, Wissensdatenbanken oder minderwertiger Crowdsourcing, wie im Bild rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel günstiger, aber sie sind auch verrauscht, was bedeutet, dass ein bestimmter Anteil der Annotationen falsch ist. Wenn wir neuronale Netzwerke direkt auf schwach beschrifteten Daten trainieren, neigen die neuronalen Netzwerke dazu, das Rauschen der Beschriftung zu memorieren und generalisieren nicht. Beim schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netzwerke robust unter solchem Beschriftungsrauschen zu trainieren, sodass die trainierten Modelle immer noch gut generalisieren. In jüngsten Arbeiten im WSL, also WSL steht für schwach überwachtes Lernen, ist ein häufiger Anspruch, dass die Leute sagen, dass sie nur auf schwach beschrifteten Daten trainieren und eine hohe Leistung auf sauberen Testsets erzielen. Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt einen Haken, nämlich dass die Leute annehmen, dass ein zusätzliches sauberes Validierungsset für die Modellauswahl verfügbar ist. Wir haben uns auf dieses Problem eingestellt, da dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die Annahme, die wir getroffen haben, führt uns dazu, drei Forschungsfragen zu stellen. Erstens, ist sauberes Validierungsdaten für WSL notwendig? Oder können wir vielleicht stattdessen ein verrauschtes Validierungsset verwenden? Zweitens, wenn saubere Daten erforderlich sind oder wenn saubere Daten für WSL zwingend erforderlich sind, wie viele saubere Proben benötigen wir dann? Schließlich, sollten wir nur die sauberen Proben zur Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass interessanterweise neuere WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall, wie in diesem Bild gezeigt. Wenn es keine sauberen Validierungsproben gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Beschriftungen hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um richtig zu funktionieren, und die Annotationskosten für das Erhalten sauberer Validierungsproben sollten nicht übersehen werden. Unsere zweite Feststellung ist, dass die Erhöhung der Anzahl sauberer Validierungsproben den WSL-Ansätzen hilft, eine bessere Leistung zu erzielen, wie im Bild links gezeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist noch nicht das Ende der Geschichte, denn wenn wir uns entscheiden, auf saubere Proben zuzugreifen, dann wird das direkte Training auf ihnen sogar eine bessere Leistung erzielen. Das rechte Bild zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur zur Validierung verwenden. Wie wir sehen können, beginnt die direkte Feinabstimmung, WSL-Ansätze zu übertreffen, wenn wir 10 Proben pro Klasse haben. Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansätzen beansprucht wird, leicht durch die Erlaubnis zur kontinuierlichen Feinabstimmung auf den sauberen Validierungsproben erreicht werden. Wie wir aus den Bildern sehen können, unterschreitet das Vanilla-Modell, das als FTW bezeichnet wird, anfangs kompliziertere WSL-Methoden wie CoMatch. Wenn wir jedoch die kontinuierliche Feinabstimmung auf den sauberen Proben zulassen, dann leistet FTW genauso gut wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Proben benötigen, damit sie richtig funktionieren. Ihre Leistungsgewinne und Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie die Modellauswahlkriterien. Zum Beispiel berichten Sie, ob die Modellauswahl mit sauberen Validierungsproben durchgeführt wird. Zweitens sollten WSL-Ansätze mit Few-Shot-Learning-Basislinien verglichen werden, da beide auf sauberen Proben arbeiten. Drittens ist die kontinuierliche Feinabstimmung eine einfache, aber starke Basislinie, die in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code open source gemacht. Sie können ihn mit dem QR-Code auf diesem Folien finden. Bitte zögern Sie nicht, ihn sich anzusehen. Vielen Dank und viel Spaß auf der Konferenz."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist David Villar und ich werde eine kurze Übersicht über den Artikel \"Prompting Palm for Translation: Assessing Strategies and Performance\" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. Palm ist ein Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfassen. Zum Zeitpunkt der Veröffentlichung erreicht es den Stand der Technik in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Prompting von großen Sprachmodellen für maschinelle Übersetzung. Wir haben die Übersetzungskapazität solcher Modelle unter Verwendung der besten Praktiken der IMT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testsets, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir vergleichen zwei Systeme des Standes der Technik, also die bestleistenden Systeme der WMT-Bewertung. Wir verwenden Metriken des Standes der Technik für maschinelle Übersetzung und zeigen zusätzlich Ergebnisse von Experten-basierten menschlichen Bewertungen. Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs für die Übersetzung. Wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwenden und zwei verschiedene Prompts für einen Satz bereitstellen, beträgt der Unterschied bei der Mehrheit der Sätze, 516 von 1.000, mehr als einen BLEU-Punkt. Und dies kann in extremen Fällen bis zu 40 BLEU-Punkten gehen. Es ist also wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine Five-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der er geschrieben ist. In diesem Beispiel hier, bei dem wir eine Übersetzung von Deutsch nach Englisch durchführen, sind die deutschen Sätze, die Quellsätze, mit einem deutschen Doppelpunkt markiert und die englischen Übersetzungen mit einem englischen Doppelpunkt. Wir haben festgestellt, dass die tatsächliche Form des Promptings im Fall von mehreren kurzen Prompts keinen großen Einfluss hat. Es ist entscheidend für Null- und One-Shot-Prompting, und wenn wir, wie in unserem Fall, zu Five-Shot-Prompting übergehen, gibt es nahezu keinen Unterschied in der tatsächlichen Form des Promptings. Es sind die Beispiele, die das meiste Gewicht tragen. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quellsatz. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Bewertungen oder den Dev-Daten. Die Dev-Daten sind viel sorgfältiger erstellt und von höherer Qualität als die Trainingsdaten, die es sind, und die Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte Systeme des Standes der Technik einen erheblichen Vorteil gegenüber den Palm-Übersetzungen, aber Palm kommt einem kommerziellen System ziemlich nahe. In unserem Fall haben wir uns entschieden, mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sind, dass die Flüssigkeit von Palm mit der von Systemen des Standes der Technik vergleichbar ist, aber der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass Palm sich entscheidet, eine besser klingende Übersetzung zu produzieren, manchmal indem Teile des Quellsatzes weggelassen werden, die in der Übersetzung irrelevant sind. Die Kategorie \"Ausdrucksstil\" für Palm ist jedoch niedriger als für die Systeme des Standes der Technik, was ein zusätzliches Signal dafür ist, dass Palm wirklich flüssige Ausgaben liefert, aber immer noch einige Genauigkeitsprobleme hat. Das war's für diese wirklich kurze Übersicht. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Artikels. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Jin Wei Yi von der Universität für Wissenschaft und Technologie in China. Es ist mir eine Freude, ein kurzes Werbevideo über das Papier \"Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding as Services via Backdoor Watermark\" zu präsentieren. Lassen Sie uns zunächst den Hintergrund zu Embedding as Services einführen. Derzeit sind große Sprachmodelle wie TPT, Lama, Palm außergewöhnlich in der natürlichen Sprachverarbeitung und -generierung. Embedding as Services ist einer der Dienste, die auf großen Sprachmodellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Embedding-API an. Allerdings haben aktuelle Arbeiten gezeigt, dass der Angreifer das Modell durch Lernen aus dem Embedding stehlen und ähnliche Dienste anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding as Services zu schützen. Um das Urheberrecht von Embedding as Services zu schützen, ist eine der Lösungen, ein Wasserzeichen in den Dienst des Anbieters einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Embedding as Services anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt genug sein, oder der Angreifer kann das Wasserzeichen leicht entfernen. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methoden sind jedoch entweder nicht auf Embedding as Services anwendbar oder fehlt es an Übertragbarkeit. Daher schlagen wir in diesem Papier Embedding Marker vor, eine auf Backdoor basierende Wasserzeichenmethode, die auf Embedding as Services anwendbar ist. Lassen Sie mich nun die Details unseres Embedding Markers vorstellen. Embedding Marker enthält zwei Hauptschritte: Wasserzeicheninjektion und Urheberrechtsverifikation. Bevor diese Hauptschritte durchgeführt werden, wählen wir zunächst einen Auslösesatz aus. Der Auslösesatz ist eine Gruppe von Wörtern in einem moderaten Häufigkeitsintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeicheninjektion definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Dienst des Anbieters sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als M ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifikation besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zunächst einen Backdoor- und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, bei denen alle Wörter zum Auslösesatz gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zum Auslösesatz gehören. Dann fordert der Anbieter die Embeddings vom gestohlenen Dienst mit dem Datensatz an. Die Cosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem harmlosen und dem Backdoor-Datensatz, die als Delta Cosinus und Delta L2 definiert ist. Gleichzeitig wenden wir auch den KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente an vier Datensätzen durch: AG News, Mind, SST2 und ErosBAM. Wir nehmen an, dass der Anbieter den Wikitext-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse an vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielen kann, während er eine hervorragende Nützlichkeit für Downstream-Aufgaben beibehält. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir das Embedding von Sätzen auf 40 Datensätzen mit BOPC visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Auslöser in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwer, zwischen den Backdoor-Embeddings und den normalen Embeddings zu unterscheiden. Das war's, vielen Dank. Wir kommen, um mit uns zu diskutieren."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, mein Name ist Ying, und mein Kollege Ziyang und ich werden unsere Forschung zu Multi-Instruct vorstellen, bei der die multimodale Zero-Shot-Learning durch Instruktionsabstimmung verbessert wird. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, bei denen vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben auf eine parameter- und dateneffiziente Weise wiederverwendet werden. In letzter Zeit haben viele Studien gezeigt, dass die Instruktionsabstimmung große Sprachmodelle in der Lage versetzt, auf unsichtbare Aufgaben in einer Zero-Shot-Manier zu arbeiten, indem sie natürlichen Anweisungen folgen. Allerdings konzentrierten sich die meisten früheren Arbeiten zur Instruktionsabstimmung darauf, die Zero-Shot-Leistung bei sprachbasierten Aufgaben zu verbessern, während Computer Vision und multimodale Aufgaben außen vor gelassen wurden. Daher wollen wir in dieser Arbeit untersuchen, ob die Instruktionsabstimmung bei multimodalen vortrainierten Modellen tatsächlich die Generalisierung auf unsichtbare multimodale Aufgaben verbessern kann. Zusätzlich stellten wir bei unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instruktionsdatensätzen zwischen NLP und multimodalen Aufgaben fest. Es gibt mehr als 1.600 sprachbasierte Instruktionsaufgaben. Es gibt jedoch keine groß angelegte, öffentlich verfügbare multimodale Instruktionsaufgabe. Dies motivierte uns, einen multimodalen Instruktionsabstimmungsdatensatz zu erstellen. Hier stellen wir Multi-Instruct vor, den ersten multimodalen Instruktionsabstimmungs-Benchmark-Datensatz, der aus 62 verschiedenen multimodalen Aufgaben besteht, die 10 breite Kategorien abdecken. Diese Aufgaben leiten sich von 21 bestehenden Open-Source-Datensätzen ab, und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet. Um die multimodale Instruktionsabstimmung an unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA, ein vereinheitlichtes multimodales vortrainiertes Modell, als unser Basis-Modell. OFA verwendet ein vereinheitlichtes Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instruct-Datensatz. Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem der Eingabetext, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt werden. Okay, jetzt werde ich über die multimodale Instruktionsabstimmung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus der NAG-Gruppe zum Training und wir nehmen 10.000 Instanzen pro Aufgabe. Zum Testen reservieren wir die gesamte Common Sense Reasoning-Gruppe zum Testen und wir wählen zusätzliche fünf Aufgaben aus der VQA- und Miscellaneous-Gruppe aus. Wir verwenden alle Instanzen im Test-Split für jede Aufgabe. Zusätzlich nehmen wir zufällig 20 Aufgaben aus dem Test-Split von Natural Instruction als unsichtbare Aufgabe für NLP. Wir verwenden ein vortrainiertes OFA Large-Modell als Basis-Modell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Instruktionsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell unter Verwendung einer der fünf Anweisungen in jedem Experiment bewerten. Wir berichten über die minimale und maximale Leistung und die Standardabweichung der Leistung über alle fünf Experimente. Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, berichten wir über Rouge L. Für eine NLP-Aufgabe berichten wir ebenfalls über Rouge L. Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt. Diese misst die Fähigkeit des Modells, konsistent die gleichen Ausgaben für die gleiche Aufgabe zu produzieren, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier sind unsere Hauptresultate. Wie wir sehen können, kann die Instruktionsabstimmung die Leistung von OFA bei unsichtbaren multimodalen Aufgaben erheblich verbessern. Auch das Transfer-Lernen von Natural Instruction-Datensätzen kann die Instruktionsabstimmung unterstützen. Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig eine geringere Sensitivität aufweist. Wir haben auch ein Experiment durchgeführt, bei dem wir eine Anweisung gegen fünf Anweisungen verwendet haben. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transfer-Lernen von Natural Instruction-Datensätzen eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell erreichen. Wir können auch sehen, dass das Transfer-Lernen von Natural Instruction-Datensätzen OFA helfen kann, eine viel bessere Leistung auf dem Natural Instruct-Datensatz zu erzielen. Insgesamt schlagen wir den ersten groß angelegten multimodalen Instruktionsabstimmungsdatensatz vor. Wir verbessern die Zero-Shot-Fähigkeit von OFA erheblich und untersuchen verschiedene Transfer-Learning-Techniken und zeigen deren Vorteile. Wir haben eine neue Metrik namens Sensitivität entwickelt. Noch eine Sache, wir sammeln einen viel größeren multimodalen Instruktionsabstimmungsdatensatz mit etwa 150 zusätzlichen visuellen Sprachaufgaben und werden diese veröffentlichen. Dies ist ein QR-Code für unsere Daten und das Modell. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Mein Name ist Justin John von der Penn State University. Heute werde ich unsere Arbeit vorstellen, Exemplar, Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Semantisches Parsing ist die Aufgabe, semantische Repräsentationen von Benutzerabfragen zu erstellen, wie SQL und Lambda-Kalkül. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Abfrage in mehreren natürlichen Sprachen unter Verwendung neuronaler Modelle in SQL, Lambda oder FungQL usw. übersetzen. Bestehende Cross-Lingual Semantic Parsing-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel gibt es keine Abdeckung für bestimmte natürliche Sprachen. Chinesisch fehlt und es gibt keine Abdeckung für bestimmte Bedeutungsrepräsentationen. Der Lambda-Kalkül fehlt. Oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten. Zu diesem Zweck schlagen wir Exemplar vor. Wir bieten einen einheitlichen Datensatz Exemplar für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Er enthält 90 Datensätze in verschiedenen Domänen, fünf semantische Parsing-Aufgaben, acht Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unseren Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für das Training und die Bewertung. Die erste ist Übersetzen Test. Wir verwenden die Google Translate API, um die Quellsprache in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell zum Trainieren und Bewerten. Und zum Beispiel trainieren wir das englische Modell auf englischen Abfragen und während der Inferenz übersetzen wir die deutsche Abfrage mit der API in Englisch und verwenden dann das trainierte Modell, um das SQL vorherzusagen. Und wir testen auch monolinguale Modelle. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache. Zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch monolinguale Few-Shot-Einstellungen, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen ein multilinguales Modell, bei dem wir ein multilinguales Modell für alle Sprachen trainieren. Zum Beispiel setzen wir die deutschen, englischen und chinesischen Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Und wir berücksichtigen auch Cross-Lingual Zero-Shot- und Few-Shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir auf englischen Abfragen oder der Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein multilinguales Modell zu trainieren und die SQL-Ausgabe vorherzusagen. Und wir finden auch viele interessante Ergebnisse. Was die Analyse monolingualer Modelle betrifft, bewerten wir zwei Gruppen von Modellen, einschließlich Encoder PDR, was für multilinguale vorab trainierte Encoder mit pointerbasierten Decodern steht, wie XLM-R plus PDR und BERT plus PDR. Und wir bewerten auch Encoder-Decoder-Modelle, die multilinguale vorab trainierte Encoder-Decoder-Modelle sind, wie MBART und MT5. Wir stellten fest, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielt. Und wir bewerten MT5 und XLM-R plus PDR in einer multilingualen Einstellung. Wir stellten fest, dass Encoder-Decoder oder Encoder PDR durch Training in einer Mischung verschiedener Sprachen verbessert werden können. Und wir stellten fest, dass dies daran liegt, dass die meisten der wichtigsten natürlichen Sprachen eine Leistungssteigerung erzielen können, außer dass die englische Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen gewinnt. Ich denke, das ist als Fluch der Multilingualität bekannt. Wir vergleichen auch die Cross-Lingual-Leistungslücke. In dieser Abbildung ist die blaue Linie Cross-Lingual Few-Shot-Transfer. Die orange Linie ist Cross-Lingual Zero-Shot-Transfer, während die grüne Linie die monolinguale Einstellung ist. Wir stellten fest, dass durch den Vergleich der grünen und orangen Linie die Cross-Lingual-Transferleistung in der Zero-Shot-Einstellung signifikant ist. Und durch den Vergleich der blauen und orangen Linie stellten wir fest, dass sich die Transferlücke in der Few-Shot-Einstellung schnell verkürzt. Wir finden auch einige andere interessante Ergebnisse. Zum Beispiel übertrifft Encoder-Decoder die vorherige Arbeit oder erzielt vergleichbare Ergebnisse. Das Training auf englischer natürlicher Sprache kann die Leistung von Few-Shot auf Zielnatürlichen Sprachen erheblich steigern. Und wir stellten fest, dass multilinguale Sprachmodelle wie CODA und BLOOM für Cross-Lingual Semantic Parsing-Aufgaben immer noch unzureichend sind. Zusammenfassend haben wir Exemplar aufgebaut, einen einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Arten von multilingualen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse usw. Und besuchen Sie unseren Artikel und Code. Vielen Dank fürs Zuhören."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Adam Spirkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, nehmen verschiedene Theorien und Korpusansätze unterschiedliche Abhängigkeitsstrukturen an. Zum Beispiel wird in den universellen Abhängigkeiten die Struktur der Koordination Lisa, Bart und Maggie so angenommen, dass das erste Konjunkt das Hauptwort der gesamten Koordinationsstruktur ist, in diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungstexttheorie angenommen, wo wiederum die gesamte Koordinationsstruktur durch das erste Konjunkt geleitet wird. Diese beiden Ansätze sind also asymmetrisch, richtig? Sie heben eines der Konjunkte hervor. Es gibt jedoch auch symmetrische Ansätze zu Koordinationsstrukturen, wie den Prager Ansatz, den konjunktionsgeleiteten Ansatz, der in den Prager Abhängigkeitsbäumen angenommen wird, wo Koordinationsstrukturen durch die Konjunktion geleitet werden. Wir erhalten also Abhängigkeiten von und zu allen Konjunkten. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der zum Beispiel in DeCatsons Wortgrammatik verwendet wird, wo sozusagen alle Konjunkte die Köpfe der Koordinationsstruktur sind. Wir erhalten also Abhängigkeiten vom Regenten, hier Loves, zu allen Konjunkten getrennt. Diese sind Bart und Maggie. Der Zweck dieses Papiers ist es, ein neues Argument für die symmetrischen Strukturen der Koordination wie diese beiden und gegen die asymmetrischen Strukturen der Koordination wie diese beiden zu liefern. Okay, das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. Wie Sie vielleicht wissen, bevorzugen direkte Objekte in Englisch, in der Nähe des Verbs zu sein, während Adjunkte weiter entfernt sein können, richtig? Also ist \"March read it yesterday\" in Ordnung, weil das direkte Objekt \"it\" in der Nähe des Verbs ist, während \"March read yesterday it\" viel schlechter ist, richtig? Denn hier befindet sich zwischen dem Verb und dem direkten Objekt ein Adjunkt \"yesterday\". Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, denn dann kann es in die Position nach dem Adjunkt verschoben werden. Dies wird hier veranschaulicht. Beide Sätze sind also in Ordnung. \"March read this absolutely fascinating book about the bees yesterday\" ist in Ordnung. Anstelle von \"it\" haben wir diesen langen NP. Aber es ist auch in Ordnung zu sagen: \"March read yesterday this absolutely fascinating book about bees.\" Der Grund dafür ist, dass dies möglich ist, weil dieser Satz, obwohl er das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also diejenigen, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir also eine Abhängigkeit von \"read\" zum Adjunkt der Länge 7, gemessen in Wörtern, und von \"read\" zu \"book\" der Länge 4. Zusammen sind es also 11. Wenn Sie diese beiden Konstituenten vertauschen, wird die Summe dieser beiden Abhängigkeiten zu 6, richtig? Also anstatt 11, 6, viel kürzer. Deshalb klingt das ziemlich in Ordnung, richtig? Es verletzt ein Prinzip, erfüllt aber ein anderes. Okay, was wir getan haben, wir haben verschiedene Statistiken über die Koordination aus der erweiterten Version des Penn Treebank extrahiert und sehen Sie das Papier, warum wir keine universellen Abhängigkeiten verwendet haben. Und diese Statistiken bestätigen die Beobachtung, die viele Male zuvor gemacht wurde, dass linke Konjunkte dazu neigen, kürzer zu sein. Also Salz und Pfeffer und nicht Pfeffer und Salz, gemessen in Silben. Und auch die Beobachtung, die nebenbei gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Wenn also der Unterschied zwischen den Längen der beiden Konjunkte wächst, bevorzugt das kürzere Konjunkt stärker, das erste zu sein, richtig? Also ist der Anteil der linken kurzen Konjunkte größer. Aber was neu an diesem Papier ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn die Regenten auf der linken Seite fehlen, richtig? Also fehlt der Regent auf der linken Seite in diesem Beispiel. Ich sah Bart und Lisa, also ist der Regent auf der linken Seite. Er fehlt im zweiten Beispiel. Homer kam und nieste. Hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regent, richtig? In solchen Fällen bevorzugt das linke Konjunkt, kürzer zu sein, umso mehr, je größer der Unterschied zwischen den beiden Konjunkten ist. Wenn jedoch der Regent auf der rechten Seite ist, wie hier, regiert Loves die Koordination Ted und Ned, verschwindet dieser Effekt. Wir haben gezeigt, dass durch die Messung der Länge in Zeichen, das ist die erste Spalte, in Silben die mittlere Spalte und in Wörtern die rechte Spalte, also werde ich mich auf die rechte konzentrieren. Was wir hier sehen, ist, dass, wenn der Regent auf der linken Seite ist, die Tendenz, dass das linke Konjunkt kürzer ist, mit der absoluten Differenz in Wörtern stetig wächst. Und dasselbe wird beobachtet, wenn es keinen Regent gibt, wie bei der Koordination von Sätzen. Aber wenn der Regent auf der rechten Seite ist, verschwindet diese Tendenz. Und wir zeigen im Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination wie diese beiden und für die symmetrischen Strukturen wie diese beiden liefert. Sehen Sie also das Papier für die vollständigen Argumente und sprechen Sie mit uns in der Postersitzung. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kailin und ich werde unsere Arbeit mit dem Titel \"Wann erfordert die Übersetzung einen Kontext? Eine datengesteuerte mehrsprachige Untersuchung\" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emily Liu, Andre F. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir \"Mole\" in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautete: \"Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden\", dann bezieht sich \"Mole\" auf einen Spion. Wenn der vorherige Satz jedoch lautete: \"Könnte es etwas Ernstes sein, Doktor?\", dann bezieht sich \"Mole\" auf ein Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Die Bewertung, wie gut Modelle Fälle wie diesen übersetzen können, ist jedoch ziemlich schwierig. Erstens, weil nur ein kleiner Teil der Übersetzungen kontextabhängig ist, was es Metriken auf Korpusebene wie BLEU unmöglich macht, diese Übersetzungen zu erfassen. Und einige Leute haben eine gezielte Bewertung von kontextabhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachpaare, da sie in der Regel auf Domänenwissen und menschliche Kuratierung angewiesen sind. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert die Übersetzung einen Kontext? Und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, begannen wir damit, zu messen, wie sehr ein Wort bei der Übersetzung vom Kontext abhängt. In der vorherigen Arbeit führten wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodelle ein. Dies geschieht durch die Messung, wie viel Information der Kontext C über das Ziel Y bei gegebenem Quelltext X liefert. Man kann CXMI als die Information betrachten, die durch die Bereitstellung von Kontext an das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu punktweiser CXMI, die die Kontextnutzung auf Satzebene oder auf Worteil messen kann. Wir können uns Wörter mit hohem P6MI als solche vorstellen, die für die Übersetzung Kontext erfordern. Nun analysieren wir Wörter mit hohem PCXMI, um Muster zwischen diesen Wörtern zu finden. Und wir führen unsere Analyse an Transkripten von TED-Talks durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst betrachten wir Teile der Rede, die hohe Mittelwerte für PCXMI aufweisen. Und dies ermöglicht es uns, zum Beispiel duale Pronomen im Arabischen zu finden, die einen hohen P6MI aufweisen. Und dies lässt sich damit erklären, dass das Englische keine dualen Pronomen hat, sodass der Kontext bestimmt, ob ein Pronomen dual ist, wenn es ins Arabische übersetzt wird. Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die richtige Verbform wählen wollen. Dann betrachten wir Vokabeln, die einen hohen PCXMI aufweisen, gemittelt über alle ihre verschiedenen Vorkommen. Und dies hilft uns, Fälle wie den hier zu identifizieren, bei dem im Chinesischen Kontext benötigt wird, um Eigennamen richtig zu übersetzen, um sicherzustellen, dass dieselbe Übersetzung innerhalb des Dokuments verwendet wird. Und ähnlich finden wir, dass Kontext benötigt wird, um die richtige Förmlichkeit zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token, die einen hohen PCXMI aufweisen. Und dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z.B. die Auflösung von Ellipsen. Nun verwenden wir unsere Erkenntnisse aus unserer Analyse, um eine Benchmark für die Übersetzung auf Dokumentebene zu erstellen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Und wir nennen unseren Tagger den mehrsprachigen diskursbewussten oder MUDA-Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen. Wir verwenden dann den MUDA-Tagger, indem wir den Tagger auf einem parallelen Korpus anwenden, das wir für die Bewertung verwenden möchten. Und wir wenden unsere gewählten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MUDA-Tagger identifiziert hat. Und schließlich verwenden wir unsere Benchmark sowie andere Metriken, um verschiedene Modelle bei der maschinellen Übersetzung auf Dokumentebene zu bewerten. Erstens, wenn wir Metriken auf Korpusebene verwenden, also für BLEU, finden wir, dass kontextagnostische Modelle die beste Leistung erbringen. Wenn wir jedoch COMET verwenden, schneiden kontextbewusste Modelle am besten ab. Und wenn wir das Wort-F-Maß verwenden, haben Modelle mit und ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem auf Dokumentebene zu bestimmen, wenn man nur Metriken auf Korpusebene verwendet. Nun verwenden wir die MUDA-Benchmark, um Modelle zu bewerten, und wir finden, dass kontextbewusste Modelle für bestimmte Diskursphänomene, wie z.B. Förmlichkeit und lexikalische Kohäsion, signifikant genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipsen, Pronomen und Verbformen verwenden. Dies deutet darauf hin, wo wir mehr Fortschritte bei der Übersetzung auf Dokumentebene sehen müssten. Wir vergleichen auch verschiedene kommerzielle Systeme, und unsere Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate für die Übersetzung auf Dokumentebene. Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und dann verwenden wir unsere Erkenntnisse, um eine Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, die uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut bewältigen können oder nicht, und welche Übersetzungssysteme gut in der Übersetzung auf Dokumentebene sind. Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich Ihnen meine Arbeit \"NL Positionality: Charakterisierung von Design durch CZ-Datensätze und Modelle\" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Sante, Ronan Le Bras, Katarina Rynneka und Martin Sapp. Beginnen wir damit, uns vorzustellen, dass Sie für eine Zeitung arbeiten und Kommentare unter Ihrem Nachrichtenartikel durchgehen, um toxische Inhalte zu entfernen. Sie könnten sich an eine beliebte API wie die Perspective API für die Erkennung von Toxizität wenden, und das funktioniert wirklich gut, wenn Sie Carl Jones sind, wo die Perspective API in der Lage ist, toxische Inhalte korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Dithia Sharma, wo die Perspective API nicht so empfindlich auf beleidigende Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Bevölkerungsgruppen sehen. Design-Biases wie der, den wir gerade gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen. Dies ist ein Konzept, das in kritischen Studien weit verbreitet ist, insbesondere in feministischen und queeren akademischen Räumen. Und als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, weil sie die Entscheidungen, die Forscher treffen, verändern kann. Und so könnte eine Frage, die sich die Leute stellen, lauten: Haben Datensätze und Modelle eine Positionalität? Und wir wollen nicht sagen, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können daher bestimmte Positionalitäten gegenüber anderen repräsentieren. Frühere Arbeiten haben einige anekdotische Beweise für die Positionalität, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modell-Positionalität vorgeschlagen. Diese Arbeiten vergleichen jedoch nicht wirklich Endbenutzer mit den Datensätzen und Modellen selbst. Und das Studium der Modell- und Datensatz-Positionalität wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozialer werden. Und es ist eine Herausforderung, zu charakterisieren, wie diese Positionalitäten verzerrt sind, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. Um die Datensatz- und Modell-Positionalität zu untersuchen, vergleichen wir tatsächlich die Annotationen mit echten Benutzern mit bestehenden Datensätzen und Modellen. Dies tun wir durch unseren Rahmen NL Positionality. Unser Rahmen arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren neu zu annotieren. Und wir entscheiden uns dafür, dies zu tun, indem wir die Demografie der ursprünglichen Datensatz-Annotatoren betrachten, weil normalerweise nur wenige Annotatoren jede Instanz annotieren und weil Demografien selten gesammelt und geteilt werden. Und so entscheiden wir uns, Daten neu zu annotieren, um viele Annotationen pro Instanz zu erhalten und um einen reichen Satz demografischer Daten zu erhalten. Wir nehmen dann die Annotationen nach Demografie und vergleichen sie mit den Modellen und Datensätzen unter Verwendung des Pearson's R-Korrelationswerts. Und so unterscheidet sich unser Rahmen tatsächlich von der Literatur zur Annotator-Diskrepanz, indem er Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleicht, anstatt nur die Annotator-Übereinstimmung oder die Modellierung von Annotator-Verteilungen zu betrachten. Unser Rahmen wird größtenteils durch Lab in the Wild, eine Online-Crowdsourcing-Plattform, ermöglicht, ein ehemaliger HCI-Kollaborateur. Und Lab in the Wild ist eine Online-Experimentierplattform, auf der wir im Vergleich zu Plattformen wie MTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, diverse Freiwillige rekrutieren können. Und weiter ist Lab in the Wild in der Lage, hochwertige Daten zu erhalten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptabilität. Und so funktioniert das, dass die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann schreiben, wie sozial akzeptabel eine Situation ist. Danach können sie, um im Studium engagiert zu bleiben, ihre Antworten mit denen einer KI und anderer vergleichen. Wir vergleichen dann diese Annotationen mit Social Chemistry, Delphi und GPT-4. Wir replizieren dann eine sehr ähnliche Einrichtung für die Aufgabe der Erkennung von Toxizität und Hassrede, bei der sie eine Instanz aus Dynahate lesen und schreiben, ob sie denken, dass es sich um eine Instanz von Hassrede handelt. Wir vergleichen dann diese Annotationen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT-4. Unsere Studie umfasste am Ende über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern. Jetzt sind wir besser in der Lage zu beantworten, mit wem sich NLP-Datensätze und -Modelle am meisten decken. Wir stellen fest, dass es eine Positionalität im NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. So finden wir bei der GPT-4-Analyse der sozialen Akzeptabilität, dass sie am meisten mit Konfuzianismus und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass Dynahate ebenfalls am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch eine zusätzliche Übereinstimmung mit Menschen fest, die eine Hochschulbildung haben. So finden wir bei GPT-4 in der Aufgabe der sozialen Akzeptabilität, dass sie am meisten mit Menschen übereinstimmt, die eine Hochschul- oder Graduiertenausbildung haben. Und wir finden dasselbe für Dynahate, wo es am meisten mit Menschen übereinstimmt, die eine Hochschulbildung haben. Wenn jedoch Modelle und Datensätze mit bestimmten Bevölkerungsgruppen übereinstimmen, werden einige unvermeidlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit ihren männlichen und weiblichen Gegenstücken. Wir finden dies in der GPT-4-Aufgabe zur sozialen Akzeptabilität sowie in der Dynahate-Aufgabenanalyse. Da es also eine Positionalität im NLP gibt, was können wir dagegen tun? Wir haben dafür einige Empfehlungen. Die erste ist, ein Protokoll aller relevanten Designentscheidungen während des Forschungsprozesses zu führen. Und die andere ist, NLP-Forschung mit der Brille des Perspektivismus zu betreiben. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb bestimmter Gemeinschaften aufzubauen, und ein gutes Beispiel dafür ist die Masakane-Initiative. Wir möchten betonen, dass inklusives NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Und das schließt unsere Präsentation ab, aber wenn Sie mehr erfahren möchten, schauen Sie sich gerne unser Dashboard für die aktuellsten Analyseergebnisse und unseren Artikel an. Vielen Dank."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, ich werde über unsere Arbeit zur Auflösung indirekter Verweise für die Entitätsauswahl sprechen, in der wir den Alt-Entitäts-Korpus einführen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Philip Radinsky, Sylvia Parity und Annie Lewis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen wollen. Betrachten Sie diese alternative Frage: Meinten Sie \"Easy on Me\" oder \"I Got a Feeling\"? Hier möchte ein Benutzer zwischen diesen beiden Liedern wählen. Das Offensichtlichste ist, einen direkten Verweis zu verwenden, zum Beispiel durch Nennung des Liedtitels \"Easy on Me\" oder seiner Position, das erste. Aber manchmal ist ein indirekter Verweis angemessener, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich nicht an den Namen des Liedes erinnern kann oder die Aussprachen zu ähnlich sind und schwer zu unterscheiden. Oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Verweise, zum Beispiel das neuere oder das Lied, das nicht energiegeladen ist. Dies ist ein wichtiges Problem in konversationsbasierten Systemen und auch für die Bewertung der Entitätsverständnis von LLMs. Uns ist kein öffentlicher Datensatz, ein groß angelegter öffentlicher Datensatz für diese Aufgabe bekannt, daher haben wir einen mit Crowd-Annotation gesammelt. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensatz-Sammelmethodik betont die Informalität durch eine Cartoon-Vervollständigungseinrichtung. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: \"Erinnerst du dich an das Lied, das wir gestern gehört haben?\" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: \"Meinst du 'Easy on Me' oder 'I Got a Feeling'?\", was die alternative Frage ist. Und in der dritten Sprechblase verwendet Bob einen indirekten Verweis, um eine dieser Entitäten auszuwählen, zum Beispiel das neuere. Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Aufforderungen pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer eine einfache Vorlage: \"Meinst du A oder B?\", wobei A und B aus Wikipedia entnommen werden. Hier sind die verschiedenen Stichprobenmethoden, die wir verwendet haben. Wenn wir in der Liste nach oben gehen, werden die Entitäten ähnlicher und es ist in der Regel schwieriger, die Unterscheidung zu treffen. Die erste ist gleichmäßig zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Titel \"The Return\". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel dasselbe Genre oder denselben Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie die Namen dieser Entitäten, wissen aber nicht unbedingt etwas über die Entitäten. Was wir also tun, ist, dass wir einige Hintergrundinformationen über die beiden Entitäten zeigen. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten die Annotatoren dann, mindestens einen Teil jedes Liedes zu hören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied \"Easy on Me\". Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wieder aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Verweisen zu beschreiben. Zum Beispiel das mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel das ohne Worte, nicht das mit dem 12-jährigen Jungen oder das fiktive oder das aus Aserbaidschan und so weiter. Der Alt-Entitäts-Korpus hat 6.000 alternative Fragen in drei Domänen und 42.000 indirekte Verweise. Die Ergebnisse mit dem T5X-Large-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugang zu den gleichen Hintergrundinformationen wie die Annotatoren hat, dann ist die Genauigkeit wirklich hoch. Sie liegt bei etwa 92 bis 95 %. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu einigen teilweise überlappenden Hintergrundinformationen hat, liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugang zu Entitätsnamen hat, liegt die Genauigkeit nur bei 60 %, sodass es viel Raum für Verbesserungen gibt. Wir haben auch gezeigt, dass die Modelle domänengeneralisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank."}
