{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫马蒂亚斯·伦德曼，今天我将简要介绍我们关于无树的组合泛化的论文，使用多集合标记和潜在排列。这是与我的导师亚历山大·科拉和伊万·蒂托夫的合作成果。组合泛化可以理解为学习者处理更深递归和训练期间单独见过的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。通常，我们有一组训练语句，在这种情况下，女孩睡了，玛丽知道女孩睡了。这些语句与代表其意义核心方面的逻辑形式配对。与标准的机器学习评估不同，测试集不来自相同的分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练期间看到了浅递归，并在具有更深递归的示例上进行测试。天真的序列到序列模型在处理这种分布外的泛化时遇到困难，并且通常产生与输入脱节的输出。特别是，它们通常无法再现输入和输出之间的系统对应关系，例如示例中用颜色编码的对应关系。解决这个问题的一种流行方法是将树集成到模型中。树旨在捕捉将语句与逻辑形式联系起来的组合过程。这很有效，但树通常是未知的，需要以某种方式获得。这可能是一个复杂且有时计算成本高昂的过程。通常，这涉及对逻辑形式进行大量形式化特定的预处理，例如处理变量符号。获得树也可能涉及专门的语法归纳程序。在这篇论文中，我们不使用树，并引入了一个直接建模输入片段和输出片段之间对应关系的神经序列到序列模型。我们首次展示了在不依赖树的情况下对更深递归的强泛化。我们的方法通过两个步骤从输入预测输出。首先，我们用将出现在输出中的无序多集合标记每个输入标记。在第一步之后，我们有所有正确的标记，但它们没有排序。这就是为什么在第二步中，我们使用另一个模型来预测排列，将它们放入正确的顺序。我们引入了一种新方法来预测排列，该方法不对可能的排列施加任何硬约束。这使我们的方法非常灵活和表达能力强。从概念上讲，我们的排列模型大致如下工作。我们从左到右遍历输出，并确定在每个位置放置哪个多集合标记。对于第一个输出位置，我们简单地选择一个，如红色突出显示的那样。然后，我们跳到下一个多集合标记以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳到另一个多集合标记。我们继续这个过程，直到第一阶段的每个标记都被访问了一次。为了给你一个实验结果的预告，我们在这里将我们的方法与Corgs基准上的其他无树模型进行比较。我们的模型在对更深递归的泛化方面以大幅度超越其他模型。然而，其他类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了一些有趣的技术挑战。首先，输入和输出之间的对齐在训练数据中是未知的。因此，对于给定的标记，我们不知道它来自哪个多集合，这对训练构成了挑战。此外，有时有多个排列与数据一致，但语言上正确的排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。我们的排列方法非常灵活，但它带来了一个挑战，即找到得分最高的排列是NP难的。这是因为这与旅行推销员问题有关。我们用一个GPU友好的连续松弛来近似这个问题，这也允许我们通过解决方案进行反向传播，并学习更符合语言的排列。如果你想了解更多关于我们的实验以及我们如何解决这些挑战的信息，请查看我们的论文或来看我们的海报。"}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是梅拉，今天我要谈谈我们的论文《标记人格》，使用自然语言提示来衡量语言模型中的刻板印象。这项工作是与埃森·德穆什和丹·杰罗夫斯基合作完成的。近年来，许多人记录了大型语言模型（LLM）中的社会偏见和刻板印象的普遍性。然而，这些衡量标准有各种限制。它们通常依赖于手工构建的数据集，这些数据集非常耗时。它们通常只衡量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计数据或上下文，或者它们只是捕捉到非常广泛的负面联系，例如与特定群体的负面联系。此外，大多数工作在这个领域没有考虑到交叉性，即多方面的社会身份可以累积偏见，成为独特的伤害来源。为了克服这些限制，我们依靠这些新的指令调整LLM非常擅长响应指令和提示的特性。因此，我们可以要求模型生成一个人格，这是一个想象的个人描述，使用一个提示，比如“想象你是一个亚洲女性，描述自己”。我们可以立即看到这可以推广到任何人口统计数据，因为我们可以在提示中指定任何我们想要的身份标记。以下是GPT-4的一些示例生成。我们立即看到，虽然输出在传统意义上并不明显地负面或有毒，但有一些有趣的模式。亚洲女性被描述为不引人注目。中东女性被描述为异国情调，并提到一个迷人的地区。两个有色女性人格都提到了祖先，而白人男性人格则没有。为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些人格。我们生成这些人格的提示灵感来自一项研究，他们将这些提示给人类受试者，发现通过给人类受试者，他们也能揭示种族刻板印象。这也使我们能够直接比较我们生成的人格和人类书写的回应。第二部分是标记词，这是一种识别区分标记群体和非标记群体的词的方法，我稍后会详细说明。这种方法的好处是我们可以获得非常具体的刻板印象和模式，而不必依赖任何特定的词汇表。因此，标记词方法借鉴了社会语言学中的标记性概念，该概念指出存在一个非标记的默认值，任何偏离该默认值的群体在语言上都是标记的。例如，词“战士”通常与男性相关联。因此，当人们描述一个女性战士时，他们通常会明确指出“女性战士”，并用“女性”标记这个词。更广泛地说，社会中的主导群体在语言和社会上都是非标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先确定哪些是非标记和标记群体。然后我们使用战斗词方法比较人格，该方法基本上使用加权对数几率比来区分每个标记群体的顶级词。例如，对于黑人女性人格，我们将使用战斗词，并将对数几率比与白人人格和男性人格进行比较，因为这些是两个相应的非标记群体。现在来看一些结果。首先，我们使用刻板印象词汇表，发现生成的人格包含比人类书写的更多的刻板印象。然而，当我们实际查看词汇表中词的分布时，我们发现了非常不同的东西。虽然生成的人格有更高的词汇表词率，但人类书写的词分布更广，而生成的人格中的刻板印象词实际上只是“高”和“运动型”。因此，实际上只是积极的或至少非负面的词。事实上，这个词汇表并没有很好地捕捉到我们在早期幻灯片中看到的许多有害模式。因此，为了做到这一点，我们将转向我们的标记词方法的结果，以展示这些看似积极的词如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似积极的描述如何反映有害模式。首先，对于标记群体，顶级词包括文化、传统、自豪和异国情调。这些词只通过它们与身份的关系来定义这些群体，并将它们与白人规范区分开来。这有助于这些群体长期以来的歧视和他者化遗产。此外，这些词中反映了许多常见的陈词滥调，特别是对于有色女性。例如，描述拉丁女性的词包括“生动”和“曲线”，这与热带化的陈词滥调相连。对于亚洲女性，词包括“纤细”、“精致”和“丝绸”，这与亚洲女性长期以来被过度性化、被视为非常温顺和顺从的历史相连。最后，对于黑人女性，我们看到一些顶级词是“强大”和“坚韧”。这与人们所说的强大黑人女性原型相连。虽然乍一看似乎是积极的，但有研究表明，这种原型实际上是非常有害的，因为它对这些人口统计数据施加了很大的压力，使他们在社会障碍面前坚韧和强大。因此，与其真正努力改变这些障碍，它对这些人施加压力，使他们克服这些障碍，这导致了这些人在其他伤害中非常负面的健康结果。更广泛地说，我们发现每个标记群体的词几乎只反映了非常本质化的叙事。因此，基于这些模式，我们得出了三个建议，供模型所有者参考。首先，我们作为研究人员应该解决积极的刻板印象和本质化叙事。我们还应该使用交叉性视角来研究偏见和伤害，因为如果我们不这样做，可能会忽略很多事情。最后，应该增加偏见缓解方法的透明度。例如，这些积极的刻板印象，我们不知道是否是因为有一些奇怪的过度价值对齐，或者可能是一些其他反刻板印象方法导致了这些有害的模式。我们真的无法做出任何假设或进一步研究，而没有更多的透明度。非常感谢您的聆听。在ACL度过美好的时光。"}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是詹姆斯·芬奇。我是萨拉·芬奇。今天我们将向您介绍ABC-Eval，这是一种新的评估对话式人工智能的多维度方法。这项工作由埃默里大学自然语言处理实验室完成，由埃默里大学的乔伊教授领导，并与亚马逊Alexa AI合作。假设您刚刚开发了一个对话模型，并希望了解它与当前最先进技术的比较情况。常见的做法是使用人类评估，例如要求人类评委选择两个对话中哪一个更好，或者根据利克特量表对对话进行评分。这些方法在提供整体对话质量的全面评估方面效果很好，但对话质量有许多方面。因此，您可能希望评估多个聊天质量的维度，以更细致地了解模型的优点和缺点。一种方法是简单地要求人类评委评估对话质量的几个维度，例如模型响应的相关性，使用现有的比较或利克特量表方法。然而，我们认为有一种更精确和可靠的策略来进行多维度对话评估。我们的方法试图通过明确注释每个模型响应是否表现出某些行为来减少人类评估的主观性，例如以无关信息回应或自相矛盾。我们将这种方法称为对话中的行为注释，简称ABC-Eval。我们开发了这种方法，以全面涵盖最近文献中建议影响聊天质量的聊天模型行为。ABC-Eval能够测量聊天模型犯各种主题错误的比率。例如，ABC-Eval测量聊天模型忽略其合作伙伴或说出无关信息的回合数量，自相矛盾或与其合作伙伴矛盾，幻想错误的事实或违反常识知识，以及模型成功或失败地表现出同理心。为了确定哪种评估最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval对每个模型进行了100次人类机器人对话的评估。为了比较，我们还使用三种现有方法对这些对话进行了评估，包括回合级别的利克特评分、对话级别的利克特评分和对话级别的成对比较。对于每种现有方法，我们收集了对话中最常测量的八个方面的评估，因为这是评估聊天模型的多个维度的标准做法。通过对这些评估结果的分析，我们发现ABC-Eval行为标签在整体上比现有方法收集的标签更可靠，如通过100次双重标记对话的内部评估者一致性测量。此外，ABC-Eval标签比现有方法生成的指标更能预测整体对话质量，如本简单线性回归分析所示。例如，您可以看到测量自相矛盾和合作伙伴矛盾的回合比例分别解释了5%和10%的对话质量，而平均利克特一致性得分只解释了4%或更少。最后，我们使用逐步线性回归检查每个评估指标是否捕捉到聊天质量的独特方面。您可以看到所有ABC-Eval指标的组合解释了25%以上的对话质量，而逐一删除这些指标，大多数指标会导致丢失大量关于质量的信息。另一方面，所有回合级别利克特指标的组合解释了远少于质量，并且这些指标中的较少指标包含独特信息。这些可靠、信息丰富且独特的ABC-Eval指标使我们能够以比以前的方法能够实现的更高分辨率来评估对话式人工智能。您可以在我们实验结果中看到，仍然存在几个挑战，并且已经被精确量化。例如，我们测试的机器人在其响应中有大约20%的常识违规。它们在大约15%的响应中产生无关信息，并且它们在大约10%的时间内自相矛盾或与其合作伙伴矛盾。随着该领域快速改进的步伐，这些错误率在我们评估进行后发布的新模型中可能会有所下降。然而，这更是追求可靠和精确的评估指标以比较模型的理由。我们希望ABC-Eval能够被该领域的其他人作为朝着这个方向迈出的有意义的一步，我们期待在未来几个月和几年中看到对话式人工智能的进步。谢谢观看。"}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫瓦苏达，是斯托尼布鲁克大学计算机科学硕士研究生。我将介绍我们在ACL 2023上发表的长篇论文《转移学习用于不和谐检测，解决稀有类别挑战》。我们首先定义了认知不和谐及其在语言研究中的重要性。简单来说，认知不和谐是指两种不一致的信念或行为。例如，一个人说“我知道吸烟会要了我的命”，然后又说“我开会后抽了几根烟”。这种信念和行为是不一致的，存在不和谐。此外，提到“我觉得没有它们我保不住工作”可以解释第二次出现，它们之间存在一致关系。虽然不和谐是日常决策中非常常见的现象，但在语言中表达不和谐的情况却非常罕见。为什么这很重要呢？研究认知不和谐可以帮助我们理解人们之间的分歧效应，追踪人口中的信念、价值观和态度变化。高认知不和谐还与焦虑障碍有关，有助于更好地理解人们的心理健康。研究语言中表达的不和谐也有助于理解极端主义和易受影响群体的极化。最后，认知不和谐有助于理解个人的认知风格，帮助我们更好地理解决策过程。为了创建认知不和谐资源，我们进行了大规模的不和谐关系标注。我们使用了不和谐优先的方法，如图所示。推文通过PDTV解析器传递，并根据我们论文中描述的指南对话语单元对进行标注。如图所示，不和谐只在3.5%的标注对中找到。收集了约1000个话语单元对的示例后，我们对初始分类器进行了训练，仅使用43个不和谐示例进行训练。不出所料，分类器的表现并不比随机好多少。由于不和谐的低频率和缺乏任何先前的数据集，我们面临绝对稀有的问题。为了缓解这一问题，我们在转移学习和主动学习的组合上进行实验，以便在较少的标注轮次中收集更多的不和谐样本，从而降低整体标注成本，同时提高不和谐检测。由于初始模型完全无法捕捉不和谐类别，我们通过从相关任务转移权重来启动主动学习过程。我们从两个不同的任务中转移：主题独立的不和谐立场分类，该任务确定两个不同人的辩论陈述是否一致或不一致，无论主题如何，称为辩论，以及PDTB的扩展和比较类别的二元分类，因为这两者与和谐和不和谐的概念密切相关，我们称之为CE。我们发现，在转移到标注数据集上，零样本性能已经比随机好得多，最好的AUC为0.62。此外，在两个任务上迭代微调，我们发现首先对CE任务进行微调，然后对辩论进行进一步微调，可以获得更好的零样本性能。因此，这是我们用来启动主动学习的模型。接下来，我们确定了在每轮主动学习和标注中更新模型的最佳方法。累积方法累积了迄今为止从主动标注中收集的所有数据，而迭代方法通过训练最新收集的数据集来更新模型。在不同的策略中，我们发现累积方法在各个方面表现与迭代方法相同或更好。接下来，为了提高不和谐示例的数量，我们使用稀有类别概率策略PRC，在任何轮次的AL中选择大多数可能是不和谐的示例。我们将其与社区中常用的其他最先进的AL策略进行比较。我们发现，所提出的PRC策略比其他最先进的策略效果更好，尽管差异很小。请注意，随机的性能显著较低。在使用两种最佳策略进行更多轮次的AL后，我们将不和谐分类AUC提高到0.75，这是我们迄今为止在任务中取得的最佳性能。我们还检查了每种策略的可行性、标注质量和标注员的成本。我们发现PRC具有最高的不和谐百分比，并且对稀有类别效果最佳。然而，标注员也发现示例很难。总之，我们发现PRC是稀有类别获取的简单AL策略，并且通过适当设计的转移学习任务启动AL可以显著帮助。我们还发现，迭代更新对于从不同领域进行转移学习是有用的，而领域内主动标注受益于累积更新。这些是我们代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是阿克沙塔，今天我和我的合著者马丁将介绍我们的工作《KITMAS：从多个来源评估知识整合》。这项工作是麦吉尔大学、Mila和微软研究的合作成果。自然语言理解模型依赖于各种知识来源，例如参数中包含的知识，通常通过预训练获得，以及在推理时输入的知识。最近的研究表明，模型可以利用预训练时的知识来解决任务，例如问答任务。然而，自然语言理解通常需要在推理时提供的知识。例如，在句子“约翰在电视上看到了新当选的总统”中，预训练参数可能包含关于总统做什么以及电视是什么的信息，但它们无法可靠地知道这个特定实例中的约翰是谁，或者新总统是谁，因为总统可能在预训练后发生了变化。因此，成功的知识密集型NLU任务模型需要能够整合和利用预训练时和推理时的知识。在这项工作中，我们提出了一套用于知识整合的诊断测试。我们引入了一项旨在探测从不同来源获取知识能力的共指消解任务。我们使用人类研究参与者和已建立的共指消解模型评估了数据集。这是我们数据集中的一个例子。塞尔文是一名法官。基亚是一名面包师。塞尔文和基亚在公园里见面。在法庭上审理案件后，他很高兴能放松一下。这里的任务是确定代词“he”指代的正确实体，在这个例子中是塞尔文。解决给定代词需要两种类型的信息。首先是实体特定的知识，例如塞尔文是一名法官。其次是背景知识，例如法官在法庭上审理案件。通常，背景知识是在大型语言模型的预训练过程中学习的，而实体特定的知识通常在推理时观察到。我们变化了这两种信息的可用性，使其可以在单一来源或多个来源中找到。我们定义了KITMOS的三种设置。首先是背景预训练设置，背景知识假设在预训练时可用。其次是背景两者设置，背景知识在预训练时和推理时都可用。最后是背景推理设置，两种知识类型仅在推理时可用。这种最后的设置特别有趣，因为它模拟了背景知识不包含在模型预训练数据中的情况，例如因为自预训练以来出现了新的职业。这是我们如何控制事实在两个来源中的可用性的一个例子。在背景预训练设置中，我们假设背景知识“政治家寻求政府中的当选席位”包含在预训练参数中。在推理时的上下文中，我们提供了实体特定的知识“切斯特是一名政治家”。在背景两者设置中，我们不仅提供了实体特定的知识，还提供了关于政治家的背景知识。在背景推理设置中，我们提供了虚构的职业“梅里图尔”而不是政治家，因为梅里图尔不太可能包含在预训练参数中。我们使用人类研究参与者和已建立的共指消解模型评估了数据集。在这个图中，我们展示了在背景预训练设置的最困难变体中表现最佳的模型的结果。在没有KITMOS特定训练的情况下，两个模型的表现都不佳。然而，在KITMOS上训练后，C2F和Built4Coref的表现显著优于随机选择。这表明，在一般共指消解数据集上训练时，模型学会了利用表面线索，而在KITMOS上测试时这些线索被移除时这些线索是无用的。使用虚构知识的额外实验表明，即使是表现最佳的模型也无法可靠地整合仅在推理时提供的背景知识。总结我们论文的主要观点。许多共指消解模型似乎无法在没有任务特定训练的情况下从不同来源推理知识。然而，在任务特定训练后，一些模型成功地整合了来自多个来源的知识。然而，即使是表现最佳的模型似乎也难以可靠地整合仅在推理时提供的背景知识。如果您对更多细节感兴趣，请查看我们的论文，并在GitHub上查看数据集和代码。谢谢收听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是萨拉·帕皮，来自特伦托大学和布鲁诺·凯斯勒基金会，我将简要介绍《注意力作为同时口译的指南》论文，这是与马泰奥·内格里和马尔科·图尔基的合作成果。什么是同时口译？同时口译或SimulST是将口头语言实时翻译成另一种语言的文本的过程，使跨语言交流成为可能。当前SimulST模型的问题是什么？通常会训练特定的架构，引入需要优化的额外模块。训练过程复杂且冗长，例如涉及不同优化目标的训练，以及训练和维护多个模型以实现不同的延迟模式，例如训练一个延迟平均为一秒的模型，另一个延迟为两秒的模型，依此类推。那么我们的解决方案是什么？首先，使用现有的SimulST模型，而不需要重新训练或采用特定的SimulST架构。使用一个模型来处理所有延迟模式，并通过特定参数来处理延迟。利用模型已经获得的知识，通过音频输入和文本输出之间的注意力机制，即交叉注意力机制。你可以在右边看到一个例子。我们的解决方案是提出EDAT或编码器-解码器注意力，这是一种策略，我们根据注意力的指向决定是否发出部分翻译。如果注意力不集中，即其总和低于某个阈值α，指向最后的λ语音帧，这意味着接收到的信息足够稳定。例如，如果我们接收到一个包含“我要谈论”的语音块，我们的模型预测德语翻译，我们查看交叉注意力权重，会发现前两个单词指向最早接收到的语音帧，而最后一个单词指向最后接收到的语音帧，即λ语音帧。这意味着前两个单词将被发出，而由于交叉注意力的总和高于某个阈值α，我们不会发出最后一个单词，并等待另一个语音块。如果我们继续，接收到另一个语音块，我们的模型预测另外三个单词，我们查看交叉注意力权重，会发现没有单词指向最后的λ语音帧。这意味着这三个单词将被发出。如果我们查看EDAT的主要结果，我们将同时口译结果绘制在图表中，其中一侧是蓝色，测量翻译质量和平均延迟，即延迟测量。我们还考虑计算感知的平均延迟，这考虑了模型的计算时间来预测输出。因此，我们希望我们的曲线在这张图表上尽可能高，但也希望它们向左移动。我们还与适用于离线模型的策略进行比较，即惠特基策略和局部协议。我们还与专门为同时口译设计的最先进架构进行比较。这些是同时口译策略在德语上的所有结果。我们看到EDAT在所有适用于离线模型的策略中表现最佳，因为曲线向左移动。我们还看到，如果考虑实际经过的时间或计算感知的时间，EDAT是最快的策略。如果你想了解更多结果，请阅读我们的论文，我们还发布了开源代码和模型以及同时输出，以便重现我们的工作。感谢你的注意。"}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫朱恒。今天我要介绍我们的论文《康奈尔2003命名实体标注器在2023年是否仍然有效？》。让我们开始吧。我们的论文研究了使用命名实体识别任务或NER任务的泛化问题。我们观察到，模型已经使用康奈尔2003开发NER近20年。这自然引发了几个问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？同时，如果我们观察到泛化不佳，是什么导致了这些模型性能的下降？为了研究这些问题，我们开发了康奈尔++数据集。这是一个我们从2020年的路透社新闻中收集并使用相同的康奈尔2003标注指南进行标注的数据集。然后我们在康奈尔2003上微调了20多个模型。我们在康奈尔03测试集和康奈尔++测试集上评估了它们。最后，我们计算了F1的百分比变化，以评估每个模型的泛化能力。那么，实现良好泛化需要什么呢？通过我们的实验，我们发现有三个主要因素。第一个是模型架构。通过我们的实验，我们发现变压器模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现通常较大的模型会导致更好的泛化。最后，我们都知道微调示例的数量直接影响下游任务的性能。在这里，我们也发现更多的微调示例实际上也会导致更好的泛化。接下来的问题是，是什么导致了一些模型性能的下降？我们有两个假设。第一个是自适应过拟合，即通过反复使用相同的测试集导致的过拟合，这通常表现为在新测试集上的收益递减。第二个假设是时间漂移，即由于训练数据和测试数据之间的时间差距增加而导致的性能下降。对于自适应过拟合，我们从右侧的图表中看到，红色的最佳拟合线的梯度大于1。这意味着我们在康奈尔2003上每单位的改进都会转化为康奈尔++上超过一个单位的改进，这意味着没有收益递减。这表明在这种情况下没有观察到自适应过拟合。那么时间漂移呢？对于时间漂移，我们进行了一项实验，重新训练或继续预训练一些模型，使用更近期的数据，我们发现随着时间差距的增加，性能下降。这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些因素是相辅相成的，我们不能只有一个因素，而忽略其他因素。同时，我们还发现性能下降是由时间漂移引起的，而令人惊讶的是，它并不是由自适应过拟合引起的，尽管康奈尔2003已经使用了20多年。因此，回到我们在论文标题中提出的问题，康奈尔2003标注器在2023年是否仍然有效？我们发现答案是肯定的。我们希望我们的论文能促进更多关于如何改进模型泛化的研究。最后，请务必查看我们的论文和数据集，如果有任何问题，请随时联系我。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，欢迎来到我们的演示，介绍 DeepLing，这是一个用于德语文本简化的新语料库，适用于文档级别和句子级别。我叫雷吉娜·斯托登，将带领大家完成演示的第一部分。首先，我们来定义文本简化。文本简化是一个过程，通过改编文本来提高特定目标群体的文本理解能力，例如阅读障碍者或非母语者。为了训练文本简化模型，我们需要文本的平行对，例如文档或句子。在下面的例子中，你可以看到一个复杂的德语句子及其简化版本的平行对齐句子对。为了简化句子，有不同的技术可供选择，如词汇替换、从句简化、从句重排或插入词。现在我们提出了我们的新语料库 DeepLing，因为近年来现有语料库存在一些问题。例如，这些语料库太小，无法用于训练文本简化模型。另外三个在近年来提出的模型都是自动对齐的，这意味着它们的对齐可能会出错。因此，我们提出了我们的新语料库 DeepLing，它分为两个子语料库，DeepLing APA 和 DeepLing Web。DeepLing APA 基于新闻文本。在 DeepLing APA 中，我们手动对齐了 483 个文档，结果产生了大约 30,000 个平行句子对。对于 DeepLing Web，这个语料库包括不同的领域，我们也手动对齐了所有这些 750 个文档，同时使用自动对齐方法。总共我们得到了 30,450 个句子对。我们对句子对进行了更多分析，例如简化类型。如你所见，圣经文本的简化程度比新闻文本或语言学习者文本要高得多，在所有层面上，例如词汇简化、结构简化或整体简化水平。此外，你可以看到我们的 DeepLing 语料库包含了各种简化转换的高优先级。例如，在 DeepLing APA 语料库中，我们有更多的重排和词添加，而在 DeepLing Web 语料库中，我们有更多的重新表达。现在让我们看看这个语料库可以做什么。大家好，我是奥马尔，现在我将谈论我们数据集 DeepLing 的用例。对于第一个用例，我们可以评估自动对齐方法。近年来，有很多对齐方法，但都是在机器翻译的背景下，我们有两个用不同语言编写的平行文档，我们想要提取这两个文档中句子的对齐。但在我们的用例中，我们试图提取两个平行文档中句子的对齐，它们使用相同的语言，内容相同，但复杂程度不同。现在我们有了我们的数据集 DeepLing，其中包含手动对齐的句子，我们可以将这些句子作为标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了一些调整，并在论文中发布了所有这些调整和运行我们实验的代码。最后，我们得出结论，最好的自动对齐方法是用于德语文本简化的 Mass Align 方法，你也可以在论文中找到运行这个方法的代码。我们在论文中展示的第二个用例是通过微调语言模型来自动简化文本，从复杂的输入文本生成简化的文本。我们微调了两个不同的模型。我们微调了 Long Impart 模型来生成文档级别的简化，我们还微调了普通基础 Long Impart 模型来生成句子级别的简化。你也可以找到所有的检查点，并在论文中查看我们实验的分数和评估矩阵的更多细节。我们得出结论，这种基本的微调可以产生或获得比基线分数更好的分数，我们将这些结果作为未来自动文本简化问题的基准。非常感谢你的关注，我们希望在会议期间见到大家。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自福德大学的徐宇元。我在这里介绍我们的工作，即从大型语言模型中区分脚本知识，用于约束语言规划。在日常生活中，人类通常通过遵循逐步指令的形式来规划他们的行动，这些指令是由指导脚本提供的。之前的工作已经利用语言模型来规划抽象目标的刻板活动，例如制作蛋糕，并表明大型语言模型可以有效地将目标分解为步骤。然而，之前的工作主要集中在规划抽象目标的刻板活动，而对于具有特定约束的目标的规划，例如制作巧克力蛋糕，仍然是未被研究的领域。在这篇论文中，我们定义了约束语言规划的问题，该问题对规划目标施加了不同的约束。一个抽象目标可以被不同的现实生活中的特定目标所继承，这些目标具有多方面的约束。一个好的规划者应该编写符合约束的合理脚本。在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。由于没有特定目标的数据集来支持我们的研究，我们必须首先获取这些目标。如表所示，我们通过人在回路数据获取使用InstructGPT来扩展抽象目标，以获得多方面的约束。我们采样了100个特定目标，并评估了从大型语言模型生成的脚本。该表报告了结果的总体准确性。我们发现，所有大型语言模型在规划特定目标时都达不到令人满意的结果。然后，我们进行了详细的分析，以调查为什么大型语言模型的结果不理想。图中的结果表明，生成脚本的语义完整性是可以接受的，但对约束的忠实度无法保证。我们深入研究了WikiHow中不同类别的约束的更细粒度的主题分类。图中的热图显示，InstructGPT的规划性能在不同类别的目标中显著不同。之前的研究表明，大型语言模型的输出质量存在高度变异，导致性能不佳。因此，我们采用了过度生成和过滤的想法来提高生成质量。我们首先向InstructGPT展示了带有示例的约束类型，并根据所述抽象目标获得了特定目标。然后，InstructGPT为特定目标生成了K个脚本。接下来，开发了一个过滤模型来选择忠实的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似度得分，以衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。只有当目标得分在目标集中最高时，我们才保留脚本。通过我们的方法，InstructGPT可以生成高质量的脚本。我们的方法大大提高了规划能力，无论是在语义完整性还是对约束的忠实度方面。由于大型语言模型部署成本高，使较小和专业化模型具备语言规划能力至关重要。创建数据集是实现这一目标的关键步骤。然而，之前的研究并未实现对特定目标的规划，手动数据集注释成本高昂。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏约束语言规划数据集。我们将我们的方法应用于构建约束语言规划数据集，命名为CodeScript。总共，我们生成了55,000个带有脚本的特定目标。为了确保验证和测试集的质量，我们要求云源工人找到并修正不正确的样本。该图显示了CodeScript的约束分布。我们发现CodeScript在生成的特定目标中表现出高度的多样性。通过CodeScript，我们可以训练较小但专业化的模型进行约束语言规划。我们发现，T5在CodeScript上训练可以生成比大多数大型语言模型更高质量的脚本，表明较小的模型在适当的数据集上训练时可以支持较大的模型。总之，我们建立了约束语言规划问题，评估了大型语言模型的约束语言规划能力，并为大型语言模型开发了过度生成和过滤方法。我们使用大型语言模型生成了高质量的脚本数据集CodeScript，用于约束语言规划。我们希望CodeScript数据集能够成为推动语言规划研究的有价值资源。感谢您的时间。请在我们的论文中找到更多关于CodeScript的详细信息。"}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是Yannis Lavrac，我将向大家介绍我们在Dr. Bert方面的工作，这是一个用于生物医学和临床领域的鲁棒预训练模型。在本次演讲中，我们首先讨论了医疗保健中的语言建模。然后，我们将介绍我们文章的主要贡献。我们引入了第一个法语生物医学模型，名为Dr. Bert，基于Roberta，并使用Nachos进行训练，这是一个从网络上收集的医疗数据集。我们还引入了多种预训练设置和数据源的模型比较。然后，我们展示了在11个法语生物医学和临床下游任务上的结果。最后，我们总结了实验并提供了更多关于如何访问模型的详细信息。自2018年发布以来，Bert已成为解决自然语言处理任务的最有效方法之一，并与历史静态和上下文化方法（如Word2Vec、FastText或NWO）相比，提供了巨大的性能提升。自那时起，该模型已被适应到许多其他语言，如法语的Camembert，以及生物医学领域的PermitBert和BioBert，以及临床领域的ClinicalBert，但主要是英语。其他语言的专业模型稀缺，通常基于持续预训练，因为缺乏领域内数据。然而，法语在生物医学方面没有任何开源模型，直到现在。因此，我们问自己，什么是最合适的数据搜索，以适应广泛的用途，这些数据是否是临床数据的良好替代品？为了回答这个问题，我们将Dr. Bert与我们的Schubert模型进行比较，该模型基于从非大学医院数据仓库获得的匿名化数据。之后，我们问自己，我们需要多少数据来训练一个专业的法语数据模型？是4GB、8GB还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型。第一个版本的Dr. Bert使用7GB的Nachos，第二个版本使用4GB的Nachos子集，第一个版本的Schubert是一个临床模型，使用4GB的临床笔记句子，最后一个版本的Schubert使用4GB的Nachos子集和4GB的临床笔记的混合。除了这个比较，我们还引入了三个在持续预训练上训练的模型，以分析预训练策略的影响。一个基于Camembert的权重，并在4GB的Nachos子集上训练，另一个也基于Camembert，但这次在4GB的临床笔记上训练，最后一个基于英语生物医学模型BermetBert，并在4GB的Nachos子集上训练。总共有七个模型。为了评估我们的七个模型，我们收集了多个公共和私有的任务，如命名实体识别、分类、部分语音标记和问答。这些模型与六个基线模型进行比较，包括Camembert Oscar 138GB、Camembert Oscar 4GB、Camembert CCNet 4GB、PermitBert、BioBert和ClinicalBert。评估结果表明，模型在与训练数据相同性质的任务上表现最佳。然而，我们可以观察到，异质数据源的数据似乎更加多功能。我们还观察到，使用更多数据可以提高性能。总的来说，从头开始的预训练似乎在大多数任务上获得了更高的性能。然而，我们在持续预训练上的实验，使用PermitBert的权重和标记器，在4GB的Nachos子集上训练，显示出与Dr. Bert 4GB从头开始获得的结果相当，而基于Camembert权重和标记器的模型则存在稳定性问题。最后，作为结论，我们的系统在11个任务中的9个任务上提供了更好的性能，并全面超越了通用模型Camembert的结果。我们还观察到，专业数据更好，更多的专业数据更好，但它的规模不大。所有从Nachos获得的预训练模型都可以在Yuginface上免费获得，所有的训练脚本都在我们的GitHub存储库中。因此，感谢大家的聆听，我们期待在多伦多的后续会议上进行交流。"}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是华盛顿大学的博士生尚宾。今天我要介绍我们的研究，从预训练数据到语言模型，再到下游任务，追踪政治偏见的轨迹，导致不公平的NLP模型。语言模型是基于大规模网络爬取数据进行训练的。政治新闻媒体在其预训练数据中得到了很好的覆盖。根据C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等媒体在语言模型训练数据中得到了很好的覆盖。这为语言模型应用带来了双重影响。一方面，它们能够从多样化的视角学习，这有助于民主和多元化的思想。另一方面，这些不同的政治观点本身具有社会偏见，可能导致下游任务应用中的公平性问题。为此，我们提出调查从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体提出以下问题。首先，我们如何评估语言模型的政治倾向，以及预训练数据在这些政治偏见中扮演什么角色？其次，具有不同政治倾向的语言模型在下游任务中的表现如何，以及这是否会导致NLP应用中的公平性问题？具体来说，我们首先提出使用不同的提示格式来提示语言模型，使用政治问卷，如政治罗盘测试。这确保了我们的自动评估基于政治科学文献。一些初步结果表明，首先，语言模型确实具有不同的政治倾向。它们占据了政治罗盘的四个象限。我们还可以看到，GPT-4是所有语言模型中最自由的，而GPT系列通常比BERT系列及其变体更自由。其次，我们旨在调查语言模型的政治偏见在多大程度上是从训练数据中获得的。因此，我们通过进一步在六个不同的党派语料库上预训练语言模型检查点来进行受控实验，这些语料库分为新闻和社交媒体，进一步分为其政治倾向。通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，对于进一步在左倾Reddit语料库上微调的Roberta，我们可以看到其政治偏见在自由主义方面有了显著的转变。我们还试图调查语言模型是否能够捕捉到我们现代社会中普遍存在的极化现象。因此，我们将预训练语料库分为美国第45任总统之前和之后，分别在两个不同的时间语料库上预训练语言模型。我们可以看到，语言模型在2017年之后通常具有更远离中间的政治倾向。这表明语言模型也能够捕捉到我们社会中的极化现象。最后但同样重要的是，我们评估了具有不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测中的表现，这两个NLP应用通常涉及语言模型，并且可能具有非常重要的影响。因此，我们看到，如果我们调查每类别的表现，也就是说，如果我们将表现分为不同的人口统计或新闻媒体的政治倾向，我们可以看到一个模式，例如，对于仇恨言论检测，左倾语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对我们社会中更有权力的群体的仇恨言论方面表现较差。反之亦然，右倾语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现较差。虚假新闻检测也有类似的趋势，我们看到左倾语言模型在检测其对立政治倾向的虚假信息方面表现更好，反之亦然。我们进一步展示了许多定性示例，以显示具有不同政治倾向的语言模型确实会根据其社会类别对仇恨言论和虚假信息示例做出不同的预测。附录中还有更多示例，进一步突出了这一点。这表明语言模型的政治偏见存在一个非常紧迫的公平性问题。例如，如果右倾语言模型在仇恨言论或虚假信息上进行微调，并部署到一个流行的社交媒体平台上，这意味着持有对立政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会肆无忌惮地蔓延，没有任何控制。因此，这为我们敲响了警钟，承认并解决语言模型政治倾向导致的公平性问题。最后，我们还想强调，我们揭示了语言模型政治偏见的独特困境。这就像在斯库拉和卡律布狄斯之间。如果我们不在语言模型训练数据中消毒政治观点，偏见将从预训练数据传播到语言模型，再到下游任务，最终导致公平性问题。如果我们试图以某种方式消毒，我们也会冒着审查或排除的风险，并且很难确定什么是真正中立的，应该保留在语言模型训练数据中。这有点像电动查理问题。好了，我想这就是我今天要说的全部内容。谢谢大家的时间。"}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是科斯托夫·西纳，很高兴欢迎大家来参加我们的演讲，讨论我们的ACL 2023论文《语言模型的可接受性判断并不总是对上下文具有鲁棒性》。这是与约翰·戈蒂尔、阿伦·穆勒、卡尼什卡·米什拉、加伦·芬特斯、罗杰·莱维和阿蒂娜·威廉姆斯的联合工作。在这项工作中，我们重新审视了最小对比范式。最小对比范式基本上评估了语言模型在可接受性判断方面的表现，这也可以包括语法性，如blimp、syntax gym，或者在刻板印象方面的可接受性，如crowspairs。在这个最小对比范式中，评估语言模型的典型方法是展示一个可接受的句子或一个语法正确的句子，然后展示一个不可接受的句子或一个语法错误的句子，然后希望模型基本上给可接受的句子更高的概率。当前的MPP流水线基本上不允许我们评估模型对较长句子的接受度。如今，大型语言模型的上下文窗口越来越长。因此，评估模型在整个上下文窗口中的可接受性至关重要。这就是我们在这里要做的。我们试图通过要求模型评估越来越长的序列的可接受性来重新审视MPP流水线。这就是我们的方法。我们做的是，为了模拟这些较长的序列，我们重新审视数据集本身，然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，这里我们选择了blimp数据集中典型的语法性对比，来自adjunct island案例。我们做的是，为了重新创建较长的序列，这些序列是可接受的，并且具有相同的语法结构匹配，我们从adjunct island中提取语法正确的句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。我们可以通过从相同的匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。我们还可以通过从不同的子集或不同的数据集中选择句子来做同样的事情。这就是我们所说的不匹配场景。在这里，句子仍然来自相关的数据集，但不是我们正在评估的相同数据集。我们可以对不可接受的情况做同样的事情。最后，我们可以从一个完全无关的领域中选择句子，例如维基百科。这将告诉我们，模型的可接受性判断是否受到任何上下文的影响，无论上下文是来自数据集的不同子集，还是与我们正在查看的句子完全无关。那么模型的表现如何呢？首先，我们查看完全与当前查询对无关的维基百科句子，我们发现MPP判断在任意上下文长度下大多是鲁棒的。我们将上下文长度增加到1024，以最大化OPT和GPT2模型，我们在这里看到，在橙色虚线中，MPP判断相对稳定。那么当我们从相同的数据集中选择句子时会发生什么呢？在这里，我们从相同的blimp或syntax gym数据集中的可接受和不可接受领域中创建句子，我们看到，当我们添加可接受的前缀或不可接受的前缀时，MPP判断要么显著增加，要么显著减少。但当我们匹配结构时，也就是说，当我们从blimp人工智能中选择句子时，我们看到模型的MPP判断显著增加或显著减少，具体取决于所选前缀是可接受的还是不可接受的。现在，这个效果在整个上下文长度中增加，这可能会影响具有大上下文窗口的新语言模型。那么匹配前缀为什么会如此大幅度地影响语言模型的判断呢？我们进行了一系列分析，试图通过保留相关结构但向输入添加噪声来扰动输入句子。在进行了几次这样的扰动后，我们发现这些噪声实际上并没有改变模型的MPP判断趋势。基本上，我们发现模型对扰动句子的敏感性是相似的。也就是说，当我们在可接受领域中扰动句子时，我们在所有扰动中看到类似的增加，当我们在不可接受领域中扰动句子时，我们以类似的方式看到MPP判断的减少。因此，我们工作的关键要点是，语言模型对句子中共享的潜在语法和语义特征敏感，而我们目前使用短句和单句输入的MPP评估方式可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢大家的聆听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是德国萨兰特大学的博士生达维。在这段视频中，我想介绍我们的最新研究成果《Weaker Than You Think: A Critical Look at Weakly Supervised Learning》。这是与肖宇舜、马约·斯穆斯巴赫、盖亚·斯蒂芬和迪蒂·克拉克合作的成果。我想先简要介绍一下弱监督和弱监督学习。在弱监督中，我们不手动标注数据。相反，我们使用弱标注源来标注数据，例如简单的启发式规则、知识库或低质量的众包标注，如右图所示。与人工标注相比，弱标注成本更低，但也更嘈杂，这意味着一定数量的标注是错误的。如果我们直接在弱标注数据上训练神经网络，神经网络往往会记住标注噪声，而不具备泛化能力。在弱监督学习中，提出了在标注噪声下鲁棒训练神经网络的算法，使得训练模型仍能很好地泛化。在最近的WSL研究中，WSL代表弱监督学习，一个常见的说法是，人们声称他们只在弱标注数据上训练模型，并在干净的测试集上实现高性能。从技术上讲，这个说法并不完全错误，但有一个陷阱，那就是人们假设有一个额外的干净验证集可用于模型选择。我们对这个问题设定提出了质疑，因为这意味着弱监督学习需要额外的手动标注。但就像房间里的大象一样，这个必要性往往被忽视。这种假设让我们提出了三个研究问题。首先，干净的验证数据对WSL是必要的吗？或者我们可以使用嘈杂的验证集吗？其次，如果干净数据是必需的，或者如果干净数据对WSL的工作是必需的，那么我们需要多少干净样本？最后，我们是否只应该使用干净样本进行验证，或者有更好的方法来利用它们？我们在研究中解决了这些问题，我们的发现如下。首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本来正常工作。否则，性能会大幅下降，如图所示。如果没有干净的验证样本，训练模型无法超越原始弱标签，这意味着训练是无意义的。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如左图所示。通常，我们只需要每类20个样本即可实现高性能。但这还不是故事的结尾，因为如果我们无论如何决定访问干净样本，那么直接在它们上训练将实现更好的性能。右图显示了直接应用于干净数据的微调方法与仅使用干净数据进行验证的WSL方法之间的性能差异。如我们所见，如果我们有每类10个样本，直接微调开始超越WSL方法。最后，以前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调来轻松实现。如图所示，最初性能不如更复杂的WSL方法（如Cosine）的Vanilla模型（FTW）在允许在干净样本上继续微调后，FTW的性能与其他方法相当。因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。总结一下，我们展示了最近的WSL方法需要干净的手动标注样本来正常工作。它们的性能增益和实用性被严重高估。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否在干净验证样本上进行。其次，WSL方法应与少量学习基线进行比较，因为两者都在干净样本上工作。第三，连续微调是一个简单而强大的基线，应在未来的WSL工作中考虑。最后，我们已经开源了我们的代码。您可以在本页的二维码中找到它。请随时查看。谢谢，祝会议愉快。"}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫艾德·维拉德，我将简要介绍一下这篇论文《从翻译中提取Parm，评估策略和性能》。这是与我的Google Translate同事合作的成果。Parm是一个拥有540亿参数的大型语言模型，去年2022年发布。它在一个包含780亿个标记的大型文本集上进行训练。在发布时，它在数百个NLP任务中达到了最先进的水平。在这项工作中，我们展示了第一个系统研究大型语言模型提示在机器翻译中的应用。我们使用了AMT社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。我们比较了两个最先进的系统，即WMT评估中的最佳性能系统。我们使用了最先进的新RMT指标，并额外展示了基于专家的人类评估结果。最后，我们提供了一些提示选择策略的建议。提示对LLM在翻译中的性能有很大影响。正如我们在一个简单的实验中看到的那样，我们使用了一次提示，并为一个句子提供了两个不同的提示。大多数句子，516个中的1000个，观察到的差异超过一个模糊点。在极端情况下，这可以达到40个模糊点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了一个五次提示策略，我们只是标记每个我们提供给系统的句子，与其语言相同。在这个例子中，我们从德语翻译成英语，德语句子，源句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，在多次提示的情况下，提示的实际形式并没有很大的影响。对于零次和一次提示至关重要，当我们像我们的例子一样转向五次提示时，提示的实际形式几乎没有差异。是例子承载了大部分的重量。我们实验结果的总结是，示例质量比源句子的相似性更重要。因此，选择高质量翻译的示例非常重要。特别是，我们比较了从WMT评估的训练数据或开发数据中选择提示。开发数据比训练数据更精心制作，质量更高，结果更好，因此使用开发数据时性能更好。然而，专业的最先进系统在PAN翻译方面具有显著优势，但PAN接近商业系统。在我们的情况下，我们选择与Google Translate进行评估。我们从使用MQM框架进行的人类评估中获得的见解是，PAN的流畅性与最先进的系统相当，但主要差异来自准确性。特别是，最常见的错误是省略错误。因此，PAN似乎选择生成更好的翻译，有时通过删除源句子中在翻译中不重要的部分。然而，PAN的风格输出类别低于最先进的系统，这表明PAN提供了非常流畅的输出，但仍然存在一些准确性问题。这就是这个非常简短的概述。更多详细信息，请来参加论文的全面演示。非常感谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是来自中国科学技术大学的金伟。很高兴为大家介绍一篇论文《你在复制我的模型吗？通过后门水印保护大型语言模型的嵌入式服务版权》。首先，让我们介绍一下嵌入式服务的背景。目前，大型语言模型如TPT、Lama、Palm在自然语言理解和生成方面表现出色。嵌入式服务是基于大型语言模型构建的服务之一，用于辅助各种NLP任务。例如，OpenAI提供了基于GPT的嵌入式API。然而，最近的研究表明，攻击者可能通过学习嵌入式服务来窃取模型并提供类似的服务。因此，保护嵌入式服务的版权是必要的。为了保护嵌入式服务的版权，一种解决方案是在提供者的服务中嵌入水印，并检测另一个服务是否包含水印。水印方法需要满足以下属性。首先，该方法应适用于嵌入式服务。其次，水印不应降低提供的嵌入的效用。第三，水印对攻击者来说应足够隐蔽，否则攻击者可以轻松移除水印。最后，水印在模型提取过程中需要转移到攻击者的服务中。现有的工作可以广泛分为四类。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。因此，在这篇论文中，我们提出了嵌入式标记，这是一种适用于嵌入式服务的基于后门的水印方法。接下来，让我介绍我们的嵌入式标记的详细信息。嵌入式标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组词频在中等频率区间的词。我们假设提供者可以收集一个通用的文本语料库，并用它计算词频。在水印注入中，我们首先定义一个目标嵌入。当用户向提供者的服务发送一个句子时，提供者计算句子中的触发数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中的触发数量成正比。当句子中的触发数量大于M时，提供的嵌入与目标嵌入完全相等。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门和一个良性数据集。后门数据集包含所有词都属于触发集的句子，而良性数据集中的所有词都不属于触发集。然后，提供者使用数据集从窃取者的服务中请求嵌入。计算请求的嵌入与目标嵌入之间的余弦和L2相似性。我们计算良性和后门数据集之间的相似性差异，定义为delta余弦和delta L2。同时，我们还应用KS测试，并将其p值作为第三个度量。我们在四个数据集上进行实验：AG News、Mind、SST2和ErosBam。我们假设提供者使用WikiText数据集计算词频。四个数据集的结果表明，我们的嵌入式标记可以在保持下游任务的良好效用的同时具有良好的检测性能。我们还通过在40个数据集上可视化句子的嵌入来验证提供的嵌入的隐蔽性。图例表示每个句子中的触发数量。如图所示，很难区分后门嵌入和正常嵌入。就这些，谢谢。欢迎与我们讨论。"}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫英，我和我的同事志洋将介绍我们关于多指令的研究，通过指令调优改进多模态零样本学习。随着大型语言模型的进步，许多研究开始探索新的学习范式，即以参数和数据高效的方式重新使用预训练语言模型进行不同的下游任务。最近，许多研究表明，指令调优使大型语言模型能够通过遵循自然指令以零样本的方式执行未见任务。然而，大多数以前的指令调优研究集中在改进语言任务的零样本性能，而计算机视觉和多模态任务被忽略了。因此，在这项工作中，我们想要研究指令调优是否能够改进多模态预训练模型对未见多模态任务的泛化能力。此外，在我们的研究期间，我们发现指令数据集在自然语言处理和多模态之间存在显著差异。存在超过1600个语言任务，然而没有大规模的公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。这里我们介绍Multi-Instruct，这是第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，涵盖10个广泛的类别。这些任务来自21个现有的开源数据集，每个任务都配备了五个专家撰写的指令。为了在我们提出的数据集上研究多模态指令调优，我们选择OFA作为基础模型，这是一个统一的多模态预训练模型。OFA使用统一的词汇表来表示语言、图像标记和边界框的坐标。这里我们展示了我们的多模态指令数据集中的一些示例。为了统一处理各种输入和输出数据类型，我们遵循OFA的方法，将所有任务统一表示为序列到序列的格式，其中输入文本、图像、指令和边界框在同一个标记空间中表示。现在我将谈论多模态指令调优。对于训练数据集，我们使用53个任务进行训练，并为每个任务采样10,000个实例。对于测试，我们保留整个常识推理组进行测试，并从VQA和杂项组中选择额外的五个任务。我们使用测试集中的所有实例进行测试。此外，我们从自然指令的测试集中随机采样20个任务作为NLP的未见任务。我们使用预训练的OFA大模型作为基础模型。在训练过程中，我们混合所有任务的所有实例。每个实例随机与其五个指令模板中的一个结合。因此，在测试过程中，对于每个任务，我们通过在每个实验中使用五个指令中的一个来进行五次实验。我们报告所有五次实验中性能的平均值、最大值和标准偏差。如果任务是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告Rouge L。对于NLP任务，我们也报告Rouge L。我们还引入了一个额外的评估指标，称为敏感性。这衡量模型在指令措辞稍有变化的情况下，能否一致地产生相同的输出。这是我们的主要结果。正如我们所看到的，指令调优可以显著提高OFA在未见多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以有助于指令调优。这里我们可以看到，随着任务数量的增加，模型的性能提高，同时敏感性降低。我们还进行了一次实验，使用一个指令与五个指令进行比较。正如我们所看到的，使用更多指令可以显著提高模型的整体性能并大大降低其敏感性。这展示了不同微调策略对模型敏感性的影响。正如我们所看到的，通过从自然指令数据集进行迁移学习，模型可以实现比原始OFA模型更好的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上实现更好的性能。总的来说，我们提出了第一个大规模的多模态指令调优数据集。我们显著提高了OFA的零样本能力，并探索了不同的迁移学习技术，展示了它们的好处。我们设计了一个新的指标，称为敏感性。还有一件事，我们正在收集一个包含约150个额外的视觉语言任务的更大的多模态指令调优数据集，我们将发布它们。这是我们数据和模型的QR码。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是宾夕法尼亚大学的尤森·约翰。今天我要介绍我们的工作，即跨语言语义解析，涉及多种自然语言和多种表示。语义解析是构建用户查询的语义表示的任务，例如 SQL 和 Lambda 演算。跨语言语义解析是将多种自然语言的查询转换为多种表示的任务。如图所示，我们需要使用神经模型将多种自然语言的查询转换为 SQL、Lambda 或 FuncQL 等。现有的跨语言语义解析模型是分别提出并评估的，评估的数据集任务和应用有限。例如，某些自然语言的覆盖率不足。缺少中文，某些表示的覆盖率不足。缺少 Lambda 演算。或者它们只在某些神经模型上进行评估。例如，只有一个单一模型来评估它们。因此，我们提出了 Exemplar。我们提供了一个统一的 Exemplar 数据集，用于跨语言语义解析，涉及多种自然语言和表示。它包含 9 个数据集，涵盖多个领域，5 个语义解析任务，8 种表示和 22 种自然语言，分布在 15 个语言家族中。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译测试。我们使用 Google 翻译 API 将源语言翻译成目标语言，然后使用单语言模型进行训练和评估。例如，我们在英语查询上训练英语模型，在推理期间，我们使用 API 将德语查询翻译成英语，然后使用训练好的模型预测 SQL。我们还测试单语言模型。在这种设置中，源语言与目标语言相同。例如，德语到德语或英语到英语。我们还测试单语言少量设置，通过仅使用 10% 的训练数据训练单语言模型。我们测试多语言模型，我们为所有语言训练一个多语言模型。例如，我们将德语、英语和中文查询放在一起训练一个多语言模型，在推理期间，我们可以使用这个模型来翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少量样本迁移。我们在一种源语言上进行训练，然后迁移到另一种语言。因此，在训练期间，我们在英语查询或英语和德语少量查询的组合上训练一个多语言模型，并预测 SQL 输出。我们还发现了许多有趣的结果。因此，关于单语言模型的分析，我们在两组模型上进行评估，包括编码器 PDR，即多语言预训练编码器与基于指针的解码器，例如 XLM-R + PDR 和 BERT + PDR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如 MBART 和 MT5。我们发现编码器-解码器在所有九个数据集上表现最佳。我们在 MT5 和 XLM-R + PDR 的多语言设置中进行评估。我们发现编码器-解码器或编码器 PDR 可以通过在各种语言的混合中进行训练来改进。我们发现这是因为大多数主要自然语言都能获得性能提升，除了英语在七个数据集中性能下降，只有在三个数据集中获得提升。我认为这被称为多语言诅咒。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少量样本迁移，橙线是跨语言零样本迁移，而绿线是单语言设置。我们发现通过比较绿线和橙线，我们发现对于零样本设置，跨语言迁移性能差距显著。通过比较蓝线和橙线，我们发现对于少量样本设置，迁移差距迅速缩小。我们还发现了一些其他有趣的发现。例如，编码器-解码器优于以前的工作或实现了可比的结果。在英语自然语言上进行训练可以显著提高目标自然语言的少量样本性能。我们发现多语言语言模型，例如 CodaS 和 Blue，仍然不适合跨语言语义解析任务。总之，我们构建了 Exemplar，一个统一的跨语言语义解析基准，涉及多种自然语言和表示。我们对三种代表性的多语言语言模型类型进行了全面的基准研究，我们的结果显示了许多有趣的发现等。欢迎访问我们的论文和代码。谢谢收听。"}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是亚当·斯皮尔科夫斯基，今天的演讲主题是协调的依赖结构。正如你们所知，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依赖中，协调结构“丽莎、巴特和玛吉”的结构是第一个并列成分是整个协调结构的头部，即丽莎。伊戈尔·米尔丘克的意义文本理论也采用了类似的方法，整个协调结构由第一个并列成分主导。因此，这两种方法是不对称的，它们突出了一个并列成分。此外，还有对称的协调结构方法，如布拉格方法，布拉格依赖树库中的连词主导方法，其中协调结构由连词主导。因此，我们从“和”到所有并列成分都有依赖关系。最后，还有一种多头方法，例如在卡茨的词法语法中使用，所有并列成分都是协调结构的头部。因此，我们从统治者（这里是“爱”）到所有并列成分都有依赖关系。这些是巴特和玛吉。现在，这篇论文的目的是为对称的协调结构（如这两种）提出新的论据，反对不对称的协调结构（如这两种）。好的，论据基于依赖长度最小化原则，我将通过这些例子来解释。在英语中，正如你们所知，直接宾语倾向于靠近动词，而状语可能更远。因此，“玛奇昨天读了它”是可以的，因为直接宾语“它”靠近动词，而“玛奇昨天读了它”则要差得多，因为动词和直接宾语之间有一个状语“昨天”。然而，当直接宾语非常长且非常重时，这种效果可能会减轻，因为它可以移动到状语之后的位置。这在下面的例子中得到了说明。因此，这两个句子都是可以的。玛奇读了这本关于蜜蜂的绝对迷人的书，昨天是好的。以这种方式，我们有这个长的名词短语，而不是“它”。但说“玛奇昨天读了这本关于蜜蜂的绝对迷人的书”也是可以的。因此，原因是这可能是因为，尽管这个句子违反了一般语法原则，即直接宾语应该靠近动词，但它满足了依赖长度最小化原则，该原则认为较短的依赖关系是优先的。因此，这两棵树只显示了关键依赖关系的长度，即在这两种结构中不恒定的依赖关系。因此，我们有从“读”到状语的长度为7个单词的依赖关系，从“读”到“书”的长度为4个单词。因此，总共是11个。当你移动、交换这两个成分时，这两个依赖关系的总和变成了6个。因此，11变成了6，要短得多。这就是为什么这听起来还不错。它违反了一个原则，但满足了另一个原则。好的，我们做了什么？我们从增强版的宾语树库中提取了各种关于协调的统计数据，并查看了论文，为什么我们不使用通用依赖。这些统计数据证实了以前多次观察到的现象，即左并列成分倾向于较短，例如“盐和胡椒”，而不是“胡椒和盐”，以音节计算。还有一个观察是，这种倾向随着长度差异的增加而增强。因此，当两个并列成分的长度差异增加时，较短的并列成分更倾向于成为第一个。因此，左短并列成分的比例更大。但这篇论文的新颖之处在于，我们观察到这种倾向只在左侧没有统治者时发生。因此，在这个例子中，左侧的统治者是“我看到了巴特和丽莎”，因此统治者在左侧。在第二个例子中，它是缺失的，霍默来了并打了个喷嚏。这里我们有两个动词的协调，没有外部统治者。因此，在这种情况下，左并列成分更倾向于较短，两个并列成分的差异越大，这种倾向就越强。然而，当统治者在右侧时，例如“左”统治协调“网络”，这种效果消失了。我们通过以字符、音节和单词为单位测量长度来展示这一点，因此我将集中在右侧。我们在这里看到的是，当统治者在左侧时，左并列成分较短的倾向随着单词的绝对差异稳步增长，当没有统治者时，例如在句子的协调中，也观察到相同的情况，但当统治者在右侧时，这种倾向消失了。我们在论文中展示了这如何为不对称的协调结构（如这两种）提供论据，并为对称的协调结构（如这两种）提供论据。因此，请查看论文以获取完整的论据，并在海报会议上与我们交流。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我叫凯奥·杨，我将介绍我们的研究成果，题目是《翻译何时需要上下文？基于数据的多语言探索》。这项研究是与帕特里克·弗兰纳、艾米·刘、安德烈·F.D.马丁斯和格雷姆·纽维格合作完成的。许多翻译都依赖于上下文。例如，我们如何翻译这个句子中的“mole”？如果前一个句子是“如果部长们发现了，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一个句子是“医生，这可能是严重的吗？”那么“mole”指的是痣。因此，根据上下文，单词的含义会改变，翻译也会随之改变。然而，评估模型如何处理这些情况是非常困难的。首先，因为只有少量的翻译依赖于上下文，这使得像蓝色这样的语料库级别的指标无法捕捉到这些翻译。一些人建议对上下文依赖的翻译进行针对性评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工策划。在这项研究中，我们试图回答这两个问题。首先，翻译何时需要上下文？其次，模型如何处理这些情况？为了回答第一个问题，我们从测量单词在翻译中对上下文的依赖程度开始。在之前的研究中，我们引入了CXMI作为机器翻译模型使用上下文的度量。这通过测量上下文C在给定源X的情况下提供关于目标Y的信息量来实现。你可以将CXMI视为从给模型提供上下文中获得的信息。在这项研究中，我们将CXMI扩展到点YCXMI，它可以在句子级别或单词级别测量上下文的使用。我们可以将具有高P6MI的单词视为需要上下文进行翻译的单词。现在我们分析具有高PCXMI的单词，以寻找这些单词之间的模式。我们在从英语翻译成14种不同语言的TED演讲稿中进行分析。我们在三个不同的层面上进行分析。首先，我们查看具有高平均PCXMI的语音标签。这使我们能够找到，例如，在阿拉伯语中具有高P6MI的双重代词。这可以解释为英语没有双重代词，因此在翻译成阿拉伯语时，需要上下文来确定代词是否是双重的。类似地，我们发现某些语言在选择适当的动词形式时也需要上下文。然后我们查看具有高P6XMI的词汇项目，平均在其所有不同的出现中。这帮助我们识别出像这里这样的情况，在中文中，你需要上下文来翻译正确的名词，以确保在文档中使用相同的翻译。类似地，我们发现上下文支持翻译正确的正式性。最后，我们查看具有高P6XMI的不同个别标记。这使我们能够识别出无法通过单词本身捕捉到的现象，但这些现象在句子结构中表达出来，例如省略号解析。现在我们使用分析结果来设计文档级别翻译的基准。对于我们识别出的每个话语现象，我们创建标记器来自动识别属于该现象的单词。我们称我们的标记器为多语言话语感知或MUDA标记器。我们还注意到不同的语言具有不同比例的这些话语现象。然后我们通过将标记器应用于我们想要用于评估的平行语料库来使用MUDA标记器。我们在MUDA标记器识别出的上下文依赖示例上应用我们选择的翻译指标。最后，我们使用我们的基准以及其他指标来评估不同模型在文档级别机器翻译中的表现。首先，当我们使用语料库级别的指标时，例如蓝色，我们发现上下文无关模型表现最佳。但如果我们使用Comet，上下文感知模型表现最佳。如果我们使用Word F度量，那么无论是否使用上下文，模型的表现都相当。这再次表明，如果我们仅使用语料库级别的指标，很难确定最佳的文档级别翻译系统。现在我们使用MUDA基准来评估模型，我们发现对于某些话语现象，例如正式性和词汇凝聚力，上下文感知模型比不使用上下文的模型更准确。但这些模型在其他现象上，如省略号、代词和动词形式，并不比不使用上下文的模型好多少。这在某种程度上表明我们需要在文档级别翻译方面取得更多进展。我们还比较了不同的商业系统，我们的基准显示，DeepL通常比Google Translate在文档级别翻译中更准确。总结一下，我们在14种语言对中进行了基于数据的分析，以确定翻译何时需要上下文。然后我们使用我们的发现来构建文档级别机器翻译的基准，这可以帮助我们确定模型能否很好地处理哪些话语现象，以及哪些翻译系统擅长文档级别翻译。非常感谢大家的注意。在多伦多见。"}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我是珍妮，卡内基梅隆大学的一年级博士生，今天我将介绍我们的工作《NL位置性：通过数据集和模型来描述设计偏见》。这项工作是与华盛顿大学和艾伦人工智能研究所的几位同事合作完成的，包括塞巴斯蒂安·桑蒂、罗南·拉布拉斯、卡塔里娜·阿里尼卡和马丁·萨普。让我们从想象你在报社工作，并试图删除新闻文章下的有害评论开始。你可能会使用像Perspective API这样的流行API来检测有害内容，这对卡尔·琼斯来说效果很好，Perspective API能够正确检测有害实例。但这对迪蒂亚·沙尔马来说并不是这样，Perspective API对在印度语境中更常见的冒犯性词汇并不敏感。这是一个设计偏见的例子，我们看到技术在不同人群之间的系统性表现差异。像我们刚才看到的设计偏见可能是由于NLP研究人员和模型开发人员的位置性。位置性只是人们由于其人口统计学、身份和生活经历而持有的观点。这是一个在批判性研究中广泛使用的概念，特别是在女性主义和同性恋学术空间中。作为研究人员，位置性可以影响研究过程及其结果，因为它可以改变研究人员的决策。因此，人们可能会问，数据集和模型是否有位置性？我们并不是说模型本身和数据集本身有人口统计学身份和生活经历，但它们确实汇集了真实人的判断和意见，因此可以代表某些位置性而不是其他位置性。因此，先前的工作已经提出了一些位置性的轶事证据，例如模型和数据集中的文化差距，以及模型位置性的理论定义。然而，这些工作并没有比较最终用户与数据集和模型本身。随着NLP任务变得更加主观和社会导向，研究模型和数据集的位置性变得越来越重要。要描述这些位置性如何偏斜是具有挑战性的，因为并非所有决策都有记录，许多模型隐藏在API后面。因此，为了研究数据集和模型的位置性，我们实际上比较了真实用户的标注与现有数据集和模型。我们通过我们的框架NL位置性来实现这一点。我们的框架分为两个主要步骤。第一步是用多样化的标注者重新标注数据集。我们选择这样做，而不是查看原始数据集标注者的人口统计学，因为通常只有少数标注者标注每个实例，而且人口统计学很少被收集和共享。因此，我们选择重新标注数据，以获得每个实例的许多标注，并获得丰富的人口统计学数据集。然后我们按人口统计学分类标注，并使用皮尔逊相关系数与模型和数据集进行比较。因此，我们的框架实际上不同于标注者不一致的文献，因为它比较了最终用户与模型和数据集的预测和标签，而不是仅查看标注者的同意或建模标注者的分布。我们的框架主要通过Lab in the Wild实现，这是一个在线众包平台，前HCI合作者。Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，而像MTurk这样的平台主要有来自美国或印度的参与者。此外，Lab in the Wild仍然能够获得高质量的数据。我们在Lab in the Wild上托管了两项任务，其中一项是社会可接受性。其工作方式是参与者将阅读来自社会化学数据集的情况，然后他们将写下情况的社会可接受性。之后，为了保持对研究的参与，他们可以将他们的回答与AI和其他人进行比较。然后我们将这些标注与社会化学、Delphi和GPT-4进行比较。然后我们为有害言论检测任务复制了一个非常相似的设置，参与者将阅读来自DynaHate的实例，并写下他们认为是否是有害言论的实例。然后我们将这些标注与DynaHate、Perspective API、Rewire API、Hate Roberta和GPT-4进行比较。我们的研究最终汇集了来自87个国家的1000多名标注者的16,000多个标注。现在我们更有能力回答NLP数据集和模型与谁最一致。我们发现NLP中存在位置性。例如，我们发现数据集和模型与英语国家最一致。因此，对于GPT-4的社会可接受性分析，我们发现它与英语国家最一致。我们还发现DynaHate与英语国家最一致。我们还发现与受过大学教育的人有更多的额外一致性。因此，对于GPT-4的社会可接受性任务，我们发现它与受过大学教育或研究生教育的人最一致。我们在DynaHate中也发现了同样的情况，它与受过大学教育的人最一致。然而，当模型和数据集与特定人群一致时，一些人不可避免地被落下。一个例子是数据集和模型与非二元性别的人相比，与男性和女性的对应人群一致性较低。我们在GPT-4的社会可接受性任务以及DynaHate任务分析中也发现了这一点。因此，既然NLP中存在位置性，我们能做些什么呢？我们对此有一些建议。第一个是记录研究过程中的所有相关设计选择。另一个是以多元视角进行NLP研究。我们的第三个建议是为特定社区内的特定社区建立专门的数据集和模型。一个很好的例子是Masakane倡议。我们想强调的是，包容性NLP不仅仅是让所有技术为每个人工作。这结束了我们的演示，但如果你想了解更多，请随时查看我们的仪表板，获取最新的分析结果和我们的论文。谢谢。"}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "zh", "output": "大家好，我要谈谈我们在实体选择中的间接引用表达式解析方面的工作，我们引入了Alt Entities Corpus。我叫Javad Hosseini，这是与Philip Radinsky、Sylvia Parity和Annie Lewis的联合工作。我们的目标是理解用户在做出选择时的语言。考虑这个替代问题。你是指《Easy on Me》还是《I Got a Feeling》？这里，用户想在两首歌中做出选择。最明显的方法是使用直接引用。例如，通过说出歌曲的名字《Easy on Me》或它的位置，第一首。但有时间接引用更适合进行更自然的对话。这可能发生在用户无法记住歌曲名字的情况下。或者发音太相似，难以区分。或者当用户想指定一个偏好时。以下是一些间接引用的例子。例如，较新的那首或那首不活泼的歌。这是对话系统中的一个重要问题，也是评估LLM实体理解的基准。我们不知道有公开的大规模数据集用于这个任务，所以我们使用众包注释收集了一个数据集。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用卡通完成设置。卡通有三个语音气泡。在第一个气泡中，Bob说：“记得我们昨天听的那首歌吗？”Bob设定了对话的背景。在第二个语音气泡中，Alice说：“你是指《Easy on Me》还是《I Got a Feeling》？”这是替代问题。在第三个语音气泡中，Bob使用间接引用选择其中一个实体。例如，较新的那首。我们自动提供第一个和第二个语音气泡，但第三个由注释者填写。第一个语音气泡是从每个领域的几个手动提示中选择的。第二个，即替代问题，是通过以下方式生成的。我们总是使用一个简单的模板。你是指A还是B？其中A和B是从维基百科中抽样的。以下是我们使用的不同抽样方法。当我们在列表中向上移动时，实体变得更加相似，通常更难进行消歧。第一个是均匀随机。第二个是当实体有相似的标题时。例如，两本书的名字是《The Return》。第三个是当它们在维基百科上有相似的描述时。最后，当它们在维基百科上有相似的信息框或属性时。例如，相同的流派或相同的艺术家的歌曲。当我们向注释者展示这个替代问题时，他们知道这些实体的名字，但不一定知道实体。所以我们做的是展示关于这两个实体的一些背景知识。对于歌曲，我们只是展示每首歌的Google搜索链接，然后要求注释者至少听每首歌的一部分，并阅读每首歌的信息。例如，这是《Easy on Me》的Google搜索结果。对于食谱和书籍领域，我们展示一些来自维基百科的背景文本。对于食谱，我们还展示它们的图像，同样来自维基百科，这样注释者就知道它们的样子。然后我们要求注释者选择其中一个实体，例如这里的第一个，并用三到五个间接引用表达式描述它们。例如，带钢琴音乐的那首。以下是我们数据集的一些例子。例如，没有歌词的那首，不是带12岁男孩的那首，或者虚构的那首，或者来自阿塞拜疆的那首，等等。Alt Entities Corpus有6000个替代问题，涵盖三个领域，有42000个间接引用表达式。T5X大模型的结果总结如下。如果语言模型有与注释者完全相同的背景知识，那么准确率非常高。大约在92到95%。但这不现实。如果语言模型有部分重叠的背景知识，那么准确率在82到87%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率只有60%。所以有很大的改进空间。我们还展示了模型是领域泛化的。这是我们数据集的链接。谢谢。"}
