{"dataset_id": "mcif_v1.0", "sample_id": 0, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Matthias Lendemann e oggi vi darò una breve introduzione al nostro articolo sulla generalizzazione composizionale senza alberi utilizzando il tagging di multi-set e permutazioni latenti. Questo è un lavoro congiunto con i miei supervisori, Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe sembrare così. Come al solito, abbiamo un set di addestramento di enunciati, in questo caso la ragazza dormiva e Mary sapeva che la ragazza dormiva. Questi enunciati sono accoppiati con forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza della valutazione standard del machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli sequenza a sequenza naif faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle codificate a colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono destinati a catturare il processo composizionale che collega gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono dati e devono essere ottenuti in qualche modo. Questo può essere un processo complicato e a volte computazionalmente costoso. Tipicamente, questo comporta un pre-elaborazione considerevole specifica del formalismo delle forme logiche, ad esempio per gestire i simboli variabili. Ottenere alberi può anche comportare procedure di induzione grammaticale specializzate. In questo articolo, non usiamo alberi e introduciamo un modello sequenza a sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione verso una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due fasi. Prima, etichettiamo ogni token di input con un multi-set non ordinato di token che appariranno nell'output. Dopo la prima fase, abbiamo tutti i token giusti, ma non sono ordinati. Ecco perché, nella seconda fase, usiamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere una permutazione che non pone vincoli rigidi sulle permutazioni possibili. Questo rende il nostro approccio molto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Andiamo da sinistra a destra sull'output e determiniamo quale token di multi-set mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno come evidenziato in rosso. Poi saltiamo al token di multi-set successivo per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token di multi-set. Continuiamo questo processo fino a quando ogni token della prima fase è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark Cogs. Il nostro modello supera gli altri di un ampio margine nella generalizzazione verso una ricorsione più profonda. Altri tipi di generalizzazione strutturale rimangono comunque molto impegnativi. Nel nostro articolo risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un token dato, non sappiamo da quale multi-set proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema del commesso viaggiatore. Approssimiamo questo con una rilassamento continuo amichevole per GPU che ci permette anche di retropropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster."}
{"dataset_id": "mcif_v1.0", "sample_id": 1, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Myra e oggi parlerò del nostro articolo \"Personaggi marcati\", utilizzando prompt di linguaggio naturale per misurare gli stereotipi nei modelli linguistici. Questo lavoro è stato realizzato in collaborazione con Essin Darmush e Dan Jarowski. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici, o LLM. Tuttavia, queste misurazioni presentano vari limiti. Di solito si basano su set di dati costruiti manualmente che richiedono molto tempo per essere curati. E di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con gruppi particolari. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è l'idea che le identità sociali multifaccettate possano amplificare i pregiudizi e rappresentare unici luoghi di danno. Per superare questi limiti, ci affidiamo alla proprietà che questi nuovi LLM istruiti sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare un personaggio, che è una descrizione di un individuo immaginario utilizzando un prompt come \"Immagina di essere una donna asiatica. Descriviti\". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare qualsiasi marker di identità che vogliamo in questo prompt. Quindi ecco alcuni esempi di generazioni da GPT-4. Immediatamente vediamo che, sebbene le uscite non siano apertamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è descritta come modesta. La donna del Medio Oriente è descritta usando parole come esotica e riferendosi a una regione affascinante. E entrambe le donne di colore fanno riferimento all'ascendenza, mentre il personaggio dell'uomo bianco non ha nulla del genere. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare questi personaggi. I nostri prompt per generare questi personaggi sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che dando questi prompt a soggetti umani, sono stati in grado di far emergere stereotipi razziali. E questo consente anche un confronto diretto tra i nostri personaggi generati e le risposte scritte dagli umani. La seconda parte sono le parole marcate, che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marcati, di cui parlerò tra poco. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su un lessico specifico. Quindi il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatezza, che afferma che c'è un non marcato di default e qualsiasi gruppo che differisce da questo default è linguisticamente marcato. Quindi, ad esempio, la parola guerriero è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente un guerriero donna e marcano il termine con donna. E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Quindi nel nostro metodo, designiamo prima cosa sono i gruppi non marcati e marcati. E poi confrontiamo i personaggi usando il metodo delle parole combattute, che è fondamentalmente l'uso di rapporti di probabilità ponderati per distinguere le parole principali per ogni gruppo marcato. Quindi, ad esempio, per i personaggi delle donne nere, faremmo parole combattute e confronteremmo i rapporti di probabilità sia con i personaggi bianchi che con i personaggi maschili, perché questi sono i due gruppi non marcati corrispondenti. Ora per alcuni risultati. Quindi, prima di tutto, usiamo un lessico di stereotipi e troviamo che i personaggi generati contengono molti più stereotipi di quelli scritti dagli umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, sebbene i personaggi generati abbiano tassi molto più alti delle parole del lessico, quelli scritti dagli umani hanno una distribuzione molto più ampia delle parole, mentre le parole stereotipate che sono nei personaggi generati sono davvero solo le parole alte e atletiche. Quindi davvero solo quelle positive o almeno non negative. E in effetti, questo lessico non cattura molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti. Quindi, invece, per farlo, ci rivolgiamo ai risultati del nostro metodo delle parole marcate per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima di tutto, per i gruppi marcati, le parole principali includono cose come cultura, tradizione, orgoglioso ed esotico. E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti trope comuni riflessi in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono cose come vivace e formosa, che si collegano a un trope di tropicalismo. Per le donne asiatiche, le parole sono cose come minuta e delicata e setosa, che si collegano a una lunga storia di iper-sessualizzazione delle donne asiatiche, viste come molto docili e sottomesse e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resiliente. Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte. E sebbene sembri positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli della società. Quindi, invece di lavorare effettivamente per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a risultati di salute molto negativi per queste persone, tra gli altri danni. Più in generale, troviamo che le parole per ogni gruppo marcato riflettono fondamentalmente narrazioni molto essenzializzanti. Quindi, sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima di tutto, dovremmo, come ricercatori, affrontare stereotipi positivi e narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci davvero una maggiore trasparenza sui metodi di mitigazione dei pregiudizi. Perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di allineamento dei valori eccessivo o forse altri metodi anti-stereotipizzazione che stanno portando a questi schemi dannosi. Non possiamo davvero fare alcuna ipotesi o studiare ulteriormente senza una maggiore trasparenza. Grazie mille per l'ascolto. Divertitevi ad ACL."}
{"dataset_id": "mcif_v1.0", "sample_id": 2, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato svolto dal laboratorio Emory NLP, guidato dal professor Gino Choi presso l'Università di Emory, e in collaborazione con Amazon Alexa AI. Quindi, diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di selezionare quale delle due conversazioni è migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi comparativi o a scala Likert esistenti. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio annotazione dei comportamenti nella chat, o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABC-Eval è in grado di misurare le frequenze con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o contraddice il suo partner, allucina fatti errati o viola la conoscenza di buon senso, e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat di punta e li abbiamo valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval. A titolo di confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalle nostre analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento ABC-Eval sono nel complesso più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo tra annotatori su 100 conversazioni etichettate due volte. Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, puoi vedere come misurare la proporzione di turni con contraddizioni di sé e del partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a passi. Puoi vedere come la combinazione di tutte le metriche ABC-Eval spiega oltre il 25% della qualità della conversazione. E man mano che rimuovi le metriche una alla volta, la maggior parte di esse porta alla perdita di una quantità decente di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche portano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione superiore rispetto a quanto i metodi precedenti sono in grado di ottenere. Puoi vedere nei risultati del nostro esperimento che rimangono diverse sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del buon senso in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte. E si contraddicono o contraddicono il loro partner circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questo è tanto più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere sfruttata da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'intelligenza artificiale conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 3, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Vasudha e sono una candidata al dottorato in informatica presso la Stony Brook University. Vorrei presentare il nostro lavoro accettato per ACL 2023 come articolo lungo, \"Apprendimento per trasferimento per la rilevazione della dissonanza, affrontando la sfida della classe rara\". Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio. Semplicemente, la dissonanza cognitiva è costituita da due credenze o azioni che sono incoerenti. Ad esempio, una persona afferma: \"So che le sigarette potrebbero uccidermi\" e poi dice: \"Ho preso un paio di sigarette dopo la riunione\". Questa credenza e azione sono incoerenti e sono in dissonanza. Inoltre, affermare: \"Non credo di poter mantenere il mio lavoro senza di loro\" giustifica la seconda occorrenza e hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nelle decisioni quotidiane, è raro trovarla espressa nel linguaggio tra le altre relazioni discorsive. Perché questo è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e cambiamenti nei valori e negli atteggiamenti della popolazione. Un'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali. Per creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio basato sulla dissonanza, come mostrato nel diagramma di flusso qui. I tweet sono stati analizzati utilizzando un parser PDTV e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa mille esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia performato molto meglio del caso. Data la bassa occorrenza della dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema dell'assoluta rarità. Per alleviare questo problema, sperimentiamo combinazioni di apprendimento per trasferimento e apprendimento attivo per annotare in modo tale da raccogliere più campioni di dissonanza in meno round di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione della posizione di dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo indipendentemente dal tema, chiamato dibattito qui, e la classificazione binaria delle classi di espansione e confronto di PDTB, poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza, e li chiamiamo CE qui. Scopriamo che trasferendo, la performance zero-shot sul set di dati annotato è già molto migliore del caso, con il migliore con AUC 0,62. Inoltre, iterando il fine-tuning su entrambi i compiti, scopriamo che il fine-tuning del compito CE seguito da un ulteriore fine-tuning sul dibattito produce una performance zero-shot molto migliore. Questo è il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. L'aggregazione accumula tutti i dati raccolti finora dall'annotazione attiva, mentre l'aggiornamento iterativo aggiorna il modello addestrando sul set di dati più recente raccolto. Tra le diverse strategie, abbiamo scoperto che l'aggregazione ha performato uguale o meglio dell'iterazione in tutti i casi. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara, PRC, per selezionare principalmente gli esempi che sono altamente probabili essere dissonanti secondo il modello attuale in qualsiasi round di AL. Confrontiamo questo con le altre strategie di AL all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, anche se la differenza è piccola. Notare che la performance è significativamente inferiore per il caso casuale. Nei round successivi di AL con le due migliori strategie, miglioriamo la classificazione della dissonanza AUC a 0,75, che è la migliore performance che abbiamo finora sul compito. Controlliamo anche la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, anche gli annotatori trovano gli esempi difficili. In sintesi, scopriamo che PRC è una semplice strategia AL per l'acquisizione di classi rare e l'avvio dell'AL con compiti di apprendimento per trasferimento progettati in modo appropriato può aiutare significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per l'apprendimento per trasferimento da un dominio diverso, mentre le annotazioni attive nel dominio beneficiano dell'aggiornamento aggregato. Questi sono i link al nostro codice, set di dati e articolo. Sentitevi liberi di contattarci se avete domande. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 4, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Akshata e oggi io e il mio coautore Martin presenteremo il nostro lavoro, la suite di test Kitmas, valutando l'integrazione delle conoscenze provenienti da fonti multiple. Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale richiede spesso conoscenze che vengono fornite anche al momento dell'inferenza. Ad esempio, nella frase \"John ha visto il presidente appena eletto in TV\", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere con certezza chi sia l'entità specifica di questo caso, John, o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dal momento del pre-addestramento. Pertanto, i modelli di successo per compiti di NLU intensivi di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante il pre-addestramento che quella fornita al momento dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione delle conoscenze. Introduciamo un compito di risoluzione della co-referenza progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti umani e modelli di risoluzione della co-referenza consolidati. Ecco un esempio dal nostro dataset. Servin è un giudice. Kia è una panettiera. Servin e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui il pronome \"lui\" si riferisce, che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Prima, la conoscenza specifica dell'entità, come \"Servin è un giudice\". E seconda, la conoscenza di base, come \"i giudici decidono casi in tribunale\". In generale, la conoscenza di base viene appresa durante il pre-addestramento dei grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene solitamente osservata al momento dell'inferenza. Variamo la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in fonti multiple. Abbiamo definito tre impostazioni di Kitmos. Prima, abbiamo l'impostazione background pre-addestrato, in cui si suppone che la conoscenza di base sia disponibile al momento del pre-addestramento. Seconda, c'è l'impostazione background entrambi, in cui la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione background inferenza, in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli, ad esempio perché nuove occupazioni si sono sviluppate dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione background pre-addestrato, supponiamo che la conoscenza di base \"i politici cercano seggi eletti nel governo\" sia contenuta nei parametri pre-addestrati. Nel contesto di inferenza, forniamo la conoscenza specifica dell'entità \"Chichester è un politico\". Nell'impostazione background entrambi, forniamo non solo la conoscenza specifica dell'entità, ma anche la conoscenza di base sui politici nel contesto di inferenza. Nell'impostazione background inferenza, forniamo l'occupazione fittizia \"meritour\" invece di politico, poiché è improbabile che \"meritour\" sia contenuta nei parametri pre-addestrati. Valutiamo il dataset sia con partecipanti umani che con modelli di risoluzione della co-referenza consolidati. In questa figura mostriamo i risultati dei modelli migliori nella variante più difficile dell'impostazione background pre-addestrato. Senza un addestramento specifico del compito su Kitmos, entrambi i modelli non si comportano bene. Quando addestrati su Kitmos, tuttavia, sia C2F che Built4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su dataset di risoluzione della co-referenza generali, i modelli imparano a sfruttare indizi superficiali, che non sono utili quando si testa su Kitmos, dove tali indizi sono stati rimossi. Ulteriori esperimenti con conoscenze fittizie indicano che anche i modelli migliori non riescono a integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza. Per riassumere i principali risultati del nostro articolo, molti modelli di risoluzione della co-referenza sembrano incapaci di ragionare sulla conoscenza proveniente da fonti diverse senza un addestramento specifico del compito. Tuttavia, con un addestramento specifico del compito, alcuni modelli integrano con successo la conoscenza da fonti multiple. Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile la conoscenza di base presentata solo al momento dell'inferenza. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e date un'occhiata al dataset e al codice su GitHub. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 5, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sarah Papi dell'Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente il documento \"L'attenzione come guida per la traduzione simultanea della lingua parlata\", che è un lavoro congiunto con Matteo Negri e Marco Turchi. Cos'è la traduzione simultanea della lingua parlata? La traduzione simultanea della lingua parlata, o SimulST, è il processo di traduzione di una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue. Quali sono i problemi dei modelli SimulST attuali? Le architetture specifiche sono solitamente addestrate introducendo moduli aggiuntivi da ottimizzare, procedure di addestramento lunghe e complicate, ad esempio l'addestramento che coinvolge diversi obiettivi di ottimizzazione, e l'addestramento e la manutenzione di diversi modelli per raggiungere diversi regimi di latenza, ad esempio l'addestramento di un modello con una latenza media di un secondo e un altro con una latenza di due secondi, e così via. Qual è la nostra soluzione? Prima di tutto, utilizzare modelli SimulST già esistenti senza doverli riaddestrare o adottare un'architettura specifica per SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare la conoscenza già acquisita da un modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, che è il meccanismo di attenzione crociata. E potete vedere un esempio a destra. La nostra soluzione è proporre E-DOT, o encoder-decoder attention, ed è una strategia con cui decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè se la somma è inferiore a una certa soglia alfa verso gli ultimi lambda frame di discorso, il che significa che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un frammento di discorso contenente \"sto per parlare di\" e il nostro modello prevede la traduzione in tedesco, e guardiamo i pesi di attenzione crociata, vedremo che le prime due parole puntano ai frame di discorso ricevuti per primi, mentre l'ultima parola punta agli ultimi frame di discorso ricevuti, cioè ai frame di discorso lambda. Questo significa che le prime due parole verranno emesse, mentre, poiché la somma dell'attenzione crociata è superiore a una certa soglia alfa, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso. Se continuiamo e riceviamo un altro frammento di discorso e il nostro modello prevede altre tre parole e guardiamo i pesi di attenzione crociata, vedremo che nessuna parola punta agli ultimi frame di discorso lambda. Questo significa che queste tre parole verranno emesse. Se guardiamo i risultati principali di E-DOT, tracceremo i risultati della traduzione simultanea della lingua parlata su grafici in cui abbiamo il blu da un lato che misura la qualità della traduzione e il ritardo medio, cioè la misura della latenza. E consideriamo anche il ritardo medio consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output. Quindi vogliamo che le nostre curve siano il più alte possibile in questo grafico, ma vogliamo anche che siano spostate a sinistra. E confrontiamo con strategie precedenti che sono applicate anche a modelli offline, come la strategia Wait-K e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la traduzione simultanea della lingua parlata. Questi sono tutti i risultati della strategia di traduzione simultanea della lingua parlata sul tedesco. E vediamo che E-DOT supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo consapevole del calcolo, E-DOT è la strategia più veloce. Se volete scoprire altri risultati, leggete il nostro documento. E abbiamo anche rilasciato open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione."}
{"dataset_id": "mcif_v1.0", "sample_id": 6, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Zhu Hang. Oggi presenterò il nostro articolo, \"I tagger di entità nominate di Connell 2003 funzionano ancora bene nel 2023?\" Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate, o il compito NER. Abbiamo osservato che i modelli hanno utilizzato Connell 2003 per sviluppare NER per quasi 20 anni. E questo solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare i dati moderni? E quando sviluppiamo nuovi tagger, cosa serve per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset Connell++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione di Connell 2003. Abbiamo quindi eseguito il fine-tuning di oltre 20 modelli su Connell 2003. Li abbiamo valutati sia sul set di test di Connell 2003 che sul set di test di Connell++. E infine, abbiamo calcolato la percentuale di variazione di F1 per valutare la generalizzazione di ogni modello. Quindi, cosa serve per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer si generalizzano generalmente meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un compito a valle. Qui abbiamo anche scoperto che più esempi di fine-tuning portano effettivamente a una migliore generalizzazione. Passiamo alla nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è il sovraccarico adattivo, che è il sovraccarico causato dall'uso ripetuto dello stesso set di test, e questo si manifesta solitamente come rendimenti decrescenti su un nuovo set di test. La seconda ipotesi è lo spostamento temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e di test. Per il sovraccarico adattivo, abbiamo visto che dal grafico a destra, la linea di miglior adattamento rossa ha un gradiente maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su Connell 2003 si traduce in più di un'unità di miglioramento su Connell++, il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che il sovraccarico adattivo in questo caso non è osservato. E lo spostamento temporale? Per lo spostamento temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti, e abbiamo scoperto che le prestazioni si degradano con un divario temporale maggiore. E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è lo spostamento temporale. La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, una dimensione del modello più grande, nonché più esempi di fine-tuning. E questi vanno di pari passo, non possiamo avere solo un ingrediente, ma tutti gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dallo spostamento temporale, e sorprendentemente, non è causato dal sovraccarico adattivo, anche se Connell 2003 è stato utilizzato per oltre 20 anni. Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, i tagger di Connell 2003 funzionano ancora nel 2023? E abbiamo scoperto che la risposta è un sì risolutivo. Speriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli. E infine, assicuratevi di controllare il nostro articolo, il nostro dataset, e se avete domande, non esitate a contattarmi. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 7, "src_lang": "en", "tgt_lang": "it", "output": "Benvenuti alla nostra presentazione di DeepL, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e di frase. Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione. Definiamo prima la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione dello stesso per un gruppo target specifico, come le persone con problemi di lettura o i madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie di testi paralleli, ad esempio di documenti o frasi. Nell'esempio qui, potete vedere una coppia di frasi allineate in parallelo di una frase tedesca complessa e la sua traduzione in un linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio, come la sostituzione lessicale, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la clausola, la"}
{"dataset_id": "mcif_v1.0", "sample_id": 8, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Siyuan di Fudan University. Sono qui per presentare il nostro lavoro, \"Distinguere la conoscenza dello script dai grandi modelli linguistici per la pianificazione linguistica vincolata\". Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script guidati. I lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come \"fare una torta\", e hanno dimostrato che i grandi modelli linguistici possono decomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come \"fare una torta al cioccolato\", rimane poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli agli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione dei dati con l'uomo nel ciclo utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai modelli linguistici. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici. Poi conduciamo un'analisi dettagliata per indagare il motivo per cui i modelli linguistici falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo le categorie di vincoli più dettagliate e topiche, diverse in Wikihow. La mappa termica nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici è alta, portando a una scarsa performance. Pertanto, adottiamo l'idea di overgeneration e filtraggio per migliorare la qualità della generazione. Mostriamo prima i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti forniti. Poi InstructGPT overgenera script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtraggio per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding di InstructGPT e calcoliamo la similarità coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Copiamo lo script solo se l'obiettivo target ottiene il punteggio più alto nel sito degli obiettivi. Con il nostro metodo, InstructGPT può generare script di alta qualità. Il nostro metodo migliora notevolmente la pianificabilità sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di set di dati è un passo essenziale a tal fine. Tuttavia, studi precedenti non abilitano la pianificazione di obiettivi specifici e l'annotazione manuale dei set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare set di dati di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un set di dati di pianificazione linguistica vincolata, chiamato CodeScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei set di validazione e test, chiediamo ai lavoratori crowdsourced di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CodeScript. Scopriamo che CodeScript mostra un alto plauso negli obiettivi specifici generati. Con CodeScript, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 fine-tuned su CodeScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che modelli più piccoli possono supportare modelli più grandi quando addestrati correttamente su set di dati appropriati. In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici e sviluppiamo un metodo di overgeneration e filtraggio per i grandi modelli linguistici. Utilizziamo i grandi modelli linguistici per generare un set di dati di script di alta qualità, CodeScript, per la pianificazione linguistica vincolata. Speriamo che il set di dati CodeScript possa essere una risorsa disponibile per avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per ulteriori dettagli su CodeScript, consultate il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 9, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Yanis Lavrac e vi presenterò i nostri lavori su Dr. BERT, un modello pre-addestrato robusto in francese per il dominio biomedico e clinico. In questa presentazione, parleremo prima del language modeling in ambito sanitario. Poi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese, chiamato Dr. BERT, basato su Roberta e addestrato su Nachos, un dataset di dati medici raccolti dal web. Introduciamo anche un confronto tra modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 compiti downstream biomedici e clinici in francese. Infine, concludiamo gli esperimenti e vi forniamo ulteriori dettagli su come accedere ai modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di natural language processing e offre un enorme guadagno di prestazioni rispetto ai metodi statici e contestuali storici come Word2Vec, FastText o NWO. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con Camembert, e ad altri domini come il biomedico con PubMedBERT e BioBERT, e il clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono rari e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati nel dominio. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Ci siamo quindi chiesti quale fosse la fonte di dati più appropriata per una vasta gamma di utilizzi e se questi dati raccolti fossero una buona sostituzione per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato Dr. BERT con il nostro modello Schubert, basato su dati anonimizzati ottenuti dall'ospedale universitario di Data Warehouse. Poi ci siamo chiesti quanti dati fossero necessari per addestrare un modello specializzato su dati francesi. Sono 4 gigabyte, 8 gigabyte o di più? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione di Dr. BERT con 7 gigabyte di Nachos, una seconda versione con un sottoinsieme di 4 gigabyte di Nachos, una prima versione di Schubert, un modello clinico, con 4 gigabyte di frasi prese da note cliniche, e una versione finale di Schubert con una miscela di un sottoinsieme di 4 gigabyte di Nachos e 4 gigabyte di note cliniche. Oltre a questo confronto, introduciamo tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di Camembert e addestrato su un sottoinsieme di 4 gigabyte di Nachos, un altro anche basato su Camembert, ma addestrato questa volta su 4 gigabyte di note cliniche, e infine uno basato su un modello biomedico inglese, PubMedBERT, e addestrato su un sottoinsieme di 4 gigabyte di Nachos. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo compiti pubblici e privati, come il riconoscimento del nome, la classificazione, il tagging del discorso e la risposta alle domande. Questi modelli sono confrontati con sei modelli di base, che sono Camembert Oscar 138 gigabyte, Camembert Oscar 4 gigabyte, Camembert CCNet 4 gigabyte, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli ottengono le migliori prestazioni nei compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'uso di più dati si traduce in migliori prestazioni. In generale, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento continuo, utilizzando il peso e il tokenizzatore di PubMedBERT, addestrato su un sottoinsieme di 4 gigabyte di Nachos, mostra risultati comparabili a quelli ottenuti con Dr. BERT 4 gigabyte da zero, cosa che non accade per il modello basato su Camembert, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema offre migliori prestazioni in nove dei 11 compiti downstream e supera globalmente i risultati del modello generico Camembert. Osserviamo anche che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non si scalano bene. Tutti i modelli pre-addestrati ottenuti da Nachos sono disponibili gratuitamente su Hugging Face e tutti gli script di addestramento sono nel nostro repository GitHub. Quindi, grazie per questa presentazione e non vediamo l'ora di interagire nella sessione di domande e risposte a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 10, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Sean Bing, dottorando presso l'Università di Washington. Oggi presenterò il nostro lavoro che va dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, tracciando le tracce dei pregiudizi politici che portano a modelli NLP ingiusti. Quindi, i modelli linguistici sono addestrati su grandi quantità di dati web. I media politici sono ben rappresentati nei loro dati di pre-addestramento. Secondo un sondaggio del corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, il Huffington Post, ecc., sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente prevenute e potrebbero portare a potenziali problemi di equità nelle applicazioni dei compiti a valle. A questo proposito, proponiamo di indagare il flusso di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, ponendo specificamente le seguenti domande. Prima di tutto, come valutiamo la linea politica dei modelli linguistici e quale ruolo potrebbero avere i dati di pre-addestramento su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diverse linee politiche nei compiti a valle e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Quindi, specificamente, proponiamo prima di tutto di sollecitare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il test della bussola politica. Questo garantisce una valutazione automatica ben fondata nella letteratura scientifica politica. Quindi, alcuni risultati preliminari dimostrano che, prima di tutto, i modelli linguistici hanno diverse tendenze politiche. Occupano tutti e quattro i quadranti della bussola politica. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti e le teorie GPT sono generalmente più socialmente liberali delle teorie BERT e delle sue varianti. In secondo luogo, ci proponiamo di indagare fino a che punto i pregiudizi politici dei modelli linguistici sono effettivamente presi dai dati di addestramento. Quindi, abbiamo condotto un esperimento controllato pre-addestrando ulteriormente i checkpoint dei modelli linguistici su sei diversi corpora di partigiani, separati in notizie e social media, ulteriormente divisi nella loro tendenza politica. Pre-addestrando ulteriormente i modelli linguistici su tali corpora di partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano corrispondentemente. Ad esempio, per Roberta, ulteriormente addestrata sul corpus Reddit di sinistra, possiamo vedere uno spostamento liberale sostanziale in termini di pregiudizi politici. E abbiamo anche cercato di indagare se i modelli linguistici possono cogliere la polarizzazione che è prevalente nella nostra società moderna. Quindi, dividiamo i corpora di pre-addestramento in pre-45° presidente degli Stati Uniti e dopo il 45° presidente degli Stati Uniti. Pre-addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente una tendenza politica che era più lontana dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società. Infine, ma non meno importante, valutiamo i modelli linguistici con diverse tendenze politiche nella rilevazione del discorso d'odio e nella rilevazione delle notizie false, due applicazioni NLP che spesso coinvolgono i modelli linguistici e potrebbero avere implicazioni molto significative. Quindi, vediamo che se indaghiamo la performance per categoria, cioè se separiamo la performance in diverse demografie o tendenze politiche dei media, possiamo vedere uno schema in cui, ad esempio, per la rilevazione del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che prende di mira i gruppi socialmente minoritari, tuttavia sono peggiori nel rilevare il discorso d'odio che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che prende di mira bianchi e uomini, tuttavia sono peggiori nel rilevare il discorso d'odio che prende di mira comunità nere, LGBTQ+ e altre minoranze. Tendenze simili si verificano anche per la rilevazione delle notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dalla loro tendenza politica opposta e viceversa. Mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diverse tendenze politiche danno effettivamente diverse previsioni sugli esempi di discorso d'odio e disinformazione in base alla loro categoria sociale. Ci sono un sacco di altri esempi nell'appendice per evidenziare ulteriormente questo. Questo indica che c'è un problema di equità molto pressante riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere ulteriormente addestrato sul discorso d'odio o sulla disinformazione o altro e implementato su una popolare piattaforma di social media, questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso d'odio che prende di mira i gruppi minoritari potrebbe semplicemente imperversare senza alcun controllo. Quindi, questo ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità derivanti dalle tendenze politiche dei modelli linguistici. Quindi, un po' di discussione. Vorremmo anche sottolineare che abbiamo esposto il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi, se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pre-addestramento ai modelli linguistici fino ai compiti a valle, creando alla fine problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutro e dovrebbe essere mantenuto nei dati di addestramento dei modelli linguistici. Quindi, è un po' come il problema del carrello elettrico. Okay, fantastico. Penso che sia più o meno tutto quello che ho fatto. Cinque per oggi. Grazie per il vostro tempo."}
{"dataset_id": "mcif_v1.0", "sample_id": 11, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Sono Kostov Sina e sono lieto di darvi il benvenuto al nostro intervento sul nostro articolo ACL 2023, \"I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto\". Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy e Athena Williams. In questo lavoro, rivediamo il paradigma delle coppie minime. Il paradigma delle coppie minime valuta essenzialmente i modelli linguistici in base ai giudizi di accettabilità, che possono includere anche la grammaticalità, come Blimp, Syntax Gym, o l'accettabilità in termini di stereotipi, come Crowspairs. In questo paradigma delle coppie minime, il modo tipico di valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi mostrare una frase inaccettabile o ingrammaticale, e la speranza è che il modello attribuisca più probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe. Oggi i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe, quindi è cruciale che valutiamo l'accettabilità dei modelli in tutta la finestra di contesto. E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Questo è l'approccio. Quindi, per simulare queste sequenze più lunghe, rivediamo i dataset stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Ad esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal dataset Blimp, dal caso dell'Adjunct Island. E quello che facciamo è che, per ricreare sequenze più lunghe e che siano accettabili e che abbiano la stessa struttura grammaticale corrispondente, estraiamo frasi grammaticali da Adjunct Island e poi le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento e questo potrebbe essere utilizzato anche per testare l'accettabilità del modello. E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo scenario di non corrispondenza. Qui le frasi provengono ancora da dataset rilevanti, ma non dallo stesso dataset che stiamo valutando. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente irrilevante, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, come se il contesto provenga da un sottoinsieme diverso del dataset o se è completamente irrilevante rispetto alla frase che stiamo esaminando. Quindi, come se la cava il modello? Prima di tutto, guardiamo alle frasi di Wikipedia, che sono completamente irrilevanti rispetto alla coppia di query attuale, e lì troviamo che i giudizi MPP sono per lo più robusti per una lunghezza di contesto arbitraria. Abbiamo aumentato la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT2 e abbiamo visto qui nella linea arancione che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso dataset Blimp o Syntax Gym e lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o inaccettabili. Ma quando abbinamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in Blimp o Syntax Gym, vediamo un aumento massiccio o una diminuzione massiccia del giudizio MPP per il modello a seconda che il prefisso scelto sia accettabile o inaccettabile. Ora, questo effetto aumenta in tutta la lunghezza del contesto e questo probabilmente influenzerà i nuovi modelli linguistici che hanno una grande finestra di contesto. Quindi, perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico? Abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, troviamo che nessuno di questi rumori fa effettivamente cambiare il modello, ovviamente, in termini di come ci mostra la tendenza del giudizio MPP. In pratica, troviamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, il modo in cui la facciamo attualmente con input di frasi brevi e singole, potrebbe non catturare completamente la conoscenza astratta del modello linguistico in tutta la finestra di contesto. Leggete il nostro articolo per ulteriori dettagli sui nostri esperimenti. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 12, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, sono Dawei, uno studente di dottorato presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro, \"Weaker than you think\", un'analisi critica dell'apprendimento supervisionato debole. Questo è un lavoro congiunto con Xiaoyu Shen, Mario Musbach, e Gias Stefan e Diti Clarko. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento supervisionato debole. Nella supervisione debole, non si etichettano manualmente i dati. Invece, si etichettano i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Quando confrontati con le annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali sui dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento supervisionato debole, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali sotto tale rumore dell'etichetta, in modo che i modelli addestrati generalizzino ancora bene. Nei recenti lavori su WSL, quindi WSL sta per apprendimento supervisionato debole, un'affermazione comune è che le persone dicono che addestrano modelli solo sui dati etichettati debolmente e ottengono alte prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un trucco, ovvero le persone danno per scontato che ci sia un set di validazione pulito aggiuntivo disponibile per la selezione del modello. Ci siamo fermati su questo problema, poiché implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato debole. Ma, come un elefante nella stanza, questa necessità viene spesso trascurata. L'affermazione adottata ci porta a porre tre domande di ricerca. Prima, i dati di validazione puliti sono necessari per WSL? O possiamo forse usare un set di validazione rumoroso invece? Secondo, se i dati puliti sono necessari, o se i dati puliti sono obbligatori per WSL per funzionare, allora quante campioni puliti abbiamo bisogno? Infine, dovremmo usare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Prima, scopriamo che, interessantemente, i metodi WSL recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni, come mostrato in questa figura. Se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Questo indica che gli approcci WSL richiedono effettivamente dati etichettati correttamente per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere migliori prestazioni, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere alte prestazioni. Ma questa non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora addestrarci direttamente su di essi otterrà ancora migliori prestazioni. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL. Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto permettendo il continuo fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello Vanilla, chiamato FTW, inizialmente sotto-performa rispetto ai metodi WSL più complicati come Cosine. Tuttavia, se permettiamo il continuo fine-tuning sui campioni puliti, allora FTW si comporta altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni annotati manualmente e puliti per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per i futuri lavori sono le seguenti. Prima, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è fatta su campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con i basi di apprendimento a pochi spari, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è un basi semplice ma forte che dovrebbe essere considerato nei futuri lavori su WSL. Infine, abbiamo reso open source il nostro codice. Puoi trovarlo con il codice QR su questa diapositiva. Sentiti libero di controllarlo. Grazie e buon divertimento alla conferenza."}
{"dataset_id": "mcif_v1.0", "sample_id": 13, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo David Villar e vi fornirò una breve panoramica del documento \"Prompting PaLM from Translation: Assessing Strategies and Performance\". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testi, comprendente 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di elaborazione del linguaggio naturale. In questo lavoro, presentiamo il primo studio sistematico sul prompting dei modelli linguistici di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità di traduzione automatica. Questo comporta l'uso degli ultimi set di test per evitare sovrapposizioni tra i dati di test e i dati di addestramento del modello linguistico. E confrontiamo due sistemi all'avanguardia, ovvero i sistemi con le migliori prestazioni o la valutazione WMT. Utilizziamo metriche di traduzione automatica all'avanguardia e, inoltre, mostriamo anche i risultati delle valutazioni umane basate su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt. Il prompting ha una grande influenza sulle prestazioni dei modelli linguistici di grandi dimensioni per la traduzione. Come possiamo vedere in un semplice esperimento in cui utilizziamo il prompting one-shot e forniamo due prompt diversi per una frase. La maggior parte delle frasi, 516 su 1.000, la differenza osservata è di oltre un punto di BLEU. E questo può arrivare, nei casi estremi, fino a 40 punti di BLEU. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting five-shot, in cui segnaliamo semplicemente ogni frase che forniamo al sistema con la lingua di destinazione. Quindi, in questo esempio, in cui eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono segnate con il simbolo del tedesco e le traduzioni in inglese con il simbolo dell'inglese. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso del prompting five-shot. È cruciale per il prompting zero e one-shot e quando passiamo, come nel nostro caso, al prompting five-shot, c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riassunto dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più disordinati, e i risultati mostrano una migliore prestazione quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM, ma PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di confrontarci con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana, che abbiamo eseguito utilizzando il framework MQM, sono che la fluidità di PaLM è paragonabile a quella dei sistemi all'avanguardia, ma la principale differenza risiede nell'accuratezza. In particolare, l'errore più comune è l'omissione. Quindi sembra che PaLM scelga di produrre una traduzione che suona meglio, a volte eliminando parti della frase di origine che sono irrilevanti nella traduzione. Tuttavia, la categoria di output stilistico per PaLM è inferiore rispetto ai sistemi all'avanguardia, il che è un segnale aggiuntivo che PaLM fornisce un output molto fluido, ma con ancora alcuni problemi di accuratezza. E questo è tutto per questa panoramica molto breve. Per ulteriori dettagli, vi prego di venire alla presentazione completa del documento. Grazie mille."}
{"dataset_id": "mcif_v1.0", "sample_id": 14, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Jin Wei e sono dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video pubblicitario sul documento \"Stai copiando il mio modello? Protezione del copyright dei grandi modelli linguistici per servizi di embedding tramite watermarking backdoor\". Iniziamo con un'introduzione sullo sfondo dei servizi di embedding. Attualmente, i grandi modelli linguistici come TPT, Lama, Palm sono eccezionali nella comprensione e generazione del linguaggio naturale. I servizi di embedding sono uno dei servizi costruiti su grandi modelli linguistici per assistere vari compiti di elaborazione del linguaggio naturale. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, lavori recenti hanno dimostrato che un attaccante potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dei servizi di embedding. Per proteggere il copyright dei servizi di embedding, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contiene il watermark. Il metodo di watermarking deve soddisfare le seguenti proprietà. Prima di tutto, il metodo deve essere applicabile ai servizi di embedding. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. Terzo, il watermark deve essere abbastanza furtivo per l'attaccante, altrimenti l'attaccante potrebbe rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili ai servizi di embedding o mancano di trasferibilità. Pertanto, in questo documento, proponiamo Embedding Marker, che è un metodo di watermarking basato su backdoor applicabile ai servizi di embedding. Ora vi presento i dettagli del nostro Embedding Marker. Embedding Marker contiene due passaggi principali: iniezione del watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di M, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo prima un dataset backdoor e un dataset benigno. Il dataset backdoor contiene frasi in cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di trigger. Quindi il fornitore richiede gli embedding dal servizio del ladro con il dataset. La similarità coseno e L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Calcoliamo la differenza di similarità tra il dataset benigno e il dataset backdoor, definita come delta coseno e delta L2. Allo stesso tempo, applichiamo anche il test KS e utilizziamo il suo valore p come terza metrica. Abbiamo condotto esperimenti su quattro dataset: AG News, Mind, SST2 e ErosBAM. Supponiamo che il fornitore applichi il dataset Wikitext per contare la frequenza delle parole. I risultati su quattro dataset mostrano che il nostro Embedding Marker può avere un'eccellente prestazione di rilevamento mantenendo un'eccellente utilità per i compiti a valle. Abbiamo anche validato la furtività dell'embedding fornito visualizzando l'embedding delle frasi su quattro dataset VOPCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e quelli normali. Questo è tutto, grazie. Benvenuti a discutere con noi."}
{"dataset_id": "mcif_v1.0", "sample_id": 15, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, mi chiamo Ying e io e il mio collega Ziyang presenteremo la nostra ricerca su Multi-Instruct, migliorando l'apprendimento zero-shot multimodale tramite la regolazione delle istruzioni. Con i progressi nei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento riutilizzando modelli linguistici pre-addestrati per diversi compiti downstream in modo parametrico ed efficiente dal punto di vista dei dati. Recentemente, molti studi hanno dimostrato che la regolazione delle istruzioni consente ai modelli linguistici di grandi dimensioni di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sulla regolazione delle istruzioni si è concentrata sul miglioramento delle prestazioni zero-shot nei compiti basati solo sul linguaggio, mentre la visione artificiale e i compiti multimodali sono stati trascurati. Pertanto, in questo lavoro, vogliamo indagare se la regolazione delle istruzioni sui modelli multimodali pre-addestrati possa effettivamente migliorare la generalizzazione ai compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di set di dati di istruzioni tra NLP e multimodale. Esistono più di 1.600 compiti di istruzioni basati solo sul linguaggio. Tuttavia, non esiste un set di compiti di istruzioni multimodali di grandi dimensioni disponibile pubblicamente. Pertanto, questo ci ha motivato a costruire un set di dati di regolazione delle istruzioni multimodali. Qui presentiamo Multi-Instruct, il primo set di dati di benchmark per la regolazione delle istruzioni multimodali che consiste in 62 compiti multimodali diversi, coprendo 10 categorie ampie. Questi compiti sono derivati da 21 set di dati open source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare sulla regolazione delle istruzioni multimodali sul nostro set di dati proposto, prendiamo OFA, un modello multimodale pre-addestrato unificato come nostro modello di base. OFA utilizza un vocabolario unificato per token linguistici, immagini e coordinate di una casella di delimitazione. Qui mostriamo alcuni esempi di istanze dal nostro set di dati Multi-Instruct. Per unificare l'elaborazione di vari tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato unificato di sequenza a sequenza, in cui il testo di input, le immagini, le istruzioni e le caselle di delimitazione sono rappresentati nello stesso spazio di token. Okay, ora parlerò della regolazione delle istruzioni multimodali. Quindi, per il set di dati di addestramento, utilizziamo 53 compiti dal gruppo NAG per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento di buon senso per il test e selezioniamo cinque compiti aggiuntivi dai gruppi VQA e Miscellaneous. Utilizziamo tutte le istanze nella suddivisione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla suddivisione di test di Natural Instruction come compiti non visti per NLP. Quindi utilizziamo un modello OFA pre-addestrato di grandi dimensioni come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei suoi cinque modelli di istruzione. Quindi, durante il test per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Riferiamo la prestazione minima e massima e la deviazione standard della prestazione in tutti e cinque gli esperimenti. Se il compito è un compito di classificazione multimodale, riferiamo l'accuratezza. Se è un compito di generazione multimodale, riferiamo ROUGE-L. Per un compito NLP, riferiamo ROUGE-L. Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente da una leggera variazione nella formulazione dell'istruzione. Ecco i nostri risultati principali. Come possiamo vedere, la regolazione delle istruzioni può migliorare significativamente le prestazioni di OFA nei compiti multimodali non visti. Inoltre, l'apprendimento trasferito dal set di dati di istruzioni naturali può beneficiare la regolazione delle istruzioni. Qui possiamo vedere che man mano che aumenta il numero di compiti, il modello ottiene prestazioni migliori e, allo stesso tempo, una sensibilità inferiore. Quindi abbiamo anche fatto un esperimento. Utilizziamo un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilità. Questo mostra l'effetto di diverse strategie di regolazione fine sulla sensibilità del modello. Come possiamo vedere, tramite l'apprendimento trasferito dal set di dati di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che l'apprendimento trasferito dal set di dati di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul set di dati Natural Instruct. Quindi, in generale, abbiamo proposto il primo set di dati di regolazione delle istruzioni multimodali di grandi dimensioni. Abbiamo migliorato significativamente la capacità zero-shot di OFA e abbiamo esplorato diverse tecniche di apprendimento trasferito e mostrato i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità. Quindi un'altra cosa, stiamo raccogliendo un set di dati di regolazione delle istruzioni multimodali molto più grande con circa 150 compiti di istruzioni di lingua e immagini aggiuntivi e li rilasceremo. Questo è un codice QR per i nostri dati e il modello. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 16, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti. Mi chiamo Justin John della Penn State University. Oggi presenterò il nostro lavoro, Exemplar, parsing semantico cross-lingue in più lingue naturali e rappresentazioni di significato. Quindi, il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti, come SQL e calcolo lambda. E il parsing semantico cross-lingue è il compito di tradurre query in più lingue naturali in più rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, lambda o FunQL, ecc. I modelli di parsing semantico cross-lingue esistenti sono proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono lacune di copertura su certe lingue naturali. Il cinese è mancante e lacune di copertura su certe rappresentazioni di significato. Il calcolo lambda è mancante. O sono valutati solo su un certo modello neurale. Ad esempio, c'è solo un singolo modello per valutare. Quindi, a questo scopo, abbiamo proposto Exemplar. Forniamo un set di dati uniforme, Exemplar, per il parsing semantico cross-lingue in più lingue naturali e rappresentazioni di significato. Contiene 90 set di dati in vari domini, cinque compiti di parsing semantico, otto rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio la nostra benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è tradurre il test. Utilizziamo l'API di Google Translate per tradurre la fonte nella lingua target, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. E, ad esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza traduciamo la query in tedesco utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere l'SQL. E testiamo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua target. Ad esempio, tedesco a tedesco o inglese a inglese. Testiamo anche l'impostazione monolingue few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il modello multilingue, che addestriamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue. E durante l'inferenza, possiamo utilizzare questo modello per tradurre query in tedesco o in cinese, ecc. E consideriamo anche il trasferimento cross-lingue zero-shot e few-shot. Addestriamo su una lingua di origine e trasferiamo a un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query in inglese o la combinazione di query in inglese e tedesco few-shot per addestrare un modello multilingue e prevedere l'output SQL. E troviamo anche molti risultati interessanti. Quindi, riguardo l'analisi dei modelli monolingue, valutiamo su due gruppi di modelli, inclusi encoder PDR, che sta per encoder multilingue pre-addestrati con decoder basati su puntatori, come XLM-R più PDR e BERT più PDR. E valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come MBART e MT5. Abbiamo scoperto che l'encoder-decoder ottiene le migliori prestazioni su tutti e nove i set di dati. E valutiamo su MT5 e XLM-R più PDR su impostazione multilingue. Abbiamo scoperto che l'encoder-decoder o l'encoder PDR possono essere migliorati addestrando in una miscela di varie lingue. E abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni in inglese diminuiscono in sette set di dati e guadagnano solo in tre set di dati. Penso che questo sia noto come maledizione della multilingualità. Confrontiamo anche il divario di prestazioni cross-lingue. In questa figura, la linea blu è il trasferimento cross-lingue few-shot, la linea arancione è il trasferimento cross-lingue zero-shot, mentre la linea verde è l'impostazione monolingue. Abbiamo scoperto che confrontando la linea verde e arancione, abbiamo scoperto che per l'impostazione zero-shot, il divario di prestazioni di trasferimento cross-lingue è significativo. E confrontando la linea blu e arancione, abbiamo scoperto che per l'impostazione few-shot, il divario di trasferimento si accorcia rapidamente. Troviamo anche altre scoperte interessanti. Ad esempio, l'encoder-decoder supera i lavori precedenti o ottiene risultati comparabili. L'addestramento sulla lingua naturale inglese può migliorare significativamente le prestazioni few-shot sulle lingue naturali target. E abbiamo scoperto che i modelli linguistici multilingue come CODA e BLOOM sono ancora inadeguati per i compiti di parsing semantico cross-lingue. In sintesi, costruiamo Exemplar, una benchmark unificata per il parsing semantico cross-lingue con più lingue naturali e rappresentazioni di significato. Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molte scoperte interessanti e altro. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'ascolto."}
{"dataset_id": "mcif_v1.0", "sample_id": 17, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Adam Spirkowski e questo discorso riguarda la struttura di dipendenza della coordinazione. Come potreste sapere, diverse strutture di dipendenza sono assunte da diverse teorie e approcci al corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione Lisa, Bart e Maggie è tale che il primo congiunto è la testa dell'intera struttura coordinata, quindi in questo caso Lisa. Un approccio simile è assunto nella teoria del significato del testo di Igor Mel'čuk, dove anche l'intera struttura coordinata è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici, giusto? Mettono in evidenza uno dei congiunti. Ora, ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio con la congiunzione come testa, assunto nei tre banchi di dipendenza di Praga, dove le strutture coordinate sono guidate dalla congiunzione. Quindi otteniamo dipendenze da \"and\" a tutti i congiunti. E infine, c'è anche un approccio multi-testa che viene utilizzato, ad esempio, nella grammatica delle parole di Hudson, dove, per così dire, tutti i congiunti sono le teste delle strutture coordinate. Quindi otteniamo dipendenze dal governatore, qui \"loves\", a tutti i congiunti separatamente. Questi sono Bart e Maggie. Ora, lo scopo di questo documento è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. Okay, l'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi, in inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani, giusto? Quindi \"March read it yesterday\" va bene perché l'oggetto diretto \"it\" è vicino al verbo, mentre \"March read yesterday it\" è molto peggio, giusto? Perché qui tra il verbo e l'oggetto diretto c'è un avverbio \"yesterday\". Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'avverbio. Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. \"March read this absolutely fascinating book about bees yesterday\" va bene. Invece di \"it\", abbiamo questo lungo NP. Ma va bene anche dire \"March read yesterday this absolutely fascinating book about bees\". Quindi la ragione qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più corte sono preferite. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da \"read\" all'avverbio di lunghezza 7, misurata in parole, e da \"read\" a \"book\" di lunghezza 4. Quindi in totale fa 11. Quando si spostano, quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6, giusto? Quindi invece di 11, 6, molto più corta. Ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro. Okay. Quindi quello che abbiamo fatto, abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedete il documento perché non abbiamo usato le dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono a essere più corti. Quindi \"salt and pepper\" e non \"pepper and salt\" misurati in sillabe. E anche l'osservazione che è stata fatta di passaggio che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo più forte, giusto? Quindi la proporzione è maggiore dei congiunti di sinistra corti. Ma ciò che è nuovo in questo documento è che abbiamo osservato che questa tendenza si verifica solo quando i governatori a sinistra sono assenti, giusto? Quindi i governatori a sinistra in questo esempio, ho visto Bart e Lisa, quindi il governatore è a sinistra. È assente nel secondo esempio, Homer è venuto e ha starnutito. Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno, giusto? Quindi in tali casi, il congiunto di sinistra preferisce essere più corto, tanto più grande è la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, \"left\" governa la coordinazione \"tell and net\", questo effetto scompare. Quindi mostriamo che misurando la lunghezza in caratteri, questa è la prima colonna, in sillabe la colonna centrale e in parole la colonna di destra. Quindi mi concentrerò su quella di destra. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra a essere più corto cresce costantemente con la differenza assoluta in parole. E lo stesso si osserva quando non c'è governatore, come nella coordinazione di frasi, ma quando il governatore è a destra, questa tendenza scompare. E mostriamo nel documento come questo fornisca un argomento contro le strutture asimmetriche di coordinazione come queste due e a favore delle strutture simmetriche come queste due. Quindi vedete il documento per l'accordo completo e gli argomenti, scusate, e parlate con noi durante la sessione poster. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 18, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, mi chiamo Kyle Yen e presenterò il nostro lavoro intitolato \"Quando la traduzione richiede un contesto, un'esplorazione multilinguistica basata sui dati\". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emily Liu, Andre F. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo \"mole\" in questa frase? Beh, se la frase precedente fosse \"le cose potrebbero diventare pericolose se i ministri lo scoprono\", allora \"mole\" si riferisce a una spia. Ma se la frase precedente fosse \"potrebbe essere qualcosa di serio, dottore?\", allora \"mole\" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, cambia anche la sua traduzione. Tuttavia, valutare quanto bene i modelli riescono a tradurre casi come questo è piuttosto difficile. Innanzitutto, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla cura umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Prima, quando la traduzione richiede un contesto? E seconda, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quante informazioni il contesto C fornisce sull'obiettivo Y, dato il sorgente X. Potete pensare a CXMI come al guadagno di informazioni derivante dal dare il contesto al modello. In questo lavoro, estendiamo CXMI a pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un alto P6MI come a quelle che richiedono un contesto per la traduzione. Ora analizziamo le parole con un alto P6MI per cercare schemi tra queste parole. E svolgiamo la nostra analisi sui trascritti di TED Talks che sono stati tradotti dall'inglese in 14 lingue diverse. Svolgiamo la nostra analisi a tre livelli diversi. Prima, guardiamo le parti del discorso che hanno un alto P6MI. E questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un alto P6MI. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi hai bisogno del contesto per determinare se un pronome è duale quando traduci in arabo. E allo stesso modo, troviamo che alcune lingue richiedono anche un contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo gli elementi del vocabolario che hanno un alto P6MI mediato su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come quello qui, dove in cinese hai bisogno del contesto per tradurre i sostantivi propri per assicurarti di usare la stessa traduzione all'interno del documento. E allo stesso modo, troviamo che il contesto è supportato per tradurre nella formalità giusta. E infine, guardiamo diversi token individuali che hanno un alto P6MI. E questo ci permette di identificare fenomeni che non possono essere davvero catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi. Ora usiamo i nostri risultati dall'analisi per progettare una benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno. E chiamiamo il nostro tagger il tagger multilingue consapevole del discorso o Muda. Possiamo anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni del discorso. Poi usiamo il tagger Muda applicando il tagger a un corpus parallelo che vogliamo usare per la valutazione. E applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger Muda ha identificato. E infine, usiamo la nostra benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione automatica a livello di documento. Prima di tutto, quando usiamo metriche a livello di corpus, quindi per BLEU, troviamo che i modelli agnostici del contesto hanno le migliori prestazioni. Ma poi, se usiamo COMET, i modelli consapevoli del contesto hanno le migliori prestazioni. E se usiamo la misura F delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metriche a livello di corpus. Ora usiamo la benchmark Muda per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni del discorso, come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano il contesto su altri fenomeni come l'ellissi, i pronomi e la forma verbale. Quindi questo suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento. Confrontiamo anche diversi sistemi commerciali e la nostra benchmark mostra che DeepL è di solito più accurato di Google Translate per la traduzione a livello di documento. In sintesi, svolgiamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono un contesto. E poi usiamo i nostri risultati per costruire una benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o no, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per l'attenzione. Ci vediamo a Toronto."}
{"dataset_id": "mcif_v1.0", "sample_id": 19, "src_lang": "en", "tgt_lang": "it", "output": "Ciao a tutti, sono Jenny, una studentessa del primo anno di dottorato alla Carnegie Mellon University, e oggi presenterò il mio lavoro, NL Positionality, caratterizzando il design attraverso set di dati e modelli di NLP. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Sante, Ronan Le Bras, Katarina Rynneka e Martin Sapp. Iniziamo immaginando di lavorare per un giornale e di esaminare i commenti sotto un articolo di notizie, cercando di rimuovere contenuti tossici. Potremmo rivolgerci a un'API popolare come Perspective API per la rilevazione della tossicità, e questo funziona molto bene se sei Carl Jones, dove Perspective API è in grado di rilevare correttamente le istanze tossiche. Ma non è questo il caso per Aditya Sharma, dove Perspective API non è così sensibile ai termini offensivi che sono più comuni nei contesti indiani. Questo è un esempio di pregiudizio di design, dove vediamo differenze sistematiche di prestazioni della tecnologia tra le popolazioni. I pregiudizi di design come quello che abbiamo appena visto potrebbero verificarsi a causa della posizione degli studiosi di NLP e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno a causa delle loro demografie, identità ed esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femminili e queer. E come ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero porsi è: i set di dati e i modelli hanno una posizione? E non stiamo cercando di dire che i modelli stessi e i set di dati stessi hanno identità demografiche ed esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre. Quindi lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizione, come lacune culturali nei modelli e nei set di dati, nonché definizioni teoriche della posizione del modello. Tuttavia, questi lavori non guardano davvero al confronto tra gli utenti finali e i set di dati e i modelli stessi. E studiare la posizione del modello e del set di dati è sempre più importante man mano che i compiti di NLP diventano più soggettivi e orientati socialmente. E risulta difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Quindi, per studiare la posizione del set di dati e del modello, confrontiamo effettivamente le annotazioni con gli utenti reali con i set di dati e i modelli esistenti. Lo facciamo attraverso il nostro framework NL Positionality. Il nostro framework funziona in due fasi principali. Il primo passo è re-annotare i set di dati con annotatori diversificati. E optiamo per farlo guardando le demografie degli annotatori dei set di dati originali, perché di solito solo pochi annotatori annotano ogni istanza e perché le demografie vengono raramente raccolte e condivise. E quindi optiamo per re-annotare i dati per ottenere molte annotazioni per istanza e per ottenere un set ricco di dati demografici. Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il punteggio di correlazione di Pearson. E quindi il nostro framework differisce effettivamente dalla letteratura sulla discrepanza degli annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, piuttosto che guardare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori. Il nostro framework è in gran parte abilitato da Lab in the Wild, una piattaforma di crowdsourcing online, un ex collaboratore HCI. E Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversificati rispetto alle piattaforme come MTurk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. E inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo due compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal set di dati Social Chemistry e poi scriveranno quanto è socialmente accettabile una situazione. Successivamente, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'AI e con altri. Quindi confrontiamo queste annotazioni con Social Chemistry, Delphi e GPT-4. Quindi repliciamo una configurazione molto simile per il compito di rilevazione della tossicità e del discorso d'odio, dove leggeranno un'istanza da DynaHate e scriveranno se pensano che sia un'istanza di discorso d'odio. Quindi confrontiamo queste annotazioni con DynaHate, Perspective API, Rewire API, Hate Roberta e GPT-4. Il nostro studio alla fine ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori di 87 paesi. Quindi ora siamo meglio attrezzati per rispondere a chi si allineano di più i set di dati e i modelli di NLP. Scopriamo che c'è una posizione nell'NLP. Ad esempio, scopriamo che i set di dati e i modelli sono più allineati con i paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di GPT-4, scopriamo che è più allineata con la confusione e i paesi di lingua inglese. Scopriamo che DynaHate è anche più allineata con i paesi di lingua inglese. Scopriamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nel compito di accettabilità sociale, scopriamo che è più allineata con le persone con un'istruzione universitaria o con un'istruzione post-laurea. E troviamo lo stesso per DynaHate, dove è più allineata con le persone con un'istruzione universitaria. Tuttavia, quando i modelli e i set di dati sono allineati con popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di ciò è che i set di dati e i modelli sono meno allineati con le persone non binarie rispetto ai loro omologhi maschi e femmine. Lo troviamo nel compito di accettabilità sociale di GPT-4, nonché nell'analisi del compito DynaHate. Quindi, dato che c'è una posizione nell'NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutte le scelte di design rilevanti durante il processo di ricerca. E l'altra è fare ricerca di NLP con la lente del perspectivismo. La nostra terza raccomandazione è costruire set di dati e modelli specializzati all'interno di comunità specifiche. E un buon esempio di questo è l'iniziativa Masakane. Vogliamo sottolineare che l'NLP inclusivo non consiste solo nel far funzionare tutte le tecnologie per tutti. E questo conclude la nostra presentazione, ma se desiderate saperne di più, sentitevi liberi di consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie."}
{"dataset_id": "mcif_v1.0", "sample_id": 20, "src_lang": "en", "tgt_lang": "it", "output": "Ciao, parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità, in cui introduciamo il corpus AltEntities. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Philip Radinsky, Silvia Parity e Annie Lewis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Consideriamo questa domanda alternativa. Intendevi \"Easy on Me\" o \"I Got a Feeling\"? Qui, un utente vuole scegliere tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone, \"Easy on Me\", o la sua posizione, la prima. Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. O le pronunce sono troppo simili tra loro e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di differenze indirette. Ad esempio, la più recente o la canzone che non è energica. Questo è un problema importante nei sistemi di conversazione e anche per la valutazione della comprensione delle entità negli LLM. Non siamo a conoscenza di un dataset pubblico, un dataset pubblico su larga scala per il compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione della folla. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta del dataset enfatizza l'informalità utilizzando una configurazione di completamento di fumetti. Il fumetto ha tre bolle di discorso. Nella prima bolla, Bob dice: \"Ricordi quella canzone che ascoltavamo ieri?\" E con questo, Bob stabilisce il contesto del dialogo. Nella seconda bolla di discorso, Alice dice: \"Intendi 'Easy on Me' o 'I Got a Feeling'?\" che è la domanda alternativa. E nella terza bolla di discorso, Bob usa un riferimento indiretto per selezionare una di queste entità. Ad esempio, la più recente. Forniamo le prime due bolle di discorso automaticamente, ma la terza viene compilata dall'annotatore. La prima bolla di discorso è scelta tra alcuni prompt manuali per dominio. La seconda, che è la domanda alternativa, viene generata come segue. Usiamo sempre un semplice modello. Intendi A o B? Dove A e B sono campionati da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili. Ad esempio, due libri con il nome \"The Return\". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno infobox simili o attributi su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente conoscono le entità. Quindi quello che facciamo è mostrare alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno parte di ogni canzone e leggere su ogni canzone. Ecco, ad esempio, il risultato della ricerca Google per la canzone \"Easy on Me\". Per il dominio delle ricette e dei libri, mostriamo del testo di sfondo da Wikipedia. Per le ricette, mostriamo anche le loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio qui la prima, e descriverle utilizzando tre o cinque espressioni di riferimento indiretto. Ad esempio, quella con la musica al pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, quella senza parole, non quella con il ragazzo di 12 anni, o quella fittizia, o quella che proviene dall'Azerbaigian, e così via. Il corpus AltEntities ha 6.000 domande alternative in tre domini e ha 42.000 espressioni di riferimento indiretto. I risultati con il modello T5X Large sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base esatte degli annotatori, allora l'accuratezza è davvero alta. È intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso a conoscenze di base parzialmente sovrapposte, allora l'accuratezza è compresa tra l'82 e l'87%, che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per il miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro dataset. Grazie."}
