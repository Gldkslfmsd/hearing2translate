{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我叫Assa Farari，我将展示我们的研究论文《使用精细调优变换器架构的FUESHOT TABLAR数据增强》（FUESHOT TABLAR DATA ARCHICHTMENT Using FINE TUNIC TRANSFORMERS ARCHITTURES）。数据科学家分析数据并主要关注操作数据的现有特征，但有时这些特征是有限的。使用另一个数据源进行特征生成可以添加大量信息。我们的研究目标是使用外部来源的文本自动对表格数据进行丰富。假设我们有一个表格数据集和一个知识库。我们需要一个自动的过程，该过程涉及实体链接和文本分析，从知识库的文本中提取新特征。我们框架FAST正是这个自动过程。让我们来看一个例子。一个数据集被输入到FAST中。在这个例子中，数据集是大学数据集，其目标是将大学分类为低排名大学和高排名大学。作为知识库，我们使用维基百科。FEST的第一阶段是实体链接，在这个例子中，每个实体（即大学名称）被链接到知识库中的一个实体，并提取知识库实体的文本并添加到数据集中。在这个例子中，文本是维基百科页面摘要。现在我们需要从检索的文本中生成或提取特征。因此，我们需要一个特征提取阶段，包括文本分析，这是本文的主要创新点，我将在下一张幻灯片深入探讨。特征提取阶段之后是一个特征生成阶段，我们使用提取的特征来生成少量新特征。首先，根据原始数据集的类别数量生成特征。在这个例子中，原始数据集有两个类别，所以首先生成两个新特征。但如果数据集有五个类别，首先生成五个新特征。每个特征表示每个类别的可能性。为了分析文本，我们使用文本分析的当前最先进技术，即基于变换器的语言模型，如BERT、GPT、XLERT等。然而，使用输入数据集训练语言模型是不太可能的。因此，一个简单的方法是目标任务精细调整。所以在特征提取阶段，我们可以下载预训练的语言模型，对语言模型进行精细调整以适应目标数据集，在这个例子中，对语言模型进行精细调整以将文本分类为类别，摘要分类为低或高，接收语言模型的输出，即每个类别的可能性，并将其用作新特征。这个方法的问题是数据集可能包含很少的独特实体、文本。在我们的实验中，几乎一半的数据集包含不到400个样本，最小的数据集训练集包含35个样本。因此，对这些数据集进行语言模型的精细调整将是无效的。但我们可以使用对预分析数据集的先验知识，因为FAST是我们在多个数据集上应用FAST。我们可以使用n-1个数据集来收集n-1个数据集的信息，并在分析n个数据集时使用这些信息。我们建议添加另一个精细调整阶段，即初步的多任务精细调整阶段，我们对语言模型进行n-1个数据集的精细调整，然后执行另一个精细调整阶段，即目标任务精细调整，我们对语言模型进行n个目标数据集的精细调整。多任务精细调整的当前最先进技术称为空DNN（Empty DNN）。Empty DNN维护与训练集中任务数量相等的头，所以在这个例子中，如果训练集中有四个任务，Empty DNN将维护四个头，如图所示，它从训练集中采样一个随机批次，如果随机批次属于例如分类任务，则执行第一个头的向前和反向路径。如果随机批次属于配对排名任务，则执行最后一个头的向前和反向路径。在我们的场景中，表或数据集的类别数量各不相同。因此，有多个任务。MTDNN维护类别数量的头输出层，并且MTDNN需要为新的数据集和新的任务初始化新的头。我们的方法称为任务重述精细调整，在我们的方法中，任务重述精细调整而不是维护多个头，我们将每个数据集重述为一个句子，每个分类问题是一个二元分类任务。让我们来看一个例子。这是我们的输入数据集，由实体、特征、文本和类别组成。我们将任务从将文本分类为低和高重述为将文本、摘要和类别分类为真或假。或者说，我们训练语言模型来分类摘要和类别，如果摘要属于类别与否，所以标签向量在这种情况下总是包含两个类别，这是我们找到或重述精细调整方法的算法。让我们看看完整的框架，一个数据集输入到FAST，然后FAST执行链接阶段，从知识库中提取文本，在这个例子中是维基百科页面的摘要，然后将任务重述为每个分类任务的句子，将语言模型应用于新任务并输出每个类别的可能性。请注意，语言模型已经对n-1个数据集进行了精细调整，使用初步的多任务精细调整。然后我们使用语言模型的输出向量作为新生成特征的数量。为了评估我们的框架，我们使用一个包含17个表的分类数据集，它们在大小、特征、平衡、领域和初始性能方面各不相同。作为知识库，我们使用维基百科。我们设计我们的实验为留一评估，我们对FAST进行16个数据集的训练，并将其应用于第17个数据集。我们还将每个数据集划分为四份，并应用四叉交叉验证。然后我们生成新特征并使用五个评估分类器对其进行评估。我们在实验中使用了基于BERT的架构。这是我们实验的结果。您可以看到，我们将我们的框架与目标数据集精细调整、目标任务精细调整和MTDNN初步精细调整进行了比较，我们的改革精细调整取得了最佳结果和最佳性能，而MTDNN在目标数据集精细调整基础上提高了2%，我们的方法在小数据集上提高了6%，我们可以看到MTDNN的性能下降，初步多任务精细调整阶段的改进降至1.5%，但我们的性能提高到11%，与仅目标任务精细调整相比。总之，FAST实现了从我们实验中的35个样本开始的快速增强。它为所有任务数据集使用一个架构，并保持模型的头。但它添加了三个公式阶段，增强了训练集，并需要一个具有语义意义的目标值，因此我们可以将其输入到语言模型中，并将其用于每个分类问题的句子。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "zh", "output": "大家好。今天我将介绍我们的研究工作《学习归纳推理，地铁问题求解作为复杂区域提取》。我是来自拜登人工智能实验室的Alan，这是与德克萨斯大学奥斯汀分校的Thierry和SUDD的Wayloo的合作研究。首先，我想谈谈我们进行推理的动机。在这里，我们展示了一个多步推理有帮助的例子。这个图表来自POWN论文，他们在融合学习场景下通过提示解决地铁问题。在右边，我们可以看到如果我们只提供问题和答案的例子，可能无法得到正确答案。但如果我们提供更多的推理描述，模型能够预测推理描述并给出正确预测。因此，输出可解释的多步推理是很好的。我们也认为数学问题是评估这种推理能力的一个直接应用。在我们的问题设置中，给定问题，我们需要解决这个问题并得到数值答案。在我们的数据集中，我们也提供了导致特定答案的数学表达式。某些假设也适用于之前的工作。我们假设量的精度是已知的，我们只考虑基本运算符，如加法、减法、乘法、除法和指数。此外，复杂的运算符实际上可以解码为这些基本运算符。\n\n之前的工作在数学问题求解方面可以分为序列到序列模型和序列到树模型。传统的序列到序列模型将表达式转换为特定的序列进行生成，实现起来很简单，并且可以推广到许多不同的复杂问题。但缺点是性能通常不如结构模型，并且预测缺乏可解释性。但由于变压器模型，这个方向仍然很受欢迎。在基于树的模型中，我们将这些表达式结构化为树的形式，并遵循树生成中的前序遍历。在这里，我们不断生成运算符，直到到达叶子节点，即量。优点是它给我们提供了二叉树结构。但实际上相当直观。因为我们首先生成运算符，然后在最后生成量。其次，它也包含一些重复计算。在这里，如果我们看这个表达式，a乘以3加3，实际上生成了两次。但事实上，我们应该重用结果。\n\n在我们提出的方法中，我们希望以逐步和可解释的方式解决这些问题。例如，在这里的第二步，我们可以得到这个除数，即27。我们也可以回顾原始问题以找到相关内容。在这些步骤中，我们得到除数。然后在第三步，我们实际上得到商。在经过这三个步骤后，我们可以重用第二步的结果，然后得到第四步的结果。最后，我们可以得到被除数。在这里，我们实际上直接生成整个表达式，而不是生成单个运算符或量。这使过程更加准确。\n\n在我们的归纳推理系统中，我们首先从问题中呈现的一组量开始，并包括一些常量作为初始状态。表达式表示为EIJOP，其中我们执行从Qi到Qj的运算符，这样的表达式是方向性的。我们也在这里反转减法以表示相反的方向。这与关系提取非常相似。在正式的归纳推理系统中，在时间步t，我们在Qi和Qj对之间应用运算符，然后得到新的表达式。我们将其添加到下一个状态以成为新的量。这个幻灯片实际上可视化了状态的演变，我们不断将表达式添加到当前状态中。\n\n在我们的模型实现中，我们首先使用预训练的网络模型，如BERT或RoBERTa，然后编码句子并得到量表示。一旦我们得到量表示，我们就可以开始进行推理。这里我们展示了一个Q1的例子，以获得Q1的表示，它将被Q2除以，然后乘以Q3。首先，我们得到对表示，基本上是Q1和Q2的连接，然后我们应用一个由运算符参数化的前馈网络。最后，我们得到表达式表示Q1除以Q2。但实际上，在推理阶段，我们也可能得到不正确的表达式。这里，所有可能的表达式等于运算符数量的三倍。好处是我们可以轻松添加约束以控制搜索空间。例如，如果这个表达式不允许，我们可以简单地将其从搜索空间中删除。\n\n在第二步中，我们做同样的事情，但唯一的区别是多了一个量。这个量来自之前计算的表达式。最后，我们可以得到最终表达式Q3乘以Q4。我们也可以看到所有可能表达式的数量与之前步骤不同。这种差异使得很难应用束搜索，因为这两个步骤之间的概率分布不平衡。\n\n训练过程类似于训练序列到序列模型，我们优化每个时间步的损失，这里我们也使用τ来表示我们应该何时终止生成过程。空间与序列到序列不同，因为每个时间步的空间不同，而在传统的序列到序列模型中是词汇表的大小。它还允许我们根据先验知识施加某些约束。\n\n我们在常用的数学问题数据集MAWPS、Math 23K、MathQA和SWAM上进行了实验。这里我们简要展示与之前最佳方法的比较结果。我们表现最佳的变体是RoBERTa归纳推理器。事实上，我们没有使用束搜索，而之前的方法使用了束搜索。最佳方法通常是基于树的模型。总的来说，我们的推理器能够显著超越基于树的模型，但我们可以看到MathQA或SWAM上的绝对数字并不太高。\n\n我们进一步调查了SWAM上的结果。这个数据集具有挑战性，因为作者试图手动添加一些内容来混淆NLP模型，例如添加无关信息和额外量。在我们的预测中，我们发现一些中间值实际上是负数。例如，在这个问题中，我们询问Jake有多少个苹果，但我们有一些额外的信息，如少了十七个投球，Stephen有八个投球，这完全无关。我们的模型做出这样的预测，产生负值。我们观察到这两个表达式实际上有相似性。我们可以通过删除结果为负数以使答案正确来限制搜索空间。我们进一步发现，这种约束对某些模型的改进很大。例如，对于BERT，我们提高了7个点，对于RoBERTa基础模型，我们提高了2个点。更好的语言模型具有更好的语言理解能力，因此这里的数字对于RoBERTa更高，对于BERT更低。\n\n我们还尝试分析这些数据集背后的难度。我们假设未使用的量的数量可以被视为无关信息。在这里，我们可以看到具有未使用量的样本百分比，SWAM数据集具有最大份额。我们还展示了这些样本的整体表现，没有未使用量。整体表现实际上高于具有未使用量的样本。\n\n最后，我们想通过一个错误和修正的例子来展示可解释性。在这里，我们的模型在第一步做出错误预测。我们可以将这个表达式与句子相关联。我们认为这个句子可能在误导模型做出错误预测。种植另外三十五个让模型认为应该是额外的运算符。我们尝试将句子修订为类似于“梨树的数量比苹果树少三十五个”。我们使它传达更准确的语义，以便模型能够正确预测。这项研究展示了可解释预测如何帮助我们理解模型行为。\n\n总结我们的工作，首先我们的模型非常高效，我们能够提供可解释的求解过程，并且可以轻松地将先验知识作为约束纳入其中，以帮助提高性能。最后，潜在机制不仅适用于数学问题求解任务，也适用于涉及多步推理的其他任务。但我们也有某些局限性。如果我们有大量运算符或常量，内存消耗可能会很高。其次，正如提到的，由于不同时间步的概率分布不平衡，应用束搜索策略也相当具有挑战性。这是演讲的结束，欢迎提问。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我叫安托万，来自马斯特里赫特大学。我将与杰里共同展示我们关于新的法定条款检索数据集的工作。法律问题是许多人生活中不可或缺的一部分，但大多数公民对自己的权利和基本法律程序了解甚少。因此，许多无法负担法律专家昂贵协助的弱势公民得不到保护，甚至被剥削。我们的工作旨在通过开发有效的法定条款检索系统来弥合人民与法律之间的鸿沟，为缺乏技能的人提供免费的专业法律帮助服务。\n\n在深入探讨这项工作的核心贡献之前，让我们先描述一下法定条款检索的问题。假设给定一个关于法律事务的简单问题，例如“我违反职业保密会面临什么风险？”模型需要从大量立法中检索所有相关的法定条款。这个信息检索任务本身面临着一系列挑战。首先，它涉及两种语言：问题使用常见自然语言，而法规使用复杂的法律语言。这种语言分布的差异使得系统更难检索相关候选项，因为它间接需要一个内在的解释系统，将自然问题翻译成与法规术语相匹配的法律问题。此外，法定法不是可以独立处理、自行形成完整信息源的堆叠文章，比如新闻或食谱。相反，它是一个结构化的法律条款集合，只有在整体上下文中才有完整的意义，即与相邻条款的补充信息、它们所属的领域和子领域以及它们在法律结构中的位置一起考虑。最后，法定条款不是通常在大多数检索工作中作为检索单位的短段落。在这里，它们是长达六千字的长文档。\n\n自然语言处理（NLP）的最新进展在法律判断预测、自动化合同审查等许多法律任务中激发了巨大兴趣，但法定条款检索由于缺乏大量高质量的标签数据集而基本未被触及。在本工作中，我们提出了一个新的以法国本土公民为中心的数据集，以研究检索模型是否能够在法定条款检索任务中近似法律专家的效率和可靠性。我们的比利时法定条款检索数据集包含超过一千一百个由比利时公民提出的法律问题。这些问题涵盖了从家庭、住房、金钱到工作和社会保障等广泛主题。每个问题都由经验丰富的法官标注，标注的参考来自比利时法律代码的二十二万六千六百多篇法律条款的语料库。\n\n现在让我们谈谈如何收集这个数据集。首先，我们编译了一个庞大的法律条款语料库。我们考虑了三十两个公开可用的比利时法律代码，提取了所有条款以及相应的章节标题。然后，我们收集了带有相关法规参考的法律问题。为此，我们与一家比利时律师事务所合作，该事务所有每年收到约四千封来自比利时公民的电子邮件，寻求关于个人法律问题的建议。我们有幸获得了他们网站的访问权限，他们的经验丰富的法官团队在网站上解答比利时最常见的法律问题。我们收集了数千个问题，这些问题被标注了类别、子类别和相关法规参考。最后，我们根据法规参考过滤了问题，并剔除了参考不是我们所考虑的法律代码中条款的问题。剩余的参考被匹配并转换为我们语料库中相应条款的ID。最终，我们得到了一千一百零八个问题，每个问题都仔细标注了我们二十二万六千六百三十三个法定条款大语料库中相关条款的ID。此外，每个问题还附带了一个主标题，以及法律结构中其子序列标题的连接。这些额外信息在本工作中未被使用，但可能对未来法律信息检索或法律文本分类研究有兴趣。\n\n让我们看看数据集的某些特征。问题长度在五到四十四个字之间，中位数是四十个字。条款要长得多，中位数长度是七十七个字，其中一百四十二个超过一千字。最长的条款长达五千七百九十个字。如前所述，问题涵盖了广泛的主题，大约百分之八十五的问题要么是关于家庭、住房、金钱或司法，而剩余的百分之十五则涉及社会保障、外国人或工作。条款也非常多样化，因为它们来自三十两个不同的比利时法律代码，涵盖了大量非法定条款。在二十二六百三十三个条款中，只有千六百十二个被引用为至少一个问题中的相关条款，这些被引用的条款中大约百分之八十来自民法、司法法、刑事调查法或刑法。同时，三十两个代码中有十八个代码中提到的相关条款少于五个，这可以解释为这些代码对个人及其关注点关注较少。总体而言，这些被引用的条款的中位数引用次数是两个，不到百分之二十五个的条款被引用超过五次。\n\n使用我们的数据集，我们对包括词法和密集架构在内的多种检索方法进行了基准测试。给定一个查询和条款，词法模型通过计算查询项在该条款中的权重之和来为查询-条款对分配分数。我们实验了标准的TF-IDF和BM 25排名函数。这些方法的主要问题是它们只能检索包含查询中关键字的条款。为了克服这个限制，我们实验了一个基于神经网络的架构，该架构能够捕捉查询和条款之间的语义关系。我们使用B编码模型将查询和条款映射到密集向量表示中，并通过它们嵌入之间的相似性计算查询-条款对的相关分数。这些嵌入通常来自词嵌入模型输出的池化操作。首先，我们研究了Siamese B编码器在零样本评估设置中的有效性，这意味着预训练的词嵌入模型直接应用，没有额外的微调。我们实验了上下文独立的文本编码器，即Word2Vec和FastText，以及上下文依赖的嵌入模型，即RoBERTa，更具体地说，是Camembert，一个法语RoBERTa模型。此外，我们在所有数据集上训练了我们自己的Camembert模型Biancoders。请注意，在训练时，我们实验了Biancoder架构的两个变体。Siamese变体使用一个唯一的词嵌入模型，将查询和条款一起映射到一个共享的密集向量空间中。两塔变体使用两个独立的词嵌入模型，分别将查询和条款编码为不同的嵌入空间。我们实验了均值、最大值和CLS池化以及点积和余弦计算相似性。以下是我们在测试集上的基准结果，上面是上述词法方法，中间是Siamese B编码器在零样本设置下的评估，下面是微调的B编码器。总体而言，微调的B编码器显著优于所有其他基准。两塔模型在百分之百的召回率上优于其Siamese变体，但在其他指标上表现相似。虽然BM 25明显低于训练后的Biancoder，但其表现表明它仍然是特定领域检索的强基准。\n\n关于Siamese Biancoder的零样本评估，我们发现直接使用预训练的Camembert模型的嵌入，而没有针对信息检索任务进行优化，会产生糟糕的结果，这与先前的发现一致。基于Word2Vec的Biancoder显著优于基于FastText和Bird的模型，这表明预训练的词级嵌入可能比字符级或子词级嵌入更适合该任务，当直接使用时。尽管这些结果很有前景，但与熟练的法律专家相比，他们最终能够检索出任何问题的全部相关条款，从而获得满分，这些结果表明还有大量的改进空间。\n\n最后，让我们讨论这两个数据集的局限性。首先，条款语料库仅限于从所考虑的三十两个比利时法律代码中收集的条款，这并不能涵盖整个比利时法律，因为来自法令、指令和条例的条款缺失。在数据集构建期间，对这些未收集的条款的所有引用都被忽略，导致一些问题最终只有最初相关条款的一小部分。这种信息丢失意味着剩余相关条款中的答案可能不完整，尽管它们仍然完全适用。其次，我们应该注意到并非所有法律问题都能仅用法规来回答。例如，“如果房客制造过多噪音，我能驱逐他们吗？”可能在法定法中没有详细的答案，量化允许驱逐的特定噪音阈值。相反，房东可能更应该依赖判例法，寻找与他们当前情况相似的先例。例如，房客每周举办两次派对，直到凌晨2点。因此，一些问题比其他问题更适合法定条款检索任务，而最不适合的领域仍有待确定。我们希望这项工作能够激发对开发实用且可靠的法定条款检索模型的兴趣，这些模型可以帮助改善所有人的司法公正。您可以在以下链接查看我们的论文、数据集和代码。谢谢。"}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我们很高兴地介绍我们关于VALS的工作，这是一个独立于任务的基准，旨在测试视觉和语言模型对特定语言现象的敏感度。为什么我们要花精力建立这个基准呢？近年来，我们见证了基于转换器的视觉和语言模型的爆发式增长，这些模型在大量图像文本对上进行了预训练。这些模型在视觉和语言任务上不断突破现状，如视觉问答、视觉常识推理、图像检索、短语定位等。虽然这些任务特定基准上的准确率在不断提高，但我们真的了解这些模型实际上学到了什么吗？当一个视觉和语言模型为这张图片和这句话的匹配给出高分，为另一对给出低分时，它理解了什么？视觉和语言模型是否关注正确的内容，还是像之前的研究所示，它们受到偏见的影响？为了更好地了解这一点，我们提出了一个更独立于任务的方向，并引入了VALS，它测试视觉和语言模型对影响语言和视觉模式的特定语言现象的敏感度。我们针对存在性、复数、计数、空间关系、动作和实体共指等方面。\n\n那么，如何测试视觉和语言模型是否捕捉到了这些现象呢？我们采用了一种称为“挫败（foiling）”的方法，该方法之前仅应用于视觉和语言模型中的名词短语，由Ravi Shakar及其合作者提出，以及我们之前在计数方面的工作。挫败的基本原理是，我们取一张图片的标题，并通过更改标题使其不再描述图片来生成一个“挫败者（foil）”。我们通过关注六个特定方面来进行短语更改：存在性、复数、计数、空间关系、动作和实体共指。每个方面可以由一个或多个工具组成，如果我们发现创建挫败实例的有趣方式不止一种。例如，在动作方面，我们有两个工具，一个是改变动作动词，另一个是交换动作的参与者。计数和共指也有多个工具。我们创建这些挫败者，确保它们不能描述图片，同时是语法上正确且有效的句子。这并不容易，因为一个挫败的标题可能比原始标题更不符合实际。例如，虽然不是不可能的，但从统计学上讲，植物切割男人比男人切割植物更不常见，大型视觉和语言模型可能会捕捉到这一点。因此，为了获得有效的挫败者，我们必须采取行动。首先，我们利用强大的语言模型提出挫败者。其次，我们使用自然语言推理（NLI）来过滤掉可能仍然描述图片的挫败者，因为在创建挫败者时，我们需要确保它们不能描述图片。为了自动测试这一点，我们应用自然语言推理，其逻辑如下：我们将图片视为前提，其标题视为隐含的假设。此外，我们将标题视为前提，挫败者作为其假设。如果NLI模型预测挫败者与标题相矛盾或中立，我们将其视为有效挫败者的指标。如果挫败者被隐含为标题，那么它不能是一个好的挫败者，因为根据传递性，它将给出关于图片的真实描述，我们会过滤掉这些挫败者。但这个过程并不完美。它只是有效挫败者的指标，因此，作为生成有效挫败者的第三个措施，我们雇用了人类标注者来验证VALS中使用数据。\n\n经过过滤和人类评估，我们拥有了本表中描述的那样多的测试实例。请注意，VALS不提供任何训练数据，而只提供测试数据，因为它是一个零样本测试基准。它的设计目的是利用视觉和语言模型在预训练后的现有能力。微调只会使模型利用数据中的人工产物或统计偏见。我们都知道这些模型喜欢作弊和走捷径。正如我们所说，我们有兴趣评估视觉和语言模型在预训练后的能力。我们在VALS上实验了五个视觉和语言模型，即CLIP、Wilbert、Wilbert Kelvin 1和Visual Bert。我们两个最重要的评估指标是模型在将图像句子对分类为标题和挫败者方面的准确率。也许对本视频更相关的是，我们将展示我们更宽松的指标——配对准确率，它测量正确的图像文本对的对齐分数是否大于其挫败对的分数。有关更多指标及其结果，请参阅我们的论文。配对准确率的结果与我们从其他指标获得的结果一致，即Wilbert 12 in 1在零样本性能方面表现最佳，其次是Wilbert、Alex Mert、CLIP，最后是Visual Bird。值得注意的是，以单个物体为中心的工具，如存在性和名词短语，几乎被Wilbert 12 in 1解决，这表明模型能够识别命名物体及其在图像中的存在。然而，在我们的对抗性挫败设置中，没有一个剩余的方面能被可靠地解决。我们从复数和计数工具中看到，视觉和语言模型在区分单个物体与多个物体的引用或在图像中对它们进行计数方面存在困难。关系工具显示，它们在正确分类图像中物体之间的命名空间关系方面存在困难。它们也难以区分动作并识别其参与者，即使有可信度偏见的支持，正如动作工具所示。从共指工具中，我们发现使用代词跟踪图像中同一物体的多个引用对视觉和语言模型来说也很困难。\n\n作为合理性检查和有趣的实验，我们还对两个仅基于文本的模型GPT 1和GPT 2进行了基准测试，以评估VALS是否能被这些单模态模型解决，我们计算了正确标题和挫败标题的困惑度（没有图像），并预测困惑度较低的条目。如果挫败标题的困惑度更高，我们将其视为一个迹象，表明挫败标题可能受到可信度偏见或其他语言偏见的影响。有趣的是，在某些情况下，仅基于文本的GPT模型比视觉和语言模型更好地捕捉了世界的可信度。\n\n总之，VALS是一个使用语言构造的视角来帮助社区改进视觉和语言模型的基准。我们的实验表明，视觉和语言模型能够很好地识别图像中命名物体的存在，正如存在性工具所示，但当被迫遵守语言指标时，它们在视觉场景中难以将它们相互依存和关系具体化。我们真诚地鼓励社区使用VALS来衡量在语言与视觉对齐方面的进展。更进一步，VALS可以作为对数据集的间接评估，因为模型可以在训练或微调前后的不同阶段进行评估，以了解数据集是否有助于模型在VALS测试的任何方面取得改进。如果您感兴趣，请在GitHub上查看VALS数据，如果您有任何问题，请随时与我们联系。"}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "zh", "output": "你好，我是来自东京大学的Kamisara。我将展示一篇题为《R和SAM：自动风险非持续时间的大规模数据集通过提交日志总结》的论文。我将按以下顺序进行解释。首先，我将介绍我们在此研究中致力于的自动风险非持续时间。发布说明是一份技术文档，总结了软件产品每个版本发布时分发的更改。图中显示的是GBUJS库版本2.6.4的发布说明。这些节点在开源开发中发挥着重要作用，但手动准备起来非常耗时。因此，能够自动生成高质量的发布节点将非常有用。\n\n我将参考两项关于自动发布说明生成的先前研究。第一项是名为Arena的系统，于2014年发布。它采取基于规则的方法，例如，使用更改提取器从发布之间的差异中提取核心库更改和文档更改，最后将它们结合起来。该系统的最显著特征是右上角的议题提取器，它必须链接到Jira，议题跟踪系统，并且只能应用于使用Jira的项目。换句话说，它不能用于GitHub上的许多项目。\n\n第二项是Griff，最近在2020年宣布。它在互联网上可用，可以通过PIP存储。该系统具有简单的基于学习的文本分类模型，并为每个输入的提交消息输出五个标签之一，如功能或错误修复。图中是一个示例用法，返回更正或错误修复标签。训练数据集相当小，大约5000个，将在下面描述的实验中展示。文本分类模型的性能并不高。\n\n我介绍了两项相关研究，但存在适用性有限和数据资源稀缺的问题。我们的论文解决了这两个问题，并自动生成高质量的发布说明。对于适用性有限的问题，我们提出了一种仅使用提交消息作为输入的高质量分类器总结方法。这种拟议的方法可以用于所有英语仓库。对于数据资源稀缺的第二个问题，我们构建了一个RNSAM数据集，由大约82,000条数据组成，通过使用GitHub API从公共GitHub仓库收集数据。\n\n接下来，我将描述我们的数据集。这里是一个数据示例。左侧是提交消息，右侧是发布节点。发布节点被分为改进、错误修复等级别。我们设置了一个任务，将提交消息作为输入，并输出分级的发布节点。这可以被视为一个总结任务。我们预定义了四个级别：功能、改进、错误修复、弃用、可删除和破坏性更改。这些是基于先前研究和其他因素设定的。底部右下角的发布节点是从底部左侧显示的发布节点中提取的。此时，需要检测四个预先设定的标签，但标签并不总是与每个仓库一致。例如，改进标签包括改进、增强、优化等。我们为每个记法变体准备了研究标签的词汇表。使用它来检测RISNOD类，并根据类纠正RIS文本，使其成为RISNOD句子。\n\n接下来是提交消息。提交消息并不与每个RIS相关联。如下图所示，如果当前RIS是波斯语2.5到19，我们需要识别前一个RISP 2.5到18并获取其diff。这有点繁琐，仅仅获取RIS列表并查看前后并不足以。我们创建了一个启发式匹配蓝，以获取前一个和下一个页面。最终的描述分析，7,200个仓库和82,000条数据被纠正。此外，发布节点标记的平均数量是63，这对于总结任务来说相当高。独特标记的数量也相当大，达到8,830,000。这是由于仓库中发现的独特成本和方法名称的数量很大。\n\n接下来，我将解释拟议的方法。横向提取和抽象总结模型由两个新模块组成：使用bot或codebot的分类器和使用bot的生成器。首先，GEAS使用分类器将每个提交消息分类为五个原因类：功能、改进、错误修复、弃用和其他。分类为其他的消息被丢弃。然后，GEAS应用生成器独立地对四个标签文档进行处理，并为每个类生成原因说明。在这个任务中，提交消息和原因说明之间的直接对应关系是未知的。因此，为了训练类线，我们使用每个提交消息的前十个字符为每个输入提交消息分配两个标签。我们通过两种不同方法对类线的抽象总结方法进行建模。第一种模型，我们称之为GAS单，由单个seq-to-seq网络组成，生成单个长列表说明文本，将输入提交消息进行连接。输出文本可以根据特殊类特定端点符号按类进行分割。第二种方法，我们称之为GSMAUC，由四个不同的seq-to-seq网络组成，每个网络对应一个列表说明类。\n\n好，让我解释实验。比较了五种方法：GS、GS单、GS行、Rustling和先前研究的Grief。关于流产，在某些情况下，这些说明以多句输出。由于计算句子数量为零很困难，因此它们被空格组合，被视为一个长句。当系统输出短句时，系统会受到惩罚。这种惩罚导致实验中描述的下一个结果中流产值降低。最后，我们还计算了特异度，因为Rouge和Brew无法计算，如果发布节点为空。高特异度意味着模型在发布节点假设为空的情况下正确输出为空文本。\n\n以下是结果。由于数据集包含电子邮件地址、哈希值等，我们还消除了不包括它们的绿色数据集。GAS和GAS获得了Rouge错误分数，比基线高出10多分。然而，在绿色测试集中，拟议方法与基线的分数差距跃升至20多分。这些结果表明GAS和GAS显著有效。GAS比GAS获得了更好的流产分数，表明将分类器和生成器结合起来，使用伪多训练分类器是有效的。GAS的高覆盖率可能是因为分类器可以专注于为每个类选择相关的提交消息。它倾向于产生比它单个更高的规则，表明为这些非类独立开发不同的视角总结模型也是有效的。\n\n这里是错误分析。它方法倾向于输出比人类参考句子短的句子。在右边的图中，参考句子有三个或四个句子，而它只有一个。这种模型不情愿的原因是，在训练数据中，只有33%的句子存在于功能标签中，40%在改进标签中。此外，GS方法在没有额外信息的情况下无法生成准确的原因说明。右上方的示例是非常混乱的提交消息的示例，如果不参考相应的并行请求或问题，则无法生成完整句子。下面的示例显示输入的两个提交消息相关，应该组合为一个句子，但它没有做到这一点。\n\n最后，一个结论。我们构建了一个新的自动原因生成数据集。我们还形成了输入提交消息并对其进行总结的任务，使其适用于所有用英语撰写的项目。我们的实验表明，拟议的方法生成比基线更少的噪声原因，覆盖率更高。请在GitHub上查看我们的数据集。谢谢。"}
