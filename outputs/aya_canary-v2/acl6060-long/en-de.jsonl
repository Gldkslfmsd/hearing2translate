{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Assa Farari und ich werde unseren Artikel FUESHOT TABLAR DATA ARCHICHTMENT MIT FINE TUNIC TRANSFORMERS ARCHITTECTURES präsentieren. Datenwissenschaftler analysieren Daten und konzentrieren sich hauptsächlich auf die Manipulation der vorhandenen Datenmerkmale, aber manchmal sind diese Merkmale begrenzt. Die Merkmalsgenerierung unter Verwendung einer anderen Datenquelle kann erhebliche Informationen hinzufügen. Unser Forschungsziel ist die automatische Bereicherung tabellarischer Daten mit Hilfe externer Quellen freier Texte. Nehmen wir an, wir haben einen tabellarischen Datensatz und eine Wissensbasis. Wir benötigen einen automatischen Prozess, der Entity Linking und Textanalyse umfasst, um neue Merkmale aus dem freien Text der Wissensbasis zu extrahieren. Unser Framework FAST ist genau dieser automatische Prozess. Schauen wir uns ein Beispiel an. Ein Datensatz wird in FAST eingespeist. In diesem Beispiel ist der Datensatz ein Universitätsdatensatz, dessen Ziel es ist, Universitäten in niedrig eingestufte Universitäten und hoch eingestufte Universitäten zu klassifizieren. Als Wissensbasis verwenden wir Wikipedia. Die erste Phase von FAST ist das Entity Linking, bei dem jede Entität, in diesem Beispiel der Universitätsname, mit einer Entität innerhalb der Wissensbasis verknüpft wird, und der Text der Entitäten der Wissensbasis wird extrahiert und dem Datensatz hinzugefügt. In diesem Beispiel ist der Text der Abstract der Wikipedia-Seite. Nun müssen wir Merkmale aus dem abgerufenen Text generieren oder extrahieren. Wir benötigen daher eine Merkmalsextraktionsphase, die Textanalyse umfasst, und dies ist die Hauptneuheit dieses Artikels, auf die ich in den nächsten Folien eingehen werde. Nach der Merkmalsextraktionsphase folgt eine Merkmalsgenerierungsphase, in der wir die extrahierten Merkmale verwenden, um eine kleine Anzahl neuer Merkmale zu generieren. Zuerst generieren wir Merkmale in der Anzahl der Klassen des ursprünglichen Datensatzes. In diesem Beispiel hat der ursprüngliche Datensatz zwei Klassen, also generieren wir zuerst zwei neue Merkmale. Wenn der Datensatz jedoch fünf Klassen hat, generieren wir zuerst fünf neue Merkmale. Jedes Merkmal stellt die Wahrscheinlichkeit für jede Klasse dar. Um den Text zu analysieren, verwenden wir den aktuellen Stand der Technik in der Textanalyse, der auf Transformator-basierten Sprachmodellen wie BERT, GPT, XLERT usw. basiert. Es ist jedoch unwahrscheinlich, dass wir Sprachmodelle mit den Eingabedatensätzen trainieren können. Ein naiver Ansatz wäre daher eine Zielaufgabefeinabstimmung. In der Merkmalsextraktionsphase können wir also ein vorab trainiertes Sprachmodell herunterladen, das Sprachmodell über den Ziel-Datensatz feinabstimmen – in diesem Beispiel, um das Sprachmodell zur Klassifizierung von Texten in Klassen, Abstracts in Klassen, niedrig oder hoch, zu feinabstimmen. Wir erhalten die Ausgabe des Sprachmodells, die die Wahrscheinlichkeit für jede Klasse darstellt, und verwenden sie als neue Merkmale. Das Problem bei diesem Ansatz ist, dass Datensätze möglicherweise nur wenige eindeutige Entitäten oder Texte enthalten. In unserem Experiment enthalten fast die Hälfte der Datensätze weniger als 400 Beispiele, und der kleinste Datensatz hat in seinem Trainingsdatensatz 35 Beispiele. Daher wäre es ineffektiv, ein Sprachmodell über diesen Datensatz zu feinabstimmen. Aber wir können auf vorheriges Wissen über vorab analysierte Datensätze zurückgreifen, da FAST auf mehreren Datensätzen angewendet wird. Wir können die n-1 Datensätze verwenden, um Informationen über die n-1 Datensätze zu sammeln und diese Informationen verwenden, wenn wir den n-ten Datensatz analysieren. Was wir vorschlagen, ist, eine weitere Feinabstimmungsphase hinzuzufügen, eine vorläufige mehrstufige Feinabstimmungsphase, in der wir das Sprachmodell über n-1 Datensätze feinabstimmen und dann eine weitere Feinabstimmungsphase ausführen, die eine Zielaufgabefeinabstimmung ist, wenn wir das Sprachmodell über den n-ten Ziel-Datensatz feinabstimmen. Der Stand der Technik in der mehrstufigen Feinabstimmung ist das sogenannte Empty DNN. Empty DNN unterhält Köpfe in der Anzahl der Aufgaben im Trainingsdatensatz, sodass in diesem Beispiel, in dem es vier Aufgaben im Trainingsdatensatz gibt, Empty DNN vier Köpfe unterhält, wie Sie im Bild sehen können. Es wählt einen zufälligen Batch aus dem Trainingsdatensatz aus, und wenn der zufällige Batch beispielsweise zu den Klassifizierungsaufgaben Sing und Selton gehört, führt es die Vorwärts- und Rückwärtswege durch den ersten Kopf aus. Wenn der zufällige Batch zu Paarvergleichs-Ranking-Aufgaben gehört, führt es die Vorwärts- und Rückwärtswege durch den letzten Kopf aus. In unserem Szenario variiert die Anzahl der Klassen in einem Tabellen- oder Datensatz, sodass es viele Aufgaben gibt. MTDNN unterhält eine Anzahl von Klassen-Ausgabeschichten und muss zusätzlich für einen neuen Datensatz mit einer neuen Aufgabe neue Köpfe initialisieren. Unser Ansatz, den wir Task-Reformulierungs-Feinabstimmung nennen, besteht darin, anstelle der Verwaltung mehrerer Köpfe jeden Datensatz in einen Satz pro Klassifizierungsaufgabe umzuformulieren, was eine Zweiklassen-Aufgabe ist. Schauen wir uns ein Beispiel an. Hier ist unser Eingabedatensatz, der aus Entitäten, Merkmalen, Texten und Klassen besteht. Und wir reformulieren die Aufgabe, den Text in niedrig und hoch zu klassifizieren, in eine Aufgabe, den Text, den Abstract und die Klasse in wahr oder falsch zu klassifizieren. Mit anderen Worten, wir trainieren das Sprachmodell, um Abstract und Klasse zu klassifizieren, ob das Abstract zur Klasse gehört oder nicht, sodass der Label-Vektor in diesem Fall immer aus zwei Klassen besteht. Dies ist der Algorithmus für unseren reformulierten Feinabstimmungsansatz. Schauen wir uns das gesamte Framework an: Ein Datensatz wird in FAST eingespeist, und dann führt FAST die Verknüpfungsphase aus, extrahiert den Text aus der Wissensbasis, der in diesem Beispiel der Abstract der Wikipedia-Seite ist, reformuliert dann die Aufgabe in Aufgaben mit einem Satz pro Klassifizierung, wendet das Sprachmodell auf die neue Aufgabe an und gibt die Wahrscheinlichkeit für jede Klasse aus. Beachten Sie, dass das Sprachmodell bereits über n-1 Datensätze unter Verwendung einer vorläufigen mehrstufigen Feinabstimmung feinabgestimmt wurde. Dann verwenden wir den Ausgabevektor des Sprachmodells als neu generiertes Merkmal in der Anzahl der Klassen. Um unser Framework zu bewerten, verwenden wir einen 17-tabelarischen Klassifikationsdatensatz, der in Bezug auf Größe, Merkmale, Ausgewogenheit, Domäne und anfängliche Leistung variiert. Als Wissensbasis verwenden wir Wikipedia. Wir entwerfen unser Experiment als Leave-One-Out-Bewertung, bei der wir FAST über 16 Datensätze trainieren und es auf den 17. Datensatz anwenden. Wir teilen auch jeden Datensatz in vier Faltungen auf und führen eine Kreuzvalidierung durch. Dann generieren wir die neuen Merkmale und bewerten sie mit fünf Evaluierungs-Klassifikatoren. In unserem Experiment verwenden wir eine BERT-basierte Architektur. Hier sind die Ergebnisse unseres Experiments. Sie können sehen, dass wir unser Framework mit der Ziel-Datensatz-Feinabstimmung, der Zielaufgaben-Feinabstimmung und der MTDNN-vorläufigen Feinabstimmung vergleichen, und unsere reformulierte Feinabstimmung erreicht das beste Ergebnis, die beste Leistung, während MTDNN eine Verbesserung von 2 % gegenüber der Ziel-Datensatz-Feinabstimmung erzielte. Unser Ansatz erreichte eine Verbesserung von 6 %, und wenn wir uns die kleinen Datensätze ansehen, können wir sehen, dass die Leistung von MTDNN abnimmt und die Verbesserung der vorläufigen mehrstufigen Feinabstimmungsphase auf 1,5 % sinkt, während unsere Leistung auf 11 % im Vergleich zur alleinigen Zielaufgaben-Feinabstimmung ansteigt. Zusammenfassend ermöglicht FAST eine schnelle Bereicherung ab 35 Beispielen in unserem Experiment. Es verwendet eine Architektur für alle Aufgaben und Datensätze und behält den Kopf des Modells bei. Es fügt jedoch drei Formulierungsphasen hinzu, erweitert den Trainingsdatensatz und erfordert einen Zielwert mit semantischer Bedeutung, der in das Sprachmodell eingespeist und in der Aufgabe mit einem Satz pro Klassifizierung verwendet werden kann. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "de", "output": "Hallo zusammen. Heute werde ich unsere Forschungsarbeit \"Learning to Reason Deductively\" vorstellen, wobei ich das Metro-Problemlösen als komplexe Regionsextraktion betrachte. Ich bin Alan vom Biden's AI Lab, und dies ist eine gemeinsame Arbeit mit Thierry von der University of Texas at Austin und Wayloo von SUDD. Zunächst möchte ich über unsere Motivation für das Schlussfolgern sprechen. Hier zeigen wir ein Beispiel, in dem mehrstufiges Schlussfolgern hilfreich ist. Diese Abbildung stammt aus dem POWN-Papier, in dem durch Prompting das Metro-Problem in einem Fusion-Lernszenario gelöst wird. Auf der linken Seite sehen wir, dass wir bei der Bereitstellung von Beispielen mit lediglich Fragen und Antworten möglicherweise nicht die korrekten Antworten erhalten. Aber wenn wir eine detailliertere Schlussfolgerungsbeschreibung geben, kann das Modell diese beschreiben und auch hier eine korrekte Vorhersage treffen. Es ist also vorteilhaft, interpretierbares, mehrstufiges Schlussfolgern als Ausgabe zu haben. Wir sind der Meinung, dass das Method-Problem eine direkte Anwendung zur Bewertung solcher Schlussfolgerungsfähigkeiten darstellt. In unserem Problemsetup müssen wir, basierend auf den Fragen, diese lösen und numerische Antworten erhalten. In unseren Datensätzen wird uns auch die mathematische Expression bereitgestellt, die zu dieser bestimmten Antwort führt. Bestimmte Annahmen gelten auch wie in früheren Arbeiten. Wir gehen davon aus, dass die Genauigkeit der Mengen bekannt ist, und wir berücksichtigen nur grundlegende Operatoren wie Addition, Subtraktion, Multiplikation, Division und Exponentiation. Komplexere Operatoren können tatsächlich in diese grundlegenden Operatoren dekomponiert werden. Frühere Arbeiten zur Lösung des Method-Problems lassen sich in Sequenz-zu-Sequenz- und Sequenz-zu-Baum-Modelle einteilen. Das traditionelle Sequenz-zu-Sequenz-Modell wandelt die Expression in eine spezifische Sequenz für die Generierung um, was recht einfach zu implementieren ist und sich auf viele verschiedene komplexe Probleme verallgemeinern lässt. Die Nachteile sind jedoch, dass die Leistung im Allgemeinen nicht besser ist als bei strukturierten Modellen und dass die Vorhersagen nicht interpretierbar sind. Diese Richtung ist dennoch sehr beliebt, aufgrund des Transformer-Modells. Bei baumbasierten Modellen strukturieren wir diese Ausdrücke in Form eines Baums und folgen einer präordenierten Traversierung bei der Baumgenerierung. Hier generieren wir weiterhin Operatoren, bis wir die Blätter erreichen, die die Mengen darstellen. Der Vorteil dabei ist, dass wir eine binäre Baumstruktur erhalten. Es ist jedoch kontraintuitiv, da wir zunächst den Operator und dann am Ende die Mengen generieren. Ein weiterer Punkt ist, dass es auch zu wiederholten Berechnungen kommt. Wenn wir uns diesen Ausdruck \"a mal 3 plus 3\" ansehen, wird er tatsächlich zweimal generiert. Tatsächlich sollten wir jedoch das Ergebnis wiederverwenden. In unserem vorgeschlagenen Ansatz möchten wir diese Probleme schrittweise und interpretierbar lösen. Zum Beispiel können wir hier im zweiten Schritt diesen Divisor, der 27 ist, erhalten. Wir können auch auf die ursprüngliche Frage zurückgreifen, um relevante Inhalte zu finden. In diesen Schritten erhalten wir die Divisoren. Und dann in diesem dritten Schritt erhalten wir tatsächlich den Quotienten. Nach diesen drei Schritten können wir die Ergebnisse aus dem zweiten Schritt wiederverwenden und dann die Ergebnisse des vierten Schritts erhalten. Schließlich können wir die Dividenden erhalten. Hier generieren wir den gesamten Ausdruck direkt, anstatt einzelne Operatoren oder Mengen zu generieren. Dies macht den Prozess genauer. In unserem deduktiven System beginnen wir mit einer Reihe von Mengen, die in den Fragen präsentiert werden, und schließen auch einige Konstanten als anfängliche Zustände ein. Die Expression wird durch EIJOP dargestellt, wobei wir Operatoren von Qi bis Qj ausführen, und diese Expression ist gerichtet. Wir haben auch eine umgekehrte Subtraktion, um die entgegengesetzte Richtung darzustellen. Dies ist ähnlich wie bei der Relationsextraktion. In einem formalen deduktiven System wenden wir am Zeit Schritt t den Operator zwischen dem Qi- und Qj-Paar an und erhalten so eine neue Expression. Wir fügen sie zu den nächsten Zuständen hinzu, um eine neue Menge zu erhalten. Diese Folie visualisiert die Evolution der Zustände, wobei wir weiterhin Expressions zum aktuellen Zustand hinzufügen. Bei der Implementierung unserer Modelle verwenden wir zunächst ein vortrainiertes Netzwerkmodell, das Birds oder Robots sein kann, und kodieren dann einen Satz, um diese Mengenrepräsentationen zu erhalten. Sobald wir die Mengenrepräsentationen haben, können wir mit der Inferenz beginnen. Hier zeigen wir ein Beispiel, wie wir die Repräsentation für Q1 erhalten, indem wir Q1 durch Q2 teilen und dann mit Q3 multiplizieren. Zuerst erhalten wir die Paarrepräsentation, die im Wesentlichen nur die Verkettung zwischen Q1 und Q2 ist, und wenden dann ein Feedforward-Netzwerk an, das durch den Operator parametrisiert wird. Schließlich erhalten wir die Expressionsrepräsentation Q1 geteilt durch Q2. In der Praxis können wir während der Inferenzphase jedoch auch falsche Expressions erhalten. Hier ist die Anzahl aller möglichen Expressions gleich dreimal der Anzahl der Operatoren. Der Vorteil ist, dass wir leicht Einschränkungen hinzufügen können, um diesen Suchraum zu kontrollieren. Wenn diese Expression beispielsweise nicht zulässig ist, können wir sie einfach aus unserem Suchraum entfernen. Im zweiten Schritt tun wir das Gleiche, aber der einzige Unterschied besteht darin, dass eine weitere Menge hinzukommt. Diese Menge stammt aus der zuvor berechneten Expression. Schließlich erhalten wir diese endgültige Expression Q3 mal Q4. Wir können auch sehen, dass die Anzahl aller möglichen Expressions sich von dem vorherigen Schritt unterscheidet. Dieser Unterschied macht es schwierig, Beam Search anzuwenden, da die Wahrscheinlichkeitsverteilung zwischen diesen beiden Schritten unausgeglichen ist. Das Trainingsverfahren ähnelt dem Training eines Sequenz-zu-Sequenz-Modells, bei dem wir den Verlust bei jedem Zeit Schritt optimieren. Hier verwenden wir auch Tau, um anzugeben, wann wir diesen Generierungsprozess beenden sollten. Der Raum unterscheidet sich von dem Sequenz-zu-Sequenz-Modell, da er sich bei jedem Zeit Schritt unterscheidet, während es beim traditionellen Sequenz-zu-Sequenz-Modell die Anzahl des Vokabulars ist. Es ermöglicht uns auch, bestimmte Einschränkungen aus vorherigem Wissen zu übernehmen. Wir führen Experimente mit den häufig verwendeten Method-Problem-Datensätzen MAWPS, Math 23K, MathQA und SWAM durch. Hier zeigen wir kurz die Ergebnisse im Vergleich zu den vorherigen besten Ansätzen. Unsere beste Variante ist der Roberta Deductive Reasoner. Im Gegensatz zu früheren Ansätzen, die Beam Search verwendeten, verwenden wir dies nicht. Die besten Ansätze sind oft baumbasierte Modelle. Insgesamt kann unser Reasoner diese baumbasierten Modelle erheblich übertreffen, aber die absoluten Werte bei MathQA oder SWAM sind nicht besonders hoch. Wir untersuchen die Ergebnisse auf SWAM weiter. Dieser Datensatz ist anspruchsvoll, da der Autor versucht hat, etwas hinzuzufügen, um NLP-Modelle zu verwirren, wie z. B. das Hinzufügen irrelevanter Informationen und zusätzlicher Mengen. In unseren Vorhersagen stellen wir fest, dass einige Zwischenwerte tatsächlich negativ sind. In dieser Frage beispielsweise fragen wir, wie viele Äpfel Jake hat, aber es gibt zusätzliche Informationen wie \"siebenzehn weniger Würfe\" und \"Stephen hat acht Würfe\", die völlig irrelevant sind. Unser Modell macht Vorhersagen wie diese, die negative Werte erzeugen. Wir beobachten, dass diese beiden Expressions eine Ähnlichkeit aufweisen. Wir können diesen Suchraum tatsächlich einschränken, indem wir Ergebnisse entfernen, die negativ sind, um die Antwort korrekt zu machen. Wir stellen fest, dass diese Einschränkung die Leistung einiger Modelle erheblich verbessert. Für Birds verbessern wir beispielsweise um sieben Punkte, und für das Roberta-Basismodell verbessern wir um zwei Punkte. Bessere Sprachmodelle haben eine bessere Sprachverständnisfähigkeit, daher ist die Zahl hier für Roberta höher und für Birds niedriger. Wir versuchen auch, die Schwierigkeit hinter diesem Datensatz zu analysieren. Wir gehen davon aus, dass die Anzahl nicht verwendeter Mengen als irrelevante Informationen betrachtet werden kann. Hier sehen wir den Prozentsatz der Proben mit nicht verwendeten Mengen und der SWAM-Datensatz hat den größten Anteil. Wir zeigen auch die Gesamtleistung für Proben ohne nicht verwendete Mengen. Die Gesamtleistung ist tatsächlich höher als die Gesamtleistung. Aber bei Proben mit nicht verwendeten Mengen ist die Leistung deutlich schlechter als die Gesamtleistung. Bei MAWPS haben wir nicht viele Fälle mit nicht verwendeten Mengen, daher ignoriere ich diesen Teil. Abschließend möchten wir die Interpretierbarkeit durch ein Beispiel mit Absturz und Teilnahme demonstrieren. Hier macht unser Modell in dem ersten Schritt eine falsche Vorhersage. Wir können diese Expression mit dem Satz hier korrelieren. Wir denken, dass dieser Satz das Modell zu einer falschen Vorhersage verleitet. Das Pflanzen von weiteren 35 macht das Modell glauben, dass es zusätzliche Operatoren sein sollten. Wir versuchen, den Satz zu revidieren, damit er etwas wie \"die Anzahl der Birnbäume ist 35 weniger als die der Apfelbäume\" lautet. Wir formulieren es um, um genauere Semantik zu vermitteln, sodass das Modell die Vorhersage korrekt machen kann. Diese Studie zeigt, wie interpretierbare Vorhersagen uns helfen, das Verhalten des Modells zu verstehen. Zusammenfassend ist unser Modell sehr effizient, wir können ein interpretierbares Lösungsprozedere bereitstellen und leicht vorheriges Wissen als Einschränkung einbeziehen, was die Leistung verbessern kann. Die zugrunde liegende Mechanik gilt nicht nur für Netzwerkproblemlösungsaufgaben, sondern auch für andere Aufgaben, die mehrstufiges Schlussfolgern beinhalten. Wir haben jedoch auch bestimmte Einschränkungen. Bei einer großen Anzahl von Operatoren oder Konstanten kann der Speicherverbrauch recht hoch sein. Und da die Wahrscheinlichkeitsverteilung in verschiedenen Zeit Schritten unausgeglichen ist, ist es auch eine Herausforderung, eine Beam-Search-Strategie anzuwenden. Das war es mit dem Vortrag, und Fragen sind willkommen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Antoine und ich komme von der Universität Maastricht. Ich werde meine gemeinsame Arbeit mit Jerry präsentieren, die sich mit einem neuen Datensatz für die Rechtsartikel-Rückgewinnung befasst. Rechtliche Fragen sind ein integraler Bestandteil des Lebens vieler Menschen, aber die Mehrheit der Bürger hat wenig bis gar kein Wissen über ihre Rechte und grundlegenden rechtlichen Prozesse. Als Folge bleiben viele schutzbedürftige Bürger, die sich die kostspielige Hilfe eines Rechtsexperten nicht leisten können, ungeschützt oder werden sogar ausgenutzt. Ziel dieser Arbeit ist es, die Lücke zwischen den Menschen und dem Recht zu schließen, indem ein effektives Rückgewinnungssystem für Rechtsartikel entwickelt wird. Ein solches System könnte einen kostenlosen professionellen Rechtsbeistand für ungeübte Personen bereitstellen.\n\nBevor wir zur Hauptbeitrag dieser Arbeit kommen, beschreiben wir zunächst das Problem der Rechtsartikel-Rückgewinnung. Bei einer einfachen Frage zu einem Rechtsfall, wie z.B. \"Welche Risiken drohen mir, wenn ich die berufliche Schweigepflicht verletze?\", ist ein Modell erforderlich, das alle relevanten Rechtsartikel aus einem großen Gesetzeskorpus extrahiert. Diese Informationsrückgewinnungsaufgabe bringt ihre eigenen Herausforderungen mit sich. Erstens befasst sie sich mit zwei Arten von Sprache: natürlicher Sprache für die Fragen und komplexer juristischer Sprache für die Statuten. Dieser Unterschied in der Sprachverteilung erschwert es einem System, relevante Kandidaten zu finden, da es indirekt ein inhärentes Interpretationssystem erfordert, das eine natürliche Frage in eine rechtliche Frage übersetzen kann, die der Terminologie der Statuten entspricht.\n\nZudem ist das Statutrecht keine Sammlung unabhängiger Artikel, die als vollständige Informationsquelle für sich stehen können, wie z.B. Nachrichten oder Rezepte. Stattdessen handelt es sich um eine strukturierte Sammlung rechtlicher Bestimmungen, die nur in ihrer Gesamtheit Sinn ergeben, also zusammen mit den ergänzenden Informationen aus den benachbarten Artikeln, den Feldern und Unterfeldern, zu denen sie gehören, und ihrer Position in der Struktur des Gesetzes. Schließlich sind die Rechtsartikel nicht kurze Absätze, die üblicherweise die typische Rückgewinnungseinheit in den meisten Rückgewinnungsarbeiten darstellen. Hier handelt es sich um lange Dokumente, die bis zu sechstausend Wörter umfassen können.\n\nDie jüngsten Fortschritte im Bereich der NLP haben ein großes Interesse an vielen rechtlichen Aufgaben geweckt, wie z.B. der Vorhersage rechtlicher Urteile oder der automatisierten Vertragsprüfung, aber die Rechtsartikel-Rückgewinnung ist aufgrund des Mangels an großen und hochwertigen beschrifteten Datensätzen weitgehend unberührt geblieben. In dieser Arbeit stellen wir einen neuen, auf französische Staatsbürger ausgerichteten Datensatz vor, um zu untersuchen, ob Rückgewinnungsmodelle die Effizienz und Zuverlässigkeit eines Rechtsexperten für die Aufgabe der Rechtsartikel-Rückgewinnung annähern können.\n\nUnsere belgischen Rechtsartikel-Rückgewinnungsdatensätze umfassen mehr als eintausendhundert Rechtsfragen, die von belgischen Bürgern gestellt wurden. Diese Fragen decken ein breites Themenspektrum ab, von Familie, Wohnen und Geld bis hin zu Arbeit und Sozialversicherung. Jede Frage wurde von erfahrenen Juristen mit Verweisen auf relevante Artikel aus einem Korpus von mehr als zweiundzwanzigtausendsechshundert Rechtsartikeln aus belgischen Gesetzbüchern beschriftet.\n\nLassen Sie uns nun darüber sprechen, wie wir diesen Datensatz erstellt haben. Zunächst stellten wir einen großen Korpus von Rechtsartikeln zusammen. Wir berücksichtigten dreißig zwei öffentlich zugängliche belgische Gesetzbücher und extrahierten alle Artikel sowie die entsprechenden Abschnittüberschriften. Anschließend sammelten wir Rechtsfragen mit Verweisen auf relevante Statuten. Dazu arbeiteten wir mit einer belgischen Anwaltskanzlei zusammen, die jedes Jahr etwa viertausend E-Mails von belgischen Bürgern erhält, die Rat zu einer persönlichen Rechtsangelegenheit suchen. Wir hatten das Glück, Zugang zu ihrer Website zu erhalten, auf der ihr Team erfahrener Juristen die häufigsten Rechtsfragen der Belgier beantwortet. Wir sammelten Tausende von Fragen, die mit Kategorien, Unterkategorien und rechtlichen Verweisen auf relevante Statuten versehen waren. Schließlich überprüften wir die rechtlichen Verweise und filterten die Fragen heraus, deren Verweise keine Artikel aus einem der von uns berücksichtigten Gesetzbücher waren. Die verbleibenden Verweise wurden abgeglichen und in die entsprechenden Artikel-IDs aus unserem Korpus umgewandelt. Am Ende hatten wir eintausendeinhundertacht Fragen, die jeweils sorgfältig mit den IDs der relevanten Artikel aus unserem großen Korpus von zweiundzwanzigtausendsechshundertdreiunddreißig Rechtsartikeln beschriftet waren. Darüber hinaus enthält jede Frage eine Hauptkategorie, die durch Verkettung ihrer Unterkategorien in der Struktur des Gesetzes gebildet wird. Diese zusätzlichen Informationen werden in der vorliegenden Arbeit nicht verwendet, könnten aber für zukünftige Forschungen im Bereich der Rechtsinformationsrückgewinnung oder der Klassifizierung rechtlicher Texte von Interesse sein.\n\nLassen Sie uns einige Merkmale unseres Datensatzes betrachten. Die Fragen umfassen fünf bis vierundvierzig Wörter, mit einem Median von vierzig Wörtern. Die Artikel sind viel länger, mit einem Median von siebzigsieben Wörtern, und einhundertvierundzwanzig von ihnen überschreiten eintausend Wörter. Die längsten Artikel sind bis zu fünftausendsiebshundertsiebenundneunzig Wörter lang. Wie bereits erwähnt, decken die Fragen ein breites Themenspektrum ab, wobei etwa achtundachtzig Prozent von ihnen sich mit Familie, Wohnen, Geld oder Gerechtigkeit befassen, während die restlichen fünfzehn Prozent die Sozialversicherung, Ausländer oder Arbeit betreffen. Auch die Artikel sind sehr vielfältig, da sie aus dreißig zwei verschiedenen belgischen Gesetzbüchern stammen, die eine große Anzahl von Artikeln umfassen. Von den 22.633 Artikeln werden nur 1.612 als relevant für mindestens eine Frage im Datensatz bezeichnet, und etwa achtzig Prozent dieser zitierten Artikel stammen entweder aus dem Bürgerlichen Gesetzbuch, dem Gerichtsverfassungsgesetz, dem Strafprozessgesetzbuch oder dem Strafgesetzbuch. Währenddessen haben 18 der 32 Gesetzbücher weniger als 5 Artikel, die als relevant für mindestens eine Frage bezeichnet werden, was darauf zurückzuführen sein kann, dass sich diese Gesetzbücher weniger auf Einzelpersonen und ihre Anliegen konzentrieren. Insgesamt beträgt die mittlere Zitierzahl für diese zitierten Artikel zwei, und weniger als fünfundzwanzig Prozent von ihnen werden mehr als fünfmal zitiert.\n\nMit unseren Datensätzen haben wir mehrere Rückgewinnungsansätze, einschließlich lexikalischer und dichter Architekturen, bewertet. Bei einer Abfrage in einem Artikel weist ein lexikalisches Modell der Abfrage-Artikel-Paarung einen Score zu, indem es die Summe der Gewichte jedes dieser Begriffe in diesem Artikel über die Abfragetermine berechnet. Wir experimentieren mit den Standard-TFIDF- und BM-25-Rangfunktionen. Das Hauptproblem dieser Ansätze besteht darin, dass sie nur Artikel zurückgewinnen können, die Schlüsselwörter enthalten, die in der Abfrage vorhanden sind. Um diese Einschränkung zu überwinden, experimentieren wir mit einer auf Neuronen basierenden Architektur, die semantische Beziehungen zwischen Abfragen und Artikeln erfassen kann. Wir verwenden ein B-Encoder-Modell, das Abfragen und Artikel in dichte Vektorrepräsentationen abbildet und einen Relevanzscore zwischen einer Abfrage-Artikel-Paarung durch die Ähnlichkeit ihrer Einbettungen berechnet. Diese Einbettungen resultieren typischerweise aus einer Pooling-Operation auf der Ausgabe eines Wort-Einbettungsmodells.\n\nZunächst untersuchen wir die Effektivität von Siamese B-Encodern in einem Zero-Shot-Bewertungs-Setup, was bedeutet, dass vorab trainierte Wort-Einbettungsmodelle direkt ohne zusätzliche Feinabstimmung angewendet werden. Wir experimentieren mit kontextunabhängigen Text-Encodern, nämlich Word2Vec und FastText, sowie kontextabhängigen Einbettungsmodellen, nämlich RoBERTa und spezifischer Camembert, einem französischen RoBERTa-Modell. Darüber hinaus trainieren wir unser eigenes Camembert-basiertes Biancoder-Modell auf allen Datensätzen. Es sei darauf hingewiesen, dass wir bei der Ausbildung zwei Varianten der Biancoder-Architektur untersuchen: Siamese, das ein einziges Wort-Einbettungsmodell verwendet, das die Abfrage und den Artikel gemeinsam in einen gemeinsamen dichten Vektorraum abbildet, und Two-Tower, das zwei unabhängige Wort-Einbettungsmodelle verwendet, die die Abfrage und den Artikel getrennt in verschiedene Einbettungsräume abbilden. Wir experimentieren mit Mittelwert-, Maximal- und CLS-Pooling sowie Punktprodukt und Kosinus für die Berechnung von Ähnlichkeiten.\n\nHier sind die Ergebnisse unserer Baseline-Tests auf den Testdatensätzen mit den oben genannten lexikalischen Methoden, den Siamese B-Encodern in einem Zero-Shot-Setup in der Mitte und den feinabgestimmten B-Encodern darunter. Insgesamt übertreffen die feinabgestimmten B-Encoder alle anderen Baseline-Ansätze deutlich. Das Two-Tower-Modell verbessert seine Siamese-Variante in Bezug auf die Rückrufquote bei hundert, zeigt sich aber in den anderen Metriken ähnlich. Obwohl BM-25 im Vergleich zum trainierten Biancoder deutlich schlechter abschneidet, deutet seine Leistung darauf hin, dass es immer noch eine starke Baseline für die domänenspezifische Rückgewinnung ist.\n\nIn Bezug auf die Zero-Shot-Bewertung der Siamese Biancoder stellen wir fest, dass die direkte Verwendung der Einbettungen eines vorab trainierten Camembert-Modells ohne Optimierung für die Informationsrückgewinnungsaufgabe zu schlechten Ergebnissen führt, was mit früheren Erkenntnissen übereinstimmt. Und das auf Word2Vec basierende Biancoder-Modell übertrifft die FastText- und Bird-basierten Modelle deutlich, was darauf hindeutet, dass vorab trainierte Wort-Ebene-Einbettungen für diese Aufgabe möglicherweise besser geeignet sind als Zeichen-Ebene- oder Unterwort-Ebene-Einbettungen, wenn sie direkt verwendet werden.\n\nObwohl vielversprechend, deuten diese Ergebnisse auf reichlich Möglichkeiten für Verbesserungen hin, verglichen mit einem geschickten Rechtsexperten, der letztendlich alle relevanten Artikel zu jeder Frage abrufen und damit perfekte Ergebnisse erzielen kann.\n\nAbschließend wollen wir zwei Einschränkungen unserer Datensätze diskutieren. Erstens ist der Korpus der Artikel auf die aus den dreißig zwei berücksichtigten belgischen Gesetzbüchern gesammelten beschränkt, was nicht das gesamte belgische Recht abdeckt, da Artikel aus Verordnungen, Richtlinien und Verordnungen fehlen. Während der Erstellung des Datensatzes wurden alle Verweise auf diese nicht gesammelten Artikel ignoriert, was dazu führt, dass einige Fragen nur einen Bruchteil der anfänglichen Anzahl relevanter Artikel haben. Dieser Informationsverlust bedeutet, dass die Antworten, die in den verbleibenden relevanten Artikeln enthalten sind, unvollständig sein könnten, obwohl sie immer noch völlig angemessen sind.\n\nZweitens sollten wir beachten, dass nicht alle Rechtsfragen allein mit Statuten beantwortet werden können. Beispielsweise könnte die Frage \"Kann ich meine Mieter rauswerfen, wenn sie zu viel Lärm machen?\" innerhalb des Statutrechts keine detaillierte Antwort haben, die einen spezifischen Lärmschwellenwert für eine Räumung quantifiziert. Stattdessen sollte sich der Vermieter wahrscheinlich eher auf die Rechtsprechung stützen und Präzedenzfälle finden, die seiner aktuellen Situation ähnlich sind. Beispielsweise macht der Mieter zweimal pro Woche bis 2 Uhr morgens eine Party. Daher eignen sich einige Fragen besser für die Aufgabe der Rechtsartikel-Rückgewinnung als andere, und der Bereich der weniger geeigneten Fragen bleibt zu bestimmen.\n\nWir hoffen, dass unsere Arbeit das Interesse an der Entwicklung praktischer und zuverlässiger Rechtsartikel-Rückgewinnungsmodelle weckt, die dazu beitragen können, den Zugang zur Gerechtigkeit für alle zu verbessern. Sie können unseren Artikel, Datensatz und Code unter den folgenden Links einsehen. Vielen Dank."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, wir freuen uns, unsere Arbeit zu VALS vorzustellen, einem aufgabenunabhängigen Benchmark, der dazu dient, Vision- und Sprachmodelle mit spezifischen linguistischen Phänomenen zu testen. Warum haben wir uns die Mühe gemacht, diesen Benchmark zu erstellen? In den letzten Jahren haben wir eine Explosion von transformerbasierten Vision- und Sprachmodellen erlebt, die mit großen Mengen an Bild-Text-Paaren vorabtrainiert wurden. Jedes dieser Modelle verbessert den Stand der Technik bei aufgabenbezogenen Vision- und Sprachaufgaben wie visueller Fragebeantwortung, visuellem gesundem Menschenverstand, Bildabruf und Phrasenverankerung. Die Genauigkeiten bei diesen aufgabenbezogenen Benchmarks steigen stetig an, aber wissen wir, was die Modelle tatsächlich gelernt haben? Was versteht ein Vision- und Sprach-Transformer, wenn er für dieses Bild und diesen Satz eine hohe Übereinstimmung bewertet und für diesen anderen eine niedrige Bewertung abgibt? Konzentrieren sich Vision- und Sprachmodelle auf das Richtige oder auf Verzerrungen, wie vorherige Arbeiten gezeigt haben? Um mehr Licht auf diese Aspekte zu werfen, schlagen wir eine eher aufgabenunabhängige Richtung vor und stellen VALS vor, das die Sensitivität von Vision- und Sprachmodellen gegenüber spezifischen linguistischen Phänomenen testet, die sowohl die sprachliche als auch die visuelle Modalität betreffen. Wir konzentrieren uns auf Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz.\n\nAber wie testen wir, ob Vision- und Sprachmodelle diese Phänomene erfasst haben? Foiling, eine Methode, die zuvor nur für Vision- und Sprachmodelle für Nominalphrasen von Ravi Shakar und Mitarbeitern und für Zählen von uns in früheren Arbeiten angewendet wurde. Foiling bedeutet im Grunde, dass wir die Bildunterschrift eines Bildes nehmen und eine Fälschung erstellen, indem wir die Unterschrift so verändern, dass sie das Bild nicht mehr beschreibt. Und wir führen diese Phrasenänderungen durch, indem wir uns auf sechs spezifische Bereiche konzentrieren: Existenz, Pluralität, Zählen, räumliche Beziehungen, Handlungen und Entitäts-Coreferenz. Jeder Bereich kann aus einem oder mehreren Instrumenten bestehen, falls wir mehr als eine interessante Möglichkeit gefunden haben, Foil-Instanzen zu erstellen. Beispielsweise haben wir im Bereich Handlungen zwei Instrumente: eines, bei dem das Handlungsverb durch ein anderes Handlungsverb ersetzt wird, und eines, bei dem die Akteure ausgetauscht werden. Zählen und Coreferenz sind ebenfalls Bereiche mit mehr als einem Instrument.\n\nUm diese Fälschungen zu erstellen, stellen wir sicher, dass sie das Bild nicht beschreiben, grammatikalisch korrekt und ansonsten gültige Sätze sind. Dies ist nicht einfach, da eine gefälschte Bildunterschrift weniger wahrscheinlich sein kann als die ursprüngliche Bildunterschrift. Es ist zwar nicht unmöglich, aber statistisch gesehen ist es weniger wahrscheinlich, dass Pflanzen einen Mann schneiden als dass ein Mann Pflanzen schneidet, und große Vision- und Sprachmodelle könnten diese Verzerrung erkennen. Daher müssen wir Maßnahmen ergreifen, um gültige Fälschungen zu erhalten. Erstens nutzen wir starke Sprachmodelle, um Fälschungen vorzuschlagen. Zweitens verwenden wir natürliche Sprachinferenz (NLI), um Fälschungen auszufiltern, die das Bild möglicherweise immer noch beschreiben, da wir beim Erstellen von Fälschungen sicherstellen müssen, dass sie das Bild nicht beschreiben. Um dies automatisch zu testen, wenden wir NLI mit der folgenden Logik an: Wir betrachten ein Bild als Prämisse und seine Bildunterschrift als implizierte Hypothese. Darüber hinaus betrachten wir die Bildunterschrift als Prämisse und die Fälschung als Hypothese. Wenn ein NLI-Modell vorhersagt, dass die Fälschung die Bildunterschrift widerspricht oder neutral ist, nehmen wir dies als Hinweis auf eine gültige Fälschung. Wenn sie von der Bildunterschrift impliziert wird, kann sie keine gute Fälschung sein, da sie durch Transitivität eine wahrheitsgemäße Beschreibung des Bildes liefern würde, und wir filtern diese Fälschungen aus.\n\nDieses Verfahren ist jedoch nicht perfekt. Es ist nur ein Hinweis auf gültige Fälschungen. Daher setzen wir als dritte Maßnahme zur Erstellung gültiger Fälschungen menschliche Annotatoren ein, um die in VALS verwendeten Daten zu validieren. Nach dem Filtern und der menschlichen Bewertung verfügen wir über so viele Testinstanzen wie in dieser Tabelle beschrieben. Beachten Sie, dass VALS keine Trainingsdaten liefert, sondern nur Testdaten, da es sich um einen Zero-Shot-Test-Benchmark handelt. Es ist darauf ausgelegt, die bestehenden Fähigkeiten von Vision- und Sprachmodellen nach dem Vorabtraining zu nutzen. Feinabstimmung würde es den Modellen nur ermöglichen, Artefakte oder statistische Verzerrungen in den Daten auszunutzen. Und wir wissen alle, dass diese Modelle gerne schummeln und Abkürzungen nehmen. Wie bereits erwähnt, sind wir daran interessiert, zu bewerten, über welche Fähigkeiten Vision- und Sprachmodelle nach dem Vorabtraining verfügen.\n\nWir experimentieren mit fünf Vision- und Sprachmodellen mit VALS, nämlich CLIP, WiLBERT, WiLBERT-KELVIN-1 und VisualBERT. Zwei unserer wichtigsten Bewertungsmetriken sind die Genauigkeit der Modelle bei der Klassifizierung von Bild-Satz-Paaren in Bildunterschriften und Fälschungen. Für dieses Video relevanter ist unsere permissivere Metrik, die paarweise Genauigkeit, die misst, ob die Bild-Satz-Ausrichtung für das korrekte Bild-Text-Paar größer ist als für sein gefälschtes Paar. Weitere Metriken und Ergebnisse finden Sie in unserem Papier. Die Ergebnisse mit paarweiser Genauigkeit sind hier gezeigt und stimmen mit den Ergebnissen überein, die wir mit den anderen Metriken erhalten haben: Die beste Zero-Shot-Leistung wird von WiLBERT-12-1 erreicht, gefolgt von WiLBERT, ALEXMERT, CLIP und schließlich VisualBERT.\n\nEs ist bemerkenswert, dass Instrumente, die sich auf einzelne Objekte wie Existenz und Nominalphrasen konzentrieren, von WiLBERT-12-1 fast gelöst werden, was darauf hinweist, dass Modelle in der Lage sind, benannte Objekte und ihre Anwesenheit in Bildern zu identifizieren. Keiner der verbleibenden Bereiche kann jedoch in unseren adversären Foiling-Einstellungen zuverlässig gelöst werden. Wir sehen bei den Instrumenten Pluralität und Zählen, dass Vision- und Sprachmodelle Schwierigkeiten haben, Referenzen auf einzelne oder mehrere Objekte zu unterscheiden oder sie in einem Bild zu zählen. Der Bereich Beziehungen zeigt, dass sie Schwierigkeiten haben, eine benannte räumliche Beziehung zwischen Objekten in einem Bild korrekt zu klassifizieren. Sie haben auch Schwierigkeiten, Handlungen zu unterscheiden und ihre Teilnehmer zu identifizieren, auch wenn sie durch Plausibilitätsverzerrungen unterstützt werden, wie wir im Bereich Handlungen sehen. Aus dem Coreferenz-Bereich geht hervor, dass es für Vision- und Sprachmodelle schwierig ist, mehrere Referenzen auf dasselbe Objekt in einem Bild mithilfe von Pronomen zu verfolgen.\n\nAls Plausibilitätsprüfung und weil es ein interessantes Experiment ist, benchmarken wir auch zwei Text-only-Modelle, GPT-1 und GPT-2, um zu bewerten, ob VALS von diesen unimodalen Modellen gelöst werden kann, indem wir die Perplexität der korrekten und gefälschten Bildunterschrift (ohne Bild) berechnen und den Eintrag mit der niedrigsten Perplexität vorhersagen. Wenn die Perplexität für die Fälschung höher ist, nehmen wir dies als Hinweis darauf, dass die gefälschte Bildunterschrift möglicherweise unter Plausibilitätsverzerrung oder anderen linguistischen Verzerrungen leidet. Interessanterweise haben die Text-only-GPT-Modelle in einigen Fällen die Plausibilität der Welt besser erfasst als die Vision- und Sprachmodelle.\n\nZusammenfassend ist VALS ein Benchmark, der die linguistischen Konstrukte als Linse verwendet, um der Gemeinschaft zu helfen, Vision- und Sprachmodelle durch die harte Prüfung ihrer visuellen Verankerungsfähigkeiten zu verbessern. Unsere Experimente zeigen, dass Vision- und Sprachmodelle benannte Objekte und ihre Anwesenheit in Bildern gut identifizieren können, wie der Existenz-Bereich zeigt, aber Schwierigkeiten haben, ihre gegenseitige Abhängigkeit und Beziehungen in visuellen Szenen zu verankern, wenn sie gezwungen sind, linguistische Indikatoren zu respektieren. Wir möchten die Gemeinschaft wirklich ermutigen, VALS zur Messung des Fortschritts bei der Sprachverankerung mit Vision- und Sprachmodellen zu verwenden. Und noch mehr: VALS könnte als indirekte Bewertung von Datensätzen verwendet werden, da Modelle vor und nach dem Training oder der Feinabstimmung bewertet werden könnten, um zu sehen, ob ein Datensatz den Modellen hilft, sich in einem der von VALS getesteten Aspekte zu verbessern. Wenn Sie interessiert sind, werfen Sie einen Blick auf die VALS-Daten auf GitHub, und zögern Sie nicht, uns bei Fragen zu kontaktieren."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "de", "output": "Hallo, mein Name ist Kamisara von der Universität Tokio. Ich werde einen Artikel mit dem Titel „R und SAM, ein umfangreicher Datensatz für die automatische Risiko- und nicht-Dauer-Ermittlung über Commit-Log-Summarisierung“ vorstellen. Ich werde wie folgt vorgehen. Zunächst werde ich das automatische Risiko und die nicht-Dauer vorstellen, an dem wir in dieser Forschung arbeiten. Eine Versionsnotiz ist ein technisches Dokument, das die Änderungen zusammenfasst, die mit jeder Version eines Softwareprodukts verteilt werden. Das Bild zeigt die Versionsnotizen für Version 2.6.4 der GBUJS-Bibliothek. Diese Knoten spielen in der Open-Source-Entwicklung eine wichtige Rolle, aber ihre manuelle Erstellung ist zeitaufwändig. Daher wäre es sehr nützlich, hochwertige Versionsknoten automatisch generieren zu können. Ich werde auf zwei frühere Forschungen zur automatischen Erstellung von Versionsnotizen eingehen. Die erste ist ein System namens Arena, das 2014 veröffentlicht wurde. Es verfolgt einen regelbasierten Ansatz, beispielsweise unter Verwendung eines Änderungsextraktors, um die wesentlichen Unterschiede zwischen Bibliotheksänderungen und Dokumentänderungen aus den Unterschieden zwischen den Versionen zu extrahieren und sie schließlich zu kombinieren. Das auffälligste Merkmal dieses Systems ist der Issue-Extaktor in der oberen rechten Ecke, der mit Jira, dem Issue-Tracking-System, verknüpft sein muss und nur auf Projekte angewendet werden kann, die Jira verwenden. Mit anderen Worten, es kann nicht für viele Projekte auf GitHub verwendet werden. Die zweite ist Griff, das kürzlich im Jahr 2020 angekündigt wurde. Es ist im Internet verfügbar und kann über PIP installiert werden. Dieses System verfügt über ein einfaches, auf dem Lernen basierendes Textklassifizierungsmodell und gibt für jede Eingabe-Commit-Nachricht eine von fünf Kategorien wie Funktionen oder Fehlerbehebungen aus. Das Bild zeigt ein Beispiel für die Verwendung, das eine Korrektur- oder Fehlerbehebungskategorie zurückgibt. Der Trainingsdatensatz ist relativ klein, mit etwa 5.000 Einträgen, und wird in den unten beschriebenen Experimenten gezeigt. Die Leistung des Textklassifizierungsmodells ist nicht hoch. Ich präsentiere zwei verwandte Forschungen, aber es gibt Probleme mit der eingeschränkten Anwendbarkeit und knappen Datenressourcen. Unser Artikel löst diese beiden Probleme und generiert automatisch hochwertige Versionsnotizen. Für das Problem der eingeschränkten Anwendbarkeit schlagen wir eine Methode zur hochwertigen Klassifizierersummarisierung vor, die nur Commit-Nachrichten als Eingabe verwendet. Diese vorgeschlagene Methode kann für alle englischen Repositories verwendet werden. Für das zweite Problem knapper Datenressourcen haben wir einen RNSAM-Datensatz mit etwa 82.000 Datensätzen erstellt, indem wir Daten aus öffentlichen GitHub-Repositories über die GitHub-API gesammelt haben. Als Nächstes beschreibe ich unseren Datensatz. Hier ist ein Beispiel für Daten. Auf der linken Seite befindet sich eine Commit-Nachricht, und auf der rechten Seite stehen die Versionsnotizen. Die Versionsnotizen sind in Kategorien wie Verbesserungen, Fehlerbehebungen usw. eingeteilt. Wir haben eine Aufgabe eingerichtet, die die Commit-Nachrichten als Eingabe nimmt und die kategorisierten Versionsnotizen ausgibt. Dies kann als Summarisierung Aufgabe betrachtet werden. Wir haben vier Ebenen vorgegeben: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecations, Removables und Breaking Changes. Diese wurden auf der Grundlage früherer Forschungen und anderer Faktoren festgelegt. Die Versionsnotizen unten rechts sind aus den Versionsnotizen unten links extrahiert. Zu diesem Zeitpunkt ist es notwendig, die vier im Voraus festgelegten Kategorien zu erkennen, aber die Kategorien sind nicht immer konsistent mit jedem Repository. Beispielsweise umfasst die Kategorie „Verbesserungen“ Verbesserungen, Erweiterungen, Optimierungen usw. Wir haben für jede dieser notationalen Variationen eine Vokabelliste der Kategorien erstellt. Verwenden Sie sie, um die RISNOD-Klasse zu erkennen und den Text der RIS, der folgt, als RISNOD-Satz für die Klasse zu korrigieren. Als Nächstes steht eine Commit-Nachricht an. Commit-Nachrichten sind nicht an jede RIS gebunden. Wie im folgenden Bild gezeigt, müssen wir, wenn die aktuelle RIS von 2.5.19 auf 2.5.18 zurückgeht, die vorherige RISP 2.5.18 identifizieren und deren Diff erhalten. Dies ist etwas mühsam, und es reicht nicht aus, einfach eine Liste der RIS zu erhalten und die vorherigen und nachfolgenden Änderungen zu betrachten. Wir haben eine heuristische Übereinstimmungsmethode entwickelt, um die vorherigen und nächsten Seiten zu erhalten. Am Ende wurden 7.200 Repositories und 82.000 Datensätze korrigiert. Außerdem beträgt die durchschnittliche Anzahl der Versionsnotiz-Token 63, was für eine Summarisierungsaufgabe recht hoch ist. Auch die Anzahl der eindeutigen Token ist mit 8.830.000 recht groß. Dies ist auf die große Anzahl eindeutiger Kosten und Methodenbezeichnungen zurückzuführen, die im Repository gefunden wurden. Als Nächstes werde ich die vorgeschlagene Methode erläutern. Das kreuzweise extraktive und abstrakte Summarisierungsmodell besteht aus zwei neuen Modulen: einem Klassifikator, der BERT oder CodeBERT verwendet, und einem Generator, der BERT verwendet. Zunächst klassifiziert GEAS jede Commit-Nachricht in fünf Kategorien: Funktionen, Verbesserungen, Fehlerbehebungen, Deprecations und andere. Die als „andere“ klassifizierten Commit-Nachrichten werden verworfen. Dann wendet GEAS einen Generator auf die vier Kategoriedokumente unabhängig an und generiert Versionsnotizen für jede Klasse. Bei dieser Aufgabe sind die direkten Korrespondenzen zwischen Commit-Nachrichten und Versionsnotizen nicht bekannt. Daher weisen wir zur Schulung der Klassenkette zwei Kategorien jeder Eingabe-Commit-Nachricht zu, indem wir die ersten zehn Zeichen jeder Commit-Nachricht verwenden. Wir modellieren den abstrakten Summarisierungsansatz der Klassenkette mit zwei verschiedenen Methoden. Das erste Modell, das wir GAS-Single nennen, besteht aus einem einzelnen Seq-to-Seq-Netzwerk und generiert eine einzelne lange Listen-Notiz-Text, indem es die Eingabe-Commit-Nachrichten verkettet. Der Ausgabetext kann basierend auf speziellen, klassenbezogenen Endsymbolen in Klassen unterteilt werden. Die zweite Methode, die wir GSMAUC nennen, besteht aus vier verschiedenen Seq-to-Seq-Netzwerken, von denen jedes einer der Listen-Notiz-Klassen entspricht.\n\nLassen Sie mich nun das Experiment erklären. Es wurden fünf Methoden verglichen: GS, GS-Single, GS-Match, Rustling und die vorherige Studie Griff. Bezüglich der Abtreibung werden diese Notizen in einigen Fällen in mehreren Sätzen ausgegeben. Da es schwierig ist, die Anzahl der Sätze bei Null zu berechnen, werden sie mit Leerzeichen kombiniert und als ein langer Satz behandelt. Das System wird bestraft, wenn es einen kurzen Satz ausgibt. Diese Strafe führt zu einem niedrigeren BLEU-Wert in den nächsten Experimentergebnissen. Schließlich berechnen wir auch die Spezifität, da ROUGE und BLEU nicht berechnet werden können, wenn die Versionsnotizen leer sind. Eine hohe Spezifität bedeutet, dass das Modell in Fällen, in denen die Versionsnotizen leer sein sollten, korrekt leeren Text ausgibt. Hier sind die Ergebnisse. Da der Datensatz E-Mail-Adressen, Hash-Werte usw. enthält, haben wir auch einen grünen Datensatz erstellt, der diese ausschließt. GAS und GAS erreichten ROUGE-Fehlerwerte, die mehr als zehn Punkte über dem Basiswert lagen. Allerdings sprang die Punktedifferenz zwischen der vorgeschlagenen Methode und dem Basiswert im grünen Testdatensatz auf mehr als zwanzig Punkte. Diese Ergebnisse zeigen, dass GAS und GAS signifikant wirksam sind. GAS erzielte einen besseren ROUGE-Score als GAS, was darauf hindeutet, dass die Kombination eines Klassifikators und eines Generators bei der Schulung des Klassifikators unter Verwendung von Pseudodaten wirksam ist. Die hohe Abdeckung von GAS kann wahrscheinlich erreicht werden, da der Klassifikator sich auf die Auswahl relevanter Commit-Nachrichten für jede Klasse konzentrieren kann. Es neigt dazu, höhere Regeln zu erzeugen als es Single ist, was darauf hindeutet, dass es auch wirksam ist, für jede dieser Klassen unterschiedliche Perspektiven-Summarisierungsmodelle unabhängig zu entwickeln. Hier ist eine Fehleranalyse. Es-Methoden neigen dazu, kürzere Sätze als menschliche Referenzsätze auszugeben. Im rechten Bild hat der Referenzsatz drei oder vier Sätze, während es nur einen hat. Der Grund für die Zurückhaltung des Modells ist, dass in den Trainingsdaten nur 33 % der Sätze in der Funktionen-Kategorie und 40 % in der Verbesserungen-Kategorie vorhanden sind. Darüber hinaus können GS-Methoden ohne zusätzliche Informationen keine genauen Versionsnotizen generieren. Das obere Beispiel rechts ist ein Beispiel für eine sehr unordentliche Commit-Nachricht, und der vollständige Satz kann nicht generiert werden, ohne den entsprechenden parallelen Antrag oder die Issue-Beschreibung zu kennen. Das untere Beispiel zeigt, dass die beiden Commit-Nachrichten in der Eingabe zusammenhängen und zu einem Satz kombiniert werden sollten, was jedoch fehlschlägt.\n\nAbschließend haben wir einen neuen Datensatz für die automatische Generierung von Versionsnotizen erstellt. Wir haben auch die Aufgabe definiert, Commit-Nachrichten einzugeben und sie zu summarisieren, sodass sie auf alle in Englisch geschriebenen Projekte anwendbar ist. Unsere Experimente zeigen, dass die vorgeschlagene Methode im Vergleich zur Basislinie weniger rauschhafte Versionsnotizen mit höherer Abdeckung generiert. Bitte sehen Sie sich unseren Datensatz auf GitHub an. Vielen Dank."}
