{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Assa Farari et je vais présenter notre article FUESHOT TABLAR DATA ARCHICHTMENT À L'AIDE D'ARCHITECTURES DE TRANSFORMATEURS AFFINÉS. Les scientifiques des données analysent les données et se concentrent principalement sur la manipulation des caractéristiques existantes des données, mais parfois ces caractéristiques sont limitées. La génération de caractéristiques à partir d'une autre source de données peut ajouter une information substantielle. Notre objectif de recherche est l'enrichissement automatique de données tabulaires à l'aide de sources externes de texte libre. Supposons que nous disposions d'un jeu de données tabulaires et d'une base de connaissances. Nous avons besoin d'un processus automatique qui implique la liaison d'entités et l'analyse de texte pour extraire de nouvelles caractéristiques à partir du texte libre de la base de connaissances. Notre cadre FAST est exactement ce processus automatique. Examinons donc un exemple. Un jeu de données est introduit dans FAST. Dans cet exemple, le jeu de données est un jeu de données universitaires dont l'objectif est de classer les universités en universités mal classées et bien classées. En tant que base de connaissances, nous utilisons Wikipédia. La première phase de FAST est la liaison d'entités, où chaque entité, dans cet exemple, le nom de l'université, est liée à une entité au sein de la base de connaissances, et le texte des entités de la base de connaissances est extrait et ajouté au jeu de données. Dans cet exemple, le texte est l'abstrait de la page Wikipédia. Nous devons maintenant générer ou extraire des caractéristiques à partir du texte récupéré. Nous avons donc besoin d'une phase d'extraction de caractéristiques qui inclut l'analyse de texte, et c'est la principale nouveauté de cet article, et j'y reviendrai en détail dans les diapositives suivantes. Après la phase d'extraction de caractéristiques, il y a une phase de génération de caractéristiques où nous utilisons les caractéristiques extraites pour générer un petit nombre de nouvelles caractéristiques. Premièrement, générez des caractéristiques en fonction du nombre de classes du jeu de données d'origine. Dans cet exemple, le jeu de données d'origine comporte deux classes, générez donc d'abord deux nouvelles caractéristiques. Mais si le jeu de données comporte cinq classes, générez d'abord cinq nouvelles caractéristiques. Chaque caractéristique représente la probabilité pour chaque classe. Pour analyser le texte, nous utilisons l'état actuel de l'art de l'analyse de texte, qui sont les modèles de langage basés sur les transformateurs tels que BERT, GPT, XLERT, etc. Il est peu probable que nous puissions entraîner des modèles de langage à l'aide des jeux de données d'entrée. Une approche naïve serait donc un réglage fin sur la tâche cible. Ainsi, dans la phase d'extraction de caractéristiques, nous pouvons télécharger un modèle de langage pré-entraîné, affiner le modèle de langage sur le jeu de données cible, dans cet exemple, pour affiner le modèle de langage afin de classer le texte en classes, abstrait en classes, bas ou haut, recevoir la sortie du modèle de langage, qui est la probabilité pour chaque classe, et l'utiliser comme nouvelles caractéristiques. Le problème avec cette approche est que les jeux de données peuvent avoir peu d'entités distinctes, de texte. Dans notre expérience, près de la moitié des jeux de données contiennent moins de 400 échantillons, et le plus petit jeu de données contient 35 échantillons dans son ensemble d'entraînement. Affiner un modèle de langage sur ce jeu de données serait donc inefficace. Mais nous pouvons utiliser des connaissances préalables sur des jeux de données pré-analysés parce que FAST, nous appliquons FAST sur plusieurs jeux de données. Nous pouvons utiliser les n-1 jeux de données pour recueillir des informations sur les n-1 jeux de données et utiliser ces informations lorsque nous analysons le nème jeu de données. Ce que nous suggérons, c'est d'ajouter une autre phase d'affinage, une phase d'affinage multitareau préliminaire, lorsque nous affinons le modèle de langage sur n-1 jeux de données, puis nous exécutons une autre phase d'affinage, qui est un affinage sur la tâche cible, lorsque nous affinons le modèle de langage sur le nème jeu de données cible. L'état de l'art dans l'affinage multitareau est appelé DNN vide, dans lequel le DNN vide maintient des têtes au nombre de tâches dans l'ensemble d'entraînement, donc si, dans cet exemple, il y a quatre tâches dans l'ensemble d'entraînement, le DNN vide maintient quatre têtes comme vous pouvez le voir sur l'image, et il échantillonne un lot aléatoire à partir de l'ensemble d'entraînement, et si le lot aléatoire appartient, par exemple, aux tâches de classification de Singapour et Selton, il exécute les chemins avant et arrière à travers la première tête. Et si le lot aléatoire appartient aux tâches de classement par paires, il exécute les chemins avant et arrière à travers la dernière tête. Dans notre scénario, un tableau ou un jeu de données varie le nombre de classes. Il y a donc de nombreuses tâches. Le MTDNN maintient des couches de sortie de têtes au nombre de classes et, en outre, le MTDNN doit initialiser de nouvelles têtes pour un nouveau jeu de données avec une nouvelle tâche. Notre approche, appelée affinage de reformulation de tâche, consiste, au lieu de maintenir plusieurs têtes, à reformuler chaque jeu de données en une phrase par problème de classification, qui est une tâche à deux classes. Examinons un exemple. Voici notre jeu de données d'entrée, qui se compose d'entités, de caractéristiques, de texte et de classes. Et nous reformulons la tâche de classification du texte en bas et haut en classifiant le texte, l'abstrait et la classe en vrai ou faux. Autrement dit, nous entraînons le modèle de langage à classer l'abstrait et la classe en fonction de l'appartenance ou non de l'abstrait à la classe, de sorte que le vecteur d'étiquette dans ce cas reste toujours composé de deux classes, et voici l'algorithme de notre approche d'affinage reformulé. Examinons maintenant le cadre complet, un jeu de données introduit dans FAST, puis FAST exécute la phase de liaison, extrait le texte de la base de connaissances, qui dans cet exemple est l'abstrait de la page Wikipédia, puis reformule la tâche en tâches de classification par phrase, applique le modèle de langage à la nouvelle tâche et fournit la probabilité pour chaque classe. Notez que le modèle de langage est déjà affiné sur n-1 jeux de données à l'aide d'un affinage multitareau préliminaire. Ensuite, nous utilisons le vecteur de sortie du modèle de langage comme nouvelle caractéristique générée au nombre de classes. Pour évaluer notre cadre, nous utilisons un jeu de 17 classifications tabulaires, qui varient en taille, en caractéristiques, en équilibre, en domaine et en performance initiale. Et en tant que base de connaissances, nous utilisons Wikipédia. Nous concevons notre expérience comme une évaluation en laissant une de côté, où nous entraînons FAST sur 16 jeux de données et l'appliquons au 17ème jeu de données. Nous divisons également chaque jeu de données en quatre parties, en appliquant une validation croisée. Ensuite, nous générons la nouvelle caractéristique et les évaluons à l'aide de cinq classifieurs d'évaluation. Nous utilisons dans notre expérience une architecture basée sur BERT. Voici les résultats de notre expérience. Vous pouvez voir que nous comparons notre cadre à l'affinage sur le jeu de données cible, à l'affinage sur la tâche cible et à l'affinage multitareau préliminaire MTDNN, et notre affinage reformulé obtient le meilleur résultat, la meilleure performance, tandis que MTDNN a obtenu une amélioration de 2 % par rapport à l'affinage sur le jeu de données cible. Notre approche a obtenu une amélioration de 6 % lorsque nous examinons les petits jeux de données, nous pouvons voir que la performance de MTDNN diminue et que l'amélioration de la phase d'affinage multitareau préliminaire diminue à 1,5 %, mais notre performance augmente à 11 % par rapport à l'affinage sur la tâche cible seul. En résumé, FAST permet un enrichissement avec peu de données à partir de 35 échantillons dans notre expérience. Il utilise une seule architecture pour toutes les tâches et jeux de données et conserve la tête du modèle. Mais il ajoute une phase de reformulation, augmente l'ensemble d'entraînement et nécessite une valeur cible avec une signification sémantique pour l'introduire dans le modèle de langage et l'utiliser dans le problème de classification par phrase. Merci."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour à tous. Aujourd'hui, je vais présenter notre travail de recherche intitulé « Apprendre à Raisonner Déductivement : Résolution du Problème du Métro comme Extraction de Régions Complexes ». Je suis Alan du Biden's AI Lab, et cette recherche est un travail collaboratif avec Thierry de l'Université du Texas à Austin et Wayloo du SUDD.\n\nTout d'abord, je voudrais aborder notre motivation pour le raisonnement. Voici un exemple où le raisonnement multi-étapes est bénéfique. Cette figure, tirée du papier POWN, illustre l'utilisation de l'inférence pour résoudre le problème du métro dans un scénario d'apprentissage par fusion. Sur le côté gauche, nous voyons que fournir uniquement des questions et réponses peut ne pas permettre d'obtenir les réponses correctes. En revanche, en ajoutant une description du raisonnement, le modèle peut prédire cette description et fournir une réponse correcte. Il est donc avantageux d'avoir un raisonnement multi-étapes interprétable en sortie. Nous pensons également que le problème de méthode est une application directe pour évaluer de telles capacités de raisonnement.\n\nDans notre configuration de problème, étant données les questions, nous devons les résoudre et obtenir des réponses numériques. Nos ensembles de données fournissent également l'expression mathématique conduisant à ces réponses. Certaines hypothèses, similaires à celles des travaux précédents, s'appliquent. Nous supposons que la précision des quantités est connue et nous ne considérons que les opérateurs de base tels que l'addition, la soustraction, la multiplication, la division et l'exponentiation. De plus, les opérateurs complexes peuvent être décomposés en ces opérateurs de base.\n\nLes travaux antérieurs sur la résolution de problèmes de méthode peuvent être classés en deux catégories : modèles séquence-séquence et séquence-arbre. Le modèle séquence-séquence traditionnel convertit l'expression en une séquence spécifique pour la génération, ce qui est facile à implémenter et peut s'adapter à de nombreux problèmes complexes. Cependant, ses inconvénients sont que les performances sont généralement inférieures à celles des modèles structurés et qu'il manque d'interprétabilité dans les prédictions. Cette approche reste néanmoins populaire en raison du modèle transformateur.\n\nDans les modèles basés sur des arbres, nous structurons les expressions sous forme d'arbre et suivons une traversée préfixe lors de la génération des arbres. Nous générons les opérateurs jusqu'à atteindre les feuilles, qui sont les quantités. Cela nous donne une structure d'arbre binaire, mais elle est contre-intuitive car nous générons d'abord les opérateurs, puis les quantités à la fin. De plus, il y a des calculs redondants, comme dans l'expression « a fois trois plus trois » qui est générée deux fois, alors que nous devrions réutiliser les résultats.\n\nDans notre approche proposée, nous voulons résoudre ces problèmes de manière progressive et interprétable. Par exemple, à la deuxième étape, nous pouvons obtenir ce diviseur, qui est 27, et nous pouvons également nous référer à la question originale pour trouver le contenu pertinent. Nous obtenons ainsi les diviseurs. À la troisième étape, nous obtenons le quotient. Après ces trois étapes, nous pouvons réutiliser les résultats de la deuxième étape pour obtenir les résultats de la quatrième étape, puis finalement, nous pouvons calculer les dividendes. Nous générons l'expression complète directement, plutôt que des opérateurs ou quantités isolés, ce qui améliore la précision.\n\nDans notre système déductif, nous commençons avec un ensemble de quantités présentées dans les questions, incluant également des constantes comme états initiaux. L'expression est représentée par EIJOP, où nous appliquons les opérateurs de Qi à Qj, et cette expression est dirigée. Nous avons également la soustraction inversée pour représenter la direction opposée, ce qui est similaire à l'extraction de relations. À l'étape temporelle t dans un système déductif formel, nous appliquons l'opérateur entre la paire Qi et Qj, obtenant ainsi une nouvelle expression. Nous l'ajoutons aux états suivants pour devenir une nouvelle quantité. Cette diapositive visualise l'évolution des états, où nous ajoutons continuellement des expressions aux états courants.\n\nDans l'implémentation de notre modèle, nous utilisons d'abord un modèle de réseau pré-entraîné, tel que BERT ou RoBERTa, puis nous encodons une phrase pour obtenir les représentations des quantités. Une fois ces représentations obtenues, nous pouvons commencer l'inférence. Nous montrons ici un exemple pour Q1 : sa représentation est obtenue en divisant Q1 par Q2 puis en multipliant par Q3. Premièrement, nous obtenons la représentation de la paire, qui est la concaténation de Q1 et Q2, puis nous appliquons un réseau de neurones à propagation directe, paramétré par l'opérateur. Enfin, nous obtenons la représentation de l'expression Q1 divisé par Q2. Cependant, lors de l'inférence, nous pouvons également obtenir des expressions incorrectes. Le nombre total d'expressions possibles est égal à trois fois le nombre d'opérateurs. Nous pouvons facilement ajouter des contraintes pour contrôler cet espace de recherche. Par exemple, si une expression n'est pas autorisée, nous l'éliminons de l'espace de recherche.\n\nÀ la deuxième étape, le processus est similaire, mais avec une quantité supplémentaire, issue de l'expression calculée précédemment. Nous obtenons ainsi l'expression finale Q3 fois Q4. Le nombre d'expressions possibles diffère de l'étape précédente, ce qui rend difficile l'application d'une recherche par faisceau (beam search) en raison de la distribution de probabilité déséquilibrée entre les deux étapes.\n\nLa procédure d'entraînement est similaire à celle d'un modèle séquence-séquence, où nous optimisons la perte à chaque étape temporelle. Nous utilisons également τ pour indiquer la fin du processus de génération. L'espace ici est différent d'un modèle séquence-séquence car il varie à chaque étape temporelle, alors que dans un modèle traditionnel, l'espace est le vocabulaire. Cela nous permet également d'imposer des contraintes basées sur des connaissances préalables.\n\nNous avons mené des expériences sur les ensembles de données de problèmes de méthode couramment utilisés : MAWPS, Math 23K, MathQA et SWAM. Nous présentons ici brièvement les résultats comparés aux approches précédentes les plus performantes. Notre variante la plus efficace est le « Roberta Deductive Reasoner ». Notons que nous n'utilisons pas de recherche par faisceau, contrairement aux approches précédentes. Les meilleures approches sont souvent des modèles basés sur des arbres. Globalement, notre raisonneur surpasse significativement ces modèles basés sur des arbres, mais les scores absolus sur MathQA et SWAM ne sont pas très élevés.\n\nNous avons donc analysé plus en détail les résultats sur SWAM. Cet ensemble de données est complexe car l'auteur a ajouté manuellement des éléments pour tromper les modèles de traitement du langage naturel, comme des informations irrelevantes et des quantités supplémentaires. Dans nos prédictions, nous avons trouvé des valeurs intermédiaires négatives. Par exemple, pour la question « Combien de pommes Jake a-t-il ? », nous avons des informations supplémentaires comme « dix-sept lancers en moins » et « Stephen a huit lancers », qui sont irrelevantes. Notre modèle prédit des valeurs négatives, et nous avons observé une similarité entre ces deux expressions. Nous pouvons limiter l'espace de recherche en éliminant les résultats négatifs, ce qui corrige la réponse. Cette contrainte améliore significativement les performances pour certains modèles, comme BERT (+7 points) et RoBERTa (+2 points). Les meilleurs modèles de langage, avec de meilleures capacités de compréhension, obtiennent des scores plus élevés.\n\nNous avons également tenté d'analyser la difficulté de ces ensembles de données, en supposant que le nombre de quantités non utilisées peut être considéré comme des informations irrelevantes. Nous observons que le pourcentage d'échantillons avec des quantités non utilisées est plus élevé dans SWAM. Les performances globales sur les échantillons sans quantités non utilisées sont supérieures à celles de l'ensemble de données complet. En revanche, les performances sur les échantillons avec quantités non utilisées sont bien inférieures. Pour MAWPS, nous n'avons pas assez de cas pour tirer des conclusions.\n\nEnfin, nous voulons démontrer l'interprétabilité à travers un exemple concret. Dans cet exemple, notre modèle fait une prédiction incorrecte à la première étape. Nous pouvons corréler cette expression avec la phrase correspondante. Nous pensons que cette phrase induit le modèle en erreur. En modifiant la phrase pour clarifier la sémantique, comme « le nombre de poiriers est trente-cinq de moins que le nombre de pommiers », le modèle fournit la bonne prédiction. Cette étude illustre comment des prédictions interprétables aident à comprendre le comportement du modèle.\n\nPour conclure, notre modèle est efficace, fournit une procédure de résolution interprétable, et peut intégrer facilement des contraintes basées sur des connaissances préalables, améliorant ainsi les performances. Le mécanisme sous-jacent s'applique non seulement à la résolution de problèmes de réseau, mais aussi à d'autres tâches impliquant un raisonnement multi-étapes. Cependant, nous avons des limitations : une mémoire élevée pour un grand nombre d'opérateurs ou de constantes, et la difficulté d'appliquer une recherche par faisceau en raison de la distribution de probabilité déséquilibrée. Merci pour votre attention, et n'hésitez pas à poser des questions."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Antoine et je viens de l'Université de Maastricht. Je vais présenter mon travail en collaboration avec Jerry, qui porte sur un nouveau jeu de données pour la récupération d'articles de loi. Les questions juridiques font partie intégrante de la vie de nombreuses personnes, mais la majorité des citoyens ont peu ou pas de connaissances sur leurs droits et les processus légaux fondamentaux. En conséquence, de nombreux citoyens vulnérables qui ne peuvent pas se permettre l'assistance coûteuse d'un expert juridique restent sans protection ou, pire, sont exploités. Ce travail vise à combler le fossé entre les individus et la loi en développant un système de récupération efficace pour les articles de loi. Un tel système pourrait offrir un service d'aide juridique professionnelle gratuite pour les personnes non qualifiées.\n\nAvant d'aborder la principale contribution de ce travail, décrivons le problème de la récupération d'articles de loi. Étant donné une question simple sur un sujet juridique, comme « Quels risques encours-je si je viole le secret professionnel ? », un modèle est requis pour récupérer tous les articles de loi pertinents à partir d'un vaste corpus de législation. Cette tâche de récupération d'informations présente ses propres défis. Premièrement, elle implique deux types de langage : le langage naturel courant pour les questions et le langage juridique complexe pour les statuts. Cette différence dans la distribution des langues rend plus difficile pour un système la récupération de candidats pertinents, car elle nécessite indirectement un système d'interprétation inhérent capable de traduire une question naturelle en une question juridique correspondant à la terminologie des statuts.\n\nDe plus, le droit statutaire n'est pas une simple collection d'articles indépendants qui peuvent être traités comme une source d'information complète en soi, comme les actualités ou les recettes, par exemple. Au contraire, il s'agit d'une collection structurée de dispositions légales qui n'ont de sens complet que lorsqu'elles sont considérées dans leur contexte global, c'est-à-dire avec les informations complémentaires des articles voisins, des domaines et sous-domaines auxquels ils appartiennent, et leur place dans la structure de la loi. Enfin, les articles de loi ne sont pas de petits paragraphes, qui sont généralement l'unité de récupération typique dans la plupart des travaux de récupération. Ici, ce sont de longs documents pouvant atteindre six mille mots.\n\nLes récentes avancées en traitement automatique du langage naturel (TALN) ont suscité un grand intérêt pour de nombreuses tâches juridiques, telles que la prédiction de jugements légaux ou la révision automatisée de contrats, mais la récupération d'articles de loi est restée largement inchangée en raison du manque de grands jeux de données de haute qualité. Dans ce travail, nous présentons un nouveau jeu de données centré sur les citoyens francophones pour étudier si un modèle de récupération peut approcher l'efficacité et la fiabilité d'un expert juridique pour la tâche de récupération d'articles de loi.\n\nNos jeux de données belges de récupération d'articles de loi comprennent plus de mille cent questions juridiques posées par des citoyens belges. Ces questions couvrent un large éventail de sujets, allant de la famille, du logement et de l'argent au travail et à la sécurité sociale. Chacune d'entre elles a été étiquetée par des juristes expérimentés avec des références aux articles pertinents d'un corpus de plus de vingt-deux mille six cents articles juridiques issus des codes de loi belges.\n\nPassons maintenant à la description de la collecte de ce jeu de données. Tout d'abord, nous avons compilé un grand corpus d'articles juridiques. Nous avons considéré trente-deux codes belges publics et extrait tous leurs articles ainsi que les titres de sections correspondants. Ensuite, nous avons rassemblé des questions juridiques avec des références aux statuts pertinents. Pour ce faire, nous avons collaboré avec un cabinet d'avocats belge qui reçoit chaque année environ quatre mille courriels de citoyens belges demandant des conseils sur un problème juridique personnel. Nous avons eu la chance d'obtenir accès à leur site web où leur équipe de juristes expérimentés aborde les problèmes juridiques les plus courants en Belgique. Nous avons collecté des milliers de questions annotées avec des catégories, des sous-catégories et des références juridiques aux statuts pertinents. Enfin, nous avons filtré les références et exclu les questions dont les références n'étaient pas des articles de l'un des codes de loi que nous avons considérés. Les références restantes ont été mises en correspondance et converties en identifiants d'articles correspondants dans notre corpus. Nous avons ainsi obtenu mille cent huit questions, chacune soigneusement étiquetée avec les identifiants des articles pertinents de notre grand corpus de vingt-deux mille six cent trente-trois articles de loi.\n\nEn outre, chaque question est accompagnée d'un contexte formé par la concaténation de ses titres de sous-sections dans la structure de la loi. Cette information supplémentaire n'est pas utilisée dans le présent travail, mais pourrait être intéressante pour des recherches futures sur la récupération d'informations juridiques ou la classification de textes juridiques.\n\nExaminons maintenant quelques caractéristiques de notre jeu de données. Les questions varient de cinq à quarante-quatre mots, avec une médiane de quarante mots. Les articles sont beaucoup plus longs, avec une médiane de soixante-dix-sept mots, et cent quarante-deux d'entre eux dépassent mille mots. Les plus longs atteignent jusqu'à cinq mille sept cent quatre-vingt-dix mots. Comme mentionné précédemment, les questions couvrent un large éventail de sujets, avec environ quatre-vingt-cinq pour cent d'entre elles portant sur la famille, le logement, l'argent ou la justice, tandis que les quinze pour cent restants concernent la sécurité sociale, les étrangers ou le travail. Les articles sont également très divers, car ils proviennent de trente-deux codes belges différents couvrant un grand nombre d'articles juridiques. Sur les 22 633 articles, seuls 1 612 sont cités comme pertinents pour au moins une question du jeu de données, et environ quatre-vingts pour cent de ces articles cités proviennent du code civil, du code judiciaire, du code d'instruction criminelle ou du code pénal. Pendant ce temps, dix-huit des trente-deux codes ont moins de cinq articles mentionnés comme pertinents pour au moins une question, ce qui peut s'expliquer par le fait que ces codes se concentrent moins sur les individus et leurs préoccupations. Dans l'ensemble, le nombre médian de citations pour ces articles cités est de deux, et moins de vingt-cinq pour cent d'entre eux sont cités plus de cinq fois.\n\nEn utilisant nos jeux de données, nous avons évalué plusieurs approches de récupération, y compris les architectures lexicales et denses. Étant donné une requête et un article, un modèle lexical attribue un score à la paire requête-article en calculant la somme des poids de chaque terme de la requête dans cet article. Nous avons expérimenté avec les fonctions de classement TF-IDF et BM 25 standard. Le principal problème de ces approches est qu'elles ne peuvent récupérer que les articles contenant des mots-clés présents dans la requête.\n\nPour surmonter cette limitation, nous avons expérimenté une architecture basée sur les réseaux neuronaux capable de capturer les relations sémantiques entre les requêtes et les articles. Nous utilisons un modèle B encodeur qui mappe les requêtes et les articles en représentations vectorielles denses et calcule un score de pertinence entre une paire requête-article en fonction de la similarité de leurs incorporations. Ces incorporations résultent généralement d'une opération de regroupement sur la sortie d'un modèle d'incorporation de mots.\n\nPremièrement, nous étudions l'efficacité des encodeurs B siamois dans un contexte d'évaluation zéro-tir, ce qui signifie que les modèles d'incorporation de mots pré-entraînés sont appliqués directement, sans aucun réglage supplémentaire. Nous avons expérimenté avec des encodeurs de texte indépendants du contexte, à savoir Word2Vec et FastText, et des modèles d'incorporation dépendants du contexte, à savoir RoBERTa et plus spécifiquement CamemBERT, qui est un modèle RoBERTa en français. De plus, nous avons entraîné notre propre modèle basé sur CamemBERT, appelé Biancoders, sur l'ensemble des données. Notez que pour l'entraînement, nous avons expérimenté avec les deux variantes de l'architecture Biancoder : siamois, qui utilise un seul modèle d'incorporation de mots pour mapper la requête et l'article ensemble dans un espace vectoriel dense partagé, et deux tours, qui utilise deux modèles d'incorporation de mots indépendants pour encoder séparément la requête et l'article dans des espaces d'incorporation différents. Nous avons expérimenté avec le regroupement moyen, max et CLS, ainsi qu'avec le produit scalaire et la similarité cosinus pour calculer les similarités.\n\nVoici les résultats de nos baselines sur les ensembles de test : les méthodes lexicales ci-dessus, les encodeurs B siamois évalués dans un contexte zéro-tir au milieu, et les encodeurs B réglés en dessous. Dans l'ensemble, les encodeurs B réglés surclassent considérablement toutes les autres baselines. Le modèle à deux tours améliore son variant siamois en termes de rappel à cent, mais performe de manière similaire sur les autres métriques. Bien que BM 25 ait sous-performé le Biancoder entraîné de manière significative, ses performances indiquent qu'il s'agit toujours d'une baseline solide pour la récupération spécifique au domaine.\n\nEn ce qui concerne l'évaluation zéro-tir des encodeurs Biancoder siamois, nous constatons que l'utilisation directe des incorporations d'un modèle CamemBERT pré-entraîné, sans optimisation pour la tâche de récupération d'informations, donne des résultats médiocres, ce qui est cohérent avec les découvertes précédentes. De plus, l'encodeur Biancoder basé sur Word2Vec a significativement surperformé les modèles basés sur FastText et Bird, suggérant que les incorporations de mots pré-entraînées au niveau du mot pourraient être plus appropriées pour cette tâche que les incorporations au niveau des caractères ou des sous-mots lorsqu'elles sont utilisées directement.\n\nBien que prometteurs, ces résultats suggèrent une marge d'amélioration par rapport à un expert humain qualifié qui peut éventuellement récupérer tous les articles pertinents pour n'importe quelle question et obtenir ainsi des scores parfaits.\n\nConcluons en discutant de deux limitations de nos jeux de données. Premièrement, le corpus d'articles est limité à ceux collectés à partir des trente-deux codes belges considérés, ce qui ne couvre pas l'ensemble de la loi belge, car les articles des décrets, directives et ordonnances manquent. Pendant la construction du jeu de données, toutes les références à ces articles non collectés ont été ignorées, ce qui fait que certaines questions se retrouvent avec seulement une fraction du nombre initial d'articles pertinents. Cette perte d'information implique que la réponse contenue dans les articles pertinents restants pourrait être incomplète, bien qu'elle soit toujours tout à fait appropriée.\n\nDeuxièmement, il est important de noter que toutes les questions juridiques ne peuvent pas être répondues uniquement avec des statuts. Par exemple, la question « Puis-je expulser mes locataires s'ils font trop de bruit ? » pourrait ne pas avoir de réponse détaillée dans la loi statutaire quantifiant un seuil de bruit spécifique au-delà duquel l'expulsion est autorisée. Au lieu de cela, le propriétaire devrait probablement s'appuyer davantage sur la jurisprudence et trouver des précédents similaires à sa situation actuelle. Par exemple, le locataire organise deux fêtes par semaine jusqu'à 2 heures du matin. Par conséquent, certaines questions sont mieux adaptées que d'autres à la tâche de récupération d'articles de loi, et le domaine de celles moins adaptées reste à déterminer.\n\nNous espérons que ce travail suscitera un intérêt pour le développement de modèles de récupération d'articles de loi pratiques et fiables qui peuvent aider à améliorer l'accès à la justice pour tous. Vous pouvez consulter notre article, le jeu de données et le code aux liens suivants. Merci."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, nous sommes ravis de présenter notre travail sur VALS, une référence de test indépendante de la tâche destinée à évaluer les modèles de vision et de langage avec des phénomènes linguistiques spécifiques. Pourquoi avons-nous entrepris la création de cette référence ? Ces dernières années, nous avons assisté à une explosion des modèles de vision et de langage basés sur les transformateurs, pré-entraînés sur de grandes quantités de paires image-texte. Chacun de ces modèles améliore l'état de l'art dans des tâches de vision et de langage telles que la réponse à des questions visuelles, le raisonnement de bon sens visuel, la récupération d'images, l'ancrage de phrases. Cependant, si les précisions sur ces références spécifiques augmentent, savons-nous réellement ce que les modèles ont appris ? Qu'est-ce qu'un transformateur de vision et de langage comprend lorsqu'il attribue une note élevée à cette image et cette phrase pour les associer, et une note faible pour une autre ? Les modèles de vision et de langage se concentrent-ils sur les éléments pertinents ou sont-ils influencés par des biais, comme le montrent des travaux antérieurs ?\n\nPour éclairer ce point, nous proposons une approche plus indépendante de la tâche et introduisons VALS, qui teste la sensibilité des modèles de vision et de langage à des phénomènes linguistiques spécifiques affectant à la fois les modalités linguistique et visuelle. Nous ciblons l'existence, la pluralité, le comptage, les relations spatiales, les actions et la coréférence d'entités.\n\nMais comment tester si les modèles de vision et de langage ont saisi ces phénomènes ? Nous utilisons le procédé du « foiling », précédemment appliqué aux modèles de vision et de langage uniquement pour les phrases nominales par Ravi Shakar et collaborateurs, et pour le comptage par nos soins dans un travail antérieur. Le foiling consiste à prendre la légende d'une image et à produire un « foil » en modifiant la légende de manière à ce qu'elle ne décrive plus l'image. Nous effectuons ces altérations de phrases en nous concentrant sur six éléments spécifiques : existence, pluralité, comptage, relations spatiales, actions et coréférence d'entités. Chaque élément peut comporter un ou plusieurs instruments, au cas où nous aurions trouvé plusieurs façons intéressantes de créer des instances de foil. Par exemple, dans le cas des actions, nous avons deux instruments : l'un dans lequel le verbe d'action est remplacé par un autre, et l'autre dans lequel les actants sont échangés. Le comptage et la coréférence sont également des éléments comportant plusieurs instruments.\n\nNous créons ces foils en veillant à ce qu'ils ne décrivent pas l'image, qu'ils soient grammaticalement corrects et autrement valides. Cela n'est pas aisé car une légende altérée peut être moins probable que la légende originale. Par exemple, bien que ce ne soit pas impossible, il est statistiquement moins probable que des plantes coupent un homme que l'inverse, et les grands modèles de vision et de langage pourraient détecter ce biais.\n\nPour obtenir des foils valides, nous devons donc prendre des mesures. Premièrement, nous utilisons des modèles linguistiques puissants pour proposer des foils. Deuxièmement, nous employons l'inférence en langage naturel (NLI) pour éliminer les foils qui pourraient encore décrire l'image. Lors de la construction des foils, nous devons en effet nous assurer qu'ils ne décrivent pas l'image. Pour tester cela automatiquement, nous appliquons l'inférence en langage naturel avec le raisonnement suivant : nous considérons l'image comme la prémisse et sa légende comme l'hypothèse qui en découle. De plus, nous considérons la légende comme la prémisse et le foil comme son hypothèse. Si un modèle NLI prédit que le foil contredit ou est neutre par rapport à la légende, nous l'interprétons comme un indicateur de foil valide. S'il est supposé être une conséquence de la légende, il ne peut pas être un bon foil, car par transitivité, il fournira une description véridique de l'image, et nous éliminons ces foils.\n\nCependant, cette procédure n'est pas parfaite. Il s'agit simplement d'un indicateur pour les foils valides. Par conséquent, en tant que troisième mesure pour générer des foils valides, nous faisons appel à des annotateurs humains pour valider les données utilisées dans VALS.\n\nAprès filtrage et évaluation humaine, nous obtenons autant d'instances de test que décrites dans ce tableau. Notez que VALS ne fournit aucune donnée d'entraînement, mais uniquement des données de test, car il s'agit d'une référence de test sans apprentissage. Il est conçu pour exploiter les capacités existantes des modèles de vision et de langage après pré-entraînement. L'ajustement fin n'aurait pour effet que de permettre aux modèles d'exploiter des artefacts ou des biais statistiques dans les données, et nous savons tous que ces modèles aiment tricher et prendre des raccourcis. Or, comme nous l'avons dit, nous souhaitons évaluer les capacités des modèles de vision et de langage après le pré-entraînement.\n\nNous expérimentons avec cinq modèles de vision et de langage sur VALS, à savoir CLIP, WiLBERT, WiLBERT-KELVIN-1 et VisualBERT. Deux de nos principales métriques d'évaluation sont la précision des modèles dans la classification des paires image-phrase en légendes et foils. Peut-être plus pertinent pour cette vidéo, nous présenterons notre métrique plus permissive, la précision paire à paire, qui mesure si le score d'alignement image-phrase est plus élevé pour la bonne paire image-texte que pour sa paire altérée. Pour plus de métriques et de résultats, veuillez consulter notre article.\n\nLes résultats avec la précision paire à paire montrent, de manière cohérente avec ceux obtenus par les autres métriques, que les meilleures performances sans apprentissage sont atteintes par WiLBERT-12-en-1, suivi de WiLBERT, AlexMert, CLIP et enfin VisualBERT. Il est remarquable que les instruments centrés sur des objets individuels, comme l'existence et les phrases nominales, soient presque résolus par WiLBERT-12-en-1, ce qui met en évidence la capacité des modèles à identifier des objets nommés et leur présence dans les images. Cependant, aucun des autres éléments ne peut être résolu de manière fiable dans nos conditions de foiling adversarial.\n\nNous constatons, à partir des instruments de pluralité et de comptage, que les modèles de vision et de langage ont du mal à distinguer les références à des objets uniques ou multiples, ou à les compter dans une image. L'élément de relation montre qu'ils ont des difficultés à classer correctement une relation spatiale nommée entre des objets dans une image. Ils ont également du mal à distinguer les actions et à identifier leurs participants, même lorsqu'ils sont soutenus par des biais de plausibilité, comme nous le voyons dans l'élément actions. À partir de l'élément coréférence, nous découvrons qu'il est également difficile pour les modèles de vision et de langage de suivre des références multiples au même objet dans une image en utilisant des pronoms.\n\nÀ titre de contrôle et parce que c'est une expérience intéressante, nous avons également évalué deux modèles de texte uniquement, GPT-1 et GPT-2, pour déterminer si VALS peut être résolu par ces modèles unimodaux en calculant la perplexité de la légende correcte et altérée (sans image dans ce cas) et en prédisant l'entrée avec la perplexité la plus faible. Si la perplexité est plus élevée pour le foil, nous interprétons cela comme une indication que la légende altérée peut souffrir d'un biais de plausibilité ou d'autres biais linguistiques. Il est intéressant de noter que, dans certains cas, les modèles de texte uniquement GPT ont mieux saisi la plausibilité du monde que les modèles de vision et de langage.\n\nEn résumé, VALS est une référence qui utilise le prisme des constructions linguistiques pour aider la communauté à améliorer les modèles de vision et de langage en testant rigoureusement leurs capacités d'ancrage visuel. Nos expériences montrent que les modèles de vision et de langage identifient bien les objets nommés et leur présence dans les images, comme le démontre l'élément existence, mais qu'ils ont du mal à ancrer leur interdépendance et leurs relations dans des scènes visuelles lorsqu'ils sont contraints de respecter des indicateurs linguistiques. Nous encourageons vivement la communauté à utiliser VALS pour mesurer les progrès vers l'ancrage linguistique avec les modèles de vision et de langage. De plus, VALS pourrait servir d'évaluation indirecte des ensembles de données, car les modèles pourraient être évalués avant et après l'entraînement ou l'ajustement fin pour voir si un ensemble de données les aide à s'améliorer dans l'un des aspects testés par VALS.\n\nSi vous êtes intéressé, consultez les données VALS sur GitHub, et n'hésitez pas à nous contacter si vous avez des questions."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "fr", "output": "Bonjour, je m'appelle Kamisara de l'Université de Tokyo. Je vais présenter un article intitulé R et SAM, un ensemble de données à grande échelle pour la génération automatique de notes de version, sans durée, via la synthèse des journaux de validation. Je vais expliquer cela dans l'ordre suivant. Premièrement, je vais introduire la génération automatique de notes de version sur laquelle porte notre recherche. Une note de version est un document technique qui résume les modifications distribuées avec chaque version d'un produit logiciel. L'image montre les notes de version pour la version 2.6.4 de la bibliothèque GBUJS. Ces notes jouent un rôle crucial dans le développement open source, mais leur préparation manuelle est chronophage. Il serait donc très utile de pouvoir générer automatiquement des notes de version de haute qualité.\n\nJe vais faire référence à deux recherches précédentes sur la génération automatique de notes de version. La première est un système appelé Arena, publié en 2014. Il utilise une approche basée sur des règles, par exemple, en employant un extracteur de changements pour extraire les différences principales dans les modifications de bibliothèque et de documentation à partir des différences entre les versions, puis en les combinant. La caractéristique la plus notable de ce système est l'extracteur de problèmes dans le coin supérieur droit, qui doit être lié à Jira, le écosystème de suivi de problèmes, et ne peut être appliqué qu'aux projets utilisant Jira. En d'autres termes, il ne peut pas être utilisé pour de nombreux projets sur GitHub.\n\nLe second est Griff, annoncé récemment en 2020. Il est disponible sur Internet et peut être installé via PIP. Ce système possède un modèle de classification de texte simple basé sur l'apprentissage et produit l'un des cinq types de notes suivants pour chaque message de validation en entrée : fonctionnalités, corrections de bugs, etc. L'image illustre un exemple d'utilisation qui retourne une note de corrections de bugs. Les données d'entraînement de Queryface sont relativement petites, environ cinq mille, et seront présentées dans les expériences décrites ci-dessous. La performance du modèle de classification de texte n'est pas élevée.\n\nJe présente deux recherches connexes, mais il y a des problèmes de faible applicabilité et de ressources de données limitées. Notre article résout ces deux problèmes et génère automatiquement des notes de version de haute qualité. Pour le problème de l'applicabilité limitée, nous proposons une méthode de synthèse de classificateur de haute qualité utilisant uniquement les messages de validation comme entrée. Cette méthode proposée peut être utilisée pour tous les dépôts en anglais. Pour le second problème, celui des ressources de données limitées, nous avons constitué un ensemble de données RNSAM composé d'environ quatre-vingt-deux mille données en collectant des données à partir de dépôts GitHub publics en utilisant l'API GitHub.\n\nJe vais maintenant décrire notre ensemble de données. Voici un exemple de données. À gauche se trouve un message de validation et à droite, les notes de version. Les notes de version sont classées en améliorations, corrections de bugs, etc. Nous avons défini une tâche qui prend les messages de validation comme entrée et produit les notes de version classées. Cela peut être considéré comme une tâche de synthèse. Nous avons prédéfini quatre niveaux : fonctionnalités, améliorations, corrections de bugs, dépréciations, éléments supprimables et changements rompus. Ces niveaux ont été définis sur la base de recherches précédentes et d'autres facteurs. Les notes de version en bas à droite sont extraites des notes de version présentées en bas à gauche. À ce stade, il est nécessaire de détecter les quatre types de notes prédéfinis, mais ces types ne sont pas toujours cohérents d'un dépôt à l'autre. Par exemple, le type améliorations inclut les améliorations, les enrichissements, les optimisations, etc. Nous avons préparé une liste de vocabulaire des types de notes pour chacune de ces variations. Nous l'utilisons pour détecter la classe RISNOD et corriger le texte du RIS suivant en tant que phrase RISNOD pour cette classe.\n\nEnsuite, nous avons les messages de validation. Les messages de validation ne sont pas liés à chaque RIS. Comme le montre l'image ci-dessous, si le RIS actuel est la version 2.5.19, nous devons identifier le RIS précédent, 2.5.18, et obtenir sa différence (diff). C'est un peu fastidieux et il ne suffit pas de simplement obtenir une liste de RIS et de regarder les différences. Nous avons créé une correspondance heuristique pour obtenir les versions précédente et suivante. À la fin de l'analyse, sept mille deux cents dépôts et quatre-vingt-deux mille données ont été corrigés. De plus, le nombre moyen de jetons de notes de version est de soixante-trois, ce qui est assez élevé pour une tâche de synthèse. Le nombre de jetons uniques est également très élevé, avec huit mille huit cent trente mille. Cela est dû au grand nombre de noms de coûts et de méthodes uniques trouvés dans le dépôt.\n\nJe vais maintenant expliquer la méthode proposée. Le modèle de synthèse extractive et abstractive croisée se compose de deux modules nouveaux : un classificateur utilisant Bot ou Code Bot, et un générateur utilisant Bot. Premièrement, GEAS utilise un classificateur pour classer chaque message de validation en cinq classes de raisons : fonctionnalités, améliorations, corrections de bugs, dépréciations, plus d'autres. Les messages de validation classés comme autres sont discardés. Ensuite, GEAS applique un générateur aux quatre documents de notes de version indépendamment et génère des notes de version pour chaque classe. Dans cette tâche, les correspondances directes entre les messages de validation et les notes de version ne sont pas connues. Par conséquent, pour entraîner le fil de classe, nous attribuons deux types de notes à chaque message de validation en entrée en utilisant les dix premiers caractères de chaque message de validation. Nous modélisons l'approche de synthèse abstractive du fil de classe par deux méthodes différentes. Le premier modèle, que nous appelons GAS single, se compose d'un seul réseau sect à sect et génère une seule note de liste longue en concaténant les messages de validation en entrée. Le texte de sortie peut être divisé en classes par segments basés sur des symboles d'extrémité spécifiques à chaque classe. La deuxième méthode, que nous appelons GSMAUC, se compose de quatre réseaux sect à sect différents, chacun correspondant à l'une des classes de notes de liste.\n\nPassons maintenant à l'expérience. Cinq méthodes ont été comparées : GS, GS single, GS march, rustling, et l'étude précédente grief. Concernant l'avortement, dans certains cas, ces notes sont sorties sous forme de plusieurs phrases. Comme il est difficile de calculer le nombre de phrases à zéro, elles sont combinées avec des espaces et traitées comme une seule phrase longue. La pénalité de brew est appliquée lorsque le système sort une phrase courte. Cette pénalité entraîne une valeur de brew plus faible dans les résultats d'expérience décrits ci-dessous. Enfin, nous calculons également la spécificité car Rouge et Brew ne peuvent pas être calculés si les notes de version sont vides. Une spécificité élevée signifie que le modèle sort correctement un texte vide dans les cas où les notes de version sont supposées vides.\n\nVoici les résultats. Puisque l'ensemble de données contient des adresses e-mail, des valeurs de hachage, etc., nous avons également éradiqué l'ensemble de données vert, qui les exclut. GAS et GAS ont obtenu des scores Rouge supérieurs de plus de dix points par rapport à la ligne de base. Cependant, sur l'ensemble de test vert, l'écart de score entre la méthode proposée et la ligne de base a grimpé à plus de vingt points. Ces résultats indiquent que GAS et GAS sont significativement efficaces. GAS a obtenu un meilleur score de loose que GAS, suggérant qu'une combinaison d'un classificateur et d'un générateur est efficace lors de l'entraînement du classificateur en utilisant des pseudobus. La couverture élevée de GAS peut être atteinte probablement parce que le classificateur peut se concentrer sur la sélection des messages de validation pertinents pour chaque classe.\n\nIl tend à produire des règles plus élevées que lorsqu'il est seul, suggérant qu'il est également efficace de développer indépendamment différents modèles de synthèse de perspective pour chacune de ces classes de notes. Voici une analyse des erreurs. Les méthodes GAS ont tendance à sortir des phrases plus courtes que les phrases de référence humaines. Dans la figure de droite, la phrase de référence comporte trois ou quatre phrases, tandis que GAS n'en a qu'une. La raison de cette réticence du modèle est que, dans les données d'entraînement, seulement trente-trois pour cent des phrases sont présentes dans le type de note fonctionnalités et quarante pour cent dans le type améliorations. De plus, les méthodes GS ne peuvent pas générer de notes de version précises sans informations supplémentaires. L'exemple du haut à droite est un exemple de message de validation très désordonné et la phrase complète ne peut pas être générée sans différence par rapport à la demande ou au problème parallèle correspondant. L'exemple du dessous montre que les deux messages de validation en entrée sont liés et devraient être combinés en une seule phrase, mais il échoue à le faire.\n\nEn conclusion, nous avons constitué un nouvel ensemble de données pour la génération automatique de notes de version. Nous avons également formulé la tâche consistant à entrer des messages de validation et à les résumer de manière à ce qu'elle soit applicable à tous les projets rédigés en anglais. Nos expériences montrent que la méthode proposée génère des notes de version moins bruyantes avec une couverture plus élevée que la ligne de base. Veuillez consulter notre ensemble de données sur GitHub. Merci."}
