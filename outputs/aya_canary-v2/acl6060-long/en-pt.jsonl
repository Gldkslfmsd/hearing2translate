{"dataset_id": "acl_6060", "sample_id": 416, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Assa Farari e apresentarei nosso artigo FUESHOT TABULAR DATA ARCHITECTURA Utilizando ARQUITETURAS de TRANSFORMADORES FINAMENTE Ajustados. Os cientistas de dados analisam dados e se concentram principalmente na manipulação das características existentes dos dados, mas às vezes essas características são limitadas. A geração de características usando outra fonte de dados pode adicionar informações significativas. Nosso objetivo de pesquisa é o enriquecimento automático de dados tabulares usando fontes externas de texto livre. Suponha que tenhamos um conjunto de dados tabulares e uma base de conhecimento. Precisamos de um processo automático que envolva a vinculação de entidades e a análise de texto para extrair novas características do texto livre da base de conhecimento. Nosso framework FAST é exatamente esse processo automático. Vamos ver um exemplo. Um conjunto de dados é alimentado no FAST. Neste exemplo, o conjunto de dados é um conjunto de dados de universidades, cujo objetivo é classificar universidades em universidades de baixo ranking e universidades de alto ranking. Como base de conhecimento, usamos a Wikipédia. A primeira fase do FAST é a vinculação de entidades, quando cada entidade, neste exemplo, o nome da universidade, é vinculada a uma entidade dentro da base de conhecimento, e o texto das entidades da base de conhecimento é extraído e adicionado ao conjunto de dados. Neste exemplo, o texto é o resumo da página da Wikipédia. Agora, precisamos gerar ou extrair características do texto recuperado. Então, precisamos de uma fase de extração de características que inclua análise de texto, e essa é a principal novidade deste artigo, e aprofundarei isso nas próximas slides. Após a fase de extração de características, há uma fase de geração de características, na qual usamos as características extraídas para gerar um pequeno número de novas características. Primeiro, geramos características no número de classes do conjunto de dados original. Neste exemplo, o conjunto de dados original tem duas classes, então primeiro geramos duas novas características. Mas se o conjunto de dados tiver cinco classes, primeiro geramos cinco novas características. Cada característica representa a probabilidade para cada classe. Para analisar o texto, usamos o estado da arte atual da análise de texto, que são modelos de linguagem baseados em transformadores, como BERT, GPT, XLNet, etc. No entanto, não é provável que possamos treinar modelos de linguagem usando os conjuntos de dados de entrada. Então, uma abordagem ingênua seria o ajuste fino da tarefa-alvo. Assim, na fase de extração de características, podemos baixar um modelo de linguagem pré-treinado, ajustá-lo finamente ao conjunto de dados-alvo, neste exemplo, ajustar o modelo de linguagem para classificar texto em classes, resumos em classes, baixo ou alto, receber a saída do modelo de linguagem, que é a probabilidade para cada classe, e usá-la como novas características. O problema com essa abordagem é que os conjuntos de dados podem ter poucas entidades distintas, texto. Em nossa experiência, quase metade dos conjuntos de dados contém menos de 400 amostras, e o menor conjunto de dados contém 35 amostras em seu conjunto de treinamento. Então, ajustar um modelo de linguagem a esse conjunto de dados seria ineficaz. Mas podemos usar o conhecimento prévio sobre conjuntos de dados pré-analisados porque o FAST é aplicado a vários conjuntos de dados. Podemos usar os n-1 conjuntos de dados para coletar informações sobre os n-1 conjuntos de dados e usar essas informações quando analisamos o n-ésimo conjunto de dados. O que sugerimos é adicionar outra fase de ajuste fino, uma fase de ajuste fino multitarefa preliminar, quando ajustamos finamente o modelo de linguagem nos n-1 conjuntos de dados e, em seguida, executamos outra fase de ajuste fino, que é o ajuste fino da tarefa-alvo, quando ajustamos finamente o modelo de linguagem no n-ésimo conjunto de dados-alvo. O estado da arte no ajuste fino multitarefa é chamado de MTDNN (Multi-Task Deep Neural Network), no qual o MTDNN mantém cabeças no número de tarefas no conjunto de treinamento, então, se neste exemplo houver quatro tarefas no conjunto de treinamento, o MTDNN mantém quatro cabeças, como você pode ver na imagem, e ele amostra um lote aleatório do conjunto de treinamento, e se o lote aleatório pertencer a tarefas de classificação de canto, executa os caminhos de ida e volta através da primeira cabeça. E se o lote aleatório pertencer a tarefas de classificação par a par, executa os caminhos de ida e volta através da última cabeça. Em nosso cenário, uma tabela ou conjunto de dados varia o número de classes. Então, há muitas tarefas. O MTDNN mantém camadas de saída de cabeças no número de classes e, além disso, o MTDNN precisa inicializar novas cabeças para um novo conjunto de dados com uma nova tarefa. Nossa abordagem, chamada de ajuste fino de reformulação de tarefas, em vez de manter múltiplas cabeças, reformula cada conjunto de dados em uma sentença por problema de classificação, que é uma tarefa de duas classes. Vamos ver um exemplo. Aqui está nosso conjunto de dados de entrada, que consiste em entidades, características, texto e classes. E reformulamos a tarefa de classificar o texto em baixo e alto para classificar o texto, o resumo e a classe em verdadeiro ou falso. Ou, em outras palavras, treinamos o modelo de linguagem para classificar o resumo e a classe se o resumo pertence à classe ou não, de modo que o vetor de rótulo neste caso sempre consiste em duas classes, e este é o algoritmo para nossa abordagem de ajuste fino reformulado. Vamos ver o framework completo, um conjunto de dados alimentado no FAST e, em seguida, o FAST executa a fase de vinculação, extrai o texto da base de conhecimento, que neste exemplo é o resumo da página da Wikipédia, então reformula a tarefa em tarefas de classificação de sentenças, aplica o modelo de linguagem à nova tarefa e gera a probabilidade para cada classe. Observe que o modelo de linguagem já foi ajustado finamente nos n-1 conjuntos de dados usando um ajuste fino multitarefa preliminar. Em seguida, usamos o vetor de saída do modelo de linguagem como uma característica gerada recentemente no número de classes. Para avaliar nosso framework, usamos um conjunto de dados de classificação tabular de 17 tabelas, que varia em tamanho, características, equilíbrio, domínio e desempenho inicial. E, como base de conhecimento, usamos a Wikipédia. Projetamos nossa experiência como uma avaliação de deixar um de fora, na qual treinamos o FAST em 16 conjuntos de dados e o aplicamos ao 17º conjunto de dados. Também dividimos cada conjunto de dados em quatro partes, aplicamos uma validação cruzada de quatro partes e, em seguida, geramos a nova característica e as avaliamos usando cinco classificadores de avaliação. Usamos, em nossa experiência, uma arquitetura baseada em BERT. Aqui estão os resultados de nossa experiência. Você pode ver que comparamos nosso framework com o ajuste fino do conjunto de dados-alvo, o ajuste fino da tarefa-alvo e o MTDNN com ajuste fino preliminar, e nosso ajuste fino reformulado alcançou o melhor resultado, o melhor desempenho, enquanto o MTDNN alcançou uma melhoria de 2% em relação ao ajuste fino do conjunto de dados-alvo. Nossa abordagem alcançou uma melhoria de 6% quando olhamos para o pequeno conjunto de dados, podemos ver que o desempenho do MTDNN diminui e a melhoria da fase de ajuste fino multitarefa preliminar diminui para 1,5%, mas nosso desempenho aumentou para 11% em comparação com o ajuste fino da tarefa-alvo sozinho. Em resumo, o FAST permite o enriquecimento de poucos tiros a partir de 35 amostras em nossa experiência. Ele usa uma arquitetura para todas as tarefas e conjuntos de dados e mantém a cabeça do modelo. Mas ele adiciona uma fase de reformulação, aumenta o conjunto de treinamento e precisa de um valor-alvo com significado semântico para alimentá-lo no modelo de linguagem e usá-lo no problema de classificação de sentenças. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 417, "src_lang": "en", "tgt_lang": "pt", "output": "Olá a todos. Hoje vou apresentar o nosso trabalho de pesquisa, \"Aprender a Raciocinar Dedutivamente: Resolução de Problemas do Metro como Extração de Regiões Complexas\". Sou Alan, do Biden's AI Lab, e este é um trabalho conjunto com Thierry, da Universidade do Texas em Austin, e Wayloo, do SUDD.\n\nPrimeiramente, gostaria de falar sobre a nossa motivação para o raciocínio. Aqui, apresentamos um exemplo onde o raciocínio multietapa é útil. Esta figura é extraída do artigo POWN, onde eles utilizam prompt para resolver o problema do metro em um cenário de aprendizagem de fusão. No lado esquerdo, podemos ver que, se fornecermos apenas exemplos com perguntas e respostas, podemos não obter as respostas corretas. Mas, se adicionarmos uma descrição de raciocínio, o modelo consegue prever a descrição e também fazer uma previsão correta. Portanto, é benéfico ter um raciocínio multietapa interpretável como saída. Também consideramos que o problema do método é uma aplicação direta para avaliar tais habilidades de raciocínio.\n\nNo nosso setup do problema, dado as perguntas, precisamos resolvê-las e obter respostas numéricas. Nos nossos conjuntos de dados, também fornecemos a expressão matemática que leva a essa resposta. Certas suposições também se aplicam, como em trabalhos anteriores. Assumimos que a precisão das quantidades é conhecida e consideramos apenas operadores básicos, como adição, subtração, multiplicação, divisão e exponenciação. Além disso, operadores complexos podem ser decodificados nesses operadores básicos.\n\nTrabalho anterior na resolução de problemas do método pode ser categorizado em modelos sequência-sequência e sequência-árvore. O modelo sequência-sequência tradicional converte a expressão em uma sequência específica para geração, é fácil de implementar e pode generalizar para muitos problemas complexos. No entanto, as desvantagens são que o desempenho geralmente não é melhor que o modelo estrutural e falta interpretabilidade nas previsões. Ainda assim, essa abordagem é bastante popular devido ao modelo transformador.\n\nEm modelos baseados em árvores, estruturamos as expressões em forma de árvore e seguimos uma travessia em pré-ordem na geração de árvores. Continuamos gerando operadores até alcançarmos as folhas, que são as quantidades. A vantagem é que obtemos uma estrutura de árvore binária. No entanto, é contra-intuitivo, pois geramos o operador primeiro e, só no final, as quantidades. Além disso, há computações repetitivas. Na expressão \"a vezes três mais três\", por exemplo, ela é gerada duas vezes, mas deveríamos reutilizar o resultado.\n\nNa nossa abordagem proposta, queremos resolver esses problemas de forma passo a passo e interpretável. Por exemplo, na segunda etapa, podemos obter o divisor, que é 27, e também podemos voltar às perguntas originais para encontrar o conteúdo relevante. Nessas etapas, obtemos os divisores. Na terceira etapa, obtemos o quociente. Após essas três etapas, reutilizamos os resultados da segunda etapa e obtemos os resultados da quarta etapa. Finalmente, podemos obter os dividendos. Geramos a expressão completa diretamente, em vez de gerar operadores ou quantidades individuais, tornando o processo mais preciso.\n\nNo nosso sistema dedutivo, começamos com um conjunto de quantidades apresentadas nas perguntas, incluindo algumas constantes como estados iniciais. A expressão é representada por EIJOP, onde realizamos operadores de Qi a Qj, e essa expressão é direcionada. Também temos a subtração invertida para representar a direção oposta, semelhante à extração de relações. Em um sistema dedutivo formal, no passo de tempo t, aplicamos o operador entre a dupla Qi e Qj, obtendo uma nova expressão. Adicionamos isso aos estados seguintes para se tornar uma nova quantidade. Esta slide visualiza a evolução dos estados, onde continuamos adicionando expressões aos estados atuais.\n\nNa implementação do modelo, primeiro usamos um modelo de rede pré-treinado, que pode ser BERT ou RoBERTa, e codificamos uma sentença para obter as representações das quantidades. Uma vez obtidas as representações, podemos começar a inferir. Aqui, mostramos um exemplo de Q1 para obter a representação de Q1, que será dividido por Q2 e multiplicado por Q3. Primeiro, obtemos a representação do par, que é a concatenação de Q1 e Q2, e então aplicamos uma rede feedforward, parametrizada pelo operador. Finalmente, obtemos a representação da expressão Q1 dividido por Q2. Na prática, na etapa de inferência, também podemos obter expressões incorretas. Todas as expressões possíveis são iguais a três vezes o número de operadores. Podemos facilmente adicionar restrições para controlar esse espaço de busca. Se uma expressão não for permitida, removemos-a do nosso espaço de busca.\n\nNa segunda etapa, fazemos o mesmo, mas a diferença é que há mais uma quantidade, que vem da expressão calculada anteriormente. Finalmente, obtemos a expressão final Q3 vezes Q4. Também podemos ver que o número de expressões possíveis é diferente do passo anterior, o que torna difícil aplicar a busca em feixe devido à distribuição de probabilidade desequilibrada entre os passos.\n\nO procedimento de treinamento é semelhante ao de um modelo sequência-sequência, onde otimizamos a perda em cada passo de tempo. Aqui, também usamos τ para representar quando devemos terminar o processo de geração. O espaço é diferente do modelo sequência-sequência tradicional, pois varia em cada passo de tempo, enquanto no modelo tradicional é o número de vocabulário. Também permite impor restrições de conhecimento prévio.\n\nRealizamos experimentos nos conjuntos de dados de problemas do método comumente usados: MAWPS, Math 23K, MathQA e SWAM. Aqui, mostramos brevemente os resultados em comparação com as abordagens anteriores. Nossa variante de melhor desempenho é o RoBERTa Deductive Reasoner. Em contraste com abordagens anteriores que usam busca em feixe, não utilizamos essa técnica. As melhores abordagens são frequentemente modelos baseados em árvores. No geral, nosso raciocínio supera significativamente o modelo baseado em árvores, mas os números absolutos em MathQA ou SWAM não são muito altos.\n\nInvestigamos mais a fundo os resultados em SWAM. Este conjunto de dados é desafiador porque o autor tentou adicionar manualmente informações para confundir o modelo de NLP, como informações irrelevantes e quantidades extras. Em nossas previsões, encontramos valores intermediários negativos. Por exemplo, na pergunta \"Quantas maçãs Jake tem?\", há informações extras como \"dezessete menos que as jogadas\" e \"Stephen tem oito jogadas\", totalmente irrelevantes. Nosso modelo prevê valores negativos. Observamos que essas duas expressões têm similaridade. Podemos limitar o espaço de busca removendo resultados negativos, garantindo uma resposta correta. Essa restrição melhora significativamente para alguns modelos, como BERT, com um aumento de sete pontos, e para o modelo RoBERTa base, com um aumento de dois pontos. Modelos de linguagem melhores têm melhor capacidade de compreensão, então o número é mais alto para RoBERTa e mais baixo para BERT.\n\nTambém analisamos a dificuldade por trás desses conjuntos de dados. Assumimos que a quantidade de quantidades não utilizadas pode ser considerada informação irrelevante. Aqui, vemos a porcentagem de amostras com quantidades não utilizadas, e o conjunto de dados SWAM tem a maior parte. Também mostramos o desempenho geral para amostras sem quantidades não utilizadas, que é mais alto que o desempenho geral. Mas, para amostras com quantidades não utilizadas, o desempenho é muito pior que o geral. Em MAWPS, não há muitos casos, então ignorei essa parte.\n\nPor fim, queremos demonstrar a interpretabilidade através de um exemplo de erro e correção. Aqui, nosso modelo faz uma previsão errada no primeiro passo. Podemos correlacionar essa expressão com a sentença. Acreditamos que a sentença pode estar induzindo o modelo a uma previsão incorreta. A frase \"plantando mais trinta e cinco\" faz o modelo pensar que deve haver um operador adicional. Revisamos a sentença para algo como \"o número de pereiras é trinta e cinco a menos que o de macieiras\", transmitindo semântica mais precisa, permitindo que o modelo faça a previsão correta. Este estudo mostra como previsões interpretáveis nos ajudam a entender o comportamento do modelo.\n\nPara concluir nosso trabalho, primeiro, nosso modelo é bastante eficiente e fornece um procedimento de solução interpretável. Podemos incorporar facilmente conhecimento prévio como restrição, o que melhora o desempenho. Por último, o mecanismo subjacente não se aplica apenas à resolução de problemas de rede, mas também a outras tarefas que envolvem raciocínio multietapa. No entanto, há limitações: se houver muitos operadores ou constantes, o consumo de memória pode ser alto. E, como mencionado, a distribuição de probabilidade desequilibrada em diferentes passos de tempo torna desafiador aplicar a estratégia de busca em feixe.\n\nObrigado pela atenção. Estou aberto a perguntas."}
{"dataset_id": "acl_6060", "sample_id": 418, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Antoine e sou da Universidade de Maastricht. Apresentarei meu trabalho conjunto com Jerry, que trata de um novo conjunto de dados para recuperação de artigos estatutários. Questões legais fazem parte da vida de muitas pessoas, mas a maioria dos cidadãos tem pouco ou nenhum conhecimento sobre seus direitos e processos legais fundamentais. Como resultado, muitos cidadãos vulneráveis que não podem arcar com o custooso auxílio de um especialista legal ficam desprotegidos ou, pior, são explorados. Este trabalho visa preencher a lacuna entre as pessoas e a lei, desenvolvendo um sistema de recuperação eficaz para artigos estatutários. Tal sistema poderia fornecer um serviço gratuito de ajuda legal profissional para humanos não qualificados.\n\nAntes de mergulharmos na principal contribuição deste trabalho, vamos descrever primeiro o problema da recuperação de artigos estatutários. Dada uma pergunta simples sobre um assunto legal, como \"O que eu risco se violar o sigilo profissional?\", é necessário um modelo para recuperar todos os artigos estatutários relevantes de um grande corpo de legislação. Esta tarefa de recuperação de informações traz seus próprios desafios. Primeiro, lida com dois tipos de linguagem: a linguagem natural comum para as perguntas e a linguagem legal complexa para os estatutos. Essa diferença na distribuição da linguagem torna mais difícil para um sistema recuperar candidatos relevantes, pois indiretamente requer um sistema de interpretação inerente que possa traduzir uma pergunta natural para uma pergunta legal que corresponda à terminologia dos estatutos.\n\nAlém disso, o direito estatutário não é uma pilha de artigos independentes que podem ser tratados como uma fonte completa de informação por si só, como notícias ou receitas, por exemplo. Em vez disso, é uma coleção estruturada de disposições legais que têm um significado completo apenas quando consideradas no contexto geral, ou seja, juntamente com as informações suplementares dos artigos vizinhos, os campos e subcampos aos quais pertencem e seu lugar na estrutura da lei. Por fim, os artigos estatutários não são pequenos parágrafos, que geralmente é a unidade de recuperação típica na maioria dos trabalhos de recuperação. Aqui, eles são documentos longos que podem ter até seis mil palavras.\n\nOs recentes avanços no Processamento de Linguagem Natural (PLN) despertaram grande interesse em muitas tarefas legais, como a previsão de julgamentos legais ou a revisão automatizada de contratos, mas a recuperação de artigos estatutários permaneceu em grande parte intocada devido à falta de grandes conjuntos de dados rotulados de alta qualidade. Neste trabalho, apresentamos um novo conjunto de dados centrado em cidadãos franceses para estudar se um modelo de recuperação pode aproximar a eficiência e confiabilidade de um especialista legal na tarefa de recuperação de artigos estatutários.\n\nNossos conjuntos de dados de recuperação de artigos estatutários belgas consistem em mais de mil e cem perguntas legais feitas por cidadãos belgas. Essas perguntas abrangem uma ampla gama de tópicos, desde família, habitação e dinheiro até trabalho e segurança social. Cada uma delas foi rotulada por juristas experientes com referências a artigos relevantes de um corpus de mais de vinte e dois mil e seiscentos artigos legais de códigos legais belgas.\n\nVamos agora falar sobre como coletamos este conjunto de dados. Primeiro, compilamos um grande corpus de artigos legais. Consideramos trinta e dois códigos belgas publicamente disponíveis e extraímos todos os seus artigos, bem como os títulos das seções correspondentes. Em seguida, reunimos perguntas legais com referências a estatutos relevantes. Para isso, estabelecemos uma parceria com um escritório de advocacia belga que recebe cerca de quatro mil e-mails por ano de cidadãos belgas que solicitam aconselhamento sobre um problema legal pessoal. Tivemos a sorte de obter acesso aos seus sites, onde sua equipe de juristas experientes aborda as questões legais mais comuns na Bélgica. Coletamos milhares de perguntas anotadas com categorias, subcategorias e referências legais a estatutos relevantes. Por fim, passamos as referências legais e excluímos as perguntas cujas referências não eram artigos em um dos códigos de lei que consideramos. As referências restantes foram correspondidas e convertidas para os IDs de artigo correspondentes do nosso corpus. Acabamos com mil e cem perguntas, cada uma cuidadosamente rotulada com os IDs dos artigos relevantes do nosso grande corpus de vinte e dois mil e seiscentos e trinta e três artigos estatutários. Além disso, cada pergunta vem com um título que é uma concatenação de suas subseções na estrutura da lei. Essa informação extra não é usada no presente trabalho, mas pode ser de interesse para futuras pesquisas sobre recuperação de informações legais ou classificação de textos legais.\n\nVejamos algumas características do nosso conjunto de dados. As perguntas têm entre cinco e quarenta e quatro palavras de comprimento, com uma mediana de quarenta palavras. Os artigos são muito mais longos, com uma mediana de setenta e sete palavras, sendo que cento e quarenta e dois deles excedem mil palavras. Os mais longos têm até cinco mil setecentas e noventa palavras. Como mencionado anteriormente, as perguntas abrangem uma ampla gama de tópicos, com cerca de oitenta e cinco por cento delas sendo sobre família, habitação, dinheiro ou justiça, enquanto os quinze por cento restantes dizem respeito à segurança social, estrangeiros ou trabalho. Os artigos também são muito diversos, pois vêm de trinta e dois códigos belgas diferentes que cobrem um grande número de artigos ilegais coletados de cada um desses códigos belgas. Dos 22.633 artigos, apenas 1.612 são referidos como relevantes para pelo menos uma pergunta nos conjuntos de dados, e cerca de 80% desses artigos citados vêm do Código Civil, Código Judiciário, Código de Investigação Criminal ou Código Penal. Enquanto isso, 18 dos 32 códigos têm menos de 5 artigos mencionados como relevantes para pelo menos uma pergunta, o que pode ser explicado pelo fato de que esses códigos se concentram menos em indivíduos e suas preocupações. No geral, o número médio de citações para esses artigos citados é dois, e menos de vinte e cinco por cento deles são citados mais de cinco vezes.\n\nUsando nossos conjuntos de dados, comparamos várias abordagens de recuperação, incluindo arquitetura lexical e densa. Dada uma consulta em um artigo, um modelo lexical atribui uma pontuação à dupla consulta-artigo calculando a soma sobre os termos da consulta das ponderações de cada um desses termos naquele artigo. Experimentamos com as funções de classificação TF-IDF e BM-25. O principal problema com essas abordagens é que elas só podem recuperar artigos que contenham palavras-chave presentes na consulta.\n\nPara superar essa limitação, experimentamos com uma arquitetura baseada em rede neural que pode capturar a relação semântica entre consultas e artigos. Usamos um modelo B encoder que mapeia consultas e artigos em representações vetoriais densas e calcula uma pontuação de relevância entre uma dupla consulta-artigo pela similaridade de suas incorporações. Essas incorporações geralmente resultam de uma operação de agrupamento na saída de um modelo de incorporação de palavras. Primeiro, estudamos a eficácia de B encoders siameses em um setup de avaliação zero-shot, o que significa que modelos de incorporação de palavras pré-treinados são aplicados diretamente, sem qualquer ajuste fino adicional. Experimentamos com codificadores de texto independentes do contexto, nomeadamente Word2Vec e FastText, e modelos de incorporação dependentes do contexto, nomeadamente Roberta e, mais especificamente, Camembert, que é um modelo Roberta em francês. Além disso, treinamos nosso próprio modelo Biancoders baseado em Camembert em todos os conjuntos de dados. Observe que, para o treinamento, experimentamos com as duas variantes da arquitetura Biancoder: siamesa, que usa um único modelo de incorporação de palavras que mapeia a consulta e o artigo juntos em um espaço vetorial denso compartilhado; e duas torres, que usa dois modelos de incorporação de palavras independentes que codificam a consulta e o artigo separadamente em diferentes espaços de incorporação. Experimentamos com agrupamento médio, máximo e CLS, bem como produto escalar e cosseno para calcular similaridades.\n\nAqui estão os resultados dos nossos modelos de referência nos conjuntos de teste com os métodos lexicais mencionados acima, os B encoders siameses avaliados em um setup zero-shot no meio e os B encoders ajustados finamente abaixo. No geral, os B encoders ajustados finamente superam significativamente todos os outros modelos de referência. O modelo de duas torres melhora em relação à sua variante siamesa no recall em cem, mas desempenha de forma semelhante nos outros métricos. Embora o BM-25 tenha apresentado desempenho inferior ao Biancoder treinado significativamente, seu desempenho indica que ainda é um modelo de referência forte para recuperação específica de domínio.\n\nQuanto à avaliação zero-shot dos B encoders siameses Biancoder, descobrimos que o uso direto das incorporações de um modelo Camembert pré-treinado sem otimização para a tarefa de recuperação de informações resulta em resultados pobres, o que é consistente com descobertas anteriores. E o Biancoder baseado em Word2Vec superou significativamente os modelos baseados em FastText e Bird, sugerindo que, talvez, as incorporações de palavras pré-treinadas em nível de palavra sejam mais adequadas para a tarefa do que as incorporações em nível de caractere ou subpalavra quando usadas diretamente.\n\nEmbora promissores, esses resultados sugerem uma ampla oportunidade de melhoria em comparação com um especialista legal qualificado que pode recuperar todos os artigos relevantes para qualquer pergunta e, assim, obter pontuações perfeitas.\n\nVamos concluir discutindo duas limitações dos nossos conjuntos de dados. Primeiro, o corpus de artigos é limitado àqueles coletados dos trinta e dois códigos belgas considerados, o que não cobre toda a lei belga, pois artigos de decretos, diretivas e ordenanças estão ausentes. Durante a construção do conjunto de dados, todas as referências a esses artigos não coletados são ignoradas, o que faz com que algumas perguntas acabem com apenas uma fração do número inicial de artigos relevantes. Essa perda de informação implica que a resposta contida nos artigos relevantes restantes pode ser incompleta, embora ainda seja completamente apropriada.\n\nEm segundo lugar, devemos observar que nem todas as perguntas legais podem ser respondidas apenas com estatutos. Por exemplo, a pergunta \"Posso despejar meus inquilinos se eles fizerem muito barulho?\" pode não ter uma resposta detalhada dentro do direito estatutário que quantifique um limite específico de ruído no qual a despejo é permitido. Em vez disso, o proprietário provavelmente deve confiar mais na jurisprudência e encontrar precedentes semelhantes à sua situação atual. Por exemplo, o inquilino faz duas festas por semana até as 2h da manhã. Portanto, algumas perguntas são mais adequadas do que outras para a tarefa de recuperação de artigos estatutários, e o domínio das menos adequadas ainda precisa ser determinado.\n\nEsperamos que este trabalho desperte interesse no desenvolvimento de modelos práticos e confiáveis de recuperação de artigos estatutários que possam ajudar a melhorar o acesso à justiça para todos. Você pode conferir nosso artigo, conjunto de dados e código nos seguintes links. Obrigado."}
{"dataset_id": "acl_6060", "sample_id": 419, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, temos o prazer de apresentar nosso trabalho sobre VALS, um marco de referência independente da tarefa destinado a testar modelos de visão e linguagem com fenômenos linguísticos específicos. Por que nos demos ao trabalho de criar este marco de referência? Bem, nos últimos anos, testemunhamos uma explosão de modelos de visão e linguagem baseados em transformadores, pré-treinados com grandes quantidades de pares de imagens e textos. Cada um desses modelos eleva o estado da arte em tarefas de visão e linguagem, como resposta a perguntas visuais, raciocínio de senso comum visual, recuperação de imagens, ancoragem de frases. No entanto, apesar do aumento constante na precisão nesses marcos de referência específicos, sabemos realmente o que os modelos aprenderam? O que um transformador de visão e linguagem entende ao atribuir uma alta pontuação para combinar esta imagem e esta frase e uma baixa pontuação para outra? Os modelos de visão e linguagem se concentram no aspecto correto ou em vieses, como mostrado por trabalhos anteriores?\n\nPara lançar mais luz sobre esse aspecto, propomos uma abordagem mais independente da tarefa e introduzimos VALS, que testa a sensibilidade de modelos de visão e linguagem a fenômenos linguísticos específicos que afetam tanto as modalidades linguística quanto visual. Nosso foco é a existência, pluralidade, contagem, relações espaciais, ações e coreferência de entidades. Mas como testar se os modelos de visão e linguagem capturaram esses fenômenos?\n\nUtilizamos o método de \"foiling\" (engodo), anteriormente aplicado para modelos de visão e linguagem apenas para frases nominais por Ravi Shakar e colaboradores, e para contagem em nosso trabalho anterior. \"Foiling\" significa que pegamos a legenda de uma imagem e produzimos um \"foil\" (engodo) alterando a legenda de forma que ela não descreva mais a imagem. Fazemos essas alterações nas frases concentrando-nos em seis aspectos específicos: existência, pluralidade, contagem, relações espaciais, ações e coreferência de entidades. Cada aspecto pode consistir em um ou mais \"instrumentos\", caso tenhamos encontrado mais de uma maneira interessante de criar instâncias de \"foil\". Por exemplo, no caso das ações, temos dois \"instrumentos\": um em que o verbo da ação é alterado por uma ação diferente e outro em que os agentes são trocados. Contagem e coreferência também são aspectos que possuem mais de um \"instrumento\".\n\nCriamos esses \"foils\" (engodos) garantindo que eles falhem em descrever a imagem, que sejam gramaticalmente corretos e sentenças válidas. Isso não é fácil de fazer, pois uma legenda \"foiled\" (enganada) pode ser menos provável que a original. Por exemplo, embora não seja impossível, é estatisticamente menos provável que plantas cortem um homem do que um homem corte plantas, e modelos de visão e linguagem grandes podem perceber isso. Portanto, para obter \"foils\" válidos, devemos tomar algumas providências. Primeiro, utilizamos modelos de linguagem fortes para propor \"foils\". Em segundo lugar, usamos inferência de linguagem natural (NLI) para filtrar \"foils\" que ainda poderiam descrever a imagem, pois, ao construir os \"foils\", precisamos garantir que eles falhem em descrever a imagem.\n\nPara testar isso automaticamente, aplicamos inferência de linguagem natural com a seguinte lógica. Consideramos uma imagem como a premissa e sua legenda como a hipótese implicada. Além disso, consideramos a legenda como a premissa e o \"foil\" como sua hipótese. Se um modelo de NLI prever que o \"foil\" contradiz ou é neutro em relação à legenda, tomamos isso como um indicador de um \"foil\" válido. Se ele for implicado pela legenda, não pode ser um bom \"foil\", pois, por transitividade, fornecerá uma descrição verdadeira da imagem, e filtramos esses \"foils\". No entanto, esse procedimento não é perfeito. É apenas um indicador de \"foils\" válidos, portanto, como terceira medida para gerar \"foils\" válidos, empregamos anotadores humanos para validar os dados usados no VALS.\n\nApós a filtragem e a avaliação humana, temos tantas instâncias de teste quanto descritas nesta tabela. Observe que o VALS não fornece nenhum dado de treinamento, apenas dados de teste, pois é um marco de referência de teste de zero disparo. Ele é projetado para aproveitar as capacidades existentes de modelos de visão e linguagem após o pré-treinamento. O ajuste fino apenas permitiria que os modelos explorassem artefatos ou vieses estatísticos nos dados, e todos sabemos que esses modelos gostam de trapacear e pegar atalhos. Como dissemos, estamos interessados em avaliar as capacidades que os modelos de visão e linguagem possuem após o pré-treinamento.\n\nExperimentamos com cinco modelos de visão e linguagem no VALS, a saber: CLIP, Wilbert, Wilbert Kelvin 1 e Visual Bert. Duas de nossas métricas de avaliação mais importantes são a precisão dos modelos na classificação de pares de imagem e frase em legendas e \"foils\". Talvez mais relevante para este vídeo, apresentaremos nossa métrica mais permissiva, a precisão par a par, que mede se a pontuação de alinhamento imagem-frase é maior para o par imagem-texto correto do que para seu par \"foiled\". Para mais métricas e resultados, consulte nosso artigo.\n\nOs resultados com precisão par a par são consistentes com os obtidos pelas outras métricas, mostrando que o melhor desempenho de zero disparo é alcançado pelo Wilbert 12 em 1, seguido por Wilbert, Alex Mert, CLIP e, finalmente, Visual Bird. É notável como os \"instrumentos\" centrados em objetos individuais, como existência e frases nominais, são quase resolvidos pelo Wilbert 12 em 1, destacando que os modelos são capazes de identificar objetos nomeados e sua presença em imagens. No entanto, nenhum dos aspectos restantes pode ser resolvido de forma confiável em nossas configurações de \"foiling\" adversarial.\n\nObservamos, a partir dos \"instrumentos\" de pluralidade e contagem, que os modelos de visão e linguagem têm dificuldade em distinguir referências a objetos únicos em comparação com múltiplos objetos ou em contá-los em uma imagem. O aspecto de relação mostra que eles têm dificuldades em classificar corretamente uma relação espacial nomeada entre objetos em uma imagem. Eles também têm problemas em distinguir ações e identificar seus participantes, mesmo quando apoiados por vieses de plausibilidade, como visto no aspecto de ações. A partir do aspecto de coreferência, descobrimos que rastrear múltiplas referências ao mesmo objeto em uma imagem usando pronomes também é difícil para modelos de visão e linguagem.\n\nComo um controle de sanidade e porque é um experimento interessante, também benchmarking dois modelos de texto apenas GPT 1 e GPT 2 para avaliar se o VALS é solucionável por esses modelos unimodais, calculando a perplexidade da legenda correta e do \"foil\" (sem imagem aqui) e prevendo a entrada com a menor perplexidade. Se a perplexidade for maior para o \"foil\", tomamos isso como uma indicação de que a legenda \"foiled\" pode sofrer de viés de plausibilidade ou outros vieses linguísticos. É interessante notar que, em alguns casos, os modelos de texto apenas GPT capturaram a plausibilidade do mundo melhor do que os modelos de visão e linguagem.\n\nEm resumo, o VALS é um marco de referência que utiliza a lente de construções linguísticas para ajudar a comunidade a melhorar os modelos de visão e linguagem, testando rigorosamente suas capacidades de ancoragem visual. Nossos experimentos mostram que os modelos de visão e linguagem identificam bem objetos nomeados e sua presença em imagens, como demonstrado pelo aspecto de existência, mas lutam para ancorar sua interdependência e relacionamentos em cenas visuais quando forçados a respeitar indicadores linguísticos. Gostaríamos muito de incentivar a comunidade a usar o VALS para medir o progresso em direção à ancoragem de linguagem com modelos de visão e linguagem. E mais ainda, o VALS poderia ser usado como uma avaliação indireta de conjuntos de dados, já que os modelos poderiam ser avaliados antes e depois do treinamento ou ajuste fino para ver se um conjunto de dados ajuda os modelos a melhorar em qualquer um dos aspectos testados pelo VALS. Se você estiver interessado, confira os dados do VALS no GitHub, e se tiver alguma dúvida, não hesite em entrar em contato conosco."}
{"dataset_id": "acl_6060", "sample_id": 420, "src_lang": "en", "tgt_lang": "pt", "output": "Olá, meu nome é Kamisara da Universidade de Tóquio. Apresentarei um artigo intitulado R e SAM, um conjunto de dados em larga escala para a geração automática de notas de lançamento, sem foco na duração, por meio da sumarização de logs de commit. Seguirei a seguinte ordem: primeiro, introduzirei a geração automática de notas de lançamento sem foco na duração, na qual estamos trabalhando nesta pesquisa.\n\nUma nota de lançamento é um documento técnico que resume as alterações distribuídas com cada versão de um produto de software. A imagem mostra as notas de lançamento para a versão 2.6.4 da biblioteca GBUJS. Esses nós desempenham um papel importante no desenvolvimento de código aberto, mas são demorados para serem preparados manualmente. Portanto, seria muito útil poder gerar automaticamente notas de lançamento de alta qualidade.\n\nFarei referência a duas pesquisas anteriores sobre a geração automática de notas de lançamento. A primeira é um sistema chamado Arena, lançado em 2014. Ele adota uma abordagem baseada em regras, por exemplo, usando um extrator de alterações para extrair as principais diferenças entre as alterações da biblioteca e os documentos a partir das diferenças entre as versões, combinando-as finalmente. A característica mais notável desse sistema é o extrator de problemas no canto superior direito, que deve ser vinculado ao Jira, o ecossistema de problemas, e pode ser aplicado apenas a projetos que utilizam o Jira. Em outras palavras, não pode ser usado para muitos projetos no GitHub.\n\nO segundo é Griff, anunciado recentemente em 2020. Está disponível na internet e pode ser instalado via PIP. Este sistema possui um modelo simples de classificação de texto baseado em aprendizado e produz um dos cinco tipos de notas, como recursos ou correções de bugs, para cada mensagem de commit de entrada. A imagem é um exemplo de uso que retorna uma nota de correção ou correção de bugs. Os dados de treinamento do Queryface são relativamente pequenos, cerca de cinco mil, e serão apresentados nos experimentos descritos abaixo. O desempenho do modelo de classificação de texto não é alto.\n\nApresento duas pesquisas relacionadas, mas existem problemas de aplicabilidade limitada e recursos de dados escassos. Nosso artigo resolveu esses dois problemas e gera automaticamente notas de lançamento de alta qualidade. Para o problema de aplicabilidade limitada, propomos um método de sumarização de classificadores de alta qualidade usando apenas mensagens de commit como entrada. Este método proposto pode ser usado para todos os repositórios em inglês.\n\nPara o segundo problema de recursos de dados escassos, construímos um conjunto de dados RNSAM composto por cerca de oitenta e dois mil dados, coletando dados de repositórios públicos do GitHub usando a API do GitHub. Em seguida, descrevo nosso conjunto de dados. Aqui está um exemplo de dado. O lado esquerdo é uma mensagem de commit e o lado direito são as notas de lançamento. As notas de lançamento são classificadas em níveis como melhorias, correções de bugs, etc. Configuramos uma tarefa que recebe as mensagens de commit como entrada e produz as notas de lançamento classificadas. Isso pode ser considerado uma tarefa de sumarização.\n\nPredefinimos quatro níveis: recursos, melhorias, correções de bugs, depreciações, removíveis e alterações significativas. Esses níveis foram definidos com base em pesquisas anteriores e outros fatores. As notas de lançamento na parte inferior direita são extraídas das notas de lançamento mostradas na parte inferior esquerda. Neste momento, é necessário detectar os quatro tipos de notas predefinidos, mas os tipos de notas nem sempre são consistentes com cada repositório. Por exemplo, o tipo de nota \"melhorias\" inclui melhorias, aprimoramentos, otimizações, etc. Preparamos uma lista de vocabulário de tipos de notas de estudo para cada uma dessas variações notacionais. Use-a para detectar a classe RISNOD e corrigir o texto do RIS que se segue como a frase RISNOD para a classe.\n\nA próxima etapa é uma mensagem de commit. As mensagens de commit não estão vinculadas a cada RIS. Como mostrado na imagem abaixo, se o RIS atual for da versão 2.5 para 19, é necessário identificar o RIS anterior 2.5 para 18 e obter sua diferença (diff). Isso é um pouco tedioso e não é suficiente apenas obter uma lista de RIS e verificar o antes e depois. Criamos uma correspondência heurística para obter o anterior e o próximo.\n\nApós a análise, corrigimos 7.200 repositórios e 82 mil dados. Além disso, o número médio de tokens de notas de lançamento é 63, o que é bastante alto para uma tarefa de sumarização. O número de tokens exclusivos também é bastante grande, 8.830.000. Isso se deve ao grande número de custos exclusivos e nomes de métodos encontrados no repositório.\n\nEm seguida, explicarei o método proposto. O modelo de sumarização extrativa e abstrativa entrelaçada consiste em dois módulos novos: um classificador usando Bot ou Code Bot e um gerador usando Bot. Primeiro, o GEAS usa um classificador para classificar cada mensagem de commit em cinco classes de razões: recursos, melhorias, correções de bugs, depreciações e outros. As mensagens de commit classificadas como outros são descartadas. Em seguida, o GEAS aplica um gerador aos quatro documentos de tipos de notas independentemente e gera notas de razões para cada classe.\n\nNesta tarefa, as correspondências diretas entre as mensagens de commit e as notas de razões não são conhecidas. Portanto, para treinar o fio de classe, atribuímos dois tipos de notas a cada mensagem de commit de entrada usando os primeiros dez caracteres de cada mensagem de commit. Modelamos a abordagem de sumarização abstrativa do fio de classe por dois métodos diferentes. O primeiro modelo, que chamamos de GAS single, consiste em uma única rede de seção a seção e gera uma única nota de lista longa, concatenando as mensagens de commit de entrada. O texto de saída pode ser dividido em classes por segmento com base em símbolos de extremidade específicos de classe.\n\nO segundo método, que chamamos de GSMAUC, consiste em quatro redes diferentes de seção a seção, cada uma correspondente a uma das classes de notas de lista. Bem, vamos explicar o experimento. Cinco métodos foram comparados: GS, GS single, GS march, rustling e a pesquisa anterior grief. Em relação à interrupção, em alguns casos, essas notas são saída em várias frases. Como é difícil calcular o número de frases em zero, elas são combinadas com espaços e tratadas como uma única frase longa. A penalidade é aplicada quando o sistema produz uma frase curta. Essa penalidade resulta em um valor de Brew mais baixo nos resultados do experimento descritos a seguir.\n\nFinalmente, também calculamos a especificidade, pois Rouge e Brew não podem ser calculados se as notas de lançamento estiverem vazias. Uma alta especificidade significa que o modelo produz corretamente um texto vazio em casos em que as notas de lançamento assumem um valor vazio. Aqui estão os resultados. Como o conjunto de dados contém endereços de e-mail, valores de hash, etc., também eliminamos o conjunto de dados verde, que os exclui. O GAS e o GAS alcançaram pontuações de Rouge com mais de dez pontos a mais que a linha de base. No entanto, no conjunto de teste verde, a lacuna de pontuação entre o método proposto e o final da base saltou para mais de vinte pontos.\n\nEsses resultados indicam que o GAS e o GAS são significativamente eficazes. O GAS obteve uma pontuação de Brew melhor que o GAS, sugerindo que a combinação de um classificador e um gerador é eficaz no treinamento do classificador usando pseudobus. A alta cobertura do GAS pode ser alcançada provavelmente porque o classificador pode se concentrar na seleção de mensagens de commit relevantes para cada classe. Ele tende a produzir pontuações mais altas que o GAS single, sugerindo que também é eficaz desenvolver independentemente diferentes modelos de sumarização para cada classe de notas.\n\nAqui está uma análise de erros. Os métodos GAS tendem a produzir frases mais curtas do que as frases de referência humana. Na figura à direita, a frase de referência tem três ou quatro frases, enquanto o GAS tem apenas uma. A razão para a relutância do modelo é que, nos dados de treinamento, apenas 33% das frases estão presentes no tipo de nota \"recursos\" e 40% no tipo de nota \"melhorias\". Além disso, os métodos GAS não podem gerar notas de razões precisas sem informações adicionais. O exemplo superior à direita é um exemplo de uma mensagem de commit muito confusa, e a frase completa não pode ser gerada sem diferença em relação ao pedido ou problema paralelo correspondente.\n\nO exemplo abaixo mostra que as duas mensagens de commit na entrada estão relacionadas e devem ser combinadas em uma única frase, mas ele falha nisso. Conclusão: construímos um novo conjunto de dados para geração automática de razões. Também formulamos a tarefa de inserir mensagens de commit e sumarizá-las para que seja aplicável a todos os projetos escritos em inglês. Nossos experimentos mostram que o método proposto gera razões menos ruidosas com maior cobertura do que a linha de base. Por favor, confira nosso conjunto de dados no GitHub. Obrigado."}
